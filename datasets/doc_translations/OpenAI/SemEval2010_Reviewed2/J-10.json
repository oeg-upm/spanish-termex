{
    "id": "J-10",
    "original_text": "Understanding User Behavior in Online Feedback Reporting Arjun Talwar Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland arjun@math.stanford.edu Radu Jurca Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland radu.jurca@epfl.ch Boi Faltings Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland boi.faltings@epfl.ch ABSTRACT Online reviews have become increasingly popular as a way to judge the quality of various products and services. Previous work has demonstrated that contradictory reporting and underlying user biases make judging the true worth of a service difficult. In this paper, we investigate underlying factors that influence user behavior when reporting feedback. We look at two sources of information besides numerical ratings: linguistic evidence from the textual comment accompanying a review, and patterns in the time sequence of reports. We first show that groups of users who amply discuss a certain feature are more likely to agree on a common rating for that feature. Second, we show that a users rating partly reflects the difference between true quality and prior expectation of quality as inferred from previous reviews. Both give us a less noisy way to produce rating estimates and reveal the reasons behind user bias. Our hypotheses were validated by statistical evidence from hotel reviews on the TripAdvisor website. Categories and Subject Descriptors J.4 [Social and Behavioral Sciences]: Economics General Terms Economics, Experimentation, Reliability 1. MOTIVATIONS The spread of the internet has made it possible for online feedback forums (or reputation mechanisms) to become an important channel for Word-of-mouth regarding products, services or other types of commercial interactions. Numerous empirical studies [10, 15, 13, 5] show that buyers seriously consider online feedback when making purchasing decisions, and are willing to pay reputation premiums for products or services that have a good reputation. Recent analysis, however, raises important questions regarding the ability of existing forums to reflect the real quality of a product. In the absence of clear incentives, users with a moderate outlook will not bother to voice their opinions, which leads to an unrepresentative sample of reviews. For example, [12, 1] show that Amazon1 ratings of books or CDs follow with great probability bi-modal, U-shaped distributions where most of the ratings are either very good, or very bad. Controlled experiments, on the other hand, reveal opinions on the same items that are normally distributed. Under these circumstances, using the arithmetic mean to predict quality (as most forums actually do) gives the typical user an estimator with high variance that is often false. Improving the way we aggregate the information available from online reviews requires a deep understanding of the underlying factors that bias the rating behavior of users. Hu et al. [12] propose the Brag-and-Moan Model where users rate only if their utility of the product (drawn from a normal distribution) falls outside a median interval. The authors conclude that the model explains the empirical distribution of reports, and offers insights into smarter ways of estimating the true quality of the product. In the present paper we extend this line of research, and attempt to explain further facts about the behavior of users when reporting online feedback. Using actual hotel reviews from the TripAdvisor2 website, we consider two additional sources of information besides the basic numerical ratings submitted by users. The first is simple linguistic evidence from the textual review that usually accompanies the numerical ratings. We use text-mining techniques similar to [7] and [3], however, we are only interested in identifying what aspects of the service the user is discussing, without computing the semantic orientation of the text. We find that users who comment more on the same feature are more likely to agree on a common numerical rating for that particular feature. Intuitively, lengthy comments reveal the importance of the feature to the user. Since people tend to be more knowledgeable in the aspects they consider important, users who discuss a given feature in more details might be assumed to have more authority in evaluating that feature. Second we investigate the relationship between a review 1 http://www.amazon.com 2 http://www.tripadvisor.com/ 134 Figure 1: The TripAdvisor page displaying reviews for a popular Boston hotel. Name of hotel and advertisements were deliberatively erased. and the reviews that preceded it. A perusal of online reviews shows that ratings are often part of discussion threads, where one post is not necessarily independent of other posts. One may see, for example, users who make an effort to contradict, or vehemently agree with, the remarks of previous users. By analyzing the time sequence of reports, we conclude that past reviews influence the future reports, as they create some prior expectation regarding the quality of service. The subjective perception of the user is influenced by the gap between the prior expectation and the actual performance of the service [17, 18, 16, 21] which will later reflect in the users rating. We propose a model that captures the dependence of ratings on prior expectations, and validate it using the empirical data we collected. Both results can be used to improve the way reputation mechanisms aggregate the information from individual reviews. Our first result can be used to determine a featureby-feature estimate of quality, where for each feature, a different subset of reviews (i.e., those with lengthy comments of that feature) is considered. The second leads to an algorithm that outputs a more precise estimate of the real quality. 2. THE DATA SET We use in this paper real hotel reviews collected from the popular travel site TripAdvisor. TripAdvisor indexes hotels from cities across the world, along with reviews written by travelers. Users can search the site by giving the hotels name and location (optional). The reviews for a given hotel are displayed as a list (ordered from the most recent to the oldest), with 5 reviews per page. The reviews contain: • information about the author of the review (e.g., dates of stay, username of the reviewer, location of the reviewer); • the overall rating (from 1, lowest, to 5, highest); • a textual review containing a title for the review, free comments, and the main things the reviewer liked and disliked; • numerical ratings (from 1, lowest, to 5, highest) for different features (e.g., cleanliness, service, location, etc.) Below the name of the hotel, TripAdvisor displays the address of the hotel, general information (number of rooms, number of stars, short description, etc), the average overall rating, the TripAdvisor ranking, and an average rating for each feature. Figure 1 shows the page for a popular Boston hotel whose name (along with advertisements) was explicitly erased. We selected three cities for this study: Boston, Sydney and Las Vegas. For each city we considered all hotels that had at least 10 reviews, and recorded all reviews. Table 1 presents the number of hotels considered in each city, the total number of reviews recorded for each city, and the distribution of hotels with respect to the star-rating (as available on the TripAdvisor site). Note that not all hotels have a star-rating. Table 1: A summary of the data set. City # Reviews # Hotels # of Hotels with 1,2,3,4 & 5 stars Boston 3993 58 1+3+17+15+2 Sydney 1371 47 0+0+9+13+10 Las Vegas 5593 40 0+3+10+9+6 For each review we recorded the overall rating, the textual review (title and body of the review) and the numerical rating on 7 features: Rooms(R), Service(S), Cleanliness(C), Value(V), Food(F), Location(L) and Noise(N). TripAdvisor does not require users to submit anything other than the overall rating, hence a typical review rates few additional features, regardless of the discussion in the textual comment. Only the features Rooms(R), Service(S), Cleanliness(C) and Value(V) are rated by a significant number of users. However, we also selected the features Food(F), Location(L) and Noise(N) because they are referred to in a significant number of textual comments. For each feature we record the numerical rating given by the user, or 0 when the rating is missing. The typical length of the textual comment amounts to approximately 200 words. All data was collected by crawling the TripAdvisor site in September 2006. 2.1 Formal notation We will formally refer to a review by a tuple (r, T) where: • r = (rf ) is a vector containing the ratings rf ∈ {0, 1, . . . 5} for the features f ∈ F = {O, R, S, C, V, F, L, N}; note that the overall rating, rO, is abusively recorded as the rating for the feature Overall(O); • T is the textual comment that accompanies the review. 135 Reviews are indexed according to the variable i, such that (ri , Ti ) is the ith review in our database. Since we dont record the username of the reviewer, we will also say that the ith review in our data set was submitted by user i. When we need to consider only the reviews of a given hotel, h, we will use (ri(h) , Ti(h) ) to denote the ith review about the hotel h. 3. EVIDENCE FROM TEXTUAL COMMENTS The free textual comments associated to online reviews are a valuable source of information for understanding the reasons behind the numerical ratings left by the reviewers. The text may, for example, reveal concrete examples of aspects that the user liked or disliked, thus justifying some of the high, respectively low ratings for certain features. The text may also offer guidelines for understanding the preferences of the reviewer, and the weights of different features when computing an overall rating. The problem, however, is that free textual comments are difficult to read. Users are required to scroll through many reviews and read mostly repetitive information. Significant improvements would be obtained if the reviews were automatically interpreted and aggregated. Unfortunately, this seems a difficult task for computers since human users often use witty language, abbreviations, cultural specific phrases, and the figurative style. Nevertheless, several important results use the textual comments of online reviews in an automated way. Using well established natural language techniques, reviews or parts of reviews can be classified as having a positive or negative semantic orientation. Pang et al. [2] classify movie reviews into positive/negative by training three different classifiers (Naive Bayes, Maximum Entropy and SVM) using classification features based on unigrams, bigrams or part-of-speech tags. Dave et al. [4] analyze reviews from CNet and Amazon, and surprisingly show that classification features based on unigrams or bigrams perform better than higher-order n-grams. This result is challenged by Cui et al. [3] who look at large collections of reviews crawled from the web. They show that the size of the data set is important, and that bigger training sets allow classifiers to successfully use more complex classification features based on n-grams. Hu and Liu [11] also crawl the web for product reviews and automatically identify product attributes that have been discussed by reviewers. They use Wordnet to compute the semantic orientation of product evaluations and summarize user reviews by extracting positive and negative evaluations of different product features. Popescu and Etzioni [20] analyze a similar setting, but use search engine hit-counts to identify product attributes; the semantic orientation is assigned through the relaxation labeling technique. Ghose et al. [7, 8] analyze seller reviews from the Amazon secondary market to identify the different dimensions (e.g., delivery, packaging, customer support, etc.) of reputation. They parse the text, and tag the part-of-speech for each word. Frequent nouns, noun phrases and verbal phrases are identified as dimensions of reputation, while the corresponding modifiers (i.e., adjectives and adverbs) are used to derive numerical scores for each dimension. The enhanced reputation measure correlates better with the pricing information observed in the market. Pavlou and Dimoka [19] analyze eBay reviews and find that textual comments have an important impact on reputation premiums. Our approach is similar to the previously mentioned works, in the sense that we identify the aspects (i.e., hotel features) discussed by the users in the textual reviews. However, we do not compute the semantic orientation of the text, nor attempt to infer missing ratings. We define the weight, wi f , of feature f ∈ F in the text Ti associated with the review (ri , Ti ), as the fraction of Ti dedicated to discussing aspects (both positive and negative) related to feature f. We propose an elementary method to approximate the values of these weights. For each feature we manually construct the word list Lf containing approximately 50 words that are most commonly associated to the feature f. The initial words were selected from reading some of the reviews, and seeing what words coincide with discussion of which features. The list was then extended by adding all thesaurus entries that were related to the initial words. Finally, we brainstormed for missing words that would normally be associated with each of the features. Let Lf ∩Ti be the list of terms common to both Lf and Ti. Each term of Lf is counted the number of times it appears in Ti , with two exceptions: • in cases where the user submits a title to the review, we account for the title text by appending it three times to the review text Ti . The intuitive assumption is that the users opinion is more strongly reflected in the title, rather than in the body of the review. For example, many reviews are accurately summarized by titles such as Excellent service, terrible location or Bad value for money; • certain words that occur only once in the text are counted multiple times if their relevance to that feature is particularly strong. These were root words for each feature (e.g., staff is a root word for the feature Service), and were weighted either 2 or 3. Each feature was assigned up to 3 such root words, so almost all words are counted only once. The list of words for the feature Rooms is given for reference in Appendix A. The weight wi f is computed as: wi f = |Lf ∩ Ti| f∈F |Lf ∩ Ti| (1) where |Lf ∩Ti | is the number of terms common to Lf and Ti . The weight for the feature Overall was set to min{ |T i | 5000 , 1} where |Ti | is the number of character in Ti . The following is a TripAdvisor review for a Boston hotel (the name of the hotel is omitted): Ill start by saying that Im more of a Holiday Inn person than a *** type. So I get frustrated when I pay double the room rate and get half the amenities that Id get at a Hampton Inn or Holiday Inn. The location was definitely the main asset of this place. It was only a few blocks from the Hynes Center subway stop and it was easy to walk to some good restaurants in the Back Bay area. Boylston isnt far off at all. So I had no trouble with foregoing a rental car and taking the subway from the airport to the hotel and using the subway for any other travel. Otherwise, they make you pay for anything and everything. 136 And when youve already dropped $215/night on the room, that gets frustrating.The room itself was decent, about what I would expect. Staff was also average, not bad and not excellent. Again, I think youre paying for location and the ability to walk to a lot of good stuff. But I think next time Ill stay in Brookline, get more amenities, and use the subway a bit more. This numerical ratings associated to this review are rO = 3, rR = 3, rS = 3, rC = 4, rV = 2 for features Overall(O), Rooms(R), Service(S), Cleanliness(C) and Value(V) respectively. The ratings for the features Food(F), Location(L) and Noise(N) are absent (i.e., rF = rL = rN = 0). The weights wf are computed from the following lists of common terms: LR ∩ T ={room}; wR = 0.066 LS ∩ T ={3 * Staff, amenities}; wS = 0.267 LC ∩ T = ∅; wC = 0 LV ∩ T ={$, rate}; wV = 0.133 LF ∩ T ={restaurant}; wF = 0.067 LL ∩ T ={2 * center, 2 * walk, 2 * location, area}; wL = 0.467 LN ∩ T = ∅; wN = 0 The root words Staff and Center were tripled and doubled respectively. The overall weight of the textual review is wO = 0.197. These values account reasonably well for the weights of different features in the discussion of the reviewer. One point to note is that some terms in the lists Lf possess an inherent semantic orientation. For example the word grime (belonging to the list LC ) would be used most often to assert the presence, and not the absence of grime. This is unavoidable, but care was taken to ensure words from both sides of the spectrum were used. For this reason, some lists such as LR contain only nouns of objects that one would typically describe in a room (see Appendix A). The goal of this section is to analyse the influence of the weights wi f on the numerical ratings ri f . Intuitively, users who spent a lot of their time discussing a feature f (i.e., wi f is high) had something to say about their experience with regard to this feature. Obviously, feature f is important for user i. Since people tend to be more knowledgeable in the aspects they consider important, our hypothesis is that the ratings ri f (corresponding to high weights wi f ) constitute a subset of expert ratings for feature f. Figure 2 plots the distribution of the rates r i(h) C with respect to the weights w i(h) C for the cleanliness of a Las Vegas hotel, h. Here, the high ratings are restricted to the reviews that discuss little the cleanliness. Whenever cleanliness appears in the discussion, the ratings are low. Many hotels exhibit similar rating patterns for various features. Ratings corresponding to low weights span the whole spectrum from 1 to 5, while the ratings corresponding to high weights are more grouped together (either around good or bad ratings). We therefore make the following hypothesis: Hypothesis 1. The ratings ri f corresponding to the reviews where wi f is high, are more similar to each other than to the overall collection of ratings. To test the hypothesis, we take the entire set of reviews, and feature by feature, we compute the standard deviation of the ratings with high weights, and the standard deviation of the entire set of ratings. High weights were defined as those belonging to the upper 20% of the weight range for the corresponding feature. If Hypothesis 1 were true, the standard deviation of all ratings should be higher than the standard deviation of the ratings with high weights. 0 1 2 3 4 5 6 0 0.1 0.2 0.3 0.4 0.5 0.6 Rating Weight Figure 2: The distribution of ratings against the weight of the cleanliness feature. We use a standard T-test to measure the significance of the results. City by city and feature by feature, Table 2 presents the average standard deviation of all ratings, and the average standard deviation of ratings with high weights. Indeed, the ratings with high weights have lower standard deviation, and the results are significant at the standard 0.05 significance threshold (although for certain cities taken independently there doesnt seem to be a significant difference, the results are significant for the entire data set). Please note that only the features O,R,S,C and V were considered, since for the others (F, L, and N) we didnt have enough ratings. Table 2: Average standard deviation for all ratings, and average standard deviation for ratings with high weights. In square brackets, the corresponding p-values for a positive difference between the two. City O R S C V all 1.189 0.998 1.144 0.935 1.123 Boston high 0.948 0.778 0.954 0.767 0.891 p-val [0.000] [0.004] [0.045] [0.080] [0.009] all 1.040 0.832 1.101 0.847 0.963 Sydney high 0.801 0.618 0.691 0.690 0.798 p-val [0.012] [0.023] [0.000] [0.377] [0.037] all 1.272 1.142 1.184 1.119 1.242 Vegas high 1.072 0.752 1.169 0.907 1.003 p-val [0.0185] [0.001] [0.918] [0.120] [0.126] Hypothesis 1 not only provides some basic understanding regarding the rating behavior of online users, it also suggests some ways of computing better quality estimates. We can, for example, construct a feature-by-feature quality estimate with much lower variance: for each feature we take the subset of reviews that amply discuss that feature, and output as a quality estimate the average rating for this subset. Initial experiments suggest that the average feature-by-feature ratings computed in this way are different from the average ratings computed on the whole data set. Given that, indeed, high weights are indicators of expert opinions, the estimates obtained in this way are more accurate than the current ones. Nevertheless, the validation of this underlying assumption requires further controlled experiments. 137 4. THE INFLUENCE OF PAST RATINGS Two important assumptions are generally made about reviews submitted to online forums. The first is that ratings truthfully reflect the quality observed by the users; the second is that reviews are independent from one another. While anecdotal evidence [9, 22] challenges the first assumption3 , in this section, we address the second. A perusal of online reviews shows that reviews are often part of discussion threads, where users make an effort to contradict, or vehemently agree with the remarks of previous users. Consider, for example, the following review: I dont understand the negative reviews... the hotel was a little dark, but that was the style. It was very artsy. Yes it was close to the freeway, but in my opinion the sound of an occasional loud car is better than hearing the ding ding of slot machines all night! The staff on-hand is FABULOUS. The waitresses are great (and *** does not deserve the bad review she got, she was 100% attentive to us!), the bartenders are friendly and professional at the same time... Here, the user was disturbed by previous negative reports, addressed these concerns, and set about trying to correct them. Not surprisingly, his ratings were considerably higher than the average ratings up to this point. It seems that TripAdvisor users regularly read the reports submitted by previous users before booking a hotel, or before writing a review. Past reviews create some prior expectation regarding the quality of service, and this expectation has an influence on the submitted review. We believe this observation holds for most online forums. The subjective perception of quality is directly proportional to how well the actual experience meets the prior expectation, a fact confirmed by an important line of econometric and marketing research [17, 18, 16, 21]. The correlation between the reviews has also been confirmed by recent research on the dynamics of online review forums [6]. 4.1 Prior Expectations We define the prior expectation of user i regarding the feature f, as the average of the previously available ratings on the feature f4 : ef (i) = j<i,r j f =0 rj f j<i,r j f =0 1 As a first hypothesis, we assert that the rating ri f is a function of the prior expectation ef (i): Hypothesis 2. For a given hotel and feature, given the reviews i and j such that ef (i) is high and ef (j) is low, the rating rj f exceeds the rating ri f . We define high and low expectations as those that are above, respectively below a certain cutoff value θ. The set of reviews preceded by high, respectively low expectations 3 part of Amazon reviews were recognized as strategic posts by book authors or competitors 4 if no previous ratings were assigned for feature f, ef (i) is assigned a default value of 4. Table 3: Average ratings for reviews preceded by low (first value in the cell) and high (second value in the cell) expectations. The P-values for a positive difference are given square brackets. City O R S C V 3.953 4.045 3.985 4.252 3.946 Boston 3.364 3.590 3.485 3.641 3.242 [0.011] [0.028] [0.0086] [0.0168] [0.0034] 4.284 4.358 4.064 4.530 4.428 Sydney 3.756 3.537 3.436 3.918 3.495 [0.000] [0.000] [0.035] [0.009] [0.000] 3.494 3.674 3.713 3.689 3.580 Las Vegas 3.140 3.530 2.952 3.530 3.351 [0.190] [0.529] [0.007] [0.529] [0.253] are defined as follows: Rhigh f = {ri f |ef (i) > θ} Rlow f = {ri f |ef (i) < θ} These sets are specific for each (hotel, feature) pair, and in our experiments we took θ = 4. This rather high value is close to the average rating across all features across all hotels, and is justified by the fact that our data set contains mostly high quality hotels. For each city, we take all hotels and compute the average ratings in the sets Rhigh f and Rlow f (see Table 3). The average rating amongst reviews following low prior expectations is significantly higher than the average rating following high expectations. As further evidence, we consider all hotels for which the function eV (i) (the expectation for the feature Value) has a high value (greater than 4) for some i, and a low value (less than 4) for some other i. Intuitively, these are the hotels for which there is a minimal degree of variation in the timely sequence of reviews: i.e., the cumulative average of ratings was at some point high and afterwards became low, or vice-versa. Such variations are observed for about half of all hotels in each city. Figure 3 plots the median (across considered hotels) rating, rV , when ef (i) is not more than x but greater than x − 0.5. 2.5 3 3.5 4 4.5 5 2.5 3 3.5 4 4.5 5 Medianofrating expectation Boston Sydney Vegas Figure 3: The ratings tend to decrease as the expectation increases. 138 There are two ways to interpret the function ef (i): • The expected value for feature f obtained by user i before his experience with the service, acquired by reading reports submitted by past users. In this case, an overly high value for ef (i) would drive the user to submit a negative report (or vice versa), stemming from the difference between the actual value of the service, and the inflated expectation of this value acquired before his experience. • The expected value of feature f for all subsequent visitors of the site, if user i were not to submit a report. In this case, the motivation for a negative report following an overly high value of ef is different: user i seeks to correct the expectation of future visitors to the site. Unlike the interpretation above, this does not require the user to derive an a priori expectation for the value of f. Note that neither interpretation implies that the average up to report i is inversely related to the rating at report i. There might exist a measure of influence exerted by past reports that pushes the user behind report i to submit ratings which to some extent conforms with past reports: a low value for ef (i) can influence user i to submit a low rating for feature f because, for example, he fears that submitting a high rating will make him out to be a person with low standards5 . This, at first, appears to contradict Hypothesis 2. However, this conformity rating cannot continue indefinitely: once the set of reports project a sufficiently deflated estimate for vf , future reviewers with comparatively positive impressions will seek to correct this misconception. 4.2 Impact of textual comments on quality expectation Further insight into the rating behavior of TripAdvisor users can be obtained by analyzing the relationship between the weights wf and the values ef (i). In particular, we examine the following hypothesis: Hypothesis 3. When a large proportion of the text of a review discusses a certain feature, the difference between the rating for that feature and the average rating up to that point tends to be large. The intuition behind this claim is that when the user is adamant about voicing his opinion regarding a certain feature, his opinion differs from the collective opinion of previous postings. This relies on the characteristic of reputation systems as feedback forums where a user is interested in projecting his opinion, with particular strength if this opinion differs from what he perceives to be the general opinion. To test Hypothesis 3 we measure the average absolute difference between the expectation ef (i) and the rating ri f when the weight wi f is high, respectively low. Weights are classified high or low by comparing them with certain cutoff values: wi f is low if smaller than 0.1, while wi f is high if greater than θf . Different cutoff values were used for different features: θR = 0.4, θS = 0.4, θC = 0.2, and θV = 0.7. Cleanliness has a lower cutoff since it is a feature rarely discussed; Value has a high cutoff for the opposite reason. Results are presented in Table 4. 5 The idea that negative reports can encourage further negative reporting has been suggested before [14] Table 4: Average of |ri f −ef (i)| when weights are high (first value in the cell) and low (second value in the cell) with P-values for the difference in sq. brackets. City R S C V 1.058 1.208 1.728 1.356 Boston 0.701 0.838 0.760 0.917 [0.022] [0.063] [0.000] [0.218] 1.048 1.351 1.218 1.318 Sydney 0.752 0.759 0.767 0.908 [0.179] [0.009] [0.165] [0.495] 1.184 1.378 1.472 1.642 Las Vegas 0.772 0.834 0.808 1.043 [0.071] [0.020] [0.006] [0.076] This demonstrates that when weights are unusually high, users tend to express an opinion that does not conform to the net average of previous ratings. As we might expect, for a feature that rarely was a high weight in the discussion, (e.g., cleanliness) the difference is particularly large. Even though the difference in the feature Value is quite large for Sydney, the P-value is high. This is because only few reviews discussed value heavily. The reason could be cultural or because there was less of a reason to discuss this feature. 4.3 Reporting Incentives Previous models suggest that users who are not highly opinionated will not choose to voice their opinions [12]. In this section, we extend this model to account for the influence of expectations. The motivation for submitting feedback is not only due to extreme opinions, but also to the difference between the current reputation (i.e., the prior expectation of the user) and the actual experience. Such a rating model produces ratings that most of the time deviate from the current average rating. The ratings that confirm the prior expectation will rarely be submitted. We test on our data set the proportion of ratings that attempt to correct the current estimate. We define a deviant rating as one that deviates from the current expectation by at least some threshold θ, i.e., |ri f − ef (i)| ≥ θ. For each of the three considered cities, the following tables, show the proportion of deviant ratings for θ = 0.5 and θ = 1. Table 5: Proportion of deviant ratings with θ = 0.5 City O R S C V Boston 0.696 0.619 0.676 0.604 0.684 Sydney 0.645 0.615 0.672 0.614 0.675 Las Vegas 0.721 0.641 0.694 0.662 0.724 Table 6: Proportion of deviant ratings with θ = 1 City O R S C V Boston 0.420 0.397 0.429 0.317 0.446 Sydney 0.360 0.367 0.442 0.336 0.489 Las Vegas 0.510 0.421 0.483 0.390 0.472 The above results suggest that a large proportion of users (close to one half, even for the high threshold value θ = 1) deviate from the prior average. This reinforces the idea that users are more likely to submit a report when they believe they have something distinctive to add to the current stream of opinions for some feature. Such conclusions are in total agreement with prior evidence that the distribution of reports often follows bi-modal, U-shaped distributions. 139 5. MODELLING THE BEHAVIOR OF RATERS To account for the observations described in the previous sections, we propose a model for the behavior of the users when submitting online reviews. For a given hotel, we make the assumption that the quality experienced by the users is normally distributed around some value vf , which represents the objective quality offered by the hotel on the feature f. The rating submitted by user i on feature f is: ˆri f = δf vi f + (1 − δf ) · sign vi f − ef (i) c + d(vi f , ef (i)|wi f ) (2) where: • vi f is the (unknown) quality actually experienced by the user. vi f is assumed normally distributed around some value vf ; • δf ∈ [0, 1] can be seen as a measure of the bias when reporting feedback. High values reflect the fact that users rate objectively, without being influenced by prior expectations. The value of δf may depend on various factors; we fix one value for each feature f; • c is a constant between 1 and 5; • wi f is the weight of feature f in the textual comment of review i, computed according to Eq. (1); • d(vi f , ef (i)|wi f ) is a distance function between the expectation and the observation of user i. The distance function satisfies the following properties: - d(y, z|w) ≥ 0 for all y, z ∈ [0, 5], w ∈ [0, 1]; - |d(y, z|w)| < |d(z, x|w)| if |y − z| < |z − x|; - |d(y, z|w1)| < |d(y, z|w2)| if w1 < w2; - c + d(vf , ef (i)|wi f ) ∈ [1, 5]; The second term of Eq. (2) encodes the bias of the rating. The higher the distance between the true observation vi f and the function ef , the higher the bias. 5.1 Model Validation We use the data set of TripAdvisor reviews to validate the behavior model presented above. We split for convenience the rating values in three ranges: bad (B = {1, 2}), indifferent (I = {3, 4}), and good (G = {5}), and perform the following two tests: • First, we will use our model to predict the ratings that have extremal values. For every hotel, we take the sequence of reports, and whenever we encounter a rating that is either good or bad (but not indifferent) we try to predict it using Eq. (2) • Second, instead of predicting the value of extremal ratings, we try to classify them as either good or bad. For every hotel we take the sequence of reports, and for each report (regardless of it value) we classify it as being good or bad However, to perform these tests, we need to estimate the objective value, vf , that is the average of the true quality observations, vi f . The algorithm we are using is based on the intuition that the amount of conformity rating is minimized. In other words, the value vf should be such that as often as possible, bad ratings follow expectations above vf and good ratings follow expectations below vf . Formally, we define the sets: Γ1 = {i|ef (i) < vf and ri f ∈ B}; Γ2 = {i|ef (i) > vf and ri f ∈ G}; that correspond to irregularities where even though the expectation at point i is lower than the delivered value, the rating is poor, and vice versa. We define vf as the value that minimize these union of the two sets: vf = arg min vf |Γ1 ∪ Γ2| (3) In Eq. (2) we replace vi f by the value vf computed in Eq. (3), and use the following distance function: d(vf , ef (i)|wi f ) = |vf − ef (i)| vf − ef (i) |vf 2 − ef (i)2 | · (1 + 2wi f ); The constant c ∈ I was set to min{max{ef (i), 3}}, 4}. The values for δf were fixed at {0.7, 0.7, 0.8, 0.7, 0.6} for the features {Overall, Rooms, Service, Cleanliness, Value} respectively. The weights are computed as described in Section 3. As a first experiment, we take the sets of extremal ratings {ri f |ri f /∈ I} for each hotel and feature. For every such rating, ri f , we try to estimate it by computing ˆri f using Eq. (2). We compare this estimator with the one obtained by simply averaging the ratings over all hotels and features: i.e., ¯rf = j,r j f =0 rj f j,r j f =0 1 ; Table 7 presents the ratio between the root mean square error (RMSE) when using ˆri f and ¯rf to estimate the actual ratings. In all cases the estimate produced by our model is better than the simple average. Table 7: Average of RMSE(ˆrf ) RMSE(¯rf ) City O R S C V Boston 0.987 0.849 0.879 0.776 0.913 Sydney 0.927 0.817 0.826 0.720 0.681 Las Vegas 0.952 0.870 0.881 0.947 0.904 As a second experiment, we try to distinguish the sets Bf = {i|ri f ∈ B} and Gf = {i|ri f ∈ G} of bad, respectively good ratings on the feature f. For example, we compute the set Bf using the following classifier (called σ): ri f ∈ Bf (σf (i) = 1) ⇔ ˆri f ≤ 4; Tables 8, 9 and 10 present the Precision(p), Recall(r) and s = 2pr p+r for classifier σ, and compares it with a naive majority classifier, τ, τf (i) = 1 ⇔ |Bf | ≥ |Gf |: We see that recall is always higher for σ and precision is usually slightly worse. For the s metric σ tends to add a 140 Table 8: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Boston O R S C V p 0.678 0.670 0.573 0.545 0.610 σ r 0.626 0.659 0.619 0.612 0.694 s 0.651 0.665 0.595 0.577 0.609 p 0.684 0.706 0.647 0.611 0.633 τ r 0.597 0.541 0.410 0.383 0.562 s 0.638 0.613 0.502 0.471 0.595 Table 9: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Las Vegas O R S C V p 0.654 0.748 0.592 0.712 0.583 σ r 0.608 0.536 0.791 0.474 0.610 s 0.630 0.624 0.677 0.569 0.596 p 0.685 0.761 0.621 0.748 0.606 τ r 0.542 0.505 0.767 0.445 0.441 s 0.605 0.607 0.670 0.558 0.511 1-20% improvement over τ, much higher in some cases for hotels in Sydney. This is likely because Sydney reviews are more positive than those of the American cities and cases where the number of bad reviews exceeded the number of good ones are rare. Replacing the test algorithm with one that plays a 1 with probability equal to the proportion of bad reviews improves its results for this city, but it is still outperformed by around 80%. 6. SUMMARY OF RESULTS AND CONCLUSION The goal of this paper is to explore the factors that drive a user to submit a particular rating, rather than the incentives that encouraged him to submit a report in the first place. For that we use two additional sources of information besides the vector of numerical ratings: first we look at the textual comments that accompany the reviews, and second we consider the reports that have been previously submitted by other users. Using simple natural language processing algorithms, we were able to establish a correlation between the weight of a certain feature in the textual comment accompanying the review, and the noise present in the numerical rating. Specifically, it seems that users who discuss amply a certain feature are likely to agree on a common rating. This observation allows the construction of feature-by-feature estimators of quality that have a lower variance, and are hopefully less noisy. Nevertheless, further evidence is required to support the intuition that ratings corresponding to high weights are expert opinions that deserve to be given higher priority when computing estimates of quality. Second, we emphasize the dependence of ratings on previous reports. Previous reports create an expectation of quality which affects the subjective perception of the user. We validate two facts about the hotel reviews we collected from TripAdvisor: First, the ratings following low expectations (where the expectation is computed as the average of the previous reports) are likely to be higher than the ratings Table 10: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Sydney O R S C V p 0.650 0.463 0.544 0.550 0.580 σ r 0.234 0.378 0.571 0.169 0.592 s 0.343 0.452 0.557 0.259 0.586 p 0.562 0.615 0.600 0.500 0.600 τ r 0.054 0.098 0.101 0.015 0.175 s 0.098 0.168 0.172 0.030 0.271 following high expectations. Intuitively, the perception of quality (and consequently the rating) depends on how well the actual experience of the user meets her expectation. Second, we include evidence from the textual comments, and find that when users devote a large fraction of the text to discussing a certain feature, they are likely to motivate a divergent rating (i.e., a rating that does not conform to the prior expectation). Intuitively, this supports the hypothesis that review forums act as discussion groups where users are keen on presenting and motivating their own opinion. We have captured the empirical evidence in a behavior model that predicts the ratings submitted by the users. The final rating depends, as expected, on the true observation, and on the gap between the observation and the expectation. The gap tends to have a bigger influence when an important fraction of the textual comment is dedicated to discussing a certain feature. The proposed model was validated on the empirical data and provides better estimates of the ratings actually submitted. One assumption that we make is about the existence of an objective quality value vf for the feature f. This is rarely true, especially over large spans of time. Other explanations might account for the correlation of ratings with past reports. For example, if ef (i) reflects the true value of f at a point in time, the difference in the ratings following high and low expectations can be explained by hotel revenue models that are maximized when the value is modified accordingly. However, the idea that variation in ratings is not primarily a function of variation in value turns out to be a useful one. Our approach to approximate this elusive objective value is by no means perfect, but conforms neatly to the idea behind the model. A natural direction for future work is to examine concrete applications of our results. Significant improvements of quality estimates are likely to be obtained by incorporating all empirical evidence about rating behavior. Exactly how different factors affect the decisions of the users is not clear. The answer might depend on the particular application, context and culture. 7. REFERENCES [1] A. Admati and P. Pfleiderer. Noisytalk.com: Broadcasting opinions in a noisy environment. Working Paper 1670R, Stanford University, 2000. [2] P. B., L. Lee, and S. Vaithyanathan. Thumbs up? sentiment classification using machine learning techniques. In Proceedings of the EMNLP-02, the Conference on Empirical Methods in Natural Language Processing, 2002. [3] H. Cui, V. Mittal, and M. Datar. Comparative 141 Experiments on Sentiment Classification for Online Product Reviews. In Proceedings of AAAI, 2006. [4] K. Dave, S. Lawrence, and D. Pennock. Mining the peanut gallery:opinion extraction and semantic classification of product reviews. In Proceedings of the 12th International Conference on the World Wide Web (WWW03), 2003. [5] C. Dellarocas, N. Awad, and X. Zhang. Exploring the Value of Online Product Ratings in Revenue Forecasting: The Case of Motion Pictures. Working paper, 2006. [6] C. Forman, A. Ghose, and B. Wiesenfeld. A Multi-Level Examination of the Impact of Social Identities on Economic Transactions in Electronic Markets. Available at SSRN: http://ssrn.com/abstract=918978, July 2006. [7] A. Ghose, P. Ipeirotis, and A. Sundararajan. Reputation Premiums in Electronic Peer-to-Peer Markets: Analyzing Textual Feedback and Network Structure. In Third Workshop on Economics of Peer-to-Peer Systems, (P2PECON), 2005. [8] A. Ghose, P. Ipeirotis, and A. Sundararajan. The Dimensions of Reputation in electronic Markets. Working Paper CeDER-06-02, New York University, 2006. [9] A. Harmon. Amazon Glitch Unmasks War of Reviewers. The New York Times, February 14, 2004. [10] D. Houser and J. Wooders. Reputation in Auctions: Theory and Evidence from eBay. Journal of Economics and Management Strategy, 15:353-369, 2006. [11] M. Hu and B. Liu. Mining and summarizing customer reviews. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD04), 2004. [12] N. Hu, P. Pavlou, and J. Zhang. Can Online Reviews Reveal a Products True Quality? In Proceedings of ACM Conference on Electronic Commerce (EC 06), 2006. [13] K. Kalyanam and S. McIntyre. Return on reputation in online auction market. Working Paper 02/03-10-WP, Leavey School of Business, Santa Clara University., 2001. [14] L. Khopkar and P. Resnick. Self-Selection, Slipping, Salvaging, Slacking, and Stoning: the Impacts of Negative Feedback at eBay. In Proceedings of ACM Conference on Electronic Commerce (EC 05), 2005. [15] M. Melnik and J. Alm. Does a sellers reputation matter? evidence from ebay auctions. Journal of Industrial Economics, 50(3):337-350, 2002. [16] R. Olshavsky and J. Miller. Consumer Expectations, Product Performance and Perceived Product Quality. Journal of Marketing Research, 9:19-21, February 1972. [17] A. Parasuraman, V. Zeithaml, and L. Berry. A Conceptual Model of Service Quality and Its Implications for Future Research. Journal of Marketing, 49:41-50, 1985. [18] A. Parasuraman, V. Zeithaml, and L. Berry. SERVQUAL: A Multiple-Item Scale for Measuring Consumer Perceptions of Service Quality. Journal of Retailing, 64:12-40, 1988. [19] P. Pavlou and A. Dimoka. The Nature and Role of Feedback Text Comments in Online Marketplaces: Implications for Trust Building, Price Premiums, and Seller Differentiation. Information Systems Research, 17(4):392-414, 2006. [20] A. Popescu and O. Etzioni. Extracting product features and opinions from reviews. In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, 2005. [21] R. Teas. Expectations, Performance Evaluation, and Consumers Perceptions of Quality. Journal of Marketing, 57:18-34, 1993. [22] E. White. Chatting a Singer Up the Pop Charts. The Wall Street Journal, October 15, 1999. APPENDIX A. LIST OF WORDS, LR, ASSOCIATED TO THE FEATURE ROOMS All words serve as prefixes: room, space, interior, decor, ambiance, atmosphere, comfort, bath, toilet, bed, building, wall, window, private, temperature, sheet, linen, pillow, hot, water, cold, water, shower, lobby, furniture, carpet, air, condition, mattress, layout, design, mirror, ceiling, lighting, lamp, sofa, chair, dresser, wardrobe, closet 142",
    "original_translation": "La comprensión del comportamiento del usuario en la presentación de comentarios en línea. Trabajos anteriores han demostrado que la información contradictoria y los sesgos subyacentes de los usuarios dificultan juzgar el verdadero valor de un servicio. En este artículo, investigamos los factores subyacentes que influyen en el comportamiento del usuario al informar retroalimentación. Examinamos dos fuentes de información además de las calificaciones numéricas: la evidencia lingüística del comentario textual que acompaña a una reseña y los patrones en la secuencia temporal de los informes. Primero demostramos que los grupos de usuarios que discuten ampliamente una característica específica tienen más probabilidades de ponerse de acuerdo en una calificación común para esa característica. Segundo, demostramos que la calificación de un usuario refleja en parte la diferencia entre la calidad real y la expectativa previa de calidad, tal como se infiere de revisiones anteriores. Ambos nos ofrecen una forma menos ruidosa de producir estimaciones de calificación y revelar las razones detrás del sesgo del usuario. Nuestras hipótesis fueron validadas por evidencia estadística de reseñas de hoteles en el sitio web de TripAdvisor. Categorías y Descriptores de Asignaturas J.4 [Ciencias Sociales y del Comportamiento]: Economía Términos Generales Economía, Experimentación, Confiabilidad 1. MOTIVACIONES La expansión de internet ha hecho posible que los foros de retroalimentación en línea (o mecanismos de reputación) se conviertan en un canal importante para el boca a boca sobre productos, servicios u otros tipos de interacciones comerciales. Numerosos estudios empíricos [10, 15, 13, 5] muestran que los compradores consideran seriamente los comentarios en línea al tomar decisiones de compra, y están dispuestos a pagar primas por reputación por productos o servicios que tienen una buena reputación. Sin embargo, un análisis reciente plantea preguntas importantes sobre la capacidad de los foros existentes para reflejar la verdadera calidad de un producto. En ausencia de incentivos claros, los usuarios con una perspectiva moderada no se molestarán en expresar sus opiniones, lo que conduce a una muestra no representativa de reseñas. Por ejemplo, [12, 1] muestran que las calificaciones de libros o CDs en Amazon1 siguen con gran probabilidad distribuciones bimodales en forma de U, donde la mayoría de las calificaciones son o muy buenas o muy malas. Los experimentos controlados, por otro lado, revelan opiniones sobre los mismos elementos que están distribuidas de forma normal. Bajo estas circunstancias, utilizar la media aritmética para predecir la calidad (como la mayoría de los foros realmente hacen) proporciona al usuario típico un estimador con una alta varianza que a menudo es incorrecto. Mejorar la forma en que agregamos la información disponible de las reseñas en línea requiere un profundo entendimiento de los factores subyacentes que sesgan el comportamiento de calificación de los usuarios. Hu et al. [12] proponen el Modelo Brag-and-Moan donde los usuarios califican solo si su utilidad del producto (extraída de una distribución normal) cae fuera de un intervalo mediano. Los autores concluyen que el modelo explica la distribución empírica de los informes y ofrece ideas sobre formas más inteligentes de estimar la verdadera calidad del producto. En el presente artículo ampliamos esta línea de investigación e intentamos explicar más hechos sobre el comportamiento de los usuarios al informar retroalimentación en línea. Usando reseñas reales de hoteles del sitio web TripAdvisor2, consideramos dos fuentes adicionales de información además de las calificaciones numéricas básicas enviadas por los usuarios. La primera es una evidencia lingüística simple proveniente de la revisión textual que suele acompañar a las calificaciones numéricas. Utilizamos técnicas de minería de texto similares a [7] y [3], sin embargo, solo estamos interesados en identificar qué aspectos del servicio está discutiendo el usuario, sin calcular la orientación semántica del texto. Observamos que los usuarios que comentan más sobre la misma característica tienen más probabilidades de estar de acuerdo en una calificación numérica común para esa característica en particular. Intuitivamente, los comentarios extensos revelan la importancia de la característica para el usuario. Dado que las personas tienden a ser más conocedoras en los aspectos que consideran importantes, se podría asumir que los usuarios que discuten una característica específica en más detalle tienen más autoridad para evaluar esa característica. Segundo investigamos la relación entre una reseña 1 http://www.amazon.com 2 http://www.tripadvisor.com/ 134 Figura 1: La página de TripAdvisor que muestra reseñas de un popular hotel en Boston. El nombre del hotel y los anuncios fueron deliberadamente borrados, al igual que las reseñas que lo precedieron. Un examen de las reseñas en línea muestra que las calificaciones a menudo forman parte de los hilos de discusión, donde una publicación no es necesariamente independiente de otras publicaciones. Uno puede ver, por ejemplo, usuarios que se esfuerzan por contradecir o estar vehementemente de acuerdo con los comentarios de usuarios anteriores. Al analizar la secuencia temporal de informes, concluimos que las revisiones pasadas influyen en los informes futuros, ya que crean cierta expectativa previa sobre la calidad del servicio. La percepción subjetiva del usuario está influenciada por la brecha entre la expectativa previa y el rendimiento real del servicio [17, 18, 16, 21], lo cual se reflejará más tarde en la calificación de los usuarios. Proponemos un modelo que captura la dependencia de las calificaciones en las expectativas previas, y lo validamos utilizando los datos empíricos que recopilamos. Ambos resultados pueden ser utilizados para mejorar la forma en que los mecanismos de reputación agregan la información de las revisiones individuales. Nuestro primer resultado se puede utilizar para determinar una estimación de calidad característica por característica, donde para cada característica se considera un subconjunto diferente de reseñas (es decir, aquellas con comentarios extensos sobre esa característica). El segundo conduce a un algoritmo que produce una estimación más precisa de la calidad real. 2. El conjunto de datos que utilizamos en este artículo son reseñas reales de hoteles recopiladas del popular sitio de viajes TripAdvisor. TripAdvisor indexa hoteles de ciudades de todo el mundo, junto con reseñas escritas por viajeros. Los usuarios pueden buscar en el sitio proporcionando el nombre del hotel y la ubicación (opcional). Las reseñas de un hotel dado se muestran como una lista (ordenadas de la más reciente a la más antigua), con 5 reseñas por página. Las reseñas contienen: • información sobre el autor de la reseña (por ejemplo, fechas de estancia, nombre de usuario del revisor, ubicación del revisor); • la calificación general (de 1, la más baja, a 5, la más alta); • una reseña textual que incluye un título para la reseña, comentarios libres y las principales cosas que al revisor le gustaron y no le gustaron; • calificaciones numéricas (de 1, la más baja, a 5, la más alta) para diferentes características (por ejemplo, limpieza, servicio, ubicación, etc.). Debajo del nombre del hotel, TripAdvisor muestra la dirección del hotel, información general (número de habitaciones, número de estrellas, breve descripción, etc.), la calificación promedio general, el ranking de TripAdvisor y una calificación promedio para cada característica. La Figura 1 muestra la página de un hotel popular en Boston cuyo nombre (junto con los anuncios) fue explícitamente borrado. Seleccionamos tres ciudades para este estudio: Boston, Sídney y Las Vegas. Para cada ciudad consideramos todos los hoteles que tenían al menos 10 reseñas, y registramos todas las reseñas. La Tabla 1 presenta el número de hoteles considerados en cada ciudad, el número total de reseñas registradas para cada ciudad y la distribución de hoteles con respecto a la calificación por estrellas (según está disponible en el sitio de TripAdvisor). Ten en cuenta que no todos los hoteles tienen una clasificación por estrellas. Tabla 1: Un resumen del conjunto de datos. Para cada reseña, registramos la calificación general, la reseña textual (título y cuerpo de la reseña) y la calificación numérica en 7 características: Habitaciones (R), Servicio (S), Limpieza (C), Valor (V), Comida (F), Ubicación (L) y Ruido (N). TripAdvisor no requiere que los usuarios envíen nada más que la calificación general, por lo tanto, una reseña típica evalúa pocas características adicionales, independientemente de la discusión en el comentario textual. Solo las características Habitaciones (R), Servicio (S), Limpieza (C) y Valor (V) son evaluadas por un número significativo de usuarios. Sin embargo, también seleccionamos las características Comida (F), Ubicación (L) y Ruido (N) porque son mencionadas en un número significativo de comentarios textuales. Para cada característica registramos la calificación numérica dada por el usuario, o 0 cuando la calificación está ausente. La longitud típica del comentario textual equivale aproximadamente a 200 palabras. Todos los datos fueron recopilados rastreando el sitio de TripAdvisor en septiembre de 2006. 2.1 Notación formal Nos referiremos formalmente a una reseña mediante una tupla (r, T) donde: • r = (rf ) es un vector que contiene las calificaciones rf ∈ {0, 1, . . . 5} para las características f ∈ F = {O, R, S, C, V, F, L, N}; cabe destacar que la calificación general, rO, se registra de forma abusiva como la calificación para la característica General(O); • T es el comentario textual que acompaña a la reseña. Se indexan 135 reseñas de acuerdo con la variable i, de modo que (ri , Ti ) es la i-ésima reseña en nuestra base de datos. Dado que no registramos el nombre de usuario del revisor, también diremos que la i-ésima reseña en nuestro conjunto de datos fue enviada por el usuario i. Cuando necesitemos considerar solo las reseñas de un hotel dado, h, usaremos (ri(h), Ti(h)) para denotar la i-ésima reseña sobre el hotel h. 3. EVIDENCIA DE COMENTARIOS TEXTUALES Los comentarios textuales gratuitos asociados a las reseñas en línea son una fuente valiosa de información para comprender las razones detrás de las calificaciones numéricas dejadas por los revisores. El texto puede, por ejemplo, revelar ejemplos concretos de aspectos que al usuario le gustaron o no le gustaron, justificando así algunas de las calificaciones altas o bajas respectivamente para ciertas características. El texto también puede ofrecer pautas para comprender las preferencias del revisor y los pesos de las diferentes características al calcular una calificación general. El problema, sin embargo, es que los comentarios textuales libres son difíciles de leer. Los usuarios deben desplazarse a través de muchas reseñas y leer principalmente información repetitiva. Se obtendrían mejoras significativas si las reseñas fueran interpretadas y agregadas automáticamente. Desafortunadamente, esta parece ser una tarea difícil para las computadoras ya que los usuarios humanos a menudo utilizan un lenguaje ingenioso, abreviaturas, frases específicas de la cultura y un estilo figurativo. Sin embargo, varios resultados importantes utilizan los comentarios textuales de las reseñas en línea de forma automatizada. Utilizando técnicas de lenguaje natural bien establecidas, las reseñas o partes de las reseñas pueden ser clasificadas como tener una orientación semántica positiva o negativa. Pang et al. [2] clasifican las críticas de películas en positivas/negativas entrenando tres clasificadores diferentes (Naive Bayes, Máxima Entropía y SVM) utilizando características de clasificación basadas en unigramas, bigramas o etiquetas de partes del discurso. Dave et al. [4] analizan reseñas de CNet y Amazon, y sorprendentemente demuestran que las características de clasificación basadas en unigramas o bigramas funcionan mejor que los n-gramas de orden superior. Este resultado es desafiado por Cui et al. [3], quienes examinan grandes colecciones de reseñas recopiladas de la web. Muestran que el tamaño del conjunto de datos es importante, y que conjuntos de entrenamiento más grandes permiten a los clasificadores utilizar con éxito características de clasificación más complejas basadas en n-gramas. Hu y Liu [11] también rastrean la web en busca de reseñas de productos e identifican automáticamente los atributos de productos que han sido discutidos por los revisores. Utilizan Wordnet para calcular la orientación semántica de las evaluaciones de productos y resumir las reseñas de usuarios extrayendo evaluaciones positivas y negativas de diferentes características del producto. Popescu y Etzioni [20] analizan un escenario similar, pero utilizan el recuento de resultados de búsqueda en motores de búsqueda para identificar atributos de productos; la orientación semántica se asigna a través de la técnica de etiquetado de relajación. Ghose et al. [7, 8] analizan las reseñas de vendedores del mercado secundario de Amazon para identificar las diferentes dimensiones (por ejemplo, entrega, empaque, atención al cliente, etc.) de la reputación. Analizan el texto y etiquetan la parte de la oración de cada palabra. Los sustantivos, frases nominales y frases verbales frecuentes se identifican como dimensiones de la reputación, mientras que los modificadores correspondientes (es decir, adjetivos y adverbios) se utilizan para derivar puntuaciones numéricas para cada dimensión. La medida de reputación mejorada se correlaciona mejor con la información de precios observada en el mercado. Pavlou y Dimoka [19] analizan las reseñas de eBay y encuentran que los comentarios textuales tienen un impacto importante en las primas de reputación. Nuestro enfoque es similar a los trabajos mencionados anteriormente, en el sentido de que identificamos los aspectos (es decir, las características del hotel) discutidos por los usuarios en las reseñas textuales. Sin embargo, no calculamos la orientación semántica del texto, ni intentamos inferir las calificaciones faltantes. Definimos el peso, wi f, de la característica f ∈ F en el texto Ti asociado con la reseña (ri, Ti), como la fracción de Ti dedicada a discutir aspectos (tanto positivos como negativos) relacionados con la característica f. Proponemos un método elemental para aproximar los valores de estos pesos. Para cada característica, construimos manualmente la lista de palabras Lf que contiene aproximadamente 50 palabras que están más comúnmente asociadas a la característica f. Las palabras iniciales fueron seleccionadas al leer algunas de las reseñas y ver qué palabras coinciden con la discusión de qué características. La lista fue luego ampliada agregando todas las entradas del tesauro que estaban relacionadas con las palabras iniciales. Finalmente, hicimos una lluvia de ideas para encontrar las palabras faltantes que normalmente estarían asociadas con cada una de las características. Que Lf ∩ Ti sea la lista de términos comunes a Lf y Ti. Cada término de Lf se cuenta el número de veces que aparece en Ti, con dos excepciones: • en los casos en que el usuario envía un título para la revisión, consideramos el texto del título agregándolo tres veces al texto de la revisión Ti. La suposición intuitiva es que la opinión del usuario se refleja con mayor fuerza en el título que en el cuerpo de la reseña. Por ejemplo, muchas reseñas son resumidas con precisión por títulos como \"Excelente servicio, ubicación terrible\" o \"Mala relación calidad-precio\"; ciertas palabras que ocurren solo una vez en el texto son contadas múltiples veces si su relevancia para esa característica es particularmente fuerte. Estas eran las palabras raíz para cada característica (por ejemplo, personal es una palabra raíz para la característica Servicio), y tenían un peso de 2 o 3. Cada característica fue asignada hasta 3 palabras raíz, por lo que casi todas las palabras se cuentan solo una vez. La lista de palabras para la característica Habitaciones se proporciona como referencia en el Apéndice A. El peso wi f se calcula como: wi f = |Lf ∩ Ti| f∈F |Lf ∩ Ti| (1) donde |Lf ∩ Ti| es el número de términos comunes entre Lf y Ti. El peso para la característica \"En general\" se estableció en min{ |T i | 5000 , 1} donde |Ti | es el número de caracteres en Ti. Comenzaré diciendo que soy más de la onda de Holiday Inn que de un hotel de lujo. Así que me frustro cuando pago el doble de la tarifa de la habitación y recibo la mitad de las comodidades que obtendría en un Hampton Inn o Holiday Inn. La ubicación era definitivamente el principal activo de este lugar. Estaba a solo unas cuadras de la parada de metro del Centro Hynes y era fácil caminar a algunos buenos restaurantes en la zona de Back Bay. Boylston no está lejos en absoluto. Así que no tuve problemas en renunciar a un coche de alquiler y tomar el metro desde el aeropuerto hasta el hotel y usar el metro para cualquier otro viaje. De lo contrario, te hacen pagar por todo. Y cuando ya has gastado $215 por noche en la habitación, eso resulta frustrante. La habitación en sí estaba bien, más o menos lo que esperaría. El personal también era promedio, ni malo ni excelente. Una vez más, creo que estás pagando por la ubicación y la capacidad de ir caminando a muchos lugares interesantes. Pero creo que la próxima vez me quedaré en Brookline, disfrutaré de más comodidades y usaré más el metro. Las calificaciones numéricas asociadas a esta reseña son rO = 3, rR = 3, rS = 3, rC = 4, rV = 2 para las características de General (O), Habitaciones (R), Servicio (S), Limpieza (C) y Valor (V) respectivamente. Las calificaciones de las características Comida (F), Ubicación (L) y Ruido (N) están ausentes (es decir, rF = rL = rN = 0). Los pesos wf se calculan a partir de las siguientes listas de términos comunes: LR ∩ T = {habitación}; wR = 0.066 LS ∩ T = {3 * Personal, comodidades}; wS = 0.267 LC ∩ T = ∅; wC = 0 LV ∩ T = {$, tarifa}; wV = 0.133 LF ∩ T = {restaurante}; wF = 0.067 LL ∩ T = {2 * centro, 2 * caminar, 2 * ubicación, área}; wL = 0.467 LN ∩ T = ∅; wN = 0 Las palabras raíz Personal y Centro se triplicaron y duplicaron respectivamente. El peso total de la revisión textual es wO = 0.197. Estos valores explican razonablemente bien los pesos de las diferentes características en la discusión del revisor. Un punto a tener en cuenta es que algunos términos en las listas Lf poseen una orientación semántica inherente. Por ejemplo, la palabra suciedad (perteneciente a la lista LC) se usaría con mayor frecuencia para afirmar la presencia, y no la ausencia de suciedad. Esto es inevitable, pero se tuvo cuidado de asegurar que se utilizaran palabras de ambos extremos del espectro. Por esta razón, algunas listas como LR contienen solo sustantivos de objetos que uno típicamente describiría en una habitación (ver Apéndice A). El objetivo de esta sección es analizar la influencia de los pesos wi f en las calificaciones numéricas ri f. Intuitivamente, los usuarios que pasaron mucho tiempo discutiendo una característica f (es decir, cuando wif es alto) tenían algo que decir sobre su experiencia con respecto a esta característica. Obviamente, la característica f es importante para el usuario i. Dado que las personas tienden a ser más conocedoras en los aspectos que consideran importantes, nuestra hipótesis es que las calificaciones ri f (correspondientes a los pesos altos wi f) constituyen un subconjunto de las calificaciones de expertos para la característica f. La Figura 2 traza la distribución de las tasas r i(h) C con respecto a los pesos w i(h) C para la limpieza de un hotel de Las Vegas, h. Aquí, las calificaciones altas están restringidas a las reseñas que hablan poco sobre la limpieza. Cuando se menciona la limpieza en la discusión, las calificaciones son bajas. Muchos hoteles muestran patrones de calificación similares para varias características. Las calificaciones correspondientes a pesos bajos abarcan todo el espectro del 1 al 5, mientras que las calificaciones correspondientes a pesos altos están más agrupadas (ya sea alrededor de calificaciones buenas o malas). Por lo tanto, formulamos la siguiente hipótesis: Hipótesis 1. Las calificaciones ri f correspondientes a las reseñas donde wi f es alta, son más similares entre sí que a la colección general de calificaciones. Para probar la hipótesis, tomamos todo el conjunto de reseñas y, característica por característica, calculamos la desviación estándar de las calificaciones con alto peso, y la desviación estándar de todo el conjunto de calificaciones. Los pesos altos se definieron como aquellos que pertenecen al 20% superior del rango de pesos para la característica correspondiente. Si la Hipótesis 1 fuera cierta, la desviación estándar de todas las calificaciones debería ser mayor que la desviación estándar de las calificaciones con pesos altos. 0 1 2 3 4 5 6 0 0.1 0.2 0.3 0.4 0.5 0.6 Calificación Peso Figura 2: La distribución de las calificaciones en función del peso de la característica de limpieza. Utilizamos una prueba T estándar para medir la significancia de los resultados. Ciudad por ciudad y característica por característica, la Tabla 2 presenta la desviación estándar promedio de todas las calificaciones, y la desviación estándar promedio de las calificaciones con pesos altos. De hecho, las calificaciones con pesos altos tienen una desviación estándar más baja, y los resultados son significativos en el umbral estándar de significancia de 0.05 (aunque para ciertas ciudades tomadas de forma independiente no parece haber una diferencia significativa, los resultados son significativos para todo el conjunto de datos). Por favor, tenga en cuenta que solo se consideraron las características O, R, S, C y V, ya que para las otras (F, L y N) no teníamos suficientes calificaciones. Tabla 2: Desviación estándar promedio para todas las calificaciones, y desviación estándar promedio para las calificaciones con pesos altos. En corchetes, los valores p correspondientes para una diferencia positiva entre los dos. La hipótesis 1 no solo proporciona una comprensión básica sobre el comportamiento de calificación de los usuarios en línea, sino que también sugiere algunas formas de calcular estimaciones de mejor calidad. Podemos, por ejemplo, construir una estimación de calidad característica por característica con una varianza mucho menor: para cada característica tomamos el subconjunto de reseñas que discuten ampliamente esa característica, y como estimación de calidad, obtenemos la calificación promedio de este subconjunto. Los experimentos iniciales sugieren que las calificaciones promedio de característica a característica calculadas de esta manera son diferentes de las calificaciones promedio calculadas en todo el conjunto de datos. Dado que, de hecho, los pesos altos son indicadores de opiniones expertas, las estimaciones obtenidas de esta manera son más precisas que las actuales. Sin embargo, la validación de esta suposición subyacente requiere más experimentos controlados. LA INFLUENCIA DE LAS CALIFICACIONES PASADAS Por lo general, se hacen dos suposiciones importantes sobre las reseñas enviadas a foros en línea. La primera es que las calificaciones reflejen fielmente la calidad observada por los usuarios; la segunda es que las reseñas sean independientes entre sí. Si bien la evidencia anecdótica [9, 22] cuestiona la primera suposición, en esta sección abordamos la segunda. Un examen de las reseñas en línea muestra que estas suelen formar parte de hilos de discusión, donde los usuarios se esfuerzan por contradecir o estar vehementemente de acuerdo con los comentarios de usuarios anteriores. Considera, por ejemplo, la siguiente reseña: No entiendo las críticas negativas... el hotel estaba un poco oscuro, pero ese era el estilo. Fue muy artístico. Sí, estaba cerca de la autopista, pero en mi opinión, el sonido de un coche ruidoso ocasional es mejor que escuchar el ding ding de las máquinas tragamonedas toda la noche. El personal disponible es FABULOSO. Las camareras son geniales (y *** no merece la mala crítica que recibió, ¡fue 100% atenta con nosotros!), los camareros son amigables y profesionales al mismo tiempo... Aquí, el usuario se vio perturbado por informes negativos anteriores, abordó estas preocupaciones y se dispuso a intentar corregirlas. Como era de esperar, sus calificaciones fueron considerablemente más altas que las calificaciones promedio hasta este punto. Parece que los usuarios de TripAdvisor suelen leer regularmente los informes enviados por usuarios anteriores antes de reservar un hotel, o antes de escribir una reseña. Las reseñas anteriores crean ciertas expectativas previas sobre la calidad del servicio, y estas expectativas influyen en la reseña presentada. Creemos que esta observación es válida para la mayoría de los foros en línea. La percepción subjetiva de la calidad es directamente proporcional a qué tan bien la experiencia real cumple con la expectativa previa, un hecho confirmado por una importante línea de investigación econométrica y de marketing [17, 18, 16, 21]. La correlación entre las reseñas también ha sido confirmada por investigaciones recientes sobre la dinámica de los foros de reseñas en línea [6]. 4.1 Expectativas Previas Definimos la expectativa previa del usuario i con respecto a la característica f, como el promedio de las calificaciones previamente disponibles en la característica f4: ef (i) = j<i,r j f =0 rj f j<i,r j f =0 1 Como primera hipótesis, afirmamos que la calificación ri f es una función de la expectativa previa ef (i): Hipótesis 2. Para un hotel y una característica dados, dados los comentarios i y j tales que ef (i) es alto y ef (j) es bajo, la calificación rj f supera la calificación ri f. Definimos las expectativas altas y bajas como aquellas que están por encima, respectivamente por debajo de un cierto valor umbral θ. El conjunto de reseñas precedidas por altas, respectivamente bajas expectativas 3 parte de las reseñas de Amazon fueron reconocidas como publicaciones estratégicas por autores de libros o competidores 4 si no se asignaron calificaciones previas para la característica f, ef (i) se asigna un valor predeterminado de 4. Tabla 3: Calificaciones promedio para reseñas precedidas por expectativas bajas (primer valor en la celda) y altas (segundo valor en la celda). Los valores P para una diferencia positiva se dan entre corchetes. Las ciudades O R S C V 3.953 4.045 3.985 4.252 3.946 Boston 3.364 3.590 3.485 3.641 3.242 [0.011] [0.028] [0.0086] [0.0168] [0.0034] 4.284 4.358 4.064 4.530 4.428 Sídney 3.756 3.537 3.436 3.918 3.495 [0.000] [0.000] [0.035] [0.009] [0.000] 3.494 3.674 3.713 3.689 3.580 Las Vegas 3.140 3.530 2.952 3.530 3.351 [0.190] [0.529] [0.007] [0.529] [0.253] se definen de la siguiente manera: Ralto f = {ri f |ef (i) > θ} Rbajo f = {ri f |ef (i) < θ} Estos conjuntos son específicos para cada par (hotel, característica), y en nuestros experimentos tomamos θ = 4. Este valor bastante alto está cerca del promedio de calificación de todas las características de todos los hoteles, y se justifica por el hecho de que nuestro conjunto de datos contiene principalmente hoteles de alta calidad. Para cada ciudad, tomamos todos los hoteles y calculamos las calificaciones promedio en los conjuntos Rhigh f y Rlow f (ver Tabla 3). El promedio de calificación entre las reseñas que siguen a expectativas previas bajas es significativamente mayor que el promedio de calificación después de altas expectativas. Como evidencia adicional, consideramos todos los hoteles para los cuales la función eV (i) (la expectativa para la característica Valor) tiene un valor alto (mayor que 4) para algunos i, y un valor bajo (menor que 4) para algunos otros i. Intuitivamente, estos son los hoteles para los cuales hay un grado mínimo de variación en la secuencia oportuna de reseñas: es decir, el promedio acumulativo de calificaciones fue en algún momento alto y luego se volvió bajo, o viceversa. Tales variaciones se observan en aproximadamente la mitad de todos los hoteles en cada ciudad. La Figura 3 traza la mediana (entre los hoteles considerados) de la calificación, rV, cuando ef (i) no es más que x pero mayor que x - 0.5. 2.5 3 3.5 4 4.5 5 2.5 3 3.5 4 4.5 5 Expectativa de la mediana de calificación Boston Sydney Vegas Figura 3: Las calificaciones tienden a disminuir a medida que aumenta la expectativa. 138 Hay dos formas de interpretar la función ef (i): • El valor esperado para la característica f obtenido por el usuario i antes de su experiencia con el servicio, adquirido al leer informes presentados por usuarios anteriores. En este caso, un valor excesivamente alto para ef (i) llevaría al usuario a enviar un informe negativo (o viceversa), derivado de la diferencia entre el valor real del servicio y la expectativa inflada de este valor adquirida antes de su experiencia. • El valor esperado de la característica f para todos los visitantes posteriores del sitio, si el usuario i no enviara un informe. En este caso, la motivación para un informe negativo después de un valor excesivamente alto de ef es diferente: el usuario i busca corregir la expectativa de los futuros visitantes del sitio. A diferencia de la interpretación anterior, esto no requiere que el usuario derive una expectativa a priori para el valor de f. Tenga en cuenta que ninguna de las interpretaciones implica que el promedio hasta el informe i esté inversamente relacionado con la calificación en el informe i. Podría existir una medida de influencia ejercida por informes anteriores que impulse al usuario detrás del informe i a enviar calificaciones que, en cierta medida, se ajusten a los informes anteriores: un valor bajo para ef (i) puede influir en el usuario i a enviar una calificación baja para la característica f porque, por ejemplo, teme que enviar una calificación alta lo haga parecer una persona con bajos estándares. Esto, al principio, parece contradecir la Hipótesis 2. Sin embargo, esta calificación de conformidad no puede continuar indefinidamente: una vez que el conjunto de informes proyecte una estimación suficientemente reducida para vf, los revisores futuros con impresiones comparativamente positivas buscarán corregir esta percepción errónea. 4.2 Impacto de los comentarios textuales en la expectativa de calidad. Se puede obtener una mayor comprensión del comportamiento de calificación de los usuarios de TripAdvisor analizando la relación entre los pesos wf y los valores ef (i). En particular, examinamos la siguiente hipótesis: Hipótesis 3. Cuando una gran proporción del texto de una reseña discute una característica en particular, la diferencia entre la calificación para esa característica y la calificación promedio hasta ese momento tiende a ser grande. La intuición detrás de esta afirmación es que cuando el usuario insiste en expresar su opinión sobre una característica en particular, su opinión difiere de la opinión colectiva de publicaciones anteriores. Esto se basa en la característica de los sistemas de reputación como foros de retroalimentación donde un usuario está interesado en proyectar su opinión, con particular fuerza si esta opinión difiere de lo que percibe como la opinión general. Para probar la Hipótesis 3 medimos la diferencia absoluta promedio entre la expectativa ef (i) y la calificación ri f cuando el peso wi f es alto, respectivamente bajo. Los pesos se clasifican como altos o bajos al compararlos con ciertos valores de corte: wi f es bajo si es menor que 0.1, mientras que wi f es alto si es mayor que θf. Se utilizaron diferentes valores de corte para diferentes características: θR = 0.4, θS = 0.4, θC = 0.2 y θV = 0.7. La limpieza tiene un límite inferior más bajo ya que es una característica raramente discutida; el valor tiene un límite superior alto por la razón opuesta. Los resultados se presentan en la Tabla 4. La idea de que los informes negativos pueden fomentar una mayor divulgación negativa ha sido sugerida anteriormente [14]. Tabla 4: Promedio de |ri f −ef (i)| cuando los pesos son altos (primer valor en la celda) y bajos (segundo valor en la celda) con valores P para la diferencia en corchetes cuadrados. Esto demuestra que cuando los pesos son inusualmente altos, los usuarios tienden a expresar una opinión que no se ajusta al promedio neto de calificaciones anteriores. Como podríamos esperar, para una característica que rara vez tuvo un peso importante en la discusión (por ejemplo, la limpieza), la diferencia es particularmente grande. Aunque la diferencia en el valor de la característica es bastante grande para Sídney, el valor P es alto. Esto se debe a que solo unos pocos comentarios discutieron en gran medida el valor. La razón podría ser cultural o porque había menos motivo para discutir esta característica. 4.3 Incentivos para informar Los modelos anteriores sugieren que los usuarios poco opinativos no elegirán expresar sus opiniones [12]. En esta sección, ampliamos este modelo para tener en cuenta la influencia de las expectativas. La motivación para enviar comentarios no se debe solo a opiniones extremas, sino también a la diferencia entre la reputación actual (es decir, la expectativa previa del usuario) y la experiencia real. Un modelo de calificación así produce calificaciones que la mayoría de las veces se desvían de la calificación promedio actual. Las calificaciones que confirman la expectativa previa rara vez serán enviadas. Probamos en nuestro conjunto de datos la proporción de calificaciones que intentan corregir la estimación actual. Definimos una calificación desviada como aquella que se desvía de la expectativa actual por al menos un umbral θ, es decir, |ri f − ef (i)| ≥ θ. Para cada una de las tres ciudades consideradas, las siguientes tablas muestran la proporción de calificaciones desviadas para θ = 0.5 y θ = 1. Los resultados anteriores sugieren que una gran proporción de usuarios (casi la mitad, incluso para el valor umbral alto θ = 1) se desvían del promedio previo. Esto refuerza la idea de que los usuarios son más propensos a enviar un informe cuando creen que tienen algo distintivo que agregar al flujo actual de opiniones sobre alguna característica. Tales conclusiones están en total acuerdo con evidencia previa de que la distribución de informes a menudo sigue distribuciones bimodales en forma de U. 139 5. Para dar cuenta de las observaciones descritas en las secciones anteriores, proponemos un modelo para el comportamiento de los usuarios al enviar reseñas en línea. Para un hotel dado, asumimos que la calidad experimentada por los usuarios sigue una distribución normal alrededor de algún valor vf, que representa la calidad objetiva ofrecida por el hotel en la característica f. La calificación enviada por el usuario i en la característica f es: ˆri f = δf vi f + (1 − δf) · sign vi f − ef (i) c + d(vi f, ef (i)|wi f) (2) donde: • vi f es la calidad (desconocida) experimentada realmente por el usuario. Se asume que vi f sigue una distribución normal alrededor de algún valor vf; • δf ∈ [0, 1] puede verse como una medida del sesgo al reportar retroalimentación. Los valores altos reflejan el hecho de que los usuarios califican de manera objetiva, sin ser influenciados por expectativas previas. El valor de δf puede depender de varios factores; fijamos un valor para cada característica f; • c es una constante entre 1 y 5; • wi f es el peso de la característica f en el comentario textual de la reseña i, calculado según la Ecuación (1); • d(vi f , ef (i)|wi f ) es una función de distancia entre la expectativa y la observación del usuario i. La función de distancia satisface las siguientes propiedades: - d(y, z|w) ≥ 0 para todo y, z ∈ [0, 5], w ∈ [0, 1]; - |d(y, z|w)| < |d(z, x|w)| si |y − z| < |z − x|; - |d(y, z|w1)| < |d(y, z|w2)| si w1 < w2; - c + d(vf , ef (i)|wi f ) ∈ [1, 5]; El segundo término de la Ecuación (2) codifica el sesgo de la calificación. Cuanto mayor sea la distancia entre la observación real vi f y la función ef, mayor será el sesgo. 5.1 Validación del modelo Utilizamos el conjunto de datos de las reseñas de TripAdvisor para validar el modelo de comportamiento presentado anteriormente. Dividimos por conveniencia los valores de calificación en tres rangos: malo (B = {1, 2}), indiferente (I = {3, 4}), y bueno (G = {5}), y realizamos los siguientes dos tests: • Primero, usaremos nuestro modelo para predecir las calificaciones que tienen valores extremos. Para cada hotel, tomamos la secuencia de informes, y cada vez que nos encontramos con una calificación que sea buena o mala (pero no indiferente) intentamos predecirla utilizando la Ecuación (2) • En segundo lugar, en lugar de predecir el valor de las calificaciones extremas, intentamos clasificarlas como buenas o malas. Para cada hotel tomamos la secuencia de informes, y para cada informe (independientemente de su valor) lo clasificamos como bueno o malo. Sin embargo, para realizar estas pruebas, necesitamos estimar el valor objetivo, vf, que es el promedio de las observaciones de calidad reales, vi f. El algoritmo que estamos utilizando se basa en la intuición de que la cantidad de calificación de conformidad se minimiza. En otras palabras, el valor vf debe ser tal que, tan a menudo como sea posible, las calificaciones malas sigan a expectativas por encima de vf y las calificaciones buenas sigan a expectativas por debajo de vf. Formalmente, definimos los conjuntos: Γ1 = {i|ef (i) < vf y ri f ∈ B}; Γ2 = {i|ef (i) > vf y ri f ∈ G}; que corresponden a irregularidades donde, a pesar de que la expectativa en el punto i es menor que el valor entregado, la calificación es baja, y viceversa. Definimos vf como el valor que minimiza la unión de los dos conjuntos: vf = arg min vf |Γ1 ∪ Γ2| (3) En la Ec. (2) reemplazamos vi f por el valor vf calculado en la Ec. (3), y utilizamos la siguiente función de distancia: d(vf , ef (i)|wi f ) = |vf − ef (i)| vf − ef (i) |vf 2 − ef (i)2 | · (1 + 2wi f ); La constante c ∈ I se estableció en min{max{ef (i), 3}}, 4}. Los valores para δf se fijaron en {0.7, 0.7, 0.8, 0.7, 0.6} para las características {General, Habitaciones, Servicio, Limpieza, Valor} respectivamente. Los pesos se calculan tal como se describe en la Sección 3. Como primer experimento, tomamos los conjuntos de calificaciones extremas {ri f |ri f /∈ I} para cada hotel y característica. Para cada calificación de este tipo, ri f, intentamos estimarla calculando ˆri f usando la Ecuación (2). Comparamos este estimador con el obtenido simplemente promediando las calificaciones de todos los hoteles y características: es decir, ¯rf = j,r j f =0 rj f j,r j f =0 1; la Tabla 7 presenta la relación entre el error cuadrático medio (RMSE) al usar ˆri f y ¯rf para estimar las calificaciones reales. En todos los casos, la estimación producida por nuestro modelo es mejor que el promedio simple. Tabla 7: Promedio de RMSE(ˆrf) RMSE(¯rf) Ciudad O R S C V Boston 0.987 0.849 0.879 0.776 0.913 Sídney 0.927 0.817 0.826 0.720 0.681 Las Vegas 0.952 0.870 0.881 0.947 0.904 Como segundo experimento, intentamos distinguir los conjuntos Bf = {i|ri f ∈ B} y Gf = {i|ri f ∈ G} de calificaciones malas y buenas, respectivamente, en la característica f. Por ejemplo, calculamos el conjunto Bf usando el siguiente clasificador (llamado σ): ri f ∈ Bf (σf (i) = 1) ⇔ ˆri f ≤ 4; Las Tablas 8, 9 y 10 presentan la Precisión (p), Recall (r) y s = 2pr p+r para el clasificador σ, y lo comparan con un clasificador de mayoría ingenuo, τ, τf (i) = 1 ⇔ |Bf | ≥ |Gf |: Vemos que el recall es siempre mayor para σ y la precisión suele ser ligeramente peor. Para la métrica s, σ tiende a agregar un 140. Tabla 8: Precisión (p), Recall (r), s= 2pr p+r mientras se detectan calificaciones bajas para Boston O R S C V p 0.678 0.670 0.573 0.545 0.610 σ r 0.626 0.659 0.619 0.612 0.694 s 0.651 0.665 0.595 0.577 0.609 p 0.684 0.706 0.647 0.611 0.633 τ r 0.597 0.541 0.410 0.383 0.562 s 0.638 0.613 0.502 0.471 0.595 Tabla 9: Precisión (p), Recall (r), s= 2pr p+r mientras se detectan calificaciones bajas para Las Vegas O R S C V p 0.654 0.748 0.592 0.712 0.583 σ r 0.608 0.536 0.791 0.474 0.610 s 0.630 0.624 0.677 0.569 0.596 p 0.685 0.761 0.621 0.748 0.606 τ r 0.542 0.505 0.767 0.445 0.441 s 0.605 0.607 0.670 0.558 0.511 Mejora del 1-20% sobre τ, mucho mayor en algunos casos para hoteles en Sídney. Esto se debe probablemente a que las reseñas de Sídney son más positivas que las de las ciudades estadounidenses y los casos en los que el número de reseñas negativas supera al de las positivas son raros. Reemplazar el algoritmo de prueba con uno que juega un 1 con una probabilidad igual a la proporción de malas críticas mejora sus resultados para esta ciudad, pero aún es superado por alrededor del 80%. 6. RESUMEN DE RESULTADOS Y CONCLUSIÓN El objetivo de este artículo es explorar los factores que llevan a un usuario a enviar una calificación particular, en lugar de los incentivos que lo animaron a enviar un informe en primer lugar. Para ello utilizamos dos fuentes adicionales de información además del vector de calificaciones numéricas: primero, examinamos los comentarios textuales que acompañan las reseñas, y segundo, consideramos los informes que han sido previamente enviados por otros usuarios. Usando algoritmos simples de procesamiento de lenguaje natural, pudimos establecer una correlación entre el peso de una característica específica en el comentario textual que acompaña la reseña, y el ruido presente en la calificación numérica. Específicamente, parece que los usuarios que discuten ampliamente una característica en particular tienden a estar de acuerdo en una calificación común. Esta observación permite la construcción de estimadores de calidad característica por característica que tienen una varianza más baja y, con suerte, son menos ruidosos. Sin embargo, se requiere más evidencia para respaldar la intuición de que las calificaciones correspondientes a pesos altos son opiniones de expertos que merecen ser consideradas de mayor prioridad al calcular estimaciones de calidad. En segundo lugar, enfatizamos la dependencia de las calificaciones en informes previos. Los informes anteriores crean una expectativa de calidad que afecta la percepción subjetiva del usuario. Validamos dos hechos sobre las reseñas de hoteles que recopilamos de TripAdvisor: Primero, las calificaciones que siguen expectativas bajas (donde la expectativa se calcula como el promedio de los informes anteriores) probablemente sean más altas que las calificaciones que siguen expectativas altas. Intuitivamente, la percepción de la calidad (y consecuentemente la calificación) depende de qué tan bien la experiencia real del usuario cumple con sus expectativas. Segundo, incluimos evidencia de los comentarios textuales, y encontramos que cuando los usuarios dedican una gran parte del texto a discutir una característica específica, es probable que motiven una calificación divergente (es decir, una calificación que no se ajusta a la expectativa previa). Intuitivamente, esto respalda la hipótesis de que los foros de reseñas actúan como grupos de discusión donde los usuarios están interesados en presentar y motivar su propia opinión. Hemos capturado la evidencia empírica en un modelo de comportamiento que predice las calificaciones enviadas por los usuarios. La calificación final depende, como era de esperar, de la observación real y de la brecha entre la observación y la expectativa. La brecha tiende a tener una influencia mayor cuando una fracción importante del comentario textual está dedicada a discutir una característica específica. El modelo propuesto fue validado con los datos empíricos y proporciona mejores estimaciones de las calificaciones realmente enviadas. Una suposición que hacemos es sobre la existencia de un valor de calidad objetivo vf para la característica f. Esto rara vez es cierto, especialmente a lo largo de largos periodos de tiempo. Otras explicaciones podrían dar cuenta de la correlación de las calificaciones con informes anteriores. Por ejemplo, si ef (i) refleja el valor real de f en un punto en el tiempo, la diferencia en las calificaciones siguiendo expectativas altas y bajas puede ser explicada por modelos de ingresos hoteleros que se maximizan cuando el valor es modificado en consecuencia. Sin embargo, la idea de que la variación en las calificaciones no es principalmente una función de la variación en el valor resulta ser útil. Nuestro enfoque para aproximar este valor objetivo es de ninguna manera perfecto, pero se ajusta perfectamente a la idea detrás del modelo. Una dirección natural para trabajos futuros es examinar aplicaciones concretas de nuestros resultados. Es probable que se obtengan mejoras significativas en las estimaciones de calidad al incorporar toda la evidencia empírica sobre el comportamiento de calificación. No está claro exactamente cómo diferentes factores afectan las decisiones de los usuarios. La respuesta podría depender de la aplicación particular, el contexto y la cultura. 7. REFERENCIAS [1] A. Admati y P. Pfleiderer. Noisytalk.com: Transmitiendo opiniones en un entorno ruidoso. Documento de trabajo 1670R, Universidad de Stanford, 2000. [2] P. B., L. Lee y S. Vaithyanathan. ¿Clasificación de sentimientos con técnicas de aprendizaje automático? En Actas de EMNLP-02, la Conferencia sobre Métodos Empíricos en el Procesamiento del Lenguaje Natural, 2002. [3] H. Cui, V. Mittal y M. Datar. Experimentos comparativos 141 sobre clasificación de sentimientos para reseñas de productos en línea. En Actas de AAAI, 2006. [4] K. Dave, S. Lawrence y D. Pennock. Extracción de opiniones y clasificación semántica de reseñas de productos en la galería de cacahuetes. En Actas de la 12ª Conferencia Internacional sobre la World Wide Web (WWW03), 2003. [5] C. Dellarocas, N. Awad y X. Zhang. Explorando el valor de las calificaciones de productos en línea en la previsión de ingresos: El caso de las películas en movimiento. Documento de trabajo, 2006. [6] C. Forman, A. Ghose y B. Wiesenfeld. Un Examen Multinivel del Impacto de las Identidades Sociales en las Transacciones Económicas en los Mercados Electrónicos. Disponible en SSRN: http://ssrn.com/abstract=918978, julio de 2006. [7] A. Ghose, P. Ipeirotis y A. Sundararajan. Primas de reputación en mercados electrónicos peer-to-peer: análisis de retroalimentación textual y estructura de red. En el Tercer Taller sobre Economía de Sistemas Peer-to-Peer (P2PECON), 2005. [8] A. Ghose, P. Ipeirotis y A. Sundararajan. Las dimensiones de la reputación en los mercados electrónicos. Documento de trabajo CeDER-06-02, Universidad de Nueva York, 2006. [9] A. Harmon. Error de Amazon revela la guerra de los revisores. The New York Times, 14 de febrero de 2004. [10] D. Houser y J. Wooders. Reputación en subastas: teoría y evidencia de eBay. Revista de Estrategia Económica y de Gestión, 15:353-369, 2006. [11] M. Hu y B. Liu. Extracción y resumen de reseñas de clientes. En Actas de la Conferencia Internacional de la ACM SIGKDD sobre Descubrimiento de Conocimiento y Minería de Datos (KDD04), 2004. [12] N. Hu, P. Pavlou y J. Zhang. ¿Pueden las reseñas en línea revelar la verdadera calidad de un producto? En Actas de la Conferencia ACM sobre Comercio Electrónico (EC 06), 2006. [13] K. Kalyanam y S. McIntyre. Retorno de la reputación en el mercado de subastas en línea. Documento de trabajo 02/03-10-WP, Escuela de Negocios Leavey, Universidad de Santa Clara, 2001. [14] L. Khopkar y P. Resnick. Auto-selección, deslizamiento, salvamento, holgazanería y lapidación: los impactos de la retroalimentación negativa en eBay. En Actas de la Conferencia ACM sobre Comercio Electrónico (EC 05), 2005. [15] M. Melnik y J. Alm. ¿Importa la reputación de un vendedor? evidencia de subastas en eBay. Revista de Economía Industrial, 50(3):337-350, 2002. [16] R. Olshavsky y J. Miller. Expectativas del consumidor, rendimiento del producto y calidad percibida del producto. Revista de Investigación de Marketing, 9:19-21, febrero de 1972. [17] A. Parasuraman, V. Zeithaml y L. Berry. Un Modelo Conceptual de Calidad de Servicio y sus Implicaciones para Investigaciones Futuras. Revista de Marketing, 49:41-50, 1985. [18] A. Parasuraman, V. Zeithaml y L. Berry. SERVQUAL: Una escala de múltiples ítems para medir las percepciones de calidad del servicio por parte del consumidor. Revista de Retail, 64:12-40, 1988. [19] P. Pavlou y A. Dimoka. La naturaleza y función de los comentarios de texto de retroalimentación en los mercados en línea: implicaciones para la construcción de confianza, primas de precio y diferenciación de vendedores. Investigación de Sistemas de Información, 17(4):392-414, 2006. [20] A. Popescu y O. Etzioni. Extrayendo características del producto y opiniones de las reseñas. En Actas de la Conferencia de Tecnología del Lenguaje Humano y Conferencia sobre Métodos Empíricos en Procesamiento del Lenguaje Natural, 2005. [21] R. Teas. Expectativas, Evaluación del Rendimiento y Percepciones de Calidad de los Consumidores. Revista de Marketing, 57:18-34, 1993. [22] E. White. Chateando a un cantante en las listas de éxitos pop. El Wall Street Journal, 15 de octubre de 1999. APÉNDICE A. LISTA DE PALABRAS, LR, ASOCIADAS A LA CARACTERÍSTICA HABITACIONES. Todas las palabras sirven como prefijos: habitación, espacio, interior, decoración, ambiente, atmósfera, comodidad, baño, inodoro, cama, edificio, pared, ventana, privado, temperatura, sábana, lino, almohada, caliente, agua, fría, agua, ducha, vestíbulo, muebles, alfombra, aire, acondicionado, colchón, distribución, diseño, espejo, techo, iluminación, lámpara, sofá, silla, cómoda, armario, armario.",
    "original_sentences": [
        "Understanding User Behavior in Online Feedback Reporting Arjun Talwar Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland arjun@math.stanford.edu Radu Jurca Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland radu.jurca@epfl.ch Boi Faltings Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland boi.faltings@epfl.ch ABSTRACT Online reviews have become increasingly popular as a way to judge the quality of various products and services.",
        "Previous work has demonstrated that contradictory reporting and underlying user biases make judging the true worth of a service difficult.",
        "In this paper, we investigate underlying factors that influence user behavior when reporting feedback.",
        "We look at two sources of information besides numerical ratings: linguistic evidence from the textual comment accompanying a review, and patterns in the time sequence of reports.",
        "We first show that groups of users who amply discuss a certain feature are more likely to agree on a common rating for that feature.",
        "Second, we show that a users rating partly reflects the difference between true quality and prior expectation of quality as inferred from previous reviews.",
        "Both give us a less noisy way to produce rating estimates and reveal the reasons behind user bias.",
        "Our hypotheses were validated by statistical evidence from hotel reviews on the TripAdvisor website.",
        "Categories and Subject Descriptors J.4 [Social and Behavioral Sciences]: Economics General Terms Economics, Experimentation, Reliability 1.",
        "MOTIVATIONS The spread of the internet has made it possible for online feedback forums (or reputation mechanisms) to become an important channel for Word-of-mouth regarding products, services or other types of commercial interactions.",
        "Numerous empirical studies [10, 15, 13, 5] show that buyers seriously consider online feedback when making purchasing decisions, and are willing to pay reputation premiums for products or services that have a good reputation.",
        "Recent analysis, however, raises important questions regarding the ability of existing forums to reflect the real quality of a product.",
        "In the absence of clear incentives, users with a moderate outlook will not bother to voice their opinions, which leads to an unrepresentative sample of reviews.",
        "For example, [12, 1] show that Amazon1 ratings of books or CDs follow with great probability bi-modal, U-shaped distributions where most of the ratings are either very good, or very bad.",
        "Controlled experiments, on the other hand, reveal opinions on the same items that are normally distributed.",
        "Under these circumstances, using the arithmetic mean to predict quality (as most forums actually do) gives the typical user an estimator with high variance that is often false.",
        "Improving the way we aggregate the information available from online reviews requires a deep understanding of the underlying factors that bias the rating behavior of users.",
        "Hu et al. [12] propose the Brag-and-Moan Model where users rate only if their utility of the product (drawn from a normal distribution) falls outside a median interval.",
        "The authors conclude that the model explains the empirical distribution of reports, and offers insights into smarter ways of estimating the true quality of the product.",
        "In the present paper we extend this line of research, and attempt to explain further facts about the behavior of users when reporting online feedback.",
        "Using actual hotel reviews from the TripAdvisor2 website, we consider two additional sources of information besides the basic numerical ratings submitted by users.",
        "The first is simple linguistic evidence from the textual review that usually accompanies the numerical ratings.",
        "We use text-mining techniques similar to [7] and [3], however, we are only interested in identifying what aspects of the service the user is discussing, without computing the semantic orientation of the text.",
        "We find that users who comment more on the same feature are more likely to agree on a common numerical rating for that particular feature.",
        "Intuitively, lengthy comments reveal the importance of the feature to the user.",
        "Since people tend to be more knowledgeable in the aspects they consider important, users who discuss a given feature in more details might be assumed to have more authority in evaluating that feature.",
        "Second we investigate the relationship between a review 1 http://www.amazon.com 2 http://www.tripadvisor.com/ 134 Figure 1: The TripAdvisor page displaying reviews for a popular Boston hotel.",
        "Name of hotel and advertisements were deliberatively erased. and the reviews that preceded it.",
        "A perusal of online reviews shows that ratings are often part of discussion threads, where one post is not necessarily independent of other posts.",
        "One may see, for example, users who make an effort to contradict, or vehemently agree with, the remarks of previous users.",
        "By analyzing the time sequence of reports, we conclude that past reviews influence the future reports, as they create some prior expectation regarding the quality of service.",
        "The subjective perception of the user is influenced by the gap between the prior expectation and the actual performance of the service [17, 18, 16, 21] which will later reflect in the users rating.",
        "We propose a model that captures the dependence of ratings on prior expectations, and validate it using the empirical data we collected.",
        "Both results can be used to improve the way reputation mechanisms aggregate the information from individual reviews.",
        "Our first result can be used to determine a featureby-feature estimate of quality, where for each feature, a different subset of reviews (i.e., those with lengthy comments of that feature) is considered.",
        "The second leads to an algorithm that outputs a more precise estimate of the real quality. 2.",
        "THE DATA SET We use in this paper real hotel reviews collected from the popular travel site TripAdvisor.",
        "TripAdvisor indexes hotels from cities across the world, along with reviews written by travelers.",
        "Users can search the site by giving the hotels name and location (optional).",
        "The reviews for a given hotel are displayed as a list (ordered from the most recent to the oldest), with 5 reviews per page.",
        "The reviews contain: • information about the author of the review (e.g., dates of stay, username of the reviewer, location of the reviewer); • the overall rating (from 1, lowest, to 5, highest); • a textual review containing a title for the review, free comments, and the main things the reviewer liked and disliked; • numerical ratings (from 1, lowest, to 5, highest) for different features (e.g., cleanliness, service, location, etc.)",
        "Below the name of the hotel, TripAdvisor displays the address of the hotel, general information (number of rooms, number of stars, short description, etc), the average overall rating, the TripAdvisor ranking, and an average rating for each feature.",
        "Figure 1 shows the page for a popular Boston hotel whose name (along with advertisements) was explicitly erased.",
        "We selected three cities for this study: Boston, Sydney and Las Vegas.",
        "For each city we considered all hotels that had at least 10 reviews, and recorded all reviews.",
        "Table 1 presents the number of hotels considered in each city, the total number of reviews recorded for each city, and the distribution of hotels with respect to the star-rating (as available on the TripAdvisor site).",
        "Note that not all hotels have a star-rating.",
        "Table 1: A summary of the data set.",
        "City # Reviews # Hotels # of Hotels with 1,2,3,4 & 5 stars Boston 3993 58 1+3+17+15+2 Sydney 1371 47 0+0+9+13+10 Las Vegas 5593 40 0+3+10+9+6 For each review we recorded the overall rating, the textual review (title and body of the review) and the numerical rating on 7 features: Rooms(R), Service(S), Cleanliness(C), Value(V), Food(F), Location(L) and Noise(N).",
        "TripAdvisor does not require users to submit anything other than the overall rating, hence a typical review rates few additional features, regardless of the discussion in the textual comment.",
        "Only the features Rooms(R), Service(S), Cleanliness(C) and Value(V) are rated by a significant number of users.",
        "However, we also selected the features Food(F), Location(L) and Noise(N) because they are referred to in a significant number of textual comments.",
        "For each feature we record the numerical rating given by the user, or 0 when the rating is missing.",
        "The typical length of the textual comment amounts to approximately 200 words.",
        "All data was collected by crawling the TripAdvisor site in September 2006. 2.1 Formal notation We will formally refer to a review by a tuple (r, T) where: • r = (rf ) is a vector containing the ratings rf ∈ {0, 1, . . . 5} for the features f ∈ F = {O, R, S, C, V, F, L, N}; note that the overall rating, rO, is abusively recorded as the rating for the feature Overall(O); • T is the textual comment that accompanies the review. 135 Reviews are indexed according to the variable i, such that (ri , Ti ) is the ith review in our database.",
        "Since we dont record the username of the reviewer, we will also say that the ith review in our data set was submitted by user i.",
        "When we need to consider only the reviews of a given hotel, h, we will use (ri(h) , Ti(h) ) to denote the ith review about the hotel h. 3.",
        "EVIDENCE FROM TEXTUAL COMMENTS The free textual comments associated to online reviews are a valuable source of information for understanding the reasons behind the numerical ratings left by the reviewers.",
        "The text may, for example, reveal concrete examples of aspects that the user liked or disliked, thus justifying some of the high, respectively low ratings for certain features.",
        "The text may also offer guidelines for understanding the preferences of the reviewer, and the weights of different features when computing an overall rating.",
        "The problem, however, is that free textual comments are difficult to read.",
        "Users are required to scroll through many reviews and read mostly repetitive information.",
        "Significant improvements would be obtained if the reviews were automatically interpreted and aggregated.",
        "Unfortunately, this seems a difficult task for computers since human users often use witty language, abbreviations, cultural specific phrases, and the figurative style.",
        "Nevertheless, several important results use the textual comments of online reviews in an automated way.",
        "Using well established natural language techniques, reviews or parts of reviews can be classified as having a positive or negative semantic orientation.",
        "Pang et al. [2] classify movie reviews into positive/negative by training three different classifiers (Naive Bayes, Maximum Entropy and SVM) using classification features based on unigrams, bigrams or part-of-speech tags.",
        "Dave et al. [4] analyze reviews from CNet and Amazon, and surprisingly show that classification features based on unigrams or bigrams perform better than higher-order n-grams.",
        "This result is challenged by Cui et al. [3] who look at large collections of reviews crawled from the web.",
        "They show that the size of the data set is important, and that bigger training sets allow classifiers to successfully use more complex classification features based on n-grams.",
        "Hu and Liu [11] also crawl the web for product reviews and automatically identify product attributes that have been discussed by reviewers.",
        "They use Wordnet to compute the semantic orientation of product evaluations and summarize user reviews by extracting positive and negative evaluations of different product features.",
        "Popescu and Etzioni [20] analyze a similar setting, but use search engine hit-counts to identify product attributes; the semantic orientation is assigned through the relaxation labeling technique.",
        "Ghose et al. [7, 8] analyze seller reviews from the Amazon secondary market to identify the different dimensions (e.g., delivery, packaging, customer support, etc.) of reputation.",
        "They parse the text, and tag the part-of-speech for each word.",
        "Frequent nouns, noun phrases and verbal phrases are identified as dimensions of reputation, while the corresponding modifiers (i.e., adjectives and adverbs) are used to derive numerical scores for each dimension.",
        "The enhanced reputation measure correlates better with the pricing information observed in the market.",
        "Pavlou and Dimoka [19] analyze eBay reviews and find that textual comments have an important impact on reputation premiums.",
        "Our approach is similar to the previously mentioned works, in the sense that we identify the aspects (i.e., hotel features) discussed by the users in the textual reviews.",
        "However, we do not compute the semantic orientation of the text, nor attempt to infer missing ratings.",
        "We define the weight, wi f , of feature f ∈ F in the text Ti associated with the review (ri , Ti ), as the fraction of Ti dedicated to discussing aspects (both positive and negative) related to feature f. We propose an elementary method to approximate the values of these weights.",
        "For each feature we manually construct the word list Lf containing approximately 50 words that are most commonly associated to the feature f. The initial words were selected from reading some of the reviews, and seeing what words coincide with discussion of which features.",
        "The list was then extended by adding all thesaurus entries that were related to the initial words.",
        "Finally, we brainstormed for missing words that would normally be associated with each of the features.",
        "Let Lf ∩Ti be the list of terms common to both Lf and Ti.",
        "Each term of Lf is counted the number of times it appears in Ti , with two exceptions: • in cases where the user submits a title to the review, we account for the title text by appending it three times to the review text Ti .",
        "The intuitive assumption is that the users opinion is more strongly reflected in the title, rather than in the body of the review.",
        "For example, many reviews are accurately summarized by titles such as Excellent service, terrible location or Bad value for money; • certain words that occur only once in the text are counted multiple times if their relevance to that feature is particularly strong.",
        "These were root words for each feature (e.g., staff is a root word for the feature Service), and were weighted either 2 or 3.",
        "Each feature was assigned up to 3 such root words, so almost all words are counted only once.",
        "The list of words for the feature Rooms is given for reference in Appendix A.",
        "The weight wi f is computed as: wi f = |Lf ∩ Ti| f∈F |Lf ∩ Ti| (1) where |Lf ∩Ti | is the number of terms common to Lf and Ti .",
        "The weight for the feature Overall was set to min{ |T i | 5000 , 1} where |Ti | is the number of character in Ti .",
        "The following is a TripAdvisor review for a Boston hotel (the name of the hotel is omitted): Ill start by saying that Im more of a Holiday Inn person than a *** type.",
        "So I get frustrated when I pay double the room rate and get half the amenities that Id get at a Hampton Inn or Holiday Inn.",
        "The location was definitely the main asset of this place.",
        "It was only a few blocks from the Hynes Center subway stop and it was easy to walk to some good restaurants in the Back Bay area.",
        "Boylston isnt far off at all.",
        "So I had no trouble with foregoing a rental car and taking the subway from the airport to the hotel and using the subway for any other travel.",
        "Otherwise, they make you pay for anything and everything. 136 And when youve already dropped $215/night on the room, that gets frustrating.The room itself was decent, about what I would expect.",
        "Staff was also average, not bad and not excellent.",
        "Again, I think youre paying for location and the ability to walk to a lot of good stuff.",
        "But I think next time Ill stay in Brookline, get more amenities, and use the subway a bit more.",
        "This numerical ratings associated to this review are rO = 3, rR = 3, rS = 3, rC = 4, rV = 2 for features Overall(O), Rooms(R), Service(S), Cleanliness(C) and Value(V) respectively.",
        "The ratings for the features Food(F), Location(L) and Noise(N) are absent (i.e., rF = rL = rN = 0).",
        "The weights wf are computed from the following lists of common terms: LR ∩ T ={room}; wR = 0.066 LS ∩ T ={3 * Staff, amenities}; wS = 0.267 LC ∩ T = ∅; wC = 0 LV ∩ T ={$, rate}; wV = 0.133 LF ∩ T ={restaurant}; wF = 0.067 LL ∩ T ={2 * center, 2 * walk, 2 * location, area}; wL = 0.467 LN ∩ T = ∅; wN = 0 The root words Staff and Center were tripled and doubled respectively.",
        "The overall weight of the textual review is wO = 0.197.",
        "These values account reasonably well for the weights of different features in the discussion of the reviewer.",
        "One point to note is that some terms in the lists Lf possess an inherent semantic orientation.",
        "For example the word grime (belonging to the list LC ) would be used most often to assert the presence, and not the absence of grime.",
        "This is unavoidable, but care was taken to ensure words from both sides of the spectrum were used.",
        "For this reason, some lists such as LR contain only nouns of objects that one would typically describe in a room (see Appendix A).",
        "The goal of this section is to analyse the influence of the weights wi f on the numerical ratings ri f .",
        "Intuitively, users who spent a lot of their time discussing a feature f (i.e., wi f is high) had something to say about their experience with regard to this feature.",
        "Obviously, feature f is important for user i.",
        "Since people tend to be more knowledgeable in the aspects they consider important, our hypothesis is that the ratings ri f (corresponding to high weights wi f ) constitute a subset of expert ratings for feature f. Figure 2 plots the distribution of the rates r i(h) C with respect to the weights w i(h) C for the cleanliness of a Las Vegas hotel, h. Here, the high ratings are restricted to the reviews that discuss little the cleanliness.",
        "Whenever cleanliness appears in the discussion, the ratings are low.",
        "Many hotels exhibit similar rating patterns for various features.",
        "Ratings corresponding to low weights span the whole spectrum from 1 to 5, while the ratings corresponding to high weights are more grouped together (either around good or bad ratings).",
        "We therefore make the following hypothesis: Hypothesis 1.",
        "The ratings ri f corresponding to the reviews where wi f is high, are more similar to each other than to the overall collection of ratings.",
        "To test the hypothesis, we take the entire set of reviews, and feature by feature, we compute the standard deviation of the ratings with high weights, and the standard deviation of the entire set of ratings.",
        "High weights were defined as those belonging to the upper 20% of the weight range for the corresponding feature.",
        "If Hypothesis 1 were true, the standard deviation of all ratings should be higher than the standard deviation of the ratings with high weights. 0 1 2 3 4 5 6 0 0.1 0.2 0.3 0.4 0.5 0.6 Rating Weight Figure 2: The distribution of ratings against the weight of the cleanliness feature.",
        "We use a standard T-test to measure the significance of the results.",
        "City by city and feature by feature, Table 2 presents the average standard deviation of all ratings, and the average standard deviation of ratings with high weights.",
        "Indeed, the ratings with high weights have lower standard deviation, and the results are significant at the standard 0.05 significance threshold (although for certain cities taken independently there doesnt seem to be a significant difference, the results are significant for the entire data set).",
        "Please note that only the features O,R,S,C and V were considered, since for the others (F, L, and N) we didnt have enough ratings.",
        "Table 2: Average standard deviation for all ratings, and average standard deviation for ratings with high weights.",
        "In square brackets, the corresponding p-values for a positive difference between the two.",
        "City O R S C V all 1.189 0.998 1.144 0.935 1.123 Boston high 0.948 0.778 0.954 0.767 0.891 p-val [0.000] [0.004] [0.045] [0.080] [0.009] all 1.040 0.832 1.101 0.847 0.963 Sydney high 0.801 0.618 0.691 0.690 0.798 p-val [0.012] [0.023] [0.000] [0.377] [0.037] all 1.272 1.142 1.184 1.119 1.242 Vegas high 1.072 0.752 1.169 0.907 1.003 p-val [0.0185] [0.001] [0.918] [0.120] [0.126] Hypothesis 1 not only provides some basic understanding regarding the rating behavior of online users, it also suggests some ways of computing better quality estimates.",
        "We can, for example, construct a feature-by-feature quality estimate with much lower variance: for each feature we take the subset of reviews that amply discuss that feature, and output as a quality estimate the average rating for this subset.",
        "Initial experiments suggest that the average feature-by-feature ratings computed in this way are different from the average ratings computed on the whole data set.",
        "Given that, indeed, high weights are indicators of expert opinions, the estimates obtained in this way are more accurate than the current ones.",
        "Nevertheless, the validation of this underlying assumption requires further controlled experiments. 137 4.",
        "THE INFLUENCE OF PAST RATINGS Two important assumptions are generally made about reviews submitted to online forums.",
        "The first is that ratings truthfully reflect the quality observed by the users; the second is that reviews are independent from one another.",
        "While anecdotal evidence [9, 22] challenges the first assumption3 , in this section, we address the second.",
        "A perusal of online reviews shows that reviews are often part of discussion threads, where users make an effort to contradict, or vehemently agree with the remarks of previous users.",
        "Consider, for example, the following review: I dont understand the negative reviews... the hotel was a little dark, but that was the style.",
        "It was very artsy.",
        "Yes it was close to the freeway, but in my opinion the sound of an occasional loud car is better than hearing the ding ding of slot machines all night!",
        "The staff on-hand is FABULOUS.",
        "The waitresses are great (and *** does not deserve the bad review she got, she was 100% attentive to us! ), the bartenders are friendly and professional at the same time...",
        "Here, the user was disturbed by previous negative reports, addressed these concerns, and set about trying to correct them.",
        "Not surprisingly, his ratings were considerably higher than the average ratings up to this point.",
        "It seems that TripAdvisor users regularly read the reports submitted by previous users before booking a hotel, or before writing a review.",
        "Past reviews create some prior expectation regarding the quality of service, and this expectation has an influence on the submitted review.",
        "We believe this observation holds for most online forums.",
        "The subjective perception of quality is directly proportional to how well the actual experience meets the prior expectation, a fact confirmed by an important line of econometric and marketing research [17, 18, 16, 21].",
        "The correlation between the reviews has also been confirmed by recent research on the dynamics of online review forums [6]. 4.1 Prior Expectations We define the prior expectation of user i regarding the feature f, as the average of the previously available ratings on the feature f4 : ef (i) = j<i,r j f =0 rj f j<i,r j f =0 1 As a first hypothesis, we assert that the rating ri f is a function of the prior expectation ef (i): Hypothesis 2.",
        "For a given hotel and feature, given the reviews i and j such that ef (i) is high and ef (j) is low, the rating rj f exceeds the rating ri f .",
        "We define high and low expectations as those that are above, respectively below a certain cutoff value θ.",
        "The set of reviews preceded by high, respectively low expectations 3 part of Amazon reviews were recognized as strategic posts by book authors or competitors 4 if no previous ratings were assigned for feature f, ef (i) is assigned a default value of 4.",
        "Table 3: Average ratings for reviews preceded by low (first value in the cell) and high (second value in the cell) expectations.",
        "The P-values for a positive difference are given square brackets.",
        "City O R S C V 3.953 4.045 3.985 4.252 3.946 Boston 3.364 3.590 3.485 3.641 3.242 [0.011] [0.028] [0.0086] [0.0168] [0.0034] 4.284 4.358 4.064 4.530 4.428 Sydney 3.756 3.537 3.436 3.918 3.495 [0.000] [0.000] [0.035] [0.009] [0.000] 3.494 3.674 3.713 3.689 3.580 Las Vegas 3.140 3.530 2.952 3.530 3.351 [0.190] [0.529] [0.007] [0.529] [0.253] are defined as follows: Rhigh f = {ri f |ef (i) > θ} Rlow f = {ri f |ef (i) < θ} These sets are specific for each (hotel, feature) pair, and in our experiments we took θ = 4.",
        "This rather high value is close to the average rating across all features across all hotels, and is justified by the fact that our data set contains mostly high quality hotels.",
        "For each city, we take all hotels and compute the average ratings in the sets Rhigh f and Rlow f (see Table 3).",
        "The average rating amongst reviews following low prior expectations is significantly higher than the average rating following high expectations.",
        "As further evidence, we consider all hotels for which the function eV (i) (the expectation for the feature Value) has a high value (greater than 4) for some i, and a low value (less than 4) for some other i.",
        "Intuitively, these are the hotels for which there is a minimal degree of variation in the timely sequence of reviews: i.e., the cumulative average of ratings was at some point high and afterwards became low, or vice-versa.",
        "Such variations are observed for about half of all hotels in each city.",
        "Figure 3 plots the median (across considered hotels) rating, rV , when ef (i) is not more than x but greater than x − 0.5. 2.5 3 3.5 4 4.5 5 2.5 3 3.5 4 4.5 5 Medianofrating expectation Boston Sydney Vegas Figure 3: The ratings tend to decrease as the expectation increases. 138 There are two ways to interpret the function ef (i): • The expected value for feature f obtained by user i before his experience with the service, acquired by reading reports submitted by past users.",
        "In this case, an overly high value for ef (i) would drive the user to submit a negative report (or vice versa), stemming from the difference between the actual value of the service, and the inflated expectation of this value acquired before his experience. • The expected value of feature f for all subsequent visitors of the site, if user i were not to submit a report.",
        "In this case, the motivation for a negative report following an overly high value of ef is different: user i seeks to correct the expectation of future visitors to the site.",
        "Unlike the interpretation above, this does not require the user to derive an a priori expectation for the value of f. Note that neither interpretation implies that the average up to report i is inversely related to the rating at report i.",
        "There might exist a measure of influence exerted by past reports that pushes the user behind report i to submit ratings which to some extent conforms with past reports: a low value for ef (i) can influence user i to submit a low rating for feature f because, for example, he fears that submitting a high rating will make him out to be a person with low standards5 .",
        "This, at first, appears to contradict Hypothesis 2.",
        "However, this conformity rating cannot continue indefinitely: once the set of reports project a sufficiently deflated estimate for vf , future reviewers with comparatively positive impressions will seek to correct this misconception. 4.2 Impact of textual comments on quality expectation Further insight into the rating behavior of TripAdvisor users can be obtained by analyzing the relationship between the weights wf and the values ef (i).",
        "In particular, we examine the following hypothesis: Hypothesis 3.",
        "When a large proportion of the text of a review discusses a certain feature, the difference between the rating for that feature and the average rating up to that point tends to be large.",
        "The intuition behind this claim is that when the user is adamant about voicing his opinion regarding a certain feature, his opinion differs from the collective opinion of previous postings.",
        "This relies on the characteristic of reputation systems as feedback forums where a user is interested in projecting his opinion, with particular strength if this opinion differs from what he perceives to be the general opinion.",
        "To test Hypothesis 3 we measure the average absolute difference between the expectation ef (i) and the rating ri f when the weight wi f is high, respectively low.",
        "Weights are classified high or low by comparing them with certain cutoff values: wi f is low if smaller than 0.1, while wi f is high if greater than θf .",
        "Different cutoff values were used for different features: θR = 0.4, θS = 0.4, θC = 0.2, and θV = 0.7.",
        "Cleanliness has a lower cutoff since it is a feature rarely discussed; Value has a high cutoff for the opposite reason.",
        "Results are presented in Table 4. 5 The idea that negative reports can encourage further negative reporting has been suggested before [14] Table 4: Average of |ri f −ef (i)| when weights are high (first value in the cell) and low (second value in the cell) with P-values for the difference in sq. brackets.",
        "City R S C V 1.058 1.208 1.728 1.356 Boston 0.701 0.838 0.760 0.917 [0.022] [0.063] [0.000] [0.218] 1.048 1.351 1.218 1.318 Sydney 0.752 0.759 0.767 0.908 [0.179] [0.009] [0.165] [0.495] 1.184 1.378 1.472 1.642 Las Vegas 0.772 0.834 0.808 1.043 [0.071] [0.020] [0.006] [0.076] This demonstrates that when weights are unusually high, users tend to express an opinion that does not conform to the net average of previous ratings.",
        "As we might expect, for a feature that rarely was a high weight in the discussion, (e.g., cleanliness) the difference is particularly large.",
        "Even though the difference in the feature Value is quite large for Sydney, the P-value is high.",
        "This is because only few reviews discussed value heavily.",
        "The reason could be cultural or because there was less of a reason to discuss this feature. 4.3 Reporting Incentives Previous models suggest that users who are not highly opinionated will not choose to voice their opinions [12].",
        "In this section, we extend this model to account for the influence of expectations.",
        "The motivation for submitting feedback is not only due to extreme opinions, but also to the difference between the current reputation (i.e., the prior expectation of the user) and the actual experience.",
        "Such a rating model produces ratings that most of the time deviate from the current average rating.",
        "The ratings that confirm the prior expectation will rarely be submitted.",
        "We test on our data set the proportion of ratings that attempt to correct the current estimate.",
        "We define a deviant rating as one that deviates from the current expectation by at least some threshold θ, i.e., |ri f − ef (i)| ≥ θ.",
        "For each of the three considered cities, the following tables, show the proportion of deviant ratings for θ = 0.5 and θ = 1.",
        "Table 5: Proportion of deviant ratings with θ = 0.5 City O R S C V Boston 0.696 0.619 0.676 0.604 0.684 Sydney 0.645 0.615 0.672 0.614 0.675 Las Vegas 0.721 0.641 0.694 0.662 0.724 Table 6: Proportion of deviant ratings with θ = 1 City O R S C V Boston 0.420 0.397 0.429 0.317 0.446 Sydney 0.360 0.367 0.442 0.336 0.489 Las Vegas 0.510 0.421 0.483 0.390 0.472 The above results suggest that a large proportion of users (close to one half, even for the high threshold value θ = 1) deviate from the prior average.",
        "This reinforces the idea that users are more likely to submit a report when they believe they have something distinctive to add to the current stream of opinions for some feature.",
        "Such conclusions are in total agreement with prior evidence that the distribution of reports often follows bi-modal, U-shaped distributions. 139 5.",
        "MODELLING THE BEHAVIOR OF RATERS To account for the observations described in the previous sections, we propose a model for the behavior of the users when submitting online reviews.",
        "For a given hotel, we make the assumption that the quality experienced by the users is normally distributed around some value vf , which represents the objective quality offered by the hotel on the feature f. The rating submitted by user i on feature f is: ˆri f = δf vi f + (1 − δf ) · sign vi f − ef (i) c + d(vi f , ef (i)|wi f ) (2) where: • vi f is the (unknown) quality actually experienced by the user. vi f is assumed normally distributed around some value vf ; • δf ∈ [0, 1] can be seen as a measure of the bias when reporting feedback.",
        "High values reflect the fact that users rate objectively, without being influenced by prior expectations.",
        "The value of δf may depend on various factors; we fix one value for each feature f; • c is a constant between 1 and 5; • wi f is the weight of feature f in the textual comment of review i, computed according to Eq. (1); • d(vi f , ef (i)|wi f ) is a distance function between the expectation and the observation of user i.",
        "The distance function satisfies the following properties: - d(y, z|w) ≥ 0 for all y, z ∈ [0, 5], w ∈ [0, 1]; - |d(y, z|w)| < |d(z, x|w)| if |y − z| < |z − x|; - |d(y, z|w1)| < |d(y, z|w2)| if w1 < w2; - c + d(vf , ef (i)|wi f ) ∈ [1, 5]; The second term of Eq. (2) encodes the bias of the rating.",
        "The higher the distance between the true observation vi f and the function ef , the higher the bias. 5.1 Model Validation We use the data set of TripAdvisor reviews to validate the behavior model presented above.",
        "We split for convenience the rating values in three ranges: bad (B = {1, 2}), indifferent (I = {3, 4}), and good (G = {5}), and perform the following two tests: • First, we will use our model to predict the ratings that have extremal values.",
        "For every hotel, we take the sequence of reports, and whenever we encounter a rating that is either good or bad (but not indifferent) we try to predict it using Eq. (2) • Second, instead of predicting the value of extremal ratings, we try to classify them as either good or bad.",
        "For every hotel we take the sequence of reports, and for each report (regardless of it value) we classify it as being good or bad However, to perform these tests, we need to estimate the objective value, vf , that is the average of the true quality observations, vi f .",
        "The algorithm we are using is based on the intuition that the amount of conformity rating is minimized.",
        "In other words, the value vf should be such that as often as possible, bad ratings follow expectations above vf and good ratings follow expectations below vf .",
        "Formally, we define the sets: Γ1 = {i|ef (i) < vf and ri f ∈ B}; Γ2 = {i|ef (i) > vf and ri f ∈ G}; that correspond to irregularities where even though the expectation at point i is lower than the delivered value, the rating is poor, and vice versa.",
        "We define vf as the value that minimize these union of the two sets: vf = arg min vf |Γ1 ∪ Γ2| (3) In Eq. (2) we replace vi f by the value vf computed in Eq. (3), and use the following distance function: d(vf , ef (i)|wi f ) = |vf − ef (i)| vf − ef (i) |vf 2 − ef (i)2 | · (1 + 2wi f ); The constant c ∈ I was set to min{max{ef (i), 3}}, 4}.",
        "The values for δf were fixed at {0.7, 0.7, 0.8, 0.7, 0.6} for the features {Overall, Rooms, Service, Cleanliness, Value} respectively.",
        "The weights are computed as described in Section 3.",
        "As a first experiment, we take the sets of extremal ratings {ri f |ri f /∈ I} for each hotel and feature.",
        "For every such rating, ri f , we try to estimate it by computing ˆri f using Eq. (2).",
        "We compare this estimator with the one obtained by simply averaging the ratings over all hotels and features: i.e., ¯rf = j,r j f =0 rj f j,r j f =0 1 ; Table 7 presents the ratio between the root mean square error (RMSE) when using ˆri f and ¯rf to estimate the actual ratings.",
        "In all cases the estimate produced by our model is better than the simple average.",
        "Table 7: Average of RMSE(ˆrf ) RMSE(¯rf ) City O R S C V Boston 0.987 0.849 0.879 0.776 0.913 Sydney 0.927 0.817 0.826 0.720 0.681 Las Vegas 0.952 0.870 0.881 0.947 0.904 As a second experiment, we try to distinguish the sets Bf = {i|ri f ∈ B} and Gf = {i|ri f ∈ G} of bad, respectively good ratings on the feature f. For example, we compute the set Bf using the following classifier (called σ): ri f ∈ Bf (σf (i) = 1) ⇔ ˆri f ≤ 4; Tables 8, 9 and 10 present the Precision(p), Recall(r) and s = 2pr p+r for classifier σ, and compares it with a naive majority classifier, τ, τf (i) = 1 ⇔ |Bf | ≥ |Gf |: We see that recall is always higher for σ and precision is usually slightly worse.",
        "For the s metric σ tends to add a 140 Table 8: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Boston O R S C V p 0.678 0.670 0.573 0.545 0.610 σ r 0.626 0.659 0.619 0.612 0.694 s 0.651 0.665 0.595 0.577 0.609 p 0.684 0.706 0.647 0.611 0.633 τ r 0.597 0.541 0.410 0.383 0.562 s 0.638 0.613 0.502 0.471 0.595 Table 9: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Las Vegas O R S C V p 0.654 0.748 0.592 0.712 0.583 σ r 0.608 0.536 0.791 0.474 0.610 s 0.630 0.624 0.677 0.569 0.596 p 0.685 0.761 0.621 0.748 0.606 τ r 0.542 0.505 0.767 0.445 0.441 s 0.605 0.607 0.670 0.558 0.511 1-20% improvement over τ, much higher in some cases for hotels in Sydney.",
        "This is likely because Sydney reviews are more positive than those of the American cities and cases where the number of bad reviews exceeded the number of good ones are rare.",
        "Replacing the test algorithm with one that plays a 1 with probability equal to the proportion of bad reviews improves its results for this city, but it is still outperformed by around 80%. 6.",
        "SUMMARY OF RESULTS AND CONCLUSION The goal of this paper is to explore the factors that drive a user to submit a particular rating, rather than the incentives that encouraged him to submit a report in the first place.",
        "For that we use two additional sources of information besides the vector of numerical ratings: first we look at the textual comments that accompany the reviews, and second we consider the reports that have been previously submitted by other users.",
        "Using simple natural language processing algorithms, we were able to establish a correlation between the weight of a certain feature in the textual comment accompanying the review, and the noise present in the numerical rating.",
        "Specifically, it seems that users who discuss amply a certain feature are likely to agree on a common rating.",
        "This observation allows the construction of feature-by-feature estimators of quality that have a lower variance, and are hopefully less noisy.",
        "Nevertheless, further evidence is required to support the intuition that ratings corresponding to high weights are expert opinions that deserve to be given higher priority when computing estimates of quality.",
        "Second, we emphasize the dependence of ratings on previous reports.",
        "Previous reports create an expectation of quality which affects the subjective perception of the user.",
        "We validate two facts about the hotel reviews we collected from TripAdvisor: First, the ratings following low expectations (where the expectation is computed as the average of the previous reports) are likely to be higher than the ratings Table 10: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Sydney O R S C V p 0.650 0.463 0.544 0.550 0.580 σ r 0.234 0.378 0.571 0.169 0.592 s 0.343 0.452 0.557 0.259 0.586 p 0.562 0.615 0.600 0.500 0.600 τ r 0.054 0.098 0.101 0.015 0.175 s 0.098 0.168 0.172 0.030 0.271 following high expectations.",
        "Intuitively, the perception of quality (and consequently the rating) depends on how well the actual experience of the user meets her expectation.",
        "Second, we include evidence from the textual comments, and find that when users devote a large fraction of the text to discussing a certain feature, they are likely to motivate a divergent rating (i.e., a rating that does not conform to the prior expectation).",
        "Intuitively, this supports the hypothesis that review forums act as discussion groups where users are keen on presenting and motivating their own opinion.",
        "We have captured the empirical evidence in a behavior model that predicts the ratings submitted by the users.",
        "The final rating depends, as expected, on the true observation, and on the gap between the observation and the expectation.",
        "The gap tends to have a bigger influence when an important fraction of the textual comment is dedicated to discussing a certain feature.",
        "The proposed model was validated on the empirical data and provides better estimates of the ratings actually submitted.",
        "One assumption that we make is about the existence of an objective quality value vf for the feature f. This is rarely true, especially over large spans of time.",
        "Other explanations might account for the correlation of ratings with past reports.",
        "For example, if ef (i) reflects the true value of f at a point in time, the difference in the ratings following high and low expectations can be explained by hotel revenue models that are maximized when the value is modified accordingly.",
        "However, the idea that variation in ratings is not primarily a function of variation in value turns out to be a useful one.",
        "Our approach to approximate this elusive objective value is by no means perfect, but conforms neatly to the idea behind the model.",
        "A natural direction for future work is to examine concrete applications of our results.",
        "Significant improvements of quality estimates are likely to be obtained by incorporating all empirical evidence about rating behavior.",
        "Exactly how different factors affect the decisions of the users is not clear.",
        "The answer might depend on the particular application, context and culture. 7.",
        "REFERENCES [1] A. Admati and P. Pfleiderer.",
        "Noisytalk.com: Broadcasting opinions in a noisy environment.",
        "Working Paper 1670R, Stanford University, 2000. [2] P. B., L. Lee, and S. Vaithyanathan.",
        "Thumbs up? sentiment classification using machine learning techniques.",
        "In Proceedings of the EMNLP-02, the Conference on Empirical Methods in Natural Language Processing, 2002. [3] H. Cui, V. Mittal, and M. Datar.",
        "Comparative 141 Experiments on Sentiment Classification for Online Product Reviews.",
        "In Proceedings of AAAI, 2006. [4] K. Dave, S. Lawrence, and D. Pennock.",
        "Mining the peanut gallery:opinion extraction and semantic classification of product reviews.",
        "In Proceedings of the 12th International Conference on the World Wide Web (WWW03), 2003. [5] C. Dellarocas, N. Awad, and X. Zhang.",
        "Exploring the Value of Online Product Ratings in Revenue Forecasting: The Case of Motion Pictures.",
        "Working paper, 2006. [6] C. Forman, A. Ghose, and B. Wiesenfeld.",
        "A Multi-Level Examination of the Impact of Social Identities on Economic Transactions in Electronic Markets.",
        "Available at SSRN: http://ssrn.com/abstract=918978, July 2006. [7] A. Ghose, P. Ipeirotis, and A. Sundararajan.",
        "Reputation Premiums in Electronic Peer-to-Peer Markets: Analyzing Textual Feedback and Network Structure.",
        "In Third Workshop on Economics of Peer-to-Peer Systems, (P2PECON), 2005. [8] A. Ghose, P. Ipeirotis, and A. Sundararajan.",
        "The Dimensions of Reputation in electronic Markets.",
        "Working Paper CeDER-06-02, New York University, 2006. [9] A. Harmon.",
        "Amazon Glitch Unmasks War of Reviewers.",
        "The New York Times, February 14, 2004. [10] D. Houser and J. Wooders.",
        "Reputation in Auctions: Theory and Evidence from eBay.",
        "Journal of Economics and Management Strategy, 15:353-369, 2006. [11] M. Hu and B. Liu.",
        "Mining and summarizing customer reviews.",
        "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD04), 2004. [12] N. Hu, P. Pavlou, and J. Zhang.",
        "Can Online Reviews Reveal a Products True Quality?",
        "In Proceedings of ACM Conference on Electronic Commerce (EC 06), 2006. [13] K. Kalyanam and S. McIntyre.",
        "Return on reputation in online auction market.",
        "Working Paper 02/03-10-WP, Leavey School of Business, Santa Clara University., 2001. [14] L. Khopkar and P. Resnick.",
        "Self-Selection, Slipping, Salvaging, Slacking, and Stoning: the Impacts of Negative Feedback at eBay.",
        "In Proceedings of ACM Conference on Electronic Commerce (EC 05), 2005. [15] M. Melnik and J. Alm.",
        "Does a sellers reputation matter? evidence from ebay auctions.",
        "Journal of Industrial Economics, 50(3):337-350, 2002. [16] R. Olshavsky and J. Miller.",
        "Consumer Expectations, Product Performance and Perceived Product Quality.",
        "Journal of Marketing Research, 9:19-21, February 1972. [17] A. Parasuraman, V. Zeithaml, and L. Berry.",
        "A Conceptual Model of Service Quality and Its Implications for Future Research.",
        "Journal of Marketing, 49:41-50, 1985. [18] A. Parasuraman, V. Zeithaml, and L. Berry.",
        "SERVQUAL: A Multiple-Item Scale for Measuring Consumer Perceptions of Service Quality.",
        "Journal of Retailing, 64:12-40, 1988. [19] P. Pavlou and A. Dimoka.",
        "The Nature and Role of Feedback Text Comments in Online Marketplaces: Implications for Trust Building, Price Premiums, and Seller Differentiation.",
        "Information Systems Research, 17(4):392-414, 2006. [20] A. Popescu and O. Etzioni.",
        "Extracting product features and opinions from reviews.",
        "In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, 2005. [21] R. Teas.",
        "Expectations, Performance Evaluation, and Consumers Perceptions of Quality.",
        "Journal of Marketing, 57:18-34, 1993. [22] E. White.",
        "Chatting a Singer Up the Pop Charts.",
        "The Wall Street Journal, October 15, 1999.",
        "APPENDIX A.",
        "LIST OF WORDS, LR, ASSOCIATED TO THE FEATURE ROOMS All words serve as prefixes: room, space, interior, decor, ambiance, atmosphere, comfort, bath, toilet, bed, building, wall, window, private, temperature, sheet, linen, pillow, hot, water, cold, water, shower, lobby, furniture, carpet, air, condition, mattress, layout, design, mirror, ceiling, lighting, lamp, sofa, chair, dresser, wardrobe, closet 142"
    ],
    "translated_text_sentences": [
        "La comprensión del comportamiento del usuario en la presentación de comentarios en línea.",
        "Trabajos anteriores han demostrado que la información contradictoria y los sesgos subyacentes de los usuarios dificultan juzgar el verdadero valor de un servicio.",
        "En este artículo, investigamos los factores subyacentes que influyen en el comportamiento del usuario al informar retroalimentación.",
        "Examinamos dos fuentes de información además de las calificaciones numéricas: la evidencia lingüística del comentario textual que acompaña a una reseña y los patrones en la secuencia temporal de los informes.",
        "Primero demostramos que los grupos de usuarios que discuten ampliamente una característica específica tienen más probabilidades de ponerse de acuerdo en una calificación común para esa característica.",
        "Segundo, demostramos que la calificación de un usuario refleja en parte la diferencia entre la calidad real y la expectativa previa de calidad, tal como se infiere de revisiones anteriores.",
        "Ambos nos ofrecen una forma menos ruidosa de producir estimaciones de calificación y revelar las razones detrás del sesgo del usuario.",
        "Nuestras hipótesis fueron validadas por evidencia estadística de reseñas de hoteles en el sitio web de TripAdvisor.",
        "Categorías y Descriptores de Asignaturas J.4 [Ciencias Sociales y del Comportamiento]: Economía Términos Generales Economía, Experimentación, Confiabilidad 1.",
        "MOTIVACIONES La expansión de internet ha hecho posible que los foros de retroalimentación en línea (o mecanismos de reputación) se conviertan en un canal importante para el boca a boca sobre productos, servicios u otros tipos de interacciones comerciales.",
        "Numerosos estudios empíricos [10, 15, 13, 5] muestran que los compradores consideran seriamente los comentarios en línea al tomar decisiones de compra, y están dispuestos a pagar primas por reputación por productos o servicios que tienen una buena reputación.",
        "Sin embargo, un análisis reciente plantea preguntas importantes sobre la capacidad de los foros existentes para reflejar la verdadera calidad de un producto.",
        "En ausencia de incentivos claros, los usuarios con una perspectiva moderada no se molestarán en expresar sus opiniones, lo que conduce a una muestra no representativa de reseñas.",
        "Por ejemplo, [12, 1] muestran que las calificaciones de libros o CDs en Amazon1 siguen con gran probabilidad distribuciones bimodales en forma de U, donde la mayoría de las calificaciones son o muy buenas o muy malas.",
        "Los experimentos controlados, por otro lado, revelan opiniones sobre los mismos elementos que están distribuidas de forma normal.",
        "Bajo estas circunstancias, utilizar la media aritmética para predecir la calidad (como la mayoría de los foros realmente hacen) proporciona al usuario típico un estimador con una alta varianza que a menudo es incorrecto.",
        "Mejorar la forma en que agregamos la información disponible de las reseñas en línea requiere un profundo entendimiento de los factores subyacentes que sesgan el comportamiento de calificación de los usuarios.",
        "Hu et al. [12] proponen el Modelo Brag-and-Moan donde los usuarios califican solo si su utilidad del producto (extraída de una distribución normal) cae fuera de un intervalo mediano.",
        "Los autores concluyen que el modelo explica la distribución empírica de los informes y ofrece ideas sobre formas más inteligentes de estimar la verdadera calidad del producto.",
        "En el presente artículo ampliamos esta línea de investigación e intentamos explicar más hechos sobre el comportamiento de los usuarios al informar retroalimentación en línea.",
        "Usando reseñas reales de hoteles del sitio web TripAdvisor2, consideramos dos fuentes adicionales de información además de las calificaciones numéricas básicas enviadas por los usuarios.",
        "La primera es una evidencia lingüística simple proveniente de la revisión textual que suele acompañar a las calificaciones numéricas.",
        "Utilizamos técnicas de minería de texto similares a [7] y [3], sin embargo, solo estamos interesados en identificar qué aspectos del servicio está discutiendo el usuario, sin calcular la orientación semántica del texto.",
        "Observamos que los usuarios que comentan más sobre la misma característica tienen más probabilidades de estar de acuerdo en una calificación numérica común para esa característica en particular.",
        "Intuitivamente, los comentarios extensos revelan la importancia de la característica para el usuario.",
        "Dado que las personas tienden a ser más conocedoras en los aspectos que consideran importantes, se podría asumir que los usuarios que discuten una característica específica en más detalle tienen más autoridad para evaluar esa característica.",
        "Segundo investigamos la relación entre una reseña 1 http://www.amazon.com 2 http://www.tripadvisor.com/ 134 Figura 1: La página de TripAdvisor que muestra reseñas de un popular hotel en Boston.",
        "El nombre del hotel y los anuncios fueron deliberadamente borrados, al igual que las reseñas que lo precedieron.",
        "Un examen de las reseñas en línea muestra que las calificaciones a menudo forman parte de los hilos de discusión, donde una publicación no es necesariamente independiente de otras publicaciones.",
        "Uno puede ver, por ejemplo, usuarios que se esfuerzan por contradecir o estar vehementemente de acuerdo con los comentarios de usuarios anteriores.",
        "Al analizar la secuencia temporal de informes, concluimos que las revisiones pasadas influyen en los informes futuros, ya que crean cierta expectativa previa sobre la calidad del servicio.",
        "La percepción subjetiva del usuario está influenciada por la brecha entre la expectativa previa y el rendimiento real del servicio [17, 18, 16, 21], lo cual se reflejará más tarde en la calificación de los usuarios.",
        "Proponemos un modelo que captura la dependencia de las calificaciones en las expectativas previas, y lo validamos utilizando los datos empíricos que recopilamos.",
        "Ambos resultados pueden ser utilizados para mejorar la forma en que los mecanismos de reputación agregan la información de las revisiones individuales.",
        "Nuestro primer resultado se puede utilizar para determinar una estimación de calidad característica por característica, donde para cada característica se considera un subconjunto diferente de reseñas (es decir, aquellas con comentarios extensos sobre esa característica).",
        "El segundo conduce a un algoritmo que produce una estimación más precisa de la calidad real. 2.",
        "El conjunto de datos que utilizamos en este artículo son reseñas reales de hoteles recopiladas del popular sitio de viajes TripAdvisor.",
        "TripAdvisor indexa hoteles de ciudades de todo el mundo, junto con reseñas escritas por viajeros.",
        "Los usuarios pueden buscar en el sitio proporcionando el nombre del hotel y la ubicación (opcional).",
        "Las reseñas de un hotel dado se muestran como una lista (ordenadas de la más reciente a la más antigua), con 5 reseñas por página.",
        "Las reseñas contienen: • información sobre el autor de la reseña (por ejemplo, fechas de estancia, nombre de usuario del revisor, ubicación del revisor); • la calificación general (de 1, la más baja, a 5, la más alta); • una reseña textual que incluye un título para la reseña, comentarios libres y las principales cosas que al revisor le gustaron y no le gustaron; • calificaciones numéricas (de 1, la más baja, a 5, la más alta) para diferentes características (por ejemplo, limpieza, servicio, ubicación, etc.).",
        "Debajo del nombre del hotel, TripAdvisor muestra la dirección del hotel, información general (número de habitaciones, número de estrellas, breve descripción, etc.), la calificación promedio general, el ranking de TripAdvisor y una calificación promedio para cada característica.",
        "La Figura 1 muestra la página de un hotel popular en Boston cuyo nombre (junto con los anuncios) fue explícitamente borrado.",
        "Seleccionamos tres ciudades para este estudio: Boston, Sídney y Las Vegas.",
        "Para cada ciudad consideramos todos los hoteles que tenían al menos 10 reseñas, y registramos todas las reseñas.",
        "La Tabla 1 presenta el número de hoteles considerados en cada ciudad, el número total de reseñas registradas para cada ciudad y la distribución de hoteles con respecto a la calificación por estrellas (según está disponible en el sitio de TripAdvisor).",
        "Ten en cuenta que no todos los hoteles tienen una clasificación por estrellas.",
        "Tabla 1: Un resumen del conjunto de datos.",
        "Para cada reseña, registramos la calificación general, la reseña textual (título y cuerpo de la reseña) y la calificación numérica en 7 características: Habitaciones (R), Servicio (S), Limpieza (C), Valor (V), Comida (F), Ubicación (L) y Ruido (N).",
        "TripAdvisor no requiere que los usuarios envíen nada más que la calificación general, por lo tanto, una reseña típica evalúa pocas características adicionales, independientemente de la discusión en el comentario textual.",
        "Solo las características Habitaciones (R), Servicio (S), Limpieza (C) y Valor (V) son evaluadas por un número significativo de usuarios.",
        "Sin embargo, también seleccionamos las características Comida (F), Ubicación (L) y Ruido (N) porque son mencionadas en un número significativo de comentarios textuales.",
        "Para cada característica registramos la calificación numérica dada por el usuario, o 0 cuando la calificación está ausente.",
        "La longitud típica del comentario textual equivale aproximadamente a 200 palabras.",
        "Todos los datos fueron recopilados rastreando el sitio de TripAdvisor en septiembre de 2006. 2.1 Notación formal Nos referiremos formalmente a una reseña mediante una tupla (r, T) donde: • r = (rf ) es un vector que contiene las calificaciones rf ∈ {0, 1, . . . 5} para las características f ∈ F = {O, R, S, C, V, F, L, N}; cabe destacar que la calificación general, rO, se registra de forma abusiva como la calificación para la característica General(O); • T es el comentario textual que acompaña a la reseña. Se indexan 135 reseñas de acuerdo con la variable i, de modo que (ri , Ti ) es la i-ésima reseña en nuestra base de datos.",
        "Dado que no registramos el nombre de usuario del revisor, también diremos que la i-ésima reseña en nuestro conjunto de datos fue enviada por el usuario i.",
        "Cuando necesitemos considerar solo las reseñas de un hotel dado, h, usaremos (ri(h), Ti(h)) para denotar la i-ésima reseña sobre el hotel h. 3.",
        "EVIDENCIA DE COMENTARIOS TEXTUALES Los comentarios textuales gratuitos asociados a las reseñas en línea son una fuente valiosa de información para comprender las razones detrás de las calificaciones numéricas dejadas por los revisores.",
        "El texto puede, por ejemplo, revelar ejemplos concretos de aspectos que al usuario le gustaron o no le gustaron, justificando así algunas de las calificaciones altas o bajas respectivamente para ciertas características.",
        "El texto también puede ofrecer pautas para comprender las preferencias del revisor y los pesos de las diferentes características al calcular una calificación general.",
        "El problema, sin embargo, es que los comentarios textuales libres son difíciles de leer.",
        "Los usuarios deben desplazarse a través de muchas reseñas y leer principalmente información repetitiva.",
        "Se obtendrían mejoras significativas si las reseñas fueran interpretadas y agregadas automáticamente.",
        "Desafortunadamente, esta parece ser una tarea difícil para las computadoras ya que los usuarios humanos a menudo utilizan un lenguaje ingenioso, abreviaturas, frases específicas de la cultura y un estilo figurativo.",
        "Sin embargo, varios resultados importantes utilizan los comentarios textuales de las reseñas en línea de forma automatizada.",
        "Utilizando técnicas de lenguaje natural bien establecidas, las reseñas o partes de las reseñas pueden ser clasificadas como tener una orientación semántica positiva o negativa.",
        "Pang et al. [2] clasifican las críticas de películas en positivas/negativas entrenando tres clasificadores diferentes (Naive Bayes, Máxima Entropía y SVM) utilizando características de clasificación basadas en unigramas, bigramas o etiquetas de partes del discurso.",
        "Dave et al. [4] analizan reseñas de CNet y Amazon, y sorprendentemente demuestran que las características de clasificación basadas en unigramas o bigramas funcionan mejor que los n-gramas de orden superior.",
        "Este resultado es desafiado por Cui et al. [3], quienes examinan grandes colecciones de reseñas recopiladas de la web.",
        "Muestran que el tamaño del conjunto de datos es importante, y que conjuntos de entrenamiento más grandes permiten a los clasificadores utilizar con éxito características de clasificación más complejas basadas en n-gramas.",
        "Hu y Liu [11] también rastrean la web en busca de reseñas de productos e identifican automáticamente los atributos de productos que han sido discutidos por los revisores.",
        "Utilizan Wordnet para calcular la orientación semántica de las evaluaciones de productos y resumir las reseñas de usuarios extrayendo evaluaciones positivas y negativas de diferentes características del producto.",
        "Popescu y Etzioni [20] analizan un escenario similar, pero utilizan el recuento de resultados de búsqueda en motores de búsqueda para identificar atributos de productos; la orientación semántica se asigna a través de la técnica de etiquetado de relajación.",
        "Ghose et al. [7, 8] analizan las reseñas de vendedores del mercado secundario de Amazon para identificar las diferentes dimensiones (por ejemplo, entrega, empaque, atención al cliente, etc.) de la reputación.",
        "Analizan el texto y etiquetan la parte de la oración de cada palabra.",
        "Los sustantivos, frases nominales y frases verbales frecuentes se identifican como dimensiones de la reputación, mientras que los modificadores correspondientes (es decir, adjetivos y adverbios) se utilizan para derivar puntuaciones numéricas para cada dimensión.",
        "La medida de reputación mejorada se correlaciona mejor con la información de precios observada en el mercado.",
        "Pavlou y Dimoka [19] analizan las reseñas de eBay y encuentran que los comentarios textuales tienen un impacto importante en las primas de reputación.",
        "Nuestro enfoque es similar a los trabajos mencionados anteriormente, en el sentido de que identificamos los aspectos (es decir, las características del hotel) discutidos por los usuarios en las reseñas textuales.",
        "Sin embargo, no calculamos la orientación semántica del texto, ni intentamos inferir las calificaciones faltantes.",
        "Definimos el peso, wi f, de la característica f ∈ F en el texto Ti asociado con la reseña (ri, Ti), como la fracción de Ti dedicada a discutir aspectos (tanto positivos como negativos) relacionados con la característica f. Proponemos un método elemental para aproximar los valores de estos pesos.",
        "Para cada característica, construimos manualmente la lista de palabras Lf que contiene aproximadamente 50 palabras que están más comúnmente asociadas a la característica f. Las palabras iniciales fueron seleccionadas al leer algunas de las reseñas y ver qué palabras coinciden con la discusión de qué características.",
        "La lista fue luego ampliada agregando todas las entradas del tesauro que estaban relacionadas con las palabras iniciales.",
        "Finalmente, hicimos una lluvia de ideas para encontrar las palabras faltantes que normalmente estarían asociadas con cada una de las características.",
        "Que Lf ∩ Ti sea la lista de términos comunes a Lf y Ti.",
        "Cada término de Lf se cuenta el número de veces que aparece en Ti, con dos excepciones: • en los casos en que el usuario envía un título para la revisión, consideramos el texto del título agregándolo tres veces al texto de la revisión Ti.",
        "La suposición intuitiva es que la opinión del usuario se refleja con mayor fuerza en el título que en el cuerpo de la reseña.",
        "Por ejemplo, muchas reseñas son resumidas con precisión por títulos como \"Excelente servicio, ubicación terrible\" o \"Mala relación calidad-precio\"; ciertas palabras que ocurren solo una vez en el texto son contadas múltiples veces si su relevancia para esa característica es particularmente fuerte.",
        "Estas eran las palabras raíz para cada característica (por ejemplo, personal es una palabra raíz para la característica Servicio), y tenían un peso de 2 o 3.",
        "Cada característica fue asignada hasta 3 palabras raíz, por lo que casi todas las palabras se cuentan solo una vez.",
        "La lista de palabras para la característica Habitaciones se proporciona como referencia en el Apéndice A.",
        "El peso wi f se calcula como: wi f = |Lf ∩ Ti| f∈F |Lf ∩ Ti| (1) donde |Lf ∩ Ti| es el número de términos comunes entre Lf y Ti.",
        "El peso para la característica \"En general\" se estableció en min{ |T i | 5000 , 1} donde |Ti | es el número de caracteres en Ti.",
        "Comenzaré diciendo que soy más de la onda de Holiday Inn que de un hotel de lujo.",
        "Así que me frustro cuando pago el doble de la tarifa de la habitación y recibo la mitad de las comodidades que obtendría en un Hampton Inn o Holiday Inn.",
        "La ubicación era definitivamente el principal activo de este lugar.",
        "Estaba a solo unas cuadras de la parada de metro del Centro Hynes y era fácil caminar a algunos buenos restaurantes en la zona de Back Bay.",
        "Boylston no está lejos en absoluto.",
        "Así que no tuve problemas en renunciar a un coche de alquiler y tomar el metro desde el aeropuerto hasta el hotel y usar el metro para cualquier otro viaje.",
        "De lo contrario, te hacen pagar por todo. Y cuando ya has gastado $215 por noche en la habitación, eso resulta frustrante. La habitación en sí estaba bien, más o menos lo que esperaría.",
        "El personal también era promedio, ni malo ni excelente.",
        "Una vez más, creo que estás pagando por la ubicación y la capacidad de ir caminando a muchos lugares interesantes.",
        "Pero creo que la próxima vez me quedaré en Brookline, disfrutaré de más comodidades y usaré más el metro.",
        "Las calificaciones numéricas asociadas a esta reseña son rO = 3, rR = 3, rS = 3, rC = 4, rV = 2 para las características de General (O), Habitaciones (R), Servicio (S), Limpieza (C) y Valor (V) respectivamente.",
        "Las calificaciones de las características Comida (F), Ubicación (L) y Ruido (N) están ausentes (es decir, rF = rL = rN = 0).",
        "Los pesos wf se calculan a partir de las siguientes listas de términos comunes: LR ∩ T = {habitación}; wR = 0.066 LS ∩ T = {3 * Personal, comodidades}; wS = 0.267 LC ∩ T = ∅; wC = 0 LV ∩ T = {$, tarifa}; wV = 0.133 LF ∩ T = {restaurante}; wF = 0.067 LL ∩ T = {2 * centro, 2 * caminar, 2 * ubicación, área}; wL = 0.467 LN ∩ T = ∅; wN = 0 Las palabras raíz Personal y Centro se triplicaron y duplicaron respectivamente.",
        "El peso total de la revisión textual es wO = 0.197.",
        "Estos valores explican razonablemente bien los pesos de las diferentes características en la discusión del revisor.",
        "Un punto a tener en cuenta es que algunos términos en las listas Lf poseen una orientación semántica inherente.",
        "Por ejemplo, la palabra suciedad (perteneciente a la lista LC) se usaría con mayor frecuencia para afirmar la presencia, y no la ausencia de suciedad.",
        "Esto es inevitable, pero se tuvo cuidado de asegurar que se utilizaran palabras de ambos extremos del espectro.",
        "Por esta razón, algunas listas como LR contienen solo sustantivos de objetos que uno típicamente describiría en una habitación (ver Apéndice A).",
        "El objetivo de esta sección es analizar la influencia de los pesos wi f en las calificaciones numéricas ri f.",
        "Intuitivamente, los usuarios que pasaron mucho tiempo discutiendo una característica f (es decir, cuando wif es alto) tenían algo que decir sobre su experiencia con respecto a esta característica.",
        "Obviamente, la característica f es importante para el usuario i.",
        "Dado que las personas tienden a ser más conocedoras en los aspectos que consideran importantes, nuestra hipótesis es que las calificaciones ri f (correspondientes a los pesos altos wi f) constituyen un subconjunto de las calificaciones de expertos para la característica f. La Figura 2 traza la distribución de las tasas r i(h) C con respecto a los pesos w i(h) C para la limpieza de un hotel de Las Vegas, h. Aquí, las calificaciones altas están restringidas a las reseñas que hablan poco sobre la limpieza.",
        "Cuando se menciona la limpieza en la discusión, las calificaciones son bajas.",
        "Muchos hoteles muestran patrones de calificación similares para varias características.",
        "Las calificaciones correspondientes a pesos bajos abarcan todo el espectro del 1 al 5, mientras que las calificaciones correspondientes a pesos altos están más agrupadas (ya sea alrededor de calificaciones buenas o malas).",
        "Por lo tanto, formulamos la siguiente hipótesis: Hipótesis 1.",
        "Las calificaciones ri f correspondientes a las reseñas donde wi f es alta, son más similares entre sí que a la colección general de calificaciones.",
        "Para probar la hipótesis, tomamos todo el conjunto de reseñas y, característica por característica, calculamos la desviación estándar de las calificaciones con alto peso, y la desviación estándar de todo el conjunto de calificaciones.",
        "Los pesos altos se definieron como aquellos que pertenecen al 20% superior del rango de pesos para la característica correspondiente.",
        "Si la Hipótesis 1 fuera cierta, la desviación estándar de todas las calificaciones debería ser mayor que la desviación estándar de las calificaciones con pesos altos. 0 1 2 3 4 5 6 0 0.1 0.2 0.3 0.4 0.5 0.6 Calificación Peso Figura 2: La distribución de las calificaciones en función del peso de la característica de limpieza.",
        "Utilizamos una prueba T estándar para medir la significancia de los resultados.",
        "Ciudad por ciudad y característica por característica, la Tabla 2 presenta la desviación estándar promedio de todas las calificaciones, y la desviación estándar promedio de las calificaciones con pesos altos.",
        "De hecho, las calificaciones con pesos altos tienen una desviación estándar más baja, y los resultados son significativos en el umbral estándar de significancia de 0.05 (aunque para ciertas ciudades tomadas de forma independiente no parece haber una diferencia significativa, los resultados son significativos para todo el conjunto de datos).",
        "Por favor, tenga en cuenta que solo se consideraron las características O, R, S, C y V, ya que para las otras (F, L y N) no teníamos suficientes calificaciones.",
        "Tabla 2: Desviación estándar promedio para todas las calificaciones, y desviación estándar promedio para las calificaciones con pesos altos.",
        "En corchetes, los valores p correspondientes para una diferencia positiva entre los dos.",
        "La hipótesis 1 no solo proporciona una comprensión básica sobre el comportamiento de calificación de los usuarios en línea, sino que también sugiere algunas formas de calcular estimaciones de mejor calidad.",
        "Podemos, por ejemplo, construir una estimación de calidad característica por característica con una varianza mucho menor: para cada característica tomamos el subconjunto de reseñas que discuten ampliamente esa característica, y como estimación de calidad, obtenemos la calificación promedio de este subconjunto.",
        "Los experimentos iniciales sugieren que las calificaciones promedio de característica a característica calculadas de esta manera son diferentes de las calificaciones promedio calculadas en todo el conjunto de datos.",
        "Dado que, de hecho, los pesos altos son indicadores de opiniones expertas, las estimaciones obtenidas de esta manera son más precisas que las actuales.",
        "Sin embargo, la validación de esta suposición subyacente requiere más experimentos controlados.",
        "LA INFLUENCIA DE LAS CALIFICACIONES PASADAS Por lo general, se hacen dos suposiciones importantes sobre las reseñas enviadas a foros en línea.",
        "La primera es que las calificaciones reflejen fielmente la calidad observada por los usuarios; la segunda es que las reseñas sean independientes entre sí.",
        "Si bien la evidencia anecdótica [9, 22] cuestiona la primera suposición, en esta sección abordamos la segunda.",
        "Un examen de las reseñas en línea muestra que estas suelen formar parte de hilos de discusión, donde los usuarios se esfuerzan por contradecir o estar vehementemente de acuerdo con los comentarios de usuarios anteriores.",
        "Considera, por ejemplo, la siguiente reseña: No entiendo las críticas negativas... el hotel estaba un poco oscuro, pero ese era el estilo.",
        "Fue muy artístico.",
        "Sí, estaba cerca de la autopista, pero en mi opinión, el sonido de un coche ruidoso ocasional es mejor que escuchar el ding ding de las máquinas tragamonedas toda la noche.",
        "El personal disponible es FABULOSO.",
        "Las camareras son geniales (y *** no merece la mala crítica que recibió, ¡fue 100% atenta con nosotros!), los camareros son amigables y profesionales al mismo tiempo...",
        "Aquí, el usuario se vio perturbado por informes negativos anteriores, abordó estas preocupaciones y se dispuso a intentar corregirlas.",
        "Como era de esperar, sus calificaciones fueron considerablemente más altas que las calificaciones promedio hasta este punto.",
        "Parece que los usuarios de TripAdvisor suelen leer regularmente los informes enviados por usuarios anteriores antes de reservar un hotel, o antes de escribir una reseña.",
        "Las reseñas anteriores crean ciertas expectativas previas sobre la calidad del servicio, y estas expectativas influyen en la reseña presentada.",
        "Creemos que esta observación es válida para la mayoría de los foros en línea.",
        "La percepción subjetiva de la calidad es directamente proporcional a qué tan bien la experiencia real cumple con la expectativa previa, un hecho confirmado por una importante línea de investigación econométrica y de marketing [17, 18, 16, 21].",
        "La correlación entre las reseñas también ha sido confirmada por investigaciones recientes sobre la dinámica de los foros de reseñas en línea [6]. 4.1 Expectativas Previas Definimos la expectativa previa del usuario i con respecto a la característica f, como el promedio de las calificaciones previamente disponibles en la característica f4: ef (i) = j<i,r j f =0 rj f j<i,r j f =0 1 Como primera hipótesis, afirmamos que la calificación ri f es una función de la expectativa previa ef (i): Hipótesis 2.",
        "Para un hotel y una característica dados, dados los comentarios i y j tales que ef (i) es alto y ef (j) es bajo, la calificación rj f supera la calificación ri f.",
        "Definimos las expectativas altas y bajas como aquellas que están por encima, respectivamente por debajo de un cierto valor umbral θ.",
        "El conjunto de reseñas precedidas por altas, respectivamente bajas expectativas 3 parte de las reseñas de Amazon fueron reconocidas como publicaciones estratégicas por autores de libros o competidores 4 si no se asignaron calificaciones previas para la característica f, ef (i) se asigna un valor predeterminado de 4.",
        "Tabla 3: Calificaciones promedio para reseñas precedidas por expectativas bajas (primer valor en la celda) y altas (segundo valor en la celda).",
        "Los valores P para una diferencia positiva se dan entre corchetes.",
        "Las ciudades O R S C V 3.953 4.045 3.985 4.252 3.946 Boston 3.364 3.590 3.485 3.641 3.242 [0.011] [0.028] [0.0086] [0.0168] [0.0034] 4.284 4.358 4.064 4.530 4.428 Sídney 3.756 3.537 3.436 3.918 3.495 [0.000] [0.000] [0.035] [0.009] [0.000] 3.494 3.674 3.713 3.689 3.580 Las Vegas 3.140 3.530 2.952 3.530 3.351 [0.190] [0.529] [0.007] [0.529] [0.253] se definen de la siguiente manera: Ralto f = {ri f |ef (i) > θ} Rbajo f = {ri f |ef (i) < θ} Estos conjuntos son específicos para cada par (hotel, característica), y en nuestros experimentos tomamos θ = 4.",
        "Este valor bastante alto está cerca del promedio de calificación de todas las características de todos los hoteles, y se justifica por el hecho de que nuestro conjunto de datos contiene principalmente hoteles de alta calidad.",
        "Para cada ciudad, tomamos todos los hoteles y calculamos las calificaciones promedio en los conjuntos Rhigh f y Rlow f (ver Tabla 3).",
        "El promedio de calificación entre las reseñas que siguen a expectativas previas bajas es significativamente mayor que el promedio de calificación después de altas expectativas.",
        "Como evidencia adicional, consideramos todos los hoteles para los cuales la función eV (i) (la expectativa para la característica Valor) tiene un valor alto (mayor que 4) para algunos i, y un valor bajo (menor que 4) para algunos otros i.",
        "Intuitivamente, estos son los hoteles para los cuales hay un grado mínimo de variación en la secuencia oportuna de reseñas: es decir, el promedio acumulativo de calificaciones fue en algún momento alto y luego se volvió bajo, o viceversa.",
        "Tales variaciones se observan en aproximadamente la mitad de todos los hoteles en cada ciudad.",
        "La Figura 3 traza la mediana (entre los hoteles considerados) de la calificación, rV, cuando ef (i) no es más que x pero mayor que x - 0.5. 2.5 3 3.5 4 4.5 5 2.5 3 3.5 4 4.5 5 Expectativa de la mediana de calificación Boston Sydney Vegas Figura 3: Las calificaciones tienden a disminuir a medida que aumenta la expectativa. 138 Hay dos formas de interpretar la función ef (i): • El valor esperado para la característica f obtenido por el usuario i antes de su experiencia con el servicio, adquirido al leer informes presentados por usuarios anteriores.",
        "En este caso, un valor excesivamente alto para ef (i) llevaría al usuario a enviar un informe negativo (o viceversa), derivado de la diferencia entre el valor real del servicio y la expectativa inflada de este valor adquirida antes de su experiencia. • El valor esperado de la característica f para todos los visitantes posteriores del sitio, si el usuario i no enviara un informe.",
        "En este caso, la motivación para un informe negativo después de un valor excesivamente alto de ef es diferente: el usuario i busca corregir la expectativa de los futuros visitantes del sitio.",
        "A diferencia de la interpretación anterior, esto no requiere que el usuario derive una expectativa a priori para el valor de f. Tenga en cuenta que ninguna de las interpretaciones implica que el promedio hasta el informe i esté inversamente relacionado con la calificación en el informe i.",
        "Podría existir una medida de influencia ejercida por informes anteriores que impulse al usuario detrás del informe i a enviar calificaciones que, en cierta medida, se ajusten a los informes anteriores: un valor bajo para ef (i) puede influir en el usuario i a enviar una calificación baja para la característica f porque, por ejemplo, teme que enviar una calificación alta lo haga parecer una persona con bajos estándares.",
        "Esto, al principio, parece contradecir la Hipótesis 2.",
        "Sin embargo, esta calificación de conformidad no puede continuar indefinidamente: una vez que el conjunto de informes proyecte una estimación suficientemente reducida para vf, los revisores futuros con impresiones comparativamente positivas buscarán corregir esta percepción errónea. 4.2 Impacto de los comentarios textuales en la expectativa de calidad. Se puede obtener una mayor comprensión del comportamiento de calificación de los usuarios de TripAdvisor analizando la relación entre los pesos wf y los valores ef (i).",
        "En particular, examinamos la siguiente hipótesis: Hipótesis 3.",
        "Cuando una gran proporción del texto de una reseña discute una característica en particular, la diferencia entre la calificación para esa característica y la calificación promedio hasta ese momento tiende a ser grande.",
        "La intuición detrás de esta afirmación es que cuando el usuario insiste en expresar su opinión sobre una característica en particular, su opinión difiere de la opinión colectiva de publicaciones anteriores.",
        "Esto se basa en la característica de los sistemas de reputación como foros de retroalimentación donde un usuario está interesado en proyectar su opinión, con particular fuerza si esta opinión difiere de lo que percibe como la opinión general.",
        "Para probar la Hipótesis 3 medimos la diferencia absoluta promedio entre la expectativa ef (i) y la calificación ri f cuando el peso wi f es alto, respectivamente bajo.",
        "Los pesos se clasifican como altos o bajos al compararlos con ciertos valores de corte: wi f es bajo si es menor que 0.1, mientras que wi f es alto si es mayor que θf.",
        "Se utilizaron diferentes valores de corte para diferentes características: θR = 0.4, θS = 0.4, θC = 0.2 y θV = 0.7.",
        "La limpieza tiene un límite inferior más bajo ya que es una característica raramente discutida; el valor tiene un límite superior alto por la razón opuesta.",
        "Los resultados se presentan en la Tabla 4. La idea de que los informes negativos pueden fomentar una mayor divulgación negativa ha sido sugerida anteriormente [14]. Tabla 4: Promedio de |ri f −ef (i)| cuando los pesos son altos (primer valor en la celda) y bajos (segundo valor en la celda) con valores P para la diferencia en corchetes cuadrados.",
        "Esto demuestra que cuando los pesos son inusualmente altos, los usuarios tienden a expresar una opinión que no se ajusta al promedio neto de calificaciones anteriores.",
        "Como podríamos esperar, para una característica que rara vez tuvo un peso importante en la discusión (por ejemplo, la limpieza), la diferencia es particularmente grande.",
        "Aunque la diferencia en el valor de la característica es bastante grande para Sídney, el valor P es alto.",
        "Esto se debe a que solo unos pocos comentarios discutieron en gran medida el valor.",
        "La razón podría ser cultural o porque había menos motivo para discutir esta característica. 4.3 Incentivos para informar Los modelos anteriores sugieren que los usuarios poco opinativos no elegirán expresar sus opiniones [12].",
        "En esta sección, ampliamos este modelo para tener en cuenta la influencia de las expectativas.",
        "La motivación para enviar comentarios no se debe solo a opiniones extremas, sino también a la diferencia entre la reputación actual (es decir, la expectativa previa del usuario) y la experiencia real.",
        "Un modelo de calificación así produce calificaciones que la mayoría de las veces se desvían de la calificación promedio actual.",
        "Las calificaciones que confirman la expectativa previa rara vez serán enviadas.",
        "Probamos en nuestro conjunto de datos la proporción de calificaciones que intentan corregir la estimación actual.",
        "Definimos una calificación desviada como aquella que se desvía de la expectativa actual por al menos un umbral θ, es decir, |ri f − ef (i)| ≥ θ.",
        "Para cada una de las tres ciudades consideradas, las siguientes tablas muestran la proporción de calificaciones desviadas para θ = 0.5 y θ = 1.",
        "Los resultados anteriores sugieren que una gran proporción de usuarios (casi la mitad, incluso para el valor umbral alto θ = 1) se desvían del promedio previo.",
        "Esto refuerza la idea de que los usuarios son más propensos a enviar un informe cuando creen que tienen algo distintivo que agregar al flujo actual de opiniones sobre alguna característica.",
        "Tales conclusiones están en total acuerdo con evidencia previa de que la distribución de informes a menudo sigue distribuciones bimodales en forma de U. 139 5.",
        "Para dar cuenta de las observaciones descritas en las secciones anteriores, proponemos un modelo para el comportamiento de los usuarios al enviar reseñas en línea.",
        "Para un hotel dado, asumimos que la calidad experimentada por los usuarios sigue una distribución normal alrededor de algún valor vf, que representa la calidad objetiva ofrecida por el hotel en la característica f. La calificación enviada por el usuario i en la característica f es: ˆri f = δf vi f + (1 − δf) · sign vi f − ef (i) c + d(vi f, ef (i)|wi f) (2) donde: • vi f es la calidad (desconocida) experimentada realmente por el usuario. Se asume que vi f sigue una distribución normal alrededor de algún valor vf; • δf ∈ [0, 1] puede verse como una medida del sesgo al reportar retroalimentación.",
        "Los valores altos reflejan el hecho de que los usuarios califican de manera objetiva, sin ser influenciados por expectativas previas.",
        "El valor de δf puede depender de varios factores; fijamos un valor para cada característica f; • c es una constante entre 1 y 5; • wi f es el peso de la característica f en el comentario textual de la reseña i, calculado según la Ecuación (1); • d(vi f , ef (i)|wi f ) es una función de distancia entre la expectativa y la observación del usuario i.",
        "La función de distancia satisface las siguientes propiedades: - d(y, z|w) ≥ 0 para todo y, z ∈ [0, 5], w ∈ [0, 1]; - |d(y, z|w)| < |d(z, x|w)| si |y − z| < |z − x|; - |d(y, z|w1)| < |d(y, z|w2)| si w1 < w2; - c + d(vf , ef (i)|wi f ) ∈ [1, 5]; El segundo término de la Ecuación (2) codifica el sesgo de la calificación.",
        "Cuanto mayor sea la distancia entre la observación real vi f y la función ef, mayor será el sesgo. 5.1 Validación del modelo Utilizamos el conjunto de datos de las reseñas de TripAdvisor para validar el modelo de comportamiento presentado anteriormente.",
        "Dividimos por conveniencia los valores de calificación en tres rangos: malo (B = {1, 2}), indiferente (I = {3, 4}), y bueno (G = {5}), y realizamos los siguientes dos tests: • Primero, usaremos nuestro modelo para predecir las calificaciones que tienen valores extremos.",
        "Para cada hotel, tomamos la secuencia de informes, y cada vez que nos encontramos con una calificación que sea buena o mala (pero no indiferente) intentamos predecirla utilizando la Ecuación (2) • En segundo lugar, en lugar de predecir el valor de las calificaciones extremas, intentamos clasificarlas como buenas o malas.",
        "Para cada hotel tomamos la secuencia de informes, y para cada informe (independientemente de su valor) lo clasificamos como bueno o malo. Sin embargo, para realizar estas pruebas, necesitamos estimar el valor objetivo, vf, que es el promedio de las observaciones de calidad reales, vi f.",
        "El algoritmo que estamos utilizando se basa en la intuición de que la cantidad de calificación de conformidad se minimiza.",
        "En otras palabras, el valor vf debe ser tal que, tan a menudo como sea posible, las calificaciones malas sigan a expectativas por encima de vf y las calificaciones buenas sigan a expectativas por debajo de vf.",
        "Formalmente, definimos los conjuntos: Γ1 = {i|ef (i) < vf y ri f ∈ B}; Γ2 = {i|ef (i) > vf y ri f ∈ G}; que corresponden a irregularidades donde, a pesar de que la expectativa en el punto i es menor que el valor entregado, la calificación es baja, y viceversa.",
        "Definimos vf como el valor que minimiza la unión de los dos conjuntos: vf = arg min vf |Γ1 ∪ Γ2| (3) En la Ec. (2) reemplazamos vi f por el valor vf calculado en la Ec. (3), y utilizamos la siguiente función de distancia: d(vf , ef (i)|wi f ) = |vf − ef (i)| vf − ef (i) |vf 2 − ef (i)2 | · (1 + 2wi f ); La constante c ∈ I se estableció en min{max{ef (i), 3}}, 4}.",
        "Los valores para δf se fijaron en {0.7, 0.7, 0.8, 0.7, 0.6} para las características {General, Habitaciones, Servicio, Limpieza, Valor} respectivamente.",
        "Los pesos se calculan tal como se describe en la Sección 3.",
        "Como primer experimento, tomamos los conjuntos de calificaciones extremas {ri f |ri f /∈ I} para cada hotel y característica.",
        "Para cada calificación de este tipo, ri f, intentamos estimarla calculando ˆri f usando la Ecuación (2).",
        "Comparamos este estimador con el obtenido simplemente promediando las calificaciones de todos los hoteles y características: es decir, ¯rf = j,r j f =0 rj f j,r j f =0 1; la Tabla 7 presenta la relación entre el error cuadrático medio (RMSE) al usar ˆri f y ¯rf para estimar las calificaciones reales.",
        "En todos los casos, la estimación producida por nuestro modelo es mejor que el promedio simple.",
        "Tabla 7: Promedio de RMSE(ˆrf) RMSE(¯rf) Ciudad O R S C V Boston 0.987 0.849 0.879 0.776 0.913 Sídney 0.927 0.817 0.826 0.720 0.681 Las Vegas 0.952 0.870 0.881 0.947 0.904 Como segundo experimento, intentamos distinguir los conjuntos Bf = {i|ri f ∈ B} y Gf = {i|ri f ∈ G} de calificaciones malas y buenas, respectivamente, en la característica f. Por ejemplo, calculamos el conjunto Bf usando el siguiente clasificador (llamado σ): ri f ∈ Bf (σf (i) = 1) ⇔ ˆri f ≤ 4; Las Tablas 8, 9 y 10 presentan la Precisión (p), Recall (r) y s = 2pr p+r para el clasificador σ, y lo comparan con un clasificador de mayoría ingenuo, τ, τf (i) = 1 ⇔ |Bf | ≥ |Gf |: Vemos que el recall es siempre mayor para σ y la precisión suele ser ligeramente peor.",
        "Para la métrica s, σ tiende a agregar un 140. Tabla 8: Precisión (p), Recall (r), s= 2pr p+r mientras se detectan calificaciones bajas para Boston O R S C V p 0.678 0.670 0.573 0.545 0.610 σ r 0.626 0.659 0.619 0.612 0.694 s 0.651 0.665 0.595 0.577 0.609 p 0.684 0.706 0.647 0.611 0.633 τ r 0.597 0.541 0.410 0.383 0.562 s 0.638 0.613 0.502 0.471 0.595 Tabla 9: Precisión (p), Recall (r), s= 2pr p+r mientras se detectan calificaciones bajas para Las Vegas O R S C V p 0.654 0.748 0.592 0.712 0.583 σ r 0.608 0.536 0.791 0.474 0.610 s 0.630 0.624 0.677 0.569 0.596 p 0.685 0.761 0.621 0.748 0.606 τ r 0.542 0.505 0.767 0.445 0.441 s 0.605 0.607 0.670 0.558 0.511 Mejora del 1-20% sobre τ, mucho mayor en algunos casos para hoteles en Sídney.",
        "Esto se debe probablemente a que las reseñas de Sídney son más positivas que las de las ciudades estadounidenses y los casos en los que el número de reseñas negativas supera al de las positivas son raros.",
        "Reemplazar el algoritmo de prueba con uno que juega un 1 con una probabilidad igual a la proporción de malas críticas mejora sus resultados para esta ciudad, pero aún es superado por alrededor del 80%. 6.",
        "RESUMEN DE RESULTADOS Y CONCLUSIÓN El objetivo de este artículo es explorar los factores que llevan a un usuario a enviar una calificación particular, en lugar de los incentivos que lo animaron a enviar un informe en primer lugar.",
        "Para ello utilizamos dos fuentes adicionales de información además del vector de calificaciones numéricas: primero, examinamos los comentarios textuales que acompañan las reseñas, y segundo, consideramos los informes que han sido previamente enviados por otros usuarios.",
        "Usando algoritmos simples de procesamiento de lenguaje natural, pudimos establecer una correlación entre el peso de una característica específica en el comentario textual que acompaña la reseña, y el ruido presente en la calificación numérica.",
        "Específicamente, parece que los usuarios que discuten ampliamente una característica en particular tienden a estar de acuerdo en una calificación común.",
        "Esta observación permite la construcción de estimadores de calidad característica por característica que tienen una varianza más baja y, con suerte, son menos ruidosos.",
        "Sin embargo, se requiere más evidencia para respaldar la intuición de que las calificaciones correspondientes a pesos altos son opiniones de expertos que merecen ser consideradas de mayor prioridad al calcular estimaciones de calidad.",
        "En segundo lugar, enfatizamos la dependencia de las calificaciones en informes previos.",
        "Los informes anteriores crean una expectativa de calidad que afecta la percepción subjetiva del usuario.",
        "Validamos dos hechos sobre las reseñas de hoteles que recopilamos de TripAdvisor: Primero, las calificaciones que siguen expectativas bajas (donde la expectativa se calcula como el promedio de los informes anteriores) probablemente sean más altas que las calificaciones que siguen expectativas altas.",
        "Intuitivamente, la percepción de la calidad (y consecuentemente la calificación) depende de qué tan bien la experiencia real del usuario cumple con sus expectativas.",
        "Segundo, incluimos evidencia de los comentarios textuales, y encontramos que cuando los usuarios dedican una gran parte del texto a discutir una característica específica, es probable que motiven una calificación divergente (es decir, una calificación que no se ajusta a la expectativa previa).",
        "Intuitivamente, esto respalda la hipótesis de que los foros de reseñas actúan como grupos de discusión donde los usuarios están interesados en presentar y motivar su propia opinión.",
        "Hemos capturado la evidencia empírica en un modelo de comportamiento que predice las calificaciones enviadas por los usuarios.",
        "La calificación final depende, como era de esperar, de la observación real y de la brecha entre la observación y la expectativa.",
        "La brecha tiende a tener una influencia mayor cuando una fracción importante del comentario textual está dedicada a discutir una característica específica.",
        "El modelo propuesto fue validado con los datos empíricos y proporciona mejores estimaciones de las calificaciones realmente enviadas.",
        "Una suposición que hacemos es sobre la existencia de un valor de calidad objetivo vf para la característica f. Esto rara vez es cierto, especialmente a lo largo de largos periodos de tiempo.",
        "Otras explicaciones podrían dar cuenta de la correlación de las calificaciones con informes anteriores.",
        "Por ejemplo, si ef (i) refleja el valor real de f en un punto en el tiempo, la diferencia en las calificaciones siguiendo expectativas altas y bajas puede ser explicada por modelos de ingresos hoteleros que se maximizan cuando el valor es modificado en consecuencia.",
        "Sin embargo, la idea de que la variación en las calificaciones no es principalmente una función de la variación en el valor resulta ser útil.",
        "Nuestro enfoque para aproximar este valor objetivo es de ninguna manera perfecto, pero se ajusta perfectamente a la idea detrás del modelo.",
        "Una dirección natural para trabajos futuros es examinar aplicaciones concretas de nuestros resultados.",
        "Es probable que se obtengan mejoras significativas en las estimaciones de calidad al incorporar toda la evidencia empírica sobre el comportamiento de calificación.",
        "No está claro exactamente cómo diferentes factores afectan las decisiones de los usuarios.",
        "La respuesta podría depender de la aplicación particular, el contexto y la cultura. 7.",
        "REFERENCIAS [1] A. Admati y P. Pfleiderer.",
        "Noisytalk.com: Transmitiendo opiniones en un entorno ruidoso.",
        "Documento de trabajo 1670R, Universidad de Stanford, 2000. [2] P. B., L. Lee y S. Vaithyanathan.",
        "¿Clasificación de sentimientos con técnicas de aprendizaje automático?",
        "En Actas de EMNLP-02, la Conferencia sobre Métodos Empíricos en el Procesamiento del Lenguaje Natural, 2002. [3] H. Cui, V. Mittal y M. Datar.",
        "Experimentos comparativos 141 sobre clasificación de sentimientos para reseñas de productos en línea.",
        "En Actas de AAAI, 2006. [4] K. Dave, S. Lawrence y D. Pennock.",
        "Extracción de opiniones y clasificación semántica de reseñas de productos en la galería de cacahuetes.",
        "En Actas de la 12ª Conferencia Internacional sobre la World Wide Web (WWW03), 2003. [5] C. Dellarocas, N. Awad y X. Zhang.",
        "Explorando el valor de las calificaciones de productos en línea en la previsión de ingresos: El caso de las películas en movimiento.",
        "Documento de trabajo, 2006. [6] C. Forman, A. Ghose y B. Wiesenfeld.",
        "Un Examen Multinivel del Impacto de las Identidades Sociales en las Transacciones Económicas en los Mercados Electrónicos.",
        "Disponible en SSRN: http://ssrn.com/abstract=918978, julio de 2006. [7] A. Ghose, P. Ipeirotis y A. Sundararajan.",
        "Primas de reputación en mercados electrónicos peer-to-peer: análisis de retroalimentación textual y estructura de red.",
        "En el Tercer Taller sobre Economía de Sistemas Peer-to-Peer (P2PECON), 2005. [8] A. Ghose, P. Ipeirotis y A. Sundararajan.",
        "Las dimensiones de la reputación en los mercados electrónicos.",
        "Documento de trabajo CeDER-06-02, Universidad de Nueva York, 2006. [9] A. Harmon.",
        "Error de Amazon revela la guerra de los revisores.",
        "The New York Times, 14 de febrero de 2004. [10] D. Houser y J. Wooders.",
        "Reputación en subastas: teoría y evidencia de eBay.",
        "Revista de Estrategia Económica y de Gestión, 15:353-369, 2006. [11] M. Hu y B. Liu.",
        "Extracción y resumen de reseñas de clientes.",
        "En Actas de la Conferencia Internacional de la ACM SIGKDD sobre Descubrimiento de Conocimiento y Minería de Datos (KDD04), 2004. [12] N. Hu, P. Pavlou y J. Zhang.",
        "¿Pueden las reseñas en línea revelar la verdadera calidad de un producto?",
        "En Actas de la Conferencia ACM sobre Comercio Electrónico (EC 06), 2006. [13] K. Kalyanam y S. McIntyre.",
        "Retorno de la reputación en el mercado de subastas en línea.",
        "Documento de trabajo 02/03-10-WP, Escuela de Negocios Leavey, Universidad de Santa Clara, 2001. [14] L. Khopkar y P. Resnick.",
        "Auto-selección, deslizamiento, salvamento, holgazanería y lapidación: los impactos de la retroalimentación negativa en eBay.",
        "En Actas de la Conferencia ACM sobre Comercio Electrónico (EC 05), 2005. [15] M. Melnik y J. Alm.",
        "¿Importa la reputación de un vendedor? evidencia de subastas en eBay.",
        "Revista de Economía Industrial, 50(3):337-350, 2002. [16] R. Olshavsky y J. Miller.",
        "Expectativas del consumidor, rendimiento del producto y calidad percibida del producto.",
        "Revista de Investigación de Marketing, 9:19-21, febrero de 1972. [17] A. Parasuraman, V. Zeithaml y L. Berry.",
        "Un Modelo Conceptual de Calidad de Servicio y sus Implicaciones para Investigaciones Futuras.",
        "Revista de Marketing, 49:41-50, 1985. [18] A. Parasuraman, V. Zeithaml y L. Berry.",
        "SERVQUAL: Una escala de múltiples ítems para medir las percepciones de calidad del servicio por parte del consumidor.",
        "Revista de Retail, 64:12-40, 1988. [19] P. Pavlou y A. Dimoka.",
        "La naturaleza y función de los comentarios de texto de retroalimentación en los mercados en línea: implicaciones para la construcción de confianza, primas de precio y diferenciación de vendedores.",
        "Investigación de Sistemas de Información, 17(4):392-414, 2006. [20] A. Popescu y O. Etzioni.",
        "Extrayendo características del producto y opiniones de las reseñas.",
        "En Actas de la Conferencia de Tecnología del Lenguaje Humano y Conferencia sobre Métodos Empíricos en Procesamiento del Lenguaje Natural, 2005. [21] R. Teas.",
        "Expectativas, Evaluación del Rendimiento y Percepciones de Calidad de los Consumidores.",
        "Revista de Marketing, 57:18-34, 1993. [22] E. White.",
        "Chateando a un cantante en las listas de éxitos pop.",
        "El Wall Street Journal, 15 de octubre de 1999.",
        "APÉNDICE A.",
        "LISTA DE PALABRAS, LR, ASOCIADAS A LA CARACTERÍSTICA HABITACIONES. Todas las palabras sirven como prefijos: habitación, espacio, interior, decoración, ambiente, atmósfera, comodidad, baño, inodoro, cama, edificio, pared, ventana, privado, temperatura, sábana, lino, almohada, caliente, agua, fría, agua, ducha, vestíbulo, muebles, alfombra, aire, acondicionado, colchón, distribución, diseño, espejo, techo, iluminación, lámpara, sofá, silla, cómoda, armario, armario."
    ],
    "error_count": 5,
    "keys": {
        "online review": {
            "translated_key": "reseñas en línea",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Understanding User Behavior in Online Feedback Reporting Arjun Talwar Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland arjun@math.stanford.edu Radu Jurca Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland radu.jurca@epfl.ch Boi Faltings Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland boi.faltings@epfl.ch ABSTRACT Online reviews have become increasingly popular as a way to judge the quality of various products and services.",
                "Previous work has demonstrated that contradictory reporting and underlying user biases make judging the true worth of a service difficult.",
                "In this paper, we investigate underlying factors that influence user behavior when reporting feedback.",
                "We look at two sources of information besides numerical ratings: linguistic evidence from the textual comment accompanying a review, and patterns in the time sequence of reports.",
                "We first show that groups of users who amply discuss a certain feature are more likely to agree on a common rating for that feature.",
                "Second, we show that a users rating partly reflects the difference between true quality and prior expectation of quality as inferred from previous reviews.",
                "Both give us a less noisy way to produce rating estimates and reveal the reasons behind user bias.",
                "Our hypotheses were validated by statistical evidence from hotel reviews on the TripAdvisor website.",
                "Categories and Subject Descriptors J.4 [Social and Behavioral Sciences]: Economics General Terms Economics, Experimentation, Reliability 1.",
                "MOTIVATIONS The spread of the internet has made it possible for online feedback forums (or reputation mechanisms) to become an important channel for Word-of-mouth regarding products, services or other types of commercial interactions.",
                "Numerous empirical studies [10, 15, 13, 5] show that buyers seriously consider online feedback when making purchasing decisions, and are willing to pay reputation premiums for products or services that have a good reputation.",
                "Recent analysis, however, raises important questions regarding the ability of existing forums to reflect the real quality of a product.",
                "In the absence of clear incentives, users with a moderate outlook will not bother to voice their opinions, which leads to an unrepresentative sample of reviews.",
                "For example, [12, 1] show that Amazon1 ratings of books or CDs follow with great probability bi-modal, U-shaped distributions where most of the ratings are either very good, or very bad.",
                "Controlled experiments, on the other hand, reveal opinions on the same items that are normally distributed.",
                "Under these circumstances, using the arithmetic mean to predict quality (as most forums actually do) gives the typical user an estimator with high variance that is often false.",
                "Improving the way we aggregate the information available from online reviews requires a deep understanding of the underlying factors that bias the rating behavior of users.",
                "Hu et al. [12] propose the Brag-and-Moan Model where users rate only if their utility of the product (drawn from a normal distribution) falls outside a median interval.",
                "The authors conclude that the model explains the empirical distribution of reports, and offers insights into smarter ways of estimating the true quality of the product.",
                "In the present paper we extend this line of research, and attempt to explain further facts about the behavior of users when reporting online feedback.",
                "Using actual hotel reviews from the TripAdvisor2 website, we consider two additional sources of information besides the basic numerical ratings submitted by users.",
                "The first is simple linguistic evidence from the textual review that usually accompanies the numerical ratings.",
                "We use text-mining techniques similar to [7] and [3], however, we are only interested in identifying what aspects of the service the user is discussing, without computing the semantic orientation of the text.",
                "We find that users who comment more on the same feature are more likely to agree on a common numerical rating for that particular feature.",
                "Intuitively, lengthy comments reveal the importance of the feature to the user.",
                "Since people tend to be more knowledgeable in the aspects they consider important, users who discuss a given feature in more details might be assumed to have more authority in evaluating that feature.",
                "Second we investigate the relationship between a review 1 http://www.amazon.com 2 http://www.tripadvisor.com/ 134 Figure 1: The TripAdvisor page displaying reviews for a popular Boston hotel.",
                "Name of hotel and advertisements were deliberatively erased. and the reviews that preceded it.",
                "A perusal of online reviews shows that ratings are often part of discussion threads, where one post is not necessarily independent of other posts.",
                "One may see, for example, users who make an effort to contradict, or vehemently agree with, the remarks of previous users.",
                "By analyzing the time sequence of reports, we conclude that past reviews influence the future reports, as they create some prior expectation regarding the quality of service.",
                "The subjective perception of the user is influenced by the gap between the prior expectation and the actual performance of the service [17, 18, 16, 21] which will later reflect in the users rating.",
                "We propose a model that captures the dependence of ratings on prior expectations, and validate it using the empirical data we collected.",
                "Both results can be used to improve the way reputation mechanisms aggregate the information from individual reviews.",
                "Our first result can be used to determine a featureby-feature estimate of quality, where for each feature, a different subset of reviews (i.e., those with lengthy comments of that feature) is considered.",
                "The second leads to an algorithm that outputs a more precise estimate of the real quality. 2.",
                "THE DATA SET We use in this paper real hotel reviews collected from the popular travel site TripAdvisor.",
                "TripAdvisor indexes hotels from cities across the world, along with reviews written by travelers.",
                "Users can search the site by giving the hotels name and location (optional).",
                "The reviews for a given hotel are displayed as a list (ordered from the most recent to the oldest), with 5 reviews per page.",
                "The reviews contain: • information about the author of the review (e.g., dates of stay, username of the reviewer, location of the reviewer); • the overall rating (from 1, lowest, to 5, highest); • a textual review containing a title for the review, free comments, and the main things the reviewer liked and disliked; • numerical ratings (from 1, lowest, to 5, highest) for different features (e.g., cleanliness, service, location, etc.)",
                "Below the name of the hotel, TripAdvisor displays the address of the hotel, general information (number of rooms, number of stars, short description, etc), the average overall rating, the TripAdvisor ranking, and an average rating for each feature.",
                "Figure 1 shows the page for a popular Boston hotel whose name (along with advertisements) was explicitly erased.",
                "We selected three cities for this study: Boston, Sydney and Las Vegas.",
                "For each city we considered all hotels that had at least 10 reviews, and recorded all reviews.",
                "Table 1 presents the number of hotels considered in each city, the total number of reviews recorded for each city, and the distribution of hotels with respect to the star-rating (as available on the TripAdvisor site).",
                "Note that not all hotels have a star-rating.",
                "Table 1: A summary of the data set.",
                "City # Reviews # Hotels # of Hotels with 1,2,3,4 & 5 stars Boston 3993 58 1+3+17+15+2 Sydney 1371 47 0+0+9+13+10 Las Vegas 5593 40 0+3+10+9+6 For each review we recorded the overall rating, the textual review (title and body of the review) and the numerical rating on 7 features: Rooms(R), Service(S), Cleanliness(C), Value(V), Food(F), Location(L) and Noise(N).",
                "TripAdvisor does not require users to submit anything other than the overall rating, hence a typical review rates few additional features, regardless of the discussion in the textual comment.",
                "Only the features Rooms(R), Service(S), Cleanliness(C) and Value(V) are rated by a significant number of users.",
                "However, we also selected the features Food(F), Location(L) and Noise(N) because they are referred to in a significant number of textual comments.",
                "For each feature we record the numerical rating given by the user, or 0 when the rating is missing.",
                "The typical length of the textual comment amounts to approximately 200 words.",
                "All data was collected by crawling the TripAdvisor site in September 2006. 2.1 Formal notation We will formally refer to a review by a tuple (r, T) where: • r = (rf ) is a vector containing the ratings rf ∈ {0, 1, . . . 5} for the features f ∈ F = {O, R, S, C, V, F, L, N}; note that the overall rating, rO, is abusively recorded as the rating for the feature Overall(O); • T is the textual comment that accompanies the review. 135 Reviews are indexed according to the variable i, such that (ri , Ti ) is the ith review in our database.",
                "Since we dont record the username of the reviewer, we will also say that the ith review in our data set was submitted by user i.",
                "When we need to consider only the reviews of a given hotel, h, we will use (ri(h) , Ti(h) ) to denote the ith review about the hotel h. 3.",
                "EVIDENCE FROM TEXTUAL COMMENTS The free textual comments associated to online reviews are a valuable source of information for understanding the reasons behind the numerical ratings left by the reviewers.",
                "The text may, for example, reveal concrete examples of aspects that the user liked or disliked, thus justifying some of the high, respectively low ratings for certain features.",
                "The text may also offer guidelines for understanding the preferences of the reviewer, and the weights of different features when computing an overall rating.",
                "The problem, however, is that free textual comments are difficult to read.",
                "Users are required to scroll through many reviews and read mostly repetitive information.",
                "Significant improvements would be obtained if the reviews were automatically interpreted and aggregated.",
                "Unfortunately, this seems a difficult task for computers since human users often use witty language, abbreviations, cultural specific phrases, and the figurative style.",
                "Nevertheless, several important results use the textual comments of online reviews in an automated way.",
                "Using well established natural language techniques, reviews or parts of reviews can be classified as having a positive or negative semantic orientation.",
                "Pang et al. [2] classify movie reviews into positive/negative by training three different classifiers (Naive Bayes, Maximum Entropy and SVM) using classification features based on unigrams, bigrams or part-of-speech tags.",
                "Dave et al. [4] analyze reviews from CNet and Amazon, and surprisingly show that classification features based on unigrams or bigrams perform better than higher-order n-grams.",
                "This result is challenged by Cui et al. [3] who look at large collections of reviews crawled from the web.",
                "They show that the size of the data set is important, and that bigger training sets allow classifiers to successfully use more complex classification features based on n-grams.",
                "Hu and Liu [11] also crawl the web for product reviews and automatically identify product attributes that have been discussed by reviewers.",
                "They use Wordnet to compute the semantic orientation of product evaluations and summarize user reviews by extracting positive and negative evaluations of different product features.",
                "Popescu and Etzioni [20] analyze a similar setting, but use search engine hit-counts to identify product attributes; the semantic orientation is assigned through the relaxation labeling technique.",
                "Ghose et al. [7, 8] analyze seller reviews from the Amazon secondary market to identify the different dimensions (e.g., delivery, packaging, customer support, etc.) of reputation.",
                "They parse the text, and tag the part-of-speech for each word.",
                "Frequent nouns, noun phrases and verbal phrases are identified as dimensions of reputation, while the corresponding modifiers (i.e., adjectives and adverbs) are used to derive numerical scores for each dimension.",
                "The enhanced reputation measure correlates better with the pricing information observed in the market.",
                "Pavlou and Dimoka [19] analyze eBay reviews and find that textual comments have an important impact on reputation premiums.",
                "Our approach is similar to the previously mentioned works, in the sense that we identify the aspects (i.e., hotel features) discussed by the users in the textual reviews.",
                "However, we do not compute the semantic orientation of the text, nor attempt to infer missing ratings.",
                "We define the weight, wi f , of feature f ∈ F in the text Ti associated with the review (ri , Ti ), as the fraction of Ti dedicated to discussing aspects (both positive and negative) related to feature f. We propose an elementary method to approximate the values of these weights.",
                "For each feature we manually construct the word list Lf containing approximately 50 words that are most commonly associated to the feature f. The initial words were selected from reading some of the reviews, and seeing what words coincide with discussion of which features.",
                "The list was then extended by adding all thesaurus entries that were related to the initial words.",
                "Finally, we brainstormed for missing words that would normally be associated with each of the features.",
                "Let Lf ∩Ti be the list of terms common to both Lf and Ti.",
                "Each term of Lf is counted the number of times it appears in Ti , with two exceptions: • in cases where the user submits a title to the review, we account for the title text by appending it three times to the review text Ti .",
                "The intuitive assumption is that the users opinion is more strongly reflected in the title, rather than in the body of the review.",
                "For example, many reviews are accurately summarized by titles such as Excellent service, terrible location or Bad value for money; • certain words that occur only once in the text are counted multiple times if their relevance to that feature is particularly strong.",
                "These were root words for each feature (e.g., staff is a root word for the feature Service), and were weighted either 2 or 3.",
                "Each feature was assigned up to 3 such root words, so almost all words are counted only once.",
                "The list of words for the feature Rooms is given for reference in Appendix A.",
                "The weight wi f is computed as: wi f = |Lf ∩ Ti| f∈F |Lf ∩ Ti| (1) where |Lf ∩Ti | is the number of terms common to Lf and Ti .",
                "The weight for the feature Overall was set to min{ |T i | 5000 , 1} where |Ti | is the number of character in Ti .",
                "The following is a TripAdvisor review for a Boston hotel (the name of the hotel is omitted): Ill start by saying that Im more of a Holiday Inn person than a *** type.",
                "So I get frustrated when I pay double the room rate and get half the amenities that Id get at a Hampton Inn or Holiday Inn.",
                "The location was definitely the main asset of this place.",
                "It was only a few blocks from the Hynes Center subway stop and it was easy to walk to some good restaurants in the Back Bay area.",
                "Boylston isnt far off at all.",
                "So I had no trouble with foregoing a rental car and taking the subway from the airport to the hotel and using the subway for any other travel.",
                "Otherwise, they make you pay for anything and everything. 136 And when youve already dropped $215/night on the room, that gets frustrating.The room itself was decent, about what I would expect.",
                "Staff was also average, not bad and not excellent.",
                "Again, I think youre paying for location and the ability to walk to a lot of good stuff.",
                "But I think next time Ill stay in Brookline, get more amenities, and use the subway a bit more.",
                "This numerical ratings associated to this review are rO = 3, rR = 3, rS = 3, rC = 4, rV = 2 for features Overall(O), Rooms(R), Service(S), Cleanliness(C) and Value(V) respectively.",
                "The ratings for the features Food(F), Location(L) and Noise(N) are absent (i.e., rF = rL = rN = 0).",
                "The weights wf are computed from the following lists of common terms: LR ∩ T ={room}; wR = 0.066 LS ∩ T ={3 * Staff, amenities}; wS = 0.267 LC ∩ T = ∅; wC = 0 LV ∩ T ={$, rate}; wV = 0.133 LF ∩ T ={restaurant}; wF = 0.067 LL ∩ T ={2 * center, 2 * walk, 2 * location, area}; wL = 0.467 LN ∩ T = ∅; wN = 0 The root words Staff and Center were tripled and doubled respectively.",
                "The overall weight of the textual review is wO = 0.197.",
                "These values account reasonably well for the weights of different features in the discussion of the reviewer.",
                "One point to note is that some terms in the lists Lf possess an inherent semantic orientation.",
                "For example the word grime (belonging to the list LC ) would be used most often to assert the presence, and not the absence of grime.",
                "This is unavoidable, but care was taken to ensure words from both sides of the spectrum were used.",
                "For this reason, some lists such as LR contain only nouns of objects that one would typically describe in a room (see Appendix A).",
                "The goal of this section is to analyse the influence of the weights wi f on the numerical ratings ri f .",
                "Intuitively, users who spent a lot of their time discussing a feature f (i.e., wi f is high) had something to say about their experience with regard to this feature.",
                "Obviously, feature f is important for user i.",
                "Since people tend to be more knowledgeable in the aspects they consider important, our hypothesis is that the ratings ri f (corresponding to high weights wi f ) constitute a subset of expert ratings for feature f. Figure 2 plots the distribution of the rates r i(h) C with respect to the weights w i(h) C for the cleanliness of a Las Vegas hotel, h. Here, the high ratings are restricted to the reviews that discuss little the cleanliness.",
                "Whenever cleanliness appears in the discussion, the ratings are low.",
                "Many hotels exhibit similar rating patterns for various features.",
                "Ratings corresponding to low weights span the whole spectrum from 1 to 5, while the ratings corresponding to high weights are more grouped together (either around good or bad ratings).",
                "We therefore make the following hypothesis: Hypothesis 1.",
                "The ratings ri f corresponding to the reviews where wi f is high, are more similar to each other than to the overall collection of ratings.",
                "To test the hypothesis, we take the entire set of reviews, and feature by feature, we compute the standard deviation of the ratings with high weights, and the standard deviation of the entire set of ratings.",
                "High weights were defined as those belonging to the upper 20% of the weight range for the corresponding feature.",
                "If Hypothesis 1 were true, the standard deviation of all ratings should be higher than the standard deviation of the ratings with high weights. 0 1 2 3 4 5 6 0 0.1 0.2 0.3 0.4 0.5 0.6 Rating Weight Figure 2: The distribution of ratings against the weight of the cleanliness feature.",
                "We use a standard T-test to measure the significance of the results.",
                "City by city and feature by feature, Table 2 presents the average standard deviation of all ratings, and the average standard deviation of ratings with high weights.",
                "Indeed, the ratings with high weights have lower standard deviation, and the results are significant at the standard 0.05 significance threshold (although for certain cities taken independently there doesnt seem to be a significant difference, the results are significant for the entire data set).",
                "Please note that only the features O,R,S,C and V were considered, since for the others (F, L, and N) we didnt have enough ratings.",
                "Table 2: Average standard deviation for all ratings, and average standard deviation for ratings with high weights.",
                "In square brackets, the corresponding p-values for a positive difference between the two.",
                "City O R S C V all 1.189 0.998 1.144 0.935 1.123 Boston high 0.948 0.778 0.954 0.767 0.891 p-val [0.000] [0.004] [0.045] [0.080] [0.009] all 1.040 0.832 1.101 0.847 0.963 Sydney high 0.801 0.618 0.691 0.690 0.798 p-val [0.012] [0.023] [0.000] [0.377] [0.037] all 1.272 1.142 1.184 1.119 1.242 Vegas high 1.072 0.752 1.169 0.907 1.003 p-val [0.0185] [0.001] [0.918] [0.120] [0.126] Hypothesis 1 not only provides some basic understanding regarding the rating behavior of online users, it also suggests some ways of computing better quality estimates.",
                "We can, for example, construct a feature-by-feature quality estimate with much lower variance: for each feature we take the subset of reviews that amply discuss that feature, and output as a quality estimate the average rating for this subset.",
                "Initial experiments suggest that the average feature-by-feature ratings computed in this way are different from the average ratings computed on the whole data set.",
                "Given that, indeed, high weights are indicators of expert opinions, the estimates obtained in this way are more accurate than the current ones.",
                "Nevertheless, the validation of this underlying assumption requires further controlled experiments. 137 4.",
                "THE INFLUENCE OF PAST RATINGS Two important assumptions are generally made about reviews submitted to online forums.",
                "The first is that ratings truthfully reflect the quality observed by the users; the second is that reviews are independent from one another.",
                "While anecdotal evidence [9, 22] challenges the first assumption3 , in this section, we address the second.",
                "A perusal of online reviews shows that reviews are often part of discussion threads, where users make an effort to contradict, or vehemently agree with the remarks of previous users.",
                "Consider, for example, the following review: I dont understand the negative reviews... the hotel was a little dark, but that was the style.",
                "It was very artsy.",
                "Yes it was close to the freeway, but in my opinion the sound of an occasional loud car is better than hearing the ding ding of slot machines all night!",
                "The staff on-hand is FABULOUS.",
                "The waitresses are great (and *** does not deserve the bad review she got, she was 100% attentive to us! ), the bartenders are friendly and professional at the same time...",
                "Here, the user was disturbed by previous negative reports, addressed these concerns, and set about trying to correct them.",
                "Not surprisingly, his ratings were considerably higher than the average ratings up to this point.",
                "It seems that TripAdvisor users regularly read the reports submitted by previous users before booking a hotel, or before writing a review.",
                "Past reviews create some prior expectation regarding the quality of service, and this expectation has an influence on the submitted review.",
                "We believe this observation holds for most online forums.",
                "The subjective perception of quality is directly proportional to how well the actual experience meets the prior expectation, a fact confirmed by an important line of econometric and marketing research [17, 18, 16, 21].",
                "The correlation between the reviews has also been confirmed by recent research on the dynamics of <br>online review</br> forums [6]. 4.1 Prior Expectations We define the prior expectation of user i regarding the feature f, as the average of the previously available ratings on the feature f4 : ef (i) = j<i,r j f =0 rj f j<i,r j f =0 1 As a first hypothesis, we assert that the rating ri f is a function of the prior expectation ef (i): Hypothesis 2.",
                "For a given hotel and feature, given the reviews i and j such that ef (i) is high and ef (j) is low, the rating rj f exceeds the rating ri f .",
                "We define high and low expectations as those that are above, respectively below a certain cutoff value θ.",
                "The set of reviews preceded by high, respectively low expectations 3 part of Amazon reviews were recognized as strategic posts by book authors or competitors 4 if no previous ratings were assigned for feature f, ef (i) is assigned a default value of 4.",
                "Table 3: Average ratings for reviews preceded by low (first value in the cell) and high (second value in the cell) expectations.",
                "The P-values for a positive difference are given square brackets.",
                "City O R S C V 3.953 4.045 3.985 4.252 3.946 Boston 3.364 3.590 3.485 3.641 3.242 [0.011] [0.028] [0.0086] [0.0168] [0.0034] 4.284 4.358 4.064 4.530 4.428 Sydney 3.756 3.537 3.436 3.918 3.495 [0.000] [0.000] [0.035] [0.009] [0.000] 3.494 3.674 3.713 3.689 3.580 Las Vegas 3.140 3.530 2.952 3.530 3.351 [0.190] [0.529] [0.007] [0.529] [0.253] are defined as follows: Rhigh f = {ri f |ef (i) > θ} Rlow f = {ri f |ef (i) < θ} These sets are specific for each (hotel, feature) pair, and in our experiments we took θ = 4.",
                "This rather high value is close to the average rating across all features across all hotels, and is justified by the fact that our data set contains mostly high quality hotels.",
                "For each city, we take all hotels and compute the average ratings in the sets Rhigh f and Rlow f (see Table 3).",
                "The average rating amongst reviews following low prior expectations is significantly higher than the average rating following high expectations.",
                "As further evidence, we consider all hotels for which the function eV (i) (the expectation for the feature Value) has a high value (greater than 4) for some i, and a low value (less than 4) for some other i.",
                "Intuitively, these are the hotels for which there is a minimal degree of variation in the timely sequence of reviews: i.e., the cumulative average of ratings was at some point high and afterwards became low, or vice-versa.",
                "Such variations are observed for about half of all hotels in each city.",
                "Figure 3 plots the median (across considered hotels) rating, rV , when ef (i) is not more than x but greater than x − 0.5. 2.5 3 3.5 4 4.5 5 2.5 3 3.5 4 4.5 5 Medianofrating expectation Boston Sydney Vegas Figure 3: The ratings tend to decrease as the expectation increases. 138 There are two ways to interpret the function ef (i): • The expected value for feature f obtained by user i before his experience with the service, acquired by reading reports submitted by past users.",
                "In this case, an overly high value for ef (i) would drive the user to submit a negative report (or vice versa), stemming from the difference between the actual value of the service, and the inflated expectation of this value acquired before his experience. • The expected value of feature f for all subsequent visitors of the site, if user i were not to submit a report.",
                "In this case, the motivation for a negative report following an overly high value of ef is different: user i seeks to correct the expectation of future visitors to the site.",
                "Unlike the interpretation above, this does not require the user to derive an a priori expectation for the value of f. Note that neither interpretation implies that the average up to report i is inversely related to the rating at report i.",
                "There might exist a measure of influence exerted by past reports that pushes the user behind report i to submit ratings which to some extent conforms with past reports: a low value for ef (i) can influence user i to submit a low rating for feature f because, for example, he fears that submitting a high rating will make him out to be a person with low standards5 .",
                "This, at first, appears to contradict Hypothesis 2.",
                "However, this conformity rating cannot continue indefinitely: once the set of reports project a sufficiently deflated estimate for vf , future reviewers with comparatively positive impressions will seek to correct this misconception. 4.2 Impact of textual comments on quality expectation Further insight into the rating behavior of TripAdvisor users can be obtained by analyzing the relationship between the weights wf and the values ef (i).",
                "In particular, we examine the following hypothesis: Hypothesis 3.",
                "When a large proportion of the text of a review discusses a certain feature, the difference between the rating for that feature and the average rating up to that point tends to be large.",
                "The intuition behind this claim is that when the user is adamant about voicing his opinion regarding a certain feature, his opinion differs from the collective opinion of previous postings.",
                "This relies on the characteristic of reputation systems as feedback forums where a user is interested in projecting his opinion, with particular strength if this opinion differs from what he perceives to be the general opinion.",
                "To test Hypothesis 3 we measure the average absolute difference between the expectation ef (i) and the rating ri f when the weight wi f is high, respectively low.",
                "Weights are classified high or low by comparing them with certain cutoff values: wi f is low if smaller than 0.1, while wi f is high if greater than θf .",
                "Different cutoff values were used for different features: θR = 0.4, θS = 0.4, θC = 0.2, and θV = 0.7.",
                "Cleanliness has a lower cutoff since it is a feature rarely discussed; Value has a high cutoff for the opposite reason.",
                "Results are presented in Table 4. 5 The idea that negative reports can encourage further negative reporting has been suggested before [14] Table 4: Average of |ri f −ef (i)| when weights are high (first value in the cell) and low (second value in the cell) with P-values for the difference in sq. brackets.",
                "City R S C V 1.058 1.208 1.728 1.356 Boston 0.701 0.838 0.760 0.917 [0.022] [0.063] [0.000] [0.218] 1.048 1.351 1.218 1.318 Sydney 0.752 0.759 0.767 0.908 [0.179] [0.009] [0.165] [0.495] 1.184 1.378 1.472 1.642 Las Vegas 0.772 0.834 0.808 1.043 [0.071] [0.020] [0.006] [0.076] This demonstrates that when weights are unusually high, users tend to express an opinion that does not conform to the net average of previous ratings.",
                "As we might expect, for a feature that rarely was a high weight in the discussion, (e.g., cleanliness) the difference is particularly large.",
                "Even though the difference in the feature Value is quite large for Sydney, the P-value is high.",
                "This is because only few reviews discussed value heavily.",
                "The reason could be cultural or because there was less of a reason to discuss this feature. 4.3 Reporting Incentives Previous models suggest that users who are not highly opinionated will not choose to voice their opinions [12].",
                "In this section, we extend this model to account for the influence of expectations.",
                "The motivation for submitting feedback is not only due to extreme opinions, but also to the difference between the current reputation (i.e., the prior expectation of the user) and the actual experience.",
                "Such a rating model produces ratings that most of the time deviate from the current average rating.",
                "The ratings that confirm the prior expectation will rarely be submitted.",
                "We test on our data set the proportion of ratings that attempt to correct the current estimate.",
                "We define a deviant rating as one that deviates from the current expectation by at least some threshold θ, i.e., |ri f − ef (i)| ≥ θ.",
                "For each of the three considered cities, the following tables, show the proportion of deviant ratings for θ = 0.5 and θ = 1.",
                "Table 5: Proportion of deviant ratings with θ = 0.5 City O R S C V Boston 0.696 0.619 0.676 0.604 0.684 Sydney 0.645 0.615 0.672 0.614 0.675 Las Vegas 0.721 0.641 0.694 0.662 0.724 Table 6: Proportion of deviant ratings with θ = 1 City O R S C V Boston 0.420 0.397 0.429 0.317 0.446 Sydney 0.360 0.367 0.442 0.336 0.489 Las Vegas 0.510 0.421 0.483 0.390 0.472 The above results suggest that a large proportion of users (close to one half, even for the high threshold value θ = 1) deviate from the prior average.",
                "This reinforces the idea that users are more likely to submit a report when they believe they have something distinctive to add to the current stream of opinions for some feature.",
                "Such conclusions are in total agreement with prior evidence that the distribution of reports often follows bi-modal, U-shaped distributions. 139 5.",
                "MODELLING THE BEHAVIOR OF RATERS To account for the observations described in the previous sections, we propose a model for the behavior of the users when submitting online reviews.",
                "For a given hotel, we make the assumption that the quality experienced by the users is normally distributed around some value vf , which represents the objective quality offered by the hotel on the feature f. The rating submitted by user i on feature f is: ˆri f = δf vi f + (1 − δf ) · sign vi f − ef (i) c + d(vi f , ef (i)|wi f ) (2) where: • vi f is the (unknown) quality actually experienced by the user. vi f is assumed normally distributed around some value vf ; • δf ∈ [0, 1] can be seen as a measure of the bias when reporting feedback.",
                "High values reflect the fact that users rate objectively, without being influenced by prior expectations.",
                "The value of δf may depend on various factors; we fix one value for each feature f; • c is a constant between 1 and 5; • wi f is the weight of feature f in the textual comment of review i, computed according to Eq. (1); • d(vi f , ef (i)|wi f ) is a distance function between the expectation and the observation of user i.",
                "The distance function satisfies the following properties: - d(y, z|w) ≥ 0 for all y, z ∈ [0, 5], w ∈ [0, 1]; - |d(y, z|w)| < |d(z, x|w)| if |y − z| < |z − x|; - |d(y, z|w1)| < |d(y, z|w2)| if w1 < w2; - c + d(vf , ef (i)|wi f ) ∈ [1, 5]; The second term of Eq. (2) encodes the bias of the rating.",
                "The higher the distance between the true observation vi f and the function ef , the higher the bias. 5.1 Model Validation We use the data set of TripAdvisor reviews to validate the behavior model presented above.",
                "We split for convenience the rating values in three ranges: bad (B = {1, 2}), indifferent (I = {3, 4}), and good (G = {5}), and perform the following two tests: • First, we will use our model to predict the ratings that have extremal values.",
                "For every hotel, we take the sequence of reports, and whenever we encounter a rating that is either good or bad (but not indifferent) we try to predict it using Eq. (2) • Second, instead of predicting the value of extremal ratings, we try to classify them as either good or bad.",
                "For every hotel we take the sequence of reports, and for each report (regardless of it value) we classify it as being good or bad However, to perform these tests, we need to estimate the objective value, vf , that is the average of the true quality observations, vi f .",
                "The algorithm we are using is based on the intuition that the amount of conformity rating is minimized.",
                "In other words, the value vf should be such that as often as possible, bad ratings follow expectations above vf and good ratings follow expectations below vf .",
                "Formally, we define the sets: Γ1 = {i|ef (i) < vf and ri f ∈ B}; Γ2 = {i|ef (i) > vf and ri f ∈ G}; that correspond to irregularities where even though the expectation at point i is lower than the delivered value, the rating is poor, and vice versa.",
                "We define vf as the value that minimize these union of the two sets: vf = arg min vf |Γ1 ∪ Γ2| (3) In Eq. (2) we replace vi f by the value vf computed in Eq. (3), and use the following distance function: d(vf , ef (i)|wi f ) = |vf − ef (i)| vf − ef (i) |vf 2 − ef (i)2 | · (1 + 2wi f ); The constant c ∈ I was set to min{max{ef (i), 3}}, 4}.",
                "The values for δf were fixed at {0.7, 0.7, 0.8, 0.7, 0.6} for the features {Overall, Rooms, Service, Cleanliness, Value} respectively.",
                "The weights are computed as described in Section 3.",
                "As a first experiment, we take the sets of extremal ratings {ri f |ri f /∈ I} for each hotel and feature.",
                "For every such rating, ri f , we try to estimate it by computing ˆri f using Eq. (2).",
                "We compare this estimator with the one obtained by simply averaging the ratings over all hotels and features: i.e., ¯rf = j,r j f =0 rj f j,r j f =0 1 ; Table 7 presents the ratio between the root mean square error (RMSE) when using ˆri f and ¯rf to estimate the actual ratings.",
                "In all cases the estimate produced by our model is better than the simple average.",
                "Table 7: Average of RMSE(ˆrf ) RMSE(¯rf ) City O R S C V Boston 0.987 0.849 0.879 0.776 0.913 Sydney 0.927 0.817 0.826 0.720 0.681 Las Vegas 0.952 0.870 0.881 0.947 0.904 As a second experiment, we try to distinguish the sets Bf = {i|ri f ∈ B} and Gf = {i|ri f ∈ G} of bad, respectively good ratings on the feature f. For example, we compute the set Bf using the following classifier (called σ): ri f ∈ Bf (σf (i) = 1) ⇔ ˆri f ≤ 4; Tables 8, 9 and 10 present the Precision(p), Recall(r) and s = 2pr p+r for classifier σ, and compares it with a naive majority classifier, τ, τf (i) = 1 ⇔ |Bf | ≥ |Gf |: We see that recall is always higher for σ and precision is usually slightly worse.",
                "For the s metric σ tends to add a 140 Table 8: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Boston O R S C V p 0.678 0.670 0.573 0.545 0.610 σ r 0.626 0.659 0.619 0.612 0.694 s 0.651 0.665 0.595 0.577 0.609 p 0.684 0.706 0.647 0.611 0.633 τ r 0.597 0.541 0.410 0.383 0.562 s 0.638 0.613 0.502 0.471 0.595 Table 9: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Las Vegas O R S C V p 0.654 0.748 0.592 0.712 0.583 σ r 0.608 0.536 0.791 0.474 0.610 s 0.630 0.624 0.677 0.569 0.596 p 0.685 0.761 0.621 0.748 0.606 τ r 0.542 0.505 0.767 0.445 0.441 s 0.605 0.607 0.670 0.558 0.511 1-20% improvement over τ, much higher in some cases for hotels in Sydney.",
                "This is likely because Sydney reviews are more positive than those of the American cities and cases where the number of bad reviews exceeded the number of good ones are rare.",
                "Replacing the test algorithm with one that plays a 1 with probability equal to the proportion of bad reviews improves its results for this city, but it is still outperformed by around 80%. 6.",
                "SUMMARY OF RESULTS AND CONCLUSION The goal of this paper is to explore the factors that drive a user to submit a particular rating, rather than the incentives that encouraged him to submit a report in the first place.",
                "For that we use two additional sources of information besides the vector of numerical ratings: first we look at the textual comments that accompany the reviews, and second we consider the reports that have been previously submitted by other users.",
                "Using simple natural language processing algorithms, we were able to establish a correlation between the weight of a certain feature in the textual comment accompanying the review, and the noise present in the numerical rating.",
                "Specifically, it seems that users who discuss amply a certain feature are likely to agree on a common rating.",
                "This observation allows the construction of feature-by-feature estimators of quality that have a lower variance, and are hopefully less noisy.",
                "Nevertheless, further evidence is required to support the intuition that ratings corresponding to high weights are expert opinions that deserve to be given higher priority when computing estimates of quality.",
                "Second, we emphasize the dependence of ratings on previous reports.",
                "Previous reports create an expectation of quality which affects the subjective perception of the user.",
                "We validate two facts about the hotel reviews we collected from TripAdvisor: First, the ratings following low expectations (where the expectation is computed as the average of the previous reports) are likely to be higher than the ratings Table 10: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Sydney O R S C V p 0.650 0.463 0.544 0.550 0.580 σ r 0.234 0.378 0.571 0.169 0.592 s 0.343 0.452 0.557 0.259 0.586 p 0.562 0.615 0.600 0.500 0.600 τ r 0.054 0.098 0.101 0.015 0.175 s 0.098 0.168 0.172 0.030 0.271 following high expectations.",
                "Intuitively, the perception of quality (and consequently the rating) depends on how well the actual experience of the user meets her expectation.",
                "Second, we include evidence from the textual comments, and find that when users devote a large fraction of the text to discussing a certain feature, they are likely to motivate a divergent rating (i.e., a rating that does not conform to the prior expectation).",
                "Intuitively, this supports the hypothesis that review forums act as discussion groups where users are keen on presenting and motivating their own opinion.",
                "We have captured the empirical evidence in a behavior model that predicts the ratings submitted by the users.",
                "The final rating depends, as expected, on the true observation, and on the gap between the observation and the expectation.",
                "The gap tends to have a bigger influence when an important fraction of the textual comment is dedicated to discussing a certain feature.",
                "The proposed model was validated on the empirical data and provides better estimates of the ratings actually submitted.",
                "One assumption that we make is about the existence of an objective quality value vf for the feature f. This is rarely true, especially over large spans of time.",
                "Other explanations might account for the correlation of ratings with past reports.",
                "For example, if ef (i) reflects the true value of f at a point in time, the difference in the ratings following high and low expectations can be explained by hotel revenue models that are maximized when the value is modified accordingly.",
                "However, the idea that variation in ratings is not primarily a function of variation in value turns out to be a useful one.",
                "Our approach to approximate this elusive objective value is by no means perfect, but conforms neatly to the idea behind the model.",
                "A natural direction for future work is to examine concrete applications of our results.",
                "Significant improvements of quality estimates are likely to be obtained by incorporating all empirical evidence about rating behavior.",
                "Exactly how different factors affect the decisions of the users is not clear.",
                "The answer might depend on the particular application, context and culture. 7.",
                "REFERENCES [1] A. Admati and P. Pfleiderer.",
                "Noisytalk.com: Broadcasting opinions in a noisy environment.",
                "Working Paper 1670R, Stanford University, 2000. [2] P. B., L. Lee, and S. Vaithyanathan.",
                "Thumbs up? sentiment classification using machine learning techniques.",
                "In Proceedings of the EMNLP-02, the Conference on Empirical Methods in Natural Language Processing, 2002. [3] H. Cui, V. Mittal, and M. Datar.",
                "Comparative 141 Experiments on Sentiment Classification for Online Product Reviews.",
                "In Proceedings of AAAI, 2006. [4] K. Dave, S. Lawrence, and D. Pennock.",
                "Mining the peanut gallery:opinion extraction and semantic classification of product reviews.",
                "In Proceedings of the 12th International Conference on the World Wide Web (WWW03), 2003. [5] C. Dellarocas, N. Awad, and X. Zhang.",
                "Exploring the Value of Online Product Ratings in Revenue Forecasting: The Case of Motion Pictures.",
                "Working paper, 2006. [6] C. Forman, A. Ghose, and B. Wiesenfeld.",
                "A Multi-Level Examination of the Impact of Social Identities on Economic Transactions in Electronic Markets.",
                "Available at SSRN: http://ssrn.com/abstract=918978, July 2006. [7] A. Ghose, P. Ipeirotis, and A. Sundararajan.",
                "Reputation Premiums in Electronic Peer-to-Peer Markets: Analyzing Textual Feedback and Network Structure.",
                "In Third Workshop on Economics of Peer-to-Peer Systems, (P2PECON), 2005. [8] A. Ghose, P. Ipeirotis, and A. Sundararajan.",
                "The Dimensions of Reputation in electronic Markets.",
                "Working Paper CeDER-06-02, New York University, 2006. [9] A. Harmon.",
                "Amazon Glitch Unmasks War of Reviewers.",
                "The New York Times, February 14, 2004. [10] D. Houser and J. Wooders.",
                "Reputation in Auctions: Theory and Evidence from eBay.",
                "Journal of Economics and Management Strategy, 15:353-369, 2006. [11] M. Hu and B. Liu.",
                "Mining and summarizing customer reviews.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD04), 2004. [12] N. Hu, P. Pavlou, and J. Zhang.",
                "Can Online Reviews Reveal a Products True Quality?",
                "In Proceedings of ACM Conference on Electronic Commerce (EC 06), 2006. [13] K. Kalyanam and S. McIntyre.",
                "Return on reputation in online auction market.",
                "Working Paper 02/03-10-WP, Leavey School of Business, Santa Clara University., 2001. [14] L. Khopkar and P. Resnick.",
                "Self-Selection, Slipping, Salvaging, Slacking, and Stoning: the Impacts of Negative Feedback at eBay.",
                "In Proceedings of ACM Conference on Electronic Commerce (EC 05), 2005. [15] M. Melnik and J. Alm.",
                "Does a sellers reputation matter? evidence from ebay auctions.",
                "Journal of Industrial Economics, 50(3):337-350, 2002. [16] R. Olshavsky and J. Miller.",
                "Consumer Expectations, Product Performance and Perceived Product Quality.",
                "Journal of Marketing Research, 9:19-21, February 1972. [17] A. Parasuraman, V. Zeithaml, and L. Berry.",
                "A Conceptual Model of Service Quality and Its Implications for Future Research.",
                "Journal of Marketing, 49:41-50, 1985. [18] A. Parasuraman, V. Zeithaml, and L. Berry.",
                "SERVQUAL: A Multiple-Item Scale for Measuring Consumer Perceptions of Service Quality.",
                "Journal of Retailing, 64:12-40, 1988. [19] P. Pavlou and A. Dimoka.",
                "The Nature and Role of Feedback Text Comments in Online Marketplaces: Implications for Trust Building, Price Premiums, and Seller Differentiation.",
                "Information Systems Research, 17(4):392-414, 2006. [20] A. Popescu and O. Etzioni.",
                "Extracting product features and opinions from reviews.",
                "In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, 2005. [21] R. Teas.",
                "Expectations, Performance Evaluation, and Consumers Perceptions of Quality.",
                "Journal of Marketing, 57:18-34, 1993. [22] E. White.",
                "Chatting a Singer Up the Pop Charts.",
                "The Wall Street Journal, October 15, 1999.",
                "APPENDIX A.",
                "LIST OF WORDS, LR, ASSOCIATED TO THE FEATURE ROOMS All words serve as prefixes: room, space, interior, decor, ambiance, atmosphere, comfort, bath, toilet, bed, building, wall, window, private, temperature, sheet, linen, pillow, hot, water, cold, water, shower, lobby, furniture, carpet, air, condition, mattress, layout, design, mirror, ceiling, lighting, lamp, sofa, chair, dresser, wardrobe, closet 142"
            ],
            "original_annotated_samples": [
                "The correlation between the reviews has also been confirmed by recent research on the dynamics of <br>online review</br> forums [6]. 4.1 Prior Expectations We define the prior expectation of user i regarding the feature f, as the average of the previously available ratings on the feature f4 : ef (i) = j<i,r j f =0 rj f j<i,r j f =0 1 As a first hypothesis, we assert that the rating ri f is a function of the prior expectation ef (i): Hypothesis 2."
            ],
            "translated_annotated_samples": [
                "La correlación entre las reseñas también ha sido confirmada por investigaciones recientes sobre la dinámica de los foros de <br>reseñas en línea</br> [6]. 4.1 Expectativas Previas Definimos la expectativa previa del usuario i con respecto a la característica f, como el promedio de las calificaciones previamente disponibles en la característica f4: ef (i) = j<i,r j f =0 rj f j<i,r j f =0 1 Como primera hipótesis, afirmamos que la calificación ri f es una función de la expectativa previa ef (i): Hipótesis 2."
            ],
            "translated_text": "La comprensión del comportamiento del usuario en la presentación de comentarios en línea. Trabajos anteriores han demostrado que la información contradictoria y los sesgos subyacentes de los usuarios dificultan juzgar el verdadero valor de un servicio. En este artículo, investigamos los factores subyacentes que influyen en el comportamiento del usuario al informar retroalimentación. Examinamos dos fuentes de información además de las calificaciones numéricas: la evidencia lingüística del comentario textual que acompaña a una reseña y los patrones en la secuencia temporal de los informes. Primero demostramos que los grupos de usuarios que discuten ampliamente una característica específica tienen más probabilidades de ponerse de acuerdo en una calificación común para esa característica. Segundo, demostramos que la calificación de un usuario refleja en parte la diferencia entre la calidad real y la expectativa previa de calidad, tal como se infiere de revisiones anteriores. Ambos nos ofrecen una forma menos ruidosa de producir estimaciones de calificación y revelar las razones detrás del sesgo del usuario. Nuestras hipótesis fueron validadas por evidencia estadística de reseñas de hoteles en el sitio web de TripAdvisor. Categorías y Descriptores de Asignaturas J.4 [Ciencias Sociales y del Comportamiento]: Economía Términos Generales Economía, Experimentación, Confiabilidad 1. MOTIVACIONES La expansión de internet ha hecho posible que los foros de retroalimentación en línea (o mecanismos de reputación) se conviertan en un canal importante para el boca a boca sobre productos, servicios u otros tipos de interacciones comerciales. Numerosos estudios empíricos [10, 15, 13, 5] muestran que los compradores consideran seriamente los comentarios en línea al tomar decisiones de compra, y están dispuestos a pagar primas por reputación por productos o servicios que tienen una buena reputación. Sin embargo, un análisis reciente plantea preguntas importantes sobre la capacidad de los foros existentes para reflejar la verdadera calidad de un producto. En ausencia de incentivos claros, los usuarios con una perspectiva moderada no se molestarán en expresar sus opiniones, lo que conduce a una muestra no representativa de reseñas. Por ejemplo, [12, 1] muestran que las calificaciones de libros o CDs en Amazon1 siguen con gran probabilidad distribuciones bimodales en forma de U, donde la mayoría de las calificaciones son o muy buenas o muy malas. Los experimentos controlados, por otro lado, revelan opiniones sobre los mismos elementos que están distribuidas de forma normal. Bajo estas circunstancias, utilizar la media aritmética para predecir la calidad (como la mayoría de los foros realmente hacen) proporciona al usuario típico un estimador con una alta varianza que a menudo es incorrecto. Mejorar la forma en que agregamos la información disponible de las reseñas en línea requiere un profundo entendimiento de los factores subyacentes que sesgan el comportamiento de calificación de los usuarios. Hu et al. [12] proponen el Modelo Brag-and-Moan donde los usuarios califican solo si su utilidad del producto (extraída de una distribución normal) cae fuera de un intervalo mediano. Los autores concluyen que el modelo explica la distribución empírica de los informes y ofrece ideas sobre formas más inteligentes de estimar la verdadera calidad del producto. En el presente artículo ampliamos esta línea de investigación e intentamos explicar más hechos sobre el comportamiento de los usuarios al informar retroalimentación en línea. Usando reseñas reales de hoteles del sitio web TripAdvisor2, consideramos dos fuentes adicionales de información además de las calificaciones numéricas básicas enviadas por los usuarios. La primera es una evidencia lingüística simple proveniente de la revisión textual que suele acompañar a las calificaciones numéricas. Utilizamos técnicas de minería de texto similares a [7] y [3], sin embargo, solo estamos interesados en identificar qué aspectos del servicio está discutiendo el usuario, sin calcular la orientación semántica del texto. Observamos que los usuarios que comentan más sobre la misma característica tienen más probabilidades de estar de acuerdo en una calificación numérica común para esa característica en particular. Intuitivamente, los comentarios extensos revelan la importancia de la característica para el usuario. Dado que las personas tienden a ser más conocedoras en los aspectos que consideran importantes, se podría asumir que los usuarios que discuten una característica específica en más detalle tienen más autoridad para evaluar esa característica. Segundo investigamos la relación entre una reseña 1 http://www.amazon.com 2 http://www.tripadvisor.com/ 134 Figura 1: La página de TripAdvisor que muestra reseñas de un popular hotel en Boston. El nombre del hotel y los anuncios fueron deliberadamente borrados, al igual que las reseñas que lo precedieron. Un examen de las reseñas en línea muestra que las calificaciones a menudo forman parte de los hilos de discusión, donde una publicación no es necesariamente independiente de otras publicaciones. Uno puede ver, por ejemplo, usuarios que se esfuerzan por contradecir o estar vehementemente de acuerdo con los comentarios de usuarios anteriores. Al analizar la secuencia temporal de informes, concluimos que las revisiones pasadas influyen en los informes futuros, ya que crean cierta expectativa previa sobre la calidad del servicio. La percepción subjetiva del usuario está influenciada por la brecha entre la expectativa previa y el rendimiento real del servicio [17, 18, 16, 21], lo cual se reflejará más tarde en la calificación de los usuarios. Proponemos un modelo que captura la dependencia de las calificaciones en las expectativas previas, y lo validamos utilizando los datos empíricos que recopilamos. Ambos resultados pueden ser utilizados para mejorar la forma en que los mecanismos de reputación agregan la información de las revisiones individuales. Nuestro primer resultado se puede utilizar para determinar una estimación de calidad característica por característica, donde para cada característica se considera un subconjunto diferente de reseñas (es decir, aquellas con comentarios extensos sobre esa característica). El segundo conduce a un algoritmo que produce una estimación más precisa de la calidad real. 2. El conjunto de datos que utilizamos en este artículo son reseñas reales de hoteles recopiladas del popular sitio de viajes TripAdvisor. TripAdvisor indexa hoteles de ciudades de todo el mundo, junto con reseñas escritas por viajeros. Los usuarios pueden buscar en el sitio proporcionando el nombre del hotel y la ubicación (opcional). Las reseñas de un hotel dado se muestran como una lista (ordenadas de la más reciente a la más antigua), con 5 reseñas por página. Las reseñas contienen: • información sobre el autor de la reseña (por ejemplo, fechas de estancia, nombre de usuario del revisor, ubicación del revisor); • la calificación general (de 1, la más baja, a 5, la más alta); • una reseña textual que incluye un título para la reseña, comentarios libres y las principales cosas que al revisor le gustaron y no le gustaron; • calificaciones numéricas (de 1, la más baja, a 5, la más alta) para diferentes características (por ejemplo, limpieza, servicio, ubicación, etc.). Debajo del nombre del hotel, TripAdvisor muestra la dirección del hotel, información general (número de habitaciones, número de estrellas, breve descripción, etc.), la calificación promedio general, el ranking de TripAdvisor y una calificación promedio para cada característica. La Figura 1 muestra la página de un hotel popular en Boston cuyo nombre (junto con los anuncios) fue explícitamente borrado. Seleccionamos tres ciudades para este estudio: Boston, Sídney y Las Vegas. Para cada ciudad consideramos todos los hoteles que tenían al menos 10 reseñas, y registramos todas las reseñas. La Tabla 1 presenta el número de hoteles considerados en cada ciudad, el número total de reseñas registradas para cada ciudad y la distribución de hoteles con respecto a la calificación por estrellas (según está disponible en el sitio de TripAdvisor). Ten en cuenta que no todos los hoteles tienen una clasificación por estrellas. Tabla 1: Un resumen del conjunto de datos. Para cada reseña, registramos la calificación general, la reseña textual (título y cuerpo de la reseña) y la calificación numérica en 7 características: Habitaciones (R), Servicio (S), Limpieza (C), Valor (V), Comida (F), Ubicación (L) y Ruido (N). TripAdvisor no requiere que los usuarios envíen nada más que la calificación general, por lo tanto, una reseña típica evalúa pocas características adicionales, independientemente de la discusión en el comentario textual. Solo las características Habitaciones (R), Servicio (S), Limpieza (C) y Valor (V) son evaluadas por un número significativo de usuarios. Sin embargo, también seleccionamos las características Comida (F), Ubicación (L) y Ruido (N) porque son mencionadas en un número significativo de comentarios textuales. Para cada característica registramos la calificación numérica dada por el usuario, o 0 cuando la calificación está ausente. La longitud típica del comentario textual equivale aproximadamente a 200 palabras. Todos los datos fueron recopilados rastreando el sitio de TripAdvisor en septiembre de 2006. 2.1 Notación formal Nos referiremos formalmente a una reseña mediante una tupla (r, T) donde: • r = (rf ) es un vector que contiene las calificaciones rf ∈ {0, 1, . . . 5} para las características f ∈ F = {O, R, S, C, V, F, L, N}; cabe destacar que la calificación general, rO, se registra de forma abusiva como la calificación para la característica General(O); • T es el comentario textual que acompaña a la reseña. Se indexan 135 reseñas de acuerdo con la variable i, de modo que (ri , Ti ) es la i-ésima reseña en nuestra base de datos. Dado que no registramos el nombre de usuario del revisor, también diremos que la i-ésima reseña en nuestro conjunto de datos fue enviada por el usuario i. Cuando necesitemos considerar solo las reseñas de un hotel dado, h, usaremos (ri(h), Ti(h)) para denotar la i-ésima reseña sobre el hotel h. 3. EVIDENCIA DE COMENTARIOS TEXTUALES Los comentarios textuales gratuitos asociados a las reseñas en línea son una fuente valiosa de información para comprender las razones detrás de las calificaciones numéricas dejadas por los revisores. El texto puede, por ejemplo, revelar ejemplos concretos de aspectos que al usuario le gustaron o no le gustaron, justificando así algunas de las calificaciones altas o bajas respectivamente para ciertas características. El texto también puede ofrecer pautas para comprender las preferencias del revisor y los pesos de las diferentes características al calcular una calificación general. El problema, sin embargo, es que los comentarios textuales libres son difíciles de leer. Los usuarios deben desplazarse a través de muchas reseñas y leer principalmente información repetitiva. Se obtendrían mejoras significativas si las reseñas fueran interpretadas y agregadas automáticamente. Desafortunadamente, esta parece ser una tarea difícil para las computadoras ya que los usuarios humanos a menudo utilizan un lenguaje ingenioso, abreviaturas, frases específicas de la cultura y un estilo figurativo. Sin embargo, varios resultados importantes utilizan los comentarios textuales de las reseñas en línea de forma automatizada. Utilizando técnicas de lenguaje natural bien establecidas, las reseñas o partes de las reseñas pueden ser clasificadas como tener una orientación semántica positiva o negativa. Pang et al. [2] clasifican las críticas de películas en positivas/negativas entrenando tres clasificadores diferentes (Naive Bayes, Máxima Entropía y SVM) utilizando características de clasificación basadas en unigramas, bigramas o etiquetas de partes del discurso. Dave et al. [4] analizan reseñas de CNet y Amazon, y sorprendentemente demuestran que las características de clasificación basadas en unigramas o bigramas funcionan mejor que los n-gramas de orden superior. Este resultado es desafiado por Cui et al. [3], quienes examinan grandes colecciones de reseñas recopiladas de la web. Muestran que el tamaño del conjunto de datos es importante, y que conjuntos de entrenamiento más grandes permiten a los clasificadores utilizar con éxito características de clasificación más complejas basadas en n-gramas. Hu y Liu [11] también rastrean la web en busca de reseñas de productos e identifican automáticamente los atributos de productos que han sido discutidos por los revisores. Utilizan Wordnet para calcular la orientación semántica de las evaluaciones de productos y resumir las reseñas de usuarios extrayendo evaluaciones positivas y negativas de diferentes características del producto. Popescu y Etzioni [20] analizan un escenario similar, pero utilizan el recuento de resultados de búsqueda en motores de búsqueda para identificar atributos de productos; la orientación semántica se asigna a través de la técnica de etiquetado de relajación. Ghose et al. [7, 8] analizan las reseñas de vendedores del mercado secundario de Amazon para identificar las diferentes dimensiones (por ejemplo, entrega, empaque, atención al cliente, etc.) de la reputación. Analizan el texto y etiquetan la parte de la oración de cada palabra. Los sustantivos, frases nominales y frases verbales frecuentes se identifican como dimensiones de la reputación, mientras que los modificadores correspondientes (es decir, adjetivos y adverbios) se utilizan para derivar puntuaciones numéricas para cada dimensión. La medida de reputación mejorada se correlaciona mejor con la información de precios observada en el mercado. Pavlou y Dimoka [19] analizan las reseñas de eBay y encuentran que los comentarios textuales tienen un impacto importante en las primas de reputación. Nuestro enfoque es similar a los trabajos mencionados anteriormente, en el sentido de que identificamos los aspectos (es decir, las características del hotel) discutidos por los usuarios en las reseñas textuales. Sin embargo, no calculamos la orientación semántica del texto, ni intentamos inferir las calificaciones faltantes. Definimos el peso, wi f, de la característica f ∈ F en el texto Ti asociado con la reseña (ri, Ti), como la fracción de Ti dedicada a discutir aspectos (tanto positivos como negativos) relacionados con la característica f. Proponemos un método elemental para aproximar los valores de estos pesos. Para cada característica, construimos manualmente la lista de palabras Lf que contiene aproximadamente 50 palabras que están más comúnmente asociadas a la característica f. Las palabras iniciales fueron seleccionadas al leer algunas de las reseñas y ver qué palabras coinciden con la discusión de qué características. La lista fue luego ampliada agregando todas las entradas del tesauro que estaban relacionadas con las palabras iniciales. Finalmente, hicimos una lluvia de ideas para encontrar las palabras faltantes que normalmente estarían asociadas con cada una de las características. Que Lf ∩ Ti sea la lista de términos comunes a Lf y Ti. Cada término de Lf se cuenta el número de veces que aparece en Ti, con dos excepciones: • en los casos en que el usuario envía un título para la revisión, consideramos el texto del título agregándolo tres veces al texto de la revisión Ti. La suposición intuitiva es que la opinión del usuario se refleja con mayor fuerza en el título que en el cuerpo de la reseña. Por ejemplo, muchas reseñas son resumidas con precisión por títulos como \"Excelente servicio, ubicación terrible\" o \"Mala relación calidad-precio\"; ciertas palabras que ocurren solo una vez en el texto son contadas múltiples veces si su relevancia para esa característica es particularmente fuerte. Estas eran las palabras raíz para cada característica (por ejemplo, personal es una palabra raíz para la característica Servicio), y tenían un peso de 2 o 3. Cada característica fue asignada hasta 3 palabras raíz, por lo que casi todas las palabras se cuentan solo una vez. La lista de palabras para la característica Habitaciones se proporciona como referencia en el Apéndice A. El peso wi f se calcula como: wi f = |Lf ∩ Ti| f∈F |Lf ∩ Ti| (1) donde |Lf ∩ Ti| es el número de términos comunes entre Lf y Ti. El peso para la característica \"En general\" se estableció en min{ |T i | 5000 , 1} donde |Ti | es el número de caracteres en Ti. Comenzaré diciendo que soy más de la onda de Holiday Inn que de un hotel de lujo. Así que me frustro cuando pago el doble de la tarifa de la habitación y recibo la mitad de las comodidades que obtendría en un Hampton Inn o Holiday Inn. La ubicación era definitivamente el principal activo de este lugar. Estaba a solo unas cuadras de la parada de metro del Centro Hynes y era fácil caminar a algunos buenos restaurantes en la zona de Back Bay. Boylston no está lejos en absoluto. Así que no tuve problemas en renunciar a un coche de alquiler y tomar el metro desde el aeropuerto hasta el hotel y usar el metro para cualquier otro viaje. De lo contrario, te hacen pagar por todo. Y cuando ya has gastado $215 por noche en la habitación, eso resulta frustrante. La habitación en sí estaba bien, más o menos lo que esperaría. El personal también era promedio, ni malo ni excelente. Una vez más, creo que estás pagando por la ubicación y la capacidad de ir caminando a muchos lugares interesantes. Pero creo que la próxima vez me quedaré en Brookline, disfrutaré de más comodidades y usaré más el metro. Las calificaciones numéricas asociadas a esta reseña son rO = 3, rR = 3, rS = 3, rC = 4, rV = 2 para las características de General (O), Habitaciones (R), Servicio (S), Limpieza (C) y Valor (V) respectivamente. Las calificaciones de las características Comida (F), Ubicación (L) y Ruido (N) están ausentes (es decir, rF = rL = rN = 0). Los pesos wf se calculan a partir de las siguientes listas de términos comunes: LR ∩ T = {habitación}; wR = 0.066 LS ∩ T = {3 * Personal, comodidades}; wS = 0.267 LC ∩ T = ∅; wC = 0 LV ∩ T = {$, tarifa}; wV = 0.133 LF ∩ T = {restaurante}; wF = 0.067 LL ∩ T = {2 * centro, 2 * caminar, 2 * ubicación, área}; wL = 0.467 LN ∩ T = ∅; wN = 0 Las palabras raíz Personal y Centro se triplicaron y duplicaron respectivamente. El peso total de la revisión textual es wO = 0.197. Estos valores explican razonablemente bien los pesos de las diferentes características en la discusión del revisor. Un punto a tener en cuenta es que algunos términos en las listas Lf poseen una orientación semántica inherente. Por ejemplo, la palabra suciedad (perteneciente a la lista LC) se usaría con mayor frecuencia para afirmar la presencia, y no la ausencia de suciedad. Esto es inevitable, pero se tuvo cuidado de asegurar que se utilizaran palabras de ambos extremos del espectro. Por esta razón, algunas listas como LR contienen solo sustantivos de objetos que uno típicamente describiría en una habitación (ver Apéndice A). El objetivo de esta sección es analizar la influencia de los pesos wi f en las calificaciones numéricas ri f. Intuitivamente, los usuarios que pasaron mucho tiempo discutiendo una característica f (es decir, cuando wif es alto) tenían algo que decir sobre su experiencia con respecto a esta característica. Obviamente, la característica f es importante para el usuario i. Dado que las personas tienden a ser más conocedoras en los aspectos que consideran importantes, nuestra hipótesis es que las calificaciones ri f (correspondientes a los pesos altos wi f) constituyen un subconjunto de las calificaciones de expertos para la característica f. La Figura 2 traza la distribución de las tasas r i(h) C con respecto a los pesos w i(h) C para la limpieza de un hotel de Las Vegas, h. Aquí, las calificaciones altas están restringidas a las reseñas que hablan poco sobre la limpieza. Cuando se menciona la limpieza en la discusión, las calificaciones son bajas. Muchos hoteles muestran patrones de calificación similares para varias características. Las calificaciones correspondientes a pesos bajos abarcan todo el espectro del 1 al 5, mientras que las calificaciones correspondientes a pesos altos están más agrupadas (ya sea alrededor de calificaciones buenas o malas). Por lo tanto, formulamos la siguiente hipótesis: Hipótesis 1. Las calificaciones ri f correspondientes a las reseñas donde wi f es alta, son más similares entre sí que a la colección general de calificaciones. Para probar la hipótesis, tomamos todo el conjunto de reseñas y, característica por característica, calculamos la desviación estándar de las calificaciones con alto peso, y la desviación estándar de todo el conjunto de calificaciones. Los pesos altos se definieron como aquellos que pertenecen al 20% superior del rango de pesos para la característica correspondiente. Si la Hipótesis 1 fuera cierta, la desviación estándar de todas las calificaciones debería ser mayor que la desviación estándar de las calificaciones con pesos altos. 0 1 2 3 4 5 6 0 0.1 0.2 0.3 0.4 0.5 0.6 Calificación Peso Figura 2: La distribución de las calificaciones en función del peso de la característica de limpieza. Utilizamos una prueba T estándar para medir la significancia de los resultados. Ciudad por ciudad y característica por característica, la Tabla 2 presenta la desviación estándar promedio de todas las calificaciones, y la desviación estándar promedio de las calificaciones con pesos altos. De hecho, las calificaciones con pesos altos tienen una desviación estándar más baja, y los resultados son significativos en el umbral estándar de significancia de 0.05 (aunque para ciertas ciudades tomadas de forma independiente no parece haber una diferencia significativa, los resultados son significativos para todo el conjunto de datos). Por favor, tenga en cuenta que solo se consideraron las características O, R, S, C y V, ya que para las otras (F, L y N) no teníamos suficientes calificaciones. Tabla 2: Desviación estándar promedio para todas las calificaciones, y desviación estándar promedio para las calificaciones con pesos altos. En corchetes, los valores p correspondientes para una diferencia positiva entre los dos. La hipótesis 1 no solo proporciona una comprensión básica sobre el comportamiento de calificación de los usuarios en línea, sino que también sugiere algunas formas de calcular estimaciones de mejor calidad. Podemos, por ejemplo, construir una estimación de calidad característica por característica con una varianza mucho menor: para cada característica tomamos el subconjunto de reseñas que discuten ampliamente esa característica, y como estimación de calidad, obtenemos la calificación promedio de este subconjunto. Los experimentos iniciales sugieren que las calificaciones promedio de característica a característica calculadas de esta manera son diferentes de las calificaciones promedio calculadas en todo el conjunto de datos. Dado que, de hecho, los pesos altos son indicadores de opiniones expertas, las estimaciones obtenidas de esta manera son más precisas que las actuales. Sin embargo, la validación de esta suposición subyacente requiere más experimentos controlados. LA INFLUENCIA DE LAS CALIFICACIONES PASADAS Por lo general, se hacen dos suposiciones importantes sobre las reseñas enviadas a foros en línea. La primera es que las calificaciones reflejen fielmente la calidad observada por los usuarios; la segunda es que las reseñas sean independientes entre sí. Si bien la evidencia anecdótica [9, 22] cuestiona la primera suposición, en esta sección abordamos la segunda. Un examen de las reseñas en línea muestra que estas suelen formar parte de hilos de discusión, donde los usuarios se esfuerzan por contradecir o estar vehementemente de acuerdo con los comentarios de usuarios anteriores. Considera, por ejemplo, la siguiente reseña: No entiendo las críticas negativas... el hotel estaba un poco oscuro, pero ese era el estilo. Fue muy artístico. Sí, estaba cerca de la autopista, pero en mi opinión, el sonido de un coche ruidoso ocasional es mejor que escuchar el ding ding de las máquinas tragamonedas toda la noche. El personal disponible es FABULOSO. Las camareras son geniales (y *** no merece la mala crítica que recibió, ¡fue 100% atenta con nosotros!), los camareros son amigables y profesionales al mismo tiempo... Aquí, el usuario se vio perturbado por informes negativos anteriores, abordó estas preocupaciones y se dispuso a intentar corregirlas. Como era de esperar, sus calificaciones fueron considerablemente más altas que las calificaciones promedio hasta este punto. Parece que los usuarios de TripAdvisor suelen leer regularmente los informes enviados por usuarios anteriores antes de reservar un hotel, o antes de escribir una reseña. Las reseñas anteriores crean ciertas expectativas previas sobre la calidad del servicio, y estas expectativas influyen en la reseña presentada. Creemos que esta observación es válida para la mayoría de los foros en línea. La percepción subjetiva de la calidad es directamente proporcional a qué tan bien la experiencia real cumple con la expectativa previa, un hecho confirmado por una importante línea de investigación econométrica y de marketing [17, 18, 16, 21]. La correlación entre las reseñas también ha sido confirmada por investigaciones recientes sobre la dinámica de los foros de <br>reseñas en línea</br> [6]. 4.1 Expectativas Previas Definimos la expectativa previa del usuario i con respecto a la característica f, como el promedio de las calificaciones previamente disponibles en la característica f4: ef (i) = j<i,r j f =0 rj f j<i,r j f =0 1 Como primera hipótesis, afirmamos que la calificación ri f es una función de la expectativa previa ef (i): Hipótesis 2. Para un hotel y una característica dados, dados los comentarios i y j tales que ef (i) es alto y ef (j) es bajo, la calificación rj f supera la calificación ri f. Definimos las expectativas altas y bajas como aquellas que están por encima, respectivamente por debajo de un cierto valor umbral θ. El conjunto de reseñas precedidas por altas, respectivamente bajas expectativas 3 parte de las reseñas de Amazon fueron reconocidas como publicaciones estratégicas por autores de libros o competidores 4 si no se asignaron calificaciones previas para la característica f, ef (i) se asigna un valor predeterminado de 4. Tabla 3: Calificaciones promedio para reseñas precedidas por expectativas bajas (primer valor en la celda) y altas (segundo valor en la celda). Los valores P para una diferencia positiva se dan entre corchetes. Las ciudades O R S C V 3.953 4.045 3.985 4.252 3.946 Boston 3.364 3.590 3.485 3.641 3.242 [0.011] [0.028] [0.0086] [0.0168] [0.0034] 4.284 4.358 4.064 4.530 4.428 Sídney 3.756 3.537 3.436 3.918 3.495 [0.000] [0.000] [0.035] [0.009] [0.000] 3.494 3.674 3.713 3.689 3.580 Las Vegas 3.140 3.530 2.952 3.530 3.351 [0.190] [0.529] [0.007] [0.529] [0.253] se definen de la siguiente manera: Ralto f = {ri f |ef (i) > θ} Rbajo f = {ri f |ef (i) < θ} Estos conjuntos son específicos para cada par (hotel, característica), y en nuestros experimentos tomamos θ = 4. Este valor bastante alto está cerca del promedio de calificación de todas las características de todos los hoteles, y se justifica por el hecho de que nuestro conjunto de datos contiene principalmente hoteles de alta calidad. Para cada ciudad, tomamos todos los hoteles y calculamos las calificaciones promedio en los conjuntos Rhigh f y Rlow f (ver Tabla 3). El promedio de calificación entre las reseñas que siguen a expectativas previas bajas es significativamente mayor que el promedio de calificación después de altas expectativas. Como evidencia adicional, consideramos todos los hoteles para los cuales la función eV (i) (la expectativa para la característica Valor) tiene un valor alto (mayor que 4) para algunos i, y un valor bajo (menor que 4) para algunos otros i. Intuitivamente, estos son los hoteles para los cuales hay un grado mínimo de variación en la secuencia oportuna de reseñas: es decir, el promedio acumulativo de calificaciones fue en algún momento alto y luego se volvió bajo, o viceversa. Tales variaciones se observan en aproximadamente la mitad de todos los hoteles en cada ciudad. La Figura 3 traza la mediana (entre los hoteles considerados) de la calificación, rV, cuando ef (i) no es más que x pero mayor que x - 0.5. 2.5 3 3.5 4 4.5 5 2.5 3 3.5 4 4.5 5 Expectativa de la mediana de calificación Boston Sydney Vegas Figura 3: Las calificaciones tienden a disminuir a medida que aumenta la expectativa. 138 Hay dos formas de interpretar la función ef (i): • El valor esperado para la característica f obtenido por el usuario i antes de su experiencia con el servicio, adquirido al leer informes presentados por usuarios anteriores. En este caso, un valor excesivamente alto para ef (i) llevaría al usuario a enviar un informe negativo (o viceversa), derivado de la diferencia entre el valor real del servicio y la expectativa inflada de este valor adquirida antes de su experiencia. • El valor esperado de la característica f para todos los visitantes posteriores del sitio, si el usuario i no enviara un informe. En este caso, la motivación para un informe negativo después de un valor excesivamente alto de ef es diferente: el usuario i busca corregir la expectativa de los futuros visitantes del sitio. A diferencia de la interpretación anterior, esto no requiere que el usuario derive una expectativa a priori para el valor de f. Tenga en cuenta que ninguna de las interpretaciones implica que el promedio hasta el informe i esté inversamente relacionado con la calificación en el informe i. Podría existir una medida de influencia ejercida por informes anteriores que impulse al usuario detrás del informe i a enviar calificaciones que, en cierta medida, se ajusten a los informes anteriores: un valor bajo para ef (i) puede influir en el usuario i a enviar una calificación baja para la característica f porque, por ejemplo, teme que enviar una calificación alta lo haga parecer una persona con bajos estándares. Esto, al principio, parece contradecir la Hipótesis 2. Sin embargo, esta calificación de conformidad no puede continuar indefinidamente: una vez que el conjunto de informes proyecte una estimación suficientemente reducida para vf, los revisores futuros con impresiones comparativamente positivas buscarán corregir esta percepción errónea. 4.2 Impacto de los comentarios textuales en la expectativa de calidad. Se puede obtener una mayor comprensión del comportamiento de calificación de los usuarios de TripAdvisor analizando la relación entre los pesos wf y los valores ef (i). En particular, examinamos la siguiente hipótesis: Hipótesis 3. Cuando una gran proporción del texto de una reseña discute una característica en particular, la diferencia entre la calificación para esa característica y la calificación promedio hasta ese momento tiende a ser grande. La intuición detrás de esta afirmación es que cuando el usuario insiste en expresar su opinión sobre una característica en particular, su opinión difiere de la opinión colectiva de publicaciones anteriores. Esto se basa en la característica de los sistemas de reputación como foros de retroalimentación donde un usuario está interesado en proyectar su opinión, con particular fuerza si esta opinión difiere de lo que percibe como la opinión general. Para probar la Hipótesis 3 medimos la diferencia absoluta promedio entre la expectativa ef (i) y la calificación ri f cuando el peso wi f es alto, respectivamente bajo. Los pesos se clasifican como altos o bajos al compararlos con ciertos valores de corte: wi f es bajo si es menor que 0.1, mientras que wi f es alto si es mayor que θf. Se utilizaron diferentes valores de corte para diferentes características: θR = 0.4, θS = 0.4, θC = 0.2 y θV = 0.7. La limpieza tiene un límite inferior más bajo ya que es una característica raramente discutida; el valor tiene un límite superior alto por la razón opuesta. Los resultados se presentan en la Tabla 4. La idea de que los informes negativos pueden fomentar una mayor divulgación negativa ha sido sugerida anteriormente [14]. Tabla 4: Promedio de |ri f −ef (i)| cuando los pesos son altos (primer valor en la celda) y bajos (segundo valor en la celda) con valores P para la diferencia en corchetes cuadrados. Esto demuestra que cuando los pesos son inusualmente altos, los usuarios tienden a expresar una opinión que no se ajusta al promedio neto de calificaciones anteriores. Como podríamos esperar, para una característica que rara vez tuvo un peso importante en la discusión (por ejemplo, la limpieza), la diferencia es particularmente grande. Aunque la diferencia en el valor de la característica es bastante grande para Sídney, el valor P es alto. Esto se debe a que solo unos pocos comentarios discutieron en gran medida el valor. La razón podría ser cultural o porque había menos motivo para discutir esta característica. 4.3 Incentivos para informar Los modelos anteriores sugieren que los usuarios poco opinativos no elegirán expresar sus opiniones [12]. En esta sección, ampliamos este modelo para tener en cuenta la influencia de las expectativas. La motivación para enviar comentarios no se debe solo a opiniones extremas, sino también a la diferencia entre la reputación actual (es decir, la expectativa previa del usuario) y la experiencia real. Un modelo de calificación así produce calificaciones que la mayoría de las veces se desvían de la calificación promedio actual. Las calificaciones que confirman la expectativa previa rara vez serán enviadas. Probamos en nuestro conjunto de datos la proporción de calificaciones que intentan corregir la estimación actual. Definimos una calificación desviada como aquella que se desvía de la expectativa actual por al menos un umbral θ, es decir, |ri f − ef (i)| ≥ θ. Para cada una de las tres ciudades consideradas, las siguientes tablas muestran la proporción de calificaciones desviadas para θ = 0.5 y θ = 1. Los resultados anteriores sugieren que una gran proporción de usuarios (casi la mitad, incluso para el valor umbral alto θ = 1) se desvían del promedio previo. Esto refuerza la idea de que los usuarios son más propensos a enviar un informe cuando creen que tienen algo distintivo que agregar al flujo actual de opiniones sobre alguna característica. Tales conclusiones están en total acuerdo con evidencia previa de que la distribución de informes a menudo sigue distribuciones bimodales en forma de U. 139 5. Para dar cuenta de las observaciones descritas en las secciones anteriores, proponemos un modelo para el comportamiento de los usuarios al enviar reseñas en línea. Para un hotel dado, asumimos que la calidad experimentada por los usuarios sigue una distribución normal alrededor de algún valor vf, que representa la calidad objetiva ofrecida por el hotel en la característica f. La calificación enviada por el usuario i en la característica f es: ˆri f = δf vi f + (1 − δf) · sign vi f − ef (i) c + d(vi f, ef (i)|wi f) (2) donde: • vi f es la calidad (desconocida) experimentada realmente por el usuario. Se asume que vi f sigue una distribución normal alrededor de algún valor vf; • δf ∈ [0, 1] puede verse como una medida del sesgo al reportar retroalimentación. Los valores altos reflejan el hecho de que los usuarios califican de manera objetiva, sin ser influenciados por expectativas previas. El valor de δf puede depender de varios factores; fijamos un valor para cada característica f; • c es una constante entre 1 y 5; • wi f es el peso de la característica f en el comentario textual de la reseña i, calculado según la Ecuación (1); • d(vi f , ef (i)|wi f ) es una función de distancia entre la expectativa y la observación del usuario i. La función de distancia satisface las siguientes propiedades: - d(y, z|w) ≥ 0 para todo y, z ∈ [0, 5], w ∈ [0, 1]; - |d(y, z|w)| < |d(z, x|w)| si |y − z| < |z − x|; - |d(y, z|w1)| < |d(y, z|w2)| si w1 < w2; - c + d(vf , ef (i)|wi f ) ∈ [1, 5]; El segundo término de la Ecuación (2) codifica el sesgo de la calificación. Cuanto mayor sea la distancia entre la observación real vi f y la función ef, mayor será el sesgo. 5.1 Validación del modelo Utilizamos el conjunto de datos de las reseñas de TripAdvisor para validar el modelo de comportamiento presentado anteriormente. Dividimos por conveniencia los valores de calificación en tres rangos: malo (B = {1, 2}), indiferente (I = {3, 4}), y bueno (G = {5}), y realizamos los siguientes dos tests: • Primero, usaremos nuestro modelo para predecir las calificaciones que tienen valores extremos. Para cada hotel, tomamos la secuencia de informes, y cada vez que nos encontramos con una calificación que sea buena o mala (pero no indiferente) intentamos predecirla utilizando la Ecuación (2) • En segundo lugar, en lugar de predecir el valor de las calificaciones extremas, intentamos clasificarlas como buenas o malas. Para cada hotel tomamos la secuencia de informes, y para cada informe (independientemente de su valor) lo clasificamos como bueno o malo. Sin embargo, para realizar estas pruebas, necesitamos estimar el valor objetivo, vf, que es el promedio de las observaciones de calidad reales, vi f. El algoritmo que estamos utilizando se basa en la intuición de que la cantidad de calificación de conformidad se minimiza. En otras palabras, el valor vf debe ser tal que, tan a menudo como sea posible, las calificaciones malas sigan a expectativas por encima de vf y las calificaciones buenas sigan a expectativas por debajo de vf. Formalmente, definimos los conjuntos: Γ1 = {i|ef (i) < vf y ri f ∈ B}; Γ2 = {i|ef (i) > vf y ri f ∈ G}; que corresponden a irregularidades donde, a pesar de que la expectativa en el punto i es menor que el valor entregado, la calificación es baja, y viceversa. Definimos vf como el valor que minimiza la unión de los dos conjuntos: vf = arg min vf |Γ1 ∪ Γ2| (3) En la Ec. (2) reemplazamos vi f por el valor vf calculado en la Ec. (3), y utilizamos la siguiente función de distancia: d(vf , ef (i)|wi f ) = |vf − ef (i)| vf − ef (i) |vf 2 − ef (i)2 | · (1 + 2wi f ); La constante c ∈ I se estableció en min{max{ef (i), 3}}, 4}. Los valores para δf se fijaron en {0.7, 0.7, 0.8, 0.7, 0.6} para las características {General, Habitaciones, Servicio, Limpieza, Valor} respectivamente. Los pesos se calculan tal como se describe en la Sección 3. Como primer experimento, tomamos los conjuntos de calificaciones extremas {ri f |ri f /∈ I} para cada hotel y característica. Para cada calificación de este tipo, ri f, intentamos estimarla calculando ˆri f usando la Ecuación (2). Comparamos este estimador con el obtenido simplemente promediando las calificaciones de todos los hoteles y características: es decir, ¯rf = j,r j f =0 rj f j,r j f =0 1; la Tabla 7 presenta la relación entre el error cuadrático medio (RMSE) al usar ˆri f y ¯rf para estimar las calificaciones reales. En todos los casos, la estimación producida por nuestro modelo es mejor que el promedio simple. Tabla 7: Promedio de RMSE(ˆrf) RMSE(¯rf) Ciudad O R S C V Boston 0.987 0.849 0.879 0.776 0.913 Sídney 0.927 0.817 0.826 0.720 0.681 Las Vegas 0.952 0.870 0.881 0.947 0.904 Como segundo experimento, intentamos distinguir los conjuntos Bf = {i|ri f ∈ B} y Gf = {i|ri f ∈ G} de calificaciones malas y buenas, respectivamente, en la característica f. Por ejemplo, calculamos el conjunto Bf usando el siguiente clasificador (llamado σ): ri f ∈ Bf (σf (i) = 1) ⇔ ˆri f ≤ 4; Las Tablas 8, 9 y 10 presentan la Precisión (p), Recall (r) y s = 2pr p+r para el clasificador σ, y lo comparan con un clasificador de mayoría ingenuo, τ, τf (i) = 1 ⇔ |Bf | ≥ |Gf |: Vemos que el recall es siempre mayor para σ y la precisión suele ser ligeramente peor. Para la métrica s, σ tiende a agregar un 140. Tabla 8: Precisión (p), Recall (r), s= 2pr p+r mientras se detectan calificaciones bajas para Boston O R S C V p 0.678 0.670 0.573 0.545 0.610 σ r 0.626 0.659 0.619 0.612 0.694 s 0.651 0.665 0.595 0.577 0.609 p 0.684 0.706 0.647 0.611 0.633 τ r 0.597 0.541 0.410 0.383 0.562 s 0.638 0.613 0.502 0.471 0.595 Tabla 9: Precisión (p), Recall (r), s= 2pr p+r mientras se detectan calificaciones bajas para Las Vegas O R S C V p 0.654 0.748 0.592 0.712 0.583 σ r 0.608 0.536 0.791 0.474 0.610 s 0.630 0.624 0.677 0.569 0.596 p 0.685 0.761 0.621 0.748 0.606 τ r 0.542 0.505 0.767 0.445 0.441 s 0.605 0.607 0.670 0.558 0.511 Mejora del 1-20% sobre τ, mucho mayor en algunos casos para hoteles en Sídney. Esto se debe probablemente a que las reseñas de Sídney son más positivas que las de las ciudades estadounidenses y los casos en los que el número de reseñas negativas supera al de las positivas son raros. Reemplazar el algoritmo de prueba con uno que juega un 1 con una probabilidad igual a la proporción de malas críticas mejora sus resultados para esta ciudad, pero aún es superado por alrededor del 80%. 6. RESUMEN DE RESULTADOS Y CONCLUSIÓN El objetivo de este artículo es explorar los factores que llevan a un usuario a enviar una calificación particular, en lugar de los incentivos que lo animaron a enviar un informe en primer lugar. Para ello utilizamos dos fuentes adicionales de información además del vector de calificaciones numéricas: primero, examinamos los comentarios textuales que acompañan las reseñas, y segundo, consideramos los informes que han sido previamente enviados por otros usuarios. Usando algoritmos simples de procesamiento de lenguaje natural, pudimos establecer una correlación entre el peso de una característica específica en el comentario textual que acompaña la reseña, y el ruido presente en la calificación numérica. Específicamente, parece que los usuarios que discuten ampliamente una característica en particular tienden a estar de acuerdo en una calificación común. Esta observación permite la construcción de estimadores de calidad característica por característica que tienen una varianza más baja y, con suerte, son menos ruidosos. Sin embargo, se requiere más evidencia para respaldar la intuición de que las calificaciones correspondientes a pesos altos son opiniones de expertos que merecen ser consideradas de mayor prioridad al calcular estimaciones de calidad. En segundo lugar, enfatizamos la dependencia de las calificaciones en informes previos. Los informes anteriores crean una expectativa de calidad que afecta la percepción subjetiva del usuario. Validamos dos hechos sobre las reseñas de hoteles que recopilamos de TripAdvisor: Primero, las calificaciones que siguen expectativas bajas (donde la expectativa se calcula como el promedio de los informes anteriores) probablemente sean más altas que las calificaciones que siguen expectativas altas. Intuitivamente, la percepción de la calidad (y consecuentemente la calificación) depende de qué tan bien la experiencia real del usuario cumple con sus expectativas. Segundo, incluimos evidencia de los comentarios textuales, y encontramos que cuando los usuarios dedican una gran parte del texto a discutir una característica específica, es probable que motiven una calificación divergente (es decir, una calificación que no se ajusta a la expectativa previa). Intuitivamente, esto respalda la hipótesis de que los foros de reseñas actúan como grupos de discusión donde los usuarios están interesados en presentar y motivar su propia opinión. Hemos capturado la evidencia empírica en un modelo de comportamiento que predice las calificaciones enviadas por los usuarios. La calificación final depende, como era de esperar, de la observación real y de la brecha entre la observación y la expectativa. La brecha tiende a tener una influencia mayor cuando una fracción importante del comentario textual está dedicada a discutir una característica específica. El modelo propuesto fue validado con los datos empíricos y proporciona mejores estimaciones de las calificaciones realmente enviadas. Una suposición que hacemos es sobre la existencia de un valor de calidad objetivo vf para la característica f. Esto rara vez es cierto, especialmente a lo largo de largos periodos de tiempo. Otras explicaciones podrían dar cuenta de la correlación de las calificaciones con informes anteriores. Por ejemplo, si ef (i) refleja el valor real de f en un punto en el tiempo, la diferencia en las calificaciones siguiendo expectativas altas y bajas puede ser explicada por modelos de ingresos hoteleros que se maximizan cuando el valor es modificado en consecuencia. Sin embargo, la idea de que la variación en las calificaciones no es principalmente una función de la variación en el valor resulta ser útil. Nuestro enfoque para aproximar este valor objetivo es de ninguna manera perfecto, pero se ajusta perfectamente a la idea detrás del modelo. Una dirección natural para trabajos futuros es examinar aplicaciones concretas de nuestros resultados. Es probable que se obtengan mejoras significativas en las estimaciones de calidad al incorporar toda la evidencia empírica sobre el comportamiento de calificación. No está claro exactamente cómo diferentes factores afectan las decisiones de los usuarios. La respuesta podría depender de la aplicación particular, el contexto y la cultura. 7. REFERENCIAS [1] A. Admati y P. Pfleiderer. Noisytalk.com: Transmitiendo opiniones en un entorno ruidoso. Documento de trabajo 1670R, Universidad de Stanford, 2000. [2] P. B., L. Lee y S. Vaithyanathan. ¿Clasificación de sentimientos con técnicas de aprendizaje automático? En Actas de EMNLP-02, la Conferencia sobre Métodos Empíricos en el Procesamiento del Lenguaje Natural, 2002. [3] H. Cui, V. Mittal y M. Datar. Experimentos comparativos 141 sobre clasificación de sentimientos para reseñas de productos en línea. En Actas de AAAI, 2006. [4] K. Dave, S. Lawrence y D. Pennock. Extracción de opiniones y clasificación semántica de reseñas de productos en la galería de cacahuetes. En Actas de la 12ª Conferencia Internacional sobre la World Wide Web (WWW03), 2003. [5] C. Dellarocas, N. Awad y X. Zhang. Explorando el valor de las calificaciones de productos en línea en la previsión de ingresos: El caso de las películas en movimiento. Documento de trabajo, 2006. [6] C. Forman, A. Ghose y B. Wiesenfeld. Un Examen Multinivel del Impacto de las Identidades Sociales en las Transacciones Económicas en los Mercados Electrónicos. Disponible en SSRN: http://ssrn.com/abstract=918978, julio de 2006. [7] A. Ghose, P. Ipeirotis y A. Sundararajan. Primas de reputación en mercados electrónicos peer-to-peer: análisis de retroalimentación textual y estructura de red. En el Tercer Taller sobre Economía de Sistemas Peer-to-Peer (P2PECON), 2005. [8] A. Ghose, P. Ipeirotis y A. Sundararajan. Las dimensiones de la reputación en los mercados electrónicos. Documento de trabajo CeDER-06-02, Universidad de Nueva York, 2006. [9] A. Harmon. Error de Amazon revela la guerra de los revisores. The New York Times, 14 de febrero de 2004. [10] D. Houser y J. Wooders. Reputación en subastas: teoría y evidencia de eBay. Revista de Estrategia Económica y de Gestión, 15:353-369, 2006. [11] M. Hu y B. Liu. Extracción y resumen de reseñas de clientes. En Actas de la Conferencia Internacional de la ACM SIGKDD sobre Descubrimiento de Conocimiento y Minería de Datos (KDD04), 2004. [12] N. Hu, P. Pavlou y J. Zhang. ¿Pueden las reseñas en línea revelar la verdadera calidad de un producto? En Actas de la Conferencia ACM sobre Comercio Electrónico (EC 06), 2006. [13] K. Kalyanam y S. McIntyre. Retorno de la reputación en el mercado de subastas en línea. Documento de trabajo 02/03-10-WP, Escuela de Negocios Leavey, Universidad de Santa Clara, 2001. [14] L. Khopkar y P. Resnick. Auto-selección, deslizamiento, salvamento, holgazanería y lapidación: los impactos de la retroalimentación negativa en eBay. En Actas de la Conferencia ACM sobre Comercio Electrónico (EC 05), 2005. [15] M. Melnik y J. Alm. ¿Importa la reputación de un vendedor? evidencia de subastas en eBay. Revista de Economía Industrial, 50(3):337-350, 2002. [16] R. Olshavsky y J. Miller. Expectativas del consumidor, rendimiento del producto y calidad percibida del producto. Revista de Investigación de Marketing, 9:19-21, febrero de 1972. [17] A. Parasuraman, V. Zeithaml y L. Berry. Un Modelo Conceptual de Calidad de Servicio y sus Implicaciones para Investigaciones Futuras. Revista de Marketing, 49:41-50, 1985. [18] A. Parasuraman, V. Zeithaml y L. Berry. SERVQUAL: Una escala de múltiples ítems para medir las percepciones de calidad del servicio por parte del consumidor. Revista de Retail, 64:12-40, 1988. [19] P. Pavlou y A. Dimoka. La naturaleza y función de los comentarios de texto de retroalimentación en los mercados en línea: implicaciones para la construcción de confianza, primas de precio y diferenciación de vendedores. Investigación de Sistemas de Información, 17(4):392-414, 2006. [20] A. Popescu y O. Etzioni. Extrayendo características del producto y opiniones de las reseñas. En Actas de la Conferencia de Tecnología del Lenguaje Humano y Conferencia sobre Métodos Empíricos en Procesamiento del Lenguaje Natural, 2005. [21] R. Teas. Expectativas, Evaluación del Rendimiento y Percepciones de Calidad de los Consumidores. Revista de Marketing, 57:18-34, 1993. [22] E. White. Chateando a un cantante en las listas de éxitos pop. El Wall Street Journal, 15 de octubre de 1999. APÉNDICE A. LISTA DE PALABRAS, LR, ASOCIADAS A LA CARACTERÍSTICA HABITACIONES. Todas las palabras sirven como prefijos: habitación, espacio, interior, decoración, ambiente, atmósfera, comodidad, baño, inodoro, cama, edificio, pared, ventana, privado, temperatura, sábana, lino, almohada, caliente, agua, fría, agua, ducha, vestíbulo, muebles, alfombra, aire, acondicionado, colchón, distribución, diseño, espejo, techo, iluminación, lámpara, sofá, silla, cómoda, armario, armario. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "reputation mechanism": {
            "translated_key": "mecanismos de reputación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Understanding User Behavior in Online Feedback Reporting Arjun Talwar Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland arjun@math.stanford.edu Radu Jurca Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland radu.jurca@epfl.ch Boi Faltings Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland boi.faltings@epfl.ch ABSTRACT Online reviews have become increasingly popular as a way to judge the quality of various products and services.",
                "Previous work has demonstrated that contradictory reporting and underlying user biases make judging the true worth of a service difficult.",
                "In this paper, we investigate underlying factors that influence user behavior when reporting feedback.",
                "We look at two sources of information besides numerical ratings: linguistic evidence from the textual comment accompanying a review, and patterns in the time sequence of reports.",
                "We first show that groups of users who amply discuss a certain feature are more likely to agree on a common rating for that feature.",
                "Second, we show that a users rating partly reflects the difference between true quality and prior expectation of quality as inferred from previous reviews.",
                "Both give us a less noisy way to produce rating estimates and reveal the reasons behind user bias.",
                "Our hypotheses were validated by statistical evidence from hotel reviews on the TripAdvisor website.",
                "Categories and Subject Descriptors J.4 [Social and Behavioral Sciences]: Economics General Terms Economics, Experimentation, Reliability 1.",
                "MOTIVATIONS The spread of the internet has made it possible for online feedback forums (or <br>reputation mechanism</br>s) to become an important channel for Word-of-mouth regarding products, services or other types of commercial interactions.",
                "Numerous empirical studies [10, 15, 13, 5] show that buyers seriously consider online feedback when making purchasing decisions, and are willing to pay reputation premiums for products or services that have a good reputation.",
                "Recent analysis, however, raises important questions regarding the ability of existing forums to reflect the real quality of a product.",
                "In the absence of clear incentives, users with a moderate outlook will not bother to voice their opinions, which leads to an unrepresentative sample of reviews.",
                "For example, [12, 1] show that Amazon1 ratings of books or CDs follow with great probability bi-modal, U-shaped distributions where most of the ratings are either very good, or very bad.",
                "Controlled experiments, on the other hand, reveal opinions on the same items that are normally distributed.",
                "Under these circumstances, using the arithmetic mean to predict quality (as most forums actually do) gives the typical user an estimator with high variance that is often false.",
                "Improving the way we aggregate the information available from online reviews requires a deep understanding of the underlying factors that bias the rating behavior of users.",
                "Hu et al. [12] propose the Brag-and-Moan Model where users rate only if their utility of the product (drawn from a normal distribution) falls outside a median interval.",
                "The authors conclude that the model explains the empirical distribution of reports, and offers insights into smarter ways of estimating the true quality of the product.",
                "In the present paper we extend this line of research, and attempt to explain further facts about the behavior of users when reporting online feedback.",
                "Using actual hotel reviews from the TripAdvisor2 website, we consider two additional sources of information besides the basic numerical ratings submitted by users.",
                "The first is simple linguistic evidence from the textual review that usually accompanies the numerical ratings.",
                "We use text-mining techniques similar to [7] and [3], however, we are only interested in identifying what aspects of the service the user is discussing, without computing the semantic orientation of the text.",
                "We find that users who comment more on the same feature are more likely to agree on a common numerical rating for that particular feature.",
                "Intuitively, lengthy comments reveal the importance of the feature to the user.",
                "Since people tend to be more knowledgeable in the aspects they consider important, users who discuss a given feature in more details might be assumed to have more authority in evaluating that feature.",
                "Second we investigate the relationship between a review 1 http://www.amazon.com 2 http://www.tripadvisor.com/ 134 Figure 1: The TripAdvisor page displaying reviews for a popular Boston hotel.",
                "Name of hotel and advertisements were deliberatively erased. and the reviews that preceded it.",
                "A perusal of online reviews shows that ratings are often part of discussion threads, where one post is not necessarily independent of other posts.",
                "One may see, for example, users who make an effort to contradict, or vehemently agree with, the remarks of previous users.",
                "By analyzing the time sequence of reports, we conclude that past reviews influence the future reports, as they create some prior expectation regarding the quality of service.",
                "The subjective perception of the user is influenced by the gap between the prior expectation and the actual performance of the service [17, 18, 16, 21] which will later reflect in the users rating.",
                "We propose a model that captures the dependence of ratings on prior expectations, and validate it using the empirical data we collected.",
                "Both results can be used to improve the way <br>reputation mechanism</br>s aggregate the information from individual reviews.",
                "Our first result can be used to determine a featureby-feature estimate of quality, where for each feature, a different subset of reviews (i.e., those with lengthy comments of that feature) is considered.",
                "The second leads to an algorithm that outputs a more precise estimate of the real quality. 2.",
                "THE DATA SET We use in this paper real hotel reviews collected from the popular travel site TripAdvisor.",
                "TripAdvisor indexes hotels from cities across the world, along with reviews written by travelers.",
                "Users can search the site by giving the hotels name and location (optional).",
                "The reviews for a given hotel are displayed as a list (ordered from the most recent to the oldest), with 5 reviews per page.",
                "The reviews contain: • information about the author of the review (e.g., dates of stay, username of the reviewer, location of the reviewer); • the overall rating (from 1, lowest, to 5, highest); • a textual review containing a title for the review, free comments, and the main things the reviewer liked and disliked; • numerical ratings (from 1, lowest, to 5, highest) for different features (e.g., cleanliness, service, location, etc.)",
                "Below the name of the hotel, TripAdvisor displays the address of the hotel, general information (number of rooms, number of stars, short description, etc), the average overall rating, the TripAdvisor ranking, and an average rating for each feature.",
                "Figure 1 shows the page for a popular Boston hotel whose name (along with advertisements) was explicitly erased.",
                "We selected three cities for this study: Boston, Sydney and Las Vegas.",
                "For each city we considered all hotels that had at least 10 reviews, and recorded all reviews.",
                "Table 1 presents the number of hotels considered in each city, the total number of reviews recorded for each city, and the distribution of hotels with respect to the star-rating (as available on the TripAdvisor site).",
                "Note that not all hotels have a star-rating.",
                "Table 1: A summary of the data set.",
                "City # Reviews # Hotels # of Hotels with 1,2,3,4 & 5 stars Boston 3993 58 1+3+17+15+2 Sydney 1371 47 0+0+9+13+10 Las Vegas 5593 40 0+3+10+9+6 For each review we recorded the overall rating, the textual review (title and body of the review) and the numerical rating on 7 features: Rooms(R), Service(S), Cleanliness(C), Value(V), Food(F), Location(L) and Noise(N).",
                "TripAdvisor does not require users to submit anything other than the overall rating, hence a typical review rates few additional features, regardless of the discussion in the textual comment.",
                "Only the features Rooms(R), Service(S), Cleanliness(C) and Value(V) are rated by a significant number of users.",
                "However, we also selected the features Food(F), Location(L) and Noise(N) because they are referred to in a significant number of textual comments.",
                "For each feature we record the numerical rating given by the user, or 0 when the rating is missing.",
                "The typical length of the textual comment amounts to approximately 200 words.",
                "All data was collected by crawling the TripAdvisor site in September 2006. 2.1 Formal notation We will formally refer to a review by a tuple (r, T) where: • r = (rf ) is a vector containing the ratings rf ∈ {0, 1, . . . 5} for the features f ∈ F = {O, R, S, C, V, F, L, N}; note that the overall rating, rO, is abusively recorded as the rating for the feature Overall(O); • T is the textual comment that accompanies the review. 135 Reviews are indexed according to the variable i, such that (ri , Ti ) is the ith review in our database.",
                "Since we dont record the username of the reviewer, we will also say that the ith review in our data set was submitted by user i.",
                "When we need to consider only the reviews of a given hotel, h, we will use (ri(h) , Ti(h) ) to denote the ith review about the hotel h. 3.",
                "EVIDENCE FROM TEXTUAL COMMENTS The free textual comments associated to online reviews are a valuable source of information for understanding the reasons behind the numerical ratings left by the reviewers.",
                "The text may, for example, reveal concrete examples of aspects that the user liked or disliked, thus justifying some of the high, respectively low ratings for certain features.",
                "The text may also offer guidelines for understanding the preferences of the reviewer, and the weights of different features when computing an overall rating.",
                "The problem, however, is that free textual comments are difficult to read.",
                "Users are required to scroll through many reviews and read mostly repetitive information.",
                "Significant improvements would be obtained if the reviews were automatically interpreted and aggregated.",
                "Unfortunately, this seems a difficult task for computers since human users often use witty language, abbreviations, cultural specific phrases, and the figurative style.",
                "Nevertheless, several important results use the textual comments of online reviews in an automated way.",
                "Using well established natural language techniques, reviews or parts of reviews can be classified as having a positive or negative semantic orientation.",
                "Pang et al. [2] classify movie reviews into positive/negative by training three different classifiers (Naive Bayes, Maximum Entropy and SVM) using classification features based on unigrams, bigrams or part-of-speech tags.",
                "Dave et al. [4] analyze reviews from CNet and Amazon, and surprisingly show that classification features based on unigrams or bigrams perform better than higher-order n-grams.",
                "This result is challenged by Cui et al. [3] who look at large collections of reviews crawled from the web.",
                "They show that the size of the data set is important, and that bigger training sets allow classifiers to successfully use more complex classification features based on n-grams.",
                "Hu and Liu [11] also crawl the web for product reviews and automatically identify product attributes that have been discussed by reviewers.",
                "They use Wordnet to compute the semantic orientation of product evaluations and summarize user reviews by extracting positive and negative evaluations of different product features.",
                "Popescu and Etzioni [20] analyze a similar setting, but use search engine hit-counts to identify product attributes; the semantic orientation is assigned through the relaxation labeling technique.",
                "Ghose et al. [7, 8] analyze seller reviews from the Amazon secondary market to identify the different dimensions (e.g., delivery, packaging, customer support, etc.) of reputation.",
                "They parse the text, and tag the part-of-speech for each word.",
                "Frequent nouns, noun phrases and verbal phrases are identified as dimensions of reputation, while the corresponding modifiers (i.e., adjectives and adverbs) are used to derive numerical scores for each dimension.",
                "The enhanced reputation measure correlates better with the pricing information observed in the market.",
                "Pavlou and Dimoka [19] analyze eBay reviews and find that textual comments have an important impact on reputation premiums.",
                "Our approach is similar to the previously mentioned works, in the sense that we identify the aspects (i.e., hotel features) discussed by the users in the textual reviews.",
                "However, we do not compute the semantic orientation of the text, nor attempt to infer missing ratings.",
                "We define the weight, wi f , of feature f ∈ F in the text Ti associated with the review (ri , Ti ), as the fraction of Ti dedicated to discussing aspects (both positive and negative) related to feature f. We propose an elementary method to approximate the values of these weights.",
                "For each feature we manually construct the word list Lf containing approximately 50 words that are most commonly associated to the feature f. The initial words were selected from reading some of the reviews, and seeing what words coincide with discussion of which features.",
                "The list was then extended by adding all thesaurus entries that were related to the initial words.",
                "Finally, we brainstormed for missing words that would normally be associated with each of the features.",
                "Let Lf ∩Ti be the list of terms common to both Lf and Ti.",
                "Each term of Lf is counted the number of times it appears in Ti , with two exceptions: • in cases where the user submits a title to the review, we account for the title text by appending it three times to the review text Ti .",
                "The intuitive assumption is that the users opinion is more strongly reflected in the title, rather than in the body of the review.",
                "For example, many reviews are accurately summarized by titles such as Excellent service, terrible location or Bad value for money; • certain words that occur only once in the text are counted multiple times if their relevance to that feature is particularly strong.",
                "These were root words for each feature (e.g., staff is a root word for the feature Service), and were weighted either 2 or 3.",
                "Each feature was assigned up to 3 such root words, so almost all words are counted only once.",
                "The list of words for the feature Rooms is given for reference in Appendix A.",
                "The weight wi f is computed as: wi f = |Lf ∩ Ti| f∈F |Lf ∩ Ti| (1) where |Lf ∩Ti | is the number of terms common to Lf and Ti .",
                "The weight for the feature Overall was set to min{ |T i | 5000 , 1} where |Ti | is the number of character in Ti .",
                "The following is a TripAdvisor review for a Boston hotel (the name of the hotel is omitted): Ill start by saying that Im more of a Holiday Inn person than a *** type.",
                "So I get frustrated when I pay double the room rate and get half the amenities that Id get at a Hampton Inn or Holiday Inn.",
                "The location was definitely the main asset of this place.",
                "It was only a few blocks from the Hynes Center subway stop and it was easy to walk to some good restaurants in the Back Bay area.",
                "Boylston isnt far off at all.",
                "So I had no trouble with foregoing a rental car and taking the subway from the airport to the hotel and using the subway for any other travel.",
                "Otherwise, they make you pay for anything and everything. 136 And when youve already dropped $215/night on the room, that gets frustrating.The room itself was decent, about what I would expect.",
                "Staff was also average, not bad and not excellent.",
                "Again, I think youre paying for location and the ability to walk to a lot of good stuff.",
                "But I think next time Ill stay in Brookline, get more amenities, and use the subway a bit more.",
                "This numerical ratings associated to this review are rO = 3, rR = 3, rS = 3, rC = 4, rV = 2 for features Overall(O), Rooms(R), Service(S), Cleanliness(C) and Value(V) respectively.",
                "The ratings for the features Food(F), Location(L) and Noise(N) are absent (i.e., rF = rL = rN = 0).",
                "The weights wf are computed from the following lists of common terms: LR ∩ T ={room}; wR = 0.066 LS ∩ T ={3 * Staff, amenities}; wS = 0.267 LC ∩ T = ∅; wC = 0 LV ∩ T ={$, rate}; wV = 0.133 LF ∩ T ={restaurant}; wF = 0.067 LL ∩ T ={2 * center, 2 * walk, 2 * location, area}; wL = 0.467 LN ∩ T = ∅; wN = 0 The root words Staff and Center were tripled and doubled respectively.",
                "The overall weight of the textual review is wO = 0.197.",
                "These values account reasonably well for the weights of different features in the discussion of the reviewer.",
                "One point to note is that some terms in the lists Lf possess an inherent semantic orientation.",
                "For example the word grime (belonging to the list LC ) would be used most often to assert the presence, and not the absence of grime.",
                "This is unavoidable, but care was taken to ensure words from both sides of the spectrum were used.",
                "For this reason, some lists such as LR contain only nouns of objects that one would typically describe in a room (see Appendix A).",
                "The goal of this section is to analyse the influence of the weights wi f on the numerical ratings ri f .",
                "Intuitively, users who spent a lot of their time discussing a feature f (i.e., wi f is high) had something to say about their experience with regard to this feature.",
                "Obviously, feature f is important for user i.",
                "Since people tend to be more knowledgeable in the aspects they consider important, our hypothesis is that the ratings ri f (corresponding to high weights wi f ) constitute a subset of expert ratings for feature f. Figure 2 plots the distribution of the rates r i(h) C with respect to the weights w i(h) C for the cleanliness of a Las Vegas hotel, h. Here, the high ratings are restricted to the reviews that discuss little the cleanliness.",
                "Whenever cleanliness appears in the discussion, the ratings are low.",
                "Many hotels exhibit similar rating patterns for various features.",
                "Ratings corresponding to low weights span the whole spectrum from 1 to 5, while the ratings corresponding to high weights are more grouped together (either around good or bad ratings).",
                "We therefore make the following hypothesis: Hypothesis 1.",
                "The ratings ri f corresponding to the reviews where wi f is high, are more similar to each other than to the overall collection of ratings.",
                "To test the hypothesis, we take the entire set of reviews, and feature by feature, we compute the standard deviation of the ratings with high weights, and the standard deviation of the entire set of ratings.",
                "High weights were defined as those belonging to the upper 20% of the weight range for the corresponding feature.",
                "If Hypothesis 1 were true, the standard deviation of all ratings should be higher than the standard deviation of the ratings with high weights. 0 1 2 3 4 5 6 0 0.1 0.2 0.3 0.4 0.5 0.6 Rating Weight Figure 2: The distribution of ratings against the weight of the cleanliness feature.",
                "We use a standard T-test to measure the significance of the results.",
                "City by city and feature by feature, Table 2 presents the average standard deviation of all ratings, and the average standard deviation of ratings with high weights.",
                "Indeed, the ratings with high weights have lower standard deviation, and the results are significant at the standard 0.05 significance threshold (although for certain cities taken independently there doesnt seem to be a significant difference, the results are significant for the entire data set).",
                "Please note that only the features O,R,S,C and V were considered, since for the others (F, L, and N) we didnt have enough ratings.",
                "Table 2: Average standard deviation for all ratings, and average standard deviation for ratings with high weights.",
                "In square brackets, the corresponding p-values for a positive difference between the two.",
                "City O R S C V all 1.189 0.998 1.144 0.935 1.123 Boston high 0.948 0.778 0.954 0.767 0.891 p-val [0.000] [0.004] [0.045] [0.080] [0.009] all 1.040 0.832 1.101 0.847 0.963 Sydney high 0.801 0.618 0.691 0.690 0.798 p-val [0.012] [0.023] [0.000] [0.377] [0.037] all 1.272 1.142 1.184 1.119 1.242 Vegas high 1.072 0.752 1.169 0.907 1.003 p-val [0.0185] [0.001] [0.918] [0.120] [0.126] Hypothesis 1 not only provides some basic understanding regarding the rating behavior of online users, it also suggests some ways of computing better quality estimates.",
                "We can, for example, construct a feature-by-feature quality estimate with much lower variance: for each feature we take the subset of reviews that amply discuss that feature, and output as a quality estimate the average rating for this subset.",
                "Initial experiments suggest that the average feature-by-feature ratings computed in this way are different from the average ratings computed on the whole data set.",
                "Given that, indeed, high weights are indicators of expert opinions, the estimates obtained in this way are more accurate than the current ones.",
                "Nevertheless, the validation of this underlying assumption requires further controlled experiments. 137 4.",
                "THE INFLUENCE OF PAST RATINGS Two important assumptions are generally made about reviews submitted to online forums.",
                "The first is that ratings truthfully reflect the quality observed by the users; the second is that reviews are independent from one another.",
                "While anecdotal evidence [9, 22] challenges the first assumption3 , in this section, we address the second.",
                "A perusal of online reviews shows that reviews are often part of discussion threads, where users make an effort to contradict, or vehemently agree with the remarks of previous users.",
                "Consider, for example, the following review: I dont understand the negative reviews... the hotel was a little dark, but that was the style.",
                "It was very artsy.",
                "Yes it was close to the freeway, but in my opinion the sound of an occasional loud car is better than hearing the ding ding of slot machines all night!",
                "The staff on-hand is FABULOUS.",
                "The waitresses are great (and *** does not deserve the bad review she got, she was 100% attentive to us! ), the bartenders are friendly and professional at the same time...",
                "Here, the user was disturbed by previous negative reports, addressed these concerns, and set about trying to correct them.",
                "Not surprisingly, his ratings were considerably higher than the average ratings up to this point.",
                "It seems that TripAdvisor users regularly read the reports submitted by previous users before booking a hotel, or before writing a review.",
                "Past reviews create some prior expectation regarding the quality of service, and this expectation has an influence on the submitted review.",
                "We believe this observation holds for most online forums.",
                "The subjective perception of quality is directly proportional to how well the actual experience meets the prior expectation, a fact confirmed by an important line of econometric and marketing research [17, 18, 16, 21].",
                "The correlation between the reviews has also been confirmed by recent research on the dynamics of online review forums [6]. 4.1 Prior Expectations We define the prior expectation of user i regarding the feature f, as the average of the previously available ratings on the feature f4 : ef (i) = j<i,r j f =0 rj f j<i,r j f =0 1 As a first hypothesis, we assert that the rating ri f is a function of the prior expectation ef (i): Hypothesis 2.",
                "For a given hotel and feature, given the reviews i and j such that ef (i) is high and ef (j) is low, the rating rj f exceeds the rating ri f .",
                "We define high and low expectations as those that are above, respectively below a certain cutoff value θ.",
                "The set of reviews preceded by high, respectively low expectations 3 part of Amazon reviews were recognized as strategic posts by book authors or competitors 4 if no previous ratings were assigned for feature f, ef (i) is assigned a default value of 4.",
                "Table 3: Average ratings for reviews preceded by low (first value in the cell) and high (second value in the cell) expectations.",
                "The P-values for a positive difference are given square brackets.",
                "City O R S C V 3.953 4.045 3.985 4.252 3.946 Boston 3.364 3.590 3.485 3.641 3.242 [0.011] [0.028] [0.0086] [0.0168] [0.0034] 4.284 4.358 4.064 4.530 4.428 Sydney 3.756 3.537 3.436 3.918 3.495 [0.000] [0.000] [0.035] [0.009] [0.000] 3.494 3.674 3.713 3.689 3.580 Las Vegas 3.140 3.530 2.952 3.530 3.351 [0.190] [0.529] [0.007] [0.529] [0.253] are defined as follows: Rhigh f = {ri f |ef (i) > θ} Rlow f = {ri f |ef (i) < θ} These sets are specific for each (hotel, feature) pair, and in our experiments we took θ = 4.",
                "This rather high value is close to the average rating across all features across all hotels, and is justified by the fact that our data set contains mostly high quality hotels.",
                "For each city, we take all hotels and compute the average ratings in the sets Rhigh f and Rlow f (see Table 3).",
                "The average rating amongst reviews following low prior expectations is significantly higher than the average rating following high expectations.",
                "As further evidence, we consider all hotels for which the function eV (i) (the expectation for the feature Value) has a high value (greater than 4) for some i, and a low value (less than 4) for some other i.",
                "Intuitively, these are the hotels for which there is a minimal degree of variation in the timely sequence of reviews: i.e., the cumulative average of ratings was at some point high and afterwards became low, or vice-versa.",
                "Such variations are observed for about half of all hotels in each city.",
                "Figure 3 plots the median (across considered hotels) rating, rV , when ef (i) is not more than x but greater than x − 0.5. 2.5 3 3.5 4 4.5 5 2.5 3 3.5 4 4.5 5 Medianofrating expectation Boston Sydney Vegas Figure 3: The ratings tend to decrease as the expectation increases. 138 There are two ways to interpret the function ef (i): • The expected value for feature f obtained by user i before his experience with the service, acquired by reading reports submitted by past users.",
                "In this case, an overly high value for ef (i) would drive the user to submit a negative report (or vice versa), stemming from the difference between the actual value of the service, and the inflated expectation of this value acquired before his experience. • The expected value of feature f for all subsequent visitors of the site, if user i were not to submit a report.",
                "In this case, the motivation for a negative report following an overly high value of ef is different: user i seeks to correct the expectation of future visitors to the site.",
                "Unlike the interpretation above, this does not require the user to derive an a priori expectation for the value of f. Note that neither interpretation implies that the average up to report i is inversely related to the rating at report i.",
                "There might exist a measure of influence exerted by past reports that pushes the user behind report i to submit ratings which to some extent conforms with past reports: a low value for ef (i) can influence user i to submit a low rating for feature f because, for example, he fears that submitting a high rating will make him out to be a person with low standards5 .",
                "This, at first, appears to contradict Hypothesis 2.",
                "However, this conformity rating cannot continue indefinitely: once the set of reports project a sufficiently deflated estimate for vf , future reviewers with comparatively positive impressions will seek to correct this misconception. 4.2 Impact of textual comments on quality expectation Further insight into the rating behavior of TripAdvisor users can be obtained by analyzing the relationship between the weights wf and the values ef (i).",
                "In particular, we examine the following hypothesis: Hypothesis 3.",
                "When a large proportion of the text of a review discusses a certain feature, the difference between the rating for that feature and the average rating up to that point tends to be large.",
                "The intuition behind this claim is that when the user is adamant about voicing his opinion regarding a certain feature, his opinion differs from the collective opinion of previous postings.",
                "This relies on the characteristic of reputation systems as feedback forums where a user is interested in projecting his opinion, with particular strength if this opinion differs from what he perceives to be the general opinion.",
                "To test Hypothesis 3 we measure the average absolute difference between the expectation ef (i) and the rating ri f when the weight wi f is high, respectively low.",
                "Weights are classified high or low by comparing them with certain cutoff values: wi f is low if smaller than 0.1, while wi f is high if greater than θf .",
                "Different cutoff values were used for different features: θR = 0.4, θS = 0.4, θC = 0.2, and θV = 0.7.",
                "Cleanliness has a lower cutoff since it is a feature rarely discussed; Value has a high cutoff for the opposite reason.",
                "Results are presented in Table 4. 5 The idea that negative reports can encourage further negative reporting has been suggested before [14] Table 4: Average of |ri f −ef (i)| when weights are high (first value in the cell) and low (second value in the cell) with P-values for the difference in sq. brackets.",
                "City R S C V 1.058 1.208 1.728 1.356 Boston 0.701 0.838 0.760 0.917 [0.022] [0.063] [0.000] [0.218] 1.048 1.351 1.218 1.318 Sydney 0.752 0.759 0.767 0.908 [0.179] [0.009] [0.165] [0.495] 1.184 1.378 1.472 1.642 Las Vegas 0.772 0.834 0.808 1.043 [0.071] [0.020] [0.006] [0.076] This demonstrates that when weights are unusually high, users tend to express an opinion that does not conform to the net average of previous ratings.",
                "As we might expect, for a feature that rarely was a high weight in the discussion, (e.g., cleanliness) the difference is particularly large.",
                "Even though the difference in the feature Value is quite large for Sydney, the P-value is high.",
                "This is because only few reviews discussed value heavily.",
                "The reason could be cultural or because there was less of a reason to discuss this feature. 4.3 Reporting Incentives Previous models suggest that users who are not highly opinionated will not choose to voice their opinions [12].",
                "In this section, we extend this model to account for the influence of expectations.",
                "The motivation for submitting feedback is not only due to extreme opinions, but also to the difference between the current reputation (i.e., the prior expectation of the user) and the actual experience.",
                "Such a rating model produces ratings that most of the time deviate from the current average rating.",
                "The ratings that confirm the prior expectation will rarely be submitted.",
                "We test on our data set the proportion of ratings that attempt to correct the current estimate.",
                "We define a deviant rating as one that deviates from the current expectation by at least some threshold θ, i.e., |ri f − ef (i)| ≥ θ.",
                "For each of the three considered cities, the following tables, show the proportion of deviant ratings for θ = 0.5 and θ = 1.",
                "Table 5: Proportion of deviant ratings with θ = 0.5 City O R S C V Boston 0.696 0.619 0.676 0.604 0.684 Sydney 0.645 0.615 0.672 0.614 0.675 Las Vegas 0.721 0.641 0.694 0.662 0.724 Table 6: Proportion of deviant ratings with θ = 1 City O R S C V Boston 0.420 0.397 0.429 0.317 0.446 Sydney 0.360 0.367 0.442 0.336 0.489 Las Vegas 0.510 0.421 0.483 0.390 0.472 The above results suggest that a large proportion of users (close to one half, even for the high threshold value θ = 1) deviate from the prior average.",
                "This reinforces the idea that users are more likely to submit a report when they believe they have something distinctive to add to the current stream of opinions for some feature.",
                "Such conclusions are in total agreement with prior evidence that the distribution of reports often follows bi-modal, U-shaped distributions. 139 5.",
                "MODELLING THE BEHAVIOR OF RATERS To account for the observations described in the previous sections, we propose a model for the behavior of the users when submitting online reviews.",
                "For a given hotel, we make the assumption that the quality experienced by the users is normally distributed around some value vf , which represents the objective quality offered by the hotel on the feature f. The rating submitted by user i on feature f is: ˆri f = δf vi f + (1 − δf ) · sign vi f − ef (i) c + d(vi f , ef (i)|wi f ) (2) where: • vi f is the (unknown) quality actually experienced by the user. vi f is assumed normally distributed around some value vf ; • δf ∈ [0, 1] can be seen as a measure of the bias when reporting feedback.",
                "High values reflect the fact that users rate objectively, without being influenced by prior expectations.",
                "The value of δf may depend on various factors; we fix one value for each feature f; • c is a constant between 1 and 5; • wi f is the weight of feature f in the textual comment of review i, computed according to Eq. (1); • d(vi f , ef (i)|wi f ) is a distance function between the expectation and the observation of user i.",
                "The distance function satisfies the following properties: - d(y, z|w) ≥ 0 for all y, z ∈ [0, 5], w ∈ [0, 1]; - |d(y, z|w)| < |d(z, x|w)| if |y − z| < |z − x|; - |d(y, z|w1)| < |d(y, z|w2)| if w1 < w2; - c + d(vf , ef (i)|wi f ) ∈ [1, 5]; The second term of Eq. (2) encodes the bias of the rating.",
                "The higher the distance between the true observation vi f and the function ef , the higher the bias. 5.1 Model Validation We use the data set of TripAdvisor reviews to validate the behavior model presented above.",
                "We split for convenience the rating values in three ranges: bad (B = {1, 2}), indifferent (I = {3, 4}), and good (G = {5}), and perform the following two tests: • First, we will use our model to predict the ratings that have extremal values.",
                "For every hotel, we take the sequence of reports, and whenever we encounter a rating that is either good or bad (but not indifferent) we try to predict it using Eq. (2) • Second, instead of predicting the value of extremal ratings, we try to classify them as either good or bad.",
                "For every hotel we take the sequence of reports, and for each report (regardless of it value) we classify it as being good or bad However, to perform these tests, we need to estimate the objective value, vf , that is the average of the true quality observations, vi f .",
                "The algorithm we are using is based on the intuition that the amount of conformity rating is minimized.",
                "In other words, the value vf should be such that as often as possible, bad ratings follow expectations above vf and good ratings follow expectations below vf .",
                "Formally, we define the sets: Γ1 = {i|ef (i) < vf and ri f ∈ B}; Γ2 = {i|ef (i) > vf and ri f ∈ G}; that correspond to irregularities where even though the expectation at point i is lower than the delivered value, the rating is poor, and vice versa.",
                "We define vf as the value that minimize these union of the two sets: vf = arg min vf |Γ1 ∪ Γ2| (3) In Eq. (2) we replace vi f by the value vf computed in Eq. (3), and use the following distance function: d(vf , ef (i)|wi f ) = |vf − ef (i)| vf − ef (i) |vf 2 − ef (i)2 | · (1 + 2wi f ); The constant c ∈ I was set to min{max{ef (i), 3}}, 4}.",
                "The values for δf were fixed at {0.7, 0.7, 0.8, 0.7, 0.6} for the features {Overall, Rooms, Service, Cleanliness, Value} respectively.",
                "The weights are computed as described in Section 3.",
                "As a first experiment, we take the sets of extremal ratings {ri f |ri f /∈ I} for each hotel and feature.",
                "For every such rating, ri f , we try to estimate it by computing ˆri f using Eq. (2).",
                "We compare this estimator with the one obtained by simply averaging the ratings over all hotels and features: i.e., ¯rf = j,r j f =0 rj f j,r j f =0 1 ; Table 7 presents the ratio between the root mean square error (RMSE) when using ˆri f and ¯rf to estimate the actual ratings.",
                "In all cases the estimate produced by our model is better than the simple average.",
                "Table 7: Average of RMSE(ˆrf ) RMSE(¯rf ) City O R S C V Boston 0.987 0.849 0.879 0.776 0.913 Sydney 0.927 0.817 0.826 0.720 0.681 Las Vegas 0.952 0.870 0.881 0.947 0.904 As a second experiment, we try to distinguish the sets Bf = {i|ri f ∈ B} and Gf = {i|ri f ∈ G} of bad, respectively good ratings on the feature f. For example, we compute the set Bf using the following classifier (called σ): ri f ∈ Bf (σf (i) = 1) ⇔ ˆri f ≤ 4; Tables 8, 9 and 10 present the Precision(p), Recall(r) and s = 2pr p+r for classifier σ, and compares it with a naive majority classifier, τ, τf (i) = 1 ⇔ |Bf | ≥ |Gf |: We see that recall is always higher for σ and precision is usually slightly worse.",
                "For the s metric σ tends to add a 140 Table 8: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Boston O R S C V p 0.678 0.670 0.573 0.545 0.610 σ r 0.626 0.659 0.619 0.612 0.694 s 0.651 0.665 0.595 0.577 0.609 p 0.684 0.706 0.647 0.611 0.633 τ r 0.597 0.541 0.410 0.383 0.562 s 0.638 0.613 0.502 0.471 0.595 Table 9: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Las Vegas O R S C V p 0.654 0.748 0.592 0.712 0.583 σ r 0.608 0.536 0.791 0.474 0.610 s 0.630 0.624 0.677 0.569 0.596 p 0.685 0.761 0.621 0.748 0.606 τ r 0.542 0.505 0.767 0.445 0.441 s 0.605 0.607 0.670 0.558 0.511 1-20% improvement over τ, much higher in some cases for hotels in Sydney.",
                "This is likely because Sydney reviews are more positive than those of the American cities and cases where the number of bad reviews exceeded the number of good ones are rare.",
                "Replacing the test algorithm with one that plays a 1 with probability equal to the proportion of bad reviews improves its results for this city, but it is still outperformed by around 80%. 6.",
                "SUMMARY OF RESULTS AND CONCLUSION The goal of this paper is to explore the factors that drive a user to submit a particular rating, rather than the incentives that encouraged him to submit a report in the first place.",
                "For that we use two additional sources of information besides the vector of numerical ratings: first we look at the textual comments that accompany the reviews, and second we consider the reports that have been previously submitted by other users.",
                "Using simple natural language processing algorithms, we were able to establish a correlation between the weight of a certain feature in the textual comment accompanying the review, and the noise present in the numerical rating.",
                "Specifically, it seems that users who discuss amply a certain feature are likely to agree on a common rating.",
                "This observation allows the construction of feature-by-feature estimators of quality that have a lower variance, and are hopefully less noisy.",
                "Nevertheless, further evidence is required to support the intuition that ratings corresponding to high weights are expert opinions that deserve to be given higher priority when computing estimates of quality.",
                "Second, we emphasize the dependence of ratings on previous reports.",
                "Previous reports create an expectation of quality which affects the subjective perception of the user.",
                "We validate two facts about the hotel reviews we collected from TripAdvisor: First, the ratings following low expectations (where the expectation is computed as the average of the previous reports) are likely to be higher than the ratings Table 10: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Sydney O R S C V p 0.650 0.463 0.544 0.550 0.580 σ r 0.234 0.378 0.571 0.169 0.592 s 0.343 0.452 0.557 0.259 0.586 p 0.562 0.615 0.600 0.500 0.600 τ r 0.054 0.098 0.101 0.015 0.175 s 0.098 0.168 0.172 0.030 0.271 following high expectations.",
                "Intuitively, the perception of quality (and consequently the rating) depends on how well the actual experience of the user meets her expectation.",
                "Second, we include evidence from the textual comments, and find that when users devote a large fraction of the text to discussing a certain feature, they are likely to motivate a divergent rating (i.e., a rating that does not conform to the prior expectation).",
                "Intuitively, this supports the hypothesis that review forums act as discussion groups where users are keen on presenting and motivating their own opinion.",
                "We have captured the empirical evidence in a behavior model that predicts the ratings submitted by the users.",
                "The final rating depends, as expected, on the true observation, and on the gap between the observation and the expectation.",
                "The gap tends to have a bigger influence when an important fraction of the textual comment is dedicated to discussing a certain feature.",
                "The proposed model was validated on the empirical data and provides better estimates of the ratings actually submitted.",
                "One assumption that we make is about the existence of an objective quality value vf for the feature f. This is rarely true, especially over large spans of time.",
                "Other explanations might account for the correlation of ratings with past reports.",
                "For example, if ef (i) reflects the true value of f at a point in time, the difference in the ratings following high and low expectations can be explained by hotel revenue models that are maximized when the value is modified accordingly.",
                "However, the idea that variation in ratings is not primarily a function of variation in value turns out to be a useful one.",
                "Our approach to approximate this elusive objective value is by no means perfect, but conforms neatly to the idea behind the model.",
                "A natural direction for future work is to examine concrete applications of our results.",
                "Significant improvements of quality estimates are likely to be obtained by incorporating all empirical evidence about rating behavior.",
                "Exactly how different factors affect the decisions of the users is not clear.",
                "The answer might depend on the particular application, context and culture. 7.",
                "REFERENCES [1] A. Admati and P. Pfleiderer.",
                "Noisytalk.com: Broadcasting opinions in a noisy environment.",
                "Working Paper 1670R, Stanford University, 2000. [2] P. B., L. Lee, and S. Vaithyanathan.",
                "Thumbs up? sentiment classification using machine learning techniques.",
                "In Proceedings of the EMNLP-02, the Conference on Empirical Methods in Natural Language Processing, 2002. [3] H. Cui, V. Mittal, and M. Datar.",
                "Comparative 141 Experiments on Sentiment Classification for Online Product Reviews.",
                "In Proceedings of AAAI, 2006. [4] K. Dave, S. Lawrence, and D. Pennock.",
                "Mining the peanut gallery:opinion extraction and semantic classification of product reviews.",
                "In Proceedings of the 12th International Conference on the World Wide Web (WWW03), 2003. [5] C. Dellarocas, N. Awad, and X. Zhang.",
                "Exploring the Value of Online Product Ratings in Revenue Forecasting: The Case of Motion Pictures.",
                "Working paper, 2006. [6] C. Forman, A. Ghose, and B. Wiesenfeld.",
                "A Multi-Level Examination of the Impact of Social Identities on Economic Transactions in Electronic Markets.",
                "Available at SSRN: http://ssrn.com/abstract=918978, July 2006. [7] A. Ghose, P. Ipeirotis, and A. Sundararajan.",
                "Reputation Premiums in Electronic Peer-to-Peer Markets: Analyzing Textual Feedback and Network Structure.",
                "In Third Workshop on Economics of Peer-to-Peer Systems, (P2PECON), 2005. [8] A. Ghose, P. Ipeirotis, and A. Sundararajan.",
                "The Dimensions of Reputation in electronic Markets.",
                "Working Paper CeDER-06-02, New York University, 2006. [9] A. Harmon.",
                "Amazon Glitch Unmasks War of Reviewers.",
                "The New York Times, February 14, 2004. [10] D. Houser and J. Wooders.",
                "Reputation in Auctions: Theory and Evidence from eBay.",
                "Journal of Economics and Management Strategy, 15:353-369, 2006. [11] M. Hu and B. Liu.",
                "Mining and summarizing customer reviews.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD04), 2004. [12] N. Hu, P. Pavlou, and J. Zhang.",
                "Can Online Reviews Reveal a Products True Quality?",
                "In Proceedings of ACM Conference on Electronic Commerce (EC 06), 2006. [13] K. Kalyanam and S. McIntyre.",
                "Return on reputation in online auction market.",
                "Working Paper 02/03-10-WP, Leavey School of Business, Santa Clara University., 2001. [14] L. Khopkar and P. Resnick.",
                "Self-Selection, Slipping, Salvaging, Slacking, and Stoning: the Impacts of Negative Feedback at eBay.",
                "In Proceedings of ACM Conference on Electronic Commerce (EC 05), 2005. [15] M. Melnik and J. Alm.",
                "Does a sellers reputation matter? evidence from ebay auctions.",
                "Journal of Industrial Economics, 50(3):337-350, 2002. [16] R. Olshavsky and J. Miller.",
                "Consumer Expectations, Product Performance and Perceived Product Quality.",
                "Journal of Marketing Research, 9:19-21, February 1972. [17] A. Parasuraman, V. Zeithaml, and L. Berry.",
                "A Conceptual Model of Service Quality and Its Implications for Future Research.",
                "Journal of Marketing, 49:41-50, 1985. [18] A. Parasuraman, V. Zeithaml, and L. Berry.",
                "SERVQUAL: A Multiple-Item Scale for Measuring Consumer Perceptions of Service Quality.",
                "Journal of Retailing, 64:12-40, 1988. [19] P. Pavlou and A. Dimoka.",
                "The Nature and Role of Feedback Text Comments in Online Marketplaces: Implications for Trust Building, Price Premiums, and Seller Differentiation.",
                "Information Systems Research, 17(4):392-414, 2006. [20] A. Popescu and O. Etzioni.",
                "Extracting product features and opinions from reviews.",
                "In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, 2005. [21] R. Teas.",
                "Expectations, Performance Evaluation, and Consumers Perceptions of Quality.",
                "Journal of Marketing, 57:18-34, 1993. [22] E. White.",
                "Chatting a Singer Up the Pop Charts.",
                "The Wall Street Journal, October 15, 1999.",
                "APPENDIX A.",
                "LIST OF WORDS, LR, ASSOCIATED TO THE FEATURE ROOMS All words serve as prefixes: room, space, interior, decor, ambiance, atmosphere, comfort, bath, toilet, bed, building, wall, window, private, temperature, sheet, linen, pillow, hot, water, cold, water, shower, lobby, furniture, carpet, air, condition, mattress, layout, design, mirror, ceiling, lighting, lamp, sofa, chair, dresser, wardrobe, closet 142"
            ],
            "original_annotated_samples": [
                "MOTIVATIONS The spread of the internet has made it possible for online feedback forums (or <br>reputation mechanism</br>s) to become an important channel for Word-of-mouth regarding products, services or other types of commercial interactions.",
                "Both results can be used to improve the way <br>reputation mechanism</br>s aggregate the information from individual reviews."
            ],
            "translated_annotated_samples": [
                "MOTIVACIONES La expansión de internet ha hecho posible que los foros de retroalimentación en línea (o <br>mecanismos de reputación</br>) se conviertan en un canal importante para el boca a boca sobre productos, servicios u otros tipos de interacciones comerciales.",
                "Ambos resultados pueden ser utilizados para mejorar la forma en que los <br>mecanismos de reputación</br> agregan la información de las revisiones individuales."
            ],
            "translated_text": "La comprensión del comportamiento del usuario en la presentación de comentarios en línea. Trabajos anteriores han demostrado que la información contradictoria y los sesgos subyacentes de los usuarios dificultan juzgar el verdadero valor de un servicio. En este artículo, investigamos los factores subyacentes que influyen en el comportamiento del usuario al informar retroalimentación. Examinamos dos fuentes de información además de las calificaciones numéricas: la evidencia lingüística del comentario textual que acompaña a una reseña y los patrones en la secuencia temporal de los informes. Primero demostramos que los grupos de usuarios que discuten ampliamente una característica específica tienen más probabilidades de ponerse de acuerdo en una calificación común para esa característica. Segundo, demostramos que la calificación de un usuario refleja en parte la diferencia entre la calidad real y la expectativa previa de calidad, tal como se infiere de revisiones anteriores. Ambos nos ofrecen una forma menos ruidosa de producir estimaciones de calificación y revelar las razones detrás del sesgo del usuario. Nuestras hipótesis fueron validadas por evidencia estadística de reseñas de hoteles en el sitio web de TripAdvisor. Categorías y Descriptores de Asignaturas J.4 [Ciencias Sociales y del Comportamiento]: Economía Términos Generales Economía, Experimentación, Confiabilidad 1. MOTIVACIONES La expansión de internet ha hecho posible que los foros de retroalimentación en línea (o <br>mecanismos de reputación</br>) se conviertan en un canal importante para el boca a boca sobre productos, servicios u otros tipos de interacciones comerciales. Numerosos estudios empíricos [10, 15, 13, 5] muestran que los compradores consideran seriamente los comentarios en línea al tomar decisiones de compra, y están dispuestos a pagar primas por reputación por productos o servicios que tienen una buena reputación. Sin embargo, un análisis reciente plantea preguntas importantes sobre la capacidad de los foros existentes para reflejar la verdadera calidad de un producto. En ausencia de incentivos claros, los usuarios con una perspectiva moderada no se molestarán en expresar sus opiniones, lo que conduce a una muestra no representativa de reseñas. Por ejemplo, [12, 1] muestran que las calificaciones de libros o CDs en Amazon1 siguen con gran probabilidad distribuciones bimodales en forma de U, donde la mayoría de las calificaciones son o muy buenas o muy malas. Los experimentos controlados, por otro lado, revelan opiniones sobre los mismos elementos que están distribuidas de forma normal. Bajo estas circunstancias, utilizar la media aritmética para predecir la calidad (como la mayoría de los foros realmente hacen) proporciona al usuario típico un estimador con una alta varianza que a menudo es incorrecto. Mejorar la forma en que agregamos la información disponible de las reseñas en línea requiere un profundo entendimiento de los factores subyacentes que sesgan el comportamiento de calificación de los usuarios. Hu et al. [12] proponen el Modelo Brag-and-Moan donde los usuarios califican solo si su utilidad del producto (extraída de una distribución normal) cae fuera de un intervalo mediano. Los autores concluyen que el modelo explica la distribución empírica de los informes y ofrece ideas sobre formas más inteligentes de estimar la verdadera calidad del producto. En el presente artículo ampliamos esta línea de investigación e intentamos explicar más hechos sobre el comportamiento de los usuarios al informar retroalimentación en línea. Usando reseñas reales de hoteles del sitio web TripAdvisor2, consideramos dos fuentes adicionales de información además de las calificaciones numéricas básicas enviadas por los usuarios. La primera es una evidencia lingüística simple proveniente de la revisión textual que suele acompañar a las calificaciones numéricas. Utilizamos técnicas de minería de texto similares a [7] y [3], sin embargo, solo estamos interesados en identificar qué aspectos del servicio está discutiendo el usuario, sin calcular la orientación semántica del texto. Observamos que los usuarios que comentan más sobre la misma característica tienen más probabilidades de estar de acuerdo en una calificación numérica común para esa característica en particular. Intuitivamente, los comentarios extensos revelan la importancia de la característica para el usuario. Dado que las personas tienden a ser más conocedoras en los aspectos que consideran importantes, se podría asumir que los usuarios que discuten una característica específica en más detalle tienen más autoridad para evaluar esa característica. Segundo investigamos la relación entre una reseña 1 http://www.amazon.com 2 http://www.tripadvisor.com/ 134 Figura 1: La página de TripAdvisor que muestra reseñas de un popular hotel en Boston. El nombre del hotel y los anuncios fueron deliberadamente borrados, al igual que las reseñas que lo precedieron. Un examen de las reseñas en línea muestra que las calificaciones a menudo forman parte de los hilos de discusión, donde una publicación no es necesariamente independiente de otras publicaciones. Uno puede ver, por ejemplo, usuarios que se esfuerzan por contradecir o estar vehementemente de acuerdo con los comentarios de usuarios anteriores. Al analizar la secuencia temporal de informes, concluimos que las revisiones pasadas influyen en los informes futuros, ya que crean cierta expectativa previa sobre la calidad del servicio. La percepción subjetiva del usuario está influenciada por la brecha entre la expectativa previa y el rendimiento real del servicio [17, 18, 16, 21], lo cual se reflejará más tarde en la calificación de los usuarios. Proponemos un modelo que captura la dependencia de las calificaciones en las expectativas previas, y lo validamos utilizando los datos empíricos que recopilamos. Ambos resultados pueden ser utilizados para mejorar la forma en que los <br>mecanismos de reputación</br> agregan la información de las revisiones individuales. Nuestro primer resultado se puede utilizar para determinar una estimación de calidad característica por característica, donde para cada característica se considera un subconjunto diferente de reseñas (es decir, aquellas con comentarios extensos sobre esa característica). El segundo conduce a un algoritmo que produce una estimación más precisa de la calidad real. 2. El conjunto de datos que utilizamos en este artículo son reseñas reales de hoteles recopiladas del popular sitio de viajes TripAdvisor. TripAdvisor indexa hoteles de ciudades de todo el mundo, junto con reseñas escritas por viajeros. Los usuarios pueden buscar en el sitio proporcionando el nombre del hotel y la ubicación (opcional). Las reseñas de un hotel dado se muestran como una lista (ordenadas de la más reciente a la más antigua), con 5 reseñas por página. Las reseñas contienen: • información sobre el autor de la reseña (por ejemplo, fechas de estancia, nombre de usuario del revisor, ubicación del revisor); • la calificación general (de 1, la más baja, a 5, la más alta); • una reseña textual que incluye un título para la reseña, comentarios libres y las principales cosas que al revisor le gustaron y no le gustaron; • calificaciones numéricas (de 1, la más baja, a 5, la más alta) para diferentes características (por ejemplo, limpieza, servicio, ubicación, etc.). Debajo del nombre del hotel, TripAdvisor muestra la dirección del hotel, información general (número de habitaciones, número de estrellas, breve descripción, etc.), la calificación promedio general, el ranking de TripAdvisor y una calificación promedio para cada característica. La Figura 1 muestra la página de un hotel popular en Boston cuyo nombre (junto con los anuncios) fue explícitamente borrado. Seleccionamos tres ciudades para este estudio: Boston, Sídney y Las Vegas. Para cada ciudad consideramos todos los hoteles que tenían al menos 10 reseñas, y registramos todas las reseñas. La Tabla 1 presenta el número de hoteles considerados en cada ciudad, el número total de reseñas registradas para cada ciudad y la distribución de hoteles con respecto a la calificación por estrellas (según está disponible en el sitio de TripAdvisor). Ten en cuenta que no todos los hoteles tienen una clasificación por estrellas. Tabla 1: Un resumen del conjunto de datos. Para cada reseña, registramos la calificación general, la reseña textual (título y cuerpo de la reseña) y la calificación numérica en 7 características: Habitaciones (R), Servicio (S), Limpieza (C), Valor (V), Comida (F), Ubicación (L) y Ruido (N). TripAdvisor no requiere que los usuarios envíen nada más que la calificación general, por lo tanto, una reseña típica evalúa pocas características adicionales, independientemente de la discusión en el comentario textual. Solo las características Habitaciones (R), Servicio (S), Limpieza (C) y Valor (V) son evaluadas por un número significativo de usuarios. Sin embargo, también seleccionamos las características Comida (F), Ubicación (L) y Ruido (N) porque son mencionadas en un número significativo de comentarios textuales. Para cada característica registramos la calificación numérica dada por el usuario, o 0 cuando la calificación está ausente. La longitud típica del comentario textual equivale aproximadamente a 200 palabras. Todos los datos fueron recopilados rastreando el sitio de TripAdvisor en septiembre de 2006. 2.1 Notación formal Nos referiremos formalmente a una reseña mediante una tupla (r, T) donde: • r = (rf ) es un vector que contiene las calificaciones rf ∈ {0, 1, . . . 5} para las características f ∈ F = {O, R, S, C, V, F, L, N}; cabe destacar que la calificación general, rO, se registra de forma abusiva como la calificación para la característica General(O); • T es el comentario textual que acompaña a la reseña. Se indexan 135 reseñas de acuerdo con la variable i, de modo que (ri , Ti ) es la i-ésima reseña en nuestra base de datos. Dado que no registramos el nombre de usuario del revisor, también diremos que la i-ésima reseña en nuestro conjunto de datos fue enviada por el usuario i. Cuando necesitemos considerar solo las reseñas de un hotel dado, h, usaremos (ri(h), Ti(h)) para denotar la i-ésima reseña sobre el hotel h. 3. EVIDENCIA DE COMENTARIOS TEXTUALES Los comentarios textuales gratuitos asociados a las reseñas en línea son una fuente valiosa de información para comprender las razones detrás de las calificaciones numéricas dejadas por los revisores. El texto puede, por ejemplo, revelar ejemplos concretos de aspectos que al usuario le gustaron o no le gustaron, justificando así algunas de las calificaciones altas o bajas respectivamente para ciertas características. El texto también puede ofrecer pautas para comprender las preferencias del revisor y los pesos de las diferentes características al calcular una calificación general. El problema, sin embargo, es que los comentarios textuales libres son difíciles de leer. Los usuarios deben desplazarse a través de muchas reseñas y leer principalmente información repetitiva. Se obtendrían mejoras significativas si las reseñas fueran interpretadas y agregadas automáticamente. Desafortunadamente, esta parece ser una tarea difícil para las computadoras ya que los usuarios humanos a menudo utilizan un lenguaje ingenioso, abreviaturas, frases específicas de la cultura y un estilo figurativo. Sin embargo, varios resultados importantes utilizan los comentarios textuales de las reseñas en línea de forma automatizada. Utilizando técnicas de lenguaje natural bien establecidas, las reseñas o partes de las reseñas pueden ser clasificadas como tener una orientación semántica positiva o negativa. Pang et al. [2] clasifican las críticas de películas en positivas/negativas entrenando tres clasificadores diferentes (Naive Bayes, Máxima Entropía y SVM) utilizando características de clasificación basadas en unigramas, bigramas o etiquetas de partes del discurso. Dave et al. [4] analizan reseñas de CNet y Amazon, y sorprendentemente demuestran que las características de clasificación basadas en unigramas o bigramas funcionan mejor que los n-gramas de orden superior. Este resultado es desafiado por Cui et al. [3], quienes examinan grandes colecciones de reseñas recopiladas de la web. Muestran que el tamaño del conjunto de datos es importante, y que conjuntos de entrenamiento más grandes permiten a los clasificadores utilizar con éxito características de clasificación más complejas basadas en n-gramas. Hu y Liu [11] también rastrean la web en busca de reseñas de productos e identifican automáticamente los atributos de productos que han sido discutidos por los revisores. Utilizan Wordnet para calcular la orientación semántica de las evaluaciones de productos y resumir las reseñas de usuarios extrayendo evaluaciones positivas y negativas de diferentes características del producto. Popescu y Etzioni [20] analizan un escenario similar, pero utilizan el recuento de resultados de búsqueda en motores de búsqueda para identificar atributos de productos; la orientación semántica se asigna a través de la técnica de etiquetado de relajación. Ghose et al. [7, 8] analizan las reseñas de vendedores del mercado secundario de Amazon para identificar las diferentes dimensiones (por ejemplo, entrega, empaque, atención al cliente, etc.) de la reputación. Analizan el texto y etiquetan la parte de la oración de cada palabra. Los sustantivos, frases nominales y frases verbales frecuentes se identifican como dimensiones de la reputación, mientras que los modificadores correspondientes (es decir, adjetivos y adverbios) se utilizan para derivar puntuaciones numéricas para cada dimensión. La medida de reputación mejorada se correlaciona mejor con la información de precios observada en el mercado. Pavlou y Dimoka [19] analizan las reseñas de eBay y encuentran que los comentarios textuales tienen un impacto importante en las primas de reputación. Nuestro enfoque es similar a los trabajos mencionados anteriormente, en el sentido de que identificamos los aspectos (es decir, las características del hotel) discutidos por los usuarios en las reseñas textuales. Sin embargo, no calculamos la orientación semántica del texto, ni intentamos inferir las calificaciones faltantes. Definimos el peso, wi f, de la característica f ∈ F en el texto Ti asociado con la reseña (ri, Ti), como la fracción de Ti dedicada a discutir aspectos (tanto positivos como negativos) relacionados con la característica f. Proponemos un método elemental para aproximar los valores de estos pesos. Para cada característica, construimos manualmente la lista de palabras Lf que contiene aproximadamente 50 palabras que están más comúnmente asociadas a la característica f. Las palabras iniciales fueron seleccionadas al leer algunas de las reseñas y ver qué palabras coinciden con la discusión de qué características. La lista fue luego ampliada agregando todas las entradas del tesauro que estaban relacionadas con las palabras iniciales. Finalmente, hicimos una lluvia de ideas para encontrar las palabras faltantes que normalmente estarían asociadas con cada una de las características. Que Lf ∩ Ti sea la lista de términos comunes a Lf y Ti. Cada término de Lf se cuenta el número de veces que aparece en Ti, con dos excepciones: • en los casos en que el usuario envía un título para la revisión, consideramos el texto del título agregándolo tres veces al texto de la revisión Ti. La suposición intuitiva es que la opinión del usuario se refleja con mayor fuerza en el título que en el cuerpo de la reseña. Por ejemplo, muchas reseñas son resumidas con precisión por títulos como \"Excelente servicio, ubicación terrible\" o \"Mala relación calidad-precio\"; ciertas palabras que ocurren solo una vez en el texto son contadas múltiples veces si su relevancia para esa característica es particularmente fuerte. Estas eran las palabras raíz para cada característica (por ejemplo, personal es una palabra raíz para la característica Servicio), y tenían un peso de 2 o 3. Cada característica fue asignada hasta 3 palabras raíz, por lo que casi todas las palabras se cuentan solo una vez. La lista de palabras para la característica Habitaciones se proporciona como referencia en el Apéndice A. El peso wi f se calcula como: wi f = |Lf ∩ Ti| f∈F |Lf ∩ Ti| (1) donde |Lf ∩ Ti| es el número de términos comunes entre Lf y Ti. El peso para la característica \"En general\" se estableció en min{ |T i | 5000 , 1} donde |Ti | es el número de caracteres en Ti. Comenzaré diciendo que soy más de la onda de Holiday Inn que de un hotel de lujo. Así que me frustro cuando pago el doble de la tarifa de la habitación y recibo la mitad de las comodidades que obtendría en un Hampton Inn o Holiday Inn. La ubicación era definitivamente el principal activo de este lugar. Estaba a solo unas cuadras de la parada de metro del Centro Hynes y era fácil caminar a algunos buenos restaurantes en la zona de Back Bay. Boylston no está lejos en absoluto. Así que no tuve problemas en renunciar a un coche de alquiler y tomar el metro desde el aeropuerto hasta el hotel y usar el metro para cualquier otro viaje. De lo contrario, te hacen pagar por todo. Y cuando ya has gastado $215 por noche en la habitación, eso resulta frustrante. La habitación en sí estaba bien, más o menos lo que esperaría. El personal también era promedio, ni malo ni excelente. Una vez más, creo que estás pagando por la ubicación y la capacidad de ir caminando a muchos lugares interesantes. Pero creo que la próxima vez me quedaré en Brookline, disfrutaré de más comodidades y usaré más el metro. Las calificaciones numéricas asociadas a esta reseña son rO = 3, rR = 3, rS = 3, rC = 4, rV = 2 para las características de General (O), Habitaciones (R), Servicio (S), Limpieza (C) y Valor (V) respectivamente. Las calificaciones de las características Comida (F), Ubicación (L) y Ruido (N) están ausentes (es decir, rF = rL = rN = 0). Los pesos wf se calculan a partir de las siguientes listas de términos comunes: LR ∩ T = {habitación}; wR = 0.066 LS ∩ T = {3 * Personal, comodidades}; wS = 0.267 LC ∩ T = ∅; wC = 0 LV ∩ T = {$, tarifa}; wV = 0.133 LF ∩ T = {restaurante}; wF = 0.067 LL ∩ T = {2 * centro, 2 * caminar, 2 * ubicación, área}; wL = 0.467 LN ∩ T = ∅; wN = 0 Las palabras raíz Personal y Centro se triplicaron y duplicaron respectivamente. El peso total de la revisión textual es wO = 0.197. Estos valores explican razonablemente bien los pesos de las diferentes características en la discusión del revisor. Un punto a tener en cuenta es que algunos términos en las listas Lf poseen una orientación semántica inherente. Por ejemplo, la palabra suciedad (perteneciente a la lista LC) se usaría con mayor frecuencia para afirmar la presencia, y no la ausencia de suciedad. Esto es inevitable, pero se tuvo cuidado de asegurar que se utilizaran palabras de ambos extremos del espectro. Por esta razón, algunas listas como LR contienen solo sustantivos de objetos que uno típicamente describiría en una habitación (ver Apéndice A). El objetivo de esta sección es analizar la influencia de los pesos wi f en las calificaciones numéricas ri f. Intuitivamente, los usuarios que pasaron mucho tiempo discutiendo una característica f (es decir, cuando wif es alto) tenían algo que decir sobre su experiencia con respecto a esta característica. Obviamente, la característica f es importante para el usuario i. Dado que las personas tienden a ser más conocedoras en los aspectos que consideran importantes, nuestra hipótesis es que las calificaciones ri f (correspondientes a los pesos altos wi f) constituyen un subconjunto de las calificaciones de expertos para la característica f. La Figura 2 traza la distribución de las tasas r i(h) C con respecto a los pesos w i(h) C para la limpieza de un hotel de Las Vegas, h. Aquí, las calificaciones altas están restringidas a las reseñas que hablan poco sobre la limpieza. Cuando se menciona la limpieza en la discusión, las calificaciones son bajas. Muchos hoteles muestran patrones de calificación similares para varias características. Las calificaciones correspondientes a pesos bajos abarcan todo el espectro del 1 al 5, mientras que las calificaciones correspondientes a pesos altos están más agrupadas (ya sea alrededor de calificaciones buenas o malas). Por lo tanto, formulamos la siguiente hipótesis: Hipótesis 1. Las calificaciones ri f correspondientes a las reseñas donde wi f es alta, son más similares entre sí que a la colección general de calificaciones. Para probar la hipótesis, tomamos todo el conjunto de reseñas y, característica por característica, calculamos la desviación estándar de las calificaciones con alto peso, y la desviación estándar de todo el conjunto de calificaciones. Los pesos altos se definieron como aquellos que pertenecen al 20% superior del rango de pesos para la característica correspondiente. Si la Hipótesis 1 fuera cierta, la desviación estándar de todas las calificaciones debería ser mayor que la desviación estándar de las calificaciones con pesos altos. 0 1 2 3 4 5 6 0 0.1 0.2 0.3 0.4 0.5 0.6 Calificación Peso Figura 2: La distribución de las calificaciones en función del peso de la característica de limpieza. Utilizamos una prueba T estándar para medir la significancia de los resultados. Ciudad por ciudad y característica por característica, la Tabla 2 presenta la desviación estándar promedio de todas las calificaciones, y la desviación estándar promedio de las calificaciones con pesos altos. De hecho, las calificaciones con pesos altos tienen una desviación estándar más baja, y los resultados son significativos en el umbral estándar de significancia de 0.05 (aunque para ciertas ciudades tomadas de forma independiente no parece haber una diferencia significativa, los resultados son significativos para todo el conjunto de datos). Por favor, tenga en cuenta que solo se consideraron las características O, R, S, C y V, ya que para las otras (F, L y N) no teníamos suficientes calificaciones. Tabla 2: Desviación estándar promedio para todas las calificaciones, y desviación estándar promedio para las calificaciones con pesos altos. En corchetes, los valores p correspondientes para una diferencia positiva entre los dos. La hipótesis 1 no solo proporciona una comprensión básica sobre el comportamiento de calificación de los usuarios en línea, sino que también sugiere algunas formas de calcular estimaciones de mejor calidad. Podemos, por ejemplo, construir una estimación de calidad característica por característica con una varianza mucho menor: para cada característica tomamos el subconjunto de reseñas que discuten ampliamente esa característica, y como estimación de calidad, obtenemos la calificación promedio de este subconjunto. Los experimentos iniciales sugieren que las calificaciones promedio de característica a característica calculadas de esta manera son diferentes de las calificaciones promedio calculadas en todo el conjunto de datos. Dado que, de hecho, los pesos altos son indicadores de opiniones expertas, las estimaciones obtenidas de esta manera son más precisas que las actuales. Sin embargo, la validación de esta suposición subyacente requiere más experimentos controlados. LA INFLUENCIA DE LAS CALIFICACIONES PASADAS Por lo general, se hacen dos suposiciones importantes sobre las reseñas enviadas a foros en línea. La primera es que las calificaciones reflejen fielmente la calidad observada por los usuarios; la segunda es que las reseñas sean independientes entre sí. Si bien la evidencia anecdótica [9, 22] cuestiona la primera suposición, en esta sección abordamos la segunda. Un examen de las reseñas en línea muestra que estas suelen formar parte de hilos de discusión, donde los usuarios se esfuerzan por contradecir o estar vehementemente de acuerdo con los comentarios de usuarios anteriores. Considera, por ejemplo, la siguiente reseña: No entiendo las críticas negativas... el hotel estaba un poco oscuro, pero ese era el estilo. Fue muy artístico. Sí, estaba cerca de la autopista, pero en mi opinión, el sonido de un coche ruidoso ocasional es mejor que escuchar el ding ding de las máquinas tragamonedas toda la noche. El personal disponible es FABULOSO. Las camareras son geniales (y *** no merece la mala crítica que recibió, ¡fue 100% atenta con nosotros!), los camareros son amigables y profesionales al mismo tiempo... Aquí, el usuario se vio perturbado por informes negativos anteriores, abordó estas preocupaciones y se dispuso a intentar corregirlas. Como era de esperar, sus calificaciones fueron considerablemente más altas que las calificaciones promedio hasta este punto. Parece que los usuarios de TripAdvisor suelen leer regularmente los informes enviados por usuarios anteriores antes de reservar un hotel, o antes de escribir una reseña. Las reseñas anteriores crean ciertas expectativas previas sobre la calidad del servicio, y estas expectativas influyen en la reseña presentada. Creemos que esta observación es válida para la mayoría de los foros en línea. La percepción subjetiva de la calidad es directamente proporcional a qué tan bien la experiencia real cumple con la expectativa previa, un hecho confirmado por una importante línea de investigación econométrica y de marketing [17, 18, 16, 21]. La correlación entre las reseñas también ha sido confirmada por investigaciones recientes sobre la dinámica de los foros de reseñas en línea [6]. 4.1 Expectativas Previas Definimos la expectativa previa del usuario i con respecto a la característica f, como el promedio de las calificaciones previamente disponibles en la característica f4: ef (i) = j<i,r j f =0 rj f j<i,r j f =0 1 Como primera hipótesis, afirmamos que la calificación ri f es una función de la expectativa previa ef (i): Hipótesis 2. Para un hotel y una característica dados, dados los comentarios i y j tales que ef (i) es alto y ef (j) es bajo, la calificación rj f supera la calificación ri f. Definimos las expectativas altas y bajas como aquellas que están por encima, respectivamente por debajo de un cierto valor umbral θ. El conjunto de reseñas precedidas por altas, respectivamente bajas expectativas 3 parte de las reseñas de Amazon fueron reconocidas como publicaciones estratégicas por autores de libros o competidores 4 si no se asignaron calificaciones previas para la característica f, ef (i) se asigna un valor predeterminado de 4. Tabla 3: Calificaciones promedio para reseñas precedidas por expectativas bajas (primer valor en la celda) y altas (segundo valor en la celda). Los valores P para una diferencia positiva se dan entre corchetes. Las ciudades O R S C V 3.953 4.045 3.985 4.252 3.946 Boston 3.364 3.590 3.485 3.641 3.242 [0.011] [0.028] [0.0086] [0.0168] [0.0034] 4.284 4.358 4.064 4.530 4.428 Sídney 3.756 3.537 3.436 3.918 3.495 [0.000] [0.000] [0.035] [0.009] [0.000] 3.494 3.674 3.713 3.689 3.580 Las Vegas 3.140 3.530 2.952 3.530 3.351 [0.190] [0.529] [0.007] [0.529] [0.253] se definen de la siguiente manera: Ralto f = {ri f |ef (i) > θ} Rbajo f = {ri f |ef (i) < θ} Estos conjuntos son específicos para cada par (hotel, característica), y en nuestros experimentos tomamos θ = 4. Este valor bastante alto está cerca del promedio de calificación de todas las características de todos los hoteles, y se justifica por el hecho de que nuestro conjunto de datos contiene principalmente hoteles de alta calidad. Para cada ciudad, tomamos todos los hoteles y calculamos las calificaciones promedio en los conjuntos Rhigh f y Rlow f (ver Tabla 3). El promedio de calificación entre las reseñas que siguen a expectativas previas bajas es significativamente mayor que el promedio de calificación después de altas expectativas. Como evidencia adicional, consideramos todos los hoteles para los cuales la función eV (i) (la expectativa para la característica Valor) tiene un valor alto (mayor que 4) para algunos i, y un valor bajo (menor que 4) para algunos otros i. Intuitivamente, estos son los hoteles para los cuales hay un grado mínimo de variación en la secuencia oportuna de reseñas: es decir, el promedio acumulativo de calificaciones fue en algún momento alto y luego se volvió bajo, o viceversa. Tales variaciones se observan en aproximadamente la mitad de todos los hoteles en cada ciudad. La Figura 3 traza la mediana (entre los hoteles considerados) de la calificación, rV, cuando ef (i) no es más que x pero mayor que x - 0.5. 2.5 3 3.5 4 4.5 5 2.5 3 3.5 4 4.5 5 Expectativa de la mediana de calificación Boston Sydney Vegas Figura 3: Las calificaciones tienden a disminuir a medida que aumenta la expectativa. 138 Hay dos formas de interpretar la función ef (i): • El valor esperado para la característica f obtenido por el usuario i antes de su experiencia con el servicio, adquirido al leer informes presentados por usuarios anteriores. En este caso, un valor excesivamente alto para ef (i) llevaría al usuario a enviar un informe negativo (o viceversa), derivado de la diferencia entre el valor real del servicio y la expectativa inflada de este valor adquirida antes de su experiencia. • El valor esperado de la característica f para todos los visitantes posteriores del sitio, si el usuario i no enviara un informe. En este caso, la motivación para un informe negativo después de un valor excesivamente alto de ef es diferente: el usuario i busca corregir la expectativa de los futuros visitantes del sitio. A diferencia de la interpretación anterior, esto no requiere que el usuario derive una expectativa a priori para el valor de f. Tenga en cuenta que ninguna de las interpretaciones implica que el promedio hasta el informe i esté inversamente relacionado con la calificación en el informe i. Podría existir una medida de influencia ejercida por informes anteriores que impulse al usuario detrás del informe i a enviar calificaciones que, en cierta medida, se ajusten a los informes anteriores: un valor bajo para ef (i) puede influir en el usuario i a enviar una calificación baja para la característica f porque, por ejemplo, teme que enviar una calificación alta lo haga parecer una persona con bajos estándares. Esto, al principio, parece contradecir la Hipótesis 2. Sin embargo, esta calificación de conformidad no puede continuar indefinidamente: una vez que el conjunto de informes proyecte una estimación suficientemente reducida para vf, los revisores futuros con impresiones comparativamente positivas buscarán corregir esta percepción errónea. 4.2 Impacto de los comentarios textuales en la expectativa de calidad. Se puede obtener una mayor comprensión del comportamiento de calificación de los usuarios de TripAdvisor analizando la relación entre los pesos wf y los valores ef (i). En particular, examinamos la siguiente hipótesis: Hipótesis 3. Cuando una gran proporción del texto de una reseña discute una característica en particular, la diferencia entre la calificación para esa característica y la calificación promedio hasta ese momento tiende a ser grande. La intuición detrás de esta afirmación es que cuando el usuario insiste en expresar su opinión sobre una característica en particular, su opinión difiere de la opinión colectiva de publicaciones anteriores. Esto se basa en la característica de los sistemas de reputación como foros de retroalimentación donde un usuario está interesado en proyectar su opinión, con particular fuerza si esta opinión difiere de lo que percibe como la opinión general. Para probar la Hipótesis 3 medimos la diferencia absoluta promedio entre la expectativa ef (i) y la calificación ri f cuando el peso wi f es alto, respectivamente bajo. Los pesos se clasifican como altos o bajos al compararlos con ciertos valores de corte: wi f es bajo si es menor que 0.1, mientras que wi f es alto si es mayor que θf. Se utilizaron diferentes valores de corte para diferentes características: θR = 0.4, θS = 0.4, θC = 0.2 y θV = 0.7. La limpieza tiene un límite inferior más bajo ya que es una característica raramente discutida; el valor tiene un límite superior alto por la razón opuesta. Los resultados se presentan en la Tabla 4. La idea de que los informes negativos pueden fomentar una mayor divulgación negativa ha sido sugerida anteriormente [14]. Tabla 4: Promedio de |ri f −ef (i)| cuando los pesos son altos (primer valor en la celda) y bajos (segundo valor en la celda) con valores P para la diferencia en corchetes cuadrados. Esto demuestra que cuando los pesos son inusualmente altos, los usuarios tienden a expresar una opinión que no se ajusta al promedio neto de calificaciones anteriores. Como podríamos esperar, para una característica que rara vez tuvo un peso importante en la discusión (por ejemplo, la limpieza), la diferencia es particularmente grande. Aunque la diferencia en el valor de la característica es bastante grande para Sídney, el valor P es alto. Esto se debe a que solo unos pocos comentarios discutieron en gran medida el valor. La razón podría ser cultural o porque había menos motivo para discutir esta característica. 4.3 Incentivos para informar Los modelos anteriores sugieren que los usuarios poco opinativos no elegirán expresar sus opiniones [12]. En esta sección, ampliamos este modelo para tener en cuenta la influencia de las expectativas. La motivación para enviar comentarios no se debe solo a opiniones extremas, sino también a la diferencia entre la reputación actual (es decir, la expectativa previa del usuario) y la experiencia real. Un modelo de calificación así produce calificaciones que la mayoría de las veces se desvían de la calificación promedio actual. Las calificaciones que confirman la expectativa previa rara vez serán enviadas. Probamos en nuestro conjunto de datos la proporción de calificaciones que intentan corregir la estimación actual. Definimos una calificación desviada como aquella que se desvía de la expectativa actual por al menos un umbral θ, es decir, |ri f − ef (i)| ≥ θ. Para cada una de las tres ciudades consideradas, las siguientes tablas muestran la proporción de calificaciones desviadas para θ = 0.5 y θ = 1. Los resultados anteriores sugieren que una gran proporción de usuarios (casi la mitad, incluso para el valor umbral alto θ = 1) se desvían del promedio previo. Esto refuerza la idea de que los usuarios son más propensos a enviar un informe cuando creen que tienen algo distintivo que agregar al flujo actual de opiniones sobre alguna característica. Tales conclusiones están en total acuerdo con evidencia previa de que la distribución de informes a menudo sigue distribuciones bimodales en forma de U. 139 5. Para dar cuenta de las observaciones descritas en las secciones anteriores, proponemos un modelo para el comportamiento de los usuarios al enviar reseñas en línea. Para un hotel dado, asumimos que la calidad experimentada por los usuarios sigue una distribución normal alrededor de algún valor vf, que representa la calidad objetiva ofrecida por el hotel en la característica f. La calificación enviada por el usuario i en la característica f es: ˆri f = δf vi f + (1 − δf) · sign vi f − ef (i) c + d(vi f, ef (i)|wi f) (2) donde: • vi f es la calidad (desconocida) experimentada realmente por el usuario. Se asume que vi f sigue una distribución normal alrededor de algún valor vf; • δf ∈ [0, 1] puede verse como una medida del sesgo al reportar retroalimentación. Los valores altos reflejan el hecho de que los usuarios califican de manera objetiva, sin ser influenciados por expectativas previas. El valor de δf puede depender de varios factores; fijamos un valor para cada característica f; • c es una constante entre 1 y 5; • wi f es el peso de la característica f en el comentario textual de la reseña i, calculado según la Ecuación (1); • d(vi f , ef (i)|wi f ) es una función de distancia entre la expectativa y la observación del usuario i. La función de distancia satisface las siguientes propiedades: - d(y, z|w) ≥ 0 para todo y, z ∈ [0, 5], w ∈ [0, 1]; - |d(y, z|w)| < |d(z, x|w)| si |y − z| < |z − x|; - |d(y, z|w1)| < |d(y, z|w2)| si w1 < w2; - c + d(vf , ef (i)|wi f ) ∈ [1, 5]; El segundo término de la Ecuación (2) codifica el sesgo de la calificación. Cuanto mayor sea la distancia entre la observación real vi f y la función ef, mayor será el sesgo. 5.1 Validación del modelo Utilizamos el conjunto de datos de las reseñas de TripAdvisor para validar el modelo de comportamiento presentado anteriormente. Dividimos por conveniencia los valores de calificación en tres rangos: malo (B = {1, 2}), indiferente (I = {3, 4}), y bueno (G = {5}), y realizamos los siguientes dos tests: • Primero, usaremos nuestro modelo para predecir las calificaciones que tienen valores extremos. Para cada hotel, tomamos la secuencia de informes, y cada vez que nos encontramos con una calificación que sea buena o mala (pero no indiferente) intentamos predecirla utilizando la Ecuación (2) • En segundo lugar, en lugar de predecir el valor de las calificaciones extremas, intentamos clasificarlas como buenas o malas. Para cada hotel tomamos la secuencia de informes, y para cada informe (independientemente de su valor) lo clasificamos como bueno o malo. Sin embargo, para realizar estas pruebas, necesitamos estimar el valor objetivo, vf, que es el promedio de las observaciones de calidad reales, vi f. El algoritmo que estamos utilizando se basa en la intuición de que la cantidad de calificación de conformidad se minimiza. En otras palabras, el valor vf debe ser tal que, tan a menudo como sea posible, las calificaciones malas sigan a expectativas por encima de vf y las calificaciones buenas sigan a expectativas por debajo de vf. Formalmente, definimos los conjuntos: Γ1 = {i|ef (i) < vf y ri f ∈ B}; Γ2 = {i|ef (i) > vf y ri f ∈ G}; que corresponden a irregularidades donde, a pesar de que la expectativa en el punto i es menor que el valor entregado, la calificación es baja, y viceversa. Definimos vf como el valor que minimiza la unión de los dos conjuntos: vf = arg min vf |Γ1 ∪ Γ2| (3) En la Ec. (2) reemplazamos vi f por el valor vf calculado en la Ec. (3), y utilizamos la siguiente función de distancia: d(vf , ef (i)|wi f ) = |vf − ef (i)| vf − ef (i) |vf 2 − ef (i)2 | · (1 + 2wi f ); La constante c ∈ I se estableció en min{max{ef (i), 3}}, 4}. Los valores para δf se fijaron en {0.7, 0.7, 0.8, 0.7, 0.6} para las características {General, Habitaciones, Servicio, Limpieza, Valor} respectivamente. Los pesos se calculan tal como se describe en la Sección 3. Como primer experimento, tomamos los conjuntos de calificaciones extremas {ri f |ri f /∈ I} para cada hotel y característica. Para cada calificación de este tipo, ri f, intentamos estimarla calculando ˆri f usando la Ecuación (2). Comparamos este estimador con el obtenido simplemente promediando las calificaciones de todos los hoteles y características: es decir, ¯rf = j,r j f =0 rj f j,r j f =0 1; la Tabla 7 presenta la relación entre el error cuadrático medio (RMSE) al usar ˆri f y ¯rf para estimar las calificaciones reales. En todos los casos, la estimación producida por nuestro modelo es mejor que el promedio simple. Tabla 7: Promedio de RMSE(ˆrf) RMSE(¯rf) Ciudad O R S C V Boston 0.987 0.849 0.879 0.776 0.913 Sídney 0.927 0.817 0.826 0.720 0.681 Las Vegas 0.952 0.870 0.881 0.947 0.904 Como segundo experimento, intentamos distinguir los conjuntos Bf = {i|ri f ∈ B} y Gf = {i|ri f ∈ G} de calificaciones malas y buenas, respectivamente, en la característica f. Por ejemplo, calculamos el conjunto Bf usando el siguiente clasificador (llamado σ): ri f ∈ Bf (σf (i) = 1) ⇔ ˆri f ≤ 4; Las Tablas 8, 9 y 10 presentan la Precisión (p), Recall (r) y s = 2pr p+r para el clasificador σ, y lo comparan con un clasificador de mayoría ingenuo, τ, τf (i) = 1 ⇔ |Bf | ≥ |Gf |: Vemos que el recall es siempre mayor para σ y la precisión suele ser ligeramente peor. Para la métrica s, σ tiende a agregar un 140. Tabla 8: Precisión (p), Recall (r), s= 2pr p+r mientras se detectan calificaciones bajas para Boston O R S C V p 0.678 0.670 0.573 0.545 0.610 σ r 0.626 0.659 0.619 0.612 0.694 s 0.651 0.665 0.595 0.577 0.609 p 0.684 0.706 0.647 0.611 0.633 τ r 0.597 0.541 0.410 0.383 0.562 s 0.638 0.613 0.502 0.471 0.595 Tabla 9: Precisión (p), Recall (r), s= 2pr p+r mientras se detectan calificaciones bajas para Las Vegas O R S C V p 0.654 0.748 0.592 0.712 0.583 σ r 0.608 0.536 0.791 0.474 0.610 s 0.630 0.624 0.677 0.569 0.596 p 0.685 0.761 0.621 0.748 0.606 τ r 0.542 0.505 0.767 0.445 0.441 s 0.605 0.607 0.670 0.558 0.511 Mejora del 1-20% sobre τ, mucho mayor en algunos casos para hoteles en Sídney. Esto se debe probablemente a que las reseñas de Sídney son más positivas que las de las ciudades estadounidenses y los casos en los que el número de reseñas negativas supera al de las positivas son raros. Reemplazar el algoritmo de prueba con uno que juega un 1 con una probabilidad igual a la proporción de malas críticas mejora sus resultados para esta ciudad, pero aún es superado por alrededor del 80%. 6. RESUMEN DE RESULTADOS Y CONCLUSIÓN El objetivo de este artículo es explorar los factores que llevan a un usuario a enviar una calificación particular, en lugar de los incentivos que lo animaron a enviar un informe en primer lugar. Para ello utilizamos dos fuentes adicionales de información además del vector de calificaciones numéricas: primero, examinamos los comentarios textuales que acompañan las reseñas, y segundo, consideramos los informes que han sido previamente enviados por otros usuarios. Usando algoritmos simples de procesamiento de lenguaje natural, pudimos establecer una correlación entre el peso de una característica específica en el comentario textual que acompaña la reseña, y el ruido presente en la calificación numérica. Específicamente, parece que los usuarios que discuten ampliamente una característica en particular tienden a estar de acuerdo en una calificación común. Esta observación permite la construcción de estimadores de calidad característica por característica que tienen una varianza más baja y, con suerte, son menos ruidosos. Sin embargo, se requiere más evidencia para respaldar la intuición de que las calificaciones correspondientes a pesos altos son opiniones de expertos que merecen ser consideradas de mayor prioridad al calcular estimaciones de calidad. En segundo lugar, enfatizamos la dependencia de las calificaciones en informes previos. Los informes anteriores crean una expectativa de calidad que afecta la percepción subjetiva del usuario. Validamos dos hechos sobre las reseñas de hoteles que recopilamos de TripAdvisor: Primero, las calificaciones que siguen expectativas bajas (donde la expectativa se calcula como el promedio de los informes anteriores) probablemente sean más altas que las calificaciones que siguen expectativas altas. Intuitivamente, la percepción de la calidad (y consecuentemente la calificación) depende de qué tan bien la experiencia real del usuario cumple con sus expectativas. Segundo, incluimos evidencia de los comentarios textuales, y encontramos que cuando los usuarios dedican una gran parte del texto a discutir una característica específica, es probable que motiven una calificación divergente (es decir, una calificación que no se ajusta a la expectativa previa). Intuitivamente, esto respalda la hipótesis de que los foros de reseñas actúan como grupos de discusión donde los usuarios están interesados en presentar y motivar su propia opinión. Hemos capturado la evidencia empírica en un modelo de comportamiento que predice las calificaciones enviadas por los usuarios. La calificación final depende, como era de esperar, de la observación real y de la brecha entre la observación y la expectativa. La brecha tiende a tener una influencia mayor cuando una fracción importante del comentario textual está dedicada a discutir una característica específica. El modelo propuesto fue validado con los datos empíricos y proporciona mejores estimaciones de las calificaciones realmente enviadas. Una suposición que hacemos es sobre la existencia de un valor de calidad objetivo vf para la característica f. Esto rara vez es cierto, especialmente a lo largo de largos periodos de tiempo. Otras explicaciones podrían dar cuenta de la correlación de las calificaciones con informes anteriores. Por ejemplo, si ef (i) refleja el valor real de f en un punto en el tiempo, la diferencia en las calificaciones siguiendo expectativas altas y bajas puede ser explicada por modelos de ingresos hoteleros que se maximizan cuando el valor es modificado en consecuencia. Sin embargo, la idea de que la variación en las calificaciones no es principalmente una función de la variación en el valor resulta ser útil. Nuestro enfoque para aproximar este valor objetivo es de ninguna manera perfecto, pero se ajusta perfectamente a la idea detrás del modelo. Una dirección natural para trabajos futuros es examinar aplicaciones concretas de nuestros resultados. Es probable que se obtengan mejoras significativas en las estimaciones de calidad al incorporar toda la evidencia empírica sobre el comportamiento de calificación. No está claro exactamente cómo diferentes factores afectan las decisiones de los usuarios. La respuesta podría depender de la aplicación particular, el contexto y la cultura. 7. REFERENCIAS [1] A. Admati y P. Pfleiderer. Noisytalk.com: Transmitiendo opiniones en un entorno ruidoso. Documento de trabajo 1670R, Universidad de Stanford, 2000. [2] P. B., L. Lee y S. Vaithyanathan. ¿Clasificación de sentimientos con técnicas de aprendizaje automático? En Actas de EMNLP-02, la Conferencia sobre Métodos Empíricos en el Procesamiento del Lenguaje Natural, 2002. [3] H. Cui, V. Mittal y M. Datar. Experimentos comparativos 141 sobre clasificación de sentimientos para reseñas de productos en línea. En Actas de AAAI, 2006. [4] K. Dave, S. Lawrence y D. Pennock. Extracción de opiniones y clasificación semántica de reseñas de productos en la galería de cacahuetes. En Actas de la 12ª Conferencia Internacional sobre la World Wide Web (WWW03), 2003. [5] C. Dellarocas, N. Awad y X. Zhang. Explorando el valor de las calificaciones de productos en línea en la previsión de ingresos: El caso de las películas en movimiento. Documento de trabajo, 2006. [6] C. Forman, A. Ghose y B. Wiesenfeld. Un Examen Multinivel del Impacto de las Identidades Sociales en las Transacciones Económicas en los Mercados Electrónicos. Disponible en SSRN: http://ssrn.com/abstract=918978, julio de 2006. [7] A. Ghose, P. Ipeirotis y A. Sundararajan. Primas de reputación en mercados electrónicos peer-to-peer: análisis de retroalimentación textual y estructura de red. En el Tercer Taller sobre Economía de Sistemas Peer-to-Peer (P2PECON), 2005. [8] A. Ghose, P. Ipeirotis y A. Sundararajan. Las dimensiones de la reputación en los mercados electrónicos. Documento de trabajo CeDER-06-02, Universidad de Nueva York, 2006. [9] A. Harmon. Error de Amazon revela la guerra de los revisores. The New York Times, 14 de febrero de 2004. [10] D. Houser y J. Wooders. Reputación en subastas: teoría y evidencia de eBay. Revista de Estrategia Económica y de Gestión, 15:353-369, 2006. [11] M. Hu y B. Liu. Extracción y resumen de reseñas de clientes. En Actas de la Conferencia Internacional de la ACM SIGKDD sobre Descubrimiento de Conocimiento y Minería de Datos (KDD04), 2004. [12] N. Hu, P. Pavlou y J. Zhang. ¿Pueden las reseñas en línea revelar la verdadera calidad de un producto? En Actas de la Conferencia ACM sobre Comercio Electrónico (EC 06), 2006. [13] K. Kalyanam y S. McIntyre. Retorno de la reputación en el mercado de subastas en línea. Documento de trabajo 02/03-10-WP, Escuela de Negocios Leavey, Universidad de Santa Clara, 2001. [14] L. Khopkar y P. Resnick. Auto-selección, deslizamiento, salvamento, holgazanería y lapidación: los impactos de la retroalimentación negativa en eBay. En Actas de la Conferencia ACM sobre Comercio Electrónico (EC 05), 2005. [15] M. Melnik y J. Alm. ¿Importa la reputación de un vendedor? evidencia de subastas en eBay. Revista de Economía Industrial, 50(3):337-350, 2002. [16] R. Olshavsky y J. Miller. Expectativas del consumidor, rendimiento del producto y calidad percibida del producto. Revista de Investigación de Marketing, 9:19-21, febrero de 1972. [17] A. Parasuraman, V. Zeithaml y L. Berry. Un Modelo Conceptual de Calidad de Servicio y sus Implicaciones para Investigaciones Futuras. Revista de Marketing, 49:41-50, 1985. [18] A. Parasuraman, V. Zeithaml y L. Berry. SERVQUAL: Una escala de múltiples ítems para medir las percepciones de calidad del servicio por parte del consumidor. Revista de Retail, 64:12-40, 1988. [19] P. Pavlou y A. Dimoka. La naturaleza y función de los comentarios de texto de retroalimentación en los mercados en línea: implicaciones para la construcción de confianza, primas de precio y diferenciación de vendedores. Investigación de Sistemas de Información, 17(4):392-414, 2006. [20] A. Popescu y O. Etzioni. Extrayendo características del producto y opiniones de las reseñas. En Actas de la Conferencia de Tecnología del Lenguaje Humano y Conferencia sobre Métodos Empíricos en Procesamiento del Lenguaje Natural, 2005. [21] R. Teas. Expectativas, Evaluación del Rendimiento y Percepciones de Calidad de los Consumidores. Revista de Marketing, 57:18-34, 1993. [22] E. White. Chateando a un cantante en las listas de éxitos pop. El Wall Street Journal, 15 de octubre de 1999. APÉNDICE A. LISTA DE PALABRAS, LR, ASOCIADAS A LA CARACTERÍSTICA HABITACIONES. Todas las palabras sirven como prefijos: habitación, espacio, interior, decoración, ambiente, atmósfera, comodidad, baño, inodoro, cama, edificio, pared, ventana, privado, temperatura, sábana, lino, almohada, caliente, agua, fría, agua, ducha, vestíbulo, muebles, alfombra, aire, acondicionado, colchón, distribución, diseño, espejo, techo, iluminación, lámpara, sofá, silla, cómoda, armario, armario. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "feature-by-feature estimator of quality": {
            "translated_key": "estimador de calidad característica por característica",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Understanding User Behavior in Online Feedback Reporting Arjun Talwar Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland arjun@math.stanford.edu Radu Jurca Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland radu.jurca@epfl.ch Boi Faltings Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland boi.faltings@epfl.ch ABSTRACT Online reviews have become increasingly popular as a way to judge the quality of various products and services.",
                "Previous work has demonstrated that contradictory reporting and underlying user biases make judging the true worth of a service difficult.",
                "In this paper, we investigate underlying factors that influence user behavior when reporting feedback.",
                "We look at two sources of information besides numerical ratings: linguistic evidence from the textual comment accompanying a review, and patterns in the time sequence of reports.",
                "We first show that groups of users who amply discuss a certain feature are more likely to agree on a common rating for that feature.",
                "Second, we show that a users rating partly reflects the difference between true quality and prior expectation of quality as inferred from previous reviews.",
                "Both give us a less noisy way to produce rating estimates and reveal the reasons behind user bias.",
                "Our hypotheses were validated by statistical evidence from hotel reviews on the TripAdvisor website.",
                "Categories and Subject Descriptors J.4 [Social and Behavioral Sciences]: Economics General Terms Economics, Experimentation, Reliability 1.",
                "MOTIVATIONS The spread of the internet has made it possible for online feedback forums (or reputation mechanisms) to become an important channel for Word-of-mouth regarding products, services or other types of commercial interactions.",
                "Numerous empirical studies [10, 15, 13, 5] show that buyers seriously consider online feedback when making purchasing decisions, and are willing to pay reputation premiums for products or services that have a good reputation.",
                "Recent analysis, however, raises important questions regarding the ability of existing forums to reflect the real quality of a product.",
                "In the absence of clear incentives, users with a moderate outlook will not bother to voice their opinions, which leads to an unrepresentative sample of reviews.",
                "For example, [12, 1] show that Amazon1 ratings of books or CDs follow with great probability bi-modal, U-shaped distributions where most of the ratings are either very good, or very bad.",
                "Controlled experiments, on the other hand, reveal opinions on the same items that are normally distributed.",
                "Under these circumstances, using the arithmetic mean to predict quality (as most forums actually do) gives the typical user an estimator with high variance that is often false.",
                "Improving the way we aggregate the information available from online reviews requires a deep understanding of the underlying factors that bias the rating behavior of users.",
                "Hu et al. [12] propose the Brag-and-Moan Model where users rate only if their utility of the product (drawn from a normal distribution) falls outside a median interval.",
                "The authors conclude that the model explains the empirical distribution of reports, and offers insights into smarter ways of estimating the true quality of the product.",
                "In the present paper we extend this line of research, and attempt to explain further facts about the behavior of users when reporting online feedback.",
                "Using actual hotel reviews from the TripAdvisor2 website, we consider two additional sources of information besides the basic numerical ratings submitted by users.",
                "The first is simple linguistic evidence from the textual review that usually accompanies the numerical ratings.",
                "We use text-mining techniques similar to [7] and [3], however, we are only interested in identifying what aspects of the service the user is discussing, without computing the semantic orientation of the text.",
                "We find that users who comment more on the same feature are more likely to agree on a common numerical rating for that particular feature.",
                "Intuitively, lengthy comments reveal the importance of the feature to the user.",
                "Since people tend to be more knowledgeable in the aspects they consider important, users who discuss a given feature in more details might be assumed to have more authority in evaluating that feature.",
                "Second we investigate the relationship between a review 1 http://www.amazon.com 2 http://www.tripadvisor.com/ 134 Figure 1: The TripAdvisor page displaying reviews for a popular Boston hotel.",
                "Name of hotel and advertisements were deliberatively erased. and the reviews that preceded it.",
                "A perusal of online reviews shows that ratings are often part of discussion threads, where one post is not necessarily independent of other posts.",
                "One may see, for example, users who make an effort to contradict, or vehemently agree with, the remarks of previous users.",
                "By analyzing the time sequence of reports, we conclude that past reviews influence the future reports, as they create some prior expectation regarding the quality of service.",
                "The subjective perception of the user is influenced by the gap between the prior expectation and the actual performance of the service [17, 18, 16, 21] which will later reflect in the users rating.",
                "We propose a model that captures the dependence of ratings on prior expectations, and validate it using the empirical data we collected.",
                "Both results can be used to improve the way reputation mechanisms aggregate the information from individual reviews.",
                "Our first result can be used to determine a featureby-feature estimate of quality, where for each feature, a different subset of reviews (i.e., those with lengthy comments of that feature) is considered.",
                "The second leads to an algorithm that outputs a more precise estimate of the real quality. 2.",
                "THE DATA SET We use in this paper real hotel reviews collected from the popular travel site TripAdvisor.",
                "TripAdvisor indexes hotels from cities across the world, along with reviews written by travelers.",
                "Users can search the site by giving the hotels name and location (optional).",
                "The reviews for a given hotel are displayed as a list (ordered from the most recent to the oldest), with 5 reviews per page.",
                "The reviews contain: • information about the author of the review (e.g., dates of stay, username of the reviewer, location of the reviewer); • the overall rating (from 1, lowest, to 5, highest); • a textual review containing a title for the review, free comments, and the main things the reviewer liked and disliked; • numerical ratings (from 1, lowest, to 5, highest) for different features (e.g., cleanliness, service, location, etc.)",
                "Below the name of the hotel, TripAdvisor displays the address of the hotel, general information (number of rooms, number of stars, short description, etc), the average overall rating, the TripAdvisor ranking, and an average rating for each feature.",
                "Figure 1 shows the page for a popular Boston hotel whose name (along with advertisements) was explicitly erased.",
                "We selected three cities for this study: Boston, Sydney and Las Vegas.",
                "For each city we considered all hotels that had at least 10 reviews, and recorded all reviews.",
                "Table 1 presents the number of hotels considered in each city, the total number of reviews recorded for each city, and the distribution of hotels with respect to the star-rating (as available on the TripAdvisor site).",
                "Note that not all hotels have a star-rating.",
                "Table 1: A summary of the data set.",
                "City # Reviews # Hotels # of Hotels with 1,2,3,4 & 5 stars Boston 3993 58 1+3+17+15+2 Sydney 1371 47 0+0+9+13+10 Las Vegas 5593 40 0+3+10+9+6 For each review we recorded the overall rating, the textual review (title and body of the review) and the numerical rating on 7 features: Rooms(R), Service(S), Cleanliness(C), Value(V), Food(F), Location(L) and Noise(N).",
                "TripAdvisor does not require users to submit anything other than the overall rating, hence a typical review rates few additional features, regardless of the discussion in the textual comment.",
                "Only the features Rooms(R), Service(S), Cleanliness(C) and Value(V) are rated by a significant number of users.",
                "However, we also selected the features Food(F), Location(L) and Noise(N) because they are referred to in a significant number of textual comments.",
                "For each feature we record the numerical rating given by the user, or 0 when the rating is missing.",
                "The typical length of the textual comment amounts to approximately 200 words.",
                "All data was collected by crawling the TripAdvisor site in September 2006. 2.1 Formal notation We will formally refer to a review by a tuple (r, T) where: • r = (rf ) is a vector containing the ratings rf ∈ {0, 1, . . . 5} for the features f ∈ F = {O, R, S, C, V, F, L, N}; note that the overall rating, rO, is abusively recorded as the rating for the feature Overall(O); • T is the textual comment that accompanies the review. 135 Reviews are indexed according to the variable i, such that (ri , Ti ) is the ith review in our database.",
                "Since we dont record the username of the reviewer, we will also say that the ith review in our data set was submitted by user i.",
                "When we need to consider only the reviews of a given hotel, h, we will use (ri(h) , Ti(h) ) to denote the ith review about the hotel h. 3.",
                "EVIDENCE FROM TEXTUAL COMMENTS The free textual comments associated to online reviews are a valuable source of information for understanding the reasons behind the numerical ratings left by the reviewers.",
                "The text may, for example, reveal concrete examples of aspects that the user liked or disliked, thus justifying some of the high, respectively low ratings for certain features.",
                "The text may also offer guidelines for understanding the preferences of the reviewer, and the weights of different features when computing an overall rating.",
                "The problem, however, is that free textual comments are difficult to read.",
                "Users are required to scroll through many reviews and read mostly repetitive information.",
                "Significant improvements would be obtained if the reviews were automatically interpreted and aggregated.",
                "Unfortunately, this seems a difficult task for computers since human users often use witty language, abbreviations, cultural specific phrases, and the figurative style.",
                "Nevertheless, several important results use the textual comments of online reviews in an automated way.",
                "Using well established natural language techniques, reviews or parts of reviews can be classified as having a positive or negative semantic orientation.",
                "Pang et al. [2] classify movie reviews into positive/negative by training three different classifiers (Naive Bayes, Maximum Entropy and SVM) using classification features based on unigrams, bigrams or part-of-speech tags.",
                "Dave et al. [4] analyze reviews from CNet and Amazon, and surprisingly show that classification features based on unigrams or bigrams perform better than higher-order n-grams.",
                "This result is challenged by Cui et al. [3] who look at large collections of reviews crawled from the web.",
                "They show that the size of the data set is important, and that bigger training sets allow classifiers to successfully use more complex classification features based on n-grams.",
                "Hu and Liu [11] also crawl the web for product reviews and automatically identify product attributes that have been discussed by reviewers.",
                "They use Wordnet to compute the semantic orientation of product evaluations and summarize user reviews by extracting positive and negative evaluations of different product features.",
                "Popescu and Etzioni [20] analyze a similar setting, but use search engine hit-counts to identify product attributes; the semantic orientation is assigned through the relaxation labeling technique.",
                "Ghose et al. [7, 8] analyze seller reviews from the Amazon secondary market to identify the different dimensions (e.g., delivery, packaging, customer support, etc.) of reputation.",
                "They parse the text, and tag the part-of-speech for each word.",
                "Frequent nouns, noun phrases and verbal phrases are identified as dimensions of reputation, while the corresponding modifiers (i.e., adjectives and adverbs) are used to derive numerical scores for each dimension.",
                "The enhanced reputation measure correlates better with the pricing information observed in the market.",
                "Pavlou and Dimoka [19] analyze eBay reviews and find that textual comments have an important impact on reputation premiums.",
                "Our approach is similar to the previously mentioned works, in the sense that we identify the aspects (i.e., hotel features) discussed by the users in the textual reviews.",
                "However, we do not compute the semantic orientation of the text, nor attempt to infer missing ratings.",
                "We define the weight, wi f , of feature f ∈ F in the text Ti associated with the review (ri , Ti ), as the fraction of Ti dedicated to discussing aspects (both positive and negative) related to feature f. We propose an elementary method to approximate the values of these weights.",
                "For each feature we manually construct the word list Lf containing approximately 50 words that are most commonly associated to the feature f. The initial words were selected from reading some of the reviews, and seeing what words coincide with discussion of which features.",
                "The list was then extended by adding all thesaurus entries that were related to the initial words.",
                "Finally, we brainstormed for missing words that would normally be associated with each of the features.",
                "Let Lf ∩Ti be the list of terms common to both Lf and Ti.",
                "Each term of Lf is counted the number of times it appears in Ti , with two exceptions: • in cases where the user submits a title to the review, we account for the title text by appending it three times to the review text Ti .",
                "The intuitive assumption is that the users opinion is more strongly reflected in the title, rather than in the body of the review.",
                "For example, many reviews are accurately summarized by titles such as Excellent service, terrible location or Bad value for money; • certain words that occur only once in the text are counted multiple times if their relevance to that feature is particularly strong.",
                "These were root words for each feature (e.g., staff is a root word for the feature Service), and were weighted either 2 or 3.",
                "Each feature was assigned up to 3 such root words, so almost all words are counted only once.",
                "The list of words for the feature Rooms is given for reference in Appendix A.",
                "The weight wi f is computed as: wi f = |Lf ∩ Ti| f∈F |Lf ∩ Ti| (1) where |Lf ∩Ti | is the number of terms common to Lf and Ti .",
                "The weight for the feature Overall was set to min{ |T i | 5000 , 1} where |Ti | is the number of character in Ti .",
                "The following is a TripAdvisor review for a Boston hotel (the name of the hotel is omitted): Ill start by saying that Im more of a Holiday Inn person than a *** type.",
                "So I get frustrated when I pay double the room rate and get half the amenities that Id get at a Hampton Inn or Holiday Inn.",
                "The location was definitely the main asset of this place.",
                "It was only a few blocks from the Hynes Center subway stop and it was easy to walk to some good restaurants in the Back Bay area.",
                "Boylston isnt far off at all.",
                "So I had no trouble with foregoing a rental car and taking the subway from the airport to the hotel and using the subway for any other travel.",
                "Otherwise, they make you pay for anything and everything. 136 And when youve already dropped $215/night on the room, that gets frustrating.The room itself was decent, about what I would expect.",
                "Staff was also average, not bad and not excellent.",
                "Again, I think youre paying for location and the ability to walk to a lot of good stuff.",
                "But I think next time Ill stay in Brookline, get more amenities, and use the subway a bit more.",
                "This numerical ratings associated to this review are rO = 3, rR = 3, rS = 3, rC = 4, rV = 2 for features Overall(O), Rooms(R), Service(S), Cleanliness(C) and Value(V) respectively.",
                "The ratings for the features Food(F), Location(L) and Noise(N) are absent (i.e., rF = rL = rN = 0).",
                "The weights wf are computed from the following lists of common terms: LR ∩ T ={room}; wR = 0.066 LS ∩ T ={3 * Staff, amenities}; wS = 0.267 LC ∩ T = ∅; wC = 0 LV ∩ T ={$, rate}; wV = 0.133 LF ∩ T ={restaurant}; wF = 0.067 LL ∩ T ={2 * center, 2 * walk, 2 * location, area}; wL = 0.467 LN ∩ T = ∅; wN = 0 The root words Staff and Center were tripled and doubled respectively.",
                "The overall weight of the textual review is wO = 0.197.",
                "These values account reasonably well for the weights of different features in the discussion of the reviewer.",
                "One point to note is that some terms in the lists Lf possess an inherent semantic orientation.",
                "For example the word grime (belonging to the list LC ) would be used most often to assert the presence, and not the absence of grime.",
                "This is unavoidable, but care was taken to ensure words from both sides of the spectrum were used.",
                "For this reason, some lists such as LR contain only nouns of objects that one would typically describe in a room (see Appendix A).",
                "The goal of this section is to analyse the influence of the weights wi f on the numerical ratings ri f .",
                "Intuitively, users who spent a lot of their time discussing a feature f (i.e., wi f is high) had something to say about their experience with regard to this feature.",
                "Obviously, feature f is important for user i.",
                "Since people tend to be more knowledgeable in the aspects they consider important, our hypothesis is that the ratings ri f (corresponding to high weights wi f ) constitute a subset of expert ratings for feature f. Figure 2 plots the distribution of the rates r i(h) C with respect to the weights w i(h) C for the cleanliness of a Las Vegas hotel, h. Here, the high ratings are restricted to the reviews that discuss little the cleanliness.",
                "Whenever cleanliness appears in the discussion, the ratings are low.",
                "Many hotels exhibit similar rating patterns for various features.",
                "Ratings corresponding to low weights span the whole spectrum from 1 to 5, while the ratings corresponding to high weights are more grouped together (either around good or bad ratings).",
                "We therefore make the following hypothesis: Hypothesis 1.",
                "The ratings ri f corresponding to the reviews where wi f is high, are more similar to each other than to the overall collection of ratings.",
                "To test the hypothesis, we take the entire set of reviews, and feature by feature, we compute the standard deviation of the ratings with high weights, and the standard deviation of the entire set of ratings.",
                "High weights were defined as those belonging to the upper 20% of the weight range for the corresponding feature.",
                "If Hypothesis 1 were true, the standard deviation of all ratings should be higher than the standard deviation of the ratings with high weights. 0 1 2 3 4 5 6 0 0.1 0.2 0.3 0.4 0.5 0.6 Rating Weight Figure 2: The distribution of ratings against the weight of the cleanliness feature.",
                "We use a standard T-test to measure the significance of the results.",
                "City by city and feature by feature, Table 2 presents the average standard deviation of all ratings, and the average standard deviation of ratings with high weights.",
                "Indeed, the ratings with high weights have lower standard deviation, and the results are significant at the standard 0.05 significance threshold (although for certain cities taken independently there doesnt seem to be a significant difference, the results are significant for the entire data set).",
                "Please note that only the features O,R,S,C and V were considered, since for the others (F, L, and N) we didnt have enough ratings.",
                "Table 2: Average standard deviation for all ratings, and average standard deviation for ratings with high weights.",
                "In square brackets, the corresponding p-values for a positive difference between the two.",
                "City O R S C V all 1.189 0.998 1.144 0.935 1.123 Boston high 0.948 0.778 0.954 0.767 0.891 p-val [0.000] [0.004] [0.045] [0.080] [0.009] all 1.040 0.832 1.101 0.847 0.963 Sydney high 0.801 0.618 0.691 0.690 0.798 p-val [0.012] [0.023] [0.000] [0.377] [0.037] all 1.272 1.142 1.184 1.119 1.242 Vegas high 1.072 0.752 1.169 0.907 1.003 p-val [0.0185] [0.001] [0.918] [0.120] [0.126] Hypothesis 1 not only provides some basic understanding regarding the rating behavior of online users, it also suggests some ways of computing better quality estimates.",
                "We can, for example, construct a feature-by-feature quality estimate with much lower variance: for each feature we take the subset of reviews that amply discuss that feature, and output as a quality estimate the average rating for this subset.",
                "Initial experiments suggest that the average feature-by-feature ratings computed in this way are different from the average ratings computed on the whole data set.",
                "Given that, indeed, high weights are indicators of expert opinions, the estimates obtained in this way are more accurate than the current ones.",
                "Nevertheless, the validation of this underlying assumption requires further controlled experiments. 137 4.",
                "THE INFLUENCE OF PAST RATINGS Two important assumptions are generally made about reviews submitted to online forums.",
                "The first is that ratings truthfully reflect the quality observed by the users; the second is that reviews are independent from one another.",
                "While anecdotal evidence [9, 22] challenges the first assumption3 , in this section, we address the second.",
                "A perusal of online reviews shows that reviews are often part of discussion threads, where users make an effort to contradict, or vehemently agree with the remarks of previous users.",
                "Consider, for example, the following review: I dont understand the negative reviews... the hotel was a little dark, but that was the style.",
                "It was very artsy.",
                "Yes it was close to the freeway, but in my opinion the sound of an occasional loud car is better than hearing the ding ding of slot machines all night!",
                "The staff on-hand is FABULOUS.",
                "The waitresses are great (and *** does not deserve the bad review she got, she was 100% attentive to us! ), the bartenders are friendly and professional at the same time...",
                "Here, the user was disturbed by previous negative reports, addressed these concerns, and set about trying to correct them.",
                "Not surprisingly, his ratings were considerably higher than the average ratings up to this point.",
                "It seems that TripAdvisor users regularly read the reports submitted by previous users before booking a hotel, or before writing a review.",
                "Past reviews create some prior expectation regarding the quality of service, and this expectation has an influence on the submitted review.",
                "We believe this observation holds for most online forums.",
                "The subjective perception of quality is directly proportional to how well the actual experience meets the prior expectation, a fact confirmed by an important line of econometric and marketing research [17, 18, 16, 21].",
                "The correlation between the reviews has also been confirmed by recent research on the dynamics of online review forums [6]. 4.1 Prior Expectations We define the prior expectation of user i regarding the feature f, as the average of the previously available ratings on the feature f4 : ef (i) = j<i,r j f =0 rj f j<i,r j f =0 1 As a first hypothesis, we assert that the rating ri f is a function of the prior expectation ef (i): Hypothesis 2.",
                "For a given hotel and feature, given the reviews i and j such that ef (i) is high and ef (j) is low, the rating rj f exceeds the rating ri f .",
                "We define high and low expectations as those that are above, respectively below a certain cutoff value θ.",
                "The set of reviews preceded by high, respectively low expectations 3 part of Amazon reviews were recognized as strategic posts by book authors or competitors 4 if no previous ratings were assigned for feature f, ef (i) is assigned a default value of 4.",
                "Table 3: Average ratings for reviews preceded by low (first value in the cell) and high (second value in the cell) expectations.",
                "The P-values for a positive difference are given square brackets.",
                "City O R S C V 3.953 4.045 3.985 4.252 3.946 Boston 3.364 3.590 3.485 3.641 3.242 [0.011] [0.028] [0.0086] [0.0168] [0.0034] 4.284 4.358 4.064 4.530 4.428 Sydney 3.756 3.537 3.436 3.918 3.495 [0.000] [0.000] [0.035] [0.009] [0.000] 3.494 3.674 3.713 3.689 3.580 Las Vegas 3.140 3.530 2.952 3.530 3.351 [0.190] [0.529] [0.007] [0.529] [0.253] are defined as follows: Rhigh f = {ri f |ef (i) > θ} Rlow f = {ri f |ef (i) < θ} These sets are specific for each (hotel, feature) pair, and in our experiments we took θ = 4.",
                "This rather high value is close to the average rating across all features across all hotels, and is justified by the fact that our data set contains mostly high quality hotels.",
                "For each city, we take all hotels and compute the average ratings in the sets Rhigh f and Rlow f (see Table 3).",
                "The average rating amongst reviews following low prior expectations is significantly higher than the average rating following high expectations.",
                "As further evidence, we consider all hotels for which the function eV (i) (the expectation for the feature Value) has a high value (greater than 4) for some i, and a low value (less than 4) for some other i.",
                "Intuitively, these are the hotels for which there is a minimal degree of variation in the timely sequence of reviews: i.e., the cumulative average of ratings was at some point high and afterwards became low, or vice-versa.",
                "Such variations are observed for about half of all hotels in each city.",
                "Figure 3 plots the median (across considered hotels) rating, rV , when ef (i) is not more than x but greater than x − 0.5. 2.5 3 3.5 4 4.5 5 2.5 3 3.5 4 4.5 5 Medianofrating expectation Boston Sydney Vegas Figure 3: The ratings tend to decrease as the expectation increases. 138 There are two ways to interpret the function ef (i): • The expected value for feature f obtained by user i before his experience with the service, acquired by reading reports submitted by past users.",
                "In this case, an overly high value for ef (i) would drive the user to submit a negative report (or vice versa), stemming from the difference between the actual value of the service, and the inflated expectation of this value acquired before his experience. • The expected value of feature f for all subsequent visitors of the site, if user i were not to submit a report.",
                "In this case, the motivation for a negative report following an overly high value of ef is different: user i seeks to correct the expectation of future visitors to the site.",
                "Unlike the interpretation above, this does not require the user to derive an a priori expectation for the value of f. Note that neither interpretation implies that the average up to report i is inversely related to the rating at report i.",
                "There might exist a measure of influence exerted by past reports that pushes the user behind report i to submit ratings which to some extent conforms with past reports: a low value for ef (i) can influence user i to submit a low rating for feature f because, for example, he fears that submitting a high rating will make him out to be a person with low standards5 .",
                "This, at first, appears to contradict Hypothesis 2.",
                "However, this conformity rating cannot continue indefinitely: once the set of reports project a sufficiently deflated estimate for vf , future reviewers with comparatively positive impressions will seek to correct this misconception. 4.2 Impact of textual comments on quality expectation Further insight into the rating behavior of TripAdvisor users can be obtained by analyzing the relationship between the weights wf and the values ef (i).",
                "In particular, we examine the following hypothesis: Hypothesis 3.",
                "When a large proportion of the text of a review discusses a certain feature, the difference between the rating for that feature and the average rating up to that point tends to be large.",
                "The intuition behind this claim is that when the user is adamant about voicing his opinion regarding a certain feature, his opinion differs from the collective opinion of previous postings.",
                "This relies on the characteristic of reputation systems as feedback forums where a user is interested in projecting his opinion, with particular strength if this opinion differs from what he perceives to be the general opinion.",
                "To test Hypothesis 3 we measure the average absolute difference between the expectation ef (i) and the rating ri f when the weight wi f is high, respectively low.",
                "Weights are classified high or low by comparing them with certain cutoff values: wi f is low if smaller than 0.1, while wi f is high if greater than θf .",
                "Different cutoff values were used for different features: θR = 0.4, θS = 0.4, θC = 0.2, and θV = 0.7.",
                "Cleanliness has a lower cutoff since it is a feature rarely discussed; Value has a high cutoff for the opposite reason.",
                "Results are presented in Table 4. 5 The idea that negative reports can encourage further negative reporting has been suggested before [14] Table 4: Average of |ri f −ef (i)| when weights are high (first value in the cell) and low (second value in the cell) with P-values for the difference in sq. brackets.",
                "City R S C V 1.058 1.208 1.728 1.356 Boston 0.701 0.838 0.760 0.917 [0.022] [0.063] [0.000] [0.218] 1.048 1.351 1.218 1.318 Sydney 0.752 0.759 0.767 0.908 [0.179] [0.009] [0.165] [0.495] 1.184 1.378 1.472 1.642 Las Vegas 0.772 0.834 0.808 1.043 [0.071] [0.020] [0.006] [0.076] This demonstrates that when weights are unusually high, users tend to express an opinion that does not conform to the net average of previous ratings.",
                "As we might expect, for a feature that rarely was a high weight in the discussion, (e.g., cleanliness) the difference is particularly large.",
                "Even though the difference in the feature Value is quite large for Sydney, the P-value is high.",
                "This is because only few reviews discussed value heavily.",
                "The reason could be cultural or because there was less of a reason to discuss this feature. 4.3 Reporting Incentives Previous models suggest that users who are not highly opinionated will not choose to voice their opinions [12].",
                "In this section, we extend this model to account for the influence of expectations.",
                "The motivation for submitting feedback is not only due to extreme opinions, but also to the difference between the current reputation (i.e., the prior expectation of the user) and the actual experience.",
                "Such a rating model produces ratings that most of the time deviate from the current average rating.",
                "The ratings that confirm the prior expectation will rarely be submitted.",
                "We test on our data set the proportion of ratings that attempt to correct the current estimate.",
                "We define a deviant rating as one that deviates from the current expectation by at least some threshold θ, i.e., |ri f − ef (i)| ≥ θ.",
                "For each of the three considered cities, the following tables, show the proportion of deviant ratings for θ = 0.5 and θ = 1.",
                "Table 5: Proportion of deviant ratings with θ = 0.5 City O R S C V Boston 0.696 0.619 0.676 0.604 0.684 Sydney 0.645 0.615 0.672 0.614 0.675 Las Vegas 0.721 0.641 0.694 0.662 0.724 Table 6: Proportion of deviant ratings with θ = 1 City O R S C V Boston 0.420 0.397 0.429 0.317 0.446 Sydney 0.360 0.367 0.442 0.336 0.489 Las Vegas 0.510 0.421 0.483 0.390 0.472 The above results suggest that a large proportion of users (close to one half, even for the high threshold value θ = 1) deviate from the prior average.",
                "This reinforces the idea that users are more likely to submit a report when they believe they have something distinctive to add to the current stream of opinions for some feature.",
                "Such conclusions are in total agreement with prior evidence that the distribution of reports often follows bi-modal, U-shaped distributions. 139 5.",
                "MODELLING THE BEHAVIOR OF RATERS To account for the observations described in the previous sections, we propose a model for the behavior of the users when submitting online reviews.",
                "For a given hotel, we make the assumption that the quality experienced by the users is normally distributed around some value vf , which represents the objective quality offered by the hotel on the feature f. The rating submitted by user i on feature f is: ˆri f = δf vi f + (1 − δf ) · sign vi f − ef (i) c + d(vi f , ef (i)|wi f ) (2) where: • vi f is the (unknown) quality actually experienced by the user. vi f is assumed normally distributed around some value vf ; • δf ∈ [0, 1] can be seen as a measure of the bias when reporting feedback.",
                "High values reflect the fact that users rate objectively, without being influenced by prior expectations.",
                "The value of δf may depend on various factors; we fix one value for each feature f; • c is a constant between 1 and 5; • wi f is the weight of feature f in the textual comment of review i, computed according to Eq. (1); • d(vi f , ef (i)|wi f ) is a distance function between the expectation and the observation of user i.",
                "The distance function satisfies the following properties: - d(y, z|w) ≥ 0 for all y, z ∈ [0, 5], w ∈ [0, 1]; - |d(y, z|w)| < |d(z, x|w)| if |y − z| < |z − x|; - |d(y, z|w1)| < |d(y, z|w2)| if w1 < w2; - c + d(vf , ef (i)|wi f ) ∈ [1, 5]; The second term of Eq. (2) encodes the bias of the rating.",
                "The higher the distance between the true observation vi f and the function ef , the higher the bias. 5.1 Model Validation We use the data set of TripAdvisor reviews to validate the behavior model presented above.",
                "We split for convenience the rating values in three ranges: bad (B = {1, 2}), indifferent (I = {3, 4}), and good (G = {5}), and perform the following two tests: • First, we will use our model to predict the ratings that have extremal values.",
                "For every hotel, we take the sequence of reports, and whenever we encounter a rating that is either good or bad (but not indifferent) we try to predict it using Eq. (2) • Second, instead of predicting the value of extremal ratings, we try to classify them as either good or bad.",
                "For every hotel we take the sequence of reports, and for each report (regardless of it value) we classify it as being good or bad However, to perform these tests, we need to estimate the objective value, vf , that is the average of the true quality observations, vi f .",
                "The algorithm we are using is based on the intuition that the amount of conformity rating is minimized.",
                "In other words, the value vf should be such that as often as possible, bad ratings follow expectations above vf and good ratings follow expectations below vf .",
                "Formally, we define the sets: Γ1 = {i|ef (i) < vf and ri f ∈ B}; Γ2 = {i|ef (i) > vf and ri f ∈ G}; that correspond to irregularities where even though the expectation at point i is lower than the delivered value, the rating is poor, and vice versa.",
                "We define vf as the value that minimize these union of the two sets: vf = arg min vf |Γ1 ∪ Γ2| (3) In Eq. (2) we replace vi f by the value vf computed in Eq. (3), and use the following distance function: d(vf , ef (i)|wi f ) = |vf − ef (i)| vf − ef (i) |vf 2 − ef (i)2 | · (1 + 2wi f ); The constant c ∈ I was set to min{max{ef (i), 3}}, 4}.",
                "The values for δf were fixed at {0.7, 0.7, 0.8, 0.7, 0.6} for the features {Overall, Rooms, Service, Cleanliness, Value} respectively.",
                "The weights are computed as described in Section 3.",
                "As a first experiment, we take the sets of extremal ratings {ri f |ri f /∈ I} for each hotel and feature.",
                "For every such rating, ri f , we try to estimate it by computing ˆri f using Eq. (2).",
                "We compare this estimator with the one obtained by simply averaging the ratings over all hotels and features: i.e., ¯rf = j,r j f =0 rj f j,r j f =0 1 ; Table 7 presents the ratio between the root mean square error (RMSE) when using ˆri f and ¯rf to estimate the actual ratings.",
                "In all cases the estimate produced by our model is better than the simple average.",
                "Table 7: Average of RMSE(ˆrf ) RMSE(¯rf ) City O R S C V Boston 0.987 0.849 0.879 0.776 0.913 Sydney 0.927 0.817 0.826 0.720 0.681 Las Vegas 0.952 0.870 0.881 0.947 0.904 As a second experiment, we try to distinguish the sets Bf = {i|ri f ∈ B} and Gf = {i|ri f ∈ G} of bad, respectively good ratings on the feature f. For example, we compute the set Bf using the following classifier (called σ): ri f ∈ Bf (σf (i) = 1) ⇔ ˆri f ≤ 4; Tables 8, 9 and 10 present the Precision(p), Recall(r) and s = 2pr p+r for classifier σ, and compares it with a naive majority classifier, τ, τf (i) = 1 ⇔ |Bf | ≥ |Gf |: We see that recall is always higher for σ and precision is usually slightly worse.",
                "For the s metric σ tends to add a 140 Table 8: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Boston O R S C V p 0.678 0.670 0.573 0.545 0.610 σ r 0.626 0.659 0.619 0.612 0.694 s 0.651 0.665 0.595 0.577 0.609 p 0.684 0.706 0.647 0.611 0.633 τ r 0.597 0.541 0.410 0.383 0.562 s 0.638 0.613 0.502 0.471 0.595 Table 9: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Las Vegas O R S C V p 0.654 0.748 0.592 0.712 0.583 σ r 0.608 0.536 0.791 0.474 0.610 s 0.630 0.624 0.677 0.569 0.596 p 0.685 0.761 0.621 0.748 0.606 τ r 0.542 0.505 0.767 0.445 0.441 s 0.605 0.607 0.670 0.558 0.511 1-20% improvement over τ, much higher in some cases for hotels in Sydney.",
                "This is likely because Sydney reviews are more positive than those of the American cities and cases where the number of bad reviews exceeded the number of good ones are rare.",
                "Replacing the test algorithm with one that plays a 1 with probability equal to the proportion of bad reviews improves its results for this city, but it is still outperformed by around 80%. 6.",
                "SUMMARY OF RESULTS AND CONCLUSION The goal of this paper is to explore the factors that drive a user to submit a particular rating, rather than the incentives that encouraged him to submit a report in the first place.",
                "For that we use two additional sources of information besides the vector of numerical ratings: first we look at the textual comments that accompany the reviews, and second we consider the reports that have been previously submitted by other users.",
                "Using simple natural language processing algorithms, we were able to establish a correlation between the weight of a certain feature in the textual comment accompanying the review, and the noise present in the numerical rating.",
                "Specifically, it seems that users who discuss amply a certain feature are likely to agree on a common rating.",
                "This observation allows the construction of feature-by-feature estimators of quality that have a lower variance, and are hopefully less noisy.",
                "Nevertheless, further evidence is required to support the intuition that ratings corresponding to high weights are expert opinions that deserve to be given higher priority when computing estimates of quality.",
                "Second, we emphasize the dependence of ratings on previous reports.",
                "Previous reports create an expectation of quality which affects the subjective perception of the user.",
                "We validate two facts about the hotel reviews we collected from TripAdvisor: First, the ratings following low expectations (where the expectation is computed as the average of the previous reports) are likely to be higher than the ratings Table 10: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Sydney O R S C V p 0.650 0.463 0.544 0.550 0.580 σ r 0.234 0.378 0.571 0.169 0.592 s 0.343 0.452 0.557 0.259 0.586 p 0.562 0.615 0.600 0.500 0.600 τ r 0.054 0.098 0.101 0.015 0.175 s 0.098 0.168 0.172 0.030 0.271 following high expectations.",
                "Intuitively, the perception of quality (and consequently the rating) depends on how well the actual experience of the user meets her expectation.",
                "Second, we include evidence from the textual comments, and find that when users devote a large fraction of the text to discussing a certain feature, they are likely to motivate a divergent rating (i.e., a rating that does not conform to the prior expectation).",
                "Intuitively, this supports the hypothesis that review forums act as discussion groups where users are keen on presenting and motivating their own opinion.",
                "We have captured the empirical evidence in a behavior model that predicts the ratings submitted by the users.",
                "The final rating depends, as expected, on the true observation, and on the gap between the observation and the expectation.",
                "The gap tends to have a bigger influence when an important fraction of the textual comment is dedicated to discussing a certain feature.",
                "The proposed model was validated on the empirical data and provides better estimates of the ratings actually submitted.",
                "One assumption that we make is about the existence of an objective quality value vf for the feature f. This is rarely true, especially over large spans of time.",
                "Other explanations might account for the correlation of ratings with past reports.",
                "For example, if ef (i) reflects the true value of f at a point in time, the difference in the ratings following high and low expectations can be explained by hotel revenue models that are maximized when the value is modified accordingly.",
                "However, the idea that variation in ratings is not primarily a function of variation in value turns out to be a useful one.",
                "Our approach to approximate this elusive objective value is by no means perfect, but conforms neatly to the idea behind the model.",
                "A natural direction for future work is to examine concrete applications of our results.",
                "Significant improvements of quality estimates are likely to be obtained by incorporating all empirical evidence about rating behavior.",
                "Exactly how different factors affect the decisions of the users is not clear.",
                "The answer might depend on the particular application, context and culture. 7.",
                "REFERENCES [1] A. Admati and P. Pfleiderer.",
                "Noisytalk.com: Broadcasting opinions in a noisy environment.",
                "Working Paper 1670R, Stanford University, 2000. [2] P. B., L. Lee, and S. Vaithyanathan.",
                "Thumbs up? sentiment classification using machine learning techniques.",
                "In Proceedings of the EMNLP-02, the Conference on Empirical Methods in Natural Language Processing, 2002. [3] H. Cui, V. Mittal, and M. Datar.",
                "Comparative 141 Experiments on Sentiment Classification for Online Product Reviews.",
                "In Proceedings of AAAI, 2006. [4] K. Dave, S. Lawrence, and D. Pennock.",
                "Mining the peanut gallery:opinion extraction and semantic classification of product reviews.",
                "In Proceedings of the 12th International Conference on the World Wide Web (WWW03), 2003. [5] C. Dellarocas, N. Awad, and X. Zhang.",
                "Exploring the Value of Online Product Ratings in Revenue Forecasting: The Case of Motion Pictures.",
                "Working paper, 2006. [6] C. Forman, A. Ghose, and B. Wiesenfeld.",
                "A Multi-Level Examination of the Impact of Social Identities on Economic Transactions in Electronic Markets.",
                "Available at SSRN: http://ssrn.com/abstract=918978, July 2006. [7] A. Ghose, P. Ipeirotis, and A. Sundararajan.",
                "Reputation Premiums in Electronic Peer-to-Peer Markets: Analyzing Textual Feedback and Network Structure.",
                "In Third Workshop on Economics of Peer-to-Peer Systems, (P2PECON), 2005. [8] A. Ghose, P. Ipeirotis, and A. Sundararajan.",
                "The Dimensions of Reputation in electronic Markets.",
                "Working Paper CeDER-06-02, New York University, 2006. [9] A. Harmon.",
                "Amazon Glitch Unmasks War of Reviewers.",
                "The New York Times, February 14, 2004. [10] D. Houser and J. Wooders.",
                "Reputation in Auctions: Theory and Evidence from eBay.",
                "Journal of Economics and Management Strategy, 15:353-369, 2006. [11] M. Hu and B. Liu.",
                "Mining and summarizing customer reviews.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD04), 2004. [12] N. Hu, P. Pavlou, and J. Zhang.",
                "Can Online Reviews Reveal a Products True Quality?",
                "In Proceedings of ACM Conference on Electronic Commerce (EC 06), 2006. [13] K. Kalyanam and S. McIntyre.",
                "Return on reputation in online auction market.",
                "Working Paper 02/03-10-WP, Leavey School of Business, Santa Clara University., 2001. [14] L. Khopkar and P. Resnick.",
                "Self-Selection, Slipping, Salvaging, Slacking, and Stoning: the Impacts of Negative Feedback at eBay.",
                "In Proceedings of ACM Conference on Electronic Commerce (EC 05), 2005. [15] M. Melnik and J. Alm.",
                "Does a sellers reputation matter? evidence from ebay auctions.",
                "Journal of Industrial Economics, 50(3):337-350, 2002. [16] R. Olshavsky and J. Miller.",
                "Consumer Expectations, Product Performance and Perceived Product Quality.",
                "Journal of Marketing Research, 9:19-21, February 1972. [17] A. Parasuraman, V. Zeithaml, and L. Berry.",
                "A Conceptual Model of Service Quality and Its Implications for Future Research.",
                "Journal of Marketing, 49:41-50, 1985. [18] A. Parasuraman, V. Zeithaml, and L. Berry.",
                "SERVQUAL: A Multiple-Item Scale for Measuring Consumer Perceptions of Service Quality.",
                "Journal of Retailing, 64:12-40, 1988. [19] P. Pavlou and A. Dimoka.",
                "The Nature and Role of Feedback Text Comments in Online Marketplaces: Implications for Trust Building, Price Premiums, and Seller Differentiation.",
                "Information Systems Research, 17(4):392-414, 2006. [20] A. Popescu and O. Etzioni.",
                "Extracting product features and opinions from reviews.",
                "In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, 2005. [21] R. Teas.",
                "Expectations, Performance Evaluation, and Consumers Perceptions of Quality.",
                "Journal of Marketing, 57:18-34, 1993. [22] E. White.",
                "Chatting a Singer Up the Pop Charts.",
                "The Wall Street Journal, October 15, 1999.",
                "APPENDIX A.",
                "LIST OF WORDS, LR, ASSOCIATED TO THE FEATURE ROOMS All words serve as prefixes: room, space, interior, decor, ambiance, atmosphere, comfort, bath, toilet, bed, building, wall, window, private, temperature, sheet, linen, pillow, hot, water, cold, water, shower, lobby, furniture, carpet, air, condition, mattress, layout, design, mirror, ceiling, lighting, lamp, sofa, chair, dresser, wardrobe, closet 142"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "absence of clear incentive": {
            "translated_key": "ausencia de incentivos claros",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Understanding User Behavior in Online Feedback Reporting Arjun Talwar Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland arjun@math.stanford.edu Radu Jurca Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland radu.jurca@epfl.ch Boi Faltings Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland boi.faltings@epfl.ch ABSTRACT Online reviews have become increasingly popular as a way to judge the quality of various products and services.",
                "Previous work has demonstrated that contradictory reporting and underlying user biases make judging the true worth of a service difficult.",
                "In this paper, we investigate underlying factors that influence user behavior when reporting feedback.",
                "We look at two sources of information besides numerical ratings: linguistic evidence from the textual comment accompanying a review, and patterns in the time sequence of reports.",
                "We first show that groups of users who amply discuss a certain feature are more likely to agree on a common rating for that feature.",
                "Second, we show that a users rating partly reflects the difference between true quality and prior expectation of quality as inferred from previous reviews.",
                "Both give us a less noisy way to produce rating estimates and reveal the reasons behind user bias.",
                "Our hypotheses were validated by statistical evidence from hotel reviews on the TripAdvisor website.",
                "Categories and Subject Descriptors J.4 [Social and Behavioral Sciences]: Economics General Terms Economics, Experimentation, Reliability 1.",
                "MOTIVATIONS The spread of the internet has made it possible for online feedback forums (or reputation mechanisms) to become an important channel for Word-of-mouth regarding products, services or other types of commercial interactions.",
                "Numerous empirical studies [10, 15, 13, 5] show that buyers seriously consider online feedback when making purchasing decisions, and are willing to pay reputation premiums for products or services that have a good reputation.",
                "Recent analysis, however, raises important questions regarding the ability of existing forums to reflect the real quality of a product.",
                "In the <br>absence of clear incentive</br>s, users with a moderate outlook will not bother to voice their opinions, which leads to an unrepresentative sample of reviews.",
                "For example, [12, 1] show that Amazon1 ratings of books or CDs follow with great probability bi-modal, U-shaped distributions where most of the ratings are either very good, or very bad.",
                "Controlled experiments, on the other hand, reveal opinions on the same items that are normally distributed.",
                "Under these circumstances, using the arithmetic mean to predict quality (as most forums actually do) gives the typical user an estimator with high variance that is often false.",
                "Improving the way we aggregate the information available from online reviews requires a deep understanding of the underlying factors that bias the rating behavior of users.",
                "Hu et al. [12] propose the Brag-and-Moan Model where users rate only if their utility of the product (drawn from a normal distribution) falls outside a median interval.",
                "The authors conclude that the model explains the empirical distribution of reports, and offers insights into smarter ways of estimating the true quality of the product.",
                "In the present paper we extend this line of research, and attempt to explain further facts about the behavior of users when reporting online feedback.",
                "Using actual hotel reviews from the TripAdvisor2 website, we consider two additional sources of information besides the basic numerical ratings submitted by users.",
                "The first is simple linguistic evidence from the textual review that usually accompanies the numerical ratings.",
                "We use text-mining techniques similar to [7] and [3], however, we are only interested in identifying what aspects of the service the user is discussing, without computing the semantic orientation of the text.",
                "We find that users who comment more on the same feature are more likely to agree on a common numerical rating for that particular feature.",
                "Intuitively, lengthy comments reveal the importance of the feature to the user.",
                "Since people tend to be more knowledgeable in the aspects they consider important, users who discuss a given feature in more details might be assumed to have more authority in evaluating that feature.",
                "Second we investigate the relationship between a review 1 http://www.amazon.com 2 http://www.tripadvisor.com/ 134 Figure 1: The TripAdvisor page displaying reviews for a popular Boston hotel.",
                "Name of hotel and advertisements were deliberatively erased. and the reviews that preceded it.",
                "A perusal of online reviews shows that ratings are often part of discussion threads, where one post is not necessarily independent of other posts.",
                "One may see, for example, users who make an effort to contradict, or vehemently agree with, the remarks of previous users.",
                "By analyzing the time sequence of reports, we conclude that past reviews influence the future reports, as they create some prior expectation regarding the quality of service.",
                "The subjective perception of the user is influenced by the gap between the prior expectation and the actual performance of the service [17, 18, 16, 21] which will later reflect in the users rating.",
                "We propose a model that captures the dependence of ratings on prior expectations, and validate it using the empirical data we collected.",
                "Both results can be used to improve the way reputation mechanisms aggregate the information from individual reviews.",
                "Our first result can be used to determine a featureby-feature estimate of quality, where for each feature, a different subset of reviews (i.e., those with lengthy comments of that feature) is considered.",
                "The second leads to an algorithm that outputs a more precise estimate of the real quality. 2.",
                "THE DATA SET We use in this paper real hotel reviews collected from the popular travel site TripAdvisor.",
                "TripAdvisor indexes hotels from cities across the world, along with reviews written by travelers.",
                "Users can search the site by giving the hotels name and location (optional).",
                "The reviews for a given hotel are displayed as a list (ordered from the most recent to the oldest), with 5 reviews per page.",
                "The reviews contain: • information about the author of the review (e.g., dates of stay, username of the reviewer, location of the reviewer); • the overall rating (from 1, lowest, to 5, highest); • a textual review containing a title for the review, free comments, and the main things the reviewer liked and disliked; • numerical ratings (from 1, lowest, to 5, highest) for different features (e.g., cleanliness, service, location, etc.)",
                "Below the name of the hotel, TripAdvisor displays the address of the hotel, general information (number of rooms, number of stars, short description, etc), the average overall rating, the TripAdvisor ranking, and an average rating for each feature.",
                "Figure 1 shows the page for a popular Boston hotel whose name (along with advertisements) was explicitly erased.",
                "We selected three cities for this study: Boston, Sydney and Las Vegas.",
                "For each city we considered all hotels that had at least 10 reviews, and recorded all reviews.",
                "Table 1 presents the number of hotels considered in each city, the total number of reviews recorded for each city, and the distribution of hotels with respect to the star-rating (as available on the TripAdvisor site).",
                "Note that not all hotels have a star-rating.",
                "Table 1: A summary of the data set.",
                "City # Reviews # Hotels # of Hotels with 1,2,3,4 & 5 stars Boston 3993 58 1+3+17+15+2 Sydney 1371 47 0+0+9+13+10 Las Vegas 5593 40 0+3+10+9+6 For each review we recorded the overall rating, the textual review (title and body of the review) and the numerical rating on 7 features: Rooms(R), Service(S), Cleanliness(C), Value(V), Food(F), Location(L) and Noise(N).",
                "TripAdvisor does not require users to submit anything other than the overall rating, hence a typical review rates few additional features, regardless of the discussion in the textual comment.",
                "Only the features Rooms(R), Service(S), Cleanliness(C) and Value(V) are rated by a significant number of users.",
                "However, we also selected the features Food(F), Location(L) and Noise(N) because they are referred to in a significant number of textual comments.",
                "For each feature we record the numerical rating given by the user, or 0 when the rating is missing.",
                "The typical length of the textual comment amounts to approximately 200 words.",
                "All data was collected by crawling the TripAdvisor site in September 2006. 2.1 Formal notation We will formally refer to a review by a tuple (r, T) where: • r = (rf ) is a vector containing the ratings rf ∈ {0, 1, . . . 5} for the features f ∈ F = {O, R, S, C, V, F, L, N}; note that the overall rating, rO, is abusively recorded as the rating for the feature Overall(O); • T is the textual comment that accompanies the review. 135 Reviews are indexed according to the variable i, such that (ri , Ti ) is the ith review in our database.",
                "Since we dont record the username of the reviewer, we will also say that the ith review in our data set was submitted by user i.",
                "When we need to consider only the reviews of a given hotel, h, we will use (ri(h) , Ti(h) ) to denote the ith review about the hotel h. 3.",
                "EVIDENCE FROM TEXTUAL COMMENTS The free textual comments associated to online reviews are a valuable source of information for understanding the reasons behind the numerical ratings left by the reviewers.",
                "The text may, for example, reveal concrete examples of aspects that the user liked or disliked, thus justifying some of the high, respectively low ratings for certain features.",
                "The text may also offer guidelines for understanding the preferences of the reviewer, and the weights of different features when computing an overall rating.",
                "The problem, however, is that free textual comments are difficult to read.",
                "Users are required to scroll through many reviews and read mostly repetitive information.",
                "Significant improvements would be obtained if the reviews were automatically interpreted and aggregated.",
                "Unfortunately, this seems a difficult task for computers since human users often use witty language, abbreviations, cultural specific phrases, and the figurative style.",
                "Nevertheless, several important results use the textual comments of online reviews in an automated way.",
                "Using well established natural language techniques, reviews or parts of reviews can be classified as having a positive or negative semantic orientation.",
                "Pang et al. [2] classify movie reviews into positive/negative by training three different classifiers (Naive Bayes, Maximum Entropy and SVM) using classification features based on unigrams, bigrams or part-of-speech tags.",
                "Dave et al. [4] analyze reviews from CNet and Amazon, and surprisingly show that classification features based on unigrams or bigrams perform better than higher-order n-grams.",
                "This result is challenged by Cui et al. [3] who look at large collections of reviews crawled from the web.",
                "They show that the size of the data set is important, and that bigger training sets allow classifiers to successfully use more complex classification features based on n-grams.",
                "Hu and Liu [11] also crawl the web for product reviews and automatically identify product attributes that have been discussed by reviewers.",
                "They use Wordnet to compute the semantic orientation of product evaluations and summarize user reviews by extracting positive and negative evaluations of different product features.",
                "Popescu and Etzioni [20] analyze a similar setting, but use search engine hit-counts to identify product attributes; the semantic orientation is assigned through the relaxation labeling technique.",
                "Ghose et al. [7, 8] analyze seller reviews from the Amazon secondary market to identify the different dimensions (e.g., delivery, packaging, customer support, etc.) of reputation.",
                "They parse the text, and tag the part-of-speech for each word.",
                "Frequent nouns, noun phrases and verbal phrases are identified as dimensions of reputation, while the corresponding modifiers (i.e., adjectives and adverbs) are used to derive numerical scores for each dimension.",
                "The enhanced reputation measure correlates better with the pricing information observed in the market.",
                "Pavlou and Dimoka [19] analyze eBay reviews and find that textual comments have an important impact on reputation premiums.",
                "Our approach is similar to the previously mentioned works, in the sense that we identify the aspects (i.e., hotel features) discussed by the users in the textual reviews.",
                "However, we do not compute the semantic orientation of the text, nor attempt to infer missing ratings.",
                "We define the weight, wi f , of feature f ∈ F in the text Ti associated with the review (ri , Ti ), as the fraction of Ti dedicated to discussing aspects (both positive and negative) related to feature f. We propose an elementary method to approximate the values of these weights.",
                "For each feature we manually construct the word list Lf containing approximately 50 words that are most commonly associated to the feature f. The initial words were selected from reading some of the reviews, and seeing what words coincide with discussion of which features.",
                "The list was then extended by adding all thesaurus entries that were related to the initial words.",
                "Finally, we brainstormed for missing words that would normally be associated with each of the features.",
                "Let Lf ∩Ti be the list of terms common to both Lf and Ti.",
                "Each term of Lf is counted the number of times it appears in Ti , with two exceptions: • in cases where the user submits a title to the review, we account for the title text by appending it three times to the review text Ti .",
                "The intuitive assumption is that the users opinion is more strongly reflected in the title, rather than in the body of the review.",
                "For example, many reviews are accurately summarized by titles such as Excellent service, terrible location or Bad value for money; • certain words that occur only once in the text are counted multiple times if their relevance to that feature is particularly strong.",
                "These were root words for each feature (e.g., staff is a root word for the feature Service), and were weighted either 2 or 3.",
                "Each feature was assigned up to 3 such root words, so almost all words are counted only once.",
                "The list of words for the feature Rooms is given for reference in Appendix A.",
                "The weight wi f is computed as: wi f = |Lf ∩ Ti| f∈F |Lf ∩ Ti| (1) where |Lf ∩Ti | is the number of terms common to Lf and Ti .",
                "The weight for the feature Overall was set to min{ |T i | 5000 , 1} where |Ti | is the number of character in Ti .",
                "The following is a TripAdvisor review for a Boston hotel (the name of the hotel is omitted): Ill start by saying that Im more of a Holiday Inn person than a *** type.",
                "So I get frustrated when I pay double the room rate and get half the amenities that Id get at a Hampton Inn or Holiday Inn.",
                "The location was definitely the main asset of this place.",
                "It was only a few blocks from the Hynes Center subway stop and it was easy to walk to some good restaurants in the Back Bay area.",
                "Boylston isnt far off at all.",
                "So I had no trouble with foregoing a rental car and taking the subway from the airport to the hotel and using the subway for any other travel.",
                "Otherwise, they make you pay for anything and everything. 136 And when youve already dropped $215/night on the room, that gets frustrating.The room itself was decent, about what I would expect.",
                "Staff was also average, not bad and not excellent.",
                "Again, I think youre paying for location and the ability to walk to a lot of good stuff.",
                "But I think next time Ill stay in Brookline, get more amenities, and use the subway a bit more.",
                "This numerical ratings associated to this review are rO = 3, rR = 3, rS = 3, rC = 4, rV = 2 for features Overall(O), Rooms(R), Service(S), Cleanliness(C) and Value(V) respectively.",
                "The ratings for the features Food(F), Location(L) and Noise(N) are absent (i.e., rF = rL = rN = 0).",
                "The weights wf are computed from the following lists of common terms: LR ∩ T ={room}; wR = 0.066 LS ∩ T ={3 * Staff, amenities}; wS = 0.267 LC ∩ T = ∅; wC = 0 LV ∩ T ={$, rate}; wV = 0.133 LF ∩ T ={restaurant}; wF = 0.067 LL ∩ T ={2 * center, 2 * walk, 2 * location, area}; wL = 0.467 LN ∩ T = ∅; wN = 0 The root words Staff and Center were tripled and doubled respectively.",
                "The overall weight of the textual review is wO = 0.197.",
                "These values account reasonably well for the weights of different features in the discussion of the reviewer.",
                "One point to note is that some terms in the lists Lf possess an inherent semantic orientation.",
                "For example the word grime (belonging to the list LC ) would be used most often to assert the presence, and not the absence of grime.",
                "This is unavoidable, but care was taken to ensure words from both sides of the spectrum were used.",
                "For this reason, some lists such as LR contain only nouns of objects that one would typically describe in a room (see Appendix A).",
                "The goal of this section is to analyse the influence of the weights wi f on the numerical ratings ri f .",
                "Intuitively, users who spent a lot of their time discussing a feature f (i.e., wi f is high) had something to say about their experience with regard to this feature.",
                "Obviously, feature f is important for user i.",
                "Since people tend to be more knowledgeable in the aspects they consider important, our hypothesis is that the ratings ri f (corresponding to high weights wi f ) constitute a subset of expert ratings for feature f. Figure 2 plots the distribution of the rates r i(h) C with respect to the weights w i(h) C for the cleanliness of a Las Vegas hotel, h. Here, the high ratings are restricted to the reviews that discuss little the cleanliness.",
                "Whenever cleanliness appears in the discussion, the ratings are low.",
                "Many hotels exhibit similar rating patterns for various features.",
                "Ratings corresponding to low weights span the whole spectrum from 1 to 5, while the ratings corresponding to high weights are more grouped together (either around good or bad ratings).",
                "We therefore make the following hypothesis: Hypothesis 1.",
                "The ratings ri f corresponding to the reviews where wi f is high, are more similar to each other than to the overall collection of ratings.",
                "To test the hypothesis, we take the entire set of reviews, and feature by feature, we compute the standard deviation of the ratings with high weights, and the standard deviation of the entire set of ratings.",
                "High weights were defined as those belonging to the upper 20% of the weight range for the corresponding feature.",
                "If Hypothesis 1 were true, the standard deviation of all ratings should be higher than the standard deviation of the ratings with high weights. 0 1 2 3 4 5 6 0 0.1 0.2 0.3 0.4 0.5 0.6 Rating Weight Figure 2: The distribution of ratings against the weight of the cleanliness feature.",
                "We use a standard T-test to measure the significance of the results.",
                "City by city and feature by feature, Table 2 presents the average standard deviation of all ratings, and the average standard deviation of ratings with high weights.",
                "Indeed, the ratings with high weights have lower standard deviation, and the results are significant at the standard 0.05 significance threshold (although for certain cities taken independently there doesnt seem to be a significant difference, the results are significant for the entire data set).",
                "Please note that only the features O,R,S,C and V were considered, since for the others (F, L, and N) we didnt have enough ratings.",
                "Table 2: Average standard deviation for all ratings, and average standard deviation for ratings with high weights.",
                "In square brackets, the corresponding p-values for a positive difference between the two.",
                "City O R S C V all 1.189 0.998 1.144 0.935 1.123 Boston high 0.948 0.778 0.954 0.767 0.891 p-val [0.000] [0.004] [0.045] [0.080] [0.009] all 1.040 0.832 1.101 0.847 0.963 Sydney high 0.801 0.618 0.691 0.690 0.798 p-val [0.012] [0.023] [0.000] [0.377] [0.037] all 1.272 1.142 1.184 1.119 1.242 Vegas high 1.072 0.752 1.169 0.907 1.003 p-val [0.0185] [0.001] [0.918] [0.120] [0.126] Hypothesis 1 not only provides some basic understanding regarding the rating behavior of online users, it also suggests some ways of computing better quality estimates.",
                "We can, for example, construct a feature-by-feature quality estimate with much lower variance: for each feature we take the subset of reviews that amply discuss that feature, and output as a quality estimate the average rating for this subset.",
                "Initial experiments suggest that the average feature-by-feature ratings computed in this way are different from the average ratings computed on the whole data set.",
                "Given that, indeed, high weights are indicators of expert opinions, the estimates obtained in this way are more accurate than the current ones.",
                "Nevertheless, the validation of this underlying assumption requires further controlled experiments. 137 4.",
                "THE INFLUENCE OF PAST RATINGS Two important assumptions are generally made about reviews submitted to online forums.",
                "The first is that ratings truthfully reflect the quality observed by the users; the second is that reviews are independent from one another.",
                "While anecdotal evidence [9, 22] challenges the first assumption3 , in this section, we address the second.",
                "A perusal of online reviews shows that reviews are often part of discussion threads, where users make an effort to contradict, or vehemently agree with the remarks of previous users.",
                "Consider, for example, the following review: I dont understand the negative reviews... the hotel was a little dark, but that was the style.",
                "It was very artsy.",
                "Yes it was close to the freeway, but in my opinion the sound of an occasional loud car is better than hearing the ding ding of slot machines all night!",
                "The staff on-hand is FABULOUS.",
                "The waitresses are great (and *** does not deserve the bad review she got, she was 100% attentive to us! ), the bartenders are friendly and professional at the same time...",
                "Here, the user was disturbed by previous negative reports, addressed these concerns, and set about trying to correct them.",
                "Not surprisingly, his ratings were considerably higher than the average ratings up to this point.",
                "It seems that TripAdvisor users regularly read the reports submitted by previous users before booking a hotel, or before writing a review.",
                "Past reviews create some prior expectation regarding the quality of service, and this expectation has an influence on the submitted review.",
                "We believe this observation holds for most online forums.",
                "The subjective perception of quality is directly proportional to how well the actual experience meets the prior expectation, a fact confirmed by an important line of econometric and marketing research [17, 18, 16, 21].",
                "The correlation between the reviews has also been confirmed by recent research on the dynamics of online review forums [6]. 4.1 Prior Expectations We define the prior expectation of user i regarding the feature f, as the average of the previously available ratings on the feature f4 : ef (i) = j<i,r j f =0 rj f j<i,r j f =0 1 As a first hypothesis, we assert that the rating ri f is a function of the prior expectation ef (i): Hypothesis 2.",
                "For a given hotel and feature, given the reviews i and j such that ef (i) is high and ef (j) is low, the rating rj f exceeds the rating ri f .",
                "We define high and low expectations as those that are above, respectively below a certain cutoff value θ.",
                "The set of reviews preceded by high, respectively low expectations 3 part of Amazon reviews were recognized as strategic posts by book authors or competitors 4 if no previous ratings were assigned for feature f, ef (i) is assigned a default value of 4.",
                "Table 3: Average ratings for reviews preceded by low (first value in the cell) and high (second value in the cell) expectations.",
                "The P-values for a positive difference are given square brackets.",
                "City O R S C V 3.953 4.045 3.985 4.252 3.946 Boston 3.364 3.590 3.485 3.641 3.242 [0.011] [0.028] [0.0086] [0.0168] [0.0034] 4.284 4.358 4.064 4.530 4.428 Sydney 3.756 3.537 3.436 3.918 3.495 [0.000] [0.000] [0.035] [0.009] [0.000] 3.494 3.674 3.713 3.689 3.580 Las Vegas 3.140 3.530 2.952 3.530 3.351 [0.190] [0.529] [0.007] [0.529] [0.253] are defined as follows: Rhigh f = {ri f |ef (i) > θ} Rlow f = {ri f |ef (i) < θ} These sets are specific for each (hotel, feature) pair, and in our experiments we took θ = 4.",
                "This rather high value is close to the average rating across all features across all hotels, and is justified by the fact that our data set contains mostly high quality hotels.",
                "For each city, we take all hotels and compute the average ratings in the sets Rhigh f and Rlow f (see Table 3).",
                "The average rating amongst reviews following low prior expectations is significantly higher than the average rating following high expectations.",
                "As further evidence, we consider all hotels for which the function eV (i) (the expectation for the feature Value) has a high value (greater than 4) for some i, and a low value (less than 4) for some other i.",
                "Intuitively, these are the hotels for which there is a minimal degree of variation in the timely sequence of reviews: i.e., the cumulative average of ratings was at some point high and afterwards became low, or vice-versa.",
                "Such variations are observed for about half of all hotels in each city.",
                "Figure 3 plots the median (across considered hotels) rating, rV , when ef (i) is not more than x but greater than x − 0.5. 2.5 3 3.5 4 4.5 5 2.5 3 3.5 4 4.5 5 Medianofrating expectation Boston Sydney Vegas Figure 3: The ratings tend to decrease as the expectation increases. 138 There are two ways to interpret the function ef (i): • The expected value for feature f obtained by user i before his experience with the service, acquired by reading reports submitted by past users.",
                "In this case, an overly high value for ef (i) would drive the user to submit a negative report (or vice versa), stemming from the difference between the actual value of the service, and the inflated expectation of this value acquired before his experience. • The expected value of feature f for all subsequent visitors of the site, if user i were not to submit a report.",
                "In this case, the motivation for a negative report following an overly high value of ef is different: user i seeks to correct the expectation of future visitors to the site.",
                "Unlike the interpretation above, this does not require the user to derive an a priori expectation for the value of f. Note that neither interpretation implies that the average up to report i is inversely related to the rating at report i.",
                "There might exist a measure of influence exerted by past reports that pushes the user behind report i to submit ratings which to some extent conforms with past reports: a low value for ef (i) can influence user i to submit a low rating for feature f because, for example, he fears that submitting a high rating will make him out to be a person with low standards5 .",
                "This, at first, appears to contradict Hypothesis 2.",
                "However, this conformity rating cannot continue indefinitely: once the set of reports project a sufficiently deflated estimate for vf , future reviewers with comparatively positive impressions will seek to correct this misconception. 4.2 Impact of textual comments on quality expectation Further insight into the rating behavior of TripAdvisor users can be obtained by analyzing the relationship between the weights wf and the values ef (i).",
                "In particular, we examine the following hypothesis: Hypothesis 3.",
                "When a large proportion of the text of a review discusses a certain feature, the difference between the rating for that feature and the average rating up to that point tends to be large.",
                "The intuition behind this claim is that when the user is adamant about voicing his opinion regarding a certain feature, his opinion differs from the collective opinion of previous postings.",
                "This relies on the characteristic of reputation systems as feedback forums where a user is interested in projecting his opinion, with particular strength if this opinion differs from what he perceives to be the general opinion.",
                "To test Hypothesis 3 we measure the average absolute difference between the expectation ef (i) and the rating ri f when the weight wi f is high, respectively low.",
                "Weights are classified high or low by comparing them with certain cutoff values: wi f is low if smaller than 0.1, while wi f is high if greater than θf .",
                "Different cutoff values were used for different features: θR = 0.4, θS = 0.4, θC = 0.2, and θV = 0.7.",
                "Cleanliness has a lower cutoff since it is a feature rarely discussed; Value has a high cutoff for the opposite reason.",
                "Results are presented in Table 4. 5 The idea that negative reports can encourage further negative reporting has been suggested before [14] Table 4: Average of |ri f −ef (i)| when weights are high (first value in the cell) and low (second value in the cell) with P-values for the difference in sq. brackets.",
                "City R S C V 1.058 1.208 1.728 1.356 Boston 0.701 0.838 0.760 0.917 [0.022] [0.063] [0.000] [0.218] 1.048 1.351 1.218 1.318 Sydney 0.752 0.759 0.767 0.908 [0.179] [0.009] [0.165] [0.495] 1.184 1.378 1.472 1.642 Las Vegas 0.772 0.834 0.808 1.043 [0.071] [0.020] [0.006] [0.076] This demonstrates that when weights are unusually high, users tend to express an opinion that does not conform to the net average of previous ratings.",
                "As we might expect, for a feature that rarely was a high weight in the discussion, (e.g., cleanliness) the difference is particularly large.",
                "Even though the difference in the feature Value is quite large for Sydney, the P-value is high.",
                "This is because only few reviews discussed value heavily.",
                "The reason could be cultural or because there was less of a reason to discuss this feature. 4.3 Reporting Incentives Previous models suggest that users who are not highly opinionated will not choose to voice their opinions [12].",
                "In this section, we extend this model to account for the influence of expectations.",
                "The motivation for submitting feedback is not only due to extreme opinions, but also to the difference between the current reputation (i.e., the prior expectation of the user) and the actual experience.",
                "Such a rating model produces ratings that most of the time deviate from the current average rating.",
                "The ratings that confirm the prior expectation will rarely be submitted.",
                "We test on our data set the proportion of ratings that attempt to correct the current estimate.",
                "We define a deviant rating as one that deviates from the current expectation by at least some threshold θ, i.e., |ri f − ef (i)| ≥ θ.",
                "For each of the three considered cities, the following tables, show the proportion of deviant ratings for θ = 0.5 and θ = 1.",
                "Table 5: Proportion of deviant ratings with θ = 0.5 City O R S C V Boston 0.696 0.619 0.676 0.604 0.684 Sydney 0.645 0.615 0.672 0.614 0.675 Las Vegas 0.721 0.641 0.694 0.662 0.724 Table 6: Proportion of deviant ratings with θ = 1 City O R S C V Boston 0.420 0.397 0.429 0.317 0.446 Sydney 0.360 0.367 0.442 0.336 0.489 Las Vegas 0.510 0.421 0.483 0.390 0.472 The above results suggest that a large proportion of users (close to one half, even for the high threshold value θ = 1) deviate from the prior average.",
                "This reinforces the idea that users are more likely to submit a report when they believe they have something distinctive to add to the current stream of opinions for some feature.",
                "Such conclusions are in total agreement with prior evidence that the distribution of reports often follows bi-modal, U-shaped distributions. 139 5.",
                "MODELLING THE BEHAVIOR OF RATERS To account for the observations described in the previous sections, we propose a model for the behavior of the users when submitting online reviews.",
                "For a given hotel, we make the assumption that the quality experienced by the users is normally distributed around some value vf , which represents the objective quality offered by the hotel on the feature f. The rating submitted by user i on feature f is: ˆri f = δf vi f + (1 − δf ) · sign vi f − ef (i) c + d(vi f , ef (i)|wi f ) (2) where: • vi f is the (unknown) quality actually experienced by the user. vi f is assumed normally distributed around some value vf ; • δf ∈ [0, 1] can be seen as a measure of the bias when reporting feedback.",
                "High values reflect the fact that users rate objectively, without being influenced by prior expectations.",
                "The value of δf may depend on various factors; we fix one value for each feature f; • c is a constant between 1 and 5; • wi f is the weight of feature f in the textual comment of review i, computed according to Eq. (1); • d(vi f , ef (i)|wi f ) is a distance function between the expectation and the observation of user i.",
                "The distance function satisfies the following properties: - d(y, z|w) ≥ 0 for all y, z ∈ [0, 5], w ∈ [0, 1]; - |d(y, z|w)| < |d(z, x|w)| if |y − z| < |z − x|; - |d(y, z|w1)| < |d(y, z|w2)| if w1 < w2; - c + d(vf , ef (i)|wi f ) ∈ [1, 5]; The second term of Eq. (2) encodes the bias of the rating.",
                "The higher the distance between the true observation vi f and the function ef , the higher the bias. 5.1 Model Validation We use the data set of TripAdvisor reviews to validate the behavior model presented above.",
                "We split for convenience the rating values in three ranges: bad (B = {1, 2}), indifferent (I = {3, 4}), and good (G = {5}), and perform the following two tests: • First, we will use our model to predict the ratings that have extremal values.",
                "For every hotel, we take the sequence of reports, and whenever we encounter a rating that is either good or bad (but not indifferent) we try to predict it using Eq. (2) • Second, instead of predicting the value of extremal ratings, we try to classify them as either good or bad.",
                "For every hotel we take the sequence of reports, and for each report (regardless of it value) we classify it as being good or bad However, to perform these tests, we need to estimate the objective value, vf , that is the average of the true quality observations, vi f .",
                "The algorithm we are using is based on the intuition that the amount of conformity rating is minimized.",
                "In other words, the value vf should be such that as often as possible, bad ratings follow expectations above vf and good ratings follow expectations below vf .",
                "Formally, we define the sets: Γ1 = {i|ef (i) < vf and ri f ∈ B}; Γ2 = {i|ef (i) > vf and ri f ∈ G}; that correspond to irregularities where even though the expectation at point i is lower than the delivered value, the rating is poor, and vice versa.",
                "We define vf as the value that minimize these union of the two sets: vf = arg min vf |Γ1 ∪ Γ2| (3) In Eq. (2) we replace vi f by the value vf computed in Eq. (3), and use the following distance function: d(vf , ef (i)|wi f ) = |vf − ef (i)| vf − ef (i) |vf 2 − ef (i)2 | · (1 + 2wi f ); The constant c ∈ I was set to min{max{ef (i), 3}}, 4}.",
                "The values for δf were fixed at {0.7, 0.7, 0.8, 0.7, 0.6} for the features {Overall, Rooms, Service, Cleanliness, Value} respectively.",
                "The weights are computed as described in Section 3.",
                "As a first experiment, we take the sets of extremal ratings {ri f |ri f /∈ I} for each hotel and feature.",
                "For every such rating, ri f , we try to estimate it by computing ˆri f using Eq. (2).",
                "We compare this estimator with the one obtained by simply averaging the ratings over all hotels and features: i.e., ¯rf = j,r j f =0 rj f j,r j f =0 1 ; Table 7 presents the ratio between the root mean square error (RMSE) when using ˆri f and ¯rf to estimate the actual ratings.",
                "In all cases the estimate produced by our model is better than the simple average.",
                "Table 7: Average of RMSE(ˆrf ) RMSE(¯rf ) City O R S C V Boston 0.987 0.849 0.879 0.776 0.913 Sydney 0.927 0.817 0.826 0.720 0.681 Las Vegas 0.952 0.870 0.881 0.947 0.904 As a second experiment, we try to distinguish the sets Bf = {i|ri f ∈ B} and Gf = {i|ri f ∈ G} of bad, respectively good ratings on the feature f. For example, we compute the set Bf using the following classifier (called σ): ri f ∈ Bf (σf (i) = 1) ⇔ ˆri f ≤ 4; Tables 8, 9 and 10 present the Precision(p), Recall(r) and s = 2pr p+r for classifier σ, and compares it with a naive majority classifier, τ, τf (i) = 1 ⇔ |Bf | ≥ |Gf |: We see that recall is always higher for σ and precision is usually slightly worse.",
                "For the s metric σ tends to add a 140 Table 8: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Boston O R S C V p 0.678 0.670 0.573 0.545 0.610 σ r 0.626 0.659 0.619 0.612 0.694 s 0.651 0.665 0.595 0.577 0.609 p 0.684 0.706 0.647 0.611 0.633 τ r 0.597 0.541 0.410 0.383 0.562 s 0.638 0.613 0.502 0.471 0.595 Table 9: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Las Vegas O R S C V p 0.654 0.748 0.592 0.712 0.583 σ r 0.608 0.536 0.791 0.474 0.610 s 0.630 0.624 0.677 0.569 0.596 p 0.685 0.761 0.621 0.748 0.606 τ r 0.542 0.505 0.767 0.445 0.441 s 0.605 0.607 0.670 0.558 0.511 1-20% improvement over τ, much higher in some cases for hotels in Sydney.",
                "This is likely because Sydney reviews are more positive than those of the American cities and cases where the number of bad reviews exceeded the number of good ones are rare.",
                "Replacing the test algorithm with one that plays a 1 with probability equal to the proportion of bad reviews improves its results for this city, but it is still outperformed by around 80%. 6.",
                "SUMMARY OF RESULTS AND CONCLUSION The goal of this paper is to explore the factors that drive a user to submit a particular rating, rather than the incentives that encouraged him to submit a report in the first place.",
                "For that we use two additional sources of information besides the vector of numerical ratings: first we look at the textual comments that accompany the reviews, and second we consider the reports that have been previously submitted by other users.",
                "Using simple natural language processing algorithms, we were able to establish a correlation between the weight of a certain feature in the textual comment accompanying the review, and the noise present in the numerical rating.",
                "Specifically, it seems that users who discuss amply a certain feature are likely to agree on a common rating.",
                "This observation allows the construction of feature-by-feature estimators of quality that have a lower variance, and are hopefully less noisy.",
                "Nevertheless, further evidence is required to support the intuition that ratings corresponding to high weights are expert opinions that deserve to be given higher priority when computing estimates of quality.",
                "Second, we emphasize the dependence of ratings on previous reports.",
                "Previous reports create an expectation of quality which affects the subjective perception of the user.",
                "We validate two facts about the hotel reviews we collected from TripAdvisor: First, the ratings following low expectations (where the expectation is computed as the average of the previous reports) are likely to be higher than the ratings Table 10: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Sydney O R S C V p 0.650 0.463 0.544 0.550 0.580 σ r 0.234 0.378 0.571 0.169 0.592 s 0.343 0.452 0.557 0.259 0.586 p 0.562 0.615 0.600 0.500 0.600 τ r 0.054 0.098 0.101 0.015 0.175 s 0.098 0.168 0.172 0.030 0.271 following high expectations.",
                "Intuitively, the perception of quality (and consequently the rating) depends on how well the actual experience of the user meets her expectation.",
                "Second, we include evidence from the textual comments, and find that when users devote a large fraction of the text to discussing a certain feature, they are likely to motivate a divergent rating (i.e., a rating that does not conform to the prior expectation).",
                "Intuitively, this supports the hypothesis that review forums act as discussion groups where users are keen on presenting and motivating their own opinion.",
                "We have captured the empirical evidence in a behavior model that predicts the ratings submitted by the users.",
                "The final rating depends, as expected, on the true observation, and on the gap between the observation and the expectation.",
                "The gap tends to have a bigger influence when an important fraction of the textual comment is dedicated to discussing a certain feature.",
                "The proposed model was validated on the empirical data and provides better estimates of the ratings actually submitted.",
                "One assumption that we make is about the existence of an objective quality value vf for the feature f. This is rarely true, especially over large spans of time.",
                "Other explanations might account for the correlation of ratings with past reports.",
                "For example, if ef (i) reflects the true value of f at a point in time, the difference in the ratings following high and low expectations can be explained by hotel revenue models that are maximized when the value is modified accordingly.",
                "However, the idea that variation in ratings is not primarily a function of variation in value turns out to be a useful one.",
                "Our approach to approximate this elusive objective value is by no means perfect, but conforms neatly to the idea behind the model.",
                "A natural direction for future work is to examine concrete applications of our results.",
                "Significant improvements of quality estimates are likely to be obtained by incorporating all empirical evidence about rating behavior.",
                "Exactly how different factors affect the decisions of the users is not clear.",
                "The answer might depend on the particular application, context and culture. 7.",
                "REFERENCES [1] A. Admati and P. Pfleiderer.",
                "Noisytalk.com: Broadcasting opinions in a noisy environment.",
                "Working Paper 1670R, Stanford University, 2000. [2] P. B., L. Lee, and S. Vaithyanathan.",
                "Thumbs up? sentiment classification using machine learning techniques.",
                "In Proceedings of the EMNLP-02, the Conference on Empirical Methods in Natural Language Processing, 2002. [3] H. Cui, V. Mittal, and M. Datar.",
                "Comparative 141 Experiments on Sentiment Classification for Online Product Reviews.",
                "In Proceedings of AAAI, 2006. [4] K. Dave, S. Lawrence, and D. Pennock.",
                "Mining the peanut gallery:opinion extraction and semantic classification of product reviews.",
                "In Proceedings of the 12th International Conference on the World Wide Web (WWW03), 2003. [5] C. Dellarocas, N. Awad, and X. Zhang.",
                "Exploring the Value of Online Product Ratings in Revenue Forecasting: The Case of Motion Pictures.",
                "Working paper, 2006. [6] C. Forman, A. Ghose, and B. Wiesenfeld.",
                "A Multi-Level Examination of the Impact of Social Identities on Economic Transactions in Electronic Markets.",
                "Available at SSRN: http://ssrn.com/abstract=918978, July 2006. [7] A. Ghose, P. Ipeirotis, and A. Sundararajan.",
                "Reputation Premiums in Electronic Peer-to-Peer Markets: Analyzing Textual Feedback and Network Structure.",
                "In Third Workshop on Economics of Peer-to-Peer Systems, (P2PECON), 2005. [8] A. Ghose, P. Ipeirotis, and A. Sundararajan.",
                "The Dimensions of Reputation in electronic Markets.",
                "Working Paper CeDER-06-02, New York University, 2006. [9] A. Harmon.",
                "Amazon Glitch Unmasks War of Reviewers.",
                "The New York Times, February 14, 2004. [10] D. Houser and J. Wooders.",
                "Reputation in Auctions: Theory and Evidence from eBay.",
                "Journal of Economics and Management Strategy, 15:353-369, 2006. [11] M. Hu and B. Liu.",
                "Mining and summarizing customer reviews.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD04), 2004. [12] N. Hu, P. Pavlou, and J. Zhang.",
                "Can Online Reviews Reveal a Products True Quality?",
                "In Proceedings of ACM Conference on Electronic Commerce (EC 06), 2006. [13] K. Kalyanam and S. McIntyre.",
                "Return on reputation in online auction market.",
                "Working Paper 02/03-10-WP, Leavey School of Business, Santa Clara University., 2001. [14] L. Khopkar and P. Resnick.",
                "Self-Selection, Slipping, Salvaging, Slacking, and Stoning: the Impacts of Negative Feedback at eBay.",
                "In Proceedings of ACM Conference on Electronic Commerce (EC 05), 2005. [15] M. Melnik and J. Alm.",
                "Does a sellers reputation matter? evidence from ebay auctions.",
                "Journal of Industrial Economics, 50(3):337-350, 2002. [16] R. Olshavsky and J. Miller.",
                "Consumer Expectations, Product Performance and Perceived Product Quality.",
                "Journal of Marketing Research, 9:19-21, February 1972. [17] A. Parasuraman, V. Zeithaml, and L. Berry.",
                "A Conceptual Model of Service Quality and Its Implications for Future Research.",
                "Journal of Marketing, 49:41-50, 1985. [18] A. Parasuraman, V. Zeithaml, and L. Berry.",
                "SERVQUAL: A Multiple-Item Scale for Measuring Consumer Perceptions of Service Quality.",
                "Journal of Retailing, 64:12-40, 1988. [19] P. Pavlou and A. Dimoka.",
                "The Nature and Role of Feedback Text Comments in Online Marketplaces: Implications for Trust Building, Price Premiums, and Seller Differentiation.",
                "Information Systems Research, 17(4):392-414, 2006. [20] A. Popescu and O. Etzioni.",
                "Extracting product features and opinions from reviews.",
                "In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, 2005. [21] R. Teas.",
                "Expectations, Performance Evaluation, and Consumers Perceptions of Quality.",
                "Journal of Marketing, 57:18-34, 1993. [22] E. White.",
                "Chatting a Singer Up the Pop Charts.",
                "The Wall Street Journal, October 15, 1999.",
                "APPENDIX A.",
                "LIST OF WORDS, LR, ASSOCIATED TO THE FEATURE ROOMS All words serve as prefixes: room, space, interior, decor, ambiance, atmosphere, comfort, bath, toilet, bed, building, wall, window, private, temperature, sheet, linen, pillow, hot, water, cold, water, shower, lobby, furniture, carpet, air, condition, mattress, layout, design, mirror, ceiling, lighting, lamp, sofa, chair, dresser, wardrobe, closet 142"
            ],
            "original_annotated_samples": [
                "In the <br>absence of clear incentive</br>s, users with a moderate outlook will not bother to voice their opinions, which leads to an unrepresentative sample of reviews."
            ],
            "translated_annotated_samples": [
                "En <br>ausencia de incentivos claros</br>, los usuarios con una perspectiva moderada no se molestarán en expresar sus opiniones, lo que conduce a una muestra no representativa de reseñas."
            ],
            "translated_text": "La comprensión del comportamiento del usuario en la presentación de comentarios en línea. Trabajos anteriores han demostrado que la información contradictoria y los sesgos subyacentes de los usuarios dificultan juzgar el verdadero valor de un servicio. En este artículo, investigamos los factores subyacentes que influyen en el comportamiento del usuario al informar retroalimentación. Examinamos dos fuentes de información además de las calificaciones numéricas: la evidencia lingüística del comentario textual que acompaña a una reseña y los patrones en la secuencia temporal de los informes. Primero demostramos que los grupos de usuarios que discuten ampliamente una característica específica tienen más probabilidades de ponerse de acuerdo en una calificación común para esa característica. Segundo, demostramos que la calificación de un usuario refleja en parte la diferencia entre la calidad real y la expectativa previa de calidad, tal como se infiere de revisiones anteriores. Ambos nos ofrecen una forma menos ruidosa de producir estimaciones de calificación y revelar las razones detrás del sesgo del usuario. Nuestras hipótesis fueron validadas por evidencia estadística de reseñas de hoteles en el sitio web de TripAdvisor. Categorías y Descriptores de Asignaturas J.4 [Ciencias Sociales y del Comportamiento]: Economía Términos Generales Economía, Experimentación, Confiabilidad 1. MOTIVACIONES La expansión de internet ha hecho posible que los foros de retroalimentación en línea (o mecanismos de reputación) se conviertan en un canal importante para el boca a boca sobre productos, servicios u otros tipos de interacciones comerciales. Numerosos estudios empíricos [10, 15, 13, 5] muestran que los compradores consideran seriamente los comentarios en línea al tomar decisiones de compra, y están dispuestos a pagar primas por reputación por productos o servicios que tienen una buena reputación. Sin embargo, un análisis reciente plantea preguntas importantes sobre la capacidad de los foros existentes para reflejar la verdadera calidad de un producto. En <br>ausencia de incentivos claros</br>, los usuarios con una perspectiva moderada no se molestarán en expresar sus opiniones, lo que conduce a una muestra no representativa de reseñas. Por ejemplo, [12, 1] muestran que las calificaciones de libros o CDs en Amazon1 siguen con gran probabilidad distribuciones bimodales en forma de U, donde la mayoría de las calificaciones son o muy buenas o muy malas. Los experimentos controlados, por otro lado, revelan opiniones sobre los mismos elementos que están distribuidas de forma normal. Bajo estas circunstancias, utilizar la media aritmética para predecir la calidad (como la mayoría de los foros realmente hacen) proporciona al usuario típico un estimador con una alta varianza que a menudo es incorrecto. Mejorar la forma en que agregamos la información disponible de las reseñas en línea requiere un profundo entendimiento de los factores subyacentes que sesgan el comportamiento de calificación de los usuarios. Hu et al. [12] proponen el Modelo Brag-and-Moan donde los usuarios califican solo si su utilidad del producto (extraída de una distribución normal) cae fuera de un intervalo mediano. Los autores concluyen que el modelo explica la distribución empírica de los informes y ofrece ideas sobre formas más inteligentes de estimar la verdadera calidad del producto. En el presente artículo ampliamos esta línea de investigación e intentamos explicar más hechos sobre el comportamiento de los usuarios al informar retroalimentación en línea. Usando reseñas reales de hoteles del sitio web TripAdvisor2, consideramos dos fuentes adicionales de información además de las calificaciones numéricas básicas enviadas por los usuarios. La primera es una evidencia lingüística simple proveniente de la revisión textual que suele acompañar a las calificaciones numéricas. Utilizamos técnicas de minería de texto similares a [7] y [3], sin embargo, solo estamos interesados en identificar qué aspectos del servicio está discutiendo el usuario, sin calcular la orientación semántica del texto. Observamos que los usuarios que comentan más sobre la misma característica tienen más probabilidades de estar de acuerdo en una calificación numérica común para esa característica en particular. Intuitivamente, los comentarios extensos revelan la importancia de la característica para el usuario. Dado que las personas tienden a ser más conocedoras en los aspectos que consideran importantes, se podría asumir que los usuarios que discuten una característica específica en más detalle tienen más autoridad para evaluar esa característica. Segundo investigamos la relación entre una reseña 1 http://www.amazon.com 2 http://www.tripadvisor.com/ 134 Figura 1: La página de TripAdvisor que muestra reseñas de un popular hotel en Boston. El nombre del hotel y los anuncios fueron deliberadamente borrados, al igual que las reseñas que lo precedieron. Un examen de las reseñas en línea muestra que las calificaciones a menudo forman parte de los hilos de discusión, donde una publicación no es necesariamente independiente de otras publicaciones. Uno puede ver, por ejemplo, usuarios que se esfuerzan por contradecir o estar vehementemente de acuerdo con los comentarios de usuarios anteriores. Al analizar la secuencia temporal de informes, concluimos que las revisiones pasadas influyen en los informes futuros, ya que crean cierta expectativa previa sobre la calidad del servicio. La percepción subjetiva del usuario está influenciada por la brecha entre la expectativa previa y el rendimiento real del servicio [17, 18, 16, 21], lo cual se reflejará más tarde en la calificación de los usuarios. Proponemos un modelo que captura la dependencia de las calificaciones en las expectativas previas, y lo validamos utilizando los datos empíricos que recopilamos. Ambos resultados pueden ser utilizados para mejorar la forma en que los mecanismos de reputación agregan la información de las revisiones individuales. Nuestro primer resultado se puede utilizar para determinar una estimación de calidad característica por característica, donde para cada característica se considera un subconjunto diferente de reseñas (es decir, aquellas con comentarios extensos sobre esa característica). El segundo conduce a un algoritmo que produce una estimación más precisa de la calidad real. 2. El conjunto de datos que utilizamos en este artículo son reseñas reales de hoteles recopiladas del popular sitio de viajes TripAdvisor. TripAdvisor indexa hoteles de ciudades de todo el mundo, junto con reseñas escritas por viajeros. Los usuarios pueden buscar en el sitio proporcionando el nombre del hotel y la ubicación (opcional). Las reseñas de un hotel dado se muestran como una lista (ordenadas de la más reciente a la más antigua), con 5 reseñas por página. Las reseñas contienen: • información sobre el autor de la reseña (por ejemplo, fechas de estancia, nombre de usuario del revisor, ubicación del revisor); • la calificación general (de 1, la más baja, a 5, la más alta); • una reseña textual que incluye un título para la reseña, comentarios libres y las principales cosas que al revisor le gustaron y no le gustaron; • calificaciones numéricas (de 1, la más baja, a 5, la más alta) para diferentes características (por ejemplo, limpieza, servicio, ubicación, etc.). Debajo del nombre del hotel, TripAdvisor muestra la dirección del hotel, información general (número de habitaciones, número de estrellas, breve descripción, etc.), la calificación promedio general, el ranking de TripAdvisor y una calificación promedio para cada característica. La Figura 1 muestra la página de un hotel popular en Boston cuyo nombre (junto con los anuncios) fue explícitamente borrado. Seleccionamos tres ciudades para este estudio: Boston, Sídney y Las Vegas. Para cada ciudad consideramos todos los hoteles que tenían al menos 10 reseñas, y registramos todas las reseñas. La Tabla 1 presenta el número de hoteles considerados en cada ciudad, el número total de reseñas registradas para cada ciudad y la distribución de hoteles con respecto a la calificación por estrellas (según está disponible en el sitio de TripAdvisor). Ten en cuenta que no todos los hoteles tienen una clasificación por estrellas. Tabla 1: Un resumen del conjunto de datos. Para cada reseña, registramos la calificación general, la reseña textual (título y cuerpo de la reseña) y la calificación numérica en 7 características: Habitaciones (R), Servicio (S), Limpieza (C), Valor (V), Comida (F), Ubicación (L) y Ruido (N). TripAdvisor no requiere que los usuarios envíen nada más que la calificación general, por lo tanto, una reseña típica evalúa pocas características adicionales, independientemente de la discusión en el comentario textual. Solo las características Habitaciones (R), Servicio (S), Limpieza (C) y Valor (V) son evaluadas por un número significativo de usuarios. Sin embargo, también seleccionamos las características Comida (F), Ubicación (L) y Ruido (N) porque son mencionadas en un número significativo de comentarios textuales. Para cada característica registramos la calificación numérica dada por el usuario, o 0 cuando la calificación está ausente. La longitud típica del comentario textual equivale aproximadamente a 200 palabras. Todos los datos fueron recopilados rastreando el sitio de TripAdvisor en septiembre de 2006. 2.1 Notación formal Nos referiremos formalmente a una reseña mediante una tupla (r, T) donde: • r = (rf ) es un vector que contiene las calificaciones rf ∈ {0, 1, . . . 5} para las características f ∈ F = {O, R, S, C, V, F, L, N}; cabe destacar que la calificación general, rO, se registra de forma abusiva como la calificación para la característica General(O); • T es el comentario textual que acompaña a la reseña. Se indexan 135 reseñas de acuerdo con la variable i, de modo que (ri , Ti ) es la i-ésima reseña en nuestra base de datos. Dado que no registramos el nombre de usuario del revisor, también diremos que la i-ésima reseña en nuestro conjunto de datos fue enviada por el usuario i. Cuando necesitemos considerar solo las reseñas de un hotel dado, h, usaremos (ri(h), Ti(h)) para denotar la i-ésima reseña sobre el hotel h. 3. EVIDENCIA DE COMENTARIOS TEXTUALES Los comentarios textuales gratuitos asociados a las reseñas en línea son una fuente valiosa de información para comprender las razones detrás de las calificaciones numéricas dejadas por los revisores. El texto puede, por ejemplo, revelar ejemplos concretos de aspectos que al usuario le gustaron o no le gustaron, justificando así algunas de las calificaciones altas o bajas respectivamente para ciertas características. El texto también puede ofrecer pautas para comprender las preferencias del revisor y los pesos de las diferentes características al calcular una calificación general. El problema, sin embargo, es que los comentarios textuales libres son difíciles de leer. Los usuarios deben desplazarse a través de muchas reseñas y leer principalmente información repetitiva. Se obtendrían mejoras significativas si las reseñas fueran interpretadas y agregadas automáticamente. Desafortunadamente, esta parece ser una tarea difícil para las computadoras ya que los usuarios humanos a menudo utilizan un lenguaje ingenioso, abreviaturas, frases específicas de la cultura y un estilo figurativo. Sin embargo, varios resultados importantes utilizan los comentarios textuales de las reseñas en línea de forma automatizada. Utilizando técnicas de lenguaje natural bien establecidas, las reseñas o partes de las reseñas pueden ser clasificadas como tener una orientación semántica positiva o negativa. Pang et al. [2] clasifican las críticas de películas en positivas/negativas entrenando tres clasificadores diferentes (Naive Bayes, Máxima Entropía y SVM) utilizando características de clasificación basadas en unigramas, bigramas o etiquetas de partes del discurso. Dave et al. [4] analizan reseñas de CNet y Amazon, y sorprendentemente demuestran que las características de clasificación basadas en unigramas o bigramas funcionan mejor que los n-gramas de orden superior. Este resultado es desafiado por Cui et al. [3], quienes examinan grandes colecciones de reseñas recopiladas de la web. Muestran que el tamaño del conjunto de datos es importante, y que conjuntos de entrenamiento más grandes permiten a los clasificadores utilizar con éxito características de clasificación más complejas basadas en n-gramas. Hu y Liu [11] también rastrean la web en busca de reseñas de productos e identifican automáticamente los atributos de productos que han sido discutidos por los revisores. Utilizan Wordnet para calcular la orientación semántica de las evaluaciones de productos y resumir las reseñas de usuarios extrayendo evaluaciones positivas y negativas de diferentes características del producto. Popescu y Etzioni [20] analizan un escenario similar, pero utilizan el recuento de resultados de búsqueda en motores de búsqueda para identificar atributos de productos; la orientación semántica se asigna a través de la técnica de etiquetado de relajación. Ghose et al. [7, 8] analizan las reseñas de vendedores del mercado secundario de Amazon para identificar las diferentes dimensiones (por ejemplo, entrega, empaque, atención al cliente, etc.) de la reputación. Analizan el texto y etiquetan la parte de la oración de cada palabra. Los sustantivos, frases nominales y frases verbales frecuentes se identifican como dimensiones de la reputación, mientras que los modificadores correspondientes (es decir, adjetivos y adverbios) se utilizan para derivar puntuaciones numéricas para cada dimensión. La medida de reputación mejorada se correlaciona mejor con la información de precios observada en el mercado. Pavlou y Dimoka [19] analizan las reseñas de eBay y encuentran que los comentarios textuales tienen un impacto importante en las primas de reputación. Nuestro enfoque es similar a los trabajos mencionados anteriormente, en el sentido de que identificamos los aspectos (es decir, las características del hotel) discutidos por los usuarios en las reseñas textuales. Sin embargo, no calculamos la orientación semántica del texto, ni intentamos inferir las calificaciones faltantes. Definimos el peso, wi f, de la característica f ∈ F en el texto Ti asociado con la reseña (ri, Ti), como la fracción de Ti dedicada a discutir aspectos (tanto positivos como negativos) relacionados con la característica f. Proponemos un método elemental para aproximar los valores de estos pesos. Para cada característica, construimos manualmente la lista de palabras Lf que contiene aproximadamente 50 palabras que están más comúnmente asociadas a la característica f. Las palabras iniciales fueron seleccionadas al leer algunas de las reseñas y ver qué palabras coinciden con la discusión de qué características. La lista fue luego ampliada agregando todas las entradas del tesauro que estaban relacionadas con las palabras iniciales. Finalmente, hicimos una lluvia de ideas para encontrar las palabras faltantes que normalmente estarían asociadas con cada una de las características. Que Lf ∩ Ti sea la lista de términos comunes a Lf y Ti. Cada término de Lf se cuenta el número de veces que aparece en Ti, con dos excepciones: • en los casos en que el usuario envía un título para la revisión, consideramos el texto del título agregándolo tres veces al texto de la revisión Ti. La suposición intuitiva es que la opinión del usuario se refleja con mayor fuerza en el título que en el cuerpo de la reseña. Por ejemplo, muchas reseñas son resumidas con precisión por títulos como \"Excelente servicio, ubicación terrible\" o \"Mala relación calidad-precio\"; ciertas palabras que ocurren solo una vez en el texto son contadas múltiples veces si su relevancia para esa característica es particularmente fuerte. Estas eran las palabras raíz para cada característica (por ejemplo, personal es una palabra raíz para la característica Servicio), y tenían un peso de 2 o 3. Cada característica fue asignada hasta 3 palabras raíz, por lo que casi todas las palabras se cuentan solo una vez. La lista de palabras para la característica Habitaciones se proporciona como referencia en el Apéndice A. El peso wi f se calcula como: wi f = |Lf ∩ Ti| f∈F |Lf ∩ Ti| (1) donde |Lf ∩ Ti| es el número de términos comunes entre Lf y Ti. El peso para la característica \"En general\" se estableció en min{ |T i | 5000 , 1} donde |Ti | es el número de caracteres en Ti. Comenzaré diciendo que soy más de la onda de Holiday Inn que de un hotel de lujo. Así que me frustro cuando pago el doble de la tarifa de la habitación y recibo la mitad de las comodidades que obtendría en un Hampton Inn o Holiday Inn. La ubicación era definitivamente el principal activo de este lugar. Estaba a solo unas cuadras de la parada de metro del Centro Hynes y era fácil caminar a algunos buenos restaurantes en la zona de Back Bay. Boylston no está lejos en absoluto. Así que no tuve problemas en renunciar a un coche de alquiler y tomar el metro desde el aeropuerto hasta el hotel y usar el metro para cualquier otro viaje. De lo contrario, te hacen pagar por todo. Y cuando ya has gastado $215 por noche en la habitación, eso resulta frustrante. La habitación en sí estaba bien, más o menos lo que esperaría. El personal también era promedio, ni malo ni excelente. Una vez más, creo que estás pagando por la ubicación y la capacidad de ir caminando a muchos lugares interesantes. Pero creo que la próxima vez me quedaré en Brookline, disfrutaré de más comodidades y usaré más el metro. Las calificaciones numéricas asociadas a esta reseña son rO = 3, rR = 3, rS = 3, rC = 4, rV = 2 para las características de General (O), Habitaciones (R), Servicio (S), Limpieza (C) y Valor (V) respectivamente. Las calificaciones de las características Comida (F), Ubicación (L) y Ruido (N) están ausentes (es decir, rF = rL = rN = 0). Los pesos wf se calculan a partir de las siguientes listas de términos comunes: LR ∩ T = {habitación}; wR = 0.066 LS ∩ T = {3 * Personal, comodidades}; wS = 0.267 LC ∩ T = ∅; wC = 0 LV ∩ T = {$, tarifa}; wV = 0.133 LF ∩ T = {restaurante}; wF = 0.067 LL ∩ T = {2 * centro, 2 * caminar, 2 * ubicación, área}; wL = 0.467 LN ∩ T = ∅; wN = 0 Las palabras raíz Personal y Centro se triplicaron y duplicaron respectivamente. El peso total de la revisión textual es wO = 0.197. Estos valores explican razonablemente bien los pesos de las diferentes características en la discusión del revisor. Un punto a tener en cuenta es que algunos términos en las listas Lf poseen una orientación semántica inherente. Por ejemplo, la palabra suciedad (perteneciente a la lista LC) se usaría con mayor frecuencia para afirmar la presencia, y no la ausencia de suciedad. Esto es inevitable, pero se tuvo cuidado de asegurar que se utilizaran palabras de ambos extremos del espectro. Por esta razón, algunas listas como LR contienen solo sustantivos de objetos que uno típicamente describiría en una habitación (ver Apéndice A). El objetivo de esta sección es analizar la influencia de los pesos wi f en las calificaciones numéricas ri f. Intuitivamente, los usuarios que pasaron mucho tiempo discutiendo una característica f (es decir, cuando wif es alto) tenían algo que decir sobre su experiencia con respecto a esta característica. Obviamente, la característica f es importante para el usuario i. Dado que las personas tienden a ser más conocedoras en los aspectos que consideran importantes, nuestra hipótesis es que las calificaciones ri f (correspondientes a los pesos altos wi f) constituyen un subconjunto de las calificaciones de expertos para la característica f. La Figura 2 traza la distribución de las tasas r i(h) C con respecto a los pesos w i(h) C para la limpieza de un hotel de Las Vegas, h. Aquí, las calificaciones altas están restringidas a las reseñas que hablan poco sobre la limpieza. Cuando se menciona la limpieza en la discusión, las calificaciones son bajas. Muchos hoteles muestran patrones de calificación similares para varias características. Las calificaciones correspondientes a pesos bajos abarcan todo el espectro del 1 al 5, mientras que las calificaciones correspondientes a pesos altos están más agrupadas (ya sea alrededor de calificaciones buenas o malas). Por lo tanto, formulamos la siguiente hipótesis: Hipótesis 1. Las calificaciones ri f correspondientes a las reseñas donde wi f es alta, son más similares entre sí que a la colección general de calificaciones. Para probar la hipótesis, tomamos todo el conjunto de reseñas y, característica por característica, calculamos la desviación estándar de las calificaciones con alto peso, y la desviación estándar de todo el conjunto de calificaciones. Los pesos altos se definieron como aquellos que pertenecen al 20% superior del rango de pesos para la característica correspondiente. Si la Hipótesis 1 fuera cierta, la desviación estándar de todas las calificaciones debería ser mayor que la desviación estándar de las calificaciones con pesos altos. 0 1 2 3 4 5 6 0 0.1 0.2 0.3 0.4 0.5 0.6 Calificación Peso Figura 2: La distribución de las calificaciones en función del peso de la característica de limpieza. Utilizamos una prueba T estándar para medir la significancia de los resultados. Ciudad por ciudad y característica por característica, la Tabla 2 presenta la desviación estándar promedio de todas las calificaciones, y la desviación estándar promedio de las calificaciones con pesos altos. De hecho, las calificaciones con pesos altos tienen una desviación estándar más baja, y los resultados son significativos en el umbral estándar de significancia de 0.05 (aunque para ciertas ciudades tomadas de forma independiente no parece haber una diferencia significativa, los resultados son significativos para todo el conjunto de datos). Por favor, tenga en cuenta que solo se consideraron las características O, R, S, C y V, ya que para las otras (F, L y N) no teníamos suficientes calificaciones. Tabla 2: Desviación estándar promedio para todas las calificaciones, y desviación estándar promedio para las calificaciones con pesos altos. En corchetes, los valores p correspondientes para una diferencia positiva entre los dos. La hipótesis 1 no solo proporciona una comprensión básica sobre el comportamiento de calificación de los usuarios en línea, sino que también sugiere algunas formas de calcular estimaciones de mejor calidad. Podemos, por ejemplo, construir una estimación de calidad característica por característica con una varianza mucho menor: para cada característica tomamos el subconjunto de reseñas que discuten ampliamente esa característica, y como estimación de calidad, obtenemos la calificación promedio de este subconjunto. Los experimentos iniciales sugieren que las calificaciones promedio de característica a característica calculadas de esta manera son diferentes de las calificaciones promedio calculadas en todo el conjunto de datos. Dado que, de hecho, los pesos altos son indicadores de opiniones expertas, las estimaciones obtenidas de esta manera son más precisas que las actuales. Sin embargo, la validación de esta suposición subyacente requiere más experimentos controlados. LA INFLUENCIA DE LAS CALIFICACIONES PASADAS Por lo general, se hacen dos suposiciones importantes sobre las reseñas enviadas a foros en línea. La primera es que las calificaciones reflejen fielmente la calidad observada por los usuarios; la segunda es que las reseñas sean independientes entre sí. Si bien la evidencia anecdótica [9, 22] cuestiona la primera suposición, en esta sección abordamos la segunda. Un examen de las reseñas en línea muestra que estas suelen formar parte de hilos de discusión, donde los usuarios se esfuerzan por contradecir o estar vehementemente de acuerdo con los comentarios de usuarios anteriores. Considera, por ejemplo, la siguiente reseña: No entiendo las críticas negativas... el hotel estaba un poco oscuro, pero ese era el estilo. Fue muy artístico. Sí, estaba cerca de la autopista, pero en mi opinión, el sonido de un coche ruidoso ocasional es mejor que escuchar el ding ding de las máquinas tragamonedas toda la noche. El personal disponible es FABULOSO. Las camareras son geniales (y *** no merece la mala crítica que recibió, ¡fue 100% atenta con nosotros!), los camareros son amigables y profesionales al mismo tiempo... Aquí, el usuario se vio perturbado por informes negativos anteriores, abordó estas preocupaciones y se dispuso a intentar corregirlas. Como era de esperar, sus calificaciones fueron considerablemente más altas que las calificaciones promedio hasta este punto. Parece que los usuarios de TripAdvisor suelen leer regularmente los informes enviados por usuarios anteriores antes de reservar un hotel, o antes de escribir una reseña. Las reseñas anteriores crean ciertas expectativas previas sobre la calidad del servicio, y estas expectativas influyen en la reseña presentada. Creemos que esta observación es válida para la mayoría de los foros en línea. La percepción subjetiva de la calidad es directamente proporcional a qué tan bien la experiencia real cumple con la expectativa previa, un hecho confirmado por una importante línea de investigación econométrica y de marketing [17, 18, 16, 21]. La correlación entre las reseñas también ha sido confirmada por investigaciones recientes sobre la dinámica de los foros de reseñas en línea [6]. 4.1 Expectativas Previas Definimos la expectativa previa del usuario i con respecto a la característica f, como el promedio de las calificaciones previamente disponibles en la característica f4: ef (i) = j<i,r j f =0 rj f j<i,r j f =0 1 Como primera hipótesis, afirmamos que la calificación ri f es una función de la expectativa previa ef (i): Hipótesis 2. Para un hotel y una característica dados, dados los comentarios i y j tales que ef (i) es alto y ef (j) es bajo, la calificación rj f supera la calificación ri f. Definimos las expectativas altas y bajas como aquellas que están por encima, respectivamente por debajo de un cierto valor umbral θ. El conjunto de reseñas precedidas por altas, respectivamente bajas expectativas 3 parte de las reseñas de Amazon fueron reconocidas como publicaciones estratégicas por autores de libros o competidores 4 si no se asignaron calificaciones previas para la característica f, ef (i) se asigna un valor predeterminado de 4. Tabla 3: Calificaciones promedio para reseñas precedidas por expectativas bajas (primer valor en la celda) y altas (segundo valor en la celda). Los valores P para una diferencia positiva se dan entre corchetes. Las ciudades O R S C V 3.953 4.045 3.985 4.252 3.946 Boston 3.364 3.590 3.485 3.641 3.242 [0.011] [0.028] [0.0086] [0.0168] [0.0034] 4.284 4.358 4.064 4.530 4.428 Sídney 3.756 3.537 3.436 3.918 3.495 [0.000] [0.000] [0.035] [0.009] [0.000] 3.494 3.674 3.713 3.689 3.580 Las Vegas 3.140 3.530 2.952 3.530 3.351 [0.190] [0.529] [0.007] [0.529] [0.253] se definen de la siguiente manera: Ralto f = {ri f |ef (i) > θ} Rbajo f = {ri f |ef (i) < θ} Estos conjuntos son específicos para cada par (hotel, característica), y en nuestros experimentos tomamos θ = 4. Este valor bastante alto está cerca del promedio de calificación de todas las características de todos los hoteles, y se justifica por el hecho de que nuestro conjunto de datos contiene principalmente hoteles de alta calidad. Para cada ciudad, tomamos todos los hoteles y calculamos las calificaciones promedio en los conjuntos Rhigh f y Rlow f (ver Tabla 3). El promedio de calificación entre las reseñas que siguen a expectativas previas bajas es significativamente mayor que el promedio de calificación después de altas expectativas. Como evidencia adicional, consideramos todos los hoteles para los cuales la función eV (i) (la expectativa para la característica Valor) tiene un valor alto (mayor que 4) para algunos i, y un valor bajo (menor que 4) para algunos otros i. Intuitivamente, estos son los hoteles para los cuales hay un grado mínimo de variación en la secuencia oportuna de reseñas: es decir, el promedio acumulativo de calificaciones fue en algún momento alto y luego se volvió bajo, o viceversa. Tales variaciones se observan en aproximadamente la mitad de todos los hoteles en cada ciudad. La Figura 3 traza la mediana (entre los hoteles considerados) de la calificación, rV, cuando ef (i) no es más que x pero mayor que x - 0.5. 2.5 3 3.5 4 4.5 5 2.5 3 3.5 4 4.5 5 Expectativa de la mediana de calificación Boston Sydney Vegas Figura 3: Las calificaciones tienden a disminuir a medida que aumenta la expectativa. 138 Hay dos formas de interpretar la función ef (i): • El valor esperado para la característica f obtenido por el usuario i antes de su experiencia con el servicio, adquirido al leer informes presentados por usuarios anteriores. En este caso, un valor excesivamente alto para ef (i) llevaría al usuario a enviar un informe negativo (o viceversa), derivado de la diferencia entre el valor real del servicio y la expectativa inflada de este valor adquirida antes de su experiencia. • El valor esperado de la característica f para todos los visitantes posteriores del sitio, si el usuario i no enviara un informe. En este caso, la motivación para un informe negativo después de un valor excesivamente alto de ef es diferente: el usuario i busca corregir la expectativa de los futuros visitantes del sitio. A diferencia de la interpretación anterior, esto no requiere que el usuario derive una expectativa a priori para el valor de f. Tenga en cuenta que ninguna de las interpretaciones implica que el promedio hasta el informe i esté inversamente relacionado con la calificación en el informe i. Podría existir una medida de influencia ejercida por informes anteriores que impulse al usuario detrás del informe i a enviar calificaciones que, en cierta medida, se ajusten a los informes anteriores: un valor bajo para ef (i) puede influir en el usuario i a enviar una calificación baja para la característica f porque, por ejemplo, teme que enviar una calificación alta lo haga parecer una persona con bajos estándares. Esto, al principio, parece contradecir la Hipótesis 2. Sin embargo, esta calificación de conformidad no puede continuar indefinidamente: una vez que el conjunto de informes proyecte una estimación suficientemente reducida para vf, los revisores futuros con impresiones comparativamente positivas buscarán corregir esta percepción errónea. 4.2 Impacto de los comentarios textuales en la expectativa de calidad. Se puede obtener una mayor comprensión del comportamiento de calificación de los usuarios de TripAdvisor analizando la relación entre los pesos wf y los valores ef (i). En particular, examinamos la siguiente hipótesis: Hipótesis 3. Cuando una gran proporción del texto de una reseña discute una característica en particular, la diferencia entre la calificación para esa característica y la calificación promedio hasta ese momento tiende a ser grande. La intuición detrás de esta afirmación es que cuando el usuario insiste en expresar su opinión sobre una característica en particular, su opinión difiere de la opinión colectiva de publicaciones anteriores. Esto se basa en la característica de los sistemas de reputación como foros de retroalimentación donde un usuario está interesado en proyectar su opinión, con particular fuerza si esta opinión difiere de lo que percibe como la opinión general. Para probar la Hipótesis 3 medimos la diferencia absoluta promedio entre la expectativa ef (i) y la calificación ri f cuando el peso wi f es alto, respectivamente bajo. Los pesos se clasifican como altos o bajos al compararlos con ciertos valores de corte: wi f es bajo si es menor que 0.1, mientras que wi f es alto si es mayor que θf. Se utilizaron diferentes valores de corte para diferentes características: θR = 0.4, θS = 0.4, θC = 0.2 y θV = 0.7. La limpieza tiene un límite inferior más bajo ya que es una característica raramente discutida; el valor tiene un límite superior alto por la razón opuesta. Los resultados se presentan en la Tabla 4. La idea de que los informes negativos pueden fomentar una mayor divulgación negativa ha sido sugerida anteriormente [14]. Tabla 4: Promedio de |ri f −ef (i)| cuando los pesos son altos (primer valor en la celda) y bajos (segundo valor en la celda) con valores P para la diferencia en corchetes cuadrados. Esto demuestra que cuando los pesos son inusualmente altos, los usuarios tienden a expresar una opinión que no se ajusta al promedio neto de calificaciones anteriores. Como podríamos esperar, para una característica que rara vez tuvo un peso importante en la discusión (por ejemplo, la limpieza), la diferencia es particularmente grande. Aunque la diferencia en el valor de la característica es bastante grande para Sídney, el valor P es alto. Esto se debe a que solo unos pocos comentarios discutieron en gran medida el valor. La razón podría ser cultural o porque había menos motivo para discutir esta característica. 4.3 Incentivos para informar Los modelos anteriores sugieren que los usuarios poco opinativos no elegirán expresar sus opiniones [12]. En esta sección, ampliamos este modelo para tener en cuenta la influencia de las expectativas. La motivación para enviar comentarios no se debe solo a opiniones extremas, sino también a la diferencia entre la reputación actual (es decir, la expectativa previa del usuario) y la experiencia real. Un modelo de calificación así produce calificaciones que la mayoría de las veces se desvían de la calificación promedio actual. Las calificaciones que confirman la expectativa previa rara vez serán enviadas. Probamos en nuestro conjunto de datos la proporción de calificaciones que intentan corregir la estimación actual. Definimos una calificación desviada como aquella que se desvía de la expectativa actual por al menos un umbral θ, es decir, |ri f − ef (i)| ≥ θ. Para cada una de las tres ciudades consideradas, las siguientes tablas muestran la proporción de calificaciones desviadas para θ = 0.5 y θ = 1. Los resultados anteriores sugieren que una gran proporción de usuarios (casi la mitad, incluso para el valor umbral alto θ = 1) se desvían del promedio previo. Esto refuerza la idea de que los usuarios son más propensos a enviar un informe cuando creen que tienen algo distintivo que agregar al flujo actual de opiniones sobre alguna característica. Tales conclusiones están en total acuerdo con evidencia previa de que la distribución de informes a menudo sigue distribuciones bimodales en forma de U. 139 5. Para dar cuenta de las observaciones descritas en las secciones anteriores, proponemos un modelo para el comportamiento de los usuarios al enviar reseñas en línea. Para un hotel dado, asumimos que la calidad experimentada por los usuarios sigue una distribución normal alrededor de algún valor vf, que representa la calidad objetiva ofrecida por el hotel en la característica f. La calificación enviada por el usuario i en la característica f es: ˆri f = δf vi f + (1 − δf) · sign vi f − ef (i) c + d(vi f, ef (i)|wi f) (2) donde: • vi f es la calidad (desconocida) experimentada realmente por el usuario. Se asume que vi f sigue una distribución normal alrededor de algún valor vf; • δf ∈ [0, 1] puede verse como una medida del sesgo al reportar retroalimentación. Los valores altos reflejan el hecho de que los usuarios califican de manera objetiva, sin ser influenciados por expectativas previas. El valor de δf puede depender de varios factores; fijamos un valor para cada característica f; • c es una constante entre 1 y 5; • wi f es el peso de la característica f en el comentario textual de la reseña i, calculado según la Ecuación (1); • d(vi f , ef (i)|wi f ) es una función de distancia entre la expectativa y la observación del usuario i. La función de distancia satisface las siguientes propiedades: - d(y, z|w) ≥ 0 para todo y, z ∈ [0, 5], w ∈ [0, 1]; - |d(y, z|w)| < |d(z, x|w)| si |y − z| < |z − x|; - |d(y, z|w1)| < |d(y, z|w2)| si w1 < w2; - c + d(vf , ef (i)|wi f ) ∈ [1, 5]; El segundo término de la Ecuación (2) codifica el sesgo de la calificación. Cuanto mayor sea la distancia entre la observación real vi f y la función ef, mayor será el sesgo. 5.1 Validación del modelo Utilizamos el conjunto de datos de las reseñas de TripAdvisor para validar el modelo de comportamiento presentado anteriormente. Dividimos por conveniencia los valores de calificación en tres rangos: malo (B = {1, 2}), indiferente (I = {3, 4}), y bueno (G = {5}), y realizamos los siguientes dos tests: • Primero, usaremos nuestro modelo para predecir las calificaciones que tienen valores extremos. Para cada hotel, tomamos la secuencia de informes, y cada vez que nos encontramos con una calificación que sea buena o mala (pero no indiferente) intentamos predecirla utilizando la Ecuación (2) • En segundo lugar, en lugar de predecir el valor de las calificaciones extremas, intentamos clasificarlas como buenas o malas. Para cada hotel tomamos la secuencia de informes, y para cada informe (independientemente de su valor) lo clasificamos como bueno o malo. Sin embargo, para realizar estas pruebas, necesitamos estimar el valor objetivo, vf, que es el promedio de las observaciones de calidad reales, vi f. El algoritmo que estamos utilizando se basa en la intuición de que la cantidad de calificación de conformidad se minimiza. En otras palabras, el valor vf debe ser tal que, tan a menudo como sea posible, las calificaciones malas sigan a expectativas por encima de vf y las calificaciones buenas sigan a expectativas por debajo de vf. Formalmente, definimos los conjuntos: Γ1 = {i|ef (i) < vf y ri f ∈ B}; Γ2 = {i|ef (i) > vf y ri f ∈ G}; que corresponden a irregularidades donde, a pesar de que la expectativa en el punto i es menor que el valor entregado, la calificación es baja, y viceversa. Definimos vf como el valor que minimiza la unión de los dos conjuntos: vf = arg min vf |Γ1 ∪ Γ2| (3) En la Ec. (2) reemplazamos vi f por el valor vf calculado en la Ec. (3), y utilizamos la siguiente función de distancia: d(vf , ef (i)|wi f ) = |vf − ef (i)| vf − ef (i) |vf 2 − ef (i)2 | · (1 + 2wi f ); La constante c ∈ I se estableció en min{max{ef (i), 3}}, 4}. Los valores para δf se fijaron en {0.7, 0.7, 0.8, 0.7, 0.6} para las características {General, Habitaciones, Servicio, Limpieza, Valor} respectivamente. Los pesos se calculan tal como se describe en la Sección 3. Como primer experimento, tomamos los conjuntos de calificaciones extremas {ri f |ri f /∈ I} para cada hotel y característica. Para cada calificación de este tipo, ri f, intentamos estimarla calculando ˆri f usando la Ecuación (2). Comparamos este estimador con el obtenido simplemente promediando las calificaciones de todos los hoteles y características: es decir, ¯rf = j,r j f =0 rj f j,r j f =0 1; la Tabla 7 presenta la relación entre el error cuadrático medio (RMSE) al usar ˆri f y ¯rf para estimar las calificaciones reales. En todos los casos, la estimación producida por nuestro modelo es mejor que el promedio simple. Tabla 7: Promedio de RMSE(ˆrf) RMSE(¯rf) Ciudad O R S C V Boston 0.987 0.849 0.879 0.776 0.913 Sídney 0.927 0.817 0.826 0.720 0.681 Las Vegas 0.952 0.870 0.881 0.947 0.904 Como segundo experimento, intentamos distinguir los conjuntos Bf = {i|ri f ∈ B} y Gf = {i|ri f ∈ G} de calificaciones malas y buenas, respectivamente, en la característica f. Por ejemplo, calculamos el conjunto Bf usando el siguiente clasificador (llamado σ): ri f ∈ Bf (σf (i) = 1) ⇔ ˆri f ≤ 4; Las Tablas 8, 9 y 10 presentan la Precisión (p), Recall (r) y s = 2pr p+r para el clasificador σ, y lo comparan con un clasificador de mayoría ingenuo, τ, τf (i) = 1 ⇔ |Bf | ≥ |Gf |: Vemos que el recall es siempre mayor para σ y la precisión suele ser ligeramente peor. Para la métrica s, σ tiende a agregar un 140. Tabla 8: Precisión (p), Recall (r), s= 2pr p+r mientras se detectan calificaciones bajas para Boston O R S C V p 0.678 0.670 0.573 0.545 0.610 σ r 0.626 0.659 0.619 0.612 0.694 s 0.651 0.665 0.595 0.577 0.609 p 0.684 0.706 0.647 0.611 0.633 τ r 0.597 0.541 0.410 0.383 0.562 s 0.638 0.613 0.502 0.471 0.595 Tabla 9: Precisión (p), Recall (r), s= 2pr p+r mientras se detectan calificaciones bajas para Las Vegas O R S C V p 0.654 0.748 0.592 0.712 0.583 σ r 0.608 0.536 0.791 0.474 0.610 s 0.630 0.624 0.677 0.569 0.596 p 0.685 0.761 0.621 0.748 0.606 τ r 0.542 0.505 0.767 0.445 0.441 s 0.605 0.607 0.670 0.558 0.511 Mejora del 1-20% sobre τ, mucho mayor en algunos casos para hoteles en Sídney. Esto se debe probablemente a que las reseñas de Sídney son más positivas que las de las ciudades estadounidenses y los casos en los que el número de reseñas negativas supera al de las positivas son raros. Reemplazar el algoritmo de prueba con uno que juega un 1 con una probabilidad igual a la proporción de malas críticas mejora sus resultados para esta ciudad, pero aún es superado por alrededor del 80%. 6. RESUMEN DE RESULTADOS Y CONCLUSIÓN El objetivo de este artículo es explorar los factores que llevan a un usuario a enviar una calificación particular, en lugar de los incentivos que lo animaron a enviar un informe en primer lugar. Para ello utilizamos dos fuentes adicionales de información además del vector de calificaciones numéricas: primero, examinamos los comentarios textuales que acompañan las reseñas, y segundo, consideramos los informes que han sido previamente enviados por otros usuarios. Usando algoritmos simples de procesamiento de lenguaje natural, pudimos establecer una correlación entre el peso de una característica específica en el comentario textual que acompaña la reseña, y el ruido presente en la calificación numérica. Específicamente, parece que los usuarios que discuten ampliamente una característica en particular tienden a estar de acuerdo en una calificación común. Esta observación permite la construcción de estimadores de calidad característica por característica que tienen una varianza más baja y, con suerte, son menos ruidosos. Sin embargo, se requiere más evidencia para respaldar la intuición de que las calificaciones correspondientes a pesos altos son opiniones de expertos que merecen ser consideradas de mayor prioridad al calcular estimaciones de calidad. En segundo lugar, enfatizamos la dependencia de las calificaciones en informes previos. Los informes anteriores crean una expectativa de calidad que afecta la percepción subjetiva del usuario. Validamos dos hechos sobre las reseñas de hoteles que recopilamos de TripAdvisor: Primero, las calificaciones que siguen expectativas bajas (donde la expectativa se calcula como el promedio de los informes anteriores) probablemente sean más altas que las calificaciones que siguen expectativas altas. Intuitivamente, la percepción de la calidad (y consecuentemente la calificación) depende de qué tan bien la experiencia real del usuario cumple con sus expectativas. Segundo, incluimos evidencia de los comentarios textuales, y encontramos que cuando los usuarios dedican una gran parte del texto a discutir una característica específica, es probable que motiven una calificación divergente (es decir, una calificación que no se ajusta a la expectativa previa). Intuitivamente, esto respalda la hipótesis de que los foros de reseñas actúan como grupos de discusión donde los usuarios están interesados en presentar y motivar su propia opinión. Hemos capturado la evidencia empírica en un modelo de comportamiento que predice las calificaciones enviadas por los usuarios. La calificación final depende, como era de esperar, de la observación real y de la brecha entre la observación y la expectativa. La brecha tiende a tener una influencia mayor cuando una fracción importante del comentario textual está dedicada a discutir una característica específica. El modelo propuesto fue validado con los datos empíricos y proporciona mejores estimaciones de las calificaciones realmente enviadas. Una suposición que hacemos es sobre la existencia de un valor de calidad objetivo vf para la característica f. Esto rara vez es cierto, especialmente a lo largo de largos periodos de tiempo. Otras explicaciones podrían dar cuenta de la correlación de las calificaciones con informes anteriores. Por ejemplo, si ef (i) refleja el valor real de f en un punto en el tiempo, la diferencia en las calificaciones siguiendo expectativas altas y bajas puede ser explicada por modelos de ingresos hoteleros que se maximizan cuando el valor es modificado en consecuencia. Sin embargo, la idea de que la variación en las calificaciones no es principalmente una función de la variación en el valor resulta ser útil. Nuestro enfoque para aproximar este valor objetivo es de ninguna manera perfecto, pero se ajusta perfectamente a la idea detrás del modelo. Una dirección natural para trabajos futuros es examinar aplicaciones concretas de nuestros resultados. Es probable que se obtengan mejoras significativas en las estimaciones de calidad al incorporar toda la evidencia empírica sobre el comportamiento de calificación. No está claro exactamente cómo diferentes factores afectan las decisiones de los usuarios. La respuesta podría depender de la aplicación particular, el contexto y la cultura. 7. REFERENCIAS [1] A. Admati y P. Pfleiderer. Noisytalk.com: Transmitiendo opiniones en un entorno ruidoso. Documento de trabajo 1670R, Universidad de Stanford, 2000. [2] P. B., L. Lee y S. Vaithyanathan. ¿Clasificación de sentimientos con técnicas de aprendizaje automático? En Actas de EMNLP-02, la Conferencia sobre Métodos Empíricos en el Procesamiento del Lenguaje Natural, 2002. [3] H. Cui, V. Mittal y M. Datar. Experimentos comparativos 141 sobre clasificación de sentimientos para reseñas de productos en línea. En Actas de AAAI, 2006. [4] K. Dave, S. Lawrence y D. Pennock. Extracción de opiniones y clasificación semántica de reseñas de productos en la galería de cacahuetes. En Actas de la 12ª Conferencia Internacional sobre la World Wide Web (WWW03), 2003. [5] C. Dellarocas, N. Awad y X. Zhang. Explorando el valor de las calificaciones de productos en línea en la previsión de ingresos: El caso de las películas en movimiento. Documento de trabajo, 2006. [6] C. Forman, A. Ghose y B. Wiesenfeld. Un Examen Multinivel del Impacto de las Identidades Sociales en las Transacciones Económicas en los Mercados Electrónicos. Disponible en SSRN: http://ssrn.com/abstract=918978, julio de 2006. [7] A. Ghose, P. Ipeirotis y A. Sundararajan. Primas de reputación en mercados electrónicos peer-to-peer: análisis de retroalimentación textual y estructura de red. En el Tercer Taller sobre Economía de Sistemas Peer-to-Peer (P2PECON), 2005. [8] A. Ghose, P. Ipeirotis y A. Sundararajan. Las dimensiones de la reputación en los mercados electrónicos. Documento de trabajo CeDER-06-02, Universidad de Nueva York, 2006. [9] A. Harmon. Error de Amazon revela la guerra de los revisores. The New York Times, 14 de febrero de 2004. [10] D. Houser y J. Wooders. Reputación en subastas: teoría y evidencia de eBay. Revista de Estrategia Económica y de Gestión, 15:353-369, 2006. [11] M. Hu y B. Liu. Extracción y resumen de reseñas de clientes. En Actas de la Conferencia Internacional de la ACM SIGKDD sobre Descubrimiento de Conocimiento y Minería de Datos (KDD04), 2004. [12] N. Hu, P. Pavlou y J. Zhang. ¿Pueden las reseñas en línea revelar la verdadera calidad de un producto? En Actas de la Conferencia ACM sobre Comercio Electrónico (EC 06), 2006. [13] K. Kalyanam y S. McIntyre. Retorno de la reputación en el mercado de subastas en línea. Documento de trabajo 02/03-10-WP, Escuela de Negocios Leavey, Universidad de Santa Clara, 2001. [14] L. Khopkar y P. Resnick. Auto-selección, deslizamiento, salvamento, holgazanería y lapidación: los impactos de la retroalimentación negativa en eBay. En Actas de la Conferencia ACM sobre Comercio Electrónico (EC 05), 2005. [15] M. Melnik y J. Alm. ¿Importa la reputación de un vendedor? evidencia de subastas en eBay. Revista de Economía Industrial, 50(3):337-350, 2002. [16] R. Olshavsky y J. Miller. Expectativas del consumidor, rendimiento del producto y calidad percibida del producto. Revista de Investigación de Marketing, 9:19-21, febrero de 1972. [17] A. Parasuraman, V. Zeithaml y L. Berry. Un Modelo Conceptual de Calidad de Servicio y sus Implicaciones para Investigaciones Futuras. Revista de Marketing, 49:41-50, 1985. [18] A. Parasuraman, V. Zeithaml y L. Berry. SERVQUAL: Una escala de múltiples ítems para medir las percepciones de calidad del servicio por parte del consumidor. Revista de Retail, 64:12-40, 1988. [19] P. Pavlou y A. Dimoka. La naturaleza y función de los comentarios de texto de retroalimentación en los mercados en línea: implicaciones para la construcción de confianza, primas de precio y diferenciación de vendedores. Investigación de Sistemas de Información, 17(4):392-414, 2006. [20] A. Popescu y O. Etzioni. Extrayendo características del producto y opiniones de las reseñas. En Actas de la Conferencia de Tecnología del Lenguaje Humano y Conferencia sobre Métodos Empíricos en Procesamiento del Lenguaje Natural, 2005. [21] R. Teas. Expectativas, Evaluación del Rendimiento y Percepciones de Calidad de los Consumidores. Revista de Marketing, 57:18-34, 1993. [22] E. White. Chateando a un cantante en las listas de éxitos pop. El Wall Street Journal, 15 de octubre de 1999. APÉNDICE A. LISTA DE PALABRAS, LR, ASOCIADAS A LA CARACTERÍSTICA HABITACIONES. Todas las palabras sirven como prefijos: habitación, espacio, interior, decoración, ambiente, atmósfera, comodidad, baño, inodoro, cama, edificio, pared, ventana, privado, temperatura, sábana, lino, almohada, caliente, agua, fría, agua, ducha, vestíbulo, muebles, alfombra, aire, acondicionado, colchón, distribución, diseño, espejo, techo, iluminación, lámpara, sofá, silla, cómoda, armario, armario. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "clear incentive absence": {
            "translated_key": "ausencia clara de incentivos",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Understanding User Behavior in Online Feedback Reporting Arjun Talwar Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland arjun@math.stanford.edu Radu Jurca Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland radu.jurca@epfl.ch Boi Faltings Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland boi.faltings@epfl.ch ABSTRACT Online reviews have become increasingly popular as a way to judge the quality of various products and services.",
                "Previous work has demonstrated that contradictory reporting and underlying user biases make judging the true worth of a service difficult.",
                "In this paper, we investigate underlying factors that influence user behavior when reporting feedback.",
                "We look at two sources of information besides numerical ratings: linguistic evidence from the textual comment accompanying a review, and patterns in the time sequence of reports.",
                "We first show that groups of users who amply discuss a certain feature are more likely to agree on a common rating for that feature.",
                "Second, we show that a users rating partly reflects the difference between true quality and prior expectation of quality as inferred from previous reviews.",
                "Both give us a less noisy way to produce rating estimates and reveal the reasons behind user bias.",
                "Our hypotheses were validated by statistical evidence from hotel reviews on the TripAdvisor website.",
                "Categories and Subject Descriptors J.4 [Social and Behavioral Sciences]: Economics General Terms Economics, Experimentation, Reliability 1.",
                "MOTIVATIONS The spread of the internet has made it possible for online feedback forums (or reputation mechanisms) to become an important channel for Word-of-mouth regarding products, services or other types of commercial interactions.",
                "Numerous empirical studies [10, 15, 13, 5] show that buyers seriously consider online feedback when making purchasing decisions, and are willing to pay reputation premiums for products or services that have a good reputation.",
                "Recent analysis, however, raises important questions regarding the ability of existing forums to reflect the real quality of a product.",
                "In the absence of clear incentives, users with a moderate outlook will not bother to voice their opinions, which leads to an unrepresentative sample of reviews.",
                "For example, [12, 1] show that Amazon1 ratings of books or CDs follow with great probability bi-modal, U-shaped distributions where most of the ratings are either very good, or very bad.",
                "Controlled experiments, on the other hand, reveal opinions on the same items that are normally distributed.",
                "Under these circumstances, using the arithmetic mean to predict quality (as most forums actually do) gives the typical user an estimator with high variance that is often false.",
                "Improving the way we aggregate the information available from online reviews requires a deep understanding of the underlying factors that bias the rating behavior of users.",
                "Hu et al. [12] propose the Brag-and-Moan Model where users rate only if their utility of the product (drawn from a normal distribution) falls outside a median interval.",
                "The authors conclude that the model explains the empirical distribution of reports, and offers insights into smarter ways of estimating the true quality of the product.",
                "In the present paper we extend this line of research, and attempt to explain further facts about the behavior of users when reporting online feedback.",
                "Using actual hotel reviews from the TripAdvisor2 website, we consider two additional sources of information besides the basic numerical ratings submitted by users.",
                "The first is simple linguistic evidence from the textual review that usually accompanies the numerical ratings.",
                "We use text-mining techniques similar to [7] and [3], however, we are only interested in identifying what aspects of the service the user is discussing, without computing the semantic orientation of the text.",
                "We find that users who comment more on the same feature are more likely to agree on a common numerical rating for that particular feature.",
                "Intuitively, lengthy comments reveal the importance of the feature to the user.",
                "Since people tend to be more knowledgeable in the aspects they consider important, users who discuss a given feature in more details might be assumed to have more authority in evaluating that feature.",
                "Second we investigate the relationship between a review 1 http://www.amazon.com 2 http://www.tripadvisor.com/ 134 Figure 1: The TripAdvisor page displaying reviews for a popular Boston hotel.",
                "Name of hotel and advertisements were deliberatively erased. and the reviews that preceded it.",
                "A perusal of online reviews shows that ratings are often part of discussion threads, where one post is not necessarily independent of other posts.",
                "One may see, for example, users who make an effort to contradict, or vehemently agree with, the remarks of previous users.",
                "By analyzing the time sequence of reports, we conclude that past reviews influence the future reports, as they create some prior expectation regarding the quality of service.",
                "The subjective perception of the user is influenced by the gap between the prior expectation and the actual performance of the service [17, 18, 16, 21] which will later reflect in the users rating.",
                "We propose a model that captures the dependence of ratings on prior expectations, and validate it using the empirical data we collected.",
                "Both results can be used to improve the way reputation mechanisms aggregate the information from individual reviews.",
                "Our first result can be used to determine a featureby-feature estimate of quality, where for each feature, a different subset of reviews (i.e., those with lengthy comments of that feature) is considered.",
                "The second leads to an algorithm that outputs a more precise estimate of the real quality. 2.",
                "THE DATA SET We use in this paper real hotel reviews collected from the popular travel site TripAdvisor.",
                "TripAdvisor indexes hotels from cities across the world, along with reviews written by travelers.",
                "Users can search the site by giving the hotels name and location (optional).",
                "The reviews for a given hotel are displayed as a list (ordered from the most recent to the oldest), with 5 reviews per page.",
                "The reviews contain: • information about the author of the review (e.g., dates of stay, username of the reviewer, location of the reviewer); • the overall rating (from 1, lowest, to 5, highest); • a textual review containing a title for the review, free comments, and the main things the reviewer liked and disliked; • numerical ratings (from 1, lowest, to 5, highest) for different features (e.g., cleanliness, service, location, etc.)",
                "Below the name of the hotel, TripAdvisor displays the address of the hotel, general information (number of rooms, number of stars, short description, etc), the average overall rating, the TripAdvisor ranking, and an average rating for each feature.",
                "Figure 1 shows the page for a popular Boston hotel whose name (along with advertisements) was explicitly erased.",
                "We selected three cities for this study: Boston, Sydney and Las Vegas.",
                "For each city we considered all hotels that had at least 10 reviews, and recorded all reviews.",
                "Table 1 presents the number of hotels considered in each city, the total number of reviews recorded for each city, and the distribution of hotels with respect to the star-rating (as available on the TripAdvisor site).",
                "Note that not all hotels have a star-rating.",
                "Table 1: A summary of the data set.",
                "City # Reviews # Hotels # of Hotels with 1,2,3,4 & 5 stars Boston 3993 58 1+3+17+15+2 Sydney 1371 47 0+0+9+13+10 Las Vegas 5593 40 0+3+10+9+6 For each review we recorded the overall rating, the textual review (title and body of the review) and the numerical rating on 7 features: Rooms(R), Service(S), Cleanliness(C), Value(V), Food(F), Location(L) and Noise(N).",
                "TripAdvisor does not require users to submit anything other than the overall rating, hence a typical review rates few additional features, regardless of the discussion in the textual comment.",
                "Only the features Rooms(R), Service(S), Cleanliness(C) and Value(V) are rated by a significant number of users.",
                "However, we also selected the features Food(F), Location(L) and Noise(N) because they are referred to in a significant number of textual comments.",
                "For each feature we record the numerical rating given by the user, or 0 when the rating is missing.",
                "The typical length of the textual comment amounts to approximately 200 words.",
                "All data was collected by crawling the TripAdvisor site in September 2006. 2.1 Formal notation We will formally refer to a review by a tuple (r, T) where: • r = (rf ) is a vector containing the ratings rf ∈ {0, 1, . . . 5} for the features f ∈ F = {O, R, S, C, V, F, L, N}; note that the overall rating, rO, is abusively recorded as the rating for the feature Overall(O); • T is the textual comment that accompanies the review. 135 Reviews are indexed according to the variable i, such that (ri , Ti ) is the ith review in our database.",
                "Since we dont record the username of the reviewer, we will also say that the ith review in our data set was submitted by user i.",
                "When we need to consider only the reviews of a given hotel, h, we will use (ri(h) , Ti(h) ) to denote the ith review about the hotel h. 3.",
                "EVIDENCE FROM TEXTUAL COMMENTS The free textual comments associated to online reviews are a valuable source of information for understanding the reasons behind the numerical ratings left by the reviewers.",
                "The text may, for example, reveal concrete examples of aspects that the user liked or disliked, thus justifying some of the high, respectively low ratings for certain features.",
                "The text may also offer guidelines for understanding the preferences of the reviewer, and the weights of different features when computing an overall rating.",
                "The problem, however, is that free textual comments are difficult to read.",
                "Users are required to scroll through many reviews and read mostly repetitive information.",
                "Significant improvements would be obtained if the reviews were automatically interpreted and aggregated.",
                "Unfortunately, this seems a difficult task for computers since human users often use witty language, abbreviations, cultural specific phrases, and the figurative style.",
                "Nevertheless, several important results use the textual comments of online reviews in an automated way.",
                "Using well established natural language techniques, reviews or parts of reviews can be classified as having a positive or negative semantic orientation.",
                "Pang et al. [2] classify movie reviews into positive/negative by training three different classifiers (Naive Bayes, Maximum Entropy and SVM) using classification features based on unigrams, bigrams or part-of-speech tags.",
                "Dave et al. [4] analyze reviews from CNet and Amazon, and surprisingly show that classification features based on unigrams or bigrams perform better than higher-order n-grams.",
                "This result is challenged by Cui et al. [3] who look at large collections of reviews crawled from the web.",
                "They show that the size of the data set is important, and that bigger training sets allow classifiers to successfully use more complex classification features based on n-grams.",
                "Hu and Liu [11] also crawl the web for product reviews and automatically identify product attributes that have been discussed by reviewers.",
                "They use Wordnet to compute the semantic orientation of product evaluations and summarize user reviews by extracting positive and negative evaluations of different product features.",
                "Popescu and Etzioni [20] analyze a similar setting, but use search engine hit-counts to identify product attributes; the semantic orientation is assigned through the relaxation labeling technique.",
                "Ghose et al. [7, 8] analyze seller reviews from the Amazon secondary market to identify the different dimensions (e.g., delivery, packaging, customer support, etc.) of reputation.",
                "They parse the text, and tag the part-of-speech for each word.",
                "Frequent nouns, noun phrases and verbal phrases are identified as dimensions of reputation, while the corresponding modifiers (i.e., adjectives and adverbs) are used to derive numerical scores for each dimension.",
                "The enhanced reputation measure correlates better with the pricing information observed in the market.",
                "Pavlou and Dimoka [19] analyze eBay reviews and find that textual comments have an important impact on reputation premiums.",
                "Our approach is similar to the previously mentioned works, in the sense that we identify the aspects (i.e., hotel features) discussed by the users in the textual reviews.",
                "However, we do not compute the semantic orientation of the text, nor attempt to infer missing ratings.",
                "We define the weight, wi f , of feature f ∈ F in the text Ti associated with the review (ri , Ti ), as the fraction of Ti dedicated to discussing aspects (both positive and negative) related to feature f. We propose an elementary method to approximate the values of these weights.",
                "For each feature we manually construct the word list Lf containing approximately 50 words that are most commonly associated to the feature f. The initial words were selected from reading some of the reviews, and seeing what words coincide with discussion of which features.",
                "The list was then extended by adding all thesaurus entries that were related to the initial words.",
                "Finally, we brainstormed for missing words that would normally be associated with each of the features.",
                "Let Lf ∩Ti be the list of terms common to both Lf and Ti.",
                "Each term of Lf is counted the number of times it appears in Ti , with two exceptions: • in cases where the user submits a title to the review, we account for the title text by appending it three times to the review text Ti .",
                "The intuitive assumption is that the users opinion is more strongly reflected in the title, rather than in the body of the review.",
                "For example, many reviews are accurately summarized by titles such as Excellent service, terrible location or Bad value for money; • certain words that occur only once in the text are counted multiple times if their relevance to that feature is particularly strong.",
                "These were root words for each feature (e.g., staff is a root word for the feature Service), and were weighted either 2 or 3.",
                "Each feature was assigned up to 3 such root words, so almost all words are counted only once.",
                "The list of words for the feature Rooms is given for reference in Appendix A.",
                "The weight wi f is computed as: wi f = |Lf ∩ Ti| f∈F |Lf ∩ Ti| (1) where |Lf ∩Ti | is the number of terms common to Lf and Ti .",
                "The weight for the feature Overall was set to min{ |T i | 5000 , 1} where |Ti | is the number of character in Ti .",
                "The following is a TripAdvisor review for a Boston hotel (the name of the hotel is omitted): Ill start by saying that Im more of a Holiday Inn person than a *** type.",
                "So I get frustrated when I pay double the room rate and get half the amenities that Id get at a Hampton Inn or Holiday Inn.",
                "The location was definitely the main asset of this place.",
                "It was only a few blocks from the Hynes Center subway stop and it was easy to walk to some good restaurants in the Back Bay area.",
                "Boylston isnt far off at all.",
                "So I had no trouble with foregoing a rental car and taking the subway from the airport to the hotel and using the subway for any other travel.",
                "Otherwise, they make you pay for anything and everything. 136 And when youve already dropped $215/night on the room, that gets frustrating.The room itself was decent, about what I would expect.",
                "Staff was also average, not bad and not excellent.",
                "Again, I think youre paying for location and the ability to walk to a lot of good stuff.",
                "But I think next time Ill stay in Brookline, get more amenities, and use the subway a bit more.",
                "This numerical ratings associated to this review are rO = 3, rR = 3, rS = 3, rC = 4, rV = 2 for features Overall(O), Rooms(R), Service(S), Cleanliness(C) and Value(V) respectively.",
                "The ratings for the features Food(F), Location(L) and Noise(N) are absent (i.e., rF = rL = rN = 0).",
                "The weights wf are computed from the following lists of common terms: LR ∩ T ={room}; wR = 0.066 LS ∩ T ={3 * Staff, amenities}; wS = 0.267 LC ∩ T = ∅; wC = 0 LV ∩ T ={$, rate}; wV = 0.133 LF ∩ T ={restaurant}; wF = 0.067 LL ∩ T ={2 * center, 2 * walk, 2 * location, area}; wL = 0.467 LN ∩ T = ∅; wN = 0 The root words Staff and Center were tripled and doubled respectively.",
                "The overall weight of the textual review is wO = 0.197.",
                "These values account reasonably well for the weights of different features in the discussion of the reviewer.",
                "One point to note is that some terms in the lists Lf possess an inherent semantic orientation.",
                "For example the word grime (belonging to the list LC ) would be used most often to assert the presence, and not the absence of grime.",
                "This is unavoidable, but care was taken to ensure words from both sides of the spectrum were used.",
                "For this reason, some lists such as LR contain only nouns of objects that one would typically describe in a room (see Appendix A).",
                "The goal of this section is to analyse the influence of the weights wi f on the numerical ratings ri f .",
                "Intuitively, users who spent a lot of their time discussing a feature f (i.e., wi f is high) had something to say about their experience with regard to this feature.",
                "Obviously, feature f is important for user i.",
                "Since people tend to be more knowledgeable in the aspects they consider important, our hypothesis is that the ratings ri f (corresponding to high weights wi f ) constitute a subset of expert ratings for feature f. Figure 2 plots the distribution of the rates r i(h) C with respect to the weights w i(h) C for the cleanliness of a Las Vegas hotel, h. Here, the high ratings are restricted to the reviews that discuss little the cleanliness.",
                "Whenever cleanliness appears in the discussion, the ratings are low.",
                "Many hotels exhibit similar rating patterns for various features.",
                "Ratings corresponding to low weights span the whole spectrum from 1 to 5, while the ratings corresponding to high weights are more grouped together (either around good or bad ratings).",
                "We therefore make the following hypothesis: Hypothesis 1.",
                "The ratings ri f corresponding to the reviews where wi f is high, are more similar to each other than to the overall collection of ratings.",
                "To test the hypothesis, we take the entire set of reviews, and feature by feature, we compute the standard deviation of the ratings with high weights, and the standard deviation of the entire set of ratings.",
                "High weights were defined as those belonging to the upper 20% of the weight range for the corresponding feature.",
                "If Hypothesis 1 were true, the standard deviation of all ratings should be higher than the standard deviation of the ratings with high weights. 0 1 2 3 4 5 6 0 0.1 0.2 0.3 0.4 0.5 0.6 Rating Weight Figure 2: The distribution of ratings against the weight of the cleanliness feature.",
                "We use a standard T-test to measure the significance of the results.",
                "City by city and feature by feature, Table 2 presents the average standard deviation of all ratings, and the average standard deviation of ratings with high weights.",
                "Indeed, the ratings with high weights have lower standard deviation, and the results are significant at the standard 0.05 significance threshold (although for certain cities taken independently there doesnt seem to be a significant difference, the results are significant for the entire data set).",
                "Please note that only the features O,R,S,C and V were considered, since for the others (F, L, and N) we didnt have enough ratings.",
                "Table 2: Average standard deviation for all ratings, and average standard deviation for ratings with high weights.",
                "In square brackets, the corresponding p-values for a positive difference between the two.",
                "City O R S C V all 1.189 0.998 1.144 0.935 1.123 Boston high 0.948 0.778 0.954 0.767 0.891 p-val [0.000] [0.004] [0.045] [0.080] [0.009] all 1.040 0.832 1.101 0.847 0.963 Sydney high 0.801 0.618 0.691 0.690 0.798 p-val [0.012] [0.023] [0.000] [0.377] [0.037] all 1.272 1.142 1.184 1.119 1.242 Vegas high 1.072 0.752 1.169 0.907 1.003 p-val [0.0185] [0.001] [0.918] [0.120] [0.126] Hypothesis 1 not only provides some basic understanding regarding the rating behavior of online users, it also suggests some ways of computing better quality estimates.",
                "We can, for example, construct a feature-by-feature quality estimate with much lower variance: for each feature we take the subset of reviews that amply discuss that feature, and output as a quality estimate the average rating for this subset.",
                "Initial experiments suggest that the average feature-by-feature ratings computed in this way are different from the average ratings computed on the whole data set.",
                "Given that, indeed, high weights are indicators of expert opinions, the estimates obtained in this way are more accurate than the current ones.",
                "Nevertheless, the validation of this underlying assumption requires further controlled experiments. 137 4.",
                "THE INFLUENCE OF PAST RATINGS Two important assumptions are generally made about reviews submitted to online forums.",
                "The first is that ratings truthfully reflect the quality observed by the users; the second is that reviews are independent from one another.",
                "While anecdotal evidence [9, 22] challenges the first assumption3 , in this section, we address the second.",
                "A perusal of online reviews shows that reviews are often part of discussion threads, where users make an effort to contradict, or vehemently agree with the remarks of previous users.",
                "Consider, for example, the following review: I dont understand the negative reviews... the hotel was a little dark, but that was the style.",
                "It was very artsy.",
                "Yes it was close to the freeway, but in my opinion the sound of an occasional loud car is better than hearing the ding ding of slot machines all night!",
                "The staff on-hand is FABULOUS.",
                "The waitresses are great (and *** does not deserve the bad review she got, she was 100% attentive to us! ), the bartenders are friendly and professional at the same time...",
                "Here, the user was disturbed by previous negative reports, addressed these concerns, and set about trying to correct them.",
                "Not surprisingly, his ratings were considerably higher than the average ratings up to this point.",
                "It seems that TripAdvisor users regularly read the reports submitted by previous users before booking a hotel, or before writing a review.",
                "Past reviews create some prior expectation regarding the quality of service, and this expectation has an influence on the submitted review.",
                "We believe this observation holds for most online forums.",
                "The subjective perception of quality is directly proportional to how well the actual experience meets the prior expectation, a fact confirmed by an important line of econometric and marketing research [17, 18, 16, 21].",
                "The correlation between the reviews has also been confirmed by recent research on the dynamics of online review forums [6]. 4.1 Prior Expectations We define the prior expectation of user i regarding the feature f, as the average of the previously available ratings on the feature f4 : ef (i) = j<i,r j f =0 rj f j<i,r j f =0 1 As a first hypothesis, we assert that the rating ri f is a function of the prior expectation ef (i): Hypothesis 2.",
                "For a given hotel and feature, given the reviews i and j such that ef (i) is high and ef (j) is low, the rating rj f exceeds the rating ri f .",
                "We define high and low expectations as those that are above, respectively below a certain cutoff value θ.",
                "The set of reviews preceded by high, respectively low expectations 3 part of Amazon reviews were recognized as strategic posts by book authors or competitors 4 if no previous ratings were assigned for feature f, ef (i) is assigned a default value of 4.",
                "Table 3: Average ratings for reviews preceded by low (first value in the cell) and high (second value in the cell) expectations.",
                "The P-values for a positive difference are given square brackets.",
                "City O R S C V 3.953 4.045 3.985 4.252 3.946 Boston 3.364 3.590 3.485 3.641 3.242 [0.011] [0.028] [0.0086] [0.0168] [0.0034] 4.284 4.358 4.064 4.530 4.428 Sydney 3.756 3.537 3.436 3.918 3.495 [0.000] [0.000] [0.035] [0.009] [0.000] 3.494 3.674 3.713 3.689 3.580 Las Vegas 3.140 3.530 2.952 3.530 3.351 [0.190] [0.529] [0.007] [0.529] [0.253] are defined as follows: Rhigh f = {ri f |ef (i) > θ} Rlow f = {ri f |ef (i) < θ} These sets are specific for each (hotel, feature) pair, and in our experiments we took θ = 4.",
                "This rather high value is close to the average rating across all features across all hotels, and is justified by the fact that our data set contains mostly high quality hotels.",
                "For each city, we take all hotels and compute the average ratings in the sets Rhigh f and Rlow f (see Table 3).",
                "The average rating amongst reviews following low prior expectations is significantly higher than the average rating following high expectations.",
                "As further evidence, we consider all hotels for which the function eV (i) (the expectation for the feature Value) has a high value (greater than 4) for some i, and a low value (less than 4) for some other i.",
                "Intuitively, these are the hotels for which there is a minimal degree of variation in the timely sequence of reviews: i.e., the cumulative average of ratings was at some point high and afterwards became low, or vice-versa.",
                "Such variations are observed for about half of all hotels in each city.",
                "Figure 3 plots the median (across considered hotels) rating, rV , when ef (i) is not more than x but greater than x − 0.5. 2.5 3 3.5 4 4.5 5 2.5 3 3.5 4 4.5 5 Medianofrating expectation Boston Sydney Vegas Figure 3: The ratings tend to decrease as the expectation increases. 138 There are two ways to interpret the function ef (i): • The expected value for feature f obtained by user i before his experience with the service, acquired by reading reports submitted by past users.",
                "In this case, an overly high value for ef (i) would drive the user to submit a negative report (or vice versa), stemming from the difference between the actual value of the service, and the inflated expectation of this value acquired before his experience. • The expected value of feature f for all subsequent visitors of the site, if user i were not to submit a report.",
                "In this case, the motivation for a negative report following an overly high value of ef is different: user i seeks to correct the expectation of future visitors to the site.",
                "Unlike the interpretation above, this does not require the user to derive an a priori expectation for the value of f. Note that neither interpretation implies that the average up to report i is inversely related to the rating at report i.",
                "There might exist a measure of influence exerted by past reports that pushes the user behind report i to submit ratings which to some extent conforms with past reports: a low value for ef (i) can influence user i to submit a low rating for feature f because, for example, he fears that submitting a high rating will make him out to be a person with low standards5 .",
                "This, at first, appears to contradict Hypothesis 2.",
                "However, this conformity rating cannot continue indefinitely: once the set of reports project a sufficiently deflated estimate for vf , future reviewers with comparatively positive impressions will seek to correct this misconception. 4.2 Impact of textual comments on quality expectation Further insight into the rating behavior of TripAdvisor users can be obtained by analyzing the relationship between the weights wf and the values ef (i).",
                "In particular, we examine the following hypothesis: Hypothesis 3.",
                "When a large proportion of the text of a review discusses a certain feature, the difference between the rating for that feature and the average rating up to that point tends to be large.",
                "The intuition behind this claim is that when the user is adamant about voicing his opinion regarding a certain feature, his opinion differs from the collective opinion of previous postings.",
                "This relies on the characteristic of reputation systems as feedback forums where a user is interested in projecting his opinion, with particular strength if this opinion differs from what he perceives to be the general opinion.",
                "To test Hypothesis 3 we measure the average absolute difference between the expectation ef (i) and the rating ri f when the weight wi f is high, respectively low.",
                "Weights are classified high or low by comparing them with certain cutoff values: wi f is low if smaller than 0.1, while wi f is high if greater than θf .",
                "Different cutoff values were used for different features: θR = 0.4, θS = 0.4, θC = 0.2, and θV = 0.7.",
                "Cleanliness has a lower cutoff since it is a feature rarely discussed; Value has a high cutoff for the opposite reason.",
                "Results are presented in Table 4. 5 The idea that negative reports can encourage further negative reporting has been suggested before [14] Table 4: Average of |ri f −ef (i)| when weights are high (first value in the cell) and low (second value in the cell) with P-values for the difference in sq. brackets.",
                "City R S C V 1.058 1.208 1.728 1.356 Boston 0.701 0.838 0.760 0.917 [0.022] [0.063] [0.000] [0.218] 1.048 1.351 1.218 1.318 Sydney 0.752 0.759 0.767 0.908 [0.179] [0.009] [0.165] [0.495] 1.184 1.378 1.472 1.642 Las Vegas 0.772 0.834 0.808 1.043 [0.071] [0.020] [0.006] [0.076] This demonstrates that when weights are unusually high, users tend to express an opinion that does not conform to the net average of previous ratings.",
                "As we might expect, for a feature that rarely was a high weight in the discussion, (e.g., cleanliness) the difference is particularly large.",
                "Even though the difference in the feature Value is quite large for Sydney, the P-value is high.",
                "This is because only few reviews discussed value heavily.",
                "The reason could be cultural or because there was less of a reason to discuss this feature. 4.3 Reporting Incentives Previous models suggest that users who are not highly opinionated will not choose to voice their opinions [12].",
                "In this section, we extend this model to account for the influence of expectations.",
                "The motivation for submitting feedback is not only due to extreme opinions, but also to the difference between the current reputation (i.e., the prior expectation of the user) and the actual experience.",
                "Such a rating model produces ratings that most of the time deviate from the current average rating.",
                "The ratings that confirm the prior expectation will rarely be submitted.",
                "We test on our data set the proportion of ratings that attempt to correct the current estimate.",
                "We define a deviant rating as one that deviates from the current expectation by at least some threshold θ, i.e., |ri f − ef (i)| ≥ θ.",
                "For each of the three considered cities, the following tables, show the proportion of deviant ratings for θ = 0.5 and θ = 1.",
                "Table 5: Proportion of deviant ratings with θ = 0.5 City O R S C V Boston 0.696 0.619 0.676 0.604 0.684 Sydney 0.645 0.615 0.672 0.614 0.675 Las Vegas 0.721 0.641 0.694 0.662 0.724 Table 6: Proportion of deviant ratings with θ = 1 City O R S C V Boston 0.420 0.397 0.429 0.317 0.446 Sydney 0.360 0.367 0.442 0.336 0.489 Las Vegas 0.510 0.421 0.483 0.390 0.472 The above results suggest that a large proportion of users (close to one half, even for the high threshold value θ = 1) deviate from the prior average.",
                "This reinforces the idea that users are more likely to submit a report when they believe they have something distinctive to add to the current stream of opinions for some feature.",
                "Such conclusions are in total agreement with prior evidence that the distribution of reports often follows bi-modal, U-shaped distributions. 139 5.",
                "MODELLING THE BEHAVIOR OF RATERS To account for the observations described in the previous sections, we propose a model for the behavior of the users when submitting online reviews.",
                "For a given hotel, we make the assumption that the quality experienced by the users is normally distributed around some value vf , which represents the objective quality offered by the hotel on the feature f. The rating submitted by user i on feature f is: ˆri f = δf vi f + (1 − δf ) · sign vi f − ef (i) c + d(vi f , ef (i)|wi f ) (2) where: • vi f is the (unknown) quality actually experienced by the user. vi f is assumed normally distributed around some value vf ; • δf ∈ [0, 1] can be seen as a measure of the bias when reporting feedback.",
                "High values reflect the fact that users rate objectively, without being influenced by prior expectations.",
                "The value of δf may depend on various factors; we fix one value for each feature f; • c is a constant between 1 and 5; • wi f is the weight of feature f in the textual comment of review i, computed according to Eq. (1); • d(vi f , ef (i)|wi f ) is a distance function between the expectation and the observation of user i.",
                "The distance function satisfies the following properties: - d(y, z|w) ≥ 0 for all y, z ∈ [0, 5], w ∈ [0, 1]; - |d(y, z|w)| < |d(z, x|w)| if |y − z| < |z − x|; - |d(y, z|w1)| < |d(y, z|w2)| if w1 < w2; - c + d(vf , ef (i)|wi f ) ∈ [1, 5]; The second term of Eq. (2) encodes the bias of the rating.",
                "The higher the distance between the true observation vi f and the function ef , the higher the bias. 5.1 Model Validation We use the data set of TripAdvisor reviews to validate the behavior model presented above.",
                "We split for convenience the rating values in three ranges: bad (B = {1, 2}), indifferent (I = {3, 4}), and good (G = {5}), and perform the following two tests: • First, we will use our model to predict the ratings that have extremal values.",
                "For every hotel, we take the sequence of reports, and whenever we encounter a rating that is either good or bad (but not indifferent) we try to predict it using Eq. (2) • Second, instead of predicting the value of extremal ratings, we try to classify them as either good or bad.",
                "For every hotel we take the sequence of reports, and for each report (regardless of it value) we classify it as being good or bad However, to perform these tests, we need to estimate the objective value, vf , that is the average of the true quality observations, vi f .",
                "The algorithm we are using is based on the intuition that the amount of conformity rating is minimized.",
                "In other words, the value vf should be such that as often as possible, bad ratings follow expectations above vf and good ratings follow expectations below vf .",
                "Formally, we define the sets: Γ1 = {i|ef (i) < vf and ri f ∈ B}; Γ2 = {i|ef (i) > vf and ri f ∈ G}; that correspond to irregularities where even though the expectation at point i is lower than the delivered value, the rating is poor, and vice versa.",
                "We define vf as the value that minimize these union of the two sets: vf = arg min vf |Γ1 ∪ Γ2| (3) In Eq. (2) we replace vi f by the value vf computed in Eq. (3), and use the following distance function: d(vf , ef (i)|wi f ) = |vf − ef (i)| vf − ef (i) |vf 2 − ef (i)2 | · (1 + 2wi f ); The constant c ∈ I was set to min{max{ef (i), 3}}, 4}.",
                "The values for δf were fixed at {0.7, 0.7, 0.8, 0.7, 0.6} for the features {Overall, Rooms, Service, Cleanliness, Value} respectively.",
                "The weights are computed as described in Section 3.",
                "As a first experiment, we take the sets of extremal ratings {ri f |ri f /∈ I} for each hotel and feature.",
                "For every such rating, ri f , we try to estimate it by computing ˆri f using Eq. (2).",
                "We compare this estimator with the one obtained by simply averaging the ratings over all hotels and features: i.e., ¯rf = j,r j f =0 rj f j,r j f =0 1 ; Table 7 presents the ratio between the root mean square error (RMSE) when using ˆri f and ¯rf to estimate the actual ratings.",
                "In all cases the estimate produced by our model is better than the simple average.",
                "Table 7: Average of RMSE(ˆrf ) RMSE(¯rf ) City O R S C V Boston 0.987 0.849 0.879 0.776 0.913 Sydney 0.927 0.817 0.826 0.720 0.681 Las Vegas 0.952 0.870 0.881 0.947 0.904 As a second experiment, we try to distinguish the sets Bf = {i|ri f ∈ B} and Gf = {i|ri f ∈ G} of bad, respectively good ratings on the feature f. For example, we compute the set Bf using the following classifier (called σ): ri f ∈ Bf (σf (i) = 1) ⇔ ˆri f ≤ 4; Tables 8, 9 and 10 present the Precision(p), Recall(r) and s = 2pr p+r for classifier σ, and compares it with a naive majority classifier, τ, τf (i) = 1 ⇔ |Bf | ≥ |Gf |: We see that recall is always higher for σ and precision is usually slightly worse.",
                "For the s metric σ tends to add a 140 Table 8: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Boston O R S C V p 0.678 0.670 0.573 0.545 0.610 σ r 0.626 0.659 0.619 0.612 0.694 s 0.651 0.665 0.595 0.577 0.609 p 0.684 0.706 0.647 0.611 0.633 τ r 0.597 0.541 0.410 0.383 0.562 s 0.638 0.613 0.502 0.471 0.595 Table 9: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Las Vegas O R S C V p 0.654 0.748 0.592 0.712 0.583 σ r 0.608 0.536 0.791 0.474 0.610 s 0.630 0.624 0.677 0.569 0.596 p 0.685 0.761 0.621 0.748 0.606 τ r 0.542 0.505 0.767 0.445 0.441 s 0.605 0.607 0.670 0.558 0.511 1-20% improvement over τ, much higher in some cases for hotels in Sydney.",
                "This is likely because Sydney reviews are more positive than those of the American cities and cases where the number of bad reviews exceeded the number of good ones are rare.",
                "Replacing the test algorithm with one that plays a 1 with probability equal to the proportion of bad reviews improves its results for this city, but it is still outperformed by around 80%. 6.",
                "SUMMARY OF RESULTS AND CONCLUSION The goal of this paper is to explore the factors that drive a user to submit a particular rating, rather than the incentives that encouraged him to submit a report in the first place.",
                "For that we use two additional sources of information besides the vector of numerical ratings: first we look at the textual comments that accompany the reviews, and second we consider the reports that have been previously submitted by other users.",
                "Using simple natural language processing algorithms, we were able to establish a correlation between the weight of a certain feature in the textual comment accompanying the review, and the noise present in the numerical rating.",
                "Specifically, it seems that users who discuss amply a certain feature are likely to agree on a common rating.",
                "This observation allows the construction of feature-by-feature estimators of quality that have a lower variance, and are hopefully less noisy.",
                "Nevertheless, further evidence is required to support the intuition that ratings corresponding to high weights are expert opinions that deserve to be given higher priority when computing estimates of quality.",
                "Second, we emphasize the dependence of ratings on previous reports.",
                "Previous reports create an expectation of quality which affects the subjective perception of the user.",
                "We validate two facts about the hotel reviews we collected from TripAdvisor: First, the ratings following low expectations (where the expectation is computed as the average of the previous reports) are likely to be higher than the ratings Table 10: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Sydney O R S C V p 0.650 0.463 0.544 0.550 0.580 σ r 0.234 0.378 0.571 0.169 0.592 s 0.343 0.452 0.557 0.259 0.586 p 0.562 0.615 0.600 0.500 0.600 τ r 0.054 0.098 0.101 0.015 0.175 s 0.098 0.168 0.172 0.030 0.271 following high expectations.",
                "Intuitively, the perception of quality (and consequently the rating) depends on how well the actual experience of the user meets her expectation.",
                "Second, we include evidence from the textual comments, and find that when users devote a large fraction of the text to discussing a certain feature, they are likely to motivate a divergent rating (i.e., a rating that does not conform to the prior expectation).",
                "Intuitively, this supports the hypothesis that review forums act as discussion groups where users are keen on presenting and motivating their own opinion.",
                "We have captured the empirical evidence in a behavior model that predicts the ratings submitted by the users.",
                "The final rating depends, as expected, on the true observation, and on the gap between the observation and the expectation.",
                "The gap tends to have a bigger influence when an important fraction of the textual comment is dedicated to discussing a certain feature.",
                "The proposed model was validated on the empirical data and provides better estimates of the ratings actually submitted.",
                "One assumption that we make is about the existence of an objective quality value vf for the feature f. This is rarely true, especially over large spans of time.",
                "Other explanations might account for the correlation of ratings with past reports.",
                "For example, if ef (i) reflects the true value of f at a point in time, the difference in the ratings following high and low expectations can be explained by hotel revenue models that are maximized when the value is modified accordingly.",
                "However, the idea that variation in ratings is not primarily a function of variation in value turns out to be a useful one.",
                "Our approach to approximate this elusive objective value is by no means perfect, but conforms neatly to the idea behind the model.",
                "A natural direction for future work is to examine concrete applications of our results.",
                "Significant improvements of quality estimates are likely to be obtained by incorporating all empirical evidence about rating behavior.",
                "Exactly how different factors affect the decisions of the users is not clear.",
                "The answer might depend on the particular application, context and culture. 7.",
                "REFERENCES [1] A. Admati and P. Pfleiderer.",
                "Noisytalk.com: Broadcasting opinions in a noisy environment.",
                "Working Paper 1670R, Stanford University, 2000. [2] P. B., L. Lee, and S. Vaithyanathan.",
                "Thumbs up? sentiment classification using machine learning techniques.",
                "In Proceedings of the EMNLP-02, the Conference on Empirical Methods in Natural Language Processing, 2002. [3] H. Cui, V. Mittal, and M. Datar.",
                "Comparative 141 Experiments on Sentiment Classification for Online Product Reviews.",
                "In Proceedings of AAAI, 2006. [4] K. Dave, S. Lawrence, and D. Pennock.",
                "Mining the peanut gallery:opinion extraction and semantic classification of product reviews.",
                "In Proceedings of the 12th International Conference on the World Wide Web (WWW03), 2003. [5] C. Dellarocas, N. Awad, and X. Zhang.",
                "Exploring the Value of Online Product Ratings in Revenue Forecasting: The Case of Motion Pictures.",
                "Working paper, 2006. [6] C. Forman, A. Ghose, and B. Wiesenfeld.",
                "A Multi-Level Examination of the Impact of Social Identities on Economic Transactions in Electronic Markets.",
                "Available at SSRN: http://ssrn.com/abstract=918978, July 2006. [7] A. Ghose, P. Ipeirotis, and A. Sundararajan.",
                "Reputation Premiums in Electronic Peer-to-Peer Markets: Analyzing Textual Feedback and Network Structure.",
                "In Third Workshop on Economics of Peer-to-Peer Systems, (P2PECON), 2005. [8] A. Ghose, P. Ipeirotis, and A. Sundararajan.",
                "The Dimensions of Reputation in electronic Markets.",
                "Working Paper CeDER-06-02, New York University, 2006. [9] A. Harmon.",
                "Amazon Glitch Unmasks War of Reviewers.",
                "The New York Times, February 14, 2004. [10] D. Houser and J. Wooders.",
                "Reputation in Auctions: Theory and Evidence from eBay.",
                "Journal of Economics and Management Strategy, 15:353-369, 2006. [11] M. Hu and B. Liu.",
                "Mining and summarizing customer reviews.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD04), 2004. [12] N. Hu, P. Pavlou, and J. Zhang.",
                "Can Online Reviews Reveal a Products True Quality?",
                "In Proceedings of ACM Conference on Electronic Commerce (EC 06), 2006. [13] K. Kalyanam and S. McIntyre.",
                "Return on reputation in online auction market.",
                "Working Paper 02/03-10-WP, Leavey School of Business, Santa Clara University., 2001. [14] L. Khopkar and P. Resnick.",
                "Self-Selection, Slipping, Salvaging, Slacking, and Stoning: the Impacts of Negative Feedback at eBay.",
                "In Proceedings of ACM Conference on Electronic Commerce (EC 05), 2005. [15] M. Melnik and J. Alm.",
                "Does a sellers reputation matter? evidence from ebay auctions.",
                "Journal of Industrial Economics, 50(3):337-350, 2002. [16] R. Olshavsky and J. Miller.",
                "Consumer Expectations, Product Performance and Perceived Product Quality.",
                "Journal of Marketing Research, 9:19-21, February 1972. [17] A. Parasuraman, V. Zeithaml, and L. Berry.",
                "A Conceptual Model of Service Quality and Its Implications for Future Research.",
                "Journal of Marketing, 49:41-50, 1985. [18] A. Parasuraman, V. Zeithaml, and L. Berry.",
                "SERVQUAL: A Multiple-Item Scale for Measuring Consumer Perceptions of Service Quality.",
                "Journal of Retailing, 64:12-40, 1988. [19] P. Pavlou and A. Dimoka.",
                "The Nature and Role of Feedback Text Comments in Online Marketplaces: Implications for Trust Building, Price Premiums, and Seller Differentiation.",
                "Information Systems Research, 17(4):392-414, 2006. [20] A. Popescu and O. Etzioni.",
                "Extracting product features and opinions from reviews.",
                "In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, 2005. [21] R. Teas.",
                "Expectations, Performance Evaluation, and Consumers Perceptions of Quality.",
                "Journal of Marketing, 57:18-34, 1993. [22] E. White.",
                "Chatting a Singer Up the Pop Charts.",
                "The Wall Street Journal, October 15, 1999.",
                "APPENDIX A.",
                "LIST OF WORDS, LR, ASSOCIATED TO THE FEATURE ROOMS All words serve as prefixes: room, space, interior, decor, ambiance, atmosphere, comfort, bath, toilet, bed, building, wall, window, private, temperature, sheet, linen, pillow, hot, water, cold, water, shower, lobby, furniture, carpet, air, condition, mattress, layout, design, mirror, ceiling, lighting, lamp, sofa, chair, dresser, wardrobe, closet 142"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "utility of the product": {
            "translated_key": "utilidad del producto",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Understanding User Behavior in Online Feedback Reporting Arjun Talwar Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland arjun@math.stanford.edu Radu Jurca Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland radu.jurca@epfl.ch Boi Faltings Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland boi.faltings@epfl.ch ABSTRACT Online reviews have become increasingly popular as a way to judge the quality of various products and services.",
                "Previous work has demonstrated that contradictory reporting and underlying user biases make judging the true worth of a service difficult.",
                "In this paper, we investigate underlying factors that influence user behavior when reporting feedback.",
                "We look at two sources of information besides numerical ratings: linguistic evidence from the textual comment accompanying a review, and patterns in the time sequence of reports.",
                "We first show that groups of users who amply discuss a certain feature are more likely to agree on a common rating for that feature.",
                "Second, we show that a users rating partly reflects the difference between true quality and prior expectation of quality as inferred from previous reviews.",
                "Both give us a less noisy way to produce rating estimates and reveal the reasons behind user bias.",
                "Our hypotheses were validated by statistical evidence from hotel reviews on the TripAdvisor website.",
                "Categories and Subject Descriptors J.4 [Social and Behavioral Sciences]: Economics General Terms Economics, Experimentation, Reliability 1.",
                "MOTIVATIONS The spread of the internet has made it possible for online feedback forums (or reputation mechanisms) to become an important channel for Word-of-mouth regarding products, services or other types of commercial interactions.",
                "Numerous empirical studies [10, 15, 13, 5] show that buyers seriously consider online feedback when making purchasing decisions, and are willing to pay reputation premiums for products or services that have a good reputation.",
                "Recent analysis, however, raises important questions regarding the ability of existing forums to reflect the real quality of a product.",
                "In the absence of clear incentives, users with a moderate outlook will not bother to voice their opinions, which leads to an unrepresentative sample of reviews.",
                "For example, [12, 1] show that Amazon1 ratings of books or CDs follow with great probability bi-modal, U-shaped distributions where most of the ratings are either very good, or very bad.",
                "Controlled experiments, on the other hand, reveal opinions on the same items that are normally distributed.",
                "Under these circumstances, using the arithmetic mean to predict quality (as most forums actually do) gives the typical user an estimator with high variance that is often false.",
                "Improving the way we aggregate the information available from online reviews requires a deep understanding of the underlying factors that bias the rating behavior of users.",
                "Hu et al. [12] propose the Brag-and-Moan Model where users rate only if their <br>utility of the product</br> (drawn from a normal distribution) falls outside a median interval.",
                "The authors conclude that the model explains the empirical distribution of reports, and offers insights into smarter ways of estimating the true quality of the product.",
                "In the present paper we extend this line of research, and attempt to explain further facts about the behavior of users when reporting online feedback.",
                "Using actual hotel reviews from the TripAdvisor2 website, we consider two additional sources of information besides the basic numerical ratings submitted by users.",
                "The first is simple linguistic evidence from the textual review that usually accompanies the numerical ratings.",
                "We use text-mining techniques similar to [7] and [3], however, we are only interested in identifying what aspects of the service the user is discussing, without computing the semantic orientation of the text.",
                "We find that users who comment more on the same feature are more likely to agree on a common numerical rating for that particular feature.",
                "Intuitively, lengthy comments reveal the importance of the feature to the user.",
                "Since people tend to be more knowledgeable in the aspects they consider important, users who discuss a given feature in more details might be assumed to have more authority in evaluating that feature.",
                "Second we investigate the relationship between a review 1 http://www.amazon.com 2 http://www.tripadvisor.com/ 134 Figure 1: The TripAdvisor page displaying reviews for a popular Boston hotel.",
                "Name of hotel and advertisements were deliberatively erased. and the reviews that preceded it.",
                "A perusal of online reviews shows that ratings are often part of discussion threads, where one post is not necessarily independent of other posts.",
                "One may see, for example, users who make an effort to contradict, or vehemently agree with, the remarks of previous users.",
                "By analyzing the time sequence of reports, we conclude that past reviews influence the future reports, as they create some prior expectation regarding the quality of service.",
                "The subjective perception of the user is influenced by the gap between the prior expectation and the actual performance of the service [17, 18, 16, 21] which will later reflect in the users rating.",
                "We propose a model that captures the dependence of ratings on prior expectations, and validate it using the empirical data we collected.",
                "Both results can be used to improve the way reputation mechanisms aggregate the information from individual reviews.",
                "Our first result can be used to determine a featureby-feature estimate of quality, where for each feature, a different subset of reviews (i.e., those with lengthy comments of that feature) is considered.",
                "The second leads to an algorithm that outputs a more precise estimate of the real quality. 2.",
                "THE DATA SET We use in this paper real hotel reviews collected from the popular travel site TripAdvisor.",
                "TripAdvisor indexes hotels from cities across the world, along with reviews written by travelers.",
                "Users can search the site by giving the hotels name and location (optional).",
                "The reviews for a given hotel are displayed as a list (ordered from the most recent to the oldest), with 5 reviews per page.",
                "The reviews contain: • information about the author of the review (e.g., dates of stay, username of the reviewer, location of the reviewer); • the overall rating (from 1, lowest, to 5, highest); • a textual review containing a title for the review, free comments, and the main things the reviewer liked and disliked; • numerical ratings (from 1, lowest, to 5, highest) for different features (e.g., cleanliness, service, location, etc.)",
                "Below the name of the hotel, TripAdvisor displays the address of the hotel, general information (number of rooms, number of stars, short description, etc), the average overall rating, the TripAdvisor ranking, and an average rating for each feature.",
                "Figure 1 shows the page for a popular Boston hotel whose name (along with advertisements) was explicitly erased.",
                "We selected three cities for this study: Boston, Sydney and Las Vegas.",
                "For each city we considered all hotels that had at least 10 reviews, and recorded all reviews.",
                "Table 1 presents the number of hotels considered in each city, the total number of reviews recorded for each city, and the distribution of hotels with respect to the star-rating (as available on the TripAdvisor site).",
                "Note that not all hotels have a star-rating.",
                "Table 1: A summary of the data set.",
                "City # Reviews # Hotels # of Hotels with 1,2,3,4 & 5 stars Boston 3993 58 1+3+17+15+2 Sydney 1371 47 0+0+9+13+10 Las Vegas 5593 40 0+3+10+9+6 For each review we recorded the overall rating, the textual review (title and body of the review) and the numerical rating on 7 features: Rooms(R), Service(S), Cleanliness(C), Value(V), Food(F), Location(L) and Noise(N).",
                "TripAdvisor does not require users to submit anything other than the overall rating, hence a typical review rates few additional features, regardless of the discussion in the textual comment.",
                "Only the features Rooms(R), Service(S), Cleanliness(C) and Value(V) are rated by a significant number of users.",
                "However, we also selected the features Food(F), Location(L) and Noise(N) because they are referred to in a significant number of textual comments.",
                "For each feature we record the numerical rating given by the user, or 0 when the rating is missing.",
                "The typical length of the textual comment amounts to approximately 200 words.",
                "All data was collected by crawling the TripAdvisor site in September 2006. 2.1 Formal notation We will formally refer to a review by a tuple (r, T) where: • r = (rf ) is a vector containing the ratings rf ∈ {0, 1, . . . 5} for the features f ∈ F = {O, R, S, C, V, F, L, N}; note that the overall rating, rO, is abusively recorded as the rating for the feature Overall(O); • T is the textual comment that accompanies the review. 135 Reviews are indexed according to the variable i, such that (ri , Ti ) is the ith review in our database.",
                "Since we dont record the username of the reviewer, we will also say that the ith review in our data set was submitted by user i.",
                "When we need to consider only the reviews of a given hotel, h, we will use (ri(h) , Ti(h) ) to denote the ith review about the hotel h. 3.",
                "EVIDENCE FROM TEXTUAL COMMENTS The free textual comments associated to online reviews are a valuable source of information for understanding the reasons behind the numerical ratings left by the reviewers.",
                "The text may, for example, reveal concrete examples of aspects that the user liked or disliked, thus justifying some of the high, respectively low ratings for certain features.",
                "The text may also offer guidelines for understanding the preferences of the reviewer, and the weights of different features when computing an overall rating.",
                "The problem, however, is that free textual comments are difficult to read.",
                "Users are required to scroll through many reviews and read mostly repetitive information.",
                "Significant improvements would be obtained if the reviews were automatically interpreted and aggregated.",
                "Unfortunately, this seems a difficult task for computers since human users often use witty language, abbreviations, cultural specific phrases, and the figurative style.",
                "Nevertheless, several important results use the textual comments of online reviews in an automated way.",
                "Using well established natural language techniques, reviews or parts of reviews can be classified as having a positive or negative semantic orientation.",
                "Pang et al. [2] classify movie reviews into positive/negative by training three different classifiers (Naive Bayes, Maximum Entropy and SVM) using classification features based on unigrams, bigrams or part-of-speech tags.",
                "Dave et al. [4] analyze reviews from CNet and Amazon, and surprisingly show that classification features based on unigrams or bigrams perform better than higher-order n-grams.",
                "This result is challenged by Cui et al. [3] who look at large collections of reviews crawled from the web.",
                "They show that the size of the data set is important, and that bigger training sets allow classifiers to successfully use more complex classification features based on n-grams.",
                "Hu and Liu [11] also crawl the web for product reviews and automatically identify product attributes that have been discussed by reviewers.",
                "They use Wordnet to compute the semantic orientation of product evaluations and summarize user reviews by extracting positive and negative evaluations of different product features.",
                "Popescu and Etzioni [20] analyze a similar setting, but use search engine hit-counts to identify product attributes; the semantic orientation is assigned through the relaxation labeling technique.",
                "Ghose et al. [7, 8] analyze seller reviews from the Amazon secondary market to identify the different dimensions (e.g., delivery, packaging, customer support, etc.) of reputation.",
                "They parse the text, and tag the part-of-speech for each word.",
                "Frequent nouns, noun phrases and verbal phrases are identified as dimensions of reputation, while the corresponding modifiers (i.e., adjectives and adverbs) are used to derive numerical scores for each dimension.",
                "The enhanced reputation measure correlates better with the pricing information observed in the market.",
                "Pavlou and Dimoka [19] analyze eBay reviews and find that textual comments have an important impact on reputation premiums.",
                "Our approach is similar to the previously mentioned works, in the sense that we identify the aspects (i.e., hotel features) discussed by the users in the textual reviews.",
                "However, we do not compute the semantic orientation of the text, nor attempt to infer missing ratings.",
                "We define the weight, wi f , of feature f ∈ F in the text Ti associated with the review (ri , Ti ), as the fraction of Ti dedicated to discussing aspects (both positive and negative) related to feature f. We propose an elementary method to approximate the values of these weights.",
                "For each feature we manually construct the word list Lf containing approximately 50 words that are most commonly associated to the feature f. The initial words were selected from reading some of the reviews, and seeing what words coincide with discussion of which features.",
                "The list was then extended by adding all thesaurus entries that were related to the initial words.",
                "Finally, we brainstormed for missing words that would normally be associated with each of the features.",
                "Let Lf ∩Ti be the list of terms common to both Lf and Ti.",
                "Each term of Lf is counted the number of times it appears in Ti , with two exceptions: • in cases where the user submits a title to the review, we account for the title text by appending it three times to the review text Ti .",
                "The intuitive assumption is that the users opinion is more strongly reflected in the title, rather than in the body of the review.",
                "For example, many reviews are accurately summarized by titles such as Excellent service, terrible location or Bad value for money; • certain words that occur only once in the text are counted multiple times if their relevance to that feature is particularly strong.",
                "These were root words for each feature (e.g., staff is a root word for the feature Service), and were weighted either 2 or 3.",
                "Each feature was assigned up to 3 such root words, so almost all words are counted only once.",
                "The list of words for the feature Rooms is given for reference in Appendix A.",
                "The weight wi f is computed as: wi f = |Lf ∩ Ti| f∈F |Lf ∩ Ti| (1) where |Lf ∩Ti | is the number of terms common to Lf and Ti .",
                "The weight for the feature Overall was set to min{ |T i | 5000 , 1} where |Ti | is the number of character in Ti .",
                "The following is a TripAdvisor review for a Boston hotel (the name of the hotel is omitted): Ill start by saying that Im more of a Holiday Inn person than a *** type.",
                "So I get frustrated when I pay double the room rate and get half the amenities that Id get at a Hampton Inn or Holiday Inn.",
                "The location was definitely the main asset of this place.",
                "It was only a few blocks from the Hynes Center subway stop and it was easy to walk to some good restaurants in the Back Bay area.",
                "Boylston isnt far off at all.",
                "So I had no trouble with foregoing a rental car and taking the subway from the airport to the hotel and using the subway for any other travel.",
                "Otherwise, they make you pay for anything and everything. 136 And when youve already dropped $215/night on the room, that gets frustrating.The room itself was decent, about what I would expect.",
                "Staff was also average, not bad and not excellent.",
                "Again, I think youre paying for location and the ability to walk to a lot of good stuff.",
                "But I think next time Ill stay in Brookline, get more amenities, and use the subway a bit more.",
                "This numerical ratings associated to this review are rO = 3, rR = 3, rS = 3, rC = 4, rV = 2 for features Overall(O), Rooms(R), Service(S), Cleanliness(C) and Value(V) respectively.",
                "The ratings for the features Food(F), Location(L) and Noise(N) are absent (i.e., rF = rL = rN = 0).",
                "The weights wf are computed from the following lists of common terms: LR ∩ T ={room}; wR = 0.066 LS ∩ T ={3 * Staff, amenities}; wS = 0.267 LC ∩ T = ∅; wC = 0 LV ∩ T ={$, rate}; wV = 0.133 LF ∩ T ={restaurant}; wF = 0.067 LL ∩ T ={2 * center, 2 * walk, 2 * location, area}; wL = 0.467 LN ∩ T = ∅; wN = 0 The root words Staff and Center were tripled and doubled respectively.",
                "The overall weight of the textual review is wO = 0.197.",
                "These values account reasonably well for the weights of different features in the discussion of the reviewer.",
                "One point to note is that some terms in the lists Lf possess an inherent semantic orientation.",
                "For example the word grime (belonging to the list LC ) would be used most often to assert the presence, and not the absence of grime.",
                "This is unavoidable, but care was taken to ensure words from both sides of the spectrum were used.",
                "For this reason, some lists such as LR contain only nouns of objects that one would typically describe in a room (see Appendix A).",
                "The goal of this section is to analyse the influence of the weights wi f on the numerical ratings ri f .",
                "Intuitively, users who spent a lot of their time discussing a feature f (i.e., wi f is high) had something to say about their experience with regard to this feature.",
                "Obviously, feature f is important for user i.",
                "Since people tend to be more knowledgeable in the aspects they consider important, our hypothesis is that the ratings ri f (corresponding to high weights wi f ) constitute a subset of expert ratings for feature f. Figure 2 plots the distribution of the rates r i(h) C with respect to the weights w i(h) C for the cleanliness of a Las Vegas hotel, h. Here, the high ratings are restricted to the reviews that discuss little the cleanliness.",
                "Whenever cleanliness appears in the discussion, the ratings are low.",
                "Many hotels exhibit similar rating patterns for various features.",
                "Ratings corresponding to low weights span the whole spectrum from 1 to 5, while the ratings corresponding to high weights are more grouped together (either around good or bad ratings).",
                "We therefore make the following hypothesis: Hypothesis 1.",
                "The ratings ri f corresponding to the reviews where wi f is high, are more similar to each other than to the overall collection of ratings.",
                "To test the hypothesis, we take the entire set of reviews, and feature by feature, we compute the standard deviation of the ratings with high weights, and the standard deviation of the entire set of ratings.",
                "High weights were defined as those belonging to the upper 20% of the weight range for the corresponding feature.",
                "If Hypothesis 1 were true, the standard deviation of all ratings should be higher than the standard deviation of the ratings with high weights. 0 1 2 3 4 5 6 0 0.1 0.2 0.3 0.4 0.5 0.6 Rating Weight Figure 2: The distribution of ratings against the weight of the cleanliness feature.",
                "We use a standard T-test to measure the significance of the results.",
                "City by city and feature by feature, Table 2 presents the average standard deviation of all ratings, and the average standard deviation of ratings with high weights.",
                "Indeed, the ratings with high weights have lower standard deviation, and the results are significant at the standard 0.05 significance threshold (although for certain cities taken independently there doesnt seem to be a significant difference, the results are significant for the entire data set).",
                "Please note that only the features O,R,S,C and V were considered, since for the others (F, L, and N) we didnt have enough ratings.",
                "Table 2: Average standard deviation for all ratings, and average standard deviation for ratings with high weights.",
                "In square brackets, the corresponding p-values for a positive difference between the two.",
                "City O R S C V all 1.189 0.998 1.144 0.935 1.123 Boston high 0.948 0.778 0.954 0.767 0.891 p-val [0.000] [0.004] [0.045] [0.080] [0.009] all 1.040 0.832 1.101 0.847 0.963 Sydney high 0.801 0.618 0.691 0.690 0.798 p-val [0.012] [0.023] [0.000] [0.377] [0.037] all 1.272 1.142 1.184 1.119 1.242 Vegas high 1.072 0.752 1.169 0.907 1.003 p-val [0.0185] [0.001] [0.918] [0.120] [0.126] Hypothesis 1 not only provides some basic understanding regarding the rating behavior of online users, it also suggests some ways of computing better quality estimates.",
                "We can, for example, construct a feature-by-feature quality estimate with much lower variance: for each feature we take the subset of reviews that amply discuss that feature, and output as a quality estimate the average rating for this subset.",
                "Initial experiments suggest that the average feature-by-feature ratings computed in this way are different from the average ratings computed on the whole data set.",
                "Given that, indeed, high weights are indicators of expert opinions, the estimates obtained in this way are more accurate than the current ones.",
                "Nevertheless, the validation of this underlying assumption requires further controlled experiments. 137 4.",
                "THE INFLUENCE OF PAST RATINGS Two important assumptions are generally made about reviews submitted to online forums.",
                "The first is that ratings truthfully reflect the quality observed by the users; the second is that reviews are independent from one another.",
                "While anecdotal evidence [9, 22] challenges the first assumption3 , in this section, we address the second.",
                "A perusal of online reviews shows that reviews are often part of discussion threads, where users make an effort to contradict, or vehemently agree with the remarks of previous users.",
                "Consider, for example, the following review: I dont understand the negative reviews... the hotel was a little dark, but that was the style.",
                "It was very artsy.",
                "Yes it was close to the freeway, but in my opinion the sound of an occasional loud car is better than hearing the ding ding of slot machines all night!",
                "The staff on-hand is FABULOUS.",
                "The waitresses are great (and *** does not deserve the bad review she got, she was 100% attentive to us! ), the bartenders are friendly and professional at the same time...",
                "Here, the user was disturbed by previous negative reports, addressed these concerns, and set about trying to correct them.",
                "Not surprisingly, his ratings were considerably higher than the average ratings up to this point.",
                "It seems that TripAdvisor users regularly read the reports submitted by previous users before booking a hotel, or before writing a review.",
                "Past reviews create some prior expectation regarding the quality of service, and this expectation has an influence on the submitted review.",
                "We believe this observation holds for most online forums.",
                "The subjective perception of quality is directly proportional to how well the actual experience meets the prior expectation, a fact confirmed by an important line of econometric and marketing research [17, 18, 16, 21].",
                "The correlation between the reviews has also been confirmed by recent research on the dynamics of online review forums [6]. 4.1 Prior Expectations We define the prior expectation of user i regarding the feature f, as the average of the previously available ratings on the feature f4 : ef (i) = j<i,r j f =0 rj f j<i,r j f =0 1 As a first hypothesis, we assert that the rating ri f is a function of the prior expectation ef (i): Hypothesis 2.",
                "For a given hotel and feature, given the reviews i and j such that ef (i) is high and ef (j) is low, the rating rj f exceeds the rating ri f .",
                "We define high and low expectations as those that are above, respectively below a certain cutoff value θ.",
                "The set of reviews preceded by high, respectively low expectations 3 part of Amazon reviews were recognized as strategic posts by book authors or competitors 4 if no previous ratings were assigned for feature f, ef (i) is assigned a default value of 4.",
                "Table 3: Average ratings for reviews preceded by low (first value in the cell) and high (second value in the cell) expectations.",
                "The P-values for a positive difference are given square brackets.",
                "City O R S C V 3.953 4.045 3.985 4.252 3.946 Boston 3.364 3.590 3.485 3.641 3.242 [0.011] [0.028] [0.0086] [0.0168] [0.0034] 4.284 4.358 4.064 4.530 4.428 Sydney 3.756 3.537 3.436 3.918 3.495 [0.000] [0.000] [0.035] [0.009] [0.000] 3.494 3.674 3.713 3.689 3.580 Las Vegas 3.140 3.530 2.952 3.530 3.351 [0.190] [0.529] [0.007] [0.529] [0.253] are defined as follows: Rhigh f = {ri f |ef (i) > θ} Rlow f = {ri f |ef (i) < θ} These sets are specific for each (hotel, feature) pair, and in our experiments we took θ = 4.",
                "This rather high value is close to the average rating across all features across all hotels, and is justified by the fact that our data set contains mostly high quality hotels.",
                "For each city, we take all hotels and compute the average ratings in the sets Rhigh f and Rlow f (see Table 3).",
                "The average rating amongst reviews following low prior expectations is significantly higher than the average rating following high expectations.",
                "As further evidence, we consider all hotels for which the function eV (i) (the expectation for the feature Value) has a high value (greater than 4) for some i, and a low value (less than 4) for some other i.",
                "Intuitively, these are the hotels for which there is a minimal degree of variation in the timely sequence of reviews: i.e., the cumulative average of ratings was at some point high and afterwards became low, or vice-versa.",
                "Such variations are observed for about half of all hotels in each city.",
                "Figure 3 plots the median (across considered hotels) rating, rV , when ef (i) is not more than x but greater than x − 0.5. 2.5 3 3.5 4 4.5 5 2.5 3 3.5 4 4.5 5 Medianofrating expectation Boston Sydney Vegas Figure 3: The ratings tend to decrease as the expectation increases. 138 There are two ways to interpret the function ef (i): • The expected value for feature f obtained by user i before his experience with the service, acquired by reading reports submitted by past users.",
                "In this case, an overly high value for ef (i) would drive the user to submit a negative report (or vice versa), stemming from the difference between the actual value of the service, and the inflated expectation of this value acquired before his experience. • The expected value of feature f for all subsequent visitors of the site, if user i were not to submit a report.",
                "In this case, the motivation for a negative report following an overly high value of ef is different: user i seeks to correct the expectation of future visitors to the site.",
                "Unlike the interpretation above, this does not require the user to derive an a priori expectation for the value of f. Note that neither interpretation implies that the average up to report i is inversely related to the rating at report i.",
                "There might exist a measure of influence exerted by past reports that pushes the user behind report i to submit ratings which to some extent conforms with past reports: a low value for ef (i) can influence user i to submit a low rating for feature f because, for example, he fears that submitting a high rating will make him out to be a person with low standards5 .",
                "This, at first, appears to contradict Hypothesis 2.",
                "However, this conformity rating cannot continue indefinitely: once the set of reports project a sufficiently deflated estimate for vf , future reviewers with comparatively positive impressions will seek to correct this misconception. 4.2 Impact of textual comments on quality expectation Further insight into the rating behavior of TripAdvisor users can be obtained by analyzing the relationship between the weights wf and the values ef (i).",
                "In particular, we examine the following hypothesis: Hypothesis 3.",
                "When a large proportion of the text of a review discusses a certain feature, the difference between the rating for that feature and the average rating up to that point tends to be large.",
                "The intuition behind this claim is that when the user is adamant about voicing his opinion regarding a certain feature, his opinion differs from the collective opinion of previous postings.",
                "This relies on the characteristic of reputation systems as feedback forums where a user is interested in projecting his opinion, with particular strength if this opinion differs from what he perceives to be the general opinion.",
                "To test Hypothesis 3 we measure the average absolute difference between the expectation ef (i) and the rating ri f when the weight wi f is high, respectively low.",
                "Weights are classified high or low by comparing them with certain cutoff values: wi f is low if smaller than 0.1, while wi f is high if greater than θf .",
                "Different cutoff values were used for different features: θR = 0.4, θS = 0.4, θC = 0.2, and θV = 0.7.",
                "Cleanliness has a lower cutoff since it is a feature rarely discussed; Value has a high cutoff for the opposite reason.",
                "Results are presented in Table 4. 5 The idea that negative reports can encourage further negative reporting has been suggested before [14] Table 4: Average of |ri f −ef (i)| when weights are high (first value in the cell) and low (second value in the cell) with P-values for the difference in sq. brackets.",
                "City R S C V 1.058 1.208 1.728 1.356 Boston 0.701 0.838 0.760 0.917 [0.022] [0.063] [0.000] [0.218] 1.048 1.351 1.218 1.318 Sydney 0.752 0.759 0.767 0.908 [0.179] [0.009] [0.165] [0.495] 1.184 1.378 1.472 1.642 Las Vegas 0.772 0.834 0.808 1.043 [0.071] [0.020] [0.006] [0.076] This demonstrates that when weights are unusually high, users tend to express an opinion that does not conform to the net average of previous ratings.",
                "As we might expect, for a feature that rarely was a high weight in the discussion, (e.g., cleanliness) the difference is particularly large.",
                "Even though the difference in the feature Value is quite large for Sydney, the P-value is high.",
                "This is because only few reviews discussed value heavily.",
                "The reason could be cultural or because there was less of a reason to discuss this feature. 4.3 Reporting Incentives Previous models suggest that users who are not highly opinionated will not choose to voice their opinions [12].",
                "In this section, we extend this model to account for the influence of expectations.",
                "The motivation for submitting feedback is not only due to extreme opinions, but also to the difference between the current reputation (i.e., the prior expectation of the user) and the actual experience.",
                "Such a rating model produces ratings that most of the time deviate from the current average rating.",
                "The ratings that confirm the prior expectation will rarely be submitted.",
                "We test on our data set the proportion of ratings that attempt to correct the current estimate.",
                "We define a deviant rating as one that deviates from the current expectation by at least some threshold θ, i.e., |ri f − ef (i)| ≥ θ.",
                "For each of the three considered cities, the following tables, show the proportion of deviant ratings for θ = 0.5 and θ = 1.",
                "Table 5: Proportion of deviant ratings with θ = 0.5 City O R S C V Boston 0.696 0.619 0.676 0.604 0.684 Sydney 0.645 0.615 0.672 0.614 0.675 Las Vegas 0.721 0.641 0.694 0.662 0.724 Table 6: Proportion of deviant ratings with θ = 1 City O R S C V Boston 0.420 0.397 0.429 0.317 0.446 Sydney 0.360 0.367 0.442 0.336 0.489 Las Vegas 0.510 0.421 0.483 0.390 0.472 The above results suggest that a large proportion of users (close to one half, even for the high threshold value θ = 1) deviate from the prior average.",
                "This reinforces the idea that users are more likely to submit a report when they believe they have something distinctive to add to the current stream of opinions for some feature.",
                "Such conclusions are in total agreement with prior evidence that the distribution of reports often follows bi-modal, U-shaped distributions. 139 5.",
                "MODELLING THE BEHAVIOR OF RATERS To account for the observations described in the previous sections, we propose a model for the behavior of the users when submitting online reviews.",
                "For a given hotel, we make the assumption that the quality experienced by the users is normally distributed around some value vf , which represents the objective quality offered by the hotel on the feature f. The rating submitted by user i on feature f is: ˆri f = δf vi f + (1 − δf ) · sign vi f − ef (i) c + d(vi f , ef (i)|wi f ) (2) where: • vi f is the (unknown) quality actually experienced by the user. vi f is assumed normally distributed around some value vf ; • δf ∈ [0, 1] can be seen as a measure of the bias when reporting feedback.",
                "High values reflect the fact that users rate objectively, without being influenced by prior expectations.",
                "The value of δf may depend on various factors; we fix one value for each feature f; • c is a constant between 1 and 5; • wi f is the weight of feature f in the textual comment of review i, computed according to Eq. (1); • d(vi f , ef (i)|wi f ) is a distance function between the expectation and the observation of user i.",
                "The distance function satisfies the following properties: - d(y, z|w) ≥ 0 for all y, z ∈ [0, 5], w ∈ [0, 1]; - |d(y, z|w)| < |d(z, x|w)| if |y − z| < |z − x|; - |d(y, z|w1)| < |d(y, z|w2)| if w1 < w2; - c + d(vf , ef (i)|wi f ) ∈ [1, 5]; The second term of Eq. (2) encodes the bias of the rating.",
                "The higher the distance between the true observation vi f and the function ef , the higher the bias. 5.1 Model Validation We use the data set of TripAdvisor reviews to validate the behavior model presented above.",
                "We split for convenience the rating values in three ranges: bad (B = {1, 2}), indifferent (I = {3, 4}), and good (G = {5}), and perform the following two tests: • First, we will use our model to predict the ratings that have extremal values.",
                "For every hotel, we take the sequence of reports, and whenever we encounter a rating that is either good or bad (but not indifferent) we try to predict it using Eq. (2) • Second, instead of predicting the value of extremal ratings, we try to classify them as either good or bad.",
                "For every hotel we take the sequence of reports, and for each report (regardless of it value) we classify it as being good or bad However, to perform these tests, we need to estimate the objective value, vf , that is the average of the true quality observations, vi f .",
                "The algorithm we are using is based on the intuition that the amount of conformity rating is minimized.",
                "In other words, the value vf should be such that as often as possible, bad ratings follow expectations above vf and good ratings follow expectations below vf .",
                "Formally, we define the sets: Γ1 = {i|ef (i) < vf and ri f ∈ B}; Γ2 = {i|ef (i) > vf and ri f ∈ G}; that correspond to irregularities where even though the expectation at point i is lower than the delivered value, the rating is poor, and vice versa.",
                "We define vf as the value that minimize these union of the two sets: vf = arg min vf |Γ1 ∪ Γ2| (3) In Eq. (2) we replace vi f by the value vf computed in Eq. (3), and use the following distance function: d(vf , ef (i)|wi f ) = |vf − ef (i)| vf − ef (i) |vf 2 − ef (i)2 | · (1 + 2wi f ); The constant c ∈ I was set to min{max{ef (i), 3}}, 4}.",
                "The values for δf were fixed at {0.7, 0.7, 0.8, 0.7, 0.6} for the features {Overall, Rooms, Service, Cleanliness, Value} respectively.",
                "The weights are computed as described in Section 3.",
                "As a first experiment, we take the sets of extremal ratings {ri f |ri f /∈ I} for each hotel and feature.",
                "For every such rating, ri f , we try to estimate it by computing ˆri f using Eq. (2).",
                "We compare this estimator with the one obtained by simply averaging the ratings over all hotels and features: i.e., ¯rf = j,r j f =0 rj f j,r j f =0 1 ; Table 7 presents the ratio between the root mean square error (RMSE) when using ˆri f and ¯rf to estimate the actual ratings.",
                "In all cases the estimate produced by our model is better than the simple average.",
                "Table 7: Average of RMSE(ˆrf ) RMSE(¯rf ) City O R S C V Boston 0.987 0.849 0.879 0.776 0.913 Sydney 0.927 0.817 0.826 0.720 0.681 Las Vegas 0.952 0.870 0.881 0.947 0.904 As a second experiment, we try to distinguish the sets Bf = {i|ri f ∈ B} and Gf = {i|ri f ∈ G} of bad, respectively good ratings on the feature f. For example, we compute the set Bf using the following classifier (called σ): ri f ∈ Bf (σf (i) = 1) ⇔ ˆri f ≤ 4; Tables 8, 9 and 10 present the Precision(p), Recall(r) and s = 2pr p+r for classifier σ, and compares it with a naive majority classifier, τ, τf (i) = 1 ⇔ |Bf | ≥ |Gf |: We see that recall is always higher for σ and precision is usually slightly worse.",
                "For the s metric σ tends to add a 140 Table 8: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Boston O R S C V p 0.678 0.670 0.573 0.545 0.610 σ r 0.626 0.659 0.619 0.612 0.694 s 0.651 0.665 0.595 0.577 0.609 p 0.684 0.706 0.647 0.611 0.633 τ r 0.597 0.541 0.410 0.383 0.562 s 0.638 0.613 0.502 0.471 0.595 Table 9: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Las Vegas O R S C V p 0.654 0.748 0.592 0.712 0.583 σ r 0.608 0.536 0.791 0.474 0.610 s 0.630 0.624 0.677 0.569 0.596 p 0.685 0.761 0.621 0.748 0.606 τ r 0.542 0.505 0.767 0.445 0.441 s 0.605 0.607 0.670 0.558 0.511 1-20% improvement over τ, much higher in some cases for hotels in Sydney.",
                "This is likely because Sydney reviews are more positive than those of the American cities and cases where the number of bad reviews exceeded the number of good ones are rare.",
                "Replacing the test algorithm with one that plays a 1 with probability equal to the proportion of bad reviews improves its results for this city, but it is still outperformed by around 80%. 6.",
                "SUMMARY OF RESULTS AND CONCLUSION The goal of this paper is to explore the factors that drive a user to submit a particular rating, rather than the incentives that encouraged him to submit a report in the first place.",
                "For that we use two additional sources of information besides the vector of numerical ratings: first we look at the textual comments that accompany the reviews, and second we consider the reports that have been previously submitted by other users.",
                "Using simple natural language processing algorithms, we were able to establish a correlation between the weight of a certain feature in the textual comment accompanying the review, and the noise present in the numerical rating.",
                "Specifically, it seems that users who discuss amply a certain feature are likely to agree on a common rating.",
                "This observation allows the construction of feature-by-feature estimators of quality that have a lower variance, and are hopefully less noisy.",
                "Nevertheless, further evidence is required to support the intuition that ratings corresponding to high weights are expert opinions that deserve to be given higher priority when computing estimates of quality.",
                "Second, we emphasize the dependence of ratings on previous reports.",
                "Previous reports create an expectation of quality which affects the subjective perception of the user.",
                "We validate two facts about the hotel reviews we collected from TripAdvisor: First, the ratings following low expectations (where the expectation is computed as the average of the previous reports) are likely to be higher than the ratings Table 10: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Sydney O R S C V p 0.650 0.463 0.544 0.550 0.580 σ r 0.234 0.378 0.571 0.169 0.592 s 0.343 0.452 0.557 0.259 0.586 p 0.562 0.615 0.600 0.500 0.600 τ r 0.054 0.098 0.101 0.015 0.175 s 0.098 0.168 0.172 0.030 0.271 following high expectations.",
                "Intuitively, the perception of quality (and consequently the rating) depends on how well the actual experience of the user meets her expectation.",
                "Second, we include evidence from the textual comments, and find that when users devote a large fraction of the text to discussing a certain feature, they are likely to motivate a divergent rating (i.e., a rating that does not conform to the prior expectation).",
                "Intuitively, this supports the hypothesis that review forums act as discussion groups where users are keen on presenting and motivating their own opinion.",
                "We have captured the empirical evidence in a behavior model that predicts the ratings submitted by the users.",
                "The final rating depends, as expected, on the true observation, and on the gap between the observation and the expectation.",
                "The gap tends to have a bigger influence when an important fraction of the textual comment is dedicated to discussing a certain feature.",
                "The proposed model was validated on the empirical data and provides better estimates of the ratings actually submitted.",
                "One assumption that we make is about the existence of an objective quality value vf for the feature f. This is rarely true, especially over large spans of time.",
                "Other explanations might account for the correlation of ratings with past reports.",
                "For example, if ef (i) reflects the true value of f at a point in time, the difference in the ratings following high and low expectations can be explained by hotel revenue models that are maximized when the value is modified accordingly.",
                "However, the idea that variation in ratings is not primarily a function of variation in value turns out to be a useful one.",
                "Our approach to approximate this elusive objective value is by no means perfect, but conforms neatly to the idea behind the model.",
                "A natural direction for future work is to examine concrete applications of our results.",
                "Significant improvements of quality estimates are likely to be obtained by incorporating all empirical evidence about rating behavior.",
                "Exactly how different factors affect the decisions of the users is not clear.",
                "The answer might depend on the particular application, context and culture. 7.",
                "REFERENCES [1] A. Admati and P. Pfleiderer.",
                "Noisytalk.com: Broadcasting opinions in a noisy environment.",
                "Working Paper 1670R, Stanford University, 2000. [2] P. B., L. Lee, and S. Vaithyanathan.",
                "Thumbs up? sentiment classification using machine learning techniques.",
                "In Proceedings of the EMNLP-02, the Conference on Empirical Methods in Natural Language Processing, 2002. [3] H. Cui, V. Mittal, and M. Datar.",
                "Comparative 141 Experiments on Sentiment Classification for Online Product Reviews.",
                "In Proceedings of AAAI, 2006. [4] K. Dave, S. Lawrence, and D. Pennock.",
                "Mining the peanut gallery:opinion extraction and semantic classification of product reviews.",
                "In Proceedings of the 12th International Conference on the World Wide Web (WWW03), 2003. [5] C. Dellarocas, N. Awad, and X. Zhang.",
                "Exploring the Value of Online Product Ratings in Revenue Forecasting: The Case of Motion Pictures.",
                "Working paper, 2006. [6] C. Forman, A. Ghose, and B. Wiesenfeld.",
                "A Multi-Level Examination of the Impact of Social Identities on Economic Transactions in Electronic Markets.",
                "Available at SSRN: http://ssrn.com/abstract=918978, July 2006. [7] A. Ghose, P. Ipeirotis, and A. Sundararajan.",
                "Reputation Premiums in Electronic Peer-to-Peer Markets: Analyzing Textual Feedback and Network Structure.",
                "In Third Workshop on Economics of Peer-to-Peer Systems, (P2PECON), 2005. [8] A. Ghose, P. Ipeirotis, and A. Sundararajan.",
                "The Dimensions of Reputation in electronic Markets.",
                "Working Paper CeDER-06-02, New York University, 2006. [9] A. Harmon.",
                "Amazon Glitch Unmasks War of Reviewers.",
                "The New York Times, February 14, 2004. [10] D. Houser and J. Wooders.",
                "Reputation in Auctions: Theory and Evidence from eBay.",
                "Journal of Economics and Management Strategy, 15:353-369, 2006. [11] M. Hu and B. Liu.",
                "Mining and summarizing customer reviews.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD04), 2004. [12] N. Hu, P. Pavlou, and J. Zhang.",
                "Can Online Reviews Reveal a Products True Quality?",
                "In Proceedings of ACM Conference on Electronic Commerce (EC 06), 2006. [13] K. Kalyanam and S. McIntyre.",
                "Return on reputation in online auction market.",
                "Working Paper 02/03-10-WP, Leavey School of Business, Santa Clara University., 2001. [14] L. Khopkar and P. Resnick.",
                "Self-Selection, Slipping, Salvaging, Slacking, and Stoning: the Impacts of Negative Feedback at eBay.",
                "In Proceedings of ACM Conference on Electronic Commerce (EC 05), 2005. [15] M. Melnik and J. Alm.",
                "Does a sellers reputation matter? evidence from ebay auctions.",
                "Journal of Industrial Economics, 50(3):337-350, 2002. [16] R. Olshavsky and J. Miller.",
                "Consumer Expectations, Product Performance and Perceived Product Quality.",
                "Journal of Marketing Research, 9:19-21, February 1972. [17] A. Parasuraman, V. Zeithaml, and L. Berry.",
                "A Conceptual Model of Service Quality and Its Implications for Future Research.",
                "Journal of Marketing, 49:41-50, 1985. [18] A. Parasuraman, V. Zeithaml, and L. Berry.",
                "SERVQUAL: A Multiple-Item Scale for Measuring Consumer Perceptions of Service Quality.",
                "Journal of Retailing, 64:12-40, 1988. [19] P. Pavlou and A. Dimoka.",
                "The Nature and Role of Feedback Text Comments in Online Marketplaces: Implications for Trust Building, Price Premiums, and Seller Differentiation.",
                "Information Systems Research, 17(4):392-414, 2006. [20] A. Popescu and O. Etzioni.",
                "Extracting product features and opinions from reviews.",
                "In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, 2005. [21] R. Teas.",
                "Expectations, Performance Evaluation, and Consumers Perceptions of Quality.",
                "Journal of Marketing, 57:18-34, 1993. [22] E. White.",
                "Chatting a Singer Up the Pop Charts.",
                "The Wall Street Journal, October 15, 1999.",
                "APPENDIX A.",
                "LIST OF WORDS, LR, ASSOCIATED TO THE FEATURE ROOMS All words serve as prefixes: room, space, interior, decor, ambiance, atmosphere, comfort, bath, toilet, bed, building, wall, window, private, temperature, sheet, linen, pillow, hot, water, cold, water, shower, lobby, furniture, carpet, air, condition, mattress, layout, design, mirror, ceiling, lighting, lamp, sofa, chair, dresser, wardrobe, closet 142"
            ],
            "original_annotated_samples": [
                "Hu et al. [12] propose the Brag-and-Moan Model where users rate only if their <br>utility of the product</br> (drawn from a normal distribution) falls outside a median interval."
            ],
            "translated_annotated_samples": [
                "Hu et al. [12] proponen el Modelo Brag-and-Moan donde los usuarios califican solo si su <br>utilidad del producto</br> (extraída de una distribución normal) cae fuera de un intervalo mediano."
            ],
            "translated_text": "La comprensión del comportamiento del usuario en la presentación de comentarios en línea. Trabajos anteriores han demostrado que la información contradictoria y los sesgos subyacentes de los usuarios dificultan juzgar el verdadero valor de un servicio. En este artículo, investigamos los factores subyacentes que influyen en el comportamiento del usuario al informar retroalimentación. Examinamos dos fuentes de información además de las calificaciones numéricas: la evidencia lingüística del comentario textual que acompaña a una reseña y los patrones en la secuencia temporal de los informes. Primero demostramos que los grupos de usuarios que discuten ampliamente una característica específica tienen más probabilidades de ponerse de acuerdo en una calificación común para esa característica. Segundo, demostramos que la calificación de un usuario refleja en parte la diferencia entre la calidad real y la expectativa previa de calidad, tal como se infiere de revisiones anteriores. Ambos nos ofrecen una forma menos ruidosa de producir estimaciones de calificación y revelar las razones detrás del sesgo del usuario. Nuestras hipótesis fueron validadas por evidencia estadística de reseñas de hoteles en el sitio web de TripAdvisor. Categorías y Descriptores de Asignaturas J.4 [Ciencias Sociales y del Comportamiento]: Economía Términos Generales Economía, Experimentación, Confiabilidad 1. MOTIVACIONES La expansión de internet ha hecho posible que los foros de retroalimentación en línea (o mecanismos de reputación) se conviertan en un canal importante para el boca a boca sobre productos, servicios u otros tipos de interacciones comerciales. Numerosos estudios empíricos [10, 15, 13, 5] muestran que los compradores consideran seriamente los comentarios en línea al tomar decisiones de compra, y están dispuestos a pagar primas por reputación por productos o servicios que tienen una buena reputación. Sin embargo, un análisis reciente plantea preguntas importantes sobre la capacidad de los foros existentes para reflejar la verdadera calidad de un producto. En ausencia de incentivos claros, los usuarios con una perspectiva moderada no se molestarán en expresar sus opiniones, lo que conduce a una muestra no representativa de reseñas. Por ejemplo, [12, 1] muestran que las calificaciones de libros o CDs en Amazon1 siguen con gran probabilidad distribuciones bimodales en forma de U, donde la mayoría de las calificaciones son o muy buenas o muy malas. Los experimentos controlados, por otro lado, revelan opiniones sobre los mismos elementos que están distribuidas de forma normal. Bajo estas circunstancias, utilizar la media aritmética para predecir la calidad (como la mayoría de los foros realmente hacen) proporciona al usuario típico un estimador con una alta varianza que a menudo es incorrecto. Mejorar la forma en que agregamos la información disponible de las reseñas en línea requiere un profundo entendimiento de los factores subyacentes que sesgan el comportamiento de calificación de los usuarios. Hu et al. [12] proponen el Modelo Brag-and-Moan donde los usuarios califican solo si su <br>utilidad del producto</br> (extraída de una distribución normal) cae fuera de un intervalo mediano. Los autores concluyen que el modelo explica la distribución empírica de los informes y ofrece ideas sobre formas más inteligentes de estimar la verdadera calidad del producto. En el presente artículo ampliamos esta línea de investigación e intentamos explicar más hechos sobre el comportamiento de los usuarios al informar retroalimentación en línea. Usando reseñas reales de hoteles del sitio web TripAdvisor2, consideramos dos fuentes adicionales de información además de las calificaciones numéricas básicas enviadas por los usuarios. La primera es una evidencia lingüística simple proveniente de la revisión textual que suele acompañar a las calificaciones numéricas. Utilizamos técnicas de minería de texto similares a [7] y [3], sin embargo, solo estamos interesados en identificar qué aspectos del servicio está discutiendo el usuario, sin calcular la orientación semántica del texto. Observamos que los usuarios que comentan más sobre la misma característica tienen más probabilidades de estar de acuerdo en una calificación numérica común para esa característica en particular. Intuitivamente, los comentarios extensos revelan la importancia de la característica para el usuario. Dado que las personas tienden a ser más conocedoras en los aspectos que consideran importantes, se podría asumir que los usuarios que discuten una característica específica en más detalle tienen más autoridad para evaluar esa característica. Segundo investigamos la relación entre una reseña 1 http://www.amazon.com 2 http://www.tripadvisor.com/ 134 Figura 1: La página de TripAdvisor que muestra reseñas de un popular hotel en Boston. El nombre del hotel y los anuncios fueron deliberadamente borrados, al igual que las reseñas que lo precedieron. Un examen de las reseñas en línea muestra que las calificaciones a menudo forman parte de los hilos de discusión, donde una publicación no es necesariamente independiente de otras publicaciones. Uno puede ver, por ejemplo, usuarios que se esfuerzan por contradecir o estar vehementemente de acuerdo con los comentarios de usuarios anteriores. Al analizar la secuencia temporal de informes, concluimos que las revisiones pasadas influyen en los informes futuros, ya que crean cierta expectativa previa sobre la calidad del servicio. La percepción subjetiva del usuario está influenciada por la brecha entre la expectativa previa y el rendimiento real del servicio [17, 18, 16, 21], lo cual se reflejará más tarde en la calificación de los usuarios. Proponemos un modelo que captura la dependencia de las calificaciones en las expectativas previas, y lo validamos utilizando los datos empíricos que recopilamos. Ambos resultados pueden ser utilizados para mejorar la forma en que los mecanismos de reputación agregan la información de las revisiones individuales. Nuestro primer resultado se puede utilizar para determinar una estimación de calidad característica por característica, donde para cada característica se considera un subconjunto diferente de reseñas (es decir, aquellas con comentarios extensos sobre esa característica). El segundo conduce a un algoritmo que produce una estimación más precisa de la calidad real. 2. El conjunto de datos que utilizamos en este artículo son reseñas reales de hoteles recopiladas del popular sitio de viajes TripAdvisor. TripAdvisor indexa hoteles de ciudades de todo el mundo, junto con reseñas escritas por viajeros. Los usuarios pueden buscar en el sitio proporcionando el nombre del hotel y la ubicación (opcional). Las reseñas de un hotel dado se muestran como una lista (ordenadas de la más reciente a la más antigua), con 5 reseñas por página. Las reseñas contienen: • información sobre el autor de la reseña (por ejemplo, fechas de estancia, nombre de usuario del revisor, ubicación del revisor); • la calificación general (de 1, la más baja, a 5, la más alta); • una reseña textual que incluye un título para la reseña, comentarios libres y las principales cosas que al revisor le gustaron y no le gustaron; • calificaciones numéricas (de 1, la más baja, a 5, la más alta) para diferentes características (por ejemplo, limpieza, servicio, ubicación, etc.). Debajo del nombre del hotel, TripAdvisor muestra la dirección del hotel, información general (número de habitaciones, número de estrellas, breve descripción, etc.), la calificación promedio general, el ranking de TripAdvisor y una calificación promedio para cada característica. La Figura 1 muestra la página de un hotel popular en Boston cuyo nombre (junto con los anuncios) fue explícitamente borrado. Seleccionamos tres ciudades para este estudio: Boston, Sídney y Las Vegas. Para cada ciudad consideramos todos los hoteles que tenían al menos 10 reseñas, y registramos todas las reseñas. La Tabla 1 presenta el número de hoteles considerados en cada ciudad, el número total de reseñas registradas para cada ciudad y la distribución de hoteles con respecto a la calificación por estrellas (según está disponible en el sitio de TripAdvisor). Ten en cuenta que no todos los hoteles tienen una clasificación por estrellas. Tabla 1: Un resumen del conjunto de datos. Para cada reseña, registramos la calificación general, la reseña textual (título y cuerpo de la reseña) y la calificación numérica en 7 características: Habitaciones (R), Servicio (S), Limpieza (C), Valor (V), Comida (F), Ubicación (L) y Ruido (N). TripAdvisor no requiere que los usuarios envíen nada más que la calificación general, por lo tanto, una reseña típica evalúa pocas características adicionales, independientemente de la discusión en el comentario textual. Solo las características Habitaciones (R), Servicio (S), Limpieza (C) y Valor (V) son evaluadas por un número significativo de usuarios. Sin embargo, también seleccionamos las características Comida (F), Ubicación (L) y Ruido (N) porque son mencionadas en un número significativo de comentarios textuales. Para cada característica registramos la calificación numérica dada por el usuario, o 0 cuando la calificación está ausente. La longitud típica del comentario textual equivale aproximadamente a 200 palabras. Todos los datos fueron recopilados rastreando el sitio de TripAdvisor en septiembre de 2006. 2.1 Notación formal Nos referiremos formalmente a una reseña mediante una tupla (r, T) donde: • r = (rf ) es un vector que contiene las calificaciones rf ∈ {0, 1, . . . 5} para las características f ∈ F = {O, R, S, C, V, F, L, N}; cabe destacar que la calificación general, rO, se registra de forma abusiva como la calificación para la característica General(O); • T es el comentario textual que acompaña a la reseña. Se indexan 135 reseñas de acuerdo con la variable i, de modo que (ri , Ti ) es la i-ésima reseña en nuestra base de datos. Dado que no registramos el nombre de usuario del revisor, también diremos que la i-ésima reseña en nuestro conjunto de datos fue enviada por el usuario i. Cuando necesitemos considerar solo las reseñas de un hotel dado, h, usaremos (ri(h), Ti(h)) para denotar la i-ésima reseña sobre el hotel h. 3. EVIDENCIA DE COMENTARIOS TEXTUALES Los comentarios textuales gratuitos asociados a las reseñas en línea son una fuente valiosa de información para comprender las razones detrás de las calificaciones numéricas dejadas por los revisores. El texto puede, por ejemplo, revelar ejemplos concretos de aspectos que al usuario le gustaron o no le gustaron, justificando así algunas de las calificaciones altas o bajas respectivamente para ciertas características. El texto también puede ofrecer pautas para comprender las preferencias del revisor y los pesos de las diferentes características al calcular una calificación general. El problema, sin embargo, es que los comentarios textuales libres son difíciles de leer. Los usuarios deben desplazarse a través de muchas reseñas y leer principalmente información repetitiva. Se obtendrían mejoras significativas si las reseñas fueran interpretadas y agregadas automáticamente. Desafortunadamente, esta parece ser una tarea difícil para las computadoras ya que los usuarios humanos a menudo utilizan un lenguaje ingenioso, abreviaturas, frases específicas de la cultura y un estilo figurativo. Sin embargo, varios resultados importantes utilizan los comentarios textuales de las reseñas en línea de forma automatizada. Utilizando técnicas de lenguaje natural bien establecidas, las reseñas o partes de las reseñas pueden ser clasificadas como tener una orientación semántica positiva o negativa. Pang et al. [2] clasifican las críticas de películas en positivas/negativas entrenando tres clasificadores diferentes (Naive Bayes, Máxima Entropía y SVM) utilizando características de clasificación basadas en unigramas, bigramas o etiquetas de partes del discurso. Dave et al. [4] analizan reseñas de CNet y Amazon, y sorprendentemente demuestran que las características de clasificación basadas en unigramas o bigramas funcionan mejor que los n-gramas de orden superior. Este resultado es desafiado por Cui et al. [3], quienes examinan grandes colecciones de reseñas recopiladas de la web. Muestran que el tamaño del conjunto de datos es importante, y que conjuntos de entrenamiento más grandes permiten a los clasificadores utilizar con éxito características de clasificación más complejas basadas en n-gramas. Hu y Liu [11] también rastrean la web en busca de reseñas de productos e identifican automáticamente los atributos de productos que han sido discutidos por los revisores. Utilizan Wordnet para calcular la orientación semántica de las evaluaciones de productos y resumir las reseñas de usuarios extrayendo evaluaciones positivas y negativas de diferentes características del producto. Popescu y Etzioni [20] analizan un escenario similar, pero utilizan el recuento de resultados de búsqueda en motores de búsqueda para identificar atributos de productos; la orientación semántica se asigna a través de la técnica de etiquetado de relajación. Ghose et al. [7, 8] analizan las reseñas de vendedores del mercado secundario de Amazon para identificar las diferentes dimensiones (por ejemplo, entrega, empaque, atención al cliente, etc.) de la reputación. Analizan el texto y etiquetan la parte de la oración de cada palabra. Los sustantivos, frases nominales y frases verbales frecuentes se identifican como dimensiones de la reputación, mientras que los modificadores correspondientes (es decir, adjetivos y adverbios) se utilizan para derivar puntuaciones numéricas para cada dimensión. La medida de reputación mejorada se correlaciona mejor con la información de precios observada en el mercado. Pavlou y Dimoka [19] analizan las reseñas de eBay y encuentran que los comentarios textuales tienen un impacto importante en las primas de reputación. Nuestro enfoque es similar a los trabajos mencionados anteriormente, en el sentido de que identificamos los aspectos (es decir, las características del hotel) discutidos por los usuarios en las reseñas textuales. Sin embargo, no calculamos la orientación semántica del texto, ni intentamos inferir las calificaciones faltantes. Definimos el peso, wi f, de la característica f ∈ F en el texto Ti asociado con la reseña (ri, Ti), como la fracción de Ti dedicada a discutir aspectos (tanto positivos como negativos) relacionados con la característica f. Proponemos un método elemental para aproximar los valores de estos pesos. Para cada característica, construimos manualmente la lista de palabras Lf que contiene aproximadamente 50 palabras que están más comúnmente asociadas a la característica f. Las palabras iniciales fueron seleccionadas al leer algunas de las reseñas y ver qué palabras coinciden con la discusión de qué características. La lista fue luego ampliada agregando todas las entradas del tesauro que estaban relacionadas con las palabras iniciales. Finalmente, hicimos una lluvia de ideas para encontrar las palabras faltantes que normalmente estarían asociadas con cada una de las características. Que Lf ∩ Ti sea la lista de términos comunes a Lf y Ti. Cada término de Lf se cuenta el número de veces que aparece en Ti, con dos excepciones: • en los casos en que el usuario envía un título para la revisión, consideramos el texto del título agregándolo tres veces al texto de la revisión Ti. La suposición intuitiva es que la opinión del usuario se refleja con mayor fuerza en el título que en el cuerpo de la reseña. Por ejemplo, muchas reseñas son resumidas con precisión por títulos como \"Excelente servicio, ubicación terrible\" o \"Mala relación calidad-precio\"; ciertas palabras que ocurren solo una vez en el texto son contadas múltiples veces si su relevancia para esa característica es particularmente fuerte. Estas eran las palabras raíz para cada característica (por ejemplo, personal es una palabra raíz para la característica Servicio), y tenían un peso de 2 o 3. Cada característica fue asignada hasta 3 palabras raíz, por lo que casi todas las palabras se cuentan solo una vez. La lista de palabras para la característica Habitaciones se proporciona como referencia en el Apéndice A. El peso wi f se calcula como: wi f = |Lf ∩ Ti| f∈F |Lf ∩ Ti| (1) donde |Lf ∩ Ti| es el número de términos comunes entre Lf y Ti. El peso para la característica \"En general\" se estableció en min{ |T i | 5000 , 1} donde |Ti | es el número de caracteres en Ti. Comenzaré diciendo que soy más de la onda de Holiday Inn que de un hotel de lujo. Así que me frustro cuando pago el doble de la tarifa de la habitación y recibo la mitad de las comodidades que obtendría en un Hampton Inn o Holiday Inn. La ubicación era definitivamente el principal activo de este lugar. Estaba a solo unas cuadras de la parada de metro del Centro Hynes y era fácil caminar a algunos buenos restaurantes en la zona de Back Bay. Boylston no está lejos en absoluto. Así que no tuve problemas en renunciar a un coche de alquiler y tomar el metro desde el aeropuerto hasta el hotel y usar el metro para cualquier otro viaje. De lo contrario, te hacen pagar por todo. Y cuando ya has gastado $215 por noche en la habitación, eso resulta frustrante. La habitación en sí estaba bien, más o menos lo que esperaría. El personal también era promedio, ni malo ni excelente. Una vez más, creo que estás pagando por la ubicación y la capacidad de ir caminando a muchos lugares interesantes. Pero creo que la próxima vez me quedaré en Brookline, disfrutaré de más comodidades y usaré más el metro. Las calificaciones numéricas asociadas a esta reseña son rO = 3, rR = 3, rS = 3, rC = 4, rV = 2 para las características de General (O), Habitaciones (R), Servicio (S), Limpieza (C) y Valor (V) respectivamente. Las calificaciones de las características Comida (F), Ubicación (L) y Ruido (N) están ausentes (es decir, rF = rL = rN = 0). Los pesos wf se calculan a partir de las siguientes listas de términos comunes: LR ∩ T = {habitación}; wR = 0.066 LS ∩ T = {3 * Personal, comodidades}; wS = 0.267 LC ∩ T = ∅; wC = 0 LV ∩ T = {$, tarifa}; wV = 0.133 LF ∩ T = {restaurante}; wF = 0.067 LL ∩ T = {2 * centro, 2 * caminar, 2 * ubicación, área}; wL = 0.467 LN ∩ T = ∅; wN = 0 Las palabras raíz Personal y Centro se triplicaron y duplicaron respectivamente. El peso total de la revisión textual es wO = 0.197. Estos valores explican razonablemente bien los pesos de las diferentes características en la discusión del revisor. Un punto a tener en cuenta es que algunos términos en las listas Lf poseen una orientación semántica inherente. Por ejemplo, la palabra suciedad (perteneciente a la lista LC) se usaría con mayor frecuencia para afirmar la presencia, y no la ausencia de suciedad. Esto es inevitable, pero se tuvo cuidado de asegurar que se utilizaran palabras de ambos extremos del espectro. Por esta razón, algunas listas como LR contienen solo sustantivos de objetos que uno típicamente describiría en una habitación (ver Apéndice A). El objetivo de esta sección es analizar la influencia de los pesos wi f en las calificaciones numéricas ri f. Intuitivamente, los usuarios que pasaron mucho tiempo discutiendo una característica f (es decir, cuando wif es alto) tenían algo que decir sobre su experiencia con respecto a esta característica. Obviamente, la característica f es importante para el usuario i. Dado que las personas tienden a ser más conocedoras en los aspectos que consideran importantes, nuestra hipótesis es que las calificaciones ri f (correspondientes a los pesos altos wi f) constituyen un subconjunto de las calificaciones de expertos para la característica f. La Figura 2 traza la distribución de las tasas r i(h) C con respecto a los pesos w i(h) C para la limpieza de un hotel de Las Vegas, h. Aquí, las calificaciones altas están restringidas a las reseñas que hablan poco sobre la limpieza. Cuando se menciona la limpieza en la discusión, las calificaciones son bajas. Muchos hoteles muestran patrones de calificación similares para varias características. Las calificaciones correspondientes a pesos bajos abarcan todo el espectro del 1 al 5, mientras que las calificaciones correspondientes a pesos altos están más agrupadas (ya sea alrededor de calificaciones buenas o malas). Por lo tanto, formulamos la siguiente hipótesis: Hipótesis 1. Las calificaciones ri f correspondientes a las reseñas donde wi f es alta, son más similares entre sí que a la colección general de calificaciones. Para probar la hipótesis, tomamos todo el conjunto de reseñas y, característica por característica, calculamos la desviación estándar de las calificaciones con alto peso, y la desviación estándar de todo el conjunto de calificaciones. Los pesos altos se definieron como aquellos que pertenecen al 20% superior del rango de pesos para la característica correspondiente. Si la Hipótesis 1 fuera cierta, la desviación estándar de todas las calificaciones debería ser mayor que la desviación estándar de las calificaciones con pesos altos. 0 1 2 3 4 5 6 0 0.1 0.2 0.3 0.4 0.5 0.6 Calificación Peso Figura 2: La distribución de las calificaciones en función del peso de la característica de limpieza. Utilizamos una prueba T estándar para medir la significancia de los resultados. Ciudad por ciudad y característica por característica, la Tabla 2 presenta la desviación estándar promedio de todas las calificaciones, y la desviación estándar promedio de las calificaciones con pesos altos. De hecho, las calificaciones con pesos altos tienen una desviación estándar más baja, y los resultados son significativos en el umbral estándar de significancia de 0.05 (aunque para ciertas ciudades tomadas de forma independiente no parece haber una diferencia significativa, los resultados son significativos para todo el conjunto de datos). Por favor, tenga en cuenta que solo se consideraron las características O, R, S, C y V, ya que para las otras (F, L y N) no teníamos suficientes calificaciones. Tabla 2: Desviación estándar promedio para todas las calificaciones, y desviación estándar promedio para las calificaciones con pesos altos. En corchetes, los valores p correspondientes para una diferencia positiva entre los dos. La hipótesis 1 no solo proporciona una comprensión básica sobre el comportamiento de calificación de los usuarios en línea, sino que también sugiere algunas formas de calcular estimaciones de mejor calidad. Podemos, por ejemplo, construir una estimación de calidad característica por característica con una varianza mucho menor: para cada característica tomamos el subconjunto de reseñas que discuten ampliamente esa característica, y como estimación de calidad, obtenemos la calificación promedio de este subconjunto. Los experimentos iniciales sugieren que las calificaciones promedio de característica a característica calculadas de esta manera son diferentes de las calificaciones promedio calculadas en todo el conjunto de datos. Dado que, de hecho, los pesos altos son indicadores de opiniones expertas, las estimaciones obtenidas de esta manera son más precisas que las actuales. Sin embargo, la validación de esta suposición subyacente requiere más experimentos controlados. LA INFLUENCIA DE LAS CALIFICACIONES PASADAS Por lo general, se hacen dos suposiciones importantes sobre las reseñas enviadas a foros en línea. La primera es que las calificaciones reflejen fielmente la calidad observada por los usuarios; la segunda es que las reseñas sean independientes entre sí. Si bien la evidencia anecdótica [9, 22] cuestiona la primera suposición, en esta sección abordamos la segunda. Un examen de las reseñas en línea muestra que estas suelen formar parte de hilos de discusión, donde los usuarios se esfuerzan por contradecir o estar vehementemente de acuerdo con los comentarios de usuarios anteriores. Considera, por ejemplo, la siguiente reseña: No entiendo las críticas negativas... el hotel estaba un poco oscuro, pero ese era el estilo. Fue muy artístico. Sí, estaba cerca de la autopista, pero en mi opinión, el sonido de un coche ruidoso ocasional es mejor que escuchar el ding ding de las máquinas tragamonedas toda la noche. El personal disponible es FABULOSO. Las camareras son geniales (y *** no merece la mala crítica que recibió, ¡fue 100% atenta con nosotros!), los camareros son amigables y profesionales al mismo tiempo... Aquí, el usuario se vio perturbado por informes negativos anteriores, abordó estas preocupaciones y se dispuso a intentar corregirlas. Como era de esperar, sus calificaciones fueron considerablemente más altas que las calificaciones promedio hasta este punto. Parece que los usuarios de TripAdvisor suelen leer regularmente los informes enviados por usuarios anteriores antes de reservar un hotel, o antes de escribir una reseña. Las reseñas anteriores crean ciertas expectativas previas sobre la calidad del servicio, y estas expectativas influyen en la reseña presentada. Creemos que esta observación es válida para la mayoría de los foros en línea. La percepción subjetiva de la calidad es directamente proporcional a qué tan bien la experiencia real cumple con la expectativa previa, un hecho confirmado por una importante línea de investigación econométrica y de marketing [17, 18, 16, 21]. La correlación entre las reseñas también ha sido confirmada por investigaciones recientes sobre la dinámica de los foros de reseñas en línea [6]. 4.1 Expectativas Previas Definimos la expectativa previa del usuario i con respecto a la característica f, como el promedio de las calificaciones previamente disponibles en la característica f4: ef (i) = j<i,r j f =0 rj f j<i,r j f =0 1 Como primera hipótesis, afirmamos que la calificación ri f es una función de la expectativa previa ef (i): Hipótesis 2. Para un hotel y una característica dados, dados los comentarios i y j tales que ef (i) es alto y ef (j) es bajo, la calificación rj f supera la calificación ri f. Definimos las expectativas altas y bajas como aquellas que están por encima, respectivamente por debajo de un cierto valor umbral θ. El conjunto de reseñas precedidas por altas, respectivamente bajas expectativas 3 parte de las reseñas de Amazon fueron reconocidas como publicaciones estratégicas por autores de libros o competidores 4 si no se asignaron calificaciones previas para la característica f, ef (i) se asigna un valor predeterminado de 4. Tabla 3: Calificaciones promedio para reseñas precedidas por expectativas bajas (primer valor en la celda) y altas (segundo valor en la celda). Los valores P para una diferencia positiva se dan entre corchetes. Las ciudades O R S C V 3.953 4.045 3.985 4.252 3.946 Boston 3.364 3.590 3.485 3.641 3.242 [0.011] [0.028] [0.0086] [0.0168] [0.0034] 4.284 4.358 4.064 4.530 4.428 Sídney 3.756 3.537 3.436 3.918 3.495 [0.000] [0.000] [0.035] [0.009] [0.000] 3.494 3.674 3.713 3.689 3.580 Las Vegas 3.140 3.530 2.952 3.530 3.351 [0.190] [0.529] [0.007] [0.529] [0.253] se definen de la siguiente manera: Ralto f = {ri f |ef (i) > θ} Rbajo f = {ri f |ef (i) < θ} Estos conjuntos son específicos para cada par (hotel, característica), y en nuestros experimentos tomamos θ = 4. Este valor bastante alto está cerca del promedio de calificación de todas las características de todos los hoteles, y se justifica por el hecho de que nuestro conjunto de datos contiene principalmente hoteles de alta calidad. Para cada ciudad, tomamos todos los hoteles y calculamos las calificaciones promedio en los conjuntos Rhigh f y Rlow f (ver Tabla 3). El promedio de calificación entre las reseñas que siguen a expectativas previas bajas es significativamente mayor que el promedio de calificación después de altas expectativas. Como evidencia adicional, consideramos todos los hoteles para los cuales la función eV (i) (la expectativa para la característica Valor) tiene un valor alto (mayor que 4) para algunos i, y un valor bajo (menor que 4) para algunos otros i. Intuitivamente, estos son los hoteles para los cuales hay un grado mínimo de variación en la secuencia oportuna de reseñas: es decir, el promedio acumulativo de calificaciones fue en algún momento alto y luego se volvió bajo, o viceversa. Tales variaciones se observan en aproximadamente la mitad de todos los hoteles en cada ciudad. La Figura 3 traza la mediana (entre los hoteles considerados) de la calificación, rV, cuando ef (i) no es más que x pero mayor que x - 0.5. 2.5 3 3.5 4 4.5 5 2.5 3 3.5 4 4.5 5 Expectativa de la mediana de calificación Boston Sydney Vegas Figura 3: Las calificaciones tienden a disminuir a medida que aumenta la expectativa. 138 Hay dos formas de interpretar la función ef (i): • El valor esperado para la característica f obtenido por el usuario i antes de su experiencia con el servicio, adquirido al leer informes presentados por usuarios anteriores. En este caso, un valor excesivamente alto para ef (i) llevaría al usuario a enviar un informe negativo (o viceversa), derivado de la diferencia entre el valor real del servicio y la expectativa inflada de este valor adquirida antes de su experiencia. • El valor esperado de la característica f para todos los visitantes posteriores del sitio, si el usuario i no enviara un informe. En este caso, la motivación para un informe negativo después de un valor excesivamente alto de ef es diferente: el usuario i busca corregir la expectativa de los futuros visitantes del sitio. A diferencia de la interpretación anterior, esto no requiere que el usuario derive una expectativa a priori para el valor de f. Tenga en cuenta que ninguna de las interpretaciones implica que el promedio hasta el informe i esté inversamente relacionado con la calificación en el informe i. Podría existir una medida de influencia ejercida por informes anteriores que impulse al usuario detrás del informe i a enviar calificaciones que, en cierta medida, se ajusten a los informes anteriores: un valor bajo para ef (i) puede influir en el usuario i a enviar una calificación baja para la característica f porque, por ejemplo, teme que enviar una calificación alta lo haga parecer una persona con bajos estándares. Esto, al principio, parece contradecir la Hipótesis 2. Sin embargo, esta calificación de conformidad no puede continuar indefinidamente: una vez que el conjunto de informes proyecte una estimación suficientemente reducida para vf, los revisores futuros con impresiones comparativamente positivas buscarán corregir esta percepción errónea. 4.2 Impacto de los comentarios textuales en la expectativa de calidad. Se puede obtener una mayor comprensión del comportamiento de calificación de los usuarios de TripAdvisor analizando la relación entre los pesos wf y los valores ef (i). En particular, examinamos la siguiente hipótesis: Hipótesis 3. Cuando una gran proporción del texto de una reseña discute una característica en particular, la diferencia entre la calificación para esa característica y la calificación promedio hasta ese momento tiende a ser grande. La intuición detrás de esta afirmación es que cuando el usuario insiste en expresar su opinión sobre una característica en particular, su opinión difiere de la opinión colectiva de publicaciones anteriores. Esto se basa en la característica de los sistemas de reputación como foros de retroalimentación donde un usuario está interesado en proyectar su opinión, con particular fuerza si esta opinión difiere de lo que percibe como la opinión general. Para probar la Hipótesis 3 medimos la diferencia absoluta promedio entre la expectativa ef (i) y la calificación ri f cuando el peso wi f es alto, respectivamente bajo. Los pesos se clasifican como altos o bajos al compararlos con ciertos valores de corte: wi f es bajo si es menor que 0.1, mientras que wi f es alto si es mayor que θf. Se utilizaron diferentes valores de corte para diferentes características: θR = 0.4, θS = 0.4, θC = 0.2 y θV = 0.7. La limpieza tiene un límite inferior más bajo ya que es una característica raramente discutida; el valor tiene un límite superior alto por la razón opuesta. Los resultados se presentan en la Tabla 4. La idea de que los informes negativos pueden fomentar una mayor divulgación negativa ha sido sugerida anteriormente [14]. Tabla 4: Promedio de |ri f −ef (i)| cuando los pesos son altos (primer valor en la celda) y bajos (segundo valor en la celda) con valores P para la diferencia en corchetes cuadrados. Esto demuestra que cuando los pesos son inusualmente altos, los usuarios tienden a expresar una opinión que no se ajusta al promedio neto de calificaciones anteriores. Como podríamos esperar, para una característica que rara vez tuvo un peso importante en la discusión (por ejemplo, la limpieza), la diferencia es particularmente grande. Aunque la diferencia en el valor de la característica es bastante grande para Sídney, el valor P es alto. Esto se debe a que solo unos pocos comentarios discutieron en gran medida el valor. La razón podría ser cultural o porque había menos motivo para discutir esta característica. 4.3 Incentivos para informar Los modelos anteriores sugieren que los usuarios poco opinativos no elegirán expresar sus opiniones [12]. En esta sección, ampliamos este modelo para tener en cuenta la influencia de las expectativas. La motivación para enviar comentarios no se debe solo a opiniones extremas, sino también a la diferencia entre la reputación actual (es decir, la expectativa previa del usuario) y la experiencia real. Un modelo de calificación así produce calificaciones que la mayoría de las veces se desvían de la calificación promedio actual. Las calificaciones que confirman la expectativa previa rara vez serán enviadas. Probamos en nuestro conjunto de datos la proporción de calificaciones que intentan corregir la estimación actual. Definimos una calificación desviada como aquella que se desvía de la expectativa actual por al menos un umbral θ, es decir, |ri f − ef (i)| ≥ θ. Para cada una de las tres ciudades consideradas, las siguientes tablas muestran la proporción de calificaciones desviadas para θ = 0.5 y θ = 1. Los resultados anteriores sugieren que una gran proporción de usuarios (casi la mitad, incluso para el valor umbral alto θ = 1) se desvían del promedio previo. Esto refuerza la idea de que los usuarios son más propensos a enviar un informe cuando creen que tienen algo distintivo que agregar al flujo actual de opiniones sobre alguna característica. Tales conclusiones están en total acuerdo con evidencia previa de que la distribución de informes a menudo sigue distribuciones bimodales en forma de U. 139 5. Para dar cuenta de las observaciones descritas en las secciones anteriores, proponemos un modelo para el comportamiento de los usuarios al enviar reseñas en línea. Para un hotel dado, asumimos que la calidad experimentada por los usuarios sigue una distribución normal alrededor de algún valor vf, que representa la calidad objetiva ofrecida por el hotel en la característica f. La calificación enviada por el usuario i en la característica f es: ˆri f = δf vi f + (1 − δf) · sign vi f − ef (i) c + d(vi f, ef (i)|wi f) (2) donde: • vi f es la calidad (desconocida) experimentada realmente por el usuario. Se asume que vi f sigue una distribución normal alrededor de algún valor vf; • δf ∈ [0, 1] puede verse como una medida del sesgo al reportar retroalimentación. Los valores altos reflejan el hecho de que los usuarios califican de manera objetiva, sin ser influenciados por expectativas previas. El valor de δf puede depender de varios factores; fijamos un valor para cada característica f; • c es una constante entre 1 y 5; • wi f es el peso de la característica f en el comentario textual de la reseña i, calculado según la Ecuación (1); • d(vi f , ef (i)|wi f ) es una función de distancia entre la expectativa y la observación del usuario i. La función de distancia satisface las siguientes propiedades: - d(y, z|w) ≥ 0 para todo y, z ∈ [0, 5], w ∈ [0, 1]; - |d(y, z|w)| < |d(z, x|w)| si |y − z| < |z − x|; - |d(y, z|w1)| < |d(y, z|w2)| si w1 < w2; - c + d(vf , ef (i)|wi f ) ∈ [1, 5]; El segundo término de la Ecuación (2) codifica el sesgo de la calificación. Cuanto mayor sea la distancia entre la observación real vi f y la función ef, mayor será el sesgo. 5.1 Validación del modelo Utilizamos el conjunto de datos de las reseñas de TripAdvisor para validar el modelo de comportamiento presentado anteriormente. Dividimos por conveniencia los valores de calificación en tres rangos: malo (B = {1, 2}), indiferente (I = {3, 4}), y bueno (G = {5}), y realizamos los siguientes dos tests: • Primero, usaremos nuestro modelo para predecir las calificaciones que tienen valores extremos. Para cada hotel, tomamos la secuencia de informes, y cada vez que nos encontramos con una calificación que sea buena o mala (pero no indiferente) intentamos predecirla utilizando la Ecuación (2) • En segundo lugar, en lugar de predecir el valor de las calificaciones extremas, intentamos clasificarlas como buenas o malas. Para cada hotel tomamos la secuencia de informes, y para cada informe (independientemente de su valor) lo clasificamos como bueno o malo. Sin embargo, para realizar estas pruebas, necesitamos estimar el valor objetivo, vf, que es el promedio de las observaciones de calidad reales, vi f. El algoritmo que estamos utilizando se basa en la intuición de que la cantidad de calificación de conformidad se minimiza. En otras palabras, el valor vf debe ser tal que, tan a menudo como sea posible, las calificaciones malas sigan a expectativas por encima de vf y las calificaciones buenas sigan a expectativas por debajo de vf. Formalmente, definimos los conjuntos: Γ1 = {i|ef (i) < vf y ri f ∈ B}; Γ2 = {i|ef (i) > vf y ri f ∈ G}; que corresponden a irregularidades donde, a pesar de que la expectativa en el punto i es menor que el valor entregado, la calificación es baja, y viceversa. Definimos vf como el valor que minimiza la unión de los dos conjuntos: vf = arg min vf |Γ1 ∪ Γ2| (3) En la Ec. (2) reemplazamos vi f por el valor vf calculado en la Ec. (3), y utilizamos la siguiente función de distancia: d(vf , ef (i)|wi f ) = |vf − ef (i)| vf − ef (i) |vf 2 − ef (i)2 | · (1 + 2wi f ); La constante c ∈ I se estableció en min{max{ef (i), 3}}, 4}. Los valores para δf se fijaron en {0.7, 0.7, 0.8, 0.7, 0.6} para las características {General, Habitaciones, Servicio, Limpieza, Valor} respectivamente. Los pesos se calculan tal como se describe en la Sección 3. Como primer experimento, tomamos los conjuntos de calificaciones extremas {ri f |ri f /∈ I} para cada hotel y característica. Para cada calificación de este tipo, ri f, intentamos estimarla calculando ˆri f usando la Ecuación (2). Comparamos este estimador con el obtenido simplemente promediando las calificaciones de todos los hoteles y características: es decir, ¯rf = j,r j f =0 rj f j,r j f =0 1; la Tabla 7 presenta la relación entre el error cuadrático medio (RMSE) al usar ˆri f y ¯rf para estimar las calificaciones reales. En todos los casos, la estimación producida por nuestro modelo es mejor que el promedio simple. Tabla 7: Promedio de RMSE(ˆrf) RMSE(¯rf) Ciudad O R S C V Boston 0.987 0.849 0.879 0.776 0.913 Sídney 0.927 0.817 0.826 0.720 0.681 Las Vegas 0.952 0.870 0.881 0.947 0.904 Como segundo experimento, intentamos distinguir los conjuntos Bf = {i|ri f ∈ B} y Gf = {i|ri f ∈ G} de calificaciones malas y buenas, respectivamente, en la característica f. Por ejemplo, calculamos el conjunto Bf usando el siguiente clasificador (llamado σ): ri f ∈ Bf (σf (i) = 1) ⇔ ˆri f ≤ 4; Las Tablas 8, 9 y 10 presentan la Precisión (p), Recall (r) y s = 2pr p+r para el clasificador σ, y lo comparan con un clasificador de mayoría ingenuo, τ, τf (i) = 1 ⇔ |Bf | ≥ |Gf |: Vemos que el recall es siempre mayor para σ y la precisión suele ser ligeramente peor. Para la métrica s, σ tiende a agregar un 140. Tabla 8: Precisión (p), Recall (r), s= 2pr p+r mientras se detectan calificaciones bajas para Boston O R S C V p 0.678 0.670 0.573 0.545 0.610 σ r 0.626 0.659 0.619 0.612 0.694 s 0.651 0.665 0.595 0.577 0.609 p 0.684 0.706 0.647 0.611 0.633 τ r 0.597 0.541 0.410 0.383 0.562 s 0.638 0.613 0.502 0.471 0.595 Tabla 9: Precisión (p), Recall (r), s= 2pr p+r mientras se detectan calificaciones bajas para Las Vegas O R S C V p 0.654 0.748 0.592 0.712 0.583 σ r 0.608 0.536 0.791 0.474 0.610 s 0.630 0.624 0.677 0.569 0.596 p 0.685 0.761 0.621 0.748 0.606 τ r 0.542 0.505 0.767 0.445 0.441 s 0.605 0.607 0.670 0.558 0.511 Mejora del 1-20% sobre τ, mucho mayor en algunos casos para hoteles en Sídney. Esto se debe probablemente a que las reseñas de Sídney son más positivas que las de las ciudades estadounidenses y los casos en los que el número de reseñas negativas supera al de las positivas son raros. Reemplazar el algoritmo de prueba con uno que juega un 1 con una probabilidad igual a la proporción de malas críticas mejora sus resultados para esta ciudad, pero aún es superado por alrededor del 80%. 6. RESUMEN DE RESULTADOS Y CONCLUSIÓN El objetivo de este artículo es explorar los factores que llevan a un usuario a enviar una calificación particular, en lugar de los incentivos que lo animaron a enviar un informe en primer lugar. Para ello utilizamos dos fuentes adicionales de información además del vector de calificaciones numéricas: primero, examinamos los comentarios textuales que acompañan las reseñas, y segundo, consideramos los informes que han sido previamente enviados por otros usuarios. Usando algoritmos simples de procesamiento de lenguaje natural, pudimos establecer una correlación entre el peso de una característica específica en el comentario textual que acompaña la reseña, y el ruido presente en la calificación numérica. Específicamente, parece que los usuarios que discuten ampliamente una característica en particular tienden a estar de acuerdo en una calificación común. Esta observación permite la construcción de estimadores de calidad característica por característica que tienen una varianza más baja y, con suerte, son menos ruidosos. Sin embargo, se requiere más evidencia para respaldar la intuición de que las calificaciones correspondientes a pesos altos son opiniones de expertos que merecen ser consideradas de mayor prioridad al calcular estimaciones de calidad. En segundo lugar, enfatizamos la dependencia de las calificaciones en informes previos. Los informes anteriores crean una expectativa de calidad que afecta la percepción subjetiva del usuario. Validamos dos hechos sobre las reseñas de hoteles que recopilamos de TripAdvisor: Primero, las calificaciones que siguen expectativas bajas (donde la expectativa se calcula como el promedio de los informes anteriores) probablemente sean más altas que las calificaciones que siguen expectativas altas. Intuitivamente, la percepción de la calidad (y consecuentemente la calificación) depende de qué tan bien la experiencia real del usuario cumple con sus expectativas. Segundo, incluimos evidencia de los comentarios textuales, y encontramos que cuando los usuarios dedican una gran parte del texto a discutir una característica específica, es probable que motiven una calificación divergente (es decir, una calificación que no se ajusta a la expectativa previa). Intuitivamente, esto respalda la hipótesis de que los foros de reseñas actúan como grupos de discusión donde los usuarios están interesados en presentar y motivar su propia opinión. Hemos capturado la evidencia empírica en un modelo de comportamiento que predice las calificaciones enviadas por los usuarios. La calificación final depende, como era de esperar, de la observación real y de la brecha entre la observación y la expectativa. La brecha tiende a tener una influencia mayor cuando una fracción importante del comentario textual está dedicada a discutir una característica específica. El modelo propuesto fue validado con los datos empíricos y proporciona mejores estimaciones de las calificaciones realmente enviadas. Una suposición que hacemos es sobre la existencia de un valor de calidad objetivo vf para la característica f. Esto rara vez es cierto, especialmente a lo largo de largos periodos de tiempo. Otras explicaciones podrían dar cuenta de la correlación de las calificaciones con informes anteriores. Por ejemplo, si ef (i) refleja el valor real de f en un punto en el tiempo, la diferencia en las calificaciones siguiendo expectativas altas y bajas puede ser explicada por modelos de ingresos hoteleros que se maximizan cuando el valor es modificado en consecuencia. Sin embargo, la idea de que la variación en las calificaciones no es principalmente una función de la variación en el valor resulta ser útil. Nuestro enfoque para aproximar este valor objetivo es de ninguna manera perfecto, pero se ajusta perfectamente a la idea detrás del modelo. Una dirección natural para trabajos futuros es examinar aplicaciones concretas de nuestros resultados. Es probable que se obtengan mejoras significativas en las estimaciones de calidad al incorporar toda la evidencia empírica sobre el comportamiento de calificación. No está claro exactamente cómo diferentes factores afectan las decisiones de los usuarios. La respuesta podría depender de la aplicación particular, el contexto y la cultura. 7. REFERENCIAS [1] A. Admati y P. Pfleiderer. Noisytalk.com: Transmitiendo opiniones en un entorno ruidoso. Documento de trabajo 1670R, Universidad de Stanford, 2000. [2] P. B., L. Lee y S. Vaithyanathan. ¿Clasificación de sentimientos con técnicas de aprendizaje automático? En Actas de EMNLP-02, la Conferencia sobre Métodos Empíricos en el Procesamiento del Lenguaje Natural, 2002. [3] H. Cui, V. Mittal y M. Datar. Experimentos comparativos 141 sobre clasificación de sentimientos para reseñas de productos en línea. En Actas de AAAI, 2006. [4] K. Dave, S. Lawrence y D. Pennock. Extracción de opiniones y clasificación semántica de reseñas de productos en la galería de cacahuetes. En Actas de la 12ª Conferencia Internacional sobre la World Wide Web (WWW03), 2003. [5] C. Dellarocas, N. Awad y X. Zhang. Explorando el valor de las calificaciones de productos en línea en la previsión de ingresos: El caso de las películas en movimiento. Documento de trabajo, 2006. [6] C. Forman, A. Ghose y B. Wiesenfeld. Un Examen Multinivel del Impacto de las Identidades Sociales en las Transacciones Económicas en los Mercados Electrónicos. Disponible en SSRN: http://ssrn.com/abstract=918978, julio de 2006. [7] A. Ghose, P. Ipeirotis y A. Sundararajan. Primas de reputación en mercados electrónicos peer-to-peer: análisis de retroalimentación textual y estructura de red. En el Tercer Taller sobre Economía de Sistemas Peer-to-Peer (P2PECON), 2005. [8] A. Ghose, P. Ipeirotis y A. Sundararajan. Las dimensiones de la reputación en los mercados electrónicos. Documento de trabajo CeDER-06-02, Universidad de Nueva York, 2006. [9] A. Harmon. Error de Amazon revela la guerra de los revisores. The New York Times, 14 de febrero de 2004. [10] D. Houser y J. Wooders. Reputación en subastas: teoría y evidencia de eBay. Revista de Estrategia Económica y de Gestión, 15:353-369, 2006. [11] M. Hu y B. Liu. Extracción y resumen de reseñas de clientes. En Actas de la Conferencia Internacional de la ACM SIGKDD sobre Descubrimiento de Conocimiento y Minería de Datos (KDD04), 2004. [12] N. Hu, P. Pavlou y J. Zhang. ¿Pueden las reseñas en línea revelar la verdadera calidad de un producto? En Actas de la Conferencia ACM sobre Comercio Electrónico (EC 06), 2006. [13] K. Kalyanam y S. McIntyre. Retorno de la reputación en el mercado de subastas en línea. Documento de trabajo 02/03-10-WP, Escuela de Negocios Leavey, Universidad de Santa Clara, 2001. [14] L. Khopkar y P. Resnick. Auto-selección, deslizamiento, salvamento, holgazanería y lapidación: los impactos de la retroalimentación negativa en eBay. En Actas de la Conferencia ACM sobre Comercio Electrónico (EC 05), 2005. [15] M. Melnik y J. Alm. ¿Importa la reputación de un vendedor? evidencia de subastas en eBay. Revista de Economía Industrial, 50(3):337-350, 2002. [16] R. Olshavsky y J. Miller. Expectativas del consumidor, rendimiento del producto y calidad percibida del producto. Revista de Investigación de Marketing, 9:19-21, febrero de 1972. [17] A. Parasuraman, V. Zeithaml y L. Berry. Un Modelo Conceptual de Calidad de Servicio y sus Implicaciones para Investigaciones Futuras. Revista de Marketing, 49:41-50, 1985. [18] A. Parasuraman, V. Zeithaml y L. Berry. SERVQUAL: Una escala de múltiples ítems para medir las percepciones de calidad del servicio por parte del consumidor. Revista de Retail, 64:12-40, 1988. [19] P. Pavlou y A. Dimoka. La naturaleza y función de los comentarios de texto de retroalimentación en los mercados en línea: implicaciones para la construcción de confianza, primas de precio y diferenciación de vendedores. Investigación de Sistemas de Información, 17(4):392-414, 2006. [20] A. Popescu y O. Etzioni. Extrayendo características del producto y opiniones de las reseñas. En Actas de la Conferencia de Tecnología del Lenguaje Humano y Conferencia sobre Métodos Empíricos en Procesamiento del Lenguaje Natural, 2005. [21] R. Teas. Expectativas, Evaluación del Rendimiento y Percepciones de Calidad de los Consumidores. Revista de Marketing, 57:18-34, 1993. [22] E. White. Chateando a un cantante en las listas de éxitos pop. El Wall Street Journal, 15 de octubre de 1999. APÉNDICE A. LISTA DE PALABRAS, LR, ASOCIADAS A LA CARACTERÍSTICA HABITACIONES. Todas las palabras sirven como prefijos: habitación, espacio, interior, decoración, ambiente, atmósfera, comodidad, baño, inodoro, cama, edificio, pared, ventana, privado, temperatura, sábana, lino, almohada, caliente, agua, fría, agua, ducha, vestíbulo, muebles, alfombra, aire, acondicionado, colchón, distribución, diseño, espejo, techo, iluminación, lámpara, sofá, silla, cómoda, armario, armario. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "the product utility": {
            "translated_key": "la utilidad del producto",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Understanding User Behavior in Online Feedback Reporting Arjun Talwar Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland arjun@math.stanford.edu Radu Jurca Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland radu.jurca@epfl.ch Boi Faltings Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland boi.faltings@epfl.ch ABSTRACT Online reviews have become increasingly popular as a way to judge the quality of various products and services.",
                "Previous work has demonstrated that contradictory reporting and underlying user biases make judging the true worth of a service difficult.",
                "In this paper, we investigate underlying factors that influence user behavior when reporting feedback.",
                "We look at two sources of information besides numerical ratings: linguistic evidence from the textual comment accompanying a review, and patterns in the time sequence of reports.",
                "We first show that groups of users who amply discuss a certain feature are more likely to agree on a common rating for that feature.",
                "Second, we show that a users rating partly reflects the difference between true quality and prior expectation of quality as inferred from previous reviews.",
                "Both give us a less noisy way to produce rating estimates and reveal the reasons behind user bias.",
                "Our hypotheses were validated by statistical evidence from hotel reviews on the TripAdvisor website.",
                "Categories and Subject Descriptors J.4 [Social and Behavioral Sciences]: Economics General Terms Economics, Experimentation, Reliability 1.",
                "MOTIVATIONS The spread of the internet has made it possible for online feedback forums (or reputation mechanisms) to become an important channel for Word-of-mouth regarding products, services or other types of commercial interactions.",
                "Numerous empirical studies [10, 15, 13, 5] show that buyers seriously consider online feedback when making purchasing decisions, and are willing to pay reputation premiums for products or services that have a good reputation.",
                "Recent analysis, however, raises important questions regarding the ability of existing forums to reflect the real quality of a product.",
                "In the absence of clear incentives, users with a moderate outlook will not bother to voice their opinions, which leads to an unrepresentative sample of reviews.",
                "For example, [12, 1] show that Amazon1 ratings of books or CDs follow with great probability bi-modal, U-shaped distributions where most of the ratings are either very good, or very bad.",
                "Controlled experiments, on the other hand, reveal opinions on the same items that are normally distributed.",
                "Under these circumstances, using the arithmetic mean to predict quality (as most forums actually do) gives the typical user an estimator with high variance that is often false.",
                "Improving the way we aggregate the information available from online reviews requires a deep understanding of the underlying factors that bias the rating behavior of users.",
                "Hu et al. [12] propose the Brag-and-Moan Model where users rate only if their utility of the product (drawn from a normal distribution) falls outside a median interval.",
                "The authors conclude that the model explains the empirical distribution of reports, and offers insights into smarter ways of estimating the true quality of the product.",
                "In the present paper we extend this line of research, and attempt to explain further facts about the behavior of users when reporting online feedback.",
                "Using actual hotel reviews from the TripAdvisor2 website, we consider two additional sources of information besides the basic numerical ratings submitted by users.",
                "The first is simple linguistic evidence from the textual review that usually accompanies the numerical ratings.",
                "We use text-mining techniques similar to [7] and [3], however, we are only interested in identifying what aspects of the service the user is discussing, without computing the semantic orientation of the text.",
                "We find that users who comment more on the same feature are more likely to agree on a common numerical rating for that particular feature.",
                "Intuitively, lengthy comments reveal the importance of the feature to the user.",
                "Since people tend to be more knowledgeable in the aspects they consider important, users who discuss a given feature in more details might be assumed to have more authority in evaluating that feature.",
                "Second we investigate the relationship between a review 1 http://www.amazon.com 2 http://www.tripadvisor.com/ 134 Figure 1: The TripAdvisor page displaying reviews for a popular Boston hotel.",
                "Name of hotel and advertisements were deliberatively erased. and the reviews that preceded it.",
                "A perusal of online reviews shows that ratings are often part of discussion threads, where one post is not necessarily independent of other posts.",
                "One may see, for example, users who make an effort to contradict, or vehemently agree with, the remarks of previous users.",
                "By analyzing the time sequence of reports, we conclude that past reviews influence the future reports, as they create some prior expectation regarding the quality of service.",
                "The subjective perception of the user is influenced by the gap between the prior expectation and the actual performance of the service [17, 18, 16, 21] which will later reflect in the users rating.",
                "We propose a model that captures the dependence of ratings on prior expectations, and validate it using the empirical data we collected.",
                "Both results can be used to improve the way reputation mechanisms aggregate the information from individual reviews.",
                "Our first result can be used to determine a featureby-feature estimate of quality, where for each feature, a different subset of reviews (i.e., those with lengthy comments of that feature) is considered.",
                "The second leads to an algorithm that outputs a more precise estimate of the real quality. 2.",
                "THE DATA SET We use in this paper real hotel reviews collected from the popular travel site TripAdvisor.",
                "TripAdvisor indexes hotels from cities across the world, along with reviews written by travelers.",
                "Users can search the site by giving the hotels name and location (optional).",
                "The reviews for a given hotel are displayed as a list (ordered from the most recent to the oldest), with 5 reviews per page.",
                "The reviews contain: • information about the author of the review (e.g., dates of stay, username of the reviewer, location of the reviewer); • the overall rating (from 1, lowest, to 5, highest); • a textual review containing a title for the review, free comments, and the main things the reviewer liked and disliked; • numerical ratings (from 1, lowest, to 5, highest) for different features (e.g., cleanliness, service, location, etc.)",
                "Below the name of the hotel, TripAdvisor displays the address of the hotel, general information (number of rooms, number of stars, short description, etc), the average overall rating, the TripAdvisor ranking, and an average rating for each feature.",
                "Figure 1 shows the page for a popular Boston hotel whose name (along with advertisements) was explicitly erased.",
                "We selected three cities for this study: Boston, Sydney and Las Vegas.",
                "For each city we considered all hotels that had at least 10 reviews, and recorded all reviews.",
                "Table 1 presents the number of hotels considered in each city, the total number of reviews recorded for each city, and the distribution of hotels with respect to the star-rating (as available on the TripAdvisor site).",
                "Note that not all hotels have a star-rating.",
                "Table 1: A summary of the data set.",
                "City # Reviews # Hotels # of Hotels with 1,2,3,4 & 5 stars Boston 3993 58 1+3+17+15+2 Sydney 1371 47 0+0+9+13+10 Las Vegas 5593 40 0+3+10+9+6 For each review we recorded the overall rating, the textual review (title and body of the review) and the numerical rating on 7 features: Rooms(R), Service(S), Cleanliness(C), Value(V), Food(F), Location(L) and Noise(N).",
                "TripAdvisor does not require users to submit anything other than the overall rating, hence a typical review rates few additional features, regardless of the discussion in the textual comment.",
                "Only the features Rooms(R), Service(S), Cleanliness(C) and Value(V) are rated by a significant number of users.",
                "However, we also selected the features Food(F), Location(L) and Noise(N) because they are referred to in a significant number of textual comments.",
                "For each feature we record the numerical rating given by the user, or 0 when the rating is missing.",
                "The typical length of the textual comment amounts to approximately 200 words.",
                "All data was collected by crawling the TripAdvisor site in September 2006. 2.1 Formal notation We will formally refer to a review by a tuple (r, T) where: • r = (rf ) is a vector containing the ratings rf ∈ {0, 1, . . . 5} for the features f ∈ F = {O, R, S, C, V, F, L, N}; note that the overall rating, rO, is abusively recorded as the rating for the feature Overall(O); • T is the textual comment that accompanies the review. 135 Reviews are indexed according to the variable i, such that (ri , Ti ) is the ith review in our database.",
                "Since we dont record the username of the reviewer, we will also say that the ith review in our data set was submitted by user i.",
                "When we need to consider only the reviews of a given hotel, h, we will use (ri(h) , Ti(h) ) to denote the ith review about the hotel h. 3.",
                "EVIDENCE FROM TEXTUAL COMMENTS The free textual comments associated to online reviews are a valuable source of information for understanding the reasons behind the numerical ratings left by the reviewers.",
                "The text may, for example, reveal concrete examples of aspects that the user liked or disliked, thus justifying some of the high, respectively low ratings for certain features.",
                "The text may also offer guidelines for understanding the preferences of the reviewer, and the weights of different features when computing an overall rating.",
                "The problem, however, is that free textual comments are difficult to read.",
                "Users are required to scroll through many reviews and read mostly repetitive information.",
                "Significant improvements would be obtained if the reviews were automatically interpreted and aggregated.",
                "Unfortunately, this seems a difficult task for computers since human users often use witty language, abbreviations, cultural specific phrases, and the figurative style.",
                "Nevertheless, several important results use the textual comments of online reviews in an automated way.",
                "Using well established natural language techniques, reviews or parts of reviews can be classified as having a positive or negative semantic orientation.",
                "Pang et al. [2] classify movie reviews into positive/negative by training three different classifiers (Naive Bayes, Maximum Entropy and SVM) using classification features based on unigrams, bigrams or part-of-speech tags.",
                "Dave et al. [4] analyze reviews from CNet and Amazon, and surprisingly show that classification features based on unigrams or bigrams perform better than higher-order n-grams.",
                "This result is challenged by Cui et al. [3] who look at large collections of reviews crawled from the web.",
                "They show that the size of the data set is important, and that bigger training sets allow classifiers to successfully use more complex classification features based on n-grams.",
                "Hu and Liu [11] also crawl the web for product reviews and automatically identify product attributes that have been discussed by reviewers.",
                "They use Wordnet to compute the semantic orientation of product evaluations and summarize user reviews by extracting positive and negative evaluations of different product features.",
                "Popescu and Etzioni [20] analyze a similar setting, but use search engine hit-counts to identify product attributes; the semantic orientation is assigned through the relaxation labeling technique.",
                "Ghose et al. [7, 8] analyze seller reviews from the Amazon secondary market to identify the different dimensions (e.g., delivery, packaging, customer support, etc.) of reputation.",
                "They parse the text, and tag the part-of-speech for each word.",
                "Frequent nouns, noun phrases and verbal phrases are identified as dimensions of reputation, while the corresponding modifiers (i.e., adjectives and adverbs) are used to derive numerical scores for each dimension.",
                "The enhanced reputation measure correlates better with the pricing information observed in the market.",
                "Pavlou and Dimoka [19] analyze eBay reviews and find that textual comments have an important impact on reputation premiums.",
                "Our approach is similar to the previously mentioned works, in the sense that we identify the aspects (i.e., hotel features) discussed by the users in the textual reviews.",
                "However, we do not compute the semantic orientation of the text, nor attempt to infer missing ratings.",
                "We define the weight, wi f , of feature f ∈ F in the text Ti associated with the review (ri , Ti ), as the fraction of Ti dedicated to discussing aspects (both positive and negative) related to feature f. We propose an elementary method to approximate the values of these weights.",
                "For each feature we manually construct the word list Lf containing approximately 50 words that are most commonly associated to the feature f. The initial words were selected from reading some of the reviews, and seeing what words coincide with discussion of which features.",
                "The list was then extended by adding all thesaurus entries that were related to the initial words.",
                "Finally, we brainstormed for missing words that would normally be associated with each of the features.",
                "Let Lf ∩Ti be the list of terms common to both Lf and Ti.",
                "Each term of Lf is counted the number of times it appears in Ti , with two exceptions: • in cases where the user submits a title to the review, we account for the title text by appending it three times to the review text Ti .",
                "The intuitive assumption is that the users opinion is more strongly reflected in the title, rather than in the body of the review.",
                "For example, many reviews are accurately summarized by titles such as Excellent service, terrible location or Bad value for money; • certain words that occur only once in the text are counted multiple times if their relevance to that feature is particularly strong.",
                "These were root words for each feature (e.g., staff is a root word for the feature Service), and were weighted either 2 or 3.",
                "Each feature was assigned up to 3 such root words, so almost all words are counted only once.",
                "The list of words for the feature Rooms is given for reference in Appendix A.",
                "The weight wi f is computed as: wi f = |Lf ∩ Ti| f∈F |Lf ∩ Ti| (1) where |Lf ∩Ti | is the number of terms common to Lf and Ti .",
                "The weight for the feature Overall was set to min{ |T i | 5000 , 1} where |Ti | is the number of character in Ti .",
                "The following is a TripAdvisor review for a Boston hotel (the name of the hotel is omitted): Ill start by saying that Im more of a Holiday Inn person than a *** type.",
                "So I get frustrated when I pay double the room rate and get half the amenities that Id get at a Hampton Inn or Holiday Inn.",
                "The location was definitely the main asset of this place.",
                "It was only a few blocks from the Hynes Center subway stop and it was easy to walk to some good restaurants in the Back Bay area.",
                "Boylston isnt far off at all.",
                "So I had no trouble with foregoing a rental car and taking the subway from the airport to the hotel and using the subway for any other travel.",
                "Otherwise, they make you pay for anything and everything. 136 And when youve already dropped $215/night on the room, that gets frustrating.The room itself was decent, about what I would expect.",
                "Staff was also average, not bad and not excellent.",
                "Again, I think youre paying for location and the ability to walk to a lot of good stuff.",
                "But I think next time Ill stay in Brookline, get more amenities, and use the subway a bit more.",
                "This numerical ratings associated to this review are rO = 3, rR = 3, rS = 3, rC = 4, rV = 2 for features Overall(O), Rooms(R), Service(S), Cleanliness(C) and Value(V) respectively.",
                "The ratings for the features Food(F), Location(L) and Noise(N) are absent (i.e., rF = rL = rN = 0).",
                "The weights wf are computed from the following lists of common terms: LR ∩ T ={room}; wR = 0.066 LS ∩ T ={3 * Staff, amenities}; wS = 0.267 LC ∩ T = ∅; wC = 0 LV ∩ T ={$, rate}; wV = 0.133 LF ∩ T ={restaurant}; wF = 0.067 LL ∩ T ={2 * center, 2 * walk, 2 * location, area}; wL = 0.467 LN ∩ T = ∅; wN = 0 The root words Staff and Center were tripled and doubled respectively.",
                "The overall weight of the textual review is wO = 0.197.",
                "These values account reasonably well for the weights of different features in the discussion of the reviewer.",
                "One point to note is that some terms in the lists Lf possess an inherent semantic orientation.",
                "For example the word grime (belonging to the list LC ) would be used most often to assert the presence, and not the absence of grime.",
                "This is unavoidable, but care was taken to ensure words from both sides of the spectrum were used.",
                "For this reason, some lists such as LR contain only nouns of objects that one would typically describe in a room (see Appendix A).",
                "The goal of this section is to analyse the influence of the weights wi f on the numerical ratings ri f .",
                "Intuitively, users who spent a lot of their time discussing a feature f (i.e., wi f is high) had something to say about their experience with regard to this feature.",
                "Obviously, feature f is important for user i.",
                "Since people tend to be more knowledgeable in the aspects they consider important, our hypothesis is that the ratings ri f (corresponding to high weights wi f ) constitute a subset of expert ratings for feature f. Figure 2 plots the distribution of the rates r i(h) C with respect to the weights w i(h) C for the cleanliness of a Las Vegas hotel, h. Here, the high ratings are restricted to the reviews that discuss little the cleanliness.",
                "Whenever cleanliness appears in the discussion, the ratings are low.",
                "Many hotels exhibit similar rating patterns for various features.",
                "Ratings corresponding to low weights span the whole spectrum from 1 to 5, while the ratings corresponding to high weights are more grouped together (either around good or bad ratings).",
                "We therefore make the following hypothesis: Hypothesis 1.",
                "The ratings ri f corresponding to the reviews where wi f is high, are more similar to each other than to the overall collection of ratings.",
                "To test the hypothesis, we take the entire set of reviews, and feature by feature, we compute the standard deviation of the ratings with high weights, and the standard deviation of the entire set of ratings.",
                "High weights were defined as those belonging to the upper 20% of the weight range for the corresponding feature.",
                "If Hypothesis 1 were true, the standard deviation of all ratings should be higher than the standard deviation of the ratings with high weights. 0 1 2 3 4 5 6 0 0.1 0.2 0.3 0.4 0.5 0.6 Rating Weight Figure 2: The distribution of ratings against the weight of the cleanliness feature.",
                "We use a standard T-test to measure the significance of the results.",
                "City by city and feature by feature, Table 2 presents the average standard deviation of all ratings, and the average standard deviation of ratings with high weights.",
                "Indeed, the ratings with high weights have lower standard deviation, and the results are significant at the standard 0.05 significance threshold (although for certain cities taken independently there doesnt seem to be a significant difference, the results are significant for the entire data set).",
                "Please note that only the features O,R,S,C and V were considered, since for the others (F, L, and N) we didnt have enough ratings.",
                "Table 2: Average standard deviation for all ratings, and average standard deviation for ratings with high weights.",
                "In square brackets, the corresponding p-values for a positive difference between the two.",
                "City O R S C V all 1.189 0.998 1.144 0.935 1.123 Boston high 0.948 0.778 0.954 0.767 0.891 p-val [0.000] [0.004] [0.045] [0.080] [0.009] all 1.040 0.832 1.101 0.847 0.963 Sydney high 0.801 0.618 0.691 0.690 0.798 p-val [0.012] [0.023] [0.000] [0.377] [0.037] all 1.272 1.142 1.184 1.119 1.242 Vegas high 1.072 0.752 1.169 0.907 1.003 p-val [0.0185] [0.001] [0.918] [0.120] [0.126] Hypothesis 1 not only provides some basic understanding regarding the rating behavior of online users, it also suggests some ways of computing better quality estimates.",
                "We can, for example, construct a feature-by-feature quality estimate with much lower variance: for each feature we take the subset of reviews that amply discuss that feature, and output as a quality estimate the average rating for this subset.",
                "Initial experiments suggest that the average feature-by-feature ratings computed in this way are different from the average ratings computed on the whole data set.",
                "Given that, indeed, high weights are indicators of expert opinions, the estimates obtained in this way are more accurate than the current ones.",
                "Nevertheless, the validation of this underlying assumption requires further controlled experiments. 137 4.",
                "THE INFLUENCE OF PAST RATINGS Two important assumptions are generally made about reviews submitted to online forums.",
                "The first is that ratings truthfully reflect the quality observed by the users; the second is that reviews are independent from one another.",
                "While anecdotal evidence [9, 22] challenges the first assumption3 , in this section, we address the second.",
                "A perusal of online reviews shows that reviews are often part of discussion threads, where users make an effort to contradict, or vehemently agree with the remarks of previous users.",
                "Consider, for example, the following review: I dont understand the negative reviews... the hotel was a little dark, but that was the style.",
                "It was very artsy.",
                "Yes it was close to the freeway, but in my opinion the sound of an occasional loud car is better than hearing the ding ding of slot machines all night!",
                "The staff on-hand is FABULOUS.",
                "The waitresses are great (and *** does not deserve the bad review she got, she was 100% attentive to us! ), the bartenders are friendly and professional at the same time...",
                "Here, the user was disturbed by previous negative reports, addressed these concerns, and set about trying to correct them.",
                "Not surprisingly, his ratings were considerably higher than the average ratings up to this point.",
                "It seems that TripAdvisor users regularly read the reports submitted by previous users before booking a hotel, or before writing a review.",
                "Past reviews create some prior expectation regarding the quality of service, and this expectation has an influence on the submitted review.",
                "We believe this observation holds for most online forums.",
                "The subjective perception of quality is directly proportional to how well the actual experience meets the prior expectation, a fact confirmed by an important line of econometric and marketing research [17, 18, 16, 21].",
                "The correlation between the reviews has also been confirmed by recent research on the dynamics of online review forums [6]. 4.1 Prior Expectations We define the prior expectation of user i regarding the feature f, as the average of the previously available ratings on the feature f4 : ef (i) = j<i,r j f =0 rj f j<i,r j f =0 1 As a first hypothesis, we assert that the rating ri f is a function of the prior expectation ef (i): Hypothesis 2.",
                "For a given hotel and feature, given the reviews i and j such that ef (i) is high and ef (j) is low, the rating rj f exceeds the rating ri f .",
                "We define high and low expectations as those that are above, respectively below a certain cutoff value θ.",
                "The set of reviews preceded by high, respectively low expectations 3 part of Amazon reviews were recognized as strategic posts by book authors or competitors 4 if no previous ratings were assigned for feature f, ef (i) is assigned a default value of 4.",
                "Table 3: Average ratings for reviews preceded by low (first value in the cell) and high (second value in the cell) expectations.",
                "The P-values for a positive difference are given square brackets.",
                "City O R S C V 3.953 4.045 3.985 4.252 3.946 Boston 3.364 3.590 3.485 3.641 3.242 [0.011] [0.028] [0.0086] [0.0168] [0.0034] 4.284 4.358 4.064 4.530 4.428 Sydney 3.756 3.537 3.436 3.918 3.495 [0.000] [0.000] [0.035] [0.009] [0.000] 3.494 3.674 3.713 3.689 3.580 Las Vegas 3.140 3.530 2.952 3.530 3.351 [0.190] [0.529] [0.007] [0.529] [0.253] are defined as follows: Rhigh f = {ri f |ef (i) > θ} Rlow f = {ri f |ef (i) < θ} These sets are specific for each (hotel, feature) pair, and in our experiments we took θ = 4.",
                "This rather high value is close to the average rating across all features across all hotels, and is justified by the fact that our data set contains mostly high quality hotels.",
                "For each city, we take all hotels and compute the average ratings in the sets Rhigh f and Rlow f (see Table 3).",
                "The average rating amongst reviews following low prior expectations is significantly higher than the average rating following high expectations.",
                "As further evidence, we consider all hotels for which the function eV (i) (the expectation for the feature Value) has a high value (greater than 4) for some i, and a low value (less than 4) for some other i.",
                "Intuitively, these are the hotels for which there is a minimal degree of variation in the timely sequence of reviews: i.e., the cumulative average of ratings was at some point high and afterwards became low, or vice-versa.",
                "Such variations are observed for about half of all hotels in each city.",
                "Figure 3 plots the median (across considered hotels) rating, rV , when ef (i) is not more than x but greater than x − 0.5. 2.5 3 3.5 4 4.5 5 2.5 3 3.5 4 4.5 5 Medianofrating expectation Boston Sydney Vegas Figure 3: The ratings tend to decrease as the expectation increases. 138 There are two ways to interpret the function ef (i): • The expected value for feature f obtained by user i before his experience with the service, acquired by reading reports submitted by past users.",
                "In this case, an overly high value for ef (i) would drive the user to submit a negative report (or vice versa), stemming from the difference between the actual value of the service, and the inflated expectation of this value acquired before his experience. • The expected value of feature f for all subsequent visitors of the site, if user i were not to submit a report.",
                "In this case, the motivation for a negative report following an overly high value of ef is different: user i seeks to correct the expectation of future visitors to the site.",
                "Unlike the interpretation above, this does not require the user to derive an a priori expectation for the value of f. Note that neither interpretation implies that the average up to report i is inversely related to the rating at report i.",
                "There might exist a measure of influence exerted by past reports that pushes the user behind report i to submit ratings which to some extent conforms with past reports: a low value for ef (i) can influence user i to submit a low rating for feature f because, for example, he fears that submitting a high rating will make him out to be a person with low standards5 .",
                "This, at first, appears to contradict Hypothesis 2.",
                "However, this conformity rating cannot continue indefinitely: once the set of reports project a sufficiently deflated estimate for vf , future reviewers with comparatively positive impressions will seek to correct this misconception. 4.2 Impact of textual comments on quality expectation Further insight into the rating behavior of TripAdvisor users can be obtained by analyzing the relationship between the weights wf and the values ef (i).",
                "In particular, we examine the following hypothesis: Hypothesis 3.",
                "When a large proportion of the text of a review discusses a certain feature, the difference between the rating for that feature and the average rating up to that point tends to be large.",
                "The intuition behind this claim is that when the user is adamant about voicing his opinion regarding a certain feature, his opinion differs from the collective opinion of previous postings.",
                "This relies on the characteristic of reputation systems as feedback forums where a user is interested in projecting his opinion, with particular strength if this opinion differs from what he perceives to be the general opinion.",
                "To test Hypothesis 3 we measure the average absolute difference between the expectation ef (i) and the rating ri f when the weight wi f is high, respectively low.",
                "Weights are classified high or low by comparing them with certain cutoff values: wi f is low if smaller than 0.1, while wi f is high if greater than θf .",
                "Different cutoff values were used for different features: θR = 0.4, θS = 0.4, θC = 0.2, and θV = 0.7.",
                "Cleanliness has a lower cutoff since it is a feature rarely discussed; Value has a high cutoff for the opposite reason.",
                "Results are presented in Table 4. 5 The idea that negative reports can encourage further negative reporting has been suggested before [14] Table 4: Average of |ri f −ef (i)| when weights are high (first value in the cell) and low (second value in the cell) with P-values for the difference in sq. brackets.",
                "City R S C V 1.058 1.208 1.728 1.356 Boston 0.701 0.838 0.760 0.917 [0.022] [0.063] [0.000] [0.218] 1.048 1.351 1.218 1.318 Sydney 0.752 0.759 0.767 0.908 [0.179] [0.009] [0.165] [0.495] 1.184 1.378 1.472 1.642 Las Vegas 0.772 0.834 0.808 1.043 [0.071] [0.020] [0.006] [0.076] This demonstrates that when weights are unusually high, users tend to express an opinion that does not conform to the net average of previous ratings.",
                "As we might expect, for a feature that rarely was a high weight in the discussion, (e.g., cleanliness) the difference is particularly large.",
                "Even though the difference in the feature Value is quite large for Sydney, the P-value is high.",
                "This is because only few reviews discussed value heavily.",
                "The reason could be cultural or because there was less of a reason to discuss this feature. 4.3 Reporting Incentives Previous models suggest that users who are not highly opinionated will not choose to voice their opinions [12].",
                "In this section, we extend this model to account for the influence of expectations.",
                "The motivation for submitting feedback is not only due to extreme opinions, but also to the difference between the current reputation (i.e., the prior expectation of the user) and the actual experience.",
                "Such a rating model produces ratings that most of the time deviate from the current average rating.",
                "The ratings that confirm the prior expectation will rarely be submitted.",
                "We test on our data set the proportion of ratings that attempt to correct the current estimate.",
                "We define a deviant rating as one that deviates from the current expectation by at least some threshold θ, i.e., |ri f − ef (i)| ≥ θ.",
                "For each of the three considered cities, the following tables, show the proportion of deviant ratings for θ = 0.5 and θ = 1.",
                "Table 5: Proportion of deviant ratings with θ = 0.5 City O R S C V Boston 0.696 0.619 0.676 0.604 0.684 Sydney 0.645 0.615 0.672 0.614 0.675 Las Vegas 0.721 0.641 0.694 0.662 0.724 Table 6: Proportion of deviant ratings with θ = 1 City O R S C V Boston 0.420 0.397 0.429 0.317 0.446 Sydney 0.360 0.367 0.442 0.336 0.489 Las Vegas 0.510 0.421 0.483 0.390 0.472 The above results suggest that a large proportion of users (close to one half, even for the high threshold value θ = 1) deviate from the prior average.",
                "This reinforces the idea that users are more likely to submit a report when they believe they have something distinctive to add to the current stream of opinions for some feature.",
                "Such conclusions are in total agreement with prior evidence that the distribution of reports often follows bi-modal, U-shaped distributions. 139 5.",
                "MODELLING THE BEHAVIOR OF RATERS To account for the observations described in the previous sections, we propose a model for the behavior of the users when submitting online reviews.",
                "For a given hotel, we make the assumption that the quality experienced by the users is normally distributed around some value vf , which represents the objective quality offered by the hotel on the feature f. The rating submitted by user i on feature f is: ˆri f = δf vi f + (1 − δf ) · sign vi f − ef (i) c + d(vi f , ef (i)|wi f ) (2) where: • vi f is the (unknown) quality actually experienced by the user. vi f is assumed normally distributed around some value vf ; • δf ∈ [0, 1] can be seen as a measure of the bias when reporting feedback.",
                "High values reflect the fact that users rate objectively, without being influenced by prior expectations.",
                "The value of δf may depend on various factors; we fix one value for each feature f; • c is a constant between 1 and 5; • wi f is the weight of feature f in the textual comment of review i, computed according to Eq. (1); • d(vi f , ef (i)|wi f ) is a distance function between the expectation and the observation of user i.",
                "The distance function satisfies the following properties: - d(y, z|w) ≥ 0 for all y, z ∈ [0, 5], w ∈ [0, 1]; - |d(y, z|w)| < |d(z, x|w)| if |y − z| < |z − x|; - |d(y, z|w1)| < |d(y, z|w2)| if w1 < w2; - c + d(vf , ef (i)|wi f ) ∈ [1, 5]; The second term of Eq. (2) encodes the bias of the rating.",
                "The higher the distance between the true observation vi f and the function ef , the higher the bias. 5.1 Model Validation We use the data set of TripAdvisor reviews to validate the behavior model presented above.",
                "We split for convenience the rating values in three ranges: bad (B = {1, 2}), indifferent (I = {3, 4}), and good (G = {5}), and perform the following two tests: • First, we will use our model to predict the ratings that have extremal values.",
                "For every hotel, we take the sequence of reports, and whenever we encounter a rating that is either good or bad (but not indifferent) we try to predict it using Eq. (2) • Second, instead of predicting the value of extremal ratings, we try to classify them as either good or bad.",
                "For every hotel we take the sequence of reports, and for each report (regardless of it value) we classify it as being good or bad However, to perform these tests, we need to estimate the objective value, vf , that is the average of the true quality observations, vi f .",
                "The algorithm we are using is based on the intuition that the amount of conformity rating is minimized.",
                "In other words, the value vf should be such that as often as possible, bad ratings follow expectations above vf and good ratings follow expectations below vf .",
                "Formally, we define the sets: Γ1 = {i|ef (i) < vf and ri f ∈ B}; Γ2 = {i|ef (i) > vf and ri f ∈ G}; that correspond to irregularities where even though the expectation at point i is lower than the delivered value, the rating is poor, and vice versa.",
                "We define vf as the value that minimize these union of the two sets: vf = arg min vf |Γ1 ∪ Γ2| (3) In Eq. (2) we replace vi f by the value vf computed in Eq. (3), and use the following distance function: d(vf , ef (i)|wi f ) = |vf − ef (i)| vf − ef (i) |vf 2 − ef (i)2 | · (1 + 2wi f ); The constant c ∈ I was set to min{max{ef (i), 3}}, 4}.",
                "The values for δf were fixed at {0.7, 0.7, 0.8, 0.7, 0.6} for the features {Overall, Rooms, Service, Cleanliness, Value} respectively.",
                "The weights are computed as described in Section 3.",
                "As a first experiment, we take the sets of extremal ratings {ri f |ri f /∈ I} for each hotel and feature.",
                "For every such rating, ri f , we try to estimate it by computing ˆri f using Eq. (2).",
                "We compare this estimator with the one obtained by simply averaging the ratings over all hotels and features: i.e., ¯rf = j,r j f =0 rj f j,r j f =0 1 ; Table 7 presents the ratio between the root mean square error (RMSE) when using ˆri f and ¯rf to estimate the actual ratings.",
                "In all cases the estimate produced by our model is better than the simple average.",
                "Table 7: Average of RMSE(ˆrf ) RMSE(¯rf ) City O R S C V Boston 0.987 0.849 0.879 0.776 0.913 Sydney 0.927 0.817 0.826 0.720 0.681 Las Vegas 0.952 0.870 0.881 0.947 0.904 As a second experiment, we try to distinguish the sets Bf = {i|ri f ∈ B} and Gf = {i|ri f ∈ G} of bad, respectively good ratings on the feature f. For example, we compute the set Bf using the following classifier (called σ): ri f ∈ Bf (σf (i) = 1) ⇔ ˆri f ≤ 4; Tables 8, 9 and 10 present the Precision(p), Recall(r) and s = 2pr p+r for classifier σ, and compares it with a naive majority classifier, τ, τf (i) = 1 ⇔ |Bf | ≥ |Gf |: We see that recall is always higher for σ and precision is usually slightly worse.",
                "For the s metric σ tends to add a 140 Table 8: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Boston O R S C V p 0.678 0.670 0.573 0.545 0.610 σ r 0.626 0.659 0.619 0.612 0.694 s 0.651 0.665 0.595 0.577 0.609 p 0.684 0.706 0.647 0.611 0.633 τ r 0.597 0.541 0.410 0.383 0.562 s 0.638 0.613 0.502 0.471 0.595 Table 9: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Las Vegas O R S C V p 0.654 0.748 0.592 0.712 0.583 σ r 0.608 0.536 0.791 0.474 0.610 s 0.630 0.624 0.677 0.569 0.596 p 0.685 0.761 0.621 0.748 0.606 τ r 0.542 0.505 0.767 0.445 0.441 s 0.605 0.607 0.670 0.558 0.511 1-20% improvement over τ, much higher in some cases for hotels in Sydney.",
                "This is likely because Sydney reviews are more positive than those of the American cities and cases where the number of bad reviews exceeded the number of good ones are rare.",
                "Replacing the test algorithm with one that plays a 1 with probability equal to the proportion of bad reviews improves its results for this city, but it is still outperformed by around 80%. 6.",
                "SUMMARY OF RESULTS AND CONCLUSION The goal of this paper is to explore the factors that drive a user to submit a particular rating, rather than the incentives that encouraged him to submit a report in the first place.",
                "For that we use two additional sources of information besides the vector of numerical ratings: first we look at the textual comments that accompany the reviews, and second we consider the reports that have been previously submitted by other users.",
                "Using simple natural language processing algorithms, we were able to establish a correlation between the weight of a certain feature in the textual comment accompanying the review, and the noise present in the numerical rating.",
                "Specifically, it seems that users who discuss amply a certain feature are likely to agree on a common rating.",
                "This observation allows the construction of feature-by-feature estimators of quality that have a lower variance, and are hopefully less noisy.",
                "Nevertheless, further evidence is required to support the intuition that ratings corresponding to high weights are expert opinions that deserve to be given higher priority when computing estimates of quality.",
                "Second, we emphasize the dependence of ratings on previous reports.",
                "Previous reports create an expectation of quality which affects the subjective perception of the user.",
                "We validate two facts about the hotel reviews we collected from TripAdvisor: First, the ratings following low expectations (where the expectation is computed as the average of the previous reports) are likely to be higher than the ratings Table 10: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Sydney O R S C V p 0.650 0.463 0.544 0.550 0.580 σ r 0.234 0.378 0.571 0.169 0.592 s 0.343 0.452 0.557 0.259 0.586 p 0.562 0.615 0.600 0.500 0.600 τ r 0.054 0.098 0.101 0.015 0.175 s 0.098 0.168 0.172 0.030 0.271 following high expectations.",
                "Intuitively, the perception of quality (and consequently the rating) depends on how well the actual experience of the user meets her expectation.",
                "Second, we include evidence from the textual comments, and find that when users devote a large fraction of the text to discussing a certain feature, they are likely to motivate a divergent rating (i.e., a rating that does not conform to the prior expectation).",
                "Intuitively, this supports the hypothesis that review forums act as discussion groups where users are keen on presenting and motivating their own opinion.",
                "We have captured the empirical evidence in a behavior model that predicts the ratings submitted by the users.",
                "The final rating depends, as expected, on the true observation, and on the gap between the observation and the expectation.",
                "The gap tends to have a bigger influence when an important fraction of the textual comment is dedicated to discussing a certain feature.",
                "The proposed model was validated on the empirical data and provides better estimates of the ratings actually submitted.",
                "One assumption that we make is about the existence of an objective quality value vf for the feature f. This is rarely true, especially over large spans of time.",
                "Other explanations might account for the correlation of ratings with past reports.",
                "For example, if ef (i) reflects the true value of f at a point in time, the difference in the ratings following high and low expectations can be explained by hotel revenue models that are maximized when the value is modified accordingly.",
                "However, the idea that variation in ratings is not primarily a function of variation in value turns out to be a useful one.",
                "Our approach to approximate this elusive objective value is by no means perfect, but conforms neatly to the idea behind the model.",
                "A natural direction for future work is to examine concrete applications of our results.",
                "Significant improvements of quality estimates are likely to be obtained by incorporating all empirical evidence about rating behavior.",
                "Exactly how different factors affect the decisions of the users is not clear.",
                "The answer might depend on the particular application, context and culture. 7.",
                "REFERENCES [1] A. Admati and P. Pfleiderer.",
                "Noisytalk.com: Broadcasting opinions in a noisy environment.",
                "Working Paper 1670R, Stanford University, 2000. [2] P. B., L. Lee, and S. Vaithyanathan.",
                "Thumbs up? sentiment classification using machine learning techniques.",
                "In Proceedings of the EMNLP-02, the Conference on Empirical Methods in Natural Language Processing, 2002. [3] H. Cui, V. Mittal, and M. Datar.",
                "Comparative 141 Experiments on Sentiment Classification for Online Product Reviews.",
                "In Proceedings of AAAI, 2006. [4] K. Dave, S. Lawrence, and D. Pennock.",
                "Mining the peanut gallery:opinion extraction and semantic classification of product reviews.",
                "In Proceedings of the 12th International Conference on the World Wide Web (WWW03), 2003. [5] C. Dellarocas, N. Awad, and X. Zhang.",
                "Exploring the Value of Online Product Ratings in Revenue Forecasting: The Case of Motion Pictures.",
                "Working paper, 2006. [6] C. Forman, A. Ghose, and B. Wiesenfeld.",
                "A Multi-Level Examination of the Impact of Social Identities on Economic Transactions in Electronic Markets.",
                "Available at SSRN: http://ssrn.com/abstract=918978, July 2006. [7] A. Ghose, P. Ipeirotis, and A. Sundararajan.",
                "Reputation Premiums in Electronic Peer-to-Peer Markets: Analyzing Textual Feedback and Network Structure.",
                "In Third Workshop on Economics of Peer-to-Peer Systems, (P2PECON), 2005. [8] A. Ghose, P. Ipeirotis, and A. Sundararajan.",
                "The Dimensions of Reputation in electronic Markets.",
                "Working Paper CeDER-06-02, New York University, 2006. [9] A. Harmon.",
                "Amazon Glitch Unmasks War of Reviewers.",
                "The New York Times, February 14, 2004. [10] D. Houser and J. Wooders.",
                "Reputation in Auctions: Theory and Evidence from eBay.",
                "Journal of Economics and Management Strategy, 15:353-369, 2006. [11] M. Hu and B. Liu.",
                "Mining and summarizing customer reviews.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD04), 2004. [12] N. Hu, P. Pavlou, and J. Zhang.",
                "Can Online Reviews Reveal a Products True Quality?",
                "In Proceedings of ACM Conference on Electronic Commerce (EC 06), 2006. [13] K. Kalyanam and S. McIntyre.",
                "Return on reputation in online auction market.",
                "Working Paper 02/03-10-WP, Leavey School of Business, Santa Clara University., 2001. [14] L. Khopkar and P. Resnick.",
                "Self-Selection, Slipping, Salvaging, Slacking, and Stoning: the Impacts of Negative Feedback at eBay.",
                "In Proceedings of ACM Conference on Electronic Commerce (EC 05), 2005. [15] M. Melnik and J. Alm.",
                "Does a sellers reputation matter? evidence from ebay auctions.",
                "Journal of Industrial Economics, 50(3):337-350, 2002. [16] R. Olshavsky and J. Miller.",
                "Consumer Expectations, Product Performance and Perceived Product Quality.",
                "Journal of Marketing Research, 9:19-21, February 1972. [17] A. Parasuraman, V. Zeithaml, and L. Berry.",
                "A Conceptual Model of Service Quality and Its Implications for Future Research.",
                "Journal of Marketing, 49:41-50, 1985. [18] A. Parasuraman, V. Zeithaml, and L. Berry.",
                "SERVQUAL: A Multiple-Item Scale for Measuring Consumer Perceptions of Service Quality.",
                "Journal of Retailing, 64:12-40, 1988. [19] P. Pavlou and A. Dimoka.",
                "The Nature and Role of Feedback Text Comments in Online Marketplaces: Implications for Trust Building, Price Premiums, and Seller Differentiation.",
                "Information Systems Research, 17(4):392-414, 2006. [20] A. Popescu and O. Etzioni.",
                "Extracting product features and opinions from reviews.",
                "In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, 2005. [21] R. Teas.",
                "Expectations, Performance Evaluation, and Consumers Perceptions of Quality.",
                "Journal of Marketing, 57:18-34, 1993. [22] E. White.",
                "Chatting a Singer Up the Pop Charts.",
                "The Wall Street Journal, October 15, 1999.",
                "APPENDIX A.",
                "LIST OF WORDS, LR, ASSOCIATED TO THE FEATURE ROOMS All words serve as prefixes: room, space, interior, decor, ambiance, atmosphere, comfort, bath, toilet, bed, building, wall, window, private, temperature, sheet, linen, pillow, hot, water, cold, water, shower, lobby, furniture, carpet, air, condition, mattress, layout, design, mirror, ceiling, lighting, lamp, sofa, chair, dresser, wardrobe, closet 142"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "brag-and-moan model": {
            "translated_key": "Modelo Brag-and-Moan",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Understanding User Behavior in Online Feedback Reporting Arjun Talwar Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland arjun@math.stanford.edu Radu Jurca Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland radu.jurca@epfl.ch Boi Faltings Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland boi.faltings@epfl.ch ABSTRACT Online reviews have become increasingly popular as a way to judge the quality of various products and services.",
                "Previous work has demonstrated that contradictory reporting and underlying user biases make judging the true worth of a service difficult.",
                "In this paper, we investigate underlying factors that influence user behavior when reporting feedback.",
                "We look at two sources of information besides numerical ratings: linguistic evidence from the textual comment accompanying a review, and patterns in the time sequence of reports.",
                "We first show that groups of users who amply discuss a certain feature are more likely to agree on a common rating for that feature.",
                "Second, we show that a users rating partly reflects the difference between true quality and prior expectation of quality as inferred from previous reviews.",
                "Both give us a less noisy way to produce rating estimates and reveal the reasons behind user bias.",
                "Our hypotheses were validated by statistical evidence from hotel reviews on the TripAdvisor website.",
                "Categories and Subject Descriptors J.4 [Social and Behavioral Sciences]: Economics General Terms Economics, Experimentation, Reliability 1.",
                "MOTIVATIONS The spread of the internet has made it possible for online feedback forums (or reputation mechanisms) to become an important channel for Word-of-mouth regarding products, services or other types of commercial interactions.",
                "Numerous empirical studies [10, 15, 13, 5] show that buyers seriously consider online feedback when making purchasing decisions, and are willing to pay reputation premiums for products or services that have a good reputation.",
                "Recent analysis, however, raises important questions regarding the ability of existing forums to reflect the real quality of a product.",
                "In the absence of clear incentives, users with a moderate outlook will not bother to voice their opinions, which leads to an unrepresentative sample of reviews.",
                "For example, [12, 1] show that Amazon1 ratings of books or CDs follow with great probability bi-modal, U-shaped distributions where most of the ratings are either very good, or very bad.",
                "Controlled experiments, on the other hand, reveal opinions on the same items that are normally distributed.",
                "Under these circumstances, using the arithmetic mean to predict quality (as most forums actually do) gives the typical user an estimator with high variance that is often false.",
                "Improving the way we aggregate the information available from online reviews requires a deep understanding of the underlying factors that bias the rating behavior of users.",
                "Hu et al. [12] propose the <br>brag-and-moan model</br> where users rate only if their utility of the product (drawn from a normal distribution) falls outside a median interval.",
                "The authors conclude that the model explains the empirical distribution of reports, and offers insights into smarter ways of estimating the true quality of the product.",
                "In the present paper we extend this line of research, and attempt to explain further facts about the behavior of users when reporting online feedback.",
                "Using actual hotel reviews from the TripAdvisor2 website, we consider two additional sources of information besides the basic numerical ratings submitted by users.",
                "The first is simple linguistic evidence from the textual review that usually accompanies the numerical ratings.",
                "We use text-mining techniques similar to [7] and [3], however, we are only interested in identifying what aspects of the service the user is discussing, without computing the semantic orientation of the text.",
                "We find that users who comment more on the same feature are more likely to agree on a common numerical rating for that particular feature.",
                "Intuitively, lengthy comments reveal the importance of the feature to the user.",
                "Since people tend to be more knowledgeable in the aspects they consider important, users who discuss a given feature in more details might be assumed to have more authority in evaluating that feature.",
                "Second we investigate the relationship between a review 1 http://www.amazon.com 2 http://www.tripadvisor.com/ 134 Figure 1: The TripAdvisor page displaying reviews for a popular Boston hotel.",
                "Name of hotel and advertisements were deliberatively erased. and the reviews that preceded it.",
                "A perusal of online reviews shows that ratings are often part of discussion threads, where one post is not necessarily independent of other posts.",
                "One may see, for example, users who make an effort to contradict, or vehemently agree with, the remarks of previous users.",
                "By analyzing the time sequence of reports, we conclude that past reviews influence the future reports, as they create some prior expectation regarding the quality of service.",
                "The subjective perception of the user is influenced by the gap between the prior expectation and the actual performance of the service [17, 18, 16, 21] which will later reflect in the users rating.",
                "We propose a model that captures the dependence of ratings on prior expectations, and validate it using the empirical data we collected.",
                "Both results can be used to improve the way reputation mechanisms aggregate the information from individual reviews.",
                "Our first result can be used to determine a featureby-feature estimate of quality, where for each feature, a different subset of reviews (i.e., those with lengthy comments of that feature) is considered.",
                "The second leads to an algorithm that outputs a more precise estimate of the real quality. 2.",
                "THE DATA SET We use in this paper real hotel reviews collected from the popular travel site TripAdvisor.",
                "TripAdvisor indexes hotels from cities across the world, along with reviews written by travelers.",
                "Users can search the site by giving the hotels name and location (optional).",
                "The reviews for a given hotel are displayed as a list (ordered from the most recent to the oldest), with 5 reviews per page.",
                "The reviews contain: • information about the author of the review (e.g., dates of stay, username of the reviewer, location of the reviewer); • the overall rating (from 1, lowest, to 5, highest); • a textual review containing a title for the review, free comments, and the main things the reviewer liked and disliked; • numerical ratings (from 1, lowest, to 5, highest) for different features (e.g., cleanliness, service, location, etc.)",
                "Below the name of the hotel, TripAdvisor displays the address of the hotel, general information (number of rooms, number of stars, short description, etc), the average overall rating, the TripAdvisor ranking, and an average rating for each feature.",
                "Figure 1 shows the page for a popular Boston hotel whose name (along with advertisements) was explicitly erased.",
                "We selected three cities for this study: Boston, Sydney and Las Vegas.",
                "For each city we considered all hotels that had at least 10 reviews, and recorded all reviews.",
                "Table 1 presents the number of hotels considered in each city, the total number of reviews recorded for each city, and the distribution of hotels with respect to the star-rating (as available on the TripAdvisor site).",
                "Note that not all hotels have a star-rating.",
                "Table 1: A summary of the data set.",
                "City # Reviews # Hotels # of Hotels with 1,2,3,4 & 5 stars Boston 3993 58 1+3+17+15+2 Sydney 1371 47 0+0+9+13+10 Las Vegas 5593 40 0+3+10+9+6 For each review we recorded the overall rating, the textual review (title and body of the review) and the numerical rating on 7 features: Rooms(R), Service(S), Cleanliness(C), Value(V), Food(F), Location(L) and Noise(N).",
                "TripAdvisor does not require users to submit anything other than the overall rating, hence a typical review rates few additional features, regardless of the discussion in the textual comment.",
                "Only the features Rooms(R), Service(S), Cleanliness(C) and Value(V) are rated by a significant number of users.",
                "However, we also selected the features Food(F), Location(L) and Noise(N) because they are referred to in a significant number of textual comments.",
                "For each feature we record the numerical rating given by the user, or 0 when the rating is missing.",
                "The typical length of the textual comment amounts to approximately 200 words.",
                "All data was collected by crawling the TripAdvisor site in September 2006. 2.1 Formal notation We will formally refer to a review by a tuple (r, T) where: • r = (rf ) is a vector containing the ratings rf ∈ {0, 1, . . . 5} for the features f ∈ F = {O, R, S, C, V, F, L, N}; note that the overall rating, rO, is abusively recorded as the rating for the feature Overall(O); • T is the textual comment that accompanies the review. 135 Reviews are indexed according to the variable i, such that (ri , Ti ) is the ith review in our database.",
                "Since we dont record the username of the reviewer, we will also say that the ith review in our data set was submitted by user i.",
                "When we need to consider only the reviews of a given hotel, h, we will use (ri(h) , Ti(h) ) to denote the ith review about the hotel h. 3.",
                "EVIDENCE FROM TEXTUAL COMMENTS The free textual comments associated to online reviews are a valuable source of information for understanding the reasons behind the numerical ratings left by the reviewers.",
                "The text may, for example, reveal concrete examples of aspects that the user liked or disliked, thus justifying some of the high, respectively low ratings for certain features.",
                "The text may also offer guidelines for understanding the preferences of the reviewer, and the weights of different features when computing an overall rating.",
                "The problem, however, is that free textual comments are difficult to read.",
                "Users are required to scroll through many reviews and read mostly repetitive information.",
                "Significant improvements would be obtained if the reviews were automatically interpreted and aggregated.",
                "Unfortunately, this seems a difficult task for computers since human users often use witty language, abbreviations, cultural specific phrases, and the figurative style.",
                "Nevertheless, several important results use the textual comments of online reviews in an automated way.",
                "Using well established natural language techniques, reviews or parts of reviews can be classified as having a positive or negative semantic orientation.",
                "Pang et al. [2] classify movie reviews into positive/negative by training three different classifiers (Naive Bayes, Maximum Entropy and SVM) using classification features based on unigrams, bigrams or part-of-speech tags.",
                "Dave et al. [4] analyze reviews from CNet and Amazon, and surprisingly show that classification features based on unigrams or bigrams perform better than higher-order n-grams.",
                "This result is challenged by Cui et al. [3] who look at large collections of reviews crawled from the web.",
                "They show that the size of the data set is important, and that bigger training sets allow classifiers to successfully use more complex classification features based on n-grams.",
                "Hu and Liu [11] also crawl the web for product reviews and automatically identify product attributes that have been discussed by reviewers.",
                "They use Wordnet to compute the semantic orientation of product evaluations and summarize user reviews by extracting positive and negative evaluations of different product features.",
                "Popescu and Etzioni [20] analyze a similar setting, but use search engine hit-counts to identify product attributes; the semantic orientation is assigned through the relaxation labeling technique.",
                "Ghose et al. [7, 8] analyze seller reviews from the Amazon secondary market to identify the different dimensions (e.g., delivery, packaging, customer support, etc.) of reputation.",
                "They parse the text, and tag the part-of-speech for each word.",
                "Frequent nouns, noun phrases and verbal phrases are identified as dimensions of reputation, while the corresponding modifiers (i.e., adjectives and adverbs) are used to derive numerical scores for each dimension.",
                "The enhanced reputation measure correlates better with the pricing information observed in the market.",
                "Pavlou and Dimoka [19] analyze eBay reviews and find that textual comments have an important impact on reputation premiums.",
                "Our approach is similar to the previously mentioned works, in the sense that we identify the aspects (i.e., hotel features) discussed by the users in the textual reviews.",
                "However, we do not compute the semantic orientation of the text, nor attempt to infer missing ratings.",
                "We define the weight, wi f , of feature f ∈ F in the text Ti associated with the review (ri , Ti ), as the fraction of Ti dedicated to discussing aspects (both positive and negative) related to feature f. We propose an elementary method to approximate the values of these weights.",
                "For each feature we manually construct the word list Lf containing approximately 50 words that are most commonly associated to the feature f. The initial words were selected from reading some of the reviews, and seeing what words coincide with discussion of which features.",
                "The list was then extended by adding all thesaurus entries that were related to the initial words.",
                "Finally, we brainstormed for missing words that would normally be associated with each of the features.",
                "Let Lf ∩Ti be the list of terms common to both Lf and Ti.",
                "Each term of Lf is counted the number of times it appears in Ti , with two exceptions: • in cases where the user submits a title to the review, we account for the title text by appending it three times to the review text Ti .",
                "The intuitive assumption is that the users opinion is more strongly reflected in the title, rather than in the body of the review.",
                "For example, many reviews are accurately summarized by titles such as Excellent service, terrible location or Bad value for money; • certain words that occur only once in the text are counted multiple times if their relevance to that feature is particularly strong.",
                "These were root words for each feature (e.g., staff is a root word for the feature Service), and were weighted either 2 or 3.",
                "Each feature was assigned up to 3 such root words, so almost all words are counted only once.",
                "The list of words for the feature Rooms is given for reference in Appendix A.",
                "The weight wi f is computed as: wi f = |Lf ∩ Ti| f∈F |Lf ∩ Ti| (1) where |Lf ∩Ti | is the number of terms common to Lf and Ti .",
                "The weight for the feature Overall was set to min{ |T i | 5000 , 1} where |Ti | is the number of character in Ti .",
                "The following is a TripAdvisor review for a Boston hotel (the name of the hotel is omitted): Ill start by saying that Im more of a Holiday Inn person than a *** type.",
                "So I get frustrated when I pay double the room rate and get half the amenities that Id get at a Hampton Inn or Holiday Inn.",
                "The location was definitely the main asset of this place.",
                "It was only a few blocks from the Hynes Center subway stop and it was easy to walk to some good restaurants in the Back Bay area.",
                "Boylston isnt far off at all.",
                "So I had no trouble with foregoing a rental car and taking the subway from the airport to the hotel and using the subway for any other travel.",
                "Otherwise, they make you pay for anything and everything. 136 And when youve already dropped $215/night on the room, that gets frustrating.The room itself was decent, about what I would expect.",
                "Staff was also average, not bad and not excellent.",
                "Again, I think youre paying for location and the ability to walk to a lot of good stuff.",
                "But I think next time Ill stay in Brookline, get more amenities, and use the subway a bit more.",
                "This numerical ratings associated to this review are rO = 3, rR = 3, rS = 3, rC = 4, rV = 2 for features Overall(O), Rooms(R), Service(S), Cleanliness(C) and Value(V) respectively.",
                "The ratings for the features Food(F), Location(L) and Noise(N) are absent (i.e., rF = rL = rN = 0).",
                "The weights wf are computed from the following lists of common terms: LR ∩ T ={room}; wR = 0.066 LS ∩ T ={3 * Staff, amenities}; wS = 0.267 LC ∩ T = ∅; wC = 0 LV ∩ T ={$, rate}; wV = 0.133 LF ∩ T ={restaurant}; wF = 0.067 LL ∩ T ={2 * center, 2 * walk, 2 * location, area}; wL = 0.467 LN ∩ T = ∅; wN = 0 The root words Staff and Center were tripled and doubled respectively.",
                "The overall weight of the textual review is wO = 0.197.",
                "These values account reasonably well for the weights of different features in the discussion of the reviewer.",
                "One point to note is that some terms in the lists Lf possess an inherent semantic orientation.",
                "For example the word grime (belonging to the list LC ) would be used most often to assert the presence, and not the absence of grime.",
                "This is unavoidable, but care was taken to ensure words from both sides of the spectrum were used.",
                "For this reason, some lists such as LR contain only nouns of objects that one would typically describe in a room (see Appendix A).",
                "The goal of this section is to analyse the influence of the weights wi f on the numerical ratings ri f .",
                "Intuitively, users who spent a lot of their time discussing a feature f (i.e., wi f is high) had something to say about their experience with regard to this feature.",
                "Obviously, feature f is important for user i.",
                "Since people tend to be more knowledgeable in the aspects they consider important, our hypothesis is that the ratings ri f (corresponding to high weights wi f ) constitute a subset of expert ratings for feature f. Figure 2 plots the distribution of the rates r i(h) C with respect to the weights w i(h) C for the cleanliness of a Las Vegas hotel, h. Here, the high ratings are restricted to the reviews that discuss little the cleanliness.",
                "Whenever cleanliness appears in the discussion, the ratings are low.",
                "Many hotels exhibit similar rating patterns for various features.",
                "Ratings corresponding to low weights span the whole spectrum from 1 to 5, while the ratings corresponding to high weights are more grouped together (either around good or bad ratings).",
                "We therefore make the following hypothesis: Hypothesis 1.",
                "The ratings ri f corresponding to the reviews where wi f is high, are more similar to each other than to the overall collection of ratings.",
                "To test the hypothesis, we take the entire set of reviews, and feature by feature, we compute the standard deviation of the ratings with high weights, and the standard deviation of the entire set of ratings.",
                "High weights were defined as those belonging to the upper 20% of the weight range for the corresponding feature.",
                "If Hypothesis 1 were true, the standard deviation of all ratings should be higher than the standard deviation of the ratings with high weights. 0 1 2 3 4 5 6 0 0.1 0.2 0.3 0.4 0.5 0.6 Rating Weight Figure 2: The distribution of ratings against the weight of the cleanliness feature.",
                "We use a standard T-test to measure the significance of the results.",
                "City by city and feature by feature, Table 2 presents the average standard deviation of all ratings, and the average standard deviation of ratings with high weights.",
                "Indeed, the ratings with high weights have lower standard deviation, and the results are significant at the standard 0.05 significance threshold (although for certain cities taken independently there doesnt seem to be a significant difference, the results are significant for the entire data set).",
                "Please note that only the features O,R,S,C and V were considered, since for the others (F, L, and N) we didnt have enough ratings.",
                "Table 2: Average standard deviation for all ratings, and average standard deviation for ratings with high weights.",
                "In square brackets, the corresponding p-values for a positive difference between the two.",
                "City O R S C V all 1.189 0.998 1.144 0.935 1.123 Boston high 0.948 0.778 0.954 0.767 0.891 p-val [0.000] [0.004] [0.045] [0.080] [0.009] all 1.040 0.832 1.101 0.847 0.963 Sydney high 0.801 0.618 0.691 0.690 0.798 p-val [0.012] [0.023] [0.000] [0.377] [0.037] all 1.272 1.142 1.184 1.119 1.242 Vegas high 1.072 0.752 1.169 0.907 1.003 p-val [0.0185] [0.001] [0.918] [0.120] [0.126] Hypothesis 1 not only provides some basic understanding regarding the rating behavior of online users, it also suggests some ways of computing better quality estimates.",
                "We can, for example, construct a feature-by-feature quality estimate with much lower variance: for each feature we take the subset of reviews that amply discuss that feature, and output as a quality estimate the average rating for this subset.",
                "Initial experiments suggest that the average feature-by-feature ratings computed in this way are different from the average ratings computed on the whole data set.",
                "Given that, indeed, high weights are indicators of expert opinions, the estimates obtained in this way are more accurate than the current ones.",
                "Nevertheless, the validation of this underlying assumption requires further controlled experiments. 137 4.",
                "THE INFLUENCE OF PAST RATINGS Two important assumptions are generally made about reviews submitted to online forums.",
                "The first is that ratings truthfully reflect the quality observed by the users; the second is that reviews are independent from one another.",
                "While anecdotal evidence [9, 22] challenges the first assumption3 , in this section, we address the second.",
                "A perusal of online reviews shows that reviews are often part of discussion threads, where users make an effort to contradict, or vehemently agree with the remarks of previous users.",
                "Consider, for example, the following review: I dont understand the negative reviews... the hotel was a little dark, but that was the style.",
                "It was very artsy.",
                "Yes it was close to the freeway, but in my opinion the sound of an occasional loud car is better than hearing the ding ding of slot machines all night!",
                "The staff on-hand is FABULOUS.",
                "The waitresses are great (and *** does not deserve the bad review she got, she was 100% attentive to us! ), the bartenders are friendly and professional at the same time...",
                "Here, the user was disturbed by previous negative reports, addressed these concerns, and set about trying to correct them.",
                "Not surprisingly, his ratings were considerably higher than the average ratings up to this point.",
                "It seems that TripAdvisor users regularly read the reports submitted by previous users before booking a hotel, or before writing a review.",
                "Past reviews create some prior expectation regarding the quality of service, and this expectation has an influence on the submitted review.",
                "We believe this observation holds for most online forums.",
                "The subjective perception of quality is directly proportional to how well the actual experience meets the prior expectation, a fact confirmed by an important line of econometric and marketing research [17, 18, 16, 21].",
                "The correlation between the reviews has also been confirmed by recent research on the dynamics of online review forums [6]. 4.1 Prior Expectations We define the prior expectation of user i regarding the feature f, as the average of the previously available ratings on the feature f4 : ef (i) = j<i,r j f =0 rj f j<i,r j f =0 1 As a first hypothesis, we assert that the rating ri f is a function of the prior expectation ef (i): Hypothesis 2.",
                "For a given hotel and feature, given the reviews i and j such that ef (i) is high and ef (j) is low, the rating rj f exceeds the rating ri f .",
                "We define high and low expectations as those that are above, respectively below a certain cutoff value θ.",
                "The set of reviews preceded by high, respectively low expectations 3 part of Amazon reviews were recognized as strategic posts by book authors or competitors 4 if no previous ratings were assigned for feature f, ef (i) is assigned a default value of 4.",
                "Table 3: Average ratings for reviews preceded by low (first value in the cell) and high (second value in the cell) expectations.",
                "The P-values for a positive difference are given square brackets.",
                "City O R S C V 3.953 4.045 3.985 4.252 3.946 Boston 3.364 3.590 3.485 3.641 3.242 [0.011] [0.028] [0.0086] [0.0168] [0.0034] 4.284 4.358 4.064 4.530 4.428 Sydney 3.756 3.537 3.436 3.918 3.495 [0.000] [0.000] [0.035] [0.009] [0.000] 3.494 3.674 3.713 3.689 3.580 Las Vegas 3.140 3.530 2.952 3.530 3.351 [0.190] [0.529] [0.007] [0.529] [0.253] are defined as follows: Rhigh f = {ri f |ef (i) > θ} Rlow f = {ri f |ef (i) < θ} These sets are specific for each (hotel, feature) pair, and in our experiments we took θ = 4.",
                "This rather high value is close to the average rating across all features across all hotels, and is justified by the fact that our data set contains mostly high quality hotels.",
                "For each city, we take all hotels and compute the average ratings in the sets Rhigh f and Rlow f (see Table 3).",
                "The average rating amongst reviews following low prior expectations is significantly higher than the average rating following high expectations.",
                "As further evidence, we consider all hotels for which the function eV (i) (the expectation for the feature Value) has a high value (greater than 4) for some i, and a low value (less than 4) for some other i.",
                "Intuitively, these are the hotels for which there is a minimal degree of variation in the timely sequence of reviews: i.e., the cumulative average of ratings was at some point high and afterwards became low, or vice-versa.",
                "Such variations are observed for about half of all hotels in each city.",
                "Figure 3 plots the median (across considered hotels) rating, rV , when ef (i) is not more than x but greater than x − 0.5. 2.5 3 3.5 4 4.5 5 2.5 3 3.5 4 4.5 5 Medianofrating expectation Boston Sydney Vegas Figure 3: The ratings tend to decrease as the expectation increases. 138 There are two ways to interpret the function ef (i): • The expected value for feature f obtained by user i before his experience with the service, acquired by reading reports submitted by past users.",
                "In this case, an overly high value for ef (i) would drive the user to submit a negative report (or vice versa), stemming from the difference between the actual value of the service, and the inflated expectation of this value acquired before his experience. • The expected value of feature f for all subsequent visitors of the site, if user i were not to submit a report.",
                "In this case, the motivation for a negative report following an overly high value of ef is different: user i seeks to correct the expectation of future visitors to the site.",
                "Unlike the interpretation above, this does not require the user to derive an a priori expectation for the value of f. Note that neither interpretation implies that the average up to report i is inversely related to the rating at report i.",
                "There might exist a measure of influence exerted by past reports that pushes the user behind report i to submit ratings which to some extent conforms with past reports: a low value for ef (i) can influence user i to submit a low rating for feature f because, for example, he fears that submitting a high rating will make him out to be a person with low standards5 .",
                "This, at first, appears to contradict Hypothesis 2.",
                "However, this conformity rating cannot continue indefinitely: once the set of reports project a sufficiently deflated estimate for vf , future reviewers with comparatively positive impressions will seek to correct this misconception. 4.2 Impact of textual comments on quality expectation Further insight into the rating behavior of TripAdvisor users can be obtained by analyzing the relationship between the weights wf and the values ef (i).",
                "In particular, we examine the following hypothesis: Hypothesis 3.",
                "When a large proportion of the text of a review discusses a certain feature, the difference between the rating for that feature and the average rating up to that point tends to be large.",
                "The intuition behind this claim is that when the user is adamant about voicing his opinion regarding a certain feature, his opinion differs from the collective opinion of previous postings.",
                "This relies on the characteristic of reputation systems as feedback forums where a user is interested in projecting his opinion, with particular strength if this opinion differs from what he perceives to be the general opinion.",
                "To test Hypothesis 3 we measure the average absolute difference between the expectation ef (i) and the rating ri f when the weight wi f is high, respectively low.",
                "Weights are classified high or low by comparing them with certain cutoff values: wi f is low if smaller than 0.1, while wi f is high if greater than θf .",
                "Different cutoff values were used for different features: θR = 0.4, θS = 0.4, θC = 0.2, and θV = 0.7.",
                "Cleanliness has a lower cutoff since it is a feature rarely discussed; Value has a high cutoff for the opposite reason.",
                "Results are presented in Table 4. 5 The idea that negative reports can encourage further negative reporting has been suggested before [14] Table 4: Average of |ri f −ef (i)| when weights are high (first value in the cell) and low (second value in the cell) with P-values for the difference in sq. brackets.",
                "City R S C V 1.058 1.208 1.728 1.356 Boston 0.701 0.838 0.760 0.917 [0.022] [0.063] [0.000] [0.218] 1.048 1.351 1.218 1.318 Sydney 0.752 0.759 0.767 0.908 [0.179] [0.009] [0.165] [0.495] 1.184 1.378 1.472 1.642 Las Vegas 0.772 0.834 0.808 1.043 [0.071] [0.020] [0.006] [0.076] This demonstrates that when weights are unusually high, users tend to express an opinion that does not conform to the net average of previous ratings.",
                "As we might expect, for a feature that rarely was a high weight in the discussion, (e.g., cleanliness) the difference is particularly large.",
                "Even though the difference in the feature Value is quite large for Sydney, the P-value is high.",
                "This is because only few reviews discussed value heavily.",
                "The reason could be cultural or because there was less of a reason to discuss this feature. 4.3 Reporting Incentives Previous models suggest that users who are not highly opinionated will not choose to voice their opinions [12].",
                "In this section, we extend this model to account for the influence of expectations.",
                "The motivation for submitting feedback is not only due to extreme opinions, but also to the difference between the current reputation (i.e., the prior expectation of the user) and the actual experience.",
                "Such a rating model produces ratings that most of the time deviate from the current average rating.",
                "The ratings that confirm the prior expectation will rarely be submitted.",
                "We test on our data set the proportion of ratings that attempt to correct the current estimate.",
                "We define a deviant rating as one that deviates from the current expectation by at least some threshold θ, i.e., |ri f − ef (i)| ≥ θ.",
                "For each of the three considered cities, the following tables, show the proportion of deviant ratings for θ = 0.5 and θ = 1.",
                "Table 5: Proportion of deviant ratings with θ = 0.5 City O R S C V Boston 0.696 0.619 0.676 0.604 0.684 Sydney 0.645 0.615 0.672 0.614 0.675 Las Vegas 0.721 0.641 0.694 0.662 0.724 Table 6: Proportion of deviant ratings with θ = 1 City O R S C V Boston 0.420 0.397 0.429 0.317 0.446 Sydney 0.360 0.367 0.442 0.336 0.489 Las Vegas 0.510 0.421 0.483 0.390 0.472 The above results suggest that a large proportion of users (close to one half, even for the high threshold value θ = 1) deviate from the prior average.",
                "This reinforces the idea that users are more likely to submit a report when they believe they have something distinctive to add to the current stream of opinions for some feature.",
                "Such conclusions are in total agreement with prior evidence that the distribution of reports often follows bi-modal, U-shaped distributions. 139 5.",
                "MODELLING THE BEHAVIOR OF RATERS To account for the observations described in the previous sections, we propose a model for the behavior of the users when submitting online reviews.",
                "For a given hotel, we make the assumption that the quality experienced by the users is normally distributed around some value vf , which represents the objective quality offered by the hotel on the feature f. The rating submitted by user i on feature f is: ˆri f = δf vi f + (1 − δf ) · sign vi f − ef (i) c + d(vi f , ef (i)|wi f ) (2) where: • vi f is the (unknown) quality actually experienced by the user. vi f is assumed normally distributed around some value vf ; • δf ∈ [0, 1] can be seen as a measure of the bias when reporting feedback.",
                "High values reflect the fact that users rate objectively, without being influenced by prior expectations.",
                "The value of δf may depend on various factors; we fix one value for each feature f; • c is a constant between 1 and 5; • wi f is the weight of feature f in the textual comment of review i, computed according to Eq. (1); • d(vi f , ef (i)|wi f ) is a distance function between the expectation and the observation of user i.",
                "The distance function satisfies the following properties: - d(y, z|w) ≥ 0 for all y, z ∈ [0, 5], w ∈ [0, 1]; - |d(y, z|w)| < |d(z, x|w)| if |y − z| < |z − x|; - |d(y, z|w1)| < |d(y, z|w2)| if w1 < w2; - c + d(vf , ef (i)|wi f ) ∈ [1, 5]; The second term of Eq. (2) encodes the bias of the rating.",
                "The higher the distance between the true observation vi f and the function ef , the higher the bias. 5.1 Model Validation We use the data set of TripAdvisor reviews to validate the behavior model presented above.",
                "We split for convenience the rating values in three ranges: bad (B = {1, 2}), indifferent (I = {3, 4}), and good (G = {5}), and perform the following two tests: • First, we will use our model to predict the ratings that have extremal values.",
                "For every hotel, we take the sequence of reports, and whenever we encounter a rating that is either good or bad (but not indifferent) we try to predict it using Eq. (2) • Second, instead of predicting the value of extremal ratings, we try to classify them as either good or bad.",
                "For every hotel we take the sequence of reports, and for each report (regardless of it value) we classify it as being good or bad However, to perform these tests, we need to estimate the objective value, vf , that is the average of the true quality observations, vi f .",
                "The algorithm we are using is based on the intuition that the amount of conformity rating is minimized.",
                "In other words, the value vf should be such that as often as possible, bad ratings follow expectations above vf and good ratings follow expectations below vf .",
                "Formally, we define the sets: Γ1 = {i|ef (i) < vf and ri f ∈ B}; Γ2 = {i|ef (i) > vf and ri f ∈ G}; that correspond to irregularities where even though the expectation at point i is lower than the delivered value, the rating is poor, and vice versa.",
                "We define vf as the value that minimize these union of the two sets: vf = arg min vf |Γ1 ∪ Γ2| (3) In Eq. (2) we replace vi f by the value vf computed in Eq. (3), and use the following distance function: d(vf , ef (i)|wi f ) = |vf − ef (i)| vf − ef (i) |vf 2 − ef (i)2 | · (1 + 2wi f ); The constant c ∈ I was set to min{max{ef (i), 3}}, 4}.",
                "The values for δf were fixed at {0.7, 0.7, 0.8, 0.7, 0.6} for the features {Overall, Rooms, Service, Cleanliness, Value} respectively.",
                "The weights are computed as described in Section 3.",
                "As a first experiment, we take the sets of extremal ratings {ri f |ri f /∈ I} for each hotel and feature.",
                "For every such rating, ri f , we try to estimate it by computing ˆri f using Eq. (2).",
                "We compare this estimator with the one obtained by simply averaging the ratings over all hotels and features: i.e., ¯rf = j,r j f =0 rj f j,r j f =0 1 ; Table 7 presents the ratio between the root mean square error (RMSE) when using ˆri f and ¯rf to estimate the actual ratings.",
                "In all cases the estimate produced by our model is better than the simple average.",
                "Table 7: Average of RMSE(ˆrf ) RMSE(¯rf ) City O R S C V Boston 0.987 0.849 0.879 0.776 0.913 Sydney 0.927 0.817 0.826 0.720 0.681 Las Vegas 0.952 0.870 0.881 0.947 0.904 As a second experiment, we try to distinguish the sets Bf = {i|ri f ∈ B} and Gf = {i|ri f ∈ G} of bad, respectively good ratings on the feature f. For example, we compute the set Bf using the following classifier (called σ): ri f ∈ Bf (σf (i) = 1) ⇔ ˆri f ≤ 4; Tables 8, 9 and 10 present the Precision(p), Recall(r) and s = 2pr p+r for classifier σ, and compares it with a naive majority classifier, τ, τf (i) = 1 ⇔ |Bf | ≥ |Gf |: We see that recall is always higher for σ and precision is usually slightly worse.",
                "For the s metric σ tends to add a 140 Table 8: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Boston O R S C V p 0.678 0.670 0.573 0.545 0.610 σ r 0.626 0.659 0.619 0.612 0.694 s 0.651 0.665 0.595 0.577 0.609 p 0.684 0.706 0.647 0.611 0.633 τ r 0.597 0.541 0.410 0.383 0.562 s 0.638 0.613 0.502 0.471 0.595 Table 9: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Las Vegas O R S C V p 0.654 0.748 0.592 0.712 0.583 σ r 0.608 0.536 0.791 0.474 0.610 s 0.630 0.624 0.677 0.569 0.596 p 0.685 0.761 0.621 0.748 0.606 τ r 0.542 0.505 0.767 0.445 0.441 s 0.605 0.607 0.670 0.558 0.511 1-20% improvement over τ, much higher in some cases for hotels in Sydney.",
                "This is likely because Sydney reviews are more positive than those of the American cities and cases where the number of bad reviews exceeded the number of good ones are rare.",
                "Replacing the test algorithm with one that plays a 1 with probability equal to the proportion of bad reviews improves its results for this city, but it is still outperformed by around 80%. 6.",
                "SUMMARY OF RESULTS AND CONCLUSION The goal of this paper is to explore the factors that drive a user to submit a particular rating, rather than the incentives that encouraged him to submit a report in the first place.",
                "For that we use two additional sources of information besides the vector of numerical ratings: first we look at the textual comments that accompany the reviews, and second we consider the reports that have been previously submitted by other users.",
                "Using simple natural language processing algorithms, we were able to establish a correlation between the weight of a certain feature in the textual comment accompanying the review, and the noise present in the numerical rating.",
                "Specifically, it seems that users who discuss amply a certain feature are likely to agree on a common rating.",
                "This observation allows the construction of feature-by-feature estimators of quality that have a lower variance, and are hopefully less noisy.",
                "Nevertheless, further evidence is required to support the intuition that ratings corresponding to high weights are expert opinions that deserve to be given higher priority when computing estimates of quality.",
                "Second, we emphasize the dependence of ratings on previous reports.",
                "Previous reports create an expectation of quality which affects the subjective perception of the user.",
                "We validate two facts about the hotel reviews we collected from TripAdvisor: First, the ratings following low expectations (where the expectation is computed as the average of the previous reports) are likely to be higher than the ratings Table 10: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Sydney O R S C V p 0.650 0.463 0.544 0.550 0.580 σ r 0.234 0.378 0.571 0.169 0.592 s 0.343 0.452 0.557 0.259 0.586 p 0.562 0.615 0.600 0.500 0.600 τ r 0.054 0.098 0.101 0.015 0.175 s 0.098 0.168 0.172 0.030 0.271 following high expectations.",
                "Intuitively, the perception of quality (and consequently the rating) depends on how well the actual experience of the user meets her expectation.",
                "Second, we include evidence from the textual comments, and find that when users devote a large fraction of the text to discussing a certain feature, they are likely to motivate a divergent rating (i.e., a rating that does not conform to the prior expectation).",
                "Intuitively, this supports the hypothesis that review forums act as discussion groups where users are keen on presenting and motivating their own opinion.",
                "We have captured the empirical evidence in a behavior model that predicts the ratings submitted by the users.",
                "The final rating depends, as expected, on the true observation, and on the gap between the observation and the expectation.",
                "The gap tends to have a bigger influence when an important fraction of the textual comment is dedicated to discussing a certain feature.",
                "The proposed model was validated on the empirical data and provides better estimates of the ratings actually submitted.",
                "One assumption that we make is about the existence of an objective quality value vf for the feature f. This is rarely true, especially over large spans of time.",
                "Other explanations might account for the correlation of ratings with past reports.",
                "For example, if ef (i) reflects the true value of f at a point in time, the difference in the ratings following high and low expectations can be explained by hotel revenue models that are maximized when the value is modified accordingly.",
                "However, the idea that variation in ratings is not primarily a function of variation in value turns out to be a useful one.",
                "Our approach to approximate this elusive objective value is by no means perfect, but conforms neatly to the idea behind the model.",
                "A natural direction for future work is to examine concrete applications of our results.",
                "Significant improvements of quality estimates are likely to be obtained by incorporating all empirical evidence about rating behavior.",
                "Exactly how different factors affect the decisions of the users is not clear.",
                "The answer might depend on the particular application, context and culture. 7.",
                "REFERENCES [1] A. Admati and P. Pfleiderer.",
                "Noisytalk.com: Broadcasting opinions in a noisy environment.",
                "Working Paper 1670R, Stanford University, 2000. [2] P. B., L. Lee, and S. Vaithyanathan.",
                "Thumbs up? sentiment classification using machine learning techniques.",
                "In Proceedings of the EMNLP-02, the Conference on Empirical Methods in Natural Language Processing, 2002. [3] H. Cui, V. Mittal, and M. Datar.",
                "Comparative 141 Experiments on Sentiment Classification for Online Product Reviews.",
                "In Proceedings of AAAI, 2006. [4] K. Dave, S. Lawrence, and D. Pennock.",
                "Mining the peanut gallery:opinion extraction and semantic classification of product reviews.",
                "In Proceedings of the 12th International Conference on the World Wide Web (WWW03), 2003. [5] C. Dellarocas, N. Awad, and X. Zhang.",
                "Exploring the Value of Online Product Ratings in Revenue Forecasting: The Case of Motion Pictures.",
                "Working paper, 2006. [6] C. Forman, A. Ghose, and B. Wiesenfeld.",
                "A Multi-Level Examination of the Impact of Social Identities on Economic Transactions in Electronic Markets.",
                "Available at SSRN: http://ssrn.com/abstract=918978, July 2006. [7] A. Ghose, P. Ipeirotis, and A. Sundararajan.",
                "Reputation Premiums in Electronic Peer-to-Peer Markets: Analyzing Textual Feedback and Network Structure.",
                "In Third Workshop on Economics of Peer-to-Peer Systems, (P2PECON), 2005. [8] A. Ghose, P. Ipeirotis, and A. Sundararajan.",
                "The Dimensions of Reputation in electronic Markets.",
                "Working Paper CeDER-06-02, New York University, 2006. [9] A. Harmon.",
                "Amazon Glitch Unmasks War of Reviewers.",
                "The New York Times, February 14, 2004. [10] D. Houser and J. Wooders.",
                "Reputation in Auctions: Theory and Evidence from eBay.",
                "Journal of Economics and Management Strategy, 15:353-369, 2006. [11] M. Hu and B. Liu.",
                "Mining and summarizing customer reviews.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD04), 2004. [12] N. Hu, P. Pavlou, and J. Zhang.",
                "Can Online Reviews Reveal a Products True Quality?",
                "In Proceedings of ACM Conference on Electronic Commerce (EC 06), 2006. [13] K. Kalyanam and S. McIntyre.",
                "Return on reputation in online auction market.",
                "Working Paper 02/03-10-WP, Leavey School of Business, Santa Clara University., 2001. [14] L. Khopkar and P. Resnick.",
                "Self-Selection, Slipping, Salvaging, Slacking, and Stoning: the Impacts of Negative Feedback at eBay.",
                "In Proceedings of ACM Conference on Electronic Commerce (EC 05), 2005. [15] M. Melnik and J. Alm.",
                "Does a sellers reputation matter? evidence from ebay auctions.",
                "Journal of Industrial Economics, 50(3):337-350, 2002. [16] R. Olshavsky and J. Miller.",
                "Consumer Expectations, Product Performance and Perceived Product Quality.",
                "Journal of Marketing Research, 9:19-21, February 1972. [17] A. Parasuraman, V. Zeithaml, and L. Berry.",
                "A Conceptual Model of Service Quality and Its Implications for Future Research.",
                "Journal of Marketing, 49:41-50, 1985. [18] A. Parasuraman, V. Zeithaml, and L. Berry.",
                "SERVQUAL: A Multiple-Item Scale for Measuring Consumer Perceptions of Service Quality.",
                "Journal of Retailing, 64:12-40, 1988. [19] P. Pavlou and A. Dimoka.",
                "The Nature and Role of Feedback Text Comments in Online Marketplaces: Implications for Trust Building, Price Premiums, and Seller Differentiation.",
                "Information Systems Research, 17(4):392-414, 2006. [20] A. Popescu and O. Etzioni.",
                "Extracting product features and opinions from reviews.",
                "In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, 2005. [21] R. Teas.",
                "Expectations, Performance Evaluation, and Consumers Perceptions of Quality.",
                "Journal of Marketing, 57:18-34, 1993. [22] E. White.",
                "Chatting a Singer Up the Pop Charts.",
                "The Wall Street Journal, October 15, 1999.",
                "APPENDIX A.",
                "LIST OF WORDS, LR, ASSOCIATED TO THE FEATURE ROOMS All words serve as prefixes: room, space, interior, decor, ambiance, atmosphere, comfort, bath, toilet, bed, building, wall, window, private, temperature, sheet, linen, pillow, hot, water, cold, water, shower, lobby, furniture, carpet, air, condition, mattress, layout, design, mirror, ceiling, lighting, lamp, sofa, chair, dresser, wardrobe, closet 142"
            ],
            "original_annotated_samples": [
                "Hu et al. [12] propose the <br>brag-and-moan model</br> where users rate only if their utility of the product (drawn from a normal distribution) falls outside a median interval."
            ],
            "translated_annotated_samples": [
                "Hu et al. [12] proponen el <br>Modelo Brag-and-Moan</br> donde los usuarios califican solo si su utilidad del producto (extraída de una distribución normal) cae fuera de un intervalo mediano."
            ],
            "translated_text": "La comprensión del comportamiento del usuario en la presentación de comentarios en línea. Trabajos anteriores han demostrado que la información contradictoria y los sesgos subyacentes de los usuarios dificultan juzgar el verdadero valor de un servicio. En este artículo, investigamos los factores subyacentes que influyen en el comportamiento del usuario al informar retroalimentación. Examinamos dos fuentes de información además de las calificaciones numéricas: la evidencia lingüística del comentario textual que acompaña a una reseña y los patrones en la secuencia temporal de los informes. Primero demostramos que los grupos de usuarios que discuten ampliamente una característica específica tienen más probabilidades de ponerse de acuerdo en una calificación común para esa característica. Segundo, demostramos que la calificación de un usuario refleja en parte la diferencia entre la calidad real y la expectativa previa de calidad, tal como se infiere de revisiones anteriores. Ambos nos ofrecen una forma menos ruidosa de producir estimaciones de calificación y revelar las razones detrás del sesgo del usuario. Nuestras hipótesis fueron validadas por evidencia estadística de reseñas de hoteles en el sitio web de TripAdvisor. Categorías y Descriptores de Asignaturas J.4 [Ciencias Sociales y del Comportamiento]: Economía Términos Generales Economía, Experimentación, Confiabilidad 1. MOTIVACIONES La expansión de internet ha hecho posible que los foros de retroalimentación en línea (o mecanismos de reputación) se conviertan en un canal importante para el boca a boca sobre productos, servicios u otros tipos de interacciones comerciales. Numerosos estudios empíricos [10, 15, 13, 5] muestran que los compradores consideran seriamente los comentarios en línea al tomar decisiones de compra, y están dispuestos a pagar primas por reputación por productos o servicios que tienen una buena reputación. Sin embargo, un análisis reciente plantea preguntas importantes sobre la capacidad de los foros existentes para reflejar la verdadera calidad de un producto. En ausencia de incentivos claros, los usuarios con una perspectiva moderada no se molestarán en expresar sus opiniones, lo que conduce a una muestra no representativa de reseñas. Por ejemplo, [12, 1] muestran que las calificaciones de libros o CDs en Amazon1 siguen con gran probabilidad distribuciones bimodales en forma de U, donde la mayoría de las calificaciones son o muy buenas o muy malas. Los experimentos controlados, por otro lado, revelan opiniones sobre los mismos elementos que están distribuidas de forma normal. Bajo estas circunstancias, utilizar la media aritmética para predecir la calidad (como la mayoría de los foros realmente hacen) proporciona al usuario típico un estimador con una alta varianza que a menudo es incorrecto. Mejorar la forma en que agregamos la información disponible de las reseñas en línea requiere un profundo entendimiento de los factores subyacentes que sesgan el comportamiento de calificación de los usuarios. Hu et al. [12] proponen el <br>Modelo Brag-and-Moan</br> donde los usuarios califican solo si su utilidad del producto (extraída de una distribución normal) cae fuera de un intervalo mediano. Los autores concluyen que el modelo explica la distribución empírica de los informes y ofrece ideas sobre formas más inteligentes de estimar la verdadera calidad del producto. En el presente artículo ampliamos esta línea de investigación e intentamos explicar más hechos sobre el comportamiento de los usuarios al informar retroalimentación en línea. Usando reseñas reales de hoteles del sitio web TripAdvisor2, consideramos dos fuentes adicionales de información además de las calificaciones numéricas básicas enviadas por los usuarios. La primera es una evidencia lingüística simple proveniente de la revisión textual que suele acompañar a las calificaciones numéricas. Utilizamos técnicas de minería de texto similares a [7] y [3], sin embargo, solo estamos interesados en identificar qué aspectos del servicio está discutiendo el usuario, sin calcular la orientación semántica del texto. Observamos que los usuarios que comentan más sobre la misma característica tienen más probabilidades de estar de acuerdo en una calificación numérica común para esa característica en particular. Intuitivamente, los comentarios extensos revelan la importancia de la característica para el usuario. Dado que las personas tienden a ser más conocedoras en los aspectos que consideran importantes, se podría asumir que los usuarios que discuten una característica específica en más detalle tienen más autoridad para evaluar esa característica. Segundo investigamos la relación entre una reseña 1 http://www.amazon.com 2 http://www.tripadvisor.com/ 134 Figura 1: La página de TripAdvisor que muestra reseñas de un popular hotel en Boston. El nombre del hotel y los anuncios fueron deliberadamente borrados, al igual que las reseñas que lo precedieron. Un examen de las reseñas en línea muestra que las calificaciones a menudo forman parte de los hilos de discusión, donde una publicación no es necesariamente independiente de otras publicaciones. Uno puede ver, por ejemplo, usuarios que se esfuerzan por contradecir o estar vehementemente de acuerdo con los comentarios de usuarios anteriores. Al analizar la secuencia temporal de informes, concluimos que las revisiones pasadas influyen en los informes futuros, ya que crean cierta expectativa previa sobre la calidad del servicio. La percepción subjetiva del usuario está influenciada por la brecha entre la expectativa previa y el rendimiento real del servicio [17, 18, 16, 21], lo cual se reflejará más tarde en la calificación de los usuarios. Proponemos un modelo que captura la dependencia de las calificaciones en las expectativas previas, y lo validamos utilizando los datos empíricos que recopilamos. Ambos resultados pueden ser utilizados para mejorar la forma en que los mecanismos de reputación agregan la información de las revisiones individuales. Nuestro primer resultado se puede utilizar para determinar una estimación de calidad característica por característica, donde para cada característica se considera un subconjunto diferente de reseñas (es decir, aquellas con comentarios extensos sobre esa característica). El segundo conduce a un algoritmo que produce una estimación más precisa de la calidad real. 2. El conjunto de datos que utilizamos en este artículo son reseñas reales de hoteles recopiladas del popular sitio de viajes TripAdvisor. TripAdvisor indexa hoteles de ciudades de todo el mundo, junto con reseñas escritas por viajeros. Los usuarios pueden buscar en el sitio proporcionando el nombre del hotel y la ubicación (opcional). Las reseñas de un hotel dado se muestran como una lista (ordenadas de la más reciente a la más antigua), con 5 reseñas por página. Las reseñas contienen: • información sobre el autor de la reseña (por ejemplo, fechas de estancia, nombre de usuario del revisor, ubicación del revisor); • la calificación general (de 1, la más baja, a 5, la más alta); • una reseña textual que incluye un título para la reseña, comentarios libres y las principales cosas que al revisor le gustaron y no le gustaron; • calificaciones numéricas (de 1, la más baja, a 5, la más alta) para diferentes características (por ejemplo, limpieza, servicio, ubicación, etc.). Debajo del nombre del hotel, TripAdvisor muestra la dirección del hotel, información general (número de habitaciones, número de estrellas, breve descripción, etc.), la calificación promedio general, el ranking de TripAdvisor y una calificación promedio para cada característica. La Figura 1 muestra la página de un hotel popular en Boston cuyo nombre (junto con los anuncios) fue explícitamente borrado. Seleccionamos tres ciudades para este estudio: Boston, Sídney y Las Vegas. Para cada ciudad consideramos todos los hoteles que tenían al menos 10 reseñas, y registramos todas las reseñas. La Tabla 1 presenta el número de hoteles considerados en cada ciudad, el número total de reseñas registradas para cada ciudad y la distribución de hoteles con respecto a la calificación por estrellas (según está disponible en el sitio de TripAdvisor). Ten en cuenta que no todos los hoteles tienen una clasificación por estrellas. Tabla 1: Un resumen del conjunto de datos. Para cada reseña, registramos la calificación general, la reseña textual (título y cuerpo de la reseña) y la calificación numérica en 7 características: Habitaciones (R), Servicio (S), Limpieza (C), Valor (V), Comida (F), Ubicación (L) y Ruido (N). TripAdvisor no requiere que los usuarios envíen nada más que la calificación general, por lo tanto, una reseña típica evalúa pocas características adicionales, independientemente de la discusión en el comentario textual. Solo las características Habitaciones (R), Servicio (S), Limpieza (C) y Valor (V) son evaluadas por un número significativo de usuarios. Sin embargo, también seleccionamos las características Comida (F), Ubicación (L) y Ruido (N) porque son mencionadas en un número significativo de comentarios textuales. Para cada característica registramos la calificación numérica dada por el usuario, o 0 cuando la calificación está ausente. La longitud típica del comentario textual equivale aproximadamente a 200 palabras. Todos los datos fueron recopilados rastreando el sitio de TripAdvisor en septiembre de 2006. 2.1 Notación formal Nos referiremos formalmente a una reseña mediante una tupla (r, T) donde: • r = (rf ) es un vector que contiene las calificaciones rf ∈ {0, 1, . . . 5} para las características f ∈ F = {O, R, S, C, V, F, L, N}; cabe destacar que la calificación general, rO, se registra de forma abusiva como la calificación para la característica General(O); • T es el comentario textual que acompaña a la reseña. Se indexan 135 reseñas de acuerdo con la variable i, de modo que (ri , Ti ) es la i-ésima reseña en nuestra base de datos. Dado que no registramos el nombre de usuario del revisor, también diremos que la i-ésima reseña en nuestro conjunto de datos fue enviada por el usuario i. Cuando necesitemos considerar solo las reseñas de un hotel dado, h, usaremos (ri(h), Ti(h)) para denotar la i-ésima reseña sobre el hotel h. 3. EVIDENCIA DE COMENTARIOS TEXTUALES Los comentarios textuales gratuitos asociados a las reseñas en línea son una fuente valiosa de información para comprender las razones detrás de las calificaciones numéricas dejadas por los revisores. El texto puede, por ejemplo, revelar ejemplos concretos de aspectos que al usuario le gustaron o no le gustaron, justificando así algunas de las calificaciones altas o bajas respectivamente para ciertas características. El texto también puede ofrecer pautas para comprender las preferencias del revisor y los pesos de las diferentes características al calcular una calificación general. El problema, sin embargo, es que los comentarios textuales libres son difíciles de leer. Los usuarios deben desplazarse a través de muchas reseñas y leer principalmente información repetitiva. Se obtendrían mejoras significativas si las reseñas fueran interpretadas y agregadas automáticamente. Desafortunadamente, esta parece ser una tarea difícil para las computadoras ya que los usuarios humanos a menudo utilizan un lenguaje ingenioso, abreviaturas, frases específicas de la cultura y un estilo figurativo. Sin embargo, varios resultados importantes utilizan los comentarios textuales de las reseñas en línea de forma automatizada. Utilizando técnicas de lenguaje natural bien establecidas, las reseñas o partes de las reseñas pueden ser clasificadas como tener una orientación semántica positiva o negativa. Pang et al. [2] clasifican las críticas de películas en positivas/negativas entrenando tres clasificadores diferentes (Naive Bayes, Máxima Entropía y SVM) utilizando características de clasificación basadas en unigramas, bigramas o etiquetas de partes del discurso. Dave et al. [4] analizan reseñas de CNet y Amazon, y sorprendentemente demuestran que las características de clasificación basadas en unigramas o bigramas funcionan mejor que los n-gramas de orden superior. Este resultado es desafiado por Cui et al. [3], quienes examinan grandes colecciones de reseñas recopiladas de la web. Muestran que el tamaño del conjunto de datos es importante, y que conjuntos de entrenamiento más grandes permiten a los clasificadores utilizar con éxito características de clasificación más complejas basadas en n-gramas. Hu y Liu [11] también rastrean la web en busca de reseñas de productos e identifican automáticamente los atributos de productos que han sido discutidos por los revisores. Utilizan Wordnet para calcular la orientación semántica de las evaluaciones de productos y resumir las reseñas de usuarios extrayendo evaluaciones positivas y negativas de diferentes características del producto. Popescu y Etzioni [20] analizan un escenario similar, pero utilizan el recuento de resultados de búsqueda en motores de búsqueda para identificar atributos de productos; la orientación semántica se asigna a través de la técnica de etiquetado de relajación. Ghose et al. [7, 8] analizan las reseñas de vendedores del mercado secundario de Amazon para identificar las diferentes dimensiones (por ejemplo, entrega, empaque, atención al cliente, etc.) de la reputación. Analizan el texto y etiquetan la parte de la oración de cada palabra. Los sustantivos, frases nominales y frases verbales frecuentes se identifican como dimensiones de la reputación, mientras que los modificadores correspondientes (es decir, adjetivos y adverbios) se utilizan para derivar puntuaciones numéricas para cada dimensión. La medida de reputación mejorada se correlaciona mejor con la información de precios observada en el mercado. Pavlou y Dimoka [19] analizan las reseñas de eBay y encuentran que los comentarios textuales tienen un impacto importante en las primas de reputación. Nuestro enfoque es similar a los trabajos mencionados anteriormente, en el sentido de que identificamos los aspectos (es decir, las características del hotel) discutidos por los usuarios en las reseñas textuales. Sin embargo, no calculamos la orientación semántica del texto, ni intentamos inferir las calificaciones faltantes. Definimos el peso, wi f, de la característica f ∈ F en el texto Ti asociado con la reseña (ri, Ti), como la fracción de Ti dedicada a discutir aspectos (tanto positivos como negativos) relacionados con la característica f. Proponemos un método elemental para aproximar los valores de estos pesos. Para cada característica, construimos manualmente la lista de palabras Lf que contiene aproximadamente 50 palabras que están más comúnmente asociadas a la característica f. Las palabras iniciales fueron seleccionadas al leer algunas de las reseñas y ver qué palabras coinciden con la discusión de qué características. La lista fue luego ampliada agregando todas las entradas del tesauro que estaban relacionadas con las palabras iniciales. Finalmente, hicimos una lluvia de ideas para encontrar las palabras faltantes que normalmente estarían asociadas con cada una de las características. Que Lf ∩ Ti sea la lista de términos comunes a Lf y Ti. Cada término de Lf se cuenta el número de veces que aparece en Ti, con dos excepciones: • en los casos en que el usuario envía un título para la revisión, consideramos el texto del título agregándolo tres veces al texto de la revisión Ti. La suposición intuitiva es que la opinión del usuario se refleja con mayor fuerza en el título que en el cuerpo de la reseña. Por ejemplo, muchas reseñas son resumidas con precisión por títulos como \"Excelente servicio, ubicación terrible\" o \"Mala relación calidad-precio\"; ciertas palabras que ocurren solo una vez en el texto son contadas múltiples veces si su relevancia para esa característica es particularmente fuerte. Estas eran las palabras raíz para cada característica (por ejemplo, personal es una palabra raíz para la característica Servicio), y tenían un peso de 2 o 3. Cada característica fue asignada hasta 3 palabras raíz, por lo que casi todas las palabras se cuentan solo una vez. La lista de palabras para la característica Habitaciones se proporciona como referencia en el Apéndice A. El peso wi f se calcula como: wi f = |Lf ∩ Ti| f∈F |Lf ∩ Ti| (1) donde |Lf ∩ Ti| es el número de términos comunes entre Lf y Ti. El peso para la característica \"En general\" se estableció en min{ |T i | 5000 , 1} donde |Ti | es el número de caracteres en Ti. Comenzaré diciendo que soy más de la onda de Holiday Inn que de un hotel de lujo. Así que me frustro cuando pago el doble de la tarifa de la habitación y recibo la mitad de las comodidades que obtendría en un Hampton Inn o Holiday Inn. La ubicación era definitivamente el principal activo de este lugar. Estaba a solo unas cuadras de la parada de metro del Centro Hynes y era fácil caminar a algunos buenos restaurantes en la zona de Back Bay. Boylston no está lejos en absoluto. Así que no tuve problemas en renunciar a un coche de alquiler y tomar el metro desde el aeropuerto hasta el hotel y usar el metro para cualquier otro viaje. De lo contrario, te hacen pagar por todo. Y cuando ya has gastado $215 por noche en la habitación, eso resulta frustrante. La habitación en sí estaba bien, más o menos lo que esperaría. El personal también era promedio, ni malo ni excelente. Una vez más, creo que estás pagando por la ubicación y la capacidad de ir caminando a muchos lugares interesantes. Pero creo que la próxima vez me quedaré en Brookline, disfrutaré de más comodidades y usaré más el metro. Las calificaciones numéricas asociadas a esta reseña son rO = 3, rR = 3, rS = 3, rC = 4, rV = 2 para las características de General (O), Habitaciones (R), Servicio (S), Limpieza (C) y Valor (V) respectivamente. Las calificaciones de las características Comida (F), Ubicación (L) y Ruido (N) están ausentes (es decir, rF = rL = rN = 0). Los pesos wf se calculan a partir de las siguientes listas de términos comunes: LR ∩ T = {habitación}; wR = 0.066 LS ∩ T = {3 * Personal, comodidades}; wS = 0.267 LC ∩ T = ∅; wC = 0 LV ∩ T = {$, tarifa}; wV = 0.133 LF ∩ T = {restaurante}; wF = 0.067 LL ∩ T = {2 * centro, 2 * caminar, 2 * ubicación, área}; wL = 0.467 LN ∩ T = ∅; wN = 0 Las palabras raíz Personal y Centro se triplicaron y duplicaron respectivamente. El peso total de la revisión textual es wO = 0.197. Estos valores explican razonablemente bien los pesos de las diferentes características en la discusión del revisor. Un punto a tener en cuenta es que algunos términos en las listas Lf poseen una orientación semántica inherente. Por ejemplo, la palabra suciedad (perteneciente a la lista LC) se usaría con mayor frecuencia para afirmar la presencia, y no la ausencia de suciedad. Esto es inevitable, pero se tuvo cuidado de asegurar que se utilizaran palabras de ambos extremos del espectro. Por esta razón, algunas listas como LR contienen solo sustantivos de objetos que uno típicamente describiría en una habitación (ver Apéndice A). El objetivo de esta sección es analizar la influencia de los pesos wi f en las calificaciones numéricas ri f. Intuitivamente, los usuarios que pasaron mucho tiempo discutiendo una característica f (es decir, cuando wif es alto) tenían algo que decir sobre su experiencia con respecto a esta característica. Obviamente, la característica f es importante para el usuario i. Dado que las personas tienden a ser más conocedoras en los aspectos que consideran importantes, nuestra hipótesis es que las calificaciones ri f (correspondientes a los pesos altos wi f) constituyen un subconjunto de las calificaciones de expertos para la característica f. La Figura 2 traza la distribución de las tasas r i(h) C con respecto a los pesos w i(h) C para la limpieza de un hotel de Las Vegas, h. Aquí, las calificaciones altas están restringidas a las reseñas que hablan poco sobre la limpieza. Cuando se menciona la limpieza en la discusión, las calificaciones son bajas. Muchos hoteles muestran patrones de calificación similares para varias características. Las calificaciones correspondientes a pesos bajos abarcan todo el espectro del 1 al 5, mientras que las calificaciones correspondientes a pesos altos están más agrupadas (ya sea alrededor de calificaciones buenas o malas). Por lo tanto, formulamos la siguiente hipótesis: Hipótesis 1. Las calificaciones ri f correspondientes a las reseñas donde wi f es alta, son más similares entre sí que a la colección general de calificaciones. Para probar la hipótesis, tomamos todo el conjunto de reseñas y, característica por característica, calculamos la desviación estándar de las calificaciones con alto peso, y la desviación estándar de todo el conjunto de calificaciones. Los pesos altos se definieron como aquellos que pertenecen al 20% superior del rango de pesos para la característica correspondiente. Si la Hipótesis 1 fuera cierta, la desviación estándar de todas las calificaciones debería ser mayor que la desviación estándar de las calificaciones con pesos altos. 0 1 2 3 4 5 6 0 0.1 0.2 0.3 0.4 0.5 0.6 Calificación Peso Figura 2: La distribución de las calificaciones en función del peso de la característica de limpieza. Utilizamos una prueba T estándar para medir la significancia de los resultados. Ciudad por ciudad y característica por característica, la Tabla 2 presenta la desviación estándar promedio de todas las calificaciones, y la desviación estándar promedio de las calificaciones con pesos altos. De hecho, las calificaciones con pesos altos tienen una desviación estándar más baja, y los resultados son significativos en el umbral estándar de significancia de 0.05 (aunque para ciertas ciudades tomadas de forma independiente no parece haber una diferencia significativa, los resultados son significativos para todo el conjunto de datos). Por favor, tenga en cuenta que solo se consideraron las características O, R, S, C y V, ya que para las otras (F, L y N) no teníamos suficientes calificaciones. Tabla 2: Desviación estándar promedio para todas las calificaciones, y desviación estándar promedio para las calificaciones con pesos altos. En corchetes, los valores p correspondientes para una diferencia positiva entre los dos. La hipótesis 1 no solo proporciona una comprensión básica sobre el comportamiento de calificación de los usuarios en línea, sino que también sugiere algunas formas de calcular estimaciones de mejor calidad. Podemos, por ejemplo, construir una estimación de calidad característica por característica con una varianza mucho menor: para cada característica tomamos el subconjunto de reseñas que discuten ampliamente esa característica, y como estimación de calidad, obtenemos la calificación promedio de este subconjunto. Los experimentos iniciales sugieren que las calificaciones promedio de característica a característica calculadas de esta manera son diferentes de las calificaciones promedio calculadas en todo el conjunto de datos. Dado que, de hecho, los pesos altos son indicadores de opiniones expertas, las estimaciones obtenidas de esta manera son más precisas que las actuales. Sin embargo, la validación de esta suposición subyacente requiere más experimentos controlados. LA INFLUENCIA DE LAS CALIFICACIONES PASADAS Por lo general, se hacen dos suposiciones importantes sobre las reseñas enviadas a foros en línea. La primera es que las calificaciones reflejen fielmente la calidad observada por los usuarios; la segunda es que las reseñas sean independientes entre sí. Si bien la evidencia anecdótica [9, 22] cuestiona la primera suposición, en esta sección abordamos la segunda. Un examen de las reseñas en línea muestra que estas suelen formar parte de hilos de discusión, donde los usuarios se esfuerzan por contradecir o estar vehementemente de acuerdo con los comentarios de usuarios anteriores. Considera, por ejemplo, la siguiente reseña: No entiendo las críticas negativas... el hotel estaba un poco oscuro, pero ese era el estilo. Fue muy artístico. Sí, estaba cerca de la autopista, pero en mi opinión, el sonido de un coche ruidoso ocasional es mejor que escuchar el ding ding de las máquinas tragamonedas toda la noche. El personal disponible es FABULOSO. Las camareras son geniales (y *** no merece la mala crítica que recibió, ¡fue 100% atenta con nosotros!), los camareros son amigables y profesionales al mismo tiempo... Aquí, el usuario se vio perturbado por informes negativos anteriores, abordó estas preocupaciones y se dispuso a intentar corregirlas. Como era de esperar, sus calificaciones fueron considerablemente más altas que las calificaciones promedio hasta este punto. Parece que los usuarios de TripAdvisor suelen leer regularmente los informes enviados por usuarios anteriores antes de reservar un hotel, o antes de escribir una reseña. Las reseñas anteriores crean ciertas expectativas previas sobre la calidad del servicio, y estas expectativas influyen en la reseña presentada. Creemos que esta observación es válida para la mayoría de los foros en línea. La percepción subjetiva de la calidad es directamente proporcional a qué tan bien la experiencia real cumple con la expectativa previa, un hecho confirmado por una importante línea de investigación econométrica y de marketing [17, 18, 16, 21]. La correlación entre las reseñas también ha sido confirmada por investigaciones recientes sobre la dinámica de los foros de reseñas en línea [6]. 4.1 Expectativas Previas Definimos la expectativa previa del usuario i con respecto a la característica f, como el promedio de las calificaciones previamente disponibles en la característica f4: ef (i) = j<i,r j f =0 rj f j<i,r j f =0 1 Como primera hipótesis, afirmamos que la calificación ri f es una función de la expectativa previa ef (i): Hipótesis 2. Para un hotel y una característica dados, dados los comentarios i y j tales que ef (i) es alto y ef (j) es bajo, la calificación rj f supera la calificación ri f. Definimos las expectativas altas y bajas como aquellas que están por encima, respectivamente por debajo de un cierto valor umbral θ. El conjunto de reseñas precedidas por altas, respectivamente bajas expectativas 3 parte de las reseñas de Amazon fueron reconocidas como publicaciones estratégicas por autores de libros o competidores 4 si no se asignaron calificaciones previas para la característica f, ef (i) se asigna un valor predeterminado de 4. Tabla 3: Calificaciones promedio para reseñas precedidas por expectativas bajas (primer valor en la celda) y altas (segundo valor en la celda). Los valores P para una diferencia positiva se dan entre corchetes. Las ciudades O R S C V 3.953 4.045 3.985 4.252 3.946 Boston 3.364 3.590 3.485 3.641 3.242 [0.011] [0.028] [0.0086] [0.0168] [0.0034] 4.284 4.358 4.064 4.530 4.428 Sídney 3.756 3.537 3.436 3.918 3.495 [0.000] [0.000] [0.035] [0.009] [0.000] 3.494 3.674 3.713 3.689 3.580 Las Vegas 3.140 3.530 2.952 3.530 3.351 [0.190] [0.529] [0.007] [0.529] [0.253] se definen de la siguiente manera: Ralto f = {ri f |ef (i) > θ} Rbajo f = {ri f |ef (i) < θ} Estos conjuntos son específicos para cada par (hotel, característica), y en nuestros experimentos tomamos θ = 4. Este valor bastante alto está cerca del promedio de calificación de todas las características de todos los hoteles, y se justifica por el hecho de que nuestro conjunto de datos contiene principalmente hoteles de alta calidad. Para cada ciudad, tomamos todos los hoteles y calculamos las calificaciones promedio en los conjuntos Rhigh f y Rlow f (ver Tabla 3). El promedio de calificación entre las reseñas que siguen a expectativas previas bajas es significativamente mayor que el promedio de calificación después de altas expectativas. Como evidencia adicional, consideramos todos los hoteles para los cuales la función eV (i) (la expectativa para la característica Valor) tiene un valor alto (mayor que 4) para algunos i, y un valor bajo (menor que 4) para algunos otros i. Intuitivamente, estos son los hoteles para los cuales hay un grado mínimo de variación en la secuencia oportuna de reseñas: es decir, el promedio acumulativo de calificaciones fue en algún momento alto y luego se volvió bajo, o viceversa. Tales variaciones se observan en aproximadamente la mitad de todos los hoteles en cada ciudad. La Figura 3 traza la mediana (entre los hoteles considerados) de la calificación, rV, cuando ef (i) no es más que x pero mayor que x - 0.5. 2.5 3 3.5 4 4.5 5 2.5 3 3.5 4 4.5 5 Expectativa de la mediana de calificación Boston Sydney Vegas Figura 3: Las calificaciones tienden a disminuir a medida que aumenta la expectativa. 138 Hay dos formas de interpretar la función ef (i): • El valor esperado para la característica f obtenido por el usuario i antes de su experiencia con el servicio, adquirido al leer informes presentados por usuarios anteriores. En este caso, un valor excesivamente alto para ef (i) llevaría al usuario a enviar un informe negativo (o viceversa), derivado de la diferencia entre el valor real del servicio y la expectativa inflada de este valor adquirida antes de su experiencia. • El valor esperado de la característica f para todos los visitantes posteriores del sitio, si el usuario i no enviara un informe. En este caso, la motivación para un informe negativo después de un valor excesivamente alto de ef es diferente: el usuario i busca corregir la expectativa de los futuros visitantes del sitio. A diferencia de la interpretación anterior, esto no requiere que el usuario derive una expectativa a priori para el valor de f. Tenga en cuenta que ninguna de las interpretaciones implica que el promedio hasta el informe i esté inversamente relacionado con la calificación en el informe i. Podría existir una medida de influencia ejercida por informes anteriores que impulse al usuario detrás del informe i a enviar calificaciones que, en cierta medida, se ajusten a los informes anteriores: un valor bajo para ef (i) puede influir en el usuario i a enviar una calificación baja para la característica f porque, por ejemplo, teme que enviar una calificación alta lo haga parecer una persona con bajos estándares. Esto, al principio, parece contradecir la Hipótesis 2. Sin embargo, esta calificación de conformidad no puede continuar indefinidamente: una vez que el conjunto de informes proyecte una estimación suficientemente reducida para vf, los revisores futuros con impresiones comparativamente positivas buscarán corregir esta percepción errónea. 4.2 Impacto de los comentarios textuales en la expectativa de calidad. Se puede obtener una mayor comprensión del comportamiento de calificación de los usuarios de TripAdvisor analizando la relación entre los pesos wf y los valores ef (i). En particular, examinamos la siguiente hipótesis: Hipótesis 3. Cuando una gran proporción del texto de una reseña discute una característica en particular, la diferencia entre la calificación para esa característica y la calificación promedio hasta ese momento tiende a ser grande. La intuición detrás de esta afirmación es que cuando el usuario insiste en expresar su opinión sobre una característica en particular, su opinión difiere de la opinión colectiva de publicaciones anteriores. Esto se basa en la característica de los sistemas de reputación como foros de retroalimentación donde un usuario está interesado en proyectar su opinión, con particular fuerza si esta opinión difiere de lo que percibe como la opinión general. Para probar la Hipótesis 3 medimos la diferencia absoluta promedio entre la expectativa ef (i) y la calificación ri f cuando el peso wi f es alto, respectivamente bajo. Los pesos se clasifican como altos o bajos al compararlos con ciertos valores de corte: wi f es bajo si es menor que 0.1, mientras que wi f es alto si es mayor que θf. Se utilizaron diferentes valores de corte para diferentes características: θR = 0.4, θS = 0.4, θC = 0.2 y θV = 0.7. La limpieza tiene un límite inferior más bajo ya que es una característica raramente discutida; el valor tiene un límite superior alto por la razón opuesta. Los resultados se presentan en la Tabla 4. La idea de que los informes negativos pueden fomentar una mayor divulgación negativa ha sido sugerida anteriormente [14]. Tabla 4: Promedio de |ri f −ef (i)| cuando los pesos son altos (primer valor en la celda) y bajos (segundo valor en la celda) con valores P para la diferencia en corchetes cuadrados. Esto demuestra que cuando los pesos son inusualmente altos, los usuarios tienden a expresar una opinión que no se ajusta al promedio neto de calificaciones anteriores. Como podríamos esperar, para una característica que rara vez tuvo un peso importante en la discusión (por ejemplo, la limpieza), la diferencia es particularmente grande. Aunque la diferencia en el valor de la característica es bastante grande para Sídney, el valor P es alto. Esto se debe a que solo unos pocos comentarios discutieron en gran medida el valor. La razón podría ser cultural o porque había menos motivo para discutir esta característica. 4.3 Incentivos para informar Los modelos anteriores sugieren que los usuarios poco opinativos no elegirán expresar sus opiniones [12]. En esta sección, ampliamos este modelo para tener en cuenta la influencia de las expectativas. La motivación para enviar comentarios no se debe solo a opiniones extremas, sino también a la diferencia entre la reputación actual (es decir, la expectativa previa del usuario) y la experiencia real. Un modelo de calificación así produce calificaciones que la mayoría de las veces se desvían de la calificación promedio actual. Las calificaciones que confirman la expectativa previa rara vez serán enviadas. Probamos en nuestro conjunto de datos la proporción de calificaciones que intentan corregir la estimación actual. Definimos una calificación desviada como aquella que se desvía de la expectativa actual por al menos un umbral θ, es decir, |ri f − ef (i)| ≥ θ. Para cada una de las tres ciudades consideradas, las siguientes tablas muestran la proporción de calificaciones desviadas para θ = 0.5 y θ = 1. Los resultados anteriores sugieren que una gran proporción de usuarios (casi la mitad, incluso para el valor umbral alto θ = 1) se desvían del promedio previo. Esto refuerza la idea de que los usuarios son más propensos a enviar un informe cuando creen que tienen algo distintivo que agregar al flujo actual de opiniones sobre alguna característica. Tales conclusiones están en total acuerdo con evidencia previa de que la distribución de informes a menudo sigue distribuciones bimodales en forma de U. 139 5. Para dar cuenta de las observaciones descritas en las secciones anteriores, proponemos un modelo para el comportamiento de los usuarios al enviar reseñas en línea. Para un hotel dado, asumimos que la calidad experimentada por los usuarios sigue una distribución normal alrededor de algún valor vf, que representa la calidad objetiva ofrecida por el hotel en la característica f. La calificación enviada por el usuario i en la característica f es: ˆri f = δf vi f + (1 − δf) · sign vi f − ef (i) c + d(vi f, ef (i)|wi f) (2) donde: • vi f es la calidad (desconocida) experimentada realmente por el usuario. Se asume que vi f sigue una distribución normal alrededor de algún valor vf; • δf ∈ [0, 1] puede verse como una medida del sesgo al reportar retroalimentación. Los valores altos reflejan el hecho de que los usuarios califican de manera objetiva, sin ser influenciados por expectativas previas. El valor de δf puede depender de varios factores; fijamos un valor para cada característica f; • c es una constante entre 1 y 5; • wi f es el peso de la característica f en el comentario textual de la reseña i, calculado según la Ecuación (1); • d(vi f , ef (i)|wi f ) es una función de distancia entre la expectativa y la observación del usuario i. La función de distancia satisface las siguientes propiedades: - d(y, z|w) ≥ 0 para todo y, z ∈ [0, 5], w ∈ [0, 1]; - |d(y, z|w)| < |d(z, x|w)| si |y − z| < |z − x|; - |d(y, z|w1)| < |d(y, z|w2)| si w1 < w2; - c + d(vf , ef (i)|wi f ) ∈ [1, 5]; El segundo término de la Ecuación (2) codifica el sesgo de la calificación. Cuanto mayor sea la distancia entre la observación real vi f y la función ef, mayor será el sesgo. 5.1 Validación del modelo Utilizamos el conjunto de datos de las reseñas de TripAdvisor para validar el modelo de comportamiento presentado anteriormente. Dividimos por conveniencia los valores de calificación en tres rangos: malo (B = {1, 2}), indiferente (I = {3, 4}), y bueno (G = {5}), y realizamos los siguientes dos tests: • Primero, usaremos nuestro modelo para predecir las calificaciones que tienen valores extremos. Para cada hotel, tomamos la secuencia de informes, y cada vez que nos encontramos con una calificación que sea buena o mala (pero no indiferente) intentamos predecirla utilizando la Ecuación (2) • En segundo lugar, en lugar de predecir el valor de las calificaciones extremas, intentamos clasificarlas como buenas o malas. Para cada hotel tomamos la secuencia de informes, y para cada informe (independientemente de su valor) lo clasificamos como bueno o malo. Sin embargo, para realizar estas pruebas, necesitamos estimar el valor objetivo, vf, que es el promedio de las observaciones de calidad reales, vi f. El algoritmo que estamos utilizando se basa en la intuición de que la cantidad de calificación de conformidad se minimiza. En otras palabras, el valor vf debe ser tal que, tan a menudo como sea posible, las calificaciones malas sigan a expectativas por encima de vf y las calificaciones buenas sigan a expectativas por debajo de vf. Formalmente, definimos los conjuntos: Γ1 = {i|ef (i) < vf y ri f ∈ B}; Γ2 = {i|ef (i) > vf y ri f ∈ G}; que corresponden a irregularidades donde, a pesar de que la expectativa en el punto i es menor que el valor entregado, la calificación es baja, y viceversa. Definimos vf como el valor que minimiza la unión de los dos conjuntos: vf = arg min vf |Γ1 ∪ Γ2| (3) En la Ec. (2) reemplazamos vi f por el valor vf calculado en la Ec. (3), y utilizamos la siguiente función de distancia: d(vf , ef (i)|wi f ) = |vf − ef (i)| vf − ef (i) |vf 2 − ef (i)2 | · (1 + 2wi f ); La constante c ∈ I se estableció en min{max{ef (i), 3}}, 4}. Los valores para δf se fijaron en {0.7, 0.7, 0.8, 0.7, 0.6} para las características {General, Habitaciones, Servicio, Limpieza, Valor} respectivamente. Los pesos se calculan tal como se describe en la Sección 3. Como primer experimento, tomamos los conjuntos de calificaciones extremas {ri f |ri f /∈ I} para cada hotel y característica. Para cada calificación de este tipo, ri f, intentamos estimarla calculando ˆri f usando la Ecuación (2). Comparamos este estimador con el obtenido simplemente promediando las calificaciones de todos los hoteles y características: es decir, ¯rf = j,r j f =0 rj f j,r j f =0 1; la Tabla 7 presenta la relación entre el error cuadrático medio (RMSE) al usar ˆri f y ¯rf para estimar las calificaciones reales. En todos los casos, la estimación producida por nuestro modelo es mejor que el promedio simple. Tabla 7: Promedio de RMSE(ˆrf) RMSE(¯rf) Ciudad O R S C V Boston 0.987 0.849 0.879 0.776 0.913 Sídney 0.927 0.817 0.826 0.720 0.681 Las Vegas 0.952 0.870 0.881 0.947 0.904 Como segundo experimento, intentamos distinguir los conjuntos Bf = {i|ri f ∈ B} y Gf = {i|ri f ∈ G} de calificaciones malas y buenas, respectivamente, en la característica f. Por ejemplo, calculamos el conjunto Bf usando el siguiente clasificador (llamado σ): ri f ∈ Bf (σf (i) = 1) ⇔ ˆri f ≤ 4; Las Tablas 8, 9 y 10 presentan la Precisión (p), Recall (r) y s = 2pr p+r para el clasificador σ, y lo comparan con un clasificador de mayoría ingenuo, τ, τf (i) = 1 ⇔ |Bf | ≥ |Gf |: Vemos que el recall es siempre mayor para σ y la precisión suele ser ligeramente peor. Para la métrica s, σ tiende a agregar un 140. Tabla 8: Precisión (p), Recall (r), s= 2pr p+r mientras se detectan calificaciones bajas para Boston O R S C V p 0.678 0.670 0.573 0.545 0.610 σ r 0.626 0.659 0.619 0.612 0.694 s 0.651 0.665 0.595 0.577 0.609 p 0.684 0.706 0.647 0.611 0.633 τ r 0.597 0.541 0.410 0.383 0.562 s 0.638 0.613 0.502 0.471 0.595 Tabla 9: Precisión (p), Recall (r), s= 2pr p+r mientras se detectan calificaciones bajas para Las Vegas O R S C V p 0.654 0.748 0.592 0.712 0.583 σ r 0.608 0.536 0.791 0.474 0.610 s 0.630 0.624 0.677 0.569 0.596 p 0.685 0.761 0.621 0.748 0.606 τ r 0.542 0.505 0.767 0.445 0.441 s 0.605 0.607 0.670 0.558 0.511 Mejora del 1-20% sobre τ, mucho mayor en algunos casos para hoteles en Sídney. Esto se debe probablemente a que las reseñas de Sídney son más positivas que las de las ciudades estadounidenses y los casos en los que el número de reseñas negativas supera al de las positivas son raros. Reemplazar el algoritmo de prueba con uno que juega un 1 con una probabilidad igual a la proporción de malas críticas mejora sus resultados para esta ciudad, pero aún es superado por alrededor del 80%. 6. RESUMEN DE RESULTADOS Y CONCLUSIÓN El objetivo de este artículo es explorar los factores que llevan a un usuario a enviar una calificación particular, en lugar de los incentivos que lo animaron a enviar un informe en primer lugar. Para ello utilizamos dos fuentes adicionales de información además del vector de calificaciones numéricas: primero, examinamos los comentarios textuales que acompañan las reseñas, y segundo, consideramos los informes que han sido previamente enviados por otros usuarios. Usando algoritmos simples de procesamiento de lenguaje natural, pudimos establecer una correlación entre el peso de una característica específica en el comentario textual que acompaña la reseña, y el ruido presente en la calificación numérica. Específicamente, parece que los usuarios que discuten ampliamente una característica en particular tienden a estar de acuerdo en una calificación común. Esta observación permite la construcción de estimadores de calidad característica por característica que tienen una varianza más baja y, con suerte, son menos ruidosos. Sin embargo, se requiere más evidencia para respaldar la intuición de que las calificaciones correspondientes a pesos altos son opiniones de expertos que merecen ser consideradas de mayor prioridad al calcular estimaciones de calidad. En segundo lugar, enfatizamos la dependencia de las calificaciones en informes previos. Los informes anteriores crean una expectativa de calidad que afecta la percepción subjetiva del usuario. Validamos dos hechos sobre las reseñas de hoteles que recopilamos de TripAdvisor: Primero, las calificaciones que siguen expectativas bajas (donde la expectativa se calcula como el promedio de los informes anteriores) probablemente sean más altas que las calificaciones que siguen expectativas altas. Intuitivamente, la percepción de la calidad (y consecuentemente la calificación) depende de qué tan bien la experiencia real del usuario cumple con sus expectativas. Segundo, incluimos evidencia de los comentarios textuales, y encontramos que cuando los usuarios dedican una gran parte del texto a discutir una característica específica, es probable que motiven una calificación divergente (es decir, una calificación que no se ajusta a la expectativa previa). Intuitivamente, esto respalda la hipótesis de que los foros de reseñas actúan como grupos de discusión donde los usuarios están interesados en presentar y motivar su propia opinión. Hemos capturado la evidencia empírica en un modelo de comportamiento que predice las calificaciones enviadas por los usuarios. La calificación final depende, como era de esperar, de la observación real y de la brecha entre la observación y la expectativa. La brecha tiende a tener una influencia mayor cuando una fracción importante del comentario textual está dedicada a discutir una característica específica. El modelo propuesto fue validado con los datos empíricos y proporciona mejores estimaciones de las calificaciones realmente enviadas. Una suposición que hacemos es sobre la existencia de un valor de calidad objetivo vf para la característica f. Esto rara vez es cierto, especialmente a lo largo de largos periodos de tiempo. Otras explicaciones podrían dar cuenta de la correlación de las calificaciones con informes anteriores. Por ejemplo, si ef (i) refleja el valor real de f en un punto en el tiempo, la diferencia en las calificaciones siguiendo expectativas altas y bajas puede ser explicada por modelos de ingresos hoteleros que se maximizan cuando el valor es modificado en consecuencia. Sin embargo, la idea de que la variación en las calificaciones no es principalmente una función de la variación en el valor resulta ser útil. Nuestro enfoque para aproximar este valor objetivo es de ninguna manera perfecto, pero se ajusta perfectamente a la idea detrás del modelo. Una dirección natural para trabajos futuros es examinar aplicaciones concretas de nuestros resultados. Es probable que se obtengan mejoras significativas en las estimaciones de calidad al incorporar toda la evidencia empírica sobre el comportamiento de calificación. No está claro exactamente cómo diferentes factores afectan las decisiones de los usuarios. La respuesta podría depender de la aplicación particular, el contexto y la cultura. 7. REFERENCIAS [1] A. Admati y P. Pfleiderer. Noisytalk.com: Transmitiendo opiniones en un entorno ruidoso. Documento de trabajo 1670R, Universidad de Stanford, 2000. [2] P. B., L. Lee y S. Vaithyanathan. ¿Clasificación de sentimientos con técnicas de aprendizaje automático? En Actas de EMNLP-02, la Conferencia sobre Métodos Empíricos en el Procesamiento del Lenguaje Natural, 2002. [3] H. Cui, V. Mittal y M. Datar. Experimentos comparativos 141 sobre clasificación de sentimientos para reseñas de productos en línea. En Actas de AAAI, 2006. [4] K. Dave, S. Lawrence y D. Pennock. Extracción de opiniones y clasificación semántica de reseñas de productos en la galería de cacahuetes. En Actas de la 12ª Conferencia Internacional sobre la World Wide Web (WWW03), 2003. [5] C. Dellarocas, N. Awad y X. Zhang. Explorando el valor de las calificaciones de productos en línea en la previsión de ingresos: El caso de las películas en movimiento. Documento de trabajo, 2006. [6] C. Forman, A. Ghose y B. Wiesenfeld. Un Examen Multinivel del Impacto de las Identidades Sociales en las Transacciones Económicas en los Mercados Electrónicos. Disponible en SSRN: http://ssrn.com/abstract=918978, julio de 2006. [7] A. Ghose, P. Ipeirotis y A. Sundararajan. Primas de reputación en mercados electrónicos peer-to-peer: análisis de retroalimentación textual y estructura de red. En el Tercer Taller sobre Economía de Sistemas Peer-to-Peer (P2PECON), 2005. [8] A. Ghose, P. Ipeirotis y A. Sundararajan. Las dimensiones de la reputación en los mercados electrónicos. Documento de trabajo CeDER-06-02, Universidad de Nueva York, 2006. [9] A. Harmon. Error de Amazon revela la guerra de los revisores. The New York Times, 14 de febrero de 2004. [10] D. Houser y J. Wooders. Reputación en subastas: teoría y evidencia de eBay. Revista de Estrategia Económica y de Gestión, 15:353-369, 2006. [11] M. Hu y B. Liu. Extracción y resumen de reseñas de clientes. En Actas de la Conferencia Internacional de la ACM SIGKDD sobre Descubrimiento de Conocimiento y Minería de Datos (KDD04), 2004. [12] N. Hu, P. Pavlou y J. Zhang. ¿Pueden las reseñas en línea revelar la verdadera calidad de un producto? En Actas de la Conferencia ACM sobre Comercio Electrónico (EC 06), 2006. [13] K. Kalyanam y S. McIntyre. Retorno de la reputación en el mercado de subastas en línea. Documento de trabajo 02/03-10-WP, Escuela de Negocios Leavey, Universidad de Santa Clara, 2001. [14] L. Khopkar y P. Resnick. Auto-selección, deslizamiento, salvamento, holgazanería y lapidación: los impactos de la retroalimentación negativa en eBay. En Actas de la Conferencia ACM sobre Comercio Electrónico (EC 05), 2005. [15] M. Melnik y J. Alm. ¿Importa la reputación de un vendedor? evidencia de subastas en eBay. Revista de Economía Industrial, 50(3):337-350, 2002. [16] R. Olshavsky y J. Miller. Expectativas del consumidor, rendimiento del producto y calidad percibida del producto. Revista de Investigación de Marketing, 9:19-21, febrero de 1972. [17] A. Parasuraman, V. Zeithaml y L. Berry. Un Modelo Conceptual de Calidad de Servicio y sus Implicaciones para Investigaciones Futuras. Revista de Marketing, 49:41-50, 1985. [18] A. Parasuraman, V. Zeithaml y L. Berry. SERVQUAL: Una escala de múltiples ítems para medir las percepciones de calidad del servicio por parte del consumidor. Revista de Retail, 64:12-40, 1988. [19] P. Pavlou y A. Dimoka. La naturaleza y función de los comentarios de texto de retroalimentación en los mercados en línea: implicaciones para la construcción de confianza, primas de precio y diferenciación de vendedores. Investigación de Sistemas de Información, 17(4):392-414, 2006. [20] A. Popescu y O. Etzioni. Extrayendo características del producto y opiniones de las reseñas. En Actas de la Conferencia de Tecnología del Lenguaje Humano y Conferencia sobre Métodos Empíricos en Procesamiento del Lenguaje Natural, 2005. [21] R. Teas. Expectativas, Evaluación del Rendimiento y Percepciones de Calidad de los Consumidores. Revista de Marketing, 57:18-34, 1993. [22] E. White. Chateando a un cantante en las listas de éxitos pop. El Wall Street Journal, 15 de octubre de 1999. APÉNDICE A. LISTA DE PALABRAS, LR, ASOCIADAS A LA CARACTERÍSTICA HABITACIONES. Todas las palabras sirven como prefijos: habitación, espacio, interior, decoración, ambiente, atmósfera, comodidad, baño, inodoro, cama, edificio, pared, ventana, privado, temperatura, sábana, lino, almohada, caliente, agua, fría, agua, ducha, vestíbulo, muebles, alfombra, aire, acondicionado, colchón, distribución, diseño, espejo, techo, iluminación, lámpara, sofá, silla, cómoda, armario, armario. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "rating": {
            "translated_key": "calificación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Understanding User Behavior in Online Feedback Reporting Arjun Talwar Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland arjun@math.stanford.edu Radu Jurca Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland radu.jurca@epfl.ch Boi Faltings Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland boi.faltings@epfl.ch ABSTRACT Online reviews have become increasingly popular as a way to judge the quality of various products and services.",
                "Previous work has demonstrated that contradictory reporting and underlying user biases make judging the true worth of a service difficult.",
                "In this paper, we investigate underlying factors that influence user behavior when reporting feedback.",
                "We look at two sources of information besides numerical ratings: linguistic evidence from the textual comment accompanying a review, and patterns in the time sequence of reports.",
                "We first show that groups of users who amply discuss a certain feature are more likely to agree on a common <br>rating</br> for that feature.",
                "Second, we show that a users <br>rating</br> partly reflects the difference between true quality and prior expectation of quality as inferred from previous reviews.",
                "Both give us a less noisy way to produce <br>rating</br> estimates and reveal the reasons behind user bias.",
                "Our hypotheses were validated by statistical evidence from hotel reviews on the TripAdvisor website.",
                "Categories and Subject Descriptors J.4 [Social and Behavioral Sciences]: Economics General Terms Economics, Experimentation, Reliability 1.",
                "MOTIVATIONS The spread of the internet has made it possible for online feedback forums (or reputation mechanisms) to become an important channel for Word-of-mouth regarding products, services or other types of commercial interactions.",
                "Numerous empirical studies [10, 15, 13, 5] show that buyers seriously consider online feedback when making purchasing decisions, and are willing to pay reputation premiums for products or services that have a good reputation.",
                "Recent analysis, however, raises important questions regarding the ability of existing forums to reflect the real quality of a product.",
                "In the absence of clear incentives, users with a moderate outlook will not bother to voice their opinions, which leads to an unrepresentative sample of reviews.",
                "For example, [12, 1] show that Amazon1 ratings of books or CDs follow with great probability bi-modal, U-shaped distributions where most of the ratings are either very good, or very bad.",
                "Controlled experiments, on the other hand, reveal opinions on the same items that are normally distributed.",
                "Under these circumstances, using the arithmetic mean to predict quality (as most forums actually do) gives the typical user an estimator with high variance that is often false.",
                "Improving the way we aggregate the information available from online reviews requires a deep understanding of the underlying factors that bias the <br>rating</br> behavior of users.",
                "Hu et al. [12] propose the Brag-and-Moan Model where users rate only if their utility of the product (drawn from a normal distribution) falls outside a median interval.",
                "The authors conclude that the model explains the empirical distribution of reports, and offers insights into smarter ways of estimating the true quality of the product.",
                "In the present paper we extend this line of research, and attempt to explain further facts about the behavior of users when reporting online feedback.",
                "Using actual hotel reviews from the TripAdvisor2 website, we consider two additional sources of information besides the basic numerical ratings submitted by users.",
                "The first is simple linguistic evidence from the textual review that usually accompanies the numerical ratings.",
                "We use text-mining techniques similar to [7] and [3], however, we are only interested in identifying what aspects of the service the user is discussing, without computing the semantic orientation of the text.",
                "We find that users who comment more on the same feature are more likely to agree on a common numerical <br>rating</br> for that particular feature.",
                "Intuitively, lengthy comments reveal the importance of the feature to the user.",
                "Since people tend to be more knowledgeable in the aspects they consider important, users who discuss a given feature in more details might be assumed to have more authority in evaluating that feature.",
                "Second we investigate the relationship between a review 1 http://www.amazon.com 2 http://www.tripadvisor.com/ 134 Figure 1: The TripAdvisor page displaying reviews for a popular Boston hotel.",
                "Name of hotel and advertisements were deliberatively erased. and the reviews that preceded it.",
                "A perusal of online reviews shows that ratings are often part of discussion threads, where one post is not necessarily independent of other posts.",
                "One may see, for example, users who make an effort to contradict, or vehemently agree with, the remarks of previous users.",
                "By analyzing the time sequence of reports, we conclude that past reviews influence the future reports, as they create some prior expectation regarding the quality of service.",
                "The subjective perception of the user is influenced by the gap between the prior expectation and the actual performance of the service [17, 18, 16, 21] which will later reflect in the users <br>rating</br>.",
                "We propose a model that captures the dependence of ratings on prior expectations, and validate it using the empirical data we collected.",
                "Both results can be used to improve the way reputation mechanisms aggregate the information from individual reviews.",
                "Our first result can be used to determine a featureby-feature estimate of quality, where for each feature, a different subset of reviews (i.e., those with lengthy comments of that feature) is considered.",
                "The second leads to an algorithm that outputs a more precise estimate of the real quality. 2.",
                "THE DATA SET We use in this paper real hotel reviews collected from the popular travel site TripAdvisor.",
                "TripAdvisor indexes hotels from cities across the world, along with reviews written by travelers.",
                "Users can search the site by giving the hotels name and location (optional).",
                "The reviews for a given hotel are displayed as a list (ordered from the most recent to the oldest), with 5 reviews per page.",
                "The reviews contain: • information about the author of the review (e.g., dates of stay, username of the reviewer, location of the reviewer); • the overall <br>rating</br> (from 1, lowest, to 5, highest); • a textual review containing a title for the review, free comments, and the main things the reviewer liked and disliked; • numerical ratings (from 1, lowest, to 5, highest) for different features (e.g., cleanliness, service, location, etc.)",
                "Below the name of the hotel, TripAdvisor displays the address of the hotel, general information (number of rooms, number of stars, short description, etc), the average overall <br>rating</br>, the TripAdvisor ranking, and an average <br>rating</br> for each feature.",
                "Figure 1 shows the page for a popular Boston hotel whose name (along with advertisements) was explicitly erased.",
                "We selected three cities for this study: Boston, Sydney and Las Vegas.",
                "For each city we considered all hotels that had at least 10 reviews, and recorded all reviews.",
                "Table 1 presents the number of hotels considered in each city, the total number of reviews recorded for each city, and the distribution of hotels with respect to the star-<br>rating</br> (as available on the TripAdvisor site).",
                "Note that not all hotels have a star-<br>rating</br>.",
                "Table 1: A summary of the data set.",
                "City # Reviews # Hotels # of Hotels with 1,2,3,4 & 5 stars Boston 3993 58 1+3+17+15+2 Sydney 1371 47 0+0+9+13+10 Las Vegas 5593 40 0+3+10+9+6 For each review we recorded the overall <br>rating</br>, the textual review (title and body of the review) and the numerical <br>rating</br> on 7 features: Rooms(R), Service(S), Cleanliness(C), Value(V), Food(F), Location(L) and Noise(N).",
                "TripAdvisor does not require users to submit anything other than the overall <br>rating</br>, hence a typical review rates few additional features, regardless of the discussion in the textual comment.",
                "Only the features Rooms(R), Service(S), Cleanliness(C) and Value(V) are rated by a significant number of users.",
                "However, we also selected the features Food(F), Location(L) and Noise(N) because they are referred to in a significant number of textual comments.",
                "For each feature we record the numerical <br>rating</br> given by the user, or 0 when the <br>rating</br> is missing.",
                "The typical length of the textual comment amounts to approximately 200 words.",
                "All data was collected by crawling the TripAdvisor site in September 2006. 2.1 Formal notation We will formally refer to a review by a tuple (r, T) where: • r = (rf ) is a vector containing the ratings rf ∈ {0, 1, . . . 5} for the features f ∈ F = {O, R, S, C, V, F, L, N}; note that the overall <br>rating</br>, rO, is abusively recorded as the <br>rating</br> for the feature Overall(O); • T is the textual comment that accompanies the review. 135 Reviews are indexed according to the variable i, such that (ri , Ti ) is the ith review in our database.",
                "Since we dont record the username of the reviewer, we will also say that the ith review in our data set was submitted by user i.",
                "When we need to consider only the reviews of a given hotel, h, we will use (ri(h) , Ti(h) ) to denote the ith review about the hotel h. 3.",
                "EVIDENCE FROM TEXTUAL COMMENTS The free textual comments associated to online reviews are a valuable source of information for understanding the reasons behind the numerical ratings left by the reviewers.",
                "The text may, for example, reveal concrete examples of aspects that the user liked or disliked, thus justifying some of the high, respectively low ratings for certain features.",
                "The text may also offer guidelines for understanding the preferences of the reviewer, and the weights of different features when computing an overall <br>rating</br>.",
                "The problem, however, is that free textual comments are difficult to read.",
                "Users are required to scroll through many reviews and read mostly repetitive information.",
                "Significant improvements would be obtained if the reviews were automatically interpreted and aggregated.",
                "Unfortunately, this seems a difficult task for computers since human users often use witty language, abbreviations, cultural specific phrases, and the figurative style.",
                "Nevertheless, several important results use the textual comments of online reviews in an automated way.",
                "Using well established natural language techniques, reviews or parts of reviews can be classified as having a positive or negative semantic orientation.",
                "Pang et al. [2] classify movie reviews into positive/negative by training three different classifiers (Naive Bayes, Maximum Entropy and SVM) using classification features based on unigrams, bigrams or part-of-speech tags.",
                "Dave et al. [4] analyze reviews from CNet and Amazon, and surprisingly show that classification features based on unigrams or bigrams perform better than higher-order n-grams.",
                "This result is challenged by Cui et al. [3] who look at large collections of reviews crawled from the web.",
                "They show that the size of the data set is important, and that bigger training sets allow classifiers to successfully use more complex classification features based on n-grams.",
                "Hu and Liu [11] also crawl the web for product reviews and automatically identify product attributes that have been discussed by reviewers.",
                "They use Wordnet to compute the semantic orientation of product evaluations and summarize user reviews by extracting positive and negative evaluations of different product features.",
                "Popescu and Etzioni [20] analyze a similar setting, but use search engine hit-counts to identify product attributes; the semantic orientation is assigned through the relaxation labeling technique.",
                "Ghose et al. [7, 8] analyze seller reviews from the Amazon secondary market to identify the different dimensions (e.g., delivery, packaging, customer support, etc.) of reputation.",
                "They parse the text, and tag the part-of-speech for each word.",
                "Frequent nouns, noun phrases and verbal phrases are identified as dimensions of reputation, while the corresponding modifiers (i.e., adjectives and adverbs) are used to derive numerical scores for each dimension.",
                "The enhanced reputation measure correlates better with the pricing information observed in the market.",
                "Pavlou and Dimoka [19] analyze eBay reviews and find that textual comments have an important impact on reputation premiums.",
                "Our approach is similar to the previously mentioned works, in the sense that we identify the aspects (i.e., hotel features) discussed by the users in the textual reviews.",
                "However, we do not compute the semantic orientation of the text, nor attempt to infer missing ratings.",
                "We define the weight, wi f , of feature f ∈ F in the text Ti associated with the review (ri , Ti ), as the fraction of Ti dedicated to discussing aspects (both positive and negative) related to feature f. We propose an elementary method to approximate the values of these weights.",
                "For each feature we manually construct the word list Lf containing approximately 50 words that are most commonly associated to the feature f. The initial words were selected from reading some of the reviews, and seeing what words coincide with discussion of which features.",
                "The list was then extended by adding all thesaurus entries that were related to the initial words.",
                "Finally, we brainstormed for missing words that would normally be associated with each of the features.",
                "Let Lf ∩Ti be the list of terms common to both Lf and Ti.",
                "Each term of Lf is counted the number of times it appears in Ti , with two exceptions: • in cases where the user submits a title to the review, we account for the title text by appending it three times to the review text Ti .",
                "The intuitive assumption is that the users opinion is more strongly reflected in the title, rather than in the body of the review.",
                "For example, many reviews are accurately summarized by titles such as Excellent service, terrible location or Bad value for money; • certain words that occur only once in the text are counted multiple times if their relevance to that feature is particularly strong.",
                "These were root words for each feature (e.g., staff is a root word for the feature Service), and were weighted either 2 or 3.",
                "Each feature was assigned up to 3 such root words, so almost all words are counted only once.",
                "The list of words for the feature Rooms is given for reference in Appendix A.",
                "The weight wi f is computed as: wi f = |Lf ∩ Ti| f∈F |Lf ∩ Ti| (1) where |Lf ∩Ti | is the number of terms common to Lf and Ti .",
                "The weight for the feature Overall was set to min{ |T i | 5000 , 1} where |Ti | is the number of character in Ti .",
                "The following is a TripAdvisor review for a Boston hotel (the name of the hotel is omitted): Ill start by saying that Im more of a Holiday Inn person than a *** type.",
                "So I get frustrated when I pay double the room rate and get half the amenities that Id get at a Hampton Inn or Holiday Inn.",
                "The location was definitely the main asset of this place.",
                "It was only a few blocks from the Hynes Center subway stop and it was easy to walk to some good restaurants in the Back Bay area.",
                "Boylston isnt far off at all.",
                "So I had no trouble with foregoing a rental car and taking the subway from the airport to the hotel and using the subway for any other travel.",
                "Otherwise, they make you pay for anything and everything. 136 And when youve already dropped $215/night on the room, that gets frustrating.The room itself was decent, about what I would expect.",
                "Staff was also average, not bad and not excellent.",
                "Again, I think youre paying for location and the ability to walk to a lot of good stuff.",
                "But I think next time Ill stay in Brookline, get more amenities, and use the subway a bit more.",
                "This numerical ratings associated to this review are rO = 3, rR = 3, rS = 3, rC = 4, rV = 2 for features Overall(O), Rooms(R), Service(S), Cleanliness(C) and Value(V) respectively.",
                "The ratings for the features Food(F), Location(L) and Noise(N) are absent (i.e., rF = rL = rN = 0).",
                "The weights wf are computed from the following lists of common terms: LR ∩ T ={room}; wR = 0.066 LS ∩ T ={3 * Staff, amenities}; wS = 0.267 LC ∩ T = ∅; wC = 0 LV ∩ T ={$, rate}; wV = 0.133 LF ∩ T ={restaurant}; wF = 0.067 LL ∩ T ={2 * center, 2 * walk, 2 * location, area}; wL = 0.467 LN ∩ T = ∅; wN = 0 The root words Staff and Center were tripled and doubled respectively.",
                "The overall weight of the textual review is wO = 0.197.",
                "These values account reasonably well for the weights of different features in the discussion of the reviewer.",
                "One point to note is that some terms in the lists Lf possess an inherent semantic orientation.",
                "For example the word grime (belonging to the list LC ) would be used most often to assert the presence, and not the absence of grime.",
                "This is unavoidable, but care was taken to ensure words from both sides of the spectrum were used.",
                "For this reason, some lists such as LR contain only nouns of objects that one would typically describe in a room (see Appendix A).",
                "The goal of this section is to analyse the influence of the weights wi f on the numerical ratings ri f .",
                "Intuitively, users who spent a lot of their time discussing a feature f (i.e., wi f is high) had something to say about their experience with regard to this feature.",
                "Obviously, feature f is important for user i.",
                "Since people tend to be more knowledgeable in the aspects they consider important, our hypothesis is that the ratings ri f (corresponding to high weights wi f ) constitute a subset of expert ratings for feature f. Figure 2 plots the distribution of the rates r i(h) C with respect to the weights w i(h) C for the cleanliness of a Las Vegas hotel, h. Here, the high ratings are restricted to the reviews that discuss little the cleanliness.",
                "Whenever cleanliness appears in the discussion, the ratings are low.",
                "Many hotels exhibit similar <br>rating</br> patterns for various features.",
                "Ratings corresponding to low weights span the whole spectrum from 1 to 5, while the ratings corresponding to high weights are more grouped together (either around good or bad ratings).",
                "We therefore make the following hypothesis: Hypothesis 1.",
                "The ratings ri f corresponding to the reviews where wi f is high, are more similar to each other than to the overall collection of ratings.",
                "To test the hypothesis, we take the entire set of reviews, and feature by feature, we compute the standard deviation of the ratings with high weights, and the standard deviation of the entire set of ratings.",
                "High weights were defined as those belonging to the upper 20% of the weight range for the corresponding feature.",
                "If Hypothesis 1 were true, the standard deviation of all ratings should be higher than the standard deviation of the ratings with high weights. 0 1 2 3 4 5 6 0 0.1 0.2 0.3 0.4 0.5 0.6 <br>rating</br> Weight Figure 2: The distribution of ratings against the weight of the cleanliness feature.",
                "We use a standard T-test to measure the significance of the results.",
                "City by city and feature by feature, Table 2 presents the average standard deviation of all ratings, and the average standard deviation of ratings with high weights.",
                "Indeed, the ratings with high weights have lower standard deviation, and the results are significant at the standard 0.05 significance threshold (although for certain cities taken independently there doesnt seem to be a significant difference, the results are significant for the entire data set).",
                "Please note that only the features O,R,S,C and V were considered, since for the others (F, L, and N) we didnt have enough ratings.",
                "Table 2: Average standard deviation for all ratings, and average standard deviation for ratings with high weights.",
                "In square brackets, the corresponding p-values for a positive difference between the two.",
                "City O R S C V all 1.189 0.998 1.144 0.935 1.123 Boston high 0.948 0.778 0.954 0.767 0.891 p-val [0.000] [0.004] [0.045] [0.080] [0.009] all 1.040 0.832 1.101 0.847 0.963 Sydney high 0.801 0.618 0.691 0.690 0.798 p-val [0.012] [0.023] [0.000] [0.377] [0.037] all 1.272 1.142 1.184 1.119 1.242 Vegas high 1.072 0.752 1.169 0.907 1.003 p-val [0.0185] [0.001] [0.918] [0.120] [0.126] Hypothesis 1 not only provides some basic understanding regarding the <br>rating</br> behavior of online users, it also suggests some ways of computing better quality estimates.",
                "We can, for example, construct a feature-by-feature quality estimate with much lower variance: for each feature we take the subset of reviews that amply discuss that feature, and output as a quality estimate the average <br>rating</br> for this subset.",
                "Initial experiments suggest that the average feature-by-feature ratings computed in this way are different from the average ratings computed on the whole data set.",
                "Given that, indeed, high weights are indicators of expert opinions, the estimates obtained in this way are more accurate than the current ones.",
                "Nevertheless, the validation of this underlying assumption requires further controlled experiments. 137 4.",
                "THE INFLUENCE OF PAST RATINGS Two important assumptions are generally made about reviews submitted to online forums.",
                "The first is that ratings truthfully reflect the quality observed by the users; the second is that reviews are independent from one another.",
                "While anecdotal evidence [9, 22] challenges the first assumption3 , in this section, we address the second.",
                "A perusal of online reviews shows that reviews are often part of discussion threads, where users make an effort to contradict, or vehemently agree with the remarks of previous users.",
                "Consider, for example, the following review: I dont understand the negative reviews... the hotel was a little dark, but that was the style.",
                "It was very artsy.",
                "Yes it was close to the freeway, but in my opinion the sound of an occasional loud car is better than hearing the ding ding of slot machines all night!",
                "The staff on-hand is FABULOUS.",
                "The waitresses are great (and *** does not deserve the bad review she got, she was 100% attentive to us! ), the bartenders are friendly and professional at the same time...",
                "Here, the user was disturbed by previous negative reports, addressed these concerns, and set about trying to correct them.",
                "Not surprisingly, his ratings were considerably higher than the average ratings up to this point.",
                "It seems that TripAdvisor users regularly read the reports submitted by previous users before booking a hotel, or before writing a review.",
                "Past reviews create some prior expectation regarding the quality of service, and this expectation has an influence on the submitted review.",
                "We believe this observation holds for most online forums.",
                "The subjective perception of quality is directly proportional to how well the actual experience meets the prior expectation, a fact confirmed by an important line of econometric and marketing research [17, 18, 16, 21].",
                "The correlation between the reviews has also been confirmed by recent research on the dynamics of online review forums [6]. 4.1 Prior Expectations We define the prior expectation of user i regarding the feature f, as the average of the previously available ratings on the feature f4 : ef (i) = j<i,r j f =0 rj f j<i,r j f =0 1 As a first hypothesis, we assert that the <br>rating</br> ri f is a function of the prior expectation ef (i): Hypothesis 2.",
                "For a given hotel and feature, given the reviews i and j such that ef (i) is high and ef (j) is low, the <br>rating</br> rj f exceeds the <br>rating</br> ri f .",
                "We define high and low expectations as those that are above, respectively below a certain cutoff value θ.",
                "The set of reviews preceded by high, respectively low expectations 3 part of Amazon reviews were recognized as strategic posts by book authors or competitors 4 if no previous ratings were assigned for feature f, ef (i) is assigned a default value of 4.",
                "Table 3: Average ratings for reviews preceded by low (first value in the cell) and high (second value in the cell) expectations.",
                "The P-values for a positive difference are given square brackets.",
                "City O R S C V 3.953 4.045 3.985 4.252 3.946 Boston 3.364 3.590 3.485 3.641 3.242 [0.011] [0.028] [0.0086] [0.0168] [0.0034] 4.284 4.358 4.064 4.530 4.428 Sydney 3.756 3.537 3.436 3.918 3.495 [0.000] [0.000] [0.035] [0.009] [0.000] 3.494 3.674 3.713 3.689 3.580 Las Vegas 3.140 3.530 2.952 3.530 3.351 [0.190] [0.529] [0.007] [0.529] [0.253] are defined as follows: Rhigh f = {ri f |ef (i) > θ} Rlow f = {ri f |ef (i) < θ} These sets are specific for each (hotel, feature) pair, and in our experiments we took θ = 4.",
                "This rather high value is close to the average <br>rating</br> across all features across all hotels, and is justified by the fact that our data set contains mostly high quality hotels.",
                "For each city, we take all hotels and compute the average ratings in the sets Rhigh f and Rlow f (see Table 3).",
                "The average <br>rating</br> amongst reviews following low prior expectations is significantly higher than the average <br>rating</br> following high expectations.",
                "As further evidence, we consider all hotels for which the function eV (i) (the expectation for the feature Value) has a high value (greater than 4) for some i, and a low value (less than 4) for some other i.",
                "Intuitively, these are the hotels for which there is a minimal degree of variation in the timely sequence of reviews: i.e., the cumulative average of ratings was at some point high and afterwards became low, or vice-versa.",
                "Such variations are observed for about half of all hotels in each city.",
                "Figure 3 plots the median (across considered hotels) <br>rating</br>, rV , when ef (i) is not more than x but greater than x − 0.5. 2.5 3 3.5 4 4.5 5 2.5 3 3.5 4 4.5 5 Medianofrating expectation Boston Sydney Vegas Figure 3: The ratings tend to decrease as the expectation increases. 138 There are two ways to interpret the function ef (i): • The expected value for feature f obtained by user i before his experience with the service, acquired by reading reports submitted by past users.",
                "In this case, an overly high value for ef (i) would drive the user to submit a negative report (or vice versa), stemming from the difference between the actual value of the service, and the inflated expectation of this value acquired before his experience. • The expected value of feature f for all subsequent visitors of the site, if user i were not to submit a report.",
                "In this case, the motivation for a negative report following an overly high value of ef is different: user i seeks to correct the expectation of future visitors to the site.",
                "Unlike the interpretation above, this does not require the user to derive an a priori expectation for the value of f. Note that neither interpretation implies that the average up to report i is inversely related to the <br>rating</br> at report i.",
                "There might exist a measure of influence exerted by past reports that pushes the user behind report i to submit ratings which to some extent conforms with past reports: a low value for ef (i) can influence user i to submit a low <br>rating</br> for feature f because, for example, he fears that submitting a high <br>rating</br> will make him out to be a person with low standards5 .",
                "This, at first, appears to contradict Hypothesis 2.",
                "However, this conformity <br>rating</br> cannot continue indefinitely: once the set of reports project a sufficiently deflated estimate for vf , future reviewers with comparatively positive impressions will seek to correct this misconception. 4.2 Impact of textual comments on quality expectation Further insight into the <br>rating</br> behavior of TripAdvisor users can be obtained by analyzing the relationship between the weights wf and the values ef (i).",
                "In particular, we examine the following hypothesis: Hypothesis 3.",
                "When a large proportion of the text of a review discusses a certain feature, the difference between the <br>rating</br> for that feature and the average <br>rating</br> up to that point tends to be large.",
                "The intuition behind this claim is that when the user is adamant about voicing his opinion regarding a certain feature, his opinion differs from the collective opinion of previous postings.",
                "This relies on the characteristic of reputation systems as feedback forums where a user is interested in projecting his opinion, with particular strength if this opinion differs from what he perceives to be the general opinion.",
                "To test Hypothesis 3 we measure the average absolute difference between the expectation ef (i) and the <br>rating</br> ri f when the weight wi f is high, respectively low.",
                "Weights are classified high or low by comparing them with certain cutoff values: wi f is low if smaller than 0.1, while wi f is high if greater than θf .",
                "Different cutoff values were used for different features: θR = 0.4, θS = 0.4, θC = 0.2, and θV = 0.7.",
                "Cleanliness has a lower cutoff since it is a feature rarely discussed; Value has a high cutoff for the opposite reason.",
                "Results are presented in Table 4. 5 The idea that negative reports can encourage further negative reporting has been suggested before [14] Table 4: Average of |ri f −ef (i)| when weights are high (first value in the cell) and low (second value in the cell) with P-values for the difference in sq. brackets.",
                "City R S C V 1.058 1.208 1.728 1.356 Boston 0.701 0.838 0.760 0.917 [0.022] [0.063] [0.000] [0.218] 1.048 1.351 1.218 1.318 Sydney 0.752 0.759 0.767 0.908 [0.179] [0.009] [0.165] [0.495] 1.184 1.378 1.472 1.642 Las Vegas 0.772 0.834 0.808 1.043 [0.071] [0.020] [0.006] [0.076] This demonstrates that when weights are unusually high, users tend to express an opinion that does not conform to the net average of previous ratings.",
                "As we might expect, for a feature that rarely was a high weight in the discussion, (e.g., cleanliness) the difference is particularly large.",
                "Even though the difference in the feature Value is quite large for Sydney, the P-value is high.",
                "This is because only few reviews discussed value heavily.",
                "The reason could be cultural or because there was less of a reason to discuss this feature. 4.3 Reporting Incentives Previous models suggest that users who are not highly opinionated will not choose to voice their opinions [12].",
                "In this section, we extend this model to account for the influence of expectations.",
                "The motivation for submitting feedback is not only due to extreme opinions, but also to the difference between the current reputation (i.e., the prior expectation of the user) and the actual experience.",
                "Such a <br>rating</br> model produces ratings that most of the time deviate from the current average <br>rating</br>.",
                "The ratings that confirm the prior expectation will rarely be submitted.",
                "We test on our data set the proportion of ratings that attempt to correct the current estimate.",
                "We define a deviant <br>rating</br> as one that deviates from the current expectation by at least some threshold θ, i.e., |ri f − ef (i)| ≥ θ.",
                "For each of the three considered cities, the following tables, show the proportion of deviant ratings for θ = 0.5 and θ = 1.",
                "Table 5: Proportion of deviant ratings with θ = 0.5 City O R S C V Boston 0.696 0.619 0.676 0.604 0.684 Sydney 0.645 0.615 0.672 0.614 0.675 Las Vegas 0.721 0.641 0.694 0.662 0.724 Table 6: Proportion of deviant ratings with θ = 1 City O R S C V Boston 0.420 0.397 0.429 0.317 0.446 Sydney 0.360 0.367 0.442 0.336 0.489 Las Vegas 0.510 0.421 0.483 0.390 0.472 The above results suggest that a large proportion of users (close to one half, even for the high threshold value θ = 1) deviate from the prior average.",
                "This reinforces the idea that users are more likely to submit a report when they believe they have something distinctive to add to the current stream of opinions for some feature.",
                "Such conclusions are in total agreement with prior evidence that the distribution of reports often follows bi-modal, U-shaped distributions. 139 5.",
                "MODELLING THE BEHAVIOR OF RATERS To account for the observations described in the previous sections, we propose a model for the behavior of the users when submitting online reviews.",
                "For a given hotel, we make the assumption that the quality experienced by the users is normally distributed around some value vf , which represents the objective quality offered by the hotel on the feature f. The <br>rating</br> submitted by user i on feature f is: ˆri f = δf vi f + (1 − δf ) · sign vi f − ef (i) c + d(vi f , ef (i)|wi f ) (2) where: • vi f is the (unknown) quality actually experienced by the user. vi f is assumed normally distributed around some value vf ; • δf ∈ [0, 1] can be seen as a measure of the bias when reporting feedback.",
                "High values reflect the fact that users rate objectively, without being influenced by prior expectations.",
                "The value of δf may depend on various factors; we fix one value for each feature f; • c is a constant between 1 and 5; • wi f is the weight of feature f in the textual comment of review i, computed according to Eq. (1); • d(vi f , ef (i)|wi f ) is a distance function between the expectation and the observation of user i.",
                "The distance function satisfies the following properties: - d(y, z|w) ≥ 0 for all y, z ∈ [0, 5], w ∈ [0, 1]; - |d(y, z|w)| < |d(z, x|w)| if |y − z| < |z − x|; - |d(y, z|w1)| < |d(y, z|w2)| if w1 < w2; - c + d(vf , ef (i)|wi f ) ∈ [1, 5]; The second term of Eq. (2) encodes the bias of the <br>rating</br>.",
                "The higher the distance between the true observation vi f and the function ef , the higher the bias. 5.1 Model Validation We use the data set of TripAdvisor reviews to validate the behavior model presented above.",
                "We split for convenience the <br>rating</br> values in three ranges: bad (B = {1, 2}), indifferent (I = {3, 4}), and good (G = {5}), and perform the following two tests: • First, we will use our model to predict the ratings that have extremal values.",
                "For every hotel, we take the sequence of reports, and whenever we encounter a <br>rating</br> that is either good or bad (but not indifferent) we try to predict it using Eq. (2) • Second, instead of predicting the value of extremal ratings, we try to classify them as either good or bad.",
                "For every hotel we take the sequence of reports, and for each report (regardless of it value) we classify it as being good or bad However, to perform these tests, we need to estimate the objective value, vf , that is the average of the true quality observations, vi f .",
                "The algorithm we are using is based on the intuition that the amount of conformity <br>rating</br> is minimized.",
                "In other words, the value vf should be such that as often as possible, bad ratings follow expectations above vf and good ratings follow expectations below vf .",
                "Formally, we define the sets: Γ1 = {i|ef (i) < vf and ri f ∈ B}; Γ2 = {i|ef (i) > vf and ri f ∈ G}; that correspond to irregularities where even though the expectation at point i is lower than the delivered value, the <br>rating</br> is poor, and vice versa.",
                "We define vf as the value that minimize these union of the two sets: vf = arg min vf |Γ1 ∪ Γ2| (3) In Eq. (2) we replace vi f by the value vf computed in Eq. (3), and use the following distance function: d(vf , ef (i)|wi f ) = |vf − ef (i)| vf − ef (i) |vf 2 − ef (i)2 | · (1 + 2wi f ); The constant c ∈ I was set to min{max{ef (i), 3}}, 4}.",
                "The values for δf were fixed at {0.7, 0.7, 0.8, 0.7, 0.6} for the features {Overall, Rooms, Service, Cleanliness, Value} respectively.",
                "The weights are computed as described in Section 3.",
                "As a first experiment, we take the sets of extremal ratings {ri f |ri f /∈ I} for each hotel and feature.",
                "For every such <br>rating</br>, ri f , we try to estimate it by computing ˆri f using Eq. (2).",
                "We compare this estimator with the one obtained by simply averaging the ratings over all hotels and features: i.e., ¯rf = j,r j f =0 rj f j,r j f =0 1 ; Table 7 presents the ratio between the root mean square error (RMSE) when using ˆri f and ¯rf to estimate the actual ratings.",
                "In all cases the estimate produced by our model is better than the simple average.",
                "Table 7: Average of RMSE(ˆrf ) RMSE(¯rf ) City O R S C V Boston 0.987 0.849 0.879 0.776 0.913 Sydney 0.927 0.817 0.826 0.720 0.681 Las Vegas 0.952 0.870 0.881 0.947 0.904 As a second experiment, we try to distinguish the sets Bf = {i|ri f ∈ B} and Gf = {i|ri f ∈ G} of bad, respectively good ratings on the feature f. For example, we compute the set Bf using the following classifier (called σ): ri f ∈ Bf (σf (i) = 1) ⇔ ˆri f ≤ 4; Tables 8, 9 and 10 present the Precision(p), Recall(r) and s = 2pr p+r for classifier σ, and compares it with a naive majority classifier, τ, τf (i) = 1 ⇔ |Bf | ≥ |Gf |: We see that recall is always higher for σ and precision is usually slightly worse.",
                "For the s metric σ tends to add a 140 Table 8: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Boston O R S C V p 0.678 0.670 0.573 0.545 0.610 σ r 0.626 0.659 0.619 0.612 0.694 s 0.651 0.665 0.595 0.577 0.609 p 0.684 0.706 0.647 0.611 0.633 τ r 0.597 0.541 0.410 0.383 0.562 s 0.638 0.613 0.502 0.471 0.595 Table 9: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Las Vegas O R S C V p 0.654 0.748 0.592 0.712 0.583 σ r 0.608 0.536 0.791 0.474 0.610 s 0.630 0.624 0.677 0.569 0.596 p 0.685 0.761 0.621 0.748 0.606 τ r 0.542 0.505 0.767 0.445 0.441 s 0.605 0.607 0.670 0.558 0.511 1-20% improvement over τ, much higher in some cases for hotels in Sydney.",
                "This is likely because Sydney reviews are more positive than those of the American cities and cases where the number of bad reviews exceeded the number of good ones are rare.",
                "Replacing the test algorithm with one that plays a 1 with probability equal to the proportion of bad reviews improves its results for this city, but it is still outperformed by around 80%. 6.",
                "SUMMARY OF RESULTS AND CONCLUSION The goal of this paper is to explore the factors that drive a user to submit a particular <br>rating</br>, rather than the incentives that encouraged him to submit a report in the first place.",
                "For that we use two additional sources of information besides the vector of numerical ratings: first we look at the textual comments that accompany the reviews, and second we consider the reports that have been previously submitted by other users.",
                "Using simple natural language processing algorithms, we were able to establish a correlation between the weight of a certain feature in the textual comment accompanying the review, and the noise present in the numerical <br>rating</br>.",
                "Specifically, it seems that users who discuss amply a certain feature are likely to agree on a common <br>rating</br>.",
                "This observation allows the construction of feature-by-feature estimators of quality that have a lower variance, and are hopefully less noisy.",
                "Nevertheless, further evidence is required to support the intuition that ratings corresponding to high weights are expert opinions that deserve to be given higher priority when computing estimates of quality.",
                "Second, we emphasize the dependence of ratings on previous reports.",
                "Previous reports create an expectation of quality which affects the subjective perception of the user.",
                "We validate two facts about the hotel reviews we collected from TripAdvisor: First, the ratings following low expectations (where the expectation is computed as the average of the previous reports) are likely to be higher than the ratings Table 10: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Sydney O R S C V p 0.650 0.463 0.544 0.550 0.580 σ r 0.234 0.378 0.571 0.169 0.592 s 0.343 0.452 0.557 0.259 0.586 p 0.562 0.615 0.600 0.500 0.600 τ r 0.054 0.098 0.101 0.015 0.175 s 0.098 0.168 0.172 0.030 0.271 following high expectations.",
                "Intuitively, the perception of quality (and consequently the <br>rating</br>) depends on how well the actual experience of the user meets her expectation.",
                "Second, we include evidence from the textual comments, and find that when users devote a large fraction of the text to discussing a certain feature, they are likely to motivate a divergent <br>rating</br> (i.e., a <br>rating</br> that does not conform to the prior expectation).",
                "Intuitively, this supports the hypothesis that review forums act as discussion groups where users are keen on presenting and motivating their own opinion.",
                "We have captured the empirical evidence in a behavior model that predicts the ratings submitted by the users.",
                "The final <br>rating</br> depends, as expected, on the true observation, and on the gap between the observation and the expectation.",
                "The gap tends to have a bigger influence when an important fraction of the textual comment is dedicated to discussing a certain feature.",
                "The proposed model was validated on the empirical data and provides better estimates of the ratings actually submitted.",
                "One assumption that we make is about the existence of an objective quality value vf for the feature f. This is rarely true, especially over large spans of time.",
                "Other explanations might account for the correlation of ratings with past reports.",
                "For example, if ef (i) reflects the true value of f at a point in time, the difference in the ratings following high and low expectations can be explained by hotel revenue models that are maximized when the value is modified accordingly.",
                "However, the idea that variation in ratings is not primarily a function of variation in value turns out to be a useful one.",
                "Our approach to approximate this elusive objective value is by no means perfect, but conforms neatly to the idea behind the model.",
                "A natural direction for future work is to examine concrete applications of our results.",
                "Significant improvements of quality estimates are likely to be obtained by incorporating all empirical evidence about <br>rating</br> behavior.",
                "Exactly how different factors affect the decisions of the users is not clear.",
                "The answer might depend on the particular application, context and culture. 7.",
                "REFERENCES [1] A. Admati and P. Pfleiderer.",
                "Noisytalk.com: Broadcasting opinions in a noisy environment.",
                "Working Paper 1670R, Stanford University, 2000. [2] P. B., L. Lee, and S. Vaithyanathan.",
                "Thumbs up? sentiment classification using machine learning techniques.",
                "In Proceedings of the EMNLP-02, the Conference on Empirical Methods in Natural Language Processing, 2002. [3] H. Cui, V. Mittal, and M. Datar.",
                "Comparative 141 Experiments on Sentiment Classification for Online Product Reviews.",
                "In Proceedings of AAAI, 2006. [4] K. Dave, S. Lawrence, and D. Pennock.",
                "Mining the peanut gallery:opinion extraction and semantic classification of product reviews.",
                "In Proceedings of the 12th International Conference on the World Wide Web (WWW03), 2003. [5] C. Dellarocas, N. Awad, and X. Zhang.",
                "Exploring the Value of Online Product Ratings in Revenue Forecasting: The Case of Motion Pictures.",
                "Working paper, 2006. [6] C. Forman, A. Ghose, and B. Wiesenfeld.",
                "A Multi-Level Examination of the Impact of Social Identities on Economic Transactions in Electronic Markets.",
                "Available at SSRN: http://ssrn.com/abstract=918978, July 2006. [7] A. Ghose, P. Ipeirotis, and A. Sundararajan.",
                "Reputation Premiums in Electronic Peer-to-Peer Markets: Analyzing Textual Feedback and Network Structure.",
                "In Third Workshop on Economics of Peer-to-Peer Systems, (P2PECON), 2005. [8] A. Ghose, P. Ipeirotis, and A. Sundararajan.",
                "The Dimensions of Reputation in electronic Markets.",
                "Working Paper CeDER-06-02, New York University, 2006. [9] A. Harmon.",
                "Amazon Glitch Unmasks War of Reviewers.",
                "The New York Times, February 14, 2004. [10] D. Houser and J. Wooders.",
                "Reputation in Auctions: Theory and Evidence from eBay.",
                "Journal of Economics and Management Strategy, 15:353-369, 2006. [11] M. Hu and B. Liu.",
                "Mining and summarizing customer reviews.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD04), 2004. [12] N. Hu, P. Pavlou, and J. Zhang.",
                "Can Online Reviews Reveal a Products True Quality?",
                "In Proceedings of ACM Conference on Electronic Commerce (EC 06), 2006. [13] K. Kalyanam and S. McIntyre.",
                "Return on reputation in online auction market.",
                "Working Paper 02/03-10-WP, Leavey School of Business, Santa Clara University., 2001. [14] L. Khopkar and P. Resnick.",
                "Self-Selection, Slipping, Salvaging, Slacking, and Stoning: the Impacts of Negative Feedback at eBay.",
                "In Proceedings of ACM Conference on Electronic Commerce (EC 05), 2005. [15] M. Melnik and J. Alm.",
                "Does a sellers reputation matter? evidence from ebay auctions.",
                "Journal of Industrial Economics, 50(3):337-350, 2002. [16] R. Olshavsky and J. Miller.",
                "Consumer Expectations, Product Performance and Perceived Product Quality.",
                "Journal of Marketing Research, 9:19-21, February 1972. [17] A. Parasuraman, V. Zeithaml, and L. Berry.",
                "A Conceptual Model of Service Quality and Its Implications for Future Research.",
                "Journal of Marketing, 49:41-50, 1985. [18] A. Parasuraman, V. Zeithaml, and L. Berry.",
                "SERVQUAL: A Multiple-Item Scale for Measuring Consumer Perceptions of Service Quality.",
                "Journal of Retailing, 64:12-40, 1988. [19] P. Pavlou and A. Dimoka.",
                "The Nature and Role of Feedback Text Comments in Online Marketplaces: Implications for Trust Building, Price Premiums, and Seller Differentiation.",
                "Information Systems Research, 17(4):392-414, 2006. [20] A. Popescu and O. Etzioni.",
                "Extracting product features and opinions from reviews.",
                "In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, 2005. [21] R. Teas.",
                "Expectations, Performance Evaluation, and Consumers Perceptions of Quality.",
                "Journal of Marketing, 57:18-34, 1993. [22] E. White.",
                "Chatting a Singer Up the Pop Charts.",
                "The Wall Street Journal, October 15, 1999.",
                "APPENDIX A.",
                "LIST OF WORDS, LR, ASSOCIATED TO THE FEATURE ROOMS All words serve as prefixes: room, space, interior, decor, ambiance, atmosphere, comfort, bath, toilet, bed, building, wall, window, private, temperature, sheet, linen, pillow, hot, water, cold, water, shower, lobby, furniture, carpet, air, condition, mattress, layout, design, mirror, ceiling, lighting, lamp, sofa, chair, dresser, wardrobe, closet 142"
            ],
            "original_annotated_samples": [
                "We first show that groups of users who amply discuss a certain feature are more likely to agree on a common <br>rating</br> for that feature.",
                "Second, we show that a users <br>rating</br> partly reflects the difference between true quality and prior expectation of quality as inferred from previous reviews.",
                "Both give us a less noisy way to produce <br>rating</br> estimates and reveal the reasons behind user bias.",
                "Improving the way we aggregate the information available from online reviews requires a deep understanding of the underlying factors that bias the <br>rating</br> behavior of users.",
                "We find that users who comment more on the same feature are more likely to agree on a common numerical <br>rating</br> for that particular feature."
            ],
            "translated_annotated_samples": [
                "Primero demostramos que los grupos de usuarios que discuten ampliamente una característica específica tienen más probabilidades de ponerse de acuerdo en una <br>calificación</br> común para esa característica.",
                "Segundo, demostramos que la <br>calificación</br> de un usuario refleja en parte la diferencia entre la calidad real y la expectativa previa de calidad, tal como se infiere de revisiones anteriores.",
                "Ambos nos ofrecen una forma menos ruidosa de producir estimaciones de <br>calificación</br> y revelar las razones detrás del sesgo del usuario.",
                "Mejorar la forma en que agregamos la información disponible de las reseñas en línea requiere un profundo entendimiento de los factores subyacentes que sesgan el comportamiento de <br>calificación</br> de los usuarios.",
                "Observamos que los usuarios que comentan más sobre la misma característica tienen más probabilidades de estar de acuerdo en una <br>calificación</br> numérica común para esa característica en particular."
            ],
            "translated_text": "La comprensión del comportamiento del usuario en la presentación de comentarios en línea. Trabajos anteriores han demostrado que la información contradictoria y los sesgos subyacentes de los usuarios dificultan juzgar el verdadero valor de un servicio. En este artículo, investigamos los factores subyacentes que influyen en el comportamiento del usuario al informar retroalimentación. Examinamos dos fuentes de información además de las calificaciones numéricas: la evidencia lingüística del comentario textual que acompaña a una reseña y los patrones en la secuencia temporal de los informes. Primero demostramos que los grupos de usuarios que discuten ampliamente una característica específica tienen más probabilidades de ponerse de acuerdo en una <br>calificación</br> común para esa característica. Segundo, demostramos que la <br>calificación</br> de un usuario refleja en parte la diferencia entre la calidad real y la expectativa previa de calidad, tal como se infiere de revisiones anteriores. Ambos nos ofrecen una forma menos ruidosa de producir estimaciones de <br>calificación</br> y revelar las razones detrás del sesgo del usuario. Nuestras hipótesis fueron validadas por evidencia estadística de reseñas de hoteles en el sitio web de TripAdvisor. Categorías y Descriptores de Asignaturas J.4 [Ciencias Sociales y del Comportamiento]: Economía Términos Generales Economía, Experimentación, Confiabilidad 1. MOTIVACIONES La expansión de internet ha hecho posible que los foros de retroalimentación en línea (o mecanismos de reputación) se conviertan en un canal importante para el boca a boca sobre productos, servicios u otros tipos de interacciones comerciales. Numerosos estudios empíricos [10, 15, 13, 5] muestran que los compradores consideran seriamente los comentarios en línea al tomar decisiones de compra, y están dispuestos a pagar primas por reputación por productos o servicios que tienen una buena reputación. Sin embargo, un análisis reciente plantea preguntas importantes sobre la capacidad de los foros existentes para reflejar la verdadera calidad de un producto. En ausencia de incentivos claros, los usuarios con una perspectiva moderada no se molestarán en expresar sus opiniones, lo que conduce a una muestra no representativa de reseñas. Por ejemplo, [12, 1] muestran que las calificaciones de libros o CDs en Amazon1 siguen con gran probabilidad distribuciones bimodales en forma de U, donde la mayoría de las calificaciones son o muy buenas o muy malas. Los experimentos controlados, por otro lado, revelan opiniones sobre los mismos elementos que están distribuidas de forma normal. Bajo estas circunstancias, utilizar la media aritmética para predecir la calidad (como la mayoría de los foros realmente hacen) proporciona al usuario típico un estimador con una alta varianza que a menudo es incorrecto. Mejorar la forma en que agregamos la información disponible de las reseñas en línea requiere un profundo entendimiento de los factores subyacentes que sesgan el comportamiento de <br>calificación</br> de los usuarios. Hu et al. [12] proponen el Modelo Brag-and-Moan donde los usuarios califican solo si su utilidad del producto (extraída de una distribución normal) cae fuera de un intervalo mediano. Los autores concluyen que el modelo explica la distribución empírica de los informes y ofrece ideas sobre formas más inteligentes de estimar la verdadera calidad del producto. En el presente artículo ampliamos esta línea de investigación e intentamos explicar más hechos sobre el comportamiento de los usuarios al informar retroalimentación en línea. Usando reseñas reales de hoteles del sitio web TripAdvisor2, consideramos dos fuentes adicionales de información además de las calificaciones numéricas básicas enviadas por los usuarios. La primera es una evidencia lingüística simple proveniente de la revisión textual que suele acompañar a las calificaciones numéricas. Utilizamos técnicas de minería de texto similares a [7] y [3], sin embargo, solo estamos interesados en identificar qué aspectos del servicio está discutiendo el usuario, sin calcular la orientación semántica del texto. Observamos que los usuarios que comentan más sobre la misma característica tienen más probabilidades de estar de acuerdo en una <br>calificación</br> numérica común para esa característica en particular. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "great probability bi-modal": {
            "translated_key": "distribuciones bimodales",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Understanding User Behavior in Online Feedback Reporting Arjun Talwar Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland arjun@math.stanford.edu Radu Jurca Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland radu.jurca@epfl.ch Boi Faltings Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland boi.faltings@epfl.ch ABSTRACT Online reviews have become increasingly popular as a way to judge the quality of various products and services.",
                "Previous work has demonstrated that contradictory reporting and underlying user biases make judging the true worth of a service difficult.",
                "In this paper, we investigate underlying factors that influence user behavior when reporting feedback.",
                "We look at two sources of information besides numerical ratings: linguistic evidence from the textual comment accompanying a review, and patterns in the time sequence of reports.",
                "We first show that groups of users who amply discuss a certain feature are more likely to agree on a common rating for that feature.",
                "Second, we show that a users rating partly reflects the difference between true quality and prior expectation of quality as inferred from previous reviews.",
                "Both give us a less noisy way to produce rating estimates and reveal the reasons behind user bias.",
                "Our hypotheses were validated by statistical evidence from hotel reviews on the TripAdvisor website.",
                "Categories and Subject Descriptors J.4 [Social and Behavioral Sciences]: Economics General Terms Economics, Experimentation, Reliability 1.",
                "MOTIVATIONS The spread of the internet has made it possible for online feedback forums (or reputation mechanisms) to become an important channel for Word-of-mouth regarding products, services or other types of commercial interactions.",
                "Numerous empirical studies [10, 15, 13, 5] show that buyers seriously consider online feedback when making purchasing decisions, and are willing to pay reputation premiums for products or services that have a good reputation.",
                "Recent analysis, however, raises important questions regarding the ability of existing forums to reflect the real quality of a product.",
                "In the absence of clear incentives, users with a moderate outlook will not bother to voice their opinions, which leads to an unrepresentative sample of reviews.",
                "For example, [12, 1] show that Amazon1 ratings of books or CDs follow with <br>great probability bi-modal</br>, U-shaped distributions where most of the ratings are either very good, or very bad.",
                "Controlled experiments, on the other hand, reveal opinions on the same items that are normally distributed.",
                "Under these circumstances, using the arithmetic mean to predict quality (as most forums actually do) gives the typical user an estimator with high variance that is often false.",
                "Improving the way we aggregate the information available from online reviews requires a deep understanding of the underlying factors that bias the rating behavior of users.",
                "Hu et al. [12] propose the Brag-and-Moan Model where users rate only if their utility of the product (drawn from a normal distribution) falls outside a median interval.",
                "The authors conclude that the model explains the empirical distribution of reports, and offers insights into smarter ways of estimating the true quality of the product.",
                "In the present paper we extend this line of research, and attempt to explain further facts about the behavior of users when reporting online feedback.",
                "Using actual hotel reviews from the TripAdvisor2 website, we consider two additional sources of information besides the basic numerical ratings submitted by users.",
                "The first is simple linguistic evidence from the textual review that usually accompanies the numerical ratings.",
                "We use text-mining techniques similar to [7] and [3], however, we are only interested in identifying what aspects of the service the user is discussing, without computing the semantic orientation of the text.",
                "We find that users who comment more on the same feature are more likely to agree on a common numerical rating for that particular feature.",
                "Intuitively, lengthy comments reveal the importance of the feature to the user.",
                "Since people tend to be more knowledgeable in the aspects they consider important, users who discuss a given feature in more details might be assumed to have more authority in evaluating that feature.",
                "Second we investigate the relationship between a review 1 http://www.amazon.com 2 http://www.tripadvisor.com/ 134 Figure 1: The TripAdvisor page displaying reviews for a popular Boston hotel.",
                "Name of hotel and advertisements were deliberatively erased. and the reviews that preceded it.",
                "A perusal of online reviews shows that ratings are often part of discussion threads, where one post is not necessarily independent of other posts.",
                "One may see, for example, users who make an effort to contradict, or vehemently agree with, the remarks of previous users.",
                "By analyzing the time sequence of reports, we conclude that past reviews influence the future reports, as they create some prior expectation regarding the quality of service.",
                "The subjective perception of the user is influenced by the gap between the prior expectation and the actual performance of the service [17, 18, 16, 21] which will later reflect in the users rating.",
                "We propose a model that captures the dependence of ratings on prior expectations, and validate it using the empirical data we collected.",
                "Both results can be used to improve the way reputation mechanisms aggregate the information from individual reviews.",
                "Our first result can be used to determine a featureby-feature estimate of quality, where for each feature, a different subset of reviews (i.e., those with lengthy comments of that feature) is considered.",
                "The second leads to an algorithm that outputs a more precise estimate of the real quality. 2.",
                "THE DATA SET We use in this paper real hotel reviews collected from the popular travel site TripAdvisor.",
                "TripAdvisor indexes hotels from cities across the world, along with reviews written by travelers.",
                "Users can search the site by giving the hotels name and location (optional).",
                "The reviews for a given hotel are displayed as a list (ordered from the most recent to the oldest), with 5 reviews per page.",
                "The reviews contain: • information about the author of the review (e.g., dates of stay, username of the reviewer, location of the reviewer); • the overall rating (from 1, lowest, to 5, highest); • a textual review containing a title for the review, free comments, and the main things the reviewer liked and disliked; • numerical ratings (from 1, lowest, to 5, highest) for different features (e.g., cleanliness, service, location, etc.)",
                "Below the name of the hotel, TripAdvisor displays the address of the hotel, general information (number of rooms, number of stars, short description, etc), the average overall rating, the TripAdvisor ranking, and an average rating for each feature.",
                "Figure 1 shows the page for a popular Boston hotel whose name (along with advertisements) was explicitly erased.",
                "We selected three cities for this study: Boston, Sydney and Las Vegas.",
                "For each city we considered all hotels that had at least 10 reviews, and recorded all reviews.",
                "Table 1 presents the number of hotels considered in each city, the total number of reviews recorded for each city, and the distribution of hotels with respect to the star-rating (as available on the TripAdvisor site).",
                "Note that not all hotels have a star-rating.",
                "Table 1: A summary of the data set.",
                "City # Reviews # Hotels # of Hotels with 1,2,3,4 & 5 stars Boston 3993 58 1+3+17+15+2 Sydney 1371 47 0+0+9+13+10 Las Vegas 5593 40 0+3+10+9+6 For each review we recorded the overall rating, the textual review (title and body of the review) and the numerical rating on 7 features: Rooms(R), Service(S), Cleanliness(C), Value(V), Food(F), Location(L) and Noise(N).",
                "TripAdvisor does not require users to submit anything other than the overall rating, hence a typical review rates few additional features, regardless of the discussion in the textual comment.",
                "Only the features Rooms(R), Service(S), Cleanliness(C) and Value(V) are rated by a significant number of users.",
                "However, we also selected the features Food(F), Location(L) and Noise(N) because they are referred to in a significant number of textual comments.",
                "For each feature we record the numerical rating given by the user, or 0 when the rating is missing.",
                "The typical length of the textual comment amounts to approximately 200 words.",
                "All data was collected by crawling the TripAdvisor site in September 2006. 2.1 Formal notation We will formally refer to a review by a tuple (r, T) where: • r = (rf ) is a vector containing the ratings rf ∈ {0, 1, . . . 5} for the features f ∈ F = {O, R, S, C, V, F, L, N}; note that the overall rating, rO, is abusively recorded as the rating for the feature Overall(O); • T is the textual comment that accompanies the review. 135 Reviews are indexed according to the variable i, such that (ri , Ti ) is the ith review in our database.",
                "Since we dont record the username of the reviewer, we will also say that the ith review in our data set was submitted by user i.",
                "When we need to consider only the reviews of a given hotel, h, we will use (ri(h) , Ti(h) ) to denote the ith review about the hotel h. 3.",
                "EVIDENCE FROM TEXTUAL COMMENTS The free textual comments associated to online reviews are a valuable source of information for understanding the reasons behind the numerical ratings left by the reviewers.",
                "The text may, for example, reveal concrete examples of aspects that the user liked or disliked, thus justifying some of the high, respectively low ratings for certain features.",
                "The text may also offer guidelines for understanding the preferences of the reviewer, and the weights of different features when computing an overall rating.",
                "The problem, however, is that free textual comments are difficult to read.",
                "Users are required to scroll through many reviews and read mostly repetitive information.",
                "Significant improvements would be obtained if the reviews were automatically interpreted and aggregated.",
                "Unfortunately, this seems a difficult task for computers since human users often use witty language, abbreviations, cultural specific phrases, and the figurative style.",
                "Nevertheless, several important results use the textual comments of online reviews in an automated way.",
                "Using well established natural language techniques, reviews or parts of reviews can be classified as having a positive or negative semantic orientation.",
                "Pang et al. [2] classify movie reviews into positive/negative by training three different classifiers (Naive Bayes, Maximum Entropy and SVM) using classification features based on unigrams, bigrams or part-of-speech tags.",
                "Dave et al. [4] analyze reviews from CNet and Amazon, and surprisingly show that classification features based on unigrams or bigrams perform better than higher-order n-grams.",
                "This result is challenged by Cui et al. [3] who look at large collections of reviews crawled from the web.",
                "They show that the size of the data set is important, and that bigger training sets allow classifiers to successfully use more complex classification features based on n-grams.",
                "Hu and Liu [11] also crawl the web for product reviews and automatically identify product attributes that have been discussed by reviewers.",
                "They use Wordnet to compute the semantic orientation of product evaluations and summarize user reviews by extracting positive and negative evaluations of different product features.",
                "Popescu and Etzioni [20] analyze a similar setting, but use search engine hit-counts to identify product attributes; the semantic orientation is assigned through the relaxation labeling technique.",
                "Ghose et al. [7, 8] analyze seller reviews from the Amazon secondary market to identify the different dimensions (e.g., delivery, packaging, customer support, etc.) of reputation.",
                "They parse the text, and tag the part-of-speech for each word.",
                "Frequent nouns, noun phrases and verbal phrases are identified as dimensions of reputation, while the corresponding modifiers (i.e., adjectives and adverbs) are used to derive numerical scores for each dimension.",
                "The enhanced reputation measure correlates better with the pricing information observed in the market.",
                "Pavlou and Dimoka [19] analyze eBay reviews and find that textual comments have an important impact on reputation premiums.",
                "Our approach is similar to the previously mentioned works, in the sense that we identify the aspects (i.e., hotel features) discussed by the users in the textual reviews.",
                "However, we do not compute the semantic orientation of the text, nor attempt to infer missing ratings.",
                "We define the weight, wi f , of feature f ∈ F in the text Ti associated with the review (ri , Ti ), as the fraction of Ti dedicated to discussing aspects (both positive and negative) related to feature f. We propose an elementary method to approximate the values of these weights.",
                "For each feature we manually construct the word list Lf containing approximately 50 words that are most commonly associated to the feature f. The initial words were selected from reading some of the reviews, and seeing what words coincide with discussion of which features.",
                "The list was then extended by adding all thesaurus entries that were related to the initial words.",
                "Finally, we brainstormed for missing words that would normally be associated with each of the features.",
                "Let Lf ∩Ti be the list of terms common to both Lf and Ti.",
                "Each term of Lf is counted the number of times it appears in Ti , with two exceptions: • in cases where the user submits a title to the review, we account for the title text by appending it three times to the review text Ti .",
                "The intuitive assumption is that the users opinion is more strongly reflected in the title, rather than in the body of the review.",
                "For example, many reviews are accurately summarized by titles such as Excellent service, terrible location or Bad value for money; • certain words that occur only once in the text are counted multiple times if their relevance to that feature is particularly strong.",
                "These were root words for each feature (e.g., staff is a root word for the feature Service), and were weighted either 2 or 3.",
                "Each feature was assigned up to 3 such root words, so almost all words are counted only once.",
                "The list of words for the feature Rooms is given for reference in Appendix A.",
                "The weight wi f is computed as: wi f = |Lf ∩ Ti| f∈F |Lf ∩ Ti| (1) where |Lf ∩Ti | is the number of terms common to Lf and Ti .",
                "The weight for the feature Overall was set to min{ |T i | 5000 , 1} where |Ti | is the number of character in Ti .",
                "The following is a TripAdvisor review for a Boston hotel (the name of the hotel is omitted): Ill start by saying that Im more of a Holiday Inn person than a *** type.",
                "So I get frustrated when I pay double the room rate and get half the amenities that Id get at a Hampton Inn or Holiday Inn.",
                "The location was definitely the main asset of this place.",
                "It was only a few blocks from the Hynes Center subway stop and it was easy to walk to some good restaurants in the Back Bay area.",
                "Boylston isnt far off at all.",
                "So I had no trouble with foregoing a rental car and taking the subway from the airport to the hotel and using the subway for any other travel.",
                "Otherwise, they make you pay for anything and everything. 136 And when youve already dropped $215/night on the room, that gets frustrating.The room itself was decent, about what I would expect.",
                "Staff was also average, not bad and not excellent.",
                "Again, I think youre paying for location and the ability to walk to a lot of good stuff.",
                "But I think next time Ill stay in Brookline, get more amenities, and use the subway a bit more.",
                "This numerical ratings associated to this review are rO = 3, rR = 3, rS = 3, rC = 4, rV = 2 for features Overall(O), Rooms(R), Service(S), Cleanliness(C) and Value(V) respectively.",
                "The ratings for the features Food(F), Location(L) and Noise(N) are absent (i.e., rF = rL = rN = 0).",
                "The weights wf are computed from the following lists of common terms: LR ∩ T ={room}; wR = 0.066 LS ∩ T ={3 * Staff, amenities}; wS = 0.267 LC ∩ T = ∅; wC = 0 LV ∩ T ={$, rate}; wV = 0.133 LF ∩ T ={restaurant}; wF = 0.067 LL ∩ T ={2 * center, 2 * walk, 2 * location, area}; wL = 0.467 LN ∩ T = ∅; wN = 0 The root words Staff and Center were tripled and doubled respectively.",
                "The overall weight of the textual review is wO = 0.197.",
                "These values account reasonably well for the weights of different features in the discussion of the reviewer.",
                "One point to note is that some terms in the lists Lf possess an inherent semantic orientation.",
                "For example the word grime (belonging to the list LC ) would be used most often to assert the presence, and not the absence of grime.",
                "This is unavoidable, but care was taken to ensure words from both sides of the spectrum were used.",
                "For this reason, some lists such as LR contain only nouns of objects that one would typically describe in a room (see Appendix A).",
                "The goal of this section is to analyse the influence of the weights wi f on the numerical ratings ri f .",
                "Intuitively, users who spent a lot of their time discussing a feature f (i.e., wi f is high) had something to say about their experience with regard to this feature.",
                "Obviously, feature f is important for user i.",
                "Since people tend to be more knowledgeable in the aspects they consider important, our hypothesis is that the ratings ri f (corresponding to high weights wi f ) constitute a subset of expert ratings for feature f. Figure 2 plots the distribution of the rates r i(h) C with respect to the weights w i(h) C for the cleanliness of a Las Vegas hotel, h. Here, the high ratings are restricted to the reviews that discuss little the cleanliness.",
                "Whenever cleanliness appears in the discussion, the ratings are low.",
                "Many hotels exhibit similar rating patterns for various features.",
                "Ratings corresponding to low weights span the whole spectrum from 1 to 5, while the ratings corresponding to high weights are more grouped together (either around good or bad ratings).",
                "We therefore make the following hypothesis: Hypothesis 1.",
                "The ratings ri f corresponding to the reviews where wi f is high, are more similar to each other than to the overall collection of ratings.",
                "To test the hypothesis, we take the entire set of reviews, and feature by feature, we compute the standard deviation of the ratings with high weights, and the standard deviation of the entire set of ratings.",
                "High weights were defined as those belonging to the upper 20% of the weight range for the corresponding feature.",
                "If Hypothesis 1 were true, the standard deviation of all ratings should be higher than the standard deviation of the ratings with high weights. 0 1 2 3 4 5 6 0 0.1 0.2 0.3 0.4 0.5 0.6 Rating Weight Figure 2: The distribution of ratings against the weight of the cleanliness feature.",
                "We use a standard T-test to measure the significance of the results.",
                "City by city and feature by feature, Table 2 presents the average standard deviation of all ratings, and the average standard deviation of ratings with high weights.",
                "Indeed, the ratings with high weights have lower standard deviation, and the results are significant at the standard 0.05 significance threshold (although for certain cities taken independently there doesnt seem to be a significant difference, the results are significant for the entire data set).",
                "Please note that only the features O,R,S,C and V were considered, since for the others (F, L, and N) we didnt have enough ratings.",
                "Table 2: Average standard deviation for all ratings, and average standard deviation for ratings with high weights.",
                "In square brackets, the corresponding p-values for a positive difference between the two.",
                "City O R S C V all 1.189 0.998 1.144 0.935 1.123 Boston high 0.948 0.778 0.954 0.767 0.891 p-val [0.000] [0.004] [0.045] [0.080] [0.009] all 1.040 0.832 1.101 0.847 0.963 Sydney high 0.801 0.618 0.691 0.690 0.798 p-val [0.012] [0.023] [0.000] [0.377] [0.037] all 1.272 1.142 1.184 1.119 1.242 Vegas high 1.072 0.752 1.169 0.907 1.003 p-val [0.0185] [0.001] [0.918] [0.120] [0.126] Hypothesis 1 not only provides some basic understanding regarding the rating behavior of online users, it also suggests some ways of computing better quality estimates.",
                "We can, for example, construct a feature-by-feature quality estimate with much lower variance: for each feature we take the subset of reviews that amply discuss that feature, and output as a quality estimate the average rating for this subset.",
                "Initial experiments suggest that the average feature-by-feature ratings computed in this way are different from the average ratings computed on the whole data set.",
                "Given that, indeed, high weights are indicators of expert opinions, the estimates obtained in this way are more accurate than the current ones.",
                "Nevertheless, the validation of this underlying assumption requires further controlled experiments. 137 4.",
                "THE INFLUENCE OF PAST RATINGS Two important assumptions are generally made about reviews submitted to online forums.",
                "The first is that ratings truthfully reflect the quality observed by the users; the second is that reviews are independent from one another.",
                "While anecdotal evidence [9, 22] challenges the first assumption3 , in this section, we address the second.",
                "A perusal of online reviews shows that reviews are often part of discussion threads, where users make an effort to contradict, or vehemently agree with the remarks of previous users.",
                "Consider, for example, the following review: I dont understand the negative reviews... the hotel was a little dark, but that was the style.",
                "It was very artsy.",
                "Yes it was close to the freeway, but in my opinion the sound of an occasional loud car is better than hearing the ding ding of slot machines all night!",
                "The staff on-hand is FABULOUS.",
                "The waitresses are great (and *** does not deserve the bad review she got, she was 100% attentive to us! ), the bartenders are friendly and professional at the same time...",
                "Here, the user was disturbed by previous negative reports, addressed these concerns, and set about trying to correct them.",
                "Not surprisingly, his ratings were considerably higher than the average ratings up to this point.",
                "It seems that TripAdvisor users regularly read the reports submitted by previous users before booking a hotel, or before writing a review.",
                "Past reviews create some prior expectation regarding the quality of service, and this expectation has an influence on the submitted review.",
                "We believe this observation holds for most online forums.",
                "The subjective perception of quality is directly proportional to how well the actual experience meets the prior expectation, a fact confirmed by an important line of econometric and marketing research [17, 18, 16, 21].",
                "The correlation between the reviews has also been confirmed by recent research on the dynamics of online review forums [6]. 4.1 Prior Expectations We define the prior expectation of user i regarding the feature f, as the average of the previously available ratings on the feature f4 : ef (i) = j<i,r j f =0 rj f j<i,r j f =0 1 As a first hypothesis, we assert that the rating ri f is a function of the prior expectation ef (i): Hypothesis 2.",
                "For a given hotel and feature, given the reviews i and j such that ef (i) is high and ef (j) is low, the rating rj f exceeds the rating ri f .",
                "We define high and low expectations as those that are above, respectively below a certain cutoff value θ.",
                "The set of reviews preceded by high, respectively low expectations 3 part of Amazon reviews were recognized as strategic posts by book authors or competitors 4 if no previous ratings were assigned for feature f, ef (i) is assigned a default value of 4.",
                "Table 3: Average ratings for reviews preceded by low (first value in the cell) and high (second value in the cell) expectations.",
                "The P-values for a positive difference are given square brackets.",
                "City O R S C V 3.953 4.045 3.985 4.252 3.946 Boston 3.364 3.590 3.485 3.641 3.242 [0.011] [0.028] [0.0086] [0.0168] [0.0034] 4.284 4.358 4.064 4.530 4.428 Sydney 3.756 3.537 3.436 3.918 3.495 [0.000] [0.000] [0.035] [0.009] [0.000] 3.494 3.674 3.713 3.689 3.580 Las Vegas 3.140 3.530 2.952 3.530 3.351 [0.190] [0.529] [0.007] [0.529] [0.253] are defined as follows: Rhigh f = {ri f |ef (i) > θ} Rlow f = {ri f |ef (i) < θ} These sets are specific for each (hotel, feature) pair, and in our experiments we took θ = 4.",
                "This rather high value is close to the average rating across all features across all hotels, and is justified by the fact that our data set contains mostly high quality hotels.",
                "For each city, we take all hotels and compute the average ratings in the sets Rhigh f and Rlow f (see Table 3).",
                "The average rating amongst reviews following low prior expectations is significantly higher than the average rating following high expectations.",
                "As further evidence, we consider all hotels for which the function eV (i) (the expectation for the feature Value) has a high value (greater than 4) for some i, and a low value (less than 4) for some other i.",
                "Intuitively, these are the hotels for which there is a minimal degree of variation in the timely sequence of reviews: i.e., the cumulative average of ratings was at some point high and afterwards became low, or vice-versa.",
                "Such variations are observed for about half of all hotels in each city.",
                "Figure 3 plots the median (across considered hotels) rating, rV , when ef (i) is not more than x but greater than x − 0.5. 2.5 3 3.5 4 4.5 5 2.5 3 3.5 4 4.5 5 Medianofrating expectation Boston Sydney Vegas Figure 3: The ratings tend to decrease as the expectation increases. 138 There are two ways to interpret the function ef (i): • The expected value for feature f obtained by user i before his experience with the service, acquired by reading reports submitted by past users.",
                "In this case, an overly high value for ef (i) would drive the user to submit a negative report (or vice versa), stemming from the difference between the actual value of the service, and the inflated expectation of this value acquired before his experience. • The expected value of feature f for all subsequent visitors of the site, if user i were not to submit a report.",
                "In this case, the motivation for a negative report following an overly high value of ef is different: user i seeks to correct the expectation of future visitors to the site.",
                "Unlike the interpretation above, this does not require the user to derive an a priori expectation for the value of f. Note that neither interpretation implies that the average up to report i is inversely related to the rating at report i.",
                "There might exist a measure of influence exerted by past reports that pushes the user behind report i to submit ratings which to some extent conforms with past reports: a low value for ef (i) can influence user i to submit a low rating for feature f because, for example, he fears that submitting a high rating will make him out to be a person with low standards5 .",
                "This, at first, appears to contradict Hypothesis 2.",
                "However, this conformity rating cannot continue indefinitely: once the set of reports project a sufficiently deflated estimate for vf , future reviewers with comparatively positive impressions will seek to correct this misconception. 4.2 Impact of textual comments on quality expectation Further insight into the rating behavior of TripAdvisor users can be obtained by analyzing the relationship between the weights wf and the values ef (i).",
                "In particular, we examine the following hypothesis: Hypothesis 3.",
                "When a large proportion of the text of a review discusses a certain feature, the difference between the rating for that feature and the average rating up to that point tends to be large.",
                "The intuition behind this claim is that when the user is adamant about voicing his opinion regarding a certain feature, his opinion differs from the collective opinion of previous postings.",
                "This relies on the characteristic of reputation systems as feedback forums where a user is interested in projecting his opinion, with particular strength if this opinion differs from what he perceives to be the general opinion.",
                "To test Hypothesis 3 we measure the average absolute difference between the expectation ef (i) and the rating ri f when the weight wi f is high, respectively low.",
                "Weights are classified high or low by comparing them with certain cutoff values: wi f is low if smaller than 0.1, while wi f is high if greater than θf .",
                "Different cutoff values were used for different features: θR = 0.4, θS = 0.4, θC = 0.2, and θV = 0.7.",
                "Cleanliness has a lower cutoff since it is a feature rarely discussed; Value has a high cutoff for the opposite reason.",
                "Results are presented in Table 4. 5 The idea that negative reports can encourage further negative reporting has been suggested before [14] Table 4: Average of |ri f −ef (i)| when weights are high (first value in the cell) and low (second value in the cell) with P-values for the difference in sq. brackets.",
                "City R S C V 1.058 1.208 1.728 1.356 Boston 0.701 0.838 0.760 0.917 [0.022] [0.063] [0.000] [0.218] 1.048 1.351 1.218 1.318 Sydney 0.752 0.759 0.767 0.908 [0.179] [0.009] [0.165] [0.495] 1.184 1.378 1.472 1.642 Las Vegas 0.772 0.834 0.808 1.043 [0.071] [0.020] [0.006] [0.076] This demonstrates that when weights are unusually high, users tend to express an opinion that does not conform to the net average of previous ratings.",
                "As we might expect, for a feature that rarely was a high weight in the discussion, (e.g., cleanliness) the difference is particularly large.",
                "Even though the difference in the feature Value is quite large for Sydney, the P-value is high.",
                "This is because only few reviews discussed value heavily.",
                "The reason could be cultural or because there was less of a reason to discuss this feature. 4.3 Reporting Incentives Previous models suggest that users who are not highly opinionated will not choose to voice their opinions [12].",
                "In this section, we extend this model to account for the influence of expectations.",
                "The motivation for submitting feedback is not only due to extreme opinions, but also to the difference between the current reputation (i.e., the prior expectation of the user) and the actual experience.",
                "Such a rating model produces ratings that most of the time deviate from the current average rating.",
                "The ratings that confirm the prior expectation will rarely be submitted.",
                "We test on our data set the proportion of ratings that attempt to correct the current estimate.",
                "We define a deviant rating as one that deviates from the current expectation by at least some threshold θ, i.e., |ri f − ef (i)| ≥ θ.",
                "For each of the three considered cities, the following tables, show the proportion of deviant ratings for θ = 0.5 and θ = 1.",
                "Table 5: Proportion of deviant ratings with θ = 0.5 City O R S C V Boston 0.696 0.619 0.676 0.604 0.684 Sydney 0.645 0.615 0.672 0.614 0.675 Las Vegas 0.721 0.641 0.694 0.662 0.724 Table 6: Proportion of deviant ratings with θ = 1 City O R S C V Boston 0.420 0.397 0.429 0.317 0.446 Sydney 0.360 0.367 0.442 0.336 0.489 Las Vegas 0.510 0.421 0.483 0.390 0.472 The above results suggest that a large proportion of users (close to one half, even for the high threshold value θ = 1) deviate from the prior average.",
                "This reinforces the idea that users are more likely to submit a report when they believe they have something distinctive to add to the current stream of opinions for some feature.",
                "Such conclusions are in total agreement with prior evidence that the distribution of reports often follows bi-modal, U-shaped distributions. 139 5.",
                "MODELLING THE BEHAVIOR OF RATERS To account for the observations described in the previous sections, we propose a model for the behavior of the users when submitting online reviews.",
                "For a given hotel, we make the assumption that the quality experienced by the users is normally distributed around some value vf , which represents the objective quality offered by the hotel on the feature f. The rating submitted by user i on feature f is: ˆri f = δf vi f + (1 − δf ) · sign vi f − ef (i) c + d(vi f , ef (i)|wi f ) (2) where: • vi f is the (unknown) quality actually experienced by the user. vi f is assumed normally distributed around some value vf ; • δf ∈ [0, 1] can be seen as a measure of the bias when reporting feedback.",
                "High values reflect the fact that users rate objectively, without being influenced by prior expectations.",
                "The value of δf may depend on various factors; we fix one value for each feature f; • c is a constant between 1 and 5; • wi f is the weight of feature f in the textual comment of review i, computed according to Eq. (1); • d(vi f , ef (i)|wi f ) is a distance function between the expectation and the observation of user i.",
                "The distance function satisfies the following properties: - d(y, z|w) ≥ 0 for all y, z ∈ [0, 5], w ∈ [0, 1]; - |d(y, z|w)| < |d(z, x|w)| if |y − z| < |z − x|; - |d(y, z|w1)| < |d(y, z|w2)| if w1 < w2; - c + d(vf , ef (i)|wi f ) ∈ [1, 5]; The second term of Eq. (2) encodes the bias of the rating.",
                "The higher the distance between the true observation vi f and the function ef , the higher the bias. 5.1 Model Validation We use the data set of TripAdvisor reviews to validate the behavior model presented above.",
                "We split for convenience the rating values in three ranges: bad (B = {1, 2}), indifferent (I = {3, 4}), and good (G = {5}), and perform the following two tests: • First, we will use our model to predict the ratings that have extremal values.",
                "For every hotel, we take the sequence of reports, and whenever we encounter a rating that is either good or bad (but not indifferent) we try to predict it using Eq. (2) • Second, instead of predicting the value of extremal ratings, we try to classify them as either good or bad.",
                "For every hotel we take the sequence of reports, and for each report (regardless of it value) we classify it as being good or bad However, to perform these tests, we need to estimate the objective value, vf , that is the average of the true quality observations, vi f .",
                "The algorithm we are using is based on the intuition that the amount of conformity rating is minimized.",
                "In other words, the value vf should be such that as often as possible, bad ratings follow expectations above vf and good ratings follow expectations below vf .",
                "Formally, we define the sets: Γ1 = {i|ef (i) < vf and ri f ∈ B}; Γ2 = {i|ef (i) > vf and ri f ∈ G}; that correspond to irregularities where even though the expectation at point i is lower than the delivered value, the rating is poor, and vice versa.",
                "We define vf as the value that minimize these union of the two sets: vf = arg min vf |Γ1 ∪ Γ2| (3) In Eq. (2) we replace vi f by the value vf computed in Eq. (3), and use the following distance function: d(vf , ef (i)|wi f ) = |vf − ef (i)| vf − ef (i) |vf 2 − ef (i)2 | · (1 + 2wi f ); The constant c ∈ I was set to min{max{ef (i), 3}}, 4}.",
                "The values for δf were fixed at {0.7, 0.7, 0.8, 0.7, 0.6} for the features {Overall, Rooms, Service, Cleanliness, Value} respectively.",
                "The weights are computed as described in Section 3.",
                "As a first experiment, we take the sets of extremal ratings {ri f |ri f /∈ I} for each hotel and feature.",
                "For every such rating, ri f , we try to estimate it by computing ˆri f using Eq. (2).",
                "We compare this estimator with the one obtained by simply averaging the ratings over all hotels and features: i.e., ¯rf = j,r j f =0 rj f j,r j f =0 1 ; Table 7 presents the ratio between the root mean square error (RMSE) when using ˆri f and ¯rf to estimate the actual ratings.",
                "In all cases the estimate produced by our model is better than the simple average.",
                "Table 7: Average of RMSE(ˆrf ) RMSE(¯rf ) City O R S C V Boston 0.987 0.849 0.879 0.776 0.913 Sydney 0.927 0.817 0.826 0.720 0.681 Las Vegas 0.952 0.870 0.881 0.947 0.904 As a second experiment, we try to distinguish the sets Bf = {i|ri f ∈ B} and Gf = {i|ri f ∈ G} of bad, respectively good ratings on the feature f. For example, we compute the set Bf using the following classifier (called σ): ri f ∈ Bf (σf (i) = 1) ⇔ ˆri f ≤ 4; Tables 8, 9 and 10 present the Precision(p), Recall(r) and s = 2pr p+r for classifier σ, and compares it with a naive majority classifier, τ, τf (i) = 1 ⇔ |Bf | ≥ |Gf |: We see that recall is always higher for σ and precision is usually slightly worse.",
                "For the s metric σ tends to add a 140 Table 8: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Boston O R S C V p 0.678 0.670 0.573 0.545 0.610 σ r 0.626 0.659 0.619 0.612 0.694 s 0.651 0.665 0.595 0.577 0.609 p 0.684 0.706 0.647 0.611 0.633 τ r 0.597 0.541 0.410 0.383 0.562 s 0.638 0.613 0.502 0.471 0.595 Table 9: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Las Vegas O R S C V p 0.654 0.748 0.592 0.712 0.583 σ r 0.608 0.536 0.791 0.474 0.610 s 0.630 0.624 0.677 0.569 0.596 p 0.685 0.761 0.621 0.748 0.606 τ r 0.542 0.505 0.767 0.445 0.441 s 0.605 0.607 0.670 0.558 0.511 1-20% improvement over τ, much higher in some cases for hotels in Sydney.",
                "This is likely because Sydney reviews are more positive than those of the American cities and cases where the number of bad reviews exceeded the number of good ones are rare.",
                "Replacing the test algorithm with one that plays a 1 with probability equal to the proportion of bad reviews improves its results for this city, but it is still outperformed by around 80%. 6.",
                "SUMMARY OF RESULTS AND CONCLUSION The goal of this paper is to explore the factors that drive a user to submit a particular rating, rather than the incentives that encouraged him to submit a report in the first place.",
                "For that we use two additional sources of information besides the vector of numerical ratings: first we look at the textual comments that accompany the reviews, and second we consider the reports that have been previously submitted by other users.",
                "Using simple natural language processing algorithms, we were able to establish a correlation between the weight of a certain feature in the textual comment accompanying the review, and the noise present in the numerical rating.",
                "Specifically, it seems that users who discuss amply a certain feature are likely to agree on a common rating.",
                "This observation allows the construction of feature-by-feature estimators of quality that have a lower variance, and are hopefully less noisy.",
                "Nevertheless, further evidence is required to support the intuition that ratings corresponding to high weights are expert opinions that deserve to be given higher priority when computing estimates of quality.",
                "Second, we emphasize the dependence of ratings on previous reports.",
                "Previous reports create an expectation of quality which affects the subjective perception of the user.",
                "We validate two facts about the hotel reviews we collected from TripAdvisor: First, the ratings following low expectations (where the expectation is computed as the average of the previous reports) are likely to be higher than the ratings Table 10: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Sydney O R S C V p 0.650 0.463 0.544 0.550 0.580 σ r 0.234 0.378 0.571 0.169 0.592 s 0.343 0.452 0.557 0.259 0.586 p 0.562 0.615 0.600 0.500 0.600 τ r 0.054 0.098 0.101 0.015 0.175 s 0.098 0.168 0.172 0.030 0.271 following high expectations.",
                "Intuitively, the perception of quality (and consequently the rating) depends on how well the actual experience of the user meets her expectation.",
                "Second, we include evidence from the textual comments, and find that when users devote a large fraction of the text to discussing a certain feature, they are likely to motivate a divergent rating (i.e., a rating that does not conform to the prior expectation).",
                "Intuitively, this supports the hypothesis that review forums act as discussion groups where users are keen on presenting and motivating their own opinion.",
                "We have captured the empirical evidence in a behavior model that predicts the ratings submitted by the users.",
                "The final rating depends, as expected, on the true observation, and on the gap between the observation and the expectation.",
                "The gap tends to have a bigger influence when an important fraction of the textual comment is dedicated to discussing a certain feature.",
                "The proposed model was validated on the empirical data and provides better estimates of the ratings actually submitted.",
                "One assumption that we make is about the existence of an objective quality value vf for the feature f. This is rarely true, especially over large spans of time.",
                "Other explanations might account for the correlation of ratings with past reports.",
                "For example, if ef (i) reflects the true value of f at a point in time, the difference in the ratings following high and low expectations can be explained by hotel revenue models that are maximized when the value is modified accordingly.",
                "However, the idea that variation in ratings is not primarily a function of variation in value turns out to be a useful one.",
                "Our approach to approximate this elusive objective value is by no means perfect, but conforms neatly to the idea behind the model.",
                "A natural direction for future work is to examine concrete applications of our results.",
                "Significant improvements of quality estimates are likely to be obtained by incorporating all empirical evidence about rating behavior.",
                "Exactly how different factors affect the decisions of the users is not clear.",
                "The answer might depend on the particular application, context and culture. 7.",
                "REFERENCES [1] A. Admati and P. Pfleiderer.",
                "Noisytalk.com: Broadcasting opinions in a noisy environment.",
                "Working Paper 1670R, Stanford University, 2000. [2] P. B., L. Lee, and S. Vaithyanathan.",
                "Thumbs up? sentiment classification using machine learning techniques.",
                "In Proceedings of the EMNLP-02, the Conference on Empirical Methods in Natural Language Processing, 2002. [3] H. Cui, V. Mittal, and M. Datar.",
                "Comparative 141 Experiments on Sentiment Classification for Online Product Reviews.",
                "In Proceedings of AAAI, 2006. [4] K. Dave, S. Lawrence, and D. Pennock.",
                "Mining the peanut gallery:opinion extraction and semantic classification of product reviews.",
                "In Proceedings of the 12th International Conference on the World Wide Web (WWW03), 2003. [5] C. Dellarocas, N. Awad, and X. Zhang.",
                "Exploring the Value of Online Product Ratings in Revenue Forecasting: The Case of Motion Pictures.",
                "Working paper, 2006. [6] C. Forman, A. Ghose, and B. Wiesenfeld.",
                "A Multi-Level Examination of the Impact of Social Identities on Economic Transactions in Electronic Markets.",
                "Available at SSRN: http://ssrn.com/abstract=918978, July 2006. [7] A. Ghose, P. Ipeirotis, and A. Sundararajan.",
                "Reputation Premiums in Electronic Peer-to-Peer Markets: Analyzing Textual Feedback and Network Structure.",
                "In Third Workshop on Economics of Peer-to-Peer Systems, (P2PECON), 2005. [8] A. Ghose, P. Ipeirotis, and A. Sundararajan.",
                "The Dimensions of Reputation in electronic Markets.",
                "Working Paper CeDER-06-02, New York University, 2006. [9] A. Harmon.",
                "Amazon Glitch Unmasks War of Reviewers.",
                "The New York Times, February 14, 2004. [10] D. Houser and J. Wooders.",
                "Reputation in Auctions: Theory and Evidence from eBay.",
                "Journal of Economics and Management Strategy, 15:353-369, 2006. [11] M. Hu and B. Liu.",
                "Mining and summarizing customer reviews.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD04), 2004. [12] N. Hu, P. Pavlou, and J. Zhang.",
                "Can Online Reviews Reveal a Products True Quality?",
                "In Proceedings of ACM Conference on Electronic Commerce (EC 06), 2006. [13] K. Kalyanam and S. McIntyre.",
                "Return on reputation in online auction market.",
                "Working Paper 02/03-10-WP, Leavey School of Business, Santa Clara University., 2001. [14] L. Khopkar and P. Resnick.",
                "Self-Selection, Slipping, Salvaging, Slacking, and Stoning: the Impacts of Negative Feedback at eBay.",
                "In Proceedings of ACM Conference on Electronic Commerce (EC 05), 2005. [15] M. Melnik and J. Alm.",
                "Does a sellers reputation matter? evidence from ebay auctions.",
                "Journal of Industrial Economics, 50(3):337-350, 2002. [16] R. Olshavsky and J. Miller.",
                "Consumer Expectations, Product Performance and Perceived Product Quality.",
                "Journal of Marketing Research, 9:19-21, February 1972. [17] A. Parasuraman, V. Zeithaml, and L. Berry.",
                "A Conceptual Model of Service Quality and Its Implications for Future Research.",
                "Journal of Marketing, 49:41-50, 1985. [18] A. Parasuraman, V. Zeithaml, and L. Berry.",
                "SERVQUAL: A Multiple-Item Scale for Measuring Consumer Perceptions of Service Quality.",
                "Journal of Retailing, 64:12-40, 1988. [19] P. Pavlou and A. Dimoka.",
                "The Nature and Role of Feedback Text Comments in Online Marketplaces: Implications for Trust Building, Price Premiums, and Seller Differentiation.",
                "Information Systems Research, 17(4):392-414, 2006. [20] A. Popescu and O. Etzioni.",
                "Extracting product features and opinions from reviews.",
                "In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, 2005. [21] R. Teas.",
                "Expectations, Performance Evaluation, and Consumers Perceptions of Quality.",
                "Journal of Marketing, 57:18-34, 1993. [22] E. White.",
                "Chatting a Singer Up the Pop Charts.",
                "The Wall Street Journal, October 15, 1999.",
                "APPENDIX A.",
                "LIST OF WORDS, LR, ASSOCIATED TO THE FEATURE ROOMS All words serve as prefixes: room, space, interior, decor, ambiance, atmosphere, comfort, bath, toilet, bed, building, wall, window, private, temperature, sheet, linen, pillow, hot, water, cold, water, shower, lobby, furniture, carpet, air, condition, mattress, layout, design, mirror, ceiling, lighting, lamp, sofa, chair, dresser, wardrobe, closet 142"
            ],
            "original_annotated_samples": [
                "For example, [12, 1] show that Amazon1 ratings of books or CDs follow with <br>great probability bi-modal</br>, U-shaped distributions where most of the ratings are either very good, or very bad."
            ],
            "translated_annotated_samples": [
                "Por ejemplo, [12, 1] muestran que las calificaciones de libros o CDs en Amazon1 siguen con gran probabilidad <br>distribuciones bimodales</br> en forma de U, donde la mayoría de las calificaciones son o muy buenas o muy malas."
            ],
            "translated_text": "La comprensión del comportamiento del usuario en la presentación de comentarios en línea. Trabajos anteriores han demostrado que la información contradictoria y los sesgos subyacentes de los usuarios dificultan juzgar el verdadero valor de un servicio. En este artículo, investigamos los factores subyacentes que influyen en el comportamiento del usuario al informar retroalimentación. Examinamos dos fuentes de información además de las calificaciones numéricas: la evidencia lingüística del comentario textual que acompaña a una reseña y los patrones en la secuencia temporal de los informes. Primero demostramos que los grupos de usuarios que discuten ampliamente una característica específica tienen más probabilidades de ponerse de acuerdo en una calificación común para esa característica. Segundo, demostramos que la calificación de un usuario refleja en parte la diferencia entre la calidad real y la expectativa previa de calidad, tal como se infiere de revisiones anteriores. Ambos nos ofrecen una forma menos ruidosa de producir estimaciones de calificación y revelar las razones detrás del sesgo del usuario. Nuestras hipótesis fueron validadas por evidencia estadística de reseñas de hoteles en el sitio web de TripAdvisor. Categorías y Descriptores de Asignaturas J.4 [Ciencias Sociales y del Comportamiento]: Economía Términos Generales Economía, Experimentación, Confiabilidad 1. MOTIVACIONES La expansión de internet ha hecho posible que los foros de retroalimentación en línea (o mecanismos de reputación) se conviertan en un canal importante para el boca a boca sobre productos, servicios u otros tipos de interacciones comerciales. Numerosos estudios empíricos [10, 15, 13, 5] muestran que los compradores consideran seriamente los comentarios en línea al tomar decisiones de compra, y están dispuestos a pagar primas por reputación por productos o servicios que tienen una buena reputación. Sin embargo, un análisis reciente plantea preguntas importantes sobre la capacidad de los foros existentes para reflejar la verdadera calidad de un producto. En ausencia de incentivos claros, los usuarios con una perspectiva moderada no se molestarán en expresar sus opiniones, lo que conduce a una muestra no representativa de reseñas. Por ejemplo, [12, 1] muestran que las calificaciones de libros o CDs en Amazon1 siguen con gran probabilidad <br>distribuciones bimodales</br> en forma de U, donde la mayoría de las calificaciones son o muy buenas o muy malas. Los experimentos controlados, por otro lado, revelan opiniones sobre los mismos elementos que están distribuidas de forma normal. Bajo estas circunstancias, utilizar la media aritmética para predecir la calidad (como la mayoría de los foros realmente hacen) proporciona al usuario típico un estimador con una alta varianza que a menudo es incorrecto. Mejorar la forma en que agregamos la información disponible de las reseñas en línea requiere un profundo entendimiento de los factores subyacentes que sesgan el comportamiento de calificación de los usuarios. Hu et al. [12] proponen el Modelo Brag-and-Moan donde los usuarios califican solo si su utilidad del producto (extraída de una distribución normal) cae fuera de un intervalo mediano. Los autores concluyen que el modelo explica la distribución empírica de los informes y ofrece ideas sobre formas más inteligentes de estimar la verdadera calidad del producto. En el presente artículo ampliamos esta línea de investigación e intentamos explicar más hechos sobre el comportamiento de los usuarios al informar retroalimentación en línea. Usando reseñas reales de hoteles del sitio web TripAdvisor2, consideramos dos fuentes adicionales de información además de las calificaciones numéricas básicas enviadas por los usuarios. La primera es una evidencia lingüística simple proveniente de la revisión textual que suele acompañar a las calificaciones numéricas. Utilizamos técnicas de minería de texto similares a [7] y [3], sin embargo, solo estamos interesados en identificar qué aspectos del servicio está discutiendo el usuario, sin calcular la orientación semántica del texto. Observamos que los usuarios que comentan más sobre la misma característica tienen más probabilidades de estar de acuerdo en una calificación numérica común para esa característica en particular. Intuitivamente, los comentarios extensos revelan la importancia de la característica para el usuario. Dado que las personas tienden a ser más conocedoras en los aspectos que consideran importantes, se podría asumir que los usuarios que discuten una característica específica en más detalle tienen más autoridad para evaluar esa característica. Segundo investigamos la relación entre una reseña 1 http://www.amazon.com 2 http://www.tripadvisor.com/ 134 Figura 1: La página de TripAdvisor que muestra reseñas de un popular hotel en Boston. El nombre del hotel y los anuncios fueron deliberadamente borrados, al igual que las reseñas que lo precedieron. Un examen de las reseñas en línea muestra que las calificaciones a menudo forman parte de los hilos de discusión, donde una publicación no es necesariamente independiente de otras publicaciones. Uno puede ver, por ejemplo, usuarios que se esfuerzan por contradecir o estar vehementemente de acuerdo con los comentarios de usuarios anteriores. Al analizar la secuencia temporal de informes, concluimos que las revisiones pasadas influyen en los informes futuros, ya que crean cierta expectativa previa sobre la calidad del servicio. La percepción subjetiva del usuario está influenciada por la brecha entre la expectativa previa y el rendimiento real del servicio [17, 18, 16, 21], lo cual se reflejará más tarde en la calificación de los usuarios. Proponemos un modelo que captura la dependencia de las calificaciones en las expectativas previas, y lo validamos utilizando los datos empíricos que recopilamos. Ambos resultados pueden ser utilizados para mejorar la forma en que los mecanismos de reputación agregan la información de las revisiones individuales. Nuestro primer resultado se puede utilizar para determinar una estimación de calidad característica por característica, donde para cada característica se considera un subconjunto diferente de reseñas (es decir, aquellas con comentarios extensos sobre esa característica). El segundo conduce a un algoritmo que produce una estimación más precisa de la calidad real. 2. El conjunto de datos que utilizamos en este artículo son reseñas reales de hoteles recopiladas del popular sitio de viajes TripAdvisor. TripAdvisor indexa hoteles de ciudades de todo el mundo, junto con reseñas escritas por viajeros. Los usuarios pueden buscar en el sitio proporcionando el nombre del hotel y la ubicación (opcional). Las reseñas de un hotel dado se muestran como una lista (ordenadas de la más reciente a la más antigua), con 5 reseñas por página. Las reseñas contienen: • información sobre el autor de la reseña (por ejemplo, fechas de estancia, nombre de usuario del revisor, ubicación del revisor); • la calificación general (de 1, la más baja, a 5, la más alta); • una reseña textual que incluye un título para la reseña, comentarios libres y las principales cosas que al revisor le gustaron y no le gustaron; • calificaciones numéricas (de 1, la más baja, a 5, la más alta) para diferentes características (por ejemplo, limpieza, servicio, ubicación, etc.). Debajo del nombre del hotel, TripAdvisor muestra la dirección del hotel, información general (número de habitaciones, número de estrellas, breve descripción, etc.), la calificación promedio general, el ranking de TripAdvisor y una calificación promedio para cada característica. La Figura 1 muestra la página de un hotel popular en Boston cuyo nombre (junto con los anuncios) fue explícitamente borrado. Seleccionamos tres ciudades para este estudio: Boston, Sídney y Las Vegas. Para cada ciudad consideramos todos los hoteles que tenían al menos 10 reseñas, y registramos todas las reseñas. La Tabla 1 presenta el número de hoteles considerados en cada ciudad, el número total de reseñas registradas para cada ciudad y la distribución de hoteles con respecto a la calificación por estrellas (según está disponible en el sitio de TripAdvisor). Ten en cuenta que no todos los hoteles tienen una clasificación por estrellas. Tabla 1: Un resumen del conjunto de datos. Para cada reseña, registramos la calificación general, la reseña textual (título y cuerpo de la reseña) y la calificación numérica en 7 características: Habitaciones (R), Servicio (S), Limpieza (C), Valor (V), Comida (F), Ubicación (L) y Ruido (N). TripAdvisor no requiere que los usuarios envíen nada más que la calificación general, por lo tanto, una reseña típica evalúa pocas características adicionales, independientemente de la discusión en el comentario textual. Solo las características Habitaciones (R), Servicio (S), Limpieza (C) y Valor (V) son evaluadas por un número significativo de usuarios. Sin embargo, también seleccionamos las características Comida (F), Ubicación (L) y Ruido (N) porque son mencionadas en un número significativo de comentarios textuales. Para cada característica registramos la calificación numérica dada por el usuario, o 0 cuando la calificación está ausente. La longitud típica del comentario textual equivale aproximadamente a 200 palabras. Todos los datos fueron recopilados rastreando el sitio de TripAdvisor en septiembre de 2006. 2.1 Notación formal Nos referiremos formalmente a una reseña mediante una tupla (r, T) donde: • r = (rf ) es un vector que contiene las calificaciones rf ∈ {0, 1, . . . 5} para las características f ∈ F = {O, R, S, C, V, F, L, N}; cabe destacar que la calificación general, rO, se registra de forma abusiva como la calificación para la característica General(O); • T es el comentario textual que acompaña a la reseña. Se indexan 135 reseñas de acuerdo con la variable i, de modo que (ri , Ti ) es la i-ésima reseña en nuestra base de datos. Dado que no registramos el nombre de usuario del revisor, también diremos que la i-ésima reseña en nuestro conjunto de datos fue enviada por el usuario i. Cuando necesitemos considerar solo las reseñas de un hotel dado, h, usaremos (ri(h), Ti(h)) para denotar la i-ésima reseña sobre el hotel h. 3. EVIDENCIA DE COMENTARIOS TEXTUALES Los comentarios textuales gratuitos asociados a las reseñas en línea son una fuente valiosa de información para comprender las razones detrás de las calificaciones numéricas dejadas por los revisores. El texto puede, por ejemplo, revelar ejemplos concretos de aspectos que al usuario le gustaron o no le gustaron, justificando así algunas de las calificaciones altas o bajas respectivamente para ciertas características. El texto también puede ofrecer pautas para comprender las preferencias del revisor y los pesos de las diferentes características al calcular una calificación general. El problema, sin embargo, es que los comentarios textuales libres son difíciles de leer. Los usuarios deben desplazarse a través de muchas reseñas y leer principalmente información repetitiva. Se obtendrían mejoras significativas si las reseñas fueran interpretadas y agregadas automáticamente. Desafortunadamente, esta parece ser una tarea difícil para las computadoras ya que los usuarios humanos a menudo utilizan un lenguaje ingenioso, abreviaturas, frases específicas de la cultura y un estilo figurativo. Sin embargo, varios resultados importantes utilizan los comentarios textuales de las reseñas en línea de forma automatizada. Utilizando técnicas de lenguaje natural bien establecidas, las reseñas o partes de las reseñas pueden ser clasificadas como tener una orientación semántica positiva o negativa. Pang et al. [2] clasifican las críticas de películas en positivas/negativas entrenando tres clasificadores diferentes (Naive Bayes, Máxima Entropía y SVM) utilizando características de clasificación basadas en unigramas, bigramas o etiquetas de partes del discurso. Dave et al. [4] analizan reseñas de CNet y Amazon, y sorprendentemente demuestran que las características de clasificación basadas en unigramas o bigramas funcionan mejor que los n-gramas de orden superior. Este resultado es desafiado por Cui et al. [3], quienes examinan grandes colecciones de reseñas recopiladas de la web. Muestran que el tamaño del conjunto de datos es importante, y que conjuntos de entrenamiento más grandes permiten a los clasificadores utilizar con éxito características de clasificación más complejas basadas en n-gramas. Hu y Liu [11] también rastrean la web en busca de reseñas de productos e identifican automáticamente los atributos de productos que han sido discutidos por los revisores. Utilizan Wordnet para calcular la orientación semántica de las evaluaciones de productos y resumir las reseñas de usuarios extrayendo evaluaciones positivas y negativas de diferentes características del producto. Popescu y Etzioni [20] analizan un escenario similar, pero utilizan el recuento de resultados de búsqueda en motores de búsqueda para identificar atributos de productos; la orientación semántica se asigna a través de la técnica de etiquetado de relajación. Ghose et al. [7, 8] analizan las reseñas de vendedores del mercado secundario de Amazon para identificar las diferentes dimensiones (por ejemplo, entrega, empaque, atención al cliente, etc.) de la reputación. Analizan el texto y etiquetan la parte de la oración de cada palabra. Los sustantivos, frases nominales y frases verbales frecuentes se identifican como dimensiones de la reputación, mientras que los modificadores correspondientes (es decir, adjetivos y adverbios) se utilizan para derivar puntuaciones numéricas para cada dimensión. La medida de reputación mejorada se correlaciona mejor con la información de precios observada en el mercado. Pavlou y Dimoka [19] analizan las reseñas de eBay y encuentran que los comentarios textuales tienen un impacto importante en las primas de reputación. Nuestro enfoque es similar a los trabajos mencionados anteriormente, en el sentido de que identificamos los aspectos (es decir, las características del hotel) discutidos por los usuarios en las reseñas textuales. Sin embargo, no calculamos la orientación semántica del texto, ni intentamos inferir las calificaciones faltantes. Definimos el peso, wi f, de la característica f ∈ F en el texto Ti asociado con la reseña (ri, Ti), como la fracción de Ti dedicada a discutir aspectos (tanto positivos como negativos) relacionados con la característica f. Proponemos un método elemental para aproximar los valores de estos pesos. Para cada característica, construimos manualmente la lista de palabras Lf que contiene aproximadamente 50 palabras que están más comúnmente asociadas a la característica f. Las palabras iniciales fueron seleccionadas al leer algunas de las reseñas y ver qué palabras coinciden con la discusión de qué características. La lista fue luego ampliada agregando todas las entradas del tesauro que estaban relacionadas con las palabras iniciales. Finalmente, hicimos una lluvia de ideas para encontrar las palabras faltantes que normalmente estarían asociadas con cada una de las características. Que Lf ∩ Ti sea la lista de términos comunes a Lf y Ti. Cada término de Lf se cuenta el número de veces que aparece en Ti, con dos excepciones: • en los casos en que el usuario envía un título para la revisión, consideramos el texto del título agregándolo tres veces al texto de la revisión Ti. La suposición intuitiva es que la opinión del usuario se refleja con mayor fuerza en el título que en el cuerpo de la reseña. Por ejemplo, muchas reseñas son resumidas con precisión por títulos como \"Excelente servicio, ubicación terrible\" o \"Mala relación calidad-precio\"; ciertas palabras que ocurren solo una vez en el texto son contadas múltiples veces si su relevancia para esa característica es particularmente fuerte. Estas eran las palabras raíz para cada característica (por ejemplo, personal es una palabra raíz para la característica Servicio), y tenían un peso de 2 o 3. Cada característica fue asignada hasta 3 palabras raíz, por lo que casi todas las palabras se cuentan solo una vez. La lista de palabras para la característica Habitaciones se proporciona como referencia en el Apéndice A. El peso wi f se calcula como: wi f = |Lf ∩ Ti| f∈F |Lf ∩ Ti| (1) donde |Lf ∩ Ti| es el número de términos comunes entre Lf y Ti. El peso para la característica \"En general\" se estableció en min{ |T i | 5000 , 1} donde |Ti | es el número de caracteres en Ti. Comenzaré diciendo que soy más de la onda de Holiday Inn que de un hotel de lujo. Así que me frustro cuando pago el doble de la tarifa de la habitación y recibo la mitad de las comodidades que obtendría en un Hampton Inn o Holiday Inn. La ubicación era definitivamente el principal activo de este lugar. Estaba a solo unas cuadras de la parada de metro del Centro Hynes y era fácil caminar a algunos buenos restaurantes en la zona de Back Bay. Boylston no está lejos en absoluto. Así que no tuve problemas en renunciar a un coche de alquiler y tomar el metro desde el aeropuerto hasta el hotel y usar el metro para cualquier otro viaje. De lo contrario, te hacen pagar por todo. Y cuando ya has gastado $215 por noche en la habitación, eso resulta frustrante. La habitación en sí estaba bien, más o menos lo que esperaría. El personal también era promedio, ni malo ni excelente. Una vez más, creo que estás pagando por la ubicación y la capacidad de ir caminando a muchos lugares interesantes. Pero creo que la próxima vez me quedaré en Brookline, disfrutaré de más comodidades y usaré más el metro. Las calificaciones numéricas asociadas a esta reseña son rO = 3, rR = 3, rS = 3, rC = 4, rV = 2 para las características de General (O), Habitaciones (R), Servicio (S), Limpieza (C) y Valor (V) respectivamente. Las calificaciones de las características Comida (F), Ubicación (L) y Ruido (N) están ausentes (es decir, rF = rL = rN = 0). Los pesos wf se calculan a partir de las siguientes listas de términos comunes: LR ∩ T = {habitación}; wR = 0.066 LS ∩ T = {3 * Personal, comodidades}; wS = 0.267 LC ∩ T = ∅; wC = 0 LV ∩ T = {$, tarifa}; wV = 0.133 LF ∩ T = {restaurante}; wF = 0.067 LL ∩ T = {2 * centro, 2 * caminar, 2 * ubicación, área}; wL = 0.467 LN ∩ T = ∅; wN = 0 Las palabras raíz Personal y Centro se triplicaron y duplicaron respectivamente. El peso total de la revisión textual es wO = 0.197. Estos valores explican razonablemente bien los pesos de las diferentes características en la discusión del revisor. Un punto a tener en cuenta es que algunos términos en las listas Lf poseen una orientación semántica inherente. Por ejemplo, la palabra suciedad (perteneciente a la lista LC) se usaría con mayor frecuencia para afirmar la presencia, y no la ausencia de suciedad. Esto es inevitable, pero se tuvo cuidado de asegurar que se utilizaran palabras de ambos extremos del espectro. Por esta razón, algunas listas como LR contienen solo sustantivos de objetos que uno típicamente describiría en una habitación (ver Apéndice A). El objetivo de esta sección es analizar la influencia de los pesos wi f en las calificaciones numéricas ri f. Intuitivamente, los usuarios que pasaron mucho tiempo discutiendo una característica f (es decir, cuando wif es alto) tenían algo que decir sobre su experiencia con respecto a esta característica. Obviamente, la característica f es importante para el usuario i. Dado que las personas tienden a ser más conocedoras en los aspectos que consideran importantes, nuestra hipótesis es que las calificaciones ri f (correspondientes a los pesos altos wi f) constituyen un subconjunto de las calificaciones de expertos para la característica f. La Figura 2 traza la distribución de las tasas r i(h) C con respecto a los pesos w i(h) C para la limpieza de un hotel de Las Vegas, h. Aquí, las calificaciones altas están restringidas a las reseñas que hablan poco sobre la limpieza. Cuando se menciona la limpieza en la discusión, las calificaciones son bajas. Muchos hoteles muestran patrones de calificación similares para varias características. Las calificaciones correspondientes a pesos bajos abarcan todo el espectro del 1 al 5, mientras que las calificaciones correspondientes a pesos altos están más agrupadas (ya sea alrededor de calificaciones buenas o malas). Por lo tanto, formulamos la siguiente hipótesis: Hipótesis 1. Las calificaciones ri f correspondientes a las reseñas donde wi f es alta, son más similares entre sí que a la colección general de calificaciones. Para probar la hipótesis, tomamos todo el conjunto de reseñas y, característica por característica, calculamos la desviación estándar de las calificaciones con alto peso, y la desviación estándar de todo el conjunto de calificaciones. Los pesos altos se definieron como aquellos que pertenecen al 20% superior del rango de pesos para la característica correspondiente. Si la Hipótesis 1 fuera cierta, la desviación estándar de todas las calificaciones debería ser mayor que la desviación estándar de las calificaciones con pesos altos. 0 1 2 3 4 5 6 0 0.1 0.2 0.3 0.4 0.5 0.6 Calificación Peso Figura 2: La distribución de las calificaciones en función del peso de la característica de limpieza. Utilizamos una prueba T estándar para medir la significancia de los resultados. Ciudad por ciudad y característica por característica, la Tabla 2 presenta la desviación estándar promedio de todas las calificaciones, y la desviación estándar promedio de las calificaciones con pesos altos. De hecho, las calificaciones con pesos altos tienen una desviación estándar más baja, y los resultados son significativos en el umbral estándar de significancia de 0.05 (aunque para ciertas ciudades tomadas de forma independiente no parece haber una diferencia significativa, los resultados son significativos para todo el conjunto de datos). Por favor, tenga en cuenta que solo se consideraron las características O, R, S, C y V, ya que para las otras (F, L y N) no teníamos suficientes calificaciones. Tabla 2: Desviación estándar promedio para todas las calificaciones, y desviación estándar promedio para las calificaciones con pesos altos. En corchetes, los valores p correspondientes para una diferencia positiva entre los dos. La hipótesis 1 no solo proporciona una comprensión básica sobre el comportamiento de calificación de los usuarios en línea, sino que también sugiere algunas formas de calcular estimaciones de mejor calidad. Podemos, por ejemplo, construir una estimación de calidad característica por característica con una varianza mucho menor: para cada característica tomamos el subconjunto de reseñas que discuten ampliamente esa característica, y como estimación de calidad, obtenemos la calificación promedio de este subconjunto. Los experimentos iniciales sugieren que las calificaciones promedio de característica a característica calculadas de esta manera son diferentes de las calificaciones promedio calculadas en todo el conjunto de datos. Dado que, de hecho, los pesos altos son indicadores de opiniones expertas, las estimaciones obtenidas de esta manera son más precisas que las actuales. Sin embargo, la validación de esta suposición subyacente requiere más experimentos controlados. LA INFLUENCIA DE LAS CALIFICACIONES PASADAS Por lo general, se hacen dos suposiciones importantes sobre las reseñas enviadas a foros en línea. La primera es que las calificaciones reflejen fielmente la calidad observada por los usuarios; la segunda es que las reseñas sean independientes entre sí. Si bien la evidencia anecdótica [9, 22] cuestiona la primera suposición, en esta sección abordamos la segunda. Un examen de las reseñas en línea muestra que estas suelen formar parte de hilos de discusión, donde los usuarios se esfuerzan por contradecir o estar vehementemente de acuerdo con los comentarios de usuarios anteriores. Considera, por ejemplo, la siguiente reseña: No entiendo las críticas negativas... el hotel estaba un poco oscuro, pero ese era el estilo. Fue muy artístico. Sí, estaba cerca de la autopista, pero en mi opinión, el sonido de un coche ruidoso ocasional es mejor que escuchar el ding ding de las máquinas tragamonedas toda la noche. El personal disponible es FABULOSO. Las camareras son geniales (y *** no merece la mala crítica que recibió, ¡fue 100% atenta con nosotros!), los camareros son amigables y profesionales al mismo tiempo... Aquí, el usuario se vio perturbado por informes negativos anteriores, abordó estas preocupaciones y se dispuso a intentar corregirlas. Como era de esperar, sus calificaciones fueron considerablemente más altas que las calificaciones promedio hasta este punto. Parece que los usuarios de TripAdvisor suelen leer regularmente los informes enviados por usuarios anteriores antes de reservar un hotel, o antes de escribir una reseña. Las reseñas anteriores crean ciertas expectativas previas sobre la calidad del servicio, y estas expectativas influyen en la reseña presentada. Creemos que esta observación es válida para la mayoría de los foros en línea. La percepción subjetiva de la calidad es directamente proporcional a qué tan bien la experiencia real cumple con la expectativa previa, un hecho confirmado por una importante línea de investigación econométrica y de marketing [17, 18, 16, 21]. La correlación entre las reseñas también ha sido confirmada por investigaciones recientes sobre la dinámica de los foros de reseñas en línea [6]. 4.1 Expectativas Previas Definimos la expectativa previa del usuario i con respecto a la característica f, como el promedio de las calificaciones previamente disponibles en la característica f4: ef (i) = j<i,r j f =0 rj f j<i,r j f =0 1 Como primera hipótesis, afirmamos que la calificación ri f es una función de la expectativa previa ef (i): Hipótesis 2. Para un hotel y una característica dados, dados los comentarios i y j tales que ef (i) es alto y ef (j) es bajo, la calificación rj f supera la calificación ri f. Definimos las expectativas altas y bajas como aquellas que están por encima, respectivamente por debajo de un cierto valor umbral θ. El conjunto de reseñas precedidas por altas, respectivamente bajas expectativas 3 parte de las reseñas de Amazon fueron reconocidas como publicaciones estratégicas por autores de libros o competidores 4 si no se asignaron calificaciones previas para la característica f, ef (i) se asigna un valor predeterminado de 4. Tabla 3: Calificaciones promedio para reseñas precedidas por expectativas bajas (primer valor en la celda) y altas (segundo valor en la celda). Los valores P para una diferencia positiva se dan entre corchetes. Las ciudades O R S C V 3.953 4.045 3.985 4.252 3.946 Boston 3.364 3.590 3.485 3.641 3.242 [0.011] [0.028] [0.0086] [0.0168] [0.0034] 4.284 4.358 4.064 4.530 4.428 Sídney 3.756 3.537 3.436 3.918 3.495 [0.000] [0.000] [0.035] [0.009] [0.000] 3.494 3.674 3.713 3.689 3.580 Las Vegas 3.140 3.530 2.952 3.530 3.351 [0.190] [0.529] [0.007] [0.529] [0.253] se definen de la siguiente manera: Ralto f = {ri f |ef (i) > θ} Rbajo f = {ri f |ef (i) < θ} Estos conjuntos son específicos para cada par (hotel, característica), y en nuestros experimentos tomamos θ = 4. Este valor bastante alto está cerca del promedio de calificación de todas las características de todos los hoteles, y se justifica por el hecho de que nuestro conjunto de datos contiene principalmente hoteles de alta calidad. Para cada ciudad, tomamos todos los hoteles y calculamos las calificaciones promedio en los conjuntos Rhigh f y Rlow f (ver Tabla 3). El promedio de calificación entre las reseñas que siguen a expectativas previas bajas es significativamente mayor que el promedio de calificación después de altas expectativas. Como evidencia adicional, consideramos todos los hoteles para los cuales la función eV (i) (la expectativa para la característica Valor) tiene un valor alto (mayor que 4) para algunos i, y un valor bajo (menor que 4) para algunos otros i. Intuitivamente, estos son los hoteles para los cuales hay un grado mínimo de variación en la secuencia oportuna de reseñas: es decir, el promedio acumulativo de calificaciones fue en algún momento alto y luego se volvió bajo, o viceversa. Tales variaciones se observan en aproximadamente la mitad de todos los hoteles en cada ciudad. La Figura 3 traza la mediana (entre los hoteles considerados) de la calificación, rV, cuando ef (i) no es más que x pero mayor que x - 0.5. 2.5 3 3.5 4 4.5 5 2.5 3 3.5 4 4.5 5 Expectativa de la mediana de calificación Boston Sydney Vegas Figura 3: Las calificaciones tienden a disminuir a medida que aumenta la expectativa. 138 Hay dos formas de interpretar la función ef (i): • El valor esperado para la característica f obtenido por el usuario i antes de su experiencia con el servicio, adquirido al leer informes presentados por usuarios anteriores. En este caso, un valor excesivamente alto para ef (i) llevaría al usuario a enviar un informe negativo (o viceversa), derivado de la diferencia entre el valor real del servicio y la expectativa inflada de este valor adquirida antes de su experiencia. • El valor esperado de la característica f para todos los visitantes posteriores del sitio, si el usuario i no enviara un informe. En este caso, la motivación para un informe negativo después de un valor excesivamente alto de ef es diferente: el usuario i busca corregir la expectativa de los futuros visitantes del sitio. A diferencia de la interpretación anterior, esto no requiere que el usuario derive una expectativa a priori para el valor de f. Tenga en cuenta que ninguna de las interpretaciones implica que el promedio hasta el informe i esté inversamente relacionado con la calificación en el informe i. Podría existir una medida de influencia ejercida por informes anteriores que impulse al usuario detrás del informe i a enviar calificaciones que, en cierta medida, se ajusten a los informes anteriores: un valor bajo para ef (i) puede influir en el usuario i a enviar una calificación baja para la característica f porque, por ejemplo, teme que enviar una calificación alta lo haga parecer una persona con bajos estándares. Esto, al principio, parece contradecir la Hipótesis 2. Sin embargo, esta calificación de conformidad no puede continuar indefinidamente: una vez que el conjunto de informes proyecte una estimación suficientemente reducida para vf, los revisores futuros con impresiones comparativamente positivas buscarán corregir esta percepción errónea. 4.2 Impacto de los comentarios textuales en la expectativa de calidad. Se puede obtener una mayor comprensión del comportamiento de calificación de los usuarios de TripAdvisor analizando la relación entre los pesos wf y los valores ef (i). En particular, examinamos la siguiente hipótesis: Hipótesis 3. Cuando una gran proporción del texto de una reseña discute una característica en particular, la diferencia entre la calificación para esa característica y la calificación promedio hasta ese momento tiende a ser grande. La intuición detrás de esta afirmación es que cuando el usuario insiste en expresar su opinión sobre una característica en particular, su opinión difiere de la opinión colectiva de publicaciones anteriores. Esto se basa en la característica de los sistemas de reputación como foros de retroalimentación donde un usuario está interesado en proyectar su opinión, con particular fuerza si esta opinión difiere de lo que percibe como la opinión general. Para probar la Hipótesis 3 medimos la diferencia absoluta promedio entre la expectativa ef (i) y la calificación ri f cuando el peso wi f es alto, respectivamente bajo. Los pesos se clasifican como altos o bajos al compararlos con ciertos valores de corte: wi f es bajo si es menor que 0.1, mientras que wi f es alto si es mayor que θf. Se utilizaron diferentes valores de corte para diferentes características: θR = 0.4, θS = 0.4, θC = 0.2 y θV = 0.7. La limpieza tiene un límite inferior más bajo ya que es una característica raramente discutida; el valor tiene un límite superior alto por la razón opuesta. Los resultados se presentan en la Tabla 4. La idea de que los informes negativos pueden fomentar una mayor divulgación negativa ha sido sugerida anteriormente [14]. Tabla 4: Promedio de |ri f −ef (i)| cuando los pesos son altos (primer valor en la celda) y bajos (segundo valor en la celda) con valores P para la diferencia en corchetes cuadrados. Esto demuestra que cuando los pesos son inusualmente altos, los usuarios tienden a expresar una opinión que no se ajusta al promedio neto de calificaciones anteriores. Como podríamos esperar, para una característica que rara vez tuvo un peso importante en la discusión (por ejemplo, la limpieza), la diferencia es particularmente grande. Aunque la diferencia en el valor de la característica es bastante grande para Sídney, el valor P es alto. Esto se debe a que solo unos pocos comentarios discutieron en gran medida el valor. La razón podría ser cultural o porque había menos motivo para discutir esta característica. 4.3 Incentivos para informar Los modelos anteriores sugieren que los usuarios poco opinativos no elegirán expresar sus opiniones [12]. En esta sección, ampliamos este modelo para tener en cuenta la influencia de las expectativas. La motivación para enviar comentarios no se debe solo a opiniones extremas, sino también a la diferencia entre la reputación actual (es decir, la expectativa previa del usuario) y la experiencia real. Un modelo de calificación así produce calificaciones que la mayoría de las veces se desvían de la calificación promedio actual. Las calificaciones que confirman la expectativa previa rara vez serán enviadas. Probamos en nuestro conjunto de datos la proporción de calificaciones que intentan corregir la estimación actual. Definimos una calificación desviada como aquella que se desvía de la expectativa actual por al menos un umbral θ, es decir, |ri f − ef (i)| ≥ θ. Para cada una de las tres ciudades consideradas, las siguientes tablas muestran la proporción de calificaciones desviadas para θ = 0.5 y θ = 1. Los resultados anteriores sugieren que una gran proporción de usuarios (casi la mitad, incluso para el valor umbral alto θ = 1) se desvían del promedio previo. Esto refuerza la idea de que los usuarios son más propensos a enviar un informe cuando creen que tienen algo distintivo que agregar al flujo actual de opiniones sobre alguna característica. Tales conclusiones están en total acuerdo con evidencia previa de que la distribución de informes a menudo sigue distribuciones bimodales en forma de U. 139 5. Para dar cuenta de las observaciones descritas en las secciones anteriores, proponemos un modelo para el comportamiento de los usuarios al enviar reseñas en línea. Para un hotel dado, asumimos que la calidad experimentada por los usuarios sigue una distribución normal alrededor de algún valor vf, que representa la calidad objetiva ofrecida por el hotel en la característica f. La calificación enviada por el usuario i en la característica f es: ˆri f = δf vi f + (1 − δf) · sign vi f − ef (i) c + d(vi f, ef (i)|wi f) (2) donde: • vi f es la calidad (desconocida) experimentada realmente por el usuario. Se asume que vi f sigue una distribución normal alrededor de algún valor vf; • δf ∈ [0, 1] puede verse como una medida del sesgo al reportar retroalimentación. Los valores altos reflejan el hecho de que los usuarios califican de manera objetiva, sin ser influenciados por expectativas previas. El valor de δf puede depender de varios factores; fijamos un valor para cada característica f; • c es una constante entre 1 y 5; • wi f es el peso de la característica f en el comentario textual de la reseña i, calculado según la Ecuación (1); • d(vi f , ef (i)|wi f ) es una función de distancia entre la expectativa y la observación del usuario i. La función de distancia satisface las siguientes propiedades: - d(y, z|w) ≥ 0 para todo y, z ∈ [0, 5], w ∈ [0, 1]; - |d(y, z|w)| < |d(z, x|w)| si |y − z| < |z − x|; - |d(y, z|w1)| < |d(y, z|w2)| si w1 < w2; - c + d(vf , ef (i)|wi f ) ∈ [1, 5]; El segundo término de la Ecuación (2) codifica el sesgo de la calificación. Cuanto mayor sea la distancia entre la observación real vi f y la función ef, mayor será el sesgo. 5.1 Validación del modelo Utilizamos el conjunto de datos de las reseñas de TripAdvisor para validar el modelo de comportamiento presentado anteriormente. Dividimos por conveniencia los valores de calificación en tres rangos: malo (B = {1, 2}), indiferente (I = {3, 4}), y bueno (G = {5}), y realizamos los siguientes dos tests: • Primero, usaremos nuestro modelo para predecir las calificaciones que tienen valores extremos. Para cada hotel, tomamos la secuencia de informes, y cada vez que nos encontramos con una calificación que sea buena o mala (pero no indiferente) intentamos predecirla utilizando la Ecuación (2) • En segundo lugar, en lugar de predecir el valor de las calificaciones extremas, intentamos clasificarlas como buenas o malas. Para cada hotel tomamos la secuencia de informes, y para cada informe (independientemente de su valor) lo clasificamos como bueno o malo. Sin embargo, para realizar estas pruebas, necesitamos estimar el valor objetivo, vf, que es el promedio de las observaciones de calidad reales, vi f. El algoritmo que estamos utilizando se basa en la intuición de que la cantidad de calificación de conformidad se minimiza. En otras palabras, el valor vf debe ser tal que, tan a menudo como sea posible, las calificaciones malas sigan a expectativas por encima de vf y las calificaciones buenas sigan a expectativas por debajo de vf. Formalmente, definimos los conjuntos: Γ1 = {i|ef (i) < vf y ri f ∈ B}; Γ2 = {i|ef (i) > vf y ri f ∈ G}; que corresponden a irregularidades donde, a pesar de que la expectativa en el punto i es menor que el valor entregado, la calificación es baja, y viceversa. Definimos vf como el valor que minimiza la unión de los dos conjuntos: vf = arg min vf |Γ1 ∪ Γ2| (3) En la Ec. (2) reemplazamos vi f por el valor vf calculado en la Ec. (3), y utilizamos la siguiente función de distancia: d(vf , ef (i)|wi f ) = |vf − ef (i)| vf − ef (i) |vf 2 − ef (i)2 | · (1 + 2wi f ); La constante c ∈ I se estableció en min{max{ef (i), 3}}, 4}. Los valores para δf se fijaron en {0.7, 0.7, 0.8, 0.7, 0.6} para las características {General, Habitaciones, Servicio, Limpieza, Valor} respectivamente. Los pesos se calculan tal como se describe en la Sección 3. Como primer experimento, tomamos los conjuntos de calificaciones extremas {ri f |ri f /∈ I} para cada hotel y característica. Para cada calificación de este tipo, ri f, intentamos estimarla calculando ˆri f usando la Ecuación (2). Comparamos este estimador con el obtenido simplemente promediando las calificaciones de todos los hoteles y características: es decir, ¯rf = j,r j f =0 rj f j,r j f =0 1; la Tabla 7 presenta la relación entre el error cuadrático medio (RMSE) al usar ˆri f y ¯rf para estimar las calificaciones reales. En todos los casos, la estimación producida por nuestro modelo es mejor que el promedio simple. Tabla 7: Promedio de RMSE(ˆrf) RMSE(¯rf) Ciudad O R S C V Boston 0.987 0.849 0.879 0.776 0.913 Sídney 0.927 0.817 0.826 0.720 0.681 Las Vegas 0.952 0.870 0.881 0.947 0.904 Como segundo experimento, intentamos distinguir los conjuntos Bf = {i|ri f ∈ B} y Gf = {i|ri f ∈ G} de calificaciones malas y buenas, respectivamente, en la característica f. Por ejemplo, calculamos el conjunto Bf usando el siguiente clasificador (llamado σ): ri f ∈ Bf (σf (i) = 1) ⇔ ˆri f ≤ 4; Las Tablas 8, 9 y 10 presentan la Precisión (p), Recall (r) y s = 2pr p+r para el clasificador σ, y lo comparan con un clasificador de mayoría ingenuo, τ, τf (i) = 1 ⇔ |Bf | ≥ |Gf |: Vemos que el recall es siempre mayor para σ y la precisión suele ser ligeramente peor. Para la métrica s, σ tiende a agregar un 140. Tabla 8: Precisión (p), Recall (r), s= 2pr p+r mientras se detectan calificaciones bajas para Boston O R S C V p 0.678 0.670 0.573 0.545 0.610 σ r 0.626 0.659 0.619 0.612 0.694 s 0.651 0.665 0.595 0.577 0.609 p 0.684 0.706 0.647 0.611 0.633 τ r 0.597 0.541 0.410 0.383 0.562 s 0.638 0.613 0.502 0.471 0.595 Tabla 9: Precisión (p), Recall (r), s= 2pr p+r mientras se detectan calificaciones bajas para Las Vegas O R S C V p 0.654 0.748 0.592 0.712 0.583 σ r 0.608 0.536 0.791 0.474 0.610 s 0.630 0.624 0.677 0.569 0.596 p 0.685 0.761 0.621 0.748 0.606 τ r 0.542 0.505 0.767 0.445 0.441 s 0.605 0.607 0.670 0.558 0.511 Mejora del 1-20% sobre τ, mucho mayor en algunos casos para hoteles en Sídney. Esto se debe probablemente a que las reseñas de Sídney son más positivas que las de las ciudades estadounidenses y los casos en los que el número de reseñas negativas supera al de las positivas son raros. Reemplazar el algoritmo de prueba con uno que juega un 1 con una probabilidad igual a la proporción de malas críticas mejora sus resultados para esta ciudad, pero aún es superado por alrededor del 80%. 6. RESUMEN DE RESULTADOS Y CONCLUSIÓN El objetivo de este artículo es explorar los factores que llevan a un usuario a enviar una calificación particular, en lugar de los incentivos que lo animaron a enviar un informe en primer lugar. Para ello utilizamos dos fuentes adicionales de información además del vector de calificaciones numéricas: primero, examinamos los comentarios textuales que acompañan las reseñas, y segundo, consideramos los informes que han sido previamente enviados por otros usuarios. Usando algoritmos simples de procesamiento de lenguaje natural, pudimos establecer una correlación entre el peso de una característica específica en el comentario textual que acompaña la reseña, y el ruido presente en la calificación numérica. Específicamente, parece que los usuarios que discuten ampliamente una característica en particular tienden a estar de acuerdo en una calificación común. Esta observación permite la construcción de estimadores de calidad característica por característica que tienen una varianza más baja y, con suerte, son menos ruidosos. Sin embargo, se requiere más evidencia para respaldar la intuición de que las calificaciones correspondientes a pesos altos son opiniones de expertos que merecen ser consideradas de mayor prioridad al calcular estimaciones de calidad. En segundo lugar, enfatizamos la dependencia de las calificaciones en informes previos. Los informes anteriores crean una expectativa de calidad que afecta la percepción subjetiva del usuario. Validamos dos hechos sobre las reseñas de hoteles que recopilamos de TripAdvisor: Primero, las calificaciones que siguen expectativas bajas (donde la expectativa se calcula como el promedio de los informes anteriores) probablemente sean más altas que las calificaciones que siguen expectativas altas. Intuitivamente, la percepción de la calidad (y consecuentemente la calificación) depende de qué tan bien la experiencia real del usuario cumple con sus expectativas. Segundo, incluimos evidencia de los comentarios textuales, y encontramos que cuando los usuarios dedican una gran parte del texto a discutir una característica específica, es probable que motiven una calificación divergente (es decir, una calificación que no se ajusta a la expectativa previa). Intuitivamente, esto respalda la hipótesis de que los foros de reseñas actúan como grupos de discusión donde los usuarios están interesados en presentar y motivar su propia opinión. Hemos capturado la evidencia empírica en un modelo de comportamiento que predice las calificaciones enviadas por los usuarios. La calificación final depende, como era de esperar, de la observación real y de la brecha entre la observación y la expectativa. La brecha tiende a tener una influencia mayor cuando una fracción importante del comentario textual está dedicada a discutir una característica específica. El modelo propuesto fue validado con los datos empíricos y proporciona mejores estimaciones de las calificaciones realmente enviadas. Una suposición que hacemos es sobre la existencia de un valor de calidad objetivo vf para la característica f. Esto rara vez es cierto, especialmente a lo largo de largos periodos de tiempo. Otras explicaciones podrían dar cuenta de la correlación de las calificaciones con informes anteriores. Por ejemplo, si ef (i) refleja el valor real de f en un punto en el tiempo, la diferencia en las calificaciones siguiendo expectativas altas y bajas puede ser explicada por modelos de ingresos hoteleros que se maximizan cuando el valor es modificado en consecuencia. Sin embargo, la idea de que la variación en las calificaciones no es principalmente una función de la variación en el valor resulta ser útil. Nuestro enfoque para aproximar este valor objetivo es de ninguna manera perfecto, pero se ajusta perfectamente a la idea detrás del modelo. Una dirección natural para trabajos futuros es examinar aplicaciones concretas de nuestros resultados. Es probable que se obtengan mejoras significativas en las estimaciones de calidad al incorporar toda la evidencia empírica sobre el comportamiento de calificación. No está claro exactamente cómo diferentes factores afectan las decisiones de los usuarios. La respuesta podría depender de la aplicación particular, el contexto y la cultura. 7. REFERENCIAS [1] A. Admati y P. Pfleiderer. Noisytalk.com: Transmitiendo opiniones en un entorno ruidoso. Documento de trabajo 1670R, Universidad de Stanford, 2000. [2] P. B., L. Lee y S. Vaithyanathan. ¿Clasificación de sentimientos con técnicas de aprendizaje automático? En Actas de EMNLP-02, la Conferencia sobre Métodos Empíricos en el Procesamiento del Lenguaje Natural, 2002. [3] H. Cui, V. Mittal y M. Datar. Experimentos comparativos 141 sobre clasificación de sentimientos para reseñas de productos en línea. En Actas de AAAI, 2006. [4] K. Dave, S. Lawrence y D. Pennock. Extracción de opiniones y clasificación semántica de reseñas de productos en la galería de cacahuetes. En Actas de la 12ª Conferencia Internacional sobre la World Wide Web (WWW03), 2003. [5] C. Dellarocas, N. Awad y X. Zhang. Explorando el valor de las calificaciones de productos en línea en la previsión de ingresos: El caso de las películas en movimiento. Documento de trabajo, 2006. [6] C. Forman, A. Ghose y B. Wiesenfeld. Un Examen Multinivel del Impacto de las Identidades Sociales en las Transacciones Económicas en los Mercados Electrónicos. Disponible en SSRN: http://ssrn.com/abstract=918978, julio de 2006. [7] A. Ghose, P. Ipeirotis y A. Sundararajan. Primas de reputación en mercados electrónicos peer-to-peer: análisis de retroalimentación textual y estructura de red. En el Tercer Taller sobre Economía de Sistemas Peer-to-Peer (P2PECON), 2005. [8] A. Ghose, P. Ipeirotis y A. Sundararajan. Las dimensiones de la reputación en los mercados electrónicos. Documento de trabajo CeDER-06-02, Universidad de Nueva York, 2006. [9] A. Harmon. Error de Amazon revela la guerra de los revisores. The New York Times, 14 de febrero de 2004. [10] D. Houser y J. Wooders. Reputación en subastas: teoría y evidencia de eBay. Revista de Estrategia Económica y de Gestión, 15:353-369, 2006. [11] M. Hu y B. Liu. Extracción y resumen de reseñas de clientes. En Actas de la Conferencia Internacional de la ACM SIGKDD sobre Descubrimiento de Conocimiento y Minería de Datos (KDD04), 2004. [12] N. Hu, P. Pavlou y J. Zhang. ¿Pueden las reseñas en línea revelar la verdadera calidad de un producto? En Actas de la Conferencia ACM sobre Comercio Electrónico (EC 06), 2006. [13] K. Kalyanam y S. McIntyre. Retorno de la reputación en el mercado de subastas en línea. Documento de trabajo 02/03-10-WP, Escuela de Negocios Leavey, Universidad de Santa Clara, 2001. [14] L. Khopkar y P. Resnick. Auto-selección, deslizamiento, salvamento, holgazanería y lapidación: los impactos de la retroalimentación negativa en eBay. En Actas de la Conferencia ACM sobre Comercio Electrónico (EC 05), 2005. [15] M. Melnik y J. Alm. ¿Importa la reputación de un vendedor? evidencia de subastas en eBay. Revista de Economía Industrial, 50(3):337-350, 2002. [16] R. Olshavsky y J. Miller. Expectativas del consumidor, rendimiento del producto y calidad percibida del producto. Revista de Investigación de Marketing, 9:19-21, febrero de 1972. [17] A. Parasuraman, V. Zeithaml y L. Berry. Un Modelo Conceptual de Calidad de Servicio y sus Implicaciones para Investigaciones Futuras. Revista de Marketing, 49:41-50, 1985. [18] A. Parasuraman, V. Zeithaml y L. Berry. SERVQUAL: Una escala de múltiples ítems para medir las percepciones de calidad del servicio por parte del consumidor. Revista de Retail, 64:12-40, 1988. [19] P. Pavlou y A. Dimoka. La naturaleza y función de los comentarios de texto de retroalimentación en los mercados en línea: implicaciones para la construcción de confianza, primas de precio y diferenciación de vendedores. Investigación de Sistemas de Información, 17(4):392-414, 2006. [20] A. Popescu y O. Etzioni. Extrayendo características del producto y opiniones de las reseñas. En Actas de la Conferencia de Tecnología del Lenguaje Humano y Conferencia sobre Métodos Empíricos en Procesamiento del Lenguaje Natural, 2005. [21] R. Teas. Expectativas, Evaluación del Rendimiento y Percepciones de Calidad de los Consumidores. Revista de Marketing, 57:18-34, 1993. [22] E. White. Chateando a un cantante en las listas de éxitos pop. El Wall Street Journal, 15 de octubre de 1999. APÉNDICE A. LISTA DE PALABRAS, LR, ASOCIADAS A LA CARACTERÍSTICA HABITACIONES. Todas las palabras sirven como prefijos: habitación, espacio, interior, decoración, ambiente, atmósfera, comodidad, baño, inodoro, cama, edificio, pared, ventana, privado, temperatura, sábana, lino, almohada, caliente, agua, fría, agua, ducha, vestíbulo, muebles, alfombra, aire, acondicionado, colchón, distribución, diseño, espejo, techo, iluminación, lámpara, sofá, silla, cómoda, armario, armario. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "u-shaped distribution": {
            "translated_key": "distribución en forma de U",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Understanding User Behavior in Online Feedback Reporting Arjun Talwar Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland arjun@math.stanford.edu Radu Jurca Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland radu.jurca@epfl.ch Boi Faltings Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland boi.faltings@epfl.ch ABSTRACT Online reviews have become increasingly popular as a way to judge the quality of various products and services.",
                "Previous work has demonstrated that contradictory reporting and underlying user biases make judging the true worth of a service difficult.",
                "In this paper, we investigate underlying factors that influence user behavior when reporting feedback.",
                "We look at two sources of information besides numerical ratings: linguistic evidence from the textual comment accompanying a review, and patterns in the time sequence of reports.",
                "We first show that groups of users who amply discuss a certain feature are more likely to agree on a common rating for that feature.",
                "Second, we show that a users rating partly reflects the difference between true quality and prior expectation of quality as inferred from previous reviews.",
                "Both give us a less noisy way to produce rating estimates and reveal the reasons behind user bias.",
                "Our hypotheses were validated by statistical evidence from hotel reviews on the TripAdvisor website.",
                "Categories and Subject Descriptors J.4 [Social and Behavioral Sciences]: Economics General Terms Economics, Experimentation, Reliability 1.",
                "MOTIVATIONS The spread of the internet has made it possible for online feedback forums (or reputation mechanisms) to become an important channel for Word-of-mouth regarding products, services or other types of commercial interactions.",
                "Numerous empirical studies [10, 15, 13, 5] show that buyers seriously consider online feedback when making purchasing decisions, and are willing to pay reputation premiums for products or services that have a good reputation.",
                "Recent analysis, however, raises important questions regarding the ability of existing forums to reflect the real quality of a product.",
                "In the absence of clear incentives, users with a moderate outlook will not bother to voice their opinions, which leads to an unrepresentative sample of reviews.",
                "For example, [12, 1] show that Amazon1 ratings of books or CDs follow with great probability bi-modal, U-shaped distributions where most of the ratings are either very good, or very bad.",
                "Controlled experiments, on the other hand, reveal opinions on the same items that are normally distributed.",
                "Under these circumstances, using the arithmetic mean to predict quality (as most forums actually do) gives the typical user an estimator with high variance that is often false.",
                "Improving the way we aggregate the information available from online reviews requires a deep understanding of the underlying factors that bias the rating behavior of users.",
                "Hu et al. [12] propose the Brag-and-Moan Model where users rate only if their utility of the product (drawn from a normal distribution) falls outside a median interval.",
                "The authors conclude that the model explains the empirical distribution of reports, and offers insights into smarter ways of estimating the true quality of the product.",
                "In the present paper we extend this line of research, and attempt to explain further facts about the behavior of users when reporting online feedback.",
                "Using actual hotel reviews from the TripAdvisor2 website, we consider two additional sources of information besides the basic numerical ratings submitted by users.",
                "The first is simple linguistic evidence from the textual review that usually accompanies the numerical ratings.",
                "We use text-mining techniques similar to [7] and [3], however, we are only interested in identifying what aspects of the service the user is discussing, without computing the semantic orientation of the text.",
                "We find that users who comment more on the same feature are more likely to agree on a common numerical rating for that particular feature.",
                "Intuitively, lengthy comments reveal the importance of the feature to the user.",
                "Since people tend to be more knowledgeable in the aspects they consider important, users who discuss a given feature in more details might be assumed to have more authority in evaluating that feature.",
                "Second we investigate the relationship between a review 1 http://www.amazon.com 2 http://www.tripadvisor.com/ 134 Figure 1: The TripAdvisor page displaying reviews for a popular Boston hotel.",
                "Name of hotel and advertisements were deliberatively erased. and the reviews that preceded it.",
                "A perusal of online reviews shows that ratings are often part of discussion threads, where one post is not necessarily independent of other posts.",
                "One may see, for example, users who make an effort to contradict, or vehemently agree with, the remarks of previous users.",
                "By analyzing the time sequence of reports, we conclude that past reviews influence the future reports, as they create some prior expectation regarding the quality of service.",
                "The subjective perception of the user is influenced by the gap between the prior expectation and the actual performance of the service [17, 18, 16, 21] which will later reflect in the users rating.",
                "We propose a model that captures the dependence of ratings on prior expectations, and validate it using the empirical data we collected.",
                "Both results can be used to improve the way reputation mechanisms aggregate the information from individual reviews.",
                "Our first result can be used to determine a featureby-feature estimate of quality, where for each feature, a different subset of reviews (i.e., those with lengthy comments of that feature) is considered.",
                "The second leads to an algorithm that outputs a more precise estimate of the real quality. 2.",
                "THE DATA SET We use in this paper real hotel reviews collected from the popular travel site TripAdvisor.",
                "TripAdvisor indexes hotels from cities across the world, along with reviews written by travelers.",
                "Users can search the site by giving the hotels name and location (optional).",
                "The reviews for a given hotel are displayed as a list (ordered from the most recent to the oldest), with 5 reviews per page.",
                "The reviews contain: • information about the author of the review (e.g., dates of stay, username of the reviewer, location of the reviewer); • the overall rating (from 1, lowest, to 5, highest); • a textual review containing a title for the review, free comments, and the main things the reviewer liked and disliked; • numerical ratings (from 1, lowest, to 5, highest) for different features (e.g., cleanliness, service, location, etc.)",
                "Below the name of the hotel, TripAdvisor displays the address of the hotel, general information (number of rooms, number of stars, short description, etc), the average overall rating, the TripAdvisor ranking, and an average rating for each feature.",
                "Figure 1 shows the page for a popular Boston hotel whose name (along with advertisements) was explicitly erased.",
                "We selected three cities for this study: Boston, Sydney and Las Vegas.",
                "For each city we considered all hotels that had at least 10 reviews, and recorded all reviews.",
                "Table 1 presents the number of hotels considered in each city, the total number of reviews recorded for each city, and the distribution of hotels with respect to the star-rating (as available on the TripAdvisor site).",
                "Note that not all hotels have a star-rating.",
                "Table 1: A summary of the data set.",
                "City # Reviews # Hotels # of Hotels with 1,2,3,4 & 5 stars Boston 3993 58 1+3+17+15+2 Sydney 1371 47 0+0+9+13+10 Las Vegas 5593 40 0+3+10+9+6 For each review we recorded the overall rating, the textual review (title and body of the review) and the numerical rating on 7 features: Rooms(R), Service(S), Cleanliness(C), Value(V), Food(F), Location(L) and Noise(N).",
                "TripAdvisor does not require users to submit anything other than the overall rating, hence a typical review rates few additional features, regardless of the discussion in the textual comment.",
                "Only the features Rooms(R), Service(S), Cleanliness(C) and Value(V) are rated by a significant number of users.",
                "However, we also selected the features Food(F), Location(L) and Noise(N) because they are referred to in a significant number of textual comments.",
                "For each feature we record the numerical rating given by the user, or 0 when the rating is missing.",
                "The typical length of the textual comment amounts to approximately 200 words.",
                "All data was collected by crawling the TripAdvisor site in September 2006. 2.1 Formal notation We will formally refer to a review by a tuple (r, T) where: • r = (rf ) is a vector containing the ratings rf ∈ {0, 1, . . . 5} for the features f ∈ F = {O, R, S, C, V, F, L, N}; note that the overall rating, rO, is abusively recorded as the rating for the feature Overall(O); • T is the textual comment that accompanies the review. 135 Reviews are indexed according to the variable i, such that (ri , Ti ) is the ith review in our database.",
                "Since we dont record the username of the reviewer, we will also say that the ith review in our data set was submitted by user i.",
                "When we need to consider only the reviews of a given hotel, h, we will use (ri(h) , Ti(h) ) to denote the ith review about the hotel h. 3.",
                "EVIDENCE FROM TEXTUAL COMMENTS The free textual comments associated to online reviews are a valuable source of information for understanding the reasons behind the numerical ratings left by the reviewers.",
                "The text may, for example, reveal concrete examples of aspects that the user liked or disliked, thus justifying some of the high, respectively low ratings for certain features.",
                "The text may also offer guidelines for understanding the preferences of the reviewer, and the weights of different features when computing an overall rating.",
                "The problem, however, is that free textual comments are difficult to read.",
                "Users are required to scroll through many reviews and read mostly repetitive information.",
                "Significant improvements would be obtained if the reviews were automatically interpreted and aggregated.",
                "Unfortunately, this seems a difficult task for computers since human users often use witty language, abbreviations, cultural specific phrases, and the figurative style.",
                "Nevertheless, several important results use the textual comments of online reviews in an automated way.",
                "Using well established natural language techniques, reviews or parts of reviews can be classified as having a positive or negative semantic orientation.",
                "Pang et al. [2] classify movie reviews into positive/negative by training three different classifiers (Naive Bayes, Maximum Entropy and SVM) using classification features based on unigrams, bigrams or part-of-speech tags.",
                "Dave et al. [4] analyze reviews from CNet and Amazon, and surprisingly show that classification features based on unigrams or bigrams perform better than higher-order n-grams.",
                "This result is challenged by Cui et al. [3] who look at large collections of reviews crawled from the web.",
                "They show that the size of the data set is important, and that bigger training sets allow classifiers to successfully use more complex classification features based on n-grams.",
                "Hu and Liu [11] also crawl the web for product reviews and automatically identify product attributes that have been discussed by reviewers.",
                "They use Wordnet to compute the semantic orientation of product evaluations and summarize user reviews by extracting positive and negative evaluations of different product features.",
                "Popescu and Etzioni [20] analyze a similar setting, but use search engine hit-counts to identify product attributes; the semantic orientation is assigned through the relaxation labeling technique.",
                "Ghose et al. [7, 8] analyze seller reviews from the Amazon secondary market to identify the different dimensions (e.g., delivery, packaging, customer support, etc.) of reputation.",
                "They parse the text, and tag the part-of-speech for each word.",
                "Frequent nouns, noun phrases and verbal phrases are identified as dimensions of reputation, while the corresponding modifiers (i.e., adjectives and adverbs) are used to derive numerical scores for each dimension.",
                "The enhanced reputation measure correlates better with the pricing information observed in the market.",
                "Pavlou and Dimoka [19] analyze eBay reviews and find that textual comments have an important impact on reputation premiums.",
                "Our approach is similar to the previously mentioned works, in the sense that we identify the aspects (i.e., hotel features) discussed by the users in the textual reviews.",
                "However, we do not compute the semantic orientation of the text, nor attempt to infer missing ratings.",
                "We define the weight, wi f , of feature f ∈ F in the text Ti associated with the review (ri , Ti ), as the fraction of Ti dedicated to discussing aspects (both positive and negative) related to feature f. We propose an elementary method to approximate the values of these weights.",
                "For each feature we manually construct the word list Lf containing approximately 50 words that are most commonly associated to the feature f. The initial words were selected from reading some of the reviews, and seeing what words coincide with discussion of which features.",
                "The list was then extended by adding all thesaurus entries that were related to the initial words.",
                "Finally, we brainstormed for missing words that would normally be associated with each of the features.",
                "Let Lf ∩Ti be the list of terms common to both Lf and Ti.",
                "Each term of Lf is counted the number of times it appears in Ti , with two exceptions: • in cases where the user submits a title to the review, we account for the title text by appending it three times to the review text Ti .",
                "The intuitive assumption is that the users opinion is more strongly reflected in the title, rather than in the body of the review.",
                "For example, many reviews are accurately summarized by titles such as Excellent service, terrible location or Bad value for money; • certain words that occur only once in the text are counted multiple times if their relevance to that feature is particularly strong.",
                "These were root words for each feature (e.g., staff is a root word for the feature Service), and were weighted either 2 or 3.",
                "Each feature was assigned up to 3 such root words, so almost all words are counted only once.",
                "The list of words for the feature Rooms is given for reference in Appendix A.",
                "The weight wi f is computed as: wi f = |Lf ∩ Ti| f∈F |Lf ∩ Ti| (1) where |Lf ∩Ti | is the number of terms common to Lf and Ti .",
                "The weight for the feature Overall was set to min{ |T i | 5000 , 1} where |Ti | is the number of character in Ti .",
                "The following is a TripAdvisor review for a Boston hotel (the name of the hotel is omitted): Ill start by saying that Im more of a Holiday Inn person than a *** type.",
                "So I get frustrated when I pay double the room rate and get half the amenities that Id get at a Hampton Inn or Holiday Inn.",
                "The location was definitely the main asset of this place.",
                "It was only a few blocks from the Hynes Center subway stop and it was easy to walk to some good restaurants in the Back Bay area.",
                "Boylston isnt far off at all.",
                "So I had no trouble with foregoing a rental car and taking the subway from the airport to the hotel and using the subway for any other travel.",
                "Otherwise, they make you pay for anything and everything. 136 And when youve already dropped $215/night on the room, that gets frustrating.The room itself was decent, about what I would expect.",
                "Staff was also average, not bad and not excellent.",
                "Again, I think youre paying for location and the ability to walk to a lot of good stuff.",
                "But I think next time Ill stay in Brookline, get more amenities, and use the subway a bit more.",
                "This numerical ratings associated to this review are rO = 3, rR = 3, rS = 3, rC = 4, rV = 2 for features Overall(O), Rooms(R), Service(S), Cleanliness(C) and Value(V) respectively.",
                "The ratings for the features Food(F), Location(L) and Noise(N) are absent (i.e., rF = rL = rN = 0).",
                "The weights wf are computed from the following lists of common terms: LR ∩ T ={room}; wR = 0.066 LS ∩ T ={3 * Staff, amenities}; wS = 0.267 LC ∩ T = ∅; wC = 0 LV ∩ T ={$, rate}; wV = 0.133 LF ∩ T ={restaurant}; wF = 0.067 LL ∩ T ={2 * center, 2 * walk, 2 * location, area}; wL = 0.467 LN ∩ T = ∅; wN = 0 The root words Staff and Center were tripled and doubled respectively.",
                "The overall weight of the textual review is wO = 0.197.",
                "These values account reasonably well for the weights of different features in the discussion of the reviewer.",
                "One point to note is that some terms in the lists Lf possess an inherent semantic orientation.",
                "For example the word grime (belonging to the list LC ) would be used most often to assert the presence, and not the absence of grime.",
                "This is unavoidable, but care was taken to ensure words from both sides of the spectrum were used.",
                "For this reason, some lists such as LR contain only nouns of objects that one would typically describe in a room (see Appendix A).",
                "The goal of this section is to analyse the influence of the weights wi f on the numerical ratings ri f .",
                "Intuitively, users who spent a lot of their time discussing a feature f (i.e., wi f is high) had something to say about their experience with regard to this feature.",
                "Obviously, feature f is important for user i.",
                "Since people tend to be more knowledgeable in the aspects they consider important, our hypothesis is that the ratings ri f (corresponding to high weights wi f ) constitute a subset of expert ratings for feature f. Figure 2 plots the distribution of the rates r i(h) C with respect to the weights w i(h) C for the cleanliness of a Las Vegas hotel, h. Here, the high ratings are restricted to the reviews that discuss little the cleanliness.",
                "Whenever cleanliness appears in the discussion, the ratings are low.",
                "Many hotels exhibit similar rating patterns for various features.",
                "Ratings corresponding to low weights span the whole spectrum from 1 to 5, while the ratings corresponding to high weights are more grouped together (either around good or bad ratings).",
                "We therefore make the following hypothesis: Hypothesis 1.",
                "The ratings ri f corresponding to the reviews where wi f is high, are more similar to each other than to the overall collection of ratings.",
                "To test the hypothesis, we take the entire set of reviews, and feature by feature, we compute the standard deviation of the ratings with high weights, and the standard deviation of the entire set of ratings.",
                "High weights were defined as those belonging to the upper 20% of the weight range for the corresponding feature.",
                "If Hypothesis 1 were true, the standard deviation of all ratings should be higher than the standard deviation of the ratings with high weights. 0 1 2 3 4 5 6 0 0.1 0.2 0.3 0.4 0.5 0.6 Rating Weight Figure 2: The distribution of ratings against the weight of the cleanliness feature.",
                "We use a standard T-test to measure the significance of the results.",
                "City by city and feature by feature, Table 2 presents the average standard deviation of all ratings, and the average standard deviation of ratings with high weights.",
                "Indeed, the ratings with high weights have lower standard deviation, and the results are significant at the standard 0.05 significance threshold (although for certain cities taken independently there doesnt seem to be a significant difference, the results are significant for the entire data set).",
                "Please note that only the features O,R,S,C and V were considered, since for the others (F, L, and N) we didnt have enough ratings.",
                "Table 2: Average standard deviation for all ratings, and average standard deviation for ratings with high weights.",
                "In square brackets, the corresponding p-values for a positive difference between the two.",
                "City O R S C V all 1.189 0.998 1.144 0.935 1.123 Boston high 0.948 0.778 0.954 0.767 0.891 p-val [0.000] [0.004] [0.045] [0.080] [0.009] all 1.040 0.832 1.101 0.847 0.963 Sydney high 0.801 0.618 0.691 0.690 0.798 p-val [0.012] [0.023] [0.000] [0.377] [0.037] all 1.272 1.142 1.184 1.119 1.242 Vegas high 1.072 0.752 1.169 0.907 1.003 p-val [0.0185] [0.001] [0.918] [0.120] [0.126] Hypothesis 1 not only provides some basic understanding regarding the rating behavior of online users, it also suggests some ways of computing better quality estimates.",
                "We can, for example, construct a feature-by-feature quality estimate with much lower variance: for each feature we take the subset of reviews that amply discuss that feature, and output as a quality estimate the average rating for this subset.",
                "Initial experiments suggest that the average feature-by-feature ratings computed in this way are different from the average ratings computed on the whole data set.",
                "Given that, indeed, high weights are indicators of expert opinions, the estimates obtained in this way are more accurate than the current ones.",
                "Nevertheless, the validation of this underlying assumption requires further controlled experiments. 137 4.",
                "THE INFLUENCE OF PAST RATINGS Two important assumptions are generally made about reviews submitted to online forums.",
                "The first is that ratings truthfully reflect the quality observed by the users; the second is that reviews are independent from one another.",
                "While anecdotal evidence [9, 22] challenges the first assumption3 , in this section, we address the second.",
                "A perusal of online reviews shows that reviews are often part of discussion threads, where users make an effort to contradict, or vehemently agree with the remarks of previous users.",
                "Consider, for example, the following review: I dont understand the negative reviews... the hotel was a little dark, but that was the style.",
                "It was very artsy.",
                "Yes it was close to the freeway, but in my opinion the sound of an occasional loud car is better than hearing the ding ding of slot machines all night!",
                "The staff on-hand is FABULOUS.",
                "The waitresses are great (and *** does not deserve the bad review she got, she was 100% attentive to us! ), the bartenders are friendly and professional at the same time...",
                "Here, the user was disturbed by previous negative reports, addressed these concerns, and set about trying to correct them.",
                "Not surprisingly, his ratings were considerably higher than the average ratings up to this point.",
                "It seems that TripAdvisor users regularly read the reports submitted by previous users before booking a hotel, or before writing a review.",
                "Past reviews create some prior expectation regarding the quality of service, and this expectation has an influence on the submitted review.",
                "We believe this observation holds for most online forums.",
                "The subjective perception of quality is directly proportional to how well the actual experience meets the prior expectation, a fact confirmed by an important line of econometric and marketing research [17, 18, 16, 21].",
                "The correlation between the reviews has also been confirmed by recent research on the dynamics of online review forums [6]. 4.1 Prior Expectations We define the prior expectation of user i regarding the feature f, as the average of the previously available ratings on the feature f4 : ef (i) = j<i,r j f =0 rj f j<i,r j f =0 1 As a first hypothesis, we assert that the rating ri f is a function of the prior expectation ef (i): Hypothesis 2.",
                "For a given hotel and feature, given the reviews i and j such that ef (i) is high and ef (j) is low, the rating rj f exceeds the rating ri f .",
                "We define high and low expectations as those that are above, respectively below a certain cutoff value θ.",
                "The set of reviews preceded by high, respectively low expectations 3 part of Amazon reviews were recognized as strategic posts by book authors or competitors 4 if no previous ratings were assigned for feature f, ef (i) is assigned a default value of 4.",
                "Table 3: Average ratings for reviews preceded by low (first value in the cell) and high (second value in the cell) expectations.",
                "The P-values for a positive difference are given square brackets.",
                "City O R S C V 3.953 4.045 3.985 4.252 3.946 Boston 3.364 3.590 3.485 3.641 3.242 [0.011] [0.028] [0.0086] [0.0168] [0.0034] 4.284 4.358 4.064 4.530 4.428 Sydney 3.756 3.537 3.436 3.918 3.495 [0.000] [0.000] [0.035] [0.009] [0.000] 3.494 3.674 3.713 3.689 3.580 Las Vegas 3.140 3.530 2.952 3.530 3.351 [0.190] [0.529] [0.007] [0.529] [0.253] are defined as follows: Rhigh f = {ri f |ef (i) > θ} Rlow f = {ri f |ef (i) < θ} These sets are specific for each (hotel, feature) pair, and in our experiments we took θ = 4.",
                "This rather high value is close to the average rating across all features across all hotels, and is justified by the fact that our data set contains mostly high quality hotels.",
                "For each city, we take all hotels and compute the average ratings in the sets Rhigh f and Rlow f (see Table 3).",
                "The average rating amongst reviews following low prior expectations is significantly higher than the average rating following high expectations.",
                "As further evidence, we consider all hotels for which the function eV (i) (the expectation for the feature Value) has a high value (greater than 4) for some i, and a low value (less than 4) for some other i.",
                "Intuitively, these are the hotels for which there is a minimal degree of variation in the timely sequence of reviews: i.e., the cumulative average of ratings was at some point high and afterwards became low, or vice-versa.",
                "Such variations are observed for about half of all hotels in each city.",
                "Figure 3 plots the median (across considered hotels) rating, rV , when ef (i) is not more than x but greater than x − 0.5. 2.5 3 3.5 4 4.5 5 2.5 3 3.5 4 4.5 5 Medianofrating expectation Boston Sydney Vegas Figure 3: The ratings tend to decrease as the expectation increases. 138 There are two ways to interpret the function ef (i): • The expected value for feature f obtained by user i before his experience with the service, acquired by reading reports submitted by past users.",
                "In this case, an overly high value for ef (i) would drive the user to submit a negative report (or vice versa), stemming from the difference between the actual value of the service, and the inflated expectation of this value acquired before his experience. • The expected value of feature f for all subsequent visitors of the site, if user i were not to submit a report.",
                "In this case, the motivation for a negative report following an overly high value of ef is different: user i seeks to correct the expectation of future visitors to the site.",
                "Unlike the interpretation above, this does not require the user to derive an a priori expectation for the value of f. Note that neither interpretation implies that the average up to report i is inversely related to the rating at report i.",
                "There might exist a measure of influence exerted by past reports that pushes the user behind report i to submit ratings which to some extent conforms with past reports: a low value for ef (i) can influence user i to submit a low rating for feature f because, for example, he fears that submitting a high rating will make him out to be a person with low standards5 .",
                "This, at first, appears to contradict Hypothesis 2.",
                "However, this conformity rating cannot continue indefinitely: once the set of reports project a sufficiently deflated estimate for vf , future reviewers with comparatively positive impressions will seek to correct this misconception. 4.2 Impact of textual comments on quality expectation Further insight into the rating behavior of TripAdvisor users can be obtained by analyzing the relationship between the weights wf and the values ef (i).",
                "In particular, we examine the following hypothesis: Hypothesis 3.",
                "When a large proportion of the text of a review discusses a certain feature, the difference between the rating for that feature and the average rating up to that point tends to be large.",
                "The intuition behind this claim is that when the user is adamant about voicing his opinion regarding a certain feature, his opinion differs from the collective opinion of previous postings.",
                "This relies on the characteristic of reputation systems as feedback forums where a user is interested in projecting his opinion, with particular strength if this opinion differs from what he perceives to be the general opinion.",
                "To test Hypothesis 3 we measure the average absolute difference between the expectation ef (i) and the rating ri f when the weight wi f is high, respectively low.",
                "Weights are classified high or low by comparing them with certain cutoff values: wi f is low if smaller than 0.1, while wi f is high if greater than θf .",
                "Different cutoff values were used for different features: θR = 0.4, θS = 0.4, θC = 0.2, and θV = 0.7.",
                "Cleanliness has a lower cutoff since it is a feature rarely discussed; Value has a high cutoff for the opposite reason.",
                "Results are presented in Table 4. 5 The idea that negative reports can encourage further negative reporting has been suggested before [14] Table 4: Average of |ri f −ef (i)| when weights are high (first value in the cell) and low (second value in the cell) with P-values for the difference in sq. brackets.",
                "City R S C V 1.058 1.208 1.728 1.356 Boston 0.701 0.838 0.760 0.917 [0.022] [0.063] [0.000] [0.218] 1.048 1.351 1.218 1.318 Sydney 0.752 0.759 0.767 0.908 [0.179] [0.009] [0.165] [0.495] 1.184 1.378 1.472 1.642 Las Vegas 0.772 0.834 0.808 1.043 [0.071] [0.020] [0.006] [0.076] This demonstrates that when weights are unusually high, users tend to express an opinion that does not conform to the net average of previous ratings.",
                "As we might expect, for a feature that rarely was a high weight in the discussion, (e.g., cleanliness) the difference is particularly large.",
                "Even though the difference in the feature Value is quite large for Sydney, the P-value is high.",
                "This is because only few reviews discussed value heavily.",
                "The reason could be cultural or because there was less of a reason to discuss this feature. 4.3 Reporting Incentives Previous models suggest that users who are not highly opinionated will not choose to voice their opinions [12].",
                "In this section, we extend this model to account for the influence of expectations.",
                "The motivation for submitting feedback is not only due to extreme opinions, but also to the difference between the current reputation (i.e., the prior expectation of the user) and the actual experience.",
                "Such a rating model produces ratings that most of the time deviate from the current average rating.",
                "The ratings that confirm the prior expectation will rarely be submitted.",
                "We test on our data set the proportion of ratings that attempt to correct the current estimate.",
                "We define a deviant rating as one that deviates from the current expectation by at least some threshold θ, i.e., |ri f − ef (i)| ≥ θ.",
                "For each of the three considered cities, the following tables, show the proportion of deviant ratings for θ = 0.5 and θ = 1.",
                "Table 5: Proportion of deviant ratings with θ = 0.5 City O R S C V Boston 0.696 0.619 0.676 0.604 0.684 Sydney 0.645 0.615 0.672 0.614 0.675 Las Vegas 0.721 0.641 0.694 0.662 0.724 Table 6: Proportion of deviant ratings with θ = 1 City O R S C V Boston 0.420 0.397 0.429 0.317 0.446 Sydney 0.360 0.367 0.442 0.336 0.489 Las Vegas 0.510 0.421 0.483 0.390 0.472 The above results suggest that a large proportion of users (close to one half, even for the high threshold value θ = 1) deviate from the prior average.",
                "This reinforces the idea that users are more likely to submit a report when they believe they have something distinctive to add to the current stream of opinions for some feature.",
                "Such conclusions are in total agreement with prior evidence that the distribution of reports often follows bi-modal, U-shaped distributions. 139 5.",
                "MODELLING THE BEHAVIOR OF RATERS To account for the observations described in the previous sections, we propose a model for the behavior of the users when submitting online reviews.",
                "For a given hotel, we make the assumption that the quality experienced by the users is normally distributed around some value vf , which represents the objective quality offered by the hotel on the feature f. The rating submitted by user i on feature f is: ˆri f = δf vi f + (1 − δf ) · sign vi f − ef (i) c + d(vi f , ef (i)|wi f ) (2) where: • vi f is the (unknown) quality actually experienced by the user. vi f is assumed normally distributed around some value vf ; • δf ∈ [0, 1] can be seen as a measure of the bias when reporting feedback.",
                "High values reflect the fact that users rate objectively, without being influenced by prior expectations.",
                "The value of δf may depend on various factors; we fix one value for each feature f; • c is a constant between 1 and 5; • wi f is the weight of feature f in the textual comment of review i, computed according to Eq. (1); • d(vi f , ef (i)|wi f ) is a distance function between the expectation and the observation of user i.",
                "The distance function satisfies the following properties: - d(y, z|w) ≥ 0 for all y, z ∈ [0, 5], w ∈ [0, 1]; - |d(y, z|w)| < |d(z, x|w)| if |y − z| < |z − x|; - |d(y, z|w1)| < |d(y, z|w2)| if w1 < w2; - c + d(vf , ef (i)|wi f ) ∈ [1, 5]; The second term of Eq. (2) encodes the bias of the rating.",
                "The higher the distance between the true observation vi f and the function ef , the higher the bias. 5.1 Model Validation We use the data set of TripAdvisor reviews to validate the behavior model presented above.",
                "We split for convenience the rating values in three ranges: bad (B = {1, 2}), indifferent (I = {3, 4}), and good (G = {5}), and perform the following two tests: • First, we will use our model to predict the ratings that have extremal values.",
                "For every hotel, we take the sequence of reports, and whenever we encounter a rating that is either good or bad (but not indifferent) we try to predict it using Eq. (2) • Second, instead of predicting the value of extremal ratings, we try to classify them as either good or bad.",
                "For every hotel we take the sequence of reports, and for each report (regardless of it value) we classify it as being good or bad However, to perform these tests, we need to estimate the objective value, vf , that is the average of the true quality observations, vi f .",
                "The algorithm we are using is based on the intuition that the amount of conformity rating is minimized.",
                "In other words, the value vf should be such that as often as possible, bad ratings follow expectations above vf and good ratings follow expectations below vf .",
                "Formally, we define the sets: Γ1 = {i|ef (i) < vf and ri f ∈ B}; Γ2 = {i|ef (i) > vf and ri f ∈ G}; that correspond to irregularities where even though the expectation at point i is lower than the delivered value, the rating is poor, and vice versa.",
                "We define vf as the value that minimize these union of the two sets: vf = arg min vf |Γ1 ∪ Γ2| (3) In Eq. (2) we replace vi f by the value vf computed in Eq. (3), and use the following distance function: d(vf , ef (i)|wi f ) = |vf − ef (i)| vf − ef (i) |vf 2 − ef (i)2 | · (1 + 2wi f ); The constant c ∈ I was set to min{max{ef (i), 3}}, 4}.",
                "The values for δf were fixed at {0.7, 0.7, 0.8, 0.7, 0.6} for the features {Overall, Rooms, Service, Cleanliness, Value} respectively.",
                "The weights are computed as described in Section 3.",
                "As a first experiment, we take the sets of extremal ratings {ri f |ri f /∈ I} for each hotel and feature.",
                "For every such rating, ri f , we try to estimate it by computing ˆri f using Eq. (2).",
                "We compare this estimator with the one obtained by simply averaging the ratings over all hotels and features: i.e., ¯rf = j,r j f =0 rj f j,r j f =0 1 ; Table 7 presents the ratio between the root mean square error (RMSE) when using ˆri f and ¯rf to estimate the actual ratings.",
                "In all cases the estimate produced by our model is better than the simple average.",
                "Table 7: Average of RMSE(ˆrf ) RMSE(¯rf ) City O R S C V Boston 0.987 0.849 0.879 0.776 0.913 Sydney 0.927 0.817 0.826 0.720 0.681 Las Vegas 0.952 0.870 0.881 0.947 0.904 As a second experiment, we try to distinguish the sets Bf = {i|ri f ∈ B} and Gf = {i|ri f ∈ G} of bad, respectively good ratings on the feature f. For example, we compute the set Bf using the following classifier (called σ): ri f ∈ Bf (σf (i) = 1) ⇔ ˆri f ≤ 4; Tables 8, 9 and 10 present the Precision(p), Recall(r) and s = 2pr p+r for classifier σ, and compares it with a naive majority classifier, τ, τf (i) = 1 ⇔ |Bf | ≥ |Gf |: We see that recall is always higher for σ and precision is usually slightly worse.",
                "For the s metric σ tends to add a 140 Table 8: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Boston O R S C V p 0.678 0.670 0.573 0.545 0.610 σ r 0.626 0.659 0.619 0.612 0.694 s 0.651 0.665 0.595 0.577 0.609 p 0.684 0.706 0.647 0.611 0.633 τ r 0.597 0.541 0.410 0.383 0.562 s 0.638 0.613 0.502 0.471 0.595 Table 9: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Las Vegas O R S C V p 0.654 0.748 0.592 0.712 0.583 σ r 0.608 0.536 0.791 0.474 0.610 s 0.630 0.624 0.677 0.569 0.596 p 0.685 0.761 0.621 0.748 0.606 τ r 0.542 0.505 0.767 0.445 0.441 s 0.605 0.607 0.670 0.558 0.511 1-20% improvement over τ, much higher in some cases for hotels in Sydney.",
                "This is likely because Sydney reviews are more positive than those of the American cities and cases where the number of bad reviews exceeded the number of good ones are rare.",
                "Replacing the test algorithm with one that plays a 1 with probability equal to the proportion of bad reviews improves its results for this city, but it is still outperformed by around 80%. 6.",
                "SUMMARY OF RESULTS AND CONCLUSION The goal of this paper is to explore the factors that drive a user to submit a particular rating, rather than the incentives that encouraged him to submit a report in the first place.",
                "For that we use two additional sources of information besides the vector of numerical ratings: first we look at the textual comments that accompany the reviews, and second we consider the reports that have been previously submitted by other users.",
                "Using simple natural language processing algorithms, we were able to establish a correlation between the weight of a certain feature in the textual comment accompanying the review, and the noise present in the numerical rating.",
                "Specifically, it seems that users who discuss amply a certain feature are likely to agree on a common rating.",
                "This observation allows the construction of feature-by-feature estimators of quality that have a lower variance, and are hopefully less noisy.",
                "Nevertheless, further evidence is required to support the intuition that ratings corresponding to high weights are expert opinions that deserve to be given higher priority when computing estimates of quality.",
                "Second, we emphasize the dependence of ratings on previous reports.",
                "Previous reports create an expectation of quality which affects the subjective perception of the user.",
                "We validate two facts about the hotel reviews we collected from TripAdvisor: First, the ratings following low expectations (where the expectation is computed as the average of the previous reports) are likely to be higher than the ratings Table 10: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Sydney O R S C V p 0.650 0.463 0.544 0.550 0.580 σ r 0.234 0.378 0.571 0.169 0.592 s 0.343 0.452 0.557 0.259 0.586 p 0.562 0.615 0.600 0.500 0.600 τ r 0.054 0.098 0.101 0.015 0.175 s 0.098 0.168 0.172 0.030 0.271 following high expectations.",
                "Intuitively, the perception of quality (and consequently the rating) depends on how well the actual experience of the user meets her expectation.",
                "Second, we include evidence from the textual comments, and find that when users devote a large fraction of the text to discussing a certain feature, they are likely to motivate a divergent rating (i.e., a rating that does not conform to the prior expectation).",
                "Intuitively, this supports the hypothesis that review forums act as discussion groups where users are keen on presenting and motivating their own opinion.",
                "We have captured the empirical evidence in a behavior model that predicts the ratings submitted by the users.",
                "The final rating depends, as expected, on the true observation, and on the gap between the observation and the expectation.",
                "The gap tends to have a bigger influence when an important fraction of the textual comment is dedicated to discussing a certain feature.",
                "The proposed model was validated on the empirical data and provides better estimates of the ratings actually submitted.",
                "One assumption that we make is about the existence of an objective quality value vf for the feature f. This is rarely true, especially over large spans of time.",
                "Other explanations might account for the correlation of ratings with past reports.",
                "For example, if ef (i) reflects the true value of f at a point in time, the difference in the ratings following high and low expectations can be explained by hotel revenue models that are maximized when the value is modified accordingly.",
                "However, the idea that variation in ratings is not primarily a function of variation in value turns out to be a useful one.",
                "Our approach to approximate this elusive objective value is by no means perfect, but conforms neatly to the idea behind the model.",
                "A natural direction for future work is to examine concrete applications of our results.",
                "Significant improvements of quality estimates are likely to be obtained by incorporating all empirical evidence about rating behavior.",
                "Exactly how different factors affect the decisions of the users is not clear.",
                "The answer might depend on the particular application, context and culture. 7.",
                "REFERENCES [1] A. Admati and P. Pfleiderer.",
                "Noisytalk.com: Broadcasting opinions in a noisy environment.",
                "Working Paper 1670R, Stanford University, 2000. [2] P. B., L. Lee, and S. Vaithyanathan.",
                "Thumbs up? sentiment classification using machine learning techniques.",
                "In Proceedings of the EMNLP-02, the Conference on Empirical Methods in Natural Language Processing, 2002. [3] H. Cui, V. Mittal, and M. Datar.",
                "Comparative 141 Experiments on Sentiment Classification for Online Product Reviews.",
                "In Proceedings of AAAI, 2006. [4] K. Dave, S. Lawrence, and D. Pennock.",
                "Mining the peanut gallery:opinion extraction and semantic classification of product reviews.",
                "In Proceedings of the 12th International Conference on the World Wide Web (WWW03), 2003. [5] C. Dellarocas, N. Awad, and X. Zhang.",
                "Exploring the Value of Online Product Ratings in Revenue Forecasting: The Case of Motion Pictures.",
                "Working paper, 2006. [6] C. Forman, A. Ghose, and B. Wiesenfeld.",
                "A Multi-Level Examination of the Impact of Social Identities on Economic Transactions in Electronic Markets.",
                "Available at SSRN: http://ssrn.com/abstract=918978, July 2006. [7] A. Ghose, P. Ipeirotis, and A. Sundararajan.",
                "Reputation Premiums in Electronic Peer-to-Peer Markets: Analyzing Textual Feedback and Network Structure.",
                "In Third Workshop on Economics of Peer-to-Peer Systems, (P2PECON), 2005. [8] A. Ghose, P. Ipeirotis, and A. Sundararajan.",
                "The Dimensions of Reputation in electronic Markets.",
                "Working Paper CeDER-06-02, New York University, 2006. [9] A. Harmon.",
                "Amazon Glitch Unmasks War of Reviewers.",
                "The New York Times, February 14, 2004. [10] D. Houser and J. Wooders.",
                "Reputation in Auctions: Theory and Evidence from eBay.",
                "Journal of Economics and Management Strategy, 15:353-369, 2006. [11] M. Hu and B. Liu.",
                "Mining and summarizing customer reviews.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD04), 2004. [12] N. Hu, P. Pavlou, and J. Zhang.",
                "Can Online Reviews Reveal a Products True Quality?",
                "In Proceedings of ACM Conference on Electronic Commerce (EC 06), 2006. [13] K. Kalyanam and S. McIntyre.",
                "Return on reputation in online auction market.",
                "Working Paper 02/03-10-WP, Leavey School of Business, Santa Clara University., 2001. [14] L. Khopkar and P. Resnick.",
                "Self-Selection, Slipping, Salvaging, Slacking, and Stoning: the Impacts of Negative Feedback at eBay.",
                "In Proceedings of ACM Conference on Electronic Commerce (EC 05), 2005. [15] M. Melnik and J. Alm.",
                "Does a sellers reputation matter? evidence from ebay auctions.",
                "Journal of Industrial Economics, 50(3):337-350, 2002. [16] R. Olshavsky and J. Miller.",
                "Consumer Expectations, Product Performance and Perceived Product Quality.",
                "Journal of Marketing Research, 9:19-21, February 1972. [17] A. Parasuraman, V. Zeithaml, and L. Berry.",
                "A Conceptual Model of Service Quality and Its Implications for Future Research.",
                "Journal of Marketing, 49:41-50, 1985. [18] A. Parasuraman, V. Zeithaml, and L. Berry.",
                "SERVQUAL: A Multiple-Item Scale for Measuring Consumer Perceptions of Service Quality.",
                "Journal of Retailing, 64:12-40, 1988. [19] P. Pavlou and A. Dimoka.",
                "The Nature and Role of Feedback Text Comments in Online Marketplaces: Implications for Trust Building, Price Premiums, and Seller Differentiation.",
                "Information Systems Research, 17(4):392-414, 2006. [20] A. Popescu and O. Etzioni.",
                "Extracting product features and opinions from reviews.",
                "In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, 2005. [21] R. Teas.",
                "Expectations, Performance Evaluation, and Consumers Perceptions of Quality.",
                "Journal of Marketing, 57:18-34, 1993. [22] E. White.",
                "Chatting a Singer Up the Pop Charts.",
                "The Wall Street Journal, October 15, 1999.",
                "APPENDIX A.",
                "LIST OF WORDS, LR, ASSOCIATED TO THE FEATURE ROOMS All words serve as prefixes: room, space, interior, decor, ambiance, atmosphere, comfort, bath, toilet, bed, building, wall, window, private, temperature, sheet, linen, pillow, hot, water, cold, water, shower, lobby, furniture, carpet, air, condition, mattress, layout, design, mirror, ceiling, lighting, lamp, sofa, chair, dresser, wardrobe, closet 142"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "semantic orientation of product evaluation": {
            "translated_key": "orientación semántica de las evaluaciones de productos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Understanding User Behavior in Online Feedback Reporting Arjun Talwar Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland arjun@math.stanford.edu Radu Jurca Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland radu.jurca@epfl.ch Boi Faltings Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland boi.faltings@epfl.ch ABSTRACT Online reviews have become increasingly popular as a way to judge the quality of various products and services.",
                "Previous work has demonstrated that contradictory reporting and underlying user biases make judging the true worth of a service difficult.",
                "In this paper, we investigate underlying factors that influence user behavior when reporting feedback.",
                "We look at two sources of information besides numerical ratings: linguistic evidence from the textual comment accompanying a review, and patterns in the time sequence of reports.",
                "We first show that groups of users who amply discuss a certain feature are more likely to agree on a common rating for that feature.",
                "Second, we show that a users rating partly reflects the difference between true quality and prior expectation of quality as inferred from previous reviews.",
                "Both give us a less noisy way to produce rating estimates and reveal the reasons behind user bias.",
                "Our hypotheses were validated by statistical evidence from hotel reviews on the TripAdvisor website.",
                "Categories and Subject Descriptors J.4 [Social and Behavioral Sciences]: Economics General Terms Economics, Experimentation, Reliability 1.",
                "MOTIVATIONS The spread of the internet has made it possible for online feedback forums (or reputation mechanisms) to become an important channel for Word-of-mouth regarding products, services or other types of commercial interactions.",
                "Numerous empirical studies [10, 15, 13, 5] show that buyers seriously consider online feedback when making purchasing decisions, and are willing to pay reputation premiums for products or services that have a good reputation.",
                "Recent analysis, however, raises important questions regarding the ability of existing forums to reflect the real quality of a product.",
                "In the absence of clear incentives, users with a moderate outlook will not bother to voice their opinions, which leads to an unrepresentative sample of reviews.",
                "For example, [12, 1] show that Amazon1 ratings of books or CDs follow with great probability bi-modal, U-shaped distributions where most of the ratings are either very good, or very bad.",
                "Controlled experiments, on the other hand, reveal opinions on the same items that are normally distributed.",
                "Under these circumstances, using the arithmetic mean to predict quality (as most forums actually do) gives the typical user an estimator with high variance that is often false.",
                "Improving the way we aggregate the information available from online reviews requires a deep understanding of the underlying factors that bias the rating behavior of users.",
                "Hu et al. [12] propose the Brag-and-Moan Model where users rate only if their utility of the product (drawn from a normal distribution) falls outside a median interval.",
                "The authors conclude that the model explains the empirical distribution of reports, and offers insights into smarter ways of estimating the true quality of the product.",
                "In the present paper we extend this line of research, and attempt to explain further facts about the behavior of users when reporting online feedback.",
                "Using actual hotel reviews from the TripAdvisor2 website, we consider two additional sources of information besides the basic numerical ratings submitted by users.",
                "The first is simple linguistic evidence from the textual review that usually accompanies the numerical ratings.",
                "We use text-mining techniques similar to [7] and [3], however, we are only interested in identifying what aspects of the service the user is discussing, without computing the semantic orientation of the text.",
                "We find that users who comment more on the same feature are more likely to agree on a common numerical rating for that particular feature.",
                "Intuitively, lengthy comments reveal the importance of the feature to the user.",
                "Since people tend to be more knowledgeable in the aspects they consider important, users who discuss a given feature in more details might be assumed to have more authority in evaluating that feature.",
                "Second we investigate the relationship between a review 1 http://www.amazon.com 2 http://www.tripadvisor.com/ 134 Figure 1: The TripAdvisor page displaying reviews for a popular Boston hotel.",
                "Name of hotel and advertisements were deliberatively erased. and the reviews that preceded it.",
                "A perusal of online reviews shows that ratings are often part of discussion threads, where one post is not necessarily independent of other posts.",
                "One may see, for example, users who make an effort to contradict, or vehemently agree with, the remarks of previous users.",
                "By analyzing the time sequence of reports, we conclude that past reviews influence the future reports, as they create some prior expectation regarding the quality of service.",
                "The subjective perception of the user is influenced by the gap between the prior expectation and the actual performance of the service [17, 18, 16, 21] which will later reflect in the users rating.",
                "We propose a model that captures the dependence of ratings on prior expectations, and validate it using the empirical data we collected.",
                "Both results can be used to improve the way reputation mechanisms aggregate the information from individual reviews.",
                "Our first result can be used to determine a featureby-feature estimate of quality, where for each feature, a different subset of reviews (i.e., those with lengthy comments of that feature) is considered.",
                "The second leads to an algorithm that outputs a more precise estimate of the real quality. 2.",
                "THE DATA SET We use in this paper real hotel reviews collected from the popular travel site TripAdvisor.",
                "TripAdvisor indexes hotels from cities across the world, along with reviews written by travelers.",
                "Users can search the site by giving the hotels name and location (optional).",
                "The reviews for a given hotel are displayed as a list (ordered from the most recent to the oldest), with 5 reviews per page.",
                "The reviews contain: • information about the author of the review (e.g., dates of stay, username of the reviewer, location of the reviewer); • the overall rating (from 1, lowest, to 5, highest); • a textual review containing a title for the review, free comments, and the main things the reviewer liked and disliked; • numerical ratings (from 1, lowest, to 5, highest) for different features (e.g., cleanliness, service, location, etc.)",
                "Below the name of the hotel, TripAdvisor displays the address of the hotel, general information (number of rooms, number of stars, short description, etc), the average overall rating, the TripAdvisor ranking, and an average rating for each feature.",
                "Figure 1 shows the page for a popular Boston hotel whose name (along with advertisements) was explicitly erased.",
                "We selected three cities for this study: Boston, Sydney and Las Vegas.",
                "For each city we considered all hotels that had at least 10 reviews, and recorded all reviews.",
                "Table 1 presents the number of hotels considered in each city, the total number of reviews recorded for each city, and the distribution of hotels with respect to the star-rating (as available on the TripAdvisor site).",
                "Note that not all hotels have a star-rating.",
                "Table 1: A summary of the data set.",
                "City # Reviews # Hotels # of Hotels with 1,2,3,4 & 5 stars Boston 3993 58 1+3+17+15+2 Sydney 1371 47 0+0+9+13+10 Las Vegas 5593 40 0+3+10+9+6 For each review we recorded the overall rating, the textual review (title and body of the review) and the numerical rating on 7 features: Rooms(R), Service(S), Cleanliness(C), Value(V), Food(F), Location(L) and Noise(N).",
                "TripAdvisor does not require users to submit anything other than the overall rating, hence a typical review rates few additional features, regardless of the discussion in the textual comment.",
                "Only the features Rooms(R), Service(S), Cleanliness(C) and Value(V) are rated by a significant number of users.",
                "However, we also selected the features Food(F), Location(L) and Noise(N) because they are referred to in a significant number of textual comments.",
                "For each feature we record the numerical rating given by the user, or 0 when the rating is missing.",
                "The typical length of the textual comment amounts to approximately 200 words.",
                "All data was collected by crawling the TripAdvisor site in September 2006. 2.1 Formal notation We will formally refer to a review by a tuple (r, T) where: • r = (rf ) is a vector containing the ratings rf ∈ {0, 1, . . . 5} for the features f ∈ F = {O, R, S, C, V, F, L, N}; note that the overall rating, rO, is abusively recorded as the rating for the feature Overall(O); • T is the textual comment that accompanies the review. 135 Reviews are indexed according to the variable i, such that (ri , Ti ) is the ith review in our database.",
                "Since we dont record the username of the reviewer, we will also say that the ith review in our data set was submitted by user i.",
                "When we need to consider only the reviews of a given hotel, h, we will use (ri(h) , Ti(h) ) to denote the ith review about the hotel h. 3.",
                "EVIDENCE FROM TEXTUAL COMMENTS The free textual comments associated to online reviews are a valuable source of information for understanding the reasons behind the numerical ratings left by the reviewers.",
                "The text may, for example, reveal concrete examples of aspects that the user liked or disliked, thus justifying some of the high, respectively low ratings for certain features.",
                "The text may also offer guidelines for understanding the preferences of the reviewer, and the weights of different features when computing an overall rating.",
                "The problem, however, is that free textual comments are difficult to read.",
                "Users are required to scroll through many reviews and read mostly repetitive information.",
                "Significant improvements would be obtained if the reviews were automatically interpreted and aggregated.",
                "Unfortunately, this seems a difficult task for computers since human users often use witty language, abbreviations, cultural specific phrases, and the figurative style.",
                "Nevertheless, several important results use the textual comments of online reviews in an automated way.",
                "Using well established natural language techniques, reviews or parts of reviews can be classified as having a positive or negative semantic orientation.",
                "Pang et al. [2] classify movie reviews into positive/negative by training three different classifiers (Naive Bayes, Maximum Entropy and SVM) using classification features based on unigrams, bigrams or part-of-speech tags.",
                "Dave et al. [4] analyze reviews from CNet and Amazon, and surprisingly show that classification features based on unigrams or bigrams perform better than higher-order n-grams.",
                "This result is challenged by Cui et al. [3] who look at large collections of reviews crawled from the web.",
                "They show that the size of the data set is important, and that bigger training sets allow classifiers to successfully use more complex classification features based on n-grams.",
                "Hu and Liu [11] also crawl the web for product reviews and automatically identify product attributes that have been discussed by reviewers.",
                "They use Wordnet to compute the <br>semantic orientation of product evaluation</br>s and summarize user reviews by extracting positive and negative evaluations of different product features.",
                "Popescu and Etzioni [20] analyze a similar setting, but use search engine hit-counts to identify product attributes; the semantic orientation is assigned through the relaxation labeling technique.",
                "Ghose et al. [7, 8] analyze seller reviews from the Amazon secondary market to identify the different dimensions (e.g., delivery, packaging, customer support, etc.) of reputation.",
                "They parse the text, and tag the part-of-speech for each word.",
                "Frequent nouns, noun phrases and verbal phrases are identified as dimensions of reputation, while the corresponding modifiers (i.e., adjectives and adverbs) are used to derive numerical scores for each dimension.",
                "The enhanced reputation measure correlates better with the pricing information observed in the market.",
                "Pavlou and Dimoka [19] analyze eBay reviews and find that textual comments have an important impact on reputation premiums.",
                "Our approach is similar to the previously mentioned works, in the sense that we identify the aspects (i.e., hotel features) discussed by the users in the textual reviews.",
                "However, we do not compute the semantic orientation of the text, nor attempt to infer missing ratings.",
                "We define the weight, wi f , of feature f ∈ F in the text Ti associated with the review (ri , Ti ), as the fraction of Ti dedicated to discussing aspects (both positive and negative) related to feature f. We propose an elementary method to approximate the values of these weights.",
                "For each feature we manually construct the word list Lf containing approximately 50 words that are most commonly associated to the feature f. The initial words were selected from reading some of the reviews, and seeing what words coincide with discussion of which features.",
                "The list was then extended by adding all thesaurus entries that were related to the initial words.",
                "Finally, we brainstormed for missing words that would normally be associated with each of the features.",
                "Let Lf ∩Ti be the list of terms common to both Lf and Ti.",
                "Each term of Lf is counted the number of times it appears in Ti , with two exceptions: • in cases where the user submits a title to the review, we account for the title text by appending it three times to the review text Ti .",
                "The intuitive assumption is that the users opinion is more strongly reflected in the title, rather than in the body of the review.",
                "For example, many reviews are accurately summarized by titles such as Excellent service, terrible location or Bad value for money; • certain words that occur only once in the text are counted multiple times if their relevance to that feature is particularly strong.",
                "These were root words for each feature (e.g., staff is a root word for the feature Service), and were weighted either 2 or 3.",
                "Each feature was assigned up to 3 such root words, so almost all words are counted only once.",
                "The list of words for the feature Rooms is given for reference in Appendix A.",
                "The weight wi f is computed as: wi f = |Lf ∩ Ti| f∈F |Lf ∩ Ti| (1) where |Lf ∩Ti | is the number of terms common to Lf and Ti .",
                "The weight for the feature Overall was set to min{ |T i | 5000 , 1} where |Ti | is the number of character in Ti .",
                "The following is a TripAdvisor review for a Boston hotel (the name of the hotel is omitted): Ill start by saying that Im more of a Holiday Inn person than a *** type.",
                "So I get frustrated when I pay double the room rate and get half the amenities that Id get at a Hampton Inn or Holiday Inn.",
                "The location was definitely the main asset of this place.",
                "It was only a few blocks from the Hynes Center subway stop and it was easy to walk to some good restaurants in the Back Bay area.",
                "Boylston isnt far off at all.",
                "So I had no trouble with foregoing a rental car and taking the subway from the airport to the hotel and using the subway for any other travel.",
                "Otherwise, they make you pay for anything and everything. 136 And when youve already dropped $215/night on the room, that gets frustrating.The room itself was decent, about what I would expect.",
                "Staff was also average, not bad and not excellent.",
                "Again, I think youre paying for location and the ability to walk to a lot of good stuff.",
                "But I think next time Ill stay in Brookline, get more amenities, and use the subway a bit more.",
                "This numerical ratings associated to this review are rO = 3, rR = 3, rS = 3, rC = 4, rV = 2 for features Overall(O), Rooms(R), Service(S), Cleanliness(C) and Value(V) respectively.",
                "The ratings for the features Food(F), Location(L) and Noise(N) are absent (i.e., rF = rL = rN = 0).",
                "The weights wf are computed from the following lists of common terms: LR ∩ T ={room}; wR = 0.066 LS ∩ T ={3 * Staff, amenities}; wS = 0.267 LC ∩ T = ∅; wC = 0 LV ∩ T ={$, rate}; wV = 0.133 LF ∩ T ={restaurant}; wF = 0.067 LL ∩ T ={2 * center, 2 * walk, 2 * location, area}; wL = 0.467 LN ∩ T = ∅; wN = 0 The root words Staff and Center were tripled and doubled respectively.",
                "The overall weight of the textual review is wO = 0.197.",
                "These values account reasonably well for the weights of different features in the discussion of the reviewer.",
                "One point to note is that some terms in the lists Lf possess an inherent semantic orientation.",
                "For example the word grime (belonging to the list LC ) would be used most often to assert the presence, and not the absence of grime.",
                "This is unavoidable, but care was taken to ensure words from both sides of the spectrum were used.",
                "For this reason, some lists such as LR contain only nouns of objects that one would typically describe in a room (see Appendix A).",
                "The goal of this section is to analyse the influence of the weights wi f on the numerical ratings ri f .",
                "Intuitively, users who spent a lot of their time discussing a feature f (i.e., wi f is high) had something to say about their experience with regard to this feature.",
                "Obviously, feature f is important for user i.",
                "Since people tend to be more knowledgeable in the aspects they consider important, our hypothesis is that the ratings ri f (corresponding to high weights wi f ) constitute a subset of expert ratings for feature f. Figure 2 plots the distribution of the rates r i(h) C with respect to the weights w i(h) C for the cleanliness of a Las Vegas hotel, h. Here, the high ratings are restricted to the reviews that discuss little the cleanliness.",
                "Whenever cleanliness appears in the discussion, the ratings are low.",
                "Many hotels exhibit similar rating patterns for various features.",
                "Ratings corresponding to low weights span the whole spectrum from 1 to 5, while the ratings corresponding to high weights are more grouped together (either around good or bad ratings).",
                "We therefore make the following hypothesis: Hypothesis 1.",
                "The ratings ri f corresponding to the reviews where wi f is high, are more similar to each other than to the overall collection of ratings.",
                "To test the hypothesis, we take the entire set of reviews, and feature by feature, we compute the standard deviation of the ratings with high weights, and the standard deviation of the entire set of ratings.",
                "High weights were defined as those belonging to the upper 20% of the weight range for the corresponding feature.",
                "If Hypothesis 1 were true, the standard deviation of all ratings should be higher than the standard deviation of the ratings with high weights. 0 1 2 3 4 5 6 0 0.1 0.2 0.3 0.4 0.5 0.6 Rating Weight Figure 2: The distribution of ratings against the weight of the cleanliness feature.",
                "We use a standard T-test to measure the significance of the results.",
                "City by city and feature by feature, Table 2 presents the average standard deviation of all ratings, and the average standard deviation of ratings with high weights.",
                "Indeed, the ratings with high weights have lower standard deviation, and the results are significant at the standard 0.05 significance threshold (although for certain cities taken independently there doesnt seem to be a significant difference, the results are significant for the entire data set).",
                "Please note that only the features O,R,S,C and V were considered, since for the others (F, L, and N) we didnt have enough ratings.",
                "Table 2: Average standard deviation for all ratings, and average standard deviation for ratings with high weights.",
                "In square brackets, the corresponding p-values for a positive difference between the two.",
                "City O R S C V all 1.189 0.998 1.144 0.935 1.123 Boston high 0.948 0.778 0.954 0.767 0.891 p-val [0.000] [0.004] [0.045] [0.080] [0.009] all 1.040 0.832 1.101 0.847 0.963 Sydney high 0.801 0.618 0.691 0.690 0.798 p-val [0.012] [0.023] [0.000] [0.377] [0.037] all 1.272 1.142 1.184 1.119 1.242 Vegas high 1.072 0.752 1.169 0.907 1.003 p-val [0.0185] [0.001] [0.918] [0.120] [0.126] Hypothesis 1 not only provides some basic understanding regarding the rating behavior of online users, it also suggests some ways of computing better quality estimates.",
                "We can, for example, construct a feature-by-feature quality estimate with much lower variance: for each feature we take the subset of reviews that amply discuss that feature, and output as a quality estimate the average rating for this subset.",
                "Initial experiments suggest that the average feature-by-feature ratings computed in this way are different from the average ratings computed on the whole data set.",
                "Given that, indeed, high weights are indicators of expert opinions, the estimates obtained in this way are more accurate than the current ones.",
                "Nevertheless, the validation of this underlying assumption requires further controlled experiments. 137 4.",
                "THE INFLUENCE OF PAST RATINGS Two important assumptions are generally made about reviews submitted to online forums.",
                "The first is that ratings truthfully reflect the quality observed by the users; the second is that reviews are independent from one another.",
                "While anecdotal evidence [9, 22] challenges the first assumption3 , in this section, we address the second.",
                "A perusal of online reviews shows that reviews are often part of discussion threads, where users make an effort to contradict, or vehemently agree with the remarks of previous users.",
                "Consider, for example, the following review: I dont understand the negative reviews... the hotel was a little dark, but that was the style.",
                "It was very artsy.",
                "Yes it was close to the freeway, but in my opinion the sound of an occasional loud car is better than hearing the ding ding of slot machines all night!",
                "The staff on-hand is FABULOUS.",
                "The waitresses are great (and *** does not deserve the bad review she got, she was 100% attentive to us! ), the bartenders are friendly and professional at the same time...",
                "Here, the user was disturbed by previous negative reports, addressed these concerns, and set about trying to correct them.",
                "Not surprisingly, his ratings were considerably higher than the average ratings up to this point.",
                "It seems that TripAdvisor users regularly read the reports submitted by previous users before booking a hotel, or before writing a review.",
                "Past reviews create some prior expectation regarding the quality of service, and this expectation has an influence on the submitted review.",
                "We believe this observation holds for most online forums.",
                "The subjective perception of quality is directly proportional to how well the actual experience meets the prior expectation, a fact confirmed by an important line of econometric and marketing research [17, 18, 16, 21].",
                "The correlation between the reviews has also been confirmed by recent research on the dynamics of online review forums [6]. 4.1 Prior Expectations We define the prior expectation of user i regarding the feature f, as the average of the previously available ratings on the feature f4 : ef (i) = j<i,r j f =0 rj f j<i,r j f =0 1 As a first hypothesis, we assert that the rating ri f is a function of the prior expectation ef (i): Hypothesis 2.",
                "For a given hotel and feature, given the reviews i and j such that ef (i) is high and ef (j) is low, the rating rj f exceeds the rating ri f .",
                "We define high and low expectations as those that are above, respectively below a certain cutoff value θ.",
                "The set of reviews preceded by high, respectively low expectations 3 part of Amazon reviews were recognized as strategic posts by book authors or competitors 4 if no previous ratings were assigned for feature f, ef (i) is assigned a default value of 4.",
                "Table 3: Average ratings for reviews preceded by low (first value in the cell) and high (second value in the cell) expectations.",
                "The P-values for a positive difference are given square brackets.",
                "City O R S C V 3.953 4.045 3.985 4.252 3.946 Boston 3.364 3.590 3.485 3.641 3.242 [0.011] [0.028] [0.0086] [0.0168] [0.0034] 4.284 4.358 4.064 4.530 4.428 Sydney 3.756 3.537 3.436 3.918 3.495 [0.000] [0.000] [0.035] [0.009] [0.000] 3.494 3.674 3.713 3.689 3.580 Las Vegas 3.140 3.530 2.952 3.530 3.351 [0.190] [0.529] [0.007] [0.529] [0.253] are defined as follows: Rhigh f = {ri f |ef (i) > θ} Rlow f = {ri f |ef (i) < θ} These sets are specific for each (hotel, feature) pair, and in our experiments we took θ = 4.",
                "This rather high value is close to the average rating across all features across all hotels, and is justified by the fact that our data set contains mostly high quality hotels.",
                "For each city, we take all hotels and compute the average ratings in the sets Rhigh f and Rlow f (see Table 3).",
                "The average rating amongst reviews following low prior expectations is significantly higher than the average rating following high expectations.",
                "As further evidence, we consider all hotels for which the function eV (i) (the expectation for the feature Value) has a high value (greater than 4) for some i, and a low value (less than 4) for some other i.",
                "Intuitively, these are the hotels for which there is a minimal degree of variation in the timely sequence of reviews: i.e., the cumulative average of ratings was at some point high and afterwards became low, or vice-versa.",
                "Such variations are observed for about half of all hotels in each city.",
                "Figure 3 plots the median (across considered hotels) rating, rV , when ef (i) is not more than x but greater than x − 0.5. 2.5 3 3.5 4 4.5 5 2.5 3 3.5 4 4.5 5 Medianofrating expectation Boston Sydney Vegas Figure 3: The ratings tend to decrease as the expectation increases. 138 There are two ways to interpret the function ef (i): • The expected value for feature f obtained by user i before his experience with the service, acquired by reading reports submitted by past users.",
                "In this case, an overly high value for ef (i) would drive the user to submit a negative report (or vice versa), stemming from the difference between the actual value of the service, and the inflated expectation of this value acquired before his experience. • The expected value of feature f for all subsequent visitors of the site, if user i were not to submit a report.",
                "In this case, the motivation for a negative report following an overly high value of ef is different: user i seeks to correct the expectation of future visitors to the site.",
                "Unlike the interpretation above, this does not require the user to derive an a priori expectation for the value of f. Note that neither interpretation implies that the average up to report i is inversely related to the rating at report i.",
                "There might exist a measure of influence exerted by past reports that pushes the user behind report i to submit ratings which to some extent conforms with past reports: a low value for ef (i) can influence user i to submit a low rating for feature f because, for example, he fears that submitting a high rating will make him out to be a person with low standards5 .",
                "This, at first, appears to contradict Hypothesis 2.",
                "However, this conformity rating cannot continue indefinitely: once the set of reports project a sufficiently deflated estimate for vf , future reviewers with comparatively positive impressions will seek to correct this misconception. 4.2 Impact of textual comments on quality expectation Further insight into the rating behavior of TripAdvisor users can be obtained by analyzing the relationship between the weights wf and the values ef (i).",
                "In particular, we examine the following hypothesis: Hypothesis 3.",
                "When a large proportion of the text of a review discusses a certain feature, the difference between the rating for that feature and the average rating up to that point tends to be large.",
                "The intuition behind this claim is that when the user is adamant about voicing his opinion regarding a certain feature, his opinion differs from the collective opinion of previous postings.",
                "This relies on the characteristic of reputation systems as feedback forums where a user is interested in projecting his opinion, with particular strength if this opinion differs from what he perceives to be the general opinion.",
                "To test Hypothesis 3 we measure the average absolute difference between the expectation ef (i) and the rating ri f when the weight wi f is high, respectively low.",
                "Weights are classified high or low by comparing them with certain cutoff values: wi f is low if smaller than 0.1, while wi f is high if greater than θf .",
                "Different cutoff values were used for different features: θR = 0.4, θS = 0.4, θC = 0.2, and θV = 0.7.",
                "Cleanliness has a lower cutoff since it is a feature rarely discussed; Value has a high cutoff for the opposite reason.",
                "Results are presented in Table 4. 5 The idea that negative reports can encourage further negative reporting has been suggested before [14] Table 4: Average of |ri f −ef (i)| when weights are high (first value in the cell) and low (second value in the cell) with P-values for the difference in sq. brackets.",
                "City R S C V 1.058 1.208 1.728 1.356 Boston 0.701 0.838 0.760 0.917 [0.022] [0.063] [0.000] [0.218] 1.048 1.351 1.218 1.318 Sydney 0.752 0.759 0.767 0.908 [0.179] [0.009] [0.165] [0.495] 1.184 1.378 1.472 1.642 Las Vegas 0.772 0.834 0.808 1.043 [0.071] [0.020] [0.006] [0.076] This demonstrates that when weights are unusually high, users tend to express an opinion that does not conform to the net average of previous ratings.",
                "As we might expect, for a feature that rarely was a high weight in the discussion, (e.g., cleanliness) the difference is particularly large.",
                "Even though the difference in the feature Value is quite large for Sydney, the P-value is high.",
                "This is because only few reviews discussed value heavily.",
                "The reason could be cultural or because there was less of a reason to discuss this feature. 4.3 Reporting Incentives Previous models suggest that users who are not highly opinionated will not choose to voice their opinions [12].",
                "In this section, we extend this model to account for the influence of expectations.",
                "The motivation for submitting feedback is not only due to extreme opinions, but also to the difference between the current reputation (i.e., the prior expectation of the user) and the actual experience.",
                "Such a rating model produces ratings that most of the time deviate from the current average rating.",
                "The ratings that confirm the prior expectation will rarely be submitted.",
                "We test on our data set the proportion of ratings that attempt to correct the current estimate.",
                "We define a deviant rating as one that deviates from the current expectation by at least some threshold θ, i.e., |ri f − ef (i)| ≥ θ.",
                "For each of the three considered cities, the following tables, show the proportion of deviant ratings for θ = 0.5 and θ = 1.",
                "Table 5: Proportion of deviant ratings with θ = 0.5 City O R S C V Boston 0.696 0.619 0.676 0.604 0.684 Sydney 0.645 0.615 0.672 0.614 0.675 Las Vegas 0.721 0.641 0.694 0.662 0.724 Table 6: Proportion of deviant ratings with θ = 1 City O R S C V Boston 0.420 0.397 0.429 0.317 0.446 Sydney 0.360 0.367 0.442 0.336 0.489 Las Vegas 0.510 0.421 0.483 0.390 0.472 The above results suggest that a large proportion of users (close to one half, even for the high threshold value θ = 1) deviate from the prior average.",
                "This reinforces the idea that users are more likely to submit a report when they believe they have something distinctive to add to the current stream of opinions for some feature.",
                "Such conclusions are in total agreement with prior evidence that the distribution of reports often follows bi-modal, U-shaped distributions. 139 5.",
                "MODELLING THE BEHAVIOR OF RATERS To account for the observations described in the previous sections, we propose a model for the behavior of the users when submitting online reviews.",
                "For a given hotel, we make the assumption that the quality experienced by the users is normally distributed around some value vf , which represents the objective quality offered by the hotel on the feature f. The rating submitted by user i on feature f is: ˆri f = δf vi f + (1 − δf ) · sign vi f − ef (i) c + d(vi f , ef (i)|wi f ) (2) where: • vi f is the (unknown) quality actually experienced by the user. vi f is assumed normally distributed around some value vf ; • δf ∈ [0, 1] can be seen as a measure of the bias when reporting feedback.",
                "High values reflect the fact that users rate objectively, without being influenced by prior expectations.",
                "The value of δf may depend on various factors; we fix one value for each feature f; • c is a constant between 1 and 5; • wi f is the weight of feature f in the textual comment of review i, computed according to Eq. (1); • d(vi f , ef (i)|wi f ) is a distance function between the expectation and the observation of user i.",
                "The distance function satisfies the following properties: - d(y, z|w) ≥ 0 for all y, z ∈ [0, 5], w ∈ [0, 1]; - |d(y, z|w)| < |d(z, x|w)| if |y − z| < |z − x|; - |d(y, z|w1)| < |d(y, z|w2)| if w1 < w2; - c + d(vf , ef (i)|wi f ) ∈ [1, 5]; The second term of Eq. (2) encodes the bias of the rating.",
                "The higher the distance between the true observation vi f and the function ef , the higher the bias. 5.1 Model Validation We use the data set of TripAdvisor reviews to validate the behavior model presented above.",
                "We split for convenience the rating values in three ranges: bad (B = {1, 2}), indifferent (I = {3, 4}), and good (G = {5}), and perform the following two tests: • First, we will use our model to predict the ratings that have extremal values.",
                "For every hotel, we take the sequence of reports, and whenever we encounter a rating that is either good or bad (but not indifferent) we try to predict it using Eq. (2) • Second, instead of predicting the value of extremal ratings, we try to classify them as either good or bad.",
                "For every hotel we take the sequence of reports, and for each report (regardless of it value) we classify it as being good or bad However, to perform these tests, we need to estimate the objective value, vf , that is the average of the true quality observations, vi f .",
                "The algorithm we are using is based on the intuition that the amount of conformity rating is minimized.",
                "In other words, the value vf should be such that as often as possible, bad ratings follow expectations above vf and good ratings follow expectations below vf .",
                "Formally, we define the sets: Γ1 = {i|ef (i) < vf and ri f ∈ B}; Γ2 = {i|ef (i) > vf and ri f ∈ G}; that correspond to irregularities where even though the expectation at point i is lower than the delivered value, the rating is poor, and vice versa.",
                "We define vf as the value that minimize these union of the two sets: vf = arg min vf |Γ1 ∪ Γ2| (3) In Eq. (2) we replace vi f by the value vf computed in Eq. (3), and use the following distance function: d(vf , ef (i)|wi f ) = |vf − ef (i)| vf − ef (i) |vf 2 − ef (i)2 | · (1 + 2wi f ); The constant c ∈ I was set to min{max{ef (i), 3}}, 4}.",
                "The values for δf were fixed at {0.7, 0.7, 0.8, 0.7, 0.6} for the features {Overall, Rooms, Service, Cleanliness, Value} respectively.",
                "The weights are computed as described in Section 3.",
                "As a first experiment, we take the sets of extremal ratings {ri f |ri f /∈ I} for each hotel and feature.",
                "For every such rating, ri f , we try to estimate it by computing ˆri f using Eq. (2).",
                "We compare this estimator with the one obtained by simply averaging the ratings over all hotels and features: i.e., ¯rf = j,r j f =0 rj f j,r j f =0 1 ; Table 7 presents the ratio between the root mean square error (RMSE) when using ˆri f and ¯rf to estimate the actual ratings.",
                "In all cases the estimate produced by our model is better than the simple average.",
                "Table 7: Average of RMSE(ˆrf ) RMSE(¯rf ) City O R S C V Boston 0.987 0.849 0.879 0.776 0.913 Sydney 0.927 0.817 0.826 0.720 0.681 Las Vegas 0.952 0.870 0.881 0.947 0.904 As a second experiment, we try to distinguish the sets Bf = {i|ri f ∈ B} and Gf = {i|ri f ∈ G} of bad, respectively good ratings on the feature f. For example, we compute the set Bf using the following classifier (called σ): ri f ∈ Bf (σf (i) = 1) ⇔ ˆri f ≤ 4; Tables 8, 9 and 10 present the Precision(p), Recall(r) and s = 2pr p+r for classifier σ, and compares it with a naive majority classifier, τ, τf (i) = 1 ⇔ |Bf | ≥ |Gf |: We see that recall is always higher for σ and precision is usually slightly worse.",
                "For the s metric σ tends to add a 140 Table 8: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Boston O R S C V p 0.678 0.670 0.573 0.545 0.610 σ r 0.626 0.659 0.619 0.612 0.694 s 0.651 0.665 0.595 0.577 0.609 p 0.684 0.706 0.647 0.611 0.633 τ r 0.597 0.541 0.410 0.383 0.562 s 0.638 0.613 0.502 0.471 0.595 Table 9: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Las Vegas O R S C V p 0.654 0.748 0.592 0.712 0.583 σ r 0.608 0.536 0.791 0.474 0.610 s 0.630 0.624 0.677 0.569 0.596 p 0.685 0.761 0.621 0.748 0.606 τ r 0.542 0.505 0.767 0.445 0.441 s 0.605 0.607 0.670 0.558 0.511 1-20% improvement over τ, much higher in some cases for hotels in Sydney.",
                "This is likely because Sydney reviews are more positive than those of the American cities and cases where the number of bad reviews exceeded the number of good ones are rare.",
                "Replacing the test algorithm with one that plays a 1 with probability equal to the proportion of bad reviews improves its results for this city, but it is still outperformed by around 80%. 6.",
                "SUMMARY OF RESULTS AND CONCLUSION The goal of this paper is to explore the factors that drive a user to submit a particular rating, rather than the incentives that encouraged him to submit a report in the first place.",
                "For that we use two additional sources of information besides the vector of numerical ratings: first we look at the textual comments that accompany the reviews, and second we consider the reports that have been previously submitted by other users.",
                "Using simple natural language processing algorithms, we were able to establish a correlation between the weight of a certain feature in the textual comment accompanying the review, and the noise present in the numerical rating.",
                "Specifically, it seems that users who discuss amply a certain feature are likely to agree on a common rating.",
                "This observation allows the construction of feature-by-feature estimators of quality that have a lower variance, and are hopefully less noisy.",
                "Nevertheless, further evidence is required to support the intuition that ratings corresponding to high weights are expert opinions that deserve to be given higher priority when computing estimates of quality.",
                "Second, we emphasize the dependence of ratings on previous reports.",
                "Previous reports create an expectation of quality which affects the subjective perception of the user.",
                "We validate two facts about the hotel reviews we collected from TripAdvisor: First, the ratings following low expectations (where the expectation is computed as the average of the previous reports) are likely to be higher than the ratings Table 10: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Sydney O R S C V p 0.650 0.463 0.544 0.550 0.580 σ r 0.234 0.378 0.571 0.169 0.592 s 0.343 0.452 0.557 0.259 0.586 p 0.562 0.615 0.600 0.500 0.600 τ r 0.054 0.098 0.101 0.015 0.175 s 0.098 0.168 0.172 0.030 0.271 following high expectations.",
                "Intuitively, the perception of quality (and consequently the rating) depends on how well the actual experience of the user meets her expectation.",
                "Second, we include evidence from the textual comments, and find that when users devote a large fraction of the text to discussing a certain feature, they are likely to motivate a divergent rating (i.e., a rating that does not conform to the prior expectation).",
                "Intuitively, this supports the hypothesis that review forums act as discussion groups where users are keen on presenting and motivating their own opinion.",
                "We have captured the empirical evidence in a behavior model that predicts the ratings submitted by the users.",
                "The final rating depends, as expected, on the true observation, and on the gap between the observation and the expectation.",
                "The gap tends to have a bigger influence when an important fraction of the textual comment is dedicated to discussing a certain feature.",
                "The proposed model was validated on the empirical data and provides better estimates of the ratings actually submitted.",
                "One assumption that we make is about the existence of an objective quality value vf for the feature f. This is rarely true, especially over large spans of time.",
                "Other explanations might account for the correlation of ratings with past reports.",
                "For example, if ef (i) reflects the true value of f at a point in time, the difference in the ratings following high and low expectations can be explained by hotel revenue models that are maximized when the value is modified accordingly.",
                "However, the idea that variation in ratings is not primarily a function of variation in value turns out to be a useful one.",
                "Our approach to approximate this elusive objective value is by no means perfect, but conforms neatly to the idea behind the model.",
                "A natural direction for future work is to examine concrete applications of our results.",
                "Significant improvements of quality estimates are likely to be obtained by incorporating all empirical evidence about rating behavior.",
                "Exactly how different factors affect the decisions of the users is not clear.",
                "The answer might depend on the particular application, context and culture. 7.",
                "REFERENCES [1] A. Admati and P. Pfleiderer.",
                "Noisytalk.com: Broadcasting opinions in a noisy environment.",
                "Working Paper 1670R, Stanford University, 2000. [2] P. B., L. Lee, and S. Vaithyanathan.",
                "Thumbs up? sentiment classification using machine learning techniques.",
                "In Proceedings of the EMNLP-02, the Conference on Empirical Methods in Natural Language Processing, 2002. [3] H. Cui, V. Mittal, and M. Datar.",
                "Comparative 141 Experiments on Sentiment Classification for Online Product Reviews.",
                "In Proceedings of AAAI, 2006. [4] K. Dave, S. Lawrence, and D. Pennock.",
                "Mining the peanut gallery:opinion extraction and semantic classification of product reviews.",
                "In Proceedings of the 12th International Conference on the World Wide Web (WWW03), 2003. [5] C. Dellarocas, N. Awad, and X. Zhang.",
                "Exploring the Value of Online Product Ratings in Revenue Forecasting: The Case of Motion Pictures.",
                "Working paper, 2006. [6] C. Forman, A. Ghose, and B. Wiesenfeld.",
                "A Multi-Level Examination of the Impact of Social Identities on Economic Transactions in Electronic Markets.",
                "Available at SSRN: http://ssrn.com/abstract=918978, July 2006. [7] A. Ghose, P. Ipeirotis, and A. Sundararajan.",
                "Reputation Premiums in Electronic Peer-to-Peer Markets: Analyzing Textual Feedback and Network Structure.",
                "In Third Workshop on Economics of Peer-to-Peer Systems, (P2PECON), 2005. [8] A. Ghose, P. Ipeirotis, and A. Sundararajan.",
                "The Dimensions of Reputation in electronic Markets.",
                "Working Paper CeDER-06-02, New York University, 2006. [9] A. Harmon.",
                "Amazon Glitch Unmasks War of Reviewers.",
                "The New York Times, February 14, 2004. [10] D. Houser and J. Wooders.",
                "Reputation in Auctions: Theory and Evidence from eBay.",
                "Journal of Economics and Management Strategy, 15:353-369, 2006. [11] M. Hu and B. Liu.",
                "Mining and summarizing customer reviews.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD04), 2004. [12] N. Hu, P. Pavlou, and J. Zhang.",
                "Can Online Reviews Reveal a Products True Quality?",
                "In Proceedings of ACM Conference on Electronic Commerce (EC 06), 2006. [13] K. Kalyanam and S. McIntyre.",
                "Return on reputation in online auction market.",
                "Working Paper 02/03-10-WP, Leavey School of Business, Santa Clara University., 2001. [14] L. Khopkar and P. Resnick.",
                "Self-Selection, Slipping, Salvaging, Slacking, and Stoning: the Impacts of Negative Feedback at eBay.",
                "In Proceedings of ACM Conference on Electronic Commerce (EC 05), 2005. [15] M. Melnik and J. Alm.",
                "Does a sellers reputation matter? evidence from ebay auctions.",
                "Journal of Industrial Economics, 50(3):337-350, 2002. [16] R. Olshavsky and J. Miller.",
                "Consumer Expectations, Product Performance and Perceived Product Quality.",
                "Journal of Marketing Research, 9:19-21, February 1972. [17] A. Parasuraman, V. Zeithaml, and L. Berry.",
                "A Conceptual Model of Service Quality and Its Implications for Future Research.",
                "Journal of Marketing, 49:41-50, 1985. [18] A. Parasuraman, V. Zeithaml, and L. Berry.",
                "SERVQUAL: A Multiple-Item Scale for Measuring Consumer Perceptions of Service Quality.",
                "Journal of Retailing, 64:12-40, 1988. [19] P. Pavlou and A. Dimoka.",
                "The Nature and Role of Feedback Text Comments in Online Marketplaces: Implications for Trust Building, Price Premiums, and Seller Differentiation.",
                "Information Systems Research, 17(4):392-414, 2006. [20] A. Popescu and O. Etzioni.",
                "Extracting product features and opinions from reviews.",
                "In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, 2005. [21] R. Teas.",
                "Expectations, Performance Evaluation, and Consumers Perceptions of Quality.",
                "Journal of Marketing, 57:18-34, 1993. [22] E. White.",
                "Chatting a Singer Up the Pop Charts.",
                "The Wall Street Journal, October 15, 1999.",
                "APPENDIX A.",
                "LIST OF WORDS, LR, ASSOCIATED TO THE FEATURE ROOMS All words serve as prefixes: room, space, interior, decor, ambiance, atmosphere, comfort, bath, toilet, bed, building, wall, window, private, temperature, sheet, linen, pillow, hot, water, cold, water, shower, lobby, furniture, carpet, air, condition, mattress, layout, design, mirror, ceiling, lighting, lamp, sofa, chair, dresser, wardrobe, closet 142"
            ],
            "original_annotated_samples": [
                "They use Wordnet to compute the <br>semantic orientation of product evaluation</br>s and summarize user reviews by extracting positive and negative evaluations of different product features."
            ],
            "translated_annotated_samples": [
                "Utilizan Wordnet para calcular la <br>orientación semántica de las evaluaciones de productos</br> y resumir las reseñas de usuarios extrayendo evaluaciones positivas y negativas de diferentes características del producto."
            ],
            "translated_text": "La comprensión del comportamiento del usuario en la presentación de comentarios en línea. Trabajos anteriores han demostrado que la información contradictoria y los sesgos subyacentes de los usuarios dificultan juzgar el verdadero valor de un servicio. En este artículo, investigamos los factores subyacentes que influyen en el comportamiento del usuario al informar retroalimentación. Examinamos dos fuentes de información además de las calificaciones numéricas: la evidencia lingüística del comentario textual que acompaña a una reseña y los patrones en la secuencia temporal de los informes. Primero demostramos que los grupos de usuarios que discuten ampliamente una característica específica tienen más probabilidades de ponerse de acuerdo en una calificación común para esa característica. Segundo, demostramos que la calificación de un usuario refleja en parte la diferencia entre la calidad real y la expectativa previa de calidad, tal como se infiere de revisiones anteriores. Ambos nos ofrecen una forma menos ruidosa de producir estimaciones de calificación y revelar las razones detrás del sesgo del usuario. Nuestras hipótesis fueron validadas por evidencia estadística de reseñas de hoteles en el sitio web de TripAdvisor. Categorías y Descriptores de Asignaturas J.4 [Ciencias Sociales y del Comportamiento]: Economía Términos Generales Economía, Experimentación, Confiabilidad 1. MOTIVACIONES La expansión de internet ha hecho posible que los foros de retroalimentación en línea (o mecanismos de reputación) se conviertan en un canal importante para el boca a boca sobre productos, servicios u otros tipos de interacciones comerciales. Numerosos estudios empíricos [10, 15, 13, 5] muestran que los compradores consideran seriamente los comentarios en línea al tomar decisiones de compra, y están dispuestos a pagar primas por reputación por productos o servicios que tienen una buena reputación. Sin embargo, un análisis reciente plantea preguntas importantes sobre la capacidad de los foros existentes para reflejar la verdadera calidad de un producto. En ausencia de incentivos claros, los usuarios con una perspectiva moderada no se molestarán en expresar sus opiniones, lo que conduce a una muestra no representativa de reseñas. Por ejemplo, [12, 1] muestran que las calificaciones de libros o CDs en Amazon1 siguen con gran probabilidad distribuciones bimodales en forma de U, donde la mayoría de las calificaciones son o muy buenas o muy malas. Los experimentos controlados, por otro lado, revelan opiniones sobre los mismos elementos que están distribuidas de forma normal. Bajo estas circunstancias, utilizar la media aritmética para predecir la calidad (como la mayoría de los foros realmente hacen) proporciona al usuario típico un estimador con una alta varianza que a menudo es incorrecto. Mejorar la forma en que agregamos la información disponible de las reseñas en línea requiere un profundo entendimiento de los factores subyacentes que sesgan el comportamiento de calificación de los usuarios. Hu et al. [12] proponen el Modelo Brag-and-Moan donde los usuarios califican solo si su utilidad del producto (extraída de una distribución normal) cae fuera de un intervalo mediano. Los autores concluyen que el modelo explica la distribución empírica de los informes y ofrece ideas sobre formas más inteligentes de estimar la verdadera calidad del producto. En el presente artículo ampliamos esta línea de investigación e intentamos explicar más hechos sobre el comportamiento de los usuarios al informar retroalimentación en línea. Usando reseñas reales de hoteles del sitio web TripAdvisor2, consideramos dos fuentes adicionales de información además de las calificaciones numéricas básicas enviadas por los usuarios. La primera es una evidencia lingüística simple proveniente de la revisión textual que suele acompañar a las calificaciones numéricas. Utilizamos técnicas de minería de texto similares a [7] y [3], sin embargo, solo estamos interesados en identificar qué aspectos del servicio está discutiendo el usuario, sin calcular la orientación semántica del texto. Observamos que los usuarios que comentan más sobre la misma característica tienen más probabilidades de estar de acuerdo en una calificación numérica común para esa característica en particular. Intuitivamente, los comentarios extensos revelan la importancia de la característica para el usuario. Dado que las personas tienden a ser más conocedoras en los aspectos que consideran importantes, se podría asumir que los usuarios que discuten una característica específica en más detalle tienen más autoridad para evaluar esa característica. Segundo investigamos la relación entre una reseña 1 http://www.amazon.com 2 http://www.tripadvisor.com/ 134 Figura 1: La página de TripAdvisor que muestra reseñas de un popular hotel en Boston. El nombre del hotel y los anuncios fueron deliberadamente borrados, al igual que las reseñas que lo precedieron. Un examen de las reseñas en línea muestra que las calificaciones a menudo forman parte de los hilos de discusión, donde una publicación no es necesariamente independiente de otras publicaciones. Uno puede ver, por ejemplo, usuarios que se esfuerzan por contradecir o estar vehementemente de acuerdo con los comentarios de usuarios anteriores. Al analizar la secuencia temporal de informes, concluimos que las revisiones pasadas influyen en los informes futuros, ya que crean cierta expectativa previa sobre la calidad del servicio. La percepción subjetiva del usuario está influenciada por la brecha entre la expectativa previa y el rendimiento real del servicio [17, 18, 16, 21], lo cual se reflejará más tarde en la calificación de los usuarios. Proponemos un modelo que captura la dependencia de las calificaciones en las expectativas previas, y lo validamos utilizando los datos empíricos que recopilamos. Ambos resultados pueden ser utilizados para mejorar la forma en que los mecanismos de reputación agregan la información de las revisiones individuales. Nuestro primer resultado se puede utilizar para determinar una estimación de calidad característica por característica, donde para cada característica se considera un subconjunto diferente de reseñas (es decir, aquellas con comentarios extensos sobre esa característica). El segundo conduce a un algoritmo que produce una estimación más precisa de la calidad real. 2. El conjunto de datos que utilizamos en este artículo son reseñas reales de hoteles recopiladas del popular sitio de viajes TripAdvisor. TripAdvisor indexa hoteles de ciudades de todo el mundo, junto con reseñas escritas por viajeros. Los usuarios pueden buscar en el sitio proporcionando el nombre del hotel y la ubicación (opcional). Las reseñas de un hotel dado se muestran como una lista (ordenadas de la más reciente a la más antigua), con 5 reseñas por página. Las reseñas contienen: • información sobre el autor de la reseña (por ejemplo, fechas de estancia, nombre de usuario del revisor, ubicación del revisor); • la calificación general (de 1, la más baja, a 5, la más alta); • una reseña textual que incluye un título para la reseña, comentarios libres y las principales cosas que al revisor le gustaron y no le gustaron; • calificaciones numéricas (de 1, la más baja, a 5, la más alta) para diferentes características (por ejemplo, limpieza, servicio, ubicación, etc.). Debajo del nombre del hotel, TripAdvisor muestra la dirección del hotel, información general (número de habitaciones, número de estrellas, breve descripción, etc.), la calificación promedio general, el ranking de TripAdvisor y una calificación promedio para cada característica. La Figura 1 muestra la página de un hotel popular en Boston cuyo nombre (junto con los anuncios) fue explícitamente borrado. Seleccionamos tres ciudades para este estudio: Boston, Sídney y Las Vegas. Para cada ciudad consideramos todos los hoteles que tenían al menos 10 reseñas, y registramos todas las reseñas. La Tabla 1 presenta el número de hoteles considerados en cada ciudad, el número total de reseñas registradas para cada ciudad y la distribución de hoteles con respecto a la calificación por estrellas (según está disponible en el sitio de TripAdvisor). Ten en cuenta que no todos los hoteles tienen una clasificación por estrellas. Tabla 1: Un resumen del conjunto de datos. Para cada reseña, registramos la calificación general, la reseña textual (título y cuerpo de la reseña) y la calificación numérica en 7 características: Habitaciones (R), Servicio (S), Limpieza (C), Valor (V), Comida (F), Ubicación (L) y Ruido (N). TripAdvisor no requiere que los usuarios envíen nada más que la calificación general, por lo tanto, una reseña típica evalúa pocas características adicionales, independientemente de la discusión en el comentario textual. Solo las características Habitaciones (R), Servicio (S), Limpieza (C) y Valor (V) son evaluadas por un número significativo de usuarios. Sin embargo, también seleccionamos las características Comida (F), Ubicación (L) y Ruido (N) porque son mencionadas en un número significativo de comentarios textuales. Para cada característica registramos la calificación numérica dada por el usuario, o 0 cuando la calificación está ausente. La longitud típica del comentario textual equivale aproximadamente a 200 palabras. Todos los datos fueron recopilados rastreando el sitio de TripAdvisor en septiembre de 2006. 2.1 Notación formal Nos referiremos formalmente a una reseña mediante una tupla (r, T) donde: • r = (rf ) es un vector que contiene las calificaciones rf ∈ {0, 1, . . . 5} para las características f ∈ F = {O, R, S, C, V, F, L, N}; cabe destacar que la calificación general, rO, se registra de forma abusiva como la calificación para la característica General(O); • T es el comentario textual que acompaña a la reseña. Se indexan 135 reseñas de acuerdo con la variable i, de modo que (ri , Ti ) es la i-ésima reseña en nuestra base de datos. Dado que no registramos el nombre de usuario del revisor, también diremos que la i-ésima reseña en nuestro conjunto de datos fue enviada por el usuario i. Cuando necesitemos considerar solo las reseñas de un hotel dado, h, usaremos (ri(h), Ti(h)) para denotar la i-ésima reseña sobre el hotel h. 3. EVIDENCIA DE COMENTARIOS TEXTUALES Los comentarios textuales gratuitos asociados a las reseñas en línea son una fuente valiosa de información para comprender las razones detrás de las calificaciones numéricas dejadas por los revisores. El texto puede, por ejemplo, revelar ejemplos concretos de aspectos que al usuario le gustaron o no le gustaron, justificando así algunas de las calificaciones altas o bajas respectivamente para ciertas características. El texto también puede ofrecer pautas para comprender las preferencias del revisor y los pesos de las diferentes características al calcular una calificación general. El problema, sin embargo, es que los comentarios textuales libres son difíciles de leer. Los usuarios deben desplazarse a través de muchas reseñas y leer principalmente información repetitiva. Se obtendrían mejoras significativas si las reseñas fueran interpretadas y agregadas automáticamente. Desafortunadamente, esta parece ser una tarea difícil para las computadoras ya que los usuarios humanos a menudo utilizan un lenguaje ingenioso, abreviaturas, frases específicas de la cultura y un estilo figurativo. Sin embargo, varios resultados importantes utilizan los comentarios textuales de las reseñas en línea de forma automatizada. Utilizando técnicas de lenguaje natural bien establecidas, las reseñas o partes de las reseñas pueden ser clasificadas como tener una orientación semántica positiva o negativa. Pang et al. [2] clasifican las críticas de películas en positivas/negativas entrenando tres clasificadores diferentes (Naive Bayes, Máxima Entropía y SVM) utilizando características de clasificación basadas en unigramas, bigramas o etiquetas de partes del discurso. Dave et al. [4] analizan reseñas de CNet y Amazon, y sorprendentemente demuestran que las características de clasificación basadas en unigramas o bigramas funcionan mejor que los n-gramas de orden superior. Este resultado es desafiado por Cui et al. [3], quienes examinan grandes colecciones de reseñas recopiladas de la web. Muestran que el tamaño del conjunto de datos es importante, y que conjuntos de entrenamiento más grandes permiten a los clasificadores utilizar con éxito características de clasificación más complejas basadas en n-gramas. Hu y Liu [11] también rastrean la web en busca de reseñas de productos e identifican automáticamente los atributos de productos que han sido discutidos por los revisores. Utilizan Wordnet para calcular la <br>orientación semántica de las evaluaciones de productos</br> y resumir las reseñas de usuarios extrayendo evaluaciones positivas y negativas de diferentes características del producto. Popescu y Etzioni [20] analizan un escenario similar, pero utilizan el recuento de resultados de búsqueda en motores de búsqueda para identificar atributos de productos; la orientación semántica se asigna a través de la técnica de etiquetado de relajación. Ghose et al. [7, 8] analizan las reseñas de vendedores del mercado secundario de Amazon para identificar las diferentes dimensiones (por ejemplo, entrega, empaque, atención al cliente, etc.) de la reputación. Analizan el texto y etiquetan la parte de la oración de cada palabra. Los sustantivos, frases nominales y frases verbales frecuentes se identifican como dimensiones de la reputación, mientras que los modificadores correspondientes (es decir, adjetivos y adverbios) se utilizan para derivar puntuaciones numéricas para cada dimensión. La medida de reputación mejorada se correlaciona mejor con la información de precios observada en el mercado. Pavlou y Dimoka [19] analizan las reseñas de eBay y encuentran que los comentarios textuales tienen un impacto importante en las primas de reputación. Nuestro enfoque es similar a los trabajos mencionados anteriormente, en el sentido de que identificamos los aspectos (es decir, las características del hotel) discutidos por los usuarios en las reseñas textuales. Sin embargo, no calculamos la orientación semántica del texto, ni intentamos inferir las calificaciones faltantes. Definimos el peso, wi f, de la característica f ∈ F en el texto Ti asociado con la reseña (ri, Ti), como la fracción de Ti dedicada a discutir aspectos (tanto positivos como negativos) relacionados con la característica f. Proponemos un método elemental para aproximar los valores de estos pesos. Para cada característica, construimos manualmente la lista de palabras Lf que contiene aproximadamente 50 palabras que están más comúnmente asociadas a la característica f. Las palabras iniciales fueron seleccionadas al leer algunas de las reseñas y ver qué palabras coinciden con la discusión de qué características. La lista fue luego ampliada agregando todas las entradas del tesauro que estaban relacionadas con las palabras iniciales. Finalmente, hicimos una lluvia de ideas para encontrar las palabras faltantes que normalmente estarían asociadas con cada una de las características. Que Lf ∩ Ti sea la lista de términos comunes a Lf y Ti. Cada término de Lf se cuenta el número de veces que aparece en Ti, con dos excepciones: • en los casos en que el usuario envía un título para la revisión, consideramos el texto del título agregándolo tres veces al texto de la revisión Ti. La suposición intuitiva es que la opinión del usuario se refleja con mayor fuerza en el título que en el cuerpo de la reseña. Por ejemplo, muchas reseñas son resumidas con precisión por títulos como \"Excelente servicio, ubicación terrible\" o \"Mala relación calidad-precio\"; ciertas palabras que ocurren solo una vez en el texto son contadas múltiples veces si su relevancia para esa característica es particularmente fuerte. Estas eran las palabras raíz para cada característica (por ejemplo, personal es una palabra raíz para la característica Servicio), y tenían un peso de 2 o 3. Cada característica fue asignada hasta 3 palabras raíz, por lo que casi todas las palabras se cuentan solo una vez. La lista de palabras para la característica Habitaciones se proporciona como referencia en el Apéndice A. El peso wi f se calcula como: wi f = |Lf ∩ Ti| f∈F |Lf ∩ Ti| (1) donde |Lf ∩ Ti| es el número de términos comunes entre Lf y Ti. El peso para la característica \"En general\" se estableció en min{ |T i | 5000 , 1} donde |Ti | es el número de caracteres en Ti. Comenzaré diciendo que soy más de la onda de Holiday Inn que de un hotel de lujo. Así que me frustro cuando pago el doble de la tarifa de la habitación y recibo la mitad de las comodidades que obtendría en un Hampton Inn o Holiday Inn. La ubicación era definitivamente el principal activo de este lugar. Estaba a solo unas cuadras de la parada de metro del Centro Hynes y era fácil caminar a algunos buenos restaurantes en la zona de Back Bay. Boylston no está lejos en absoluto. Así que no tuve problemas en renunciar a un coche de alquiler y tomar el metro desde el aeropuerto hasta el hotel y usar el metro para cualquier otro viaje. De lo contrario, te hacen pagar por todo. Y cuando ya has gastado $215 por noche en la habitación, eso resulta frustrante. La habitación en sí estaba bien, más o menos lo que esperaría. El personal también era promedio, ni malo ni excelente. Una vez más, creo que estás pagando por la ubicación y la capacidad de ir caminando a muchos lugares interesantes. Pero creo que la próxima vez me quedaré en Brookline, disfrutaré de más comodidades y usaré más el metro. Las calificaciones numéricas asociadas a esta reseña son rO = 3, rR = 3, rS = 3, rC = 4, rV = 2 para las características de General (O), Habitaciones (R), Servicio (S), Limpieza (C) y Valor (V) respectivamente. Las calificaciones de las características Comida (F), Ubicación (L) y Ruido (N) están ausentes (es decir, rF = rL = rN = 0). Los pesos wf se calculan a partir de las siguientes listas de términos comunes: LR ∩ T = {habitación}; wR = 0.066 LS ∩ T = {3 * Personal, comodidades}; wS = 0.267 LC ∩ T = ∅; wC = 0 LV ∩ T = {$, tarifa}; wV = 0.133 LF ∩ T = {restaurante}; wF = 0.067 LL ∩ T = {2 * centro, 2 * caminar, 2 * ubicación, área}; wL = 0.467 LN ∩ T = ∅; wN = 0 Las palabras raíz Personal y Centro se triplicaron y duplicaron respectivamente. El peso total de la revisión textual es wO = 0.197. Estos valores explican razonablemente bien los pesos de las diferentes características en la discusión del revisor. Un punto a tener en cuenta es que algunos términos en las listas Lf poseen una orientación semántica inherente. Por ejemplo, la palabra suciedad (perteneciente a la lista LC) se usaría con mayor frecuencia para afirmar la presencia, y no la ausencia de suciedad. Esto es inevitable, pero se tuvo cuidado de asegurar que se utilizaran palabras de ambos extremos del espectro. Por esta razón, algunas listas como LR contienen solo sustantivos de objetos que uno típicamente describiría en una habitación (ver Apéndice A). El objetivo de esta sección es analizar la influencia de los pesos wi f en las calificaciones numéricas ri f. Intuitivamente, los usuarios que pasaron mucho tiempo discutiendo una característica f (es decir, cuando wif es alto) tenían algo que decir sobre su experiencia con respecto a esta característica. Obviamente, la característica f es importante para el usuario i. Dado que las personas tienden a ser más conocedoras en los aspectos que consideran importantes, nuestra hipótesis es que las calificaciones ri f (correspondientes a los pesos altos wi f) constituyen un subconjunto de las calificaciones de expertos para la característica f. La Figura 2 traza la distribución de las tasas r i(h) C con respecto a los pesos w i(h) C para la limpieza de un hotel de Las Vegas, h. Aquí, las calificaciones altas están restringidas a las reseñas que hablan poco sobre la limpieza. Cuando se menciona la limpieza en la discusión, las calificaciones son bajas. Muchos hoteles muestran patrones de calificación similares para varias características. Las calificaciones correspondientes a pesos bajos abarcan todo el espectro del 1 al 5, mientras que las calificaciones correspondientes a pesos altos están más agrupadas (ya sea alrededor de calificaciones buenas o malas). Por lo tanto, formulamos la siguiente hipótesis: Hipótesis 1. Las calificaciones ri f correspondientes a las reseñas donde wi f es alta, son más similares entre sí que a la colección general de calificaciones. Para probar la hipótesis, tomamos todo el conjunto de reseñas y, característica por característica, calculamos la desviación estándar de las calificaciones con alto peso, y la desviación estándar de todo el conjunto de calificaciones. Los pesos altos se definieron como aquellos que pertenecen al 20% superior del rango de pesos para la característica correspondiente. Si la Hipótesis 1 fuera cierta, la desviación estándar de todas las calificaciones debería ser mayor que la desviación estándar de las calificaciones con pesos altos. 0 1 2 3 4 5 6 0 0.1 0.2 0.3 0.4 0.5 0.6 Calificación Peso Figura 2: La distribución de las calificaciones en función del peso de la característica de limpieza. Utilizamos una prueba T estándar para medir la significancia de los resultados. Ciudad por ciudad y característica por característica, la Tabla 2 presenta la desviación estándar promedio de todas las calificaciones, y la desviación estándar promedio de las calificaciones con pesos altos. De hecho, las calificaciones con pesos altos tienen una desviación estándar más baja, y los resultados son significativos en el umbral estándar de significancia de 0.05 (aunque para ciertas ciudades tomadas de forma independiente no parece haber una diferencia significativa, los resultados son significativos para todo el conjunto de datos). Por favor, tenga en cuenta que solo se consideraron las características O, R, S, C y V, ya que para las otras (F, L y N) no teníamos suficientes calificaciones. Tabla 2: Desviación estándar promedio para todas las calificaciones, y desviación estándar promedio para las calificaciones con pesos altos. En corchetes, los valores p correspondientes para una diferencia positiva entre los dos. La hipótesis 1 no solo proporciona una comprensión básica sobre el comportamiento de calificación de los usuarios en línea, sino que también sugiere algunas formas de calcular estimaciones de mejor calidad. Podemos, por ejemplo, construir una estimación de calidad característica por característica con una varianza mucho menor: para cada característica tomamos el subconjunto de reseñas que discuten ampliamente esa característica, y como estimación de calidad, obtenemos la calificación promedio de este subconjunto. Los experimentos iniciales sugieren que las calificaciones promedio de característica a característica calculadas de esta manera son diferentes de las calificaciones promedio calculadas en todo el conjunto de datos. Dado que, de hecho, los pesos altos son indicadores de opiniones expertas, las estimaciones obtenidas de esta manera son más precisas que las actuales. Sin embargo, la validación de esta suposición subyacente requiere más experimentos controlados. LA INFLUENCIA DE LAS CALIFICACIONES PASADAS Por lo general, se hacen dos suposiciones importantes sobre las reseñas enviadas a foros en línea. La primera es que las calificaciones reflejen fielmente la calidad observada por los usuarios; la segunda es que las reseñas sean independientes entre sí. Si bien la evidencia anecdótica [9, 22] cuestiona la primera suposición, en esta sección abordamos la segunda. Un examen de las reseñas en línea muestra que estas suelen formar parte de hilos de discusión, donde los usuarios se esfuerzan por contradecir o estar vehementemente de acuerdo con los comentarios de usuarios anteriores. Considera, por ejemplo, la siguiente reseña: No entiendo las críticas negativas... el hotel estaba un poco oscuro, pero ese era el estilo. Fue muy artístico. Sí, estaba cerca de la autopista, pero en mi opinión, el sonido de un coche ruidoso ocasional es mejor que escuchar el ding ding de las máquinas tragamonedas toda la noche. El personal disponible es FABULOSO. Las camareras son geniales (y *** no merece la mala crítica que recibió, ¡fue 100% atenta con nosotros!), los camareros son amigables y profesionales al mismo tiempo... Aquí, el usuario se vio perturbado por informes negativos anteriores, abordó estas preocupaciones y se dispuso a intentar corregirlas. Como era de esperar, sus calificaciones fueron considerablemente más altas que las calificaciones promedio hasta este punto. Parece que los usuarios de TripAdvisor suelen leer regularmente los informes enviados por usuarios anteriores antes de reservar un hotel, o antes de escribir una reseña. Las reseñas anteriores crean ciertas expectativas previas sobre la calidad del servicio, y estas expectativas influyen en la reseña presentada. Creemos que esta observación es válida para la mayoría de los foros en línea. La percepción subjetiva de la calidad es directamente proporcional a qué tan bien la experiencia real cumple con la expectativa previa, un hecho confirmado por una importante línea de investigación econométrica y de marketing [17, 18, 16, 21]. La correlación entre las reseñas también ha sido confirmada por investigaciones recientes sobre la dinámica de los foros de reseñas en línea [6]. 4.1 Expectativas Previas Definimos la expectativa previa del usuario i con respecto a la característica f, como el promedio de las calificaciones previamente disponibles en la característica f4: ef (i) = j<i,r j f =0 rj f j<i,r j f =0 1 Como primera hipótesis, afirmamos que la calificación ri f es una función de la expectativa previa ef (i): Hipótesis 2. Para un hotel y una característica dados, dados los comentarios i y j tales que ef (i) es alto y ef (j) es bajo, la calificación rj f supera la calificación ri f. Definimos las expectativas altas y bajas como aquellas que están por encima, respectivamente por debajo de un cierto valor umbral θ. El conjunto de reseñas precedidas por altas, respectivamente bajas expectativas 3 parte de las reseñas de Amazon fueron reconocidas como publicaciones estratégicas por autores de libros o competidores 4 si no se asignaron calificaciones previas para la característica f, ef (i) se asigna un valor predeterminado de 4. Tabla 3: Calificaciones promedio para reseñas precedidas por expectativas bajas (primer valor en la celda) y altas (segundo valor en la celda). Los valores P para una diferencia positiva se dan entre corchetes. Las ciudades O R S C V 3.953 4.045 3.985 4.252 3.946 Boston 3.364 3.590 3.485 3.641 3.242 [0.011] [0.028] [0.0086] [0.0168] [0.0034] 4.284 4.358 4.064 4.530 4.428 Sídney 3.756 3.537 3.436 3.918 3.495 [0.000] [0.000] [0.035] [0.009] [0.000] 3.494 3.674 3.713 3.689 3.580 Las Vegas 3.140 3.530 2.952 3.530 3.351 [0.190] [0.529] [0.007] [0.529] [0.253] se definen de la siguiente manera: Ralto f = {ri f |ef (i) > θ} Rbajo f = {ri f |ef (i) < θ} Estos conjuntos son específicos para cada par (hotel, característica), y en nuestros experimentos tomamos θ = 4. Este valor bastante alto está cerca del promedio de calificación de todas las características de todos los hoteles, y se justifica por el hecho de que nuestro conjunto de datos contiene principalmente hoteles de alta calidad. Para cada ciudad, tomamos todos los hoteles y calculamos las calificaciones promedio en los conjuntos Rhigh f y Rlow f (ver Tabla 3). El promedio de calificación entre las reseñas que siguen a expectativas previas bajas es significativamente mayor que el promedio de calificación después de altas expectativas. Como evidencia adicional, consideramos todos los hoteles para los cuales la función eV (i) (la expectativa para la característica Valor) tiene un valor alto (mayor que 4) para algunos i, y un valor bajo (menor que 4) para algunos otros i. Intuitivamente, estos son los hoteles para los cuales hay un grado mínimo de variación en la secuencia oportuna de reseñas: es decir, el promedio acumulativo de calificaciones fue en algún momento alto y luego se volvió bajo, o viceversa. Tales variaciones se observan en aproximadamente la mitad de todos los hoteles en cada ciudad. La Figura 3 traza la mediana (entre los hoteles considerados) de la calificación, rV, cuando ef (i) no es más que x pero mayor que x - 0.5. 2.5 3 3.5 4 4.5 5 2.5 3 3.5 4 4.5 5 Expectativa de la mediana de calificación Boston Sydney Vegas Figura 3: Las calificaciones tienden a disminuir a medida que aumenta la expectativa. 138 Hay dos formas de interpretar la función ef (i): • El valor esperado para la característica f obtenido por el usuario i antes de su experiencia con el servicio, adquirido al leer informes presentados por usuarios anteriores. En este caso, un valor excesivamente alto para ef (i) llevaría al usuario a enviar un informe negativo (o viceversa), derivado de la diferencia entre el valor real del servicio y la expectativa inflada de este valor adquirida antes de su experiencia. • El valor esperado de la característica f para todos los visitantes posteriores del sitio, si el usuario i no enviara un informe. En este caso, la motivación para un informe negativo después de un valor excesivamente alto de ef es diferente: el usuario i busca corregir la expectativa de los futuros visitantes del sitio. A diferencia de la interpretación anterior, esto no requiere que el usuario derive una expectativa a priori para el valor de f. Tenga en cuenta que ninguna de las interpretaciones implica que el promedio hasta el informe i esté inversamente relacionado con la calificación en el informe i. Podría existir una medida de influencia ejercida por informes anteriores que impulse al usuario detrás del informe i a enviar calificaciones que, en cierta medida, se ajusten a los informes anteriores: un valor bajo para ef (i) puede influir en el usuario i a enviar una calificación baja para la característica f porque, por ejemplo, teme que enviar una calificación alta lo haga parecer una persona con bajos estándares. Esto, al principio, parece contradecir la Hipótesis 2. Sin embargo, esta calificación de conformidad no puede continuar indefinidamente: una vez que el conjunto de informes proyecte una estimación suficientemente reducida para vf, los revisores futuros con impresiones comparativamente positivas buscarán corregir esta percepción errónea. 4.2 Impacto de los comentarios textuales en la expectativa de calidad. Se puede obtener una mayor comprensión del comportamiento de calificación de los usuarios de TripAdvisor analizando la relación entre los pesos wf y los valores ef (i). En particular, examinamos la siguiente hipótesis: Hipótesis 3. Cuando una gran proporción del texto de una reseña discute una característica en particular, la diferencia entre la calificación para esa característica y la calificación promedio hasta ese momento tiende a ser grande. La intuición detrás de esta afirmación es que cuando el usuario insiste en expresar su opinión sobre una característica en particular, su opinión difiere de la opinión colectiva de publicaciones anteriores. Esto se basa en la característica de los sistemas de reputación como foros de retroalimentación donde un usuario está interesado en proyectar su opinión, con particular fuerza si esta opinión difiere de lo que percibe como la opinión general. Para probar la Hipótesis 3 medimos la diferencia absoluta promedio entre la expectativa ef (i) y la calificación ri f cuando el peso wi f es alto, respectivamente bajo. Los pesos se clasifican como altos o bajos al compararlos con ciertos valores de corte: wi f es bajo si es menor que 0.1, mientras que wi f es alto si es mayor que θf. Se utilizaron diferentes valores de corte para diferentes características: θR = 0.4, θS = 0.4, θC = 0.2 y θV = 0.7. La limpieza tiene un límite inferior más bajo ya que es una característica raramente discutida; el valor tiene un límite superior alto por la razón opuesta. Los resultados se presentan en la Tabla 4. La idea de que los informes negativos pueden fomentar una mayor divulgación negativa ha sido sugerida anteriormente [14]. Tabla 4: Promedio de |ri f −ef (i)| cuando los pesos son altos (primer valor en la celda) y bajos (segundo valor en la celda) con valores P para la diferencia en corchetes cuadrados. Esto demuestra que cuando los pesos son inusualmente altos, los usuarios tienden a expresar una opinión que no se ajusta al promedio neto de calificaciones anteriores. Como podríamos esperar, para una característica que rara vez tuvo un peso importante en la discusión (por ejemplo, la limpieza), la diferencia es particularmente grande. Aunque la diferencia en el valor de la característica es bastante grande para Sídney, el valor P es alto. Esto se debe a que solo unos pocos comentarios discutieron en gran medida el valor. La razón podría ser cultural o porque había menos motivo para discutir esta característica. 4.3 Incentivos para informar Los modelos anteriores sugieren que los usuarios poco opinativos no elegirán expresar sus opiniones [12]. En esta sección, ampliamos este modelo para tener en cuenta la influencia de las expectativas. La motivación para enviar comentarios no se debe solo a opiniones extremas, sino también a la diferencia entre la reputación actual (es decir, la expectativa previa del usuario) y la experiencia real. Un modelo de calificación así produce calificaciones que la mayoría de las veces se desvían de la calificación promedio actual. Las calificaciones que confirman la expectativa previa rara vez serán enviadas. Probamos en nuestro conjunto de datos la proporción de calificaciones que intentan corregir la estimación actual. Definimos una calificación desviada como aquella que se desvía de la expectativa actual por al menos un umbral θ, es decir, |ri f − ef (i)| ≥ θ. Para cada una de las tres ciudades consideradas, las siguientes tablas muestran la proporción de calificaciones desviadas para θ = 0.5 y θ = 1. Los resultados anteriores sugieren que una gran proporción de usuarios (casi la mitad, incluso para el valor umbral alto θ = 1) se desvían del promedio previo. Esto refuerza la idea de que los usuarios son más propensos a enviar un informe cuando creen que tienen algo distintivo que agregar al flujo actual de opiniones sobre alguna característica. Tales conclusiones están en total acuerdo con evidencia previa de que la distribución de informes a menudo sigue distribuciones bimodales en forma de U. 139 5. Para dar cuenta de las observaciones descritas en las secciones anteriores, proponemos un modelo para el comportamiento de los usuarios al enviar reseñas en línea. Para un hotel dado, asumimos que la calidad experimentada por los usuarios sigue una distribución normal alrededor de algún valor vf, que representa la calidad objetiva ofrecida por el hotel en la característica f. La calificación enviada por el usuario i en la característica f es: ˆri f = δf vi f + (1 − δf) · sign vi f − ef (i) c + d(vi f, ef (i)|wi f) (2) donde: • vi f es la calidad (desconocida) experimentada realmente por el usuario. Se asume que vi f sigue una distribución normal alrededor de algún valor vf; • δf ∈ [0, 1] puede verse como una medida del sesgo al reportar retroalimentación. Los valores altos reflejan el hecho de que los usuarios califican de manera objetiva, sin ser influenciados por expectativas previas. El valor de δf puede depender de varios factores; fijamos un valor para cada característica f; • c es una constante entre 1 y 5; • wi f es el peso de la característica f en el comentario textual de la reseña i, calculado según la Ecuación (1); • d(vi f , ef (i)|wi f ) es una función de distancia entre la expectativa y la observación del usuario i. La función de distancia satisface las siguientes propiedades: - d(y, z|w) ≥ 0 para todo y, z ∈ [0, 5], w ∈ [0, 1]; - |d(y, z|w)| < |d(z, x|w)| si |y − z| < |z − x|; - |d(y, z|w1)| < |d(y, z|w2)| si w1 < w2; - c + d(vf , ef (i)|wi f ) ∈ [1, 5]; El segundo término de la Ecuación (2) codifica el sesgo de la calificación. Cuanto mayor sea la distancia entre la observación real vi f y la función ef, mayor será el sesgo. 5.1 Validación del modelo Utilizamos el conjunto de datos de las reseñas de TripAdvisor para validar el modelo de comportamiento presentado anteriormente. Dividimos por conveniencia los valores de calificación en tres rangos: malo (B = {1, 2}), indiferente (I = {3, 4}), y bueno (G = {5}), y realizamos los siguientes dos tests: • Primero, usaremos nuestro modelo para predecir las calificaciones que tienen valores extremos. Para cada hotel, tomamos la secuencia de informes, y cada vez que nos encontramos con una calificación que sea buena o mala (pero no indiferente) intentamos predecirla utilizando la Ecuación (2) • En segundo lugar, en lugar de predecir el valor de las calificaciones extremas, intentamos clasificarlas como buenas o malas. Para cada hotel tomamos la secuencia de informes, y para cada informe (independientemente de su valor) lo clasificamos como bueno o malo. Sin embargo, para realizar estas pruebas, necesitamos estimar el valor objetivo, vf, que es el promedio de las observaciones de calidad reales, vi f. El algoritmo que estamos utilizando se basa en la intuición de que la cantidad de calificación de conformidad se minimiza. En otras palabras, el valor vf debe ser tal que, tan a menudo como sea posible, las calificaciones malas sigan a expectativas por encima de vf y las calificaciones buenas sigan a expectativas por debajo de vf. Formalmente, definimos los conjuntos: Γ1 = {i|ef (i) < vf y ri f ∈ B}; Γ2 = {i|ef (i) > vf y ri f ∈ G}; que corresponden a irregularidades donde, a pesar de que la expectativa en el punto i es menor que el valor entregado, la calificación es baja, y viceversa. Definimos vf como el valor que minimiza la unión de los dos conjuntos: vf = arg min vf |Γ1 ∪ Γ2| (3) En la Ec. (2) reemplazamos vi f por el valor vf calculado en la Ec. (3), y utilizamos la siguiente función de distancia: d(vf , ef (i)|wi f ) = |vf − ef (i)| vf − ef (i) |vf 2 − ef (i)2 | · (1 + 2wi f ); La constante c ∈ I se estableció en min{max{ef (i), 3}}, 4}. Los valores para δf se fijaron en {0.7, 0.7, 0.8, 0.7, 0.6} para las características {General, Habitaciones, Servicio, Limpieza, Valor} respectivamente. Los pesos se calculan tal como se describe en la Sección 3. Como primer experimento, tomamos los conjuntos de calificaciones extremas {ri f |ri f /∈ I} para cada hotel y característica. Para cada calificación de este tipo, ri f, intentamos estimarla calculando ˆri f usando la Ecuación (2). Comparamos este estimador con el obtenido simplemente promediando las calificaciones de todos los hoteles y características: es decir, ¯rf = j,r j f =0 rj f j,r j f =0 1; la Tabla 7 presenta la relación entre el error cuadrático medio (RMSE) al usar ˆri f y ¯rf para estimar las calificaciones reales. En todos los casos, la estimación producida por nuestro modelo es mejor que el promedio simple. Tabla 7: Promedio de RMSE(ˆrf) RMSE(¯rf) Ciudad O R S C V Boston 0.987 0.849 0.879 0.776 0.913 Sídney 0.927 0.817 0.826 0.720 0.681 Las Vegas 0.952 0.870 0.881 0.947 0.904 Como segundo experimento, intentamos distinguir los conjuntos Bf = {i|ri f ∈ B} y Gf = {i|ri f ∈ G} de calificaciones malas y buenas, respectivamente, en la característica f. Por ejemplo, calculamos el conjunto Bf usando el siguiente clasificador (llamado σ): ri f ∈ Bf (σf (i) = 1) ⇔ ˆri f ≤ 4; Las Tablas 8, 9 y 10 presentan la Precisión (p), Recall (r) y s = 2pr p+r para el clasificador σ, y lo comparan con un clasificador de mayoría ingenuo, τ, τf (i) = 1 ⇔ |Bf | ≥ |Gf |: Vemos que el recall es siempre mayor para σ y la precisión suele ser ligeramente peor. Para la métrica s, σ tiende a agregar un 140. Tabla 8: Precisión (p), Recall (r), s= 2pr p+r mientras se detectan calificaciones bajas para Boston O R S C V p 0.678 0.670 0.573 0.545 0.610 σ r 0.626 0.659 0.619 0.612 0.694 s 0.651 0.665 0.595 0.577 0.609 p 0.684 0.706 0.647 0.611 0.633 τ r 0.597 0.541 0.410 0.383 0.562 s 0.638 0.613 0.502 0.471 0.595 Tabla 9: Precisión (p), Recall (r), s= 2pr p+r mientras se detectan calificaciones bajas para Las Vegas O R S C V p 0.654 0.748 0.592 0.712 0.583 σ r 0.608 0.536 0.791 0.474 0.610 s 0.630 0.624 0.677 0.569 0.596 p 0.685 0.761 0.621 0.748 0.606 τ r 0.542 0.505 0.767 0.445 0.441 s 0.605 0.607 0.670 0.558 0.511 Mejora del 1-20% sobre τ, mucho mayor en algunos casos para hoteles en Sídney. Esto se debe probablemente a que las reseñas de Sídney son más positivas que las de las ciudades estadounidenses y los casos en los que el número de reseñas negativas supera al de las positivas son raros. Reemplazar el algoritmo de prueba con uno que juega un 1 con una probabilidad igual a la proporción de malas críticas mejora sus resultados para esta ciudad, pero aún es superado por alrededor del 80%. 6. RESUMEN DE RESULTADOS Y CONCLUSIÓN El objetivo de este artículo es explorar los factores que llevan a un usuario a enviar una calificación particular, en lugar de los incentivos que lo animaron a enviar un informe en primer lugar. Para ello utilizamos dos fuentes adicionales de información además del vector de calificaciones numéricas: primero, examinamos los comentarios textuales que acompañan las reseñas, y segundo, consideramos los informes que han sido previamente enviados por otros usuarios. Usando algoritmos simples de procesamiento de lenguaje natural, pudimos establecer una correlación entre el peso de una característica específica en el comentario textual que acompaña la reseña, y el ruido presente en la calificación numérica. Específicamente, parece que los usuarios que discuten ampliamente una característica en particular tienden a estar de acuerdo en una calificación común. Esta observación permite la construcción de estimadores de calidad característica por característica que tienen una varianza más baja y, con suerte, son menos ruidosos. Sin embargo, se requiere más evidencia para respaldar la intuición de que las calificaciones correspondientes a pesos altos son opiniones de expertos que merecen ser consideradas de mayor prioridad al calcular estimaciones de calidad. En segundo lugar, enfatizamos la dependencia de las calificaciones en informes previos. Los informes anteriores crean una expectativa de calidad que afecta la percepción subjetiva del usuario. Validamos dos hechos sobre las reseñas de hoteles que recopilamos de TripAdvisor: Primero, las calificaciones que siguen expectativas bajas (donde la expectativa se calcula como el promedio de los informes anteriores) probablemente sean más altas que las calificaciones que siguen expectativas altas. Intuitivamente, la percepción de la calidad (y consecuentemente la calificación) depende de qué tan bien la experiencia real del usuario cumple con sus expectativas. Segundo, incluimos evidencia de los comentarios textuales, y encontramos que cuando los usuarios dedican una gran parte del texto a discutir una característica específica, es probable que motiven una calificación divergente (es decir, una calificación que no se ajusta a la expectativa previa). Intuitivamente, esto respalda la hipótesis de que los foros de reseñas actúan como grupos de discusión donde los usuarios están interesados en presentar y motivar su propia opinión. Hemos capturado la evidencia empírica en un modelo de comportamiento que predice las calificaciones enviadas por los usuarios. La calificación final depende, como era de esperar, de la observación real y de la brecha entre la observación y la expectativa. La brecha tiende a tener una influencia mayor cuando una fracción importante del comentario textual está dedicada a discutir una característica específica. El modelo propuesto fue validado con los datos empíricos y proporciona mejores estimaciones de las calificaciones realmente enviadas. Una suposición que hacemos es sobre la existencia de un valor de calidad objetivo vf para la característica f. Esto rara vez es cierto, especialmente a lo largo de largos periodos de tiempo. Otras explicaciones podrían dar cuenta de la correlación de las calificaciones con informes anteriores. Por ejemplo, si ef (i) refleja el valor real de f en un punto en el tiempo, la diferencia en las calificaciones siguiendo expectativas altas y bajas puede ser explicada por modelos de ingresos hoteleros que se maximizan cuando el valor es modificado en consecuencia. Sin embargo, la idea de que la variación en las calificaciones no es principalmente una función de la variación en el valor resulta ser útil. Nuestro enfoque para aproximar este valor objetivo es de ninguna manera perfecto, pero se ajusta perfectamente a la idea detrás del modelo. Una dirección natural para trabajos futuros es examinar aplicaciones concretas de nuestros resultados. Es probable que se obtengan mejoras significativas en las estimaciones de calidad al incorporar toda la evidencia empírica sobre el comportamiento de calificación. No está claro exactamente cómo diferentes factores afectan las decisiones de los usuarios. La respuesta podría depender de la aplicación particular, el contexto y la cultura. 7. REFERENCIAS [1] A. Admati y P. Pfleiderer. Noisytalk.com: Transmitiendo opiniones en un entorno ruidoso. Documento de trabajo 1670R, Universidad de Stanford, 2000. [2] P. B., L. Lee y S. Vaithyanathan. ¿Clasificación de sentimientos con técnicas de aprendizaje automático? En Actas de EMNLP-02, la Conferencia sobre Métodos Empíricos en el Procesamiento del Lenguaje Natural, 2002. [3] H. Cui, V. Mittal y M. Datar. Experimentos comparativos 141 sobre clasificación de sentimientos para reseñas de productos en línea. En Actas de AAAI, 2006. [4] K. Dave, S. Lawrence y D. Pennock. Extracción de opiniones y clasificación semántica de reseñas de productos en la galería de cacahuetes. En Actas de la 12ª Conferencia Internacional sobre la World Wide Web (WWW03), 2003. [5] C. Dellarocas, N. Awad y X. Zhang. Explorando el valor de las calificaciones de productos en línea en la previsión de ingresos: El caso de las películas en movimiento. Documento de trabajo, 2006. [6] C. Forman, A. Ghose y B. Wiesenfeld. Un Examen Multinivel del Impacto de las Identidades Sociales en las Transacciones Económicas en los Mercados Electrónicos. Disponible en SSRN: http://ssrn.com/abstract=918978, julio de 2006. [7] A. Ghose, P. Ipeirotis y A. Sundararajan. Primas de reputación en mercados electrónicos peer-to-peer: análisis de retroalimentación textual y estructura de red. En el Tercer Taller sobre Economía de Sistemas Peer-to-Peer (P2PECON), 2005. [8] A. Ghose, P. Ipeirotis y A. Sundararajan. Las dimensiones de la reputación en los mercados electrónicos. Documento de trabajo CeDER-06-02, Universidad de Nueva York, 2006. [9] A. Harmon. Error de Amazon revela la guerra de los revisores. The New York Times, 14 de febrero de 2004. [10] D. Houser y J. Wooders. Reputación en subastas: teoría y evidencia de eBay. Revista de Estrategia Económica y de Gestión, 15:353-369, 2006. [11] M. Hu y B. Liu. Extracción y resumen de reseñas de clientes. En Actas de la Conferencia Internacional de la ACM SIGKDD sobre Descubrimiento de Conocimiento y Minería de Datos (KDD04), 2004. [12] N. Hu, P. Pavlou y J. Zhang. ¿Pueden las reseñas en línea revelar la verdadera calidad de un producto? En Actas de la Conferencia ACM sobre Comercio Electrónico (EC 06), 2006. [13] K. Kalyanam y S. McIntyre. Retorno de la reputación en el mercado de subastas en línea. Documento de trabajo 02/03-10-WP, Escuela de Negocios Leavey, Universidad de Santa Clara, 2001. [14] L. Khopkar y P. Resnick. Auto-selección, deslizamiento, salvamento, holgazanería y lapidación: los impactos de la retroalimentación negativa en eBay. En Actas de la Conferencia ACM sobre Comercio Electrónico (EC 05), 2005. [15] M. Melnik y J. Alm. ¿Importa la reputación de un vendedor? evidencia de subastas en eBay. Revista de Economía Industrial, 50(3):337-350, 2002. [16] R. Olshavsky y J. Miller. Expectativas del consumidor, rendimiento del producto y calidad percibida del producto. Revista de Investigación de Marketing, 9:19-21, febrero de 1972. [17] A. Parasuraman, V. Zeithaml y L. Berry. Un Modelo Conceptual de Calidad de Servicio y sus Implicaciones para Investigaciones Futuras. Revista de Marketing, 49:41-50, 1985. [18] A. Parasuraman, V. Zeithaml y L. Berry. SERVQUAL: Una escala de múltiples ítems para medir las percepciones de calidad del servicio por parte del consumidor. Revista de Retail, 64:12-40, 1988. [19] P. Pavlou y A. Dimoka. La naturaleza y función de los comentarios de texto de retroalimentación en los mercados en línea: implicaciones para la construcción de confianza, primas de precio y diferenciación de vendedores. Investigación de Sistemas de Información, 17(4):392-414, 2006. [20] A. Popescu y O. Etzioni. Extrayendo características del producto y opiniones de las reseñas. En Actas de la Conferencia de Tecnología del Lenguaje Humano y Conferencia sobre Métodos Empíricos en Procesamiento del Lenguaje Natural, 2005. [21] R. Teas. Expectativas, Evaluación del Rendimiento y Percepciones de Calidad de los Consumidores. Revista de Marketing, 57:18-34, 1993. [22] E. White. Chateando a un cantante en las listas de éxitos pop. El Wall Street Journal, 15 de octubre de 1999. APÉNDICE A. LISTA DE PALABRAS, LR, ASOCIADAS A LA CARACTERÍSTICA HABITACIONES. Todas las palabras sirven como prefijos: habitación, espacio, interior, decoración, ambiente, atmósfera, comodidad, baño, inodoro, cama, edificio, pared, ventana, privado, temperatura, sábana, lino, almohada, caliente, agua, fría, agua, ducha, vestíbulo, muebles, alfombra, aire, acondicionado, colchón, distribución, diseño, espejo, techo, iluminación, lámpara, sofá, silla, cómoda, armario, armario. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "correlation": {
            "translated_key": "correlación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Understanding User Behavior in Online Feedback Reporting Arjun Talwar Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland arjun@math.stanford.edu Radu Jurca Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland radu.jurca@epfl.ch Boi Faltings Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland boi.faltings@epfl.ch ABSTRACT Online reviews have become increasingly popular as a way to judge the quality of various products and services.",
                "Previous work has demonstrated that contradictory reporting and underlying user biases make judging the true worth of a service difficult.",
                "In this paper, we investigate underlying factors that influence user behavior when reporting feedback.",
                "We look at two sources of information besides numerical ratings: linguistic evidence from the textual comment accompanying a review, and patterns in the time sequence of reports.",
                "We first show that groups of users who amply discuss a certain feature are more likely to agree on a common rating for that feature.",
                "Second, we show that a users rating partly reflects the difference between true quality and prior expectation of quality as inferred from previous reviews.",
                "Both give us a less noisy way to produce rating estimates and reveal the reasons behind user bias.",
                "Our hypotheses were validated by statistical evidence from hotel reviews on the TripAdvisor website.",
                "Categories and Subject Descriptors J.4 [Social and Behavioral Sciences]: Economics General Terms Economics, Experimentation, Reliability 1.",
                "MOTIVATIONS The spread of the internet has made it possible for online feedback forums (or reputation mechanisms) to become an important channel for Word-of-mouth regarding products, services or other types of commercial interactions.",
                "Numerous empirical studies [10, 15, 13, 5] show that buyers seriously consider online feedback when making purchasing decisions, and are willing to pay reputation premiums for products or services that have a good reputation.",
                "Recent analysis, however, raises important questions regarding the ability of existing forums to reflect the real quality of a product.",
                "In the absence of clear incentives, users with a moderate outlook will not bother to voice their opinions, which leads to an unrepresentative sample of reviews.",
                "For example, [12, 1] show that Amazon1 ratings of books or CDs follow with great probability bi-modal, U-shaped distributions where most of the ratings are either very good, or very bad.",
                "Controlled experiments, on the other hand, reveal opinions on the same items that are normally distributed.",
                "Under these circumstances, using the arithmetic mean to predict quality (as most forums actually do) gives the typical user an estimator with high variance that is often false.",
                "Improving the way we aggregate the information available from online reviews requires a deep understanding of the underlying factors that bias the rating behavior of users.",
                "Hu et al. [12] propose the Brag-and-Moan Model where users rate only if their utility of the product (drawn from a normal distribution) falls outside a median interval.",
                "The authors conclude that the model explains the empirical distribution of reports, and offers insights into smarter ways of estimating the true quality of the product.",
                "In the present paper we extend this line of research, and attempt to explain further facts about the behavior of users when reporting online feedback.",
                "Using actual hotel reviews from the TripAdvisor2 website, we consider two additional sources of information besides the basic numerical ratings submitted by users.",
                "The first is simple linguistic evidence from the textual review that usually accompanies the numerical ratings.",
                "We use text-mining techniques similar to [7] and [3], however, we are only interested in identifying what aspects of the service the user is discussing, without computing the semantic orientation of the text.",
                "We find that users who comment more on the same feature are more likely to agree on a common numerical rating for that particular feature.",
                "Intuitively, lengthy comments reveal the importance of the feature to the user.",
                "Since people tend to be more knowledgeable in the aspects they consider important, users who discuss a given feature in more details might be assumed to have more authority in evaluating that feature.",
                "Second we investigate the relationship between a review 1 http://www.amazon.com 2 http://www.tripadvisor.com/ 134 Figure 1: The TripAdvisor page displaying reviews for a popular Boston hotel.",
                "Name of hotel and advertisements were deliberatively erased. and the reviews that preceded it.",
                "A perusal of online reviews shows that ratings are often part of discussion threads, where one post is not necessarily independent of other posts.",
                "One may see, for example, users who make an effort to contradict, or vehemently agree with, the remarks of previous users.",
                "By analyzing the time sequence of reports, we conclude that past reviews influence the future reports, as they create some prior expectation regarding the quality of service.",
                "The subjective perception of the user is influenced by the gap between the prior expectation and the actual performance of the service [17, 18, 16, 21] which will later reflect in the users rating.",
                "We propose a model that captures the dependence of ratings on prior expectations, and validate it using the empirical data we collected.",
                "Both results can be used to improve the way reputation mechanisms aggregate the information from individual reviews.",
                "Our first result can be used to determine a featureby-feature estimate of quality, where for each feature, a different subset of reviews (i.e., those with lengthy comments of that feature) is considered.",
                "The second leads to an algorithm that outputs a more precise estimate of the real quality. 2.",
                "THE DATA SET We use in this paper real hotel reviews collected from the popular travel site TripAdvisor.",
                "TripAdvisor indexes hotels from cities across the world, along with reviews written by travelers.",
                "Users can search the site by giving the hotels name and location (optional).",
                "The reviews for a given hotel are displayed as a list (ordered from the most recent to the oldest), with 5 reviews per page.",
                "The reviews contain: • information about the author of the review (e.g., dates of stay, username of the reviewer, location of the reviewer); • the overall rating (from 1, lowest, to 5, highest); • a textual review containing a title for the review, free comments, and the main things the reviewer liked and disliked; • numerical ratings (from 1, lowest, to 5, highest) for different features (e.g., cleanliness, service, location, etc.)",
                "Below the name of the hotel, TripAdvisor displays the address of the hotel, general information (number of rooms, number of stars, short description, etc), the average overall rating, the TripAdvisor ranking, and an average rating for each feature.",
                "Figure 1 shows the page for a popular Boston hotel whose name (along with advertisements) was explicitly erased.",
                "We selected three cities for this study: Boston, Sydney and Las Vegas.",
                "For each city we considered all hotels that had at least 10 reviews, and recorded all reviews.",
                "Table 1 presents the number of hotels considered in each city, the total number of reviews recorded for each city, and the distribution of hotels with respect to the star-rating (as available on the TripAdvisor site).",
                "Note that not all hotels have a star-rating.",
                "Table 1: A summary of the data set.",
                "City # Reviews # Hotels # of Hotels with 1,2,3,4 & 5 stars Boston 3993 58 1+3+17+15+2 Sydney 1371 47 0+0+9+13+10 Las Vegas 5593 40 0+3+10+9+6 For each review we recorded the overall rating, the textual review (title and body of the review) and the numerical rating on 7 features: Rooms(R), Service(S), Cleanliness(C), Value(V), Food(F), Location(L) and Noise(N).",
                "TripAdvisor does not require users to submit anything other than the overall rating, hence a typical review rates few additional features, regardless of the discussion in the textual comment.",
                "Only the features Rooms(R), Service(S), Cleanliness(C) and Value(V) are rated by a significant number of users.",
                "However, we also selected the features Food(F), Location(L) and Noise(N) because they are referred to in a significant number of textual comments.",
                "For each feature we record the numerical rating given by the user, or 0 when the rating is missing.",
                "The typical length of the textual comment amounts to approximately 200 words.",
                "All data was collected by crawling the TripAdvisor site in September 2006. 2.1 Formal notation We will formally refer to a review by a tuple (r, T) where: • r = (rf ) is a vector containing the ratings rf ∈ {0, 1, . . . 5} for the features f ∈ F = {O, R, S, C, V, F, L, N}; note that the overall rating, rO, is abusively recorded as the rating for the feature Overall(O); • T is the textual comment that accompanies the review. 135 Reviews are indexed according to the variable i, such that (ri , Ti ) is the ith review in our database.",
                "Since we dont record the username of the reviewer, we will also say that the ith review in our data set was submitted by user i.",
                "When we need to consider only the reviews of a given hotel, h, we will use (ri(h) , Ti(h) ) to denote the ith review about the hotel h. 3.",
                "EVIDENCE FROM TEXTUAL COMMENTS The free textual comments associated to online reviews are a valuable source of information for understanding the reasons behind the numerical ratings left by the reviewers.",
                "The text may, for example, reveal concrete examples of aspects that the user liked or disliked, thus justifying some of the high, respectively low ratings for certain features.",
                "The text may also offer guidelines for understanding the preferences of the reviewer, and the weights of different features when computing an overall rating.",
                "The problem, however, is that free textual comments are difficult to read.",
                "Users are required to scroll through many reviews and read mostly repetitive information.",
                "Significant improvements would be obtained if the reviews were automatically interpreted and aggregated.",
                "Unfortunately, this seems a difficult task for computers since human users often use witty language, abbreviations, cultural specific phrases, and the figurative style.",
                "Nevertheless, several important results use the textual comments of online reviews in an automated way.",
                "Using well established natural language techniques, reviews or parts of reviews can be classified as having a positive or negative semantic orientation.",
                "Pang et al. [2] classify movie reviews into positive/negative by training three different classifiers (Naive Bayes, Maximum Entropy and SVM) using classification features based on unigrams, bigrams or part-of-speech tags.",
                "Dave et al. [4] analyze reviews from CNet and Amazon, and surprisingly show that classification features based on unigrams or bigrams perform better than higher-order n-grams.",
                "This result is challenged by Cui et al. [3] who look at large collections of reviews crawled from the web.",
                "They show that the size of the data set is important, and that bigger training sets allow classifiers to successfully use more complex classification features based on n-grams.",
                "Hu and Liu [11] also crawl the web for product reviews and automatically identify product attributes that have been discussed by reviewers.",
                "They use Wordnet to compute the semantic orientation of product evaluations and summarize user reviews by extracting positive and negative evaluations of different product features.",
                "Popescu and Etzioni [20] analyze a similar setting, but use search engine hit-counts to identify product attributes; the semantic orientation is assigned through the relaxation labeling technique.",
                "Ghose et al. [7, 8] analyze seller reviews from the Amazon secondary market to identify the different dimensions (e.g., delivery, packaging, customer support, etc.) of reputation.",
                "They parse the text, and tag the part-of-speech for each word.",
                "Frequent nouns, noun phrases and verbal phrases are identified as dimensions of reputation, while the corresponding modifiers (i.e., adjectives and adverbs) are used to derive numerical scores for each dimension.",
                "The enhanced reputation measure correlates better with the pricing information observed in the market.",
                "Pavlou and Dimoka [19] analyze eBay reviews and find that textual comments have an important impact on reputation premiums.",
                "Our approach is similar to the previously mentioned works, in the sense that we identify the aspects (i.e., hotel features) discussed by the users in the textual reviews.",
                "However, we do not compute the semantic orientation of the text, nor attempt to infer missing ratings.",
                "We define the weight, wi f , of feature f ∈ F in the text Ti associated with the review (ri , Ti ), as the fraction of Ti dedicated to discussing aspects (both positive and negative) related to feature f. We propose an elementary method to approximate the values of these weights.",
                "For each feature we manually construct the word list Lf containing approximately 50 words that are most commonly associated to the feature f. The initial words were selected from reading some of the reviews, and seeing what words coincide with discussion of which features.",
                "The list was then extended by adding all thesaurus entries that were related to the initial words.",
                "Finally, we brainstormed for missing words that would normally be associated with each of the features.",
                "Let Lf ∩Ti be the list of terms common to both Lf and Ti.",
                "Each term of Lf is counted the number of times it appears in Ti , with two exceptions: • in cases where the user submits a title to the review, we account for the title text by appending it three times to the review text Ti .",
                "The intuitive assumption is that the users opinion is more strongly reflected in the title, rather than in the body of the review.",
                "For example, many reviews are accurately summarized by titles such as Excellent service, terrible location or Bad value for money; • certain words that occur only once in the text are counted multiple times if their relevance to that feature is particularly strong.",
                "These were root words for each feature (e.g., staff is a root word for the feature Service), and were weighted either 2 or 3.",
                "Each feature was assigned up to 3 such root words, so almost all words are counted only once.",
                "The list of words for the feature Rooms is given for reference in Appendix A.",
                "The weight wi f is computed as: wi f = |Lf ∩ Ti| f∈F |Lf ∩ Ti| (1) where |Lf ∩Ti | is the number of terms common to Lf and Ti .",
                "The weight for the feature Overall was set to min{ |T i | 5000 , 1} where |Ti | is the number of character in Ti .",
                "The following is a TripAdvisor review for a Boston hotel (the name of the hotel is omitted): Ill start by saying that Im more of a Holiday Inn person than a *** type.",
                "So I get frustrated when I pay double the room rate and get half the amenities that Id get at a Hampton Inn or Holiday Inn.",
                "The location was definitely the main asset of this place.",
                "It was only a few blocks from the Hynes Center subway stop and it was easy to walk to some good restaurants in the Back Bay area.",
                "Boylston isnt far off at all.",
                "So I had no trouble with foregoing a rental car and taking the subway from the airport to the hotel and using the subway for any other travel.",
                "Otherwise, they make you pay for anything and everything. 136 And when youve already dropped $215/night on the room, that gets frustrating.The room itself was decent, about what I would expect.",
                "Staff was also average, not bad and not excellent.",
                "Again, I think youre paying for location and the ability to walk to a lot of good stuff.",
                "But I think next time Ill stay in Brookline, get more amenities, and use the subway a bit more.",
                "This numerical ratings associated to this review are rO = 3, rR = 3, rS = 3, rC = 4, rV = 2 for features Overall(O), Rooms(R), Service(S), Cleanliness(C) and Value(V) respectively.",
                "The ratings for the features Food(F), Location(L) and Noise(N) are absent (i.e., rF = rL = rN = 0).",
                "The weights wf are computed from the following lists of common terms: LR ∩ T ={room}; wR = 0.066 LS ∩ T ={3 * Staff, amenities}; wS = 0.267 LC ∩ T = ∅; wC = 0 LV ∩ T ={$, rate}; wV = 0.133 LF ∩ T ={restaurant}; wF = 0.067 LL ∩ T ={2 * center, 2 * walk, 2 * location, area}; wL = 0.467 LN ∩ T = ∅; wN = 0 The root words Staff and Center were tripled and doubled respectively.",
                "The overall weight of the textual review is wO = 0.197.",
                "These values account reasonably well for the weights of different features in the discussion of the reviewer.",
                "One point to note is that some terms in the lists Lf possess an inherent semantic orientation.",
                "For example the word grime (belonging to the list LC ) would be used most often to assert the presence, and not the absence of grime.",
                "This is unavoidable, but care was taken to ensure words from both sides of the spectrum were used.",
                "For this reason, some lists such as LR contain only nouns of objects that one would typically describe in a room (see Appendix A).",
                "The goal of this section is to analyse the influence of the weights wi f on the numerical ratings ri f .",
                "Intuitively, users who spent a lot of their time discussing a feature f (i.e., wi f is high) had something to say about their experience with regard to this feature.",
                "Obviously, feature f is important for user i.",
                "Since people tend to be more knowledgeable in the aspects they consider important, our hypothesis is that the ratings ri f (corresponding to high weights wi f ) constitute a subset of expert ratings for feature f. Figure 2 plots the distribution of the rates r i(h) C with respect to the weights w i(h) C for the cleanliness of a Las Vegas hotel, h. Here, the high ratings are restricted to the reviews that discuss little the cleanliness.",
                "Whenever cleanliness appears in the discussion, the ratings are low.",
                "Many hotels exhibit similar rating patterns for various features.",
                "Ratings corresponding to low weights span the whole spectrum from 1 to 5, while the ratings corresponding to high weights are more grouped together (either around good or bad ratings).",
                "We therefore make the following hypothesis: Hypothesis 1.",
                "The ratings ri f corresponding to the reviews where wi f is high, are more similar to each other than to the overall collection of ratings.",
                "To test the hypothesis, we take the entire set of reviews, and feature by feature, we compute the standard deviation of the ratings with high weights, and the standard deviation of the entire set of ratings.",
                "High weights were defined as those belonging to the upper 20% of the weight range for the corresponding feature.",
                "If Hypothesis 1 were true, the standard deviation of all ratings should be higher than the standard deviation of the ratings with high weights. 0 1 2 3 4 5 6 0 0.1 0.2 0.3 0.4 0.5 0.6 Rating Weight Figure 2: The distribution of ratings against the weight of the cleanliness feature.",
                "We use a standard T-test to measure the significance of the results.",
                "City by city and feature by feature, Table 2 presents the average standard deviation of all ratings, and the average standard deviation of ratings with high weights.",
                "Indeed, the ratings with high weights have lower standard deviation, and the results are significant at the standard 0.05 significance threshold (although for certain cities taken independently there doesnt seem to be a significant difference, the results are significant for the entire data set).",
                "Please note that only the features O,R,S,C and V were considered, since for the others (F, L, and N) we didnt have enough ratings.",
                "Table 2: Average standard deviation for all ratings, and average standard deviation for ratings with high weights.",
                "In square brackets, the corresponding p-values for a positive difference between the two.",
                "City O R S C V all 1.189 0.998 1.144 0.935 1.123 Boston high 0.948 0.778 0.954 0.767 0.891 p-val [0.000] [0.004] [0.045] [0.080] [0.009] all 1.040 0.832 1.101 0.847 0.963 Sydney high 0.801 0.618 0.691 0.690 0.798 p-val [0.012] [0.023] [0.000] [0.377] [0.037] all 1.272 1.142 1.184 1.119 1.242 Vegas high 1.072 0.752 1.169 0.907 1.003 p-val [0.0185] [0.001] [0.918] [0.120] [0.126] Hypothesis 1 not only provides some basic understanding regarding the rating behavior of online users, it also suggests some ways of computing better quality estimates.",
                "We can, for example, construct a feature-by-feature quality estimate with much lower variance: for each feature we take the subset of reviews that amply discuss that feature, and output as a quality estimate the average rating for this subset.",
                "Initial experiments suggest that the average feature-by-feature ratings computed in this way are different from the average ratings computed on the whole data set.",
                "Given that, indeed, high weights are indicators of expert opinions, the estimates obtained in this way are more accurate than the current ones.",
                "Nevertheless, the validation of this underlying assumption requires further controlled experiments. 137 4.",
                "THE INFLUENCE OF PAST RATINGS Two important assumptions are generally made about reviews submitted to online forums.",
                "The first is that ratings truthfully reflect the quality observed by the users; the second is that reviews are independent from one another.",
                "While anecdotal evidence [9, 22] challenges the first assumption3 , in this section, we address the second.",
                "A perusal of online reviews shows that reviews are often part of discussion threads, where users make an effort to contradict, or vehemently agree with the remarks of previous users.",
                "Consider, for example, the following review: I dont understand the negative reviews... the hotel was a little dark, but that was the style.",
                "It was very artsy.",
                "Yes it was close to the freeway, but in my opinion the sound of an occasional loud car is better than hearing the ding ding of slot machines all night!",
                "The staff on-hand is FABULOUS.",
                "The waitresses are great (and *** does not deserve the bad review she got, she was 100% attentive to us! ), the bartenders are friendly and professional at the same time...",
                "Here, the user was disturbed by previous negative reports, addressed these concerns, and set about trying to correct them.",
                "Not surprisingly, his ratings were considerably higher than the average ratings up to this point.",
                "It seems that TripAdvisor users regularly read the reports submitted by previous users before booking a hotel, or before writing a review.",
                "Past reviews create some prior expectation regarding the quality of service, and this expectation has an influence on the submitted review.",
                "We believe this observation holds for most online forums.",
                "The subjective perception of quality is directly proportional to how well the actual experience meets the prior expectation, a fact confirmed by an important line of econometric and marketing research [17, 18, 16, 21].",
                "The <br>correlation</br> between the reviews has also been confirmed by recent research on the dynamics of online review forums [6]. 4.1 Prior Expectations We define the prior expectation of user i regarding the feature f, as the average of the previously available ratings on the feature f4 : ef (i) = j<i,r j f =0 rj f j<i,r j f =0 1 As a first hypothesis, we assert that the rating ri f is a function of the prior expectation ef (i): Hypothesis 2.",
                "For a given hotel and feature, given the reviews i and j such that ef (i) is high and ef (j) is low, the rating rj f exceeds the rating ri f .",
                "We define high and low expectations as those that are above, respectively below a certain cutoff value θ.",
                "The set of reviews preceded by high, respectively low expectations 3 part of Amazon reviews were recognized as strategic posts by book authors or competitors 4 if no previous ratings were assigned for feature f, ef (i) is assigned a default value of 4.",
                "Table 3: Average ratings for reviews preceded by low (first value in the cell) and high (second value in the cell) expectations.",
                "The P-values for a positive difference are given square brackets.",
                "City O R S C V 3.953 4.045 3.985 4.252 3.946 Boston 3.364 3.590 3.485 3.641 3.242 [0.011] [0.028] [0.0086] [0.0168] [0.0034] 4.284 4.358 4.064 4.530 4.428 Sydney 3.756 3.537 3.436 3.918 3.495 [0.000] [0.000] [0.035] [0.009] [0.000] 3.494 3.674 3.713 3.689 3.580 Las Vegas 3.140 3.530 2.952 3.530 3.351 [0.190] [0.529] [0.007] [0.529] [0.253] are defined as follows: Rhigh f = {ri f |ef (i) > θ} Rlow f = {ri f |ef (i) < θ} These sets are specific for each (hotel, feature) pair, and in our experiments we took θ = 4.",
                "This rather high value is close to the average rating across all features across all hotels, and is justified by the fact that our data set contains mostly high quality hotels.",
                "For each city, we take all hotels and compute the average ratings in the sets Rhigh f and Rlow f (see Table 3).",
                "The average rating amongst reviews following low prior expectations is significantly higher than the average rating following high expectations.",
                "As further evidence, we consider all hotels for which the function eV (i) (the expectation for the feature Value) has a high value (greater than 4) for some i, and a low value (less than 4) for some other i.",
                "Intuitively, these are the hotels for which there is a minimal degree of variation in the timely sequence of reviews: i.e., the cumulative average of ratings was at some point high and afterwards became low, or vice-versa.",
                "Such variations are observed for about half of all hotels in each city.",
                "Figure 3 plots the median (across considered hotels) rating, rV , when ef (i) is not more than x but greater than x − 0.5. 2.5 3 3.5 4 4.5 5 2.5 3 3.5 4 4.5 5 Medianofrating expectation Boston Sydney Vegas Figure 3: The ratings tend to decrease as the expectation increases. 138 There are two ways to interpret the function ef (i): • The expected value for feature f obtained by user i before his experience with the service, acquired by reading reports submitted by past users.",
                "In this case, an overly high value for ef (i) would drive the user to submit a negative report (or vice versa), stemming from the difference between the actual value of the service, and the inflated expectation of this value acquired before his experience. • The expected value of feature f for all subsequent visitors of the site, if user i were not to submit a report.",
                "In this case, the motivation for a negative report following an overly high value of ef is different: user i seeks to correct the expectation of future visitors to the site.",
                "Unlike the interpretation above, this does not require the user to derive an a priori expectation for the value of f. Note that neither interpretation implies that the average up to report i is inversely related to the rating at report i.",
                "There might exist a measure of influence exerted by past reports that pushes the user behind report i to submit ratings which to some extent conforms with past reports: a low value for ef (i) can influence user i to submit a low rating for feature f because, for example, he fears that submitting a high rating will make him out to be a person with low standards5 .",
                "This, at first, appears to contradict Hypothesis 2.",
                "However, this conformity rating cannot continue indefinitely: once the set of reports project a sufficiently deflated estimate for vf , future reviewers with comparatively positive impressions will seek to correct this misconception. 4.2 Impact of textual comments on quality expectation Further insight into the rating behavior of TripAdvisor users can be obtained by analyzing the relationship between the weights wf and the values ef (i).",
                "In particular, we examine the following hypothesis: Hypothesis 3.",
                "When a large proportion of the text of a review discusses a certain feature, the difference between the rating for that feature and the average rating up to that point tends to be large.",
                "The intuition behind this claim is that when the user is adamant about voicing his opinion regarding a certain feature, his opinion differs from the collective opinion of previous postings.",
                "This relies on the characteristic of reputation systems as feedback forums where a user is interested in projecting his opinion, with particular strength if this opinion differs from what he perceives to be the general opinion.",
                "To test Hypothesis 3 we measure the average absolute difference between the expectation ef (i) and the rating ri f when the weight wi f is high, respectively low.",
                "Weights are classified high or low by comparing them with certain cutoff values: wi f is low if smaller than 0.1, while wi f is high if greater than θf .",
                "Different cutoff values were used for different features: θR = 0.4, θS = 0.4, θC = 0.2, and θV = 0.7.",
                "Cleanliness has a lower cutoff since it is a feature rarely discussed; Value has a high cutoff for the opposite reason.",
                "Results are presented in Table 4. 5 The idea that negative reports can encourage further negative reporting has been suggested before [14] Table 4: Average of |ri f −ef (i)| when weights are high (first value in the cell) and low (second value in the cell) with P-values for the difference in sq. brackets.",
                "City R S C V 1.058 1.208 1.728 1.356 Boston 0.701 0.838 0.760 0.917 [0.022] [0.063] [0.000] [0.218] 1.048 1.351 1.218 1.318 Sydney 0.752 0.759 0.767 0.908 [0.179] [0.009] [0.165] [0.495] 1.184 1.378 1.472 1.642 Las Vegas 0.772 0.834 0.808 1.043 [0.071] [0.020] [0.006] [0.076] This demonstrates that when weights are unusually high, users tend to express an opinion that does not conform to the net average of previous ratings.",
                "As we might expect, for a feature that rarely was a high weight in the discussion, (e.g., cleanliness) the difference is particularly large.",
                "Even though the difference in the feature Value is quite large for Sydney, the P-value is high.",
                "This is because only few reviews discussed value heavily.",
                "The reason could be cultural or because there was less of a reason to discuss this feature. 4.3 Reporting Incentives Previous models suggest that users who are not highly opinionated will not choose to voice their opinions [12].",
                "In this section, we extend this model to account for the influence of expectations.",
                "The motivation for submitting feedback is not only due to extreme opinions, but also to the difference between the current reputation (i.e., the prior expectation of the user) and the actual experience.",
                "Such a rating model produces ratings that most of the time deviate from the current average rating.",
                "The ratings that confirm the prior expectation will rarely be submitted.",
                "We test on our data set the proportion of ratings that attempt to correct the current estimate.",
                "We define a deviant rating as one that deviates from the current expectation by at least some threshold θ, i.e., |ri f − ef (i)| ≥ θ.",
                "For each of the three considered cities, the following tables, show the proportion of deviant ratings for θ = 0.5 and θ = 1.",
                "Table 5: Proportion of deviant ratings with θ = 0.5 City O R S C V Boston 0.696 0.619 0.676 0.604 0.684 Sydney 0.645 0.615 0.672 0.614 0.675 Las Vegas 0.721 0.641 0.694 0.662 0.724 Table 6: Proportion of deviant ratings with θ = 1 City O R S C V Boston 0.420 0.397 0.429 0.317 0.446 Sydney 0.360 0.367 0.442 0.336 0.489 Las Vegas 0.510 0.421 0.483 0.390 0.472 The above results suggest that a large proportion of users (close to one half, even for the high threshold value θ = 1) deviate from the prior average.",
                "This reinforces the idea that users are more likely to submit a report when they believe they have something distinctive to add to the current stream of opinions for some feature.",
                "Such conclusions are in total agreement with prior evidence that the distribution of reports often follows bi-modal, U-shaped distributions. 139 5.",
                "MODELLING THE BEHAVIOR OF RATERS To account for the observations described in the previous sections, we propose a model for the behavior of the users when submitting online reviews.",
                "For a given hotel, we make the assumption that the quality experienced by the users is normally distributed around some value vf , which represents the objective quality offered by the hotel on the feature f. The rating submitted by user i on feature f is: ˆri f = δf vi f + (1 − δf ) · sign vi f − ef (i) c + d(vi f , ef (i)|wi f ) (2) where: • vi f is the (unknown) quality actually experienced by the user. vi f is assumed normally distributed around some value vf ; • δf ∈ [0, 1] can be seen as a measure of the bias when reporting feedback.",
                "High values reflect the fact that users rate objectively, without being influenced by prior expectations.",
                "The value of δf may depend on various factors; we fix one value for each feature f; • c is a constant between 1 and 5; • wi f is the weight of feature f in the textual comment of review i, computed according to Eq. (1); • d(vi f , ef (i)|wi f ) is a distance function between the expectation and the observation of user i.",
                "The distance function satisfies the following properties: - d(y, z|w) ≥ 0 for all y, z ∈ [0, 5], w ∈ [0, 1]; - |d(y, z|w)| < |d(z, x|w)| if |y − z| < |z − x|; - |d(y, z|w1)| < |d(y, z|w2)| if w1 < w2; - c + d(vf , ef (i)|wi f ) ∈ [1, 5]; The second term of Eq. (2) encodes the bias of the rating.",
                "The higher the distance between the true observation vi f and the function ef , the higher the bias. 5.1 Model Validation We use the data set of TripAdvisor reviews to validate the behavior model presented above.",
                "We split for convenience the rating values in three ranges: bad (B = {1, 2}), indifferent (I = {3, 4}), and good (G = {5}), and perform the following two tests: • First, we will use our model to predict the ratings that have extremal values.",
                "For every hotel, we take the sequence of reports, and whenever we encounter a rating that is either good or bad (but not indifferent) we try to predict it using Eq. (2) • Second, instead of predicting the value of extremal ratings, we try to classify them as either good or bad.",
                "For every hotel we take the sequence of reports, and for each report (regardless of it value) we classify it as being good or bad However, to perform these tests, we need to estimate the objective value, vf , that is the average of the true quality observations, vi f .",
                "The algorithm we are using is based on the intuition that the amount of conformity rating is minimized.",
                "In other words, the value vf should be such that as often as possible, bad ratings follow expectations above vf and good ratings follow expectations below vf .",
                "Formally, we define the sets: Γ1 = {i|ef (i) < vf and ri f ∈ B}; Γ2 = {i|ef (i) > vf and ri f ∈ G}; that correspond to irregularities where even though the expectation at point i is lower than the delivered value, the rating is poor, and vice versa.",
                "We define vf as the value that minimize these union of the two sets: vf = arg min vf |Γ1 ∪ Γ2| (3) In Eq. (2) we replace vi f by the value vf computed in Eq. (3), and use the following distance function: d(vf , ef (i)|wi f ) = |vf − ef (i)| vf − ef (i) |vf 2 − ef (i)2 | · (1 + 2wi f ); The constant c ∈ I was set to min{max{ef (i), 3}}, 4}.",
                "The values for δf were fixed at {0.7, 0.7, 0.8, 0.7, 0.6} for the features {Overall, Rooms, Service, Cleanliness, Value} respectively.",
                "The weights are computed as described in Section 3.",
                "As a first experiment, we take the sets of extremal ratings {ri f |ri f /∈ I} for each hotel and feature.",
                "For every such rating, ri f , we try to estimate it by computing ˆri f using Eq. (2).",
                "We compare this estimator with the one obtained by simply averaging the ratings over all hotels and features: i.e., ¯rf = j,r j f =0 rj f j,r j f =0 1 ; Table 7 presents the ratio between the root mean square error (RMSE) when using ˆri f and ¯rf to estimate the actual ratings.",
                "In all cases the estimate produced by our model is better than the simple average.",
                "Table 7: Average of RMSE(ˆrf ) RMSE(¯rf ) City O R S C V Boston 0.987 0.849 0.879 0.776 0.913 Sydney 0.927 0.817 0.826 0.720 0.681 Las Vegas 0.952 0.870 0.881 0.947 0.904 As a second experiment, we try to distinguish the sets Bf = {i|ri f ∈ B} and Gf = {i|ri f ∈ G} of bad, respectively good ratings on the feature f. For example, we compute the set Bf using the following classifier (called σ): ri f ∈ Bf (σf (i) = 1) ⇔ ˆri f ≤ 4; Tables 8, 9 and 10 present the Precision(p), Recall(r) and s = 2pr p+r for classifier σ, and compares it with a naive majority classifier, τ, τf (i) = 1 ⇔ |Bf | ≥ |Gf |: We see that recall is always higher for σ and precision is usually slightly worse.",
                "For the s metric σ tends to add a 140 Table 8: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Boston O R S C V p 0.678 0.670 0.573 0.545 0.610 σ r 0.626 0.659 0.619 0.612 0.694 s 0.651 0.665 0.595 0.577 0.609 p 0.684 0.706 0.647 0.611 0.633 τ r 0.597 0.541 0.410 0.383 0.562 s 0.638 0.613 0.502 0.471 0.595 Table 9: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Las Vegas O R S C V p 0.654 0.748 0.592 0.712 0.583 σ r 0.608 0.536 0.791 0.474 0.610 s 0.630 0.624 0.677 0.569 0.596 p 0.685 0.761 0.621 0.748 0.606 τ r 0.542 0.505 0.767 0.445 0.441 s 0.605 0.607 0.670 0.558 0.511 1-20% improvement over τ, much higher in some cases for hotels in Sydney.",
                "This is likely because Sydney reviews are more positive than those of the American cities and cases where the number of bad reviews exceeded the number of good ones are rare.",
                "Replacing the test algorithm with one that plays a 1 with probability equal to the proportion of bad reviews improves its results for this city, but it is still outperformed by around 80%. 6.",
                "SUMMARY OF RESULTS AND CONCLUSION The goal of this paper is to explore the factors that drive a user to submit a particular rating, rather than the incentives that encouraged him to submit a report in the first place.",
                "For that we use two additional sources of information besides the vector of numerical ratings: first we look at the textual comments that accompany the reviews, and second we consider the reports that have been previously submitted by other users.",
                "Using simple natural language processing algorithms, we were able to establish a <br>correlation</br> between the weight of a certain feature in the textual comment accompanying the review, and the noise present in the numerical rating.",
                "Specifically, it seems that users who discuss amply a certain feature are likely to agree on a common rating.",
                "This observation allows the construction of feature-by-feature estimators of quality that have a lower variance, and are hopefully less noisy.",
                "Nevertheless, further evidence is required to support the intuition that ratings corresponding to high weights are expert opinions that deserve to be given higher priority when computing estimates of quality.",
                "Second, we emphasize the dependence of ratings on previous reports.",
                "Previous reports create an expectation of quality which affects the subjective perception of the user.",
                "We validate two facts about the hotel reviews we collected from TripAdvisor: First, the ratings following low expectations (where the expectation is computed as the average of the previous reports) are likely to be higher than the ratings Table 10: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Sydney O R S C V p 0.650 0.463 0.544 0.550 0.580 σ r 0.234 0.378 0.571 0.169 0.592 s 0.343 0.452 0.557 0.259 0.586 p 0.562 0.615 0.600 0.500 0.600 τ r 0.054 0.098 0.101 0.015 0.175 s 0.098 0.168 0.172 0.030 0.271 following high expectations.",
                "Intuitively, the perception of quality (and consequently the rating) depends on how well the actual experience of the user meets her expectation.",
                "Second, we include evidence from the textual comments, and find that when users devote a large fraction of the text to discussing a certain feature, they are likely to motivate a divergent rating (i.e., a rating that does not conform to the prior expectation).",
                "Intuitively, this supports the hypothesis that review forums act as discussion groups where users are keen on presenting and motivating their own opinion.",
                "We have captured the empirical evidence in a behavior model that predicts the ratings submitted by the users.",
                "The final rating depends, as expected, on the true observation, and on the gap between the observation and the expectation.",
                "The gap tends to have a bigger influence when an important fraction of the textual comment is dedicated to discussing a certain feature.",
                "The proposed model was validated on the empirical data and provides better estimates of the ratings actually submitted.",
                "One assumption that we make is about the existence of an objective quality value vf for the feature f. This is rarely true, especially over large spans of time.",
                "Other explanations might account for the <br>correlation</br> of ratings with past reports.",
                "For example, if ef (i) reflects the true value of f at a point in time, the difference in the ratings following high and low expectations can be explained by hotel revenue models that are maximized when the value is modified accordingly.",
                "However, the idea that variation in ratings is not primarily a function of variation in value turns out to be a useful one.",
                "Our approach to approximate this elusive objective value is by no means perfect, but conforms neatly to the idea behind the model.",
                "A natural direction for future work is to examine concrete applications of our results.",
                "Significant improvements of quality estimates are likely to be obtained by incorporating all empirical evidence about rating behavior.",
                "Exactly how different factors affect the decisions of the users is not clear.",
                "The answer might depend on the particular application, context and culture. 7.",
                "REFERENCES [1] A. Admati and P. Pfleiderer.",
                "Noisytalk.com: Broadcasting opinions in a noisy environment.",
                "Working Paper 1670R, Stanford University, 2000. [2] P. B., L. Lee, and S. Vaithyanathan.",
                "Thumbs up? sentiment classification using machine learning techniques.",
                "In Proceedings of the EMNLP-02, the Conference on Empirical Methods in Natural Language Processing, 2002. [3] H. Cui, V. Mittal, and M. Datar.",
                "Comparative 141 Experiments on Sentiment Classification for Online Product Reviews.",
                "In Proceedings of AAAI, 2006. [4] K. Dave, S. Lawrence, and D. Pennock.",
                "Mining the peanut gallery:opinion extraction and semantic classification of product reviews.",
                "In Proceedings of the 12th International Conference on the World Wide Web (WWW03), 2003. [5] C. Dellarocas, N. Awad, and X. Zhang.",
                "Exploring the Value of Online Product Ratings in Revenue Forecasting: The Case of Motion Pictures.",
                "Working paper, 2006. [6] C. Forman, A. Ghose, and B. Wiesenfeld.",
                "A Multi-Level Examination of the Impact of Social Identities on Economic Transactions in Electronic Markets.",
                "Available at SSRN: http://ssrn.com/abstract=918978, July 2006. [7] A. Ghose, P. Ipeirotis, and A. Sundararajan.",
                "Reputation Premiums in Electronic Peer-to-Peer Markets: Analyzing Textual Feedback and Network Structure.",
                "In Third Workshop on Economics of Peer-to-Peer Systems, (P2PECON), 2005. [8] A. Ghose, P. Ipeirotis, and A. Sundararajan.",
                "The Dimensions of Reputation in electronic Markets.",
                "Working Paper CeDER-06-02, New York University, 2006. [9] A. Harmon.",
                "Amazon Glitch Unmasks War of Reviewers.",
                "The New York Times, February 14, 2004. [10] D. Houser and J. Wooders.",
                "Reputation in Auctions: Theory and Evidence from eBay.",
                "Journal of Economics and Management Strategy, 15:353-369, 2006. [11] M. Hu and B. Liu.",
                "Mining and summarizing customer reviews.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD04), 2004. [12] N. Hu, P. Pavlou, and J. Zhang.",
                "Can Online Reviews Reveal a Products True Quality?",
                "In Proceedings of ACM Conference on Electronic Commerce (EC 06), 2006. [13] K. Kalyanam and S. McIntyre.",
                "Return on reputation in online auction market.",
                "Working Paper 02/03-10-WP, Leavey School of Business, Santa Clara University., 2001. [14] L. Khopkar and P. Resnick.",
                "Self-Selection, Slipping, Salvaging, Slacking, and Stoning: the Impacts of Negative Feedback at eBay.",
                "In Proceedings of ACM Conference on Electronic Commerce (EC 05), 2005. [15] M. Melnik and J. Alm.",
                "Does a sellers reputation matter? evidence from ebay auctions.",
                "Journal of Industrial Economics, 50(3):337-350, 2002. [16] R. Olshavsky and J. Miller.",
                "Consumer Expectations, Product Performance and Perceived Product Quality.",
                "Journal of Marketing Research, 9:19-21, February 1972. [17] A. Parasuraman, V. Zeithaml, and L. Berry.",
                "A Conceptual Model of Service Quality and Its Implications for Future Research.",
                "Journal of Marketing, 49:41-50, 1985. [18] A. Parasuraman, V. Zeithaml, and L. Berry.",
                "SERVQUAL: A Multiple-Item Scale for Measuring Consumer Perceptions of Service Quality.",
                "Journal of Retailing, 64:12-40, 1988. [19] P. Pavlou and A. Dimoka.",
                "The Nature and Role of Feedback Text Comments in Online Marketplaces: Implications for Trust Building, Price Premiums, and Seller Differentiation.",
                "Information Systems Research, 17(4):392-414, 2006. [20] A. Popescu and O. Etzioni.",
                "Extracting product features and opinions from reviews.",
                "In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, 2005. [21] R. Teas.",
                "Expectations, Performance Evaluation, and Consumers Perceptions of Quality.",
                "Journal of Marketing, 57:18-34, 1993. [22] E. White.",
                "Chatting a Singer Up the Pop Charts.",
                "The Wall Street Journal, October 15, 1999.",
                "APPENDIX A.",
                "LIST OF WORDS, LR, ASSOCIATED TO THE FEATURE ROOMS All words serve as prefixes: room, space, interior, decor, ambiance, atmosphere, comfort, bath, toilet, bed, building, wall, window, private, temperature, sheet, linen, pillow, hot, water, cold, water, shower, lobby, furniture, carpet, air, condition, mattress, layout, design, mirror, ceiling, lighting, lamp, sofa, chair, dresser, wardrobe, closet 142"
            ],
            "original_annotated_samples": [
                "The <br>correlation</br> between the reviews has also been confirmed by recent research on the dynamics of online review forums [6]. 4.1 Prior Expectations We define the prior expectation of user i regarding the feature f, as the average of the previously available ratings on the feature f4 : ef (i) = j<i,r j f =0 rj f j<i,r j f =0 1 As a first hypothesis, we assert that the rating ri f is a function of the prior expectation ef (i): Hypothesis 2.",
                "Using simple natural language processing algorithms, we were able to establish a <br>correlation</br> between the weight of a certain feature in the textual comment accompanying the review, and the noise present in the numerical rating.",
                "Other explanations might account for the <br>correlation</br> of ratings with past reports."
            ],
            "translated_annotated_samples": [
                "La <br>correlación</br> entre las reseñas también ha sido confirmada por investigaciones recientes sobre la dinámica de los foros de reseñas en línea [6]. 4.1 Expectativas Previas Definimos la expectativa previa del usuario i con respecto a la característica f, como el promedio de las calificaciones previamente disponibles en la característica f4: ef (i) = j<i,r j f =0 rj f j<i,r j f =0 1 Como primera hipótesis, afirmamos que la calificación ri f es una función de la expectativa previa ef (i): Hipótesis 2.",
                "Usando algoritmos simples de procesamiento de lenguaje natural, pudimos establecer una <br>correlación</br> entre el peso de una característica específica en el comentario textual que acompaña la reseña, y el ruido presente en la calificación numérica.",
                "Otras explicaciones podrían dar cuenta de la <br>correlación</br> de las calificaciones con informes anteriores."
            ],
            "translated_text": "La comprensión del comportamiento del usuario en la presentación de comentarios en línea. Trabajos anteriores han demostrado que la información contradictoria y los sesgos subyacentes de los usuarios dificultan juzgar el verdadero valor de un servicio. En este artículo, investigamos los factores subyacentes que influyen en el comportamiento del usuario al informar retroalimentación. Examinamos dos fuentes de información además de las calificaciones numéricas: la evidencia lingüística del comentario textual que acompaña a una reseña y los patrones en la secuencia temporal de los informes. Primero demostramos que los grupos de usuarios que discuten ampliamente una característica específica tienen más probabilidades de ponerse de acuerdo en una calificación común para esa característica. Segundo, demostramos que la calificación de un usuario refleja en parte la diferencia entre la calidad real y la expectativa previa de calidad, tal como se infiere de revisiones anteriores. Ambos nos ofrecen una forma menos ruidosa de producir estimaciones de calificación y revelar las razones detrás del sesgo del usuario. Nuestras hipótesis fueron validadas por evidencia estadística de reseñas de hoteles en el sitio web de TripAdvisor. Categorías y Descriptores de Asignaturas J.4 [Ciencias Sociales y del Comportamiento]: Economía Términos Generales Economía, Experimentación, Confiabilidad 1. MOTIVACIONES La expansión de internet ha hecho posible que los foros de retroalimentación en línea (o mecanismos de reputación) se conviertan en un canal importante para el boca a boca sobre productos, servicios u otros tipos de interacciones comerciales. Numerosos estudios empíricos [10, 15, 13, 5] muestran que los compradores consideran seriamente los comentarios en línea al tomar decisiones de compra, y están dispuestos a pagar primas por reputación por productos o servicios que tienen una buena reputación. Sin embargo, un análisis reciente plantea preguntas importantes sobre la capacidad de los foros existentes para reflejar la verdadera calidad de un producto. En ausencia de incentivos claros, los usuarios con una perspectiva moderada no se molestarán en expresar sus opiniones, lo que conduce a una muestra no representativa de reseñas. Por ejemplo, [12, 1] muestran que las calificaciones de libros o CDs en Amazon1 siguen con gran probabilidad distribuciones bimodales en forma de U, donde la mayoría de las calificaciones son o muy buenas o muy malas. Los experimentos controlados, por otro lado, revelan opiniones sobre los mismos elementos que están distribuidas de forma normal. Bajo estas circunstancias, utilizar la media aritmética para predecir la calidad (como la mayoría de los foros realmente hacen) proporciona al usuario típico un estimador con una alta varianza que a menudo es incorrecto. Mejorar la forma en que agregamos la información disponible de las reseñas en línea requiere un profundo entendimiento de los factores subyacentes que sesgan el comportamiento de calificación de los usuarios. Hu et al. [12] proponen el Modelo Brag-and-Moan donde los usuarios califican solo si su utilidad del producto (extraída de una distribución normal) cae fuera de un intervalo mediano. Los autores concluyen que el modelo explica la distribución empírica de los informes y ofrece ideas sobre formas más inteligentes de estimar la verdadera calidad del producto. En el presente artículo ampliamos esta línea de investigación e intentamos explicar más hechos sobre el comportamiento de los usuarios al informar retroalimentación en línea. Usando reseñas reales de hoteles del sitio web TripAdvisor2, consideramos dos fuentes adicionales de información además de las calificaciones numéricas básicas enviadas por los usuarios. La primera es una evidencia lingüística simple proveniente de la revisión textual que suele acompañar a las calificaciones numéricas. Utilizamos técnicas de minería de texto similares a [7] y [3], sin embargo, solo estamos interesados en identificar qué aspectos del servicio está discutiendo el usuario, sin calcular la orientación semántica del texto. Observamos que los usuarios que comentan más sobre la misma característica tienen más probabilidades de estar de acuerdo en una calificación numérica común para esa característica en particular. Intuitivamente, los comentarios extensos revelan la importancia de la característica para el usuario. Dado que las personas tienden a ser más conocedoras en los aspectos que consideran importantes, se podría asumir que los usuarios que discuten una característica específica en más detalle tienen más autoridad para evaluar esa característica. Segundo investigamos la relación entre una reseña 1 http://www.amazon.com 2 http://www.tripadvisor.com/ 134 Figura 1: La página de TripAdvisor que muestra reseñas de un popular hotel en Boston. El nombre del hotel y los anuncios fueron deliberadamente borrados, al igual que las reseñas que lo precedieron. Un examen de las reseñas en línea muestra que las calificaciones a menudo forman parte de los hilos de discusión, donde una publicación no es necesariamente independiente de otras publicaciones. Uno puede ver, por ejemplo, usuarios que se esfuerzan por contradecir o estar vehementemente de acuerdo con los comentarios de usuarios anteriores. Al analizar la secuencia temporal de informes, concluimos que las revisiones pasadas influyen en los informes futuros, ya que crean cierta expectativa previa sobre la calidad del servicio. La percepción subjetiva del usuario está influenciada por la brecha entre la expectativa previa y el rendimiento real del servicio [17, 18, 16, 21], lo cual se reflejará más tarde en la calificación de los usuarios. Proponemos un modelo que captura la dependencia de las calificaciones en las expectativas previas, y lo validamos utilizando los datos empíricos que recopilamos. Ambos resultados pueden ser utilizados para mejorar la forma en que los mecanismos de reputación agregan la información de las revisiones individuales. Nuestro primer resultado se puede utilizar para determinar una estimación de calidad característica por característica, donde para cada característica se considera un subconjunto diferente de reseñas (es decir, aquellas con comentarios extensos sobre esa característica). El segundo conduce a un algoritmo que produce una estimación más precisa de la calidad real. 2. El conjunto de datos que utilizamos en este artículo son reseñas reales de hoteles recopiladas del popular sitio de viajes TripAdvisor. TripAdvisor indexa hoteles de ciudades de todo el mundo, junto con reseñas escritas por viajeros. Los usuarios pueden buscar en el sitio proporcionando el nombre del hotel y la ubicación (opcional). Las reseñas de un hotel dado se muestran como una lista (ordenadas de la más reciente a la más antigua), con 5 reseñas por página. Las reseñas contienen: • información sobre el autor de la reseña (por ejemplo, fechas de estancia, nombre de usuario del revisor, ubicación del revisor); • la calificación general (de 1, la más baja, a 5, la más alta); • una reseña textual que incluye un título para la reseña, comentarios libres y las principales cosas que al revisor le gustaron y no le gustaron; • calificaciones numéricas (de 1, la más baja, a 5, la más alta) para diferentes características (por ejemplo, limpieza, servicio, ubicación, etc.). Debajo del nombre del hotel, TripAdvisor muestra la dirección del hotel, información general (número de habitaciones, número de estrellas, breve descripción, etc.), la calificación promedio general, el ranking de TripAdvisor y una calificación promedio para cada característica. La Figura 1 muestra la página de un hotel popular en Boston cuyo nombre (junto con los anuncios) fue explícitamente borrado. Seleccionamos tres ciudades para este estudio: Boston, Sídney y Las Vegas. Para cada ciudad consideramos todos los hoteles que tenían al menos 10 reseñas, y registramos todas las reseñas. La Tabla 1 presenta el número de hoteles considerados en cada ciudad, el número total de reseñas registradas para cada ciudad y la distribución de hoteles con respecto a la calificación por estrellas (según está disponible en el sitio de TripAdvisor). Ten en cuenta que no todos los hoteles tienen una clasificación por estrellas. Tabla 1: Un resumen del conjunto de datos. Para cada reseña, registramos la calificación general, la reseña textual (título y cuerpo de la reseña) y la calificación numérica en 7 características: Habitaciones (R), Servicio (S), Limpieza (C), Valor (V), Comida (F), Ubicación (L) y Ruido (N). TripAdvisor no requiere que los usuarios envíen nada más que la calificación general, por lo tanto, una reseña típica evalúa pocas características adicionales, independientemente de la discusión en el comentario textual. Solo las características Habitaciones (R), Servicio (S), Limpieza (C) y Valor (V) son evaluadas por un número significativo de usuarios. Sin embargo, también seleccionamos las características Comida (F), Ubicación (L) y Ruido (N) porque son mencionadas en un número significativo de comentarios textuales. Para cada característica registramos la calificación numérica dada por el usuario, o 0 cuando la calificación está ausente. La longitud típica del comentario textual equivale aproximadamente a 200 palabras. Todos los datos fueron recopilados rastreando el sitio de TripAdvisor en septiembre de 2006. 2.1 Notación formal Nos referiremos formalmente a una reseña mediante una tupla (r, T) donde: • r = (rf ) es un vector que contiene las calificaciones rf ∈ {0, 1, . . . 5} para las características f ∈ F = {O, R, S, C, V, F, L, N}; cabe destacar que la calificación general, rO, se registra de forma abusiva como la calificación para la característica General(O); • T es el comentario textual que acompaña a la reseña. Se indexan 135 reseñas de acuerdo con la variable i, de modo que (ri , Ti ) es la i-ésima reseña en nuestra base de datos. Dado que no registramos el nombre de usuario del revisor, también diremos que la i-ésima reseña en nuestro conjunto de datos fue enviada por el usuario i. Cuando necesitemos considerar solo las reseñas de un hotel dado, h, usaremos (ri(h), Ti(h)) para denotar la i-ésima reseña sobre el hotel h. 3. EVIDENCIA DE COMENTARIOS TEXTUALES Los comentarios textuales gratuitos asociados a las reseñas en línea son una fuente valiosa de información para comprender las razones detrás de las calificaciones numéricas dejadas por los revisores. El texto puede, por ejemplo, revelar ejemplos concretos de aspectos que al usuario le gustaron o no le gustaron, justificando así algunas de las calificaciones altas o bajas respectivamente para ciertas características. El texto también puede ofrecer pautas para comprender las preferencias del revisor y los pesos de las diferentes características al calcular una calificación general. El problema, sin embargo, es que los comentarios textuales libres son difíciles de leer. Los usuarios deben desplazarse a través de muchas reseñas y leer principalmente información repetitiva. Se obtendrían mejoras significativas si las reseñas fueran interpretadas y agregadas automáticamente. Desafortunadamente, esta parece ser una tarea difícil para las computadoras ya que los usuarios humanos a menudo utilizan un lenguaje ingenioso, abreviaturas, frases específicas de la cultura y un estilo figurativo. Sin embargo, varios resultados importantes utilizan los comentarios textuales de las reseñas en línea de forma automatizada. Utilizando técnicas de lenguaje natural bien establecidas, las reseñas o partes de las reseñas pueden ser clasificadas como tener una orientación semántica positiva o negativa. Pang et al. [2] clasifican las críticas de películas en positivas/negativas entrenando tres clasificadores diferentes (Naive Bayes, Máxima Entropía y SVM) utilizando características de clasificación basadas en unigramas, bigramas o etiquetas de partes del discurso. Dave et al. [4] analizan reseñas de CNet y Amazon, y sorprendentemente demuestran que las características de clasificación basadas en unigramas o bigramas funcionan mejor que los n-gramas de orden superior. Este resultado es desafiado por Cui et al. [3], quienes examinan grandes colecciones de reseñas recopiladas de la web. Muestran que el tamaño del conjunto de datos es importante, y que conjuntos de entrenamiento más grandes permiten a los clasificadores utilizar con éxito características de clasificación más complejas basadas en n-gramas. Hu y Liu [11] también rastrean la web en busca de reseñas de productos e identifican automáticamente los atributos de productos que han sido discutidos por los revisores. Utilizan Wordnet para calcular la orientación semántica de las evaluaciones de productos y resumir las reseñas de usuarios extrayendo evaluaciones positivas y negativas de diferentes características del producto. Popescu y Etzioni [20] analizan un escenario similar, pero utilizan el recuento de resultados de búsqueda en motores de búsqueda para identificar atributos de productos; la orientación semántica se asigna a través de la técnica de etiquetado de relajación. Ghose et al. [7, 8] analizan las reseñas de vendedores del mercado secundario de Amazon para identificar las diferentes dimensiones (por ejemplo, entrega, empaque, atención al cliente, etc.) de la reputación. Analizan el texto y etiquetan la parte de la oración de cada palabra. Los sustantivos, frases nominales y frases verbales frecuentes se identifican como dimensiones de la reputación, mientras que los modificadores correspondientes (es decir, adjetivos y adverbios) se utilizan para derivar puntuaciones numéricas para cada dimensión. La medida de reputación mejorada se correlaciona mejor con la información de precios observada en el mercado. Pavlou y Dimoka [19] analizan las reseñas de eBay y encuentran que los comentarios textuales tienen un impacto importante en las primas de reputación. Nuestro enfoque es similar a los trabajos mencionados anteriormente, en el sentido de que identificamos los aspectos (es decir, las características del hotel) discutidos por los usuarios en las reseñas textuales. Sin embargo, no calculamos la orientación semántica del texto, ni intentamos inferir las calificaciones faltantes. Definimos el peso, wi f, de la característica f ∈ F en el texto Ti asociado con la reseña (ri, Ti), como la fracción de Ti dedicada a discutir aspectos (tanto positivos como negativos) relacionados con la característica f. Proponemos un método elemental para aproximar los valores de estos pesos. Para cada característica, construimos manualmente la lista de palabras Lf que contiene aproximadamente 50 palabras que están más comúnmente asociadas a la característica f. Las palabras iniciales fueron seleccionadas al leer algunas de las reseñas y ver qué palabras coinciden con la discusión de qué características. La lista fue luego ampliada agregando todas las entradas del tesauro que estaban relacionadas con las palabras iniciales. Finalmente, hicimos una lluvia de ideas para encontrar las palabras faltantes que normalmente estarían asociadas con cada una de las características. Que Lf ∩ Ti sea la lista de términos comunes a Lf y Ti. Cada término de Lf se cuenta el número de veces que aparece en Ti, con dos excepciones: • en los casos en que el usuario envía un título para la revisión, consideramos el texto del título agregándolo tres veces al texto de la revisión Ti. La suposición intuitiva es que la opinión del usuario se refleja con mayor fuerza en el título que en el cuerpo de la reseña. Por ejemplo, muchas reseñas son resumidas con precisión por títulos como \"Excelente servicio, ubicación terrible\" o \"Mala relación calidad-precio\"; ciertas palabras que ocurren solo una vez en el texto son contadas múltiples veces si su relevancia para esa característica es particularmente fuerte. Estas eran las palabras raíz para cada característica (por ejemplo, personal es una palabra raíz para la característica Servicio), y tenían un peso de 2 o 3. Cada característica fue asignada hasta 3 palabras raíz, por lo que casi todas las palabras se cuentan solo una vez. La lista de palabras para la característica Habitaciones se proporciona como referencia en el Apéndice A. El peso wi f se calcula como: wi f = |Lf ∩ Ti| f∈F |Lf ∩ Ti| (1) donde |Lf ∩ Ti| es el número de términos comunes entre Lf y Ti. El peso para la característica \"En general\" se estableció en min{ |T i | 5000 , 1} donde |Ti | es el número de caracteres en Ti. Comenzaré diciendo que soy más de la onda de Holiday Inn que de un hotel de lujo. Así que me frustro cuando pago el doble de la tarifa de la habitación y recibo la mitad de las comodidades que obtendría en un Hampton Inn o Holiday Inn. La ubicación era definitivamente el principal activo de este lugar. Estaba a solo unas cuadras de la parada de metro del Centro Hynes y era fácil caminar a algunos buenos restaurantes en la zona de Back Bay. Boylston no está lejos en absoluto. Así que no tuve problemas en renunciar a un coche de alquiler y tomar el metro desde el aeropuerto hasta el hotel y usar el metro para cualquier otro viaje. De lo contrario, te hacen pagar por todo. Y cuando ya has gastado $215 por noche en la habitación, eso resulta frustrante. La habitación en sí estaba bien, más o menos lo que esperaría. El personal también era promedio, ni malo ni excelente. Una vez más, creo que estás pagando por la ubicación y la capacidad de ir caminando a muchos lugares interesantes. Pero creo que la próxima vez me quedaré en Brookline, disfrutaré de más comodidades y usaré más el metro. Las calificaciones numéricas asociadas a esta reseña son rO = 3, rR = 3, rS = 3, rC = 4, rV = 2 para las características de General (O), Habitaciones (R), Servicio (S), Limpieza (C) y Valor (V) respectivamente. Las calificaciones de las características Comida (F), Ubicación (L) y Ruido (N) están ausentes (es decir, rF = rL = rN = 0). Los pesos wf se calculan a partir de las siguientes listas de términos comunes: LR ∩ T = {habitación}; wR = 0.066 LS ∩ T = {3 * Personal, comodidades}; wS = 0.267 LC ∩ T = ∅; wC = 0 LV ∩ T = {$, tarifa}; wV = 0.133 LF ∩ T = {restaurante}; wF = 0.067 LL ∩ T = {2 * centro, 2 * caminar, 2 * ubicación, área}; wL = 0.467 LN ∩ T = ∅; wN = 0 Las palabras raíz Personal y Centro se triplicaron y duplicaron respectivamente. El peso total de la revisión textual es wO = 0.197. Estos valores explican razonablemente bien los pesos de las diferentes características en la discusión del revisor. Un punto a tener en cuenta es que algunos términos en las listas Lf poseen una orientación semántica inherente. Por ejemplo, la palabra suciedad (perteneciente a la lista LC) se usaría con mayor frecuencia para afirmar la presencia, y no la ausencia de suciedad. Esto es inevitable, pero se tuvo cuidado de asegurar que se utilizaran palabras de ambos extremos del espectro. Por esta razón, algunas listas como LR contienen solo sustantivos de objetos que uno típicamente describiría en una habitación (ver Apéndice A). El objetivo de esta sección es analizar la influencia de los pesos wi f en las calificaciones numéricas ri f. Intuitivamente, los usuarios que pasaron mucho tiempo discutiendo una característica f (es decir, cuando wif es alto) tenían algo que decir sobre su experiencia con respecto a esta característica. Obviamente, la característica f es importante para el usuario i. Dado que las personas tienden a ser más conocedoras en los aspectos que consideran importantes, nuestra hipótesis es que las calificaciones ri f (correspondientes a los pesos altos wi f) constituyen un subconjunto de las calificaciones de expertos para la característica f. La Figura 2 traza la distribución de las tasas r i(h) C con respecto a los pesos w i(h) C para la limpieza de un hotel de Las Vegas, h. Aquí, las calificaciones altas están restringidas a las reseñas que hablan poco sobre la limpieza. Cuando se menciona la limpieza en la discusión, las calificaciones son bajas. Muchos hoteles muestran patrones de calificación similares para varias características. Las calificaciones correspondientes a pesos bajos abarcan todo el espectro del 1 al 5, mientras que las calificaciones correspondientes a pesos altos están más agrupadas (ya sea alrededor de calificaciones buenas o malas). Por lo tanto, formulamos la siguiente hipótesis: Hipótesis 1. Las calificaciones ri f correspondientes a las reseñas donde wi f es alta, son más similares entre sí que a la colección general de calificaciones. Para probar la hipótesis, tomamos todo el conjunto de reseñas y, característica por característica, calculamos la desviación estándar de las calificaciones con alto peso, y la desviación estándar de todo el conjunto de calificaciones. Los pesos altos se definieron como aquellos que pertenecen al 20% superior del rango de pesos para la característica correspondiente. Si la Hipótesis 1 fuera cierta, la desviación estándar de todas las calificaciones debería ser mayor que la desviación estándar de las calificaciones con pesos altos. 0 1 2 3 4 5 6 0 0.1 0.2 0.3 0.4 0.5 0.6 Calificación Peso Figura 2: La distribución de las calificaciones en función del peso de la característica de limpieza. Utilizamos una prueba T estándar para medir la significancia de los resultados. Ciudad por ciudad y característica por característica, la Tabla 2 presenta la desviación estándar promedio de todas las calificaciones, y la desviación estándar promedio de las calificaciones con pesos altos. De hecho, las calificaciones con pesos altos tienen una desviación estándar más baja, y los resultados son significativos en el umbral estándar de significancia de 0.05 (aunque para ciertas ciudades tomadas de forma independiente no parece haber una diferencia significativa, los resultados son significativos para todo el conjunto de datos). Por favor, tenga en cuenta que solo se consideraron las características O, R, S, C y V, ya que para las otras (F, L y N) no teníamos suficientes calificaciones. Tabla 2: Desviación estándar promedio para todas las calificaciones, y desviación estándar promedio para las calificaciones con pesos altos. En corchetes, los valores p correspondientes para una diferencia positiva entre los dos. La hipótesis 1 no solo proporciona una comprensión básica sobre el comportamiento de calificación de los usuarios en línea, sino que también sugiere algunas formas de calcular estimaciones de mejor calidad. Podemos, por ejemplo, construir una estimación de calidad característica por característica con una varianza mucho menor: para cada característica tomamos el subconjunto de reseñas que discuten ampliamente esa característica, y como estimación de calidad, obtenemos la calificación promedio de este subconjunto. Los experimentos iniciales sugieren que las calificaciones promedio de característica a característica calculadas de esta manera son diferentes de las calificaciones promedio calculadas en todo el conjunto de datos. Dado que, de hecho, los pesos altos son indicadores de opiniones expertas, las estimaciones obtenidas de esta manera son más precisas que las actuales. Sin embargo, la validación de esta suposición subyacente requiere más experimentos controlados. LA INFLUENCIA DE LAS CALIFICACIONES PASADAS Por lo general, se hacen dos suposiciones importantes sobre las reseñas enviadas a foros en línea. La primera es que las calificaciones reflejen fielmente la calidad observada por los usuarios; la segunda es que las reseñas sean independientes entre sí. Si bien la evidencia anecdótica [9, 22] cuestiona la primera suposición, en esta sección abordamos la segunda. Un examen de las reseñas en línea muestra que estas suelen formar parte de hilos de discusión, donde los usuarios se esfuerzan por contradecir o estar vehementemente de acuerdo con los comentarios de usuarios anteriores. Considera, por ejemplo, la siguiente reseña: No entiendo las críticas negativas... el hotel estaba un poco oscuro, pero ese era el estilo. Fue muy artístico. Sí, estaba cerca de la autopista, pero en mi opinión, el sonido de un coche ruidoso ocasional es mejor que escuchar el ding ding de las máquinas tragamonedas toda la noche. El personal disponible es FABULOSO. Las camareras son geniales (y *** no merece la mala crítica que recibió, ¡fue 100% atenta con nosotros!), los camareros son amigables y profesionales al mismo tiempo... Aquí, el usuario se vio perturbado por informes negativos anteriores, abordó estas preocupaciones y se dispuso a intentar corregirlas. Como era de esperar, sus calificaciones fueron considerablemente más altas que las calificaciones promedio hasta este punto. Parece que los usuarios de TripAdvisor suelen leer regularmente los informes enviados por usuarios anteriores antes de reservar un hotel, o antes de escribir una reseña. Las reseñas anteriores crean ciertas expectativas previas sobre la calidad del servicio, y estas expectativas influyen en la reseña presentada. Creemos que esta observación es válida para la mayoría de los foros en línea. La percepción subjetiva de la calidad es directamente proporcional a qué tan bien la experiencia real cumple con la expectativa previa, un hecho confirmado por una importante línea de investigación econométrica y de marketing [17, 18, 16, 21]. La <br>correlación</br> entre las reseñas también ha sido confirmada por investigaciones recientes sobre la dinámica de los foros de reseñas en línea [6]. 4.1 Expectativas Previas Definimos la expectativa previa del usuario i con respecto a la característica f, como el promedio de las calificaciones previamente disponibles en la característica f4: ef (i) = j<i,r j f =0 rj f j<i,r j f =0 1 Como primera hipótesis, afirmamos que la calificación ri f es una función de la expectativa previa ef (i): Hipótesis 2. Para un hotel y una característica dados, dados los comentarios i y j tales que ef (i) es alto y ef (j) es bajo, la calificación rj f supera la calificación ri f. Definimos las expectativas altas y bajas como aquellas que están por encima, respectivamente por debajo de un cierto valor umbral θ. El conjunto de reseñas precedidas por altas, respectivamente bajas expectativas 3 parte de las reseñas de Amazon fueron reconocidas como publicaciones estratégicas por autores de libros o competidores 4 si no se asignaron calificaciones previas para la característica f, ef (i) se asigna un valor predeterminado de 4. Tabla 3: Calificaciones promedio para reseñas precedidas por expectativas bajas (primer valor en la celda) y altas (segundo valor en la celda). Los valores P para una diferencia positiva se dan entre corchetes. Las ciudades O R S C V 3.953 4.045 3.985 4.252 3.946 Boston 3.364 3.590 3.485 3.641 3.242 [0.011] [0.028] [0.0086] [0.0168] [0.0034] 4.284 4.358 4.064 4.530 4.428 Sídney 3.756 3.537 3.436 3.918 3.495 [0.000] [0.000] [0.035] [0.009] [0.000] 3.494 3.674 3.713 3.689 3.580 Las Vegas 3.140 3.530 2.952 3.530 3.351 [0.190] [0.529] [0.007] [0.529] [0.253] se definen de la siguiente manera: Ralto f = {ri f |ef (i) > θ} Rbajo f = {ri f |ef (i) < θ} Estos conjuntos son específicos para cada par (hotel, característica), y en nuestros experimentos tomamos θ = 4. Este valor bastante alto está cerca del promedio de calificación de todas las características de todos los hoteles, y se justifica por el hecho de que nuestro conjunto de datos contiene principalmente hoteles de alta calidad. Para cada ciudad, tomamos todos los hoteles y calculamos las calificaciones promedio en los conjuntos Rhigh f y Rlow f (ver Tabla 3). El promedio de calificación entre las reseñas que siguen a expectativas previas bajas es significativamente mayor que el promedio de calificación después de altas expectativas. Como evidencia adicional, consideramos todos los hoteles para los cuales la función eV (i) (la expectativa para la característica Valor) tiene un valor alto (mayor que 4) para algunos i, y un valor bajo (menor que 4) para algunos otros i. Intuitivamente, estos son los hoteles para los cuales hay un grado mínimo de variación en la secuencia oportuna de reseñas: es decir, el promedio acumulativo de calificaciones fue en algún momento alto y luego se volvió bajo, o viceversa. Tales variaciones se observan en aproximadamente la mitad de todos los hoteles en cada ciudad. La Figura 3 traza la mediana (entre los hoteles considerados) de la calificación, rV, cuando ef (i) no es más que x pero mayor que x - 0.5. 2.5 3 3.5 4 4.5 5 2.5 3 3.5 4 4.5 5 Expectativa de la mediana de calificación Boston Sydney Vegas Figura 3: Las calificaciones tienden a disminuir a medida que aumenta la expectativa. 138 Hay dos formas de interpretar la función ef (i): • El valor esperado para la característica f obtenido por el usuario i antes de su experiencia con el servicio, adquirido al leer informes presentados por usuarios anteriores. En este caso, un valor excesivamente alto para ef (i) llevaría al usuario a enviar un informe negativo (o viceversa), derivado de la diferencia entre el valor real del servicio y la expectativa inflada de este valor adquirida antes de su experiencia. • El valor esperado de la característica f para todos los visitantes posteriores del sitio, si el usuario i no enviara un informe. En este caso, la motivación para un informe negativo después de un valor excesivamente alto de ef es diferente: el usuario i busca corregir la expectativa de los futuros visitantes del sitio. A diferencia de la interpretación anterior, esto no requiere que el usuario derive una expectativa a priori para el valor de f. Tenga en cuenta que ninguna de las interpretaciones implica que el promedio hasta el informe i esté inversamente relacionado con la calificación en el informe i. Podría existir una medida de influencia ejercida por informes anteriores que impulse al usuario detrás del informe i a enviar calificaciones que, en cierta medida, se ajusten a los informes anteriores: un valor bajo para ef (i) puede influir en el usuario i a enviar una calificación baja para la característica f porque, por ejemplo, teme que enviar una calificación alta lo haga parecer una persona con bajos estándares. Esto, al principio, parece contradecir la Hipótesis 2. Sin embargo, esta calificación de conformidad no puede continuar indefinidamente: una vez que el conjunto de informes proyecte una estimación suficientemente reducida para vf, los revisores futuros con impresiones comparativamente positivas buscarán corregir esta percepción errónea. 4.2 Impacto de los comentarios textuales en la expectativa de calidad. Se puede obtener una mayor comprensión del comportamiento de calificación de los usuarios de TripAdvisor analizando la relación entre los pesos wf y los valores ef (i). En particular, examinamos la siguiente hipótesis: Hipótesis 3. Cuando una gran proporción del texto de una reseña discute una característica en particular, la diferencia entre la calificación para esa característica y la calificación promedio hasta ese momento tiende a ser grande. La intuición detrás de esta afirmación es que cuando el usuario insiste en expresar su opinión sobre una característica en particular, su opinión difiere de la opinión colectiva de publicaciones anteriores. Esto se basa en la característica de los sistemas de reputación como foros de retroalimentación donde un usuario está interesado en proyectar su opinión, con particular fuerza si esta opinión difiere de lo que percibe como la opinión general. Para probar la Hipótesis 3 medimos la diferencia absoluta promedio entre la expectativa ef (i) y la calificación ri f cuando el peso wi f es alto, respectivamente bajo. Los pesos se clasifican como altos o bajos al compararlos con ciertos valores de corte: wi f es bajo si es menor que 0.1, mientras que wi f es alto si es mayor que θf. Se utilizaron diferentes valores de corte para diferentes características: θR = 0.4, θS = 0.4, θC = 0.2 y θV = 0.7. La limpieza tiene un límite inferior más bajo ya que es una característica raramente discutida; el valor tiene un límite superior alto por la razón opuesta. Los resultados se presentan en la Tabla 4. La idea de que los informes negativos pueden fomentar una mayor divulgación negativa ha sido sugerida anteriormente [14]. Tabla 4: Promedio de |ri f −ef (i)| cuando los pesos son altos (primer valor en la celda) y bajos (segundo valor en la celda) con valores P para la diferencia en corchetes cuadrados. Esto demuestra que cuando los pesos son inusualmente altos, los usuarios tienden a expresar una opinión que no se ajusta al promedio neto de calificaciones anteriores. Como podríamos esperar, para una característica que rara vez tuvo un peso importante en la discusión (por ejemplo, la limpieza), la diferencia es particularmente grande. Aunque la diferencia en el valor de la característica es bastante grande para Sídney, el valor P es alto. Esto se debe a que solo unos pocos comentarios discutieron en gran medida el valor. La razón podría ser cultural o porque había menos motivo para discutir esta característica. 4.3 Incentivos para informar Los modelos anteriores sugieren que los usuarios poco opinativos no elegirán expresar sus opiniones [12]. En esta sección, ampliamos este modelo para tener en cuenta la influencia de las expectativas. La motivación para enviar comentarios no se debe solo a opiniones extremas, sino también a la diferencia entre la reputación actual (es decir, la expectativa previa del usuario) y la experiencia real. Un modelo de calificación así produce calificaciones que la mayoría de las veces se desvían de la calificación promedio actual. Las calificaciones que confirman la expectativa previa rara vez serán enviadas. Probamos en nuestro conjunto de datos la proporción de calificaciones que intentan corregir la estimación actual. Definimos una calificación desviada como aquella que se desvía de la expectativa actual por al menos un umbral θ, es decir, |ri f − ef (i)| ≥ θ. Para cada una de las tres ciudades consideradas, las siguientes tablas muestran la proporción de calificaciones desviadas para θ = 0.5 y θ = 1. Los resultados anteriores sugieren que una gran proporción de usuarios (casi la mitad, incluso para el valor umbral alto θ = 1) se desvían del promedio previo. Esto refuerza la idea de que los usuarios son más propensos a enviar un informe cuando creen que tienen algo distintivo que agregar al flujo actual de opiniones sobre alguna característica. Tales conclusiones están en total acuerdo con evidencia previa de que la distribución de informes a menudo sigue distribuciones bimodales en forma de U. 139 5. Para dar cuenta de las observaciones descritas en las secciones anteriores, proponemos un modelo para el comportamiento de los usuarios al enviar reseñas en línea. Para un hotel dado, asumimos que la calidad experimentada por los usuarios sigue una distribución normal alrededor de algún valor vf, que representa la calidad objetiva ofrecida por el hotel en la característica f. La calificación enviada por el usuario i en la característica f es: ˆri f = δf vi f + (1 − δf) · sign vi f − ef (i) c + d(vi f, ef (i)|wi f) (2) donde: • vi f es la calidad (desconocida) experimentada realmente por el usuario. Se asume que vi f sigue una distribución normal alrededor de algún valor vf; • δf ∈ [0, 1] puede verse como una medida del sesgo al reportar retroalimentación. Los valores altos reflejan el hecho de que los usuarios califican de manera objetiva, sin ser influenciados por expectativas previas. El valor de δf puede depender de varios factores; fijamos un valor para cada característica f; • c es una constante entre 1 y 5; • wi f es el peso de la característica f en el comentario textual de la reseña i, calculado según la Ecuación (1); • d(vi f , ef (i)|wi f ) es una función de distancia entre la expectativa y la observación del usuario i. La función de distancia satisface las siguientes propiedades: - d(y, z|w) ≥ 0 para todo y, z ∈ [0, 5], w ∈ [0, 1]; - |d(y, z|w)| < |d(z, x|w)| si |y − z| < |z − x|; - |d(y, z|w1)| < |d(y, z|w2)| si w1 < w2; - c + d(vf , ef (i)|wi f ) ∈ [1, 5]; El segundo término de la Ecuación (2) codifica el sesgo de la calificación. Cuanto mayor sea la distancia entre la observación real vi f y la función ef, mayor será el sesgo. 5.1 Validación del modelo Utilizamos el conjunto de datos de las reseñas de TripAdvisor para validar el modelo de comportamiento presentado anteriormente. Dividimos por conveniencia los valores de calificación en tres rangos: malo (B = {1, 2}), indiferente (I = {3, 4}), y bueno (G = {5}), y realizamos los siguientes dos tests: • Primero, usaremos nuestro modelo para predecir las calificaciones que tienen valores extremos. Para cada hotel, tomamos la secuencia de informes, y cada vez que nos encontramos con una calificación que sea buena o mala (pero no indiferente) intentamos predecirla utilizando la Ecuación (2) • En segundo lugar, en lugar de predecir el valor de las calificaciones extremas, intentamos clasificarlas como buenas o malas. Para cada hotel tomamos la secuencia de informes, y para cada informe (independientemente de su valor) lo clasificamos como bueno o malo. Sin embargo, para realizar estas pruebas, necesitamos estimar el valor objetivo, vf, que es el promedio de las observaciones de calidad reales, vi f. El algoritmo que estamos utilizando se basa en la intuición de que la cantidad de calificación de conformidad se minimiza. En otras palabras, el valor vf debe ser tal que, tan a menudo como sea posible, las calificaciones malas sigan a expectativas por encima de vf y las calificaciones buenas sigan a expectativas por debajo de vf. Formalmente, definimos los conjuntos: Γ1 = {i|ef (i) < vf y ri f ∈ B}; Γ2 = {i|ef (i) > vf y ri f ∈ G}; que corresponden a irregularidades donde, a pesar de que la expectativa en el punto i es menor que el valor entregado, la calificación es baja, y viceversa. Definimos vf como el valor que minimiza la unión de los dos conjuntos: vf = arg min vf |Γ1 ∪ Γ2| (3) En la Ec. (2) reemplazamos vi f por el valor vf calculado en la Ec. (3), y utilizamos la siguiente función de distancia: d(vf , ef (i)|wi f ) = |vf − ef (i)| vf − ef (i) |vf 2 − ef (i)2 | · (1 + 2wi f ); La constante c ∈ I se estableció en min{max{ef (i), 3}}, 4}. Los valores para δf se fijaron en {0.7, 0.7, 0.8, 0.7, 0.6} para las características {General, Habitaciones, Servicio, Limpieza, Valor} respectivamente. Los pesos se calculan tal como se describe en la Sección 3. Como primer experimento, tomamos los conjuntos de calificaciones extremas {ri f |ri f /∈ I} para cada hotel y característica. Para cada calificación de este tipo, ri f, intentamos estimarla calculando ˆri f usando la Ecuación (2). Comparamos este estimador con el obtenido simplemente promediando las calificaciones de todos los hoteles y características: es decir, ¯rf = j,r j f =0 rj f j,r j f =0 1; la Tabla 7 presenta la relación entre el error cuadrático medio (RMSE) al usar ˆri f y ¯rf para estimar las calificaciones reales. En todos los casos, la estimación producida por nuestro modelo es mejor que el promedio simple. Tabla 7: Promedio de RMSE(ˆrf) RMSE(¯rf) Ciudad O R S C V Boston 0.987 0.849 0.879 0.776 0.913 Sídney 0.927 0.817 0.826 0.720 0.681 Las Vegas 0.952 0.870 0.881 0.947 0.904 Como segundo experimento, intentamos distinguir los conjuntos Bf = {i|ri f ∈ B} y Gf = {i|ri f ∈ G} de calificaciones malas y buenas, respectivamente, en la característica f. Por ejemplo, calculamos el conjunto Bf usando el siguiente clasificador (llamado σ): ri f ∈ Bf (σf (i) = 1) ⇔ ˆri f ≤ 4; Las Tablas 8, 9 y 10 presentan la Precisión (p), Recall (r) y s = 2pr p+r para el clasificador σ, y lo comparan con un clasificador de mayoría ingenuo, τ, τf (i) = 1 ⇔ |Bf | ≥ |Gf |: Vemos que el recall es siempre mayor para σ y la precisión suele ser ligeramente peor. Para la métrica s, σ tiende a agregar un 140. Tabla 8: Precisión (p), Recall (r), s= 2pr p+r mientras se detectan calificaciones bajas para Boston O R S C V p 0.678 0.670 0.573 0.545 0.610 σ r 0.626 0.659 0.619 0.612 0.694 s 0.651 0.665 0.595 0.577 0.609 p 0.684 0.706 0.647 0.611 0.633 τ r 0.597 0.541 0.410 0.383 0.562 s 0.638 0.613 0.502 0.471 0.595 Tabla 9: Precisión (p), Recall (r), s= 2pr p+r mientras se detectan calificaciones bajas para Las Vegas O R S C V p 0.654 0.748 0.592 0.712 0.583 σ r 0.608 0.536 0.791 0.474 0.610 s 0.630 0.624 0.677 0.569 0.596 p 0.685 0.761 0.621 0.748 0.606 τ r 0.542 0.505 0.767 0.445 0.441 s 0.605 0.607 0.670 0.558 0.511 Mejora del 1-20% sobre τ, mucho mayor en algunos casos para hoteles en Sídney. Esto se debe probablemente a que las reseñas de Sídney son más positivas que las de las ciudades estadounidenses y los casos en los que el número de reseñas negativas supera al de las positivas son raros. Reemplazar el algoritmo de prueba con uno que juega un 1 con una probabilidad igual a la proporción de malas críticas mejora sus resultados para esta ciudad, pero aún es superado por alrededor del 80%. 6. RESUMEN DE RESULTADOS Y CONCLUSIÓN El objetivo de este artículo es explorar los factores que llevan a un usuario a enviar una calificación particular, en lugar de los incentivos que lo animaron a enviar un informe en primer lugar. Para ello utilizamos dos fuentes adicionales de información además del vector de calificaciones numéricas: primero, examinamos los comentarios textuales que acompañan las reseñas, y segundo, consideramos los informes que han sido previamente enviados por otros usuarios. Usando algoritmos simples de procesamiento de lenguaje natural, pudimos establecer una <br>correlación</br> entre el peso de una característica específica en el comentario textual que acompaña la reseña, y el ruido presente en la calificación numérica. Específicamente, parece que los usuarios que discuten ampliamente una característica en particular tienden a estar de acuerdo en una calificación común. Esta observación permite la construcción de estimadores de calidad característica por característica que tienen una varianza más baja y, con suerte, son menos ruidosos. Sin embargo, se requiere más evidencia para respaldar la intuición de que las calificaciones correspondientes a pesos altos son opiniones de expertos que merecen ser consideradas de mayor prioridad al calcular estimaciones de calidad. En segundo lugar, enfatizamos la dependencia de las calificaciones en informes previos. Los informes anteriores crean una expectativa de calidad que afecta la percepción subjetiva del usuario. Validamos dos hechos sobre las reseñas de hoteles que recopilamos de TripAdvisor: Primero, las calificaciones que siguen expectativas bajas (donde la expectativa se calcula como el promedio de los informes anteriores) probablemente sean más altas que las calificaciones que siguen expectativas altas. Intuitivamente, la percepción de la calidad (y consecuentemente la calificación) depende de qué tan bien la experiencia real del usuario cumple con sus expectativas. Segundo, incluimos evidencia de los comentarios textuales, y encontramos que cuando los usuarios dedican una gran parte del texto a discutir una característica específica, es probable que motiven una calificación divergente (es decir, una calificación que no se ajusta a la expectativa previa). Intuitivamente, esto respalda la hipótesis de que los foros de reseñas actúan como grupos de discusión donde los usuarios están interesados en presentar y motivar su propia opinión. Hemos capturado la evidencia empírica en un modelo de comportamiento que predice las calificaciones enviadas por los usuarios. La calificación final depende, como era de esperar, de la observación real y de la brecha entre la observación y la expectativa. La brecha tiende a tener una influencia mayor cuando una fracción importante del comentario textual está dedicada a discutir una característica específica. El modelo propuesto fue validado con los datos empíricos y proporciona mejores estimaciones de las calificaciones realmente enviadas. Una suposición que hacemos es sobre la existencia de un valor de calidad objetivo vf para la característica f. Esto rara vez es cierto, especialmente a lo largo de largos periodos de tiempo. Otras explicaciones podrían dar cuenta de la <br>correlación</br> de las calificaciones con informes anteriores. Por ejemplo, si ef (i) refleja el valor real de f en un punto en el tiempo, la diferencia en las calificaciones siguiendo expectativas altas y bajas puede ser explicada por modelos de ingresos hoteleros que se maximizan cuando el valor es modificado en consecuencia. Sin embargo, la idea de que la variación en las calificaciones no es principalmente una función de la variación en el valor resulta ser útil. Nuestro enfoque para aproximar este valor objetivo es de ninguna manera perfecto, pero se ajusta perfectamente a la idea detrás del modelo. Una dirección natural para trabajos futuros es examinar aplicaciones concretas de nuestros resultados. Es probable que se obtengan mejoras significativas en las estimaciones de calidad al incorporar toda la evidencia empírica sobre el comportamiento de calificación. No está claro exactamente cómo diferentes factores afectan las decisiones de los usuarios. La respuesta podría depender de la aplicación particular, el contexto y la cultura. 7. REFERENCIAS [1] A. Admati y P. Pfleiderer. Noisytalk.com: Transmitiendo opiniones en un entorno ruidoso. Documento de trabajo 1670R, Universidad de Stanford, 2000. [2] P. B., L. Lee y S. Vaithyanathan. ¿Clasificación de sentimientos con técnicas de aprendizaje automático? En Actas de EMNLP-02, la Conferencia sobre Métodos Empíricos en el Procesamiento del Lenguaje Natural, 2002. [3] H. Cui, V. Mittal y M. Datar. Experimentos comparativos 141 sobre clasificación de sentimientos para reseñas de productos en línea. En Actas de AAAI, 2006. [4] K. Dave, S. Lawrence y D. Pennock. Extracción de opiniones y clasificación semántica de reseñas de productos en la galería de cacahuetes. En Actas de la 12ª Conferencia Internacional sobre la World Wide Web (WWW03), 2003. [5] C. Dellarocas, N. Awad y X. Zhang. Explorando el valor de las calificaciones de productos en línea en la previsión de ingresos: El caso de las películas en movimiento. Documento de trabajo, 2006. [6] C. Forman, A. Ghose y B. Wiesenfeld. Un Examen Multinivel del Impacto de las Identidades Sociales en las Transacciones Económicas en los Mercados Electrónicos. Disponible en SSRN: http://ssrn.com/abstract=918978, julio de 2006. [7] A. Ghose, P. Ipeirotis y A. Sundararajan. Primas de reputación en mercados electrónicos peer-to-peer: análisis de retroalimentación textual y estructura de red. En el Tercer Taller sobre Economía de Sistemas Peer-to-Peer (P2PECON), 2005. [8] A. Ghose, P. Ipeirotis y A. Sundararajan. Las dimensiones de la reputación en los mercados electrónicos. Documento de trabajo CeDER-06-02, Universidad de Nueva York, 2006. [9] A. Harmon. Error de Amazon revela la guerra de los revisores. The New York Times, 14 de febrero de 2004. [10] D. Houser y J. Wooders. Reputación en subastas: teoría y evidencia de eBay. Revista de Estrategia Económica y de Gestión, 15:353-369, 2006. [11] M. Hu y B. Liu. Extracción y resumen de reseñas de clientes. En Actas de la Conferencia Internacional de la ACM SIGKDD sobre Descubrimiento de Conocimiento y Minería de Datos (KDD04), 2004. [12] N. Hu, P. Pavlou y J. Zhang. ¿Pueden las reseñas en línea revelar la verdadera calidad de un producto? En Actas de la Conferencia ACM sobre Comercio Electrónico (EC 06), 2006. [13] K. Kalyanam y S. McIntyre. Retorno de la reputación en el mercado de subastas en línea. Documento de trabajo 02/03-10-WP, Escuela de Negocios Leavey, Universidad de Santa Clara, 2001. [14] L. Khopkar y P. Resnick. Auto-selección, deslizamiento, salvamento, holgazanería y lapidación: los impactos de la retroalimentación negativa en eBay. En Actas de la Conferencia ACM sobre Comercio Electrónico (EC 05), 2005. [15] M. Melnik y J. Alm. ¿Importa la reputación de un vendedor? evidencia de subastas en eBay. Revista de Economía Industrial, 50(3):337-350, 2002. [16] R. Olshavsky y J. Miller. Expectativas del consumidor, rendimiento del producto y calidad percibida del producto. Revista de Investigación de Marketing, 9:19-21, febrero de 1972. [17] A. Parasuraman, V. Zeithaml y L. Berry. Un Modelo Conceptual de Calidad de Servicio y sus Implicaciones para Investigaciones Futuras. Revista de Marketing, 49:41-50, 1985. [18] A. Parasuraman, V. Zeithaml y L. Berry. SERVQUAL: Una escala de múltiples ítems para medir las percepciones de calidad del servicio por parte del consumidor. Revista de Retail, 64:12-40, 1988. [19] P. Pavlou y A. Dimoka. La naturaleza y función de los comentarios de texto de retroalimentación en los mercados en línea: implicaciones para la construcción de confianza, primas de precio y diferenciación de vendedores. Investigación de Sistemas de Información, 17(4):392-414, 2006. [20] A. Popescu y O. Etzioni. Extrayendo características del producto y opiniones de las reseñas. En Actas de la Conferencia de Tecnología del Lenguaje Humano y Conferencia sobre Métodos Empíricos en Procesamiento del Lenguaje Natural, 2005. [21] R. Teas. Expectativas, Evaluación del Rendimiento y Percepciones de Calidad de los Consumidores. Revista de Marketing, 57:18-34, 1993. [22] E. White. Chateando a un cantante en las listas de éxitos pop. El Wall Street Journal, 15 de octubre de 1999. APÉNDICE A. LISTA DE PALABRAS, LR, ASOCIADAS A LA CARACTERÍSTICA HABITACIONES. Todas las palabras sirven como prefijos: habitación, espacio, interior, decoración, ambiente, atmósfera, comodidad, baño, inodoro, cama, edificio, pared, ventana, privado, temperatura, sábana, lino, almohada, caliente, agua, fría, agua, ducha, vestíbulo, muebles, alfombra, aire, acondicionado, colchón, distribución, diseño, espejo, techo, iluminación, lámpara, sofá, silla, cómoda, armario, armario. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "large span of time": {
            "translated_key": "largo período de tiempo",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Understanding User Behavior in Online Feedback Reporting Arjun Talwar Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland arjun@math.stanford.edu Radu Jurca Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland radu.jurca@epfl.ch Boi Faltings Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Artificial Intelligence Lab Lausanne, Switzerland boi.faltings@epfl.ch ABSTRACT Online reviews have become increasingly popular as a way to judge the quality of various products and services.",
                "Previous work has demonstrated that contradictory reporting and underlying user biases make judging the true worth of a service difficult.",
                "In this paper, we investigate underlying factors that influence user behavior when reporting feedback.",
                "We look at two sources of information besides numerical ratings: linguistic evidence from the textual comment accompanying a review, and patterns in the time sequence of reports.",
                "We first show that groups of users who amply discuss a certain feature are more likely to agree on a common rating for that feature.",
                "Second, we show that a users rating partly reflects the difference between true quality and prior expectation of quality as inferred from previous reviews.",
                "Both give us a less noisy way to produce rating estimates and reveal the reasons behind user bias.",
                "Our hypotheses were validated by statistical evidence from hotel reviews on the TripAdvisor website.",
                "Categories and Subject Descriptors J.4 [Social and Behavioral Sciences]: Economics General Terms Economics, Experimentation, Reliability 1.",
                "MOTIVATIONS The spread of the internet has made it possible for online feedback forums (or reputation mechanisms) to become an important channel for Word-of-mouth regarding products, services or other types of commercial interactions.",
                "Numerous empirical studies [10, 15, 13, 5] show that buyers seriously consider online feedback when making purchasing decisions, and are willing to pay reputation premiums for products or services that have a good reputation.",
                "Recent analysis, however, raises important questions regarding the ability of existing forums to reflect the real quality of a product.",
                "In the absence of clear incentives, users with a moderate outlook will not bother to voice their opinions, which leads to an unrepresentative sample of reviews.",
                "For example, [12, 1] show that Amazon1 ratings of books or CDs follow with great probability bi-modal, U-shaped distributions where most of the ratings are either very good, or very bad.",
                "Controlled experiments, on the other hand, reveal opinions on the same items that are normally distributed.",
                "Under these circumstances, using the arithmetic mean to predict quality (as most forums actually do) gives the typical user an estimator with high variance that is often false.",
                "Improving the way we aggregate the information available from online reviews requires a deep understanding of the underlying factors that bias the rating behavior of users.",
                "Hu et al. [12] propose the Brag-and-Moan Model where users rate only if their utility of the product (drawn from a normal distribution) falls outside a median interval.",
                "The authors conclude that the model explains the empirical distribution of reports, and offers insights into smarter ways of estimating the true quality of the product.",
                "In the present paper we extend this line of research, and attempt to explain further facts about the behavior of users when reporting online feedback.",
                "Using actual hotel reviews from the TripAdvisor2 website, we consider two additional sources of information besides the basic numerical ratings submitted by users.",
                "The first is simple linguistic evidence from the textual review that usually accompanies the numerical ratings.",
                "We use text-mining techniques similar to [7] and [3], however, we are only interested in identifying what aspects of the service the user is discussing, without computing the semantic orientation of the text.",
                "We find that users who comment more on the same feature are more likely to agree on a common numerical rating for that particular feature.",
                "Intuitively, lengthy comments reveal the importance of the feature to the user.",
                "Since people tend to be more knowledgeable in the aspects they consider important, users who discuss a given feature in more details might be assumed to have more authority in evaluating that feature.",
                "Second we investigate the relationship between a review 1 http://www.amazon.com 2 http://www.tripadvisor.com/ 134 Figure 1: The TripAdvisor page displaying reviews for a popular Boston hotel.",
                "Name of hotel and advertisements were deliberatively erased. and the reviews that preceded it.",
                "A perusal of online reviews shows that ratings are often part of discussion threads, where one post is not necessarily independent of other posts.",
                "One may see, for example, users who make an effort to contradict, or vehemently agree with, the remarks of previous users.",
                "By analyzing the time sequence of reports, we conclude that past reviews influence the future reports, as they create some prior expectation regarding the quality of service.",
                "The subjective perception of the user is influenced by the gap between the prior expectation and the actual performance of the service [17, 18, 16, 21] which will later reflect in the users rating.",
                "We propose a model that captures the dependence of ratings on prior expectations, and validate it using the empirical data we collected.",
                "Both results can be used to improve the way reputation mechanisms aggregate the information from individual reviews.",
                "Our first result can be used to determine a featureby-feature estimate of quality, where for each feature, a different subset of reviews (i.e., those with lengthy comments of that feature) is considered.",
                "The second leads to an algorithm that outputs a more precise estimate of the real quality. 2.",
                "THE DATA SET We use in this paper real hotel reviews collected from the popular travel site TripAdvisor.",
                "TripAdvisor indexes hotels from cities across the world, along with reviews written by travelers.",
                "Users can search the site by giving the hotels name and location (optional).",
                "The reviews for a given hotel are displayed as a list (ordered from the most recent to the oldest), with 5 reviews per page.",
                "The reviews contain: • information about the author of the review (e.g., dates of stay, username of the reviewer, location of the reviewer); • the overall rating (from 1, lowest, to 5, highest); • a textual review containing a title for the review, free comments, and the main things the reviewer liked and disliked; • numerical ratings (from 1, lowest, to 5, highest) for different features (e.g., cleanliness, service, location, etc.)",
                "Below the name of the hotel, TripAdvisor displays the address of the hotel, general information (number of rooms, number of stars, short description, etc), the average overall rating, the TripAdvisor ranking, and an average rating for each feature.",
                "Figure 1 shows the page for a popular Boston hotel whose name (along with advertisements) was explicitly erased.",
                "We selected three cities for this study: Boston, Sydney and Las Vegas.",
                "For each city we considered all hotels that had at least 10 reviews, and recorded all reviews.",
                "Table 1 presents the number of hotels considered in each city, the total number of reviews recorded for each city, and the distribution of hotels with respect to the star-rating (as available on the TripAdvisor site).",
                "Note that not all hotels have a star-rating.",
                "Table 1: A summary of the data set.",
                "City # Reviews # Hotels # of Hotels with 1,2,3,4 & 5 stars Boston 3993 58 1+3+17+15+2 Sydney 1371 47 0+0+9+13+10 Las Vegas 5593 40 0+3+10+9+6 For each review we recorded the overall rating, the textual review (title and body of the review) and the numerical rating on 7 features: Rooms(R), Service(S), Cleanliness(C), Value(V), Food(F), Location(L) and Noise(N).",
                "TripAdvisor does not require users to submit anything other than the overall rating, hence a typical review rates few additional features, regardless of the discussion in the textual comment.",
                "Only the features Rooms(R), Service(S), Cleanliness(C) and Value(V) are rated by a significant number of users.",
                "However, we also selected the features Food(F), Location(L) and Noise(N) because they are referred to in a significant number of textual comments.",
                "For each feature we record the numerical rating given by the user, or 0 when the rating is missing.",
                "The typical length of the textual comment amounts to approximately 200 words.",
                "All data was collected by crawling the TripAdvisor site in September 2006. 2.1 Formal notation We will formally refer to a review by a tuple (r, T) where: • r = (rf ) is a vector containing the ratings rf ∈ {0, 1, . . . 5} for the features f ∈ F = {O, R, S, C, V, F, L, N}; note that the overall rating, rO, is abusively recorded as the rating for the feature Overall(O); • T is the textual comment that accompanies the review. 135 Reviews are indexed according to the variable i, such that (ri , Ti ) is the ith review in our database.",
                "Since we dont record the username of the reviewer, we will also say that the ith review in our data set was submitted by user i.",
                "When we need to consider only the reviews of a given hotel, h, we will use (ri(h) , Ti(h) ) to denote the ith review about the hotel h. 3.",
                "EVIDENCE FROM TEXTUAL COMMENTS The free textual comments associated to online reviews are a valuable source of information for understanding the reasons behind the numerical ratings left by the reviewers.",
                "The text may, for example, reveal concrete examples of aspects that the user liked or disliked, thus justifying some of the high, respectively low ratings for certain features.",
                "The text may also offer guidelines for understanding the preferences of the reviewer, and the weights of different features when computing an overall rating.",
                "The problem, however, is that free textual comments are difficult to read.",
                "Users are required to scroll through many reviews and read mostly repetitive information.",
                "Significant improvements would be obtained if the reviews were automatically interpreted and aggregated.",
                "Unfortunately, this seems a difficult task for computers since human users often use witty language, abbreviations, cultural specific phrases, and the figurative style.",
                "Nevertheless, several important results use the textual comments of online reviews in an automated way.",
                "Using well established natural language techniques, reviews or parts of reviews can be classified as having a positive or negative semantic orientation.",
                "Pang et al. [2] classify movie reviews into positive/negative by training three different classifiers (Naive Bayes, Maximum Entropy and SVM) using classification features based on unigrams, bigrams or part-of-speech tags.",
                "Dave et al. [4] analyze reviews from CNet and Amazon, and surprisingly show that classification features based on unigrams or bigrams perform better than higher-order n-grams.",
                "This result is challenged by Cui et al. [3] who look at large collections of reviews crawled from the web.",
                "They show that the size of the data set is important, and that bigger training sets allow classifiers to successfully use more complex classification features based on n-grams.",
                "Hu and Liu [11] also crawl the web for product reviews and automatically identify product attributes that have been discussed by reviewers.",
                "They use Wordnet to compute the semantic orientation of product evaluations and summarize user reviews by extracting positive and negative evaluations of different product features.",
                "Popescu and Etzioni [20] analyze a similar setting, but use search engine hit-counts to identify product attributes; the semantic orientation is assigned through the relaxation labeling technique.",
                "Ghose et al. [7, 8] analyze seller reviews from the Amazon secondary market to identify the different dimensions (e.g., delivery, packaging, customer support, etc.) of reputation.",
                "They parse the text, and tag the part-of-speech for each word.",
                "Frequent nouns, noun phrases and verbal phrases are identified as dimensions of reputation, while the corresponding modifiers (i.e., adjectives and adverbs) are used to derive numerical scores for each dimension.",
                "The enhanced reputation measure correlates better with the pricing information observed in the market.",
                "Pavlou and Dimoka [19] analyze eBay reviews and find that textual comments have an important impact on reputation premiums.",
                "Our approach is similar to the previously mentioned works, in the sense that we identify the aspects (i.e., hotel features) discussed by the users in the textual reviews.",
                "However, we do not compute the semantic orientation of the text, nor attempt to infer missing ratings.",
                "We define the weight, wi f , of feature f ∈ F in the text Ti associated with the review (ri , Ti ), as the fraction of Ti dedicated to discussing aspects (both positive and negative) related to feature f. We propose an elementary method to approximate the values of these weights.",
                "For each feature we manually construct the word list Lf containing approximately 50 words that are most commonly associated to the feature f. The initial words were selected from reading some of the reviews, and seeing what words coincide with discussion of which features.",
                "The list was then extended by adding all thesaurus entries that were related to the initial words.",
                "Finally, we brainstormed for missing words that would normally be associated with each of the features.",
                "Let Lf ∩Ti be the list of terms common to both Lf and Ti.",
                "Each term of Lf is counted the number of times it appears in Ti , with two exceptions: • in cases where the user submits a title to the review, we account for the title text by appending it three times to the review text Ti .",
                "The intuitive assumption is that the users opinion is more strongly reflected in the title, rather than in the body of the review.",
                "For example, many reviews are accurately summarized by titles such as Excellent service, terrible location or Bad value for money; • certain words that occur only once in the text are counted multiple times if their relevance to that feature is particularly strong.",
                "These were root words for each feature (e.g., staff is a root word for the feature Service), and were weighted either 2 or 3.",
                "Each feature was assigned up to 3 such root words, so almost all words are counted only once.",
                "The list of words for the feature Rooms is given for reference in Appendix A.",
                "The weight wi f is computed as: wi f = |Lf ∩ Ti| f∈F |Lf ∩ Ti| (1) where |Lf ∩Ti | is the number of terms common to Lf and Ti .",
                "The weight for the feature Overall was set to min{ |T i | 5000 , 1} where |Ti | is the number of character in Ti .",
                "The following is a TripAdvisor review for a Boston hotel (the name of the hotel is omitted): Ill start by saying that Im more of a Holiday Inn person than a *** type.",
                "So I get frustrated when I pay double the room rate and get half the amenities that Id get at a Hampton Inn or Holiday Inn.",
                "The location was definitely the main asset of this place.",
                "It was only a few blocks from the Hynes Center subway stop and it was easy to walk to some good restaurants in the Back Bay area.",
                "Boylston isnt far off at all.",
                "So I had no trouble with foregoing a rental car and taking the subway from the airport to the hotel and using the subway for any other travel.",
                "Otherwise, they make you pay for anything and everything. 136 And when youve already dropped $215/night on the room, that gets frustrating.The room itself was decent, about what I would expect.",
                "Staff was also average, not bad and not excellent.",
                "Again, I think youre paying for location and the ability to walk to a lot of good stuff.",
                "But I think next time Ill stay in Brookline, get more amenities, and use the subway a bit more.",
                "This numerical ratings associated to this review are rO = 3, rR = 3, rS = 3, rC = 4, rV = 2 for features Overall(O), Rooms(R), Service(S), Cleanliness(C) and Value(V) respectively.",
                "The ratings for the features Food(F), Location(L) and Noise(N) are absent (i.e., rF = rL = rN = 0).",
                "The weights wf are computed from the following lists of common terms: LR ∩ T ={room}; wR = 0.066 LS ∩ T ={3 * Staff, amenities}; wS = 0.267 LC ∩ T = ∅; wC = 0 LV ∩ T ={$, rate}; wV = 0.133 LF ∩ T ={restaurant}; wF = 0.067 LL ∩ T ={2 * center, 2 * walk, 2 * location, area}; wL = 0.467 LN ∩ T = ∅; wN = 0 The root words Staff and Center were tripled and doubled respectively.",
                "The overall weight of the textual review is wO = 0.197.",
                "These values account reasonably well for the weights of different features in the discussion of the reviewer.",
                "One point to note is that some terms in the lists Lf possess an inherent semantic orientation.",
                "For example the word grime (belonging to the list LC ) would be used most often to assert the presence, and not the absence of grime.",
                "This is unavoidable, but care was taken to ensure words from both sides of the spectrum were used.",
                "For this reason, some lists such as LR contain only nouns of objects that one would typically describe in a room (see Appendix A).",
                "The goal of this section is to analyse the influence of the weights wi f on the numerical ratings ri f .",
                "Intuitively, users who spent a lot of their time discussing a feature f (i.e., wi f is high) had something to say about their experience with regard to this feature.",
                "Obviously, feature f is important for user i.",
                "Since people tend to be more knowledgeable in the aspects they consider important, our hypothesis is that the ratings ri f (corresponding to high weights wi f ) constitute a subset of expert ratings for feature f. Figure 2 plots the distribution of the rates r i(h) C with respect to the weights w i(h) C for the cleanliness of a Las Vegas hotel, h. Here, the high ratings are restricted to the reviews that discuss little the cleanliness.",
                "Whenever cleanliness appears in the discussion, the ratings are low.",
                "Many hotels exhibit similar rating patterns for various features.",
                "Ratings corresponding to low weights span the whole spectrum from 1 to 5, while the ratings corresponding to high weights are more grouped together (either around good or bad ratings).",
                "We therefore make the following hypothesis: Hypothesis 1.",
                "The ratings ri f corresponding to the reviews where wi f is high, are more similar to each other than to the overall collection of ratings.",
                "To test the hypothesis, we take the entire set of reviews, and feature by feature, we compute the standard deviation of the ratings with high weights, and the standard deviation of the entire set of ratings.",
                "High weights were defined as those belonging to the upper 20% of the weight range for the corresponding feature.",
                "If Hypothesis 1 were true, the standard deviation of all ratings should be higher than the standard deviation of the ratings with high weights. 0 1 2 3 4 5 6 0 0.1 0.2 0.3 0.4 0.5 0.6 Rating Weight Figure 2: The distribution of ratings against the weight of the cleanliness feature.",
                "We use a standard T-test to measure the significance of the results.",
                "City by city and feature by feature, Table 2 presents the average standard deviation of all ratings, and the average standard deviation of ratings with high weights.",
                "Indeed, the ratings with high weights have lower standard deviation, and the results are significant at the standard 0.05 significance threshold (although for certain cities taken independently there doesnt seem to be a significant difference, the results are significant for the entire data set).",
                "Please note that only the features O,R,S,C and V were considered, since for the others (F, L, and N) we didnt have enough ratings.",
                "Table 2: Average standard deviation for all ratings, and average standard deviation for ratings with high weights.",
                "In square brackets, the corresponding p-values for a positive difference between the two.",
                "City O R S C V all 1.189 0.998 1.144 0.935 1.123 Boston high 0.948 0.778 0.954 0.767 0.891 p-val [0.000] [0.004] [0.045] [0.080] [0.009] all 1.040 0.832 1.101 0.847 0.963 Sydney high 0.801 0.618 0.691 0.690 0.798 p-val [0.012] [0.023] [0.000] [0.377] [0.037] all 1.272 1.142 1.184 1.119 1.242 Vegas high 1.072 0.752 1.169 0.907 1.003 p-val [0.0185] [0.001] [0.918] [0.120] [0.126] Hypothesis 1 not only provides some basic understanding regarding the rating behavior of online users, it also suggests some ways of computing better quality estimates.",
                "We can, for example, construct a feature-by-feature quality estimate with much lower variance: for each feature we take the subset of reviews that amply discuss that feature, and output as a quality estimate the average rating for this subset.",
                "Initial experiments suggest that the average feature-by-feature ratings computed in this way are different from the average ratings computed on the whole data set.",
                "Given that, indeed, high weights are indicators of expert opinions, the estimates obtained in this way are more accurate than the current ones.",
                "Nevertheless, the validation of this underlying assumption requires further controlled experiments. 137 4.",
                "THE INFLUENCE OF PAST RATINGS Two important assumptions are generally made about reviews submitted to online forums.",
                "The first is that ratings truthfully reflect the quality observed by the users; the second is that reviews are independent from one another.",
                "While anecdotal evidence [9, 22] challenges the first assumption3 , in this section, we address the second.",
                "A perusal of online reviews shows that reviews are often part of discussion threads, where users make an effort to contradict, or vehemently agree with the remarks of previous users.",
                "Consider, for example, the following review: I dont understand the negative reviews... the hotel was a little dark, but that was the style.",
                "It was very artsy.",
                "Yes it was close to the freeway, but in my opinion the sound of an occasional loud car is better than hearing the ding ding of slot machines all night!",
                "The staff on-hand is FABULOUS.",
                "The waitresses are great (and *** does not deserve the bad review she got, she was 100% attentive to us! ), the bartenders are friendly and professional at the same time...",
                "Here, the user was disturbed by previous negative reports, addressed these concerns, and set about trying to correct them.",
                "Not surprisingly, his ratings were considerably higher than the average ratings up to this point.",
                "It seems that TripAdvisor users regularly read the reports submitted by previous users before booking a hotel, or before writing a review.",
                "Past reviews create some prior expectation regarding the quality of service, and this expectation has an influence on the submitted review.",
                "We believe this observation holds for most online forums.",
                "The subjective perception of quality is directly proportional to how well the actual experience meets the prior expectation, a fact confirmed by an important line of econometric and marketing research [17, 18, 16, 21].",
                "The correlation between the reviews has also been confirmed by recent research on the dynamics of online review forums [6]. 4.1 Prior Expectations We define the prior expectation of user i regarding the feature f, as the average of the previously available ratings on the feature f4 : ef (i) = j<i,r j f =0 rj f j<i,r j f =0 1 As a first hypothesis, we assert that the rating ri f is a function of the prior expectation ef (i): Hypothesis 2.",
                "For a given hotel and feature, given the reviews i and j such that ef (i) is high and ef (j) is low, the rating rj f exceeds the rating ri f .",
                "We define high and low expectations as those that are above, respectively below a certain cutoff value θ.",
                "The set of reviews preceded by high, respectively low expectations 3 part of Amazon reviews were recognized as strategic posts by book authors or competitors 4 if no previous ratings were assigned for feature f, ef (i) is assigned a default value of 4.",
                "Table 3: Average ratings for reviews preceded by low (first value in the cell) and high (second value in the cell) expectations.",
                "The P-values for a positive difference are given square brackets.",
                "City O R S C V 3.953 4.045 3.985 4.252 3.946 Boston 3.364 3.590 3.485 3.641 3.242 [0.011] [0.028] [0.0086] [0.0168] [0.0034] 4.284 4.358 4.064 4.530 4.428 Sydney 3.756 3.537 3.436 3.918 3.495 [0.000] [0.000] [0.035] [0.009] [0.000] 3.494 3.674 3.713 3.689 3.580 Las Vegas 3.140 3.530 2.952 3.530 3.351 [0.190] [0.529] [0.007] [0.529] [0.253] are defined as follows: Rhigh f = {ri f |ef (i) > θ} Rlow f = {ri f |ef (i) < θ} These sets are specific for each (hotel, feature) pair, and in our experiments we took θ = 4.",
                "This rather high value is close to the average rating across all features across all hotels, and is justified by the fact that our data set contains mostly high quality hotels.",
                "For each city, we take all hotels and compute the average ratings in the sets Rhigh f and Rlow f (see Table 3).",
                "The average rating amongst reviews following low prior expectations is significantly higher than the average rating following high expectations.",
                "As further evidence, we consider all hotels for which the function eV (i) (the expectation for the feature Value) has a high value (greater than 4) for some i, and a low value (less than 4) for some other i.",
                "Intuitively, these are the hotels for which there is a minimal degree of variation in the timely sequence of reviews: i.e., the cumulative average of ratings was at some point high and afterwards became low, or vice-versa.",
                "Such variations are observed for about half of all hotels in each city.",
                "Figure 3 plots the median (across considered hotels) rating, rV , when ef (i) is not more than x but greater than x − 0.5. 2.5 3 3.5 4 4.5 5 2.5 3 3.5 4 4.5 5 Medianofrating expectation Boston Sydney Vegas Figure 3: The ratings tend to decrease as the expectation increases. 138 There are two ways to interpret the function ef (i): • The expected value for feature f obtained by user i before his experience with the service, acquired by reading reports submitted by past users.",
                "In this case, an overly high value for ef (i) would drive the user to submit a negative report (or vice versa), stemming from the difference between the actual value of the service, and the inflated expectation of this value acquired before his experience. • The expected value of feature f for all subsequent visitors of the site, if user i were not to submit a report.",
                "In this case, the motivation for a negative report following an overly high value of ef is different: user i seeks to correct the expectation of future visitors to the site.",
                "Unlike the interpretation above, this does not require the user to derive an a priori expectation for the value of f. Note that neither interpretation implies that the average up to report i is inversely related to the rating at report i.",
                "There might exist a measure of influence exerted by past reports that pushes the user behind report i to submit ratings which to some extent conforms with past reports: a low value for ef (i) can influence user i to submit a low rating for feature f because, for example, he fears that submitting a high rating will make him out to be a person with low standards5 .",
                "This, at first, appears to contradict Hypothesis 2.",
                "However, this conformity rating cannot continue indefinitely: once the set of reports project a sufficiently deflated estimate for vf , future reviewers with comparatively positive impressions will seek to correct this misconception. 4.2 Impact of textual comments on quality expectation Further insight into the rating behavior of TripAdvisor users can be obtained by analyzing the relationship between the weights wf and the values ef (i).",
                "In particular, we examine the following hypothesis: Hypothesis 3.",
                "When a large proportion of the text of a review discusses a certain feature, the difference between the rating for that feature and the average rating up to that point tends to be large.",
                "The intuition behind this claim is that when the user is adamant about voicing his opinion regarding a certain feature, his opinion differs from the collective opinion of previous postings.",
                "This relies on the characteristic of reputation systems as feedback forums where a user is interested in projecting his opinion, with particular strength if this opinion differs from what he perceives to be the general opinion.",
                "To test Hypothesis 3 we measure the average absolute difference between the expectation ef (i) and the rating ri f when the weight wi f is high, respectively low.",
                "Weights are classified high or low by comparing them with certain cutoff values: wi f is low if smaller than 0.1, while wi f is high if greater than θf .",
                "Different cutoff values were used for different features: θR = 0.4, θS = 0.4, θC = 0.2, and θV = 0.7.",
                "Cleanliness has a lower cutoff since it is a feature rarely discussed; Value has a high cutoff for the opposite reason.",
                "Results are presented in Table 4. 5 The idea that negative reports can encourage further negative reporting has been suggested before [14] Table 4: Average of |ri f −ef (i)| when weights are high (first value in the cell) and low (second value in the cell) with P-values for the difference in sq. brackets.",
                "City R S C V 1.058 1.208 1.728 1.356 Boston 0.701 0.838 0.760 0.917 [0.022] [0.063] [0.000] [0.218] 1.048 1.351 1.218 1.318 Sydney 0.752 0.759 0.767 0.908 [0.179] [0.009] [0.165] [0.495] 1.184 1.378 1.472 1.642 Las Vegas 0.772 0.834 0.808 1.043 [0.071] [0.020] [0.006] [0.076] This demonstrates that when weights are unusually high, users tend to express an opinion that does not conform to the net average of previous ratings.",
                "As we might expect, for a feature that rarely was a high weight in the discussion, (e.g., cleanliness) the difference is particularly large.",
                "Even though the difference in the feature Value is quite large for Sydney, the P-value is high.",
                "This is because only few reviews discussed value heavily.",
                "The reason could be cultural or because there was less of a reason to discuss this feature. 4.3 Reporting Incentives Previous models suggest that users who are not highly opinionated will not choose to voice their opinions [12].",
                "In this section, we extend this model to account for the influence of expectations.",
                "The motivation for submitting feedback is not only due to extreme opinions, but also to the difference between the current reputation (i.e., the prior expectation of the user) and the actual experience.",
                "Such a rating model produces ratings that most of the time deviate from the current average rating.",
                "The ratings that confirm the prior expectation will rarely be submitted.",
                "We test on our data set the proportion of ratings that attempt to correct the current estimate.",
                "We define a deviant rating as one that deviates from the current expectation by at least some threshold θ, i.e., |ri f − ef (i)| ≥ θ.",
                "For each of the three considered cities, the following tables, show the proportion of deviant ratings for θ = 0.5 and θ = 1.",
                "Table 5: Proportion of deviant ratings with θ = 0.5 City O R S C V Boston 0.696 0.619 0.676 0.604 0.684 Sydney 0.645 0.615 0.672 0.614 0.675 Las Vegas 0.721 0.641 0.694 0.662 0.724 Table 6: Proportion of deviant ratings with θ = 1 City O R S C V Boston 0.420 0.397 0.429 0.317 0.446 Sydney 0.360 0.367 0.442 0.336 0.489 Las Vegas 0.510 0.421 0.483 0.390 0.472 The above results suggest that a large proportion of users (close to one half, even for the high threshold value θ = 1) deviate from the prior average.",
                "This reinforces the idea that users are more likely to submit a report when they believe they have something distinctive to add to the current stream of opinions for some feature.",
                "Such conclusions are in total agreement with prior evidence that the distribution of reports often follows bi-modal, U-shaped distributions. 139 5.",
                "MODELLING THE BEHAVIOR OF RATERS To account for the observations described in the previous sections, we propose a model for the behavior of the users when submitting online reviews.",
                "For a given hotel, we make the assumption that the quality experienced by the users is normally distributed around some value vf , which represents the objective quality offered by the hotel on the feature f. The rating submitted by user i on feature f is: ˆri f = δf vi f + (1 − δf ) · sign vi f − ef (i) c + d(vi f , ef (i)|wi f ) (2) where: • vi f is the (unknown) quality actually experienced by the user. vi f is assumed normally distributed around some value vf ; • δf ∈ [0, 1] can be seen as a measure of the bias when reporting feedback.",
                "High values reflect the fact that users rate objectively, without being influenced by prior expectations.",
                "The value of δf may depend on various factors; we fix one value for each feature f; • c is a constant between 1 and 5; • wi f is the weight of feature f in the textual comment of review i, computed according to Eq. (1); • d(vi f , ef (i)|wi f ) is a distance function between the expectation and the observation of user i.",
                "The distance function satisfies the following properties: - d(y, z|w) ≥ 0 for all y, z ∈ [0, 5], w ∈ [0, 1]; - |d(y, z|w)| < |d(z, x|w)| if |y − z| < |z − x|; - |d(y, z|w1)| < |d(y, z|w2)| if w1 < w2; - c + d(vf , ef (i)|wi f ) ∈ [1, 5]; The second term of Eq. (2) encodes the bias of the rating.",
                "The higher the distance between the true observation vi f and the function ef , the higher the bias. 5.1 Model Validation We use the data set of TripAdvisor reviews to validate the behavior model presented above.",
                "We split for convenience the rating values in three ranges: bad (B = {1, 2}), indifferent (I = {3, 4}), and good (G = {5}), and perform the following two tests: • First, we will use our model to predict the ratings that have extremal values.",
                "For every hotel, we take the sequence of reports, and whenever we encounter a rating that is either good or bad (but not indifferent) we try to predict it using Eq. (2) • Second, instead of predicting the value of extremal ratings, we try to classify them as either good or bad.",
                "For every hotel we take the sequence of reports, and for each report (regardless of it value) we classify it as being good or bad However, to perform these tests, we need to estimate the objective value, vf , that is the average of the true quality observations, vi f .",
                "The algorithm we are using is based on the intuition that the amount of conformity rating is minimized.",
                "In other words, the value vf should be such that as often as possible, bad ratings follow expectations above vf and good ratings follow expectations below vf .",
                "Formally, we define the sets: Γ1 = {i|ef (i) < vf and ri f ∈ B}; Γ2 = {i|ef (i) > vf and ri f ∈ G}; that correspond to irregularities where even though the expectation at point i is lower than the delivered value, the rating is poor, and vice versa.",
                "We define vf as the value that minimize these union of the two sets: vf = arg min vf |Γ1 ∪ Γ2| (3) In Eq. (2) we replace vi f by the value vf computed in Eq. (3), and use the following distance function: d(vf , ef (i)|wi f ) = |vf − ef (i)| vf − ef (i) |vf 2 − ef (i)2 | · (1 + 2wi f ); The constant c ∈ I was set to min{max{ef (i), 3}}, 4}.",
                "The values for δf were fixed at {0.7, 0.7, 0.8, 0.7, 0.6} for the features {Overall, Rooms, Service, Cleanliness, Value} respectively.",
                "The weights are computed as described in Section 3.",
                "As a first experiment, we take the sets of extremal ratings {ri f |ri f /∈ I} for each hotel and feature.",
                "For every such rating, ri f , we try to estimate it by computing ˆri f using Eq. (2).",
                "We compare this estimator with the one obtained by simply averaging the ratings over all hotels and features: i.e., ¯rf = j,r j f =0 rj f j,r j f =0 1 ; Table 7 presents the ratio between the root mean square error (RMSE) when using ˆri f and ¯rf to estimate the actual ratings.",
                "In all cases the estimate produced by our model is better than the simple average.",
                "Table 7: Average of RMSE(ˆrf ) RMSE(¯rf ) City O R S C V Boston 0.987 0.849 0.879 0.776 0.913 Sydney 0.927 0.817 0.826 0.720 0.681 Las Vegas 0.952 0.870 0.881 0.947 0.904 As a second experiment, we try to distinguish the sets Bf = {i|ri f ∈ B} and Gf = {i|ri f ∈ G} of bad, respectively good ratings on the feature f. For example, we compute the set Bf using the following classifier (called σ): ri f ∈ Bf (σf (i) = 1) ⇔ ˆri f ≤ 4; Tables 8, 9 and 10 present the Precision(p), Recall(r) and s = 2pr p+r for classifier σ, and compares it with a naive majority classifier, τ, τf (i) = 1 ⇔ |Bf | ≥ |Gf |: We see that recall is always higher for σ and precision is usually slightly worse.",
                "For the s metric σ tends to add a 140 Table 8: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Boston O R S C V p 0.678 0.670 0.573 0.545 0.610 σ r 0.626 0.659 0.619 0.612 0.694 s 0.651 0.665 0.595 0.577 0.609 p 0.684 0.706 0.647 0.611 0.633 τ r 0.597 0.541 0.410 0.383 0.562 s 0.638 0.613 0.502 0.471 0.595 Table 9: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Las Vegas O R S C V p 0.654 0.748 0.592 0.712 0.583 σ r 0.608 0.536 0.791 0.474 0.610 s 0.630 0.624 0.677 0.569 0.596 p 0.685 0.761 0.621 0.748 0.606 τ r 0.542 0.505 0.767 0.445 0.441 s 0.605 0.607 0.670 0.558 0.511 1-20% improvement over τ, much higher in some cases for hotels in Sydney.",
                "This is likely because Sydney reviews are more positive than those of the American cities and cases where the number of bad reviews exceeded the number of good ones are rare.",
                "Replacing the test algorithm with one that plays a 1 with probability equal to the proportion of bad reviews improves its results for this city, but it is still outperformed by around 80%. 6.",
                "SUMMARY OF RESULTS AND CONCLUSION The goal of this paper is to explore the factors that drive a user to submit a particular rating, rather than the incentives that encouraged him to submit a report in the first place.",
                "For that we use two additional sources of information besides the vector of numerical ratings: first we look at the textual comments that accompany the reviews, and second we consider the reports that have been previously submitted by other users.",
                "Using simple natural language processing algorithms, we were able to establish a correlation between the weight of a certain feature in the textual comment accompanying the review, and the noise present in the numerical rating.",
                "Specifically, it seems that users who discuss amply a certain feature are likely to agree on a common rating.",
                "This observation allows the construction of feature-by-feature estimators of quality that have a lower variance, and are hopefully less noisy.",
                "Nevertheless, further evidence is required to support the intuition that ratings corresponding to high weights are expert opinions that deserve to be given higher priority when computing estimates of quality.",
                "Second, we emphasize the dependence of ratings on previous reports.",
                "Previous reports create an expectation of quality which affects the subjective perception of the user.",
                "We validate two facts about the hotel reviews we collected from TripAdvisor: First, the ratings following low expectations (where the expectation is computed as the average of the previous reports) are likely to be higher than the ratings Table 10: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Sydney O R S C V p 0.650 0.463 0.544 0.550 0.580 σ r 0.234 0.378 0.571 0.169 0.592 s 0.343 0.452 0.557 0.259 0.586 p 0.562 0.615 0.600 0.500 0.600 τ r 0.054 0.098 0.101 0.015 0.175 s 0.098 0.168 0.172 0.030 0.271 following high expectations.",
                "Intuitively, the perception of quality (and consequently the rating) depends on how well the actual experience of the user meets her expectation.",
                "Second, we include evidence from the textual comments, and find that when users devote a large fraction of the text to discussing a certain feature, they are likely to motivate a divergent rating (i.e., a rating that does not conform to the prior expectation).",
                "Intuitively, this supports the hypothesis that review forums act as discussion groups where users are keen on presenting and motivating their own opinion.",
                "We have captured the empirical evidence in a behavior model that predicts the ratings submitted by the users.",
                "The final rating depends, as expected, on the true observation, and on the gap between the observation and the expectation.",
                "The gap tends to have a bigger influence when an important fraction of the textual comment is dedicated to discussing a certain feature.",
                "The proposed model was validated on the empirical data and provides better estimates of the ratings actually submitted.",
                "One assumption that we make is about the existence of an objective quality value vf for the feature f. This is rarely true, especially over large spans of time.",
                "Other explanations might account for the correlation of ratings with past reports.",
                "For example, if ef (i) reflects the true value of f at a point in time, the difference in the ratings following high and low expectations can be explained by hotel revenue models that are maximized when the value is modified accordingly.",
                "However, the idea that variation in ratings is not primarily a function of variation in value turns out to be a useful one.",
                "Our approach to approximate this elusive objective value is by no means perfect, but conforms neatly to the idea behind the model.",
                "A natural direction for future work is to examine concrete applications of our results.",
                "Significant improvements of quality estimates are likely to be obtained by incorporating all empirical evidence about rating behavior.",
                "Exactly how different factors affect the decisions of the users is not clear.",
                "The answer might depend on the particular application, context and culture. 7.",
                "REFERENCES [1] A. Admati and P. Pfleiderer.",
                "Noisytalk.com: Broadcasting opinions in a noisy environment.",
                "Working Paper 1670R, Stanford University, 2000. [2] P. B., L. Lee, and S. Vaithyanathan.",
                "Thumbs up? sentiment classification using machine learning techniques.",
                "In Proceedings of the EMNLP-02, the Conference on Empirical Methods in Natural Language Processing, 2002. [3] H. Cui, V. Mittal, and M. Datar.",
                "Comparative 141 Experiments on Sentiment Classification for Online Product Reviews.",
                "In Proceedings of AAAI, 2006. [4] K. Dave, S. Lawrence, and D. Pennock.",
                "Mining the peanut gallery:opinion extraction and semantic classification of product reviews.",
                "In Proceedings of the 12th International Conference on the World Wide Web (WWW03), 2003. [5] C. Dellarocas, N. Awad, and X. Zhang.",
                "Exploring the Value of Online Product Ratings in Revenue Forecasting: The Case of Motion Pictures.",
                "Working paper, 2006. [6] C. Forman, A. Ghose, and B. Wiesenfeld.",
                "A Multi-Level Examination of the Impact of Social Identities on Economic Transactions in Electronic Markets.",
                "Available at SSRN: http://ssrn.com/abstract=918978, July 2006. [7] A. Ghose, P. Ipeirotis, and A. Sundararajan.",
                "Reputation Premiums in Electronic Peer-to-Peer Markets: Analyzing Textual Feedback and Network Structure.",
                "In Third Workshop on Economics of Peer-to-Peer Systems, (P2PECON), 2005. [8] A. Ghose, P. Ipeirotis, and A. Sundararajan.",
                "The Dimensions of Reputation in electronic Markets.",
                "Working Paper CeDER-06-02, New York University, 2006. [9] A. Harmon.",
                "Amazon Glitch Unmasks War of Reviewers.",
                "The New York Times, February 14, 2004. [10] D. Houser and J. Wooders.",
                "Reputation in Auctions: Theory and Evidence from eBay.",
                "Journal of Economics and Management Strategy, 15:353-369, 2006. [11] M. Hu and B. Liu.",
                "Mining and summarizing customer reviews.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD04), 2004. [12] N. Hu, P. Pavlou, and J. Zhang.",
                "Can Online Reviews Reveal a Products True Quality?",
                "In Proceedings of ACM Conference on Electronic Commerce (EC 06), 2006. [13] K. Kalyanam and S. McIntyre.",
                "Return on reputation in online auction market.",
                "Working Paper 02/03-10-WP, Leavey School of Business, Santa Clara University., 2001. [14] L. Khopkar and P. Resnick.",
                "Self-Selection, Slipping, Salvaging, Slacking, and Stoning: the Impacts of Negative Feedback at eBay.",
                "In Proceedings of ACM Conference on Electronic Commerce (EC 05), 2005. [15] M. Melnik and J. Alm.",
                "Does a sellers reputation matter? evidence from ebay auctions.",
                "Journal of Industrial Economics, 50(3):337-350, 2002. [16] R. Olshavsky and J. Miller.",
                "Consumer Expectations, Product Performance and Perceived Product Quality.",
                "Journal of Marketing Research, 9:19-21, February 1972. [17] A. Parasuraman, V. Zeithaml, and L. Berry.",
                "A Conceptual Model of Service Quality and Its Implications for Future Research.",
                "Journal of Marketing, 49:41-50, 1985. [18] A. Parasuraman, V. Zeithaml, and L. Berry.",
                "SERVQUAL: A Multiple-Item Scale for Measuring Consumer Perceptions of Service Quality.",
                "Journal of Retailing, 64:12-40, 1988. [19] P. Pavlou and A. Dimoka.",
                "The Nature and Role of Feedback Text Comments in Online Marketplaces: Implications for Trust Building, Price Premiums, and Seller Differentiation.",
                "Information Systems Research, 17(4):392-414, 2006. [20] A. Popescu and O. Etzioni.",
                "Extracting product features and opinions from reviews.",
                "In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, 2005. [21] R. Teas.",
                "Expectations, Performance Evaluation, and Consumers Perceptions of Quality.",
                "Journal of Marketing, 57:18-34, 1993. [22] E. White.",
                "Chatting a Singer Up the Pop Charts.",
                "The Wall Street Journal, October 15, 1999.",
                "APPENDIX A.",
                "LIST OF WORDS, LR, ASSOCIATED TO THE FEATURE ROOMS All words serve as prefixes: room, space, interior, decor, ambiance, atmosphere, comfort, bath, toilet, bed, building, wall, window, private, temperature, sheet, linen, pillow, hot, water, cold, water, shower, lobby, furniture, carpet, air, condition, mattress, layout, design, mirror, ceiling, lighting, lamp, sofa, chair, dresser, wardrobe, closet 142"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        }
    }
}