{
    "id": "I-14",
    "original_text": "A Reinforcement Learning based Distributed Search Algorithm For Hierarchical Peer-to-Peer Information Retrieval Systems Haizheng Zhang College of Information Science and Technology Pennsylvania State University University Park, PA 16803 hzhang@ist.psu.edu Victor Lesser Department of Computer Science University Of Massachusetts Amherst, MA 01003 lesser@cs.umass.edu ABSTRACT The dominant existing routing strategies employed in peerto-peer(P2P) based information retrieval(IR) systems are similarity-based approaches. In these approaches, agents depend on the content similarity between incoming queries and their direct neighboring agents to direct the distributed search sessions. However, such a heuristic is myopic in that the neighboring agents may not be connected to more relevant agents. In this paper, an online reinforcement-learning based approach is developed to take advantage of the dynamic run-time characteristics of P2P IR systems as represented by information about past search sessions. Specifically, agents maintain estimates on the downstream agents abilities to provide relevant documents for incoming queries. These estimates are updated gradually by learning from the feedback information returned from previous search sessions. Based on this information, the agents derive corresponding routing policies. Thereafter, these agents route the queries based on the learned policies and update the estimates based on the new routing policies. Experimental results demonstrate that the learning algorithm improves considerably the routing performance on two test collection sets that have been used in a variety of distributed IR studies. Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Algorithms, Performance, Experimentation 1. INTRODUCTION Over the last few years there have been increasing interests in studying how to control the search processes in peer-to-peer(P2P) based information retrieval(IR) systems [6, 13, 14, 15]. In this line of research, one of the core problems that concerns researchers is to efficiently route user queries in the network to agents that are in possession of appropriate documents. In the absence of global information, the dominant strategies in addressing this problem are content-similarity based approaches [6, 13, 14, 15]. While the content similarity between queries and local nodes appears to be a creditable indicator for the number of relevant documents residing on each node, these approaches are limited by a number of factors. First of all, similaritybased metrics can be myopic since locally relevant nodes may not be connected to other relevant nodes. Second, the similarity-based approaches do not take into account the run-time characteristics of the P2P IR systems, including environmental parameters, bandwidth usage, and the historical information of the past search sessions, that provide valuable information for the query routing algorithms. In this paper, we develop a reinforcement learning based IR approach for improving the performance of distributed IR search algorithms. Agents can acquire better search strategies by collecting and analyzing feedback information from previous search sessions. Particularly, agents maintain estimates, namely expected utility, on the downstream agents capabilities of providing relevant documents for specific types of incoming queries. These estimates are updated gradually by learning from the feedback information returned from previous search sessions. Based on the updated expected utility information, the agents derive corresponding routing policies. Thereafter, these agents route the queries based on the learned policies and update the estimates on the expected utility based on the new routing policies. This process is conducted in an iterative manner. The goal of the learning algorithm, even though it consumes some network bandwidth, is to shorten the routing time so that more queries are processed per time unit while at the same time finding more relevant documents. This contrasts with the content-similarity based approaches where similar operations are repeated for every incoming query and the processing time keeps largely constant over time. Another way of viewing this paper is that our basic approach to distributed IR search is to construct a hierarchical overlay network(agent organization) based on the contentsimilarity measure among agents document collections in a bottom-up fashion. In the past work, we have shown that this organization improves search performance significantly. However, this organizational structure does not take into account the arrival patterns of queries, including their frequency, types, and where they enter the system, nor the available communication bandwidth of the network and processing capabilities of individual agents. The intention of the reinforcement learning is to adapt the agents routing decisions to the dynamic network situations and learn from past search sessions. Specifically, the contributions of this paper include: (1) a reinforcement learning based approach for agents to acquire satisfactory routing policies based on estimates of the potential contribution of their neighboring agents; (2) two strategies to speed up the learning process. To our best knowledge, this is one of the first reinforcement learning applications in addressing distributed content sharing problems and it is indicative of some of the issues in applying reinforcement in a complex application. The remainder of this paper is organized as follows: Section 2 reviews the hierarchical content sharing systems and the two-phase search algorithm based on such topology. Section 3 describes a reinforcement learning based approach to direct the routing process; Section 4 details the experimental settings and analyze the results. Section 5 discusses related studies and Section 6 concludes the paper. 2. SEARCH IN HIERARCHICAL P2P IR SYSTEMS This section briefly reviews our basic approaches to hierarchical P2P IR systems. In a hierarchical P2P IR system illustrated in Fig.1, agents are connected to each other through three types of links: upward links, downward links, and lateral links. In the following sections, we denote the set of agents that are directly connected to agent Ai as DirectConn(Ai), which is defined as DirectConn(Ai) = NEI(Ai) ∪ PAR(Ai) ∪ CHL(Ai) , where NEI(Ai) is the set of neighboring agents connected to Ai through lateral links; PAR(Ai) is the set of agents whom agent Ai is connected to through upward links and CHL(Ai) is the set of agents that agent Ai connects to through downward links. These links are established through a bottom-up content-similarity based distributed clustering process[15]. These links are then used by agents to locate other agents that contain documents relevant to the given queries. A typical agent Ai in our system uses two queues: a local search queue, LSi, and a message forwarding queue MFi. The states of the two queues constitute the internal states of an agent. The local search queue LSi stores search sessions that are scheduled for local processing. It is a priority queue and agent Ai always selects the most promising queries to process in order to maximize the global utility. MFi consists of a set of queries to forward on and is processed in a FIFO (first in first out) fashion. For the first query in MFi, agent Ai determines which subset of its neighboring agents to forward it to based on the agents routing policy πi. These routing decisions determine how the search process is conducted in the network. In this paper, we call Ai as Ajs upstream agent and Aj as Ais downstream agent if A4 A5 A6 A7 A2 A3 A9 NEI(A2)={A3} PAR(A2)={A1} CHL(A2)={A4,A5} A1 A8 Figure 1: A fraction of a hierarchical P2PIR system an agent Ai routes a query to agent Aj. The distributed search protocol of our hierarchical agent organization is composed of two steps. In the first step, upon receipt of a query qk at time tl from a user, agent Ai initiates a search session si by probing its neighboring agents Aj ∈ NEI(Ai) with the message PROBE for the similarity value Sim(qk, Aj) between qk and Aj. Here, Ai is defined as the query initiator of search session si. In the second step, Ai selects a group of the most promising agents to start the actual search process with the message SEARCH. These SEARCH messages contain a TTL (Time To Live) parameter in addition to the query. The TTL value decreases by 1 after each hop. In the search process, agents discard those queries that either have been previously processed or whose TTL drops to 0, which prevents queries from looping in the system forever. The search session ends when all the agents that receive the query drop it or TTL decreases to 0. Upon receipt of SEARCH messages for qk, agents schedule local activities including local searching, forwarding qk to their neighbors, and returning search results to the query initiator. This process and related algorithms are detailed in [15, 14]. 3. A BASIC REINFORCEMENTLEARNING BASED SEARCH APPROACH In the aforementioned distributed search algorithm, the routing decisions of an agent Ai rely on the similarity comparison between incoming queries and Ais neighboring agents in order to forward those queries to relevant agents without flooding the network with unnecessary query messages. However, this heuristic is myopic because a relevant direct neighbor is not necessarily connected to other relevant agents. In this section, we propose a more general approach by framing this problem as a reinforcement learning task. In pursuit of greater flexibility, agents can switch between two modes: learning mode and non-learning mode. In the non-learning mode, agents operate in the same way as they do in the normal distributed search processes described in [14, 15]. On the other hand, in the learning mode, in parallel with distributed search sessions, agents also participate in a learning process which will be detailed in this section. Note that in the learning protocol, the learning process does not interfere with the distributed search process. Agents can choose to initiate and stop learning processes without affecting the system performance. In particular, since the learning process consumes network resources (especially bandwidth), agents can choose to initiate learning only when the network load is relatively low, thus minimizing the extra communication costs incurred by the learning algorithm. The section is structured as follows, Section 3.1 describes 232 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) a reinforcement learning based model. Section 3.2 describes a protocol to deploy the learning algorithm in the network. Section 3.3 discusses the convergence of the learning algorithm. 3.1 The Model An agents routing policy takes the state of a search session as input and output the routing actions for that query. In our work, the state of a search session sj is stipulated as: QSj = (qk, ttlj) where ttlj is the number of hops that remains for the search session sj , qk is the specific query. QL is an attribute of qk that indicates which type of queries qk most likely belong to. The set of QL can be generated by running a simple online classification algorithm on all the queries that have been processed by the agents, or an oﬄine algorithm on a pre-designated training set. The assumption here is that the set of query types is learned ahead of time and belongs to the common knowledge of the agents in the network. Future work includes exploring how learning can be accomplished when this assumption does not hold. Given the query types set, an incoming query qi can be classified to one query class Q(qi) by the formula: Q(qi) = arg max Qj P(qi|Qj) (1) where P(qi|Qj ) indicates the likelihood that the query qi is generated by the query class Qj [8]. The set of atomic routing actions of an agent Ai is denoted as {αi}, where {αi} is defined as αi = {αi0 , αi1 , ..., αin }. An element αij represents an action to route a given query to the neighboring agent Aij ∈ DirectConn(Ai). The routing policy πi of agent Ai is stochastic and its outcome for a search session with state QSj is defined as: πi(QSj) = {(αi0 , πi(QSi, αi0 )), (αi1 , πi(QSi, αi1 )), ...} (2) Note that operator πi is overloaded to represent either the probabilistic policy for a search session with state QSj, denoted as πi(QSj); or the probability of forwarding the query to a specific neighboring agent Aik ∈ DirectConn(Ai) under the policy πi(QSj), denoted as πi(QSj, αik ). Therefore, equation (2) means that the probability of forwarding the search session to agent Ai0 is πi(QSi, αi0 ) and so on. Under this stochastic policy, the routing action is nondeterministic. The advantage of such a strategy is that the best neighboring agents will not be selected repeatedly, thereby mitigating the potential hot spots situations. The expected utility, Un i (QSj), is used to estimate the potential utility gain of routing query type QSj to agent Ai under policy πn i . The superscript n indicates the value at the nth iteration in an iterative learning process. The expected utility provides routing guidance for future search sessions. In the search process, each agent Ai maintains partial observations of its neighbors states, as shown in Fig. 2. The partial observation includes non-local information such as the potential utility estimation of its neighbor Am for query state QSj, denoted as Um(QSj), as well as the load information, Lm. These observations are updated periodically by the neighbors. The estimated utility information will be used to update Ais expected utility for its routing policy. Load Information Expected Utility For Different Query Types Neighboring Agents ... A0 A1 A3 A2 Un 0 (QS0) ... ... ... ... ...... Un 0 (QS1) Un 1 (QS1) Un 2 (QS1) Un 3 (QS1) Un 1 (QS0) Un 2 (QS0) Un 3 (QS0) Ln 0 Ln 1 Ln 2 Ln 3 ... QS0 QS1 ... Figure 2: Agent Ais Partial Observation about its neighbors(A0, A1...) The load information of Am, Lm, is defined as Lm = |MFm| Cm , where |MFm| is the length of the message-forward queue and Cm is the service rate of agent Ams message-forward queue. Therefore Lm characterizes the utilization of an agents communication channel, and thus provide non-local information for Ams neighbors to adjust the parameters of their routing policy to avoid inundating their downstream agents. Note that based on the characteristics of the queries entering the system and agents capabilities, the loading of agents may not be uniform. After collecting the utilization rate information from all its neighbors, agent Ai computes Li as a single measure for assessing the average load condition of its neighborhood: Li = P k Lk |DirectConn(Ai)| Agents exploit Li value in determining the routing probability in its routing policy. Note that, as described in Section 3.2, information about neighboring agents is piggybacked with the query message propagated among the agents whenever possible to reduce the traffic overhead. 3.1.1 Update the Policy An iterative update process is introduced for agents to learn a satisfactory stochastic routing policy. In this iterative process, agents update their estimates on the potential utility of their current routing policies and then propagate the updated estimates to their neighbors. Their neighbors then generate a new routing policy based on the updated observation and in turn they calculate the expected utility based on the new policies and continue this iterative process. In particular, at time n, given a set of expected utility, an agent Ai, whose directly connected agents set is DirectConn(Ai) = {Ai0 , ..., Aim }, determines its corresponding stochastic routing policy for a search session of state QSj based on the following steps: (1) Ai first selects a subset of agents as the potential downstream agents from set DirectConn(Ai), denoted as PDn(Ai, QSj). The size of the potential downstream agent is specified as |PDn(Ai, QSj)| = min(|NEI(Ai), dn i + k)| where k is a constant and is set to 3 in this paper; dn i , the forward width, is defined as the expected number of The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 233 neighboring agents that agent Ai can forward to at time n. This formula specifies that the potential downstream agent set PDn(Ai, QSj) is either the subset of neighboring agents with dn i + k highest expected utility value for state QSj among all the agents in DirectConn(Ai), or all their neighboring agents. The k is introduced based on the idea of a stochastic routing policy and it makes the forwarding probability of the dn i +k highest agent less than 100%. Note that if we want to limit the number of downstream agents for search session sj as 5, the probability of forwarding the query to all neighboring agents should add up to 5. Setting up dn i value properly can improve the utilization rate of the network bandwidth when much of the network is idle while mitigating the traffic load when the network is highly loaded. The dn+1 i value is updated based on dn i , the previous and current observations on the traffic situation in the neighborhood. Specifically, the update formula for dn+1 i is dn+1 i = dn i ∗ (1 + 1 − Li |DirectConn(Ai)| ) In this formula, the forward width is updated based on the traffic conditions of agent Ais neighborhood, i.e Li, and its previous value. (2) For each agent Aik in the PDn(Ai, QSj), the probability of forwarding the query to Aik is determined in the following way in order to assign higher forwarding probability to the neighboring agents with higher expected utility value: πn+1 i (QSj, αik ) = dn+1 i |PDn(Ai, QSj)| + β ∗ ` Uik (QSj) − PDU(Ai, QSj) |PDn(Ai, QSj)| ´ (3) where PDUn(Ai, QSj) = X o∈P Dn(Ai,QSj ) Uo(QSj) and QSj is the subsequent state of agent Aik after agent Ai forwards the search session with state QSj to its neighboring agent Aik ; If QSj = (qk, ttl0), then QSj = (qk, ttl0 − 1). In formula 3, the first term on the right of the equation, dn+1 i |P Dn(Ai,QSj )| , is used to to determine the forwarding probability by equally distributing the forward width, dn+1 i , to the agents in PDn(Ai, QSj) set. The second term is used to adjust the probability of being chosen so that agents with higher expected utility values will be favored. β is determined according to: β = min ` m − dn+1 i m ∗ umax − PDUn(Ai, QSj) , dn+1 i PDUn(Ai, QSj) − m ∗ umin ´ (4) where m = |PDn(Ai, QSj)|, umax = max o∈P Dn(Ao,QSj ) Uo(QSj) and umin = min o∈P Dn(Ao,QSj ) Uo(QSj) This formula guarantees that the final πn+1 i (QSj, αik ) value is well defined, i.e, 0 ≤ πn+1 i (QSj, αik ) ≤ 1 and X i πn+1 i (QSj, αik ) = dn+1 i However, such a solution does not explore all the possibilities. In order to balance between exploitation and exploration, a λ-Greedy approach is taken. In the λ-Greedy approach, in addition to assigning higher probability to those agents with higher expected utility value, as in the equation (3). Agents that appear to be not-so-good choices will also be sent queries based on a dynamic exploration rate. In particular, for agents in the set PDn(Ai, QSj), πn+1 i1 (QSj) is determined in the same way as the above, with the only difference being that dn+1 i is replaced with dn+1 i ∗ (1 − λn). The remaining search bandwidth is used for learning by assigning probability λn evenly to agents Ai2 in the set DirectConn(Ai) − PDn(Ai, QSj). πn+1 i2 (QSj, αik ) = dn+1 i ∗ λn |DirectConn(Ai) − PDn(Ai, QSj)| (5) where PDn(Ai, QSj) ⊂ DirectConn(Ai). Note that the exploration rate λ is not a constant and it decreases overtime. The λ is determined according to the following equation: λn+1 = λ0 ∗ e−c1n (6) where λ0 is the initial exploration rate, which is a constant; c1 is also a constant to adjust the decreasing rate of the exploration rate; n is the current time unit. 3.1.2 Update Expected Utility Once the routing policy at step n+1, πn+1 i , is determined based on the above formula, agent Ai can update its own expected utility, Un+1 i (QSi), based on the the updated routing policy resulted from the formula 5 and the updated U values of its neighboring agents. Under the assumption that after a query is forwarded to Ais neighbors the subsequent search sessions are independent, the update formula is similar to the Bellman update formula in Q-Learning: Un+1 i (QSj) = (1 − θi) ∗ Un i (QSj) + θi ∗ (Rn+1 i (QSj) + X k πn+1 i (QSj, αik )Un k (QSj)) (7) where QSj = (Qj, ttl − 1) is the next state of QSj = (Qj, ttl); Rn+1 i (QSj) is the expected local reward for query class Qk at agent Ai under the routing policy πn+1 i ; θi is the coefficient for deciding how much weight is given to the old value during the update process: the smaller θi value is, the faster the agent is expected to learn the real value, while the greater volatility of the algorithm, and vice versa. Rn+1 (s) is updated according to the following equation: 234 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Rn+1 i (QSj) = Rn i (QSj) +γi ∗ (r(QSj) − Rn i (QSj)) ∗ P(qj|Qj ) (8) where r(QSj) is the local reward associated with the search session. P(qj|Qj ) indicates how relevant the query qj is to the query type Qj, and γi is the learning rate for agent Ai. Depending on the similarity between a specific query qi and its corresponding query type Qi, the local reward associated with the search session has different impact on the Rn i (QSj) estimation. In the above formula, this impact is reflected by the coefficient, the P(qj|Qj) value. 3.1.3 Reward function After a search session stops when its TTL values expires, all search results are returned back to the user and are compared against the relevance judgment. Assuming the set of search results is SR, the reward Rew(SR) is defined as: Rew(SR) = j 1 if |Rel(SR)| > c |Rel(SR)| c otherwise. where SR is the set of returned search results, Rel(SR) is the set of relevant documents in the search results. This equation specifies that users give 1.0 reward if the number of returned relevant documents reaches a predefined number c. Otherwise, the reward is in proportion to the number of relevant documents returned. This rationale for setting up such a cut-off value is that the importance of recall ratio decreases with the abundance of relevant documents in real world, therefore users tend to focus on only a limited number of searched results. The details of the actual routing protocol will be introduced in Section 3.2 when we introduce how the learning algorithm is deployed in real systems. 3.2 Deployment of the Learning algorithm This section describes how the learning algorithm can be used in either a single-phase or a two-phase search process. In the single-phase search algorithm, search sessions start from the initiators of the queries. In contrast, in the two-step search algorithm, the query initiator first attempts to seek a more appropriate starting point for the query by introducing an exploratory step as described in Section 2. Despite the difference in the quality of starting points, the major part of the learning process for the two algorithms is largely the same as described in the following paragraphs. Before learning starts, each agent initializes the expected utility value for all possible states as 0. Thereafter, upon receipt of a query, in addition to the normal operations described in the previous section, an agent Ai also sets up a timer to wait for the search results returned from its downstream agents. Once the timer expires or it has received response from all its downstream agents, Ai merges and forwards the search results accrued from its downstream agents to its upstream agent. Setting up the timer speeds up the learning because agents can avoid waiting too long for the downstream agents to return search results. Note that these detailed results and corresponding agent information will still be stored at Ai until the feedback information is passed from its upstream agent and the performance of its downstream agents can be evaluated. The duration of the timer is related to the TTL value. In this paper, we set the timer to ttimer = ttli ∗ 2 + tf , where ttli ∗ 2 is the sum of the travel time of the queries in the network, and tf is the expected time period that users would like to wait. The search results will eventually be returned to the search session initiator A0. They will be compared to the relevance judgment that is provided by the final users (as described in the experiment section, the relevance judgement for the query set is provided along with the data collections). The reward will be calculated and propagated backward to the agents along the way that search results were passed. This is a reverse process of the search results propagation. In the process of propagating reward backward, agents update estimates of their own potential utility value, generate an upto-dated policy and pass their updated results to the neighboring agents based on the algorithm described in Section 3. Upon change of expected utility value, agent Ai sends out its updated utility estimation to its neighbors so that they can act upon the changed expected utility and corresponding state. This update message includes the potential reward as well as the corresponding state QSi = (qk, ttll) of agent Ai. Each neighboring agent, Aj, reacts to this kind of update message by updating the expected utility value for state QSj(qk, ttll + 1) according to the newly-announced changed expected utility value. Once they complete the update, the agents would again in turn inform related neighbors to update their values. This process goes on until the TTL value in the update message increases to the TTL limit. To speed up the learning process, while updating the expected utility values of an agent Ais neighboring agents we specify that Um(Qk, ttl0) >= Um(Qk, ttl1) iff ttl0 > ttl1 Thus, when agent Ai receives an updated expected utility value with ttl1, it also updates the expected utility values with any ttl0 > ttl1 if Um(Qk, ttl0) < Um(Qk, ttl1) to speed up convergence. This heuristic is based on the fact that the utility of a search session is a non-decreasing function of time t. 3.3 Discussion In formalizing the content routing system as a learning task, many assumptions are made. In real systems, these assumptions may not hold, and thus the learning algorithm may not converge. Two problems are of particular note, (1) This content routing problem does not have Markov properties. In contrast to IP-level based packet routing, the routing decision of each agent for a particular search session sj depends on the routing history of sj. Therefore, the assumption that all subsequent search sessions are independent does not hold in reality. This may lead to double counting problem that the relevant documents of some agents will be counted more than once for the state where the TTL value is more than 1. However, in the context of the hierarchical agent organizations, two factors mitigate this problems: first, the agents in each content group form a tree-like structure. With the absense of the cycles, the estimates inside the tree would be close to the accurate value. Secondly, the stochastic nature of the routing policy partly remedies this problem. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 235 (2) Another challenge for this learning algorithm is that in a real network environment observations on neighboring agents may not be able to be updated in time due to the communication delay or other situations. In addition, when neighboring agents update their estimates at the same time, oscillation may arise during the learning process[1]. This paper explores several approaches to speed up the learning process. Besides the aforementioned strategy of updating the expected utility values, we also employ an active update strategy where agents notify their neighbors whenever its expected utility is updated. Thus a faster convergence speed can be achieved. This strategy contrasts to the Lazy update, where agents only echo their neighboring agents with their expected utility change when they exchange information. The trade off between the two approaches is the network load versus learning speed. The advantage of this learning algorithm is that once a routing policy is learned, agents do not have to repeatedly compare the similarity of queries as long as the network topology remains unchanged. Instead, agent just have to determine the classification of the query properly and follow the learned policies. The disadvantage of this learning-based approach is that the learning process needs to be conducted whenever the network structure changes. There are many potential extensions for this learning model. For example, a single measure is currently used to indicate the traffic load for an agents neighborhood. A simple extension would be to keep track of individual load for each neighbor of the agent. 4. EXPERIMENTSSETTINGSAND RESULTS The experiments are conducted on TRANO simulation toolkit with two sets of datasets, TREC-VLC-921 and TREC123-100. The following sub-sections introduce the TRANO testbed, the datasets, and the experimental results. 4.1 TRANO Testbed TRANO (Task Routing on Agent Network Organization) is a multi-agent based network based information retrieval testbed. TRANO is built on top of the Farm [4], a time based distributed simulator that provides a data dissemination framework for large scale distributed agent network based organizations. TRANO supports importation and exportation of agent organization profiles including topological connections and other features. Each TRANO agent is composed of an agent view structure and a control unit. In simulation, each agent is pulsed regularly and the agent checks the incoming message queues, performs local operations and then forwards messages to other agents . 4.2 Experimental Settings In our experiment, we use two standard datasets, TRECVLC-921 and TREC-123-100 datasets, to simulate the collections hosted on agents. The TREC-VLC-921 and TREC123-100 datasets were created by the U.S. National Institute for Standard Technology(NIST) for its TREC conferences. In distributed information retrieval domain, the two data collections are split to 921 and 100 sub-collections. It is observed that dataset TREC-VLC-921 is more heterogeneous than TREC-123-100 in terms of source, document length, and relevant document distribution from the statistics of the two data collections listed in [13]. Hence, TREC-VLC-921 is much closer to real document distributions in P2P environments. Furthermore, TREC-123-100 is split into two sets of 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0 500 1000 1500 2000 2500 3000 ARSS Query number ARSS versus the number of incoming queries for TREC-VLC-921 SSLA-921 SSNA-921 Figure 3: ARSS(Average reward per search session) versus the number of search sessions for 1phase search in TREC-VLC-921 0 0.1 0.2 0.3 0.4 0.5 0.6 0 500 1000 1500 2000 2500 3000 ARSS Query number ARSS versus query number for TREC-VLC-921 TSLA-921 TSNA-921 Figure 4: ARSS(Average reward per search session) versus the number of search sessions for 2phase search in TREC-VLC-921 sub-collections in two ways: randomly and by source. The two partitions are denoted as TREC-123-100-Random and TREC-123-100-Source respectively. The documents in each subcollection in dataset TREC-123-100-Source are more coherent than those in TREC-123-100-Random. The two different sets of partitions allow us to observe how the distributed learning algorithm is affected by the homogeneity of the collections. The hierarchical agent organization is generated by the algorithm described in our previous algorithm [15]. During the topology generation process, degree information of each agent is estimated by the algorithm introduced by Palmer et al. [9] with parameters α = 0.5 and β = 0.6. In our experiments, we estimate the upward limit and downward degree limit using linear discount factors 0.5, 0.8 and 1.0. Once the topology is built, queries randomly selected from the query set 301−350 on TREC-VLC-921 and query set 1− 50 on TREC-123-100-Random and TREC-123-100-Source are injected to the system based on a Poisson distribution P(N(t) = n) = (λt)n n! e−λ 236 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 0 50 100 150 200 250 300 350 400 0 500 1000 1500 2000 2500 3000 Cumulativeutility Query number Cumulative utility over the number of incoming queries TSLA-921 SSNA-921 SSLA-921 TSNA-921 Figure 5: The cumulative utility versus the number of search sessions TREC-VLC-921 In addition, we assume that all agents have an equal chance of getting queries from the environment, i.e, λ is the same for every agent. In our experiments, λ is set as 0.0543 so that the mean of the incoming queries from the environment to the agent network is 50 per time unit. The service time for the communication queue and local search queue, i.e tQij and trs, is set as 0.01 time unit and 0.05 time units respectively. In our experiments, there are ten types of queries acquired by clustering the query set 301 − 350 and 1 − 50. 4.3 Results analysis and evaluation Figure 3 demonstrates the ARSS(Average Reward per Search Session) versus the number of incoming queries over time for the the Single-Step based Non-learning Algorithm (SSNA), and the Single-Step Learning Algorithm(SSLA) for data collection TREC-VLC-921. It shows that the average reward for SSNA algorithm ranges from 0.02 − 0.06 and the performance changes little over time. The average reward for SSLA approach starts at the same level with the SSNA algorithm. But the performance increases over time and the average performance gain stabilizes at about 25% after query range 2000 − 3000. Figure 4 shows the ARSS(Average Reward per Search Session) versus the number of incoming queries over time for the the Two-Step based Non-learning Algorithm(TSNA), and the Two-Step Learning Algorithm(TSLA) for data collection TREC-VLC-921. The TSNA approach has a relatively consistent performance with the average reward ranges from 0.05 − 0.15. The average reward for TSLA approach, where learning algorithm is exploited, starts at the same level with the TSNA algorithm and improves the average reward over time until 2000−2500 queries joining the system. The results show that the average performance gain for TSLA approach over TNLA approach is 35% after stabilization. Figure 5 shows the cumulative utility versus the number of incoming queries over time for SSNA, SSLA,TSNA, and TSLA respectively. It illustrates that the cumulative utility of non-learning algorithms increases largely linearly over time, while the gains of learning-based algorithms accelerate when more queries enter the system. These experimental results demonstrate that learning-based approaches consistently perform better than non-learning based routing algorithm. Moreover, two-phase learning based algorithm is better than single-phase based learning algorithm because the maximal reward an agent can receive from searching its neighborhood within TTL hops is related to the total number of the relevant documents in that area. Thus, even the optimal routing policy can do little beyond reaching these relevant documents faster. On the contrary, the two-stepbased learning algorithm can relocate the search session to a neighborhood with more relevant documents. The TSLA combines the merits of both approaches and outperforms them. Table 1 lists the cumulative utility for datasets TREC123-100-Random and TREC-123-100-Source with hierarchical organizations. The five columns show the results for four different approaches. In particular, column TSNA-Random shows the results for dataset TREC-123-100-Random with the TSNA approach. The column TSLA-Random shows the results for dataset TREC-123-100-Random with the TSLA approach. There are two numbers in each cell in the column TSLA-Random. The first number is the actual cumulative utility while the second number is the percentage gain in terms of the utility over TSNA approach. Columns TSNA-Source and TSLA-Source show the results for dataset TREC-123-100-Source with TSNA and TSLA approaches respectively. Table 1 shows that the performance improvement for TREC-123-100-Random is not as significant as the other datasets. This is because that the documents in the sub-collection of TREC-123-100-Random are selected randomly which makes the collection model, the signature of the collection, less meaningful. Since both algorithms are designed based on the assumption that document collections can be well represented by their collection model, this result is not surprising. Overall, Figures 4, 5, and Table 1 demonstrate that the reinforcement learning based approach can considerably enhance the system performance for both data collections. However, it remains as future work to discover the correlation between the magnitude of the performance gains and the size of the data collection and/or the extent of the heterogeneity between the sub-collections. 5. RELATED WORK The content routing problem differs from the networklevel routing in packet-switched communication networks in that content-based routing occurs in application-level networks. In addition, the destination agents in our contentrouting algorithms are multiple and the addresses are not known in the routing process. IP-level Routing problems have been attacked from the reinforcement learning perspective[2, 5, 11, 12]. These studies have explored fully distributed algorithms that are able, without central coordination to disseminate knowledge about the network, to find the shortest paths robustly and efficiently in the face of changing network topologies and changing link costs. There are two major classes of adaptive, distributed packet routing algorithms in the literature: distance-vector algorithms and link-state algorithms. While this line of studies carry a certain similarity with our work, it has mainly focused on packet-switched communication networks. In this domain, the destination of a packet is deterministic and unique. Each agent maintains estimations, probabilistically or deterministically, on the distance to a certain destination through its neighbors. A variant of Q-Learning techniques is deployed The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 237 Table 1: Cumulative Utility for Datasets TREC-123-100-Random and TREC-123-100-Source with Hierarchical Organization; The percentage numbers in the columns TSLA-Random and TSLA-Source demonstrate the performance gain over the algorithm without learning Query number TSNA-Random TSLA-Random TSNA-Source TSLA-Source 500 25.15 28.45 13% 24.00 21.05 -13% 1000 104.99 126.74 20% 93.95 96.44 2.6% 1250 149.79 168.40 12% 122.64 134.05 9.3% 1500 188.94 211.05 12% 155.30 189.60 22% 1750 235.49 261.60 11% 189.14 243.90 28% 2000 275.09 319.25 16% 219.0 278.80 26% to update the estimations to converge to the real distances. It has been discovered that the locality property is an important feature of information retrieval systems in user modeling studies[3]. In P2P based content sharing systems, this property is exemplified by the phenomenon that users tend to send queries that represent only a limited number of topics and conversely, users in the same neighborhood are likely to share common interests and send similar queries [10]. The learning based approach is perceived to be more beneficial for real distributed information retrieval systems which exhibit locality property. This is because the users traffic and query patterns can reduce the state space and speed up the learning process. Related work in taking advantage of this property include [7], where the authors attempted to address this problem by user modeling techniques. 6. CONCLUSIONS In this paper, a reinforcement-learning based approach is developed to improve the performance of distributed IR search algorithms. Particularly, agents maintain estimates, namely expected utility, on the downstream agents ability to provide relevant documents for incoming queries. These estimates are updated gradually by learning from the feedback information returned from previous search sessions. Based on the updated expected utility information, the agents modify their routing policies. Thereafter, these agents route the queries based on the learned policies and update the estimates on the expected utility based on the new routing policies. The experiments on two different distributed IR datasets illustrates that the reinforcement learning approach improves considerably the cumulative utility over time. 7. REFERENCES [1] S. Abdallah and V. Lesser. Learning the task allocation game. In AAMAS 06: Proceedings of the fifth international joint conference on Autonomous agents and multiagent systems, pages 850-857, New York, NY, USA, 2006. ACM Press. [2] J. A. Boyan and M. L. Littman. Packet routing in dynamically changing networks: A reinforcement learning approach. In Advances in Neural Information Processing Systems, volume 6, pages 671-678. Morgan Kaufmann Publishers, Inc., 1994. [3] J. C. French, A. L. Powell, J. P. Callan, C. L. Viles, T. Emmitt, K. J. Prey, and Y. Mou. Comparing the performance of database selection algorithms. In Research and Development in Information Retrieval, pages 238-245, 1999. [4] B. Horling, R. Mailler, and V. Lesser. Farm: A scalable environment for multi-agent development and evaluation. In Advances in Software Engineering for Multi-Agent Systems, pages 220-237, Berlin, 2004. Springer-Verlag. [5] M. Littman and J. Boyan. A distributed reinforcement learning scheme for network routing. In Proceedings of the International Workshop on Applications of Neural Networks to Telecommunications, 1993. [6] J. Lu and J. Callan. Federated search of text-based digital libraries in hierarchical peer-to-peer networks. In In ECIR05, 2005. [7] J. Lu and J. Callan. User modeling for full-text federated search in peer-to-peer networks. In ACM SIGIR 2006. ACM Press, 2006. [8] C. D. Manning and H. Sch¨utze. Foundations of Statistical Natural Language Processing. The MIT Press, Cambridge, Massachusetts, 1999. [9] C. R. Palmer and J. G. Steffan. Generating network topologies that obey power laws. In Proceedings of GLOBECOM 2000, November 2000. [10] K. Sripanidkulchai, B. Maggs, and H. Zhang. Efficient content location using interest-based locality in peer-topeer systems. In INFOCOM, 2003. [11] D. Subramanian, P. Druschel, and J. Chen. Ants and reinforcement learning: A case study in routing in dynamic networks. In In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence, pages 832-839, 1997. [12] J. N. Tao and L. Weaver. A multi-agent, policy gradient approach to network routing. In In Proceedings of the Eighteenth International Conference on Machine Learning, 2001. [13] H. Zhang, W. B. Croft, B. Levine, and V. Lesser. A multi-agent approach for peer-to-peer information retrieval. In Proceedings of Third International Joint Conference on Autonomous Agents and Multi-Agent Systems, July 2004. [14] H. Zhang and V. Lesser. Multi-agent based peer-to-peer information retrieval systems with concurrent search sessions. In Proceedings of the Fifth International Joint Conference on Autonomous Agents and Multi-Agent Systems, May 2006. [15] H. Zhang and V. R. Lesser. A dynamically formed hierarchical agent organization for a distributed content sharing system. In 2004 IEEE/WIC/ACM International Conference on Intelligent Agent Technology (IAT 2004), 20-24 September 2004, Beijing, China, pages 169-175. IEEE Computer Society, 2004. 238 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)",
    "original_translation": "Un algoritmo de búsqueda distribuida basado en aprendizaje por refuerzo para sistemas jerárquicos de recuperación de información entre pares. En estos enfoques, los agentes dependen de la similitud de contenido entre las consultas entrantes y sus agentes vecinos directos para dirigir las sesiones de búsqueda distribuida. Sin embargo, tal heurística es miope en el sentido de que los agentes vecinos pueden no estar conectados con agentes más relevantes. En este documento, se desarrolla un enfoque basado en aprendizaje por refuerzo en línea para aprovechar las características dinámicas en tiempo de ejecución de los sistemas de recuperación de información P2P, representadas por información sobre sesiones de búsqueda pasadas. Específicamente, los agentes mantienen estimaciones sobre las habilidades de los agentes aguas abajo para proporcionar documentos relevantes para las consultas entrantes. Estas estimaciones se actualizan gradualmente al aprender de la información de retroalimentación devuelta de sesiones de búsqueda anteriores. Basándose en esta información, los agentes derivan políticas de enrutamiento correspondientes. Posteriormente, estos agentes dirigen las consultas basándose en las políticas aprendidas y actualizan las estimaciones según las nuevas políticas de enrutamiento. Los resultados experimentales demuestran que el algoritmo de aprendizaje mejora considerablemente el rendimiento del enrutamiento en dos conjuntos de colecciones de pruebas que han sido utilizados en una variedad de estudios de recuperación de información distribuida. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagente Términos Generales Algoritmos, Rendimiento, Experimentación 1. En los últimos años ha habido un creciente interés en estudiar cómo controlar los procesos de búsqueda en sistemas de recuperación de información (IR) basados en pares (P2P) [6, 13, 14, 15]. En esta línea de investigación, uno de los problemas centrales que preocupa a los investigadores es dirigir de manera eficiente las consultas de los usuarios en la red a los agentes que poseen los documentos apropiados. En ausencia de información global, las estrategias dominantes para abordar este problema son enfoques basados en la similitud de contenido [6, 13, 14, 15]. Si bien la similitud de contenido entre consultas y nodos locales parece ser un indicador creíble para la cantidad de documentos relevantes que residen en cada nodo, estos enfoques están limitados por una serie de factores. En primer lugar, las métricas basadas en similitud pueden ser miope ya que los nodos localmente relevantes pueden no estar conectados a otros nodos relevantes. Segundo, los enfoques basados en la similitud no tienen en cuenta las características de tiempo de ejecución de los sistemas de IR P2P, incluidos los parámetros ambientales, el uso del ancho de banda y la información histórica de las sesiones de búsqueda pasadas, que proporcionan información valiosa para los algoritmos de enrutamiento de consultas. En este artículo, desarrollamos un enfoque de IR basado en aprendizaje por refuerzo para mejorar el rendimiento de los algoritmos de búsqueda de IR distribuidos. Los agentes pueden adquirir mejores estrategias de búsqueda al recopilar y analizar información de retroalimentación de sesiones de búsqueda anteriores. En particular, los agentes mantienen estimaciones, es decir, utilidad esperada, sobre las capacidades de los agentes aguas abajo de proporcionar documentos relevantes para tipos específicos de consultas entrantes. Estas estimaciones se actualizan gradualmente al aprender de la información de retroalimentación devuelta de sesiones de búsqueda anteriores. Basándose en la información actualizada de utilidad esperada, los agentes derivan políticas de enrutamiento correspondientes. Posteriormente, estos agentes dirigen las consultas basándose en las políticas aprendidas y actualizan las estimaciones sobre la utilidad esperada en función de las nuevas políticas de enrutamiento. Este proceso se lleva a cabo de manera iterativa. El objetivo del algoritmo de aprendizaje, aunque consume ancho de banda de red, es reducir el tiempo de enrutamiento para procesar más consultas por unidad de tiempo y al mismo tiempo encontrar más documentos relevantes. Esto contrasta con los enfoques basados en la similitud de contenido, donde se repiten operaciones similares para cada consulta entrante y el tiempo de procesamiento se mantiene en gran medida constante con el tiempo. Otra forma de ver este documento es que nuestro enfoque básico para la búsqueda de IR distribuida es construir una red de superposición jerárquica (organización de agentes) basada en la medida de similitud de contenido entre las colecciones de documentos de los agentes de manera ascendente. En trabajos anteriores, hemos demostrado que esta organización mejora significativamente el rendimiento de búsqueda. Sin embargo, esta estructura organizativa no tiene en cuenta los patrones de llegada de las consultas, incluyendo su frecuencia, tipos y dónde entran en el sistema, ni el ancho de banda de comunicación disponible en la red y las capacidades de procesamiento de los agentes individuales. La intención del aprendizaje por refuerzo es adaptar las decisiones de enrutamiento de los agentes a las situaciones dinámicas de la red y aprender de sesiones de búsqueda pasadas. Específicamente, las contribuciones de este artículo incluyen: (1) un enfoque basado en aprendizaje por refuerzo para que los agentes adquieran políticas de enrutamiento satisfactorias basadas en estimaciones de la contribución potencial de sus agentes vecinos; (2) dos estrategias para acelerar el proceso de aprendizaje. Según nuestro mejor conocimiento, esta es una de las primeras aplicaciones de aprendizaje por refuerzo para abordar problemas de compartición de contenido distribuido, lo cual indica algunos de los problemas al aplicar el refuerzo en una aplicación compleja. El resto de este documento está organizado de la siguiente manera: la Sección 2 revisa los sistemas jerárquicos de intercambio de contenido y el algoritmo de búsqueda de dos fases basado en dicha topología. La Sección 3 describe un enfoque basado en aprendizaje por refuerzo para dirigir el proceso de enrutamiento; la Sección 4 detalla la configuración experimental y analiza los resultados. La sección 5 discute estudios relacionados y la sección 6 concluye el artículo. BÚSQUEDA EN SISTEMAS JERÁRQUICOS DE IR P2P Esta sección revisa brevemente nuestros enfoques básicos para los sistemas jerárquicos de IR P2P. En un sistema jerárquico de recuperación de información P2P ilustrado en la Fig.1, los agentes están conectados entre sí a través de tres tipos de enlaces: enlaces ascendentes, enlaces descendentes y enlaces laterales. En las siguientes secciones, denotamos el conjunto de agentes que están directamente conectados al agente Ai como DirectConn(Ai), el cual se define como DirectConn(Ai) = NEI(Ai) ∪ PAR(Ai) ∪ CHL(Ai), donde NEI(Ai) es el conjunto de agentes vecinos conectados a Ai a través de enlaces laterales; PAR(Ai) es el conjunto de agentes a los que el agente Ai está conectado a través de enlaces ascendentes y CHL(Ai) es el conjunto de agentes a los que el agente Ai se conecta a través de enlaces descendentes. Estos enlaces se establecen a través de un proceso de agrupamiento distribuido basado en la similitud de contenido de abajo hacia arriba[15]. Estos enlaces son luego utilizados por agentes para localizar otros agentes que contienen documentos relevantes a las consultas dadas. Un agente típico Ai en nuestro sistema utiliza dos colas: una cola de búsqueda local, LSi, y una cola de reenvío de mensajes MFi. Los estados de las dos colas constituyen los estados internos de un agente. La cola de búsqueda local LSi almacena sesiones de búsqueda programadas para procesamiento local. Es una cola de prioridad y el agente Ai siempre selecciona las consultas más prometedoras para procesarlas con el fin de maximizar la utilidad global. MFi consiste en un conjunto de consultas para reenviar y se procesa de manera FIFO (primero en entrar, primero en salir). Para la primera consulta en MFi, el agente Ai determina a qué subconjunto de sus agentes vecinos reenviarla basándose en la política de enrutamiento de los agentes πi. Estas decisiones de enrutamiento determinan cómo se lleva a cabo el proceso de búsqueda en la red. En este documento, llamamos a Ai como el agente aguas arriba de Ajs y a Aj como el agente aguas abajo de Ais si A4 A5 A6 A7 A2 A3 A9 NEI(A2)={A3} PAR(A2)={A1} CHL(A2)={A4,A5} A1 A8 Figura 1: Una fracción de un sistema jerárquico P2PIR en el que un agente Ai dirige una consulta al agente Aj. El protocolo de búsqueda distribuida de nuestra organización de agentes jerárquicos está compuesto por dos pasos. En el primer paso, al recibir una consulta qk en el tiempo tl de un usuario, el agente Ai inicia una sesión de búsqueda si al sondear a sus agentes vecinos Aj ∈ NEI(Ai) con el mensaje PROBE para el valor de similitud Sim(qk, Aj) entre qk y Aj. Aquí, Ai se define como el iniciador de la consulta de la sesión de búsqueda si. En el segundo paso, Ai selecciona un grupo de agentes más prometedores para iniciar el proceso de búsqueda real con el mensaje BUSCAR. Estos mensajes de BÚSQUEDA contienen un parámetro TTL (Tiempo de Vida) además de la consulta. El valor TTL disminuye en 1 después de cada salto. En el proceso de búsqueda, los agentes descartan aquellas consultas que han sido procesadas previamente o cuyo TTL disminuye a 0, lo que evita que las consultas se repitan infinitamente en el sistema. La sesión de búsqueda termina cuando todos los agentes que reciben la consulta la descartan o el TTL disminuye a 0. Tras recibir mensajes de BÚSQUEDA para qk, los agentes programan actividades locales que incluyen búsqueda local, reenvío de qk a sus vecinos y devolución de resultados de búsqueda al iniciador de la consulta. Este proceso y los algoritmos relacionados se detallan en [15, 14]. 3. En el enfoque de búsqueda basado en aprendizaje por refuerzo, mencionado anteriormente, las decisiones de enrutamiento de un agente Ai dependen de la comparación de similitud entre las consultas entrantes y los agentes vecinos de Ai para reenviar esas consultas a agentes relevantes sin inundar la red con mensajes de consulta innecesarios. Sin embargo, esta heurística es miope porque un vecino directo relevante no necesariamente está conectado con otros agentes relevantes. En esta sección, proponemos un enfoque más general al plantear este problema como una tarea de aprendizaje por refuerzo. En busca de una mayor flexibilidad, los agentes pueden alternar entre dos modos: modo de aprendizaje y modo sin aprendizaje. En el modo de no aprendizaje, los agentes operan de la misma manera que lo hacen en los procesos de búsqueda distribuida normales descritos en [14, 15]. Por otro lado, en el modo de aprendizaje, en paralelo con las sesiones de búsqueda distribuida, los agentes también participan en un proceso de aprendizaje que se detallará en esta sección. Ten en cuenta que en el protocolo de aprendizaje, el proceso de aprendizaje no interfiere con el proceso de búsqueda distribuida. Los agentes pueden elegir iniciar y detener procesos de aprendizaje sin afectar el rendimiento del sistema. En particular, dado que el proceso de aprendizaje consume recursos de red (especialmente ancho de banda), los agentes pueden optar por iniciar el aprendizaje solo cuando la carga de la red es relativamente baja, minimizando así los costos adicionales de comunicación incurridos por el algoritmo de aprendizaje. La sección está estructurada de la siguiente manera, la Sección 3.1 describe 232 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) un modelo basado en aprendizaje por refuerzo. La sección 3.2 describe un protocolo para implementar el algoritmo de aprendizaje en la red. La sección 3.3 discute la convergencia del algoritmo de aprendizaje. 3.1 El Modelo La política de enrutamiento de un agente toma el estado de una sesión de búsqueda como entrada y produce las acciones de enrutamiento para esa consulta. En nuestro trabajo, el estado de una sesión de búsqueda sj se estipula como: QSj = (qk, ttlj) donde ttlj es el número de saltos que quedan para la sesión de búsqueda sj, qk es la consulta específica. QL es un atributo de qk que indica a qué tipo de consultas qk probablemente pertenecen. El conjunto de QL puede ser generado ejecutando un algoritmo de clasificación en línea simple en todas las consultas que han sido procesadas por los agentes, o un algoritmo fuera de línea en un conjunto de entrenamiento pre-designado. La suposición aquí es que el conjunto de tipos de consultas se aprende de antemano y pertenece al conocimiento común de los agentes en la red. El trabajo futuro incluye explorar cómo se puede lograr el aprendizaje cuando esta suposición no se cumple. Dado el conjunto de tipos de consultas establecido, una consulta entrante qi puede ser clasificada en una clase de consulta Q(qi) mediante la fórmula: Q(qi) = arg max Qj P(qi|Qj) (1) donde P(qi|Qj) indica la probabilidad de que la consulta qi sea generada por la clase de consulta Qj [8]. El conjunto de acciones de enrutamiento atómico de un agente Ai se denota como {αi}, donde {αi} se define como αi = {αi0 , αi1 , ..., αin }. Un elemento αij representa una acción para dirigir una consulta dada al agente vecino Aij ∈ DirectConn(Ai). La política de enrutamiento πi del agente Ai es estocástica y su resultado para una sesión de búsqueda con estado QSj se define como: πi(QSj) = {(αi0 , πi(QSi, αi0 )), (αi1 , πi(QSi, αi1 )), ...} (2) Nótese que el operador πi está sobrecargado para representar tanto la política probabilística para una sesión de búsqueda con estado QSj, denotada como πi(QSj); como la probabilidad de enviar la consulta a un agente vecino específico Aik ∈ DirectConn(Ai) bajo la política πi(QSj), denotada como πi(QSj, αik). Por lo tanto, la ecuación (2) significa que la probabilidad de reenviar la sesión de búsqueda al agente Ai0 es πi(QSi, αi0) y así sucesivamente. Bajo esta política estocástica, la acción de enrutamiento es no determinística. La ventaja de esta estrategia es que los mejores agentes vecinos no serán seleccionados repetidamente, mitigando así las posibles situaciones de puntos calientes. La utilidad esperada, Un i (QSj), se utiliza para estimar la ganancia potencial de utilidad al dirigir el tipo de consulta QSj al agente Ai bajo la política πn i. El superíndice n indica el valor en la n-ésima iteración en un proceso de aprendizaje iterativo. La utilidad esperada proporciona orientación de enrutamiento para futuras sesiones de búsqueda. En el proceso de búsqueda, cada agente Ai mantiene observaciones parciales de los estados de sus vecinos, como se muestra en la Figura 2. La observación parcial incluye información no local como la estimación de la utilidad potencial de su vecino Am para el estado de consulta QSj, denotado como Um(QSj), así como la información de carga, Lm. Estas observaciones son actualizadas periódicamente por los vecinos. La información de utilidad estimada se utilizará para actualizar la utilidad esperada de A en su política de enrutamiento. Cargar información de utilidad esperada para diferentes tipos de consultas de agentes vecinos... A0 A1 A3 A2 Un 0 (QS0) ... ... ... ... ...... Un 0 (QS1) Un 1 (QS1) Un 2 (QS1) Un 3 (QS1) Un 1 (QS0) Un 2 (QS0) Un 3 (QS0) Ln 0 Ln 1 Ln 2 Ln 3 ... QS0 QS1 ... Figura 2: La Observación Parcial del Agente Ais sobre sus vecinos (A0, A1...) La información de carga de Am, Lm, se define como Lm = |MFm| Cm, donde |MFm| es la longitud de la cola de mensajes en espera y Cm es la tasa de servicio de la cola de mensajes en espera del agente Am. Por lo tanto, Lm caracteriza la utilización de un canal de comunicación de agentes, y así proporciona información no local a los vecinos de Ams para ajustar los parámetros de su política de enrutamiento y evitar inundar a sus agentes aguas abajo. Ten en cuenta que, basado en las características de las consultas que ingresan al sistema y las capacidades de los agentes, la carga de trabajo de los agentes puede no ser uniforme. Después de recopilar la información de la tasa de utilización de todos sus vecinos, el agente Ai calcula Li como una medida única para evaluar la condición de carga promedio de su vecindario: Li = P k Lk |DirectConn(Ai)| Los agentes explotan el valor de Li para determinar la probabilidad de enrutamiento en su política de enrutamiento. Se debe tener en cuenta que, como se describe en la Sección 3.2, la información sobre agentes vecinos se transmite junto con el mensaje de consulta propagado entre los agentes siempre que sea posible para reducir la sobrecarga de tráfico. 3.1.1 Actualización de la Política Se introduce un proceso de actualización iterativo para que los agentes aprendan una política de enrutamiento estocástica satisfactoria. En este proceso iterativo, los agentes actualizan sus estimaciones sobre la utilidad potencial de sus políticas de enrutamiento actuales y luego propagan las estimaciones actualizadas a sus vecinos. Sus vecinos luego generan una nueva política de enrutamiento basada en la observación actualizada y a su vez calculan la utilidad esperada basada en las nuevas políticas y continúan este proceso iterativo. En particular, en el tiempo n, dado un conjunto de utilidad esperada, un agente Ai, cuyo conjunto de agentes directamente conectados es DirectConn(Ai) = {Ai0, ..., Aim}, determina su política de enrutamiento estocástico correspondiente para una sesión de búsqueda del estado QSj basada en los siguientes pasos: (1) Ai primero selecciona un subconjunto de agentes como los agentes potenciales aguas abajo del conjunto DirectConn(Ai), denotado como PDn(Ai, QSj). El tamaño del agente potencial aguas abajo se especifica como |PDn(Ai, QSj)| = min(|NEI(Ai), dn i + k)| donde k es una constante y se establece en 3 en este documento; dn i, el ancho hacia adelante, se define como el número esperado de The Sixth Intl. La fórmula especifica que el conjunto potencial de agentes aguas abajo PDn(Ai, QSj) es o bien el subconjunto de agentes vecinos con los k valores de utilidad esperada más altos para el estado QSj entre todos los agentes en DirectConn(Ai), o todos sus agentes vecinos. La k se introduce basada en la idea de una política de enrutamiento estocástica y hace que la probabilidad de reenvío del agente dn i + k más alto sea inferior al 100%. Ten en cuenta que si queremos limitar el número de agentes secundarios para la sesión de búsqueda sj a 5, la probabilidad de reenviar la consulta a todos los agentes vecinos debe sumar 5. Configurar adecuadamente el valor de dn i puede mejorar la tasa de utilización del ancho de banda de la red cuando gran parte de la red está inactiva, al mismo tiempo que mitiga la carga de tráfico cuando la red está altamente cargada. El valor de dn+1 i se actualiza en función de dn i, las observaciones anteriores y actuales sobre la situación del tráfico en el vecindario. Específicamente, la fórmula de actualización para dn+1 i es dn+1 i = dn i ∗ (1 + 1 − Li |DirectConn(Ai)| ) En esta fórmula, el ancho de avance se actualiza en función de las condiciones de tráfico del vecindario del agente Ai, es decir, Li, y su valor anterior. (2) Para cada agente Aik en el PDn(Ai, QSj), la probabilidad de reenviar la consulta a Aik se determina de la siguiente manera para asignar una mayor probabilidad de reenvío a los agentes vecinos con un valor de utilidad esperado más alto: πn+1 i (QSj, αik ) = dn+1 i |PDn(Ai, QSj)| + β ∗ ` Uik (QSj) − PDU(Ai, QSj) |PDn(Ai, QSj)| ´ (3) donde PDUn(Ai, QSj) = X o∈P Dn(Ai,QSj ) Uo(QSj) y QSj es el estado subsiguiente del agente Aik después de que el agente Ai reenvía la sesión de búsqueda con estado QSj a su agente vecino Aik; Si QSj = (qk, ttl0), entonces QSj = (qk, ttl0 − 1). En la fórmula 3, el primer término a la derecha de la ecuación, dn+1 i |P Dn(Ai,QSj )|, se utiliza para determinar la probabilidad de reenvío distribuyendo equitativamente el ancho de avance, dn+1 i, a los agentes en el conjunto PDn(Ai, QSj). El segundo término se utiliza para ajustar la probabilidad de ser elegido de manera que se favorezca a los agentes con valores de utilidad esperada más altos. β se determina de acuerdo a: β = min ` m − dn+1 i m ∗ umax − PDUn(Ai, QSj) , dn+1 i PDUn(Ai, QSj) − m ∗ umin ´ (4) donde m = |PDn(Ai, QSj)|, umax = max o∈P Dn(Ao,QSj ) Uo(QSj) y umin = min o∈P Dn(Ao,QSj ) Uo(QSj) Esta fórmula garantiza que el valor final de πn+1 i (QSj, αik ) esté bien definido, es decir, 0 ≤ πn+1 i (QSj, αik ) ≤ 1 y X i πn+1 i (QSj, αik ) = dn+1 i Sin embargo, esta solución no explora todas las posibilidades. Para equilibrar entre la explotación y la exploración, se adopta un enfoque λ-Greedy. En el enfoque λ-Greedy, además de asignar una probabilidad más alta a aquellos agentes con un valor de utilidad esperado más alto, como en la ecuación (3). Los agentes que parecen no ser buenas opciones también recibirán consultas basadas en una tasa de exploración dinámica. En particular, para agentes en el conjunto PDn(Ai, QSj), πn+1 i1 (QSj) se determina de la misma manera que arriba, con la única diferencia de que dn+1 i es reemplazado por dn+1 i ∗ (1 − λn). El ancho de banda de búsqueda restante se utiliza para el aprendizaje asignando la probabilidad λn de manera uniforme a los agentes Ai2 en el conjunto DirectConn(Ai) - PDn(Ai, QSj). πn+1 i2 (QSj, αik ) = dn+1 i ∗ λn |DirectConn(Ai) - PDn(Ai, QSj)| (5) donde PDn(Ai, QSj) ⊂ DirectConn(Ai). Se debe tener en cuenta que la tasa de exploración λ no es constante y disminuye con el tiempo. El λ se determina de acuerdo con la siguiente ecuación: λn+1 = λ0 ∗ e−c1n (6) donde λ0 es la tasa de exploración inicial, que es una constante; c1 también es una constante para ajustar la tasa de disminución de la tasa de exploración; n es la unidad de tiempo actual. 3.1.2 Actualización de la Utilidad Esperada Una vez que la política de enrutamiento en el paso n+1, πn+1 i, se determina según la fórmula anterior, el agente Ai puede actualizar su propia utilidad esperada, Un+1 i (QSi), basándose en la política de enrutamiento actualizada resultante de la fórmula 5 y los valores U actualizados de sus agentes vecinos. Bajo el supuesto de que después de que una consulta se envía a los vecinos de Ai, las sesiones de búsqueda subsiguientes son independientes, la fórmula de actualización es similar a la fórmula de actualización de Bellman en Q-Learning: Un+1 i (QSj) = (1 − θi) ∗ Un i (QSj) + θi ∗ (Rn+1 i (QSj) + X k πn+1 i (QSj, αik )Un k (QSj)) (7) donde QSj = (Qj, ttl − 1) es el siguiente estado de QSj = (Qj, ttl); Rn+1 i (QSj) es la recompensa local esperada para la clase de consulta Qk en el agente Ai bajo la política de enrutamiento πn+1 i; θi es el coeficiente para decidir cuánto peso se le da al valor antiguo durante el proceso de actualización: cuanto menor sea el valor de θi, se espera que el agente aprenda más rápido el valor real, mientras que mayor sea la volatilidad del algoritmo, y viceversa. Rn+1 (s) se actualiza de acuerdo con la siguiente ecuación: 234 The Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Rn+1 i (QSj) = Rn i (QSj) +γi ∗ (r(QSj) − Rn i (QSj)) ∗ P(qj|Qj ) (8) donde r(QSj) es la recompensa local asociada con la sesión de búsqueda. P(qj|Qj) indica qué tan relevante es la consulta qj para el tipo de consulta Qj, y γi es la tasa de aprendizaje para el agente Ai. Dependiendo de la similitud entre una consulta específica qi y su tipo de consulta correspondiente Qi, la recompensa local asociada con la sesión de búsqueda tiene un impacto diferente en la estimación de Rn i (QSj). En la fórmula anterior, este impacto se refleja en el coeficiente, el valor P(qj|Qj). Función de recompensa Después de que una sesión de búsqueda se detiene cuando sus valores TTL expiran, todos los resultados de búsqueda se devuelven al usuario y se comparan con la evaluación de relevancia. Suponiendo que el conjunto de resultados de búsqueda es SR, la recompensa Rew(SR) se define como: Rew(SR) = j 1 si |Rel(SR)| > c |Rel(SR)| c en caso contrario, donde SR es el conjunto de resultados de búsqueda devueltos, Rel(SR) es el conjunto de documentos relevantes en los resultados de búsqueda. Esta ecuación especifica que los usuarios otorgan una recompensa de 1.0 si el número de documentos relevantes devueltos alcanza un número predefinido c. De lo contrario, la recompensa es proporcional al número de documentos relevantes devueltos. La razón para establecer un valor de corte de este tipo es que la importancia de la proporción de recuperación disminuye con la abundancia de documentos relevantes en el mundo real, por lo tanto, los usuarios tienden a enfocarse solo en un número limitado de resultados buscados. Los detalles del protocolo de enrutamiento actual se introducirán en la Sección 3.2 cuando presentemos cómo se implementa el algoritmo de aprendizaje en sistemas reales. 3.2 Implementación del algoritmo de aprendizaje. Esta sección describe cómo se puede utilizar el algoritmo de aprendizaje en un proceso de búsqueda de una o dos fases. En el algoritmo de búsqueda de una sola fase, las sesiones de búsqueda comienzan desde los iniciadores de las consultas. Por el contrario, en el algoritmo de búsqueda de dos pasos, el iniciador de la consulta primero intenta buscar un punto de inicio más apropiado para la consulta introduciendo un paso exploratorio como se describe en la Sección 2. A pesar de la diferencia en la calidad de los puntos de partida, la mayor parte del proceso de aprendizaje para los dos algoritmos es en gran medida el mismo como se describe en los párrafos siguientes. Antes de que comience el aprendizaje, cada agente inicializa el valor de utilidad esperado para todos los posibles estados como 0. Posteriormente, al recibir una consulta, además de las operaciones normales descritas en la sección anterior, un agente Ai también configura un temporizador para esperar los resultados de búsqueda devueltos por sus agentes subordinados. Una vez que el temporizador expire o haya recibido respuesta de todos sus agentes aguas abajo, Ai fusiona y reenvía los resultados de búsqueda acumulados de sus agentes aguas abajo a su agente aguas arriba. Configurar el temporizador acelera el aprendizaje porque los agentes pueden evitar esperar demasiado tiempo a que los agentes aguas abajo devuelvan los resultados de búsqueda. Ten en cuenta que estos resultados detallados y la información correspondiente del agente seguirán almacenados en Ai hasta que la información de retroalimentación sea transmitida desde su agente aguas arriba y se pueda evaluar el rendimiento de sus agentes aguas abajo. La duración del temporizador está relacionada con el valor TTL. En este documento, configuramos el temporizador a ttimer = ttli ∗ 2 + tf, donde ttli ∗ 2 es la suma del tiempo de viaje de las consultas en la red, y tf es el período de tiempo esperado que los usuarios desearían esperar. Los resultados de la búsqueda serán finalmente devueltos al iniciador de la sesión de búsqueda A0. Serán comparados con la evaluación de relevancia proporcionada por los usuarios finales (como se describe en la sección del experimento, la evaluación de relevancia para el conjunto de consultas se proporciona junto con las colecciones de datos). La recompensa será calculada y propagada hacia atrás a los agentes a lo largo del camino por el que se pasaron los resultados de la búsqueda. Este es un proceso inverso de la propagación de los resultados de búsqueda. En el proceso de propagar la recompensa hacia atrás, los agentes actualizan las estimaciones de su propio valor potencial de utilidad, generan una política actualizada y transmiten sus resultados actualizados a los agentes vecinos basándose en el algoritmo descrito en la Sección 3. Tras el cambio del valor de utilidad esperado, el agente Ai envía su estimación de utilidad actualizada a sus vecinos para que puedan actuar según la utilidad esperada modificada y el estado correspondiente. Este mensaje de actualización incluye la recompensa potencial, así como el estado correspondiente QSi = (qk, ttll) del agente Ai. Cada agente vecino, Aj, reacciona a este tipo de mensaje de actualización actualizando el valor de utilidad esperado para el estado QSj(qk, ttll + 1) de acuerdo con el valor de utilidad esperado cambiado recién anunciado. Una vez que completen la actualización, los agentes informarían nuevamente a los vecinos relacionados para que actualicen sus valores. Este proceso continúa hasta que el valor TTL en el mensaje de actualización aumenta al límite de TTL. Para acelerar el proceso de aprendizaje, al actualizar los valores de utilidad esperada de un agente Ai con agentes vecinos, especificamos que Um(Qk, ttl0) >= Um(Qk, ttl1) si ttl0 > ttl1. Por lo tanto, cuando el agente Ai recibe un valor de utilidad esperada actualizado con ttl1, también actualiza los valores de utilidad esperada con cualquier ttl0 > ttl1 si Um(Qk, ttl0) < Um(Qk, ttl1) para acelerar la convergencia. Esta heurística se basa en el hecho de que la utilidad de una sesión de búsqueda es una función no decreciente del tiempo t. 3.3 Discusión Al formalizar el sistema de enrutamiento de contenido como una tarea de aprendizaje, se hacen muchas suposiciones. En sistemas reales, estas suposiciones pueden no cumplirse, por lo que el algoritmo de aprendizaje puede no converger. Dos problemas son de particular importancia, (1) Este problema de enrutamiento de contenido no tiene propiedades de Markov. A diferencia del enrutamiento de paquetes basado en el nivel de IP, la decisión de enrutamiento de cada agente para una sesión de búsqueda particular sj depende del historial de enrutamiento de sj. Por lo tanto, la suposición de que todas las sesiones de búsqueda posteriores son independientes no se cumple en la realidad. Esto puede llevar a un problema de doble conteo en el que los documentos relevantes de algunos agentes se contarán más de una vez para el estado donde el valor TTL es mayor que 1. Sin embargo, en el contexto de las organizaciones de agentes jerárquicos, dos factores mitigan estos problemas: primero, los agentes en cada grupo de contenido forman una estructura similar a un árbol. Con la ausencia de los ciclos, las estimaciones dentro del árbol estarían cerca del valor preciso. En segundo lugar, la naturaleza estocástica de la política de enrutamiento remedia parcialmente este problema. El Sexto Internacional. Otro desafío para este algoritmo de aprendizaje es que en un entorno de red real, las observaciones sobre agentes vecinos pueden no poder actualizarse a tiempo debido al retraso en la comunicación u otras situaciones. Además, cuando los agentes vecinos actualizan sus estimaciones al mismo tiempo, puede surgir oscilación durante el proceso de aprendizaje[1]. Este documento explora varios enfoques para acelerar el proceso de aprendizaje. Además de la estrategia mencionada de actualizar los valores de utilidad esperada, también empleamos una estrategia de actualización activa donde los agentes notifican a sus vecinos cada vez que se actualiza su utilidad esperada. Por lo tanto, se puede lograr una velocidad de convergencia más rápida. Esta estrategia contrasta con la actualización perezosa, donde los agentes solo repiten a sus agentes vecinos con el cambio en la utilidad esperada cuando intercambian información. El compromiso entre los dos enfoques es la carga de red versus la velocidad de aprendizaje. La ventaja de este algoritmo de aprendizaje es que una vez que se aprende una política de enrutamiento, los agentes no tienen que comparar repetidamente la similitud de las consultas siempre y cuando la topología de la red permanezca sin cambios. En cambio, el agente solo tiene que determinar adecuadamente la clasificación de la consulta y seguir las políticas aprendidas. La desventaja de este enfoque basado en el aprendizaje es que el proceso de aprendizaje debe llevarse a cabo cada vez que la estructura de la red cambia. Hay muchas posibles extensiones para este modelo de aprendizaje. Por ejemplo, actualmente se utiliza una sola medida para indicar la carga de tráfico en un vecindario de agentes. Una extensión sencilla sería llevar un registro de la carga individual de cada vecino del agente. 4. CONFIGURACIÓN DE EXPERIMENTOS Y RESULTADOS Los experimentos se llevan a cabo en el kit de simulación TRANO con dos conjuntos de datos, TREC-VLC-921 y TREC123-100. Las siguientes subsecciones presentan el banco de pruebas TRANO, los conjuntos de datos y los resultados experimentales. 4.1 Banco de pruebas TRANO TRANO (Task Routing on Agent Network Organization) es un banco de pruebas de recuperación de información basado en red de agentes múltiples. TRANO se construye sobre el Farm [4], un simulador distribuido basado en el tiempo que proporciona un marco de diseminación de datos para organizaciones de redes de agentes distribuidos a gran escala. TRANO apoya la importación y exportación de perfiles de organizaciones de agentes, incluyendo conexiones topológicas y otras características. Cada agente de TRANO está compuesto por una estructura de vista de agente y una unidad de control. En la simulación, cada agente es pulsado regularmente y el agente verifica las colas de mensajes entrantes, realiza operaciones locales y luego reenvía mensajes a otros agentes. 4.2 Configuración Experimental En nuestro experimento, utilizamos dos conjuntos de datos estándar, los conjuntos de datos TRECVLC-921 y TREC-123-100, para simular las colecciones alojadas en los agentes. Los conjuntos de datos TREC-VLC-921 y TREC123-100 fueron creados por el Instituto Nacional de Tecnología Estándar de los Estados Unidos (NIST) para sus conferencias TREC. En el ámbito de la recuperación de información distribuida, las dos colecciones de datos se dividen en 921 y 100 subcolecciones. Se observa que el conjunto de datos TREC-VLC-921 es más heterogéneo que TREC-123-100 en términos de origen, longitud de documento y distribución de documentos relevantes a partir de las estadísticas de las dos colecciones de datos enumeradas en [13]. Por lo tanto, TREC-VLC-921 está mucho más cerca de las distribuciones reales de documentos en entornos P2P. Además, TREC-123-100 se divide en dos conjuntos de 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0 500 1000 1500 2000 2500 3000 Número de consulta de ARSS frente al número de consultas entrantes para TREC-VLC-921 SSLA-921 SSNA-921 Figura 3: ARSS (Recompensa promedio por sesión de búsqueda) frente al número de sesiones de búsqueda para la búsqueda de 1 fase en TREC-VLC-921 0 0.1 0.2 0.3 0.4 0.5 0.6 0 500 1000 1500 2000 2500 3000 Número de consulta de ARSS frente al número de consultas para TREC-VLC-921 TSLA-921 TSNA-921 Figura 4: ARSS (Recompensa promedio por sesión de búsqueda) frente al número de sesiones de búsqueda para la búsqueda de 2 fases en TREC-VLC-921 subcolecciones de dos formas: aleatoriamente y por fuente. Las dos particiones se denominan TREC-123-100-Random y TREC-123-100-Source respectivamente. Los documentos en cada subcolección en el conjunto de datos TREC-123-100-Source son más coherentes que los de TREC-123-100-Random. Los dos conjuntos diferentes de particiones nos permiten observar cómo el algoritmo de aprendizaje distribuido se ve afectado por la homogeneidad de las colecciones. La organización jerárquica de agentes es generada por el algoritmo descrito en nuestro algoritmo anterior [15]. Durante el proceso de generación de la topología, la información de grado de cada agente es estimada por el algoritmo introducido por Palmer et al. [9] con parámetros α = 0.5 y β = 0.6. En nuestros experimentos, estimamos el límite superior y el límite de grado descendente utilizando factores de descuento lineales de 0.5, 0.8 y 1.0. Una vez que se construye la topología, las consultas seleccionadas al azar del conjunto de consultas 301-350 en TREC-VLC-921 y del conjunto de consultas 1-50 en TREC-123-100-Random y TREC-123-100-Source se inyectan en el sistema basado en una distribución de Poisson P(N(t) = n) = (λt)n n! e−λ 236 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 0 50 100 150 200 250 300 350 400 0 500 1000 1500 2000 2500 3000 Utilidad acumulada Número de consulta Utilidad acumulada sobre el número de consultas entrantes TSLA-921 SSNA-921 SSLA-921 TSNA-921 Figura 5: La utilidad acumulada versus el número de sesiones de búsqueda TREC-VLC-921 Además, asumimos que todos los agentes tienen la misma probabilidad de recibir consultas del entorno, es decir, λ es el mismo para cada agente. En nuestros experimentos, λ se establece en 0.0543 para que la media de las consultas entrantes del entorno a la red de agentes sea de 50 por unidad de tiempo. El tiempo de servicio para la cola de comunicación y la cola de búsqueda local, es decir, tQij y trs, se establece en 0.01 unidades de tiempo y 0.05 unidades de tiempo respectivamente. En nuestros experimentos, hay diez tipos de consultas adquiridas mediante la agrupación del conjunto de consultas 301-350 y 1-50. 4.3 Análisis y evaluación de resultados. La Figura 3 muestra el ARSS (Recompensa Promedio por Sesión de Búsqueda) versus el número de consultas entrantes a lo largo del tiempo para el Algoritmo No Aprendido de un Solo Paso (SSNA) y el Algoritmo de Aprendizaje de un Solo Paso (SSLA) para la recolección de datos TREC-VLC-921. Se muestra que la recompensa promedio para el algoritmo SSNA oscila entre 0.02 y 0.06 y su rendimiento cambia poco con el tiempo. El promedio de recompensa para el enfoque SSLA comienza en el mismo nivel que el algoritmo SSNA. Pero el rendimiento aumenta con el tiempo y la ganancia promedio de rendimiento se estabiliza en aproximadamente un 25% después del rango de consulta de 2000 a 3000. La Figura 4 muestra el ARSS (Recompensa Promedio por Sesión de Búsqueda) versus el número de consultas entrantes a lo largo del tiempo para el Algoritmo No Aprendido de Dos Pasos (TSNA) y el Algoritmo de Aprendizaje de Dos Pasos (TSLA) para la recolección de datos TREC-VLC-921. El enfoque TSNA tiene un rendimiento relativamente consistente con una recompensa promedio que oscila entre 0.05 y 0.15. El promedio de recompensa para el enfoque de TSLA, donde se explota el algoritmo de aprendizaje, comienza en el mismo nivel que el algoritmo TSNA y mejora la recompensa promedio con el tiempo hasta que se unen al sistema entre 2000 y 2500 consultas. Los resultados muestran que la ganancia promedio de rendimiento para el enfoque TSLA sobre el enfoque TNLA es del 35% después de la estabilización. La Figura 5 muestra la utilidad acumulada versus el número de consultas entrantes a lo largo del tiempo para SSNA, SSLA, TSNA y TSLA respectivamente. Se ilustra que la utilidad acumulativa de los algoritmos no basados en aprendizaje aumenta principalmente de forma lineal con el tiempo, mientras que las ganancias de los algoritmos basados en aprendizaje se aceleran cuando más consultas ingresan al sistema. Estos resultados experimentales demuestran que los enfoques basados en el aprendizaje tienen un rendimiento consistentemente mejor que los algoritmos de enrutamiento no basados en el aprendizaje. Además, el algoritmo basado en aprendizaje de dos fases es mejor que el algoritmo basado en aprendizaje de una sola fase porque la recompensa máxima que un agente puede recibir al buscar en su vecindario dentro de TTL saltos está relacionada con el número total de documentos relevantes en esa área. Por lo tanto, incluso la política de enrutamiento óptima puede hacer poco más allá de alcanzar estos documentos relevantes más rápido. Por el contrario, el algoritmo de aprendizaje basado en dos pasos puede reubicar la sesión de búsqueda en un vecindario con documentos más relevantes. El TSLA combina los méritos de ambos enfoques y los supera. La Tabla 1 enumera la utilidad acumulativa para los conjuntos de datos TREC123-100-Random y TREC-123-100-Source con organizaciones jerárquicas. Las cinco columnas muestran los resultados de cuatro enfoques diferentes. En particular, la columna TSNA-Random muestra los resultados para el conjunto de datos TREC-123-100-Random con el enfoque TSNA. La columna TSLA-Random muestra los resultados para el conjunto de datos TREC-123-100-Random con el enfoque TSLA. Hay dos números en cada celda de la columna TSLA-Random. El primer número es la utilidad acumulada real, mientras que el segundo número es el porcentaje de ganancia en términos de utilidad sobre el enfoque TSNA. Las columnas TSNA-Source y TSLA-Source muestran los resultados para el conjunto de datos TREC-123-100-Source con los enfoques TSNA y TSLA respectivamente. La Tabla 1 muestra que la mejora en el rendimiento para TREC-123-100-Random no es tan significativa como en los otros conjuntos de datos. Esto se debe a que los documentos en la subcolección de TREC-123-100-Random son seleccionados al azar, lo que hace que el modelo de colección, la firma de la colección, sea menos significativo. Dado que ambos algoritmos están diseñados basados en la suposición de que las colecciones de documentos pueden ser bien representadas por su modelo de colección, este resultado no es sorprendente. En general, las Figuras 4, 5 y la Tabla 1 demuestran que el enfoque basado en aprendizaje por refuerzo puede mejorar considerablemente el rendimiento del sistema para ambas colecciones de datos. Sin embargo, queda como trabajo futuro descubrir la correlación entre la magnitud de las mejoras en el rendimiento y el tamaño de la colección de datos y/o la extensión de la heterogeneidad entre las subcolecciones. TRABAJO RELACIONADO El problema de enrutamiento de contenido difiere del enrutamiento a nivel de red en redes de comunicación conmutadas por paquetes en que el enrutamiento basado en contenido ocurre en redes a nivel de aplicación. Además, los agentes de destino en nuestros algoritmos de enrutamiento de contenido son múltiples y las direcciones no son conocidas en el proceso de enrutamiento. Los problemas de enrutamiento a nivel de IP han sido abordados desde la perspectiva del aprendizaje por refuerzo[2, 5, 11, 12]. Estos estudios han explorado algoritmos distribuidos completamente que son capaces, sin coordinación central, de difundir conocimiento sobre la red, encontrar de manera robusta y eficiente los caminos más cortos frente a cambios en las topologías de red y costos de enlace cambiantes. Hay dos clases principales de algoritmos de enrutamiento de paquetes adaptativos y distribuidos en la literatura: algoritmos de vector de distancia y algoritmos de estado de enlace. Si bien esta línea de estudios tiene cierta similitud con nuestro trabajo, se ha centrado principalmente en redes de comunicación conmutadas por paquetes. En este dominio, el destino de un paquete es determinístico y único. Cada agente mantiene estimaciones, probabilísticamente o determinísticamente, sobre la distancia a un destino específico a través de sus vecinos. Una variante de las técnicas de Q-Learning se implementa en The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Utilidad Acumulativa para los Conjuntos de Datos TREC-123-100-Random y TREC-123-100-Source con Organización Jerárquica; Los números porcentuales en las columnas TSLA-Random y TSLA-Source demuestran la ganancia de rendimiento sobre el algoritmo sin aprendizaje Número de consulta TSNA-Random TSLA-Random TSNA-Source TSLA-Source 500 25.15 28.45 13% 24.00 21.05 -13% 1000 104.99 126.74 20% 93.95 96.44 2.6% 1250 149.79 168.40 12% 122.64 134.05 9.3% 1500 188.94 211.05 12% 155.30 189.60 22% 1750 235.49 261.60 11% 189.14 243.90 28% 2000 275.09 319.25 16% 219.0 278.80 26% para actualizar las estimaciones y converger a las distancias reales. Se ha descubierto que la propiedad de localidad es una característica importante de los sistemas de recuperación de información en estudios de modelado de usuarios[3]. En los sistemas de intercambio de contenido basados en P2P, esta propiedad se ejemplifica por el fenómeno de que los usuarios tienden a enviar consultas que representan solo un número limitado de temas y, por el contrario, los usuarios en el mismo vecindario probablemente compartan intereses comunes y envíen consultas similares [10]. El enfoque basado en el aprendizaje se percibe como más beneficioso para los sistemas reales de recuperación de información distribuida que exhiben la propiedad de localidad. Esto se debe a que los patrones de tráfico y consultas de los usuarios pueden reducir el espacio de estados y acelerar el proceso de aprendizaje. El trabajo relacionado en aprovechar esta propiedad incluye [7], donde los autores intentaron abordar este problema mediante técnicas de modelado de usuario. 6. CONCLUSIONES En este artículo, se desarrolla un enfoque basado en aprendizaje por refuerzo para mejorar el rendimiento de algoritmos de búsqueda de IR distribuidos. En particular, los agentes mantienen estimaciones, es decir, utilidad esperada, sobre la capacidad de los agentes aguas abajo para proporcionar documentos relevantes para las consultas entrantes. Estas estimaciones se actualizan gradualmente al aprender de la información de retroalimentación devuelta de sesiones de búsqueda anteriores. Basándose en la información actualizada de utilidad esperada, los agentes modifican sus políticas de enrutamiento. Posteriormente, estos agentes dirigen las consultas basándose en las políticas aprendidas y actualizan las estimaciones sobre la utilidad esperada en función de las nuevas políticas de enrutamiento. Los experimentos en dos conjuntos de datos IR distribuidos diferentes ilustran que el enfoque de aprendizaje por refuerzo mejora considerablemente la utilidad acumulativa con el tiempo. 7. REFERENCIAS [1] S. Abdallah y V. Lesser. Aprendiendo el juego de asignación de tareas. En AAMAS 06: Actas de la quinta conferencia internacional conjunta sobre agentes autónomos y sistemas multiagente, páginas 850-857, Nueva York, NY, EE. UU., 2006. ACM Press. [2] J. \n\nACM Press. [2] J. A. Boyan y M. L. Littman. Enrutamiento de paquetes en redes de cambio dinámico: Un enfoque de aprendizaje por refuerzo. En Avances en Sistemas de Procesamiento de Información Neural, volumen 6, páginas 671-678. Morgan Kaufmann Publishers, Inc., 1994. [3] J. C. French, A. L. Powell, J. P. Callan, C. L. Viles, T. Emmitt, K. J. Prey, y Y. Mou. Comparando el rendimiento de los algoritmos de selección de bases de datos. En Investigación y Desarrollo en Recuperación de Información, páginas 238-245, 1999. [4] B. Horling, R. Mailler y V. Lesser. Granja: Un entorno escalable para el desarrollo y evaluación de múltiples agentes. En Avances en Ingeniería de Software para Sistemas Multiagente, páginas 220-237, Berlín, 2004. Springer-Verlag. [5] M. Littman y J. Boyan. Un esquema distribuido de aprendizaje por refuerzo para enrutamiento de redes. En Actas del Taller Internacional sobre Aplicaciones de Redes Neuronales a las Telecomunicaciones, 1993. [6] J. Lu y J. Callan. Búsqueda federada de bibliotecas digitales basadas en texto en redes jerárquicas de pares. En ECIR05, 2005. [7] J. Lu y J. Callan. Modelado de usuario para búsqueda federada de texto completo en redes peer-to-peer. En ACM SIGIR 2006. ACM Press, 2006. [8] C. D. Manning y H. Schütze. Fundamentos del Procesamiento del Lenguaje Natural Estadístico. El MIT Press, Cambridge, Massachusetts, 1999. [9] C. R. Palmer y J. G. Steffan. Generando topologías de red que obedezcan leyes de potencia. En Actas de GLOBECOM 2000, noviembre de 2000. [10] K. Sripanidkulchai, B. Maggs y H. Zhang. Localización eficiente de contenido utilizando la localidad basada en intereses en sistemas peer-to-peer. En INFOCOM, 2003. [11] D. Subramanian, P. Druschel y J. Chen. Hormigas y aprendizaje por refuerzo: Un estudio de caso en enrutamiento en redes dinámicas. En Actas de la Decimoquinta Conferencia Internacional Conjunta sobre Inteligencia Artificial, páginas 832-839, 1997. [12] J. N. Tao y L. Weaver. Un enfoque de múltiples agentes y gradiente de políticas para el enrutamiento de redes. En Actas de la Decimoctava Conferencia Internacional sobre Aprendizaje Automático, 2001. [13] H. Zhang, W. B. Croft, B. Levine y V. Lesser. Un enfoque multiagente para la recuperación de información entre pares. En Actas de la Tercera Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, julio de 2004. [14] H. Zhang y V. Lesser. Sistemas de recuperación de información peer-to-peer basados en múltiples agentes con sesiones de búsqueda concurrentes. En Actas de la Quinta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, mayo de 2006. [15] H. Zhang y V. R. Lesser. Una organización de agentes jerárquica formada dinámicamente para un sistema distribuido de intercambio de contenido. En la Conferencia Internacional IEEE/WIC/ACM sobre Tecnología de Agentes Inteligentes (IAT 2004) del 20 al 24 de septiembre de 2004 en Beijing, China, páginas 169-175. IEEE Computer Society, 2004. 238 La Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07)",
    "original_sentences": [
        "A Reinforcement Learning based Distributed Search Algorithm For Hierarchical Peer-to-Peer Information Retrieval Systems Haizheng Zhang College of Information Science and Technology Pennsylvania State University University Park, PA 16803 hzhang@ist.psu.edu Victor Lesser Department of Computer Science University Of Massachusetts Amherst, MA 01003 lesser@cs.umass.edu ABSTRACT The dominant existing routing strategies employed in peerto-peer(P2P) based information retrieval(IR) systems are similarity-based approaches.",
        "In these approaches, agents depend on the content similarity between incoming queries and their direct neighboring agents to direct the distributed search sessions.",
        "However, such a heuristic is myopic in that the neighboring agents may not be connected to more relevant agents.",
        "In this paper, an online reinforcement-learning based approach is developed to take advantage of the dynamic run-time characteristics of P2P IR systems as represented by information about past search sessions.",
        "Specifically, agents maintain estimates on the downstream agents abilities to provide relevant documents for incoming queries.",
        "These estimates are updated gradually by learning from the feedback information returned from previous search sessions.",
        "Based on this information, the agents derive corresponding routing policies.",
        "Thereafter, these agents route the queries based on the learned policies and update the estimates based on the new routing policies.",
        "Experimental results demonstrate that the learning algorithm improves considerably the routing performance on two test collection sets that have been used in a variety of distributed IR studies.",
        "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Algorithms, Performance, Experimentation 1.",
        "INTRODUCTION Over the last few years there have been increasing interests in studying how to control the search processes in peer-to-peer(P2P) based information retrieval(IR) systems [6, 13, 14, 15].",
        "In this line of research, one of the core problems that concerns researchers is to efficiently route user queries in the network to agents that are in possession of appropriate documents.",
        "In the absence of global information, the dominant strategies in addressing this problem are content-similarity based approaches [6, 13, 14, 15].",
        "While the content similarity between queries and local nodes appears to be a creditable indicator for the number of relevant documents residing on each node, these approaches are limited by a number of factors.",
        "First of all, similaritybased metrics can be myopic since locally relevant nodes may not be connected to other relevant nodes.",
        "Second, the similarity-based approaches do not take into account the run-time characteristics of the P2P IR systems, including environmental parameters, bandwidth usage, and the historical information of the past search sessions, that provide valuable information for the query routing algorithms.",
        "In this paper, we develop a reinforcement learning based IR approach for improving the performance of distributed IR search algorithms.",
        "Agents can acquire better search strategies by collecting and analyzing feedback information from previous search sessions.",
        "Particularly, agents maintain estimates, namely expected utility, on the downstream agents capabilities of providing relevant documents for specific types of incoming queries.",
        "These estimates are updated gradually by learning from the feedback information returned from previous search sessions.",
        "Based on the updated expected utility information, the agents derive corresponding routing policies.",
        "Thereafter, these agents route the queries based on the learned policies and update the estimates on the expected utility based on the new routing policies.",
        "This process is conducted in an iterative manner.",
        "The goal of the learning algorithm, even though it consumes some network bandwidth, is to shorten the routing time so that more queries are processed per time unit while at the same time finding more relevant documents.",
        "This contrasts with the content-similarity based approaches where similar operations are repeated for every incoming query and the processing time keeps largely constant over time.",
        "Another way of viewing this paper is that our basic approach to distributed IR search is to construct a hierarchical overlay network(agent organization) based on the contentsimilarity measure among agents document collections in a bottom-up fashion.",
        "In the past work, we have shown that this organization improves search performance significantly.",
        "However, this organizational structure does not take into account the arrival patterns of queries, including their frequency, types, and where they enter the system, nor the available communication bandwidth of the network and processing capabilities of individual agents.",
        "The intention of the reinforcement learning is to adapt the agents routing decisions to the dynamic network situations and learn from past search sessions.",
        "Specifically, the contributions of this paper include: (1) a reinforcement learning based approach for agents to acquire satisfactory routing policies based on estimates of the potential contribution of their neighboring agents; (2) two strategies to speed up the learning process.",
        "To our best knowledge, this is one of the first reinforcement learning applications in addressing distributed content sharing problems and it is indicative of some of the issues in applying reinforcement in a complex application.",
        "The remainder of this paper is organized as follows: Section 2 reviews the hierarchical content sharing systems and the two-phase search algorithm based on such topology.",
        "Section 3 describes a reinforcement learning based approach to direct the routing process; Section 4 details the experimental settings and analyze the results.",
        "Section 5 discusses related studies and Section 6 concludes the paper. 2.",
        "SEARCH IN HIERARCHICAL P2P IR SYSTEMS This section briefly reviews our basic approaches to hierarchical P2P IR systems.",
        "In a hierarchical P2P IR system illustrated in Fig.1, agents are connected to each other through three types of links: upward links, downward links, and lateral links.",
        "In the following sections, we denote the set of agents that are directly connected to agent Ai as DirectConn(Ai), which is defined as DirectConn(Ai) = NEI(Ai) ∪ PAR(Ai) ∪ CHL(Ai) , where NEI(Ai) is the set of neighboring agents connected to Ai through lateral links; PAR(Ai) is the set of agents whom agent Ai is connected to through upward links and CHL(Ai) is the set of agents that agent Ai connects to through downward links.",
        "These links are established through a bottom-up content-similarity based distributed clustering process[15].",
        "These links are then used by agents to locate other agents that contain documents relevant to the given queries.",
        "A typical agent Ai in our system uses two queues: a local search queue, LSi, and a message forwarding queue MFi.",
        "The states of the two queues constitute the internal states of an agent.",
        "The local search queue LSi stores search sessions that are scheduled for local processing.",
        "It is a priority queue and agent Ai always selects the most promising queries to process in order to maximize the global utility.",
        "MFi consists of a set of queries to forward on and is processed in a FIFO (first in first out) fashion.",
        "For the first query in MFi, agent Ai determines which subset of its neighboring agents to forward it to based on the agents routing policy πi.",
        "These routing decisions determine how the search process is conducted in the network.",
        "In this paper, we call Ai as Ajs upstream agent and Aj as Ais downstream agent if A4 A5 A6 A7 A2 A3 A9 NEI(A2)={A3} PAR(A2)={A1} CHL(A2)={A4,A5} A1 A8 Figure 1: A fraction of a hierarchical P2PIR system an agent Ai routes a query to agent Aj.",
        "The distributed search protocol of our hierarchical agent organization is composed of two steps.",
        "In the first step, upon receipt of a query qk at time tl from a user, agent Ai initiates a search session si by probing its neighboring agents Aj ∈ NEI(Ai) with the message PROBE for the similarity value Sim(qk, Aj) between qk and Aj.",
        "Here, Ai is defined as the query initiator of search session si.",
        "In the second step, Ai selects a group of the most promising agents to start the actual search process with the message SEARCH.",
        "These SEARCH messages contain a TTL (Time To Live) parameter in addition to the query.",
        "The TTL value decreases by 1 after each hop.",
        "In the search process, agents discard those queries that either have been previously processed or whose TTL drops to 0, which prevents queries from looping in the system forever.",
        "The search session ends when all the agents that receive the query drop it or TTL decreases to 0.",
        "Upon receipt of SEARCH messages for qk, agents schedule local activities including local searching, forwarding qk to their neighbors, and returning search results to the query initiator.",
        "This process and related algorithms are detailed in [15, 14]. 3.",
        "A BASIC REINFORCEMENTLEARNING BASED SEARCH APPROACH In the aforementioned distributed search algorithm, the routing decisions of an agent Ai rely on the similarity comparison between incoming queries and Ais neighboring agents in order to forward those queries to relevant agents without flooding the network with unnecessary query messages.",
        "However, this heuristic is myopic because a relevant direct neighbor is not necessarily connected to other relevant agents.",
        "In this section, we propose a more general approach by framing this problem as a reinforcement learning task.",
        "In pursuit of greater flexibility, agents can switch between two modes: learning mode and non-learning mode.",
        "In the non-learning mode, agents operate in the same way as they do in the normal distributed search processes described in [14, 15].",
        "On the other hand, in the learning mode, in parallel with distributed search sessions, agents also participate in a learning process which will be detailed in this section.",
        "Note that in the learning protocol, the learning process does not interfere with the distributed search process.",
        "Agents can choose to initiate and stop learning processes without affecting the system performance.",
        "In particular, since the learning process consumes network resources (especially bandwidth), agents can choose to initiate learning only when the network load is relatively low, thus minimizing the extra communication costs incurred by the learning algorithm.",
        "The section is structured as follows, Section 3.1 describes 232 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) a reinforcement learning based model.",
        "Section 3.2 describes a protocol to deploy the learning algorithm in the network.",
        "Section 3.3 discusses the convergence of the learning algorithm. 3.1 The Model An agents routing policy takes the state of a search session as input and output the routing actions for that query.",
        "In our work, the state of a search session sj is stipulated as: QSj = (qk, ttlj) where ttlj is the number of hops that remains for the search session sj , qk is the specific query.",
        "QL is an attribute of qk that indicates which type of queries qk most likely belong to.",
        "The set of QL can be generated by running a simple online classification algorithm on all the queries that have been processed by the agents, or an oﬄine algorithm on a pre-designated training set.",
        "The assumption here is that the set of query types is learned ahead of time and belongs to the common knowledge of the agents in the network.",
        "Future work includes exploring how learning can be accomplished when this assumption does not hold.",
        "Given the query types set, an incoming query qi can be classified to one query class Q(qi) by the formula: Q(qi) = arg max Qj P(qi|Qj) (1) where P(qi|Qj ) indicates the likelihood that the query qi is generated by the query class Qj [8].",
        "The set of atomic routing actions of an agent Ai is denoted as {αi}, where {αi} is defined as αi = {αi0 , αi1 , ..., αin }.",
        "An element αij represents an action to route a given query to the neighboring agent Aij ∈ DirectConn(Ai).",
        "The routing policy πi of agent Ai is stochastic and its outcome for a search session with state QSj is defined as: πi(QSj) = {(αi0 , πi(QSi, αi0 )), (αi1 , πi(QSi, αi1 )), ...} (2) Note that operator πi is overloaded to represent either the probabilistic policy for a search session with state QSj, denoted as πi(QSj); or the probability of forwarding the query to a specific neighboring agent Aik ∈ DirectConn(Ai) under the policy πi(QSj), denoted as πi(QSj, αik ).",
        "Therefore, equation (2) means that the probability of forwarding the search session to agent Ai0 is πi(QSi, αi0 ) and so on.",
        "Under this stochastic policy, the routing action is nondeterministic.",
        "The advantage of such a strategy is that the best neighboring agents will not be selected repeatedly, thereby mitigating the potential hot spots situations.",
        "The expected utility, Un i (QSj), is used to estimate the potential utility gain of routing query type QSj to agent Ai under policy πn i .",
        "The superscript n indicates the value at the nth iteration in an iterative learning process.",
        "The expected utility provides routing guidance for future search sessions.",
        "In the search process, each agent Ai maintains partial observations of its neighbors states, as shown in Fig. 2.",
        "The partial observation includes non-local information such as the potential utility estimation of its neighbor Am for query state QSj, denoted as Um(QSj), as well as the load information, Lm.",
        "These observations are updated periodically by the neighbors.",
        "The estimated utility information will be used to update Ais expected utility for its routing policy.",
        "Load Information Expected Utility For Different Query Types Neighboring Agents ... A0 A1 A3 A2 Un 0 (QS0) ... ... ... ... ...... Un 0 (QS1) Un 1 (QS1) Un 2 (QS1) Un 3 (QS1) Un 1 (QS0) Un 2 (QS0) Un 3 (QS0) Ln 0 Ln 1 Ln 2 Ln 3 ... QS0 QS1 ...",
        "Figure 2: Agent Ais Partial Observation about its neighbors(A0, A1...) The load information of Am, Lm, is defined as Lm = |MFm| Cm , where |MFm| is the length of the message-forward queue and Cm is the service rate of agent Ams message-forward queue.",
        "Therefore Lm characterizes the utilization of an agents communication channel, and thus provide non-local information for Ams neighbors to adjust the parameters of their routing policy to avoid inundating their downstream agents.",
        "Note that based on the characteristics of the queries entering the system and agents capabilities, the loading of agents may not be uniform.",
        "After collecting the utilization rate information from all its neighbors, agent Ai computes Li as a single measure for assessing the average load condition of its neighborhood: Li = P k Lk |DirectConn(Ai)| Agents exploit Li value in determining the routing probability in its routing policy.",
        "Note that, as described in Section 3.2, information about neighboring agents is piggybacked with the query message propagated among the agents whenever possible to reduce the traffic overhead. 3.1.1 Update the Policy An iterative update process is introduced for agents to learn a satisfactory stochastic routing policy.",
        "In this iterative process, agents update their estimates on the potential utility of their current routing policies and then propagate the updated estimates to their neighbors.",
        "Their neighbors then generate a new routing policy based on the updated observation and in turn they calculate the expected utility based on the new policies and continue this iterative process.",
        "In particular, at time n, given a set of expected utility, an agent Ai, whose directly connected agents set is DirectConn(Ai) = {Ai0 , ..., Aim }, determines its corresponding stochastic routing policy for a search session of state QSj based on the following steps: (1) Ai first selects a subset of agents as the potential downstream agents from set DirectConn(Ai), denoted as PDn(Ai, QSj).",
        "The size of the potential downstream agent is specified as |PDn(Ai, QSj)| = min(|NEI(Ai), dn i + k)| where k is a constant and is set to 3 in this paper; dn i , the forward width, is defined as the expected number of The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 233 neighboring agents that agent Ai can forward to at time n. This formula specifies that the potential downstream agent set PDn(Ai, QSj) is either the subset of neighboring agents with dn i + k highest expected utility value for state QSj among all the agents in DirectConn(Ai), or all their neighboring agents.",
        "The k is introduced based on the idea of a stochastic routing policy and it makes the forwarding probability of the dn i +k highest agent less than 100%.",
        "Note that if we want to limit the number of downstream agents for search session sj as 5, the probability of forwarding the query to all neighboring agents should add up to 5.",
        "Setting up dn i value properly can improve the utilization rate of the network bandwidth when much of the network is idle while mitigating the traffic load when the network is highly loaded.",
        "The dn+1 i value is updated based on dn i , the previous and current observations on the traffic situation in the neighborhood.",
        "Specifically, the update formula for dn+1 i is dn+1 i = dn i ∗ (1 + 1 − Li |DirectConn(Ai)| ) In this formula, the forward width is updated based on the traffic conditions of agent Ais neighborhood, i.e Li, and its previous value. (2) For each agent Aik in the PDn(Ai, QSj), the probability of forwarding the query to Aik is determined in the following way in order to assign higher forwarding probability to the neighboring agents with higher expected utility value: πn+1 i (QSj, αik ) = dn+1 i |PDn(Ai, QSj)| + β ∗ ` Uik (QSj) − PDU(Ai, QSj) |PDn(Ai, QSj)| ´ (3) where PDUn(Ai, QSj) = X o∈P Dn(Ai,QSj ) Uo(QSj) and QSj is the subsequent state of agent Aik after agent Ai forwards the search session with state QSj to its neighboring agent Aik ; If QSj = (qk, ttl0), then QSj = (qk, ttl0 − 1).",
        "In formula 3, the first term on the right of the equation, dn+1 i |P Dn(Ai,QSj )| , is used to to determine the forwarding probability by equally distributing the forward width, dn+1 i , to the agents in PDn(Ai, QSj) set.",
        "The second term is used to adjust the probability of being chosen so that agents with higher expected utility values will be favored. β is determined according to: β = min ` m − dn+1 i m ∗ umax − PDUn(Ai, QSj) , dn+1 i PDUn(Ai, QSj) − m ∗ umin ´ (4) where m = |PDn(Ai, QSj)|, umax = max o∈P Dn(Ao,QSj ) Uo(QSj) and umin = min o∈P Dn(Ao,QSj ) Uo(QSj) This formula guarantees that the final πn+1 i (QSj, αik ) value is well defined, i.e, 0 ≤ πn+1 i (QSj, αik ) ≤ 1 and X i πn+1 i (QSj, αik ) = dn+1 i However, such a solution does not explore all the possibilities.",
        "In order to balance between exploitation and exploration, a λ-Greedy approach is taken.",
        "In the λ-Greedy approach, in addition to assigning higher probability to those agents with higher expected utility value, as in the equation (3).",
        "Agents that appear to be not-so-good choices will also be sent queries based on a dynamic exploration rate.",
        "In particular, for agents in the set PDn(Ai, QSj), πn+1 i1 (QSj) is determined in the same way as the above, with the only difference being that dn+1 i is replaced with dn+1 i ∗ (1 − λn).",
        "The remaining search bandwidth is used for learning by assigning probability λn evenly to agents Ai2 in the set DirectConn(Ai) − PDn(Ai, QSj). πn+1 i2 (QSj, αik ) = dn+1 i ∗ λn |DirectConn(Ai) − PDn(Ai, QSj)| (5) where PDn(Ai, QSj) ⊂ DirectConn(Ai).",
        "Note that the exploration rate λ is not a constant and it decreases overtime.",
        "The λ is determined according to the following equation: λn+1 = λ0 ∗ e−c1n (6) where λ0 is the initial exploration rate, which is a constant; c1 is also a constant to adjust the decreasing rate of the exploration rate; n is the current time unit. 3.1.2 Update Expected Utility Once the routing policy at step n+1, πn+1 i , is determined based on the above formula, agent Ai can update its own expected utility, Un+1 i (QSi), based on the the updated routing policy resulted from the formula 5 and the updated U values of its neighboring agents.",
        "Under the assumption that after a query is forwarded to Ais neighbors the subsequent search sessions are independent, the update formula is similar to the Bellman update formula in Q-Learning: Un+1 i (QSj) = (1 − θi) ∗ Un i (QSj) + θi ∗ (Rn+1 i (QSj) + X k πn+1 i (QSj, αik )Un k (QSj)) (7) where QSj = (Qj, ttl − 1) is the next state of QSj = (Qj, ttl); Rn+1 i (QSj) is the expected local reward for query class Qk at agent Ai under the routing policy πn+1 i ; θi is the coefficient for deciding how much weight is given to the old value during the update process: the smaller θi value is, the faster the agent is expected to learn the real value, while the greater volatility of the algorithm, and vice versa.",
        "Rn+1 (s) is updated according to the following equation: 234 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Rn+1 i (QSj) = Rn i (QSj) +γi ∗ (r(QSj) − Rn i (QSj)) ∗ P(qj|Qj ) (8) where r(QSj) is the local reward associated with the search session.",
        "P(qj|Qj ) indicates how relevant the query qj is to the query type Qj, and γi is the learning rate for agent Ai.",
        "Depending on the similarity between a specific query qi and its corresponding query type Qi, the local reward associated with the search session has different impact on the Rn i (QSj) estimation.",
        "In the above formula, this impact is reflected by the coefficient, the P(qj|Qj) value. 3.1.3 Reward function After a search session stops when its TTL values expires, all search results are returned back to the user and are compared against the relevance judgment.",
        "Assuming the set of search results is SR, the reward Rew(SR) is defined as: Rew(SR) = j 1 if |Rel(SR)| > c |Rel(SR)| c otherwise. where SR is the set of returned search results, Rel(SR) is the set of relevant documents in the search results.",
        "This equation specifies that users give 1.0 reward if the number of returned relevant documents reaches a predefined number c. Otherwise, the reward is in proportion to the number of relevant documents returned.",
        "This rationale for setting up such a cut-off value is that the importance of recall ratio decreases with the abundance of relevant documents in real world, therefore users tend to focus on only a limited number of searched results.",
        "The details of the actual routing protocol will be introduced in Section 3.2 when we introduce how the learning algorithm is deployed in real systems. 3.2 Deployment of the Learning algorithm This section describes how the learning algorithm can be used in either a single-phase or a two-phase search process.",
        "In the single-phase search algorithm, search sessions start from the initiators of the queries.",
        "In contrast, in the two-step search algorithm, the query initiator first attempts to seek a more appropriate starting point for the query by introducing an exploratory step as described in Section 2.",
        "Despite the difference in the quality of starting points, the major part of the learning process for the two algorithms is largely the same as described in the following paragraphs.",
        "Before learning starts, each agent initializes the expected utility value for all possible states as 0.",
        "Thereafter, upon receipt of a query, in addition to the normal operations described in the previous section, an agent Ai also sets up a timer to wait for the search results returned from its downstream agents.",
        "Once the timer expires or it has received response from all its downstream agents, Ai merges and forwards the search results accrued from its downstream agents to its upstream agent.",
        "Setting up the timer speeds up the learning because agents can avoid waiting too long for the downstream agents to return search results.",
        "Note that these detailed results and corresponding agent information will still be stored at Ai until the feedback information is passed from its upstream agent and the performance of its downstream agents can be evaluated.",
        "The duration of the timer is related to the TTL value.",
        "In this paper, we set the timer to ttimer = ttli ∗ 2 + tf , where ttli ∗ 2 is the sum of the travel time of the queries in the network, and tf is the expected time period that users would like to wait.",
        "The search results will eventually be returned to the search session initiator A0.",
        "They will be compared to the relevance judgment that is provided by the final users (as described in the experiment section, the relevance judgement for the query set is provided along with the data collections).",
        "The reward will be calculated and propagated backward to the agents along the way that search results were passed.",
        "This is a reverse process of the search results propagation.",
        "In the process of propagating reward backward, agents update estimates of their own potential utility value, generate an upto-dated policy and pass their updated results to the neighboring agents based on the algorithm described in Section 3.",
        "Upon change of expected utility value, agent Ai sends out its updated utility estimation to its neighbors so that they can act upon the changed expected utility and corresponding state.",
        "This update message includes the potential reward as well as the corresponding state QSi = (qk, ttll) of agent Ai.",
        "Each neighboring agent, Aj, reacts to this kind of update message by updating the expected utility value for state QSj(qk, ttll + 1) according to the newly-announced changed expected utility value.",
        "Once they complete the update, the agents would again in turn inform related neighbors to update their values.",
        "This process goes on until the TTL value in the update message increases to the TTL limit.",
        "To speed up the learning process, while updating the expected utility values of an agent Ais neighboring agents we specify that Um(Qk, ttl0) >= Um(Qk, ttl1) iff ttl0 > ttl1 Thus, when agent Ai receives an updated expected utility value with ttl1, it also updates the expected utility values with any ttl0 > ttl1 if Um(Qk, ttl0) < Um(Qk, ttl1) to speed up convergence.",
        "This heuristic is based on the fact that the utility of a search session is a non-decreasing function of time t. 3.3 Discussion In formalizing the content routing system as a learning task, many assumptions are made.",
        "In real systems, these assumptions may not hold, and thus the learning algorithm may not converge.",
        "Two problems are of particular note, (1) This content routing problem does not have Markov properties.",
        "In contrast to IP-level based packet routing, the routing decision of each agent for a particular search session sj depends on the routing history of sj.",
        "Therefore, the assumption that all subsequent search sessions are independent does not hold in reality.",
        "This may lead to double counting problem that the relevant documents of some agents will be counted more than once for the state where the TTL value is more than 1.",
        "However, in the context of the hierarchical agent organizations, two factors mitigate this problems: first, the agents in each content group form a tree-like structure.",
        "With the absense of the cycles, the estimates inside the tree would be close to the accurate value.",
        "Secondly, the stochastic nature of the routing policy partly remedies this problem.",
        "The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 235 (2) Another challenge for this learning algorithm is that in a real network environment observations on neighboring agents may not be able to be updated in time due to the communication delay or other situations.",
        "In addition, when neighboring agents update their estimates at the same time, oscillation may arise during the learning process[1].",
        "This paper explores several approaches to speed up the learning process.",
        "Besides the aforementioned strategy of updating the expected utility values, we also employ an active update strategy where agents notify their neighbors whenever its expected utility is updated.",
        "Thus a faster convergence speed can be achieved.",
        "This strategy contrasts to the Lazy update, where agents only echo their neighboring agents with their expected utility change when they exchange information.",
        "The trade off between the two approaches is the network load versus learning speed.",
        "The advantage of this learning algorithm is that once a routing policy is learned, agents do not have to repeatedly compare the similarity of queries as long as the network topology remains unchanged.",
        "Instead, agent just have to determine the classification of the query properly and follow the learned policies.",
        "The disadvantage of this learning-based approach is that the learning process needs to be conducted whenever the network structure changes.",
        "There are many potential extensions for this learning model.",
        "For example, a single measure is currently used to indicate the traffic load for an agents neighborhood.",
        "A simple extension would be to keep track of individual load for each neighbor of the agent. 4.",
        "EXPERIMENTSSETTINGSAND RESULTS The experiments are conducted on TRANO simulation toolkit with two sets of datasets, TREC-VLC-921 and TREC123-100.",
        "The following sub-sections introduce the TRANO testbed, the datasets, and the experimental results. 4.1 TRANO Testbed TRANO (Task Routing on Agent Network Organization) is a multi-agent based network based information retrieval testbed.",
        "TRANO is built on top of the Farm [4], a time based distributed simulator that provides a data dissemination framework for large scale distributed agent network based organizations.",
        "TRANO supports importation and exportation of agent organization profiles including topological connections and other features.",
        "Each TRANO agent is composed of an agent view structure and a control unit.",
        "In simulation, each agent is pulsed regularly and the agent checks the incoming message queues, performs local operations and then forwards messages to other agents . 4.2 Experimental Settings In our experiment, we use two standard datasets, TRECVLC-921 and TREC-123-100 datasets, to simulate the collections hosted on agents.",
        "The TREC-VLC-921 and TREC123-100 datasets were created by the U.S. National Institute for Standard Technology(NIST) for its TREC conferences.",
        "In distributed information retrieval domain, the two data collections are split to 921 and 100 sub-collections.",
        "It is observed that dataset TREC-VLC-921 is more heterogeneous than TREC-123-100 in terms of source, document length, and relevant document distribution from the statistics of the two data collections listed in [13].",
        "Hence, TREC-VLC-921 is much closer to real document distributions in P2P environments.",
        "Furthermore, TREC-123-100 is split into two sets of 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0 500 1000 1500 2000 2500 3000 ARSS Query number ARSS versus the number of incoming queries for TREC-VLC-921 SSLA-921 SSNA-921 Figure 3: ARSS(Average reward per search session) versus the number of search sessions for 1phase search in TREC-VLC-921 0 0.1 0.2 0.3 0.4 0.5 0.6 0 500 1000 1500 2000 2500 3000 ARSS Query number ARSS versus query number for TREC-VLC-921 TSLA-921 TSNA-921 Figure 4: ARSS(Average reward per search session) versus the number of search sessions for 2phase search in TREC-VLC-921 sub-collections in two ways: randomly and by source.",
        "The two partitions are denoted as TREC-123-100-Random and TREC-123-100-Source respectively.",
        "The documents in each subcollection in dataset TREC-123-100-Source are more coherent than those in TREC-123-100-Random.",
        "The two different sets of partitions allow us to observe how the distributed learning algorithm is affected by the homogeneity of the collections.",
        "The hierarchical agent organization is generated by the algorithm described in our previous algorithm [15].",
        "During the topology generation process, degree information of each agent is estimated by the algorithm introduced by Palmer et al. [9] with parameters α = 0.5 and β = 0.6.",
        "In our experiments, we estimate the upward limit and downward degree limit using linear discount factors 0.5, 0.8 and 1.0.",
        "Once the topology is built, queries randomly selected from the query set 301−350 on TREC-VLC-921 and query set 1− 50 on TREC-123-100-Random and TREC-123-100-Source are injected to the system based on a Poisson distribution P(N(t) = n) = (λt)n n! e−λ 236 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 0 50 100 150 200 250 300 350 400 0 500 1000 1500 2000 2500 3000 Cumulativeutility Query number Cumulative utility over the number of incoming queries TSLA-921 SSNA-921 SSLA-921 TSNA-921 Figure 5: The cumulative utility versus the number of search sessions TREC-VLC-921 In addition, we assume that all agents have an equal chance of getting queries from the environment, i.e, λ is the same for every agent.",
        "In our experiments, λ is set as 0.0543 so that the mean of the incoming queries from the environment to the agent network is 50 per time unit.",
        "The service time for the communication queue and local search queue, i.e tQij and trs, is set as 0.01 time unit and 0.05 time units respectively.",
        "In our experiments, there are ten types of queries acquired by clustering the query set 301 − 350 and 1 − 50. 4.3 Results analysis and evaluation Figure 3 demonstrates the ARSS(Average Reward per Search Session) versus the number of incoming queries over time for the the Single-Step based Non-learning Algorithm (SSNA), and the Single-Step Learning Algorithm(SSLA) for data collection TREC-VLC-921.",
        "It shows that the average reward for SSNA algorithm ranges from 0.02 − 0.06 and the performance changes little over time.",
        "The average reward for SSLA approach starts at the same level with the SSNA algorithm.",
        "But the performance increases over time and the average performance gain stabilizes at about 25% after query range 2000 − 3000.",
        "Figure 4 shows the ARSS(Average Reward per Search Session) versus the number of incoming queries over time for the the Two-Step based Non-learning Algorithm(TSNA), and the Two-Step Learning Algorithm(TSLA) for data collection TREC-VLC-921.",
        "The TSNA approach has a relatively consistent performance with the average reward ranges from 0.05 − 0.15.",
        "The average reward for TSLA approach, where learning algorithm is exploited, starts at the same level with the TSNA algorithm and improves the average reward over time until 2000−2500 queries joining the system.",
        "The results show that the average performance gain for TSLA approach over TNLA approach is 35% after stabilization.",
        "Figure 5 shows the cumulative utility versus the number of incoming queries over time for SSNA, SSLA,TSNA, and TSLA respectively.",
        "It illustrates that the cumulative utility of non-learning algorithms increases largely linearly over time, while the gains of learning-based algorithms accelerate when more queries enter the system.",
        "These experimental results demonstrate that learning-based approaches consistently perform better than non-learning based routing algorithm.",
        "Moreover, two-phase learning based algorithm is better than single-phase based learning algorithm because the maximal reward an agent can receive from searching its neighborhood within TTL hops is related to the total number of the relevant documents in that area.",
        "Thus, even the optimal routing policy can do little beyond reaching these relevant documents faster.",
        "On the contrary, the two-stepbased learning algorithm can relocate the search session to a neighborhood with more relevant documents.",
        "The TSLA combines the merits of both approaches and outperforms them.",
        "Table 1 lists the cumulative utility for datasets TREC123-100-Random and TREC-123-100-Source with hierarchical organizations.",
        "The five columns show the results for four different approaches.",
        "In particular, column TSNA-Random shows the results for dataset TREC-123-100-Random with the TSNA approach.",
        "The column TSLA-Random shows the results for dataset TREC-123-100-Random with the TSLA approach.",
        "There are two numbers in each cell in the column TSLA-Random.",
        "The first number is the actual cumulative utility while the second number is the percentage gain in terms of the utility over TSNA approach.",
        "Columns TSNA-Source and TSLA-Source show the results for dataset TREC-123-100-Source with TSNA and TSLA approaches respectively.",
        "Table 1 shows that the performance improvement for TREC-123-100-Random is not as significant as the other datasets.",
        "This is because that the documents in the sub-collection of TREC-123-100-Random are selected randomly which makes the collection model, the signature of the collection, less meaningful.",
        "Since both algorithms are designed based on the assumption that document collections can be well represented by their collection model, this result is not surprising.",
        "Overall, Figures 4, 5, and Table 1 demonstrate that the reinforcement learning based approach can considerably enhance the system performance for both data collections.",
        "However, it remains as future work to discover the correlation between the magnitude of the performance gains and the size of the data collection and/or the extent of the heterogeneity between the sub-collections. 5.",
        "RELATED WORK The content routing problem differs from the networklevel routing in packet-switched communication networks in that content-based routing occurs in application-level networks.",
        "In addition, the destination agents in our contentrouting algorithms are multiple and the addresses are not known in the routing process.",
        "IP-level Routing problems have been attacked from the reinforcement learning perspective[2, 5, 11, 12].",
        "These studies have explored fully distributed algorithms that are able, without central coordination to disseminate knowledge about the network, to find the shortest paths robustly and efficiently in the face of changing network topologies and changing link costs.",
        "There are two major classes of adaptive, distributed packet routing algorithms in the literature: distance-vector algorithms and link-state algorithms.",
        "While this line of studies carry a certain similarity with our work, it has mainly focused on packet-switched communication networks.",
        "In this domain, the destination of a packet is deterministic and unique.",
        "Each agent maintains estimations, probabilistically or deterministically, on the distance to a certain destination through its neighbors.",
        "A variant of Q-Learning techniques is deployed The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 237 Table 1: Cumulative Utility for Datasets TREC-123-100-Random and TREC-123-100-Source with Hierarchical Organization; The percentage numbers in the columns TSLA-Random and TSLA-Source demonstrate the performance gain over the algorithm without learning Query number TSNA-Random TSLA-Random TSNA-Source TSLA-Source 500 25.15 28.45 13% 24.00 21.05 -13% 1000 104.99 126.74 20% 93.95 96.44 2.6% 1250 149.79 168.40 12% 122.64 134.05 9.3% 1500 188.94 211.05 12% 155.30 189.60 22% 1750 235.49 261.60 11% 189.14 243.90 28% 2000 275.09 319.25 16% 219.0 278.80 26% to update the estimations to converge to the real distances.",
        "It has been discovered that the locality property is an important feature of information retrieval systems in user modeling studies[3].",
        "In P2P based content sharing systems, this property is exemplified by the phenomenon that users tend to send queries that represent only a limited number of topics and conversely, users in the same neighborhood are likely to share common interests and send similar queries [10].",
        "The learning based approach is perceived to be more beneficial for real distributed information retrieval systems which exhibit locality property.",
        "This is because the users traffic and query patterns can reduce the state space and speed up the learning process.",
        "Related work in taking advantage of this property include [7], where the authors attempted to address this problem by user modeling techniques. 6.",
        "CONCLUSIONS In this paper, a reinforcement-learning based approach is developed to improve the performance of distributed IR search algorithms.",
        "Particularly, agents maintain estimates, namely expected utility, on the downstream agents ability to provide relevant documents for incoming queries.",
        "These estimates are updated gradually by learning from the feedback information returned from previous search sessions.",
        "Based on the updated expected utility information, the agents modify their routing policies.",
        "Thereafter, these agents route the queries based on the learned policies and update the estimates on the expected utility based on the new routing policies.",
        "The experiments on two different distributed IR datasets illustrates that the reinforcement learning approach improves considerably the cumulative utility over time. 7.",
        "REFERENCES [1] S. Abdallah and V. Lesser.",
        "Learning the task allocation game.",
        "In AAMAS 06: Proceedings of the fifth international joint conference on Autonomous agents and multiagent systems, pages 850-857, New York, NY, USA, 2006.",
        "ACM Press. [2] J.",
        "A. Boyan and M. L. Littman.",
        "Packet routing in dynamically changing networks: A reinforcement learning approach.",
        "In Advances in Neural Information Processing Systems, volume 6, pages 671-678.",
        "Morgan Kaufmann Publishers, Inc., 1994. [3] J. C. French, A. L. Powell, J. P. Callan, C. L. Viles, T. Emmitt, K. J. Prey, and Y. Mou.",
        "Comparing the performance of database selection algorithms.",
        "In Research and Development in Information Retrieval, pages 238-245, 1999. [4] B. Horling, R. Mailler, and V. Lesser.",
        "Farm: A scalable environment for multi-agent development and evaluation.",
        "In Advances in Software Engineering for Multi-Agent Systems, pages 220-237, Berlin, 2004.",
        "Springer-Verlag. [5] M. Littman and J. Boyan.",
        "A distributed reinforcement learning scheme for network routing.",
        "In Proceedings of the International Workshop on Applications of Neural Networks to Telecommunications, 1993. [6] J. Lu and J. Callan.",
        "Federated search of text-based digital libraries in hierarchical peer-to-peer networks.",
        "In In ECIR05, 2005. [7] J. Lu and J. Callan.",
        "User modeling for full-text federated search in peer-to-peer networks.",
        "In ACM SIGIR 2006.",
        "ACM Press, 2006. [8] C. D. Manning and H. Sch¨utze.",
        "Foundations of Statistical Natural Language Processing.",
        "The MIT Press, Cambridge, Massachusetts, 1999. [9] C. R. Palmer and J. G. Steffan.",
        "Generating network topologies that obey power laws.",
        "In Proceedings of GLOBECOM 2000, November 2000. [10] K. Sripanidkulchai, B. Maggs, and H. Zhang.",
        "Efficient content location using interest-based locality in peer-topeer systems.",
        "In INFOCOM, 2003. [11] D. Subramanian, P. Druschel, and J. Chen.",
        "Ants and reinforcement learning: A case study in routing in dynamic networks.",
        "In In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence, pages 832-839, 1997. [12] J. N. Tao and L. Weaver.",
        "A multi-agent, policy gradient approach to network routing.",
        "In In Proceedings of the Eighteenth International Conference on Machine Learning, 2001. [13] H. Zhang, W. B. Croft, B. Levine, and V. Lesser.",
        "A multi-agent approach for peer-to-peer information retrieval.",
        "In Proceedings of Third International Joint Conference on Autonomous Agents and Multi-Agent Systems, July 2004. [14] H. Zhang and V. Lesser.",
        "Multi-agent based peer-to-peer information retrieval systems with concurrent search sessions.",
        "In Proceedings of the Fifth International Joint Conference on Autonomous Agents and Multi-Agent Systems, May 2006. [15] H. Zhang and V. R. Lesser.",
        "A dynamically formed hierarchical agent organization for a distributed content sharing system.",
        "In 2004 IEEE/WIC/ACM International Conference on Intelligent Agent Technology (IAT 2004), 20-24 September 2004, Beijing, China, pages 169-175.",
        "IEEE Computer Society, 2004. 238 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
    ],
    "translated_text_sentences": [
        "Un algoritmo de búsqueda distribuida basado en aprendizaje por refuerzo para sistemas jerárquicos de recuperación de información entre pares.",
        "En estos enfoques, los agentes dependen de la similitud de contenido entre las consultas entrantes y sus agentes vecinos directos para dirigir las sesiones de búsqueda distribuida.",
        "Sin embargo, tal heurística es miope en el sentido de que los agentes vecinos pueden no estar conectados con agentes más relevantes.",
        "En este documento, se desarrolla un enfoque basado en aprendizaje por refuerzo en línea para aprovechar las características dinámicas en tiempo de ejecución de los sistemas de recuperación de información P2P, representadas por información sobre sesiones de búsqueda pasadas.",
        "Específicamente, los agentes mantienen estimaciones sobre las habilidades de los agentes aguas abajo para proporcionar documentos relevantes para las consultas entrantes.",
        "Estas estimaciones se actualizan gradualmente al aprender de la información de retroalimentación devuelta de sesiones de búsqueda anteriores.",
        "Basándose en esta información, los agentes derivan políticas de enrutamiento correspondientes.",
        "Posteriormente, estos agentes dirigen las consultas basándose en las políticas aprendidas y actualizan las estimaciones según las nuevas políticas de enrutamiento.",
        "Los resultados experimentales demuestran que el algoritmo de aprendizaje mejora considerablemente el rendimiento del enrutamiento en dos conjuntos de colecciones de pruebas que han sido utilizados en una variedad de estudios de recuperación de información distribuida.",
        "Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagente Términos Generales Algoritmos, Rendimiento, Experimentación 1.",
        "En los últimos años ha habido un creciente interés en estudiar cómo controlar los procesos de búsqueda en sistemas de recuperación de información (IR) basados en pares (P2P) [6, 13, 14, 15].",
        "En esta línea de investigación, uno de los problemas centrales que preocupa a los investigadores es dirigir de manera eficiente las consultas de los usuarios en la red a los agentes que poseen los documentos apropiados.",
        "En ausencia de información global, las estrategias dominantes para abordar este problema son enfoques basados en la similitud de contenido [6, 13, 14, 15].",
        "Si bien la similitud de contenido entre consultas y nodos locales parece ser un indicador creíble para la cantidad de documentos relevantes que residen en cada nodo, estos enfoques están limitados por una serie de factores.",
        "En primer lugar, las métricas basadas en similitud pueden ser miope ya que los nodos localmente relevantes pueden no estar conectados a otros nodos relevantes.",
        "Segundo, los enfoques basados en la similitud no tienen en cuenta las características de tiempo de ejecución de los sistemas de IR P2P, incluidos los parámetros ambientales, el uso del ancho de banda y la información histórica de las sesiones de búsqueda pasadas, que proporcionan información valiosa para los algoritmos de enrutamiento de consultas.",
        "En este artículo, desarrollamos un enfoque de IR basado en aprendizaje por refuerzo para mejorar el rendimiento de los algoritmos de búsqueda de IR distribuidos.",
        "Los agentes pueden adquirir mejores estrategias de búsqueda al recopilar y analizar información de retroalimentación de sesiones de búsqueda anteriores.",
        "En particular, los agentes mantienen estimaciones, es decir, utilidad esperada, sobre las capacidades de los agentes aguas abajo de proporcionar documentos relevantes para tipos específicos de consultas entrantes.",
        "Estas estimaciones se actualizan gradualmente al aprender de la información de retroalimentación devuelta de sesiones de búsqueda anteriores.",
        "Basándose en la información actualizada de utilidad esperada, los agentes derivan políticas de enrutamiento correspondientes.",
        "Posteriormente, estos agentes dirigen las consultas basándose en las políticas aprendidas y actualizan las estimaciones sobre la utilidad esperada en función de las nuevas políticas de enrutamiento.",
        "Este proceso se lleva a cabo de manera iterativa.",
        "El objetivo del algoritmo de aprendizaje, aunque consume ancho de banda de red, es reducir el tiempo de enrutamiento para procesar más consultas por unidad de tiempo y al mismo tiempo encontrar más documentos relevantes.",
        "Esto contrasta con los enfoques basados en la similitud de contenido, donde se repiten operaciones similares para cada consulta entrante y el tiempo de procesamiento se mantiene en gran medida constante con el tiempo.",
        "Otra forma de ver este documento es que nuestro enfoque básico para la búsqueda de IR distribuida es construir una red de superposición jerárquica (organización de agentes) basada en la medida de similitud de contenido entre las colecciones de documentos de los agentes de manera ascendente.",
        "En trabajos anteriores, hemos demostrado que esta organización mejora significativamente el rendimiento de búsqueda.",
        "Sin embargo, esta estructura organizativa no tiene en cuenta los patrones de llegada de las consultas, incluyendo su frecuencia, tipos y dónde entran en el sistema, ni el ancho de banda de comunicación disponible en la red y las capacidades de procesamiento de los agentes individuales.",
        "La intención del aprendizaje por refuerzo es adaptar las decisiones de enrutamiento de los agentes a las situaciones dinámicas de la red y aprender de sesiones de búsqueda pasadas.",
        "Específicamente, las contribuciones de este artículo incluyen: (1) un enfoque basado en aprendizaje por refuerzo para que los agentes adquieran políticas de enrutamiento satisfactorias basadas en estimaciones de la contribución potencial de sus agentes vecinos; (2) dos estrategias para acelerar el proceso de aprendizaje.",
        "Según nuestro mejor conocimiento, esta es una de las primeras aplicaciones de aprendizaje por refuerzo para abordar problemas de compartición de contenido distribuido, lo cual indica algunos de los problemas al aplicar el refuerzo en una aplicación compleja.",
        "El resto de este documento está organizado de la siguiente manera: la Sección 2 revisa los sistemas jerárquicos de intercambio de contenido y el algoritmo de búsqueda de dos fases basado en dicha topología.",
        "La Sección 3 describe un enfoque basado en aprendizaje por refuerzo para dirigir el proceso de enrutamiento; la Sección 4 detalla la configuración experimental y analiza los resultados.",
        "La sección 5 discute estudios relacionados y la sección 6 concluye el artículo.",
        "BÚSQUEDA EN SISTEMAS JERÁRQUICOS DE IR P2P Esta sección revisa brevemente nuestros enfoques básicos para los sistemas jerárquicos de IR P2P.",
        "En un sistema jerárquico de recuperación de información P2P ilustrado en la Fig.1, los agentes están conectados entre sí a través de tres tipos de enlaces: enlaces ascendentes, enlaces descendentes y enlaces laterales.",
        "En las siguientes secciones, denotamos el conjunto de agentes que están directamente conectados al agente Ai como DirectConn(Ai), el cual se define como DirectConn(Ai) = NEI(Ai) ∪ PAR(Ai) ∪ CHL(Ai), donde NEI(Ai) es el conjunto de agentes vecinos conectados a Ai a través de enlaces laterales; PAR(Ai) es el conjunto de agentes a los que el agente Ai está conectado a través de enlaces ascendentes y CHL(Ai) es el conjunto de agentes a los que el agente Ai se conecta a través de enlaces descendentes.",
        "Estos enlaces se establecen a través de un proceso de agrupamiento distribuido basado en la similitud de contenido de abajo hacia arriba[15].",
        "Estos enlaces son luego utilizados por agentes para localizar otros agentes que contienen documentos relevantes a las consultas dadas.",
        "Un agente típico Ai en nuestro sistema utiliza dos colas: una cola de búsqueda local, LSi, y una cola de reenvío de mensajes MFi.",
        "Los estados de las dos colas constituyen los estados internos de un agente.",
        "La cola de búsqueda local LSi almacena sesiones de búsqueda programadas para procesamiento local.",
        "Es una cola de prioridad y el agente Ai siempre selecciona las consultas más prometedoras para procesarlas con el fin de maximizar la utilidad global.",
        "MFi consiste en un conjunto de consultas para reenviar y se procesa de manera FIFO (primero en entrar, primero en salir).",
        "Para la primera consulta en MFi, el agente Ai determina a qué subconjunto de sus agentes vecinos reenviarla basándose en la política de enrutamiento de los agentes πi.",
        "Estas decisiones de enrutamiento determinan cómo se lleva a cabo el proceso de búsqueda en la red.",
        "En este documento, llamamos a Ai como el agente aguas arriba de Ajs y a Aj como el agente aguas abajo de Ais si A4 A5 A6 A7 A2 A3 A9 NEI(A2)={A3} PAR(A2)={A1} CHL(A2)={A4,A5} A1 A8 Figura 1: Una fracción de un sistema jerárquico P2PIR en el que un agente Ai dirige una consulta al agente Aj.",
        "El protocolo de búsqueda distribuida de nuestra organización de agentes jerárquicos está compuesto por dos pasos.",
        "En el primer paso, al recibir una consulta qk en el tiempo tl de un usuario, el agente Ai inicia una sesión de búsqueda si al sondear a sus agentes vecinos Aj ∈ NEI(Ai) con el mensaje PROBE para el valor de similitud Sim(qk, Aj) entre qk y Aj.",
        "Aquí, Ai se define como el iniciador de la consulta de la sesión de búsqueda si.",
        "En el segundo paso, Ai selecciona un grupo de agentes más prometedores para iniciar el proceso de búsqueda real con el mensaje BUSCAR.",
        "Estos mensajes de BÚSQUEDA contienen un parámetro TTL (Tiempo de Vida) además de la consulta.",
        "El valor TTL disminuye en 1 después de cada salto.",
        "En el proceso de búsqueda, los agentes descartan aquellas consultas que han sido procesadas previamente o cuyo TTL disminuye a 0, lo que evita que las consultas se repitan infinitamente en el sistema.",
        "La sesión de búsqueda termina cuando todos los agentes que reciben la consulta la descartan o el TTL disminuye a 0.",
        "Tras recibir mensajes de BÚSQUEDA para qk, los agentes programan actividades locales que incluyen búsqueda local, reenvío de qk a sus vecinos y devolución de resultados de búsqueda al iniciador de la consulta.",
        "Este proceso y los algoritmos relacionados se detallan en [15, 14]. 3.",
        "En el enfoque de búsqueda basado en aprendizaje por refuerzo, mencionado anteriormente, las decisiones de enrutamiento de un agente Ai dependen de la comparación de similitud entre las consultas entrantes y los agentes vecinos de Ai para reenviar esas consultas a agentes relevantes sin inundar la red con mensajes de consulta innecesarios.",
        "Sin embargo, esta heurística es miope porque un vecino directo relevante no necesariamente está conectado con otros agentes relevantes.",
        "En esta sección, proponemos un enfoque más general al plantear este problema como una tarea de aprendizaje por refuerzo.",
        "En busca de una mayor flexibilidad, los agentes pueden alternar entre dos modos: modo de aprendizaje y modo sin aprendizaje.",
        "En el modo de no aprendizaje, los agentes operan de la misma manera que lo hacen en los procesos de búsqueda distribuida normales descritos en [14, 15].",
        "Por otro lado, en el modo de aprendizaje, en paralelo con las sesiones de búsqueda distribuida, los agentes también participan en un proceso de aprendizaje que se detallará en esta sección.",
        "Ten en cuenta que en el protocolo de aprendizaje, el proceso de aprendizaje no interfiere con el proceso de búsqueda distribuida.",
        "Los agentes pueden elegir iniciar y detener procesos de aprendizaje sin afectar el rendimiento del sistema.",
        "En particular, dado que el proceso de aprendizaje consume recursos de red (especialmente ancho de banda), los agentes pueden optar por iniciar el aprendizaje solo cuando la carga de la red es relativamente baja, minimizando así los costos adicionales de comunicación incurridos por el algoritmo de aprendizaje.",
        "La sección está estructurada de la siguiente manera, la Sección 3.1 describe 232 The Sixth Intl.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) un modelo basado en aprendizaje por refuerzo.",
        "La sección 3.2 describe un protocolo para implementar el algoritmo de aprendizaje en la red.",
        "La sección 3.3 discute la convergencia del algoritmo de aprendizaje. 3.1 El Modelo La política de enrutamiento de un agente toma el estado de una sesión de búsqueda como entrada y produce las acciones de enrutamiento para esa consulta.",
        "En nuestro trabajo, el estado de una sesión de búsqueda sj se estipula como: QSj = (qk, ttlj) donde ttlj es el número de saltos que quedan para la sesión de búsqueda sj, qk es la consulta específica.",
        "QL es un atributo de qk que indica a qué tipo de consultas qk probablemente pertenecen.",
        "El conjunto de QL puede ser generado ejecutando un algoritmo de clasificación en línea simple en todas las consultas que han sido procesadas por los agentes, o un algoritmo fuera de línea en un conjunto de entrenamiento pre-designado.",
        "La suposición aquí es que el conjunto de tipos de consultas se aprende de antemano y pertenece al conocimiento común de los agentes en la red.",
        "El trabajo futuro incluye explorar cómo se puede lograr el aprendizaje cuando esta suposición no se cumple.",
        "Dado el conjunto de tipos de consultas establecido, una consulta entrante qi puede ser clasificada en una clase de consulta Q(qi) mediante la fórmula: Q(qi) = arg max Qj P(qi|Qj) (1) donde P(qi|Qj) indica la probabilidad de que la consulta qi sea generada por la clase de consulta Qj [8].",
        "El conjunto de acciones de enrutamiento atómico de un agente Ai se denota como {αi}, donde {αi} se define como αi = {αi0 , αi1 , ..., αin }.",
        "Un elemento αij representa una acción para dirigir una consulta dada al agente vecino Aij ∈ DirectConn(Ai).",
        "La política de enrutamiento πi del agente Ai es estocástica y su resultado para una sesión de búsqueda con estado QSj se define como: πi(QSj) = {(αi0 , πi(QSi, αi0 )), (αi1 , πi(QSi, αi1 )), ...} (2) Nótese que el operador πi está sobrecargado para representar tanto la política probabilística para una sesión de búsqueda con estado QSj, denotada como πi(QSj); como la probabilidad de enviar la consulta a un agente vecino específico Aik ∈ DirectConn(Ai) bajo la política πi(QSj), denotada como πi(QSj, αik).",
        "Por lo tanto, la ecuación (2) significa que la probabilidad de reenviar la sesión de búsqueda al agente Ai0 es πi(QSi, αi0) y así sucesivamente.",
        "Bajo esta política estocástica, la acción de enrutamiento es no determinística.",
        "La ventaja de esta estrategia es que los mejores agentes vecinos no serán seleccionados repetidamente, mitigando así las posibles situaciones de puntos calientes.",
        "La utilidad esperada, Un i (QSj), se utiliza para estimar la ganancia potencial de utilidad al dirigir el tipo de consulta QSj al agente Ai bajo la política πn i.",
        "El superíndice n indica el valor en la n-ésima iteración en un proceso de aprendizaje iterativo.",
        "La utilidad esperada proporciona orientación de enrutamiento para futuras sesiones de búsqueda.",
        "En el proceso de búsqueda, cada agente Ai mantiene observaciones parciales de los estados de sus vecinos, como se muestra en la Figura 2.",
        "La observación parcial incluye información no local como la estimación de la utilidad potencial de su vecino Am para el estado de consulta QSj, denotado como Um(QSj), así como la información de carga, Lm.",
        "Estas observaciones son actualizadas periódicamente por los vecinos.",
        "La información de utilidad estimada se utilizará para actualizar la utilidad esperada de A en su política de enrutamiento.",
        "Cargar información de utilidad esperada para diferentes tipos de consultas de agentes vecinos... A0 A1 A3 A2 Un 0 (QS0) ... ... ... ... ...... Un 0 (QS1) Un 1 (QS1) Un 2 (QS1) Un 3 (QS1) Un 1 (QS0) Un 2 (QS0) Un 3 (QS0) Ln 0 Ln 1 Ln 2 Ln 3 ... QS0 QS1 ...",
        "Figura 2: La Observación Parcial del Agente Ais sobre sus vecinos (A0, A1...) La información de carga de Am, Lm, se define como Lm = |MFm| Cm, donde |MFm| es la longitud de la cola de mensajes en espera y Cm es la tasa de servicio de la cola de mensajes en espera del agente Am.",
        "Por lo tanto, Lm caracteriza la utilización de un canal de comunicación de agentes, y así proporciona información no local a los vecinos de Ams para ajustar los parámetros de su política de enrutamiento y evitar inundar a sus agentes aguas abajo.",
        "Ten en cuenta que, basado en las características de las consultas que ingresan al sistema y las capacidades de los agentes, la carga de trabajo de los agentes puede no ser uniforme.",
        "Después de recopilar la información de la tasa de utilización de todos sus vecinos, el agente Ai calcula Li como una medida única para evaluar la condición de carga promedio de su vecindario: Li = P k Lk |DirectConn(Ai)| Los agentes explotan el valor de Li para determinar la probabilidad de enrutamiento en su política de enrutamiento.",
        "Se debe tener en cuenta que, como se describe en la Sección 3.2, la información sobre agentes vecinos se transmite junto con el mensaje de consulta propagado entre los agentes siempre que sea posible para reducir la sobrecarga de tráfico. 3.1.1 Actualización de la Política Se introduce un proceso de actualización iterativo para que los agentes aprendan una política de enrutamiento estocástica satisfactoria.",
        "En este proceso iterativo, los agentes actualizan sus estimaciones sobre la utilidad potencial de sus políticas de enrutamiento actuales y luego propagan las estimaciones actualizadas a sus vecinos.",
        "Sus vecinos luego generan una nueva política de enrutamiento basada en la observación actualizada y a su vez calculan la utilidad esperada basada en las nuevas políticas y continúan este proceso iterativo.",
        "En particular, en el tiempo n, dado un conjunto de utilidad esperada, un agente Ai, cuyo conjunto de agentes directamente conectados es DirectConn(Ai) = {Ai0, ..., Aim}, determina su política de enrutamiento estocástico correspondiente para una sesión de búsqueda del estado QSj basada en los siguientes pasos: (1) Ai primero selecciona un subconjunto de agentes como los agentes potenciales aguas abajo del conjunto DirectConn(Ai), denotado como PDn(Ai, QSj).",
        "El tamaño del agente potencial aguas abajo se especifica como |PDn(Ai, QSj)| = min(|NEI(Ai), dn i + k)| donde k es una constante y se establece en 3 en este documento; dn i, el ancho hacia adelante, se define como el número esperado de The Sixth Intl.",
        "La fórmula especifica que el conjunto potencial de agentes aguas abajo PDn(Ai, QSj) es o bien el subconjunto de agentes vecinos con los k valores de utilidad esperada más altos para el estado QSj entre todos los agentes en DirectConn(Ai), o todos sus agentes vecinos.",
        "La k se introduce basada en la idea de una política de enrutamiento estocástica y hace que la probabilidad de reenvío del agente dn i + k más alto sea inferior al 100%.",
        "Ten en cuenta que si queremos limitar el número de agentes secundarios para la sesión de búsqueda sj a 5, la probabilidad de reenviar la consulta a todos los agentes vecinos debe sumar 5.",
        "Configurar adecuadamente el valor de dn i puede mejorar la tasa de utilización del ancho de banda de la red cuando gran parte de la red está inactiva, al mismo tiempo que mitiga la carga de tráfico cuando la red está altamente cargada.",
        "El valor de dn+1 i se actualiza en función de dn i, las observaciones anteriores y actuales sobre la situación del tráfico en el vecindario.",
        "Específicamente, la fórmula de actualización para dn+1 i es dn+1 i = dn i ∗ (1 + 1 − Li |DirectConn(Ai)| ) En esta fórmula, el ancho de avance se actualiza en función de las condiciones de tráfico del vecindario del agente Ai, es decir, Li, y su valor anterior. (2) Para cada agente Aik en el PDn(Ai, QSj), la probabilidad de reenviar la consulta a Aik se determina de la siguiente manera para asignar una mayor probabilidad de reenvío a los agentes vecinos con un valor de utilidad esperado más alto: πn+1 i (QSj, αik ) = dn+1 i |PDn(Ai, QSj)| + β ∗ ` Uik (QSj) − PDU(Ai, QSj) |PDn(Ai, QSj)| ´ (3) donde PDUn(Ai, QSj) = X o∈P Dn(Ai,QSj ) Uo(QSj) y QSj es el estado subsiguiente del agente Aik después de que el agente Ai reenvía la sesión de búsqueda con estado QSj a su agente vecino Aik; Si QSj = (qk, ttl0), entonces QSj = (qk, ttl0 − 1).",
        "En la fórmula 3, el primer término a la derecha de la ecuación, dn+1 i |P Dn(Ai,QSj )|, se utiliza para determinar la probabilidad de reenvío distribuyendo equitativamente el ancho de avance, dn+1 i, a los agentes en el conjunto PDn(Ai, QSj).",
        "El segundo término se utiliza para ajustar la probabilidad de ser elegido de manera que se favorezca a los agentes con valores de utilidad esperada más altos. β se determina de acuerdo a: β = min ` m − dn+1 i m ∗ umax − PDUn(Ai, QSj) , dn+1 i PDUn(Ai, QSj) − m ∗ umin ´ (4) donde m = |PDn(Ai, QSj)|, umax = max o∈P Dn(Ao,QSj ) Uo(QSj) y umin = min o∈P Dn(Ao,QSj ) Uo(QSj) Esta fórmula garantiza que el valor final de πn+1 i (QSj, αik ) esté bien definido, es decir, 0 ≤ πn+1 i (QSj, αik ) ≤ 1 y X i πn+1 i (QSj, αik ) = dn+1 i Sin embargo, esta solución no explora todas las posibilidades.",
        "Para equilibrar entre la explotación y la exploración, se adopta un enfoque λ-Greedy.",
        "En el enfoque λ-Greedy, además de asignar una probabilidad más alta a aquellos agentes con un valor de utilidad esperado más alto, como en la ecuación (3).",
        "Los agentes que parecen no ser buenas opciones también recibirán consultas basadas en una tasa de exploración dinámica.",
        "En particular, para agentes en el conjunto PDn(Ai, QSj), πn+1 i1 (QSj) se determina de la misma manera que arriba, con la única diferencia de que dn+1 i es reemplazado por dn+1 i ∗ (1 − λn).",
        "El ancho de banda de búsqueda restante se utiliza para el aprendizaje asignando la probabilidad λn de manera uniforme a los agentes Ai2 en el conjunto DirectConn(Ai) - PDn(Ai, QSj). πn+1 i2 (QSj, αik ) = dn+1 i ∗ λn |DirectConn(Ai) - PDn(Ai, QSj)| (5) donde PDn(Ai, QSj) ⊂ DirectConn(Ai).",
        "Se debe tener en cuenta que la tasa de exploración λ no es constante y disminuye con el tiempo.",
        "El λ se determina de acuerdo con la siguiente ecuación: λn+1 = λ0 ∗ e−c1n (6) donde λ0 es la tasa de exploración inicial, que es una constante; c1 también es una constante para ajustar la tasa de disminución de la tasa de exploración; n es la unidad de tiempo actual. 3.1.2 Actualización de la Utilidad Esperada Una vez que la política de enrutamiento en el paso n+1, πn+1 i, se determina según la fórmula anterior, el agente Ai puede actualizar su propia utilidad esperada, Un+1 i (QSi), basándose en la política de enrutamiento actualizada resultante de la fórmula 5 y los valores U actualizados de sus agentes vecinos.",
        "Bajo el supuesto de que después de que una consulta se envía a los vecinos de Ai, las sesiones de búsqueda subsiguientes son independientes, la fórmula de actualización es similar a la fórmula de actualización de Bellman en Q-Learning: Un+1 i (QSj) = (1 − θi) ∗ Un i (QSj) + θi ∗ (Rn+1 i (QSj) + X k πn+1 i (QSj, αik )Un k (QSj)) (7) donde QSj = (Qj, ttl − 1) es el siguiente estado de QSj = (Qj, ttl); Rn+1 i (QSj) es la recompensa local esperada para la clase de consulta Qk en el agente Ai bajo la política de enrutamiento πn+1 i; θi es el coeficiente para decidir cuánto peso se le da al valor antiguo durante el proceso de actualización: cuanto menor sea el valor de θi, se espera que el agente aprenda más rápido el valor real, mientras que mayor sea la volatilidad del algoritmo, y viceversa.",
        "Rn+1 (s) se actualiza de acuerdo con la siguiente ecuación: 234 The Sixth Intl.",
        "La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Rn+1 i (QSj) = Rn i (QSj) +γi ∗ (r(QSj) − Rn i (QSj)) ∗ P(qj|Qj ) (8) donde r(QSj) es la recompensa local asociada con la sesión de búsqueda.",
        "P(qj|Qj) indica qué tan relevante es la consulta qj para el tipo de consulta Qj, y γi es la tasa de aprendizaje para el agente Ai.",
        "Dependiendo de la similitud entre una consulta específica qi y su tipo de consulta correspondiente Qi, la recompensa local asociada con la sesión de búsqueda tiene un impacto diferente en la estimación de Rn i (QSj).",
        "En la fórmula anterior, este impacto se refleja en el coeficiente, el valor P(qj|Qj). Función de recompensa Después de que una sesión de búsqueda se detiene cuando sus valores TTL expiran, todos los resultados de búsqueda se devuelven al usuario y se comparan con la evaluación de relevancia.",
        "Suponiendo que el conjunto de resultados de búsqueda es SR, la recompensa Rew(SR) se define como: Rew(SR) = j 1 si |Rel(SR)| > c |Rel(SR)| c en caso contrario, donde SR es el conjunto de resultados de búsqueda devueltos, Rel(SR) es el conjunto de documentos relevantes en los resultados de búsqueda.",
        "Esta ecuación especifica que los usuarios otorgan una recompensa de 1.0 si el número de documentos relevantes devueltos alcanza un número predefinido c. De lo contrario, la recompensa es proporcional al número de documentos relevantes devueltos.",
        "La razón para establecer un valor de corte de este tipo es que la importancia de la proporción de recuperación disminuye con la abundancia de documentos relevantes en el mundo real, por lo tanto, los usuarios tienden a enfocarse solo en un número limitado de resultados buscados.",
        "Los detalles del protocolo de enrutamiento actual se introducirán en la Sección 3.2 cuando presentemos cómo se implementa el algoritmo de aprendizaje en sistemas reales. 3.2 Implementación del algoritmo de aprendizaje. Esta sección describe cómo se puede utilizar el algoritmo de aprendizaje en un proceso de búsqueda de una o dos fases.",
        "En el algoritmo de búsqueda de una sola fase, las sesiones de búsqueda comienzan desde los iniciadores de las consultas.",
        "Por el contrario, en el algoritmo de búsqueda de dos pasos, el iniciador de la consulta primero intenta buscar un punto de inicio más apropiado para la consulta introduciendo un paso exploratorio como se describe en la Sección 2.",
        "A pesar de la diferencia en la calidad de los puntos de partida, la mayor parte del proceso de aprendizaje para los dos algoritmos es en gran medida el mismo como se describe en los párrafos siguientes.",
        "Antes de que comience el aprendizaje, cada agente inicializa el valor de utilidad esperado para todos los posibles estados como 0.",
        "Posteriormente, al recibir una consulta, además de las operaciones normales descritas en la sección anterior, un agente Ai también configura un temporizador para esperar los resultados de búsqueda devueltos por sus agentes subordinados.",
        "Una vez que el temporizador expire o haya recibido respuesta de todos sus agentes aguas abajo, Ai fusiona y reenvía los resultados de búsqueda acumulados de sus agentes aguas abajo a su agente aguas arriba.",
        "Configurar el temporizador acelera el aprendizaje porque los agentes pueden evitar esperar demasiado tiempo a que los agentes aguas abajo devuelvan los resultados de búsqueda.",
        "Ten en cuenta que estos resultados detallados y la información correspondiente del agente seguirán almacenados en Ai hasta que la información de retroalimentación sea transmitida desde su agente aguas arriba y se pueda evaluar el rendimiento de sus agentes aguas abajo.",
        "La duración del temporizador está relacionada con el valor TTL.",
        "En este documento, configuramos el temporizador a ttimer = ttli ∗ 2 + tf, donde ttli ∗ 2 es la suma del tiempo de viaje de las consultas en la red, y tf es el período de tiempo esperado que los usuarios desearían esperar.",
        "Los resultados de la búsqueda serán finalmente devueltos al iniciador de la sesión de búsqueda A0.",
        "Serán comparados con la evaluación de relevancia proporcionada por los usuarios finales (como se describe en la sección del experimento, la evaluación de relevancia para el conjunto de consultas se proporciona junto con las colecciones de datos).",
        "La recompensa será calculada y propagada hacia atrás a los agentes a lo largo del camino por el que se pasaron los resultados de la búsqueda.",
        "Este es un proceso inverso de la propagación de los resultados de búsqueda.",
        "En el proceso de propagar la recompensa hacia atrás, los agentes actualizan las estimaciones de su propio valor potencial de utilidad, generan una política actualizada y transmiten sus resultados actualizados a los agentes vecinos basándose en el algoritmo descrito en la Sección 3.",
        "Tras el cambio del valor de utilidad esperado, el agente Ai envía su estimación de utilidad actualizada a sus vecinos para que puedan actuar según la utilidad esperada modificada y el estado correspondiente.",
        "Este mensaje de actualización incluye la recompensa potencial, así como el estado correspondiente QSi = (qk, ttll) del agente Ai.",
        "Cada agente vecino, Aj, reacciona a este tipo de mensaje de actualización actualizando el valor de utilidad esperado para el estado QSj(qk, ttll + 1) de acuerdo con el valor de utilidad esperado cambiado recién anunciado.",
        "Una vez que completen la actualización, los agentes informarían nuevamente a los vecinos relacionados para que actualicen sus valores.",
        "Este proceso continúa hasta que el valor TTL en el mensaje de actualización aumenta al límite de TTL.",
        "Para acelerar el proceso de aprendizaje, al actualizar los valores de utilidad esperada de un agente Ai con agentes vecinos, especificamos que Um(Qk, ttl0) >= Um(Qk, ttl1) si ttl0 > ttl1. Por lo tanto, cuando el agente Ai recibe un valor de utilidad esperada actualizado con ttl1, también actualiza los valores de utilidad esperada con cualquier ttl0 > ttl1 si Um(Qk, ttl0) < Um(Qk, ttl1) para acelerar la convergencia.",
        "Esta heurística se basa en el hecho de que la utilidad de una sesión de búsqueda es una función no decreciente del tiempo t. 3.3 Discusión Al formalizar el sistema de enrutamiento de contenido como una tarea de aprendizaje, se hacen muchas suposiciones.",
        "En sistemas reales, estas suposiciones pueden no cumplirse, por lo que el algoritmo de aprendizaje puede no converger.",
        "Dos problemas son de particular importancia, (1) Este problema de enrutamiento de contenido no tiene propiedades de Markov.",
        "A diferencia del enrutamiento de paquetes basado en el nivel de IP, la decisión de enrutamiento de cada agente para una sesión de búsqueda particular sj depende del historial de enrutamiento de sj.",
        "Por lo tanto, la suposición de que todas las sesiones de búsqueda posteriores son independientes no se cumple en la realidad.",
        "Esto puede llevar a un problema de doble conteo en el que los documentos relevantes de algunos agentes se contarán más de una vez para el estado donde el valor TTL es mayor que 1.",
        "Sin embargo, en el contexto de las organizaciones de agentes jerárquicos, dos factores mitigan estos problemas: primero, los agentes en cada grupo de contenido forman una estructura similar a un árbol.",
        "Con la ausencia de los ciclos, las estimaciones dentro del árbol estarían cerca del valor preciso.",
        "En segundo lugar, la naturaleza estocástica de la política de enrutamiento remedia parcialmente este problema.",
        "El Sexto Internacional.",
        "Otro desafío para este algoritmo de aprendizaje es que en un entorno de red real, las observaciones sobre agentes vecinos pueden no poder actualizarse a tiempo debido al retraso en la comunicación u otras situaciones.",
        "Además, cuando los agentes vecinos actualizan sus estimaciones al mismo tiempo, puede surgir oscilación durante el proceso de aprendizaje[1].",
        "Este documento explora varios enfoques para acelerar el proceso de aprendizaje.",
        "Además de la estrategia mencionada de actualizar los valores de utilidad esperada, también empleamos una estrategia de actualización activa donde los agentes notifican a sus vecinos cada vez que se actualiza su utilidad esperada.",
        "Por lo tanto, se puede lograr una velocidad de convergencia más rápida.",
        "Esta estrategia contrasta con la actualización perezosa, donde los agentes solo repiten a sus agentes vecinos con el cambio en la utilidad esperada cuando intercambian información.",
        "El compromiso entre los dos enfoques es la carga de red versus la velocidad de aprendizaje.",
        "La ventaja de este algoritmo de aprendizaje es que una vez que se aprende una política de enrutamiento, los agentes no tienen que comparar repetidamente la similitud de las consultas siempre y cuando la topología de la red permanezca sin cambios.",
        "En cambio, el agente solo tiene que determinar adecuadamente la clasificación de la consulta y seguir las políticas aprendidas.",
        "La desventaja de este enfoque basado en el aprendizaje es que el proceso de aprendizaje debe llevarse a cabo cada vez que la estructura de la red cambia.",
        "Hay muchas posibles extensiones para este modelo de aprendizaje.",
        "Por ejemplo, actualmente se utiliza una sola medida para indicar la carga de tráfico en un vecindario de agentes.",
        "Una extensión sencilla sería llevar un registro de la carga individual de cada vecino del agente. 4.",
        "CONFIGURACIÓN DE EXPERIMENTOS Y RESULTADOS Los experimentos se llevan a cabo en el kit de simulación TRANO con dos conjuntos de datos, TREC-VLC-921 y TREC123-100.",
        "Las siguientes subsecciones presentan el banco de pruebas TRANO, los conjuntos de datos y los resultados experimentales. 4.1 Banco de pruebas TRANO TRANO (Task Routing on Agent Network Organization) es un banco de pruebas de recuperación de información basado en red de agentes múltiples.",
        "TRANO se construye sobre el Farm [4], un simulador distribuido basado en el tiempo que proporciona un marco de diseminación de datos para organizaciones de redes de agentes distribuidos a gran escala.",
        "TRANO apoya la importación y exportación de perfiles de organizaciones de agentes, incluyendo conexiones topológicas y otras características.",
        "Cada agente de TRANO está compuesto por una estructura de vista de agente y una unidad de control.",
        "En la simulación, cada agente es pulsado regularmente y el agente verifica las colas de mensajes entrantes, realiza operaciones locales y luego reenvía mensajes a otros agentes. 4.2 Configuración Experimental En nuestro experimento, utilizamos dos conjuntos de datos estándar, los conjuntos de datos TRECVLC-921 y TREC-123-100, para simular las colecciones alojadas en los agentes.",
        "Los conjuntos de datos TREC-VLC-921 y TREC123-100 fueron creados por el Instituto Nacional de Tecnología Estándar de los Estados Unidos (NIST) para sus conferencias TREC.",
        "En el ámbito de la recuperación de información distribuida, las dos colecciones de datos se dividen en 921 y 100 subcolecciones.",
        "Se observa que el conjunto de datos TREC-VLC-921 es más heterogéneo que TREC-123-100 en términos de origen, longitud de documento y distribución de documentos relevantes a partir de las estadísticas de las dos colecciones de datos enumeradas en [13].",
        "Por lo tanto, TREC-VLC-921 está mucho más cerca de las distribuciones reales de documentos en entornos P2P.",
        "Además, TREC-123-100 se divide en dos conjuntos de 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0 500 1000 1500 2000 2500 3000 Número de consulta de ARSS frente al número de consultas entrantes para TREC-VLC-921 SSLA-921 SSNA-921 Figura 3: ARSS (Recompensa promedio por sesión de búsqueda) frente al número de sesiones de búsqueda para la búsqueda de 1 fase en TREC-VLC-921 0 0.1 0.2 0.3 0.4 0.5 0.6 0 500 1000 1500 2000 2500 3000 Número de consulta de ARSS frente al número de consultas para TREC-VLC-921 TSLA-921 TSNA-921 Figura 4: ARSS (Recompensa promedio por sesión de búsqueda) frente al número de sesiones de búsqueda para la búsqueda de 2 fases en TREC-VLC-921 subcolecciones de dos formas: aleatoriamente y por fuente.",
        "Las dos particiones se denominan TREC-123-100-Random y TREC-123-100-Source respectivamente.",
        "Los documentos en cada subcolección en el conjunto de datos TREC-123-100-Source son más coherentes que los de TREC-123-100-Random.",
        "Los dos conjuntos diferentes de particiones nos permiten observar cómo el algoritmo de aprendizaje distribuido se ve afectado por la homogeneidad de las colecciones.",
        "La organización jerárquica de agentes es generada por el algoritmo descrito en nuestro algoritmo anterior [15].",
        "Durante el proceso de generación de la topología, la información de grado de cada agente es estimada por el algoritmo introducido por Palmer et al. [9] con parámetros α = 0.5 y β = 0.6.",
        "En nuestros experimentos, estimamos el límite superior y el límite de grado descendente utilizando factores de descuento lineales de 0.5, 0.8 y 1.0.",
        "Una vez que se construye la topología, las consultas seleccionadas al azar del conjunto de consultas 301-350 en TREC-VLC-921 y del conjunto de consultas 1-50 en TREC-123-100-Random y TREC-123-100-Source se inyectan en el sistema basado en una distribución de Poisson P(N(t) = n) = (λt)n n! e−λ 236 The Sixth Intl.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 0 50 100 150 200 250 300 350 400 0 500 1000 1500 2000 2500 3000 Utilidad acumulada Número de consulta Utilidad acumulada sobre el número de consultas entrantes TSLA-921 SSNA-921 SSLA-921 TSNA-921 Figura 5: La utilidad acumulada versus el número de sesiones de búsqueda TREC-VLC-921 Además, asumimos que todos los agentes tienen la misma probabilidad de recibir consultas del entorno, es decir, λ es el mismo para cada agente.",
        "En nuestros experimentos, λ se establece en 0.0543 para que la media de las consultas entrantes del entorno a la red de agentes sea de 50 por unidad de tiempo.",
        "El tiempo de servicio para la cola de comunicación y la cola de búsqueda local, es decir, tQij y trs, se establece en 0.01 unidades de tiempo y 0.05 unidades de tiempo respectivamente.",
        "En nuestros experimentos, hay diez tipos de consultas adquiridas mediante la agrupación del conjunto de consultas 301-350 y 1-50. 4.3 Análisis y evaluación de resultados. La Figura 3 muestra el ARSS (Recompensa Promedio por Sesión de Búsqueda) versus el número de consultas entrantes a lo largo del tiempo para el Algoritmo No Aprendido de un Solo Paso (SSNA) y el Algoritmo de Aprendizaje de un Solo Paso (SSLA) para la recolección de datos TREC-VLC-921.",
        "Se muestra que la recompensa promedio para el algoritmo SSNA oscila entre 0.02 y 0.06 y su rendimiento cambia poco con el tiempo.",
        "El promedio de recompensa para el enfoque SSLA comienza en el mismo nivel que el algoritmo SSNA.",
        "Pero el rendimiento aumenta con el tiempo y la ganancia promedio de rendimiento se estabiliza en aproximadamente un 25% después del rango de consulta de 2000 a 3000.",
        "La Figura 4 muestra el ARSS (Recompensa Promedio por Sesión de Búsqueda) versus el número de consultas entrantes a lo largo del tiempo para el Algoritmo No Aprendido de Dos Pasos (TSNA) y el Algoritmo de Aprendizaje de Dos Pasos (TSLA) para la recolección de datos TREC-VLC-921.",
        "El enfoque TSNA tiene un rendimiento relativamente consistente con una recompensa promedio que oscila entre 0.05 y 0.15.",
        "El promedio de recompensa para el enfoque de TSLA, donde se explota el algoritmo de aprendizaje, comienza en el mismo nivel que el algoritmo TSNA y mejora la recompensa promedio con el tiempo hasta que se unen al sistema entre 2000 y 2500 consultas.",
        "Los resultados muestran que la ganancia promedio de rendimiento para el enfoque TSLA sobre el enfoque TNLA es del 35% después de la estabilización.",
        "La Figura 5 muestra la utilidad acumulada versus el número de consultas entrantes a lo largo del tiempo para SSNA, SSLA, TSNA y TSLA respectivamente.",
        "Se ilustra que la utilidad acumulativa de los algoritmos no basados en aprendizaje aumenta principalmente de forma lineal con el tiempo, mientras que las ganancias de los algoritmos basados en aprendizaje se aceleran cuando más consultas ingresan al sistema.",
        "Estos resultados experimentales demuestran que los enfoques basados en el aprendizaje tienen un rendimiento consistentemente mejor que los algoritmos de enrutamiento no basados en el aprendizaje.",
        "Además, el algoritmo basado en aprendizaje de dos fases es mejor que el algoritmo basado en aprendizaje de una sola fase porque la recompensa máxima que un agente puede recibir al buscar en su vecindario dentro de TTL saltos está relacionada con el número total de documentos relevantes en esa área.",
        "Por lo tanto, incluso la política de enrutamiento óptima puede hacer poco más allá de alcanzar estos documentos relevantes más rápido.",
        "Por el contrario, el algoritmo de aprendizaje basado en dos pasos puede reubicar la sesión de búsqueda en un vecindario con documentos más relevantes.",
        "El TSLA combina los méritos de ambos enfoques y los supera.",
        "La Tabla 1 enumera la utilidad acumulativa para los conjuntos de datos TREC123-100-Random y TREC-123-100-Source con organizaciones jerárquicas.",
        "Las cinco columnas muestran los resultados de cuatro enfoques diferentes.",
        "En particular, la columna TSNA-Random muestra los resultados para el conjunto de datos TREC-123-100-Random con el enfoque TSNA.",
        "La columna TSLA-Random muestra los resultados para el conjunto de datos TREC-123-100-Random con el enfoque TSLA.",
        "Hay dos números en cada celda de la columna TSLA-Random.",
        "El primer número es la utilidad acumulada real, mientras que el segundo número es el porcentaje de ganancia en términos de utilidad sobre el enfoque TSNA.",
        "Las columnas TSNA-Source y TSLA-Source muestran los resultados para el conjunto de datos TREC-123-100-Source con los enfoques TSNA y TSLA respectivamente.",
        "La Tabla 1 muestra que la mejora en el rendimiento para TREC-123-100-Random no es tan significativa como en los otros conjuntos de datos.",
        "Esto se debe a que los documentos en la subcolección de TREC-123-100-Random son seleccionados al azar, lo que hace que el modelo de colección, la firma de la colección, sea menos significativo.",
        "Dado que ambos algoritmos están diseñados basados en la suposición de que las colecciones de documentos pueden ser bien representadas por su modelo de colección, este resultado no es sorprendente.",
        "En general, las Figuras 4, 5 y la Tabla 1 demuestran que el enfoque basado en aprendizaje por refuerzo puede mejorar considerablemente el rendimiento del sistema para ambas colecciones de datos.",
        "Sin embargo, queda como trabajo futuro descubrir la correlación entre la magnitud de las mejoras en el rendimiento y el tamaño de la colección de datos y/o la extensión de la heterogeneidad entre las subcolecciones.",
        "TRABAJO RELACIONADO El problema de enrutamiento de contenido difiere del enrutamiento a nivel de red en redes de comunicación conmutadas por paquetes en que el enrutamiento basado en contenido ocurre en redes a nivel de aplicación.",
        "Además, los agentes de destino en nuestros algoritmos de enrutamiento de contenido son múltiples y las direcciones no son conocidas en el proceso de enrutamiento.",
        "Los problemas de enrutamiento a nivel de IP han sido abordados desde la perspectiva del aprendizaje por refuerzo[2, 5, 11, 12].",
        "Estos estudios han explorado algoritmos distribuidos completamente que son capaces, sin coordinación central, de difundir conocimiento sobre la red, encontrar de manera robusta y eficiente los caminos más cortos frente a cambios en las topologías de red y costos de enlace cambiantes.",
        "Hay dos clases principales de algoritmos de enrutamiento de paquetes adaptativos y distribuidos en la literatura: algoritmos de vector de distancia y algoritmos de estado de enlace.",
        "Si bien esta línea de estudios tiene cierta similitud con nuestro trabajo, se ha centrado principalmente en redes de comunicación conmutadas por paquetes.",
        "En este dominio, el destino de un paquete es determinístico y único.",
        "Cada agente mantiene estimaciones, probabilísticamente o determinísticamente, sobre la distancia a un destino específico a través de sus vecinos.",
        "Una variante de las técnicas de Q-Learning se implementa en The Sixth Intl.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Utilidad Acumulativa para los Conjuntos de Datos TREC-123-100-Random y TREC-123-100-Source con Organización Jerárquica; Los números porcentuales en las columnas TSLA-Random y TSLA-Source demuestran la ganancia de rendimiento sobre el algoritmo sin aprendizaje Número de consulta TSNA-Random TSLA-Random TSNA-Source TSLA-Source 500 25.15 28.45 13% 24.00 21.05 -13% 1000 104.99 126.74 20% 93.95 96.44 2.6% 1250 149.79 168.40 12% 122.64 134.05 9.3% 1500 188.94 211.05 12% 155.30 189.60 22% 1750 235.49 261.60 11% 189.14 243.90 28% 2000 275.09 319.25 16% 219.0 278.80 26% para actualizar las estimaciones y converger a las distancias reales.",
        "Se ha descubierto que la propiedad de localidad es una característica importante de los sistemas de recuperación de información en estudios de modelado de usuarios[3].",
        "En los sistemas de intercambio de contenido basados en P2P, esta propiedad se ejemplifica por el fenómeno de que los usuarios tienden a enviar consultas que representan solo un número limitado de temas y, por el contrario, los usuarios en el mismo vecindario probablemente compartan intereses comunes y envíen consultas similares [10].",
        "El enfoque basado en el aprendizaje se percibe como más beneficioso para los sistemas reales de recuperación de información distribuida que exhiben la propiedad de localidad.",
        "Esto se debe a que los patrones de tráfico y consultas de los usuarios pueden reducir el espacio de estados y acelerar el proceso de aprendizaje.",
        "El trabajo relacionado en aprovechar esta propiedad incluye [7], donde los autores intentaron abordar este problema mediante técnicas de modelado de usuario. 6.",
        "CONCLUSIONES En este artículo, se desarrolla un enfoque basado en aprendizaje por refuerzo para mejorar el rendimiento de algoritmos de búsqueda de IR distribuidos.",
        "En particular, los agentes mantienen estimaciones, es decir, utilidad esperada, sobre la capacidad de los agentes aguas abajo para proporcionar documentos relevantes para las consultas entrantes.",
        "Estas estimaciones se actualizan gradualmente al aprender de la información de retroalimentación devuelta de sesiones de búsqueda anteriores.",
        "Basándose en la información actualizada de utilidad esperada, los agentes modifican sus políticas de enrutamiento.",
        "Posteriormente, estos agentes dirigen las consultas basándose en las políticas aprendidas y actualizan las estimaciones sobre la utilidad esperada en función de las nuevas políticas de enrutamiento.",
        "Los experimentos en dos conjuntos de datos IR distribuidos diferentes ilustran que el enfoque de aprendizaje por refuerzo mejora considerablemente la utilidad acumulativa con el tiempo. 7.",
        "REFERENCIAS [1] S. Abdallah y V. Lesser.",
        "Aprendiendo el juego de asignación de tareas.",
        "En AAMAS 06: Actas de la quinta conferencia internacional conjunta sobre agentes autónomos y sistemas multiagente, páginas 850-857, Nueva York, NY, EE. UU., 2006.",
        "ACM Press. [2] J. \n\nACM Press. [2] J.",
        "A. Boyan y M. L. Littman.",
        "Enrutamiento de paquetes en redes de cambio dinámico: Un enfoque de aprendizaje por refuerzo.",
        "En Avances en Sistemas de Procesamiento de Información Neural, volumen 6, páginas 671-678.",
        "Morgan Kaufmann Publishers, Inc., 1994. [3] J. C. French, A. L. Powell, J. P. Callan, C. L. Viles, T. Emmitt, K. J. Prey, y Y. Mou.",
        "Comparando el rendimiento de los algoritmos de selección de bases de datos.",
        "En Investigación y Desarrollo en Recuperación de Información, páginas 238-245, 1999. [4] B. Horling, R. Mailler y V. Lesser.",
        "Granja: Un entorno escalable para el desarrollo y evaluación de múltiples agentes.",
        "En Avances en Ingeniería de Software para Sistemas Multiagente, páginas 220-237, Berlín, 2004.",
        "Springer-Verlag. [5] M. Littman y J. Boyan.",
        "Un esquema distribuido de aprendizaje por refuerzo para enrutamiento de redes.",
        "En Actas del Taller Internacional sobre Aplicaciones de Redes Neuronales a las Telecomunicaciones, 1993. [6] J. Lu y J. Callan.",
        "Búsqueda federada de bibliotecas digitales basadas en texto en redes jerárquicas de pares.",
        "En ECIR05, 2005. [7] J. Lu y J. Callan.",
        "Modelado de usuario para búsqueda federada de texto completo en redes peer-to-peer.",
        "En ACM SIGIR 2006.",
        "ACM Press, 2006. [8] C. D. Manning y H. Schütze.",
        "Fundamentos del Procesamiento del Lenguaje Natural Estadístico.",
        "El MIT Press, Cambridge, Massachusetts, 1999. [9] C. R. Palmer y J. G. Steffan.",
        "Generando topologías de red que obedezcan leyes de potencia.",
        "En Actas de GLOBECOM 2000, noviembre de 2000. [10] K. Sripanidkulchai, B. Maggs y H. Zhang.",
        "Localización eficiente de contenido utilizando la localidad basada en intereses en sistemas peer-to-peer.",
        "En INFOCOM, 2003. [11] D. Subramanian, P. Druschel y J. Chen.",
        "Hormigas y aprendizaje por refuerzo: Un estudio de caso en enrutamiento en redes dinámicas.",
        "En Actas de la Decimoquinta Conferencia Internacional Conjunta sobre Inteligencia Artificial, páginas 832-839, 1997. [12] J. N. Tao y L. Weaver.",
        "Un enfoque de múltiples agentes y gradiente de políticas para el enrutamiento de redes.",
        "En Actas de la Decimoctava Conferencia Internacional sobre Aprendizaje Automático, 2001. [13] H. Zhang, W. B. Croft, B. Levine y V. Lesser.",
        "Un enfoque multiagente para la recuperación de información entre pares.",
        "En Actas de la Tercera Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, julio de 2004. [14] H. Zhang y V. Lesser.",
        "Sistemas de recuperación de información peer-to-peer basados en múltiples agentes con sesiones de búsqueda concurrentes.",
        "En Actas de la Quinta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, mayo de 2006. [15] H. Zhang y V. R. Lesser.",
        "Una organización de agentes jerárquica formada dinámicamente para un sistema distribuido de intercambio de contenido.",
        "En la Conferencia Internacional IEEE/WIC/ACM sobre Tecnología de Agentes Inteligentes (IAT 2004) del 20 al 24 de septiembre de 2004 en Beijing, China, páginas 169-175.",
        "IEEE Computer Society, 2004. 238 La Sexta Conferencia Internacional.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07)"
    ],
    "error_count": 2,
    "keys": {
        "peer-to-peer information retrieval system": {
            "translated_key": "Sistema de recuperación de información entre pares",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Reinforcement Learning based Distributed Search Algorithm For Hierarchical Peer-to-Peer Information Retrieval Systems Haizheng Zhang College of Information Science and Technology Pennsylvania State University University Park, PA 16803 hzhang@ist.psu.edu Victor Lesser Department of Computer Science University Of Massachusetts Amherst, MA 01003 lesser@cs.umass.edu ABSTRACT The dominant existing routing strategies employed in peerto-peer(P2P) based information retrieval(IR) systems are similarity-based approaches.",
                "In these approaches, agents depend on the content similarity between incoming queries and their direct neighboring agents to direct the distributed search sessions.",
                "However, such a heuristic is myopic in that the neighboring agents may not be connected to more relevant agents.",
                "In this paper, an online reinforcement-learning based approach is developed to take advantage of the dynamic run-time characteristics of P2P IR systems as represented by information about past search sessions.",
                "Specifically, agents maintain estimates on the downstream agents abilities to provide relevant documents for incoming queries.",
                "These estimates are updated gradually by learning from the feedback information returned from previous search sessions.",
                "Based on this information, the agents derive corresponding routing policies.",
                "Thereafter, these agents route the queries based on the learned policies and update the estimates based on the new routing policies.",
                "Experimental results demonstrate that the learning algorithm improves considerably the routing performance on two test collection sets that have been used in a variety of distributed IR studies.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Algorithms, Performance, Experimentation 1.",
                "INTRODUCTION Over the last few years there have been increasing interests in studying how to control the search processes in peer-to-peer(P2P) based information retrieval(IR) systems [6, 13, 14, 15].",
                "In this line of research, one of the core problems that concerns researchers is to efficiently route user queries in the network to agents that are in possession of appropriate documents.",
                "In the absence of global information, the dominant strategies in addressing this problem are content-similarity based approaches [6, 13, 14, 15].",
                "While the content similarity between queries and local nodes appears to be a creditable indicator for the number of relevant documents residing on each node, these approaches are limited by a number of factors.",
                "First of all, similaritybased metrics can be myopic since locally relevant nodes may not be connected to other relevant nodes.",
                "Second, the similarity-based approaches do not take into account the run-time characteristics of the P2P IR systems, including environmental parameters, bandwidth usage, and the historical information of the past search sessions, that provide valuable information for the query routing algorithms.",
                "In this paper, we develop a reinforcement learning based IR approach for improving the performance of distributed IR search algorithms.",
                "Agents can acquire better search strategies by collecting and analyzing feedback information from previous search sessions.",
                "Particularly, agents maintain estimates, namely expected utility, on the downstream agents capabilities of providing relevant documents for specific types of incoming queries.",
                "These estimates are updated gradually by learning from the feedback information returned from previous search sessions.",
                "Based on the updated expected utility information, the agents derive corresponding routing policies.",
                "Thereafter, these agents route the queries based on the learned policies and update the estimates on the expected utility based on the new routing policies.",
                "This process is conducted in an iterative manner.",
                "The goal of the learning algorithm, even though it consumes some network bandwidth, is to shorten the routing time so that more queries are processed per time unit while at the same time finding more relevant documents.",
                "This contrasts with the content-similarity based approaches where similar operations are repeated for every incoming query and the processing time keeps largely constant over time.",
                "Another way of viewing this paper is that our basic approach to distributed IR search is to construct a hierarchical overlay network(agent organization) based on the contentsimilarity measure among agents document collections in a bottom-up fashion.",
                "In the past work, we have shown that this organization improves search performance significantly.",
                "However, this organizational structure does not take into account the arrival patterns of queries, including their frequency, types, and where they enter the system, nor the available communication bandwidth of the network and processing capabilities of individual agents.",
                "The intention of the reinforcement learning is to adapt the agents routing decisions to the dynamic network situations and learn from past search sessions.",
                "Specifically, the contributions of this paper include: (1) a reinforcement learning based approach for agents to acquire satisfactory routing policies based on estimates of the potential contribution of their neighboring agents; (2) two strategies to speed up the learning process.",
                "To our best knowledge, this is one of the first reinforcement learning applications in addressing distributed content sharing problems and it is indicative of some of the issues in applying reinforcement in a complex application.",
                "The remainder of this paper is organized as follows: Section 2 reviews the hierarchical content sharing systems and the two-phase search algorithm based on such topology.",
                "Section 3 describes a reinforcement learning based approach to direct the routing process; Section 4 details the experimental settings and analyze the results.",
                "Section 5 discusses related studies and Section 6 concludes the paper. 2.",
                "SEARCH IN HIERARCHICAL P2P IR SYSTEMS This section briefly reviews our basic approaches to hierarchical P2P IR systems.",
                "In a hierarchical P2P IR system illustrated in Fig.1, agents are connected to each other through three types of links: upward links, downward links, and lateral links.",
                "In the following sections, we denote the set of agents that are directly connected to agent Ai as DirectConn(Ai), which is defined as DirectConn(Ai) = NEI(Ai) ∪ PAR(Ai) ∪ CHL(Ai) , where NEI(Ai) is the set of neighboring agents connected to Ai through lateral links; PAR(Ai) is the set of agents whom agent Ai is connected to through upward links and CHL(Ai) is the set of agents that agent Ai connects to through downward links.",
                "These links are established through a bottom-up content-similarity based distributed clustering process[15].",
                "These links are then used by agents to locate other agents that contain documents relevant to the given queries.",
                "A typical agent Ai in our system uses two queues: a local search queue, LSi, and a message forwarding queue MFi.",
                "The states of the two queues constitute the internal states of an agent.",
                "The local search queue LSi stores search sessions that are scheduled for local processing.",
                "It is a priority queue and agent Ai always selects the most promising queries to process in order to maximize the global utility.",
                "MFi consists of a set of queries to forward on and is processed in a FIFO (first in first out) fashion.",
                "For the first query in MFi, agent Ai determines which subset of its neighboring agents to forward it to based on the agents routing policy πi.",
                "These routing decisions determine how the search process is conducted in the network.",
                "In this paper, we call Ai as Ajs upstream agent and Aj as Ais downstream agent if A4 A5 A6 A7 A2 A3 A9 NEI(A2)={A3} PAR(A2)={A1} CHL(A2)={A4,A5} A1 A8 Figure 1: A fraction of a hierarchical P2PIR system an agent Ai routes a query to agent Aj.",
                "The distributed search protocol of our hierarchical agent organization is composed of two steps.",
                "In the first step, upon receipt of a query qk at time tl from a user, agent Ai initiates a search session si by probing its neighboring agents Aj ∈ NEI(Ai) with the message PROBE for the similarity value Sim(qk, Aj) between qk and Aj.",
                "Here, Ai is defined as the query initiator of search session si.",
                "In the second step, Ai selects a group of the most promising agents to start the actual search process with the message SEARCH.",
                "These SEARCH messages contain a TTL (Time To Live) parameter in addition to the query.",
                "The TTL value decreases by 1 after each hop.",
                "In the search process, agents discard those queries that either have been previously processed or whose TTL drops to 0, which prevents queries from looping in the system forever.",
                "The search session ends when all the agents that receive the query drop it or TTL decreases to 0.",
                "Upon receipt of SEARCH messages for qk, agents schedule local activities including local searching, forwarding qk to their neighbors, and returning search results to the query initiator.",
                "This process and related algorithms are detailed in [15, 14]. 3.",
                "A BASIC REINFORCEMENTLEARNING BASED SEARCH APPROACH In the aforementioned distributed search algorithm, the routing decisions of an agent Ai rely on the similarity comparison between incoming queries and Ais neighboring agents in order to forward those queries to relevant agents without flooding the network with unnecessary query messages.",
                "However, this heuristic is myopic because a relevant direct neighbor is not necessarily connected to other relevant agents.",
                "In this section, we propose a more general approach by framing this problem as a reinforcement learning task.",
                "In pursuit of greater flexibility, agents can switch between two modes: learning mode and non-learning mode.",
                "In the non-learning mode, agents operate in the same way as they do in the normal distributed search processes described in [14, 15].",
                "On the other hand, in the learning mode, in parallel with distributed search sessions, agents also participate in a learning process which will be detailed in this section.",
                "Note that in the learning protocol, the learning process does not interfere with the distributed search process.",
                "Agents can choose to initiate and stop learning processes without affecting the system performance.",
                "In particular, since the learning process consumes network resources (especially bandwidth), agents can choose to initiate learning only when the network load is relatively low, thus minimizing the extra communication costs incurred by the learning algorithm.",
                "The section is structured as follows, Section 3.1 describes 232 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) a reinforcement learning based model.",
                "Section 3.2 describes a protocol to deploy the learning algorithm in the network.",
                "Section 3.3 discusses the convergence of the learning algorithm. 3.1 The Model An agents routing policy takes the state of a search session as input and output the routing actions for that query.",
                "In our work, the state of a search session sj is stipulated as: QSj = (qk, ttlj) where ttlj is the number of hops that remains for the search session sj , qk is the specific query.",
                "QL is an attribute of qk that indicates which type of queries qk most likely belong to.",
                "The set of QL can be generated by running a simple online classification algorithm on all the queries that have been processed by the agents, or an oﬄine algorithm on a pre-designated training set.",
                "The assumption here is that the set of query types is learned ahead of time and belongs to the common knowledge of the agents in the network.",
                "Future work includes exploring how learning can be accomplished when this assumption does not hold.",
                "Given the query types set, an incoming query qi can be classified to one query class Q(qi) by the formula: Q(qi) = arg max Qj P(qi|Qj) (1) where P(qi|Qj ) indicates the likelihood that the query qi is generated by the query class Qj [8].",
                "The set of atomic routing actions of an agent Ai is denoted as {αi}, where {αi} is defined as αi = {αi0 , αi1 , ..., αin }.",
                "An element αij represents an action to route a given query to the neighboring agent Aij ∈ DirectConn(Ai).",
                "The routing policy πi of agent Ai is stochastic and its outcome for a search session with state QSj is defined as: πi(QSj) = {(αi0 , πi(QSi, αi0 )), (αi1 , πi(QSi, αi1 )), ...} (2) Note that operator πi is overloaded to represent either the probabilistic policy for a search session with state QSj, denoted as πi(QSj); or the probability of forwarding the query to a specific neighboring agent Aik ∈ DirectConn(Ai) under the policy πi(QSj), denoted as πi(QSj, αik ).",
                "Therefore, equation (2) means that the probability of forwarding the search session to agent Ai0 is πi(QSi, αi0 ) and so on.",
                "Under this stochastic policy, the routing action is nondeterministic.",
                "The advantage of such a strategy is that the best neighboring agents will not be selected repeatedly, thereby mitigating the potential hot spots situations.",
                "The expected utility, Un i (QSj), is used to estimate the potential utility gain of routing query type QSj to agent Ai under policy πn i .",
                "The superscript n indicates the value at the nth iteration in an iterative learning process.",
                "The expected utility provides routing guidance for future search sessions.",
                "In the search process, each agent Ai maintains partial observations of its neighbors states, as shown in Fig. 2.",
                "The partial observation includes non-local information such as the potential utility estimation of its neighbor Am for query state QSj, denoted as Um(QSj), as well as the load information, Lm.",
                "These observations are updated periodically by the neighbors.",
                "The estimated utility information will be used to update Ais expected utility for its routing policy.",
                "Load Information Expected Utility For Different Query Types Neighboring Agents ... A0 A1 A3 A2 Un 0 (QS0) ... ... ... ... ...... Un 0 (QS1) Un 1 (QS1) Un 2 (QS1) Un 3 (QS1) Un 1 (QS0) Un 2 (QS0) Un 3 (QS0) Ln 0 Ln 1 Ln 2 Ln 3 ... QS0 QS1 ...",
                "Figure 2: Agent Ais Partial Observation about its neighbors(A0, A1...) The load information of Am, Lm, is defined as Lm = |MFm| Cm , where |MFm| is the length of the message-forward queue and Cm is the service rate of agent Ams message-forward queue.",
                "Therefore Lm characterizes the utilization of an agents communication channel, and thus provide non-local information for Ams neighbors to adjust the parameters of their routing policy to avoid inundating their downstream agents.",
                "Note that based on the characteristics of the queries entering the system and agents capabilities, the loading of agents may not be uniform.",
                "After collecting the utilization rate information from all its neighbors, agent Ai computes Li as a single measure for assessing the average load condition of its neighborhood: Li = P k Lk |DirectConn(Ai)| Agents exploit Li value in determining the routing probability in its routing policy.",
                "Note that, as described in Section 3.2, information about neighboring agents is piggybacked with the query message propagated among the agents whenever possible to reduce the traffic overhead. 3.1.1 Update the Policy An iterative update process is introduced for agents to learn a satisfactory stochastic routing policy.",
                "In this iterative process, agents update their estimates on the potential utility of their current routing policies and then propagate the updated estimates to their neighbors.",
                "Their neighbors then generate a new routing policy based on the updated observation and in turn they calculate the expected utility based on the new policies and continue this iterative process.",
                "In particular, at time n, given a set of expected utility, an agent Ai, whose directly connected agents set is DirectConn(Ai) = {Ai0 , ..., Aim }, determines its corresponding stochastic routing policy for a search session of state QSj based on the following steps: (1) Ai first selects a subset of agents as the potential downstream agents from set DirectConn(Ai), denoted as PDn(Ai, QSj).",
                "The size of the potential downstream agent is specified as |PDn(Ai, QSj)| = min(|NEI(Ai), dn i + k)| where k is a constant and is set to 3 in this paper; dn i , the forward width, is defined as the expected number of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 233 neighboring agents that agent Ai can forward to at time n. This formula specifies that the potential downstream agent set PDn(Ai, QSj) is either the subset of neighboring agents with dn i + k highest expected utility value for state QSj among all the agents in DirectConn(Ai), or all their neighboring agents.",
                "The k is introduced based on the idea of a stochastic routing policy and it makes the forwarding probability of the dn i +k highest agent less than 100%.",
                "Note that if we want to limit the number of downstream agents for search session sj as 5, the probability of forwarding the query to all neighboring agents should add up to 5.",
                "Setting up dn i value properly can improve the utilization rate of the network bandwidth when much of the network is idle while mitigating the traffic load when the network is highly loaded.",
                "The dn+1 i value is updated based on dn i , the previous and current observations on the traffic situation in the neighborhood.",
                "Specifically, the update formula for dn+1 i is dn+1 i = dn i ∗ (1 + 1 − Li |DirectConn(Ai)| ) In this formula, the forward width is updated based on the traffic conditions of agent Ais neighborhood, i.e Li, and its previous value. (2) For each agent Aik in the PDn(Ai, QSj), the probability of forwarding the query to Aik is determined in the following way in order to assign higher forwarding probability to the neighboring agents with higher expected utility value: πn+1 i (QSj, αik ) = dn+1 i |PDn(Ai, QSj)| + β ∗ ` Uik (QSj) − PDU(Ai, QSj) |PDn(Ai, QSj)| ´ (3) where PDUn(Ai, QSj) = X o∈P Dn(Ai,QSj ) Uo(QSj) and QSj is the subsequent state of agent Aik after agent Ai forwards the search session with state QSj to its neighboring agent Aik ; If QSj = (qk, ttl0), then QSj = (qk, ttl0 − 1).",
                "In formula 3, the first term on the right of the equation, dn+1 i |P Dn(Ai,QSj )| , is used to to determine the forwarding probability by equally distributing the forward width, dn+1 i , to the agents in PDn(Ai, QSj) set.",
                "The second term is used to adjust the probability of being chosen so that agents with higher expected utility values will be favored. β is determined according to: β = min ` m − dn+1 i m ∗ umax − PDUn(Ai, QSj) , dn+1 i PDUn(Ai, QSj) − m ∗ umin ´ (4) where m = |PDn(Ai, QSj)|, umax = max o∈P Dn(Ao,QSj ) Uo(QSj) and umin = min o∈P Dn(Ao,QSj ) Uo(QSj) This formula guarantees that the final πn+1 i (QSj, αik ) value is well defined, i.e, 0 ≤ πn+1 i (QSj, αik ) ≤ 1 and X i πn+1 i (QSj, αik ) = dn+1 i However, such a solution does not explore all the possibilities.",
                "In order to balance between exploitation and exploration, a λ-Greedy approach is taken.",
                "In the λ-Greedy approach, in addition to assigning higher probability to those agents with higher expected utility value, as in the equation (3).",
                "Agents that appear to be not-so-good choices will also be sent queries based on a dynamic exploration rate.",
                "In particular, for agents in the set PDn(Ai, QSj), πn+1 i1 (QSj) is determined in the same way as the above, with the only difference being that dn+1 i is replaced with dn+1 i ∗ (1 − λn).",
                "The remaining search bandwidth is used for learning by assigning probability λn evenly to agents Ai2 in the set DirectConn(Ai) − PDn(Ai, QSj). πn+1 i2 (QSj, αik ) = dn+1 i ∗ λn |DirectConn(Ai) − PDn(Ai, QSj)| (5) where PDn(Ai, QSj) ⊂ DirectConn(Ai).",
                "Note that the exploration rate λ is not a constant and it decreases overtime.",
                "The λ is determined according to the following equation: λn+1 = λ0 ∗ e−c1n (6) where λ0 is the initial exploration rate, which is a constant; c1 is also a constant to adjust the decreasing rate of the exploration rate; n is the current time unit. 3.1.2 Update Expected Utility Once the routing policy at step n+1, πn+1 i , is determined based on the above formula, agent Ai can update its own expected utility, Un+1 i (QSi), based on the the updated routing policy resulted from the formula 5 and the updated U values of its neighboring agents.",
                "Under the assumption that after a query is forwarded to Ais neighbors the subsequent search sessions are independent, the update formula is similar to the Bellman update formula in Q-Learning: Un+1 i (QSj) = (1 − θi) ∗ Un i (QSj) + θi ∗ (Rn+1 i (QSj) + X k πn+1 i (QSj, αik )Un k (QSj)) (7) where QSj = (Qj, ttl − 1) is the next state of QSj = (Qj, ttl); Rn+1 i (QSj) is the expected local reward for query class Qk at agent Ai under the routing policy πn+1 i ; θi is the coefficient for deciding how much weight is given to the old value during the update process: the smaller θi value is, the faster the agent is expected to learn the real value, while the greater volatility of the algorithm, and vice versa.",
                "Rn+1 (s) is updated according to the following equation: 234 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Rn+1 i (QSj) = Rn i (QSj) +γi ∗ (r(QSj) − Rn i (QSj)) ∗ P(qj|Qj ) (8) where r(QSj) is the local reward associated with the search session.",
                "P(qj|Qj ) indicates how relevant the query qj is to the query type Qj, and γi is the learning rate for agent Ai.",
                "Depending on the similarity between a specific query qi and its corresponding query type Qi, the local reward associated with the search session has different impact on the Rn i (QSj) estimation.",
                "In the above formula, this impact is reflected by the coefficient, the P(qj|Qj) value. 3.1.3 Reward function After a search session stops when its TTL values expires, all search results are returned back to the user and are compared against the relevance judgment.",
                "Assuming the set of search results is SR, the reward Rew(SR) is defined as: Rew(SR) = j 1 if |Rel(SR)| > c |Rel(SR)| c otherwise. where SR is the set of returned search results, Rel(SR) is the set of relevant documents in the search results.",
                "This equation specifies that users give 1.0 reward if the number of returned relevant documents reaches a predefined number c. Otherwise, the reward is in proportion to the number of relevant documents returned.",
                "This rationale for setting up such a cut-off value is that the importance of recall ratio decreases with the abundance of relevant documents in real world, therefore users tend to focus on only a limited number of searched results.",
                "The details of the actual routing protocol will be introduced in Section 3.2 when we introduce how the learning algorithm is deployed in real systems. 3.2 Deployment of the Learning algorithm This section describes how the learning algorithm can be used in either a single-phase or a two-phase search process.",
                "In the single-phase search algorithm, search sessions start from the initiators of the queries.",
                "In contrast, in the two-step search algorithm, the query initiator first attempts to seek a more appropriate starting point for the query by introducing an exploratory step as described in Section 2.",
                "Despite the difference in the quality of starting points, the major part of the learning process for the two algorithms is largely the same as described in the following paragraphs.",
                "Before learning starts, each agent initializes the expected utility value for all possible states as 0.",
                "Thereafter, upon receipt of a query, in addition to the normal operations described in the previous section, an agent Ai also sets up a timer to wait for the search results returned from its downstream agents.",
                "Once the timer expires or it has received response from all its downstream agents, Ai merges and forwards the search results accrued from its downstream agents to its upstream agent.",
                "Setting up the timer speeds up the learning because agents can avoid waiting too long for the downstream agents to return search results.",
                "Note that these detailed results and corresponding agent information will still be stored at Ai until the feedback information is passed from its upstream agent and the performance of its downstream agents can be evaluated.",
                "The duration of the timer is related to the TTL value.",
                "In this paper, we set the timer to ttimer = ttli ∗ 2 + tf , where ttli ∗ 2 is the sum of the travel time of the queries in the network, and tf is the expected time period that users would like to wait.",
                "The search results will eventually be returned to the search session initiator A0.",
                "They will be compared to the relevance judgment that is provided by the final users (as described in the experiment section, the relevance judgement for the query set is provided along with the data collections).",
                "The reward will be calculated and propagated backward to the agents along the way that search results were passed.",
                "This is a reverse process of the search results propagation.",
                "In the process of propagating reward backward, agents update estimates of their own potential utility value, generate an upto-dated policy and pass their updated results to the neighboring agents based on the algorithm described in Section 3.",
                "Upon change of expected utility value, agent Ai sends out its updated utility estimation to its neighbors so that they can act upon the changed expected utility and corresponding state.",
                "This update message includes the potential reward as well as the corresponding state QSi = (qk, ttll) of agent Ai.",
                "Each neighboring agent, Aj, reacts to this kind of update message by updating the expected utility value for state QSj(qk, ttll + 1) according to the newly-announced changed expected utility value.",
                "Once they complete the update, the agents would again in turn inform related neighbors to update their values.",
                "This process goes on until the TTL value in the update message increases to the TTL limit.",
                "To speed up the learning process, while updating the expected utility values of an agent Ais neighboring agents we specify that Um(Qk, ttl0) >= Um(Qk, ttl1) iff ttl0 > ttl1 Thus, when agent Ai receives an updated expected utility value with ttl1, it also updates the expected utility values with any ttl0 > ttl1 if Um(Qk, ttl0) < Um(Qk, ttl1) to speed up convergence.",
                "This heuristic is based on the fact that the utility of a search session is a non-decreasing function of time t. 3.3 Discussion In formalizing the content routing system as a learning task, many assumptions are made.",
                "In real systems, these assumptions may not hold, and thus the learning algorithm may not converge.",
                "Two problems are of particular note, (1) This content routing problem does not have Markov properties.",
                "In contrast to IP-level based packet routing, the routing decision of each agent for a particular search session sj depends on the routing history of sj.",
                "Therefore, the assumption that all subsequent search sessions are independent does not hold in reality.",
                "This may lead to double counting problem that the relevant documents of some agents will be counted more than once for the state where the TTL value is more than 1.",
                "However, in the context of the hierarchical agent organizations, two factors mitigate this problems: first, the agents in each content group form a tree-like structure.",
                "With the absense of the cycles, the estimates inside the tree would be close to the accurate value.",
                "Secondly, the stochastic nature of the routing policy partly remedies this problem.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 235 (2) Another challenge for this learning algorithm is that in a real network environment observations on neighboring agents may not be able to be updated in time due to the communication delay or other situations.",
                "In addition, when neighboring agents update their estimates at the same time, oscillation may arise during the learning process[1].",
                "This paper explores several approaches to speed up the learning process.",
                "Besides the aforementioned strategy of updating the expected utility values, we also employ an active update strategy where agents notify their neighbors whenever its expected utility is updated.",
                "Thus a faster convergence speed can be achieved.",
                "This strategy contrasts to the Lazy update, where agents only echo their neighboring agents with their expected utility change when they exchange information.",
                "The trade off between the two approaches is the network load versus learning speed.",
                "The advantage of this learning algorithm is that once a routing policy is learned, agents do not have to repeatedly compare the similarity of queries as long as the network topology remains unchanged.",
                "Instead, agent just have to determine the classification of the query properly and follow the learned policies.",
                "The disadvantage of this learning-based approach is that the learning process needs to be conducted whenever the network structure changes.",
                "There are many potential extensions for this learning model.",
                "For example, a single measure is currently used to indicate the traffic load for an agents neighborhood.",
                "A simple extension would be to keep track of individual load for each neighbor of the agent. 4.",
                "EXPERIMENTSSETTINGSAND RESULTS The experiments are conducted on TRANO simulation toolkit with two sets of datasets, TREC-VLC-921 and TREC123-100.",
                "The following sub-sections introduce the TRANO testbed, the datasets, and the experimental results. 4.1 TRANO Testbed TRANO (Task Routing on Agent Network Organization) is a multi-agent based network based information retrieval testbed.",
                "TRANO is built on top of the Farm [4], a time based distributed simulator that provides a data dissemination framework for large scale distributed agent network based organizations.",
                "TRANO supports importation and exportation of agent organization profiles including topological connections and other features.",
                "Each TRANO agent is composed of an agent view structure and a control unit.",
                "In simulation, each agent is pulsed regularly and the agent checks the incoming message queues, performs local operations and then forwards messages to other agents . 4.2 Experimental Settings In our experiment, we use two standard datasets, TRECVLC-921 and TREC-123-100 datasets, to simulate the collections hosted on agents.",
                "The TREC-VLC-921 and TREC123-100 datasets were created by the U.S. National Institute for Standard Technology(NIST) for its TREC conferences.",
                "In distributed information retrieval domain, the two data collections are split to 921 and 100 sub-collections.",
                "It is observed that dataset TREC-VLC-921 is more heterogeneous than TREC-123-100 in terms of source, document length, and relevant document distribution from the statistics of the two data collections listed in [13].",
                "Hence, TREC-VLC-921 is much closer to real document distributions in P2P environments.",
                "Furthermore, TREC-123-100 is split into two sets of 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0 500 1000 1500 2000 2500 3000 ARSS Query number ARSS versus the number of incoming queries for TREC-VLC-921 SSLA-921 SSNA-921 Figure 3: ARSS(Average reward per search session) versus the number of search sessions for 1phase search in TREC-VLC-921 0 0.1 0.2 0.3 0.4 0.5 0.6 0 500 1000 1500 2000 2500 3000 ARSS Query number ARSS versus query number for TREC-VLC-921 TSLA-921 TSNA-921 Figure 4: ARSS(Average reward per search session) versus the number of search sessions for 2phase search in TREC-VLC-921 sub-collections in two ways: randomly and by source.",
                "The two partitions are denoted as TREC-123-100-Random and TREC-123-100-Source respectively.",
                "The documents in each subcollection in dataset TREC-123-100-Source are more coherent than those in TREC-123-100-Random.",
                "The two different sets of partitions allow us to observe how the distributed learning algorithm is affected by the homogeneity of the collections.",
                "The hierarchical agent organization is generated by the algorithm described in our previous algorithm [15].",
                "During the topology generation process, degree information of each agent is estimated by the algorithm introduced by Palmer et al. [9] with parameters α = 0.5 and β = 0.6.",
                "In our experiments, we estimate the upward limit and downward degree limit using linear discount factors 0.5, 0.8 and 1.0.",
                "Once the topology is built, queries randomly selected from the query set 301−350 on TREC-VLC-921 and query set 1− 50 on TREC-123-100-Random and TREC-123-100-Source are injected to the system based on a Poisson distribution P(N(t) = n) = (λt)n n! e−λ 236 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 0 50 100 150 200 250 300 350 400 0 500 1000 1500 2000 2500 3000 Cumulativeutility Query number Cumulative utility over the number of incoming queries TSLA-921 SSNA-921 SSLA-921 TSNA-921 Figure 5: The cumulative utility versus the number of search sessions TREC-VLC-921 In addition, we assume that all agents have an equal chance of getting queries from the environment, i.e, λ is the same for every agent.",
                "In our experiments, λ is set as 0.0543 so that the mean of the incoming queries from the environment to the agent network is 50 per time unit.",
                "The service time for the communication queue and local search queue, i.e tQij and trs, is set as 0.01 time unit and 0.05 time units respectively.",
                "In our experiments, there are ten types of queries acquired by clustering the query set 301 − 350 and 1 − 50. 4.3 Results analysis and evaluation Figure 3 demonstrates the ARSS(Average Reward per Search Session) versus the number of incoming queries over time for the the Single-Step based Non-learning Algorithm (SSNA), and the Single-Step Learning Algorithm(SSLA) for data collection TREC-VLC-921.",
                "It shows that the average reward for SSNA algorithm ranges from 0.02 − 0.06 and the performance changes little over time.",
                "The average reward for SSLA approach starts at the same level with the SSNA algorithm.",
                "But the performance increases over time and the average performance gain stabilizes at about 25% after query range 2000 − 3000.",
                "Figure 4 shows the ARSS(Average Reward per Search Session) versus the number of incoming queries over time for the the Two-Step based Non-learning Algorithm(TSNA), and the Two-Step Learning Algorithm(TSLA) for data collection TREC-VLC-921.",
                "The TSNA approach has a relatively consistent performance with the average reward ranges from 0.05 − 0.15.",
                "The average reward for TSLA approach, where learning algorithm is exploited, starts at the same level with the TSNA algorithm and improves the average reward over time until 2000−2500 queries joining the system.",
                "The results show that the average performance gain for TSLA approach over TNLA approach is 35% after stabilization.",
                "Figure 5 shows the cumulative utility versus the number of incoming queries over time for SSNA, SSLA,TSNA, and TSLA respectively.",
                "It illustrates that the cumulative utility of non-learning algorithms increases largely linearly over time, while the gains of learning-based algorithms accelerate when more queries enter the system.",
                "These experimental results demonstrate that learning-based approaches consistently perform better than non-learning based routing algorithm.",
                "Moreover, two-phase learning based algorithm is better than single-phase based learning algorithm because the maximal reward an agent can receive from searching its neighborhood within TTL hops is related to the total number of the relevant documents in that area.",
                "Thus, even the optimal routing policy can do little beyond reaching these relevant documents faster.",
                "On the contrary, the two-stepbased learning algorithm can relocate the search session to a neighborhood with more relevant documents.",
                "The TSLA combines the merits of both approaches and outperforms them.",
                "Table 1 lists the cumulative utility for datasets TREC123-100-Random and TREC-123-100-Source with hierarchical organizations.",
                "The five columns show the results for four different approaches.",
                "In particular, column TSNA-Random shows the results for dataset TREC-123-100-Random with the TSNA approach.",
                "The column TSLA-Random shows the results for dataset TREC-123-100-Random with the TSLA approach.",
                "There are two numbers in each cell in the column TSLA-Random.",
                "The first number is the actual cumulative utility while the second number is the percentage gain in terms of the utility over TSNA approach.",
                "Columns TSNA-Source and TSLA-Source show the results for dataset TREC-123-100-Source with TSNA and TSLA approaches respectively.",
                "Table 1 shows that the performance improvement for TREC-123-100-Random is not as significant as the other datasets.",
                "This is because that the documents in the sub-collection of TREC-123-100-Random are selected randomly which makes the collection model, the signature of the collection, less meaningful.",
                "Since both algorithms are designed based on the assumption that document collections can be well represented by their collection model, this result is not surprising.",
                "Overall, Figures 4, 5, and Table 1 demonstrate that the reinforcement learning based approach can considerably enhance the system performance for both data collections.",
                "However, it remains as future work to discover the correlation between the magnitude of the performance gains and the size of the data collection and/or the extent of the heterogeneity between the sub-collections. 5.",
                "RELATED WORK The content routing problem differs from the networklevel routing in packet-switched communication networks in that content-based routing occurs in application-level networks.",
                "In addition, the destination agents in our contentrouting algorithms are multiple and the addresses are not known in the routing process.",
                "IP-level Routing problems have been attacked from the reinforcement learning perspective[2, 5, 11, 12].",
                "These studies have explored fully distributed algorithms that are able, without central coordination to disseminate knowledge about the network, to find the shortest paths robustly and efficiently in the face of changing network topologies and changing link costs.",
                "There are two major classes of adaptive, distributed packet routing algorithms in the literature: distance-vector algorithms and link-state algorithms.",
                "While this line of studies carry a certain similarity with our work, it has mainly focused on packet-switched communication networks.",
                "In this domain, the destination of a packet is deterministic and unique.",
                "Each agent maintains estimations, probabilistically or deterministically, on the distance to a certain destination through its neighbors.",
                "A variant of Q-Learning techniques is deployed The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 237 Table 1: Cumulative Utility for Datasets TREC-123-100-Random and TREC-123-100-Source with Hierarchical Organization; The percentage numbers in the columns TSLA-Random and TSLA-Source demonstrate the performance gain over the algorithm without learning Query number TSNA-Random TSLA-Random TSNA-Source TSLA-Source 500 25.15 28.45 13% 24.00 21.05 -13% 1000 104.99 126.74 20% 93.95 96.44 2.6% 1250 149.79 168.40 12% 122.64 134.05 9.3% 1500 188.94 211.05 12% 155.30 189.60 22% 1750 235.49 261.60 11% 189.14 243.90 28% 2000 275.09 319.25 16% 219.0 278.80 26% to update the estimations to converge to the real distances.",
                "It has been discovered that the locality property is an important feature of information retrieval systems in user modeling studies[3].",
                "In P2P based content sharing systems, this property is exemplified by the phenomenon that users tend to send queries that represent only a limited number of topics and conversely, users in the same neighborhood are likely to share common interests and send similar queries [10].",
                "The learning based approach is perceived to be more beneficial for real distributed information retrieval systems which exhibit locality property.",
                "This is because the users traffic and query patterns can reduce the state space and speed up the learning process.",
                "Related work in taking advantage of this property include [7], where the authors attempted to address this problem by user modeling techniques. 6.",
                "CONCLUSIONS In this paper, a reinforcement-learning based approach is developed to improve the performance of distributed IR search algorithms.",
                "Particularly, agents maintain estimates, namely expected utility, on the downstream agents ability to provide relevant documents for incoming queries.",
                "These estimates are updated gradually by learning from the feedback information returned from previous search sessions.",
                "Based on the updated expected utility information, the agents modify their routing policies.",
                "Thereafter, these agents route the queries based on the learned policies and update the estimates on the expected utility based on the new routing policies.",
                "The experiments on two different distributed IR datasets illustrates that the reinforcement learning approach improves considerably the cumulative utility over time. 7.",
                "REFERENCES [1] S. Abdallah and V. Lesser.",
                "Learning the task allocation game.",
                "In AAMAS 06: Proceedings of the fifth international joint conference on Autonomous agents and multiagent systems, pages 850-857, New York, NY, USA, 2006.",
                "ACM Press. [2] J.",
                "A. Boyan and M. L. Littman.",
                "Packet routing in dynamically changing networks: A reinforcement learning approach.",
                "In Advances in Neural Information Processing Systems, volume 6, pages 671-678.",
                "Morgan Kaufmann Publishers, Inc., 1994. [3] J. C. French, A. L. Powell, J. P. Callan, C. L. Viles, T. Emmitt, K. J. Prey, and Y. Mou.",
                "Comparing the performance of database selection algorithms.",
                "In Research and Development in Information Retrieval, pages 238-245, 1999. [4] B. Horling, R. Mailler, and V. Lesser.",
                "Farm: A scalable environment for multi-agent development and evaluation.",
                "In Advances in Software Engineering for Multi-Agent Systems, pages 220-237, Berlin, 2004.",
                "Springer-Verlag. [5] M. Littman and J. Boyan.",
                "A distributed reinforcement learning scheme for network routing.",
                "In Proceedings of the International Workshop on Applications of Neural Networks to Telecommunications, 1993. [6] J. Lu and J. Callan.",
                "Federated search of text-based digital libraries in hierarchical peer-to-peer networks.",
                "In In ECIR05, 2005. [7] J. Lu and J. Callan.",
                "User modeling for full-text federated search in peer-to-peer networks.",
                "In ACM SIGIR 2006.",
                "ACM Press, 2006. [8] C. D. Manning and H. Sch¨utze.",
                "Foundations of Statistical Natural Language Processing.",
                "The MIT Press, Cambridge, Massachusetts, 1999. [9] C. R. Palmer and J. G. Steffan.",
                "Generating network topologies that obey power laws.",
                "In Proceedings of GLOBECOM 2000, November 2000. [10] K. Sripanidkulchai, B. Maggs, and H. Zhang.",
                "Efficient content location using interest-based locality in peer-topeer systems.",
                "In INFOCOM, 2003. [11] D. Subramanian, P. Druschel, and J. Chen.",
                "Ants and reinforcement learning: A case study in routing in dynamic networks.",
                "In In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence, pages 832-839, 1997. [12] J. N. Tao and L. Weaver.",
                "A multi-agent, policy gradient approach to network routing.",
                "In In Proceedings of the Eighteenth International Conference on Machine Learning, 2001. [13] H. Zhang, W. B. Croft, B. Levine, and V. Lesser.",
                "A multi-agent approach for peer-to-peer information retrieval.",
                "In Proceedings of Third International Joint Conference on Autonomous Agents and Multi-Agent Systems, July 2004. [14] H. Zhang and V. Lesser.",
                "Multi-agent based <br>peer-to-peer information retrieval system</br>s with concurrent search sessions.",
                "In Proceedings of the Fifth International Joint Conference on Autonomous Agents and Multi-Agent Systems, May 2006. [15] H. Zhang and V. R. Lesser.",
                "A dynamically formed hierarchical agent organization for a distributed content sharing system.",
                "In 2004 IEEE/WIC/ACM International Conference on Intelligent Agent Technology (IAT 2004), 20-24 September 2004, Beijing, China, pages 169-175.",
                "IEEE Computer Society, 2004. 238 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "Multi-agent based <br>peer-to-peer information retrieval system</br>s with concurrent search sessions."
            ],
            "translated_annotated_samples": [
                "Sistemas de recuperación de información peer-to-peer basados en múltiples agentes con sesiones de búsqueda concurrentes."
            ],
            "translated_text": "Un algoritmo de búsqueda distribuida basado en aprendizaje por refuerzo para sistemas jerárquicos de recuperación de información entre pares. En estos enfoques, los agentes dependen de la similitud de contenido entre las consultas entrantes y sus agentes vecinos directos para dirigir las sesiones de búsqueda distribuida. Sin embargo, tal heurística es miope en el sentido de que los agentes vecinos pueden no estar conectados con agentes más relevantes. En este documento, se desarrolla un enfoque basado en aprendizaje por refuerzo en línea para aprovechar las características dinámicas en tiempo de ejecución de los sistemas de recuperación de información P2P, representadas por información sobre sesiones de búsqueda pasadas. Específicamente, los agentes mantienen estimaciones sobre las habilidades de los agentes aguas abajo para proporcionar documentos relevantes para las consultas entrantes. Estas estimaciones se actualizan gradualmente al aprender de la información de retroalimentación devuelta de sesiones de búsqueda anteriores. Basándose en esta información, los agentes derivan políticas de enrutamiento correspondientes. Posteriormente, estos agentes dirigen las consultas basándose en las políticas aprendidas y actualizan las estimaciones según las nuevas políticas de enrutamiento. Los resultados experimentales demuestran que el algoritmo de aprendizaje mejora considerablemente el rendimiento del enrutamiento en dos conjuntos de colecciones de pruebas que han sido utilizados en una variedad de estudios de recuperación de información distribuida. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagente Términos Generales Algoritmos, Rendimiento, Experimentación 1. En los últimos años ha habido un creciente interés en estudiar cómo controlar los procesos de búsqueda en sistemas de recuperación de información (IR) basados en pares (P2P) [6, 13, 14, 15]. En esta línea de investigación, uno de los problemas centrales que preocupa a los investigadores es dirigir de manera eficiente las consultas de los usuarios en la red a los agentes que poseen los documentos apropiados. En ausencia de información global, las estrategias dominantes para abordar este problema son enfoques basados en la similitud de contenido [6, 13, 14, 15]. Si bien la similitud de contenido entre consultas y nodos locales parece ser un indicador creíble para la cantidad de documentos relevantes que residen en cada nodo, estos enfoques están limitados por una serie de factores. En primer lugar, las métricas basadas en similitud pueden ser miope ya que los nodos localmente relevantes pueden no estar conectados a otros nodos relevantes. Segundo, los enfoques basados en la similitud no tienen en cuenta las características de tiempo de ejecución de los sistemas de IR P2P, incluidos los parámetros ambientales, el uso del ancho de banda y la información histórica de las sesiones de búsqueda pasadas, que proporcionan información valiosa para los algoritmos de enrutamiento de consultas. En este artículo, desarrollamos un enfoque de IR basado en aprendizaje por refuerzo para mejorar el rendimiento de los algoritmos de búsqueda de IR distribuidos. Los agentes pueden adquirir mejores estrategias de búsqueda al recopilar y analizar información de retroalimentación de sesiones de búsqueda anteriores. En particular, los agentes mantienen estimaciones, es decir, utilidad esperada, sobre las capacidades de los agentes aguas abajo de proporcionar documentos relevantes para tipos específicos de consultas entrantes. Estas estimaciones se actualizan gradualmente al aprender de la información de retroalimentación devuelta de sesiones de búsqueda anteriores. Basándose en la información actualizada de utilidad esperada, los agentes derivan políticas de enrutamiento correspondientes. Posteriormente, estos agentes dirigen las consultas basándose en las políticas aprendidas y actualizan las estimaciones sobre la utilidad esperada en función de las nuevas políticas de enrutamiento. Este proceso se lleva a cabo de manera iterativa. El objetivo del algoritmo de aprendizaje, aunque consume ancho de banda de red, es reducir el tiempo de enrutamiento para procesar más consultas por unidad de tiempo y al mismo tiempo encontrar más documentos relevantes. Esto contrasta con los enfoques basados en la similitud de contenido, donde se repiten operaciones similares para cada consulta entrante y el tiempo de procesamiento se mantiene en gran medida constante con el tiempo. Otra forma de ver este documento es que nuestro enfoque básico para la búsqueda de IR distribuida es construir una red de superposición jerárquica (organización de agentes) basada en la medida de similitud de contenido entre las colecciones de documentos de los agentes de manera ascendente. En trabajos anteriores, hemos demostrado que esta organización mejora significativamente el rendimiento de búsqueda. Sin embargo, esta estructura organizativa no tiene en cuenta los patrones de llegada de las consultas, incluyendo su frecuencia, tipos y dónde entran en el sistema, ni el ancho de banda de comunicación disponible en la red y las capacidades de procesamiento de los agentes individuales. La intención del aprendizaje por refuerzo es adaptar las decisiones de enrutamiento de los agentes a las situaciones dinámicas de la red y aprender de sesiones de búsqueda pasadas. Específicamente, las contribuciones de este artículo incluyen: (1) un enfoque basado en aprendizaje por refuerzo para que los agentes adquieran políticas de enrutamiento satisfactorias basadas en estimaciones de la contribución potencial de sus agentes vecinos; (2) dos estrategias para acelerar el proceso de aprendizaje. Según nuestro mejor conocimiento, esta es una de las primeras aplicaciones de aprendizaje por refuerzo para abordar problemas de compartición de contenido distribuido, lo cual indica algunos de los problemas al aplicar el refuerzo en una aplicación compleja. El resto de este documento está organizado de la siguiente manera: la Sección 2 revisa los sistemas jerárquicos de intercambio de contenido y el algoritmo de búsqueda de dos fases basado en dicha topología. La Sección 3 describe un enfoque basado en aprendizaje por refuerzo para dirigir el proceso de enrutamiento; la Sección 4 detalla la configuración experimental y analiza los resultados. La sección 5 discute estudios relacionados y la sección 6 concluye el artículo. BÚSQUEDA EN SISTEMAS JERÁRQUICOS DE IR P2P Esta sección revisa brevemente nuestros enfoques básicos para los sistemas jerárquicos de IR P2P. En un sistema jerárquico de recuperación de información P2P ilustrado en la Fig.1, los agentes están conectados entre sí a través de tres tipos de enlaces: enlaces ascendentes, enlaces descendentes y enlaces laterales. En las siguientes secciones, denotamos el conjunto de agentes que están directamente conectados al agente Ai como DirectConn(Ai), el cual se define como DirectConn(Ai) = NEI(Ai) ∪ PAR(Ai) ∪ CHL(Ai), donde NEI(Ai) es el conjunto de agentes vecinos conectados a Ai a través de enlaces laterales; PAR(Ai) es el conjunto de agentes a los que el agente Ai está conectado a través de enlaces ascendentes y CHL(Ai) es el conjunto de agentes a los que el agente Ai se conecta a través de enlaces descendentes. Estos enlaces se establecen a través de un proceso de agrupamiento distribuido basado en la similitud de contenido de abajo hacia arriba[15]. Estos enlaces son luego utilizados por agentes para localizar otros agentes que contienen documentos relevantes a las consultas dadas. Un agente típico Ai en nuestro sistema utiliza dos colas: una cola de búsqueda local, LSi, y una cola de reenvío de mensajes MFi. Los estados de las dos colas constituyen los estados internos de un agente. La cola de búsqueda local LSi almacena sesiones de búsqueda programadas para procesamiento local. Es una cola de prioridad y el agente Ai siempre selecciona las consultas más prometedoras para procesarlas con el fin de maximizar la utilidad global. MFi consiste en un conjunto de consultas para reenviar y se procesa de manera FIFO (primero en entrar, primero en salir). Para la primera consulta en MFi, el agente Ai determina a qué subconjunto de sus agentes vecinos reenviarla basándose en la política de enrutamiento de los agentes πi. Estas decisiones de enrutamiento determinan cómo se lleva a cabo el proceso de búsqueda en la red. En este documento, llamamos a Ai como el agente aguas arriba de Ajs y a Aj como el agente aguas abajo de Ais si A4 A5 A6 A7 A2 A3 A9 NEI(A2)={A3} PAR(A2)={A1} CHL(A2)={A4,A5} A1 A8 Figura 1: Una fracción de un sistema jerárquico P2PIR en el que un agente Ai dirige una consulta al agente Aj. El protocolo de búsqueda distribuida de nuestra organización de agentes jerárquicos está compuesto por dos pasos. En el primer paso, al recibir una consulta qk en el tiempo tl de un usuario, el agente Ai inicia una sesión de búsqueda si al sondear a sus agentes vecinos Aj ∈ NEI(Ai) con el mensaje PROBE para el valor de similitud Sim(qk, Aj) entre qk y Aj. Aquí, Ai se define como el iniciador de la consulta de la sesión de búsqueda si. En el segundo paso, Ai selecciona un grupo de agentes más prometedores para iniciar el proceso de búsqueda real con el mensaje BUSCAR. Estos mensajes de BÚSQUEDA contienen un parámetro TTL (Tiempo de Vida) además de la consulta. El valor TTL disminuye en 1 después de cada salto. En el proceso de búsqueda, los agentes descartan aquellas consultas que han sido procesadas previamente o cuyo TTL disminuye a 0, lo que evita que las consultas se repitan infinitamente en el sistema. La sesión de búsqueda termina cuando todos los agentes que reciben la consulta la descartan o el TTL disminuye a 0. Tras recibir mensajes de BÚSQUEDA para qk, los agentes programan actividades locales que incluyen búsqueda local, reenvío de qk a sus vecinos y devolución de resultados de búsqueda al iniciador de la consulta. Este proceso y los algoritmos relacionados se detallan en [15, 14]. 3. En el enfoque de búsqueda basado en aprendizaje por refuerzo, mencionado anteriormente, las decisiones de enrutamiento de un agente Ai dependen de la comparación de similitud entre las consultas entrantes y los agentes vecinos de Ai para reenviar esas consultas a agentes relevantes sin inundar la red con mensajes de consulta innecesarios. Sin embargo, esta heurística es miope porque un vecino directo relevante no necesariamente está conectado con otros agentes relevantes. En esta sección, proponemos un enfoque más general al plantear este problema como una tarea de aprendizaje por refuerzo. En busca de una mayor flexibilidad, los agentes pueden alternar entre dos modos: modo de aprendizaje y modo sin aprendizaje. En el modo de no aprendizaje, los agentes operan de la misma manera que lo hacen en los procesos de búsqueda distribuida normales descritos en [14, 15]. Por otro lado, en el modo de aprendizaje, en paralelo con las sesiones de búsqueda distribuida, los agentes también participan en un proceso de aprendizaje que se detallará en esta sección. Ten en cuenta que en el protocolo de aprendizaje, el proceso de aprendizaje no interfiere con el proceso de búsqueda distribuida. Los agentes pueden elegir iniciar y detener procesos de aprendizaje sin afectar el rendimiento del sistema. En particular, dado que el proceso de aprendizaje consume recursos de red (especialmente ancho de banda), los agentes pueden optar por iniciar el aprendizaje solo cuando la carga de la red es relativamente baja, minimizando así los costos adicionales de comunicación incurridos por el algoritmo de aprendizaje. La sección está estructurada de la siguiente manera, la Sección 3.1 describe 232 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) un modelo basado en aprendizaje por refuerzo. La sección 3.2 describe un protocolo para implementar el algoritmo de aprendizaje en la red. La sección 3.3 discute la convergencia del algoritmo de aprendizaje. 3.1 El Modelo La política de enrutamiento de un agente toma el estado de una sesión de búsqueda como entrada y produce las acciones de enrutamiento para esa consulta. En nuestro trabajo, el estado de una sesión de búsqueda sj se estipula como: QSj = (qk, ttlj) donde ttlj es el número de saltos que quedan para la sesión de búsqueda sj, qk es la consulta específica. QL es un atributo de qk que indica a qué tipo de consultas qk probablemente pertenecen. El conjunto de QL puede ser generado ejecutando un algoritmo de clasificación en línea simple en todas las consultas que han sido procesadas por los agentes, o un algoritmo fuera de línea en un conjunto de entrenamiento pre-designado. La suposición aquí es que el conjunto de tipos de consultas se aprende de antemano y pertenece al conocimiento común de los agentes en la red. El trabajo futuro incluye explorar cómo se puede lograr el aprendizaje cuando esta suposición no se cumple. Dado el conjunto de tipos de consultas establecido, una consulta entrante qi puede ser clasificada en una clase de consulta Q(qi) mediante la fórmula: Q(qi) = arg max Qj P(qi|Qj) (1) donde P(qi|Qj) indica la probabilidad de que la consulta qi sea generada por la clase de consulta Qj [8]. El conjunto de acciones de enrutamiento atómico de un agente Ai se denota como {αi}, donde {αi} se define como αi = {αi0 , αi1 , ..., αin }. Un elemento αij representa una acción para dirigir una consulta dada al agente vecino Aij ∈ DirectConn(Ai). La política de enrutamiento πi del agente Ai es estocástica y su resultado para una sesión de búsqueda con estado QSj se define como: πi(QSj) = {(αi0 , πi(QSi, αi0 )), (αi1 , πi(QSi, αi1 )), ...} (2) Nótese que el operador πi está sobrecargado para representar tanto la política probabilística para una sesión de búsqueda con estado QSj, denotada como πi(QSj); como la probabilidad de enviar la consulta a un agente vecino específico Aik ∈ DirectConn(Ai) bajo la política πi(QSj), denotada como πi(QSj, αik). Por lo tanto, la ecuación (2) significa que la probabilidad de reenviar la sesión de búsqueda al agente Ai0 es πi(QSi, αi0) y así sucesivamente. Bajo esta política estocástica, la acción de enrutamiento es no determinística. La ventaja de esta estrategia es que los mejores agentes vecinos no serán seleccionados repetidamente, mitigando así las posibles situaciones de puntos calientes. La utilidad esperada, Un i (QSj), se utiliza para estimar la ganancia potencial de utilidad al dirigir el tipo de consulta QSj al agente Ai bajo la política πn i. El superíndice n indica el valor en la n-ésima iteración en un proceso de aprendizaje iterativo. La utilidad esperada proporciona orientación de enrutamiento para futuras sesiones de búsqueda. En el proceso de búsqueda, cada agente Ai mantiene observaciones parciales de los estados de sus vecinos, como se muestra en la Figura 2. La observación parcial incluye información no local como la estimación de la utilidad potencial de su vecino Am para el estado de consulta QSj, denotado como Um(QSj), así como la información de carga, Lm. Estas observaciones son actualizadas periódicamente por los vecinos. La información de utilidad estimada se utilizará para actualizar la utilidad esperada de A en su política de enrutamiento. Cargar información de utilidad esperada para diferentes tipos de consultas de agentes vecinos... A0 A1 A3 A2 Un 0 (QS0) ... ... ... ... ...... Un 0 (QS1) Un 1 (QS1) Un 2 (QS1) Un 3 (QS1) Un 1 (QS0) Un 2 (QS0) Un 3 (QS0) Ln 0 Ln 1 Ln 2 Ln 3 ... QS0 QS1 ... Figura 2: La Observación Parcial del Agente Ais sobre sus vecinos (A0, A1...) La información de carga de Am, Lm, se define como Lm = |MFm| Cm, donde |MFm| es la longitud de la cola de mensajes en espera y Cm es la tasa de servicio de la cola de mensajes en espera del agente Am. Por lo tanto, Lm caracteriza la utilización de un canal de comunicación de agentes, y así proporciona información no local a los vecinos de Ams para ajustar los parámetros de su política de enrutamiento y evitar inundar a sus agentes aguas abajo. Ten en cuenta que, basado en las características de las consultas que ingresan al sistema y las capacidades de los agentes, la carga de trabajo de los agentes puede no ser uniforme. Después de recopilar la información de la tasa de utilización de todos sus vecinos, el agente Ai calcula Li como una medida única para evaluar la condición de carga promedio de su vecindario: Li = P k Lk |DirectConn(Ai)| Los agentes explotan el valor de Li para determinar la probabilidad de enrutamiento en su política de enrutamiento. Se debe tener en cuenta que, como se describe en la Sección 3.2, la información sobre agentes vecinos se transmite junto con el mensaje de consulta propagado entre los agentes siempre que sea posible para reducir la sobrecarga de tráfico. 3.1.1 Actualización de la Política Se introduce un proceso de actualización iterativo para que los agentes aprendan una política de enrutamiento estocástica satisfactoria. En este proceso iterativo, los agentes actualizan sus estimaciones sobre la utilidad potencial de sus políticas de enrutamiento actuales y luego propagan las estimaciones actualizadas a sus vecinos. Sus vecinos luego generan una nueva política de enrutamiento basada en la observación actualizada y a su vez calculan la utilidad esperada basada en las nuevas políticas y continúan este proceso iterativo. En particular, en el tiempo n, dado un conjunto de utilidad esperada, un agente Ai, cuyo conjunto de agentes directamente conectados es DirectConn(Ai) = {Ai0, ..., Aim}, determina su política de enrutamiento estocástico correspondiente para una sesión de búsqueda del estado QSj basada en los siguientes pasos: (1) Ai primero selecciona un subconjunto de agentes como los agentes potenciales aguas abajo del conjunto DirectConn(Ai), denotado como PDn(Ai, QSj). El tamaño del agente potencial aguas abajo se especifica como |PDn(Ai, QSj)| = min(|NEI(Ai), dn i + k)| donde k es una constante y se establece en 3 en este documento; dn i, el ancho hacia adelante, se define como el número esperado de The Sixth Intl. La fórmula especifica que el conjunto potencial de agentes aguas abajo PDn(Ai, QSj) es o bien el subconjunto de agentes vecinos con los k valores de utilidad esperada más altos para el estado QSj entre todos los agentes en DirectConn(Ai), o todos sus agentes vecinos. La k se introduce basada en la idea de una política de enrutamiento estocástica y hace que la probabilidad de reenvío del agente dn i + k más alto sea inferior al 100%. Ten en cuenta que si queremos limitar el número de agentes secundarios para la sesión de búsqueda sj a 5, la probabilidad de reenviar la consulta a todos los agentes vecinos debe sumar 5. Configurar adecuadamente el valor de dn i puede mejorar la tasa de utilización del ancho de banda de la red cuando gran parte de la red está inactiva, al mismo tiempo que mitiga la carga de tráfico cuando la red está altamente cargada. El valor de dn+1 i se actualiza en función de dn i, las observaciones anteriores y actuales sobre la situación del tráfico en el vecindario. Específicamente, la fórmula de actualización para dn+1 i es dn+1 i = dn i ∗ (1 + 1 − Li |DirectConn(Ai)| ) En esta fórmula, el ancho de avance se actualiza en función de las condiciones de tráfico del vecindario del agente Ai, es decir, Li, y su valor anterior. (2) Para cada agente Aik en el PDn(Ai, QSj), la probabilidad de reenviar la consulta a Aik se determina de la siguiente manera para asignar una mayor probabilidad de reenvío a los agentes vecinos con un valor de utilidad esperado más alto: πn+1 i (QSj, αik ) = dn+1 i |PDn(Ai, QSj)| + β ∗ ` Uik (QSj) − PDU(Ai, QSj) |PDn(Ai, QSj)| ´ (3) donde PDUn(Ai, QSj) = X o∈P Dn(Ai,QSj ) Uo(QSj) y QSj es el estado subsiguiente del agente Aik después de que el agente Ai reenvía la sesión de búsqueda con estado QSj a su agente vecino Aik; Si QSj = (qk, ttl0), entonces QSj = (qk, ttl0 − 1). En la fórmula 3, el primer término a la derecha de la ecuación, dn+1 i |P Dn(Ai,QSj )|, se utiliza para determinar la probabilidad de reenvío distribuyendo equitativamente el ancho de avance, dn+1 i, a los agentes en el conjunto PDn(Ai, QSj). El segundo término se utiliza para ajustar la probabilidad de ser elegido de manera que se favorezca a los agentes con valores de utilidad esperada más altos. β se determina de acuerdo a: β = min ` m − dn+1 i m ∗ umax − PDUn(Ai, QSj) , dn+1 i PDUn(Ai, QSj) − m ∗ umin ´ (4) donde m = |PDn(Ai, QSj)|, umax = max o∈P Dn(Ao,QSj ) Uo(QSj) y umin = min o∈P Dn(Ao,QSj ) Uo(QSj) Esta fórmula garantiza que el valor final de πn+1 i (QSj, αik ) esté bien definido, es decir, 0 ≤ πn+1 i (QSj, αik ) ≤ 1 y X i πn+1 i (QSj, αik ) = dn+1 i Sin embargo, esta solución no explora todas las posibilidades. Para equilibrar entre la explotación y la exploración, se adopta un enfoque λ-Greedy. En el enfoque λ-Greedy, además de asignar una probabilidad más alta a aquellos agentes con un valor de utilidad esperado más alto, como en la ecuación (3). Los agentes que parecen no ser buenas opciones también recibirán consultas basadas en una tasa de exploración dinámica. En particular, para agentes en el conjunto PDn(Ai, QSj), πn+1 i1 (QSj) se determina de la misma manera que arriba, con la única diferencia de que dn+1 i es reemplazado por dn+1 i ∗ (1 − λn). El ancho de banda de búsqueda restante se utiliza para el aprendizaje asignando la probabilidad λn de manera uniforme a los agentes Ai2 en el conjunto DirectConn(Ai) - PDn(Ai, QSj). πn+1 i2 (QSj, αik ) = dn+1 i ∗ λn |DirectConn(Ai) - PDn(Ai, QSj)| (5) donde PDn(Ai, QSj) ⊂ DirectConn(Ai). Se debe tener en cuenta que la tasa de exploración λ no es constante y disminuye con el tiempo. El λ se determina de acuerdo con la siguiente ecuación: λn+1 = λ0 ∗ e−c1n (6) donde λ0 es la tasa de exploración inicial, que es una constante; c1 también es una constante para ajustar la tasa de disminución de la tasa de exploración; n es la unidad de tiempo actual. 3.1.2 Actualización de la Utilidad Esperada Una vez que la política de enrutamiento en el paso n+1, πn+1 i, se determina según la fórmula anterior, el agente Ai puede actualizar su propia utilidad esperada, Un+1 i (QSi), basándose en la política de enrutamiento actualizada resultante de la fórmula 5 y los valores U actualizados de sus agentes vecinos. Bajo el supuesto de que después de que una consulta se envía a los vecinos de Ai, las sesiones de búsqueda subsiguientes son independientes, la fórmula de actualización es similar a la fórmula de actualización de Bellman en Q-Learning: Un+1 i (QSj) = (1 − θi) ∗ Un i (QSj) + θi ∗ (Rn+1 i (QSj) + X k πn+1 i (QSj, αik )Un k (QSj)) (7) donde QSj = (Qj, ttl − 1) es el siguiente estado de QSj = (Qj, ttl); Rn+1 i (QSj) es la recompensa local esperada para la clase de consulta Qk en el agente Ai bajo la política de enrutamiento πn+1 i; θi es el coeficiente para decidir cuánto peso se le da al valor antiguo durante el proceso de actualización: cuanto menor sea el valor de θi, se espera que el agente aprenda más rápido el valor real, mientras que mayor sea la volatilidad del algoritmo, y viceversa. Rn+1 (s) se actualiza de acuerdo con la siguiente ecuación: 234 The Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Rn+1 i (QSj) = Rn i (QSj) +γi ∗ (r(QSj) − Rn i (QSj)) ∗ P(qj|Qj ) (8) donde r(QSj) es la recompensa local asociada con la sesión de búsqueda. P(qj|Qj) indica qué tan relevante es la consulta qj para el tipo de consulta Qj, y γi es la tasa de aprendizaje para el agente Ai. Dependiendo de la similitud entre una consulta específica qi y su tipo de consulta correspondiente Qi, la recompensa local asociada con la sesión de búsqueda tiene un impacto diferente en la estimación de Rn i (QSj). En la fórmula anterior, este impacto se refleja en el coeficiente, el valor P(qj|Qj). Función de recompensa Después de que una sesión de búsqueda se detiene cuando sus valores TTL expiran, todos los resultados de búsqueda se devuelven al usuario y se comparan con la evaluación de relevancia. Suponiendo que el conjunto de resultados de búsqueda es SR, la recompensa Rew(SR) se define como: Rew(SR) = j 1 si |Rel(SR)| > c |Rel(SR)| c en caso contrario, donde SR es el conjunto de resultados de búsqueda devueltos, Rel(SR) es el conjunto de documentos relevantes en los resultados de búsqueda. Esta ecuación especifica que los usuarios otorgan una recompensa de 1.0 si el número de documentos relevantes devueltos alcanza un número predefinido c. De lo contrario, la recompensa es proporcional al número de documentos relevantes devueltos. La razón para establecer un valor de corte de este tipo es que la importancia de la proporción de recuperación disminuye con la abundancia de documentos relevantes en el mundo real, por lo tanto, los usuarios tienden a enfocarse solo en un número limitado de resultados buscados. Los detalles del protocolo de enrutamiento actual se introducirán en la Sección 3.2 cuando presentemos cómo se implementa el algoritmo de aprendizaje en sistemas reales. 3.2 Implementación del algoritmo de aprendizaje. Esta sección describe cómo se puede utilizar el algoritmo de aprendizaje en un proceso de búsqueda de una o dos fases. En el algoritmo de búsqueda de una sola fase, las sesiones de búsqueda comienzan desde los iniciadores de las consultas. Por el contrario, en el algoritmo de búsqueda de dos pasos, el iniciador de la consulta primero intenta buscar un punto de inicio más apropiado para la consulta introduciendo un paso exploratorio como se describe en la Sección 2. A pesar de la diferencia en la calidad de los puntos de partida, la mayor parte del proceso de aprendizaje para los dos algoritmos es en gran medida el mismo como se describe en los párrafos siguientes. Antes de que comience el aprendizaje, cada agente inicializa el valor de utilidad esperado para todos los posibles estados como 0. Posteriormente, al recibir una consulta, además de las operaciones normales descritas en la sección anterior, un agente Ai también configura un temporizador para esperar los resultados de búsqueda devueltos por sus agentes subordinados. Una vez que el temporizador expire o haya recibido respuesta de todos sus agentes aguas abajo, Ai fusiona y reenvía los resultados de búsqueda acumulados de sus agentes aguas abajo a su agente aguas arriba. Configurar el temporizador acelera el aprendizaje porque los agentes pueden evitar esperar demasiado tiempo a que los agentes aguas abajo devuelvan los resultados de búsqueda. Ten en cuenta que estos resultados detallados y la información correspondiente del agente seguirán almacenados en Ai hasta que la información de retroalimentación sea transmitida desde su agente aguas arriba y se pueda evaluar el rendimiento de sus agentes aguas abajo. La duración del temporizador está relacionada con el valor TTL. En este documento, configuramos el temporizador a ttimer = ttli ∗ 2 + tf, donde ttli ∗ 2 es la suma del tiempo de viaje de las consultas en la red, y tf es el período de tiempo esperado que los usuarios desearían esperar. Los resultados de la búsqueda serán finalmente devueltos al iniciador de la sesión de búsqueda A0. Serán comparados con la evaluación de relevancia proporcionada por los usuarios finales (como se describe en la sección del experimento, la evaluación de relevancia para el conjunto de consultas se proporciona junto con las colecciones de datos). La recompensa será calculada y propagada hacia atrás a los agentes a lo largo del camino por el que se pasaron los resultados de la búsqueda. Este es un proceso inverso de la propagación de los resultados de búsqueda. En el proceso de propagar la recompensa hacia atrás, los agentes actualizan las estimaciones de su propio valor potencial de utilidad, generan una política actualizada y transmiten sus resultados actualizados a los agentes vecinos basándose en el algoritmo descrito en la Sección 3. Tras el cambio del valor de utilidad esperado, el agente Ai envía su estimación de utilidad actualizada a sus vecinos para que puedan actuar según la utilidad esperada modificada y el estado correspondiente. Este mensaje de actualización incluye la recompensa potencial, así como el estado correspondiente QSi = (qk, ttll) del agente Ai. Cada agente vecino, Aj, reacciona a este tipo de mensaje de actualización actualizando el valor de utilidad esperado para el estado QSj(qk, ttll + 1) de acuerdo con el valor de utilidad esperado cambiado recién anunciado. Una vez que completen la actualización, los agentes informarían nuevamente a los vecinos relacionados para que actualicen sus valores. Este proceso continúa hasta que el valor TTL en el mensaje de actualización aumenta al límite de TTL. Para acelerar el proceso de aprendizaje, al actualizar los valores de utilidad esperada de un agente Ai con agentes vecinos, especificamos que Um(Qk, ttl0) >= Um(Qk, ttl1) si ttl0 > ttl1. Por lo tanto, cuando el agente Ai recibe un valor de utilidad esperada actualizado con ttl1, también actualiza los valores de utilidad esperada con cualquier ttl0 > ttl1 si Um(Qk, ttl0) < Um(Qk, ttl1) para acelerar la convergencia. Esta heurística se basa en el hecho de que la utilidad de una sesión de búsqueda es una función no decreciente del tiempo t. 3.3 Discusión Al formalizar el sistema de enrutamiento de contenido como una tarea de aprendizaje, se hacen muchas suposiciones. En sistemas reales, estas suposiciones pueden no cumplirse, por lo que el algoritmo de aprendizaje puede no converger. Dos problemas son de particular importancia, (1) Este problema de enrutamiento de contenido no tiene propiedades de Markov. A diferencia del enrutamiento de paquetes basado en el nivel de IP, la decisión de enrutamiento de cada agente para una sesión de búsqueda particular sj depende del historial de enrutamiento de sj. Por lo tanto, la suposición de que todas las sesiones de búsqueda posteriores son independientes no se cumple en la realidad. Esto puede llevar a un problema de doble conteo en el que los documentos relevantes de algunos agentes se contarán más de una vez para el estado donde el valor TTL es mayor que 1. Sin embargo, en el contexto de las organizaciones de agentes jerárquicos, dos factores mitigan estos problemas: primero, los agentes en cada grupo de contenido forman una estructura similar a un árbol. Con la ausencia de los ciclos, las estimaciones dentro del árbol estarían cerca del valor preciso. En segundo lugar, la naturaleza estocástica de la política de enrutamiento remedia parcialmente este problema. El Sexto Internacional. Otro desafío para este algoritmo de aprendizaje es que en un entorno de red real, las observaciones sobre agentes vecinos pueden no poder actualizarse a tiempo debido al retraso en la comunicación u otras situaciones. Además, cuando los agentes vecinos actualizan sus estimaciones al mismo tiempo, puede surgir oscilación durante el proceso de aprendizaje[1]. Este documento explora varios enfoques para acelerar el proceso de aprendizaje. Además de la estrategia mencionada de actualizar los valores de utilidad esperada, también empleamos una estrategia de actualización activa donde los agentes notifican a sus vecinos cada vez que se actualiza su utilidad esperada. Por lo tanto, se puede lograr una velocidad de convergencia más rápida. Esta estrategia contrasta con la actualización perezosa, donde los agentes solo repiten a sus agentes vecinos con el cambio en la utilidad esperada cuando intercambian información. El compromiso entre los dos enfoques es la carga de red versus la velocidad de aprendizaje. La ventaja de este algoritmo de aprendizaje es que una vez que se aprende una política de enrutamiento, los agentes no tienen que comparar repetidamente la similitud de las consultas siempre y cuando la topología de la red permanezca sin cambios. En cambio, el agente solo tiene que determinar adecuadamente la clasificación de la consulta y seguir las políticas aprendidas. La desventaja de este enfoque basado en el aprendizaje es que el proceso de aprendizaje debe llevarse a cabo cada vez que la estructura de la red cambia. Hay muchas posibles extensiones para este modelo de aprendizaje. Por ejemplo, actualmente se utiliza una sola medida para indicar la carga de tráfico en un vecindario de agentes. Una extensión sencilla sería llevar un registro de la carga individual de cada vecino del agente. 4. CONFIGURACIÓN DE EXPERIMENTOS Y RESULTADOS Los experimentos se llevan a cabo en el kit de simulación TRANO con dos conjuntos de datos, TREC-VLC-921 y TREC123-100. Las siguientes subsecciones presentan el banco de pruebas TRANO, los conjuntos de datos y los resultados experimentales. 4.1 Banco de pruebas TRANO TRANO (Task Routing on Agent Network Organization) es un banco de pruebas de recuperación de información basado en red de agentes múltiples. TRANO se construye sobre el Farm [4], un simulador distribuido basado en el tiempo que proporciona un marco de diseminación de datos para organizaciones de redes de agentes distribuidos a gran escala. TRANO apoya la importación y exportación de perfiles de organizaciones de agentes, incluyendo conexiones topológicas y otras características. Cada agente de TRANO está compuesto por una estructura de vista de agente y una unidad de control. En la simulación, cada agente es pulsado regularmente y el agente verifica las colas de mensajes entrantes, realiza operaciones locales y luego reenvía mensajes a otros agentes. 4.2 Configuración Experimental En nuestro experimento, utilizamos dos conjuntos de datos estándar, los conjuntos de datos TRECVLC-921 y TREC-123-100, para simular las colecciones alojadas en los agentes. Los conjuntos de datos TREC-VLC-921 y TREC123-100 fueron creados por el Instituto Nacional de Tecnología Estándar de los Estados Unidos (NIST) para sus conferencias TREC. En el ámbito de la recuperación de información distribuida, las dos colecciones de datos se dividen en 921 y 100 subcolecciones. Se observa que el conjunto de datos TREC-VLC-921 es más heterogéneo que TREC-123-100 en términos de origen, longitud de documento y distribución de documentos relevantes a partir de las estadísticas de las dos colecciones de datos enumeradas en [13]. Por lo tanto, TREC-VLC-921 está mucho más cerca de las distribuciones reales de documentos en entornos P2P. Además, TREC-123-100 se divide en dos conjuntos de 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0 500 1000 1500 2000 2500 3000 Número de consulta de ARSS frente al número de consultas entrantes para TREC-VLC-921 SSLA-921 SSNA-921 Figura 3: ARSS (Recompensa promedio por sesión de búsqueda) frente al número de sesiones de búsqueda para la búsqueda de 1 fase en TREC-VLC-921 0 0.1 0.2 0.3 0.4 0.5 0.6 0 500 1000 1500 2000 2500 3000 Número de consulta de ARSS frente al número de consultas para TREC-VLC-921 TSLA-921 TSNA-921 Figura 4: ARSS (Recompensa promedio por sesión de búsqueda) frente al número de sesiones de búsqueda para la búsqueda de 2 fases en TREC-VLC-921 subcolecciones de dos formas: aleatoriamente y por fuente. Las dos particiones se denominan TREC-123-100-Random y TREC-123-100-Source respectivamente. Los documentos en cada subcolección en el conjunto de datos TREC-123-100-Source son más coherentes que los de TREC-123-100-Random. Los dos conjuntos diferentes de particiones nos permiten observar cómo el algoritmo de aprendizaje distribuido se ve afectado por la homogeneidad de las colecciones. La organización jerárquica de agentes es generada por el algoritmo descrito en nuestro algoritmo anterior [15]. Durante el proceso de generación de la topología, la información de grado de cada agente es estimada por el algoritmo introducido por Palmer et al. [9] con parámetros α = 0.5 y β = 0.6. En nuestros experimentos, estimamos el límite superior y el límite de grado descendente utilizando factores de descuento lineales de 0.5, 0.8 y 1.0. Una vez que se construye la topología, las consultas seleccionadas al azar del conjunto de consultas 301-350 en TREC-VLC-921 y del conjunto de consultas 1-50 en TREC-123-100-Random y TREC-123-100-Source se inyectan en el sistema basado en una distribución de Poisson P(N(t) = n) = (λt)n n! e−λ 236 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 0 50 100 150 200 250 300 350 400 0 500 1000 1500 2000 2500 3000 Utilidad acumulada Número de consulta Utilidad acumulada sobre el número de consultas entrantes TSLA-921 SSNA-921 SSLA-921 TSNA-921 Figura 5: La utilidad acumulada versus el número de sesiones de búsqueda TREC-VLC-921 Además, asumimos que todos los agentes tienen la misma probabilidad de recibir consultas del entorno, es decir, λ es el mismo para cada agente. En nuestros experimentos, λ se establece en 0.0543 para que la media de las consultas entrantes del entorno a la red de agentes sea de 50 por unidad de tiempo. El tiempo de servicio para la cola de comunicación y la cola de búsqueda local, es decir, tQij y trs, se establece en 0.01 unidades de tiempo y 0.05 unidades de tiempo respectivamente. En nuestros experimentos, hay diez tipos de consultas adquiridas mediante la agrupación del conjunto de consultas 301-350 y 1-50. 4.3 Análisis y evaluación de resultados. La Figura 3 muestra el ARSS (Recompensa Promedio por Sesión de Búsqueda) versus el número de consultas entrantes a lo largo del tiempo para el Algoritmo No Aprendido de un Solo Paso (SSNA) y el Algoritmo de Aprendizaje de un Solo Paso (SSLA) para la recolección de datos TREC-VLC-921. Se muestra que la recompensa promedio para el algoritmo SSNA oscila entre 0.02 y 0.06 y su rendimiento cambia poco con el tiempo. El promedio de recompensa para el enfoque SSLA comienza en el mismo nivel que el algoritmo SSNA. Pero el rendimiento aumenta con el tiempo y la ganancia promedio de rendimiento se estabiliza en aproximadamente un 25% después del rango de consulta de 2000 a 3000. La Figura 4 muestra el ARSS (Recompensa Promedio por Sesión de Búsqueda) versus el número de consultas entrantes a lo largo del tiempo para el Algoritmo No Aprendido de Dos Pasos (TSNA) y el Algoritmo de Aprendizaje de Dos Pasos (TSLA) para la recolección de datos TREC-VLC-921. El enfoque TSNA tiene un rendimiento relativamente consistente con una recompensa promedio que oscila entre 0.05 y 0.15. El promedio de recompensa para el enfoque de TSLA, donde se explota el algoritmo de aprendizaje, comienza en el mismo nivel que el algoritmo TSNA y mejora la recompensa promedio con el tiempo hasta que se unen al sistema entre 2000 y 2500 consultas. Los resultados muestran que la ganancia promedio de rendimiento para el enfoque TSLA sobre el enfoque TNLA es del 35% después de la estabilización. La Figura 5 muestra la utilidad acumulada versus el número de consultas entrantes a lo largo del tiempo para SSNA, SSLA, TSNA y TSLA respectivamente. Se ilustra que la utilidad acumulativa de los algoritmos no basados en aprendizaje aumenta principalmente de forma lineal con el tiempo, mientras que las ganancias de los algoritmos basados en aprendizaje se aceleran cuando más consultas ingresan al sistema. Estos resultados experimentales demuestran que los enfoques basados en el aprendizaje tienen un rendimiento consistentemente mejor que los algoritmos de enrutamiento no basados en el aprendizaje. Además, el algoritmo basado en aprendizaje de dos fases es mejor que el algoritmo basado en aprendizaje de una sola fase porque la recompensa máxima que un agente puede recibir al buscar en su vecindario dentro de TTL saltos está relacionada con el número total de documentos relevantes en esa área. Por lo tanto, incluso la política de enrutamiento óptima puede hacer poco más allá de alcanzar estos documentos relevantes más rápido. Por el contrario, el algoritmo de aprendizaje basado en dos pasos puede reubicar la sesión de búsqueda en un vecindario con documentos más relevantes. El TSLA combina los méritos de ambos enfoques y los supera. La Tabla 1 enumera la utilidad acumulativa para los conjuntos de datos TREC123-100-Random y TREC-123-100-Source con organizaciones jerárquicas. Las cinco columnas muestran los resultados de cuatro enfoques diferentes. En particular, la columna TSNA-Random muestra los resultados para el conjunto de datos TREC-123-100-Random con el enfoque TSNA. La columna TSLA-Random muestra los resultados para el conjunto de datos TREC-123-100-Random con el enfoque TSLA. Hay dos números en cada celda de la columna TSLA-Random. El primer número es la utilidad acumulada real, mientras que el segundo número es el porcentaje de ganancia en términos de utilidad sobre el enfoque TSNA. Las columnas TSNA-Source y TSLA-Source muestran los resultados para el conjunto de datos TREC-123-100-Source con los enfoques TSNA y TSLA respectivamente. La Tabla 1 muestra que la mejora en el rendimiento para TREC-123-100-Random no es tan significativa como en los otros conjuntos de datos. Esto se debe a que los documentos en la subcolección de TREC-123-100-Random son seleccionados al azar, lo que hace que el modelo de colección, la firma de la colección, sea menos significativo. Dado que ambos algoritmos están diseñados basados en la suposición de que las colecciones de documentos pueden ser bien representadas por su modelo de colección, este resultado no es sorprendente. En general, las Figuras 4, 5 y la Tabla 1 demuestran que el enfoque basado en aprendizaje por refuerzo puede mejorar considerablemente el rendimiento del sistema para ambas colecciones de datos. Sin embargo, queda como trabajo futuro descubrir la correlación entre la magnitud de las mejoras en el rendimiento y el tamaño de la colección de datos y/o la extensión de la heterogeneidad entre las subcolecciones. TRABAJO RELACIONADO El problema de enrutamiento de contenido difiere del enrutamiento a nivel de red en redes de comunicación conmutadas por paquetes en que el enrutamiento basado en contenido ocurre en redes a nivel de aplicación. Además, los agentes de destino en nuestros algoritmos de enrutamiento de contenido son múltiples y las direcciones no son conocidas en el proceso de enrutamiento. Los problemas de enrutamiento a nivel de IP han sido abordados desde la perspectiva del aprendizaje por refuerzo[2, 5, 11, 12]. Estos estudios han explorado algoritmos distribuidos completamente que son capaces, sin coordinación central, de difundir conocimiento sobre la red, encontrar de manera robusta y eficiente los caminos más cortos frente a cambios en las topologías de red y costos de enlace cambiantes. Hay dos clases principales de algoritmos de enrutamiento de paquetes adaptativos y distribuidos en la literatura: algoritmos de vector de distancia y algoritmos de estado de enlace. Si bien esta línea de estudios tiene cierta similitud con nuestro trabajo, se ha centrado principalmente en redes de comunicación conmutadas por paquetes. En este dominio, el destino de un paquete es determinístico y único. Cada agente mantiene estimaciones, probabilísticamente o determinísticamente, sobre la distancia a un destino específico a través de sus vecinos. Una variante de las técnicas de Q-Learning se implementa en The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Utilidad Acumulativa para los Conjuntos de Datos TREC-123-100-Random y TREC-123-100-Source con Organización Jerárquica; Los números porcentuales en las columnas TSLA-Random y TSLA-Source demuestran la ganancia de rendimiento sobre el algoritmo sin aprendizaje Número de consulta TSNA-Random TSLA-Random TSNA-Source TSLA-Source 500 25.15 28.45 13% 24.00 21.05 -13% 1000 104.99 126.74 20% 93.95 96.44 2.6% 1250 149.79 168.40 12% 122.64 134.05 9.3% 1500 188.94 211.05 12% 155.30 189.60 22% 1750 235.49 261.60 11% 189.14 243.90 28% 2000 275.09 319.25 16% 219.0 278.80 26% para actualizar las estimaciones y converger a las distancias reales. Se ha descubierto que la propiedad de localidad es una característica importante de los sistemas de recuperación de información en estudios de modelado de usuarios[3]. En los sistemas de intercambio de contenido basados en P2P, esta propiedad se ejemplifica por el fenómeno de que los usuarios tienden a enviar consultas que representan solo un número limitado de temas y, por el contrario, los usuarios en el mismo vecindario probablemente compartan intereses comunes y envíen consultas similares [10]. El enfoque basado en el aprendizaje se percibe como más beneficioso para los sistemas reales de recuperación de información distribuida que exhiben la propiedad de localidad. Esto se debe a que los patrones de tráfico y consultas de los usuarios pueden reducir el espacio de estados y acelerar el proceso de aprendizaje. El trabajo relacionado en aprovechar esta propiedad incluye [7], donde los autores intentaron abordar este problema mediante técnicas de modelado de usuario. 6. CONCLUSIONES En este artículo, se desarrolla un enfoque basado en aprendizaje por refuerzo para mejorar el rendimiento de algoritmos de búsqueda de IR distribuidos. En particular, los agentes mantienen estimaciones, es decir, utilidad esperada, sobre la capacidad de los agentes aguas abajo para proporcionar documentos relevantes para las consultas entrantes. Estas estimaciones se actualizan gradualmente al aprender de la información de retroalimentación devuelta de sesiones de búsqueda anteriores. Basándose en la información actualizada de utilidad esperada, los agentes modifican sus políticas de enrutamiento. Posteriormente, estos agentes dirigen las consultas basándose en las políticas aprendidas y actualizan las estimaciones sobre la utilidad esperada en función de las nuevas políticas de enrutamiento. Los experimentos en dos conjuntos de datos IR distribuidos diferentes ilustran que el enfoque de aprendizaje por refuerzo mejora considerablemente la utilidad acumulativa con el tiempo. 7. REFERENCIAS [1] S. Abdallah y V. Lesser. Aprendiendo el juego de asignación de tareas. En AAMAS 06: Actas de la quinta conferencia internacional conjunta sobre agentes autónomos y sistemas multiagente, páginas 850-857, Nueva York, NY, EE. UU., 2006. ACM Press. [2] J. \n\nACM Press. [2] J. A. Boyan y M. L. Littman. Enrutamiento de paquetes en redes de cambio dinámico: Un enfoque de aprendizaje por refuerzo. En Avances en Sistemas de Procesamiento de Información Neural, volumen 6, páginas 671-678. Morgan Kaufmann Publishers, Inc., 1994. [3] J. C. French, A. L. Powell, J. P. Callan, C. L. Viles, T. Emmitt, K. J. Prey, y Y. Mou. Comparando el rendimiento de los algoritmos de selección de bases de datos. En Investigación y Desarrollo en Recuperación de Información, páginas 238-245, 1999. [4] B. Horling, R. Mailler y V. Lesser. Granja: Un entorno escalable para el desarrollo y evaluación de múltiples agentes. En Avances en Ingeniería de Software para Sistemas Multiagente, páginas 220-237, Berlín, 2004. Springer-Verlag. [5] M. Littman y J. Boyan. Un esquema distribuido de aprendizaje por refuerzo para enrutamiento de redes. En Actas del Taller Internacional sobre Aplicaciones de Redes Neuronales a las Telecomunicaciones, 1993. [6] J. Lu y J. Callan. Búsqueda federada de bibliotecas digitales basadas en texto en redes jerárquicas de pares. En ECIR05, 2005. [7] J. Lu y J. Callan. Modelado de usuario para búsqueda federada de texto completo en redes peer-to-peer. En ACM SIGIR 2006. ACM Press, 2006. [8] C. D. Manning y H. Schütze. Fundamentos del Procesamiento del Lenguaje Natural Estadístico. El MIT Press, Cambridge, Massachusetts, 1999. [9] C. R. Palmer y J. G. Steffan. Generando topologías de red que obedezcan leyes de potencia. En Actas de GLOBECOM 2000, noviembre de 2000. [10] K. Sripanidkulchai, B. Maggs y H. Zhang. Localización eficiente de contenido utilizando la localidad basada en intereses en sistemas peer-to-peer. En INFOCOM, 2003. [11] D. Subramanian, P. Druschel y J. Chen. Hormigas y aprendizaje por refuerzo: Un estudio de caso en enrutamiento en redes dinámicas. En Actas de la Decimoquinta Conferencia Internacional Conjunta sobre Inteligencia Artificial, páginas 832-839, 1997. [12] J. N. Tao y L. Weaver. Un enfoque de múltiples agentes y gradiente de políticas para el enrutamiento de redes. En Actas de la Decimoctava Conferencia Internacional sobre Aprendizaje Automático, 2001. [13] H. Zhang, W. B. Croft, B. Levine y V. Lesser. Un enfoque multiagente para la recuperación de información entre pares. En Actas de la Tercera Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, julio de 2004. [14] H. Zhang y V. Lesser. Sistemas de recuperación de información peer-to-peer basados en múltiples agentes con sesiones de búsqueda concurrentes. En Actas de la Quinta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, mayo de 2006. [15] H. Zhang y V. R. Lesser. Una organización de agentes jerárquica formada dinámicamente para un sistema distribuido de intercambio de contenido. En la Conferencia Internacional IEEE/WIC/ACM sobre Tecnología de Agentes Inteligentes (IAT 2004) del 20 al 24 de septiembre de 2004 en Beijing, China, páginas 169-175. IEEE Computer Society, 2004. 238 La Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ",
            "candidates": [],
            "error": [
                []
            ]
        },
        "reinforcement learning": {
            "translated_key": "aprendizaje por refuerzo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A <br>reinforcement learning</br> based Distributed Search Algorithm For Hierarchical Peer-to-Peer Information Retrieval Systems Haizheng Zhang College of Information Science and Technology Pennsylvania State University University Park, PA 16803 hzhang@ist.psu.edu Victor Lesser Department of Computer Science University Of Massachusetts Amherst, MA 01003 lesser@cs.umass.edu ABSTRACT The dominant existing routing strategies employed in peerto-peer(P2P) based information retrieval(IR) systems are similarity-based approaches.",
                "In these approaches, agents depend on the content similarity between incoming queries and their direct neighboring agents to direct the distributed search sessions.",
                "However, such a heuristic is myopic in that the neighboring agents may not be connected to more relevant agents.",
                "In this paper, an online reinforcement-learning based approach is developed to take advantage of the dynamic run-time characteristics of P2P IR systems as represented by information about past search sessions.",
                "Specifically, agents maintain estimates on the downstream agents abilities to provide relevant documents for incoming queries.",
                "These estimates are updated gradually by learning from the feedback information returned from previous search sessions.",
                "Based on this information, the agents derive corresponding routing policies.",
                "Thereafter, these agents route the queries based on the learned policies and update the estimates based on the new routing policies.",
                "Experimental results demonstrate that the learning algorithm improves considerably the routing performance on two test collection sets that have been used in a variety of distributed IR studies.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Algorithms, Performance, Experimentation 1.",
                "INTRODUCTION Over the last few years there have been increasing interests in studying how to control the search processes in peer-to-peer(P2P) based information retrieval(IR) systems [6, 13, 14, 15].",
                "In this line of research, one of the core problems that concerns researchers is to efficiently route user queries in the network to agents that are in possession of appropriate documents.",
                "In the absence of global information, the dominant strategies in addressing this problem are content-similarity based approaches [6, 13, 14, 15].",
                "While the content similarity between queries and local nodes appears to be a creditable indicator for the number of relevant documents residing on each node, these approaches are limited by a number of factors.",
                "First of all, similaritybased metrics can be myopic since locally relevant nodes may not be connected to other relevant nodes.",
                "Second, the similarity-based approaches do not take into account the run-time characteristics of the P2P IR systems, including environmental parameters, bandwidth usage, and the historical information of the past search sessions, that provide valuable information for the query routing algorithms.",
                "In this paper, we develop a <br>reinforcement learning</br> based IR approach for improving the performance of distributed IR search algorithms.",
                "Agents can acquire better search strategies by collecting and analyzing feedback information from previous search sessions.",
                "Particularly, agents maintain estimates, namely expected utility, on the downstream agents capabilities of providing relevant documents for specific types of incoming queries.",
                "These estimates are updated gradually by learning from the feedback information returned from previous search sessions.",
                "Based on the updated expected utility information, the agents derive corresponding routing policies.",
                "Thereafter, these agents route the queries based on the learned policies and update the estimates on the expected utility based on the new routing policies.",
                "This process is conducted in an iterative manner.",
                "The goal of the learning algorithm, even though it consumes some network bandwidth, is to shorten the routing time so that more queries are processed per time unit while at the same time finding more relevant documents.",
                "This contrasts with the content-similarity based approaches where similar operations are repeated for every incoming query and the processing time keeps largely constant over time.",
                "Another way of viewing this paper is that our basic approach to distributed IR search is to construct a hierarchical overlay network(agent organization) based on the contentsimilarity measure among agents document collections in a bottom-up fashion.",
                "In the past work, we have shown that this organization improves search performance significantly.",
                "However, this organizational structure does not take into account the arrival patterns of queries, including their frequency, types, and where they enter the system, nor the available communication bandwidth of the network and processing capabilities of individual agents.",
                "The intention of the <br>reinforcement learning</br> is to adapt the agents routing decisions to the dynamic network situations and learn from past search sessions.",
                "Specifically, the contributions of this paper include: (1) a <br>reinforcement learning</br> based approach for agents to acquire satisfactory routing policies based on estimates of the potential contribution of their neighboring agents; (2) two strategies to speed up the learning process.",
                "To our best knowledge, this is one of the first <br>reinforcement learning</br> applications in addressing distributed content sharing problems and it is indicative of some of the issues in applying reinforcement in a complex application.",
                "The remainder of this paper is organized as follows: Section 2 reviews the hierarchical content sharing systems and the two-phase search algorithm based on such topology.",
                "Section 3 describes a <br>reinforcement learning</br> based approach to direct the routing process; Section 4 details the experimental settings and analyze the results.",
                "Section 5 discusses related studies and Section 6 concludes the paper. 2.",
                "SEARCH IN HIERARCHICAL P2P IR SYSTEMS This section briefly reviews our basic approaches to hierarchical P2P IR systems.",
                "In a hierarchical P2P IR system illustrated in Fig.1, agents are connected to each other through three types of links: upward links, downward links, and lateral links.",
                "In the following sections, we denote the set of agents that are directly connected to agent Ai as DirectConn(Ai), which is defined as DirectConn(Ai) = NEI(Ai) ∪ PAR(Ai) ∪ CHL(Ai) , where NEI(Ai) is the set of neighboring agents connected to Ai through lateral links; PAR(Ai) is the set of agents whom agent Ai is connected to through upward links and CHL(Ai) is the set of agents that agent Ai connects to through downward links.",
                "These links are established through a bottom-up content-similarity based distributed clustering process[15].",
                "These links are then used by agents to locate other agents that contain documents relevant to the given queries.",
                "A typical agent Ai in our system uses two queues: a local search queue, LSi, and a message forwarding queue MFi.",
                "The states of the two queues constitute the internal states of an agent.",
                "The local search queue LSi stores search sessions that are scheduled for local processing.",
                "It is a priority queue and agent Ai always selects the most promising queries to process in order to maximize the global utility.",
                "MFi consists of a set of queries to forward on and is processed in a FIFO (first in first out) fashion.",
                "For the first query in MFi, agent Ai determines which subset of its neighboring agents to forward it to based on the agents routing policy πi.",
                "These routing decisions determine how the search process is conducted in the network.",
                "In this paper, we call Ai as Ajs upstream agent and Aj as Ais downstream agent if A4 A5 A6 A7 A2 A3 A9 NEI(A2)={A3} PAR(A2)={A1} CHL(A2)={A4,A5} A1 A8 Figure 1: A fraction of a hierarchical P2PIR system an agent Ai routes a query to agent Aj.",
                "The distributed search protocol of our hierarchical agent organization is composed of two steps.",
                "In the first step, upon receipt of a query qk at time tl from a user, agent Ai initiates a search session si by probing its neighboring agents Aj ∈ NEI(Ai) with the message PROBE for the similarity value Sim(qk, Aj) between qk and Aj.",
                "Here, Ai is defined as the query initiator of search session si.",
                "In the second step, Ai selects a group of the most promising agents to start the actual search process with the message SEARCH.",
                "These SEARCH messages contain a TTL (Time To Live) parameter in addition to the query.",
                "The TTL value decreases by 1 after each hop.",
                "In the search process, agents discard those queries that either have been previously processed or whose TTL drops to 0, which prevents queries from looping in the system forever.",
                "The search session ends when all the agents that receive the query drop it or TTL decreases to 0.",
                "Upon receipt of SEARCH messages for qk, agents schedule local activities including local searching, forwarding qk to their neighbors, and returning search results to the query initiator.",
                "This process and related algorithms are detailed in [15, 14]. 3.",
                "A BASIC REINFORCEMENTLEARNING BASED SEARCH APPROACH In the aforementioned distributed search algorithm, the routing decisions of an agent Ai rely on the similarity comparison between incoming queries and Ais neighboring agents in order to forward those queries to relevant agents without flooding the network with unnecessary query messages.",
                "However, this heuristic is myopic because a relevant direct neighbor is not necessarily connected to other relevant agents.",
                "In this section, we propose a more general approach by framing this problem as a <br>reinforcement learning</br> task.",
                "In pursuit of greater flexibility, agents can switch between two modes: learning mode and non-learning mode.",
                "In the non-learning mode, agents operate in the same way as they do in the normal distributed search processes described in [14, 15].",
                "On the other hand, in the learning mode, in parallel with distributed search sessions, agents also participate in a learning process which will be detailed in this section.",
                "Note that in the learning protocol, the learning process does not interfere with the distributed search process.",
                "Agents can choose to initiate and stop learning processes without affecting the system performance.",
                "In particular, since the learning process consumes network resources (especially bandwidth), agents can choose to initiate learning only when the network load is relatively low, thus minimizing the extra communication costs incurred by the learning algorithm.",
                "The section is structured as follows, Section 3.1 describes 232 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) a <br>reinforcement learning</br> based model.",
                "Section 3.2 describes a protocol to deploy the learning algorithm in the network.",
                "Section 3.3 discusses the convergence of the learning algorithm. 3.1 The Model An agents routing policy takes the state of a search session as input and output the routing actions for that query.",
                "In our work, the state of a search session sj is stipulated as: QSj = (qk, ttlj) where ttlj is the number of hops that remains for the search session sj , qk is the specific query.",
                "QL is an attribute of qk that indicates which type of queries qk most likely belong to.",
                "The set of QL can be generated by running a simple online classification algorithm on all the queries that have been processed by the agents, or an oﬄine algorithm on a pre-designated training set.",
                "The assumption here is that the set of query types is learned ahead of time and belongs to the common knowledge of the agents in the network.",
                "Future work includes exploring how learning can be accomplished when this assumption does not hold.",
                "Given the query types set, an incoming query qi can be classified to one query class Q(qi) by the formula: Q(qi) = arg max Qj P(qi|Qj) (1) where P(qi|Qj ) indicates the likelihood that the query qi is generated by the query class Qj [8].",
                "The set of atomic routing actions of an agent Ai is denoted as {αi}, where {αi} is defined as αi = {αi0 , αi1 , ..., αin }.",
                "An element αij represents an action to route a given query to the neighboring agent Aij ∈ DirectConn(Ai).",
                "The routing policy πi of agent Ai is stochastic and its outcome for a search session with state QSj is defined as: πi(QSj) = {(αi0 , πi(QSi, αi0 )), (αi1 , πi(QSi, αi1 )), ...} (2) Note that operator πi is overloaded to represent either the probabilistic policy for a search session with state QSj, denoted as πi(QSj); or the probability of forwarding the query to a specific neighboring agent Aik ∈ DirectConn(Ai) under the policy πi(QSj), denoted as πi(QSj, αik ).",
                "Therefore, equation (2) means that the probability of forwarding the search session to agent Ai0 is πi(QSi, αi0 ) and so on.",
                "Under this stochastic policy, the routing action is nondeterministic.",
                "The advantage of such a strategy is that the best neighboring agents will not be selected repeatedly, thereby mitigating the potential hot spots situations.",
                "The expected utility, Un i (QSj), is used to estimate the potential utility gain of routing query type QSj to agent Ai under policy πn i .",
                "The superscript n indicates the value at the nth iteration in an iterative learning process.",
                "The expected utility provides routing guidance for future search sessions.",
                "In the search process, each agent Ai maintains partial observations of its neighbors states, as shown in Fig. 2.",
                "The partial observation includes non-local information such as the potential utility estimation of its neighbor Am for query state QSj, denoted as Um(QSj), as well as the load information, Lm.",
                "These observations are updated periodically by the neighbors.",
                "The estimated utility information will be used to update Ais expected utility for its routing policy.",
                "Load Information Expected Utility For Different Query Types Neighboring Agents ... A0 A1 A3 A2 Un 0 (QS0) ... ... ... ... ...... Un 0 (QS1) Un 1 (QS1) Un 2 (QS1) Un 3 (QS1) Un 1 (QS0) Un 2 (QS0) Un 3 (QS0) Ln 0 Ln 1 Ln 2 Ln 3 ... QS0 QS1 ...",
                "Figure 2: Agent Ais Partial Observation about its neighbors(A0, A1...) The load information of Am, Lm, is defined as Lm = |MFm| Cm , where |MFm| is the length of the message-forward queue and Cm is the service rate of agent Ams message-forward queue.",
                "Therefore Lm characterizes the utilization of an agents communication channel, and thus provide non-local information for Ams neighbors to adjust the parameters of their routing policy to avoid inundating their downstream agents.",
                "Note that based on the characteristics of the queries entering the system and agents capabilities, the loading of agents may not be uniform.",
                "After collecting the utilization rate information from all its neighbors, agent Ai computes Li as a single measure for assessing the average load condition of its neighborhood: Li = P k Lk |DirectConn(Ai)| Agents exploit Li value in determining the routing probability in its routing policy.",
                "Note that, as described in Section 3.2, information about neighboring agents is piggybacked with the query message propagated among the agents whenever possible to reduce the traffic overhead. 3.1.1 Update the Policy An iterative update process is introduced for agents to learn a satisfactory stochastic routing policy.",
                "In this iterative process, agents update their estimates on the potential utility of their current routing policies and then propagate the updated estimates to their neighbors.",
                "Their neighbors then generate a new routing policy based on the updated observation and in turn they calculate the expected utility based on the new policies and continue this iterative process.",
                "In particular, at time n, given a set of expected utility, an agent Ai, whose directly connected agents set is DirectConn(Ai) = {Ai0 , ..., Aim }, determines its corresponding stochastic routing policy for a search session of state QSj based on the following steps: (1) Ai first selects a subset of agents as the potential downstream agents from set DirectConn(Ai), denoted as PDn(Ai, QSj).",
                "The size of the potential downstream agent is specified as |PDn(Ai, QSj)| = min(|NEI(Ai), dn i + k)| where k is a constant and is set to 3 in this paper; dn i , the forward width, is defined as the expected number of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 233 neighboring agents that agent Ai can forward to at time n. This formula specifies that the potential downstream agent set PDn(Ai, QSj) is either the subset of neighboring agents with dn i + k highest expected utility value for state QSj among all the agents in DirectConn(Ai), or all their neighboring agents.",
                "The k is introduced based on the idea of a stochastic routing policy and it makes the forwarding probability of the dn i +k highest agent less than 100%.",
                "Note that if we want to limit the number of downstream agents for search session sj as 5, the probability of forwarding the query to all neighboring agents should add up to 5.",
                "Setting up dn i value properly can improve the utilization rate of the network bandwidth when much of the network is idle while mitigating the traffic load when the network is highly loaded.",
                "The dn+1 i value is updated based on dn i , the previous and current observations on the traffic situation in the neighborhood.",
                "Specifically, the update formula for dn+1 i is dn+1 i = dn i ∗ (1 + 1 − Li |DirectConn(Ai)| ) In this formula, the forward width is updated based on the traffic conditions of agent Ais neighborhood, i.e Li, and its previous value. (2) For each agent Aik in the PDn(Ai, QSj), the probability of forwarding the query to Aik is determined in the following way in order to assign higher forwarding probability to the neighboring agents with higher expected utility value: πn+1 i (QSj, αik ) = dn+1 i |PDn(Ai, QSj)| + β ∗ ` Uik (QSj) − PDU(Ai, QSj) |PDn(Ai, QSj)| ´ (3) where PDUn(Ai, QSj) = X o∈P Dn(Ai,QSj ) Uo(QSj) and QSj is the subsequent state of agent Aik after agent Ai forwards the search session with state QSj to its neighboring agent Aik ; If QSj = (qk, ttl0), then QSj = (qk, ttl0 − 1).",
                "In formula 3, the first term on the right of the equation, dn+1 i |P Dn(Ai,QSj )| , is used to to determine the forwarding probability by equally distributing the forward width, dn+1 i , to the agents in PDn(Ai, QSj) set.",
                "The second term is used to adjust the probability of being chosen so that agents with higher expected utility values will be favored. β is determined according to: β = min ` m − dn+1 i m ∗ umax − PDUn(Ai, QSj) , dn+1 i PDUn(Ai, QSj) − m ∗ umin ´ (4) where m = |PDn(Ai, QSj)|, umax = max o∈P Dn(Ao,QSj ) Uo(QSj) and umin = min o∈P Dn(Ao,QSj ) Uo(QSj) This formula guarantees that the final πn+1 i (QSj, αik ) value is well defined, i.e, 0 ≤ πn+1 i (QSj, αik ) ≤ 1 and X i πn+1 i (QSj, αik ) = dn+1 i However, such a solution does not explore all the possibilities.",
                "In order to balance between exploitation and exploration, a λ-Greedy approach is taken.",
                "In the λ-Greedy approach, in addition to assigning higher probability to those agents with higher expected utility value, as in the equation (3).",
                "Agents that appear to be not-so-good choices will also be sent queries based on a dynamic exploration rate.",
                "In particular, for agents in the set PDn(Ai, QSj), πn+1 i1 (QSj) is determined in the same way as the above, with the only difference being that dn+1 i is replaced with dn+1 i ∗ (1 − λn).",
                "The remaining search bandwidth is used for learning by assigning probability λn evenly to agents Ai2 in the set DirectConn(Ai) − PDn(Ai, QSj). πn+1 i2 (QSj, αik ) = dn+1 i ∗ λn |DirectConn(Ai) − PDn(Ai, QSj)| (5) where PDn(Ai, QSj) ⊂ DirectConn(Ai).",
                "Note that the exploration rate λ is not a constant and it decreases overtime.",
                "The λ is determined according to the following equation: λn+1 = λ0 ∗ e−c1n (6) where λ0 is the initial exploration rate, which is a constant; c1 is also a constant to adjust the decreasing rate of the exploration rate; n is the current time unit. 3.1.2 Update Expected Utility Once the routing policy at step n+1, πn+1 i , is determined based on the above formula, agent Ai can update its own expected utility, Un+1 i (QSi), based on the the updated routing policy resulted from the formula 5 and the updated U values of its neighboring agents.",
                "Under the assumption that after a query is forwarded to Ais neighbors the subsequent search sessions are independent, the update formula is similar to the Bellman update formula in Q-Learning: Un+1 i (QSj) = (1 − θi) ∗ Un i (QSj) + θi ∗ (Rn+1 i (QSj) + X k πn+1 i (QSj, αik )Un k (QSj)) (7) where QSj = (Qj, ttl − 1) is the next state of QSj = (Qj, ttl); Rn+1 i (QSj) is the expected local reward for query class Qk at agent Ai under the routing policy πn+1 i ; θi is the coefficient for deciding how much weight is given to the old value during the update process: the smaller θi value is, the faster the agent is expected to learn the real value, while the greater volatility of the algorithm, and vice versa.",
                "Rn+1 (s) is updated according to the following equation: 234 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Rn+1 i (QSj) = Rn i (QSj) +γi ∗ (r(QSj) − Rn i (QSj)) ∗ P(qj|Qj ) (8) where r(QSj) is the local reward associated with the search session.",
                "P(qj|Qj ) indicates how relevant the query qj is to the query type Qj, and γi is the learning rate for agent Ai.",
                "Depending on the similarity between a specific query qi and its corresponding query type Qi, the local reward associated with the search session has different impact on the Rn i (QSj) estimation.",
                "In the above formula, this impact is reflected by the coefficient, the P(qj|Qj) value. 3.1.3 Reward function After a search session stops when its TTL values expires, all search results are returned back to the user and are compared against the relevance judgment.",
                "Assuming the set of search results is SR, the reward Rew(SR) is defined as: Rew(SR) = j 1 if |Rel(SR)| > c |Rel(SR)| c otherwise. where SR is the set of returned search results, Rel(SR) is the set of relevant documents in the search results.",
                "This equation specifies that users give 1.0 reward if the number of returned relevant documents reaches a predefined number c. Otherwise, the reward is in proportion to the number of relevant documents returned.",
                "This rationale for setting up such a cut-off value is that the importance of recall ratio decreases with the abundance of relevant documents in real world, therefore users tend to focus on only a limited number of searched results.",
                "The details of the actual routing protocol will be introduced in Section 3.2 when we introduce how the learning algorithm is deployed in real systems. 3.2 Deployment of the Learning algorithm This section describes how the learning algorithm can be used in either a single-phase or a two-phase search process.",
                "In the single-phase search algorithm, search sessions start from the initiators of the queries.",
                "In contrast, in the two-step search algorithm, the query initiator first attempts to seek a more appropriate starting point for the query by introducing an exploratory step as described in Section 2.",
                "Despite the difference in the quality of starting points, the major part of the learning process for the two algorithms is largely the same as described in the following paragraphs.",
                "Before learning starts, each agent initializes the expected utility value for all possible states as 0.",
                "Thereafter, upon receipt of a query, in addition to the normal operations described in the previous section, an agent Ai also sets up a timer to wait for the search results returned from its downstream agents.",
                "Once the timer expires or it has received response from all its downstream agents, Ai merges and forwards the search results accrued from its downstream agents to its upstream agent.",
                "Setting up the timer speeds up the learning because agents can avoid waiting too long for the downstream agents to return search results.",
                "Note that these detailed results and corresponding agent information will still be stored at Ai until the feedback information is passed from its upstream agent and the performance of its downstream agents can be evaluated.",
                "The duration of the timer is related to the TTL value.",
                "In this paper, we set the timer to ttimer = ttli ∗ 2 + tf , where ttli ∗ 2 is the sum of the travel time of the queries in the network, and tf is the expected time period that users would like to wait.",
                "The search results will eventually be returned to the search session initiator A0.",
                "They will be compared to the relevance judgment that is provided by the final users (as described in the experiment section, the relevance judgement for the query set is provided along with the data collections).",
                "The reward will be calculated and propagated backward to the agents along the way that search results were passed.",
                "This is a reverse process of the search results propagation.",
                "In the process of propagating reward backward, agents update estimates of their own potential utility value, generate an upto-dated policy and pass their updated results to the neighboring agents based on the algorithm described in Section 3.",
                "Upon change of expected utility value, agent Ai sends out its updated utility estimation to its neighbors so that they can act upon the changed expected utility and corresponding state.",
                "This update message includes the potential reward as well as the corresponding state QSi = (qk, ttll) of agent Ai.",
                "Each neighboring agent, Aj, reacts to this kind of update message by updating the expected utility value for state QSj(qk, ttll + 1) according to the newly-announced changed expected utility value.",
                "Once they complete the update, the agents would again in turn inform related neighbors to update their values.",
                "This process goes on until the TTL value in the update message increases to the TTL limit.",
                "To speed up the learning process, while updating the expected utility values of an agent Ais neighboring agents we specify that Um(Qk, ttl0) >= Um(Qk, ttl1) iff ttl0 > ttl1 Thus, when agent Ai receives an updated expected utility value with ttl1, it also updates the expected utility values with any ttl0 > ttl1 if Um(Qk, ttl0) < Um(Qk, ttl1) to speed up convergence.",
                "This heuristic is based on the fact that the utility of a search session is a non-decreasing function of time t. 3.3 Discussion In formalizing the content routing system as a learning task, many assumptions are made.",
                "In real systems, these assumptions may not hold, and thus the learning algorithm may not converge.",
                "Two problems are of particular note, (1) This content routing problem does not have Markov properties.",
                "In contrast to IP-level based packet routing, the routing decision of each agent for a particular search session sj depends on the routing history of sj.",
                "Therefore, the assumption that all subsequent search sessions are independent does not hold in reality.",
                "This may lead to double counting problem that the relevant documents of some agents will be counted more than once for the state where the TTL value is more than 1.",
                "However, in the context of the hierarchical agent organizations, two factors mitigate this problems: first, the agents in each content group form a tree-like structure.",
                "With the absense of the cycles, the estimates inside the tree would be close to the accurate value.",
                "Secondly, the stochastic nature of the routing policy partly remedies this problem.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 235 (2) Another challenge for this learning algorithm is that in a real network environment observations on neighboring agents may not be able to be updated in time due to the communication delay or other situations.",
                "In addition, when neighboring agents update their estimates at the same time, oscillation may arise during the learning process[1].",
                "This paper explores several approaches to speed up the learning process.",
                "Besides the aforementioned strategy of updating the expected utility values, we also employ an active update strategy where agents notify their neighbors whenever its expected utility is updated.",
                "Thus a faster convergence speed can be achieved.",
                "This strategy contrasts to the Lazy update, where agents only echo their neighboring agents with their expected utility change when they exchange information.",
                "The trade off between the two approaches is the network load versus learning speed.",
                "The advantage of this learning algorithm is that once a routing policy is learned, agents do not have to repeatedly compare the similarity of queries as long as the network topology remains unchanged.",
                "Instead, agent just have to determine the classification of the query properly and follow the learned policies.",
                "The disadvantage of this learning-based approach is that the learning process needs to be conducted whenever the network structure changes.",
                "There are many potential extensions for this learning model.",
                "For example, a single measure is currently used to indicate the traffic load for an agents neighborhood.",
                "A simple extension would be to keep track of individual load for each neighbor of the agent. 4.",
                "EXPERIMENTSSETTINGSAND RESULTS The experiments are conducted on TRANO simulation toolkit with two sets of datasets, TREC-VLC-921 and TREC123-100.",
                "The following sub-sections introduce the TRANO testbed, the datasets, and the experimental results. 4.1 TRANO Testbed TRANO (Task Routing on Agent Network Organization) is a multi-agent based network based information retrieval testbed.",
                "TRANO is built on top of the Farm [4], a time based distributed simulator that provides a data dissemination framework for large scale distributed agent network based organizations.",
                "TRANO supports importation and exportation of agent organization profiles including topological connections and other features.",
                "Each TRANO agent is composed of an agent view structure and a control unit.",
                "In simulation, each agent is pulsed regularly and the agent checks the incoming message queues, performs local operations and then forwards messages to other agents . 4.2 Experimental Settings In our experiment, we use two standard datasets, TRECVLC-921 and TREC-123-100 datasets, to simulate the collections hosted on agents.",
                "The TREC-VLC-921 and TREC123-100 datasets were created by the U.S. National Institute for Standard Technology(NIST) for its TREC conferences.",
                "In distributed information retrieval domain, the two data collections are split to 921 and 100 sub-collections.",
                "It is observed that dataset TREC-VLC-921 is more heterogeneous than TREC-123-100 in terms of source, document length, and relevant document distribution from the statistics of the two data collections listed in [13].",
                "Hence, TREC-VLC-921 is much closer to real document distributions in P2P environments.",
                "Furthermore, TREC-123-100 is split into two sets of 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0 500 1000 1500 2000 2500 3000 ARSS Query number ARSS versus the number of incoming queries for TREC-VLC-921 SSLA-921 SSNA-921 Figure 3: ARSS(Average reward per search session) versus the number of search sessions for 1phase search in TREC-VLC-921 0 0.1 0.2 0.3 0.4 0.5 0.6 0 500 1000 1500 2000 2500 3000 ARSS Query number ARSS versus query number for TREC-VLC-921 TSLA-921 TSNA-921 Figure 4: ARSS(Average reward per search session) versus the number of search sessions for 2phase search in TREC-VLC-921 sub-collections in two ways: randomly and by source.",
                "The two partitions are denoted as TREC-123-100-Random and TREC-123-100-Source respectively.",
                "The documents in each subcollection in dataset TREC-123-100-Source are more coherent than those in TREC-123-100-Random.",
                "The two different sets of partitions allow us to observe how the distributed learning algorithm is affected by the homogeneity of the collections.",
                "The hierarchical agent organization is generated by the algorithm described in our previous algorithm [15].",
                "During the topology generation process, degree information of each agent is estimated by the algorithm introduced by Palmer et al. [9] with parameters α = 0.5 and β = 0.6.",
                "In our experiments, we estimate the upward limit and downward degree limit using linear discount factors 0.5, 0.8 and 1.0.",
                "Once the topology is built, queries randomly selected from the query set 301−350 on TREC-VLC-921 and query set 1− 50 on TREC-123-100-Random and TREC-123-100-Source are injected to the system based on a Poisson distribution P(N(t) = n) = (λt)n n! e−λ 236 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 0 50 100 150 200 250 300 350 400 0 500 1000 1500 2000 2500 3000 Cumulativeutility Query number Cumulative utility over the number of incoming queries TSLA-921 SSNA-921 SSLA-921 TSNA-921 Figure 5: The cumulative utility versus the number of search sessions TREC-VLC-921 In addition, we assume that all agents have an equal chance of getting queries from the environment, i.e, λ is the same for every agent.",
                "In our experiments, λ is set as 0.0543 so that the mean of the incoming queries from the environment to the agent network is 50 per time unit.",
                "The service time for the communication queue and local search queue, i.e tQij and trs, is set as 0.01 time unit and 0.05 time units respectively.",
                "In our experiments, there are ten types of queries acquired by clustering the query set 301 − 350 and 1 − 50. 4.3 Results analysis and evaluation Figure 3 demonstrates the ARSS(Average Reward per Search Session) versus the number of incoming queries over time for the the Single-Step based Non-learning Algorithm (SSNA), and the Single-Step Learning Algorithm(SSLA) for data collection TREC-VLC-921.",
                "It shows that the average reward for SSNA algorithm ranges from 0.02 − 0.06 and the performance changes little over time.",
                "The average reward for SSLA approach starts at the same level with the SSNA algorithm.",
                "But the performance increases over time and the average performance gain stabilizes at about 25% after query range 2000 − 3000.",
                "Figure 4 shows the ARSS(Average Reward per Search Session) versus the number of incoming queries over time for the the Two-Step based Non-learning Algorithm(TSNA), and the Two-Step Learning Algorithm(TSLA) for data collection TREC-VLC-921.",
                "The TSNA approach has a relatively consistent performance with the average reward ranges from 0.05 − 0.15.",
                "The average reward for TSLA approach, where learning algorithm is exploited, starts at the same level with the TSNA algorithm and improves the average reward over time until 2000−2500 queries joining the system.",
                "The results show that the average performance gain for TSLA approach over TNLA approach is 35% after stabilization.",
                "Figure 5 shows the cumulative utility versus the number of incoming queries over time for SSNA, SSLA,TSNA, and TSLA respectively.",
                "It illustrates that the cumulative utility of non-learning algorithms increases largely linearly over time, while the gains of learning-based algorithms accelerate when more queries enter the system.",
                "These experimental results demonstrate that learning-based approaches consistently perform better than non-learning based routing algorithm.",
                "Moreover, two-phase learning based algorithm is better than single-phase based learning algorithm because the maximal reward an agent can receive from searching its neighborhood within TTL hops is related to the total number of the relevant documents in that area.",
                "Thus, even the optimal routing policy can do little beyond reaching these relevant documents faster.",
                "On the contrary, the two-stepbased learning algorithm can relocate the search session to a neighborhood with more relevant documents.",
                "The TSLA combines the merits of both approaches and outperforms them.",
                "Table 1 lists the cumulative utility for datasets TREC123-100-Random and TREC-123-100-Source with hierarchical organizations.",
                "The five columns show the results for four different approaches.",
                "In particular, column TSNA-Random shows the results for dataset TREC-123-100-Random with the TSNA approach.",
                "The column TSLA-Random shows the results for dataset TREC-123-100-Random with the TSLA approach.",
                "There are two numbers in each cell in the column TSLA-Random.",
                "The first number is the actual cumulative utility while the second number is the percentage gain in terms of the utility over TSNA approach.",
                "Columns TSNA-Source and TSLA-Source show the results for dataset TREC-123-100-Source with TSNA and TSLA approaches respectively.",
                "Table 1 shows that the performance improvement for TREC-123-100-Random is not as significant as the other datasets.",
                "This is because that the documents in the sub-collection of TREC-123-100-Random are selected randomly which makes the collection model, the signature of the collection, less meaningful.",
                "Since both algorithms are designed based on the assumption that document collections can be well represented by their collection model, this result is not surprising.",
                "Overall, Figures 4, 5, and Table 1 demonstrate that the <br>reinforcement learning</br> based approach can considerably enhance the system performance for both data collections.",
                "However, it remains as future work to discover the correlation between the magnitude of the performance gains and the size of the data collection and/or the extent of the heterogeneity between the sub-collections. 5.",
                "RELATED WORK The content routing problem differs from the networklevel routing in packet-switched communication networks in that content-based routing occurs in application-level networks.",
                "In addition, the destination agents in our contentrouting algorithms are multiple and the addresses are not known in the routing process.",
                "IP-level Routing problems have been attacked from the <br>reinforcement learning</br> perspective[2, 5, 11, 12].",
                "These studies have explored fully distributed algorithms that are able, without central coordination to disseminate knowledge about the network, to find the shortest paths robustly and efficiently in the face of changing network topologies and changing link costs.",
                "There are two major classes of adaptive, distributed packet routing algorithms in the literature: distance-vector algorithms and link-state algorithms.",
                "While this line of studies carry a certain similarity with our work, it has mainly focused on packet-switched communication networks.",
                "In this domain, the destination of a packet is deterministic and unique.",
                "Each agent maintains estimations, probabilistically or deterministically, on the distance to a certain destination through its neighbors.",
                "A variant of Q-Learning techniques is deployed The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 237 Table 1: Cumulative Utility for Datasets TREC-123-100-Random and TREC-123-100-Source with Hierarchical Organization; The percentage numbers in the columns TSLA-Random and TSLA-Source demonstrate the performance gain over the algorithm without learning Query number TSNA-Random TSLA-Random TSNA-Source TSLA-Source 500 25.15 28.45 13% 24.00 21.05 -13% 1000 104.99 126.74 20% 93.95 96.44 2.6% 1250 149.79 168.40 12% 122.64 134.05 9.3% 1500 188.94 211.05 12% 155.30 189.60 22% 1750 235.49 261.60 11% 189.14 243.90 28% 2000 275.09 319.25 16% 219.0 278.80 26% to update the estimations to converge to the real distances.",
                "It has been discovered that the locality property is an important feature of information retrieval systems in user modeling studies[3].",
                "In P2P based content sharing systems, this property is exemplified by the phenomenon that users tend to send queries that represent only a limited number of topics and conversely, users in the same neighborhood are likely to share common interests and send similar queries [10].",
                "The learning based approach is perceived to be more beneficial for real distributed information retrieval systems which exhibit locality property.",
                "This is because the users traffic and query patterns can reduce the state space and speed up the learning process.",
                "Related work in taking advantage of this property include [7], where the authors attempted to address this problem by user modeling techniques. 6.",
                "CONCLUSIONS In this paper, a reinforcement-learning based approach is developed to improve the performance of distributed IR search algorithms.",
                "Particularly, agents maintain estimates, namely expected utility, on the downstream agents ability to provide relevant documents for incoming queries.",
                "These estimates are updated gradually by learning from the feedback information returned from previous search sessions.",
                "Based on the updated expected utility information, the agents modify their routing policies.",
                "Thereafter, these agents route the queries based on the learned policies and update the estimates on the expected utility based on the new routing policies.",
                "The experiments on two different distributed IR datasets illustrates that the <br>reinforcement learning</br> approach improves considerably the cumulative utility over time. 7.",
                "REFERENCES [1] S. Abdallah and V. Lesser.",
                "Learning the task allocation game.",
                "In AAMAS 06: Proceedings of the fifth international joint conference on Autonomous agents and multiagent systems, pages 850-857, New York, NY, USA, 2006.",
                "ACM Press. [2] J.",
                "A. Boyan and M. L. Littman.",
                "Packet routing in dynamically changing networks: A <br>reinforcement learning</br> approach.",
                "In Advances in Neural Information Processing Systems, volume 6, pages 671-678.",
                "Morgan Kaufmann Publishers, Inc., 1994. [3] J. C. French, A. L. Powell, J. P. Callan, C. L. Viles, T. Emmitt, K. J. Prey, and Y. Mou.",
                "Comparing the performance of database selection algorithms.",
                "In Research and Development in Information Retrieval, pages 238-245, 1999. [4] B. Horling, R. Mailler, and V. Lesser.",
                "Farm: A scalable environment for multi-agent development and evaluation.",
                "In Advances in Software Engineering for Multi-Agent Systems, pages 220-237, Berlin, 2004.",
                "Springer-Verlag. [5] M. Littman and J. Boyan.",
                "A distributed <br>reinforcement learning</br> scheme for network routing.",
                "In Proceedings of the International Workshop on Applications of Neural Networks to Telecommunications, 1993. [6] J. Lu and J. Callan.",
                "Federated search of text-based digital libraries in hierarchical peer-to-peer networks.",
                "In In ECIR05, 2005. [7] J. Lu and J. Callan.",
                "User modeling for full-text federated search in peer-to-peer networks.",
                "In ACM SIGIR 2006.",
                "ACM Press, 2006. [8] C. D. Manning and H. Sch¨utze.",
                "Foundations of Statistical Natural Language Processing.",
                "The MIT Press, Cambridge, Massachusetts, 1999. [9] C. R. Palmer and J. G. Steffan.",
                "Generating network topologies that obey power laws.",
                "In Proceedings of GLOBECOM 2000, November 2000. [10] K. Sripanidkulchai, B. Maggs, and H. Zhang.",
                "Efficient content location using interest-based locality in peer-topeer systems.",
                "In INFOCOM, 2003. [11] D. Subramanian, P. Druschel, and J. Chen.",
                "Ants and <br>reinforcement learning</br>: A case study in routing in dynamic networks.",
                "In In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence, pages 832-839, 1997. [12] J. N. Tao and L. Weaver.",
                "A multi-agent, policy gradient approach to network routing.",
                "In In Proceedings of the Eighteenth International Conference on Machine Learning, 2001. [13] H. Zhang, W. B. Croft, B. Levine, and V. Lesser.",
                "A multi-agent approach for peer-to-peer information retrieval.",
                "In Proceedings of Third International Joint Conference on Autonomous Agents and Multi-Agent Systems, July 2004. [14] H. Zhang and V. Lesser.",
                "Multi-agent based peer-to-peer information retrieval systems with concurrent search sessions.",
                "In Proceedings of the Fifth International Joint Conference on Autonomous Agents and Multi-Agent Systems, May 2006. [15] H. Zhang and V. R. Lesser.",
                "A dynamically formed hierarchical agent organization for a distributed content sharing system.",
                "In 2004 IEEE/WIC/ACM International Conference on Intelligent Agent Technology (IAT 2004), 20-24 September 2004, Beijing, China, pages 169-175.",
                "IEEE Computer Society, 2004. 238 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "A <br>reinforcement learning</br> based Distributed Search Algorithm For Hierarchical Peer-to-Peer Information Retrieval Systems Haizheng Zhang College of Information Science and Technology Pennsylvania State University University Park, PA 16803 hzhang@ist.psu.edu Victor Lesser Department of Computer Science University Of Massachusetts Amherst, MA 01003 lesser@cs.umass.edu ABSTRACT The dominant existing routing strategies employed in peerto-peer(P2P) based information retrieval(IR) systems are similarity-based approaches.",
                "In this paper, we develop a <br>reinforcement learning</br> based IR approach for improving the performance of distributed IR search algorithms.",
                "The intention of the <br>reinforcement learning</br> is to adapt the agents routing decisions to the dynamic network situations and learn from past search sessions.",
                "Specifically, the contributions of this paper include: (1) a <br>reinforcement learning</br> based approach for agents to acquire satisfactory routing policies based on estimates of the potential contribution of their neighboring agents; (2) two strategies to speed up the learning process.",
                "To our best knowledge, this is one of the first <br>reinforcement learning</br> applications in addressing distributed content sharing problems and it is indicative of some of the issues in applying reinforcement in a complex application."
            ],
            "translated_annotated_samples": [
                "Un algoritmo de búsqueda distribuida basado en <br>aprendizaje por refuerzo</br> para sistemas jerárquicos de recuperación de información entre pares.",
                "En este artículo, desarrollamos un enfoque de IR basado en <br>aprendizaje por refuerzo</br> para mejorar el rendimiento de los algoritmos de búsqueda de IR distribuidos.",
                "La intención del <br>aprendizaje por refuerzo</br> es adaptar las decisiones de enrutamiento de los agentes a las situaciones dinámicas de la red y aprender de sesiones de búsqueda pasadas.",
                "Específicamente, las contribuciones de este artículo incluyen: (1) un enfoque basado en <br>aprendizaje por refuerzo</br> para que los agentes adquieran políticas de enrutamiento satisfactorias basadas en estimaciones de la contribución potencial de sus agentes vecinos; (2) dos estrategias para acelerar el proceso de aprendizaje.",
                "Según nuestro mejor conocimiento, esta es una de las primeras aplicaciones de <br>aprendizaje por refuerzo</br> para abordar problemas de compartición de contenido distribuido, lo cual indica algunos de los problemas al aplicar el refuerzo en una aplicación compleja."
            ],
            "translated_text": "Un algoritmo de búsqueda distribuida basado en <br>aprendizaje por refuerzo</br> para sistemas jerárquicos de recuperación de información entre pares. En estos enfoques, los agentes dependen de la similitud de contenido entre las consultas entrantes y sus agentes vecinos directos para dirigir las sesiones de búsqueda distribuida. Sin embargo, tal heurística es miope en el sentido de que los agentes vecinos pueden no estar conectados con agentes más relevantes. En este documento, se desarrolla un enfoque basado en aprendizaje por refuerzo en línea para aprovechar las características dinámicas en tiempo de ejecución de los sistemas de recuperación de información P2P, representadas por información sobre sesiones de búsqueda pasadas. Específicamente, los agentes mantienen estimaciones sobre las habilidades de los agentes aguas abajo para proporcionar documentos relevantes para las consultas entrantes. Estas estimaciones se actualizan gradualmente al aprender de la información de retroalimentación devuelta de sesiones de búsqueda anteriores. Basándose en esta información, los agentes derivan políticas de enrutamiento correspondientes. Posteriormente, estos agentes dirigen las consultas basándose en las políticas aprendidas y actualizan las estimaciones según las nuevas políticas de enrutamiento. Los resultados experimentales demuestran que el algoritmo de aprendizaje mejora considerablemente el rendimiento del enrutamiento en dos conjuntos de colecciones de pruebas que han sido utilizados en una variedad de estudios de recuperación de información distribuida. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagente Términos Generales Algoritmos, Rendimiento, Experimentación 1. En los últimos años ha habido un creciente interés en estudiar cómo controlar los procesos de búsqueda en sistemas de recuperación de información (IR) basados en pares (P2P) [6, 13, 14, 15]. En esta línea de investigación, uno de los problemas centrales que preocupa a los investigadores es dirigir de manera eficiente las consultas de los usuarios en la red a los agentes que poseen los documentos apropiados. En ausencia de información global, las estrategias dominantes para abordar este problema son enfoques basados en la similitud de contenido [6, 13, 14, 15]. Si bien la similitud de contenido entre consultas y nodos locales parece ser un indicador creíble para la cantidad de documentos relevantes que residen en cada nodo, estos enfoques están limitados por una serie de factores. En primer lugar, las métricas basadas en similitud pueden ser miope ya que los nodos localmente relevantes pueden no estar conectados a otros nodos relevantes. Segundo, los enfoques basados en la similitud no tienen en cuenta las características de tiempo de ejecución de los sistemas de IR P2P, incluidos los parámetros ambientales, el uso del ancho de banda y la información histórica de las sesiones de búsqueda pasadas, que proporcionan información valiosa para los algoritmos de enrutamiento de consultas. En este artículo, desarrollamos un enfoque de IR basado en <br>aprendizaje por refuerzo</br> para mejorar el rendimiento de los algoritmos de búsqueda de IR distribuidos. Los agentes pueden adquirir mejores estrategias de búsqueda al recopilar y analizar información de retroalimentación de sesiones de búsqueda anteriores. En particular, los agentes mantienen estimaciones, es decir, utilidad esperada, sobre las capacidades de los agentes aguas abajo de proporcionar documentos relevantes para tipos específicos de consultas entrantes. Estas estimaciones se actualizan gradualmente al aprender de la información de retroalimentación devuelta de sesiones de búsqueda anteriores. Basándose en la información actualizada de utilidad esperada, los agentes derivan políticas de enrutamiento correspondientes. Posteriormente, estos agentes dirigen las consultas basándose en las políticas aprendidas y actualizan las estimaciones sobre la utilidad esperada en función de las nuevas políticas de enrutamiento. Este proceso se lleva a cabo de manera iterativa. El objetivo del algoritmo de aprendizaje, aunque consume ancho de banda de red, es reducir el tiempo de enrutamiento para procesar más consultas por unidad de tiempo y al mismo tiempo encontrar más documentos relevantes. Esto contrasta con los enfoques basados en la similitud de contenido, donde se repiten operaciones similares para cada consulta entrante y el tiempo de procesamiento se mantiene en gran medida constante con el tiempo. Otra forma de ver este documento es que nuestro enfoque básico para la búsqueda de IR distribuida es construir una red de superposición jerárquica (organización de agentes) basada en la medida de similitud de contenido entre las colecciones de documentos de los agentes de manera ascendente. En trabajos anteriores, hemos demostrado que esta organización mejora significativamente el rendimiento de búsqueda. Sin embargo, esta estructura organizativa no tiene en cuenta los patrones de llegada de las consultas, incluyendo su frecuencia, tipos y dónde entran en el sistema, ni el ancho de banda de comunicación disponible en la red y las capacidades de procesamiento de los agentes individuales. La intención del <br>aprendizaje por refuerzo</br> es adaptar las decisiones de enrutamiento de los agentes a las situaciones dinámicas de la red y aprender de sesiones de búsqueda pasadas. Específicamente, las contribuciones de este artículo incluyen: (1) un enfoque basado en <br>aprendizaje por refuerzo</br> para que los agentes adquieran políticas de enrutamiento satisfactorias basadas en estimaciones de la contribución potencial de sus agentes vecinos; (2) dos estrategias para acelerar el proceso de aprendizaje. Según nuestro mejor conocimiento, esta es una de las primeras aplicaciones de <br>aprendizaje por refuerzo</br> para abordar problemas de compartición de contenido distribuido, lo cual indica algunos de los problemas al aplicar el refuerzo en una aplicación compleja. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "distributed search algorithm": {
            "translated_key": "algoritmo de búsqueda distribuida",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Reinforcement Learning based <br>distributed search algorithm</br> For Hierarchical Peer-to-Peer Information Retrieval Systems Haizheng Zhang College of Information Science and Technology Pennsylvania State University University Park, PA 16803 hzhang@ist.psu.edu Victor Lesser Department of Computer Science University Of Massachusetts Amherst, MA 01003 lesser@cs.umass.edu ABSTRACT The dominant existing routing strategies employed in peerto-peer(P2P) based information retrieval(IR) systems are similarity-based approaches.",
                "In these approaches, agents depend on the content similarity between incoming queries and their direct neighboring agents to direct the distributed search sessions.",
                "However, such a heuristic is myopic in that the neighboring agents may not be connected to more relevant agents.",
                "In this paper, an online reinforcement-learning based approach is developed to take advantage of the dynamic run-time characteristics of P2P IR systems as represented by information about past search sessions.",
                "Specifically, agents maintain estimates on the downstream agents abilities to provide relevant documents for incoming queries.",
                "These estimates are updated gradually by learning from the feedback information returned from previous search sessions.",
                "Based on this information, the agents derive corresponding routing policies.",
                "Thereafter, these agents route the queries based on the learned policies and update the estimates based on the new routing policies.",
                "Experimental results demonstrate that the learning algorithm improves considerably the routing performance on two test collection sets that have been used in a variety of distributed IR studies.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Algorithms, Performance, Experimentation 1.",
                "INTRODUCTION Over the last few years there have been increasing interests in studying how to control the search processes in peer-to-peer(P2P) based information retrieval(IR) systems [6, 13, 14, 15].",
                "In this line of research, one of the core problems that concerns researchers is to efficiently route user queries in the network to agents that are in possession of appropriate documents.",
                "In the absence of global information, the dominant strategies in addressing this problem are content-similarity based approaches [6, 13, 14, 15].",
                "While the content similarity between queries and local nodes appears to be a creditable indicator for the number of relevant documents residing on each node, these approaches are limited by a number of factors.",
                "First of all, similaritybased metrics can be myopic since locally relevant nodes may not be connected to other relevant nodes.",
                "Second, the similarity-based approaches do not take into account the run-time characteristics of the P2P IR systems, including environmental parameters, bandwidth usage, and the historical information of the past search sessions, that provide valuable information for the query routing algorithms.",
                "In this paper, we develop a reinforcement learning based IR approach for improving the performance of distributed IR search algorithms.",
                "Agents can acquire better search strategies by collecting and analyzing feedback information from previous search sessions.",
                "Particularly, agents maintain estimates, namely expected utility, on the downstream agents capabilities of providing relevant documents for specific types of incoming queries.",
                "These estimates are updated gradually by learning from the feedback information returned from previous search sessions.",
                "Based on the updated expected utility information, the agents derive corresponding routing policies.",
                "Thereafter, these agents route the queries based on the learned policies and update the estimates on the expected utility based on the new routing policies.",
                "This process is conducted in an iterative manner.",
                "The goal of the learning algorithm, even though it consumes some network bandwidth, is to shorten the routing time so that more queries are processed per time unit while at the same time finding more relevant documents.",
                "This contrasts with the content-similarity based approaches where similar operations are repeated for every incoming query and the processing time keeps largely constant over time.",
                "Another way of viewing this paper is that our basic approach to distributed IR search is to construct a hierarchical overlay network(agent organization) based on the contentsimilarity measure among agents document collections in a bottom-up fashion.",
                "In the past work, we have shown that this organization improves search performance significantly.",
                "However, this organizational structure does not take into account the arrival patterns of queries, including their frequency, types, and where they enter the system, nor the available communication bandwidth of the network and processing capabilities of individual agents.",
                "The intention of the reinforcement learning is to adapt the agents routing decisions to the dynamic network situations and learn from past search sessions.",
                "Specifically, the contributions of this paper include: (1) a reinforcement learning based approach for agents to acquire satisfactory routing policies based on estimates of the potential contribution of their neighboring agents; (2) two strategies to speed up the learning process.",
                "To our best knowledge, this is one of the first reinforcement learning applications in addressing distributed content sharing problems and it is indicative of some of the issues in applying reinforcement in a complex application.",
                "The remainder of this paper is organized as follows: Section 2 reviews the hierarchical content sharing systems and the two-phase search algorithm based on such topology.",
                "Section 3 describes a reinforcement learning based approach to direct the routing process; Section 4 details the experimental settings and analyze the results.",
                "Section 5 discusses related studies and Section 6 concludes the paper. 2.",
                "SEARCH IN HIERARCHICAL P2P IR SYSTEMS This section briefly reviews our basic approaches to hierarchical P2P IR systems.",
                "In a hierarchical P2P IR system illustrated in Fig.1, agents are connected to each other through three types of links: upward links, downward links, and lateral links.",
                "In the following sections, we denote the set of agents that are directly connected to agent Ai as DirectConn(Ai), which is defined as DirectConn(Ai) = NEI(Ai) ∪ PAR(Ai) ∪ CHL(Ai) , where NEI(Ai) is the set of neighboring agents connected to Ai through lateral links; PAR(Ai) is the set of agents whom agent Ai is connected to through upward links and CHL(Ai) is the set of agents that agent Ai connects to through downward links.",
                "These links are established through a bottom-up content-similarity based distributed clustering process[15].",
                "These links are then used by agents to locate other agents that contain documents relevant to the given queries.",
                "A typical agent Ai in our system uses two queues: a local search queue, LSi, and a message forwarding queue MFi.",
                "The states of the two queues constitute the internal states of an agent.",
                "The local search queue LSi stores search sessions that are scheduled for local processing.",
                "It is a priority queue and agent Ai always selects the most promising queries to process in order to maximize the global utility.",
                "MFi consists of a set of queries to forward on and is processed in a FIFO (first in first out) fashion.",
                "For the first query in MFi, agent Ai determines which subset of its neighboring agents to forward it to based on the agents routing policy πi.",
                "These routing decisions determine how the search process is conducted in the network.",
                "In this paper, we call Ai as Ajs upstream agent and Aj as Ais downstream agent if A4 A5 A6 A7 A2 A3 A9 NEI(A2)={A3} PAR(A2)={A1} CHL(A2)={A4,A5} A1 A8 Figure 1: A fraction of a hierarchical P2PIR system an agent Ai routes a query to agent Aj.",
                "The distributed search protocol of our hierarchical agent organization is composed of two steps.",
                "In the first step, upon receipt of a query qk at time tl from a user, agent Ai initiates a search session si by probing its neighboring agents Aj ∈ NEI(Ai) with the message PROBE for the similarity value Sim(qk, Aj) between qk and Aj.",
                "Here, Ai is defined as the query initiator of search session si.",
                "In the second step, Ai selects a group of the most promising agents to start the actual search process with the message SEARCH.",
                "These SEARCH messages contain a TTL (Time To Live) parameter in addition to the query.",
                "The TTL value decreases by 1 after each hop.",
                "In the search process, agents discard those queries that either have been previously processed or whose TTL drops to 0, which prevents queries from looping in the system forever.",
                "The search session ends when all the agents that receive the query drop it or TTL decreases to 0.",
                "Upon receipt of SEARCH messages for qk, agents schedule local activities including local searching, forwarding qk to their neighbors, and returning search results to the query initiator.",
                "This process and related algorithms are detailed in [15, 14]. 3.",
                "A BASIC REINFORCEMENTLEARNING BASED SEARCH APPROACH In the aforementioned <br>distributed search algorithm</br>, the routing decisions of an agent Ai rely on the similarity comparison between incoming queries and Ais neighboring agents in order to forward those queries to relevant agents without flooding the network with unnecessary query messages.",
                "However, this heuristic is myopic because a relevant direct neighbor is not necessarily connected to other relevant agents.",
                "In this section, we propose a more general approach by framing this problem as a reinforcement learning task.",
                "In pursuit of greater flexibility, agents can switch between two modes: learning mode and non-learning mode.",
                "In the non-learning mode, agents operate in the same way as they do in the normal distributed search processes described in [14, 15].",
                "On the other hand, in the learning mode, in parallel with distributed search sessions, agents also participate in a learning process which will be detailed in this section.",
                "Note that in the learning protocol, the learning process does not interfere with the distributed search process.",
                "Agents can choose to initiate and stop learning processes without affecting the system performance.",
                "In particular, since the learning process consumes network resources (especially bandwidth), agents can choose to initiate learning only when the network load is relatively low, thus minimizing the extra communication costs incurred by the learning algorithm.",
                "The section is structured as follows, Section 3.1 describes 232 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) a reinforcement learning based model.",
                "Section 3.2 describes a protocol to deploy the learning algorithm in the network.",
                "Section 3.3 discusses the convergence of the learning algorithm. 3.1 The Model An agents routing policy takes the state of a search session as input and output the routing actions for that query.",
                "In our work, the state of a search session sj is stipulated as: QSj = (qk, ttlj) where ttlj is the number of hops that remains for the search session sj , qk is the specific query.",
                "QL is an attribute of qk that indicates which type of queries qk most likely belong to.",
                "The set of QL can be generated by running a simple online classification algorithm on all the queries that have been processed by the agents, or an oﬄine algorithm on a pre-designated training set.",
                "The assumption here is that the set of query types is learned ahead of time and belongs to the common knowledge of the agents in the network.",
                "Future work includes exploring how learning can be accomplished when this assumption does not hold.",
                "Given the query types set, an incoming query qi can be classified to one query class Q(qi) by the formula: Q(qi) = arg max Qj P(qi|Qj) (1) where P(qi|Qj ) indicates the likelihood that the query qi is generated by the query class Qj [8].",
                "The set of atomic routing actions of an agent Ai is denoted as {αi}, where {αi} is defined as αi = {αi0 , αi1 , ..., αin }.",
                "An element αij represents an action to route a given query to the neighboring agent Aij ∈ DirectConn(Ai).",
                "The routing policy πi of agent Ai is stochastic and its outcome for a search session with state QSj is defined as: πi(QSj) = {(αi0 , πi(QSi, αi0 )), (αi1 , πi(QSi, αi1 )), ...} (2) Note that operator πi is overloaded to represent either the probabilistic policy for a search session with state QSj, denoted as πi(QSj); or the probability of forwarding the query to a specific neighboring agent Aik ∈ DirectConn(Ai) under the policy πi(QSj), denoted as πi(QSj, αik ).",
                "Therefore, equation (2) means that the probability of forwarding the search session to agent Ai0 is πi(QSi, αi0 ) and so on.",
                "Under this stochastic policy, the routing action is nondeterministic.",
                "The advantage of such a strategy is that the best neighboring agents will not be selected repeatedly, thereby mitigating the potential hot spots situations.",
                "The expected utility, Un i (QSj), is used to estimate the potential utility gain of routing query type QSj to agent Ai under policy πn i .",
                "The superscript n indicates the value at the nth iteration in an iterative learning process.",
                "The expected utility provides routing guidance for future search sessions.",
                "In the search process, each agent Ai maintains partial observations of its neighbors states, as shown in Fig. 2.",
                "The partial observation includes non-local information such as the potential utility estimation of its neighbor Am for query state QSj, denoted as Um(QSj), as well as the load information, Lm.",
                "These observations are updated periodically by the neighbors.",
                "The estimated utility information will be used to update Ais expected utility for its routing policy.",
                "Load Information Expected Utility For Different Query Types Neighboring Agents ... A0 A1 A3 A2 Un 0 (QS0) ... ... ... ... ...... Un 0 (QS1) Un 1 (QS1) Un 2 (QS1) Un 3 (QS1) Un 1 (QS0) Un 2 (QS0) Un 3 (QS0) Ln 0 Ln 1 Ln 2 Ln 3 ... QS0 QS1 ...",
                "Figure 2: Agent Ais Partial Observation about its neighbors(A0, A1...) The load information of Am, Lm, is defined as Lm = |MFm| Cm , where |MFm| is the length of the message-forward queue and Cm is the service rate of agent Ams message-forward queue.",
                "Therefore Lm characterizes the utilization of an agents communication channel, and thus provide non-local information for Ams neighbors to adjust the parameters of their routing policy to avoid inundating their downstream agents.",
                "Note that based on the characteristics of the queries entering the system and agents capabilities, the loading of agents may not be uniform.",
                "After collecting the utilization rate information from all its neighbors, agent Ai computes Li as a single measure for assessing the average load condition of its neighborhood: Li = P k Lk |DirectConn(Ai)| Agents exploit Li value in determining the routing probability in its routing policy.",
                "Note that, as described in Section 3.2, information about neighboring agents is piggybacked with the query message propagated among the agents whenever possible to reduce the traffic overhead. 3.1.1 Update the Policy An iterative update process is introduced for agents to learn a satisfactory stochastic routing policy.",
                "In this iterative process, agents update their estimates on the potential utility of their current routing policies and then propagate the updated estimates to their neighbors.",
                "Their neighbors then generate a new routing policy based on the updated observation and in turn they calculate the expected utility based on the new policies and continue this iterative process.",
                "In particular, at time n, given a set of expected utility, an agent Ai, whose directly connected agents set is DirectConn(Ai) = {Ai0 , ..., Aim }, determines its corresponding stochastic routing policy for a search session of state QSj based on the following steps: (1) Ai first selects a subset of agents as the potential downstream agents from set DirectConn(Ai), denoted as PDn(Ai, QSj).",
                "The size of the potential downstream agent is specified as |PDn(Ai, QSj)| = min(|NEI(Ai), dn i + k)| where k is a constant and is set to 3 in this paper; dn i , the forward width, is defined as the expected number of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 233 neighboring agents that agent Ai can forward to at time n. This formula specifies that the potential downstream agent set PDn(Ai, QSj) is either the subset of neighboring agents with dn i + k highest expected utility value for state QSj among all the agents in DirectConn(Ai), or all their neighboring agents.",
                "The k is introduced based on the idea of a stochastic routing policy and it makes the forwarding probability of the dn i +k highest agent less than 100%.",
                "Note that if we want to limit the number of downstream agents for search session sj as 5, the probability of forwarding the query to all neighboring agents should add up to 5.",
                "Setting up dn i value properly can improve the utilization rate of the network bandwidth when much of the network is idle while mitigating the traffic load when the network is highly loaded.",
                "The dn+1 i value is updated based on dn i , the previous and current observations on the traffic situation in the neighborhood.",
                "Specifically, the update formula for dn+1 i is dn+1 i = dn i ∗ (1 + 1 − Li |DirectConn(Ai)| ) In this formula, the forward width is updated based on the traffic conditions of agent Ais neighborhood, i.e Li, and its previous value. (2) For each agent Aik in the PDn(Ai, QSj), the probability of forwarding the query to Aik is determined in the following way in order to assign higher forwarding probability to the neighboring agents with higher expected utility value: πn+1 i (QSj, αik ) = dn+1 i |PDn(Ai, QSj)| + β ∗ ` Uik (QSj) − PDU(Ai, QSj) |PDn(Ai, QSj)| ´ (3) where PDUn(Ai, QSj) = X o∈P Dn(Ai,QSj ) Uo(QSj) and QSj is the subsequent state of agent Aik after agent Ai forwards the search session with state QSj to its neighboring agent Aik ; If QSj = (qk, ttl0), then QSj = (qk, ttl0 − 1).",
                "In formula 3, the first term on the right of the equation, dn+1 i |P Dn(Ai,QSj )| , is used to to determine the forwarding probability by equally distributing the forward width, dn+1 i , to the agents in PDn(Ai, QSj) set.",
                "The second term is used to adjust the probability of being chosen so that agents with higher expected utility values will be favored. β is determined according to: β = min ` m − dn+1 i m ∗ umax − PDUn(Ai, QSj) , dn+1 i PDUn(Ai, QSj) − m ∗ umin ´ (4) where m = |PDn(Ai, QSj)|, umax = max o∈P Dn(Ao,QSj ) Uo(QSj) and umin = min o∈P Dn(Ao,QSj ) Uo(QSj) This formula guarantees that the final πn+1 i (QSj, αik ) value is well defined, i.e, 0 ≤ πn+1 i (QSj, αik ) ≤ 1 and X i πn+1 i (QSj, αik ) = dn+1 i However, such a solution does not explore all the possibilities.",
                "In order to balance between exploitation and exploration, a λ-Greedy approach is taken.",
                "In the λ-Greedy approach, in addition to assigning higher probability to those agents with higher expected utility value, as in the equation (3).",
                "Agents that appear to be not-so-good choices will also be sent queries based on a dynamic exploration rate.",
                "In particular, for agents in the set PDn(Ai, QSj), πn+1 i1 (QSj) is determined in the same way as the above, with the only difference being that dn+1 i is replaced with dn+1 i ∗ (1 − λn).",
                "The remaining search bandwidth is used for learning by assigning probability λn evenly to agents Ai2 in the set DirectConn(Ai) − PDn(Ai, QSj). πn+1 i2 (QSj, αik ) = dn+1 i ∗ λn |DirectConn(Ai) − PDn(Ai, QSj)| (5) where PDn(Ai, QSj) ⊂ DirectConn(Ai).",
                "Note that the exploration rate λ is not a constant and it decreases overtime.",
                "The λ is determined according to the following equation: λn+1 = λ0 ∗ e−c1n (6) where λ0 is the initial exploration rate, which is a constant; c1 is also a constant to adjust the decreasing rate of the exploration rate; n is the current time unit. 3.1.2 Update Expected Utility Once the routing policy at step n+1, πn+1 i , is determined based on the above formula, agent Ai can update its own expected utility, Un+1 i (QSi), based on the the updated routing policy resulted from the formula 5 and the updated U values of its neighboring agents.",
                "Under the assumption that after a query is forwarded to Ais neighbors the subsequent search sessions are independent, the update formula is similar to the Bellman update formula in Q-Learning: Un+1 i (QSj) = (1 − θi) ∗ Un i (QSj) + θi ∗ (Rn+1 i (QSj) + X k πn+1 i (QSj, αik )Un k (QSj)) (7) where QSj = (Qj, ttl − 1) is the next state of QSj = (Qj, ttl); Rn+1 i (QSj) is the expected local reward for query class Qk at agent Ai under the routing policy πn+1 i ; θi is the coefficient for deciding how much weight is given to the old value during the update process: the smaller θi value is, the faster the agent is expected to learn the real value, while the greater volatility of the algorithm, and vice versa.",
                "Rn+1 (s) is updated according to the following equation: 234 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Rn+1 i (QSj) = Rn i (QSj) +γi ∗ (r(QSj) − Rn i (QSj)) ∗ P(qj|Qj ) (8) where r(QSj) is the local reward associated with the search session.",
                "P(qj|Qj ) indicates how relevant the query qj is to the query type Qj, and γi is the learning rate for agent Ai.",
                "Depending on the similarity between a specific query qi and its corresponding query type Qi, the local reward associated with the search session has different impact on the Rn i (QSj) estimation.",
                "In the above formula, this impact is reflected by the coefficient, the P(qj|Qj) value. 3.1.3 Reward function After a search session stops when its TTL values expires, all search results are returned back to the user and are compared against the relevance judgment.",
                "Assuming the set of search results is SR, the reward Rew(SR) is defined as: Rew(SR) = j 1 if |Rel(SR)| > c |Rel(SR)| c otherwise. where SR is the set of returned search results, Rel(SR) is the set of relevant documents in the search results.",
                "This equation specifies that users give 1.0 reward if the number of returned relevant documents reaches a predefined number c. Otherwise, the reward is in proportion to the number of relevant documents returned.",
                "This rationale for setting up such a cut-off value is that the importance of recall ratio decreases with the abundance of relevant documents in real world, therefore users tend to focus on only a limited number of searched results.",
                "The details of the actual routing protocol will be introduced in Section 3.2 when we introduce how the learning algorithm is deployed in real systems. 3.2 Deployment of the Learning algorithm This section describes how the learning algorithm can be used in either a single-phase or a two-phase search process.",
                "In the single-phase search algorithm, search sessions start from the initiators of the queries.",
                "In contrast, in the two-step search algorithm, the query initiator first attempts to seek a more appropriate starting point for the query by introducing an exploratory step as described in Section 2.",
                "Despite the difference in the quality of starting points, the major part of the learning process for the two algorithms is largely the same as described in the following paragraphs.",
                "Before learning starts, each agent initializes the expected utility value for all possible states as 0.",
                "Thereafter, upon receipt of a query, in addition to the normal operations described in the previous section, an agent Ai also sets up a timer to wait for the search results returned from its downstream agents.",
                "Once the timer expires or it has received response from all its downstream agents, Ai merges and forwards the search results accrued from its downstream agents to its upstream agent.",
                "Setting up the timer speeds up the learning because agents can avoid waiting too long for the downstream agents to return search results.",
                "Note that these detailed results and corresponding agent information will still be stored at Ai until the feedback information is passed from its upstream agent and the performance of its downstream agents can be evaluated.",
                "The duration of the timer is related to the TTL value.",
                "In this paper, we set the timer to ttimer = ttli ∗ 2 + tf , where ttli ∗ 2 is the sum of the travel time of the queries in the network, and tf is the expected time period that users would like to wait.",
                "The search results will eventually be returned to the search session initiator A0.",
                "They will be compared to the relevance judgment that is provided by the final users (as described in the experiment section, the relevance judgement for the query set is provided along with the data collections).",
                "The reward will be calculated and propagated backward to the agents along the way that search results were passed.",
                "This is a reverse process of the search results propagation.",
                "In the process of propagating reward backward, agents update estimates of their own potential utility value, generate an upto-dated policy and pass their updated results to the neighboring agents based on the algorithm described in Section 3.",
                "Upon change of expected utility value, agent Ai sends out its updated utility estimation to its neighbors so that they can act upon the changed expected utility and corresponding state.",
                "This update message includes the potential reward as well as the corresponding state QSi = (qk, ttll) of agent Ai.",
                "Each neighboring agent, Aj, reacts to this kind of update message by updating the expected utility value for state QSj(qk, ttll + 1) according to the newly-announced changed expected utility value.",
                "Once they complete the update, the agents would again in turn inform related neighbors to update their values.",
                "This process goes on until the TTL value in the update message increases to the TTL limit.",
                "To speed up the learning process, while updating the expected utility values of an agent Ais neighboring agents we specify that Um(Qk, ttl0) >= Um(Qk, ttl1) iff ttl0 > ttl1 Thus, when agent Ai receives an updated expected utility value with ttl1, it also updates the expected utility values with any ttl0 > ttl1 if Um(Qk, ttl0) < Um(Qk, ttl1) to speed up convergence.",
                "This heuristic is based on the fact that the utility of a search session is a non-decreasing function of time t. 3.3 Discussion In formalizing the content routing system as a learning task, many assumptions are made.",
                "In real systems, these assumptions may not hold, and thus the learning algorithm may not converge.",
                "Two problems are of particular note, (1) This content routing problem does not have Markov properties.",
                "In contrast to IP-level based packet routing, the routing decision of each agent for a particular search session sj depends on the routing history of sj.",
                "Therefore, the assumption that all subsequent search sessions are independent does not hold in reality.",
                "This may lead to double counting problem that the relevant documents of some agents will be counted more than once for the state where the TTL value is more than 1.",
                "However, in the context of the hierarchical agent organizations, two factors mitigate this problems: first, the agents in each content group form a tree-like structure.",
                "With the absense of the cycles, the estimates inside the tree would be close to the accurate value.",
                "Secondly, the stochastic nature of the routing policy partly remedies this problem.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 235 (2) Another challenge for this learning algorithm is that in a real network environment observations on neighboring agents may not be able to be updated in time due to the communication delay or other situations.",
                "In addition, when neighboring agents update their estimates at the same time, oscillation may arise during the learning process[1].",
                "This paper explores several approaches to speed up the learning process.",
                "Besides the aforementioned strategy of updating the expected utility values, we also employ an active update strategy where agents notify their neighbors whenever its expected utility is updated.",
                "Thus a faster convergence speed can be achieved.",
                "This strategy contrasts to the Lazy update, where agents only echo their neighboring agents with their expected utility change when they exchange information.",
                "The trade off between the two approaches is the network load versus learning speed.",
                "The advantage of this learning algorithm is that once a routing policy is learned, agents do not have to repeatedly compare the similarity of queries as long as the network topology remains unchanged.",
                "Instead, agent just have to determine the classification of the query properly and follow the learned policies.",
                "The disadvantage of this learning-based approach is that the learning process needs to be conducted whenever the network structure changes.",
                "There are many potential extensions for this learning model.",
                "For example, a single measure is currently used to indicate the traffic load for an agents neighborhood.",
                "A simple extension would be to keep track of individual load for each neighbor of the agent. 4.",
                "EXPERIMENTSSETTINGSAND RESULTS The experiments are conducted on TRANO simulation toolkit with two sets of datasets, TREC-VLC-921 and TREC123-100.",
                "The following sub-sections introduce the TRANO testbed, the datasets, and the experimental results. 4.1 TRANO Testbed TRANO (Task Routing on Agent Network Organization) is a multi-agent based network based information retrieval testbed.",
                "TRANO is built on top of the Farm [4], a time based distributed simulator that provides a data dissemination framework for large scale distributed agent network based organizations.",
                "TRANO supports importation and exportation of agent organization profiles including topological connections and other features.",
                "Each TRANO agent is composed of an agent view structure and a control unit.",
                "In simulation, each agent is pulsed regularly and the agent checks the incoming message queues, performs local operations and then forwards messages to other agents . 4.2 Experimental Settings In our experiment, we use two standard datasets, TRECVLC-921 and TREC-123-100 datasets, to simulate the collections hosted on agents.",
                "The TREC-VLC-921 and TREC123-100 datasets were created by the U.S. National Institute for Standard Technology(NIST) for its TREC conferences.",
                "In distributed information retrieval domain, the two data collections are split to 921 and 100 sub-collections.",
                "It is observed that dataset TREC-VLC-921 is more heterogeneous than TREC-123-100 in terms of source, document length, and relevant document distribution from the statistics of the two data collections listed in [13].",
                "Hence, TREC-VLC-921 is much closer to real document distributions in P2P environments.",
                "Furthermore, TREC-123-100 is split into two sets of 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0 500 1000 1500 2000 2500 3000 ARSS Query number ARSS versus the number of incoming queries for TREC-VLC-921 SSLA-921 SSNA-921 Figure 3: ARSS(Average reward per search session) versus the number of search sessions for 1phase search in TREC-VLC-921 0 0.1 0.2 0.3 0.4 0.5 0.6 0 500 1000 1500 2000 2500 3000 ARSS Query number ARSS versus query number for TREC-VLC-921 TSLA-921 TSNA-921 Figure 4: ARSS(Average reward per search session) versus the number of search sessions for 2phase search in TREC-VLC-921 sub-collections in two ways: randomly and by source.",
                "The two partitions are denoted as TREC-123-100-Random and TREC-123-100-Source respectively.",
                "The documents in each subcollection in dataset TREC-123-100-Source are more coherent than those in TREC-123-100-Random.",
                "The two different sets of partitions allow us to observe how the distributed learning algorithm is affected by the homogeneity of the collections.",
                "The hierarchical agent organization is generated by the algorithm described in our previous algorithm [15].",
                "During the topology generation process, degree information of each agent is estimated by the algorithm introduced by Palmer et al. [9] with parameters α = 0.5 and β = 0.6.",
                "In our experiments, we estimate the upward limit and downward degree limit using linear discount factors 0.5, 0.8 and 1.0.",
                "Once the topology is built, queries randomly selected from the query set 301−350 on TREC-VLC-921 and query set 1− 50 on TREC-123-100-Random and TREC-123-100-Source are injected to the system based on a Poisson distribution P(N(t) = n) = (λt)n n! e−λ 236 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 0 50 100 150 200 250 300 350 400 0 500 1000 1500 2000 2500 3000 Cumulativeutility Query number Cumulative utility over the number of incoming queries TSLA-921 SSNA-921 SSLA-921 TSNA-921 Figure 5: The cumulative utility versus the number of search sessions TREC-VLC-921 In addition, we assume that all agents have an equal chance of getting queries from the environment, i.e, λ is the same for every agent.",
                "In our experiments, λ is set as 0.0543 so that the mean of the incoming queries from the environment to the agent network is 50 per time unit.",
                "The service time for the communication queue and local search queue, i.e tQij and trs, is set as 0.01 time unit and 0.05 time units respectively.",
                "In our experiments, there are ten types of queries acquired by clustering the query set 301 − 350 and 1 − 50. 4.3 Results analysis and evaluation Figure 3 demonstrates the ARSS(Average Reward per Search Session) versus the number of incoming queries over time for the the Single-Step based Non-learning Algorithm (SSNA), and the Single-Step Learning Algorithm(SSLA) for data collection TREC-VLC-921.",
                "It shows that the average reward for SSNA algorithm ranges from 0.02 − 0.06 and the performance changes little over time.",
                "The average reward for SSLA approach starts at the same level with the SSNA algorithm.",
                "But the performance increases over time and the average performance gain stabilizes at about 25% after query range 2000 − 3000.",
                "Figure 4 shows the ARSS(Average Reward per Search Session) versus the number of incoming queries over time for the the Two-Step based Non-learning Algorithm(TSNA), and the Two-Step Learning Algorithm(TSLA) for data collection TREC-VLC-921.",
                "The TSNA approach has a relatively consistent performance with the average reward ranges from 0.05 − 0.15.",
                "The average reward for TSLA approach, where learning algorithm is exploited, starts at the same level with the TSNA algorithm and improves the average reward over time until 2000−2500 queries joining the system.",
                "The results show that the average performance gain for TSLA approach over TNLA approach is 35% after stabilization.",
                "Figure 5 shows the cumulative utility versus the number of incoming queries over time for SSNA, SSLA,TSNA, and TSLA respectively.",
                "It illustrates that the cumulative utility of non-learning algorithms increases largely linearly over time, while the gains of learning-based algorithms accelerate when more queries enter the system.",
                "These experimental results demonstrate that learning-based approaches consistently perform better than non-learning based routing algorithm.",
                "Moreover, two-phase learning based algorithm is better than single-phase based learning algorithm because the maximal reward an agent can receive from searching its neighborhood within TTL hops is related to the total number of the relevant documents in that area.",
                "Thus, even the optimal routing policy can do little beyond reaching these relevant documents faster.",
                "On the contrary, the two-stepbased learning algorithm can relocate the search session to a neighborhood with more relevant documents.",
                "The TSLA combines the merits of both approaches and outperforms them.",
                "Table 1 lists the cumulative utility for datasets TREC123-100-Random and TREC-123-100-Source with hierarchical organizations.",
                "The five columns show the results for four different approaches.",
                "In particular, column TSNA-Random shows the results for dataset TREC-123-100-Random with the TSNA approach.",
                "The column TSLA-Random shows the results for dataset TREC-123-100-Random with the TSLA approach.",
                "There are two numbers in each cell in the column TSLA-Random.",
                "The first number is the actual cumulative utility while the second number is the percentage gain in terms of the utility over TSNA approach.",
                "Columns TSNA-Source and TSLA-Source show the results for dataset TREC-123-100-Source with TSNA and TSLA approaches respectively.",
                "Table 1 shows that the performance improvement for TREC-123-100-Random is not as significant as the other datasets.",
                "This is because that the documents in the sub-collection of TREC-123-100-Random are selected randomly which makes the collection model, the signature of the collection, less meaningful.",
                "Since both algorithms are designed based on the assumption that document collections can be well represented by their collection model, this result is not surprising.",
                "Overall, Figures 4, 5, and Table 1 demonstrate that the reinforcement learning based approach can considerably enhance the system performance for both data collections.",
                "However, it remains as future work to discover the correlation between the magnitude of the performance gains and the size of the data collection and/or the extent of the heterogeneity between the sub-collections. 5.",
                "RELATED WORK The content routing problem differs from the networklevel routing in packet-switched communication networks in that content-based routing occurs in application-level networks.",
                "In addition, the destination agents in our contentrouting algorithms are multiple and the addresses are not known in the routing process.",
                "IP-level Routing problems have been attacked from the reinforcement learning perspective[2, 5, 11, 12].",
                "These studies have explored fully distributed algorithms that are able, without central coordination to disseminate knowledge about the network, to find the shortest paths robustly and efficiently in the face of changing network topologies and changing link costs.",
                "There are two major classes of adaptive, distributed packet routing algorithms in the literature: distance-vector algorithms and link-state algorithms.",
                "While this line of studies carry a certain similarity with our work, it has mainly focused on packet-switched communication networks.",
                "In this domain, the destination of a packet is deterministic and unique.",
                "Each agent maintains estimations, probabilistically or deterministically, on the distance to a certain destination through its neighbors.",
                "A variant of Q-Learning techniques is deployed The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 237 Table 1: Cumulative Utility for Datasets TREC-123-100-Random and TREC-123-100-Source with Hierarchical Organization; The percentage numbers in the columns TSLA-Random and TSLA-Source demonstrate the performance gain over the algorithm without learning Query number TSNA-Random TSLA-Random TSNA-Source TSLA-Source 500 25.15 28.45 13% 24.00 21.05 -13% 1000 104.99 126.74 20% 93.95 96.44 2.6% 1250 149.79 168.40 12% 122.64 134.05 9.3% 1500 188.94 211.05 12% 155.30 189.60 22% 1750 235.49 261.60 11% 189.14 243.90 28% 2000 275.09 319.25 16% 219.0 278.80 26% to update the estimations to converge to the real distances.",
                "It has been discovered that the locality property is an important feature of information retrieval systems in user modeling studies[3].",
                "In P2P based content sharing systems, this property is exemplified by the phenomenon that users tend to send queries that represent only a limited number of topics and conversely, users in the same neighborhood are likely to share common interests and send similar queries [10].",
                "The learning based approach is perceived to be more beneficial for real distributed information retrieval systems which exhibit locality property.",
                "This is because the users traffic and query patterns can reduce the state space and speed up the learning process.",
                "Related work in taking advantage of this property include [7], where the authors attempted to address this problem by user modeling techniques. 6.",
                "CONCLUSIONS In this paper, a reinforcement-learning based approach is developed to improve the performance of distributed IR search algorithms.",
                "Particularly, agents maintain estimates, namely expected utility, on the downstream agents ability to provide relevant documents for incoming queries.",
                "These estimates are updated gradually by learning from the feedback information returned from previous search sessions.",
                "Based on the updated expected utility information, the agents modify their routing policies.",
                "Thereafter, these agents route the queries based on the learned policies and update the estimates on the expected utility based on the new routing policies.",
                "The experiments on two different distributed IR datasets illustrates that the reinforcement learning approach improves considerably the cumulative utility over time. 7.",
                "REFERENCES [1] S. Abdallah and V. Lesser.",
                "Learning the task allocation game.",
                "In AAMAS 06: Proceedings of the fifth international joint conference on Autonomous agents and multiagent systems, pages 850-857, New York, NY, USA, 2006.",
                "ACM Press. [2] J.",
                "A. Boyan and M. L. Littman.",
                "Packet routing in dynamically changing networks: A reinforcement learning approach.",
                "In Advances in Neural Information Processing Systems, volume 6, pages 671-678.",
                "Morgan Kaufmann Publishers, Inc., 1994. [3] J. C. French, A. L. Powell, J. P. Callan, C. L. Viles, T. Emmitt, K. J. Prey, and Y. Mou.",
                "Comparing the performance of database selection algorithms.",
                "In Research and Development in Information Retrieval, pages 238-245, 1999. [4] B. Horling, R. Mailler, and V. Lesser.",
                "Farm: A scalable environment for multi-agent development and evaluation.",
                "In Advances in Software Engineering for Multi-Agent Systems, pages 220-237, Berlin, 2004.",
                "Springer-Verlag. [5] M. Littman and J. Boyan.",
                "A distributed reinforcement learning scheme for network routing.",
                "In Proceedings of the International Workshop on Applications of Neural Networks to Telecommunications, 1993. [6] J. Lu and J. Callan.",
                "Federated search of text-based digital libraries in hierarchical peer-to-peer networks.",
                "In In ECIR05, 2005. [7] J. Lu and J. Callan.",
                "User modeling for full-text federated search in peer-to-peer networks.",
                "In ACM SIGIR 2006.",
                "ACM Press, 2006. [8] C. D. Manning and H. Sch¨utze.",
                "Foundations of Statistical Natural Language Processing.",
                "The MIT Press, Cambridge, Massachusetts, 1999. [9] C. R. Palmer and J. G. Steffan.",
                "Generating network topologies that obey power laws.",
                "In Proceedings of GLOBECOM 2000, November 2000. [10] K. Sripanidkulchai, B. Maggs, and H. Zhang.",
                "Efficient content location using interest-based locality in peer-topeer systems.",
                "In INFOCOM, 2003. [11] D. Subramanian, P. Druschel, and J. Chen.",
                "Ants and reinforcement learning: A case study in routing in dynamic networks.",
                "In In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence, pages 832-839, 1997. [12] J. N. Tao and L. Weaver.",
                "A multi-agent, policy gradient approach to network routing.",
                "In In Proceedings of the Eighteenth International Conference on Machine Learning, 2001. [13] H. Zhang, W. B. Croft, B. Levine, and V. Lesser.",
                "A multi-agent approach for peer-to-peer information retrieval.",
                "In Proceedings of Third International Joint Conference on Autonomous Agents and Multi-Agent Systems, July 2004. [14] H. Zhang and V. Lesser.",
                "Multi-agent based peer-to-peer information retrieval systems with concurrent search sessions.",
                "In Proceedings of the Fifth International Joint Conference on Autonomous Agents and Multi-Agent Systems, May 2006. [15] H. Zhang and V. R. Lesser.",
                "A dynamically formed hierarchical agent organization for a distributed content sharing system.",
                "In 2004 IEEE/WIC/ACM International Conference on Intelligent Agent Technology (IAT 2004), 20-24 September 2004, Beijing, China, pages 169-175.",
                "IEEE Computer Society, 2004. 238 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "A Reinforcement Learning based <br>distributed search algorithm</br> For Hierarchical Peer-to-Peer Information Retrieval Systems Haizheng Zhang College of Information Science and Technology Pennsylvania State University University Park, PA 16803 hzhang@ist.psu.edu Victor Lesser Department of Computer Science University Of Massachusetts Amherst, MA 01003 lesser@cs.umass.edu ABSTRACT The dominant existing routing strategies employed in peerto-peer(P2P) based information retrieval(IR) systems are similarity-based approaches.",
                "A BASIC REINFORCEMENTLEARNING BASED SEARCH APPROACH In the aforementioned <br>distributed search algorithm</br>, the routing decisions of an agent Ai rely on the similarity comparison between incoming queries and Ais neighboring agents in order to forward those queries to relevant agents without flooding the network with unnecessary query messages."
            ],
            "translated_annotated_samples": [
                "Un <br>algoritmo de búsqueda distribuida</br> basado en aprendizaje por refuerzo para sistemas jerárquicos de recuperación de información entre pares.",
                "En el enfoque de búsqueda basado en aprendizaje por refuerzo, mencionado anteriormente, las decisiones de enrutamiento de un agente Ai dependen de la comparación de similitud entre las consultas entrantes y los agentes vecinos de Ai para reenviar esas consultas a agentes relevantes sin inundar la red con mensajes de consulta innecesarios."
            ],
            "translated_text": "Un <br>algoritmo de búsqueda distribuida</br> basado en aprendizaje por refuerzo para sistemas jerárquicos de recuperación de información entre pares. En estos enfoques, los agentes dependen de la similitud de contenido entre las consultas entrantes y sus agentes vecinos directos para dirigir las sesiones de búsqueda distribuida. Sin embargo, tal heurística es miope en el sentido de que los agentes vecinos pueden no estar conectados con agentes más relevantes. En este documento, se desarrolla un enfoque basado en aprendizaje por refuerzo en línea para aprovechar las características dinámicas en tiempo de ejecución de los sistemas de recuperación de información P2P, representadas por información sobre sesiones de búsqueda pasadas. Específicamente, los agentes mantienen estimaciones sobre las habilidades de los agentes aguas abajo para proporcionar documentos relevantes para las consultas entrantes. Estas estimaciones se actualizan gradualmente al aprender de la información de retroalimentación devuelta de sesiones de búsqueda anteriores. Basándose en esta información, los agentes derivan políticas de enrutamiento correspondientes. Posteriormente, estos agentes dirigen las consultas basándose en las políticas aprendidas y actualizan las estimaciones según las nuevas políticas de enrutamiento. Los resultados experimentales demuestran que el algoritmo de aprendizaje mejora considerablemente el rendimiento del enrutamiento en dos conjuntos de colecciones de pruebas que han sido utilizados en una variedad de estudios de recuperación de información distribuida. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagente Términos Generales Algoritmos, Rendimiento, Experimentación 1. En los últimos años ha habido un creciente interés en estudiar cómo controlar los procesos de búsqueda en sistemas de recuperación de información (IR) basados en pares (P2P) [6, 13, 14, 15]. En esta línea de investigación, uno de los problemas centrales que preocupa a los investigadores es dirigir de manera eficiente las consultas de los usuarios en la red a los agentes que poseen los documentos apropiados. En ausencia de información global, las estrategias dominantes para abordar este problema son enfoques basados en la similitud de contenido [6, 13, 14, 15]. Si bien la similitud de contenido entre consultas y nodos locales parece ser un indicador creíble para la cantidad de documentos relevantes que residen en cada nodo, estos enfoques están limitados por una serie de factores. En primer lugar, las métricas basadas en similitud pueden ser miope ya que los nodos localmente relevantes pueden no estar conectados a otros nodos relevantes. Segundo, los enfoques basados en la similitud no tienen en cuenta las características de tiempo de ejecución de los sistemas de IR P2P, incluidos los parámetros ambientales, el uso del ancho de banda y la información histórica de las sesiones de búsqueda pasadas, que proporcionan información valiosa para los algoritmos de enrutamiento de consultas. En este artículo, desarrollamos un enfoque de IR basado en aprendizaje por refuerzo para mejorar el rendimiento de los algoritmos de búsqueda de IR distribuidos. Los agentes pueden adquirir mejores estrategias de búsqueda al recopilar y analizar información de retroalimentación de sesiones de búsqueda anteriores. En particular, los agentes mantienen estimaciones, es decir, utilidad esperada, sobre las capacidades de los agentes aguas abajo de proporcionar documentos relevantes para tipos específicos de consultas entrantes. Estas estimaciones se actualizan gradualmente al aprender de la información de retroalimentación devuelta de sesiones de búsqueda anteriores. Basándose en la información actualizada de utilidad esperada, los agentes derivan políticas de enrutamiento correspondientes. Posteriormente, estos agentes dirigen las consultas basándose en las políticas aprendidas y actualizan las estimaciones sobre la utilidad esperada en función de las nuevas políticas de enrutamiento. Este proceso se lleva a cabo de manera iterativa. El objetivo del algoritmo de aprendizaje, aunque consume ancho de banda de red, es reducir el tiempo de enrutamiento para procesar más consultas por unidad de tiempo y al mismo tiempo encontrar más documentos relevantes. Esto contrasta con los enfoques basados en la similitud de contenido, donde se repiten operaciones similares para cada consulta entrante y el tiempo de procesamiento se mantiene en gran medida constante con el tiempo. Otra forma de ver este documento es que nuestro enfoque básico para la búsqueda de IR distribuida es construir una red de superposición jerárquica (organización de agentes) basada en la medida de similitud de contenido entre las colecciones de documentos de los agentes de manera ascendente. En trabajos anteriores, hemos demostrado que esta organización mejora significativamente el rendimiento de búsqueda. Sin embargo, esta estructura organizativa no tiene en cuenta los patrones de llegada de las consultas, incluyendo su frecuencia, tipos y dónde entran en el sistema, ni el ancho de banda de comunicación disponible en la red y las capacidades de procesamiento de los agentes individuales. La intención del aprendizaje por refuerzo es adaptar las decisiones de enrutamiento de los agentes a las situaciones dinámicas de la red y aprender de sesiones de búsqueda pasadas. Específicamente, las contribuciones de este artículo incluyen: (1) un enfoque basado en aprendizaje por refuerzo para que los agentes adquieran políticas de enrutamiento satisfactorias basadas en estimaciones de la contribución potencial de sus agentes vecinos; (2) dos estrategias para acelerar el proceso de aprendizaje. Según nuestro mejor conocimiento, esta es una de las primeras aplicaciones de aprendizaje por refuerzo para abordar problemas de compartición de contenido distribuido, lo cual indica algunos de los problemas al aplicar el refuerzo en una aplicación compleja. El resto de este documento está organizado de la siguiente manera: la Sección 2 revisa los sistemas jerárquicos de intercambio de contenido y el algoritmo de búsqueda de dos fases basado en dicha topología. La Sección 3 describe un enfoque basado en aprendizaje por refuerzo para dirigir el proceso de enrutamiento; la Sección 4 detalla la configuración experimental y analiza los resultados. La sección 5 discute estudios relacionados y la sección 6 concluye el artículo. BÚSQUEDA EN SISTEMAS JERÁRQUICOS DE IR P2P Esta sección revisa brevemente nuestros enfoques básicos para los sistemas jerárquicos de IR P2P. En un sistema jerárquico de recuperación de información P2P ilustrado en la Fig.1, los agentes están conectados entre sí a través de tres tipos de enlaces: enlaces ascendentes, enlaces descendentes y enlaces laterales. En las siguientes secciones, denotamos el conjunto de agentes que están directamente conectados al agente Ai como DirectConn(Ai), el cual se define como DirectConn(Ai) = NEI(Ai) ∪ PAR(Ai) ∪ CHL(Ai), donde NEI(Ai) es el conjunto de agentes vecinos conectados a Ai a través de enlaces laterales; PAR(Ai) es el conjunto de agentes a los que el agente Ai está conectado a través de enlaces ascendentes y CHL(Ai) es el conjunto de agentes a los que el agente Ai se conecta a través de enlaces descendentes. Estos enlaces se establecen a través de un proceso de agrupamiento distribuido basado en la similitud de contenido de abajo hacia arriba[15]. Estos enlaces son luego utilizados por agentes para localizar otros agentes que contienen documentos relevantes a las consultas dadas. Un agente típico Ai en nuestro sistema utiliza dos colas: una cola de búsqueda local, LSi, y una cola de reenvío de mensajes MFi. Los estados de las dos colas constituyen los estados internos de un agente. La cola de búsqueda local LSi almacena sesiones de búsqueda programadas para procesamiento local. Es una cola de prioridad y el agente Ai siempre selecciona las consultas más prometedoras para procesarlas con el fin de maximizar la utilidad global. MFi consiste en un conjunto de consultas para reenviar y se procesa de manera FIFO (primero en entrar, primero en salir). Para la primera consulta en MFi, el agente Ai determina a qué subconjunto de sus agentes vecinos reenviarla basándose en la política de enrutamiento de los agentes πi. Estas decisiones de enrutamiento determinan cómo se lleva a cabo el proceso de búsqueda en la red. En este documento, llamamos a Ai como el agente aguas arriba de Ajs y a Aj como el agente aguas abajo de Ais si A4 A5 A6 A7 A2 A3 A9 NEI(A2)={A3} PAR(A2)={A1} CHL(A2)={A4,A5} A1 A8 Figura 1: Una fracción de un sistema jerárquico P2PIR en el que un agente Ai dirige una consulta al agente Aj. El protocolo de búsqueda distribuida de nuestra organización de agentes jerárquicos está compuesto por dos pasos. En el primer paso, al recibir una consulta qk en el tiempo tl de un usuario, el agente Ai inicia una sesión de búsqueda si al sondear a sus agentes vecinos Aj ∈ NEI(Ai) con el mensaje PROBE para el valor de similitud Sim(qk, Aj) entre qk y Aj. Aquí, Ai se define como el iniciador de la consulta de la sesión de búsqueda si. En el segundo paso, Ai selecciona un grupo de agentes más prometedores para iniciar el proceso de búsqueda real con el mensaje BUSCAR. Estos mensajes de BÚSQUEDA contienen un parámetro TTL (Tiempo de Vida) además de la consulta. El valor TTL disminuye en 1 después de cada salto. En el proceso de búsqueda, los agentes descartan aquellas consultas que han sido procesadas previamente o cuyo TTL disminuye a 0, lo que evita que las consultas se repitan infinitamente en el sistema. La sesión de búsqueda termina cuando todos los agentes que reciben la consulta la descartan o el TTL disminuye a 0. Tras recibir mensajes de BÚSQUEDA para qk, los agentes programan actividades locales que incluyen búsqueda local, reenvío de qk a sus vecinos y devolución de resultados de búsqueda al iniciador de la consulta. Este proceso y los algoritmos relacionados se detallan en [15, 14]. 3. En el enfoque de búsqueda basado en aprendizaje por refuerzo, mencionado anteriormente, las decisiones de enrutamiento de un agente Ai dependen de la comparación de similitud entre las consultas entrantes y los agentes vecinos de Ai para reenviar esas consultas a agentes relevantes sin inundar la red con mensajes de consulta innecesarios. Sin embargo, esta heurística es miope porque un vecino directo relevante no necesariamente está conectado con otros agentes relevantes. En esta sección, proponemos un enfoque más general al plantear este problema como una tarea de aprendizaje por refuerzo. En busca de una mayor flexibilidad, los agentes pueden alternar entre dos modos: modo de aprendizaje y modo sin aprendizaje. En el modo de no aprendizaje, los agentes operan de la misma manera que lo hacen en los procesos de búsqueda distribuida normales descritos en [14, 15]. Por otro lado, en el modo de aprendizaje, en paralelo con las sesiones de búsqueda distribuida, los agentes también participan en un proceso de aprendizaje que se detallará en esta sección. Ten en cuenta que en el protocolo de aprendizaje, el proceso de aprendizaje no interfiere con el proceso de búsqueda distribuida. Los agentes pueden elegir iniciar y detener procesos de aprendizaje sin afectar el rendimiento del sistema. En particular, dado que el proceso de aprendizaje consume recursos de red (especialmente ancho de banda), los agentes pueden optar por iniciar el aprendizaje solo cuando la carga de la red es relativamente baja, minimizando así los costos adicionales de comunicación incurridos por el algoritmo de aprendizaje. La sección está estructurada de la siguiente manera, la Sección 3.1 describe 232 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) un modelo basado en aprendizaje por refuerzo. La sección 3.2 describe un protocolo para implementar el algoritmo de aprendizaje en la red. La sección 3.3 discute la convergencia del algoritmo de aprendizaje. 3.1 El Modelo La política de enrutamiento de un agente toma el estado de una sesión de búsqueda como entrada y produce las acciones de enrutamiento para esa consulta. En nuestro trabajo, el estado de una sesión de búsqueda sj se estipula como: QSj = (qk, ttlj) donde ttlj es el número de saltos que quedan para la sesión de búsqueda sj, qk es la consulta específica. QL es un atributo de qk que indica a qué tipo de consultas qk probablemente pertenecen. El conjunto de QL puede ser generado ejecutando un algoritmo de clasificación en línea simple en todas las consultas que han sido procesadas por los agentes, o un algoritmo fuera de línea en un conjunto de entrenamiento pre-designado. La suposición aquí es que el conjunto de tipos de consultas se aprende de antemano y pertenece al conocimiento común de los agentes en la red. El trabajo futuro incluye explorar cómo se puede lograr el aprendizaje cuando esta suposición no se cumple. Dado el conjunto de tipos de consultas establecido, una consulta entrante qi puede ser clasificada en una clase de consulta Q(qi) mediante la fórmula: Q(qi) = arg max Qj P(qi|Qj) (1) donde P(qi|Qj) indica la probabilidad de que la consulta qi sea generada por la clase de consulta Qj [8]. El conjunto de acciones de enrutamiento atómico de un agente Ai se denota como {αi}, donde {αi} se define como αi = {αi0 , αi1 , ..., αin }. Un elemento αij representa una acción para dirigir una consulta dada al agente vecino Aij ∈ DirectConn(Ai). La política de enrutamiento πi del agente Ai es estocástica y su resultado para una sesión de búsqueda con estado QSj se define como: πi(QSj) = {(αi0 , πi(QSi, αi0 )), (αi1 , πi(QSi, αi1 )), ...} (2) Nótese que el operador πi está sobrecargado para representar tanto la política probabilística para una sesión de búsqueda con estado QSj, denotada como πi(QSj); como la probabilidad de enviar la consulta a un agente vecino específico Aik ∈ DirectConn(Ai) bajo la política πi(QSj), denotada como πi(QSj, αik). Por lo tanto, la ecuación (2) significa que la probabilidad de reenviar la sesión de búsqueda al agente Ai0 es πi(QSi, αi0) y así sucesivamente. Bajo esta política estocástica, la acción de enrutamiento es no determinística. La ventaja de esta estrategia es que los mejores agentes vecinos no serán seleccionados repetidamente, mitigando así las posibles situaciones de puntos calientes. La utilidad esperada, Un i (QSj), se utiliza para estimar la ganancia potencial de utilidad al dirigir el tipo de consulta QSj al agente Ai bajo la política πn i. El superíndice n indica el valor en la n-ésima iteración en un proceso de aprendizaje iterativo. La utilidad esperada proporciona orientación de enrutamiento para futuras sesiones de búsqueda. En el proceso de búsqueda, cada agente Ai mantiene observaciones parciales de los estados de sus vecinos, como se muestra en la Figura 2. La observación parcial incluye información no local como la estimación de la utilidad potencial de su vecino Am para el estado de consulta QSj, denotado como Um(QSj), así como la información de carga, Lm. Estas observaciones son actualizadas periódicamente por los vecinos. La información de utilidad estimada se utilizará para actualizar la utilidad esperada de A en su política de enrutamiento. Cargar información de utilidad esperada para diferentes tipos de consultas de agentes vecinos... A0 A1 A3 A2 Un 0 (QS0) ... ... ... ... ...... Un 0 (QS1) Un 1 (QS1) Un 2 (QS1) Un 3 (QS1) Un 1 (QS0) Un 2 (QS0) Un 3 (QS0) Ln 0 Ln 1 Ln 2 Ln 3 ... QS0 QS1 ... Figura 2: La Observación Parcial del Agente Ais sobre sus vecinos (A0, A1...) La información de carga de Am, Lm, se define como Lm = |MFm| Cm, donde |MFm| es la longitud de la cola de mensajes en espera y Cm es la tasa de servicio de la cola de mensajes en espera del agente Am. Por lo tanto, Lm caracteriza la utilización de un canal de comunicación de agentes, y así proporciona información no local a los vecinos de Ams para ajustar los parámetros de su política de enrutamiento y evitar inundar a sus agentes aguas abajo. Ten en cuenta que, basado en las características de las consultas que ingresan al sistema y las capacidades de los agentes, la carga de trabajo de los agentes puede no ser uniforme. Después de recopilar la información de la tasa de utilización de todos sus vecinos, el agente Ai calcula Li como una medida única para evaluar la condición de carga promedio de su vecindario: Li = P k Lk |DirectConn(Ai)| Los agentes explotan el valor de Li para determinar la probabilidad de enrutamiento en su política de enrutamiento. Se debe tener en cuenta que, como se describe en la Sección 3.2, la información sobre agentes vecinos se transmite junto con el mensaje de consulta propagado entre los agentes siempre que sea posible para reducir la sobrecarga de tráfico. 3.1.1 Actualización de la Política Se introduce un proceso de actualización iterativo para que los agentes aprendan una política de enrutamiento estocástica satisfactoria. En este proceso iterativo, los agentes actualizan sus estimaciones sobre la utilidad potencial de sus políticas de enrutamiento actuales y luego propagan las estimaciones actualizadas a sus vecinos. Sus vecinos luego generan una nueva política de enrutamiento basada en la observación actualizada y a su vez calculan la utilidad esperada basada en las nuevas políticas y continúan este proceso iterativo. En particular, en el tiempo n, dado un conjunto de utilidad esperada, un agente Ai, cuyo conjunto de agentes directamente conectados es DirectConn(Ai) = {Ai0, ..., Aim}, determina su política de enrutamiento estocástico correspondiente para una sesión de búsqueda del estado QSj basada en los siguientes pasos: (1) Ai primero selecciona un subconjunto de agentes como los agentes potenciales aguas abajo del conjunto DirectConn(Ai), denotado como PDn(Ai, QSj). El tamaño del agente potencial aguas abajo se especifica como |PDn(Ai, QSj)| = min(|NEI(Ai), dn i + k)| donde k es una constante y se establece en 3 en este documento; dn i, el ancho hacia adelante, se define como el número esperado de The Sixth Intl. La fórmula especifica que el conjunto potencial de agentes aguas abajo PDn(Ai, QSj) es o bien el subconjunto de agentes vecinos con los k valores de utilidad esperada más altos para el estado QSj entre todos los agentes en DirectConn(Ai), o todos sus agentes vecinos. La k se introduce basada en la idea de una política de enrutamiento estocástica y hace que la probabilidad de reenvío del agente dn i + k más alto sea inferior al 100%. Ten en cuenta que si queremos limitar el número de agentes secundarios para la sesión de búsqueda sj a 5, la probabilidad de reenviar la consulta a todos los agentes vecinos debe sumar 5. Configurar adecuadamente el valor de dn i puede mejorar la tasa de utilización del ancho de banda de la red cuando gran parte de la red está inactiva, al mismo tiempo que mitiga la carga de tráfico cuando la red está altamente cargada. El valor de dn+1 i se actualiza en función de dn i, las observaciones anteriores y actuales sobre la situación del tráfico en el vecindario. Específicamente, la fórmula de actualización para dn+1 i es dn+1 i = dn i ∗ (1 + 1 − Li |DirectConn(Ai)| ) En esta fórmula, el ancho de avance se actualiza en función de las condiciones de tráfico del vecindario del agente Ai, es decir, Li, y su valor anterior. (2) Para cada agente Aik en el PDn(Ai, QSj), la probabilidad de reenviar la consulta a Aik se determina de la siguiente manera para asignar una mayor probabilidad de reenvío a los agentes vecinos con un valor de utilidad esperado más alto: πn+1 i (QSj, αik ) = dn+1 i |PDn(Ai, QSj)| + β ∗ ` Uik (QSj) − PDU(Ai, QSj) |PDn(Ai, QSj)| ´ (3) donde PDUn(Ai, QSj) = X o∈P Dn(Ai,QSj ) Uo(QSj) y QSj es el estado subsiguiente del agente Aik después de que el agente Ai reenvía la sesión de búsqueda con estado QSj a su agente vecino Aik; Si QSj = (qk, ttl0), entonces QSj = (qk, ttl0 − 1). En la fórmula 3, el primer término a la derecha de la ecuación, dn+1 i |P Dn(Ai,QSj )|, se utiliza para determinar la probabilidad de reenvío distribuyendo equitativamente el ancho de avance, dn+1 i, a los agentes en el conjunto PDn(Ai, QSj). El segundo término se utiliza para ajustar la probabilidad de ser elegido de manera que se favorezca a los agentes con valores de utilidad esperada más altos. β se determina de acuerdo a: β = min ` m − dn+1 i m ∗ umax − PDUn(Ai, QSj) , dn+1 i PDUn(Ai, QSj) − m ∗ umin ´ (4) donde m = |PDn(Ai, QSj)|, umax = max o∈P Dn(Ao,QSj ) Uo(QSj) y umin = min o∈P Dn(Ao,QSj ) Uo(QSj) Esta fórmula garantiza que el valor final de πn+1 i (QSj, αik ) esté bien definido, es decir, 0 ≤ πn+1 i (QSj, αik ) ≤ 1 y X i πn+1 i (QSj, αik ) = dn+1 i Sin embargo, esta solución no explora todas las posibilidades. Para equilibrar entre la explotación y la exploración, se adopta un enfoque λ-Greedy. En el enfoque λ-Greedy, además de asignar una probabilidad más alta a aquellos agentes con un valor de utilidad esperado más alto, como en la ecuación (3). Los agentes que parecen no ser buenas opciones también recibirán consultas basadas en una tasa de exploración dinámica. En particular, para agentes en el conjunto PDn(Ai, QSj), πn+1 i1 (QSj) se determina de la misma manera que arriba, con la única diferencia de que dn+1 i es reemplazado por dn+1 i ∗ (1 − λn). El ancho de banda de búsqueda restante se utiliza para el aprendizaje asignando la probabilidad λn de manera uniforme a los agentes Ai2 en el conjunto DirectConn(Ai) - PDn(Ai, QSj). πn+1 i2 (QSj, αik ) = dn+1 i ∗ λn |DirectConn(Ai) - PDn(Ai, QSj)| (5) donde PDn(Ai, QSj) ⊂ DirectConn(Ai). Se debe tener en cuenta que la tasa de exploración λ no es constante y disminuye con el tiempo. El λ se determina de acuerdo con la siguiente ecuación: λn+1 = λ0 ∗ e−c1n (6) donde λ0 es la tasa de exploración inicial, que es una constante; c1 también es una constante para ajustar la tasa de disminución de la tasa de exploración; n es la unidad de tiempo actual. 3.1.2 Actualización de la Utilidad Esperada Una vez que la política de enrutamiento en el paso n+1, πn+1 i, se determina según la fórmula anterior, el agente Ai puede actualizar su propia utilidad esperada, Un+1 i (QSi), basándose en la política de enrutamiento actualizada resultante de la fórmula 5 y los valores U actualizados de sus agentes vecinos. Bajo el supuesto de que después de que una consulta se envía a los vecinos de Ai, las sesiones de búsqueda subsiguientes son independientes, la fórmula de actualización es similar a la fórmula de actualización de Bellman en Q-Learning: Un+1 i (QSj) = (1 − θi) ∗ Un i (QSj) + θi ∗ (Rn+1 i (QSj) + X k πn+1 i (QSj, αik )Un k (QSj)) (7) donde QSj = (Qj, ttl − 1) es el siguiente estado de QSj = (Qj, ttl); Rn+1 i (QSj) es la recompensa local esperada para la clase de consulta Qk en el agente Ai bajo la política de enrutamiento πn+1 i; θi es el coeficiente para decidir cuánto peso se le da al valor antiguo durante el proceso de actualización: cuanto menor sea el valor de θi, se espera que el agente aprenda más rápido el valor real, mientras que mayor sea la volatilidad del algoritmo, y viceversa. Rn+1 (s) se actualiza de acuerdo con la siguiente ecuación: 234 The Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Rn+1 i (QSj) = Rn i (QSj) +γi ∗ (r(QSj) − Rn i (QSj)) ∗ P(qj|Qj ) (8) donde r(QSj) es la recompensa local asociada con la sesión de búsqueda. P(qj|Qj) indica qué tan relevante es la consulta qj para el tipo de consulta Qj, y γi es la tasa de aprendizaje para el agente Ai. Dependiendo de la similitud entre una consulta específica qi y su tipo de consulta correspondiente Qi, la recompensa local asociada con la sesión de búsqueda tiene un impacto diferente en la estimación de Rn i (QSj). En la fórmula anterior, este impacto se refleja en el coeficiente, el valor P(qj|Qj). Función de recompensa Después de que una sesión de búsqueda se detiene cuando sus valores TTL expiran, todos los resultados de búsqueda se devuelven al usuario y se comparan con la evaluación de relevancia. Suponiendo que el conjunto de resultados de búsqueda es SR, la recompensa Rew(SR) se define como: Rew(SR) = j 1 si |Rel(SR)| > c |Rel(SR)| c en caso contrario, donde SR es el conjunto de resultados de búsqueda devueltos, Rel(SR) es el conjunto de documentos relevantes en los resultados de búsqueda. Esta ecuación especifica que los usuarios otorgan una recompensa de 1.0 si el número de documentos relevantes devueltos alcanza un número predefinido c. De lo contrario, la recompensa es proporcional al número de documentos relevantes devueltos. La razón para establecer un valor de corte de este tipo es que la importancia de la proporción de recuperación disminuye con la abundancia de documentos relevantes en el mundo real, por lo tanto, los usuarios tienden a enfocarse solo en un número limitado de resultados buscados. Los detalles del protocolo de enrutamiento actual se introducirán en la Sección 3.2 cuando presentemos cómo se implementa el algoritmo de aprendizaje en sistemas reales. 3.2 Implementación del algoritmo de aprendizaje. Esta sección describe cómo se puede utilizar el algoritmo de aprendizaje en un proceso de búsqueda de una o dos fases. En el algoritmo de búsqueda de una sola fase, las sesiones de búsqueda comienzan desde los iniciadores de las consultas. Por el contrario, en el algoritmo de búsqueda de dos pasos, el iniciador de la consulta primero intenta buscar un punto de inicio más apropiado para la consulta introduciendo un paso exploratorio como se describe en la Sección 2. A pesar de la diferencia en la calidad de los puntos de partida, la mayor parte del proceso de aprendizaje para los dos algoritmos es en gran medida el mismo como se describe en los párrafos siguientes. Antes de que comience el aprendizaje, cada agente inicializa el valor de utilidad esperado para todos los posibles estados como 0. Posteriormente, al recibir una consulta, además de las operaciones normales descritas en la sección anterior, un agente Ai también configura un temporizador para esperar los resultados de búsqueda devueltos por sus agentes subordinados. Una vez que el temporizador expire o haya recibido respuesta de todos sus agentes aguas abajo, Ai fusiona y reenvía los resultados de búsqueda acumulados de sus agentes aguas abajo a su agente aguas arriba. Configurar el temporizador acelera el aprendizaje porque los agentes pueden evitar esperar demasiado tiempo a que los agentes aguas abajo devuelvan los resultados de búsqueda. Ten en cuenta que estos resultados detallados y la información correspondiente del agente seguirán almacenados en Ai hasta que la información de retroalimentación sea transmitida desde su agente aguas arriba y se pueda evaluar el rendimiento de sus agentes aguas abajo. La duración del temporizador está relacionada con el valor TTL. En este documento, configuramos el temporizador a ttimer = ttli ∗ 2 + tf, donde ttli ∗ 2 es la suma del tiempo de viaje de las consultas en la red, y tf es el período de tiempo esperado que los usuarios desearían esperar. Los resultados de la búsqueda serán finalmente devueltos al iniciador de la sesión de búsqueda A0. Serán comparados con la evaluación de relevancia proporcionada por los usuarios finales (como se describe en la sección del experimento, la evaluación de relevancia para el conjunto de consultas se proporciona junto con las colecciones de datos). La recompensa será calculada y propagada hacia atrás a los agentes a lo largo del camino por el que se pasaron los resultados de la búsqueda. Este es un proceso inverso de la propagación de los resultados de búsqueda. En el proceso de propagar la recompensa hacia atrás, los agentes actualizan las estimaciones de su propio valor potencial de utilidad, generan una política actualizada y transmiten sus resultados actualizados a los agentes vecinos basándose en el algoritmo descrito en la Sección 3. Tras el cambio del valor de utilidad esperado, el agente Ai envía su estimación de utilidad actualizada a sus vecinos para que puedan actuar según la utilidad esperada modificada y el estado correspondiente. Este mensaje de actualización incluye la recompensa potencial, así como el estado correspondiente QSi = (qk, ttll) del agente Ai. Cada agente vecino, Aj, reacciona a este tipo de mensaje de actualización actualizando el valor de utilidad esperado para el estado QSj(qk, ttll + 1) de acuerdo con el valor de utilidad esperado cambiado recién anunciado. Una vez que completen la actualización, los agentes informarían nuevamente a los vecinos relacionados para que actualicen sus valores. Este proceso continúa hasta que el valor TTL en el mensaje de actualización aumenta al límite de TTL. Para acelerar el proceso de aprendizaje, al actualizar los valores de utilidad esperada de un agente Ai con agentes vecinos, especificamos que Um(Qk, ttl0) >= Um(Qk, ttl1) si ttl0 > ttl1. Por lo tanto, cuando el agente Ai recibe un valor de utilidad esperada actualizado con ttl1, también actualiza los valores de utilidad esperada con cualquier ttl0 > ttl1 si Um(Qk, ttl0) < Um(Qk, ttl1) para acelerar la convergencia. Esta heurística se basa en el hecho de que la utilidad de una sesión de búsqueda es una función no decreciente del tiempo t. 3.3 Discusión Al formalizar el sistema de enrutamiento de contenido como una tarea de aprendizaje, se hacen muchas suposiciones. En sistemas reales, estas suposiciones pueden no cumplirse, por lo que el algoritmo de aprendizaje puede no converger. Dos problemas son de particular importancia, (1) Este problema de enrutamiento de contenido no tiene propiedades de Markov. A diferencia del enrutamiento de paquetes basado en el nivel de IP, la decisión de enrutamiento de cada agente para una sesión de búsqueda particular sj depende del historial de enrutamiento de sj. Por lo tanto, la suposición de que todas las sesiones de búsqueda posteriores son independientes no se cumple en la realidad. Esto puede llevar a un problema de doble conteo en el que los documentos relevantes de algunos agentes se contarán más de una vez para el estado donde el valor TTL es mayor que 1. Sin embargo, en el contexto de las organizaciones de agentes jerárquicos, dos factores mitigan estos problemas: primero, los agentes en cada grupo de contenido forman una estructura similar a un árbol. Con la ausencia de los ciclos, las estimaciones dentro del árbol estarían cerca del valor preciso. En segundo lugar, la naturaleza estocástica de la política de enrutamiento remedia parcialmente este problema. El Sexto Internacional. Otro desafío para este algoritmo de aprendizaje es que en un entorno de red real, las observaciones sobre agentes vecinos pueden no poder actualizarse a tiempo debido al retraso en la comunicación u otras situaciones. Además, cuando los agentes vecinos actualizan sus estimaciones al mismo tiempo, puede surgir oscilación durante el proceso de aprendizaje[1]. Este documento explora varios enfoques para acelerar el proceso de aprendizaje. Además de la estrategia mencionada de actualizar los valores de utilidad esperada, también empleamos una estrategia de actualización activa donde los agentes notifican a sus vecinos cada vez que se actualiza su utilidad esperada. Por lo tanto, se puede lograr una velocidad de convergencia más rápida. Esta estrategia contrasta con la actualización perezosa, donde los agentes solo repiten a sus agentes vecinos con el cambio en la utilidad esperada cuando intercambian información. El compromiso entre los dos enfoques es la carga de red versus la velocidad de aprendizaje. La ventaja de este algoritmo de aprendizaje es que una vez que se aprende una política de enrutamiento, los agentes no tienen que comparar repetidamente la similitud de las consultas siempre y cuando la topología de la red permanezca sin cambios. En cambio, el agente solo tiene que determinar adecuadamente la clasificación de la consulta y seguir las políticas aprendidas. La desventaja de este enfoque basado en el aprendizaje es que el proceso de aprendizaje debe llevarse a cabo cada vez que la estructura de la red cambia. Hay muchas posibles extensiones para este modelo de aprendizaje. Por ejemplo, actualmente se utiliza una sola medida para indicar la carga de tráfico en un vecindario de agentes. Una extensión sencilla sería llevar un registro de la carga individual de cada vecino del agente. 4. CONFIGURACIÓN DE EXPERIMENTOS Y RESULTADOS Los experimentos se llevan a cabo en el kit de simulación TRANO con dos conjuntos de datos, TREC-VLC-921 y TREC123-100. Las siguientes subsecciones presentan el banco de pruebas TRANO, los conjuntos de datos y los resultados experimentales. 4.1 Banco de pruebas TRANO TRANO (Task Routing on Agent Network Organization) es un banco de pruebas de recuperación de información basado en red de agentes múltiples. TRANO se construye sobre el Farm [4], un simulador distribuido basado en el tiempo que proporciona un marco de diseminación de datos para organizaciones de redes de agentes distribuidos a gran escala. TRANO apoya la importación y exportación de perfiles de organizaciones de agentes, incluyendo conexiones topológicas y otras características. Cada agente de TRANO está compuesto por una estructura de vista de agente y una unidad de control. En la simulación, cada agente es pulsado regularmente y el agente verifica las colas de mensajes entrantes, realiza operaciones locales y luego reenvía mensajes a otros agentes. 4.2 Configuración Experimental En nuestro experimento, utilizamos dos conjuntos de datos estándar, los conjuntos de datos TRECVLC-921 y TREC-123-100, para simular las colecciones alojadas en los agentes. Los conjuntos de datos TREC-VLC-921 y TREC123-100 fueron creados por el Instituto Nacional de Tecnología Estándar de los Estados Unidos (NIST) para sus conferencias TREC. En el ámbito de la recuperación de información distribuida, las dos colecciones de datos se dividen en 921 y 100 subcolecciones. Se observa que el conjunto de datos TREC-VLC-921 es más heterogéneo que TREC-123-100 en términos de origen, longitud de documento y distribución de documentos relevantes a partir de las estadísticas de las dos colecciones de datos enumeradas en [13]. Por lo tanto, TREC-VLC-921 está mucho más cerca de las distribuciones reales de documentos en entornos P2P. Además, TREC-123-100 se divide en dos conjuntos de 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0 500 1000 1500 2000 2500 3000 Número de consulta de ARSS frente al número de consultas entrantes para TREC-VLC-921 SSLA-921 SSNA-921 Figura 3: ARSS (Recompensa promedio por sesión de búsqueda) frente al número de sesiones de búsqueda para la búsqueda de 1 fase en TREC-VLC-921 0 0.1 0.2 0.3 0.4 0.5 0.6 0 500 1000 1500 2000 2500 3000 Número de consulta de ARSS frente al número de consultas para TREC-VLC-921 TSLA-921 TSNA-921 Figura 4: ARSS (Recompensa promedio por sesión de búsqueda) frente al número de sesiones de búsqueda para la búsqueda de 2 fases en TREC-VLC-921 subcolecciones de dos formas: aleatoriamente y por fuente. Las dos particiones se denominan TREC-123-100-Random y TREC-123-100-Source respectivamente. Los documentos en cada subcolección en el conjunto de datos TREC-123-100-Source son más coherentes que los de TREC-123-100-Random. Los dos conjuntos diferentes de particiones nos permiten observar cómo el algoritmo de aprendizaje distribuido se ve afectado por la homogeneidad de las colecciones. La organización jerárquica de agentes es generada por el algoritmo descrito en nuestro algoritmo anterior [15]. Durante el proceso de generación de la topología, la información de grado de cada agente es estimada por el algoritmo introducido por Palmer et al. [9] con parámetros α = 0.5 y β = 0.6. En nuestros experimentos, estimamos el límite superior y el límite de grado descendente utilizando factores de descuento lineales de 0.5, 0.8 y 1.0. Una vez que se construye la topología, las consultas seleccionadas al azar del conjunto de consultas 301-350 en TREC-VLC-921 y del conjunto de consultas 1-50 en TREC-123-100-Random y TREC-123-100-Source se inyectan en el sistema basado en una distribución de Poisson P(N(t) = n) = (λt)n n! e−λ 236 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 0 50 100 150 200 250 300 350 400 0 500 1000 1500 2000 2500 3000 Utilidad acumulada Número de consulta Utilidad acumulada sobre el número de consultas entrantes TSLA-921 SSNA-921 SSLA-921 TSNA-921 Figura 5: La utilidad acumulada versus el número de sesiones de búsqueda TREC-VLC-921 Además, asumimos que todos los agentes tienen la misma probabilidad de recibir consultas del entorno, es decir, λ es el mismo para cada agente. En nuestros experimentos, λ se establece en 0.0543 para que la media de las consultas entrantes del entorno a la red de agentes sea de 50 por unidad de tiempo. El tiempo de servicio para la cola de comunicación y la cola de búsqueda local, es decir, tQij y trs, se establece en 0.01 unidades de tiempo y 0.05 unidades de tiempo respectivamente. En nuestros experimentos, hay diez tipos de consultas adquiridas mediante la agrupación del conjunto de consultas 301-350 y 1-50. 4.3 Análisis y evaluación de resultados. La Figura 3 muestra el ARSS (Recompensa Promedio por Sesión de Búsqueda) versus el número de consultas entrantes a lo largo del tiempo para el Algoritmo No Aprendido de un Solo Paso (SSNA) y el Algoritmo de Aprendizaje de un Solo Paso (SSLA) para la recolección de datos TREC-VLC-921. Se muestra que la recompensa promedio para el algoritmo SSNA oscila entre 0.02 y 0.06 y su rendimiento cambia poco con el tiempo. El promedio de recompensa para el enfoque SSLA comienza en el mismo nivel que el algoritmo SSNA. Pero el rendimiento aumenta con el tiempo y la ganancia promedio de rendimiento se estabiliza en aproximadamente un 25% después del rango de consulta de 2000 a 3000. La Figura 4 muestra el ARSS (Recompensa Promedio por Sesión de Búsqueda) versus el número de consultas entrantes a lo largo del tiempo para el Algoritmo No Aprendido de Dos Pasos (TSNA) y el Algoritmo de Aprendizaje de Dos Pasos (TSLA) para la recolección de datos TREC-VLC-921. El enfoque TSNA tiene un rendimiento relativamente consistente con una recompensa promedio que oscila entre 0.05 y 0.15. El promedio de recompensa para el enfoque de TSLA, donde se explota el algoritmo de aprendizaje, comienza en el mismo nivel que el algoritmo TSNA y mejora la recompensa promedio con el tiempo hasta que se unen al sistema entre 2000 y 2500 consultas. Los resultados muestran que la ganancia promedio de rendimiento para el enfoque TSLA sobre el enfoque TNLA es del 35% después de la estabilización. La Figura 5 muestra la utilidad acumulada versus el número de consultas entrantes a lo largo del tiempo para SSNA, SSLA, TSNA y TSLA respectivamente. Se ilustra que la utilidad acumulativa de los algoritmos no basados en aprendizaje aumenta principalmente de forma lineal con el tiempo, mientras que las ganancias de los algoritmos basados en aprendizaje se aceleran cuando más consultas ingresan al sistema. Estos resultados experimentales demuestran que los enfoques basados en el aprendizaje tienen un rendimiento consistentemente mejor que los algoritmos de enrutamiento no basados en el aprendizaje. Además, el algoritmo basado en aprendizaje de dos fases es mejor que el algoritmo basado en aprendizaje de una sola fase porque la recompensa máxima que un agente puede recibir al buscar en su vecindario dentro de TTL saltos está relacionada con el número total de documentos relevantes en esa área. Por lo tanto, incluso la política de enrutamiento óptima puede hacer poco más allá de alcanzar estos documentos relevantes más rápido. Por el contrario, el algoritmo de aprendizaje basado en dos pasos puede reubicar la sesión de búsqueda en un vecindario con documentos más relevantes. El TSLA combina los méritos de ambos enfoques y los supera. La Tabla 1 enumera la utilidad acumulativa para los conjuntos de datos TREC123-100-Random y TREC-123-100-Source con organizaciones jerárquicas. Las cinco columnas muestran los resultados de cuatro enfoques diferentes. En particular, la columna TSNA-Random muestra los resultados para el conjunto de datos TREC-123-100-Random con el enfoque TSNA. La columna TSLA-Random muestra los resultados para el conjunto de datos TREC-123-100-Random con el enfoque TSLA. Hay dos números en cada celda de la columna TSLA-Random. El primer número es la utilidad acumulada real, mientras que el segundo número es el porcentaje de ganancia en términos de utilidad sobre el enfoque TSNA. Las columnas TSNA-Source y TSLA-Source muestran los resultados para el conjunto de datos TREC-123-100-Source con los enfoques TSNA y TSLA respectivamente. La Tabla 1 muestra que la mejora en el rendimiento para TREC-123-100-Random no es tan significativa como en los otros conjuntos de datos. Esto se debe a que los documentos en la subcolección de TREC-123-100-Random son seleccionados al azar, lo que hace que el modelo de colección, la firma de la colección, sea menos significativo. Dado que ambos algoritmos están diseñados basados en la suposición de que las colecciones de documentos pueden ser bien representadas por su modelo de colección, este resultado no es sorprendente. En general, las Figuras 4, 5 y la Tabla 1 demuestran que el enfoque basado en aprendizaje por refuerzo puede mejorar considerablemente el rendimiento del sistema para ambas colecciones de datos. Sin embargo, queda como trabajo futuro descubrir la correlación entre la magnitud de las mejoras en el rendimiento y el tamaño de la colección de datos y/o la extensión de la heterogeneidad entre las subcolecciones. TRABAJO RELACIONADO El problema de enrutamiento de contenido difiere del enrutamiento a nivel de red en redes de comunicación conmutadas por paquetes en que el enrutamiento basado en contenido ocurre en redes a nivel de aplicación. Además, los agentes de destino en nuestros algoritmos de enrutamiento de contenido son múltiples y las direcciones no son conocidas en el proceso de enrutamiento. Los problemas de enrutamiento a nivel de IP han sido abordados desde la perspectiva del aprendizaje por refuerzo[2, 5, 11, 12]. Estos estudios han explorado algoritmos distribuidos completamente que son capaces, sin coordinación central, de difundir conocimiento sobre la red, encontrar de manera robusta y eficiente los caminos más cortos frente a cambios en las topologías de red y costos de enlace cambiantes. Hay dos clases principales de algoritmos de enrutamiento de paquetes adaptativos y distribuidos en la literatura: algoritmos de vector de distancia y algoritmos de estado de enlace. Si bien esta línea de estudios tiene cierta similitud con nuestro trabajo, se ha centrado principalmente en redes de comunicación conmutadas por paquetes. En este dominio, el destino de un paquete es determinístico y único. Cada agente mantiene estimaciones, probabilísticamente o determinísticamente, sobre la distancia a un destino específico a través de sus vecinos. Una variante de las técnicas de Q-Learning se implementa en The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Utilidad Acumulativa para los Conjuntos de Datos TREC-123-100-Random y TREC-123-100-Source con Organización Jerárquica; Los números porcentuales en las columnas TSLA-Random y TSLA-Source demuestran la ganancia de rendimiento sobre el algoritmo sin aprendizaje Número de consulta TSNA-Random TSLA-Random TSNA-Source TSLA-Source 500 25.15 28.45 13% 24.00 21.05 -13% 1000 104.99 126.74 20% 93.95 96.44 2.6% 1250 149.79 168.40 12% 122.64 134.05 9.3% 1500 188.94 211.05 12% 155.30 189.60 22% 1750 235.49 261.60 11% 189.14 243.90 28% 2000 275.09 319.25 16% 219.0 278.80 26% para actualizar las estimaciones y converger a las distancias reales. Se ha descubierto que la propiedad de localidad es una característica importante de los sistemas de recuperación de información en estudios de modelado de usuarios[3]. En los sistemas de intercambio de contenido basados en P2P, esta propiedad se ejemplifica por el fenómeno de que los usuarios tienden a enviar consultas que representan solo un número limitado de temas y, por el contrario, los usuarios en el mismo vecindario probablemente compartan intereses comunes y envíen consultas similares [10]. El enfoque basado en el aprendizaje se percibe como más beneficioso para los sistemas reales de recuperación de información distribuida que exhiben la propiedad de localidad. Esto se debe a que los patrones de tráfico y consultas de los usuarios pueden reducir el espacio de estados y acelerar el proceso de aprendizaje. El trabajo relacionado en aprovechar esta propiedad incluye [7], donde los autores intentaron abordar este problema mediante técnicas de modelado de usuario. 6. CONCLUSIONES En este artículo, se desarrolla un enfoque basado en aprendizaje por refuerzo para mejorar el rendimiento de algoritmos de búsqueda de IR distribuidos. En particular, los agentes mantienen estimaciones, es decir, utilidad esperada, sobre la capacidad de los agentes aguas abajo para proporcionar documentos relevantes para las consultas entrantes. Estas estimaciones se actualizan gradualmente al aprender de la información de retroalimentación devuelta de sesiones de búsqueda anteriores. Basándose en la información actualizada de utilidad esperada, los agentes modifican sus políticas de enrutamiento. Posteriormente, estos agentes dirigen las consultas basándose en las políticas aprendidas y actualizan las estimaciones sobre la utilidad esperada en función de las nuevas políticas de enrutamiento. Los experimentos en dos conjuntos de datos IR distribuidos diferentes ilustran que el enfoque de aprendizaje por refuerzo mejora considerablemente la utilidad acumulativa con el tiempo. 7. REFERENCIAS [1] S. Abdallah y V. Lesser. Aprendiendo el juego de asignación de tareas. En AAMAS 06: Actas de la quinta conferencia internacional conjunta sobre agentes autónomos y sistemas multiagente, páginas 850-857, Nueva York, NY, EE. UU., 2006. ACM Press. [2] J. \n\nACM Press. [2] J. A. Boyan y M. L. Littman. Enrutamiento de paquetes en redes de cambio dinámico: Un enfoque de aprendizaje por refuerzo. En Avances en Sistemas de Procesamiento de Información Neural, volumen 6, páginas 671-678. Morgan Kaufmann Publishers, Inc., 1994. [3] J. C. French, A. L. Powell, J. P. Callan, C. L. Viles, T. Emmitt, K. J. Prey, y Y. Mou. Comparando el rendimiento de los algoritmos de selección de bases de datos. En Investigación y Desarrollo en Recuperación de Información, páginas 238-245, 1999. [4] B. Horling, R. Mailler y V. Lesser. Granja: Un entorno escalable para el desarrollo y evaluación de múltiples agentes. En Avances en Ingeniería de Software para Sistemas Multiagente, páginas 220-237, Berlín, 2004. Springer-Verlag. [5] M. Littman y J. Boyan. Un esquema distribuido de aprendizaje por refuerzo para enrutamiento de redes. En Actas del Taller Internacional sobre Aplicaciones de Redes Neuronales a las Telecomunicaciones, 1993. [6] J. Lu y J. Callan. Búsqueda federada de bibliotecas digitales basadas en texto en redes jerárquicas de pares. En ECIR05, 2005. [7] J. Lu y J. Callan. Modelado de usuario para búsqueda federada de texto completo en redes peer-to-peer. En ACM SIGIR 2006. ACM Press, 2006. [8] C. D. Manning y H. Schütze. Fundamentos del Procesamiento del Lenguaje Natural Estadístico. El MIT Press, Cambridge, Massachusetts, 1999. [9] C. R. Palmer y J. G. Steffan. Generando topologías de red que obedezcan leyes de potencia. En Actas de GLOBECOM 2000, noviembre de 2000. [10] K. Sripanidkulchai, B. Maggs y H. Zhang. Localización eficiente de contenido utilizando la localidad basada en intereses en sistemas peer-to-peer. En INFOCOM, 2003. [11] D. Subramanian, P. Druschel y J. Chen. Hormigas y aprendizaje por refuerzo: Un estudio de caso en enrutamiento en redes dinámicas. En Actas de la Decimoquinta Conferencia Internacional Conjunta sobre Inteligencia Artificial, páginas 832-839, 1997. [12] J. N. Tao y L. Weaver. Un enfoque de múltiples agentes y gradiente de políticas para el enrutamiento de redes. En Actas de la Decimoctava Conferencia Internacional sobre Aprendizaje Automático, 2001. [13] H. Zhang, W. B. Croft, B. Levine y V. Lesser. Un enfoque multiagente para la recuperación de información entre pares. En Actas de la Tercera Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, julio de 2004. [14] H. Zhang y V. Lesser. Sistemas de recuperación de información peer-to-peer basados en múltiples agentes con sesiones de búsqueda concurrentes. En Actas de la Quinta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, mayo de 2006. [15] H. Zhang y V. R. Lesser. Una organización de agentes jerárquica formada dinámicamente para un sistema distribuido de intercambio de contenido. En la Conferencia Internacional IEEE/WIC/ACM sobre Tecnología de Agentes Inteligentes (IAT 2004) del 20 al 24 de septiembre de 2004 en Beijing, China, páginas 169-175. IEEE Computer Society, 2004. 238 La Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "routing decision": {
            "translated_key": "decisión de enrutamiento",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Reinforcement Learning based Distributed Search Algorithm For Hierarchical Peer-to-Peer Information Retrieval Systems Haizheng Zhang College of Information Science and Technology Pennsylvania State University University Park, PA 16803 hzhang@ist.psu.edu Victor Lesser Department of Computer Science University Of Massachusetts Amherst, MA 01003 lesser@cs.umass.edu ABSTRACT The dominant existing routing strategies employed in peerto-peer(P2P) based information retrieval(IR) systems are similarity-based approaches.",
                "In these approaches, agents depend on the content similarity between incoming queries and their direct neighboring agents to direct the distributed search sessions.",
                "However, such a heuristic is myopic in that the neighboring agents may not be connected to more relevant agents.",
                "In this paper, an online reinforcement-learning based approach is developed to take advantage of the dynamic run-time characteristics of P2P IR systems as represented by information about past search sessions.",
                "Specifically, agents maintain estimates on the downstream agents abilities to provide relevant documents for incoming queries.",
                "These estimates are updated gradually by learning from the feedback information returned from previous search sessions.",
                "Based on this information, the agents derive corresponding routing policies.",
                "Thereafter, these agents route the queries based on the learned policies and update the estimates based on the new routing policies.",
                "Experimental results demonstrate that the learning algorithm improves considerably the routing performance on two test collection sets that have been used in a variety of distributed IR studies.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Algorithms, Performance, Experimentation 1.",
                "INTRODUCTION Over the last few years there have been increasing interests in studying how to control the search processes in peer-to-peer(P2P) based information retrieval(IR) systems [6, 13, 14, 15].",
                "In this line of research, one of the core problems that concerns researchers is to efficiently route user queries in the network to agents that are in possession of appropriate documents.",
                "In the absence of global information, the dominant strategies in addressing this problem are content-similarity based approaches [6, 13, 14, 15].",
                "While the content similarity between queries and local nodes appears to be a creditable indicator for the number of relevant documents residing on each node, these approaches are limited by a number of factors.",
                "First of all, similaritybased metrics can be myopic since locally relevant nodes may not be connected to other relevant nodes.",
                "Second, the similarity-based approaches do not take into account the run-time characteristics of the P2P IR systems, including environmental parameters, bandwidth usage, and the historical information of the past search sessions, that provide valuable information for the query routing algorithms.",
                "In this paper, we develop a reinforcement learning based IR approach for improving the performance of distributed IR search algorithms.",
                "Agents can acquire better search strategies by collecting and analyzing feedback information from previous search sessions.",
                "Particularly, agents maintain estimates, namely expected utility, on the downstream agents capabilities of providing relevant documents for specific types of incoming queries.",
                "These estimates are updated gradually by learning from the feedback information returned from previous search sessions.",
                "Based on the updated expected utility information, the agents derive corresponding routing policies.",
                "Thereafter, these agents route the queries based on the learned policies and update the estimates on the expected utility based on the new routing policies.",
                "This process is conducted in an iterative manner.",
                "The goal of the learning algorithm, even though it consumes some network bandwidth, is to shorten the routing time so that more queries are processed per time unit while at the same time finding more relevant documents.",
                "This contrasts with the content-similarity based approaches where similar operations are repeated for every incoming query and the processing time keeps largely constant over time.",
                "Another way of viewing this paper is that our basic approach to distributed IR search is to construct a hierarchical overlay network(agent organization) based on the contentsimilarity measure among agents document collections in a bottom-up fashion.",
                "In the past work, we have shown that this organization improves search performance significantly.",
                "However, this organizational structure does not take into account the arrival patterns of queries, including their frequency, types, and where they enter the system, nor the available communication bandwidth of the network and processing capabilities of individual agents.",
                "The intention of the reinforcement learning is to adapt the agents routing decisions to the dynamic network situations and learn from past search sessions.",
                "Specifically, the contributions of this paper include: (1) a reinforcement learning based approach for agents to acquire satisfactory routing policies based on estimates of the potential contribution of their neighboring agents; (2) two strategies to speed up the learning process.",
                "To our best knowledge, this is one of the first reinforcement learning applications in addressing distributed content sharing problems and it is indicative of some of the issues in applying reinforcement in a complex application.",
                "The remainder of this paper is organized as follows: Section 2 reviews the hierarchical content sharing systems and the two-phase search algorithm based on such topology.",
                "Section 3 describes a reinforcement learning based approach to direct the routing process; Section 4 details the experimental settings and analyze the results.",
                "Section 5 discusses related studies and Section 6 concludes the paper. 2.",
                "SEARCH IN HIERARCHICAL P2P IR SYSTEMS This section briefly reviews our basic approaches to hierarchical P2P IR systems.",
                "In a hierarchical P2P IR system illustrated in Fig.1, agents are connected to each other through three types of links: upward links, downward links, and lateral links.",
                "In the following sections, we denote the set of agents that are directly connected to agent Ai as DirectConn(Ai), which is defined as DirectConn(Ai) = NEI(Ai) ∪ PAR(Ai) ∪ CHL(Ai) , where NEI(Ai) is the set of neighboring agents connected to Ai through lateral links; PAR(Ai) is the set of agents whom agent Ai is connected to through upward links and CHL(Ai) is the set of agents that agent Ai connects to through downward links.",
                "These links are established through a bottom-up content-similarity based distributed clustering process[15].",
                "These links are then used by agents to locate other agents that contain documents relevant to the given queries.",
                "A typical agent Ai in our system uses two queues: a local search queue, LSi, and a message forwarding queue MFi.",
                "The states of the two queues constitute the internal states of an agent.",
                "The local search queue LSi stores search sessions that are scheduled for local processing.",
                "It is a priority queue and agent Ai always selects the most promising queries to process in order to maximize the global utility.",
                "MFi consists of a set of queries to forward on and is processed in a FIFO (first in first out) fashion.",
                "For the first query in MFi, agent Ai determines which subset of its neighboring agents to forward it to based on the agents routing policy πi.",
                "These routing decisions determine how the search process is conducted in the network.",
                "In this paper, we call Ai as Ajs upstream agent and Aj as Ais downstream agent if A4 A5 A6 A7 A2 A3 A9 NEI(A2)={A3} PAR(A2)={A1} CHL(A2)={A4,A5} A1 A8 Figure 1: A fraction of a hierarchical P2PIR system an agent Ai routes a query to agent Aj.",
                "The distributed search protocol of our hierarchical agent organization is composed of two steps.",
                "In the first step, upon receipt of a query qk at time tl from a user, agent Ai initiates a search session si by probing its neighboring agents Aj ∈ NEI(Ai) with the message PROBE for the similarity value Sim(qk, Aj) between qk and Aj.",
                "Here, Ai is defined as the query initiator of search session si.",
                "In the second step, Ai selects a group of the most promising agents to start the actual search process with the message SEARCH.",
                "These SEARCH messages contain a TTL (Time To Live) parameter in addition to the query.",
                "The TTL value decreases by 1 after each hop.",
                "In the search process, agents discard those queries that either have been previously processed or whose TTL drops to 0, which prevents queries from looping in the system forever.",
                "The search session ends when all the agents that receive the query drop it or TTL decreases to 0.",
                "Upon receipt of SEARCH messages for qk, agents schedule local activities including local searching, forwarding qk to their neighbors, and returning search results to the query initiator.",
                "This process and related algorithms are detailed in [15, 14]. 3.",
                "A BASIC REINFORCEMENTLEARNING BASED SEARCH APPROACH In the aforementioned distributed search algorithm, the routing decisions of an agent Ai rely on the similarity comparison between incoming queries and Ais neighboring agents in order to forward those queries to relevant agents without flooding the network with unnecessary query messages.",
                "However, this heuristic is myopic because a relevant direct neighbor is not necessarily connected to other relevant agents.",
                "In this section, we propose a more general approach by framing this problem as a reinforcement learning task.",
                "In pursuit of greater flexibility, agents can switch between two modes: learning mode and non-learning mode.",
                "In the non-learning mode, agents operate in the same way as they do in the normal distributed search processes described in [14, 15].",
                "On the other hand, in the learning mode, in parallel with distributed search sessions, agents also participate in a learning process which will be detailed in this section.",
                "Note that in the learning protocol, the learning process does not interfere with the distributed search process.",
                "Agents can choose to initiate and stop learning processes without affecting the system performance.",
                "In particular, since the learning process consumes network resources (especially bandwidth), agents can choose to initiate learning only when the network load is relatively low, thus minimizing the extra communication costs incurred by the learning algorithm.",
                "The section is structured as follows, Section 3.1 describes 232 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) a reinforcement learning based model.",
                "Section 3.2 describes a protocol to deploy the learning algorithm in the network.",
                "Section 3.3 discusses the convergence of the learning algorithm. 3.1 The Model An agents routing policy takes the state of a search session as input and output the routing actions for that query.",
                "In our work, the state of a search session sj is stipulated as: QSj = (qk, ttlj) where ttlj is the number of hops that remains for the search session sj , qk is the specific query.",
                "QL is an attribute of qk that indicates which type of queries qk most likely belong to.",
                "The set of QL can be generated by running a simple online classification algorithm on all the queries that have been processed by the agents, or an oﬄine algorithm on a pre-designated training set.",
                "The assumption here is that the set of query types is learned ahead of time and belongs to the common knowledge of the agents in the network.",
                "Future work includes exploring how learning can be accomplished when this assumption does not hold.",
                "Given the query types set, an incoming query qi can be classified to one query class Q(qi) by the formula: Q(qi) = arg max Qj P(qi|Qj) (1) where P(qi|Qj ) indicates the likelihood that the query qi is generated by the query class Qj [8].",
                "The set of atomic routing actions of an agent Ai is denoted as {αi}, where {αi} is defined as αi = {αi0 , αi1 , ..., αin }.",
                "An element αij represents an action to route a given query to the neighboring agent Aij ∈ DirectConn(Ai).",
                "The routing policy πi of agent Ai is stochastic and its outcome for a search session with state QSj is defined as: πi(QSj) = {(αi0 , πi(QSi, αi0 )), (αi1 , πi(QSi, αi1 )), ...} (2) Note that operator πi is overloaded to represent either the probabilistic policy for a search session with state QSj, denoted as πi(QSj); or the probability of forwarding the query to a specific neighboring agent Aik ∈ DirectConn(Ai) under the policy πi(QSj), denoted as πi(QSj, αik ).",
                "Therefore, equation (2) means that the probability of forwarding the search session to agent Ai0 is πi(QSi, αi0 ) and so on.",
                "Under this stochastic policy, the routing action is nondeterministic.",
                "The advantage of such a strategy is that the best neighboring agents will not be selected repeatedly, thereby mitigating the potential hot spots situations.",
                "The expected utility, Un i (QSj), is used to estimate the potential utility gain of routing query type QSj to agent Ai under policy πn i .",
                "The superscript n indicates the value at the nth iteration in an iterative learning process.",
                "The expected utility provides routing guidance for future search sessions.",
                "In the search process, each agent Ai maintains partial observations of its neighbors states, as shown in Fig. 2.",
                "The partial observation includes non-local information such as the potential utility estimation of its neighbor Am for query state QSj, denoted as Um(QSj), as well as the load information, Lm.",
                "These observations are updated periodically by the neighbors.",
                "The estimated utility information will be used to update Ais expected utility for its routing policy.",
                "Load Information Expected Utility For Different Query Types Neighboring Agents ... A0 A1 A3 A2 Un 0 (QS0) ... ... ... ... ...... Un 0 (QS1) Un 1 (QS1) Un 2 (QS1) Un 3 (QS1) Un 1 (QS0) Un 2 (QS0) Un 3 (QS0) Ln 0 Ln 1 Ln 2 Ln 3 ... QS0 QS1 ...",
                "Figure 2: Agent Ais Partial Observation about its neighbors(A0, A1...) The load information of Am, Lm, is defined as Lm = |MFm| Cm , where |MFm| is the length of the message-forward queue and Cm is the service rate of agent Ams message-forward queue.",
                "Therefore Lm characterizes the utilization of an agents communication channel, and thus provide non-local information for Ams neighbors to adjust the parameters of their routing policy to avoid inundating their downstream agents.",
                "Note that based on the characteristics of the queries entering the system and agents capabilities, the loading of agents may not be uniform.",
                "After collecting the utilization rate information from all its neighbors, agent Ai computes Li as a single measure for assessing the average load condition of its neighborhood: Li = P k Lk |DirectConn(Ai)| Agents exploit Li value in determining the routing probability in its routing policy.",
                "Note that, as described in Section 3.2, information about neighboring agents is piggybacked with the query message propagated among the agents whenever possible to reduce the traffic overhead. 3.1.1 Update the Policy An iterative update process is introduced for agents to learn a satisfactory stochastic routing policy.",
                "In this iterative process, agents update their estimates on the potential utility of their current routing policies and then propagate the updated estimates to their neighbors.",
                "Their neighbors then generate a new routing policy based on the updated observation and in turn they calculate the expected utility based on the new policies and continue this iterative process.",
                "In particular, at time n, given a set of expected utility, an agent Ai, whose directly connected agents set is DirectConn(Ai) = {Ai0 , ..., Aim }, determines its corresponding stochastic routing policy for a search session of state QSj based on the following steps: (1) Ai first selects a subset of agents as the potential downstream agents from set DirectConn(Ai), denoted as PDn(Ai, QSj).",
                "The size of the potential downstream agent is specified as |PDn(Ai, QSj)| = min(|NEI(Ai), dn i + k)| where k is a constant and is set to 3 in this paper; dn i , the forward width, is defined as the expected number of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 233 neighboring agents that agent Ai can forward to at time n. This formula specifies that the potential downstream agent set PDn(Ai, QSj) is either the subset of neighboring agents with dn i + k highest expected utility value for state QSj among all the agents in DirectConn(Ai), or all their neighboring agents.",
                "The k is introduced based on the idea of a stochastic routing policy and it makes the forwarding probability of the dn i +k highest agent less than 100%.",
                "Note that if we want to limit the number of downstream agents for search session sj as 5, the probability of forwarding the query to all neighboring agents should add up to 5.",
                "Setting up dn i value properly can improve the utilization rate of the network bandwidth when much of the network is idle while mitigating the traffic load when the network is highly loaded.",
                "The dn+1 i value is updated based on dn i , the previous and current observations on the traffic situation in the neighborhood.",
                "Specifically, the update formula for dn+1 i is dn+1 i = dn i ∗ (1 + 1 − Li |DirectConn(Ai)| ) In this formula, the forward width is updated based on the traffic conditions of agent Ais neighborhood, i.e Li, and its previous value. (2) For each agent Aik in the PDn(Ai, QSj), the probability of forwarding the query to Aik is determined in the following way in order to assign higher forwarding probability to the neighboring agents with higher expected utility value: πn+1 i (QSj, αik ) = dn+1 i |PDn(Ai, QSj)| + β ∗ ` Uik (QSj) − PDU(Ai, QSj) |PDn(Ai, QSj)| ´ (3) where PDUn(Ai, QSj) = X o∈P Dn(Ai,QSj ) Uo(QSj) and QSj is the subsequent state of agent Aik after agent Ai forwards the search session with state QSj to its neighboring agent Aik ; If QSj = (qk, ttl0), then QSj = (qk, ttl0 − 1).",
                "In formula 3, the first term on the right of the equation, dn+1 i |P Dn(Ai,QSj )| , is used to to determine the forwarding probability by equally distributing the forward width, dn+1 i , to the agents in PDn(Ai, QSj) set.",
                "The second term is used to adjust the probability of being chosen so that agents with higher expected utility values will be favored. β is determined according to: β = min ` m − dn+1 i m ∗ umax − PDUn(Ai, QSj) , dn+1 i PDUn(Ai, QSj) − m ∗ umin ´ (4) where m = |PDn(Ai, QSj)|, umax = max o∈P Dn(Ao,QSj ) Uo(QSj) and umin = min o∈P Dn(Ao,QSj ) Uo(QSj) This formula guarantees that the final πn+1 i (QSj, αik ) value is well defined, i.e, 0 ≤ πn+1 i (QSj, αik ) ≤ 1 and X i πn+1 i (QSj, αik ) = dn+1 i However, such a solution does not explore all the possibilities.",
                "In order to balance between exploitation and exploration, a λ-Greedy approach is taken.",
                "In the λ-Greedy approach, in addition to assigning higher probability to those agents with higher expected utility value, as in the equation (3).",
                "Agents that appear to be not-so-good choices will also be sent queries based on a dynamic exploration rate.",
                "In particular, for agents in the set PDn(Ai, QSj), πn+1 i1 (QSj) is determined in the same way as the above, with the only difference being that dn+1 i is replaced with dn+1 i ∗ (1 − λn).",
                "The remaining search bandwidth is used for learning by assigning probability λn evenly to agents Ai2 in the set DirectConn(Ai) − PDn(Ai, QSj). πn+1 i2 (QSj, αik ) = dn+1 i ∗ λn |DirectConn(Ai) − PDn(Ai, QSj)| (5) where PDn(Ai, QSj) ⊂ DirectConn(Ai).",
                "Note that the exploration rate λ is not a constant and it decreases overtime.",
                "The λ is determined according to the following equation: λn+1 = λ0 ∗ e−c1n (6) where λ0 is the initial exploration rate, which is a constant; c1 is also a constant to adjust the decreasing rate of the exploration rate; n is the current time unit. 3.1.2 Update Expected Utility Once the routing policy at step n+1, πn+1 i , is determined based on the above formula, agent Ai can update its own expected utility, Un+1 i (QSi), based on the the updated routing policy resulted from the formula 5 and the updated U values of its neighboring agents.",
                "Under the assumption that after a query is forwarded to Ais neighbors the subsequent search sessions are independent, the update formula is similar to the Bellman update formula in Q-Learning: Un+1 i (QSj) = (1 − θi) ∗ Un i (QSj) + θi ∗ (Rn+1 i (QSj) + X k πn+1 i (QSj, αik )Un k (QSj)) (7) where QSj = (Qj, ttl − 1) is the next state of QSj = (Qj, ttl); Rn+1 i (QSj) is the expected local reward for query class Qk at agent Ai under the routing policy πn+1 i ; θi is the coefficient for deciding how much weight is given to the old value during the update process: the smaller θi value is, the faster the agent is expected to learn the real value, while the greater volatility of the algorithm, and vice versa.",
                "Rn+1 (s) is updated according to the following equation: 234 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Rn+1 i (QSj) = Rn i (QSj) +γi ∗ (r(QSj) − Rn i (QSj)) ∗ P(qj|Qj ) (8) where r(QSj) is the local reward associated with the search session.",
                "P(qj|Qj ) indicates how relevant the query qj is to the query type Qj, and γi is the learning rate for agent Ai.",
                "Depending on the similarity between a specific query qi and its corresponding query type Qi, the local reward associated with the search session has different impact on the Rn i (QSj) estimation.",
                "In the above formula, this impact is reflected by the coefficient, the P(qj|Qj) value. 3.1.3 Reward function After a search session stops when its TTL values expires, all search results are returned back to the user and are compared against the relevance judgment.",
                "Assuming the set of search results is SR, the reward Rew(SR) is defined as: Rew(SR) = j 1 if |Rel(SR)| > c |Rel(SR)| c otherwise. where SR is the set of returned search results, Rel(SR) is the set of relevant documents in the search results.",
                "This equation specifies that users give 1.0 reward if the number of returned relevant documents reaches a predefined number c. Otherwise, the reward is in proportion to the number of relevant documents returned.",
                "This rationale for setting up such a cut-off value is that the importance of recall ratio decreases with the abundance of relevant documents in real world, therefore users tend to focus on only a limited number of searched results.",
                "The details of the actual routing protocol will be introduced in Section 3.2 when we introduce how the learning algorithm is deployed in real systems. 3.2 Deployment of the Learning algorithm This section describes how the learning algorithm can be used in either a single-phase or a two-phase search process.",
                "In the single-phase search algorithm, search sessions start from the initiators of the queries.",
                "In contrast, in the two-step search algorithm, the query initiator first attempts to seek a more appropriate starting point for the query by introducing an exploratory step as described in Section 2.",
                "Despite the difference in the quality of starting points, the major part of the learning process for the two algorithms is largely the same as described in the following paragraphs.",
                "Before learning starts, each agent initializes the expected utility value for all possible states as 0.",
                "Thereafter, upon receipt of a query, in addition to the normal operations described in the previous section, an agent Ai also sets up a timer to wait for the search results returned from its downstream agents.",
                "Once the timer expires or it has received response from all its downstream agents, Ai merges and forwards the search results accrued from its downstream agents to its upstream agent.",
                "Setting up the timer speeds up the learning because agents can avoid waiting too long for the downstream agents to return search results.",
                "Note that these detailed results and corresponding agent information will still be stored at Ai until the feedback information is passed from its upstream agent and the performance of its downstream agents can be evaluated.",
                "The duration of the timer is related to the TTL value.",
                "In this paper, we set the timer to ttimer = ttli ∗ 2 + tf , where ttli ∗ 2 is the sum of the travel time of the queries in the network, and tf is the expected time period that users would like to wait.",
                "The search results will eventually be returned to the search session initiator A0.",
                "They will be compared to the relevance judgment that is provided by the final users (as described in the experiment section, the relevance judgement for the query set is provided along with the data collections).",
                "The reward will be calculated and propagated backward to the agents along the way that search results were passed.",
                "This is a reverse process of the search results propagation.",
                "In the process of propagating reward backward, agents update estimates of their own potential utility value, generate an upto-dated policy and pass their updated results to the neighboring agents based on the algorithm described in Section 3.",
                "Upon change of expected utility value, agent Ai sends out its updated utility estimation to its neighbors so that they can act upon the changed expected utility and corresponding state.",
                "This update message includes the potential reward as well as the corresponding state QSi = (qk, ttll) of agent Ai.",
                "Each neighboring agent, Aj, reacts to this kind of update message by updating the expected utility value for state QSj(qk, ttll + 1) according to the newly-announced changed expected utility value.",
                "Once they complete the update, the agents would again in turn inform related neighbors to update their values.",
                "This process goes on until the TTL value in the update message increases to the TTL limit.",
                "To speed up the learning process, while updating the expected utility values of an agent Ais neighboring agents we specify that Um(Qk, ttl0) >= Um(Qk, ttl1) iff ttl0 > ttl1 Thus, when agent Ai receives an updated expected utility value with ttl1, it also updates the expected utility values with any ttl0 > ttl1 if Um(Qk, ttl0) < Um(Qk, ttl1) to speed up convergence.",
                "This heuristic is based on the fact that the utility of a search session is a non-decreasing function of time t. 3.3 Discussion In formalizing the content routing system as a learning task, many assumptions are made.",
                "In real systems, these assumptions may not hold, and thus the learning algorithm may not converge.",
                "Two problems are of particular note, (1) This content routing problem does not have Markov properties.",
                "In contrast to IP-level based packet routing, the <br>routing decision</br> of each agent for a particular search session sj depends on the routing history of sj.",
                "Therefore, the assumption that all subsequent search sessions are independent does not hold in reality.",
                "This may lead to double counting problem that the relevant documents of some agents will be counted more than once for the state where the TTL value is more than 1.",
                "However, in the context of the hierarchical agent organizations, two factors mitigate this problems: first, the agents in each content group form a tree-like structure.",
                "With the absense of the cycles, the estimates inside the tree would be close to the accurate value.",
                "Secondly, the stochastic nature of the routing policy partly remedies this problem.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 235 (2) Another challenge for this learning algorithm is that in a real network environment observations on neighboring agents may not be able to be updated in time due to the communication delay or other situations.",
                "In addition, when neighboring agents update their estimates at the same time, oscillation may arise during the learning process[1].",
                "This paper explores several approaches to speed up the learning process.",
                "Besides the aforementioned strategy of updating the expected utility values, we also employ an active update strategy where agents notify their neighbors whenever its expected utility is updated.",
                "Thus a faster convergence speed can be achieved.",
                "This strategy contrasts to the Lazy update, where agents only echo their neighboring agents with their expected utility change when they exchange information.",
                "The trade off between the two approaches is the network load versus learning speed.",
                "The advantage of this learning algorithm is that once a routing policy is learned, agents do not have to repeatedly compare the similarity of queries as long as the network topology remains unchanged.",
                "Instead, agent just have to determine the classification of the query properly and follow the learned policies.",
                "The disadvantage of this learning-based approach is that the learning process needs to be conducted whenever the network structure changes.",
                "There are many potential extensions for this learning model.",
                "For example, a single measure is currently used to indicate the traffic load for an agents neighborhood.",
                "A simple extension would be to keep track of individual load for each neighbor of the agent. 4.",
                "EXPERIMENTSSETTINGSAND RESULTS The experiments are conducted on TRANO simulation toolkit with two sets of datasets, TREC-VLC-921 and TREC123-100.",
                "The following sub-sections introduce the TRANO testbed, the datasets, and the experimental results. 4.1 TRANO Testbed TRANO (Task Routing on Agent Network Organization) is a multi-agent based network based information retrieval testbed.",
                "TRANO is built on top of the Farm [4], a time based distributed simulator that provides a data dissemination framework for large scale distributed agent network based organizations.",
                "TRANO supports importation and exportation of agent organization profiles including topological connections and other features.",
                "Each TRANO agent is composed of an agent view structure and a control unit.",
                "In simulation, each agent is pulsed regularly and the agent checks the incoming message queues, performs local operations and then forwards messages to other agents . 4.2 Experimental Settings In our experiment, we use two standard datasets, TRECVLC-921 and TREC-123-100 datasets, to simulate the collections hosted on agents.",
                "The TREC-VLC-921 and TREC123-100 datasets were created by the U.S. National Institute for Standard Technology(NIST) for its TREC conferences.",
                "In distributed information retrieval domain, the two data collections are split to 921 and 100 sub-collections.",
                "It is observed that dataset TREC-VLC-921 is more heterogeneous than TREC-123-100 in terms of source, document length, and relevant document distribution from the statistics of the two data collections listed in [13].",
                "Hence, TREC-VLC-921 is much closer to real document distributions in P2P environments.",
                "Furthermore, TREC-123-100 is split into two sets of 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0 500 1000 1500 2000 2500 3000 ARSS Query number ARSS versus the number of incoming queries for TREC-VLC-921 SSLA-921 SSNA-921 Figure 3: ARSS(Average reward per search session) versus the number of search sessions for 1phase search in TREC-VLC-921 0 0.1 0.2 0.3 0.4 0.5 0.6 0 500 1000 1500 2000 2500 3000 ARSS Query number ARSS versus query number for TREC-VLC-921 TSLA-921 TSNA-921 Figure 4: ARSS(Average reward per search session) versus the number of search sessions for 2phase search in TREC-VLC-921 sub-collections in two ways: randomly and by source.",
                "The two partitions are denoted as TREC-123-100-Random and TREC-123-100-Source respectively.",
                "The documents in each subcollection in dataset TREC-123-100-Source are more coherent than those in TREC-123-100-Random.",
                "The two different sets of partitions allow us to observe how the distributed learning algorithm is affected by the homogeneity of the collections.",
                "The hierarchical agent organization is generated by the algorithm described in our previous algorithm [15].",
                "During the topology generation process, degree information of each agent is estimated by the algorithm introduced by Palmer et al. [9] with parameters α = 0.5 and β = 0.6.",
                "In our experiments, we estimate the upward limit and downward degree limit using linear discount factors 0.5, 0.8 and 1.0.",
                "Once the topology is built, queries randomly selected from the query set 301−350 on TREC-VLC-921 and query set 1− 50 on TREC-123-100-Random and TREC-123-100-Source are injected to the system based on a Poisson distribution P(N(t) = n) = (λt)n n! e−λ 236 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 0 50 100 150 200 250 300 350 400 0 500 1000 1500 2000 2500 3000 Cumulativeutility Query number Cumulative utility over the number of incoming queries TSLA-921 SSNA-921 SSLA-921 TSNA-921 Figure 5: The cumulative utility versus the number of search sessions TREC-VLC-921 In addition, we assume that all agents have an equal chance of getting queries from the environment, i.e, λ is the same for every agent.",
                "In our experiments, λ is set as 0.0543 so that the mean of the incoming queries from the environment to the agent network is 50 per time unit.",
                "The service time for the communication queue and local search queue, i.e tQij and trs, is set as 0.01 time unit and 0.05 time units respectively.",
                "In our experiments, there are ten types of queries acquired by clustering the query set 301 − 350 and 1 − 50. 4.3 Results analysis and evaluation Figure 3 demonstrates the ARSS(Average Reward per Search Session) versus the number of incoming queries over time for the the Single-Step based Non-learning Algorithm (SSNA), and the Single-Step Learning Algorithm(SSLA) for data collection TREC-VLC-921.",
                "It shows that the average reward for SSNA algorithm ranges from 0.02 − 0.06 and the performance changes little over time.",
                "The average reward for SSLA approach starts at the same level with the SSNA algorithm.",
                "But the performance increases over time and the average performance gain stabilizes at about 25% after query range 2000 − 3000.",
                "Figure 4 shows the ARSS(Average Reward per Search Session) versus the number of incoming queries over time for the the Two-Step based Non-learning Algorithm(TSNA), and the Two-Step Learning Algorithm(TSLA) for data collection TREC-VLC-921.",
                "The TSNA approach has a relatively consistent performance with the average reward ranges from 0.05 − 0.15.",
                "The average reward for TSLA approach, where learning algorithm is exploited, starts at the same level with the TSNA algorithm and improves the average reward over time until 2000−2500 queries joining the system.",
                "The results show that the average performance gain for TSLA approach over TNLA approach is 35% after stabilization.",
                "Figure 5 shows the cumulative utility versus the number of incoming queries over time for SSNA, SSLA,TSNA, and TSLA respectively.",
                "It illustrates that the cumulative utility of non-learning algorithms increases largely linearly over time, while the gains of learning-based algorithms accelerate when more queries enter the system.",
                "These experimental results demonstrate that learning-based approaches consistently perform better than non-learning based routing algorithm.",
                "Moreover, two-phase learning based algorithm is better than single-phase based learning algorithm because the maximal reward an agent can receive from searching its neighborhood within TTL hops is related to the total number of the relevant documents in that area.",
                "Thus, even the optimal routing policy can do little beyond reaching these relevant documents faster.",
                "On the contrary, the two-stepbased learning algorithm can relocate the search session to a neighborhood with more relevant documents.",
                "The TSLA combines the merits of both approaches and outperforms them.",
                "Table 1 lists the cumulative utility for datasets TREC123-100-Random and TREC-123-100-Source with hierarchical organizations.",
                "The five columns show the results for four different approaches.",
                "In particular, column TSNA-Random shows the results for dataset TREC-123-100-Random with the TSNA approach.",
                "The column TSLA-Random shows the results for dataset TREC-123-100-Random with the TSLA approach.",
                "There are two numbers in each cell in the column TSLA-Random.",
                "The first number is the actual cumulative utility while the second number is the percentage gain in terms of the utility over TSNA approach.",
                "Columns TSNA-Source and TSLA-Source show the results for dataset TREC-123-100-Source with TSNA and TSLA approaches respectively.",
                "Table 1 shows that the performance improvement for TREC-123-100-Random is not as significant as the other datasets.",
                "This is because that the documents in the sub-collection of TREC-123-100-Random are selected randomly which makes the collection model, the signature of the collection, less meaningful.",
                "Since both algorithms are designed based on the assumption that document collections can be well represented by their collection model, this result is not surprising.",
                "Overall, Figures 4, 5, and Table 1 demonstrate that the reinforcement learning based approach can considerably enhance the system performance for both data collections.",
                "However, it remains as future work to discover the correlation between the magnitude of the performance gains and the size of the data collection and/or the extent of the heterogeneity between the sub-collections. 5.",
                "RELATED WORK The content routing problem differs from the networklevel routing in packet-switched communication networks in that content-based routing occurs in application-level networks.",
                "In addition, the destination agents in our contentrouting algorithms are multiple and the addresses are not known in the routing process.",
                "IP-level Routing problems have been attacked from the reinforcement learning perspective[2, 5, 11, 12].",
                "These studies have explored fully distributed algorithms that are able, without central coordination to disseminate knowledge about the network, to find the shortest paths robustly and efficiently in the face of changing network topologies and changing link costs.",
                "There are two major classes of adaptive, distributed packet routing algorithms in the literature: distance-vector algorithms and link-state algorithms.",
                "While this line of studies carry a certain similarity with our work, it has mainly focused on packet-switched communication networks.",
                "In this domain, the destination of a packet is deterministic and unique.",
                "Each agent maintains estimations, probabilistically or deterministically, on the distance to a certain destination through its neighbors.",
                "A variant of Q-Learning techniques is deployed The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 237 Table 1: Cumulative Utility for Datasets TREC-123-100-Random and TREC-123-100-Source with Hierarchical Organization; The percentage numbers in the columns TSLA-Random and TSLA-Source demonstrate the performance gain over the algorithm without learning Query number TSNA-Random TSLA-Random TSNA-Source TSLA-Source 500 25.15 28.45 13% 24.00 21.05 -13% 1000 104.99 126.74 20% 93.95 96.44 2.6% 1250 149.79 168.40 12% 122.64 134.05 9.3% 1500 188.94 211.05 12% 155.30 189.60 22% 1750 235.49 261.60 11% 189.14 243.90 28% 2000 275.09 319.25 16% 219.0 278.80 26% to update the estimations to converge to the real distances.",
                "It has been discovered that the locality property is an important feature of information retrieval systems in user modeling studies[3].",
                "In P2P based content sharing systems, this property is exemplified by the phenomenon that users tend to send queries that represent only a limited number of topics and conversely, users in the same neighborhood are likely to share common interests and send similar queries [10].",
                "The learning based approach is perceived to be more beneficial for real distributed information retrieval systems which exhibit locality property.",
                "This is because the users traffic and query patterns can reduce the state space and speed up the learning process.",
                "Related work in taking advantage of this property include [7], where the authors attempted to address this problem by user modeling techniques. 6.",
                "CONCLUSIONS In this paper, a reinforcement-learning based approach is developed to improve the performance of distributed IR search algorithms.",
                "Particularly, agents maintain estimates, namely expected utility, on the downstream agents ability to provide relevant documents for incoming queries.",
                "These estimates are updated gradually by learning from the feedback information returned from previous search sessions.",
                "Based on the updated expected utility information, the agents modify their routing policies.",
                "Thereafter, these agents route the queries based on the learned policies and update the estimates on the expected utility based on the new routing policies.",
                "The experiments on two different distributed IR datasets illustrates that the reinforcement learning approach improves considerably the cumulative utility over time. 7.",
                "REFERENCES [1] S. Abdallah and V. Lesser.",
                "Learning the task allocation game.",
                "In AAMAS 06: Proceedings of the fifth international joint conference on Autonomous agents and multiagent systems, pages 850-857, New York, NY, USA, 2006.",
                "ACM Press. [2] J.",
                "A. Boyan and M. L. Littman.",
                "Packet routing in dynamically changing networks: A reinforcement learning approach.",
                "In Advances in Neural Information Processing Systems, volume 6, pages 671-678.",
                "Morgan Kaufmann Publishers, Inc., 1994. [3] J. C. French, A. L. Powell, J. P. Callan, C. L. Viles, T. Emmitt, K. J. Prey, and Y. Mou.",
                "Comparing the performance of database selection algorithms.",
                "In Research and Development in Information Retrieval, pages 238-245, 1999. [4] B. Horling, R. Mailler, and V. Lesser.",
                "Farm: A scalable environment for multi-agent development and evaluation.",
                "In Advances in Software Engineering for Multi-Agent Systems, pages 220-237, Berlin, 2004.",
                "Springer-Verlag. [5] M. Littman and J. Boyan.",
                "A distributed reinforcement learning scheme for network routing.",
                "In Proceedings of the International Workshop on Applications of Neural Networks to Telecommunications, 1993. [6] J. Lu and J. Callan.",
                "Federated search of text-based digital libraries in hierarchical peer-to-peer networks.",
                "In In ECIR05, 2005. [7] J. Lu and J. Callan.",
                "User modeling for full-text federated search in peer-to-peer networks.",
                "In ACM SIGIR 2006.",
                "ACM Press, 2006. [8] C. D. Manning and H. Sch¨utze.",
                "Foundations of Statistical Natural Language Processing.",
                "The MIT Press, Cambridge, Massachusetts, 1999. [9] C. R. Palmer and J. G. Steffan.",
                "Generating network topologies that obey power laws.",
                "In Proceedings of GLOBECOM 2000, November 2000. [10] K. Sripanidkulchai, B. Maggs, and H. Zhang.",
                "Efficient content location using interest-based locality in peer-topeer systems.",
                "In INFOCOM, 2003. [11] D. Subramanian, P. Druschel, and J. Chen.",
                "Ants and reinforcement learning: A case study in routing in dynamic networks.",
                "In In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence, pages 832-839, 1997. [12] J. N. Tao and L. Weaver.",
                "A multi-agent, policy gradient approach to network routing.",
                "In In Proceedings of the Eighteenth International Conference on Machine Learning, 2001. [13] H. Zhang, W. B. Croft, B. Levine, and V. Lesser.",
                "A multi-agent approach for peer-to-peer information retrieval.",
                "In Proceedings of Third International Joint Conference on Autonomous Agents and Multi-Agent Systems, July 2004. [14] H. Zhang and V. Lesser.",
                "Multi-agent based peer-to-peer information retrieval systems with concurrent search sessions.",
                "In Proceedings of the Fifth International Joint Conference on Autonomous Agents and Multi-Agent Systems, May 2006. [15] H. Zhang and V. R. Lesser.",
                "A dynamically formed hierarchical agent organization for a distributed content sharing system.",
                "In 2004 IEEE/WIC/ACM International Conference on Intelligent Agent Technology (IAT 2004), 20-24 September 2004, Beijing, China, pages 169-175.",
                "IEEE Computer Society, 2004. 238 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "In contrast to IP-level based packet routing, the <br>routing decision</br> of each agent for a particular search session sj depends on the routing history of sj."
            ],
            "translated_annotated_samples": [
                "A diferencia del enrutamiento de paquetes basado en el nivel de IP, la <br>decisión de enrutamiento</br> de cada agente para una sesión de búsqueda particular sj depende del historial de enrutamiento de sj."
            ],
            "translated_text": "Un algoritmo de búsqueda distribuida basado en aprendizaje por refuerzo para sistemas jerárquicos de recuperación de información entre pares. En estos enfoques, los agentes dependen de la similitud de contenido entre las consultas entrantes y sus agentes vecinos directos para dirigir las sesiones de búsqueda distribuida. Sin embargo, tal heurística es miope en el sentido de que los agentes vecinos pueden no estar conectados con agentes más relevantes. En este documento, se desarrolla un enfoque basado en aprendizaje por refuerzo en línea para aprovechar las características dinámicas en tiempo de ejecución de los sistemas de recuperación de información P2P, representadas por información sobre sesiones de búsqueda pasadas. Específicamente, los agentes mantienen estimaciones sobre las habilidades de los agentes aguas abajo para proporcionar documentos relevantes para las consultas entrantes. Estas estimaciones se actualizan gradualmente al aprender de la información de retroalimentación devuelta de sesiones de búsqueda anteriores. Basándose en esta información, los agentes derivan políticas de enrutamiento correspondientes. Posteriormente, estos agentes dirigen las consultas basándose en las políticas aprendidas y actualizan las estimaciones según las nuevas políticas de enrutamiento. Los resultados experimentales demuestran que el algoritmo de aprendizaje mejora considerablemente el rendimiento del enrutamiento en dos conjuntos de colecciones de pruebas que han sido utilizados en una variedad de estudios de recuperación de información distribuida. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagente Términos Generales Algoritmos, Rendimiento, Experimentación 1. En los últimos años ha habido un creciente interés en estudiar cómo controlar los procesos de búsqueda en sistemas de recuperación de información (IR) basados en pares (P2P) [6, 13, 14, 15]. En esta línea de investigación, uno de los problemas centrales que preocupa a los investigadores es dirigir de manera eficiente las consultas de los usuarios en la red a los agentes que poseen los documentos apropiados. En ausencia de información global, las estrategias dominantes para abordar este problema son enfoques basados en la similitud de contenido [6, 13, 14, 15]. Si bien la similitud de contenido entre consultas y nodos locales parece ser un indicador creíble para la cantidad de documentos relevantes que residen en cada nodo, estos enfoques están limitados por una serie de factores. En primer lugar, las métricas basadas en similitud pueden ser miope ya que los nodos localmente relevantes pueden no estar conectados a otros nodos relevantes. Segundo, los enfoques basados en la similitud no tienen en cuenta las características de tiempo de ejecución de los sistemas de IR P2P, incluidos los parámetros ambientales, el uso del ancho de banda y la información histórica de las sesiones de búsqueda pasadas, que proporcionan información valiosa para los algoritmos de enrutamiento de consultas. En este artículo, desarrollamos un enfoque de IR basado en aprendizaje por refuerzo para mejorar el rendimiento de los algoritmos de búsqueda de IR distribuidos. Los agentes pueden adquirir mejores estrategias de búsqueda al recopilar y analizar información de retroalimentación de sesiones de búsqueda anteriores. En particular, los agentes mantienen estimaciones, es decir, utilidad esperada, sobre las capacidades de los agentes aguas abajo de proporcionar documentos relevantes para tipos específicos de consultas entrantes. Estas estimaciones se actualizan gradualmente al aprender de la información de retroalimentación devuelta de sesiones de búsqueda anteriores. Basándose en la información actualizada de utilidad esperada, los agentes derivan políticas de enrutamiento correspondientes. Posteriormente, estos agentes dirigen las consultas basándose en las políticas aprendidas y actualizan las estimaciones sobre la utilidad esperada en función de las nuevas políticas de enrutamiento. Este proceso se lleva a cabo de manera iterativa. El objetivo del algoritmo de aprendizaje, aunque consume ancho de banda de red, es reducir el tiempo de enrutamiento para procesar más consultas por unidad de tiempo y al mismo tiempo encontrar más documentos relevantes. Esto contrasta con los enfoques basados en la similitud de contenido, donde se repiten operaciones similares para cada consulta entrante y el tiempo de procesamiento se mantiene en gran medida constante con el tiempo. Otra forma de ver este documento es que nuestro enfoque básico para la búsqueda de IR distribuida es construir una red de superposición jerárquica (organización de agentes) basada en la medida de similitud de contenido entre las colecciones de documentos de los agentes de manera ascendente. En trabajos anteriores, hemos demostrado que esta organización mejora significativamente el rendimiento de búsqueda. Sin embargo, esta estructura organizativa no tiene en cuenta los patrones de llegada de las consultas, incluyendo su frecuencia, tipos y dónde entran en el sistema, ni el ancho de banda de comunicación disponible en la red y las capacidades de procesamiento de los agentes individuales. La intención del aprendizaje por refuerzo es adaptar las decisiones de enrutamiento de los agentes a las situaciones dinámicas de la red y aprender de sesiones de búsqueda pasadas. Específicamente, las contribuciones de este artículo incluyen: (1) un enfoque basado en aprendizaje por refuerzo para que los agentes adquieran políticas de enrutamiento satisfactorias basadas en estimaciones de la contribución potencial de sus agentes vecinos; (2) dos estrategias para acelerar el proceso de aprendizaje. Según nuestro mejor conocimiento, esta es una de las primeras aplicaciones de aprendizaje por refuerzo para abordar problemas de compartición de contenido distribuido, lo cual indica algunos de los problemas al aplicar el refuerzo en una aplicación compleja. El resto de este documento está organizado de la siguiente manera: la Sección 2 revisa los sistemas jerárquicos de intercambio de contenido y el algoritmo de búsqueda de dos fases basado en dicha topología. La Sección 3 describe un enfoque basado en aprendizaje por refuerzo para dirigir el proceso de enrutamiento; la Sección 4 detalla la configuración experimental y analiza los resultados. La sección 5 discute estudios relacionados y la sección 6 concluye el artículo. BÚSQUEDA EN SISTEMAS JERÁRQUICOS DE IR P2P Esta sección revisa brevemente nuestros enfoques básicos para los sistemas jerárquicos de IR P2P. En un sistema jerárquico de recuperación de información P2P ilustrado en la Fig.1, los agentes están conectados entre sí a través de tres tipos de enlaces: enlaces ascendentes, enlaces descendentes y enlaces laterales. En las siguientes secciones, denotamos el conjunto de agentes que están directamente conectados al agente Ai como DirectConn(Ai), el cual se define como DirectConn(Ai) = NEI(Ai) ∪ PAR(Ai) ∪ CHL(Ai), donde NEI(Ai) es el conjunto de agentes vecinos conectados a Ai a través de enlaces laterales; PAR(Ai) es el conjunto de agentes a los que el agente Ai está conectado a través de enlaces ascendentes y CHL(Ai) es el conjunto de agentes a los que el agente Ai se conecta a través de enlaces descendentes. Estos enlaces se establecen a través de un proceso de agrupamiento distribuido basado en la similitud de contenido de abajo hacia arriba[15]. Estos enlaces son luego utilizados por agentes para localizar otros agentes que contienen documentos relevantes a las consultas dadas. Un agente típico Ai en nuestro sistema utiliza dos colas: una cola de búsqueda local, LSi, y una cola de reenvío de mensajes MFi. Los estados de las dos colas constituyen los estados internos de un agente. La cola de búsqueda local LSi almacena sesiones de búsqueda programadas para procesamiento local. Es una cola de prioridad y el agente Ai siempre selecciona las consultas más prometedoras para procesarlas con el fin de maximizar la utilidad global. MFi consiste en un conjunto de consultas para reenviar y se procesa de manera FIFO (primero en entrar, primero en salir). Para la primera consulta en MFi, el agente Ai determina a qué subconjunto de sus agentes vecinos reenviarla basándose en la política de enrutamiento de los agentes πi. Estas decisiones de enrutamiento determinan cómo se lleva a cabo el proceso de búsqueda en la red. En este documento, llamamos a Ai como el agente aguas arriba de Ajs y a Aj como el agente aguas abajo de Ais si A4 A5 A6 A7 A2 A3 A9 NEI(A2)={A3} PAR(A2)={A1} CHL(A2)={A4,A5} A1 A8 Figura 1: Una fracción de un sistema jerárquico P2PIR en el que un agente Ai dirige una consulta al agente Aj. El protocolo de búsqueda distribuida de nuestra organización de agentes jerárquicos está compuesto por dos pasos. En el primer paso, al recibir una consulta qk en el tiempo tl de un usuario, el agente Ai inicia una sesión de búsqueda si al sondear a sus agentes vecinos Aj ∈ NEI(Ai) con el mensaje PROBE para el valor de similitud Sim(qk, Aj) entre qk y Aj. Aquí, Ai se define como el iniciador de la consulta de la sesión de búsqueda si. En el segundo paso, Ai selecciona un grupo de agentes más prometedores para iniciar el proceso de búsqueda real con el mensaje BUSCAR. Estos mensajes de BÚSQUEDA contienen un parámetro TTL (Tiempo de Vida) además de la consulta. El valor TTL disminuye en 1 después de cada salto. En el proceso de búsqueda, los agentes descartan aquellas consultas que han sido procesadas previamente o cuyo TTL disminuye a 0, lo que evita que las consultas se repitan infinitamente en el sistema. La sesión de búsqueda termina cuando todos los agentes que reciben la consulta la descartan o el TTL disminuye a 0. Tras recibir mensajes de BÚSQUEDA para qk, los agentes programan actividades locales que incluyen búsqueda local, reenvío de qk a sus vecinos y devolución de resultados de búsqueda al iniciador de la consulta. Este proceso y los algoritmos relacionados se detallan en [15, 14]. 3. En el enfoque de búsqueda basado en aprendizaje por refuerzo, mencionado anteriormente, las decisiones de enrutamiento de un agente Ai dependen de la comparación de similitud entre las consultas entrantes y los agentes vecinos de Ai para reenviar esas consultas a agentes relevantes sin inundar la red con mensajes de consulta innecesarios. Sin embargo, esta heurística es miope porque un vecino directo relevante no necesariamente está conectado con otros agentes relevantes. En esta sección, proponemos un enfoque más general al plantear este problema como una tarea de aprendizaje por refuerzo. En busca de una mayor flexibilidad, los agentes pueden alternar entre dos modos: modo de aprendizaje y modo sin aprendizaje. En el modo de no aprendizaje, los agentes operan de la misma manera que lo hacen en los procesos de búsqueda distribuida normales descritos en [14, 15]. Por otro lado, en el modo de aprendizaje, en paralelo con las sesiones de búsqueda distribuida, los agentes también participan en un proceso de aprendizaje que se detallará en esta sección. Ten en cuenta que en el protocolo de aprendizaje, el proceso de aprendizaje no interfiere con el proceso de búsqueda distribuida. Los agentes pueden elegir iniciar y detener procesos de aprendizaje sin afectar el rendimiento del sistema. En particular, dado que el proceso de aprendizaje consume recursos de red (especialmente ancho de banda), los agentes pueden optar por iniciar el aprendizaje solo cuando la carga de la red es relativamente baja, minimizando así los costos adicionales de comunicación incurridos por el algoritmo de aprendizaje. La sección está estructurada de la siguiente manera, la Sección 3.1 describe 232 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) un modelo basado en aprendizaje por refuerzo. La sección 3.2 describe un protocolo para implementar el algoritmo de aprendizaje en la red. La sección 3.3 discute la convergencia del algoritmo de aprendizaje. 3.1 El Modelo La política de enrutamiento de un agente toma el estado de una sesión de búsqueda como entrada y produce las acciones de enrutamiento para esa consulta. En nuestro trabajo, el estado de una sesión de búsqueda sj se estipula como: QSj = (qk, ttlj) donde ttlj es el número de saltos que quedan para la sesión de búsqueda sj, qk es la consulta específica. QL es un atributo de qk que indica a qué tipo de consultas qk probablemente pertenecen. El conjunto de QL puede ser generado ejecutando un algoritmo de clasificación en línea simple en todas las consultas que han sido procesadas por los agentes, o un algoritmo fuera de línea en un conjunto de entrenamiento pre-designado. La suposición aquí es que el conjunto de tipos de consultas se aprende de antemano y pertenece al conocimiento común de los agentes en la red. El trabajo futuro incluye explorar cómo se puede lograr el aprendizaje cuando esta suposición no se cumple. Dado el conjunto de tipos de consultas establecido, una consulta entrante qi puede ser clasificada en una clase de consulta Q(qi) mediante la fórmula: Q(qi) = arg max Qj P(qi|Qj) (1) donde P(qi|Qj) indica la probabilidad de que la consulta qi sea generada por la clase de consulta Qj [8]. El conjunto de acciones de enrutamiento atómico de un agente Ai se denota como {αi}, donde {αi} se define como αi = {αi0 , αi1 , ..., αin }. Un elemento αij representa una acción para dirigir una consulta dada al agente vecino Aij ∈ DirectConn(Ai). La política de enrutamiento πi del agente Ai es estocástica y su resultado para una sesión de búsqueda con estado QSj se define como: πi(QSj) = {(αi0 , πi(QSi, αi0 )), (αi1 , πi(QSi, αi1 )), ...} (2) Nótese que el operador πi está sobrecargado para representar tanto la política probabilística para una sesión de búsqueda con estado QSj, denotada como πi(QSj); como la probabilidad de enviar la consulta a un agente vecino específico Aik ∈ DirectConn(Ai) bajo la política πi(QSj), denotada como πi(QSj, αik). Por lo tanto, la ecuación (2) significa que la probabilidad de reenviar la sesión de búsqueda al agente Ai0 es πi(QSi, αi0) y así sucesivamente. Bajo esta política estocástica, la acción de enrutamiento es no determinística. La ventaja de esta estrategia es que los mejores agentes vecinos no serán seleccionados repetidamente, mitigando así las posibles situaciones de puntos calientes. La utilidad esperada, Un i (QSj), se utiliza para estimar la ganancia potencial de utilidad al dirigir el tipo de consulta QSj al agente Ai bajo la política πn i. El superíndice n indica el valor en la n-ésima iteración en un proceso de aprendizaje iterativo. La utilidad esperada proporciona orientación de enrutamiento para futuras sesiones de búsqueda. En el proceso de búsqueda, cada agente Ai mantiene observaciones parciales de los estados de sus vecinos, como se muestra en la Figura 2. La observación parcial incluye información no local como la estimación de la utilidad potencial de su vecino Am para el estado de consulta QSj, denotado como Um(QSj), así como la información de carga, Lm. Estas observaciones son actualizadas periódicamente por los vecinos. La información de utilidad estimada se utilizará para actualizar la utilidad esperada de A en su política de enrutamiento. Cargar información de utilidad esperada para diferentes tipos de consultas de agentes vecinos... A0 A1 A3 A2 Un 0 (QS0) ... ... ... ... ...... Un 0 (QS1) Un 1 (QS1) Un 2 (QS1) Un 3 (QS1) Un 1 (QS0) Un 2 (QS0) Un 3 (QS0) Ln 0 Ln 1 Ln 2 Ln 3 ... QS0 QS1 ... Figura 2: La Observación Parcial del Agente Ais sobre sus vecinos (A0, A1...) La información de carga de Am, Lm, se define como Lm = |MFm| Cm, donde |MFm| es la longitud de la cola de mensajes en espera y Cm es la tasa de servicio de la cola de mensajes en espera del agente Am. Por lo tanto, Lm caracteriza la utilización de un canal de comunicación de agentes, y así proporciona información no local a los vecinos de Ams para ajustar los parámetros de su política de enrutamiento y evitar inundar a sus agentes aguas abajo. Ten en cuenta que, basado en las características de las consultas que ingresan al sistema y las capacidades de los agentes, la carga de trabajo de los agentes puede no ser uniforme. Después de recopilar la información de la tasa de utilización de todos sus vecinos, el agente Ai calcula Li como una medida única para evaluar la condición de carga promedio de su vecindario: Li = P k Lk |DirectConn(Ai)| Los agentes explotan el valor de Li para determinar la probabilidad de enrutamiento en su política de enrutamiento. Se debe tener en cuenta que, como se describe en la Sección 3.2, la información sobre agentes vecinos se transmite junto con el mensaje de consulta propagado entre los agentes siempre que sea posible para reducir la sobrecarga de tráfico. 3.1.1 Actualización de la Política Se introduce un proceso de actualización iterativo para que los agentes aprendan una política de enrutamiento estocástica satisfactoria. En este proceso iterativo, los agentes actualizan sus estimaciones sobre la utilidad potencial de sus políticas de enrutamiento actuales y luego propagan las estimaciones actualizadas a sus vecinos. Sus vecinos luego generan una nueva política de enrutamiento basada en la observación actualizada y a su vez calculan la utilidad esperada basada en las nuevas políticas y continúan este proceso iterativo. En particular, en el tiempo n, dado un conjunto de utilidad esperada, un agente Ai, cuyo conjunto de agentes directamente conectados es DirectConn(Ai) = {Ai0, ..., Aim}, determina su política de enrutamiento estocástico correspondiente para una sesión de búsqueda del estado QSj basada en los siguientes pasos: (1) Ai primero selecciona un subconjunto de agentes como los agentes potenciales aguas abajo del conjunto DirectConn(Ai), denotado como PDn(Ai, QSj). El tamaño del agente potencial aguas abajo se especifica como |PDn(Ai, QSj)| = min(|NEI(Ai), dn i + k)| donde k es una constante y se establece en 3 en este documento; dn i, el ancho hacia adelante, se define como el número esperado de The Sixth Intl. La fórmula especifica que el conjunto potencial de agentes aguas abajo PDn(Ai, QSj) es o bien el subconjunto de agentes vecinos con los k valores de utilidad esperada más altos para el estado QSj entre todos los agentes en DirectConn(Ai), o todos sus agentes vecinos. La k se introduce basada en la idea de una política de enrutamiento estocástica y hace que la probabilidad de reenvío del agente dn i + k más alto sea inferior al 100%. Ten en cuenta que si queremos limitar el número de agentes secundarios para la sesión de búsqueda sj a 5, la probabilidad de reenviar la consulta a todos los agentes vecinos debe sumar 5. Configurar adecuadamente el valor de dn i puede mejorar la tasa de utilización del ancho de banda de la red cuando gran parte de la red está inactiva, al mismo tiempo que mitiga la carga de tráfico cuando la red está altamente cargada. El valor de dn+1 i se actualiza en función de dn i, las observaciones anteriores y actuales sobre la situación del tráfico en el vecindario. Específicamente, la fórmula de actualización para dn+1 i es dn+1 i = dn i ∗ (1 + 1 − Li |DirectConn(Ai)| ) En esta fórmula, el ancho de avance se actualiza en función de las condiciones de tráfico del vecindario del agente Ai, es decir, Li, y su valor anterior. (2) Para cada agente Aik en el PDn(Ai, QSj), la probabilidad de reenviar la consulta a Aik se determina de la siguiente manera para asignar una mayor probabilidad de reenvío a los agentes vecinos con un valor de utilidad esperado más alto: πn+1 i (QSj, αik ) = dn+1 i |PDn(Ai, QSj)| + β ∗ ` Uik (QSj) − PDU(Ai, QSj) |PDn(Ai, QSj)| ´ (3) donde PDUn(Ai, QSj) = X o∈P Dn(Ai,QSj ) Uo(QSj) y QSj es el estado subsiguiente del agente Aik después de que el agente Ai reenvía la sesión de búsqueda con estado QSj a su agente vecino Aik; Si QSj = (qk, ttl0), entonces QSj = (qk, ttl0 − 1). En la fórmula 3, el primer término a la derecha de la ecuación, dn+1 i |P Dn(Ai,QSj )|, se utiliza para determinar la probabilidad de reenvío distribuyendo equitativamente el ancho de avance, dn+1 i, a los agentes en el conjunto PDn(Ai, QSj). El segundo término se utiliza para ajustar la probabilidad de ser elegido de manera que se favorezca a los agentes con valores de utilidad esperada más altos. β se determina de acuerdo a: β = min ` m − dn+1 i m ∗ umax − PDUn(Ai, QSj) , dn+1 i PDUn(Ai, QSj) − m ∗ umin ´ (4) donde m = |PDn(Ai, QSj)|, umax = max o∈P Dn(Ao,QSj ) Uo(QSj) y umin = min o∈P Dn(Ao,QSj ) Uo(QSj) Esta fórmula garantiza que el valor final de πn+1 i (QSj, αik ) esté bien definido, es decir, 0 ≤ πn+1 i (QSj, αik ) ≤ 1 y X i πn+1 i (QSj, αik ) = dn+1 i Sin embargo, esta solución no explora todas las posibilidades. Para equilibrar entre la explotación y la exploración, se adopta un enfoque λ-Greedy. En el enfoque λ-Greedy, además de asignar una probabilidad más alta a aquellos agentes con un valor de utilidad esperado más alto, como en la ecuación (3). Los agentes que parecen no ser buenas opciones también recibirán consultas basadas en una tasa de exploración dinámica. En particular, para agentes en el conjunto PDn(Ai, QSj), πn+1 i1 (QSj) se determina de la misma manera que arriba, con la única diferencia de que dn+1 i es reemplazado por dn+1 i ∗ (1 − λn). El ancho de banda de búsqueda restante se utiliza para el aprendizaje asignando la probabilidad λn de manera uniforme a los agentes Ai2 en el conjunto DirectConn(Ai) - PDn(Ai, QSj). πn+1 i2 (QSj, αik ) = dn+1 i ∗ λn |DirectConn(Ai) - PDn(Ai, QSj)| (5) donde PDn(Ai, QSj) ⊂ DirectConn(Ai). Se debe tener en cuenta que la tasa de exploración λ no es constante y disminuye con el tiempo. El λ se determina de acuerdo con la siguiente ecuación: λn+1 = λ0 ∗ e−c1n (6) donde λ0 es la tasa de exploración inicial, que es una constante; c1 también es una constante para ajustar la tasa de disminución de la tasa de exploración; n es la unidad de tiempo actual. 3.1.2 Actualización de la Utilidad Esperada Una vez que la política de enrutamiento en el paso n+1, πn+1 i, se determina según la fórmula anterior, el agente Ai puede actualizar su propia utilidad esperada, Un+1 i (QSi), basándose en la política de enrutamiento actualizada resultante de la fórmula 5 y los valores U actualizados de sus agentes vecinos. Bajo el supuesto de que después de que una consulta se envía a los vecinos de Ai, las sesiones de búsqueda subsiguientes son independientes, la fórmula de actualización es similar a la fórmula de actualización de Bellman en Q-Learning: Un+1 i (QSj) = (1 − θi) ∗ Un i (QSj) + θi ∗ (Rn+1 i (QSj) + X k πn+1 i (QSj, αik )Un k (QSj)) (7) donde QSj = (Qj, ttl − 1) es el siguiente estado de QSj = (Qj, ttl); Rn+1 i (QSj) es la recompensa local esperada para la clase de consulta Qk en el agente Ai bajo la política de enrutamiento πn+1 i; θi es el coeficiente para decidir cuánto peso se le da al valor antiguo durante el proceso de actualización: cuanto menor sea el valor de θi, se espera que el agente aprenda más rápido el valor real, mientras que mayor sea la volatilidad del algoritmo, y viceversa. Rn+1 (s) se actualiza de acuerdo con la siguiente ecuación: 234 The Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Rn+1 i (QSj) = Rn i (QSj) +γi ∗ (r(QSj) − Rn i (QSj)) ∗ P(qj|Qj ) (8) donde r(QSj) es la recompensa local asociada con la sesión de búsqueda. P(qj|Qj) indica qué tan relevante es la consulta qj para el tipo de consulta Qj, y γi es la tasa de aprendizaje para el agente Ai. Dependiendo de la similitud entre una consulta específica qi y su tipo de consulta correspondiente Qi, la recompensa local asociada con la sesión de búsqueda tiene un impacto diferente en la estimación de Rn i (QSj). En la fórmula anterior, este impacto se refleja en el coeficiente, el valor P(qj|Qj). Función de recompensa Después de que una sesión de búsqueda se detiene cuando sus valores TTL expiran, todos los resultados de búsqueda se devuelven al usuario y se comparan con la evaluación de relevancia. Suponiendo que el conjunto de resultados de búsqueda es SR, la recompensa Rew(SR) se define como: Rew(SR) = j 1 si |Rel(SR)| > c |Rel(SR)| c en caso contrario, donde SR es el conjunto de resultados de búsqueda devueltos, Rel(SR) es el conjunto de documentos relevantes en los resultados de búsqueda. Esta ecuación especifica que los usuarios otorgan una recompensa de 1.0 si el número de documentos relevantes devueltos alcanza un número predefinido c. De lo contrario, la recompensa es proporcional al número de documentos relevantes devueltos. La razón para establecer un valor de corte de este tipo es que la importancia de la proporción de recuperación disminuye con la abundancia de documentos relevantes en el mundo real, por lo tanto, los usuarios tienden a enfocarse solo en un número limitado de resultados buscados. Los detalles del protocolo de enrutamiento actual se introducirán en la Sección 3.2 cuando presentemos cómo se implementa el algoritmo de aprendizaje en sistemas reales. 3.2 Implementación del algoritmo de aprendizaje. Esta sección describe cómo se puede utilizar el algoritmo de aprendizaje en un proceso de búsqueda de una o dos fases. En el algoritmo de búsqueda de una sola fase, las sesiones de búsqueda comienzan desde los iniciadores de las consultas. Por el contrario, en el algoritmo de búsqueda de dos pasos, el iniciador de la consulta primero intenta buscar un punto de inicio más apropiado para la consulta introduciendo un paso exploratorio como se describe en la Sección 2. A pesar de la diferencia en la calidad de los puntos de partida, la mayor parte del proceso de aprendizaje para los dos algoritmos es en gran medida el mismo como se describe en los párrafos siguientes. Antes de que comience el aprendizaje, cada agente inicializa el valor de utilidad esperado para todos los posibles estados como 0. Posteriormente, al recibir una consulta, además de las operaciones normales descritas en la sección anterior, un agente Ai también configura un temporizador para esperar los resultados de búsqueda devueltos por sus agentes subordinados. Una vez que el temporizador expire o haya recibido respuesta de todos sus agentes aguas abajo, Ai fusiona y reenvía los resultados de búsqueda acumulados de sus agentes aguas abajo a su agente aguas arriba. Configurar el temporizador acelera el aprendizaje porque los agentes pueden evitar esperar demasiado tiempo a que los agentes aguas abajo devuelvan los resultados de búsqueda. Ten en cuenta que estos resultados detallados y la información correspondiente del agente seguirán almacenados en Ai hasta que la información de retroalimentación sea transmitida desde su agente aguas arriba y se pueda evaluar el rendimiento de sus agentes aguas abajo. La duración del temporizador está relacionada con el valor TTL. En este documento, configuramos el temporizador a ttimer = ttli ∗ 2 + tf, donde ttli ∗ 2 es la suma del tiempo de viaje de las consultas en la red, y tf es el período de tiempo esperado que los usuarios desearían esperar. Los resultados de la búsqueda serán finalmente devueltos al iniciador de la sesión de búsqueda A0. Serán comparados con la evaluación de relevancia proporcionada por los usuarios finales (como se describe en la sección del experimento, la evaluación de relevancia para el conjunto de consultas se proporciona junto con las colecciones de datos). La recompensa será calculada y propagada hacia atrás a los agentes a lo largo del camino por el que se pasaron los resultados de la búsqueda. Este es un proceso inverso de la propagación de los resultados de búsqueda. En el proceso de propagar la recompensa hacia atrás, los agentes actualizan las estimaciones de su propio valor potencial de utilidad, generan una política actualizada y transmiten sus resultados actualizados a los agentes vecinos basándose en el algoritmo descrito en la Sección 3. Tras el cambio del valor de utilidad esperado, el agente Ai envía su estimación de utilidad actualizada a sus vecinos para que puedan actuar según la utilidad esperada modificada y el estado correspondiente. Este mensaje de actualización incluye la recompensa potencial, así como el estado correspondiente QSi = (qk, ttll) del agente Ai. Cada agente vecino, Aj, reacciona a este tipo de mensaje de actualización actualizando el valor de utilidad esperado para el estado QSj(qk, ttll + 1) de acuerdo con el valor de utilidad esperado cambiado recién anunciado. Una vez que completen la actualización, los agentes informarían nuevamente a los vecinos relacionados para que actualicen sus valores. Este proceso continúa hasta que el valor TTL en el mensaje de actualización aumenta al límite de TTL. Para acelerar el proceso de aprendizaje, al actualizar los valores de utilidad esperada de un agente Ai con agentes vecinos, especificamos que Um(Qk, ttl0) >= Um(Qk, ttl1) si ttl0 > ttl1. Por lo tanto, cuando el agente Ai recibe un valor de utilidad esperada actualizado con ttl1, también actualiza los valores de utilidad esperada con cualquier ttl0 > ttl1 si Um(Qk, ttl0) < Um(Qk, ttl1) para acelerar la convergencia. Esta heurística se basa en el hecho de que la utilidad de una sesión de búsqueda es una función no decreciente del tiempo t. 3.3 Discusión Al formalizar el sistema de enrutamiento de contenido como una tarea de aprendizaje, se hacen muchas suposiciones. En sistemas reales, estas suposiciones pueden no cumplirse, por lo que el algoritmo de aprendizaje puede no converger. Dos problemas son de particular importancia, (1) Este problema de enrutamiento de contenido no tiene propiedades de Markov. A diferencia del enrutamiento de paquetes basado en el nivel de IP, la <br>decisión de enrutamiento</br> de cada agente para una sesión de búsqueda particular sj depende del historial de enrutamiento de sj. Por lo tanto, la suposición de que todas las sesiones de búsqueda posteriores son independientes no se cumple en la realidad. Esto puede llevar a un problema de doble conteo en el que los documentos relevantes de algunos agentes se contarán más de una vez para el estado donde el valor TTL es mayor que 1. Sin embargo, en el contexto de las organizaciones de agentes jerárquicos, dos factores mitigan estos problemas: primero, los agentes en cada grupo de contenido forman una estructura similar a un árbol. Con la ausencia de los ciclos, las estimaciones dentro del árbol estarían cerca del valor preciso. En segundo lugar, la naturaleza estocástica de la política de enrutamiento remedia parcialmente este problema. El Sexto Internacional. Otro desafío para este algoritmo de aprendizaje es que en un entorno de red real, las observaciones sobre agentes vecinos pueden no poder actualizarse a tiempo debido al retraso en la comunicación u otras situaciones. Además, cuando los agentes vecinos actualizan sus estimaciones al mismo tiempo, puede surgir oscilación durante el proceso de aprendizaje[1]. Este documento explora varios enfoques para acelerar el proceso de aprendizaje. Además de la estrategia mencionada de actualizar los valores de utilidad esperada, también empleamos una estrategia de actualización activa donde los agentes notifican a sus vecinos cada vez que se actualiza su utilidad esperada. Por lo tanto, se puede lograr una velocidad de convergencia más rápida. Esta estrategia contrasta con la actualización perezosa, donde los agentes solo repiten a sus agentes vecinos con el cambio en la utilidad esperada cuando intercambian información. El compromiso entre los dos enfoques es la carga de red versus la velocidad de aprendizaje. La ventaja de este algoritmo de aprendizaje es que una vez que se aprende una política de enrutamiento, los agentes no tienen que comparar repetidamente la similitud de las consultas siempre y cuando la topología de la red permanezca sin cambios. En cambio, el agente solo tiene que determinar adecuadamente la clasificación de la consulta y seguir las políticas aprendidas. La desventaja de este enfoque basado en el aprendizaje es que el proceso de aprendizaje debe llevarse a cabo cada vez que la estructura de la red cambia. Hay muchas posibles extensiones para este modelo de aprendizaje. Por ejemplo, actualmente se utiliza una sola medida para indicar la carga de tráfico en un vecindario de agentes. Una extensión sencilla sería llevar un registro de la carga individual de cada vecino del agente. 4. CONFIGURACIÓN DE EXPERIMENTOS Y RESULTADOS Los experimentos se llevan a cabo en el kit de simulación TRANO con dos conjuntos de datos, TREC-VLC-921 y TREC123-100. Las siguientes subsecciones presentan el banco de pruebas TRANO, los conjuntos de datos y los resultados experimentales. 4.1 Banco de pruebas TRANO TRANO (Task Routing on Agent Network Organization) es un banco de pruebas de recuperación de información basado en red de agentes múltiples. TRANO se construye sobre el Farm [4], un simulador distribuido basado en el tiempo que proporciona un marco de diseminación de datos para organizaciones de redes de agentes distribuidos a gran escala. TRANO apoya la importación y exportación de perfiles de organizaciones de agentes, incluyendo conexiones topológicas y otras características. Cada agente de TRANO está compuesto por una estructura de vista de agente y una unidad de control. En la simulación, cada agente es pulsado regularmente y el agente verifica las colas de mensajes entrantes, realiza operaciones locales y luego reenvía mensajes a otros agentes. 4.2 Configuración Experimental En nuestro experimento, utilizamos dos conjuntos de datos estándar, los conjuntos de datos TRECVLC-921 y TREC-123-100, para simular las colecciones alojadas en los agentes. Los conjuntos de datos TREC-VLC-921 y TREC123-100 fueron creados por el Instituto Nacional de Tecnología Estándar de los Estados Unidos (NIST) para sus conferencias TREC. En el ámbito de la recuperación de información distribuida, las dos colecciones de datos se dividen en 921 y 100 subcolecciones. Se observa que el conjunto de datos TREC-VLC-921 es más heterogéneo que TREC-123-100 en términos de origen, longitud de documento y distribución de documentos relevantes a partir de las estadísticas de las dos colecciones de datos enumeradas en [13]. Por lo tanto, TREC-VLC-921 está mucho más cerca de las distribuciones reales de documentos en entornos P2P. Además, TREC-123-100 se divide en dos conjuntos de 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0 500 1000 1500 2000 2500 3000 Número de consulta de ARSS frente al número de consultas entrantes para TREC-VLC-921 SSLA-921 SSNA-921 Figura 3: ARSS (Recompensa promedio por sesión de búsqueda) frente al número de sesiones de búsqueda para la búsqueda de 1 fase en TREC-VLC-921 0 0.1 0.2 0.3 0.4 0.5 0.6 0 500 1000 1500 2000 2500 3000 Número de consulta de ARSS frente al número de consultas para TREC-VLC-921 TSLA-921 TSNA-921 Figura 4: ARSS (Recompensa promedio por sesión de búsqueda) frente al número de sesiones de búsqueda para la búsqueda de 2 fases en TREC-VLC-921 subcolecciones de dos formas: aleatoriamente y por fuente. Las dos particiones se denominan TREC-123-100-Random y TREC-123-100-Source respectivamente. Los documentos en cada subcolección en el conjunto de datos TREC-123-100-Source son más coherentes que los de TREC-123-100-Random. Los dos conjuntos diferentes de particiones nos permiten observar cómo el algoritmo de aprendizaje distribuido se ve afectado por la homogeneidad de las colecciones. La organización jerárquica de agentes es generada por el algoritmo descrito en nuestro algoritmo anterior [15]. Durante el proceso de generación de la topología, la información de grado de cada agente es estimada por el algoritmo introducido por Palmer et al. [9] con parámetros α = 0.5 y β = 0.6. En nuestros experimentos, estimamos el límite superior y el límite de grado descendente utilizando factores de descuento lineales de 0.5, 0.8 y 1.0. Una vez que se construye la topología, las consultas seleccionadas al azar del conjunto de consultas 301-350 en TREC-VLC-921 y del conjunto de consultas 1-50 en TREC-123-100-Random y TREC-123-100-Source se inyectan en el sistema basado en una distribución de Poisson P(N(t) = n) = (λt)n n! e−λ 236 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 0 50 100 150 200 250 300 350 400 0 500 1000 1500 2000 2500 3000 Utilidad acumulada Número de consulta Utilidad acumulada sobre el número de consultas entrantes TSLA-921 SSNA-921 SSLA-921 TSNA-921 Figura 5: La utilidad acumulada versus el número de sesiones de búsqueda TREC-VLC-921 Además, asumimos que todos los agentes tienen la misma probabilidad de recibir consultas del entorno, es decir, λ es el mismo para cada agente. En nuestros experimentos, λ se establece en 0.0543 para que la media de las consultas entrantes del entorno a la red de agentes sea de 50 por unidad de tiempo. El tiempo de servicio para la cola de comunicación y la cola de búsqueda local, es decir, tQij y trs, se establece en 0.01 unidades de tiempo y 0.05 unidades de tiempo respectivamente. En nuestros experimentos, hay diez tipos de consultas adquiridas mediante la agrupación del conjunto de consultas 301-350 y 1-50. 4.3 Análisis y evaluación de resultados. La Figura 3 muestra el ARSS (Recompensa Promedio por Sesión de Búsqueda) versus el número de consultas entrantes a lo largo del tiempo para el Algoritmo No Aprendido de un Solo Paso (SSNA) y el Algoritmo de Aprendizaje de un Solo Paso (SSLA) para la recolección de datos TREC-VLC-921. Se muestra que la recompensa promedio para el algoritmo SSNA oscila entre 0.02 y 0.06 y su rendimiento cambia poco con el tiempo. El promedio de recompensa para el enfoque SSLA comienza en el mismo nivel que el algoritmo SSNA. Pero el rendimiento aumenta con el tiempo y la ganancia promedio de rendimiento se estabiliza en aproximadamente un 25% después del rango de consulta de 2000 a 3000. La Figura 4 muestra el ARSS (Recompensa Promedio por Sesión de Búsqueda) versus el número de consultas entrantes a lo largo del tiempo para el Algoritmo No Aprendido de Dos Pasos (TSNA) y el Algoritmo de Aprendizaje de Dos Pasos (TSLA) para la recolección de datos TREC-VLC-921. El enfoque TSNA tiene un rendimiento relativamente consistente con una recompensa promedio que oscila entre 0.05 y 0.15. El promedio de recompensa para el enfoque de TSLA, donde se explota el algoritmo de aprendizaje, comienza en el mismo nivel que el algoritmo TSNA y mejora la recompensa promedio con el tiempo hasta que se unen al sistema entre 2000 y 2500 consultas. Los resultados muestran que la ganancia promedio de rendimiento para el enfoque TSLA sobre el enfoque TNLA es del 35% después de la estabilización. La Figura 5 muestra la utilidad acumulada versus el número de consultas entrantes a lo largo del tiempo para SSNA, SSLA, TSNA y TSLA respectivamente. Se ilustra que la utilidad acumulativa de los algoritmos no basados en aprendizaje aumenta principalmente de forma lineal con el tiempo, mientras que las ganancias de los algoritmos basados en aprendizaje se aceleran cuando más consultas ingresan al sistema. Estos resultados experimentales demuestran que los enfoques basados en el aprendizaje tienen un rendimiento consistentemente mejor que los algoritmos de enrutamiento no basados en el aprendizaje. Además, el algoritmo basado en aprendizaje de dos fases es mejor que el algoritmo basado en aprendizaje de una sola fase porque la recompensa máxima que un agente puede recibir al buscar en su vecindario dentro de TTL saltos está relacionada con el número total de documentos relevantes en esa área. Por lo tanto, incluso la política de enrutamiento óptima puede hacer poco más allá de alcanzar estos documentos relevantes más rápido. Por el contrario, el algoritmo de aprendizaje basado en dos pasos puede reubicar la sesión de búsqueda en un vecindario con documentos más relevantes. El TSLA combina los méritos de ambos enfoques y los supera. La Tabla 1 enumera la utilidad acumulativa para los conjuntos de datos TREC123-100-Random y TREC-123-100-Source con organizaciones jerárquicas. Las cinco columnas muestran los resultados de cuatro enfoques diferentes. En particular, la columna TSNA-Random muestra los resultados para el conjunto de datos TREC-123-100-Random con el enfoque TSNA. La columna TSLA-Random muestra los resultados para el conjunto de datos TREC-123-100-Random con el enfoque TSLA. Hay dos números en cada celda de la columna TSLA-Random. El primer número es la utilidad acumulada real, mientras que el segundo número es el porcentaje de ganancia en términos de utilidad sobre el enfoque TSNA. Las columnas TSNA-Source y TSLA-Source muestran los resultados para el conjunto de datos TREC-123-100-Source con los enfoques TSNA y TSLA respectivamente. La Tabla 1 muestra que la mejora en el rendimiento para TREC-123-100-Random no es tan significativa como en los otros conjuntos de datos. Esto se debe a que los documentos en la subcolección de TREC-123-100-Random son seleccionados al azar, lo que hace que el modelo de colección, la firma de la colección, sea menos significativo. Dado que ambos algoritmos están diseñados basados en la suposición de que las colecciones de documentos pueden ser bien representadas por su modelo de colección, este resultado no es sorprendente. En general, las Figuras 4, 5 y la Tabla 1 demuestran que el enfoque basado en aprendizaje por refuerzo puede mejorar considerablemente el rendimiento del sistema para ambas colecciones de datos. Sin embargo, queda como trabajo futuro descubrir la correlación entre la magnitud de las mejoras en el rendimiento y el tamaño de la colección de datos y/o la extensión de la heterogeneidad entre las subcolecciones. TRABAJO RELACIONADO El problema de enrutamiento de contenido difiere del enrutamiento a nivel de red en redes de comunicación conmutadas por paquetes en que el enrutamiento basado en contenido ocurre en redes a nivel de aplicación. Además, los agentes de destino en nuestros algoritmos de enrutamiento de contenido son múltiples y las direcciones no son conocidas en el proceso de enrutamiento. Los problemas de enrutamiento a nivel de IP han sido abordados desde la perspectiva del aprendizaje por refuerzo[2, 5, 11, 12]. Estos estudios han explorado algoritmos distribuidos completamente que son capaces, sin coordinación central, de difundir conocimiento sobre la red, encontrar de manera robusta y eficiente los caminos más cortos frente a cambios en las topologías de red y costos de enlace cambiantes. Hay dos clases principales de algoritmos de enrutamiento de paquetes adaptativos y distribuidos en la literatura: algoritmos de vector de distancia y algoritmos de estado de enlace. Si bien esta línea de estudios tiene cierta similitud con nuestro trabajo, se ha centrado principalmente en redes de comunicación conmutadas por paquetes. En este dominio, el destino de un paquete es determinístico y único. Cada agente mantiene estimaciones, probabilísticamente o determinísticamente, sobre la distancia a un destino específico a través de sus vecinos. Una variante de las técnicas de Q-Learning se implementa en The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Utilidad Acumulativa para los Conjuntos de Datos TREC-123-100-Random y TREC-123-100-Source con Organización Jerárquica; Los números porcentuales en las columnas TSLA-Random y TSLA-Source demuestran la ganancia de rendimiento sobre el algoritmo sin aprendizaje Número de consulta TSNA-Random TSLA-Random TSNA-Source TSLA-Source 500 25.15 28.45 13% 24.00 21.05 -13% 1000 104.99 126.74 20% 93.95 96.44 2.6% 1250 149.79 168.40 12% 122.64 134.05 9.3% 1500 188.94 211.05 12% 155.30 189.60 22% 1750 235.49 261.60 11% 189.14 243.90 28% 2000 275.09 319.25 16% 219.0 278.80 26% para actualizar las estimaciones y converger a las distancias reales. Se ha descubierto que la propiedad de localidad es una característica importante de los sistemas de recuperación de información en estudios de modelado de usuarios[3]. En los sistemas de intercambio de contenido basados en P2P, esta propiedad se ejemplifica por el fenómeno de que los usuarios tienden a enviar consultas que representan solo un número limitado de temas y, por el contrario, los usuarios en el mismo vecindario probablemente compartan intereses comunes y envíen consultas similares [10]. El enfoque basado en el aprendizaje se percibe como más beneficioso para los sistemas reales de recuperación de información distribuida que exhiben la propiedad de localidad. Esto se debe a que los patrones de tráfico y consultas de los usuarios pueden reducir el espacio de estados y acelerar el proceso de aprendizaje. El trabajo relacionado en aprovechar esta propiedad incluye [7], donde los autores intentaron abordar este problema mediante técnicas de modelado de usuario. 6. CONCLUSIONES En este artículo, se desarrolla un enfoque basado en aprendizaje por refuerzo para mejorar el rendimiento de algoritmos de búsqueda de IR distribuidos. En particular, los agentes mantienen estimaciones, es decir, utilidad esperada, sobre la capacidad de los agentes aguas abajo para proporcionar documentos relevantes para las consultas entrantes. Estas estimaciones se actualizan gradualmente al aprender de la información de retroalimentación devuelta de sesiones de búsqueda anteriores. Basándose en la información actualizada de utilidad esperada, los agentes modifican sus políticas de enrutamiento. Posteriormente, estos agentes dirigen las consultas basándose en las políticas aprendidas y actualizan las estimaciones sobre la utilidad esperada en función de las nuevas políticas de enrutamiento. Los experimentos en dos conjuntos de datos IR distribuidos diferentes ilustran que el enfoque de aprendizaje por refuerzo mejora considerablemente la utilidad acumulativa con el tiempo. 7. REFERENCIAS [1] S. Abdallah y V. Lesser. Aprendiendo el juego de asignación de tareas. En AAMAS 06: Actas de la quinta conferencia internacional conjunta sobre agentes autónomos y sistemas multiagente, páginas 850-857, Nueva York, NY, EE. UU., 2006. ACM Press. [2] J. \n\nACM Press. [2] J. A. Boyan y M. L. Littman. Enrutamiento de paquetes en redes de cambio dinámico: Un enfoque de aprendizaje por refuerzo. En Avances en Sistemas de Procesamiento de Información Neural, volumen 6, páginas 671-678. Morgan Kaufmann Publishers, Inc., 1994. [3] J. C. French, A. L. Powell, J. P. Callan, C. L. Viles, T. Emmitt, K. J. Prey, y Y. Mou. Comparando el rendimiento de los algoritmos de selección de bases de datos. En Investigación y Desarrollo en Recuperación de Información, páginas 238-245, 1999. [4] B. Horling, R. Mailler y V. Lesser. Granja: Un entorno escalable para el desarrollo y evaluación de múltiples agentes. En Avances en Ingeniería de Software para Sistemas Multiagente, páginas 220-237, Berlín, 2004. Springer-Verlag. [5] M. Littman y J. Boyan. Un esquema distribuido de aprendizaje por refuerzo para enrutamiento de redes. En Actas del Taller Internacional sobre Aplicaciones de Redes Neuronales a las Telecomunicaciones, 1993. [6] J. Lu y J. Callan. Búsqueda federada de bibliotecas digitales basadas en texto en redes jerárquicas de pares. En ECIR05, 2005. [7] J. Lu y J. Callan. Modelado de usuario para búsqueda federada de texto completo en redes peer-to-peer. En ACM SIGIR 2006. ACM Press, 2006. [8] C. D. Manning y H. Schütze. Fundamentos del Procesamiento del Lenguaje Natural Estadístico. El MIT Press, Cambridge, Massachusetts, 1999. [9] C. R. Palmer y J. G. Steffan. Generando topologías de red que obedezcan leyes de potencia. En Actas de GLOBECOM 2000, noviembre de 2000. [10] K. Sripanidkulchai, B. Maggs y H. Zhang. Localización eficiente de contenido utilizando la localidad basada en intereses en sistemas peer-to-peer. En INFOCOM, 2003. [11] D. Subramanian, P. Druschel y J. Chen. Hormigas y aprendizaje por refuerzo: Un estudio de caso en enrutamiento en redes dinámicas. En Actas de la Decimoquinta Conferencia Internacional Conjunta sobre Inteligencia Artificial, páginas 832-839, 1997. [12] J. N. Tao y L. Weaver. Un enfoque de múltiples agentes y gradiente de políticas para el enrutamiento de redes. En Actas de la Decimoctava Conferencia Internacional sobre Aprendizaje Automático, 2001. [13] H. Zhang, W. B. Croft, B. Levine y V. Lesser. Un enfoque multiagente para la recuperación de información entre pares. En Actas de la Tercera Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, julio de 2004. [14] H. Zhang y V. Lesser. Sistemas de recuperación de información peer-to-peer basados en múltiples agentes con sesiones de búsqueda concurrentes. En Actas de la Quinta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, mayo de 2006. [15] H. Zhang y V. R. Lesser. Una organización de agentes jerárquica formada dinámicamente para un sistema distribuido de intercambio de contenido. En la Conferencia Internacional IEEE/WIC/ACM sobre Tecnología de Agentes Inteligentes (IAT 2004) del 20 al 24 de septiembre de 2004 en Beijing, China, páginas 169-175. IEEE Computer Society, 2004. 238 La Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "utility": {
            "translated_key": "utilidad",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Reinforcement Learning based Distributed Search Algorithm For Hierarchical Peer-to-Peer Information Retrieval Systems Haizheng Zhang College of Information Science and Technology Pennsylvania State University University Park, PA 16803 hzhang@ist.psu.edu Victor Lesser Department of Computer Science University Of Massachusetts Amherst, MA 01003 lesser@cs.umass.edu ABSTRACT The dominant existing routing strategies employed in peerto-peer(P2P) based information retrieval(IR) systems are similarity-based approaches.",
                "In these approaches, agents depend on the content similarity between incoming queries and their direct neighboring agents to direct the distributed search sessions.",
                "However, such a heuristic is myopic in that the neighboring agents may not be connected to more relevant agents.",
                "In this paper, an online reinforcement-learning based approach is developed to take advantage of the dynamic run-time characteristics of P2P IR systems as represented by information about past search sessions.",
                "Specifically, agents maintain estimates on the downstream agents abilities to provide relevant documents for incoming queries.",
                "These estimates are updated gradually by learning from the feedback information returned from previous search sessions.",
                "Based on this information, the agents derive corresponding routing policies.",
                "Thereafter, these agents route the queries based on the learned policies and update the estimates based on the new routing policies.",
                "Experimental results demonstrate that the learning algorithm improves considerably the routing performance on two test collection sets that have been used in a variety of distributed IR studies.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Algorithms, Performance, Experimentation 1.",
                "INTRODUCTION Over the last few years there have been increasing interests in studying how to control the search processes in peer-to-peer(P2P) based information retrieval(IR) systems [6, 13, 14, 15].",
                "In this line of research, one of the core problems that concerns researchers is to efficiently route user queries in the network to agents that are in possession of appropriate documents.",
                "In the absence of global information, the dominant strategies in addressing this problem are content-similarity based approaches [6, 13, 14, 15].",
                "While the content similarity between queries and local nodes appears to be a creditable indicator for the number of relevant documents residing on each node, these approaches are limited by a number of factors.",
                "First of all, similaritybased metrics can be myopic since locally relevant nodes may not be connected to other relevant nodes.",
                "Second, the similarity-based approaches do not take into account the run-time characteristics of the P2P IR systems, including environmental parameters, bandwidth usage, and the historical information of the past search sessions, that provide valuable information for the query routing algorithms.",
                "In this paper, we develop a reinforcement learning based IR approach for improving the performance of distributed IR search algorithms.",
                "Agents can acquire better search strategies by collecting and analyzing feedback information from previous search sessions.",
                "Particularly, agents maintain estimates, namely expected <br>utility</br>, on the downstream agents capabilities of providing relevant documents for specific types of incoming queries.",
                "These estimates are updated gradually by learning from the feedback information returned from previous search sessions.",
                "Based on the updated expected <br>utility</br> information, the agents derive corresponding routing policies.",
                "Thereafter, these agents route the queries based on the learned policies and update the estimates on the expected <br>utility</br> based on the new routing policies.",
                "This process is conducted in an iterative manner.",
                "The goal of the learning algorithm, even though it consumes some network bandwidth, is to shorten the routing time so that more queries are processed per time unit while at the same time finding more relevant documents.",
                "This contrasts with the content-similarity based approaches where similar operations are repeated for every incoming query and the processing time keeps largely constant over time.",
                "Another way of viewing this paper is that our basic approach to distributed IR search is to construct a hierarchical overlay network(agent organization) based on the contentsimilarity measure among agents document collections in a bottom-up fashion.",
                "In the past work, we have shown that this organization improves search performance significantly.",
                "However, this organizational structure does not take into account the arrival patterns of queries, including their frequency, types, and where they enter the system, nor the available communication bandwidth of the network and processing capabilities of individual agents.",
                "The intention of the reinforcement learning is to adapt the agents routing decisions to the dynamic network situations and learn from past search sessions.",
                "Specifically, the contributions of this paper include: (1) a reinforcement learning based approach for agents to acquire satisfactory routing policies based on estimates of the potential contribution of their neighboring agents; (2) two strategies to speed up the learning process.",
                "To our best knowledge, this is one of the first reinforcement learning applications in addressing distributed content sharing problems and it is indicative of some of the issues in applying reinforcement in a complex application.",
                "The remainder of this paper is organized as follows: Section 2 reviews the hierarchical content sharing systems and the two-phase search algorithm based on such topology.",
                "Section 3 describes a reinforcement learning based approach to direct the routing process; Section 4 details the experimental settings and analyze the results.",
                "Section 5 discusses related studies and Section 6 concludes the paper. 2.",
                "SEARCH IN HIERARCHICAL P2P IR SYSTEMS This section briefly reviews our basic approaches to hierarchical P2P IR systems.",
                "In a hierarchical P2P IR system illustrated in Fig.1, agents are connected to each other through three types of links: upward links, downward links, and lateral links.",
                "In the following sections, we denote the set of agents that are directly connected to agent Ai as DirectConn(Ai), which is defined as DirectConn(Ai) = NEI(Ai) ∪ PAR(Ai) ∪ CHL(Ai) , where NEI(Ai) is the set of neighboring agents connected to Ai through lateral links; PAR(Ai) is the set of agents whom agent Ai is connected to through upward links and CHL(Ai) is the set of agents that agent Ai connects to through downward links.",
                "These links are established through a bottom-up content-similarity based distributed clustering process[15].",
                "These links are then used by agents to locate other agents that contain documents relevant to the given queries.",
                "A typical agent Ai in our system uses two queues: a local search queue, LSi, and a message forwarding queue MFi.",
                "The states of the two queues constitute the internal states of an agent.",
                "The local search queue LSi stores search sessions that are scheduled for local processing.",
                "It is a priority queue and agent Ai always selects the most promising queries to process in order to maximize the global <br>utility</br>.",
                "MFi consists of a set of queries to forward on and is processed in a FIFO (first in first out) fashion.",
                "For the first query in MFi, agent Ai determines which subset of its neighboring agents to forward it to based on the agents routing policy πi.",
                "These routing decisions determine how the search process is conducted in the network.",
                "In this paper, we call Ai as Ajs upstream agent and Aj as Ais downstream agent if A4 A5 A6 A7 A2 A3 A9 NEI(A2)={A3} PAR(A2)={A1} CHL(A2)={A4,A5} A1 A8 Figure 1: A fraction of a hierarchical P2PIR system an agent Ai routes a query to agent Aj.",
                "The distributed search protocol of our hierarchical agent organization is composed of two steps.",
                "In the first step, upon receipt of a query qk at time tl from a user, agent Ai initiates a search session si by probing its neighboring agents Aj ∈ NEI(Ai) with the message PROBE for the similarity value Sim(qk, Aj) between qk and Aj.",
                "Here, Ai is defined as the query initiator of search session si.",
                "In the second step, Ai selects a group of the most promising agents to start the actual search process with the message SEARCH.",
                "These SEARCH messages contain a TTL (Time To Live) parameter in addition to the query.",
                "The TTL value decreases by 1 after each hop.",
                "In the search process, agents discard those queries that either have been previously processed or whose TTL drops to 0, which prevents queries from looping in the system forever.",
                "The search session ends when all the agents that receive the query drop it or TTL decreases to 0.",
                "Upon receipt of SEARCH messages for qk, agents schedule local activities including local searching, forwarding qk to their neighbors, and returning search results to the query initiator.",
                "This process and related algorithms are detailed in [15, 14]. 3.",
                "A BASIC REINFORCEMENTLEARNING BASED SEARCH APPROACH In the aforementioned distributed search algorithm, the routing decisions of an agent Ai rely on the similarity comparison between incoming queries and Ais neighboring agents in order to forward those queries to relevant agents without flooding the network with unnecessary query messages.",
                "However, this heuristic is myopic because a relevant direct neighbor is not necessarily connected to other relevant agents.",
                "In this section, we propose a more general approach by framing this problem as a reinforcement learning task.",
                "In pursuit of greater flexibility, agents can switch between two modes: learning mode and non-learning mode.",
                "In the non-learning mode, agents operate in the same way as they do in the normal distributed search processes described in [14, 15].",
                "On the other hand, in the learning mode, in parallel with distributed search sessions, agents also participate in a learning process which will be detailed in this section.",
                "Note that in the learning protocol, the learning process does not interfere with the distributed search process.",
                "Agents can choose to initiate and stop learning processes without affecting the system performance.",
                "In particular, since the learning process consumes network resources (especially bandwidth), agents can choose to initiate learning only when the network load is relatively low, thus minimizing the extra communication costs incurred by the learning algorithm.",
                "The section is structured as follows, Section 3.1 describes 232 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) a reinforcement learning based model.",
                "Section 3.2 describes a protocol to deploy the learning algorithm in the network.",
                "Section 3.3 discusses the convergence of the learning algorithm. 3.1 The Model An agents routing policy takes the state of a search session as input and output the routing actions for that query.",
                "In our work, the state of a search session sj is stipulated as: QSj = (qk, ttlj) where ttlj is the number of hops that remains for the search session sj , qk is the specific query.",
                "QL is an attribute of qk that indicates which type of queries qk most likely belong to.",
                "The set of QL can be generated by running a simple online classification algorithm on all the queries that have been processed by the agents, or an oﬄine algorithm on a pre-designated training set.",
                "The assumption here is that the set of query types is learned ahead of time and belongs to the common knowledge of the agents in the network.",
                "Future work includes exploring how learning can be accomplished when this assumption does not hold.",
                "Given the query types set, an incoming query qi can be classified to one query class Q(qi) by the formula: Q(qi) = arg max Qj P(qi|Qj) (1) where P(qi|Qj ) indicates the likelihood that the query qi is generated by the query class Qj [8].",
                "The set of atomic routing actions of an agent Ai is denoted as {αi}, where {αi} is defined as αi = {αi0 , αi1 , ..., αin }.",
                "An element αij represents an action to route a given query to the neighboring agent Aij ∈ DirectConn(Ai).",
                "The routing policy πi of agent Ai is stochastic and its outcome for a search session with state QSj is defined as: πi(QSj) = {(αi0 , πi(QSi, αi0 )), (αi1 , πi(QSi, αi1 )), ...} (2) Note that operator πi is overloaded to represent either the probabilistic policy for a search session with state QSj, denoted as πi(QSj); or the probability of forwarding the query to a specific neighboring agent Aik ∈ DirectConn(Ai) under the policy πi(QSj), denoted as πi(QSj, αik ).",
                "Therefore, equation (2) means that the probability of forwarding the search session to agent Ai0 is πi(QSi, αi0 ) and so on.",
                "Under this stochastic policy, the routing action is nondeterministic.",
                "The advantage of such a strategy is that the best neighboring agents will not be selected repeatedly, thereby mitigating the potential hot spots situations.",
                "The expected <br>utility</br>, Un i (QSj), is used to estimate the potential <br>utility</br> gain of routing query type QSj to agent Ai under policy πn i .",
                "The superscript n indicates the value at the nth iteration in an iterative learning process.",
                "The expected <br>utility</br> provides routing guidance for future search sessions.",
                "In the search process, each agent Ai maintains partial observations of its neighbors states, as shown in Fig. 2.",
                "The partial observation includes non-local information such as the potential <br>utility</br> estimation of its neighbor Am for query state QSj, denoted as Um(QSj), as well as the load information, Lm.",
                "These observations are updated periodically by the neighbors.",
                "The estimated <br>utility</br> information will be used to update Ais expected <br>utility</br> for its routing policy.",
                "Load Information Expected <br>utility</br> For Different Query Types Neighboring Agents ... A0 A1 A3 A2 Un 0 (QS0) ... ... ... ... ...... Un 0 (QS1) Un 1 (QS1) Un 2 (QS1) Un 3 (QS1) Un 1 (QS0) Un 2 (QS0) Un 3 (QS0) Ln 0 Ln 1 Ln 2 Ln 3 ... QS0 QS1 ...",
                "Figure 2: Agent Ais Partial Observation about its neighbors(A0, A1...) The load information of Am, Lm, is defined as Lm = |MFm| Cm , where |MFm| is the length of the message-forward queue and Cm is the service rate of agent Ams message-forward queue.",
                "Therefore Lm characterizes the utilization of an agents communication channel, and thus provide non-local information for Ams neighbors to adjust the parameters of their routing policy to avoid inundating their downstream agents.",
                "Note that based on the characteristics of the queries entering the system and agents capabilities, the loading of agents may not be uniform.",
                "After collecting the utilization rate information from all its neighbors, agent Ai computes Li as a single measure for assessing the average load condition of its neighborhood: Li = P k Lk |DirectConn(Ai)| Agents exploit Li value in determining the routing probability in its routing policy.",
                "Note that, as described in Section 3.2, information about neighboring agents is piggybacked with the query message propagated among the agents whenever possible to reduce the traffic overhead. 3.1.1 Update the Policy An iterative update process is introduced for agents to learn a satisfactory stochastic routing policy.",
                "In this iterative process, agents update their estimates on the potential <br>utility</br> of their current routing policies and then propagate the updated estimates to their neighbors.",
                "Their neighbors then generate a new routing policy based on the updated observation and in turn they calculate the expected <br>utility</br> based on the new policies and continue this iterative process.",
                "In particular, at time n, given a set of expected <br>utility</br>, an agent Ai, whose directly connected agents set is DirectConn(Ai) = {Ai0 , ..., Aim }, determines its corresponding stochastic routing policy for a search session of state QSj based on the following steps: (1) Ai first selects a subset of agents as the potential downstream agents from set DirectConn(Ai), denoted as PDn(Ai, QSj).",
                "The size of the potential downstream agent is specified as |PDn(Ai, QSj)| = min(|NEI(Ai), dn i + k)| where k is a constant and is set to 3 in this paper; dn i , the forward width, is defined as the expected number of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 233 neighboring agents that agent Ai can forward to at time n. This formula specifies that the potential downstream agent set PDn(Ai, QSj) is either the subset of neighboring agents with dn i + k highest expected <br>utility</br> value for state QSj among all the agents in DirectConn(Ai), or all their neighboring agents.",
                "The k is introduced based on the idea of a stochastic routing policy and it makes the forwarding probability of the dn i +k highest agent less than 100%.",
                "Note that if we want to limit the number of downstream agents for search session sj as 5, the probability of forwarding the query to all neighboring agents should add up to 5.",
                "Setting up dn i value properly can improve the utilization rate of the network bandwidth when much of the network is idle while mitigating the traffic load when the network is highly loaded.",
                "The dn+1 i value is updated based on dn i , the previous and current observations on the traffic situation in the neighborhood.",
                "Specifically, the update formula for dn+1 i is dn+1 i = dn i ∗ (1 + 1 − Li |DirectConn(Ai)| ) In this formula, the forward width is updated based on the traffic conditions of agent Ais neighborhood, i.e Li, and its previous value. (2) For each agent Aik in the PDn(Ai, QSj), the probability of forwarding the query to Aik is determined in the following way in order to assign higher forwarding probability to the neighboring agents with higher expected <br>utility</br> value: πn+1 i (QSj, αik ) = dn+1 i |PDn(Ai, QSj)| + β ∗ ` Uik (QSj) − PDU(Ai, QSj) |PDn(Ai, QSj)| ´ (3) where PDUn(Ai, QSj) = X o∈P Dn(Ai,QSj ) Uo(QSj) and QSj is the subsequent state of agent Aik after agent Ai forwards the search session with state QSj to its neighboring agent Aik ; If QSj = (qk, ttl0), then QSj = (qk, ttl0 − 1).",
                "In formula 3, the first term on the right of the equation, dn+1 i |P Dn(Ai,QSj )| , is used to to determine the forwarding probability by equally distributing the forward width, dn+1 i , to the agents in PDn(Ai, QSj) set.",
                "The second term is used to adjust the probability of being chosen so that agents with higher expected <br>utility</br> values will be favored. β is determined according to: β = min ` m − dn+1 i m ∗ umax − PDUn(Ai, QSj) , dn+1 i PDUn(Ai, QSj) − m ∗ umin ´ (4) where m = |PDn(Ai, QSj)|, umax = max o∈P Dn(Ao,QSj ) Uo(QSj) and umin = min o∈P Dn(Ao,QSj ) Uo(QSj) This formula guarantees that the final πn+1 i (QSj, αik ) value is well defined, i.e, 0 ≤ πn+1 i (QSj, αik ) ≤ 1 and X i πn+1 i (QSj, αik ) = dn+1 i However, such a solution does not explore all the possibilities.",
                "In order to balance between exploitation and exploration, a λ-Greedy approach is taken.",
                "In the λ-Greedy approach, in addition to assigning higher probability to those agents with higher expected <br>utility</br> value, as in the equation (3).",
                "Agents that appear to be not-so-good choices will also be sent queries based on a dynamic exploration rate.",
                "In particular, for agents in the set PDn(Ai, QSj), πn+1 i1 (QSj) is determined in the same way as the above, with the only difference being that dn+1 i is replaced with dn+1 i ∗ (1 − λn).",
                "The remaining search bandwidth is used for learning by assigning probability λn evenly to agents Ai2 in the set DirectConn(Ai) − PDn(Ai, QSj). πn+1 i2 (QSj, αik ) = dn+1 i ∗ λn |DirectConn(Ai) − PDn(Ai, QSj)| (5) where PDn(Ai, QSj) ⊂ DirectConn(Ai).",
                "Note that the exploration rate λ is not a constant and it decreases overtime.",
                "The λ is determined according to the following equation: λn+1 = λ0 ∗ e−c1n (6) where λ0 is the initial exploration rate, which is a constant; c1 is also a constant to adjust the decreasing rate of the exploration rate; n is the current time unit. 3.1.2 Update Expected <br>utility</br> Once the routing policy at step n+1, πn+1 i , is determined based on the above formula, agent Ai can update its own expected <br>utility</br>, Un+1 i (QSi), based on the the updated routing policy resulted from the formula 5 and the updated U values of its neighboring agents.",
                "Under the assumption that after a query is forwarded to Ais neighbors the subsequent search sessions are independent, the update formula is similar to the Bellman update formula in Q-Learning: Un+1 i (QSj) = (1 − θi) ∗ Un i (QSj) + θi ∗ (Rn+1 i (QSj) + X k πn+1 i (QSj, αik )Un k (QSj)) (7) where QSj = (Qj, ttl − 1) is the next state of QSj = (Qj, ttl); Rn+1 i (QSj) is the expected local reward for query class Qk at agent Ai under the routing policy πn+1 i ; θi is the coefficient for deciding how much weight is given to the old value during the update process: the smaller θi value is, the faster the agent is expected to learn the real value, while the greater volatility of the algorithm, and vice versa.",
                "Rn+1 (s) is updated according to the following equation: 234 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Rn+1 i (QSj) = Rn i (QSj) +γi ∗ (r(QSj) − Rn i (QSj)) ∗ P(qj|Qj ) (8) where r(QSj) is the local reward associated with the search session.",
                "P(qj|Qj ) indicates how relevant the query qj is to the query type Qj, and γi is the learning rate for agent Ai.",
                "Depending on the similarity between a specific query qi and its corresponding query type Qi, the local reward associated with the search session has different impact on the Rn i (QSj) estimation.",
                "In the above formula, this impact is reflected by the coefficient, the P(qj|Qj) value. 3.1.3 Reward function After a search session stops when its TTL values expires, all search results are returned back to the user and are compared against the relevance judgment.",
                "Assuming the set of search results is SR, the reward Rew(SR) is defined as: Rew(SR) = j 1 if |Rel(SR)| > c |Rel(SR)| c otherwise. where SR is the set of returned search results, Rel(SR) is the set of relevant documents in the search results.",
                "This equation specifies that users give 1.0 reward if the number of returned relevant documents reaches a predefined number c. Otherwise, the reward is in proportion to the number of relevant documents returned.",
                "This rationale for setting up such a cut-off value is that the importance of recall ratio decreases with the abundance of relevant documents in real world, therefore users tend to focus on only a limited number of searched results.",
                "The details of the actual routing protocol will be introduced in Section 3.2 when we introduce how the learning algorithm is deployed in real systems. 3.2 Deployment of the Learning algorithm This section describes how the learning algorithm can be used in either a single-phase or a two-phase search process.",
                "In the single-phase search algorithm, search sessions start from the initiators of the queries.",
                "In contrast, in the two-step search algorithm, the query initiator first attempts to seek a more appropriate starting point for the query by introducing an exploratory step as described in Section 2.",
                "Despite the difference in the quality of starting points, the major part of the learning process for the two algorithms is largely the same as described in the following paragraphs.",
                "Before learning starts, each agent initializes the expected <br>utility</br> value for all possible states as 0.",
                "Thereafter, upon receipt of a query, in addition to the normal operations described in the previous section, an agent Ai also sets up a timer to wait for the search results returned from its downstream agents.",
                "Once the timer expires or it has received response from all its downstream agents, Ai merges and forwards the search results accrued from its downstream agents to its upstream agent.",
                "Setting up the timer speeds up the learning because agents can avoid waiting too long for the downstream agents to return search results.",
                "Note that these detailed results and corresponding agent information will still be stored at Ai until the feedback information is passed from its upstream agent and the performance of its downstream agents can be evaluated.",
                "The duration of the timer is related to the TTL value.",
                "In this paper, we set the timer to ttimer = ttli ∗ 2 + tf , where ttli ∗ 2 is the sum of the travel time of the queries in the network, and tf is the expected time period that users would like to wait.",
                "The search results will eventually be returned to the search session initiator A0.",
                "They will be compared to the relevance judgment that is provided by the final users (as described in the experiment section, the relevance judgement for the query set is provided along with the data collections).",
                "The reward will be calculated and propagated backward to the agents along the way that search results were passed.",
                "This is a reverse process of the search results propagation.",
                "In the process of propagating reward backward, agents update estimates of their own potential <br>utility</br> value, generate an upto-dated policy and pass their updated results to the neighboring agents based on the algorithm described in Section 3.",
                "Upon change of expected <br>utility</br> value, agent Ai sends out its updated <br>utility</br> estimation to its neighbors so that they can act upon the changed expected utility and corresponding state.",
                "This update message includes the potential reward as well as the corresponding state QSi = (qk, ttll) of agent Ai.",
                "Each neighboring agent, Aj, reacts to this kind of update message by updating the expected <br>utility</br> value for state QSj(qk, ttll + 1) according to the newly-announced changed expected <br>utility</br> value.",
                "Once they complete the update, the agents would again in turn inform related neighbors to update their values.",
                "This process goes on until the TTL value in the update message increases to the TTL limit.",
                "To speed up the learning process, while updating the expected <br>utility</br> values of an agent Ais neighboring agents we specify that Um(Qk, ttl0) >= Um(Qk, ttl1) iff ttl0 > ttl1 Thus, when agent Ai receives an updated expected <br>utility</br> value with ttl1, it also updates the expected utility values with any ttl0 > ttl1 if Um(Qk, ttl0) < Um(Qk, ttl1) to speed up convergence.",
                "This heuristic is based on the fact that the <br>utility</br> of a search session is a non-decreasing function of time t. 3.3 Discussion In formalizing the content routing system as a learning task, many assumptions are made.",
                "In real systems, these assumptions may not hold, and thus the learning algorithm may not converge.",
                "Two problems are of particular note, (1) This content routing problem does not have Markov properties.",
                "In contrast to IP-level based packet routing, the routing decision of each agent for a particular search session sj depends on the routing history of sj.",
                "Therefore, the assumption that all subsequent search sessions are independent does not hold in reality.",
                "This may lead to double counting problem that the relevant documents of some agents will be counted more than once for the state where the TTL value is more than 1.",
                "However, in the context of the hierarchical agent organizations, two factors mitigate this problems: first, the agents in each content group form a tree-like structure.",
                "With the absense of the cycles, the estimates inside the tree would be close to the accurate value.",
                "Secondly, the stochastic nature of the routing policy partly remedies this problem.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 235 (2) Another challenge for this learning algorithm is that in a real network environment observations on neighboring agents may not be able to be updated in time due to the communication delay or other situations.",
                "In addition, when neighboring agents update their estimates at the same time, oscillation may arise during the learning process[1].",
                "This paper explores several approaches to speed up the learning process.",
                "Besides the aforementioned strategy of updating the expected <br>utility</br> values, we also employ an active update strategy where agents notify their neighbors whenever its expected <br>utility</br> is updated.",
                "Thus a faster convergence speed can be achieved.",
                "This strategy contrasts to the Lazy update, where agents only echo their neighboring agents with their expected <br>utility</br> change when they exchange information.",
                "The trade off between the two approaches is the network load versus learning speed.",
                "The advantage of this learning algorithm is that once a routing policy is learned, agents do not have to repeatedly compare the similarity of queries as long as the network topology remains unchanged.",
                "Instead, agent just have to determine the classification of the query properly and follow the learned policies.",
                "The disadvantage of this learning-based approach is that the learning process needs to be conducted whenever the network structure changes.",
                "There are many potential extensions for this learning model.",
                "For example, a single measure is currently used to indicate the traffic load for an agents neighborhood.",
                "A simple extension would be to keep track of individual load for each neighbor of the agent. 4.",
                "EXPERIMENTSSETTINGSAND RESULTS The experiments are conducted on TRANO simulation toolkit with two sets of datasets, TREC-VLC-921 and TREC123-100.",
                "The following sub-sections introduce the TRANO testbed, the datasets, and the experimental results. 4.1 TRANO Testbed TRANO (Task Routing on Agent Network Organization) is a multi-agent based network based information retrieval testbed.",
                "TRANO is built on top of the Farm [4], a time based distributed simulator that provides a data dissemination framework for large scale distributed agent network based organizations.",
                "TRANO supports importation and exportation of agent organization profiles including topological connections and other features.",
                "Each TRANO agent is composed of an agent view structure and a control unit.",
                "In simulation, each agent is pulsed regularly and the agent checks the incoming message queues, performs local operations and then forwards messages to other agents . 4.2 Experimental Settings In our experiment, we use two standard datasets, TRECVLC-921 and TREC-123-100 datasets, to simulate the collections hosted on agents.",
                "The TREC-VLC-921 and TREC123-100 datasets were created by the U.S. National Institute for Standard Technology(NIST) for its TREC conferences.",
                "In distributed information retrieval domain, the two data collections are split to 921 and 100 sub-collections.",
                "It is observed that dataset TREC-VLC-921 is more heterogeneous than TREC-123-100 in terms of source, document length, and relevant document distribution from the statistics of the two data collections listed in [13].",
                "Hence, TREC-VLC-921 is much closer to real document distributions in P2P environments.",
                "Furthermore, TREC-123-100 is split into two sets of 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0 500 1000 1500 2000 2500 3000 ARSS Query number ARSS versus the number of incoming queries for TREC-VLC-921 SSLA-921 SSNA-921 Figure 3: ARSS(Average reward per search session) versus the number of search sessions for 1phase search in TREC-VLC-921 0 0.1 0.2 0.3 0.4 0.5 0.6 0 500 1000 1500 2000 2500 3000 ARSS Query number ARSS versus query number for TREC-VLC-921 TSLA-921 TSNA-921 Figure 4: ARSS(Average reward per search session) versus the number of search sessions for 2phase search in TREC-VLC-921 sub-collections in two ways: randomly and by source.",
                "The two partitions are denoted as TREC-123-100-Random and TREC-123-100-Source respectively.",
                "The documents in each subcollection in dataset TREC-123-100-Source are more coherent than those in TREC-123-100-Random.",
                "The two different sets of partitions allow us to observe how the distributed learning algorithm is affected by the homogeneity of the collections.",
                "The hierarchical agent organization is generated by the algorithm described in our previous algorithm [15].",
                "During the topology generation process, degree information of each agent is estimated by the algorithm introduced by Palmer et al. [9] with parameters α = 0.5 and β = 0.6.",
                "In our experiments, we estimate the upward limit and downward degree limit using linear discount factors 0.5, 0.8 and 1.0.",
                "Once the topology is built, queries randomly selected from the query set 301−350 on TREC-VLC-921 and query set 1− 50 on TREC-123-100-Random and TREC-123-100-Source are injected to the system based on a Poisson distribution P(N(t) = n) = (λt)n n! e−λ 236 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 0 50 100 150 200 250 300 350 400 0 500 1000 1500 2000 2500 3000 Cumulativeutility Query number Cumulative <br>utility</br> over the number of incoming queries TSLA-921 SSNA-921 SSLA-921 TSNA-921 Figure 5: The cumulative <br>utility</br> versus the number of search sessions TREC-VLC-921 In addition, we assume that all agents have an equal chance of getting queries from the environment, i.e, λ is the same for every agent.",
                "In our experiments, λ is set as 0.0543 so that the mean of the incoming queries from the environment to the agent network is 50 per time unit.",
                "The service time for the communication queue and local search queue, i.e tQij and trs, is set as 0.01 time unit and 0.05 time units respectively.",
                "In our experiments, there are ten types of queries acquired by clustering the query set 301 − 350 and 1 − 50. 4.3 Results analysis and evaluation Figure 3 demonstrates the ARSS(Average Reward per Search Session) versus the number of incoming queries over time for the the Single-Step based Non-learning Algorithm (SSNA), and the Single-Step Learning Algorithm(SSLA) for data collection TREC-VLC-921.",
                "It shows that the average reward for SSNA algorithm ranges from 0.02 − 0.06 and the performance changes little over time.",
                "The average reward for SSLA approach starts at the same level with the SSNA algorithm.",
                "But the performance increases over time and the average performance gain stabilizes at about 25% after query range 2000 − 3000.",
                "Figure 4 shows the ARSS(Average Reward per Search Session) versus the number of incoming queries over time for the the Two-Step based Non-learning Algorithm(TSNA), and the Two-Step Learning Algorithm(TSLA) for data collection TREC-VLC-921.",
                "The TSNA approach has a relatively consistent performance with the average reward ranges from 0.05 − 0.15.",
                "The average reward for TSLA approach, where learning algorithm is exploited, starts at the same level with the TSNA algorithm and improves the average reward over time until 2000−2500 queries joining the system.",
                "The results show that the average performance gain for TSLA approach over TNLA approach is 35% after stabilization.",
                "Figure 5 shows the cumulative <br>utility</br> versus the number of incoming queries over time for SSNA, SSLA,TSNA, and TSLA respectively.",
                "It illustrates that the cumulative <br>utility</br> of non-learning algorithms increases largely linearly over time, while the gains of learning-based algorithms accelerate when more queries enter the system.",
                "These experimental results demonstrate that learning-based approaches consistently perform better than non-learning based routing algorithm.",
                "Moreover, two-phase learning based algorithm is better than single-phase based learning algorithm because the maximal reward an agent can receive from searching its neighborhood within TTL hops is related to the total number of the relevant documents in that area.",
                "Thus, even the optimal routing policy can do little beyond reaching these relevant documents faster.",
                "On the contrary, the two-stepbased learning algorithm can relocate the search session to a neighborhood with more relevant documents.",
                "The TSLA combines the merits of both approaches and outperforms them.",
                "Table 1 lists the cumulative <br>utility</br> for datasets TREC123-100-Random and TREC-123-100-Source with hierarchical organizations.",
                "The five columns show the results for four different approaches.",
                "In particular, column TSNA-Random shows the results for dataset TREC-123-100-Random with the TSNA approach.",
                "The column TSLA-Random shows the results for dataset TREC-123-100-Random with the TSLA approach.",
                "There are two numbers in each cell in the column TSLA-Random.",
                "The first number is the actual cumulative <br>utility</br> while the second number is the percentage gain in terms of the <br>utility</br> over TSNA approach.",
                "Columns TSNA-Source and TSLA-Source show the results for dataset TREC-123-100-Source with TSNA and TSLA approaches respectively.",
                "Table 1 shows that the performance improvement for TREC-123-100-Random is not as significant as the other datasets.",
                "This is because that the documents in the sub-collection of TREC-123-100-Random are selected randomly which makes the collection model, the signature of the collection, less meaningful.",
                "Since both algorithms are designed based on the assumption that document collections can be well represented by their collection model, this result is not surprising.",
                "Overall, Figures 4, 5, and Table 1 demonstrate that the reinforcement learning based approach can considerably enhance the system performance for both data collections.",
                "However, it remains as future work to discover the correlation between the magnitude of the performance gains and the size of the data collection and/or the extent of the heterogeneity between the sub-collections. 5.",
                "RELATED WORK The content routing problem differs from the networklevel routing in packet-switched communication networks in that content-based routing occurs in application-level networks.",
                "In addition, the destination agents in our contentrouting algorithms are multiple and the addresses are not known in the routing process.",
                "IP-level Routing problems have been attacked from the reinforcement learning perspective[2, 5, 11, 12].",
                "These studies have explored fully distributed algorithms that are able, without central coordination to disseminate knowledge about the network, to find the shortest paths robustly and efficiently in the face of changing network topologies and changing link costs.",
                "There are two major classes of adaptive, distributed packet routing algorithms in the literature: distance-vector algorithms and link-state algorithms.",
                "While this line of studies carry a certain similarity with our work, it has mainly focused on packet-switched communication networks.",
                "In this domain, the destination of a packet is deterministic and unique.",
                "Each agent maintains estimations, probabilistically or deterministically, on the distance to a certain destination through its neighbors.",
                "A variant of Q-Learning techniques is deployed The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 237 Table 1: Cumulative <br>utility</br> for Datasets TREC-123-100-Random and TREC-123-100-Source with Hierarchical Organization; The percentage numbers in the columns TSLA-Random and TSLA-Source demonstrate the performance gain over the algorithm without learning Query number TSNA-Random TSLA-Random TSNA-Source TSLA-Source 500 25.15 28.45 13% 24.00 21.05 -13% 1000 104.99 126.74 20% 93.95 96.44 2.6% 1250 149.79 168.40 12% 122.64 134.05 9.3% 1500 188.94 211.05 12% 155.30 189.60 22% 1750 235.49 261.60 11% 189.14 243.90 28% 2000 275.09 319.25 16% 219.0 278.80 26% to update the estimations to converge to the real distances.",
                "It has been discovered that the locality property is an important feature of information retrieval systems in user modeling studies[3].",
                "In P2P based content sharing systems, this property is exemplified by the phenomenon that users tend to send queries that represent only a limited number of topics and conversely, users in the same neighborhood are likely to share common interests and send similar queries [10].",
                "The learning based approach is perceived to be more beneficial for real distributed information retrieval systems which exhibit locality property.",
                "This is because the users traffic and query patterns can reduce the state space and speed up the learning process.",
                "Related work in taking advantage of this property include [7], where the authors attempted to address this problem by user modeling techniques. 6.",
                "CONCLUSIONS In this paper, a reinforcement-learning based approach is developed to improve the performance of distributed IR search algorithms.",
                "Particularly, agents maintain estimates, namely expected <br>utility</br>, on the downstream agents ability to provide relevant documents for incoming queries.",
                "These estimates are updated gradually by learning from the feedback information returned from previous search sessions.",
                "Based on the updated expected <br>utility</br> information, the agents modify their routing policies.",
                "Thereafter, these agents route the queries based on the learned policies and update the estimates on the expected <br>utility</br> based on the new routing policies.",
                "The experiments on two different distributed IR datasets illustrates that the reinforcement learning approach improves considerably the cumulative <br>utility</br> over time. 7.",
                "REFERENCES [1] S. Abdallah and V. Lesser.",
                "Learning the task allocation game.",
                "In AAMAS 06: Proceedings of the fifth international joint conference on Autonomous agents and multiagent systems, pages 850-857, New York, NY, USA, 2006.",
                "ACM Press. [2] J.",
                "A. Boyan and M. L. Littman.",
                "Packet routing in dynamically changing networks: A reinforcement learning approach.",
                "In Advances in Neural Information Processing Systems, volume 6, pages 671-678.",
                "Morgan Kaufmann Publishers, Inc., 1994. [3] J. C. French, A. L. Powell, J. P. Callan, C. L. Viles, T. Emmitt, K. J. Prey, and Y. Mou.",
                "Comparing the performance of database selection algorithms.",
                "In Research and Development in Information Retrieval, pages 238-245, 1999. [4] B. Horling, R. Mailler, and V. Lesser.",
                "Farm: A scalable environment for multi-agent development and evaluation.",
                "In Advances in Software Engineering for Multi-Agent Systems, pages 220-237, Berlin, 2004.",
                "Springer-Verlag. [5] M. Littman and J. Boyan.",
                "A distributed reinforcement learning scheme for network routing.",
                "In Proceedings of the International Workshop on Applications of Neural Networks to Telecommunications, 1993. [6] J. Lu and J. Callan.",
                "Federated search of text-based digital libraries in hierarchical peer-to-peer networks.",
                "In In ECIR05, 2005. [7] J. Lu and J. Callan.",
                "User modeling for full-text federated search in peer-to-peer networks.",
                "In ACM SIGIR 2006.",
                "ACM Press, 2006. [8] C. D. Manning and H. Sch¨utze.",
                "Foundations of Statistical Natural Language Processing.",
                "The MIT Press, Cambridge, Massachusetts, 1999. [9] C. R. Palmer and J. G. Steffan.",
                "Generating network topologies that obey power laws.",
                "In Proceedings of GLOBECOM 2000, November 2000. [10] K. Sripanidkulchai, B. Maggs, and H. Zhang.",
                "Efficient content location using interest-based locality in peer-topeer systems.",
                "In INFOCOM, 2003. [11] D. Subramanian, P. Druschel, and J. Chen.",
                "Ants and reinforcement learning: A case study in routing in dynamic networks.",
                "In In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence, pages 832-839, 1997. [12] J. N. Tao and L. Weaver.",
                "A multi-agent, policy gradient approach to network routing.",
                "In In Proceedings of the Eighteenth International Conference on Machine Learning, 2001. [13] H. Zhang, W. B. Croft, B. Levine, and V. Lesser.",
                "A multi-agent approach for peer-to-peer information retrieval.",
                "In Proceedings of Third International Joint Conference on Autonomous Agents and Multi-Agent Systems, July 2004. [14] H. Zhang and V. Lesser.",
                "Multi-agent based peer-to-peer information retrieval systems with concurrent search sessions.",
                "In Proceedings of the Fifth International Joint Conference on Autonomous Agents and Multi-Agent Systems, May 2006. [15] H. Zhang and V. R. Lesser.",
                "A dynamically formed hierarchical agent organization for a distributed content sharing system.",
                "In 2004 IEEE/WIC/ACM International Conference on Intelligent Agent Technology (IAT 2004), 20-24 September 2004, Beijing, China, pages 169-175.",
                "IEEE Computer Society, 2004. 238 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "Particularly, agents maintain estimates, namely expected <br>utility</br>, on the downstream agents capabilities of providing relevant documents for specific types of incoming queries.",
                "Based on the updated expected <br>utility</br> information, the agents derive corresponding routing policies.",
                "Thereafter, these agents route the queries based on the learned policies and update the estimates on the expected <br>utility</br> based on the new routing policies.",
                "It is a priority queue and agent Ai always selects the most promising queries to process in order to maximize the global <br>utility</br>.",
                "The expected <br>utility</br>, Un i (QSj), is used to estimate the potential <br>utility</br> gain of routing query type QSj to agent Ai under policy πn i ."
            ],
            "translated_annotated_samples": [
                "En particular, los agentes mantienen estimaciones, es decir, <br>utilidad esperada</br>, sobre las capacidades de los agentes aguas abajo de proporcionar documentos relevantes para tipos específicos de consultas entrantes.",
                "Basándose en la información actualizada de <br>utilidad</br> esperada, los agentes derivan políticas de enrutamiento correspondientes.",
                "Posteriormente, estos agentes dirigen las consultas basándose en las políticas aprendidas y actualizan las estimaciones sobre la <br>utilidad</br> esperada en función de las nuevas políticas de enrutamiento.",
                "Es una cola de prioridad y el agente Ai siempre selecciona las consultas más prometedoras para procesarlas con el fin de maximizar la <br>utilidad</br> global.",
                "La <br>utilidad</br> esperada, Un i (QSj), se utiliza para estimar la ganancia potencial de <br>utilidad</br> al dirigir el tipo de consulta QSj al agente Ai bajo la política πn i."
            ],
            "translated_text": "Un algoritmo de búsqueda distribuida basado en aprendizaje por refuerzo para sistemas jerárquicos de recuperación de información entre pares. En estos enfoques, los agentes dependen de la similitud de contenido entre las consultas entrantes y sus agentes vecinos directos para dirigir las sesiones de búsqueda distribuida. Sin embargo, tal heurística es miope en el sentido de que los agentes vecinos pueden no estar conectados con agentes más relevantes. En este documento, se desarrolla un enfoque basado en aprendizaje por refuerzo en línea para aprovechar las características dinámicas en tiempo de ejecución de los sistemas de recuperación de información P2P, representadas por información sobre sesiones de búsqueda pasadas. Específicamente, los agentes mantienen estimaciones sobre las habilidades de los agentes aguas abajo para proporcionar documentos relevantes para las consultas entrantes. Estas estimaciones se actualizan gradualmente al aprender de la información de retroalimentación devuelta de sesiones de búsqueda anteriores. Basándose en esta información, los agentes derivan políticas de enrutamiento correspondientes. Posteriormente, estos agentes dirigen las consultas basándose en las políticas aprendidas y actualizan las estimaciones según las nuevas políticas de enrutamiento. Los resultados experimentales demuestran que el algoritmo de aprendizaje mejora considerablemente el rendimiento del enrutamiento en dos conjuntos de colecciones de pruebas que han sido utilizados en una variedad de estudios de recuperación de información distribuida. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagente Términos Generales Algoritmos, Rendimiento, Experimentación 1. En los últimos años ha habido un creciente interés en estudiar cómo controlar los procesos de búsqueda en sistemas de recuperación de información (IR) basados en pares (P2P) [6, 13, 14, 15]. En esta línea de investigación, uno de los problemas centrales que preocupa a los investigadores es dirigir de manera eficiente las consultas de los usuarios en la red a los agentes que poseen los documentos apropiados. En ausencia de información global, las estrategias dominantes para abordar este problema son enfoques basados en la similitud de contenido [6, 13, 14, 15]. Si bien la similitud de contenido entre consultas y nodos locales parece ser un indicador creíble para la cantidad de documentos relevantes que residen en cada nodo, estos enfoques están limitados por una serie de factores. En primer lugar, las métricas basadas en similitud pueden ser miope ya que los nodos localmente relevantes pueden no estar conectados a otros nodos relevantes. Segundo, los enfoques basados en la similitud no tienen en cuenta las características de tiempo de ejecución de los sistemas de IR P2P, incluidos los parámetros ambientales, el uso del ancho de banda y la información histórica de las sesiones de búsqueda pasadas, que proporcionan información valiosa para los algoritmos de enrutamiento de consultas. En este artículo, desarrollamos un enfoque de IR basado en aprendizaje por refuerzo para mejorar el rendimiento de los algoritmos de búsqueda de IR distribuidos. Los agentes pueden adquirir mejores estrategias de búsqueda al recopilar y analizar información de retroalimentación de sesiones de búsqueda anteriores. En particular, los agentes mantienen estimaciones, es decir, <br>utilidad esperada</br>, sobre las capacidades de los agentes aguas abajo de proporcionar documentos relevantes para tipos específicos de consultas entrantes. Estas estimaciones se actualizan gradualmente al aprender de la información de retroalimentación devuelta de sesiones de búsqueda anteriores. Basándose en la información actualizada de <br>utilidad</br> esperada, los agentes derivan políticas de enrutamiento correspondientes. Posteriormente, estos agentes dirigen las consultas basándose en las políticas aprendidas y actualizan las estimaciones sobre la <br>utilidad</br> esperada en función de las nuevas políticas de enrutamiento. Este proceso se lleva a cabo de manera iterativa. El objetivo del algoritmo de aprendizaje, aunque consume ancho de banda de red, es reducir el tiempo de enrutamiento para procesar más consultas por unidad de tiempo y al mismo tiempo encontrar más documentos relevantes. Esto contrasta con los enfoques basados en la similitud de contenido, donde se repiten operaciones similares para cada consulta entrante y el tiempo de procesamiento se mantiene en gran medida constante con el tiempo. Otra forma de ver este documento es que nuestro enfoque básico para la búsqueda de IR distribuida es construir una red de superposición jerárquica (organización de agentes) basada en la medida de similitud de contenido entre las colecciones de documentos de los agentes de manera ascendente. En trabajos anteriores, hemos demostrado que esta organización mejora significativamente el rendimiento de búsqueda. Sin embargo, esta estructura organizativa no tiene en cuenta los patrones de llegada de las consultas, incluyendo su frecuencia, tipos y dónde entran en el sistema, ni el ancho de banda de comunicación disponible en la red y las capacidades de procesamiento de los agentes individuales. La intención del aprendizaje por refuerzo es adaptar las decisiones de enrutamiento de los agentes a las situaciones dinámicas de la red y aprender de sesiones de búsqueda pasadas. Específicamente, las contribuciones de este artículo incluyen: (1) un enfoque basado en aprendizaje por refuerzo para que los agentes adquieran políticas de enrutamiento satisfactorias basadas en estimaciones de la contribución potencial de sus agentes vecinos; (2) dos estrategias para acelerar el proceso de aprendizaje. Según nuestro mejor conocimiento, esta es una de las primeras aplicaciones de aprendizaje por refuerzo para abordar problemas de compartición de contenido distribuido, lo cual indica algunos de los problemas al aplicar el refuerzo en una aplicación compleja. El resto de este documento está organizado de la siguiente manera: la Sección 2 revisa los sistemas jerárquicos de intercambio de contenido y el algoritmo de búsqueda de dos fases basado en dicha topología. La Sección 3 describe un enfoque basado en aprendizaje por refuerzo para dirigir el proceso de enrutamiento; la Sección 4 detalla la configuración experimental y analiza los resultados. La sección 5 discute estudios relacionados y la sección 6 concluye el artículo. BÚSQUEDA EN SISTEMAS JERÁRQUICOS DE IR P2P Esta sección revisa brevemente nuestros enfoques básicos para los sistemas jerárquicos de IR P2P. En un sistema jerárquico de recuperación de información P2P ilustrado en la Fig.1, los agentes están conectados entre sí a través de tres tipos de enlaces: enlaces ascendentes, enlaces descendentes y enlaces laterales. En las siguientes secciones, denotamos el conjunto de agentes que están directamente conectados al agente Ai como DirectConn(Ai), el cual se define como DirectConn(Ai) = NEI(Ai) ∪ PAR(Ai) ∪ CHL(Ai), donde NEI(Ai) es el conjunto de agentes vecinos conectados a Ai a través de enlaces laterales; PAR(Ai) es el conjunto de agentes a los que el agente Ai está conectado a través de enlaces ascendentes y CHL(Ai) es el conjunto de agentes a los que el agente Ai se conecta a través de enlaces descendentes. Estos enlaces se establecen a través de un proceso de agrupamiento distribuido basado en la similitud de contenido de abajo hacia arriba[15]. Estos enlaces son luego utilizados por agentes para localizar otros agentes que contienen documentos relevantes a las consultas dadas. Un agente típico Ai en nuestro sistema utiliza dos colas: una cola de búsqueda local, LSi, y una cola de reenvío de mensajes MFi. Los estados de las dos colas constituyen los estados internos de un agente. La cola de búsqueda local LSi almacena sesiones de búsqueda programadas para procesamiento local. Es una cola de prioridad y el agente Ai siempre selecciona las consultas más prometedoras para procesarlas con el fin de maximizar la <br>utilidad</br> global. MFi consiste en un conjunto de consultas para reenviar y se procesa de manera FIFO (primero en entrar, primero en salir). Para la primera consulta en MFi, el agente Ai determina a qué subconjunto de sus agentes vecinos reenviarla basándose en la política de enrutamiento de los agentes πi. Estas decisiones de enrutamiento determinan cómo se lleva a cabo el proceso de búsqueda en la red. En este documento, llamamos a Ai como el agente aguas arriba de Ajs y a Aj como el agente aguas abajo de Ais si A4 A5 A6 A7 A2 A3 A9 NEI(A2)={A3} PAR(A2)={A1} CHL(A2)={A4,A5} A1 A8 Figura 1: Una fracción de un sistema jerárquico P2PIR en el que un agente Ai dirige una consulta al agente Aj. El protocolo de búsqueda distribuida de nuestra organización de agentes jerárquicos está compuesto por dos pasos. En el primer paso, al recibir una consulta qk en el tiempo tl de un usuario, el agente Ai inicia una sesión de búsqueda si al sondear a sus agentes vecinos Aj ∈ NEI(Ai) con el mensaje PROBE para el valor de similitud Sim(qk, Aj) entre qk y Aj. Aquí, Ai se define como el iniciador de la consulta de la sesión de búsqueda si. En el segundo paso, Ai selecciona un grupo de agentes más prometedores para iniciar el proceso de búsqueda real con el mensaje BUSCAR. Estos mensajes de BÚSQUEDA contienen un parámetro TTL (Tiempo de Vida) además de la consulta. El valor TTL disminuye en 1 después de cada salto. En el proceso de búsqueda, los agentes descartan aquellas consultas que han sido procesadas previamente o cuyo TTL disminuye a 0, lo que evita que las consultas se repitan infinitamente en el sistema. La sesión de búsqueda termina cuando todos los agentes que reciben la consulta la descartan o el TTL disminuye a 0. Tras recibir mensajes de BÚSQUEDA para qk, los agentes programan actividades locales que incluyen búsqueda local, reenvío de qk a sus vecinos y devolución de resultados de búsqueda al iniciador de la consulta. Este proceso y los algoritmos relacionados se detallan en [15, 14]. 3. En el enfoque de búsqueda basado en aprendizaje por refuerzo, mencionado anteriormente, las decisiones de enrutamiento de un agente Ai dependen de la comparación de similitud entre las consultas entrantes y los agentes vecinos de Ai para reenviar esas consultas a agentes relevantes sin inundar la red con mensajes de consulta innecesarios. Sin embargo, esta heurística es miope porque un vecino directo relevante no necesariamente está conectado con otros agentes relevantes. En esta sección, proponemos un enfoque más general al plantear este problema como una tarea de aprendizaje por refuerzo. En busca de una mayor flexibilidad, los agentes pueden alternar entre dos modos: modo de aprendizaje y modo sin aprendizaje. En el modo de no aprendizaje, los agentes operan de la misma manera que lo hacen en los procesos de búsqueda distribuida normales descritos en [14, 15]. Por otro lado, en el modo de aprendizaje, en paralelo con las sesiones de búsqueda distribuida, los agentes también participan en un proceso de aprendizaje que se detallará en esta sección. Ten en cuenta que en el protocolo de aprendizaje, el proceso de aprendizaje no interfiere con el proceso de búsqueda distribuida. Los agentes pueden elegir iniciar y detener procesos de aprendizaje sin afectar el rendimiento del sistema. En particular, dado que el proceso de aprendizaje consume recursos de red (especialmente ancho de banda), los agentes pueden optar por iniciar el aprendizaje solo cuando la carga de la red es relativamente baja, minimizando así los costos adicionales de comunicación incurridos por el algoritmo de aprendizaje. La sección está estructurada de la siguiente manera, la Sección 3.1 describe 232 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) un modelo basado en aprendizaje por refuerzo. La sección 3.2 describe un protocolo para implementar el algoritmo de aprendizaje en la red. La sección 3.3 discute la convergencia del algoritmo de aprendizaje. 3.1 El Modelo La política de enrutamiento de un agente toma el estado de una sesión de búsqueda como entrada y produce las acciones de enrutamiento para esa consulta. En nuestro trabajo, el estado de una sesión de búsqueda sj se estipula como: QSj = (qk, ttlj) donde ttlj es el número de saltos que quedan para la sesión de búsqueda sj, qk es la consulta específica. QL es un atributo de qk que indica a qué tipo de consultas qk probablemente pertenecen. El conjunto de QL puede ser generado ejecutando un algoritmo de clasificación en línea simple en todas las consultas que han sido procesadas por los agentes, o un algoritmo fuera de línea en un conjunto de entrenamiento pre-designado. La suposición aquí es que el conjunto de tipos de consultas se aprende de antemano y pertenece al conocimiento común de los agentes en la red. El trabajo futuro incluye explorar cómo se puede lograr el aprendizaje cuando esta suposición no se cumple. Dado el conjunto de tipos de consultas establecido, una consulta entrante qi puede ser clasificada en una clase de consulta Q(qi) mediante la fórmula: Q(qi) = arg max Qj P(qi|Qj) (1) donde P(qi|Qj) indica la probabilidad de que la consulta qi sea generada por la clase de consulta Qj [8]. El conjunto de acciones de enrutamiento atómico de un agente Ai se denota como {αi}, donde {αi} se define como αi = {αi0 , αi1 , ..., αin }. Un elemento αij representa una acción para dirigir una consulta dada al agente vecino Aij ∈ DirectConn(Ai). La política de enrutamiento πi del agente Ai es estocástica y su resultado para una sesión de búsqueda con estado QSj se define como: πi(QSj) = {(αi0 , πi(QSi, αi0 )), (αi1 , πi(QSi, αi1 )), ...} (2) Nótese que el operador πi está sobrecargado para representar tanto la política probabilística para una sesión de búsqueda con estado QSj, denotada como πi(QSj); como la probabilidad de enviar la consulta a un agente vecino específico Aik ∈ DirectConn(Ai) bajo la política πi(QSj), denotada como πi(QSj, αik). Por lo tanto, la ecuación (2) significa que la probabilidad de reenviar la sesión de búsqueda al agente Ai0 es πi(QSi, αi0) y así sucesivamente. Bajo esta política estocástica, la acción de enrutamiento es no determinística. La ventaja de esta estrategia es que los mejores agentes vecinos no serán seleccionados repetidamente, mitigando así las posibles situaciones de puntos calientes. La <br>utilidad</br> esperada, Un i (QSj), se utiliza para estimar la ganancia potencial de <br>utilidad</br> al dirigir el tipo de consulta QSj al agente Ai bajo la política πn i. ",
            "candidates": [],
            "error": [
                [
                    "utilidad esperada",
                    "utilidad",
                    "utilidad",
                    "utilidad",
                    "utilidad",
                    "utilidad"
                ]
            ]
        },
        "network": {
            "translated_key": "red",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Reinforcement Learning based Distributed Search Algorithm For Hierarchical Peer-to-Peer Information Retrieval Systems Haizheng Zhang College of Information Science and Technology Pennsylvania State University University Park, PA 16803 hzhang@ist.psu.edu Victor Lesser Department of Computer Science University Of Massachusetts Amherst, MA 01003 lesser@cs.umass.edu ABSTRACT The dominant existing routing strategies employed in peerto-peer(P2P) based information retrieval(IR) systems are similarity-based approaches.",
                "In these approaches, agents depend on the content similarity between incoming queries and their direct neighboring agents to direct the distributed search sessions.",
                "However, such a heuristic is myopic in that the neighboring agents may not be connected to more relevant agents.",
                "In this paper, an online reinforcement-learning based approach is developed to take advantage of the dynamic run-time characteristics of P2P IR systems as represented by information about past search sessions.",
                "Specifically, agents maintain estimates on the downstream agents abilities to provide relevant documents for incoming queries.",
                "These estimates are updated gradually by learning from the feedback information returned from previous search sessions.",
                "Based on this information, the agents derive corresponding routing policies.",
                "Thereafter, these agents route the queries based on the learned policies and update the estimates based on the new routing policies.",
                "Experimental results demonstrate that the learning algorithm improves considerably the routing performance on two test collection sets that have been used in a variety of distributed IR studies.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Algorithms, Performance, Experimentation 1.",
                "INTRODUCTION Over the last few years there have been increasing interests in studying how to control the search processes in peer-to-peer(P2P) based information retrieval(IR) systems [6, 13, 14, 15].",
                "In this line of research, one of the core problems that concerns researchers is to efficiently route user queries in the <br>network</br> to agents that are in possession of appropriate documents.",
                "In the absence of global information, the dominant strategies in addressing this problem are content-similarity based approaches [6, 13, 14, 15].",
                "While the content similarity between queries and local nodes appears to be a creditable indicator for the number of relevant documents residing on each node, these approaches are limited by a number of factors.",
                "First of all, similaritybased metrics can be myopic since locally relevant nodes may not be connected to other relevant nodes.",
                "Second, the similarity-based approaches do not take into account the run-time characteristics of the P2P IR systems, including environmental parameters, bandwidth usage, and the historical information of the past search sessions, that provide valuable information for the query routing algorithms.",
                "In this paper, we develop a reinforcement learning based IR approach for improving the performance of distributed IR search algorithms.",
                "Agents can acquire better search strategies by collecting and analyzing feedback information from previous search sessions.",
                "Particularly, agents maintain estimates, namely expected utility, on the downstream agents capabilities of providing relevant documents for specific types of incoming queries.",
                "These estimates are updated gradually by learning from the feedback information returned from previous search sessions.",
                "Based on the updated expected utility information, the agents derive corresponding routing policies.",
                "Thereafter, these agents route the queries based on the learned policies and update the estimates on the expected utility based on the new routing policies.",
                "This process is conducted in an iterative manner.",
                "The goal of the learning algorithm, even though it consumes some <br>network</br> bandwidth, is to shorten the routing time so that more queries are processed per time unit while at the same time finding more relevant documents.",
                "This contrasts with the content-similarity based approaches where similar operations are repeated for every incoming query and the processing time keeps largely constant over time.",
                "Another way of viewing this paper is that our basic approach to distributed IR search is to construct a hierarchical overlay <br>network</br>(agent organization) based on the contentsimilarity measure among agents document collections in a bottom-up fashion.",
                "In the past work, we have shown that this organization improves search performance significantly.",
                "However, this organizational structure does not take into account the arrival patterns of queries, including their frequency, types, and where they enter the system, nor the available communication bandwidth of the <br>network</br> and processing capabilities of individual agents.",
                "The intention of the reinforcement learning is to adapt the agents routing decisions to the dynamic <br>network</br> situations and learn from past search sessions.",
                "Specifically, the contributions of this paper include: (1) a reinforcement learning based approach for agents to acquire satisfactory routing policies based on estimates of the potential contribution of their neighboring agents; (2) two strategies to speed up the learning process.",
                "To our best knowledge, this is one of the first reinforcement learning applications in addressing distributed content sharing problems and it is indicative of some of the issues in applying reinforcement in a complex application.",
                "The remainder of this paper is organized as follows: Section 2 reviews the hierarchical content sharing systems and the two-phase search algorithm based on such topology.",
                "Section 3 describes a reinforcement learning based approach to direct the routing process; Section 4 details the experimental settings and analyze the results.",
                "Section 5 discusses related studies and Section 6 concludes the paper. 2.",
                "SEARCH IN HIERARCHICAL P2P IR SYSTEMS This section briefly reviews our basic approaches to hierarchical P2P IR systems.",
                "In a hierarchical P2P IR system illustrated in Fig.1, agents are connected to each other through three types of links: upward links, downward links, and lateral links.",
                "In the following sections, we denote the set of agents that are directly connected to agent Ai as DirectConn(Ai), which is defined as DirectConn(Ai) = NEI(Ai) ∪ PAR(Ai) ∪ CHL(Ai) , where NEI(Ai) is the set of neighboring agents connected to Ai through lateral links; PAR(Ai) is the set of agents whom agent Ai is connected to through upward links and CHL(Ai) is the set of agents that agent Ai connects to through downward links.",
                "These links are established through a bottom-up content-similarity based distributed clustering process[15].",
                "These links are then used by agents to locate other agents that contain documents relevant to the given queries.",
                "A typical agent Ai in our system uses two queues: a local search queue, LSi, and a message forwarding queue MFi.",
                "The states of the two queues constitute the internal states of an agent.",
                "The local search queue LSi stores search sessions that are scheduled for local processing.",
                "It is a priority queue and agent Ai always selects the most promising queries to process in order to maximize the global utility.",
                "MFi consists of a set of queries to forward on and is processed in a FIFO (first in first out) fashion.",
                "For the first query in MFi, agent Ai determines which subset of its neighboring agents to forward it to based on the agents routing policy πi.",
                "These routing decisions determine how the search process is conducted in the <br>network</br>.",
                "In this paper, we call Ai as Ajs upstream agent and Aj as Ais downstream agent if A4 A5 A6 A7 A2 A3 A9 NEI(A2)={A3} PAR(A2)={A1} CHL(A2)={A4,A5} A1 A8 Figure 1: A fraction of a hierarchical P2PIR system an agent Ai routes a query to agent Aj.",
                "The distributed search protocol of our hierarchical agent organization is composed of two steps.",
                "In the first step, upon receipt of a query qk at time tl from a user, agent Ai initiates a search session si by probing its neighboring agents Aj ∈ NEI(Ai) with the message PROBE for the similarity value Sim(qk, Aj) between qk and Aj.",
                "Here, Ai is defined as the query initiator of search session si.",
                "In the second step, Ai selects a group of the most promising agents to start the actual search process with the message SEARCH.",
                "These SEARCH messages contain a TTL (Time To Live) parameter in addition to the query.",
                "The TTL value decreases by 1 after each hop.",
                "In the search process, agents discard those queries that either have been previously processed or whose TTL drops to 0, which prevents queries from looping in the system forever.",
                "The search session ends when all the agents that receive the query drop it or TTL decreases to 0.",
                "Upon receipt of SEARCH messages for qk, agents schedule local activities including local searching, forwarding qk to their neighbors, and returning search results to the query initiator.",
                "This process and related algorithms are detailed in [15, 14]. 3.",
                "A BASIC REINFORCEMENTLEARNING BASED SEARCH APPROACH In the aforementioned distributed search algorithm, the routing decisions of an agent Ai rely on the similarity comparison between incoming queries and Ais neighboring agents in order to forward those queries to relevant agents without flooding the <br>network</br> with unnecessary query messages.",
                "However, this heuristic is myopic because a relevant direct neighbor is not necessarily connected to other relevant agents.",
                "In this section, we propose a more general approach by framing this problem as a reinforcement learning task.",
                "In pursuit of greater flexibility, agents can switch between two modes: learning mode and non-learning mode.",
                "In the non-learning mode, agents operate in the same way as they do in the normal distributed search processes described in [14, 15].",
                "On the other hand, in the learning mode, in parallel with distributed search sessions, agents also participate in a learning process which will be detailed in this section.",
                "Note that in the learning protocol, the learning process does not interfere with the distributed search process.",
                "Agents can choose to initiate and stop learning processes without affecting the system performance.",
                "In particular, since the learning process consumes <br>network</br> resources (especially bandwidth), agents can choose to initiate learning only when the <br>network</br> load is relatively low, thus minimizing the extra communication costs incurred by the learning algorithm.",
                "The section is structured as follows, Section 3.1 describes 232 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) a reinforcement learning based model.",
                "Section 3.2 describes a protocol to deploy the learning algorithm in the <br>network</br>.",
                "Section 3.3 discusses the convergence of the learning algorithm. 3.1 The Model An agents routing policy takes the state of a search session as input and output the routing actions for that query.",
                "In our work, the state of a search session sj is stipulated as: QSj = (qk, ttlj) where ttlj is the number of hops that remains for the search session sj , qk is the specific query.",
                "QL is an attribute of qk that indicates which type of queries qk most likely belong to.",
                "The set of QL can be generated by running a simple online classification algorithm on all the queries that have been processed by the agents, or an oﬄine algorithm on a pre-designated training set.",
                "The assumption here is that the set of query types is learned ahead of time and belongs to the common knowledge of the agents in the <br>network</br>.",
                "Future work includes exploring how learning can be accomplished when this assumption does not hold.",
                "Given the query types set, an incoming query qi can be classified to one query class Q(qi) by the formula: Q(qi) = arg max Qj P(qi|Qj) (1) where P(qi|Qj ) indicates the likelihood that the query qi is generated by the query class Qj [8].",
                "The set of atomic routing actions of an agent Ai is denoted as {αi}, where {αi} is defined as αi = {αi0 , αi1 , ..., αin }.",
                "An element αij represents an action to route a given query to the neighboring agent Aij ∈ DirectConn(Ai).",
                "The routing policy πi of agent Ai is stochastic and its outcome for a search session with state QSj is defined as: πi(QSj) = {(αi0 , πi(QSi, αi0 )), (αi1 , πi(QSi, αi1 )), ...} (2) Note that operator πi is overloaded to represent either the probabilistic policy for a search session with state QSj, denoted as πi(QSj); or the probability of forwarding the query to a specific neighboring agent Aik ∈ DirectConn(Ai) under the policy πi(QSj), denoted as πi(QSj, αik ).",
                "Therefore, equation (2) means that the probability of forwarding the search session to agent Ai0 is πi(QSi, αi0 ) and so on.",
                "Under this stochastic policy, the routing action is nondeterministic.",
                "The advantage of such a strategy is that the best neighboring agents will not be selected repeatedly, thereby mitigating the potential hot spots situations.",
                "The expected utility, Un i (QSj), is used to estimate the potential utility gain of routing query type QSj to agent Ai under policy πn i .",
                "The superscript n indicates the value at the nth iteration in an iterative learning process.",
                "The expected utility provides routing guidance for future search sessions.",
                "In the search process, each agent Ai maintains partial observations of its neighbors states, as shown in Fig. 2.",
                "The partial observation includes non-local information such as the potential utility estimation of its neighbor Am for query state QSj, denoted as Um(QSj), as well as the load information, Lm.",
                "These observations are updated periodically by the neighbors.",
                "The estimated utility information will be used to update Ais expected utility for its routing policy.",
                "Load Information Expected Utility For Different Query Types Neighboring Agents ... A0 A1 A3 A2 Un 0 (QS0) ... ... ... ... ...... Un 0 (QS1) Un 1 (QS1) Un 2 (QS1) Un 3 (QS1) Un 1 (QS0) Un 2 (QS0) Un 3 (QS0) Ln 0 Ln 1 Ln 2 Ln 3 ... QS0 QS1 ...",
                "Figure 2: Agent Ais Partial Observation about its neighbors(A0, A1...) The load information of Am, Lm, is defined as Lm = |MFm| Cm , where |MFm| is the length of the message-forward queue and Cm is the service rate of agent Ams message-forward queue.",
                "Therefore Lm characterizes the utilization of an agents communication channel, and thus provide non-local information for Ams neighbors to adjust the parameters of their routing policy to avoid inundating their downstream agents.",
                "Note that based on the characteristics of the queries entering the system and agents capabilities, the loading of agents may not be uniform.",
                "After collecting the utilization rate information from all its neighbors, agent Ai computes Li as a single measure for assessing the average load condition of its neighborhood: Li = P k Lk |DirectConn(Ai)| Agents exploit Li value in determining the routing probability in its routing policy.",
                "Note that, as described in Section 3.2, information about neighboring agents is piggybacked with the query message propagated among the agents whenever possible to reduce the traffic overhead. 3.1.1 Update the Policy An iterative update process is introduced for agents to learn a satisfactory stochastic routing policy.",
                "In this iterative process, agents update their estimates on the potential utility of their current routing policies and then propagate the updated estimates to their neighbors.",
                "Their neighbors then generate a new routing policy based on the updated observation and in turn they calculate the expected utility based on the new policies and continue this iterative process.",
                "In particular, at time n, given a set of expected utility, an agent Ai, whose directly connected agents set is DirectConn(Ai) = {Ai0 , ..., Aim }, determines its corresponding stochastic routing policy for a search session of state QSj based on the following steps: (1) Ai first selects a subset of agents as the potential downstream agents from set DirectConn(Ai), denoted as PDn(Ai, QSj).",
                "The size of the potential downstream agent is specified as |PDn(Ai, QSj)| = min(|NEI(Ai), dn i + k)| where k is a constant and is set to 3 in this paper; dn i , the forward width, is defined as the expected number of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 233 neighboring agents that agent Ai can forward to at time n. This formula specifies that the potential downstream agent set PDn(Ai, QSj) is either the subset of neighboring agents with dn i + k highest expected utility value for state QSj among all the agents in DirectConn(Ai), or all their neighboring agents.",
                "The k is introduced based on the idea of a stochastic routing policy and it makes the forwarding probability of the dn i +k highest agent less than 100%.",
                "Note that if we want to limit the number of downstream agents for search session sj as 5, the probability of forwarding the query to all neighboring agents should add up to 5.",
                "Setting up dn i value properly can improve the utilization rate of the <br>network</br> bandwidth when much of the <br>network</br> is idle while mitigating the traffic load when the network is highly loaded.",
                "The dn+1 i value is updated based on dn i , the previous and current observations on the traffic situation in the neighborhood.",
                "Specifically, the update formula for dn+1 i is dn+1 i = dn i ∗ (1 + 1 − Li |DirectConn(Ai)| ) In this formula, the forward width is updated based on the traffic conditions of agent Ais neighborhood, i.e Li, and its previous value. (2) For each agent Aik in the PDn(Ai, QSj), the probability of forwarding the query to Aik is determined in the following way in order to assign higher forwarding probability to the neighboring agents with higher expected utility value: πn+1 i (QSj, αik ) = dn+1 i |PDn(Ai, QSj)| + β ∗ ` Uik (QSj) − PDU(Ai, QSj) |PDn(Ai, QSj)| ´ (3) where PDUn(Ai, QSj) = X o∈P Dn(Ai,QSj ) Uo(QSj) and QSj is the subsequent state of agent Aik after agent Ai forwards the search session with state QSj to its neighboring agent Aik ; If QSj = (qk, ttl0), then QSj = (qk, ttl0 − 1).",
                "In formula 3, the first term on the right of the equation, dn+1 i |P Dn(Ai,QSj )| , is used to to determine the forwarding probability by equally distributing the forward width, dn+1 i , to the agents in PDn(Ai, QSj) set.",
                "The second term is used to adjust the probability of being chosen so that agents with higher expected utility values will be favored. β is determined according to: β = min ` m − dn+1 i m ∗ umax − PDUn(Ai, QSj) , dn+1 i PDUn(Ai, QSj) − m ∗ umin ´ (4) where m = |PDn(Ai, QSj)|, umax = max o∈P Dn(Ao,QSj ) Uo(QSj) and umin = min o∈P Dn(Ao,QSj ) Uo(QSj) This formula guarantees that the final πn+1 i (QSj, αik ) value is well defined, i.e, 0 ≤ πn+1 i (QSj, αik ) ≤ 1 and X i πn+1 i (QSj, αik ) = dn+1 i However, such a solution does not explore all the possibilities.",
                "In order to balance between exploitation and exploration, a λ-Greedy approach is taken.",
                "In the λ-Greedy approach, in addition to assigning higher probability to those agents with higher expected utility value, as in the equation (3).",
                "Agents that appear to be not-so-good choices will also be sent queries based on a dynamic exploration rate.",
                "In particular, for agents in the set PDn(Ai, QSj), πn+1 i1 (QSj) is determined in the same way as the above, with the only difference being that dn+1 i is replaced with dn+1 i ∗ (1 − λn).",
                "The remaining search bandwidth is used for learning by assigning probability λn evenly to agents Ai2 in the set DirectConn(Ai) − PDn(Ai, QSj). πn+1 i2 (QSj, αik ) = dn+1 i ∗ λn |DirectConn(Ai) − PDn(Ai, QSj)| (5) where PDn(Ai, QSj) ⊂ DirectConn(Ai).",
                "Note that the exploration rate λ is not a constant and it decreases overtime.",
                "The λ is determined according to the following equation: λn+1 = λ0 ∗ e−c1n (6) where λ0 is the initial exploration rate, which is a constant; c1 is also a constant to adjust the decreasing rate of the exploration rate; n is the current time unit. 3.1.2 Update Expected Utility Once the routing policy at step n+1, πn+1 i , is determined based on the above formula, agent Ai can update its own expected utility, Un+1 i (QSi), based on the the updated routing policy resulted from the formula 5 and the updated U values of its neighboring agents.",
                "Under the assumption that after a query is forwarded to Ais neighbors the subsequent search sessions are independent, the update formula is similar to the Bellman update formula in Q-Learning: Un+1 i (QSj) = (1 − θi) ∗ Un i (QSj) + θi ∗ (Rn+1 i (QSj) + X k πn+1 i (QSj, αik )Un k (QSj)) (7) where QSj = (Qj, ttl − 1) is the next state of QSj = (Qj, ttl); Rn+1 i (QSj) is the expected local reward for query class Qk at agent Ai under the routing policy πn+1 i ; θi is the coefficient for deciding how much weight is given to the old value during the update process: the smaller θi value is, the faster the agent is expected to learn the real value, while the greater volatility of the algorithm, and vice versa.",
                "Rn+1 (s) is updated according to the following equation: 234 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Rn+1 i (QSj) = Rn i (QSj) +γi ∗ (r(QSj) − Rn i (QSj)) ∗ P(qj|Qj ) (8) where r(QSj) is the local reward associated with the search session.",
                "P(qj|Qj ) indicates how relevant the query qj is to the query type Qj, and γi is the learning rate for agent Ai.",
                "Depending on the similarity between a specific query qi and its corresponding query type Qi, the local reward associated with the search session has different impact on the Rn i (QSj) estimation.",
                "In the above formula, this impact is reflected by the coefficient, the P(qj|Qj) value. 3.1.3 Reward function After a search session stops when its TTL values expires, all search results are returned back to the user and are compared against the relevance judgment.",
                "Assuming the set of search results is SR, the reward Rew(SR) is defined as: Rew(SR) = j 1 if |Rel(SR)| > c |Rel(SR)| c otherwise. where SR is the set of returned search results, Rel(SR) is the set of relevant documents in the search results.",
                "This equation specifies that users give 1.0 reward if the number of returned relevant documents reaches a predefined number c. Otherwise, the reward is in proportion to the number of relevant documents returned.",
                "This rationale for setting up such a cut-off value is that the importance of recall ratio decreases with the abundance of relevant documents in real world, therefore users tend to focus on only a limited number of searched results.",
                "The details of the actual routing protocol will be introduced in Section 3.2 when we introduce how the learning algorithm is deployed in real systems. 3.2 Deployment of the Learning algorithm This section describes how the learning algorithm can be used in either a single-phase or a two-phase search process.",
                "In the single-phase search algorithm, search sessions start from the initiators of the queries.",
                "In contrast, in the two-step search algorithm, the query initiator first attempts to seek a more appropriate starting point for the query by introducing an exploratory step as described in Section 2.",
                "Despite the difference in the quality of starting points, the major part of the learning process for the two algorithms is largely the same as described in the following paragraphs.",
                "Before learning starts, each agent initializes the expected utility value for all possible states as 0.",
                "Thereafter, upon receipt of a query, in addition to the normal operations described in the previous section, an agent Ai also sets up a timer to wait for the search results returned from its downstream agents.",
                "Once the timer expires or it has received response from all its downstream agents, Ai merges and forwards the search results accrued from its downstream agents to its upstream agent.",
                "Setting up the timer speeds up the learning because agents can avoid waiting too long for the downstream agents to return search results.",
                "Note that these detailed results and corresponding agent information will still be stored at Ai until the feedback information is passed from its upstream agent and the performance of its downstream agents can be evaluated.",
                "The duration of the timer is related to the TTL value.",
                "In this paper, we set the timer to ttimer = ttli ∗ 2 + tf , where ttli ∗ 2 is the sum of the travel time of the queries in the <br>network</br>, and tf is the expected time period that users would like to wait.",
                "The search results will eventually be returned to the search session initiator A0.",
                "They will be compared to the relevance judgment that is provided by the final users (as described in the experiment section, the relevance judgement for the query set is provided along with the data collections).",
                "The reward will be calculated and propagated backward to the agents along the way that search results were passed.",
                "This is a reverse process of the search results propagation.",
                "In the process of propagating reward backward, agents update estimates of their own potential utility value, generate an upto-dated policy and pass their updated results to the neighboring agents based on the algorithm described in Section 3.",
                "Upon change of expected utility value, agent Ai sends out its updated utility estimation to its neighbors so that they can act upon the changed expected utility and corresponding state.",
                "This update message includes the potential reward as well as the corresponding state QSi = (qk, ttll) of agent Ai.",
                "Each neighboring agent, Aj, reacts to this kind of update message by updating the expected utility value for state QSj(qk, ttll + 1) according to the newly-announced changed expected utility value.",
                "Once they complete the update, the agents would again in turn inform related neighbors to update their values.",
                "This process goes on until the TTL value in the update message increases to the TTL limit.",
                "To speed up the learning process, while updating the expected utility values of an agent Ais neighboring agents we specify that Um(Qk, ttl0) >= Um(Qk, ttl1) iff ttl0 > ttl1 Thus, when agent Ai receives an updated expected utility value with ttl1, it also updates the expected utility values with any ttl0 > ttl1 if Um(Qk, ttl0) < Um(Qk, ttl1) to speed up convergence.",
                "This heuristic is based on the fact that the utility of a search session is a non-decreasing function of time t. 3.3 Discussion In formalizing the content routing system as a learning task, many assumptions are made.",
                "In real systems, these assumptions may not hold, and thus the learning algorithm may not converge.",
                "Two problems are of particular note, (1) This content routing problem does not have Markov properties.",
                "In contrast to IP-level based packet routing, the routing decision of each agent for a particular search session sj depends on the routing history of sj.",
                "Therefore, the assumption that all subsequent search sessions are independent does not hold in reality.",
                "This may lead to double counting problem that the relevant documents of some agents will be counted more than once for the state where the TTL value is more than 1.",
                "However, in the context of the hierarchical agent organizations, two factors mitigate this problems: first, the agents in each content group form a tree-like structure.",
                "With the absense of the cycles, the estimates inside the tree would be close to the accurate value.",
                "Secondly, the stochastic nature of the routing policy partly remedies this problem.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 235 (2) Another challenge for this learning algorithm is that in a real <br>network</br> environment observations on neighboring agents may not be able to be updated in time due to the communication delay or other situations.",
                "In addition, when neighboring agents update their estimates at the same time, oscillation may arise during the learning process[1].",
                "This paper explores several approaches to speed up the learning process.",
                "Besides the aforementioned strategy of updating the expected utility values, we also employ an active update strategy where agents notify their neighbors whenever its expected utility is updated.",
                "Thus a faster convergence speed can be achieved.",
                "This strategy contrasts to the Lazy update, where agents only echo their neighboring agents with their expected utility change when they exchange information.",
                "The trade off between the two approaches is the <br>network</br> load versus learning speed.",
                "The advantage of this learning algorithm is that once a routing policy is learned, agents do not have to repeatedly compare the similarity of queries as long as the <br>network</br> topology remains unchanged.",
                "Instead, agent just have to determine the classification of the query properly and follow the learned policies.",
                "The disadvantage of this learning-based approach is that the learning process needs to be conducted whenever the <br>network</br> structure changes.",
                "There are many potential extensions for this learning model.",
                "For example, a single measure is currently used to indicate the traffic load for an agents neighborhood.",
                "A simple extension would be to keep track of individual load for each neighbor of the agent. 4.",
                "EXPERIMENTSSETTINGSAND RESULTS The experiments are conducted on TRANO simulation toolkit with two sets of datasets, TREC-VLC-921 and TREC123-100.",
                "The following sub-sections introduce the TRANO testbed, the datasets, and the experimental results. 4.1 TRANO Testbed TRANO (Task Routing on Agent <br>network</br> Organization) is a multi-agent based <br>network</br> based information retrieval testbed.",
                "TRANO is built on top of the Farm [4], a time based distributed simulator that provides a data dissemination framework for large scale distributed agent <br>network</br> based organizations.",
                "TRANO supports importation and exportation of agent organization profiles including topological connections and other features.",
                "Each TRANO agent is composed of an agent view structure and a control unit.",
                "In simulation, each agent is pulsed regularly and the agent checks the incoming message queues, performs local operations and then forwards messages to other agents . 4.2 Experimental Settings In our experiment, we use two standard datasets, TRECVLC-921 and TREC-123-100 datasets, to simulate the collections hosted on agents.",
                "The TREC-VLC-921 and TREC123-100 datasets were created by the U.S. National Institute for Standard Technology(NIST) for its TREC conferences.",
                "In distributed information retrieval domain, the two data collections are split to 921 and 100 sub-collections.",
                "It is observed that dataset TREC-VLC-921 is more heterogeneous than TREC-123-100 in terms of source, document length, and relevant document distribution from the statistics of the two data collections listed in [13].",
                "Hence, TREC-VLC-921 is much closer to real document distributions in P2P environments.",
                "Furthermore, TREC-123-100 is split into two sets of 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0 500 1000 1500 2000 2500 3000 ARSS Query number ARSS versus the number of incoming queries for TREC-VLC-921 SSLA-921 SSNA-921 Figure 3: ARSS(Average reward per search session) versus the number of search sessions for 1phase search in TREC-VLC-921 0 0.1 0.2 0.3 0.4 0.5 0.6 0 500 1000 1500 2000 2500 3000 ARSS Query number ARSS versus query number for TREC-VLC-921 TSLA-921 TSNA-921 Figure 4: ARSS(Average reward per search session) versus the number of search sessions for 2phase search in TREC-VLC-921 sub-collections in two ways: randomly and by source.",
                "The two partitions are denoted as TREC-123-100-Random and TREC-123-100-Source respectively.",
                "The documents in each subcollection in dataset TREC-123-100-Source are more coherent than those in TREC-123-100-Random.",
                "The two different sets of partitions allow us to observe how the distributed learning algorithm is affected by the homogeneity of the collections.",
                "The hierarchical agent organization is generated by the algorithm described in our previous algorithm [15].",
                "During the topology generation process, degree information of each agent is estimated by the algorithm introduced by Palmer et al. [9] with parameters α = 0.5 and β = 0.6.",
                "In our experiments, we estimate the upward limit and downward degree limit using linear discount factors 0.5, 0.8 and 1.0.",
                "Once the topology is built, queries randomly selected from the query set 301−350 on TREC-VLC-921 and query set 1− 50 on TREC-123-100-Random and TREC-123-100-Source are injected to the system based on a Poisson distribution P(N(t) = n) = (λt)n n! e−λ 236 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 0 50 100 150 200 250 300 350 400 0 500 1000 1500 2000 2500 3000 Cumulativeutility Query number Cumulative utility over the number of incoming queries TSLA-921 SSNA-921 SSLA-921 TSNA-921 Figure 5: The cumulative utility versus the number of search sessions TREC-VLC-921 In addition, we assume that all agents have an equal chance of getting queries from the environment, i.e, λ is the same for every agent.",
                "In our experiments, λ is set as 0.0543 so that the mean of the incoming queries from the environment to the agent <br>network</br> is 50 per time unit.",
                "The service time for the communication queue and local search queue, i.e tQij and trs, is set as 0.01 time unit and 0.05 time units respectively.",
                "In our experiments, there are ten types of queries acquired by clustering the query set 301 − 350 and 1 − 50. 4.3 Results analysis and evaluation Figure 3 demonstrates the ARSS(Average Reward per Search Session) versus the number of incoming queries over time for the the Single-Step based Non-learning Algorithm (SSNA), and the Single-Step Learning Algorithm(SSLA) for data collection TREC-VLC-921.",
                "It shows that the average reward for SSNA algorithm ranges from 0.02 − 0.06 and the performance changes little over time.",
                "The average reward for SSLA approach starts at the same level with the SSNA algorithm.",
                "But the performance increases over time and the average performance gain stabilizes at about 25% after query range 2000 − 3000.",
                "Figure 4 shows the ARSS(Average Reward per Search Session) versus the number of incoming queries over time for the the Two-Step based Non-learning Algorithm(TSNA), and the Two-Step Learning Algorithm(TSLA) for data collection TREC-VLC-921.",
                "The TSNA approach has a relatively consistent performance with the average reward ranges from 0.05 − 0.15.",
                "The average reward for TSLA approach, where learning algorithm is exploited, starts at the same level with the TSNA algorithm and improves the average reward over time until 2000−2500 queries joining the system.",
                "The results show that the average performance gain for TSLA approach over TNLA approach is 35% after stabilization.",
                "Figure 5 shows the cumulative utility versus the number of incoming queries over time for SSNA, SSLA,TSNA, and TSLA respectively.",
                "It illustrates that the cumulative utility of non-learning algorithms increases largely linearly over time, while the gains of learning-based algorithms accelerate when more queries enter the system.",
                "These experimental results demonstrate that learning-based approaches consistently perform better than non-learning based routing algorithm.",
                "Moreover, two-phase learning based algorithm is better than single-phase based learning algorithm because the maximal reward an agent can receive from searching its neighborhood within TTL hops is related to the total number of the relevant documents in that area.",
                "Thus, even the optimal routing policy can do little beyond reaching these relevant documents faster.",
                "On the contrary, the two-stepbased learning algorithm can relocate the search session to a neighborhood with more relevant documents.",
                "The TSLA combines the merits of both approaches and outperforms them.",
                "Table 1 lists the cumulative utility for datasets TREC123-100-Random and TREC-123-100-Source with hierarchical organizations.",
                "The five columns show the results for four different approaches.",
                "In particular, column TSNA-Random shows the results for dataset TREC-123-100-Random with the TSNA approach.",
                "The column TSLA-Random shows the results for dataset TREC-123-100-Random with the TSLA approach.",
                "There are two numbers in each cell in the column TSLA-Random.",
                "The first number is the actual cumulative utility while the second number is the percentage gain in terms of the utility over TSNA approach.",
                "Columns TSNA-Source and TSLA-Source show the results for dataset TREC-123-100-Source with TSNA and TSLA approaches respectively.",
                "Table 1 shows that the performance improvement for TREC-123-100-Random is not as significant as the other datasets.",
                "This is because that the documents in the sub-collection of TREC-123-100-Random are selected randomly which makes the collection model, the signature of the collection, less meaningful.",
                "Since both algorithms are designed based on the assumption that document collections can be well represented by their collection model, this result is not surprising.",
                "Overall, Figures 4, 5, and Table 1 demonstrate that the reinforcement learning based approach can considerably enhance the system performance for both data collections.",
                "However, it remains as future work to discover the correlation between the magnitude of the performance gains and the size of the data collection and/or the extent of the heterogeneity between the sub-collections. 5.",
                "RELATED WORK The content routing problem differs from the networklevel routing in packet-switched communication networks in that content-based routing occurs in application-level networks.",
                "In addition, the destination agents in our contentrouting algorithms are multiple and the addresses are not known in the routing process.",
                "IP-level Routing problems have been attacked from the reinforcement learning perspective[2, 5, 11, 12].",
                "These studies have explored fully distributed algorithms that are able, without central coordination to disseminate knowledge about the <br>network</br>, to find the shortest paths robustly and efficiently in the face of changing <br>network</br> topologies and changing link costs.",
                "There are two major classes of adaptive, distributed packet routing algorithms in the literature: distance-vector algorithms and link-state algorithms.",
                "While this line of studies carry a certain similarity with our work, it has mainly focused on packet-switched communication networks.",
                "In this domain, the destination of a packet is deterministic and unique.",
                "Each agent maintains estimations, probabilistically or deterministically, on the distance to a certain destination through its neighbors.",
                "A variant of Q-Learning techniques is deployed The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 237 Table 1: Cumulative Utility for Datasets TREC-123-100-Random and TREC-123-100-Source with Hierarchical Organization; The percentage numbers in the columns TSLA-Random and TSLA-Source demonstrate the performance gain over the algorithm without learning Query number TSNA-Random TSLA-Random TSNA-Source TSLA-Source 500 25.15 28.45 13% 24.00 21.05 -13% 1000 104.99 126.74 20% 93.95 96.44 2.6% 1250 149.79 168.40 12% 122.64 134.05 9.3% 1500 188.94 211.05 12% 155.30 189.60 22% 1750 235.49 261.60 11% 189.14 243.90 28% 2000 275.09 319.25 16% 219.0 278.80 26% to update the estimations to converge to the real distances.",
                "It has been discovered that the locality property is an important feature of information retrieval systems in user modeling studies[3].",
                "In P2P based content sharing systems, this property is exemplified by the phenomenon that users tend to send queries that represent only a limited number of topics and conversely, users in the same neighborhood are likely to share common interests and send similar queries [10].",
                "The learning based approach is perceived to be more beneficial for real distributed information retrieval systems which exhibit locality property.",
                "This is because the users traffic and query patterns can reduce the state space and speed up the learning process.",
                "Related work in taking advantage of this property include [7], where the authors attempted to address this problem by user modeling techniques. 6.",
                "CONCLUSIONS In this paper, a reinforcement-learning based approach is developed to improve the performance of distributed IR search algorithms.",
                "Particularly, agents maintain estimates, namely expected utility, on the downstream agents ability to provide relevant documents for incoming queries.",
                "These estimates are updated gradually by learning from the feedback information returned from previous search sessions.",
                "Based on the updated expected utility information, the agents modify their routing policies.",
                "Thereafter, these agents route the queries based on the learned policies and update the estimates on the expected utility based on the new routing policies.",
                "The experiments on two different distributed IR datasets illustrates that the reinforcement learning approach improves considerably the cumulative utility over time. 7.",
                "REFERENCES [1] S. Abdallah and V. Lesser.",
                "Learning the task allocation game.",
                "In AAMAS 06: Proceedings of the fifth international joint conference on Autonomous agents and multiagent systems, pages 850-857, New York, NY, USA, 2006.",
                "ACM Press. [2] J.",
                "A. Boyan and M. L. Littman.",
                "Packet routing in dynamically changing networks: A reinforcement learning approach.",
                "In Advances in Neural Information Processing Systems, volume 6, pages 671-678.",
                "Morgan Kaufmann Publishers, Inc., 1994. [3] J. C. French, A. L. Powell, J. P. Callan, C. L. Viles, T. Emmitt, K. J. Prey, and Y. Mou.",
                "Comparing the performance of database selection algorithms.",
                "In Research and Development in Information Retrieval, pages 238-245, 1999. [4] B. Horling, R. Mailler, and V. Lesser.",
                "Farm: A scalable environment for multi-agent development and evaluation.",
                "In Advances in Software Engineering for Multi-Agent Systems, pages 220-237, Berlin, 2004.",
                "Springer-Verlag. [5] M. Littman and J. Boyan.",
                "A distributed reinforcement learning scheme for <br>network</br> routing.",
                "In Proceedings of the International Workshop on Applications of Neural Networks to Telecommunications, 1993. [6] J. Lu and J. Callan.",
                "Federated search of text-based digital libraries in hierarchical peer-to-peer networks.",
                "In In ECIR05, 2005. [7] J. Lu and J. Callan.",
                "User modeling for full-text federated search in peer-to-peer networks.",
                "In ACM SIGIR 2006.",
                "ACM Press, 2006. [8] C. D. Manning and H. Sch¨utze.",
                "Foundations of Statistical Natural Language Processing.",
                "The MIT Press, Cambridge, Massachusetts, 1999. [9] C. R. Palmer and J. G. Steffan.",
                "Generating <br>network</br> topologies that obey power laws.",
                "In Proceedings of GLOBECOM 2000, November 2000. [10] K. Sripanidkulchai, B. Maggs, and H. Zhang.",
                "Efficient content location using interest-based locality in peer-topeer systems.",
                "In INFOCOM, 2003. [11] D. Subramanian, P. Druschel, and J. Chen.",
                "Ants and reinforcement learning: A case study in routing in dynamic networks.",
                "In In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence, pages 832-839, 1997. [12] J. N. Tao and L. Weaver.",
                "A multi-agent, policy gradient approach to <br>network</br> routing.",
                "In In Proceedings of the Eighteenth International Conference on Machine Learning, 2001. [13] H. Zhang, W. B. Croft, B. Levine, and V. Lesser.",
                "A multi-agent approach for peer-to-peer information retrieval.",
                "In Proceedings of Third International Joint Conference on Autonomous Agents and Multi-Agent Systems, July 2004. [14] H. Zhang and V. Lesser.",
                "Multi-agent based peer-to-peer information retrieval systems with concurrent search sessions.",
                "In Proceedings of the Fifth International Joint Conference on Autonomous Agents and Multi-Agent Systems, May 2006. [15] H. Zhang and V. R. Lesser.",
                "A dynamically formed hierarchical agent organization for a distributed content sharing system.",
                "In 2004 IEEE/WIC/ACM International Conference on Intelligent Agent Technology (IAT 2004), 20-24 September 2004, Beijing, China, pages 169-175.",
                "IEEE Computer Society, 2004. 238 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "In this line of research, one of the core problems that concerns researchers is to efficiently route user queries in the <br>network</br> to agents that are in possession of appropriate documents.",
                "The goal of the learning algorithm, even though it consumes some <br>network</br> bandwidth, is to shorten the routing time so that more queries are processed per time unit while at the same time finding more relevant documents.",
                "Another way of viewing this paper is that our basic approach to distributed IR search is to construct a hierarchical overlay <br>network</br>(agent organization) based on the contentsimilarity measure among agents document collections in a bottom-up fashion.",
                "However, this organizational structure does not take into account the arrival patterns of queries, including their frequency, types, and where they enter the system, nor the available communication bandwidth of the <br>network</br> and processing capabilities of individual agents.",
                "The intention of the reinforcement learning is to adapt the agents routing decisions to the dynamic <br>network</br> situations and learn from past search sessions."
            ],
            "translated_annotated_samples": [
                "En esta línea de investigación, uno de los problemas centrales que preocupa a los investigadores es dirigir de manera eficiente las consultas de los usuarios en la <br>red</br> a los agentes que poseen los documentos apropiados.",
                "El objetivo del algoritmo de aprendizaje, aunque consume <br>ancho de banda de red</br>, es reducir el tiempo de enrutamiento para procesar más consultas por unidad de tiempo y al mismo tiempo encontrar más documentos relevantes.",
                "Otra forma de ver este documento es que nuestro enfoque básico para la búsqueda de IR distribuida es construir una <br>red de superposición jerárquica</br> (organización de agentes) basada en la medida de similitud de contenido entre las colecciones de documentos de los agentes de manera ascendente.",
                "Sin embargo, esta estructura organizativa no tiene en cuenta los patrones de llegada de las consultas, incluyendo su frecuencia, tipos y dónde entran en el sistema, ni el ancho de banda de comunicación disponible en la <br>red</br> y las capacidades de procesamiento de los agentes individuales.",
                "La intención del aprendizaje por refuerzo es adaptar las decisiones de enrutamiento de los agentes a las situaciones dinámicas de la <br>red</br> y aprender de sesiones de búsqueda pasadas."
            ],
            "translated_text": "Un algoritmo de búsqueda distribuida basado en aprendizaje por refuerzo para sistemas jerárquicos de recuperación de información entre pares. En estos enfoques, los agentes dependen de la similitud de contenido entre las consultas entrantes y sus agentes vecinos directos para dirigir las sesiones de búsqueda distribuida. Sin embargo, tal heurística es miope en el sentido de que los agentes vecinos pueden no estar conectados con agentes más relevantes. En este documento, se desarrolla un enfoque basado en aprendizaje por refuerzo en línea para aprovechar las características dinámicas en tiempo de ejecución de los sistemas de recuperación de información P2P, representadas por información sobre sesiones de búsqueda pasadas. Específicamente, los agentes mantienen estimaciones sobre las habilidades de los agentes aguas abajo para proporcionar documentos relevantes para las consultas entrantes. Estas estimaciones se actualizan gradualmente al aprender de la información de retroalimentación devuelta de sesiones de búsqueda anteriores. Basándose en esta información, los agentes derivan políticas de enrutamiento correspondientes. Posteriormente, estos agentes dirigen las consultas basándose en las políticas aprendidas y actualizan las estimaciones según las nuevas políticas de enrutamiento. Los resultados experimentales demuestran que el algoritmo de aprendizaje mejora considerablemente el rendimiento del enrutamiento en dos conjuntos de colecciones de pruebas que han sido utilizados en una variedad de estudios de recuperación de información distribuida. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagente Términos Generales Algoritmos, Rendimiento, Experimentación 1. En los últimos años ha habido un creciente interés en estudiar cómo controlar los procesos de búsqueda en sistemas de recuperación de información (IR) basados en pares (P2P) [6, 13, 14, 15]. En esta línea de investigación, uno de los problemas centrales que preocupa a los investigadores es dirigir de manera eficiente las consultas de los usuarios en la <br>red</br> a los agentes que poseen los documentos apropiados. En ausencia de información global, las estrategias dominantes para abordar este problema son enfoques basados en la similitud de contenido [6, 13, 14, 15]. Si bien la similitud de contenido entre consultas y nodos locales parece ser un indicador creíble para la cantidad de documentos relevantes que residen en cada nodo, estos enfoques están limitados por una serie de factores. En primer lugar, las métricas basadas en similitud pueden ser miope ya que los nodos localmente relevantes pueden no estar conectados a otros nodos relevantes. Segundo, los enfoques basados en la similitud no tienen en cuenta las características de tiempo de ejecución de los sistemas de IR P2P, incluidos los parámetros ambientales, el uso del ancho de banda y la información histórica de las sesiones de búsqueda pasadas, que proporcionan información valiosa para los algoritmos de enrutamiento de consultas. En este artículo, desarrollamos un enfoque de IR basado en aprendizaje por refuerzo para mejorar el rendimiento de los algoritmos de búsqueda de IR distribuidos. Los agentes pueden adquirir mejores estrategias de búsqueda al recopilar y analizar información de retroalimentación de sesiones de búsqueda anteriores. En particular, los agentes mantienen estimaciones, es decir, utilidad esperada, sobre las capacidades de los agentes aguas abajo de proporcionar documentos relevantes para tipos específicos de consultas entrantes. Estas estimaciones se actualizan gradualmente al aprender de la información de retroalimentación devuelta de sesiones de búsqueda anteriores. Basándose en la información actualizada de utilidad esperada, los agentes derivan políticas de enrutamiento correspondientes. Posteriormente, estos agentes dirigen las consultas basándose en las políticas aprendidas y actualizan las estimaciones sobre la utilidad esperada en función de las nuevas políticas de enrutamiento. Este proceso se lleva a cabo de manera iterativa. El objetivo del algoritmo de aprendizaje, aunque consume <br>ancho de banda de red</br>, es reducir el tiempo de enrutamiento para procesar más consultas por unidad de tiempo y al mismo tiempo encontrar más documentos relevantes. Esto contrasta con los enfoques basados en la similitud de contenido, donde se repiten operaciones similares para cada consulta entrante y el tiempo de procesamiento se mantiene en gran medida constante con el tiempo. Otra forma de ver este documento es que nuestro enfoque básico para la búsqueda de IR distribuida es construir una <br>red de superposición jerárquica</br> (organización de agentes) basada en la medida de similitud de contenido entre las colecciones de documentos de los agentes de manera ascendente. En trabajos anteriores, hemos demostrado que esta organización mejora significativamente el rendimiento de búsqueda. Sin embargo, esta estructura organizativa no tiene en cuenta los patrones de llegada de las consultas, incluyendo su frecuencia, tipos y dónde entran en el sistema, ni el ancho de banda de comunicación disponible en la <br>red</br> y las capacidades de procesamiento de los agentes individuales. La intención del aprendizaje por refuerzo es adaptar las decisiones de enrutamiento de los agentes a las situaciones dinámicas de la <br>red</br> y aprender de sesiones de búsqueda pasadas. ",
            "candidates": [],
            "error": [
                [
                    "red",
                    "ancho de banda de red",
                    "red de superposición jerárquica",
                    "red",
                    "red"
                ]
            ]
        },
        "learning algorithm": {
            "translated_key": "algoritmo de aprendizaje",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Reinforcement Learning based Distributed Search Algorithm For Hierarchical Peer-to-Peer Information Retrieval Systems Haizheng Zhang College of Information Science and Technology Pennsylvania State University University Park, PA 16803 hzhang@ist.psu.edu Victor Lesser Department of Computer Science University Of Massachusetts Amherst, MA 01003 lesser@cs.umass.edu ABSTRACT The dominant existing routing strategies employed in peerto-peer(P2P) based information retrieval(IR) systems are similarity-based approaches.",
                "In these approaches, agents depend on the content similarity between incoming queries and their direct neighboring agents to direct the distributed search sessions.",
                "However, such a heuristic is myopic in that the neighboring agents may not be connected to more relevant agents.",
                "In this paper, an online reinforcement-learning based approach is developed to take advantage of the dynamic run-time characteristics of P2P IR systems as represented by information about past search sessions.",
                "Specifically, agents maintain estimates on the downstream agents abilities to provide relevant documents for incoming queries.",
                "These estimates are updated gradually by learning from the feedback information returned from previous search sessions.",
                "Based on this information, the agents derive corresponding routing policies.",
                "Thereafter, these agents route the queries based on the learned policies and update the estimates based on the new routing policies.",
                "Experimental results demonstrate that the <br>learning algorithm</br> improves considerably the routing performance on two test collection sets that have been used in a variety of distributed IR studies.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Algorithms, Performance, Experimentation 1.",
                "INTRODUCTION Over the last few years there have been increasing interests in studying how to control the search processes in peer-to-peer(P2P) based information retrieval(IR) systems [6, 13, 14, 15].",
                "In this line of research, one of the core problems that concerns researchers is to efficiently route user queries in the network to agents that are in possession of appropriate documents.",
                "In the absence of global information, the dominant strategies in addressing this problem are content-similarity based approaches [6, 13, 14, 15].",
                "While the content similarity between queries and local nodes appears to be a creditable indicator for the number of relevant documents residing on each node, these approaches are limited by a number of factors.",
                "First of all, similaritybased metrics can be myopic since locally relevant nodes may not be connected to other relevant nodes.",
                "Second, the similarity-based approaches do not take into account the run-time characteristics of the P2P IR systems, including environmental parameters, bandwidth usage, and the historical information of the past search sessions, that provide valuable information for the query routing algorithms.",
                "In this paper, we develop a reinforcement learning based IR approach for improving the performance of distributed IR search algorithms.",
                "Agents can acquire better search strategies by collecting and analyzing feedback information from previous search sessions.",
                "Particularly, agents maintain estimates, namely expected utility, on the downstream agents capabilities of providing relevant documents for specific types of incoming queries.",
                "These estimates are updated gradually by learning from the feedback information returned from previous search sessions.",
                "Based on the updated expected utility information, the agents derive corresponding routing policies.",
                "Thereafter, these agents route the queries based on the learned policies and update the estimates on the expected utility based on the new routing policies.",
                "This process is conducted in an iterative manner.",
                "The goal of the <br>learning algorithm</br>, even though it consumes some network bandwidth, is to shorten the routing time so that more queries are processed per time unit while at the same time finding more relevant documents.",
                "This contrasts with the content-similarity based approaches where similar operations are repeated for every incoming query and the processing time keeps largely constant over time.",
                "Another way of viewing this paper is that our basic approach to distributed IR search is to construct a hierarchical overlay network(agent organization) based on the contentsimilarity measure among agents document collections in a bottom-up fashion.",
                "In the past work, we have shown that this organization improves search performance significantly.",
                "However, this organizational structure does not take into account the arrival patterns of queries, including their frequency, types, and where they enter the system, nor the available communication bandwidth of the network and processing capabilities of individual agents.",
                "The intention of the reinforcement learning is to adapt the agents routing decisions to the dynamic network situations and learn from past search sessions.",
                "Specifically, the contributions of this paper include: (1) a reinforcement learning based approach for agents to acquire satisfactory routing policies based on estimates of the potential contribution of their neighboring agents; (2) two strategies to speed up the learning process.",
                "To our best knowledge, this is one of the first reinforcement learning applications in addressing distributed content sharing problems and it is indicative of some of the issues in applying reinforcement in a complex application.",
                "The remainder of this paper is organized as follows: Section 2 reviews the hierarchical content sharing systems and the two-phase search algorithm based on such topology.",
                "Section 3 describes a reinforcement learning based approach to direct the routing process; Section 4 details the experimental settings and analyze the results.",
                "Section 5 discusses related studies and Section 6 concludes the paper. 2.",
                "SEARCH IN HIERARCHICAL P2P IR SYSTEMS This section briefly reviews our basic approaches to hierarchical P2P IR systems.",
                "In a hierarchical P2P IR system illustrated in Fig.1, agents are connected to each other through three types of links: upward links, downward links, and lateral links.",
                "In the following sections, we denote the set of agents that are directly connected to agent Ai as DirectConn(Ai), which is defined as DirectConn(Ai) = NEI(Ai) ∪ PAR(Ai) ∪ CHL(Ai) , where NEI(Ai) is the set of neighboring agents connected to Ai through lateral links; PAR(Ai) is the set of agents whom agent Ai is connected to through upward links and CHL(Ai) is the set of agents that agent Ai connects to through downward links.",
                "These links are established through a bottom-up content-similarity based distributed clustering process[15].",
                "These links are then used by agents to locate other agents that contain documents relevant to the given queries.",
                "A typical agent Ai in our system uses two queues: a local search queue, LSi, and a message forwarding queue MFi.",
                "The states of the two queues constitute the internal states of an agent.",
                "The local search queue LSi stores search sessions that are scheduled for local processing.",
                "It is a priority queue and agent Ai always selects the most promising queries to process in order to maximize the global utility.",
                "MFi consists of a set of queries to forward on and is processed in a FIFO (first in first out) fashion.",
                "For the first query in MFi, agent Ai determines which subset of its neighboring agents to forward it to based on the agents routing policy πi.",
                "These routing decisions determine how the search process is conducted in the network.",
                "In this paper, we call Ai as Ajs upstream agent and Aj as Ais downstream agent if A4 A5 A6 A7 A2 A3 A9 NEI(A2)={A3} PAR(A2)={A1} CHL(A2)={A4,A5} A1 A8 Figure 1: A fraction of a hierarchical P2PIR system an agent Ai routes a query to agent Aj.",
                "The distributed search protocol of our hierarchical agent organization is composed of two steps.",
                "In the first step, upon receipt of a query qk at time tl from a user, agent Ai initiates a search session si by probing its neighboring agents Aj ∈ NEI(Ai) with the message PROBE for the similarity value Sim(qk, Aj) between qk and Aj.",
                "Here, Ai is defined as the query initiator of search session si.",
                "In the second step, Ai selects a group of the most promising agents to start the actual search process with the message SEARCH.",
                "These SEARCH messages contain a TTL (Time To Live) parameter in addition to the query.",
                "The TTL value decreases by 1 after each hop.",
                "In the search process, agents discard those queries that either have been previously processed or whose TTL drops to 0, which prevents queries from looping in the system forever.",
                "The search session ends when all the agents that receive the query drop it or TTL decreases to 0.",
                "Upon receipt of SEARCH messages for qk, agents schedule local activities including local searching, forwarding qk to their neighbors, and returning search results to the query initiator.",
                "This process and related algorithms are detailed in [15, 14]. 3.",
                "A BASIC REINFORCEMENTLEARNING BASED SEARCH APPROACH In the aforementioned distributed search algorithm, the routing decisions of an agent Ai rely on the similarity comparison between incoming queries and Ais neighboring agents in order to forward those queries to relevant agents without flooding the network with unnecessary query messages.",
                "However, this heuristic is myopic because a relevant direct neighbor is not necessarily connected to other relevant agents.",
                "In this section, we propose a more general approach by framing this problem as a reinforcement learning task.",
                "In pursuit of greater flexibility, agents can switch between two modes: learning mode and non-learning mode.",
                "In the non-learning mode, agents operate in the same way as they do in the normal distributed search processes described in [14, 15].",
                "On the other hand, in the learning mode, in parallel with distributed search sessions, agents also participate in a learning process which will be detailed in this section.",
                "Note that in the learning protocol, the learning process does not interfere with the distributed search process.",
                "Agents can choose to initiate and stop learning processes without affecting the system performance.",
                "In particular, since the learning process consumes network resources (especially bandwidth), agents can choose to initiate learning only when the network load is relatively low, thus minimizing the extra communication costs incurred by the <br>learning algorithm</br>.",
                "The section is structured as follows, Section 3.1 describes 232 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) a reinforcement learning based model.",
                "Section 3.2 describes a protocol to deploy the <br>learning algorithm</br> in the network.",
                "Section 3.3 discusses the convergence of the <br>learning algorithm</br>. 3.1 The Model An agents routing policy takes the state of a search session as input and output the routing actions for that query.",
                "In our work, the state of a search session sj is stipulated as: QSj = (qk, ttlj) where ttlj is the number of hops that remains for the search session sj , qk is the specific query.",
                "QL is an attribute of qk that indicates which type of queries qk most likely belong to.",
                "The set of QL can be generated by running a simple online classification algorithm on all the queries that have been processed by the agents, or an oﬄine algorithm on a pre-designated training set.",
                "The assumption here is that the set of query types is learned ahead of time and belongs to the common knowledge of the agents in the network.",
                "Future work includes exploring how learning can be accomplished when this assumption does not hold.",
                "Given the query types set, an incoming query qi can be classified to one query class Q(qi) by the formula: Q(qi) = arg max Qj P(qi|Qj) (1) where P(qi|Qj ) indicates the likelihood that the query qi is generated by the query class Qj [8].",
                "The set of atomic routing actions of an agent Ai is denoted as {αi}, where {αi} is defined as αi = {αi0 , αi1 , ..., αin }.",
                "An element αij represents an action to route a given query to the neighboring agent Aij ∈ DirectConn(Ai).",
                "The routing policy πi of agent Ai is stochastic and its outcome for a search session with state QSj is defined as: πi(QSj) = {(αi0 , πi(QSi, αi0 )), (αi1 , πi(QSi, αi1 )), ...} (2) Note that operator πi is overloaded to represent either the probabilistic policy for a search session with state QSj, denoted as πi(QSj); or the probability of forwarding the query to a specific neighboring agent Aik ∈ DirectConn(Ai) under the policy πi(QSj), denoted as πi(QSj, αik ).",
                "Therefore, equation (2) means that the probability of forwarding the search session to agent Ai0 is πi(QSi, αi0 ) and so on.",
                "Under this stochastic policy, the routing action is nondeterministic.",
                "The advantage of such a strategy is that the best neighboring agents will not be selected repeatedly, thereby mitigating the potential hot spots situations.",
                "The expected utility, Un i (QSj), is used to estimate the potential utility gain of routing query type QSj to agent Ai under policy πn i .",
                "The superscript n indicates the value at the nth iteration in an iterative learning process.",
                "The expected utility provides routing guidance for future search sessions.",
                "In the search process, each agent Ai maintains partial observations of its neighbors states, as shown in Fig. 2.",
                "The partial observation includes non-local information such as the potential utility estimation of its neighbor Am for query state QSj, denoted as Um(QSj), as well as the load information, Lm.",
                "These observations are updated periodically by the neighbors.",
                "The estimated utility information will be used to update Ais expected utility for its routing policy.",
                "Load Information Expected Utility For Different Query Types Neighboring Agents ... A0 A1 A3 A2 Un 0 (QS0) ... ... ... ... ...... Un 0 (QS1) Un 1 (QS1) Un 2 (QS1) Un 3 (QS1) Un 1 (QS0) Un 2 (QS0) Un 3 (QS0) Ln 0 Ln 1 Ln 2 Ln 3 ... QS0 QS1 ...",
                "Figure 2: Agent Ais Partial Observation about its neighbors(A0, A1...) The load information of Am, Lm, is defined as Lm = |MFm| Cm , where |MFm| is the length of the message-forward queue and Cm is the service rate of agent Ams message-forward queue.",
                "Therefore Lm characterizes the utilization of an agents communication channel, and thus provide non-local information for Ams neighbors to adjust the parameters of their routing policy to avoid inundating their downstream agents.",
                "Note that based on the characteristics of the queries entering the system and agents capabilities, the loading of agents may not be uniform.",
                "After collecting the utilization rate information from all its neighbors, agent Ai computes Li as a single measure for assessing the average load condition of its neighborhood: Li = P k Lk |DirectConn(Ai)| Agents exploit Li value in determining the routing probability in its routing policy.",
                "Note that, as described in Section 3.2, information about neighboring agents is piggybacked with the query message propagated among the agents whenever possible to reduce the traffic overhead. 3.1.1 Update the Policy An iterative update process is introduced for agents to learn a satisfactory stochastic routing policy.",
                "In this iterative process, agents update their estimates on the potential utility of their current routing policies and then propagate the updated estimates to their neighbors.",
                "Their neighbors then generate a new routing policy based on the updated observation and in turn they calculate the expected utility based on the new policies and continue this iterative process.",
                "In particular, at time n, given a set of expected utility, an agent Ai, whose directly connected agents set is DirectConn(Ai) = {Ai0 , ..., Aim }, determines its corresponding stochastic routing policy for a search session of state QSj based on the following steps: (1) Ai first selects a subset of agents as the potential downstream agents from set DirectConn(Ai), denoted as PDn(Ai, QSj).",
                "The size of the potential downstream agent is specified as |PDn(Ai, QSj)| = min(|NEI(Ai), dn i + k)| where k is a constant and is set to 3 in this paper; dn i , the forward width, is defined as the expected number of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 233 neighboring agents that agent Ai can forward to at time n. This formula specifies that the potential downstream agent set PDn(Ai, QSj) is either the subset of neighboring agents with dn i + k highest expected utility value for state QSj among all the agents in DirectConn(Ai), or all their neighboring agents.",
                "The k is introduced based on the idea of a stochastic routing policy and it makes the forwarding probability of the dn i +k highest agent less than 100%.",
                "Note that if we want to limit the number of downstream agents for search session sj as 5, the probability of forwarding the query to all neighboring agents should add up to 5.",
                "Setting up dn i value properly can improve the utilization rate of the network bandwidth when much of the network is idle while mitigating the traffic load when the network is highly loaded.",
                "The dn+1 i value is updated based on dn i , the previous and current observations on the traffic situation in the neighborhood.",
                "Specifically, the update formula for dn+1 i is dn+1 i = dn i ∗ (1 + 1 − Li |DirectConn(Ai)| ) In this formula, the forward width is updated based on the traffic conditions of agent Ais neighborhood, i.e Li, and its previous value. (2) For each agent Aik in the PDn(Ai, QSj), the probability of forwarding the query to Aik is determined in the following way in order to assign higher forwarding probability to the neighboring agents with higher expected utility value: πn+1 i (QSj, αik ) = dn+1 i |PDn(Ai, QSj)| + β ∗ ` Uik (QSj) − PDU(Ai, QSj) |PDn(Ai, QSj)| ´ (3) where PDUn(Ai, QSj) = X o∈P Dn(Ai,QSj ) Uo(QSj) and QSj is the subsequent state of agent Aik after agent Ai forwards the search session with state QSj to its neighboring agent Aik ; If QSj = (qk, ttl0), then QSj = (qk, ttl0 − 1).",
                "In formula 3, the first term on the right of the equation, dn+1 i |P Dn(Ai,QSj )| , is used to to determine the forwarding probability by equally distributing the forward width, dn+1 i , to the agents in PDn(Ai, QSj) set.",
                "The second term is used to adjust the probability of being chosen so that agents with higher expected utility values will be favored. β is determined according to: β = min ` m − dn+1 i m ∗ umax − PDUn(Ai, QSj) , dn+1 i PDUn(Ai, QSj) − m ∗ umin ´ (4) where m = |PDn(Ai, QSj)|, umax = max o∈P Dn(Ao,QSj ) Uo(QSj) and umin = min o∈P Dn(Ao,QSj ) Uo(QSj) This formula guarantees that the final πn+1 i (QSj, αik ) value is well defined, i.e, 0 ≤ πn+1 i (QSj, αik ) ≤ 1 and X i πn+1 i (QSj, αik ) = dn+1 i However, such a solution does not explore all the possibilities.",
                "In order to balance between exploitation and exploration, a λ-Greedy approach is taken.",
                "In the λ-Greedy approach, in addition to assigning higher probability to those agents with higher expected utility value, as in the equation (3).",
                "Agents that appear to be not-so-good choices will also be sent queries based on a dynamic exploration rate.",
                "In particular, for agents in the set PDn(Ai, QSj), πn+1 i1 (QSj) is determined in the same way as the above, with the only difference being that dn+1 i is replaced with dn+1 i ∗ (1 − λn).",
                "The remaining search bandwidth is used for learning by assigning probability λn evenly to agents Ai2 in the set DirectConn(Ai) − PDn(Ai, QSj). πn+1 i2 (QSj, αik ) = dn+1 i ∗ λn |DirectConn(Ai) − PDn(Ai, QSj)| (5) where PDn(Ai, QSj) ⊂ DirectConn(Ai).",
                "Note that the exploration rate λ is not a constant and it decreases overtime.",
                "The λ is determined according to the following equation: λn+1 = λ0 ∗ e−c1n (6) where λ0 is the initial exploration rate, which is a constant; c1 is also a constant to adjust the decreasing rate of the exploration rate; n is the current time unit. 3.1.2 Update Expected Utility Once the routing policy at step n+1, πn+1 i , is determined based on the above formula, agent Ai can update its own expected utility, Un+1 i (QSi), based on the the updated routing policy resulted from the formula 5 and the updated U values of its neighboring agents.",
                "Under the assumption that after a query is forwarded to Ais neighbors the subsequent search sessions are independent, the update formula is similar to the Bellman update formula in Q-Learning: Un+1 i (QSj) = (1 − θi) ∗ Un i (QSj) + θi ∗ (Rn+1 i (QSj) + X k πn+1 i (QSj, αik )Un k (QSj)) (7) where QSj = (Qj, ttl − 1) is the next state of QSj = (Qj, ttl); Rn+1 i (QSj) is the expected local reward for query class Qk at agent Ai under the routing policy πn+1 i ; θi is the coefficient for deciding how much weight is given to the old value during the update process: the smaller θi value is, the faster the agent is expected to learn the real value, while the greater volatility of the algorithm, and vice versa.",
                "Rn+1 (s) is updated according to the following equation: 234 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Rn+1 i (QSj) = Rn i (QSj) +γi ∗ (r(QSj) − Rn i (QSj)) ∗ P(qj|Qj ) (8) where r(QSj) is the local reward associated with the search session.",
                "P(qj|Qj ) indicates how relevant the query qj is to the query type Qj, and γi is the learning rate for agent Ai.",
                "Depending on the similarity between a specific query qi and its corresponding query type Qi, the local reward associated with the search session has different impact on the Rn i (QSj) estimation.",
                "In the above formula, this impact is reflected by the coefficient, the P(qj|Qj) value. 3.1.3 Reward function After a search session stops when its TTL values expires, all search results are returned back to the user and are compared against the relevance judgment.",
                "Assuming the set of search results is SR, the reward Rew(SR) is defined as: Rew(SR) = j 1 if |Rel(SR)| > c |Rel(SR)| c otherwise. where SR is the set of returned search results, Rel(SR) is the set of relevant documents in the search results.",
                "This equation specifies that users give 1.0 reward if the number of returned relevant documents reaches a predefined number c. Otherwise, the reward is in proportion to the number of relevant documents returned.",
                "This rationale for setting up such a cut-off value is that the importance of recall ratio decreases with the abundance of relevant documents in real world, therefore users tend to focus on only a limited number of searched results.",
                "The details of the actual routing protocol will be introduced in Section 3.2 when we introduce how the <br>learning algorithm</br> is deployed in real systems. 3.2 Deployment of the <br>learning algorithm</br> This section describes how the learning algorithm can be used in either a single-phase or a two-phase search process.",
                "In the single-phase search algorithm, search sessions start from the initiators of the queries.",
                "In contrast, in the two-step search algorithm, the query initiator first attempts to seek a more appropriate starting point for the query by introducing an exploratory step as described in Section 2.",
                "Despite the difference in the quality of starting points, the major part of the learning process for the two algorithms is largely the same as described in the following paragraphs.",
                "Before learning starts, each agent initializes the expected utility value for all possible states as 0.",
                "Thereafter, upon receipt of a query, in addition to the normal operations described in the previous section, an agent Ai also sets up a timer to wait for the search results returned from its downstream agents.",
                "Once the timer expires or it has received response from all its downstream agents, Ai merges and forwards the search results accrued from its downstream agents to its upstream agent.",
                "Setting up the timer speeds up the learning because agents can avoid waiting too long for the downstream agents to return search results.",
                "Note that these detailed results and corresponding agent information will still be stored at Ai until the feedback information is passed from its upstream agent and the performance of its downstream agents can be evaluated.",
                "The duration of the timer is related to the TTL value.",
                "In this paper, we set the timer to ttimer = ttli ∗ 2 + tf , where ttli ∗ 2 is the sum of the travel time of the queries in the network, and tf is the expected time period that users would like to wait.",
                "The search results will eventually be returned to the search session initiator A0.",
                "They will be compared to the relevance judgment that is provided by the final users (as described in the experiment section, the relevance judgement for the query set is provided along with the data collections).",
                "The reward will be calculated and propagated backward to the agents along the way that search results were passed.",
                "This is a reverse process of the search results propagation.",
                "In the process of propagating reward backward, agents update estimates of their own potential utility value, generate an upto-dated policy and pass their updated results to the neighboring agents based on the algorithm described in Section 3.",
                "Upon change of expected utility value, agent Ai sends out its updated utility estimation to its neighbors so that they can act upon the changed expected utility and corresponding state.",
                "This update message includes the potential reward as well as the corresponding state QSi = (qk, ttll) of agent Ai.",
                "Each neighboring agent, Aj, reacts to this kind of update message by updating the expected utility value for state QSj(qk, ttll + 1) according to the newly-announced changed expected utility value.",
                "Once they complete the update, the agents would again in turn inform related neighbors to update their values.",
                "This process goes on until the TTL value in the update message increases to the TTL limit.",
                "To speed up the learning process, while updating the expected utility values of an agent Ais neighboring agents we specify that Um(Qk, ttl0) >= Um(Qk, ttl1) iff ttl0 > ttl1 Thus, when agent Ai receives an updated expected utility value with ttl1, it also updates the expected utility values with any ttl0 > ttl1 if Um(Qk, ttl0) < Um(Qk, ttl1) to speed up convergence.",
                "This heuristic is based on the fact that the utility of a search session is a non-decreasing function of time t. 3.3 Discussion In formalizing the content routing system as a learning task, many assumptions are made.",
                "In real systems, these assumptions may not hold, and thus the <br>learning algorithm</br> may not converge.",
                "Two problems are of particular note, (1) This content routing problem does not have Markov properties.",
                "In contrast to IP-level based packet routing, the routing decision of each agent for a particular search session sj depends on the routing history of sj.",
                "Therefore, the assumption that all subsequent search sessions are independent does not hold in reality.",
                "This may lead to double counting problem that the relevant documents of some agents will be counted more than once for the state where the TTL value is more than 1.",
                "However, in the context of the hierarchical agent organizations, two factors mitigate this problems: first, the agents in each content group form a tree-like structure.",
                "With the absense of the cycles, the estimates inside the tree would be close to the accurate value.",
                "Secondly, the stochastic nature of the routing policy partly remedies this problem.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 235 (2) Another challenge for this <br>learning algorithm</br> is that in a real network environment observations on neighboring agents may not be able to be updated in time due to the communication delay or other situations.",
                "In addition, when neighboring agents update their estimates at the same time, oscillation may arise during the learning process[1].",
                "This paper explores several approaches to speed up the learning process.",
                "Besides the aforementioned strategy of updating the expected utility values, we also employ an active update strategy where agents notify their neighbors whenever its expected utility is updated.",
                "Thus a faster convergence speed can be achieved.",
                "This strategy contrasts to the Lazy update, where agents only echo their neighboring agents with their expected utility change when they exchange information.",
                "The trade off between the two approaches is the network load versus learning speed.",
                "The advantage of this <br>learning algorithm</br> is that once a routing policy is learned, agents do not have to repeatedly compare the similarity of queries as long as the network topology remains unchanged.",
                "Instead, agent just have to determine the classification of the query properly and follow the learned policies.",
                "The disadvantage of this learning-based approach is that the learning process needs to be conducted whenever the network structure changes.",
                "There are many potential extensions for this learning model.",
                "For example, a single measure is currently used to indicate the traffic load for an agents neighborhood.",
                "A simple extension would be to keep track of individual load for each neighbor of the agent. 4.",
                "EXPERIMENTSSETTINGSAND RESULTS The experiments are conducted on TRANO simulation toolkit with two sets of datasets, TREC-VLC-921 and TREC123-100.",
                "The following sub-sections introduce the TRANO testbed, the datasets, and the experimental results. 4.1 TRANO Testbed TRANO (Task Routing on Agent Network Organization) is a multi-agent based network based information retrieval testbed.",
                "TRANO is built on top of the Farm [4], a time based distributed simulator that provides a data dissemination framework for large scale distributed agent network based organizations.",
                "TRANO supports importation and exportation of agent organization profiles including topological connections and other features.",
                "Each TRANO agent is composed of an agent view structure and a control unit.",
                "In simulation, each agent is pulsed regularly and the agent checks the incoming message queues, performs local operations and then forwards messages to other agents . 4.2 Experimental Settings In our experiment, we use two standard datasets, TRECVLC-921 and TREC-123-100 datasets, to simulate the collections hosted on agents.",
                "The TREC-VLC-921 and TREC123-100 datasets were created by the U.S. National Institute for Standard Technology(NIST) for its TREC conferences.",
                "In distributed information retrieval domain, the two data collections are split to 921 and 100 sub-collections.",
                "It is observed that dataset TREC-VLC-921 is more heterogeneous than TREC-123-100 in terms of source, document length, and relevant document distribution from the statistics of the two data collections listed in [13].",
                "Hence, TREC-VLC-921 is much closer to real document distributions in P2P environments.",
                "Furthermore, TREC-123-100 is split into two sets of 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0 500 1000 1500 2000 2500 3000 ARSS Query number ARSS versus the number of incoming queries for TREC-VLC-921 SSLA-921 SSNA-921 Figure 3: ARSS(Average reward per search session) versus the number of search sessions for 1phase search in TREC-VLC-921 0 0.1 0.2 0.3 0.4 0.5 0.6 0 500 1000 1500 2000 2500 3000 ARSS Query number ARSS versus query number for TREC-VLC-921 TSLA-921 TSNA-921 Figure 4: ARSS(Average reward per search session) versus the number of search sessions for 2phase search in TREC-VLC-921 sub-collections in two ways: randomly and by source.",
                "The two partitions are denoted as TREC-123-100-Random and TREC-123-100-Source respectively.",
                "The documents in each subcollection in dataset TREC-123-100-Source are more coherent than those in TREC-123-100-Random.",
                "The two different sets of partitions allow us to observe how the distributed <br>learning algorithm</br> is affected by the homogeneity of the collections.",
                "The hierarchical agent organization is generated by the algorithm described in our previous algorithm [15].",
                "During the topology generation process, degree information of each agent is estimated by the algorithm introduced by Palmer et al. [9] with parameters α = 0.5 and β = 0.6.",
                "In our experiments, we estimate the upward limit and downward degree limit using linear discount factors 0.5, 0.8 and 1.0.",
                "Once the topology is built, queries randomly selected from the query set 301−350 on TREC-VLC-921 and query set 1− 50 on TREC-123-100-Random and TREC-123-100-Source are injected to the system based on a Poisson distribution P(N(t) = n) = (λt)n n! e−λ 236 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 0 50 100 150 200 250 300 350 400 0 500 1000 1500 2000 2500 3000 Cumulativeutility Query number Cumulative utility over the number of incoming queries TSLA-921 SSNA-921 SSLA-921 TSNA-921 Figure 5: The cumulative utility versus the number of search sessions TREC-VLC-921 In addition, we assume that all agents have an equal chance of getting queries from the environment, i.e, λ is the same for every agent.",
                "In our experiments, λ is set as 0.0543 so that the mean of the incoming queries from the environment to the agent network is 50 per time unit.",
                "The service time for the communication queue and local search queue, i.e tQij and trs, is set as 0.01 time unit and 0.05 time units respectively.",
                "In our experiments, there are ten types of queries acquired by clustering the query set 301 − 350 and 1 − 50. 4.3 Results analysis and evaluation Figure 3 demonstrates the ARSS(Average Reward per Search Session) versus the number of incoming queries over time for the the Single-Step based Non-<br>learning algorithm</br> (SSNA), and the Single-Step <br>learning algorithm</br>(SSLA) for data collection TREC-VLC-921.",
                "It shows that the average reward for SSNA algorithm ranges from 0.02 − 0.06 and the performance changes little over time.",
                "The average reward for SSLA approach starts at the same level with the SSNA algorithm.",
                "But the performance increases over time and the average performance gain stabilizes at about 25% after query range 2000 − 3000.",
                "Figure 4 shows the ARSS(Average Reward per Search Session) versus the number of incoming queries over time for the the Two-Step based Non-<br>learning algorithm</br>(TSNA), and the Two-Step <br>learning algorithm</br>(TSLA) for data collection TREC-VLC-921.",
                "The TSNA approach has a relatively consistent performance with the average reward ranges from 0.05 − 0.15.",
                "The average reward for TSLA approach, where <br>learning algorithm</br> is exploited, starts at the same level with the TSNA algorithm and improves the average reward over time until 2000−2500 queries joining the system.",
                "The results show that the average performance gain for TSLA approach over TNLA approach is 35% after stabilization.",
                "Figure 5 shows the cumulative utility versus the number of incoming queries over time for SSNA, SSLA,TSNA, and TSLA respectively.",
                "It illustrates that the cumulative utility of non-learning algorithms increases largely linearly over time, while the gains of learning-based algorithms accelerate when more queries enter the system.",
                "These experimental results demonstrate that learning-based approaches consistently perform better than non-learning based routing algorithm.",
                "Moreover, two-phase learning based algorithm is better than single-phase based <br>learning algorithm</br> because the maximal reward an agent can receive from searching its neighborhood within TTL hops is related to the total number of the relevant documents in that area.",
                "Thus, even the optimal routing policy can do little beyond reaching these relevant documents faster.",
                "On the contrary, the two-stepbased <br>learning algorithm</br> can relocate the search session to a neighborhood with more relevant documents.",
                "The TSLA combines the merits of both approaches and outperforms them.",
                "Table 1 lists the cumulative utility for datasets TREC123-100-Random and TREC-123-100-Source with hierarchical organizations.",
                "The five columns show the results for four different approaches.",
                "In particular, column TSNA-Random shows the results for dataset TREC-123-100-Random with the TSNA approach.",
                "The column TSLA-Random shows the results for dataset TREC-123-100-Random with the TSLA approach.",
                "There are two numbers in each cell in the column TSLA-Random.",
                "The first number is the actual cumulative utility while the second number is the percentage gain in terms of the utility over TSNA approach.",
                "Columns TSNA-Source and TSLA-Source show the results for dataset TREC-123-100-Source with TSNA and TSLA approaches respectively.",
                "Table 1 shows that the performance improvement for TREC-123-100-Random is not as significant as the other datasets.",
                "This is because that the documents in the sub-collection of TREC-123-100-Random are selected randomly which makes the collection model, the signature of the collection, less meaningful.",
                "Since both algorithms are designed based on the assumption that document collections can be well represented by their collection model, this result is not surprising.",
                "Overall, Figures 4, 5, and Table 1 demonstrate that the reinforcement learning based approach can considerably enhance the system performance for both data collections.",
                "However, it remains as future work to discover the correlation between the magnitude of the performance gains and the size of the data collection and/or the extent of the heterogeneity between the sub-collections. 5.",
                "RELATED WORK The content routing problem differs from the networklevel routing in packet-switched communication networks in that content-based routing occurs in application-level networks.",
                "In addition, the destination agents in our contentrouting algorithms are multiple and the addresses are not known in the routing process.",
                "IP-level Routing problems have been attacked from the reinforcement learning perspective[2, 5, 11, 12].",
                "These studies have explored fully distributed algorithms that are able, without central coordination to disseminate knowledge about the network, to find the shortest paths robustly and efficiently in the face of changing network topologies and changing link costs.",
                "There are two major classes of adaptive, distributed packet routing algorithms in the literature: distance-vector algorithms and link-state algorithms.",
                "While this line of studies carry a certain similarity with our work, it has mainly focused on packet-switched communication networks.",
                "In this domain, the destination of a packet is deterministic and unique.",
                "Each agent maintains estimations, probabilistically or deterministically, on the distance to a certain destination through its neighbors.",
                "A variant of Q-Learning techniques is deployed The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 237 Table 1: Cumulative Utility for Datasets TREC-123-100-Random and TREC-123-100-Source with Hierarchical Organization; The percentage numbers in the columns TSLA-Random and TSLA-Source demonstrate the performance gain over the algorithm without learning Query number TSNA-Random TSLA-Random TSNA-Source TSLA-Source 500 25.15 28.45 13% 24.00 21.05 -13% 1000 104.99 126.74 20% 93.95 96.44 2.6% 1250 149.79 168.40 12% 122.64 134.05 9.3% 1500 188.94 211.05 12% 155.30 189.60 22% 1750 235.49 261.60 11% 189.14 243.90 28% 2000 275.09 319.25 16% 219.0 278.80 26% to update the estimations to converge to the real distances.",
                "It has been discovered that the locality property is an important feature of information retrieval systems in user modeling studies[3].",
                "In P2P based content sharing systems, this property is exemplified by the phenomenon that users tend to send queries that represent only a limited number of topics and conversely, users in the same neighborhood are likely to share common interests and send similar queries [10].",
                "The learning based approach is perceived to be more beneficial for real distributed information retrieval systems which exhibit locality property.",
                "This is because the users traffic and query patterns can reduce the state space and speed up the learning process.",
                "Related work in taking advantage of this property include [7], where the authors attempted to address this problem by user modeling techniques. 6.",
                "CONCLUSIONS In this paper, a reinforcement-learning based approach is developed to improve the performance of distributed IR search algorithms.",
                "Particularly, agents maintain estimates, namely expected utility, on the downstream agents ability to provide relevant documents for incoming queries.",
                "These estimates are updated gradually by learning from the feedback information returned from previous search sessions.",
                "Based on the updated expected utility information, the agents modify their routing policies.",
                "Thereafter, these agents route the queries based on the learned policies and update the estimates on the expected utility based on the new routing policies.",
                "The experiments on two different distributed IR datasets illustrates that the reinforcement learning approach improves considerably the cumulative utility over time. 7.",
                "REFERENCES [1] S. Abdallah and V. Lesser.",
                "Learning the task allocation game.",
                "In AAMAS 06: Proceedings of the fifth international joint conference on Autonomous agents and multiagent systems, pages 850-857, New York, NY, USA, 2006.",
                "ACM Press. [2] J.",
                "A. Boyan and M. L. Littman.",
                "Packet routing in dynamically changing networks: A reinforcement learning approach.",
                "In Advances in Neural Information Processing Systems, volume 6, pages 671-678.",
                "Morgan Kaufmann Publishers, Inc., 1994. [3] J. C. French, A. L. Powell, J. P. Callan, C. L. Viles, T. Emmitt, K. J. Prey, and Y. Mou.",
                "Comparing the performance of database selection algorithms.",
                "In Research and Development in Information Retrieval, pages 238-245, 1999. [4] B. Horling, R. Mailler, and V. Lesser.",
                "Farm: A scalable environment for multi-agent development and evaluation.",
                "In Advances in Software Engineering for Multi-Agent Systems, pages 220-237, Berlin, 2004.",
                "Springer-Verlag. [5] M. Littman and J. Boyan.",
                "A distributed reinforcement learning scheme for network routing.",
                "In Proceedings of the International Workshop on Applications of Neural Networks to Telecommunications, 1993. [6] J. Lu and J. Callan.",
                "Federated search of text-based digital libraries in hierarchical peer-to-peer networks.",
                "In In ECIR05, 2005. [7] J. Lu and J. Callan.",
                "User modeling for full-text federated search in peer-to-peer networks.",
                "In ACM SIGIR 2006.",
                "ACM Press, 2006. [8] C. D. Manning and H. Sch¨utze.",
                "Foundations of Statistical Natural Language Processing.",
                "The MIT Press, Cambridge, Massachusetts, 1999. [9] C. R. Palmer and J. G. Steffan.",
                "Generating network topologies that obey power laws.",
                "In Proceedings of GLOBECOM 2000, November 2000. [10] K. Sripanidkulchai, B. Maggs, and H. Zhang.",
                "Efficient content location using interest-based locality in peer-topeer systems.",
                "In INFOCOM, 2003. [11] D. Subramanian, P. Druschel, and J. Chen.",
                "Ants and reinforcement learning: A case study in routing in dynamic networks.",
                "In In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence, pages 832-839, 1997. [12] J. N. Tao and L. Weaver.",
                "A multi-agent, policy gradient approach to network routing.",
                "In In Proceedings of the Eighteenth International Conference on Machine Learning, 2001. [13] H. Zhang, W. B. Croft, B. Levine, and V. Lesser.",
                "A multi-agent approach for peer-to-peer information retrieval.",
                "In Proceedings of Third International Joint Conference on Autonomous Agents and Multi-Agent Systems, July 2004. [14] H. Zhang and V. Lesser.",
                "Multi-agent based peer-to-peer information retrieval systems with concurrent search sessions.",
                "In Proceedings of the Fifth International Joint Conference on Autonomous Agents and Multi-Agent Systems, May 2006. [15] H. Zhang and V. R. Lesser.",
                "A dynamically formed hierarchical agent organization for a distributed content sharing system.",
                "In 2004 IEEE/WIC/ACM International Conference on Intelligent Agent Technology (IAT 2004), 20-24 September 2004, Beijing, China, pages 169-175.",
                "IEEE Computer Society, 2004. 238 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "Experimental results demonstrate that the <br>learning algorithm</br> improves considerably the routing performance on two test collection sets that have been used in a variety of distributed IR studies.",
                "The goal of the <br>learning algorithm</br>, even though it consumes some network bandwidth, is to shorten the routing time so that more queries are processed per time unit while at the same time finding more relevant documents.",
                "In particular, since the learning process consumes network resources (especially bandwidth), agents can choose to initiate learning only when the network load is relatively low, thus minimizing the extra communication costs incurred by the <br>learning algorithm</br>.",
                "Section 3.2 describes a protocol to deploy the <br>learning algorithm</br> in the network.",
                "Section 3.3 discusses the convergence of the <br>learning algorithm</br>. 3.1 The Model An agents routing policy takes the state of a search session as input and output the routing actions for that query."
            ],
            "translated_annotated_samples": [
                "Los resultados experimentales demuestran que el <br>algoritmo de aprendizaje</br> mejora considerablemente el rendimiento del enrutamiento en dos conjuntos de colecciones de pruebas que han sido utilizados en una variedad de estudios de recuperación de información distribuida.",
                "El objetivo del <br>algoritmo de aprendizaje</br>, aunque consume ancho de banda de red, es reducir el tiempo de enrutamiento para procesar más consultas por unidad de tiempo y al mismo tiempo encontrar más documentos relevantes.",
                "En particular, dado que el proceso de aprendizaje consume recursos de red (especialmente ancho de banda), los agentes pueden optar por iniciar el aprendizaje solo cuando la carga de la red es relativamente baja, minimizando así los costos adicionales de comunicación incurridos por el <br>algoritmo de aprendizaje</br>.",
                "La sección 3.2 describe un protocolo para implementar el <br>algoritmo de aprendizaje</br> en la red.",
                "La sección 3.3 discute la convergencia del <br>algoritmo de aprendizaje</br>. 3.1 El Modelo La política de enrutamiento de un agente toma el estado de una sesión de búsqueda como entrada y produce las acciones de enrutamiento para esa consulta."
            ],
            "translated_text": "Un algoritmo de búsqueda distribuida basado en aprendizaje por refuerzo para sistemas jerárquicos de recuperación de información entre pares. En estos enfoques, los agentes dependen de la similitud de contenido entre las consultas entrantes y sus agentes vecinos directos para dirigir las sesiones de búsqueda distribuida. Sin embargo, tal heurística es miope en el sentido de que los agentes vecinos pueden no estar conectados con agentes más relevantes. En este documento, se desarrolla un enfoque basado en aprendizaje por refuerzo en línea para aprovechar las características dinámicas en tiempo de ejecución de los sistemas de recuperación de información P2P, representadas por información sobre sesiones de búsqueda pasadas. Específicamente, los agentes mantienen estimaciones sobre las habilidades de los agentes aguas abajo para proporcionar documentos relevantes para las consultas entrantes. Estas estimaciones se actualizan gradualmente al aprender de la información de retroalimentación devuelta de sesiones de búsqueda anteriores. Basándose en esta información, los agentes derivan políticas de enrutamiento correspondientes. Posteriormente, estos agentes dirigen las consultas basándose en las políticas aprendidas y actualizan las estimaciones según las nuevas políticas de enrutamiento. Los resultados experimentales demuestran que el <br>algoritmo de aprendizaje</br> mejora considerablemente el rendimiento del enrutamiento en dos conjuntos de colecciones de pruebas que han sido utilizados en una variedad de estudios de recuperación de información distribuida. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagente Términos Generales Algoritmos, Rendimiento, Experimentación 1. En los últimos años ha habido un creciente interés en estudiar cómo controlar los procesos de búsqueda en sistemas de recuperación de información (IR) basados en pares (P2P) [6, 13, 14, 15]. En esta línea de investigación, uno de los problemas centrales que preocupa a los investigadores es dirigir de manera eficiente las consultas de los usuarios en la red a los agentes que poseen los documentos apropiados. En ausencia de información global, las estrategias dominantes para abordar este problema son enfoques basados en la similitud de contenido [6, 13, 14, 15]. Si bien la similitud de contenido entre consultas y nodos locales parece ser un indicador creíble para la cantidad de documentos relevantes que residen en cada nodo, estos enfoques están limitados por una serie de factores. En primer lugar, las métricas basadas en similitud pueden ser miope ya que los nodos localmente relevantes pueden no estar conectados a otros nodos relevantes. Segundo, los enfoques basados en la similitud no tienen en cuenta las características de tiempo de ejecución de los sistemas de IR P2P, incluidos los parámetros ambientales, el uso del ancho de banda y la información histórica de las sesiones de búsqueda pasadas, que proporcionan información valiosa para los algoritmos de enrutamiento de consultas. En este artículo, desarrollamos un enfoque de IR basado en aprendizaje por refuerzo para mejorar el rendimiento de los algoritmos de búsqueda de IR distribuidos. Los agentes pueden adquirir mejores estrategias de búsqueda al recopilar y analizar información de retroalimentación de sesiones de búsqueda anteriores. En particular, los agentes mantienen estimaciones, es decir, utilidad esperada, sobre las capacidades de los agentes aguas abajo de proporcionar documentos relevantes para tipos específicos de consultas entrantes. Estas estimaciones se actualizan gradualmente al aprender de la información de retroalimentación devuelta de sesiones de búsqueda anteriores. Basándose en la información actualizada de utilidad esperada, los agentes derivan políticas de enrutamiento correspondientes. Posteriormente, estos agentes dirigen las consultas basándose en las políticas aprendidas y actualizan las estimaciones sobre la utilidad esperada en función de las nuevas políticas de enrutamiento. Este proceso se lleva a cabo de manera iterativa. El objetivo del <br>algoritmo de aprendizaje</br>, aunque consume ancho de banda de red, es reducir el tiempo de enrutamiento para procesar más consultas por unidad de tiempo y al mismo tiempo encontrar más documentos relevantes. Esto contrasta con los enfoques basados en la similitud de contenido, donde se repiten operaciones similares para cada consulta entrante y el tiempo de procesamiento se mantiene en gran medida constante con el tiempo. Otra forma de ver este documento es que nuestro enfoque básico para la búsqueda de IR distribuida es construir una red de superposición jerárquica (organización de agentes) basada en la medida de similitud de contenido entre las colecciones de documentos de los agentes de manera ascendente. En trabajos anteriores, hemos demostrado que esta organización mejora significativamente el rendimiento de búsqueda. Sin embargo, esta estructura organizativa no tiene en cuenta los patrones de llegada de las consultas, incluyendo su frecuencia, tipos y dónde entran en el sistema, ni el ancho de banda de comunicación disponible en la red y las capacidades de procesamiento de los agentes individuales. La intención del aprendizaje por refuerzo es adaptar las decisiones de enrutamiento de los agentes a las situaciones dinámicas de la red y aprender de sesiones de búsqueda pasadas. Específicamente, las contribuciones de este artículo incluyen: (1) un enfoque basado en aprendizaje por refuerzo para que los agentes adquieran políticas de enrutamiento satisfactorias basadas en estimaciones de la contribución potencial de sus agentes vecinos; (2) dos estrategias para acelerar el proceso de aprendizaje. Según nuestro mejor conocimiento, esta es una de las primeras aplicaciones de aprendizaje por refuerzo para abordar problemas de compartición de contenido distribuido, lo cual indica algunos de los problemas al aplicar el refuerzo en una aplicación compleja. El resto de este documento está organizado de la siguiente manera: la Sección 2 revisa los sistemas jerárquicos de intercambio de contenido y el algoritmo de búsqueda de dos fases basado en dicha topología. La Sección 3 describe un enfoque basado en aprendizaje por refuerzo para dirigir el proceso de enrutamiento; la Sección 4 detalla la configuración experimental y analiza los resultados. La sección 5 discute estudios relacionados y la sección 6 concluye el artículo. BÚSQUEDA EN SISTEMAS JERÁRQUICOS DE IR P2P Esta sección revisa brevemente nuestros enfoques básicos para los sistemas jerárquicos de IR P2P. En un sistema jerárquico de recuperación de información P2P ilustrado en la Fig.1, los agentes están conectados entre sí a través de tres tipos de enlaces: enlaces ascendentes, enlaces descendentes y enlaces laterales. En las siguientes secciones, denotamos el conjunto de agentes que están directamente conectados al agente Ai como DirectConn(Ai), el cual se define como DirectConn(Ai) = NEI(Ai) ∪ PAR(Ai) ∪ CHL(Ai), donde NEI(Ai) es el conjunto de agentes vecinos conectados a Ai a través de enlaces laterales; PAR(Ai) es el conjunto de agentes a los que el agente Ai está conectado a través de enlaces ascendentes y CHL(Ai) es el conjunto de agentes a los que el agente Ai se conecta a través de enlaces descendentes. Estos enlaces se establecen a través de un proceso de agrupamiento distribuido basado en la similitud de contenido de abajo hacia arriba[15]. Estos enlaces son luego utilizados por agentes para localizar otros agentes que contienen documentos relevantes a las consultas dadas. Un agente típico Ai en nuestro sistema utiliza dos colas: una cola de búsqueda local, LSi, y una cola de reenvío de mensajes MFi. Los estados de las dos colas constituyen los estados internos de un agente. La cola de búsqueda local LSi almacena sesiones de búsqueda programadas para procesamiento local. Es una cola de prioridad y el agente Ai siempre selecciona las consultas más prometedoras para procesarlas con el fin de maximizar la utilidad global. MFi consiste en un conjunto de consultas para reenviar y se procesa de manera FIFO (primero en entrar, primero en salir). Para la primera consulta en MFi, el agente Ai determina a qué subconjunto de sus agentes vecinos reenviarla basándose en la política de enrutamiento de los agentes πi. Estas decisiones de enrutamiento determinan cómo se lleva a cabo el proceso de búsqueda en la red. En este documento, llamamos a Ai como el agente aguas arriba de Ajs y a Aj como el agente aguas abajo de Ais si A4 A5 A6 A7 A2 A3 A9 NEI(A2)={A3} PAR(A2)={A1} CHL(A2)={A4,A5} A1 A8 Figura 1: Una fracción de un sistema jerárquico P2PIR en el que un agente Ai dirige una consulta al agente Aj. El protocolo de búsqueda distribuida de nuestra organización de agentes jerárquicos está compuesto por dos pasos. En el primer paso, al recibir una consulta qk en el tiempo tl de un usuario, el agente Ai inicia una sesión de búsqueda si al sondear a sus agentes vecinos Aj ∈ NEI(Ai) con el mensaje PROBE para el valor de similitud Sim(qk, Aj) entre qk y Aj. Aquí, Ai se define como el iniciador de la consulta de la sesión de búsqueda si. En el segundo paso, Ai selecciona un grupo de agentes más prometedores para iniciar el proceso de búsqueda real con el mensaje BUSCAR. Estos mensajes de BÚSQUEDA contienen un parámetro TTL (Tiempo de Vida) además de la consulta. El valor TTL disminuye en 1 después de cada salto. En el proceso de búsqueda, los agentes descartan aquellas consultas que han sido procesadas previamente o cuyo TTL disminuye a 0, lo que evita que las consultas se repitan infinitamente en el sistema. La sesión de búsqueda termina cuando todos los agentes que reciben la consulta la descartan o el TTL disminuye a 0. Tras recibir mensajes de BÚSQUEDA para qk, los agentes programan actividades locales que incluyen búsqueda local, reenvío de qk a sus vecinos y devolución de resultados de búsqueda al iniciador de la consulta. Este proceso y los algoritmos relacionados se detallan en [15, 14]. 3. En el enfoque de búsqueda basado en aprendizaje por refuerzo, mencionado anteriormente, las decisiones de enrutamiento de un agente Ai dependen de la comparación de similitud entre las consultas entrantes y los agentes vecinos de Ai para reenviar esas consultas a agentes relevantes sin inundar la red con mensajes de consulta innecesarios. Sin embargo, esta heurística es miope porque un vecino directo relevante no necesariamente está conectado con otros agentes relevantes. En esta sección, proponemos un enfoque más general al plantear este problema como una tarea de aprendizaje por refuerzo. En busca de una mayor flexibilidad, los agentes pueden alternar entre dos modos: modo de aprendizaje y modo sin aprendizaje. En el modo de no aprendizaje, los agentes operan de la misma manera que lo hacen en los procesos de búsqueda distribuida normales descritos en [14, 15]. Por otro lado, en el modo de aprendizaje, en paralelo con las sesiones de búsqueda distribuida, los agentes también participan en un proceso de aprendizaje que se detallará en esta sección. Ten en cuenta que en el protocolo de aprendizaje, el proceso de aprendizaje no interfiere con el proceso de búsqueda distribuida. Los agentes pueden elegir iniciar y detener procesos de aprendizaje sin afectar el rendimiento del sistema. En particular, dado que el proceso de aprendizaje consume recursos de red (especialmente ancho de banda), los agentes pueden optar por iniciar el aprendizaje solo cuando la carga de la red es relativamente baja, minimizando así los costos adicionales de comunicación incurridos por el <br>algoritmo de aprendizaje</br>. La sección está estructurada de la siguiente manera, la Sección 3.1 describe 232 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) un modelo basado en aprendizaje por refuerzo. La sección 3.2 describe un protocolo para implementar el <br>algoritmo de aprendizaje</br> en la red. La sección 3.3 discute la convergencia del <br>algoritmo de aprendizaje</br>. 3.1 El Modelo La política de enrutamiento de un agente toma el estado de una sesión de búsqueda como entrada y produce las acciones de enrutamiento para esa consulta. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "routing policy": {
            "translated_key": "política de enrutamiento",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Reinforcement Learning based Distributed Search Algorithm For Hierarchical Peer-to-Peer Information Retrieval Systems Haizheng Zhang College of Information Science and Technology Pennsylvania State University University Park, PA 16803 hzhang@ist.psu.edu Victor Lesser Department of Computer Science University Of Massachusetts Amherst, MA 01003 lesser@cs.umass.edu ABSTRACT The dominant existing routing strategies employed in peerto-peer(P2P) based information retrieval(IR) systems are similarity-based approaches.",
                "In these approaches, agents depend on the content similarity between incoming queries and their direct neighboring agents to direct the distributed search sessions.",
                "However, such a heuristic is myopic in that the neighboring agents may not be connected to more relevant agents.",
                "In this paper, an online reinforcement-learning based approach is developed to take advantage of the dynamic run-time characteristics of P2P IR systems as represented by information about past search sessions.",
                "Specifically, agents maintain estimates on the downstream agents abilities to provide relevant documents for incoming queries.",
                "These estimates are updated gradually by learning from the feedback information returned from previous search sessions.",
                "Based on this information, the agents derive corresponding routing policies.",
                "Thereafter, these agents route the queries based on the learned policies and update the estimates based on the new routing policies.",
                "Experimental results demonstrate that the learning algorithm improves considerably the routing performance on two test collection sets that have been used in a variety of distributed IR studies.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Algorithms, Performance, Experimentation 1.",
                "INTRODUCTION Over the last few years there have been increasing interests in studying how to control the search processes in peer-to-peer(P2P) based information retrieval(IR) systems [6, 13, 14, 15].",
                "In this line of research, one of the core problems that concerns researchers is to efficiently route user queries in the network to agents that are in possession of appropriate documents.",
                "In the absence of global information, the dominant strategies in addressing this problem are content-similarity based approaches [6, 13, 14, 15].",
                "While the content similarity between queries and local nodes appears to be a creditable indicator for the number of relevant documents residing on each node, these approaches are limited by a number of factors.",
                "First of all, similaritybased metrics can be myopic since locally relevant nodes may not be connected to other relevant nodes.",
                "Second, the similarity-based approaches do not take into account the run-time characteristics of the P2P IR systems, including environmental parameters, bandwidth usage, and the historical information of the past search sessions, that provide valuable information for the query routing algorithms.",
                "In this paper, we develop a reinforcement learning based IR approach for improving the performance of distributed IR search algorithms.",
                "Agents can acquire better search strategies by collecting and analyzing feedback information from previous search sessions.",
                "Particularly, agents maintain estimates, namely expected utility, on the downstream agents capabilities of providing relevant documents for specific types of incoming queries.",
                "These estimates are updated gradually by learning from the feedback information returned from previous search sessions.",
                "Based on the updated expected utility information, the agents derive corresponding routing policies.",
                "Thereafter, these agents route the queries based on the learned policies and update the estimates on the expected utility based on the new routing policies.",
                "This process is conducted in an iterative manner.",
                "The goal of the learning algorithm, even though it consumes some network bandwidth, is to shorten the routing time so that more queries are processed per time unit while at the same time finding more relevant documents.",
                "This contrasts with the content-similarity based approaches where similar operations are repeated for every incoming query and the processing time keeps largely constant over time.",
                "Another way of viewing this paper is that our basic approach to distributed IR search is to construct a hierarchical overlay network(agent organization) based on the contentsimilarity measure among agents document collections in a bottom-up fashion.",
                "In the past work, we have shown that this organization improves search performance significantly.",
                "However, this organizational structure does not take into account the arrival patterns of queries, including their frequency, types, and where they enter the system, nor the available communication bandwidth of the network and processing capabilities of individual agents.",
                "The intention of the reinforcement learning is to adapt the agents routing decisions to the dynamic network situations and learn from past search sessions.",
                "Specifically, the contributions of this paper include: (1) a reinforcement learning based approach for agents to acquire satisfactory routing policies based on estimates of the potential contribution of their neighboring agents; (2) two strategies to speed up the learning process.",
                "To our best knowledge, this is one of the first reinforcement learning applications in addressing distributed content sharing problems and it is indicative of some of the issues in applying reinforcement in a complex application.",
                "The remainder of this paper is organized as follows: Section 2 reviews the hierarchical content sharing systems and the two-phase search algorithm based on such topology.",
                "Section 3 describes a reinforcement learning based approach to direct the routing process; Section 4 details the experimental settings and analyze the results.",
                "Section 5 discusses related studies and Section 6 concludes the paper. 2.",
                "SEARCH IN HIERARCHICAL P2P IR SYSTEMS This section briefly reviews our basic approaches to hierarchical P2P IR systems.",
                "In a hierarchical P2P IR system illustrated in Fig.1, agents are connected to each other through three types of links: upward links, downward links, and lateral links.",
                "In the following sections, we denote the set of agents that are directly connected to agent Ai as DirectConn(Ai), which is defined as DirectConn(Ai) = NEI(Ai) ∪ PAR(Ai) ∪ CHL(Ai) , where NEI(Ai) is the set of neighboring agents connected to Ai through lateral links; PAR(Ai) is the set of agents whom agent Ai is connected to through upward links and CHL(Ai) is the set of agents that agent Ai connects to through downward links.",
                "These links are established through a bottom-up content-similarity based distributed clustering process[15].",
                "These links are then used by agents to locate other agents that contain documents relevant to the given queries.",
                "A typical agent Ai in our system uses two queues: a local search queue, LSi, and a message forwarding queue MFi.",
                "The states of the two queues constitute the internal states of an agent.",
                "The local search queue LSi stores search sessions that are scheduled for local processing.",
                "It is a priority queue and agent Ai always selects the most promising queries to process in order to maximize the global utility.",
                "MFi consists of a set of queries to forward on and is processed in a FIFO (first in first out) fashion.",
                "For the first query in MFi, agent Ai determines which subset of its neighboring agents to forward it to based on the agents <br>routing policy</br> πi.",
                "These routing decisions determine how the search process is conducted in the network.",
                "In this paper, we call Ai as Ajs upstream agent and Aj as Ais downstream agent if A4 A5 A6 A7 A2 A3 A9 NEI(A2)={A3} PAR(A2)={A1} CHL(A2)={A4,A5} A1 A8 Figure 1: A fraction of a hierarchical P2PIR system an agent Ai routes a query to agent Aj.",
                "The distributed search protocol of our hierarchical agent organization is composed of two steps.",
                "In the first step, upon receipt of a query qk at time tl from a user, agent Ai initiates a search session si by probing its neighboring agents Aj ∈ NEI(Ai) with the message PROBE for the similarity value Sim(qk, Aj) between qk and Aj.",
                "Here, Ai is defined as the query initiator of search session si.",
                "In the second step, Ai selects a group of the most promising agents to start the actual search process with the message SEARCH.",
                "These SEARCH messages contain a TTL (Time To Live) parameter in addition to the query.",
                "The TTL value decreases by 1 after each hop.",
                "In the search process, agents discard those queries that either have been previously processed or whose TTL drops to 0, which prevents queries from looping in the system forever.",
                "The search session ends when all the agents that receive the query drop it or TTL decreases to 0.",
                "Upon receipt of SEARCH messages for qk, agents schedule local activities including local searching, forwarding qk to their neighbors, and returning search results to the query initiator.",
                "This process and related algorithms are detailed in [15, 14]. 3.",
                "A BASIC REINFORCEMENTLEARNING BASED SEARCH APPROACH In the aforementioned distributed search algorithm, the routing decisions of an agent Ai rely on the similarity comparison between incoming queries and Ais neighboring agents in order to forward those queries to relevant agents without flooding the network with unnecessary query messages.",
                "However, this heuristic is myopic because a relevant direct neighbor is not necessarily connected to other relevant agents.",
                "In this section, we propose a more general approach by framing this problem as a reinforcement learning task.",
                "In pursuit of greater flexibility, agents can switch between two modes: learning mode and non-learning mode.",
                "In the non-learning mode, agents operate in the same way as they do in the normal distributed search processes described in [14, 15].",
                "On the other hand, in the learning mode, in parallel with distributed search sessions, agents also participate in a learning process which will be detailed in this section.",
                "Note that in the learning protocol, the learning process does not interfere with the distributed search process.",
                "Agents can choose to initiate and stop learning processes without affecting the system performance.",
                "In particular, since the learning process consumes network resources (especially bandwidth), agents can choose to initiate learning only when the network load is relatively low, thus minimizing the extra communication costs incurred by the learning algorithm.",
                "The section is structured as follows, Section 3.1 describes 232 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) a reinforcement learning based model.",
                "Section 3.2 describes a protocol to deploy the learning algorithm in the network.",
                "Section 3.3 discusses the convergence of the learning algorithm. 3.1 The Model An agents <br>routing policy</br> takes the state of a search session as input and output the routing actions for that query.",
                "In our work, the state of a search session sj is stipulated as: QSj = (qk, ttlj) where ttlj is the number of hops that remains for the search session sj , qk is the specific query.",
                "QL is an attribute of qk that indicates which type of queries qk most likely belong to.",
                "The set of QL can be generated by running a simple online classification algorithm on all the queries that have been processed by the agents, or an oﬄine algorithm on a pre-designated training set.",
                "The assumption here is that the set of query types is learned ahead of time and belongs to the common knowledge of the agents in the network.",
                "Future work includes exploring how learning can be accomplished when this assumption does not hold.",
                "Given the query types set, an incoming query qi can be classified to one query class Q(qi) by the formula: Q(qi) = arg max Qj P(qi|Qj) (1) where P(qi|Qj ) indicates the likelihood that the query qi is generated by the query class Qj [8].",
                "The set of atomic routing actions of an agent Ai is denoted as {αi}, where {αi} is defined as αi = {αi0 , αi1 , ..., αin }.",
                "An element αij represents an action to route a given query to the neighboring agent Aij ∈ DirectConn(Ai).",
                "The <br>routing policy</br> πi of agent Ai is stochastic and its outcome for a search session with state QSj is defined as: πi(QSj) = {(αi0 , πi(QSi, αi0 )), (αi1 , πi(QSi, αi1 )), ...} (2) Note that operator πi is overloaded to represent either the probabilistic policy for a search session with state QSj, denoted as πi(QSj); or the probability of forwarding the query to a specific neighboring agent Aik ∈ DirectConn(Ai) under the policy πi(QSj), denoted as πi(QSj, αik ).",
                "Therefore, equation (2) means that the probability of forwarding the search session to agent Ai0 is πi(QSi, αi0 ) and so on.",
                "Under this stochastic policy, the routing action is nondeterministic.",
                "The advantage of such a strategy is that the best neighboring agents will not be selected repeatedly, thereby mitigating the potential hot spots situations.",
                "The expected utility, Un i (QSj), is used to estimate the potential utility gain of routing query type QSj to agent Ai under policy πn i .",
                "The superscript n indicates the value at the nth iteration in an iterative learning process.",
                "The expected utility provides routing guidance for future search sessions.",
                "In the search process, each agent Ai maintains partial observations of its neighbors states, as shown in Fig. 2.",
                "The partial observation includes non-local information such as the potential utility estimation of its neighbor Am for query state QSj, denoted as Um(QSj), as well as the load information, Lm.",
                "These observations are updated periodically by the neighbors.",
                "The estimated utility information will be used to update Ais expected utility for its <br>routing policy</br>.",
                "Load Information Expected Utility For Different Query Types Neighboring Agents ... A0 A1 A3 A2 Un 0 (QS0) ... ... ... ... ...... Un 0 (QS1) Un 1 (QS1) Un 2 (QS1) Un 3 (QS1) Un 1 (QS0) Un 2 (QS0) Un 3 (QS0) Ln 0 Ln 1 Ln 2 Ln 3 ... QS0 QS1 ...",
                "Figure 2: Agent Ais Partial Observation about its neighbors(A0, A1...) The load information of Am, Lm, is defined as Lm = |MFm| Cm , where |MFm| is the length of the message-forward queue and Cm is the service rate of agent Ams message-forward queue.",
                "Therefore Lm characterizes the utilization of an agents communication channel, and thus provide non-local information for Ams neighbors to adjust the parameters of their <br>routing policy</br> to avoid inundating their downstream agents.",
                "Note that based on the characteristics of the queries entering the system and agents capabilities, the loading of agents may not be uniform.",
                "After collecting the utilization rate information from all its neighbors, agent Ai computes Li as a single measure for assessing the average load condition of its neighborhood: Li = P k Lk |DirectConn(Ai)| Agents exploit Li value in determining the routing probability in its <br>routing policy</br>.",
                "Note that, as described in Section 3.2, information about neighboring agents is piggybacked with the query message propagated among the agents whenever possible to reduce the traffic overhead. 3.1.1 Update the Policy An iterative update process is introduced for agents to learn a satisfactory stochastic <br>routing policy</br>.",
                "In this iterative process, agents update their estimates on the potential utility of their current routing policies and then propagate the updated estimates to their neighbors.",
                "Their neighbors then generate a new <br>routing policy</br> based on the updated observation and in turn they calculate the expected utility based on the new policies and continue this iterative process.",
                "In particular, at time n, given a set of expected utility, an agent Ai, whose directly connected agents set is DirectConn(Ai) = {Ai0 , ..., Aim }, determines its corresponding stochastic <br>routing policy</br> for a search session of state QSj based on the following steps: (1) Ai first selects a subset of agents as the potential downstream agents from set DirectConn(Ai), denoted as PDn(Ai, QSj).",
                "The size of the potential downstream agent is specified as |PDn(Ai, QSj)| = min(|NEI(Ai), dn i + k)| where k is a constant and is set to 3 in this paper; dn i , the forward width, is defined as the expected number of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 233 neighboring agents that agent Ai can forward to at time n. This formula specifies that the potential downstream agent set PDn(Ai, QSj) is either the subset of neighboring agents with dn i + k highest expected utility value for state QSj among all the agents in DirectConn(Ai), or all their neighboring agents.",
                "The k is introduced based on the idea of a stochastic <br>routing policy</br> and it makes the forwarding probability of the dn i +k highest agent less than 100%.",
                "Note that if we want to limit the number of downstream agents for search session sj as 5, the probability of forwarding the query to all neighboring agents should add up to 5.",
                "Setting up dn i value properly can improve the utilization rate of the network bandwidth when much of the network is idle while mitigating the traffic load when the network is highly loaded.",
                "The dn+1 i value is updated based on dn i , the previous and current observations on the traffic situation in the neighborhood.",
                "Specifically, the update formula for dn+1 i is dn+1 i = dn i ∗ (1 + 1 − Li |DirectConn(Ai)| ) In this formula, the forward width is updated based on the traffic conditions of agent Ais neighborhood, i.e Li, and its previous value. (2) For each agent Aik in the PDn(Ai, QSj), the probability of forwarding the query to Aik is determined in the following way in order to assign higher forwarding probability to the neighboring agents with higher expected utility value: πn+1 i (QSj, αik ) = dn+1 i |PDn(Ai, QSj)| + β ∗ ` Uik (QSj) − PDU(Ai, QSj) |PDn(Ai, QSj)| ´ (3) where PDUn(Ai, QSj) = X o∈P Dn(Ai,QSj ) Uo(QSj) and QSj is the subsequent state of agent Aik after agent Ai forwards the search session with state QSj to its neighboring agent Aik ; If QSj = (qk, ttl0), then QSj = (qk, ttl0 − 1).",
                "In formula 3, the first term on the right of the equation, dn+1 i |P Dn(Ai,QSj )| , is used to to determine the forwarding probability by equally distributing the forward width, dn+1 i , to the agents in PDn(Ai, QSj) set.",
                "The second term is used to adjust the probability of being chosen so that agents with higher expected utility values will be favored. β is determined according to: β = min ` m − dn+1 i m ∗ umax − PDUn(Ai, QSj) , dn+1 i PDUn(Ai, QSj) − m ∗ umin ´ (4) where m = |PDn(Ai, QSj)|, umax = max o∈P Dn(Ao,QSj ) Uo(QSj) and umin = min o∈P Dn(Ao,QSj ) Uo(QSj) This formula guarantees that the final πn+1 i (QSj, αik ) value is well defined, i.e, 0 ≤ πn+1 i (QSj, αik ) ≤ 1 and X i πn+1 i (QSj, αik ) = dn+1 i However, such a solution does not explore all the possibilities.",
                "In order to balance between exploitation and exploration, a λ-Greedy approach is taken.",
                "In the λ-Greedy approach, in addition to assigning higher probability to those agents with higher expected utility value, as in the equation (3).",
                "Agents that appear to be not-so-good choices will also be sent queries based on a dynamic exploration rate.",
                "In particular, for agents in the set PDn(Ai, QSj), πn+1 i1 (QSj) is determined in the same way as the above, with the only difference being that dn+1 i is replaced with dn+1 i ∗ (1 − λn).",
                "The remaining search bandwidth is used for learning by assigning probability λn evenly to agents Ai2 in the set DirectConn(Ai) − PDn(Ai, QSj). πn+1 i2 (QSj, αik ) = dn+1 i ∗ λn |DirectConn(Ai) − PDn(Ai, QSj)| (5) where PDn(Ai, QSj) ⊂ DirectConn(Ai).",
                "Note that the exploration rate λ is not a constant and it decreases overtime.",
                "The λ is determined according to the following equation: λn+1 = λ0 ∗ e−c1n (6) where λ0 is the initial exploration rate, which is a constant; c1 is also a constant to adjust the decreasing rate of the exploration rate; n is the current time unit. 3.1.2 Update Expected Utility Once the <br>routing policy</br> at step n+1, πn+1 i , is determined based on the above formula, agent Ai can update its own expected utility, Un+1 i (QSi), based on the the updated <br>routing policy</br> resulted from the formula 5 and the updated U values of its neighboring agents.",
                "Under the assumption that after a query is forwarded to Ais neighbors the subsequent search sessions are independent, the update formula is similar to the Bellman update formula in Q-Learning: Un+1 i (QSj) = (1 − θi) ∗ Un i (QSj) + θi ∗ (Rn+1 i (QSj) + X k πn+1 i (QSj, αik )Un k (QSj)) (7) where QSj = (Qj, ttl − 1) is the next state of QSj = (Qj, ttl); Rn+1 i (QSj) is the expected local reward for query class Qk at agent Ai under the <br>routing policy</br> πn+1 i ; θi is the coefficient for deciding how much weight is given to the old value during the update process: the smaller θi value is, the faster the agent is expected to learn the real value, while the greater volatility of the algorithm, and vice versa.",
                "Rn+1 (s) is updated according to the following equation: 234 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Rn+1 i (QSj) = Rn i (QSj) +γi ∗ (r(QSj) − Rn i (QSj)) ∗ P(qj|Qj ) (8) where r(QSj) is the local reward associated with the search session.",
                "P(qj|Qj ) indicates how relevant the query qj is to the query type Qj, and γi is the learning rate for agent Ai.",
                "Depending on the similarity between a specific query qi and its corresponding query type Qi, the local reward associated with the search session has different impact on the Rn i (QSj) estimation.",
                "In the above formula, this impact is reflected by the coefficient, the P(qj|Qj) value. 3.1.3 Reward function After a search session stops when its TTL values expires, all search results are returned back to the user and are compared against the relevance judgment.",
                "Assuming the set of search results is SR, the reward Rew(SR) is defined as: Rew(SR) = j 1 if |Rel(SR)| > c |Rel(SR)| c otherwise. where SR is the set of returned search results, Rel(SR) is the set of relevant documents in the search results.",
                "This equation specifies that users give 1.0 reward if the number of returned relevant documents reaches a predefined number c. Otherwise, the reward is in proportion to the number of relevant documents returned.",
                "This rationale for setting up such a cut-off value is that the importance of recall ratio decreases with the abundance of relevant documents in real world, therefore users tend to focus on only a limited number of searched results.",
                "The details of the actual routing protocol will be introduced in Section 3.2 when we introduce how the learning algorithm is deployed in real systems. 3.2 Deployment of the Learning algorithm This section describes how the learning algorithm can be used in either a single-phase or a two-phase search process.",
                "In the single-phase search algorithm, search sessions start from the initiators of the queries.",
                "In contrast, in the two-step search algorithm, the query initiator first attempts to seek a more appropriate starting point for the query by introducing an exploratory step as described in Section 2.",
                "Despite the difference in the quality of starting points, the major part of the learning process for the two algorithms is largely the same as described in the following paragraphs.",
                "Before learning starts, each agent initializes the expected utility value for all possible states as 0.",
                "Thereafter, upon receipt of a query, in addition to the normal operations described in the previous section, an agent Ai also sets up a timer to wait for the search results returned from its downstream agents.",
                "Once the timer expires or it has received response from all its downstream agents, Ai merges and forwards the search results accrued from its downstream agents to its upstream agent.",
                "Setting up the timer speeds up the learning because agents can avoid waiting too long for the downstream agents to return search results.",
                "Note that these detailed results and corresponding agent information will still be stored at Ai until the feedback information is passed from its upstream agent and the performance of its downstream agents can be evaluated.",
                "The duration of the timer is related to the TTL value.",
                "In this paper, we set the timer to ttimer = ttli ∗ 2 + tf , where ttli ∗ 2 is the sum of the travel time of the queries in the network, and tf is the expected time period that users would like to wait.",
                "The search results will eventually be returned to the search session initiator A0.",
                "They will be compared to the relevance judgment that is provided by the final users (as described in the experiment section, the relevance judgement for the query set is provided along with the data collections).",
                "The reward will be calculated and propagated backward to the agents along the way that search results were passed.",
                "This is a reverse process of the search results propagation.",
                "In the process of propagating reward backward, agents update estimates of their own potential utility value, generate an upto-dated policy and pass their updated results to the neighboring agents based on the algorithm described in Section 3.",
                "Upon change of expected utility value, agent Ai sends out its updated utility estimation to its neighbors so that they can act upon the changed expected utility and corresponding state.",
                "This update message includes the potential reward as well as the corresponding state QSi = (qk, ttll) of agent Ai.",
                "Each neighboring agent, Aj, reacts to this kind of update message by updating the expected utility value for state QSj(qk, ttll + 1) according to the newly-announced changed expected utility value.",
                "Once they complete the update, the agents would again in turn inform related neighbors to update their values.",
                "This process goes on until the TTL value in the update message increases to the TTL limit.",
                "To speed up the learning process, while updating the expected utility values of an agent Ais neighboring agents we specify that Um(Qk, ttl0) >= Um(Qk, ttl1) iff ttl0 > ttl1 Thus, when agent Ai receives an updated expected utility value with ttl1, it also updates the expected utility values with any ttl0 > ttl1 if Um(Qk, ttl0) < Um(Qk, ttl1) to speed up convergence.",
                "This heuristic is based on the fact that the utility of a search session is a non-decreasing function of time t. 3.3 Discussion In formalizing the content routing system as a learning task, many assumptions are made.",
                "In real systems, these assumptions may not hold, and thus the learning algorithm may not converge.",
                "Two problems are of particular note, (1) This content routing problem does not have Markov properties.",
                "In contrast to IP-level based packet routing, the routing decision of each agent for a particular search session sj depends on the routing history of sj.",
                "Therefore, the assumption that all subsequent search sessions are independent does not hold in reality.",
                "This may lead to double counting problem that the relevant documents of some agents will be counted more than once for the state where the TTL value is more than 1.",
                "However, in the context of the hierarchical agent organizations, two factors mitigate this problems: first, the agents in each content group form a tree-like structure.",
                "With the absense of the cycles, the estimates inside the tree would be close to the accurate value.",
                "Secondly, the stochastic nature of the <br>routing policy</br> partly remedies this problem.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 235 (2) Another challenge for this learning algorithm is that in a real network environment observations on neighboring agents may not be able to be updated in time due to the communication delay or other situations.",
                "In addition, when neighboring agents update their estimates at the same time, oscillation may arise during the learning process[1].",
                "This paper explores several approaches to speed up the learning process.",
                "Besides the aforementioned strategy of updating the expected utility values, we also employ an active update strategy where agents notify their neighbors whenever its expected utility is updated.",
                "Thus a faster convergence speed can be achieved.",
                "This strategy contrasts to the Lazy update, where agents only echo their neighboring agents with their expected utility change when they exchange information.",
                "The trade off between the two approaches is the network load versus learning speed.",
                "The advantage of this learning algorithm is that once a <br>routing policy</br> is learned, agents do not have to repeatedly compare the similarity of queries as long as the network topology remains unchanged.",
                "Instead, agent just have to determine the classification of the query properly and follow the learned policies.",
                "The disadvantage of this learning-based approach is that the learning process needs to be conducted whenever the network structure changes.",
                "There are many potential extensions for this learning model.",
                "For example, a single measure is currently used to indicate the traffic load for an agents neighborhood.",
                "A simple extension would be to keep track of individual load for each neighbor of the agent. 4.",
                "EXPERIMENTSSETTINGSAND RESULTS The experiments are conducted on TRANO simulation toolkit with two sets of datasets, TREC-VLC-921 and TREC123-100.",
                "The following sub-sections introduce the TRANO testbed, the datasets, and the experimental results. 4.1 TRANO Testbed TRANO (Task Routing on Agent Network Organization) is a multi-agent based network based information retrieval testbed.",
                "TRANO is built on top of the Farm [4], a time based distributed simulator that provides a data dissemination framework for large scale distributed agent network based organizations.",
                "TRANO supports importation and exportation of agent organization profiles including topological connections and other features.",
                "Each TRANO agent is composed of an agent view structure and a control unit.",
                "In simulation, each agent is pulsed regularly and the agent checks the incoming message queues, performs local operations and then forwards messages to other agents . 4.2 Experimental Settings In our experiment, we use two standard datasets, TRECVLC-921 and TREC-123-100 datasets, to simulate the collections hosted on agents.",
                "The TREC-VLC-921 and TREC123-100 datasets were created by the U.S. National Institute for Standard Technology(NIST) for its TREC conferences.",
                "In distributed information retrieval domain, the two data collections are split to 921 and 100 sub-collections.",
                "It is observed that dataset TREC-VLC-921 is more heterogeneous than TREC-123-100 in terms of source, document length, and relevant document distribution from the statistics of the two data collections listed in [13].",
                "Hence, TREC-VLC-921 is much closer to real document distributions in P2P environments.",
                "Furthermore, TREC-123-100 is split into two sets of 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0 500 1000 1500 2000 2500 3000 ARSS Query number ARSS versus the number of incoming queries for TREC-VLC-921 SSLA-921 SSNA-921 Figure 3: ARSS(Average reward per search session) versus the number of search sessions for 1phase search in TREC-VLC-921 0 0.1 0.2 0.3 0.4 0.5 0.6 0 500 1000 1500 2000 2500 3000 ARSS Query number ARSS versus query number for TREC-VLC-921 TSLA-921 TSNA-921 Figure 4: ARSS(Average reward per search session) versus the number of search sessions for 2phase search in TREC-VLC-921 sub-collections in two ways: randomly and by source.",
                "The two partitions are denoted as TREC-123-100-Random and TREC-123-100-Source respectively.",
                "The documents in each subcollection in dataset TREC-123-100-Source are more coherent than those in TREC-123-100-Random.",
                "The two different sets of partitions allow us to observe how the distributed learning algorithm is affected by the homogeneity of the collections.",
                "The hierarchical agent organization is generated by the algorithm described in our previous algorithm [15].",
                "During the topology generation process, degree information of each agent is estimated by the algorithm introduced by Palmer et al. [9] with parameters α = 0.5 and β = 0.6.",
                "In our experiments, we estimate the upward limit and downward degree limit using linear discount factors 0.5, 0.8 and 1.0.",
                "Once the topology is built, queries randomly selected from the query set 301−350 on TREC-VLC-921 and query set 1− 50 on TREC-123-100-Random and TREC-123-100-Source are injected to the system based on a Poisson distribution P(N(t) = n) = (λt)n n! e−λ 236 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 0 50 100 150 200 250 300 350 400 0 500 1000 1500 2000 2500 3000 Cumulativeutility Query number Cumulative utility over the number of incoming queries TSLA-921 SSNA-921 SSLA-921 TSNA-921 Figure 5: The cumulative utility versus the number of search sessions TREC-VLC-921 In addition, we assume that all agents have an equal chance of getting queries from the environment, i.e, λ is the same for every agent.",
                "In our experiments, λ is set as 0.0543 so that the mean of the incoming queries from the environment to the agent network is 50 per time unit.",
                "The service time for the communication queue and local search queue, i.e tQij and trs, is set as 0.01 time unit and 0.05 time units respectively.",
                "In our experiments, there are ten types of queries acquired by clustering the query set 301 − 350 and 1 − 50. 4.3 Results analysis and evaluation Figure 3 demonstrates the ARSS(Average Reward per Search Session) versus the number of incoming queries over time for the the Single-Step based Non-learning Algorithm (SSNA), and the Single-Step Learning Algorithm(SSLA) for data collection TREC-VLC-921.",
                "It shows that the average reward for SSNA algorithm ranges from 0.02 − 0.06 and the performance changes little over time.",
                "The average reward for SSLA approach starts at the same level with the SSNA algorithm.",
                "But the performance increases over time and the average performance gain stabilizes at about 25% after query range 2000 − 3000.",
                "Figure 4 shows the ARSS(Average Reward per Search Session) versus the number of incoming queries over time for the the Two-Step based Non-learning Algorithm(TSNA), and the Two-Step Learning Algorithm(TSLA) for data collection TREC-VLC-921.",
                "The TSNA approach has a relatively consistent performance with the average reward ranges from 0.05 − 0.15.",
                "The average reward for TSLA approach, where learning algorithm is exploited, starts at the same level with the TSNA algorithm and improves the average reward over time until 2000−2500 queries joining the system.",
                "The results show that the average performance gain for TSLA approach over TNLA approach is 35% after stabilization.",
                "Figure 5 shows the cumulative utility versus the number of incoming queries over time for SSNA, SSLA,TSNA, and TSLA respectively.",
                "It illustrates that the cumulative utility of non-learning algorithms increases largely linearly over time, while the gains of learning-based algorithms accelerate when more queries enter the system.",
                "These experimental results demonstrate that learning-based approaches consistently perform better than non-learning based routing algorithm.",
                "Moreover, two-phase learning based algorithm is better than single-phase based learning algorithm because the maximal reward an agent can receive from searching its neighborhood within TTL hops is related to the total number of the relevant documents in that area.",
                "Thus, even the optimal <br>routing policy</br> can do little beyond reaching these relevant documents faster.",
                "On the contrary, the two-stepbased learning algorithm can relocate the search session to a neighborhood with more relevant documents.",
                "The TSLA combines the merits of both approaches and outperforms them.",
                "Table 1 lists the cumulative utility for datasets TREC123-100-Random and TREC-123-100-Source with hierarchical organizations.",
                "The five columns show the results for four different approaches.",
                "In particular, column TSNA-Random shows the results for dataset TREC-123-100-Random with the TSNA approach.",
                "The column TSLA-Random shows the results for dataset TREC-123-100-Random with the TSLA approach.",
                "There are two numbers in each cell in the column TSLA-Random.",
                "The first number is the actual cumulative utility while the second number is the percentage gain in terms of the utility over TSNA approach.",
                "Columns TSNA-Source and TSLA-Source show the results for dataset TREC-123-100-Source with TSNA and TSLA approaches respectively.",
                "Table 1 shows that the performance improvement for TREC-123-100-Random is not as significant as the other datasets.",
                "This is because that the documents in the sub-collection of TREC-123-100-Random are selected randomly which makes the collection model, the signature of the collection, less meaningful.",
                "Since both algorithms are designed based on the assumption that document collections can be well represented by their collection model, this result is not surprising.",
                "Overall, Figures 4, 5, and Table 1 demonstrate that the reinforcement learning based approach can considerably enhance the system performance for both data collections.",
                "However, it remains as future work to discover the correlation between the magnitude of the performance gains and the size of the data collection and/or the extent of the heterogeneity between the sub-collections. 5.",
                "RELATED WORK The content routing problem differs from the networklevel routing in packet-switched communication networks in that content-based routing occurs in application-level networks.",
                "In addition, the destination agents in our contentrouting algorithms are multiple and the addresses are not known in the routing process.",
                "IP-level Routing problems have been attacked from the reinforcement learning perspective[2, 5, 11, 12].",
                "These studies have explored fully distributed algorithms that are able, without central coordination to disseminate knowledge about the network, to find the shortest paths robustly and efficiently in the face of changing network topologies and changing link costs.",
                "There are two major classes of adaptive, distributed packet routing algorithms in the literature: distance-vector algorithms and link-state algorithms.",
                "While this line of studies carry a certain similarity with our work, it has mainly focused on packet-switched communication networks.",
                "In this domain, the destination of a packet is deterministic and unique.",
                "Each agent maintains estimations, probabilistically or deterministically, on the distance to a certain destination through its neighbors.",
                "A variant of Q-Learning techniques is deployed The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 237 Table 1: Cumulative Utility for Datasets TREC-123-100-Random and TREC-123-100-Source with Hierarchical Organization; The percentage numbers in the columns TSLA-Random and TSLA-Source demonstrate the performance gain over the algorithm without learning Query number TSNA-Random TSLA-Random TSNA-Source TSLA-Source 500 25.15 28.45 13% 24.00 21.05 -13% 1000 104.99 126.74 20% 93.95 96.44 2.6% 1250 149.79 168.40 12% 122.64 134.05 9.3% 1500 188.94 211.05 12% 155.30 189.60 22% 1750 235.49 261.60 11% 189.14 243.90 28% 2000 275.09 319.25 16% 219.0 278.80 26% to update the estimations to converge to the real distances.",
                "It has been discovered that the locality property is an important feature of information retrieval systems in user modeling studies[3].",
                "In P2P based content sharing systems, this property is exemplified by the phenomenon that users tend to send queries that represent only a limited number of topics and conversely, users in the same neighborhood are likely to share common interests and send similar queries [10].",
                "The learning based approach is perceived to be more beneficial for real distributed information retrieval systems which exhibit locality property.",
                "This is because the users traffic and query patterns can reduce the state space and speed up the learning process.",
                "Related work in taking advantage of this property include [7], where the authors attempted to address this problem by user modeling techniques. 6.",
                "CONCLUSIONS In this paper, a reinforcement-learning based approach is developed to improve the performance of distributed IR search algorithms.",
                "Particularly, agents maintain estimates, namely expected utility, on the downstream agents ability to provide relevant documents for incoming queries.",
                "These estimates are updated gradually by learning from the feedback information returned from previous search sessions.",
                "Based on the updated expected utility information, the agents modify their routing policies.",
                "Thereafter, these agents route the queries based on the learned policies and update the estimates on the expected utility based on the new routing policies.",
                "The experiments on two different distributed IR datasets illustrates that the reinforcement learning approach improves considerably the cumulative utility over time. 7.",
                "REFERENCES [1] S. Abdallah and V. Lesser.",
                "Learning the task allocation game.",
                "In AAMAS 06: Proceedings of the fifth international joint conference on Autonomous agents and multiagent systems, pages 850-857, New York, NY, USA, 2006.",
                "ACM Press. [2] J.",
                "A. Boyan and M. L. Littman.",
                "Packet routing in dynamically changing networks: A reinforcement learning approach.",
                "In Advances in Neural Information Processing Systems, volume 6, pages 671-678.",
                "Morgan Kaufmann Publishers, Inc., 1994. [3] J. C. French, A. L. Powell, J. P. Callan, C. L. Viles, T. Emmitt, K. J. Prey, and Y. Mou.",
                "Comparing the performance of database selection algorithms.",
                "In Research and Development in Information Retrieval, pages 238-245, 1999. [4] B. Horling, R. Mailler, and V. Lesser.",
                "Farm: A scalable environment for multi-agent development and evaluation.",
                "In Advances in Software Engineering for Multi-Agent Systems, pages 220-237, Berlin, 2004.",
                "Springer-Verlag. [5] M. Littman and J. Boyan.",
                "A distributed reinforcement learning scheme for network routing.",
                "In Proceedings of the International Workshop on Applications of Neural Networks to Telecommunications, 1993. [6] J. Lu and J. Callan.",
                "Federated search of text-based digital libraries in hierarchical peer-to-peer networks.",
                "In In ECIR05, 2005. [7] J. Lu and J. Callan.",
                "User modeling for full-text federated search in peer-to-peer networks.",
                "In ACM SIGIR 2006.",
                "ACM Press, 2006. [8] C. D. Manning and H. Sch¨utze.",
                "Foundations of Statistical Natural Language Processing.",
                "The MIT Press, Cambridge, Massachusetts, 1999. [9] C. R. Palmer and J. G. Steffan.",
                "Generating network topologies that obey power laws.",
                "In Proceedings of GLOBECOM 2000, November 2000. [10] K. Sripanidkulchai, B. Maggs, and H. Zhang.",
                "Efficient content location using interest-based locality in peer-topeer systems.",
                "In INFOCOM, 2003. [11] D. Subramanian, P. Druschel, and J. Chen.",
                "Ants and reinforcement learning: A case study in routing in dynamic networks.",
                "In In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence, pages 832-839, 1997. [12] J. N. Tao and L. Weaver.",
                "A multi-agent, policy gradient approach to network routing.",
                "In In Proceedings of the Eighteenth International Conference on Machine Learning, 2001. [13] H. Zhang, W. B. Croft, B. Levine, and V. Lesser.",
                "A multi-agent approach for peer-to-peer information retrieval.",
                "In Proceedings of Third International Joint Conference on Autonomous Agents and Multi-Agent Systems, July 2004. [14] H. Zhang and V. Lesser.",
                "Multi-agent based peer-to-peer information retrieval systems with concurrent search sessions.",
                "In Proceedings of the Fifth International Joint Conference on Autonomous Agents and Multi-Agent Systems, May 2006. [15] H. Zhang and V. R. Lesser.",
                "A dynamically formed hierarchical agent organization for a distributed content sharing system.",
                "In 2004 IEEE/WIC/ACM International Conference on Intelligent Agent Technology (IAT 2004), 20-24 September 2004, Beijing, China, pages 169-175.",
                "IEEE Computer Society, 2004. 238 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "For the first query in MFi, agent Ai determines which subset of its neighboring agents to forward it to based on the agents <br>routing policy</br> πi.",
                "Section 3.3 discusses the convergence of the learning algorithm. 3.1 The Model An agents <br>routing policy</br> takes the state of a search session as input and output the routing actions for that query.",
                "The <br>routing policy</br> πi of agent Ai is stochastic and its outcome for a search session with state QSj is defined as: πi(QSj) = {(αi0 , πi(QSi, αi0 )), (αi1 , πi(QSi, αi1 )), ...} (2) Note that operator πi is overloaded to represent either the probabilistic policy for a search session with state QSj, denoted as πi(QSj); or the probability of forwarding the query to a specific neighboring agent Aik ∈ DirectConn(Ai) under the policy πi(QSj), denoted as πi(QSj, αik ).",
                "The estimated utility information will be used to update Ais expected utility for its <br>routing policy</br>.",
                "Therefore Lm characterizes the utilization of an agents communication channel, and thus provide non-local information for Ams neighbors to adjust the parameters of their <br>routing policy</br> to avoid inundating their downstream agents."
            ],
            "translated_annotated_samples": [
                "Para la primera consulta en MFi, el agente Ai determina a qué subconjunto de sus agentes vecinos reenviarla basándose en la <br>política de enrutamiento</br> de los agentes πi.",
                "La sección 3.3 discute la convergencia del algoritmo de aprendizaje. 3.1 El Modelo La <br>política de enrutamiento</br> de un agente toma el estado de una sesión de búsqueda como entrada y produce las acciones de enrutamiento para esa consulta.",
                "La <br>política de enrutamiento</br> πi del agente Ai es estocástica y su resultado para una sesión de búsqueda con estado QSj se define como: πi(QSj) = {(αi0 , πi(QSi, αi0 )), (αi1 , πi(QSi, αi1 )), ...} (2) Nótese que el operador πi está sobrecargado para representar tanto la política probabilística para una sesión de búsqueda con estado QSj, denotada como πi(QSj); como la probabilidad de enviar la consulta a un agente vecino específico Aik ∈ DirectConn(Ai) bajo la política πi(QSj), denotada como πi(QSj, αik).",
                "La información de utilidad estimada se utilizará para actualizar la utilidad esperada de A en su <br>política de enrutamiento</br>.",
                "Por lo tanto, Lm caracteriza la utilización de un canal de comunicación de agentes, y así proporciona información no local a los vecinos de Ams para ajustar los parámetros de su <br>política de enrutamiento</br> y evitar inundar a sus agentes aguas abajo."
            ],
            "translated_text": "Un algoritmo de búsqueda distribuida basado en aprendizaje por refuerzo para sistemas jerárquicos de recuperación de información entre pares. En estos enfoques, los agentes dependen de la similitud de contenido entre las consultas entrantes y sus agentes vecinos directos para dirigir las sesiones de búsqueda distribuida. Sin embargo, tal heurística es miope en el sentido de que los agentes vecinos pueden no estar conectados con agentes más relevantes. En este documento, se desarrolla un enfoque basado en aprendizaje por refuerzo en línea para aprovechar las características dinámicas en tiempo de ejecución de los sistemas de recuperación de información P2P, representadas por información sobre sesiones de búsqueda pasadas. Específicamente, los agentes mantienen estimaciones sobre las habilidades de los agentes aguas abajo para proporcionar documentos relevantes para las consultas entrantes. Estas estimaciones se actualizan gradualmente al aprender de la información de retroalimentación devuelta de sesiones de búsqueda anteriores. Basándose en esta información, los agentes derivan políticas de enrutamiento correspondientes. Posteriormente, estos agentes dirigen las consultas basándose en las políticas aprendidas y actualizan las estimaciones según las nuevas políticas de enrutamiento. Los resultados experimentales demuestran que el algoritmo de aprendizaje mejora considerablemente el rendimiento del enrutamiento en dos conjuntos de colecciones de pruebas que han sido utilizados en una variedad de estudios de recuperación de información distribuida. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagente Términos Generales Algoritmos, Rendimiento, Experimentación 1. En los últimos años ha habido un creciente interés en estudiar cómo controlar los procesos de búsqueda en sistemas de recuperación de información (IR) basados en pares (P2P) [6, 13, 14, 15]. En esta línea de investigación, uno de los problemas centrales que preocupa a los investigadores es dirigir de manera eficiente las consultas de los usuarios en la red a los agentes que poseen los documentos apropiados. En ausencia de información global, las estrategias dominantes para abordar este problema son enfoques basados en la similitud de contenido [6, 13, 14, 15]. Si bien la similitud de contenido entre consultas y nodos locales parece ser un indicador creíble para la cantidad de documentos relevantes que residen en cada nodo, estos enfoques están limitados por una serie de factores. En primer lugar, las métricas basadas en similitud pueden ser miope ya que los nodos localmente relevantes pueden no estar conectados a otros nodos relevantes. Segundo, los enfoques basados en la similitud no tienen en cuenta las características de tiempo de ejecución de los sistemas de IR P2P, incluidos los parámetros ambientales, el uso del ancho de banda y la información histórica de las sesiones de búsqueda pasadas, que proporcionan información valiosa para los algoritmos de enrutamiento de consultas. En este artículo, desarrollamos un enfoque de IR basado en aprendizaje por refuerzo para mejorar el rendimiento de los algoritmos de búsqueda de IR distribuidos. Los agentes pueden adquirir mejores estrategias de búsqueda al recopilar y analizar información de retroalimentación de sesiones de búsqueda anteriores. En particular, los agentes mantienen estimaciones, es decir, utilidad esperada, sobre las capacidades de los agentes aguas abajo de proporcionar documentos relevantes para tipos específicos de consultas entrantes. Estas estimaciones se actualizan gradualmente al aprender de la información de retroalimentación devuelta de sesiones de búsqueda anteriores. Basándose en la información actualizada de utilidad esperada, los agentes derivan políticas de enrutamiento correspondientes. Posteriormente, estos agentes dirigen las consultas basándose en las políticas aprendidas y actualizan las estimaciones sobre la utilidad esperada en función de las nuevas políticas de enrutamiento. Este proceso se lleva a cabo de manera iterativa. El objetivo del algoritmo de aprendizaje, aunque consume ancho de banda de red, es reducir el tiempo de enrutamiento para procesar más consultas por unidad de tiempo y al mismo tiempo encontrar más documentos relevantes. Esto contrasta con los enfoques basados en la similitud de contenido, donde se repiten operaciones similares para cada consulta entrante y el tiempo de procesamiento se mantiene en gran medida constante con el tiempo. Otra forma de ver este documento es que nuestro enfoque básico para la búsqueda de IR distribuida es construir una red de superposición jerárquica (organización de agentes) basada en la medida de similitud de contenido entre las colecciones de documentos de los agentes de manera ascendente. En trabajos anteriores, hemos demostrado que esta organización mejora significativamente el rendimiento de búsqueda. Sin embargo, esta estructura organizativa no tiene en cuenta los patrones de llegada de las consultas, incluyendo su frecuencia, tipos y dónde entran en el sistema, ni el ancho de banda de comunicación disponible en la red y las capacidades de procesamiento de los agentes individuales. La intención del aprendizaje por refuerzo es adaptar las decisiones de enrutamiento de los agentes a las situaciones dinámicas de la red y aprender de sesiones de búsqueda pasadas. Específicamente, las contribuciones de este artículo incluyen: (1) un enfoque basado en aprendizaje por refuerzo para que los agentes adquieran políticas de enrutamiento satisfactorias basadas en estimaciones de la contribución potencial de sus agentes vecinos; (2) dos estrategias para acelerar el proceso de aprendizaje. Según nuestro mejor conocimiento, esta es una de las primeras aplicaciones de aprendizaje por refuerzo para abordar problemas de compartición de contenido distribuido, lo cual indica algunos de los problemas al aplicar el refuerzo en una aplicación compleja. El resto de este documento está organizado de la siguiente manera: la Sección 2 revisa los sistemas jerárquicos de intercambio de contenido y el algoritmo de búsqueda de dos fases basado en dicha topología. La Sección 3 describe un enfoque basado en aprendizaje por refuerzo para dirigir el proceso de enrutamiento; la Sección 4 detalla la configuración experimental y analiza los resultados. La sección 5 discute estudios relacionados y la sección 6 concluye el artículo. BÚSQUEDA EN SISTEMAS JERÁRQUICOS DE IR P2P Esta sección revisa brevemente nuestros enfoques básicos para los sistemas jerárquicos de IR P2P. En un sistema jerárquico de recuperación de información P2P ilustrado en la Fig.1, los agentes están conectados entre sí a través de tres tipos de enlaces: enlaces ascendentes, enlaces descendentes y enlaces laterales. En las siguientes secciones, denotamos el conjunto de agentes que están directamente conectados al agente Ai como DirectConn(Ai), el cual se define como DirectConn(Ai) = NEI(Ai) ∪ PAR(Ai) ∪ CHL(Ai), donde NEI(Ai) es el conjunto de agentes vecinos conectados a Ai a través de enlaces laterales; PAR(Ai) es el conjunto de agentes a los que el agente Ai está conectado a través de enlaces ascendentes y CHL(Ai) es el conjunto de agentes a los que el agente Ai se conecta a través de enlaces descendentes. Estos enlaces se establecen a través de un proceso de agrupamiento distribuido basado en la similitud de contenido de abajo hacia arriba[15]. Estos enlaces son luego utilizados por agentes para localizar otros agentes que contienen documentos relevantes a las consultas dadas. Un agente típico Ai en nuestro sistema utiliza dos colas: una cola de búsqueda local, LSi, y una cola de reenvío de mensajes MFi. Los estados de las dos colas constituyen los estados internos de un agente. La cola de búsqueda local LSi almacena sesiones de búsqueda programadas para procesamiento local. Es una cola de prioridad y el agente Ai siempre selecciona las consultas más prometedoras para procesarlas con el fin de maximizar la utilidad global. MFi consiste en un conjunto de consultas para reenviar y se procesa de manera FIFO (primero en entrar, primero en salir). Para la primera consulta en MFi, el agente Ai determina a qué subconjunto de sus agentes vecinos reenviarla basándose en la <br>política de enrutamiento</br> de los agentes πi. Estas decisiones de enrutamiento determinan cómo se lleva a cabo el proceso de búsqueda en la red. En este documento, llamamos a Ai como el agente aguas arriba de Ajs y a Aj como el agente aguas abajo de Ais si A4 A5 A6 A7 A2 A3 A9 NEI(A2)={A3} PAR(A2)={A1} CHL(A2)={A4,A5} A1 A8 Figura 1: Una fracción de un sistema jerárquico P2PIR en el que un agente Ai dirige una consulta al agente Aj. El protocolo de búsqueda distribuida de nuestra organización de agentes jerárquicos está compuesto por dos pasos. En el primer paso, al recibir una consulta qk en el tiempo tl de un usuario, el agente Ai inicia una sesión de búsqueda si al sondear a sus agentes vecinos Aj ∈ NEI(Ai) con el mensaje PROBE para el valor de similitud Sim(qk, Aj) entre qk y Aj. Aquí, Ai se define como el iniciador de la consulta de la sesión de búsqueda si. En el segundo paso, Ai selecciona un grupo de agentes más prometedores para iniciar el proceso de búsqueda real con el mensaje BUSCAR. Estos mensajes de BÚSQUEDA contienen un parámetro TTL (Tiempo de Vida) además de la consulta. El valor TTL disminuye en 1 después de cada salto. En el proceso de búsqueda, los agentes descartan aquellas consultas que han sido procesadas previamente o cuyo TTL disminuye a 0, lo que evita que las consultas se repitan infinitamente en el sistema. La sesión de búsqueda termina cuando todos los agentes que reciben la consulta la descartan o el TTL disminuye a 0. Tras recibir mensajes de BÚSQUEDA para qk, los agentes programan actividades locales que incluyen búsqueda local, reenvío de qk a sus vecinos y devolución de resultados de búsqueda al iniciador de la consulta. Este proceso y los algoritmos relacionados se detallan en [15, 14]. 3. En el enfoque de búsqueda basado en aprendizaje por refuerzo, mencionado anteriormente, las decisiones de enrutamiento de un agente Ai dependen de la comparación de similitud entre las consultas entrantes y los agentes vecinos de Ai para reenviar esas consultas a agentes relevantes sin inundar la red con mensajes de consulta innecesarios. Sin embargo, esta heurística es miope porque un vecino directo relevante no necesariamente está conectado con otros agentes relevantes. En esta sección, proponemos un enfoque más general al plantear este problema como una tarea de aprendizaje por refuerzo. En busca de una mayor flexibilidad, los agentes pueden alternar entre dos modos: modo de aprendizaje y modo sin aprendizaje. En el modo de no aprendizaje, los agentes operan de la misma manera que lo hacen en los procesos de búsqueda distribuida normales descritos en [14, 15]. Por otro lado, en el modo de aprendizaje, en paralelo con las sesiones de búsqueda distribuida, los agentes también participan en un proceso de aprendizaje que se detallará en esta sección. Ten en cuenta que en el protocolo de aprendizaje, el proceso de aprendizaje no interfiere con el proceso de búsqueda distribuida. Los agentes pueden elegir iniciar y detener procesos de aprendizaje sin afectar el rendimiento del sistema. En particular, dado que el proceso de aprendizaje consume recursos de red (especialmente ancho de banda), los agentes pueden optar por iniciar el aprendizaje solo cuando la carga de la red es relativamente baja, minimizando así los costos adicionales de comunicación incurridos por el algoritmo de aprendizaje. La sección está estructurada de la siguiente manera, la Sección 3.1 describe 232 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) un modelo basado en aprendizaje por refuerzo. La sección 3.2 describe un protocolo para implementar el algoritmo de aprendizaje en la red. La sección 3.3 discute la convergencia del algoritmo de aprendizaje. 3.1 El Modelo La <br>política de enrutamiento</br> de un agente toma el estado de una sesión de búsqueda como entrada y produce las acciones de enrutamiento para esa consulta. En nuestro trabajo, el estado de una sesión de búsqueda sj se estipula como: QSj = (qk, ttlj) donde ttlj es el número de saltos que quedan para la sesión de búsqueda sj, qk es la consulta específica. QL es un atributo de qk que indica a qué tipo de consultas qk probablemente pertenecen. El conjunto de QL puede ser generado ejecutando un algoritmo de clasificación en línea simple en todas las consultas que han sido procesadas por los agentes, o un algoritmo fuera de línea en un conjunto de entrenamiento pre-designado. La suposición aquí es que el conjunto de tipos de consultas se aprende de antemano y pertenece al conocimiento común de los agentes en la red. El trabajo futuro incluye explorar cómo se puede lograr el aprendizaje cuando esta suposición no se cumple. Dado el conjunto de tipos de consultas establecido, una consulta entrante qi puede ser clasificada en una clase de consulta Q(qi) mediante la fórmula: Q(qi) = arg max Qj P(qi|Qj) (1) donde P(qi|Qj) indica la probabilidad de que la consulta qi sea generada por la clase de consulta Qj [8]. El conjunto de acciones de enrutamiento atómico de un agente Ai se denota como {αi}, donde {αi} se define como αi = {αi0 , αi1 , ..., αin }. Un elemento αij representa una acción para dirigir una consulta dada al agente vecino Aij ∈ DirectConn(Ai). La <br>política de enrutamiento</br> πi del agente Ai es estocástica y su resultado para una sesión de búsqueda con estado QSj se define como: πi(QSj) = {(αi0 , πi(QSi, αi0 )), (αi1 , πi(QSi, αi1 )), ...} (2) Nótese que el operador πi está sobrecargado para representar tanto la política probabilística para una sesión de búsqueda con estado QSj, denotada como πi(QSj); como la probabilidad de enviar la consulta a un agente vecino específico Aik ∈ DirectConn(Ai) bajo la política πi(QSj), denotada como πi(QSj, αik). Por lo tanto, la ecuación (2) significa que la probabilidad de reenviar la sesión de búsqueda al agente Ai0 es πi(QSi, αi0) y así sucesivamente. Bajo esta política estocástica, la acción de enrutamiento es no determinística. La ventaja de esta estrategia es que los mejores agentes vecinos no serán seleccionados repetidamente, mitigando así las posibles situaciones de puntos calientes. La utilidad esperada, Un i (QSj), se utiliza para estimar la ganancia potencial de utilidad al dirigir el tipo de consulta QSj al agente Ai bajo la política πn i. El superíndice n indica el valor en la n-ésima iteración en un proceso de aprendizaje iterativo. La utilidad esperada proporciona orientación de enrutamiento para futuras sesiones de búsqueda. En el proceso de búsqueda, cada agente Ai mantiene observaciones parciales de los estados de sus vecinos, como se muestra en la Figura 2. La observación parcial incluye información no local como la estimación de la utilidad potencial de su vecino Am para el estado de consulta QSj, denotado como Um(QSj), así como la información de carga, Lm. Estas observaciones son actualizadas periódicamente por los vecinos. La información de utilidad estimada se utilizará para actualizar la utilidad esperada de A en su <br>política de enrutamiento</br>. Cargar información de utilidad esperada para diferentes tipos de consultas de agentes vecinos... A0 A1 A3 A2 Un 0 (QS0) ... ... ... ... ...... Un 0 (QS1) Un 1 (QS1) Un 2 (QS1) Un 3 (QS1) Un 1 (QS0) Un 2 (QS0) Un 3 (QS0) Ln 0 Ln 1 Ln 2 Ln 3 ... QS0 QS1 ... Figura 2: La Observación Parcial del Agente Ais sobre sus vecinos (A0, A1...) La información de carga de Am, Lm, se define como Lm = |MFm| Cm, donde |MFm| es la longitud de la cola de mensajes en espera y Cm es la tasa de servicio de la cola de mensajes en espera del agente Am. Por lo tanto, Lm caracteriza la utilización de un canal de comunicación de agentes, y así proporciona información no local a los vecinos de Ams para ajustar los parámetros de su <br>política de enrutamiento</br> y evitar inundar a sus agentes aguas abajo. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "query": {
            "translated_key": "consulta",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Reinforcement Learning based Distributed Search Algorithm For Hierarchical Peer-to-Peer Information Retrieval Systems Haizheng Zhang College of Information Science and Technology Pennsylvania State University University Park, PA 16803 hzhang@ist.psu.edu Victor Lesser Department of Computer Science University Of Massachusetts Amherst, MA 01003 lesser@cs.umass.edu ABSTRACT The dominant existing routing strategies employed in peerto-peer(P2P) based information retrieval(IR) systems are similarity-based approaches.",
                "In these approaches, agents depend on the content similarity between incoming queries and their direct neighboring agents to direct the distributed search sessions.",
                "However, such a heuristic is myopic in that the neighboring agents may not be connected to more relevant agents.",
                "In this paper, an online reinforcement-learning based approach is developed to take advantage of the dynamic run-time characteristics of P2P IR systems as represented by information about past search sessions.",
                "Specifically, agents maintain estimates on the downstream agents abilities to provide relevant documents for incoming queries.",
                "These estimates are updated gradually by learning from the feedback information returned from previous search sessions.",
                "Based on this information, the agents derive corresponding routing policies.",
                "Thereafter, these agents route the queries based on the learned policies and update the estimates based on the new routing policies.",
                "Experimental results demonstrate that the learning algorithm improves considerably the routing performance on two test collection sets that have been used in a variety of distributed IR studies.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Algorithms, Performance, Experimentation 1.",
                "INTRODUCTION Over the last few years there have been increasing interests in studying how to control the search processes in peer-to-peer(P2P) based information retrieval(IR) systems [6, 13, 14, 15].",
                "In this line of research, one of the core problems that concerns researchers is to efficiently route user queries in the network to agents that are in possession of appropriate documents.",
                "In the absence of global information, the dominant strategies in addressing this problem are content-similarity based approaches [6, 13, 14, 15].",
                "While the content similarity between queries and local nodes appears to be a creditable indicator for the number of relevant documents residing on each node, these approaches are limited by a number of factors.",
                "First of all, similaritybased metrics can be myopic since locally relevant nodes may not be connected to other relevant nodes.",
                "Second, the similarity-based approaches do not take into account the run-time characteristics of the P2P IR systems, including environmental parameters, bandwidth usage, and the historical information of the past search sessions, that provide valuable information for the <br>query</br> routing algorithms.",
                "In this paper, we develop a reinforcement learning based IR approach for improving the performance of distributed IR search algorithms.",
                "Agents can acquire better search strategies by collecting and analyzing feedback information from previous search sessions.",
                "Particularly, agents maintain estimates, namely expected utility, on the downstream agents capabilities of providing relevant documents for specific types of incoming queries.",
                "These estimates are updated gradually by learning from the feedback information returned from previous search sessions.",
                "Based on the updated expected utility information, the agents derive corresponding routing policies.",
                "Thereafter, these agents route the queries based on the learned policies and update the estimates on the expected utility based on the new routing policies.",
                "This process is conducted in an iterative manner.",
                "The goal of the learning algorithm, even though it consumes some network bandwidth, is to shorten the routing time so that more queries are processed per time unit while at the same time finding more relevant documents.",
                "This contrasts with the content-similarity based approaches where similar operations are repeated for every incoming <br>query</br> and the processing time keeps largely constant over time.",
                "Another way of viewing this paper is that our basic approach to distributed IR search is to construct a hierarchical overlay network(agent organization) based on the contentsimilarity measure among agents document collections in a bottom-up fashion.",
                "In the past work, we have shown that this organization improves search performance significantly.",
                "However, this organizational structure does not take into account the arrival patterns of queries, including their frequency, types, and where they enter the system, nor the available communication bandwidth of the network and processing capabilities of individual agents.",
                "The intention of the reinforcement learning is to adapt the agents routing decisions to the dynamic network situations and learn from past search sessions.",
                "Specifically, the contributions of this paper include: (1) a reinforcement learning based approach for agents to acquire satisfactory routing policies based on estimates of the potential contribution of their neighboring agents; (2) two strategies to speed up the learning process.",
                "To our best knowledge, this is one of the first reinforcement learning applications in addressing distributed content sharing problems and it is indicative of some of the issues in applying reinforcement in a complex application.",
                "The remainder of this paper is organized as follows: Section 2 reviews the hierarchical content sharing systems and the two-phase search algorithm based on such topology.",
                "Section 3 describes a reinforcement learning based approach to direct the routing process; Section 4 details the experimental settings and analyze the results.",
                "Section 5 discusses related studies and Section 6 concludes the paper. 2.",
                "SEARCH IN HIERARCHICAL P2P IR SYSTEMS This section briefly reviews our basic approaches to hierarchical P2P IR systems.",
                "In a hierarchical P2P IR system illustrated in Fig.1, agents are connected to each other through three types of links: upward links, downward links, and lateral links.",
                "In the following sections, we denote the set of agents that are directly connected to agent Ai as DirectConn(Ai), which is defined as DirectConn(Ai) = NEI(Ai) ∪ PAR(Ai) ∪ CHL(Ai) , where NEI(Ai) is the set of neighboring agents connected to Ai through lateral links; PAR(Ai) is the set of agents whom agent Ai is connected to through upward links and CHL(Ai) is the set of agents that agent Ai connects to through downward links.",
                "These links are established through a bottom-up content-similarity based distributed clustering process[15].",
                "These links are then used by agents to locate other agents that contain documents relevant to the given queries.",
                "A typical agent Ai in our system uses two queues: a local search queue, LSi, and a message forwarding queue MFi.",
                "The states of the two queues constitute the internal states of an agent.",
                "The local search queue LSi stores search sessions that are scheduled for local processing.",
                "It is a priority queue and agent Ai always selects the most promising queries to process in order to maximize the global utility.",
                "MFi consists of a set of queries to forward on and is processed in a FIFO (first in first out) fashion.",
                "For the first <br>query</br> in MFi, agent Ai determines which subset of its neighboring agents to forward it to based on the agents routing policy πi.",
                "These routing decisions determine how the search process is conducted in the network.",
                "In this paper, we call Ai as Ajs upstream agent and Aj as Ais downstream agent if A4 A5 A6 A7 A2 A3 A9 NEI(A2)={A3} PAR(A2)={A1} CHL(A2)={A4,A5} A1 A8 Figure 1: A fraction of a hierarchical P2PIR system an agent Ai routes a <br>query</br> to agent Aj.",
                "The distributed search protocol of our hierarchical agent organization is composed of two steps.",
                "In the first step, upon receipt of a <br>query</br> qk at time tl from a user, agent Ai initiates a search session si by probing its neighboring agents Aj ∈ NEI(Ai) with the message PROBE for the similarity value Sim(qk, Aj) between qk and Aj.",
                "Here, Ai is defined as the <br>query</br> initiator of search session si.",
                "In the second step, Ai selects a group of the most promising agents to start the actual search process with the message SEARCH.",
                "These SEARCH messages contain a TTL (Time To Live) parameter in addition to the <br>query</br>.",
                "The TTL value decreases by 1 after each hop.",
                "In the search process, agents discard those queries that either have been previously processed or whose TTL drops to 0, which prevents queries from looping in the system forever.",
                "The search session ends when all the agents that receive the <br>query</br> drop it or TTL decreases to 0.",
                "Upon receipt of SEARCH messages for qk, agents schedule local activities including local searching, forwarding qk to their neighbors, and returning search results to the <br>query</br> initiator.",
                "This process and related algorithms are detailed in [15, 14]. 3.",
                "A BASIC REINFORCEMENTLEARNING BASED SEARCH APPROACH In the aforementioned distributed search algorithm, the routing decisions of an agent Ai rely on the similarity comparison between incoming queries and Ais neighboring agents in order to forward those queries to relevant agents without flooding the network with unnecessary <br>query</br> messages.",
                "However, this heuristic is myopic because a relevant direct neighbor is not necessarily connected to other relevant agents.",
                "In this section, we propose a more general approach by framing this problem as a reinforcement learning task.",
                "In pursuit of greater flexibility, agents can switch between two modes: learning mode and non-learning mode.",
                "In the non-learning mode, agents operate in the same way as they do in the normal distributed search processes described in [14, 15].",
                "On the other hand, in the learning mode, in parallel with distributed search sessions, agents also participate in a learning process which will be detailed in this section.",
                "Note that in the learning protocol, the learning process does not interfere with the distributed search process.",
                "Agents can choose to initiate and stop learning processes without affecting the system performance.",
                "In particular, since the learning process consumes network resources (especially bandwidth), agents can choose to initiate learning only when the network load is relatively low, thus minimizing the extra communication costs incurred by the learning algorithm.",
                "The section is structured as follows, Section 3.1 describes 232 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) a reinforcement learning based model.",
                "Section 3.2 describes a protocol to deploy the learning algorithm in the network.",
                "Section 3.3 discusses the convergence of the learning algorithm. 3.1 The Model An agents routing policy takes the state of a search session as input and output the routing actions for that <br>query</br>.",
                "In our work, the state of a search session sj is stipulated as: QSj = (qk, ttlj) where ttlj is the number of hops that remains for the search session sj , qk is the specific <br>query</br>.",
                "QL is an attribute of qk that indicates which type of queries qk most likely belong to.",
                "The set of QL can be generated by running a simple online classification algorithm on all the queries that have been processed by the agents, or an oﬄine algorithm on a pre-designated training set.",
                "The assumption here is that the set of <br>query</br> types is learned ahead of time and belongs to the common knowledge of the agents in the network.",
                "Future work includes exploring how learning can be accomplished when this assumption does not hold.",
                "Given the <br>query</br> types set, an incoming <br>query</br> qi can be classified to one query class Q(qi) by the formula: Q(qi) = arg max Qj P(qi|Qj) (1) where P(qi|Qj ) indicates the likelihood that the query qi is generated by the query class Qj [8].",
                "The set of atomic routing actions of an agent Ai is denoted as {αi}, where {αi} is defined as αi = {αi0 , αi1 , ..., αin }.",
                "An element αij represents an action to route a given <br>query</br> to the neighboring agent Aij ∈ DirectConn(Ai).",
                "The routing policy πi of agent Ai is stochastic and its outcome for a search session with state QSj is defined as: πi(QSj) = {(αi0 , πi(QSi, αi0 )), (αi1 , πi(QSi, αi1 )), ...} (2) Note that operator πi is overloaded to represent either the probabilistic policy for a search session with state QSj, denoted as πi(QSj); or the probability of forwarding the <br>query</br> to a specific neighboring agent Aik ∈ DirectConn(Ai) under the policy πi(QSj), denoted as πi(QSj, αik ).",
                "Therefore, equation (2) means that the probability of forwarding the search session to agent Ai0 is πi(QSi, αi0 ) and so on.",
                "Under this stochastic policy, the routing action is nondeterministic.",
                "The advantage of such a strategy is that the best neighboring agents will not be selected repeatedly, thereby mitigating the potential hot spots situations.",
                "The expected utility, Un i (QSj), is used to estimate the potential utility gain of routing <br>query</br> type QSj to agent Ai under policy πn i .",
                "The superscript n indicates the value at the nth iteration in an iterative learning process.",
                "The expected utility provides routing guidance for future search sessions.",
                "In the search process, each agent Ai maintains partial observations of its neighbors states, as shown in Fig. 2.",
                "The partial observation includes non-local information such as the potential utility estimation of its neighbor Am for <br>query</br> state QSj, denoted as Um(QSj), as well as the load information, Lm.",
                "These observations are updated periodically by the neighbors.",
                "The estimated utility information will be used to update Ais expected utility for its routing policy.",
                "Load Information Expected Utility For Different <br>query</br> Types Neighboring Agents ... A0 A1 A3 A2 Un 0 (QS0) ... ... ... ... ...... Un 0 (QS1) Un 1 (QS1) Un 2 (QS1) Un 3 (QS1) Un 1 (QS0) Un 2 (QS0) Un 3 (QS0) Ln 0 Ln 1 Ln 2 Ln 3 ... QS0 QS1 ...",
                "Figure 2: Agent Ais Partial Observation about its neighbors(A0, A1...) The load information of Am, Lm, is defined as Lm = |MFm| Cm , where |MFm| is the length of the message-forward queue and Cm is the service rate of agent Ams message-forward queue.",
                "Therefore Lm characterizes the utilization of an agents communication channel, and thus provide non-local information for Ams neighbors to adjust the parameters of their routing policy to avoid inundating their downstream agents.",
                "Note that based on the characteristics of the queries entering the system and agents capabilities, the loading of agents may not be uniform.",
                "After collecting the utilization rate information from all its neighbors, agent Ai computes Li as a single measure for assessing the average load condition of its neighborhood: Li = P k Lk |DirectConn(Ai)| Agents exploit Li value in determining the routing probability in its routing policy.",
                "Note that, as described in Section 3.2, information about neighboring agents is piggybacked with the <br>query</br> message propagated among the agents whenever possible to reduce the traffic overhead. 3.1.1 Update the Policy An iterative update process is introduced for agents to learn a satisfactory stochastic routing policy.",
                "In this iterative process, agents update their estimates on the potential utility of their current routing policies and then propagate the updated estimates to their neighbors.",
                "Their neighbors then generate a new routing policy based on the updated observation and in turn they calculate the expected utility based on the new policies and continue this iterative process.",
                "In particular, at time n, given a set of expected utility, an agent Ai, whose directly connected agents set is DirectConn(Ai) = {Ai0 , ..., Aim }, determines its corresponding stochastic routing policy for a search session of state QSj based on the following steps: (1) Ai first selects a subset of agents as the potential downstream agents from set DirectConn(Ai), denoted as PDn(Ai, QSj).",
                "The size of the potential downstream agent is specified as |PDn(Ai, QSj)| = min(|NEI(Ai), dn i + k)| where k is a constant and is set to 3 in this paper; dn i , the forward width, is defined as the expected number of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 233 neighboring agents that agent Ai can forward to at time n. This formula specifies that the potential downstream agent set PDn(Ai, QSj) is either the subset of neighboring agents with dn i + k highest expected utility value for state QSj among all the agents in DirectConn(Ai), or all their neighboring agents.",
                "The k is introduced based on the idea of a stochastic routing policy and it makes the forwarding probability of the dn i +k highest agent less than 100%.",
                "Note that if we want to limit the number of downstream agents for search session sj as 5, the probability of forwarding the <br>query</br> to all neighboring agents should add up to 5.",
                "Setting up dn i value properly can improve the utilization rate of the network bandwidth when much of the network is idle while mitigating the traffic load when the network is highly loaded.",
                "The dn+1 i value is updated based on dn i , the previous and current observations on the traffic situation in the neighborhood.",
                "Specifically, the update formula for dn+1 i is dn+1 i = dn i ∗ (1 + 1 − Li |DirectConn(Ai)| ) In this formula, the forward width is updated based on the traffic conditions of agent Ais neighborhood, i.e Li, and its previous value. (2) For each agent Aik in the PDn(Ai, QSj), the probability of forwarding the <br>query</br> to Aik is determined in the following way in order to assign higher forwarding probability to the neighboring agents with higher expected utility value: πn+1 i (QSj, αik ) = dn+1 i |PDn(Ai, QSj)| + β ∗ ` Uik (QSj) − PDU(Ai, QSj) |PDn(Ai, QSj)| ´ (3) where PDUn(Ai, QSj) = X o∈P Dn(Ai,QSj ) Uo(QSj) and QSj is the subsequent state of agent Aik after agent Ai forwards the search session with state QSj to its neighboring agent Aik ; If QSj = (qk, ttl0), then QSj = (qk, ttl0 − 1).",
                "In formula 3, the first term on the right of the equation, dn+1 i |P Dn(Ai,QSj )| , is used to to determine the forwarding probability by equally distributing the forward width, dn+1 i , to the agents in PDn(Ai, QSj) set.",
                "The second term is used to adjust the probability of being chosen so that agents with higher expected utility values will be favored. β is determined according to: β = min ` m − dn+1 i m ∗ umax − PDUn(Ai, QSj) , dn+1 i PDUn(Ai, QSj) − m ∗ umin ´ (4) where m = |PDn(Ai, QSj)|, umax = max o∈P Dn(Ao,QSj ) Uo(QSj) and umin = min o∈P Dn(Ao,QSj ) Uo(QSj) This formula guarantees that the final πn+1 i (QSj, αik ) value is well defined, i.e, 0 ≤ πn+1 i (QSj, αik ) ≤ 1 and X i πn+1 i (QSj, αik ) = dn+1 i However, such a solution does not explore all the possibilities.",
                "In order to balance between exploitation and exploration, a λ-Greedy approach is taken.",
                "In the λ-Greedy approach, in addition to assigning higher probability to those agents with higher expected utility value, as in the equation (3).",
                "Agents that appear to be not-so-good choices will also be sent queries based on a dynamic exploration rate.",
                "In particular, for agents in the set PDn(Ai, QSj), πn+1 i1 (QSj) is determined in the same way as the above, with the only difference being that dn+1 i is replaced with dn+1 i ∗ (1 − λn).",
                "The remaining search bandwidth is used for learning by assigning probability λn evenly to agents Ai2 in the set DirectConn(Ai) − PDn(Ai, QSj). πn+1 i2 (QSj, αik ) = dn+1 i ∗ λn |DirectConn(Ai) − PDn(Ai, QSj)| (5) where PDn(Ai, QSj) ⊂ DirectConn(Ai).",
                "Note that the exploration rate λ is not a constant and it decreases overtime.",
                "The λ is determined according to the following equation: λn+1 = λ0 ∗ e−c1n (6) where λ0 is the initial exploration rate, which is a constant; c1 is also a constant to adjust the decreasing rate of the exploration rate; n is the current time unit. 3.1.2 Update Expected Utility Once the routing policy at step n+1, πn+1 i , is determined based on the above formula, agent Ai can update its own expected utility, Un+1 i (QSi), based on the the updated routing policy resulted from the formula 5 and the updated U values of its neighboring agents.",
                "Under the assumption that after a <br>query</br> is forwarded to Ais neighbors the subsequent search sessions are independent, the update formula is similar to the Bellman update formula in Q-Learning: Un+1 i (QSj) = (1 − θi) ∗ Un i (QSj) + θi ∗ (Rn+1 i (QSj) + X k πn+1 i (QSj, αik )Un k (QSj)) (7) where QSj = (Qj, ttl − 1) is the next state of QSj = (Qj, ttl); Rn+1 i (QSj) is the expected local reward for <br>query</br> class Qk at agent Ai under the routing policy πn+1 i ; θi is the coefficient for deciding how much weight is given to the old value during the update process: the smaller θi value is, the faster the agent is expected to learn the real value, while the greater volatility of the algorithm, and vice versa.",
                "Rn+1 (s) is updated according to the following equation: 234 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Rn+1 i (QSj) = Rn i (QSj) +γi ∗ (r(QSj) − Rn i (QSj)) ∗ P(qj|Qj ) (8) where r(QSj) is the local reward associated with the search session.",
                "P(qj|Qj ) indicates how relevant the <br>query</br> qj is to the <br>query</br> type Qj, and γi is the learning rate for agent Ai.",
                "Depending on the similarity between a specific <br>query</br> qi and its corresponding <br>query</br> type Qi, the local reward associated with the search session has different impact on the Rn i (QSj) estimation.",
                "In the above formula, this impact is reflected by the coefficient, the P(qj|Qj) value. 3.1.3 Reward function After a search session stops when its TTL values expires, all search results are returned back to the user and are compared against the relevance judgment.",
                "Assuming the set of search results is SR, the reward Rew(SR) is defined as: Rew(SR) = j 1 if |Rel(SR)| > c |Rel(SR)| c otherwise. where SR is the set of returned search results, Rel(SR) is the set of relevant documents in the search results.",
                "This equation specifies that users give 1.0 reward if the number of returned relevant documents reaches a predefined number c. Otherwise, the reward is in proportion to the number of relevant documents returned.",
                "This rationale for setting up such a cut-off value is that the importance of recall ratio decreases with the abundance of relevant documents in real world, therefore users tend to focus on only a limited number of searched results.",
                "The details of the actual routing protocol will be introduced in Section 3.2 when we introduce how the learning algorithm is deployed in real systems. 3.2 Deployment of the Learning algorithm This section describes how the learning algorithm can be used in either a single-phase or a two-phase search process.",
                "In the single-phase search algorithm, search sessions start from the initiators of the queries.",
                "In contrast, in the two-step search algorithm, the <br>query</br> initiator first attempts to seek a more appropriate starting point for the <br>query</br> by introducing an exploratory step as described in Section 2.",
                "Despite the difference in the quality of starting points, the major part of the learning process for the two algorithms is largely the same as described in the following paragraphs.",
                "Before learning starts, each agent initializes the expected utility value for all possible states as 0.",
                "Thereafter, upon receipt of a <br>query</br>, in addition to the normal operations described in the previous section, an agent Ai also sets up a timer to wait for the search results returned from its downstream agents.",
                "Once the timer expires or it has received response from all its downstream agents, Ai merges and forwards the search results accrued from its downstream agents to its upstream agent.",
                "Setting up the timer speeds up the learning because agents can avoid waiting too long for the downstream agents to return search results.",
                "Note that these detailed results and corresponding agent information will still be stored at Ai until the feedback information is passed from its upstream agent and the performance of its downstream agents can be evaluated.",
                "The duration of the timer is related to the TTL value.",
                "In this paper, we set the timer to ttimer = ttli ∗ 2 + tf , where ttli ∗ 2 is the sum of the travel time of the queries in the network, and tf is the expected time period that users would like to wait.",
                "The search results will eventually be returned to the search session initiator A0.",
                "They will be compared to the relevance judgment that is provided by the final users (as described in the experiment section, the relevance judgement for the <br>query</br> set is provided along with the data collections).",
                "The reward will be calculated and propagated backward to the agents along the way that search results were passed.",
                "This is a reverse process of the search results propagation.",
                "In the process of propagating reward backward, agents update estimates of their own potential utility value, generate an upto-dated policy and pass their updated results to the neighboring agents based on the algorithm described in Section 3.",
                "Upon change of expected utility value, agent Ai sends out its updated utility estimation to its neighbors so that they can act upon the changed expected utility and corresponding state.",
                "This update message includes the potential reward as well as the corresponding state QSi = (qk, ttll) of agent Ai.",
                "Each neighboring agent, Aj, reacts to this kind of update message by updating the expected utility value for state QSj(qk, ttll + 1) according to the newly-announced changed expected utility value.",
                "Once they complete the update, the agents would again in turn inform related neighbors to update their values.",
                "This process goes on until the TTL value in the update message increases to the TTL limit.",
                "To speed up the learning process, while updating the expected utility values of an agent Ais neighboring agents we specify that Um(Qk, ttl0) >= Um(Qk, ttl1) iff ttl0 > ttl1 Thus, when agent Ai receives an updated expected utility value with ttl1, it also updates the expected utility values with any ttl0 > ttl1 if Um(Qk, ttl0) < Um(Qk, ttl1) to speed up convergence.",
                "This heuristic is based on the fact that the utility of a search session is a non-decreasing function of time t. 3.3 Discussion In formalizing the content routing system as a learning task, many assumptions are made.",
                "In real systems, these assumptions may not hold, and thus the learning algorithm may not converge.",
                "Two problems are of particular note, (1) This content routing problem does not have Markov properties.",
                "In contrast to IP-level based packet routing, the routing decision of each agent for a particular search session sj depends on the routing history of sj.",
                "Therefore, the assumption that all subsequent search sessions are independent does not hold in reality.",
                "This may lead to double counting problem that the relevant documents of some agents will be counted more than once for the state where the TTL value is more than 1.",
                "However, in the context of the hierarchical agent organizations, two factors mitigate this problems: first, the agents in each content group form a tree-like structure.",
                "With the absense of the cycles, the estimates inside the tree would be close to the accurate value.",
                "Secondly, the stochastic nature of the routing policy partly remedies this problem.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 235 (2) Another challenge for this learning algorithm is that in a real network environment observations on neighboring agents may not be able to be updated in time due to the communication delay or other situations.",
                "In addition, when neighboring agents update their estimates at the same time, oscillation may arise during the learning process[1].",
                "This paper explores several approaches to speed up the learning process.",
                "Besides the aforementioned strategy of updating the expected utility values, we also employ an active update strategy where agents notify their neighbors whenever its expected utility is updated.",
                "Thus a faster convergence speed can be achieved.",
                "This strategy contrasts to the Lazy update, where agents only echo their neighboring agents with their expected utility change when they exchange information.",
                "The trade off between the two approaches is the network load versus learning speed.",
                "The advantage of this learning algorithm is that once a routing policy is learned, agents do not have to repeatedly compare the similarity of queries as long as the network topology remains unchanged.",
                "Instead, agent just have to determine the classification of the <br>query</br> properly and follow the learned policies.",
                "The disadvantage of this learning-based approach is that the learning process needs to be conducted whenever the network structure changes.",
                "There are many potential extensions for this learning model.",
                "For example, a single measure is currently used to indicate the traffic load for an agents neighborhood.",
                "A simple extension would be to keep track of individual load for each neighbor of the agent. 4.",
                "EXPERIMENTSSETTINGSAND RESULTS The experiments are conducted on TRANO simulation toolkit with two sets of datasets, TREC-VLC-921 and TREC123-100.",
                "The following sub-sections introduce the TRANO testbed, the datasets, and the experimental results. 4.1 TRANO Testbed TRANO (Task Routing on Agent Network Organization) is a multi-agent based network based information retrieval testbed.",
                "TRANO is built on top of the Farm [4], a time based distributed simulator that provides a data dissemination framework for large scale distributed agent network based organizations.",
                "TRANO supports importation and exportation of agent organization profiles including topological connections and other features.",
                "Each TRANO agent is composed of an agent view structure and a control unit.",
                "In simulation, each agent is pulsed regularly and the agent checks the incoming message queues, performs local operations and then forwards messages to other agents . 4.2 Experimental Settings In our experiment, we use two standard datasets, TRECVLC-921 and TREC-123-100 datasets, to simulate the collections hosted on agents.",
                "The TREC-VLC-921 and TREC123-100 datasets were created by the U.S. National Institute for Standard Technology(NIST) for its TREC conferences.",
                "In distributed information retrieval domain, the two data collections are split to 921 and 100 sub-collections.",
                "It is observed that dataset TREC-VLC-921 is more heterogeneous than TREC-123-100 in terms of source, document length, and relevant document distribution from the statistics of the two data collections listed in [13].",
                "Hence, TREC-VLC-921 is much closer to real document distributions in P2P environments.",
                "Furthermore, TREC-123-100 is split into two sets of 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0 500 1000 1500 2000 2500 3000 ARSS <br>query</br> number ARSS versus the number of incoming queries for TREC-VLC-921 SSLA-921 SSNA-921 Figure 3: ARSS(Average reward per search session) versus the number of search sessions for 1phase search in TREC-VLC-921 0 0.1 0.2 0.3 0.4 0.5 0.6 0 500 1000 1500 2000 2500 3000 ARSS <br>query</br> number ARSS versus query number for TREC-VLC-921 TSLA-921 TSNA-921 Figure 4: ARSS(Average reward per search session) versus the number of search sessions for 2phase search in TREC-VLC-921 sub-collections in two ways: randomly and by source.",
                "The two partitions are denoted as TREC-123-100-Random and TREC-123-100-Source respectively.",
                "The documents in each subcollection in dataset TREC-123-100-Source are more coherent than those in TREC-123-100-Random.",
                "The two different sets of partitions allow us to observe how the distributed learning algorithm is affected by the homogeneity of the collections.",
                "The hierarchical agent organization is generated by the algorithm described in our previous algorithm [15].",
                "During the topology generation process, degree information of each agent is estimated by the algorithm introduced by Palmer et al. [9] with parameters α = 0.5 and β = 0.6.",
                "In our experiments, we estimate the upward limit and downward degree limit using linear discount factors 0.5, 0.8 and 1.0.",
                "Once the topology is built, queries randomly selected from the <br>query</br> set 301−350 on TREC-VLC-921 and <br>query</br> set 1− 50 on TREC-123-100-Random and TREC-123-100-Source are injected to the system based on a Poisson distribution P(N(t) = n) = (λt)n n! e−λ 236 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 0 50 100 150 200 250 300 350 400 0 500 1000 1500 2000 2500 3000 Cumulativeutility <br>query</br> number Cumulative utility over the number of incoming queries TSLA-921 SSNA-921 SSLA-921 TSNA-921 Figure 5: The cumulative utility versus the number of search sessions TREC-VLC-921 In addition, we assume that all agents have an equal chance of getting queries from the environment, i.e, λ is the same for every agent.",
                "In our experiments, λ is set as 0.0543 so that the mean of the incoming queries from the environment to the agent network is 50 per time unit.",
                "The service time for the communication queue and local search queue, i.e tQij and trs, is set as 0.01 time unit and 0.05 time units respectively.",
                "In our experiments, there are ten types of queries acquired by clustering the <br>query</br> set 301 − 350 and 1 − 50. 4.3 Results analysis and evaluation Figure 3 demonstrates the ARSS(Average Reward per Search Session) versus the number of incoming queries over time for the the Single-Step based Non-learning Algorithm (SSNA), and the Single-Step Learning Algorithm(SSLA) for data collection TREC-VLC-921.",
                "It shows that the average reward for SSNA algorithm ranges from 0.02 − 0.06 and the performance changes little over time.",
                "The average reward for SSLA approach starts at the same level with the SSNA algorithm.",
                "But the performance increases over time and the average performance gain stabilizes at about 25% after <br>query</br> range 2000 − 3000.",
                "Figure 4 shows the ARSS(Average Reward per Search Session) versus the number of incoming queries over time for the the Two-Step based Non-learning Algorithm(TSNA), and the Two-Step Learning Algorithm(TSLA) for data collection TREC-VLC-921.",
                "The TSNA approach has a relatively consistent performance with the average reward ranges from 0.05 − 0.15.",
                "The average reward for TSLA approach, where learning algorithm is exploited, starts at the same level with the TSNA algorithm and improves the average reward over time until 2000−2500 queries joining the system.",
                "The results show that the average performance gain for TSLA approach over TNLA approach is 35% after stabilization.",
                "Figure 5 shows the cumulative utility versus the number of incoming queries over time for SSNA, SSLA,TSNA, and TSLA respectively.",
                "It illustrates that the cumulative utility of non-learning algorithms increases largely linearly over time, while the gains of learning-based algorithms accelerate when more queries enter the system.",
                "These experimental results demonstrate that learning-based approaches consistently perform better than non-learning based routing algorithm.",
                "Moreover, two-phase learning based algorithm is better than single-phase based learning algorithm because the maximal reward an agent can receive from searching its neighborhood within TTL hops is related to the total number of the relevant documents in that area.",
                "Thus, even the optimal routing policy can do little beyond reaching these relevant documents faster.",
                "On the contrary, the two-stepbased learning algorithm can relocate the search session to a neighborhood with more relevant documents.",
                "The TSLA combines the merits of both approaches and outperforms them.",
                "Table 1 lists the cumulative utility for datasets TREC123-100-Random and TREC-123-100-Source with hierarchical organizations.",
                "The five columns show the results for four different approaches.",
                "In particular, column TSNA-Random shows the results for dataset TREC-123-100-Random with the TSNA approach.",
                "The column TSLA-Random shows the results for dataset TREC-123-100-Random with the TSLA approach.",
                "There are two numbers in each cell in the column TSLA-Random.",
                "The first number is the actual cumulative utility while the second number is the percentage gain in terms of the utility over TSNA approach.",
                "Columns TSNA-Source and TSLA-Source show the results for dataset TREC-123-100-Source with TSNA and TSLA approaches respectively.",
                "Table 1 shows that the performance improvement for TREC-123-100-Random is not as significant as the other datasets.",
                "This is because that the documents in the sub-collection of TREC-123-100-Random are selected randomly which makes the collection model, the signature of the collection, less meaningful.",
                "Since both algorithms are designed based on the assumption that document collections can be well represented by their collection model, this result is not surprising.",
                "Overall, Figures 4, 5, and Table 1 demonstrate that the reinforcement learning based approach can considerably enhance the system performance for both data collections.",
                "However, it remains as future work to discover the correlation between the magnitude of the performance gains and the size of the data collection and/or the extent of the heterogeneity between the sub-collections. 5.",
                "RELATED WORK The content routing problem differs from the networklevel routing in packet-switched communication networks in that content-based routing occurs in application-level networks.",
                "In addition, the destination agents in our contentrouting algorithms are multiple and the addresses are not known in the routing process.",
                "IP-level Routing problems have been attacked from the reinforcement learning perspective[2, 5, 11, 12].",
                "These studies have explored fully distributed algorithms that are able, without central coordination to disseminate knowledge about the network, to find the shortest paths robustly and efficiently in the face of changing network topologies and changing link costs.",
                "There are two major classes of adaptive, distributed packet routing algorithms in the literature: distance-vector algorithms and link-state algorithms.",
                "While this line of studies carry a certain similarity with our work, it has mainly focused on packet-switched communication networks.",
                "In this domain, the destination of a packet is deterministic and unique.",
                "Each agent maintains estimations, probabilistically or deterministically, on the distance to a certain destination through its neighbors.",
                "A variant of Q-Learning techniques is deployed The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 237 Table 1: Cumulative Utility for Datasets TREC-123-100-Random and TREC-123-100-Source with Hierarchical Organization; The percentage numbers in the columns TSLA-Random and TSLA-Source demonstrate the performance gain over the algorithm without learning <br>query</br> number TSNA-Random TSLA-Random TSNA-Source TSLA-Source 500 25.15 28.45 13% 24.00 21.05 -13% 1000 104.99 126.74 20% 93.95 96.44 2.6% 1250 149.79 168.40 12% 122.64 134.05 9.3% 1500 188.94 211.05 12% 155.30 189.60 22% 1750 235.49 261.60 11% 189.14 243.90 28% 2000 275.09 319.25 16% 219.0 278.80 26% to update the estimations to converge to the real distances.",
                "It has been discovered that the locality property is an important feature of information retrieval systems in user modeling studies[3].",
                "In P2P based content sharing systems, this property is exemplified by the phenomenon that users tend to send queries that represent only a limited number of topics and conversely, users in the same neighborhood are likely to share common interests and send similar queries [10].",
                "The learning based approach is perceived to be more beneficial for real distributed information retrieval systems which exhibit locality property.",
                "This is because the users traffic and <br>query</br> patterns can reduce the state space and speed up the learning process.",
                "Related work in taking advantage of this property include [7], where the authors attempted to address this problem by user modeling techniques. 6.",
                "CONCLUSIONS In this paper, a reinforcement-learning based approach is developed to improve the performance of distributed IR search algorithms.",
                "Particularly, agents maintain estimates, namely expected utility, on the downstream agents ability to provide relevant documents for incoming queries.",
                "These estimates are updated gradually by learning from the feedback information returned from previous search sessions.",
                "Based on the updated expected utility information, the agents modify their routing policies.",
                "Thereafter, these agents route the queries based on the learned policies and update the estimates on the expected utility based on the new routing policies.",
                "The experiments on two different distributed IR datasets illustrates that the reinforcement learning approach improves considerably the cumulative utility over time. 7.",
                "REFERENCES [1] S. Abdallah and V. Lesser.",
                "Learning the task allocation game.",
                "In AAMAS 06: Proceedings of the fifth international joint conference on Autonomous agents and multiagent systems, pages 850-857, New York, NY, USA, 2006.",
                "ACM Press. [2] J.",
                "A. Boyan and M. L. Littman.",
                "Packet routing in dynamically changing networks: A reinforcement learning approach.",
                "In Advances in Neural Information Processing Systems, volume 6, pages 671-678.",
                "Morgan Kaufmann Publishers, Inc., 1994. [3] J. C. French, A. L. Powell, J. P. Callan, C. L. Viles, T. Emmitt, K. J. Prey, and Y. Mou.",
                "Comparing the performance of database selection algorithms.",
                "In Research and Development in Information Retrieval, pages 238-245, 1999. [4] B. Horling, R. Mailler, and V. Lesser.",
                "Farm: A scalable environment for multi-agent development and evaluation.",
                "In Advances in Software Engineering for Multi-Agent Systems, pages 220-237, Berlin, 2004.",
                "Springer-Verlag. [5] M. Littman and J. Boyan.",
                "A distributed reinforcement learning scheme for network routing.",
                "In Proceedings of the International Workshop on Applications of Neural Networks to Telecommunications, 1993. [6] J. Lu and J. Callan.",
                "Federated search of text-based digital libraries in hierarchical peer-to-peer networks.",
                "In In ECIR05, 2005. [7] J. Lu and J. Callan.",
                "User modeling for full-text federated search in peer-to-peer networks.",
                "In ACM SIGIR 2006.",
                "ACM Press, 2006. [8] C. D. Manning and H. Sch¨utze.",
                "Foundations of Statistical Natural Language Processing.",
                "The MIT Press, Cambridge, Massachusetts, 1999. [9] C. R. Palmer and J. G. Steffan.",
                "Generating network topologies that obey power laws.",
                "In Proceedings of GLOBECOM 2000, November 2000. [10] K. Sripanidkulchai, B. Maggs, and H. Zhang.",
                "Efficient content location using interest-based locality in peer-topeer systems.",
                "In INFOCOM, 2003. [11] D. Subramanian, P. Druschel, and J. Chen.",
                "Ants and reinforcement learning: A case study in routing in dynamic networks.",
                "In In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence, pages 832-839, 1997. [12] J. N. Tao and L. Weaver.",
                "A multi-agent, policy gradient approach to network routing.",
                "In In Proceedings of the Eighteenth International Conference on Machine Learning, 2001. [13] H. Zhang, W. B. Croft, B. Levine, and V. Lesser.",
                "A multi-agent approach for peer-to-peer information retrieval.",
                "In Proceedings of Third International Joint Conference on Autonomous Agents and Multi-Agent Systems, July 2004. [14] H. Zhang and V. Lesser.",
                "Multi-agent based peer-to-peer information retrieval systems with concurrent search sessions.",
                "In Proceedings of the Fifth International Joint Conference on Autonomous Agents and Multi-Agent Systems, May 2006. [15] H. Zhang and V. R. Lesser.",
                "A dynamically formed hierarchical agent organization for a distributed content sharing system.",
                "In 2004 IEEE/WIC/ACM International Conference on Intelligent Agent Technology (IAT 2004), 20-24 September 2004, Beijing, China, pages 169-175.",
                "IEEE Computer Society, 2004. 238 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "Second, the similarity-based approaches do not take into account the run-time characteristics of the P2P IR systems, including environmental parameters, bandwidth usage, and the historical information of the past search sessions, that provide valuable information for the <br>query</br> routing algorithms.",
                "This contrasts with the content-similarity based approaches where similar operations are repeated for every incoming <br>query</br> and the processing time keeps largely constant over time.",
                "For the first <br>query</br> in MFi, agent Ai determines which subset of its neighboring agents to forward it to based on the agents routing policy πi.",
                "In this paper, we call Ai as Ajs upstream agent and Aj as Ais downstream agent if A4 A5 A6 A7 A2 A3 A9 NEI(A2)={A3} PAR(A2)={A1} CHL(A2)={A4,A5} A1 A8 Figure 1: A fraction of a hierarchical P2PIR system an agent Ai routes a <br>query</br> to agent Aj.",
                "In the first step, upon receipt of a <br>query</br> qk at time tl from a user, agent Ai initiates a search session si by probing its neighboring agents Aj ∈ NEI(Ai) with the message PROBE for the similarity value Sim(qk, Aj) between qk and Aj."
            ],
            "translated_annotated_samples": [
                "Segundo, los enfoques basados en la similitud no tienen en cuenta las características de tiempo de ejecución de los sistemas de IR P2P, incluidos los parámetros ambientales, el uso del ancho de banda y la información histórica de las sesiones de búsqueda pasadas, que proporcionan información valiosa para los algoritmos de enrutamiento de <br>consulta</br>s.",
                "Esto contrasta con los enfoques basados en la similitud de contenido, donde se repiten operaciones similares para cada <br>consulta</br> entrante y el tiempo de procesamiento se mantiene en gran medida constante con el tiempo.",
                "Para la primera <br>consulta</br> en MFi, el agente Ai determina a qué subconjunto de sus agentes vecinos reenviarla basándose en la política de enrutamiento de los agentes πi.",
                "En este documento, llamamos a Ai como el agente aguas arriba de Ajs y a Aj como el agente aguas abajo de Ais si A4 A5 A6 A7 A2 A3 A9 NEI(A2)={A3} PAR(A2)={A1} CHL(A2)={A4,A5} A1 A8 Figura 1: Una fracción de un sistema jerárquico P2PIR en el que un agente Ai dirige una <br>consulta</br> al agente Aj.",
                "En el primer paso, al recibir una <br>consulta</br> qk en el tiempo tl de un usuario, el agente Ai inicia una sesión de búsqueda si al sondear a sus agentes vecinos Aj ∈ NEI(Ai) con el mensaje PROBE para el valor de similitud Sim(qk, Aj) entre qk y Aj."
            ],
            "translated_text": "Un algoritmo de búsqueda distribuida basado en aprendizaje por refuerzo para sistemas jerárquicos de recuperación de información entre pares. En estos enfoques, los agentes dependen de la similitud de contenido entre las consultas entrantes y sus agentes vecinos directos para dirigir las sesiones de búsqueda distribuida. Sin embargo, tal heurística es miope en el sentido de que los agentes vecinos pueden no estar conectados con agentes más relevantes. En este documento, se desarrolla un enfoque basado en aprendizaje por refuerzo en línea para aprovechar las características dinámicas en tiempo de ejecución de los sistemas de recuperación de información P2P, representadas por información sobre sesiones de búsqueda pasadas. Específicamente, los agentes mantienen estimaciones sobre las habilidades de los agentes aguas abajo para proporcionar documentos relevantes para las consultas entrantes. Estas estimaciones se actualizan gradualmente al aprender de la información de retroalimentación devuelta de sesiones de búsqueda anteriores. Basándose en esta información, los agentes derivan políticas de enrutamiento correspondientes. Posteriormente, estos agentes dirigen las consultas basándose en las políticas aprendidas y actualizan las estimaciones según las nuevas políticas de enrutamiento. Los resultados experimentales demuestran que el algoritmo de aprendizaje mejora considerablemente el rendimiento del enrutamiento en dos conjuntos de colecciones de pruebas que han sido utilizados en una variedad de estudios de recuperación de información distribuida. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagente Términos Generales Algoritmos, Rendimiento, Experimentación 1. En los últimos años ha habido un creciente interés en estudiar cómo controlar los procesos de búsqueda en sistemas de recuperación de información (IR) basados en pares (P2P) [6, 13, 14, 15]. En esta línea de investigación, uno de los problemas centrales que preocupa a los investigadores es dirigir de manera eficiente las consultas de los usuarios en la red a los agentes que poseen los documentos apropiados. En ausencia de información global, las estrategias dominantes para abordar este problema son enfoques basados en la similitud de contenido [6, 13, 14, 15]. Si bien la similitud de contenido entre consultas y nodos locales parece ser un indicador creíble para la cantidad de documentos relevantes que residen en cada nodo, estos enfoques están limitados por una serie de factores. En primer lugar, las métricas basadas en similitud pueden ser miope ya que los nodos localmente relevantes pueden no estar conectados a otros nodos relevantes. Segundo, los enfoques basados en la similitud no tienen en cuenta las características de tiempo de ejecución de los sistemas de IR P2P, incluidos los parámetros ambientales, el uso del ancho de banda y la información histórica de las sesiones de búsqueda pasadas, que proporcionan información valiosa para los algoritmos de enrutamiento de <br>consulta</br>s. En este artículo, desarrollamos un enfoque de IR basado en aprendizaje por refuerzo para mejorar el rendimiento de los algoritmos de búsqueda de IR distribuidos. Los agentes pueden adquirir mejores estrategias de búsqueda al recopilar y analizar información de retroalimentación de sesiones de búsqueda anteriores. En particular, los agentes mantienen estimaciones, es decir, utilidad esperada, sobre las capacidades de los agentes aguas abajo de proporcionar documentos relevantes para tipos específicos de consultas entrantes. Estas estimaciones se actualizan gradualmente al aprender de la información de retroalimentación devuelta de sesiones de búsqueda anteriores. Basándose en la información actualizada de utilidad esperada, los agentes derivan políticas de enrutamiento correspondientes. Posteriormente, estos agentes dirigen las consultas basándose en las políticas aprendidas y actualizan las estimaciones sobre la utilidad esperada en función de las nuevas políticas de enrutamiento. Este proceso se lleva a cabo de manera iterativa. El objetivo del algoritmo de aprendizaje, aunque consume ancho de banda de red, es reducir el tiempo de enrutamiento para procesar más consultas por unidad de tiempo y al mismo tiempo encontrar más documentos relevantes. Esto contrasta con los enfoques basados en la similitud de contenido, donde se repiten operaciones similares para cada <br>consulta</br> entrante y el tiempo de procesamiento se mantiene en gran medida constante con el tiempo. Otra forma de ver este documento es que nuestro enfoque básico para la búsqueda de IR distribuida es construir una red de superposición jerárquica (organización de agentes) basada en la medida de similitud de contenido entre las colecciones de documentos de los agentes de manera ascendente. En trabajos anteriores, hemos demostrado que esta organización mejora significativamente el rendimiento de búsqueda. Sin embargo, esta estructura organizativa no tiene en cuenta los patrones de llegada de las consultas, incluyendo su frecuencia, tipos y dónde entran en el sistema, ni el ancho de banda de comunicación disponible en la red y las capacidades de procesamiento de los agentes individuales. La intención del aprendizaje por refuerzo es adaptar las decisiones de enrutamiento de los agentes a las situaciones dinámicas de la red y aprender de sesiones de búsqueda pasadas. Específicamente, las contribuciones de este artículo incluyen: (1) un enfoque basado en aprendizaje por refuerzo para que los agentes adquieran políticas de enrutamiento satisfactorias basadas en estimaciones de la contribución potencial de sus agentes vecinos; (2) dos estrategias para acelerar el proceso de aprendizaje. Según nuestro mejor conocimiento, esta es una de las primeras aplicaciones de aprendizaje por refuerzo para abordar problemas de compartición de contenido distribuido, lo cual indica algunos de los problemas al aplicar el refuerzo en una aplicación compleja. El resto de este documento está organizado de la siguiente manera: la Sección 2 revisa los sistemas jerárquicos de intercambio de contenido y el algoritmo de búsqueda de dos fases basado en dicha topología. La Sección 3 describe un enfoque basado en aprendizaje por refuerzo para dirigir el proceso de enrutamiento; la Sección 4 detalla la configuración experimental y analiza los resultados. La sección 5 discute estudios relacionados y la sección 6 concluye el artículo. BÚSQUEDA EN SISTEMAS JERÁRQUICOS DE IR P2P Esta sección revisa brevemente nuestros enfoques básicos para los sistemas jerárquicos de IR P2P. En un sistema jerárquico de recuperación de información P2P ilustrado en la Fig.1, los agentes están conectados entre sí a través de tres tipos de enlaces: enlaces ascendentes, enlaces descendentes y enlaces laterales. En las siguientes secciones, denotamos el conjunto de agentes que están directamente conectados al agente Ai como DirectConn(Ai), el cual se define como DirectConn(Ai) = NEI(Ai) ∪ PAR(Ai) ∪ CHL(Ai), donde NEI(Ai) es el conjunto de agentes vecinos conectados a Ai a través de enlaces laterales; PAR(Ai) es el conjunto de agentes a los que el agente Ai está conectado a través de enlaces ascendentes y CHL(Ai) es el conjunto de agentes a los que el agente Ai se conecta a través de enlaces descendentes. Estos enlaces se establecen a través de un proceso de agrupamiento distribuido basado en la similitud de contenido de abajo hacia arriba[15]. Estos enlaces son luego utilizados por agentes para localizar otros agentes que contienen documentos relevantes a las consultas dadas. Un agente típico Ai en nuestro sistema utiliza dos colas: una cola de búsqueda local, LSi, y una cola de reenvío de mensajes MFi. Los estados de las dos colas constituyen los estados internos de un agente. La cola de búsqueda local LSi almacena sesiones de búsqueda programadas para procesamiento local. Es una cola de prioridad y el agente Ai siempre selecciona las consultas más prometedoras para procesarlas con el fin de maximizar la utilidad global. MFi consiste en un conjunto de consultas para reenviar y se procesa de manera FIFO (primero en entrar, primero en salir). Para la primera <br>consulta</br> en MFi, el agente Ai determina a qué subconjunto de sus agentes vecinos reenviarla basándose en la política de enrutamiento de los agentes πi. Estas decisiones de enrutamiento determinan cómo se lleva a cabo el proceso de búsqueda en la red. En este documento, llamamos a Ai como el agente aguas arriba de Ajs y a Aj como el agente aguas abajo de Ais si A4 A5 A6 A7 A2 A3 A9 NEI(A2)={A3} PAR(A2)={A1} CHL(A2)={A4,A5} A1 A8 Figura 1: Una fracción de un sistema jerárquico P2PIR en el que un agente Ai dirige una <br>consulta</br> al agente Aj. El protocolo de búsqueda distribuida de nuestra organización de agentes jerárquicos está compuesto por dos pasos. En el primer paso, al recibir una <br>consulta</br> qk en el tiempo tl de un usuario, el agente Ai inicia una sesión de búsqueda si al sondear a sus agentes vecinos Aj ∈ NEI(Ai) con el mensaje PROBE para el valor de similitud Sim(qk, Aj) entre qk y Aj. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "peer-to-peer information retrieval": {
            "translated_key": "recuperación de información entre pares",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Reinforcement Learning based Distributed Search Algorithm For Hierarchical <br>peer-to-peer information retrieval</br> Systems Haizheng Zhang College of Information Science and Technology Pennsylvania State University University Park, PA 16803 hzhang@ist.psu.edu Victor Lesser Department of Computer Science University Of Massachusetts Amherst, MA 01003 lesser@cs.umass.edu ABSTRACT The dominant existing routing strategies employed in peerto-peer(P2P) based information retrieval(IR) systems are similarity-based approaches.",
                "In these approaches, agents depend on the content similarity between incoming queries and their direct neighboring agents to direct the distributed search sessions.",
                "However, such a heuristic is myopic in that the neighboring agents may not be connected to more relevant agents.",
                "In this paper, an online reinforcement-learning based approach is developed to take advantage of the dynamic run-time characteristics of P2P IR systems as represented by information about past search sessions.",
                "Specifically, agents maintain estimates on the downstream agents abilities to provide relevant documents for incoming queries.",
                "These estimates are updated gradually by learning from the feedback information returned from previous search sessions.",
                "Based on this information, the agents derive corresponding routing policies.",
                "Thereafter, these agents route the queries based on the learned policies and update the estimates based on the new routing policies.",
                "Experimental results demonstrate that the learning algorithm improves considerably the routing performance on two test collection sets that have been used in a variety of distributed IR studies.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Algorithms, Performance, Experimentation 1.",
                "INTRODUCTION Over the last few years there have been increasing interests in studying how to control the search processes in peer-to-peer(P2P) based information retrieval(IR) systems [6, 13, 14, 15].",
                "In this line of research, one of the core problems that concerns researchers is to efficiently route user queries in the network to agents that are in possession of appropriate documents.",
                "In the absence of global information, the dominant strategies in addressing this problem are content-similarity based approaches [6, 13, 14, 15].",
                "While the content similarity between queries and local nodes appears to be a creditable indicator for the number of relevant documents residing on each node, these approaches are limited by a number of factors.",
                "First of all, similaritybased metrics can be myopic since locally relevant nodes may not be connected to other relevant nodes.",
                "Second, the similarity-based approaches do not take into account the run-time characteristics of the P2P IR systems, including environmental parameters, bandwidth usage, and the historical information of the past search sessions, that provide valuable information for the query routing algorithms.",
                "In this paper, we develop a reinforcement learning based IR approach for improving the performance of distributed IR search algorithms.",
                "Agents can acquire better search strategies by collecting and analyzing feedback information from previous search sessions.",
                "Particularly, agents maintain estimates, namely expected utility, on the downstream agents capabilities of providing relevant documents for specific types of incoming queries.",
                "These estimates are updated gradually by learning from the feedback information returned from previous search sessions.",
                "Based on the updated expected utility information, the agents derive corresponding routing policies.",
                "Thereafter, these agents route the queries based on the learned policies and update the estimates on the expected utility based on the new routing policies.",
                "This process is conducted in an iterative manner.",
                "The goal of the learning algorithm, even though it consumes some network bandwidth, is to shorten the routing time so that more queries are processed per time unit while at the same time finding more relevant documents.",
                "This contrasts with the content-similarity based approaches where similar operations are repeated for every incoming query and the processing time keeps largely constant over time.",
                "Another way of viewing this paper is that our basic approach to distributed IR search is to construct a hierarchical overlay network(agent organization) based on the contentsimilarity measure among agents document collections in a bottom-up fashion.",
                "In the past work, we have shown that this organization improves search performance significantly.",
                "However, this organizational structure does not take into account the arrival patterns of queries, including their frequency, types, and where they enter the system, nor the available communication bandwidth of the network and processing capabilities of individual agents.",
                "The intention of the reinforcement learning is to adapt the agents routing decisions to the dynamic network situations and learn from past search sessions.",
                "Specifically, the contributions of this paper include: (1) a reinforcement learning based approach for agents to acquire satisfactory routing policies based on estimates of the potential contribution of their neighboring agents; (2) two strategies to speed up the learning process.",
                "To our best knowledge, this is one of the first reinforcement learning applications in addressing distributed content sharing problems and it is indicative of some of the issues in applying reinforcement in a complex application.",
                "The remainder of this paper is organized as follows: Section 2 reviews the hierarchical content sharing systems and the two-phase search algorithm based on such topology.",
                "Section 3 describes a reinforcement learning based approach to direct the routing process; Section 4 details the experimental settings and analyze the results.",
                "Section 5 discusses related studies and Section 6 concludes the paper. 2.",
                "SEARCH IN HIERARCHICAL P2P IR SYSTEMS This section briefly reviews our basic approaches to hierarchical P2P IR systems.",
                "In a hierarchical P2P IR system illustrated in Fig.1, agents are connected to each other through three types of links: upward links, downward links, and lateral links.",
                "In the following sections, we denote the set of agents that are directly connected to agent Ai as DirectConn(Ai), which is defined as DirectConn(Ai) = NEI(Ai) ∪ PAR(Ai) ∪ CHL(Ai) , where NEI(Ai) is the set of neighboring agents connected to Ai through lateral links; PAR(Ai) is the set of agents whom agent Ai is connected to through upward links and CHL(Ai) is the set of agents that agent Ai connects to through downward links.",
                "These links are established through a bottom-up content-similarity based distributed clustering process[15].",
                "These links are then used by agents to locate other agents that contain documents relevant to the given queries.",
                "A typical agent Ai in our system uses two queues: a local search queue, LSi, and a message forwarding queue MFi.",
                "The states of the two queues constitute the internal states of an agent.",
                "The local search queue LSi stores search sessions that are scheduled for local processing.",
                "It is a priority queue and agent Ai always selects the most promising queries to process in order to maximize the global utility.",
                "MFi consists of a set of queries to forward on and is processed in a FIFO (first in first out) fashion.",
                "For the first query in MFi, agent Ai determines which subset of its neighboring agents to forward it to based on the agents routing policy πi.",
                "These routing decisions determine how the search process is conducted in the network.",
                "In this paper, we call Ai as Ajs upstream agent and Aj as Ais downstream agent if A4 A5 A6 A7 A2 A3 A9 NEI(A2)={A3} PAR(A2)={A1} CHL(A2)={A4,A5} A1 A8 Figure 1: A fraction of a hierarchical P2PIR system an agent Ai routes a query to agent Aj.",
                "The distributed search protocol of our hierarchical agent organization is composed of two steps.",
                "In the first step, upon receipt of a query qk at time tl from a user, agent Ai initiates a search session si by probing its neighboring agents Aj ∈ NEI(Ai) with the message PROBE for the similarity value Sim(qk, Aj) between qk and Aj.",
                "Here, Ai is defined as the query initiator of search session si.",
                "In the second step, Ai selects a group of the most promising agents to start the actual search process with the message SEARCH.",
                "These SEARCH messages contain a TTL (Time To Live) parameter in addition to the query.",
                "The TTL value decreases by 1 after each hop.",
                "In the search process, agents discard those queries that either have been previously processed or whose TTL drops to 0, which prevents queries from looping in the system forever.",
                "The search session ends when all the agents that receive the query drop it or TTL decreases to 0.",
                "Upon receipt of SEARCH messages for qk, agents schedule local activities including local searching, forwarding qk to their neighbors, and returning search results to the query initiator.",
                "This process and related algorithms are detailed in [15, 14]. 3.",
                "A BASIC REINFORCEMENTLEARNING BASED SEARCH APPROACH In the aforementioned distributed search algorithm, the routing decisions of an agent Ai rely on the similarity comparison between incoming queries and Ais neighboring agents in order to forward those queries to relevant agents without flooding the network with unnecessary query messages.",
                "However, this heuristic is myopic because a relevant direct neighbor is not necessarily connected to other relevant agents.",
                "In this section, we propose a more general approach by framing this problem as a reinforcement learning task.",
                "In pursuit of greater flexibility, agents can switch between two modes: learning mode and non-learning mode.",
                "In the non-learning mode, agents operate in the same way as they do in the normal distributed search processes described in [14, 15].",
                "On the other hand, in the learning mode, in parallel with distributed search sessions, agents also participate in a learning process which will be detailed in this section.",
                "Note that in the learning protocol, the learning process does not interfere with the distributed search process.",
                "Agents can choose to initiate and stop learning processes without affecting the system performance.",
                "In particular, since the learning process consumes network resources (especially bandwidth), agents can choose to initiate learning only when the network load is relatively low, thus minimizing the extra communication costs incurred by the learning algorithm.",
                "The section is structured as follows, Section 3.1 describes 232 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) a reinforcement learning based model.",
                "Section 3.2 describes a protocol to deploy the learning algorithm in the network.",
                "Section 3.3 discusses the convergence of the learning algorithm. 3.1 The Model An agents routing policy takes the state of a search session as input and output the routing actions for that query.",
                "In our work, the state of a search session sj is stipulated as: QSj = (qk, ttlj) where ttlj is the number of hops that remains for the search session sj , qk is the specific query.",
                "QL is an attribute of qk that indicates which type of queries qk most likely belong to.",
                "The set of QL can be generated by running a simple online classification algorithm on all the queries that have been processed by the agents, or an oﬄine algorithm on a pre-designated training set.",
                "The assumption here is that the set of query types is learned ahead of time and belongs to the common knowledge of the agents in the network.",
                "Future work includes exploring how learning can be accomplished when this assumption does not hold.",
                "Given the query types set, an incoming query qi can be classified to one query class Q(qi) by the formula: Q(qi) = arg max Qj P(qi|Qj) (1) where P(qi|Qj ) indicates the likelihood that the query qi is generated by the query class Qj [8].",
                "The set of atomic routing actions of an agent Ai is denoted as {αi}, where {αi} is defined as αi = {αi0 , αi1 , ..., αin }.",
                "An element αij represents an action to route a given query to the neighboring agent Aij ∈ DirectConn(Ai).",
                "The routing policy πi of agent Ai is stochastic and its outcome for a search session with state QSj is defined as: πi(QSj) = {(αi0 , πi(QSi, αi0 )), (αi1 , πi(QSi, αi1 )), ...} (2) Note that operator πi is overloaded to represent either the probabilistic policy for a search session with state QSj, denoted as πi(QSj); or the probability of forwarding the query to a specific neighboring agent Aik ∈ DirectConn(Ai) under the policy πi(QSj), denoted as πi(QSj, αik ).",
                "Therefore, equation (2) means that the probability of forwarding the search session to agent Ai0 is πi(QSi, αi0 ) and so on.",
                "Under this stochastic policy, the routing action is nondeterministic.",
                "The advantage of such a strategy is that the best neighboring agents will not be selected repeatedly, thereby mitigating the potential hot spots situations.",
                "The expected utility, Un i (QSj), is used to estimate the potential utility gain of routing query type QSj to agent Ai under policy πn i .",
                "The superscript n indicates the value at the nth iteration in an iterative learning process.",
                "The expected utility provides routing guidance for future search sessions.",
                "In the search process, each agent Ai maintains partial observations of its neighbors states, as shown in Fig. 2.",
                "The partial observation includes non-local information such as the potential utility estimation of its neighbor Am for query state QSj, denoted as Um(QSj), as well as the load information, Lm.",
                "These observations are updated periodically by the neighbors.",
                "The estimated utility information will be used to update Ais expected utility for its routing policy.",
                "Load Information Expected Utility For Different Query Types Neighboring Agents ... A0 A1 A3 A2 Un 0 (QS0) ... ... ... ... ...... Un 0 (QS1) Un 1 (QS1) Un 2 (QS1) Un 3 (QS1) Un 1 (QS0) Un 2 (QS0) Un 3 (QS0) Ln 0 Ln 1 Ln 2 Ln 3 ... QS0 QS1 ...",
                "Figure 2: Agent Ais Partial Observation about its neighbors(A0, A1...) The load information of Am, Lm, is defined as Lm = |MFm| Cm , where |MFm| is the length of the message-forward queue and Cm is the service rate of agent Ams message-forward queue.",
                "Therefore Lm characterizes the utilization of an agents communication channel, and thus provide non-local information for Ams neighbors to adjust the parameters of their routing policy to avoid inundating their downstream agents.",
                "Note that based on the characteristics of the queries entering the system and agents capabilities, the loading of agents may not be uniform.",
                "After collecting the utilization rate information from all its neighbors, agent Ai computes Li as a single measure for assessing the average load condition of its neighborhood: Li = P k Lk |DirectConn(Ai)| Agents exploit Li value in determining the routing probability in its routing policy.",
                "Note that, as described in Section 3.2, information about neighboring agents is piggybacked with the query message propagated among the agents whenever possible to reduce the traffic overhead. 3.1.1 Update the Policy An iterative update process is introduced for agents to learn a satisfactory stochastic routing policy.",
                "In this iterative process, agents update their estimates on the potential utility of their current routing policies and then propagate the updated estimates to their neighbors.",
                "Their neighbors then generate a new routing policy based on the updated observation and in turn they calculate the expected utility based on the new policies and continue this iterative process.",
                "In particular, at time n, given a set of expected utility, an agent Ai, whose directly connected agents set is DirectConn(Ai) = {Ai0 , ..., Aim }, determines its corresponding stochastic routing policy for a search session of state QSj based on the following steps: (1) Ai first selects a subset of agents as the potential downstream agents from set DirectConn(Ai), denoted as PDn(Ai, QSj).",
                "The size of the potential downstream agent is specified as |PDn(Ai, QSj)| = min(|NEI(Ai), dn i + k)| where k is a constant and is set to 3 in this paper; dn i , the forward width, is defined as the expected number of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 233 neighboring agents that agent Ai can forward to at time n. This formula specifies that the potential downstream agent set PDn(Ai, QSj) is either the subset of neighboring agents with dn i + k highest expected utility value for state QSj among all the agents in DirectConn(Ai), or all their neighboring agents.",
                "The k is introduced based on the idea of a stochastic routing policy and it makes the forwarding probability of the dn i +k highest agent less than 100%.",
                "Note that if we want to limit the number of downstream agents for search session sj as 5, the probability of forwarding the query to all neighboring agents should add up to 5.",
                "Setting up dn i value properly can improve the utilization rate of the network bandwidth when much of the network is idle while mitigating the traffic load when the network is highly loaded.",
                "The dn+1 i value is updated based on dn i , the previous and current observations on the traffic situation in the neighborhood.",
                "Specifically, the update formula for dn+1 i is dn+1 i = dn i ∗ (1 + 1 − Li |DirectConn(Ai)| ) In this formula, the forward width is updated based on the traffic conditions of agent Ais neighborhood, i.e Li, and its previous value. (2) For each agent Aik in the PDn(Ai, QSj), the probability of forwarding the query to Aik is determined in the following way in order to assign higher forwarding probability to the neighboring agents with higher expected utility value: πn+1 i (QSj, αik ) = dn+1 i |PDn(Ai, QSj)| + β ∗ ` Uik (QSj) − PDU(Ai, QSj) |PDn(Ai, QSj)| ´ (3) where PDUn(Ai, QSj) = X o∈P Dn(Ai,QSj ) Uo(QSj) and QSj is the subsequent state of agent Aik after agent Ai forwards the search session with state QSj to its neighboring agent Aik ; If QSj = (qk, ttl0), then QSj = (qk, ttl0 − 1).",
                "In formula 3, the first term on the right of the equation, dn+1 i |P Dn(Ai,QSj )| , is used to to determine the forwarding probability by equally distributing the forward width, dn+1 i , to the agents in PDn(Ai, QSj) set.",
                "The second term is used to adjust the probability of being chosen so that agents with higher expected utility values will be favored. β is determined according to: β = min ` m − dn+1 i m ∗ umax − PDUn(Ai, QSj) , dn+1 i PDUn(Ai, QSj) − m ∗ umin ´ (4) where m = |PDn(Ai, QSj)|, umax = max o∈P Dn(Ao,QSj ) Uo(QSj) and umin = min o∈P Dn(Ao,QSj ) Uo(QSj) This formula guarantees that the final πn+1 i (QSj, αik ) value is well defined, i.e, 0 ≤ πn+1 i (QSj, αik ) ≤ 1 and X i πn+1 i (QSj, αik ) = dn+1 i However, such a solution does not explore all the possibilities.",
                "In order to balance between exploitation and exploration, a λ-Greedy approach is taken.",
                "In the λ-Greedy approach, in addition to assigning higher probability to those agents with higher expected utility value, as in the equation (3).",
                "Agents that appear to be not-so-good choices will also be sent queries based on a dynamic exploration rate.",
                "In particular, for agents in the set PDn(Ai, QSj), πn+1 i1 (QSj) is determined in the same way as the above, with the only difference being that dn+1 i is replaced with dn+1 i ∗ (1 − λn).",
                "The remaining search bandwidth is used for learning by assigning probability λn evenly to agents Ai2 in the set DirectConn(Ai) − PDn(Ai, QSj). πn+1 i2 (QSj, αik ) = dn+1 i ∗ λn |DirectConn(Ai) − PDn(Ai, QSj)| (5) where PDn(Ai, QSj) ⊂ DirectConn(Ai).",
                "Note that the exploration rate λ is not a constant and it decreases overtime.",
                "The λ is determined according to the following equation: λn+1 = λ0 ∗ e−c1n (6) where λ0 is the initial exploration rate, which is a constant; c1 is also a constant to adjust the decreasing rate of the exploration rate; n is the current time unit. 3.1.2 Update Expected Utility Once the routing policy at step n+1, πn+1 i , is determined based on the above formula, agent Ai can update its own expected utility, Un+1 i (QSi), based on the the updated routing policy resulted from the formula 5 and the updated U values of its neighboring agents.",
                "Under the assumption that after a query is forwarded to Ais neighbors the subsequent search sessions are independent, the update formula is similar to the Bellman update formula in Q-Learning: Un+1 i (QSj) = (1 − θi) ∗ Un i (QSj) + θi ∗ (Rn+1 i (QSj) + X k πn+1 i (QSj, αik )Un k (QSj)) (7) where QSj = (Qj, ttl − 1) is the next state of QSj = (Qj, ttl); Rn+1 i (QSj) is the expected local reward for query class Qk at agent Ai under the routing policy πn+1 i ; θi is the coefficient for deciding how much weight is given to the old value during the update process: the smaller θi value is, the faster the agent is expected to learn the real value, while the greater volatility of the algorithm, and vice versa.",
                "Rn+1 (s) is updated according to the following equation: 234 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Rn+1 i (QSj) = Rn i (QSj) +γi ∗ (r(QSj) − Rn i (QSj)) ∗ P(qj|Qj ) (8) where r(QSj) is the local reward associated with the search session.",
                "P(qj|Qj ) indicates how relevant the query qj is to the query type Qj, and γi is the learning rate for agent Ai.",
                "Depending on the similarity between a specific query qi and its corresponding query type Qi, the local reward associated with the search session has different impact on the Rn i (QSj) estimation.",
                "In the above formula, this impact is reflected by the coefficient, the P(qj|Qj) value. 3.1.3 Reward function After a search session stops when its TTL values expires, all search results are returned back to the user and are compared against the relevance judgment.",
                "Assuming the set of search results is SR, the reward Rew(SR) is defined as: Rew(SR) = j 1 if |Rel(SR)| > c |Rel(SR)| c otherwise. where SR is the set of returned search results, Rel(SR) is the set of relevant documents in the search results.",
                "This equation specifies that users give 1.0 reward if the number of returned relevant documents reaches a predefined number c. Otherwise, the reward is in proportion to the number of relevant documents returned.",
                "This rationale for setting up such a cut-off value is that the importance of recall ratio decreases with the abundance of relevant documents in real world, therefore users tend to focus on only a limited number of searched results.",
                "The details of the actual routing protocol will be introduced in Section 3.2 when we introduce how the learning algorithm is deployed in real systems. 3.2 Deployment of the Learning algorithm This section describes how the learning algorithm can be used in either a single-phase or a two-phase search process.",
                "In the single-phase search algorithm, search sessions start from the initiators of the queries.",
                "In contrast, in the two-step search algorithm, the query initiator first attempts to seek a more appropriate starting point for the query by introducing an exploratory step as described in Section 2.",
                "Despite the difference in the quality of starting points, the major part of the learning process for the two algorithms is largely the same as described in the following paragraphs.",
                "Before learning starts, each agent initializes the expected utility value for all possible states as 0.",
                "Thereafter, upon receipt of a query, in addition to the normal operations described in the previous section, an agent Ai also sets up a timer to wait for the search results returned from its downstream agents.",
                "Once the timer expires or it has received response from all its downstream agents, Ai merges and forwards the search results accrued from its downstream agents to its upstream agent.",
                "Setting up the timer speeds up the learning because agents can avoid waiting too long for the downstream agents to return search results.",
                "Note that these detailed results and corresponding agent information will still be stored at Ai until the feedback information is passed from its upstream agent and the performance of its downstream agents can be evaluated.",
                "The duration of the timer is related to the TTL value.",
                "In this paper, we set the timer to ttimer = ttli ∗ 2 + tf , where ttli ∗ 2 is the sum of the travel time of the queries in the network, and tf is the expected time period that users would like to wait.",
                "The search results will eventually be returned to the search session initiator A0.",
                "They will be compared to the relevance judgment that is provided by the final users (as described in the experiment section, the relevance judgement for the query set is provided along with the data collections).",
                "The reward will be calculated and propagated backward to the agents along the way that search results were passed.",
                "This is a reverse process of the search results propagation.",
                "In the process of propagating reward backward, agents update estimates of their own potential utility value, generate an upto-dated policy and pass their updated results to the neighboring agents based on the algorithm described in Section 3.",
                "Upon change of expected utility value, agent Ai sends out its updated utility estimation to its neighbors so that they can act upon the changed expected utility and corresponding state.",
                "This update message includes the potential reward as well as the corresponding state QSi = (qk, ttll) of agent Ai.",
                "Each neighboring agent, Aj, reacts to this kind of update message by updating the expected utility value for state QSj(qk, ttll + 1) according to the newly-announced changed expected utility value.",
                "Once they complete the update, the agents would again in turn inform related neighbors to update their values.",
                "This process goes on until the TTL value in the update message increases to the TTL limit.",
                "To speed up the learning process, while updating the expected utility values of an agent Ais neighboring agents we specify that Um(Qk, ttl0) >= Um(Qk, ttl1) iff ttl0 > ttl1 Thus, when agent Ai receives an updated expected utility value with ttl1, it also updates the expected utility values with any ttl0 > ttl1 if Um(Qk, ttl0) < Um(Qk, ttl1) to speed up convergence.",
                "This heuristic is based on the fact that the utility of a search session is a non-decreasing function of time t. 3.3 Discussion In formalizing the content routing system as a learning task, many assumptions are made.",
                "In real systems, these assumptions may not hold, and thus the learning algorithm may not converge.",
                "Two problems are of particular note, (1) This content routing problem does not have Markov properties.",
                "In contrast to IP-level based packet routing, the routing decision of each agent for a particular search session sj depends on the routing history of sj.",
                "Therefore, the assumption that all subsequent search sessions are independent does not hold in reality.",
                "This may lead to double counting problem that the relevant documents of some agents will be counted more than once for the state where the TTL value is more than 1.",
                "However, in the context of the hierarchical agent organizations, two factors mitigate this problems: first, the agents in each content group form a tree-like structure.",
                "With the absense of the cycles, the estimates inside the tree would be close to the accurate value.",
                "Secondly, the stochastic nature of the routing policy partly remedies this problem.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 235 (2) Another challenge for this learning algorithm is that in a real network environment observations on neighboring agents may not be able to be updated in time due to the communication delay or other situations.",
                "In addition, when neighboring agents update their estimates at the same time, oscillation may arise during the learning process[1].",
                "This paper explores several approaches to speed up the learning process.",
                "Besides the aforementioned strategy of updating the expected utility values, we also employ an active update strategy where agents notify their neighbors whenever its expected utility is updated.",
                "Thus a faster convergence speed can be achieved.",
                "This strategy contrasts to the Lazy update, where agents only echo their neighboring agents with their expected utility change when they exchange information.",
                "The trade off between the two approaches is the network load versus learning speed.",
                "The advantage of this learning algorithm is that once a routing policy is learned, agents do not have to repeatedly compare the similarity of queries as long as the network topology remains unchanged.",
                "Instead, agent just have to determine the classification of the query properly and follow the learned policies.",
                "The disadvantage of this learning-based approach is that the learning process needs to be conducted whenever the network structure changes.",
                "There are many potential extensions for this learning model.",
                "For example, a single measure is currently used to indicate the traffic load for an agents neighborhood.",
                "A simple extension would be to keep track of individual load for each neighbor of the agent. 4.",
                "EXPERIMENTSSETTINGSAND RESULTS The experiments are conducted on TRANO simulation toolkit with two sets of datasets, TREC-VLC-921 and TREC123-100.",
                "The following sub-sections introduce the TRANO testbed, the datasets, and the experimental results. 4.1 TRANO Testbed TRANO (Task Routing on Agent Network Organization) is a multi-agent based network based information retrieval testbed.",
                "TRANO is built on top of the Farm [4], a time based distributed simulator that provides a data dissemination framework for large scale distributed agent network based organizations.",
                "TRANO supports importation and exportation of agent organization profiles including topological connections and other features.",
                "Each TRANO agent is composed of an agent view structure and a control unit.",
                "In simulation, each agent is pulsed regularly and the agent checks the incoming message queues, performs local operations and then forwards messages to other agents . 4.2 Experimental Settings In our experiment, we use two standard datasets, TRECVLC-921 and TREC-123-100 datasets, to simulate the collections hosted on agents.",
                "The TREC-VLC-921 and TREC123-100 datasets were created by the U.S. National Institute for Standard Technology(NIST) for its TREC conferences.",
                "In distributed information retrieval domain, the two data collections are split to 921 and 100 sub-collections.",
                "It is observed that dataset TREC-VLC-921 is more heterogeneous than TREC-123-100 in terms of source, document length, and relevant document distribution from the statistics of the two data collections listed in [13].",
                "Hence, TREC-VLC-921 is much closer to real document distributions in P2P environments.",
                "Furthermore, TREC-123-100 is split into two sets of 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0 500 1000 1500 2000 2500 3000 ARSS Query number ARSS versus the number of incoming queries for TREC-VLC-921 SSLA-921 SSNA-921 Figure 3: ARSS(Average reward per search session) versus the number of search sessions for 1phase search in TREC-VLC-921 0 0.1 0.2 0.3 0.4 0.5 0.6 0 500 1000 1500 2000 2500 3000 ARSS Query number ARSS versus query number for TREC-VLC-921 TSLA-921 TSNA-921 Figure 4: ARSS(Average reward per search session) versus the number of search sessions for 2phase search in TREC-VLC-921 sub-collections in two ways: randomly and by source.",
                "The two partitions are denoted as TREC-123-100-Random and TREC-123-100-Source respectively.",
                "The documents in each subcollection in dataset TREC-123-100-Source are more coherent than those in TREC-123-100-Random.",
                "The two different sets of partitions allow us to observe how the distributed learning algorithm is affected by the homogeneity of the collections.",
                "The hierarchical agent organization is generated by the algorithm described in our previous algorithm [15].",
                "During the topology generation process, degree information of each agent is estimated by the algorithm introduced by Palmer et al. [9] with parameters α = 0.5 and β = 0.6.",
                "In our experiments, we estimate the upward limit and downward degree limit using linear discount factors 0.5, 0.8 and 1.0.",
                "Once the topology is built, queries randomly selected from the query set 301−350 on TREC-VLC-921 and query set 1− 50 on TREC-123-100-Random and TREC-123-100-Source are injected to the system based on a Poisson distribution P(N(t) = n) = (λt)n n! e−λ 236 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 0 50 100 150 200 250 300 350 400 0 500 1000 1500 2000 2500 3000 Cumulativeutility Query number Cumulative utility over the number of incoming queries TSLA-921 SSNA-921 SSLA-921 TSNA-921 Figure 5: The cumulative utility versus the number of search sessions TREC-VLC-921 In addition, we assume that all agents have an equal chance of getting queries from the environment, i.e, λ is the same for every agent.",
                "In our experiments, λ is set as 0.0543 so that the mean of the incoming queries from the environment to the agent network is 50 per time unit.",
                "The service time for the communication queue and local search queue, i.e tQij and trs, is set as 0.01 time unit and 0.05 time units respectively.",
                "In our experiments, there are ten types of queries acquired by clustering the query set 301 − 350 and 1 − 50. 4.3 Results analysis and evaluation Figure 3 demonstrates the ARSS(Average Reward per Search Session) versus the number of incoming queries over time for the the Single-Step based Non-learning Algorithm (SSNA), and the Single-Step Learning Algorithm(SSLA) for data collection TREC-VLC-921.",
                "It shows that the average reward for SSNA algorithm ranges from 0.02 − 0.06 and the performance changes little over time.",
                "The average reward for SSLA approach starts at the same level with the SSNA algorithm.",
                "But the performance increases over time and the average performance gain stabilizes at about 25% after query range 2000 − 3000.",
                "Figure 4 shows the ARSS(Average Reward per Search Session) versus the number of incoming queries over time for the the Two-Step based Non-learning Algorithm(TSNA), and the Two-Step Learning Algorithm(TSLA) for data collection TREC-VLC-921.",
                "The TSNA approach has a relatively consistent performance with the average reward ranges from 0.05 − 0.15.",
                "The average reward for TSLA approach, where learning algorithm is exploited, starts at the same level with the TSNA algorithm and improves the average reward over time until 2000−2500 queries joining the system.",
                "The results show that the average performance gain for TSLA approach over TNLA approach is 35% after stabilization.",
                "Figure 5 shows the cumulative utility versus the number of incoming queries over time for SSNA, SSLA,TSNA, and TSLA respectively.",
                "It illustrates that the cumulative utility of non-learning algorithms increases largely linearly over time, while the gains of learning-based algorithms accelerate when more queries enter the system.",
                "These experimental results demonstrate that learning-based approaches consistently perform better than non-learning based routing algorithm.",
                "Moreover, two-phase learning based algorithm is better than single-phase based learning algorithm because the maximal reward an agent can receive from searching its neighborhood within TTL hops is related to the total number of the relevant documents in that area.",
                "Thus, even the optimal routing policy can do little beyond reaching these relevant documents faster.",
                "On the contrary, the two-stepbased learning algorithm can relocate the search session to a neighborhood with more relevant documents.",
                "The TSLA combines the merits of both approaches and outperforms them.",
                "Table 1 lists the cumulative utility for datasets TREC123-100-Random and TREC-123-100-Source with hierarchical organizations.",
                "The five columns show the results for four different approaches.",
                "In particular, column TSNA-Random shows the results for dataset TREC-123-100-Random with the TSNA approach.",
                "The column TSLA-Random shows the results for dataset TREC-123-100-Random with the TSLA approach.",
                "There are two numbers in each cell in the column TSLA-Random.",
                "The first number is the actual cumulative utility while the second number is the percentage gain in terms of the utility over TSNA approach.",
                "Columns TSNA-Source and TSLA-Source show the results for dataset TREC-123-100-Source with TSNA and TSLA approaches respectively.",
                "Table 1 shows that the performance improvement for TREC-123-100-Random is not as significant as the other datasets.",
                "This is because that the documents in the sub-collection of TREC-123-100-Random are selected randomly which makes the collection model, the signature of the collection, less meaningful.",
                "Since both algorithms are designed based on the assumption that document collections can be well represented by their collection model, this result is not surprising.",
                "Overall, Figures 4, 5, and Table 1 demonstrate that the reinforcement learning based approach can considerably enhance the system performance for both data collections.",
                "However, it remains as future work to discover the correlation between the magnitude of the performance gains and the size of the data collection and/or the extent of the heterogeneity between the sub-collections. 5.",
                "RELATED WORK The content routing problem differs from the networklevel routing in packet-switched communication networks in that content-based routing occurs in application-level networks.",
                "In addition, the destination agents in our contentrouting algorithms are multiple and the addresses are not known in the routing process.",
                "IP-level Routing problems have been attacked from the reinforcement learning perspective[2, 5, 11, 12].",
                "These studies have explored fully distributed algorithms that are able, without central coordination to disseminate knowledge about the network, to find the shortest paths robustly and efficiently in the face of changing network topologies and changing link costs.",
                "There are two major classes of adaptive, distributed packet routing algorithms in the literature: distance-vector algorithms and link-state algorithms.",
                "While this line of studies carry a certain similarity with our work, it has mainly focused on packet-switched communication networks.",
                "In this domain, the destination of a packet is deterministic and unique.",
                "Each agent maintains estimations, probabilistically or deterministically, on the distance to a certain destination through its neighbors.",
                "A variant of Q-Learning techniques is deployed The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 237 Table 1: Cumulative Utility for Datasets TREC-123-100-Random and TREC-123-100-Source with Hierarchical Organization; The percentage numbers in the columns TSLA-Random and TSLA-Source demonstrate the performance gain over the algorithm without learning Query number TSNA-Random TSLA-Random TSNA-Source TSLA-Source 500 25.15 28.45 13% 24.00 21.05 -13% 1000 104.99 126.74 20% 93.95 96.44 2.6% 1250 149.79 168.40 12% 122.64 134.05 9.3% 1500 188.94 211.05 12% 155.30 189.60 22% 1750 235.49 261.60 11% 189.14 243.90 28% 2000 275.09 319.25 16% 219.0 278.80 26% to update the estimations to converge to the real distances.",
                "It has been discovered that the locality property is an important feature of information retrieval systems in user modeling studies[3].",
                "In P2P based content sharing systems, this property is exemplified by the phenomenon that users tend to send queries that represent only a limited number of topics and conversely, users in the same neighborhood are likely to share common interests and send similar queries [10].",
                "The learning based approach is perceived to be more beneficial for real distributed information retrieval systems which exhibit locality property.",
                "This is because the users traffic and query patterns can reduce the state space and speed up the learning process.",
                "Related work in taking advantage of this property include [7], where the authors attempted to address this problem by user modeling techniques. 6.",
                "CONCLUSIONS In this paper, a reinforcement-learning based approach is developed to improve the performance of distributed IR search algorithms.",
                "Particularly, agents maintain estimates, namely expected utility, on the downstream agents ability to provide relevant documents for incoming queries.",
                "These estimates are updated gradually by learning from the feedback information returned from previous search sessions.",
                "Based on the updated expected utility information, the agents modify their routing policies.",
                "Thereafter, these agents route the queries based on the learned policies and update the estimates on the expected utility based on the new routing policies.",
                "The experiments on two different distributed IR datasets illustrates that the reinforcement learning approach improves considerably the cumulative utility over time. 7.",
                "REFERENCES [1] S. Abdallah and V. Lesser.",
                "Learning the task allocation game.",
                "In AAMAS 06: Proceedings of the fifth international joint conference on Autonomous agents and multiagent systems, pages 850-857, New York, NY, USA, 2006.",
                "ACM Press. [2] J.",
                "A. Boyan and M. L. Littman.",
                "Packet routing in dynamically changing networks: A reinforcement learning approach.",
                "In Advances in Neural Information Processing Systems, volume 6, pages 671-678.",
                "Morgan Kaufmann Publishers, Inc., 1994. [3] J. C. French, A. L. Powell, J. P. Callan, C. L. Viles, T. Emmitt, K. J. Prey, and Y. Mou.",
                "Comparing the performance of database selection algorithms.",
                "In Research and Development in Information Retrieval, pages 238-245, 1999. [4] B. Horling, R. Mailler, and V. Lesser.",
                "Farm: A scalable environment for multi-agent development and evaluation.",
                "In Advances in Software Engineering for Multi-Agent Systems, pages 220-237, Berlin, 2004.",
                "Springer-Verlag. [5] M. Littman and J. Boyan.",
                "A distributed reinforcement learning scheme for network routing.",
                "In Proceedings of the International Workshop on Applications of Neural Networks to Telecommunications, 1993. [6] J. Lu and J. Callan.",
                "Federated search of text-based digital libraries in hierarchical peer-to-peer networks.",
                "In In ECIR05, 2005. [7] J. Lu and J. Callan.",
                "User modeling for full-text federated search in peer-to-peer networks.",
                "In ACM SIGIR 2006.",
                "ACM Press, 2006. [8] C. D. Manning and H. Sch¨utze.",
                "Foundations of Statistical Natural Language Processing.",
                "The MIT Press, Cambridge, Massachusetts, 1999. [9] C. R. Palmer and J. G. Steffan.",
                "Generating network topologies that obey power laws.",
                "In Proceedings of GLOBECOM 2000, November 2000. [10] K. Sripanidkulchai, B. Maggs, and H. Zhang.",
                "Efficient content location using interest-based locality in peer-topeer systems.",
                "In INFOCOM, 2003. [11] D. Subramanian, P. Druschel, and J. Chen.",
                "Ants and reinforcement learning: A case study in routing in dynamic networks.",
                "In In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence, pages 832-839, 1997. [12] J. N. Tao and L. Weaver.",
                "A multi-agent, policy gradient approach to network routing.",
                "In In Proceedings of the Eighteenth International Conference on Machine Learning, 2001. [13] H. Zhang, W. B. Croft, B. Levine, and V. Lesser.",
                "A multi-agent approach for <br>peer-to-peer information retrieval</br>.",
                "In Proceedings of Third International Joint Conference on Autonomous Agents and Multi-Agent Systems, July 2004. [14] H. Zhang and V. Lesser.",
                "Multi-agent based <br>peer-to-peer information retrieval</br> systems with concurrent search sessions.",
                "In Proceedings of the Fifth International Joint Conference on Autonomous Agents and Multi-Agent Systems, May 2006. [15] H. Zhang and V. R. Lesser.",
                "A dynamically formed hierarchical agent organization for a distributed content sharing system.",
                "In 2004 IEEE/WIC/ACM International Conference on Intelligent Agent Technology (IAT 2004), 20-24 September 2004, Beijing, China, pages 169-175.",
                "IEEE Computer Society, 2004. 238 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "A Reinforcement Learning based Distributed Search Algorithm For Hierarchical <br>peer-to-peer information retrieval</br> Systems Haizheng Zhang College of Information Science and Technology Pennsylvania State University University Park, PA 16803 hzhang@ist.psu.edu Victor Lesser Department of Computer Science University Of Massachusetts Amherst, MA 01003 lesser@cs.umass.edu ABSTRACT The dominant existing routing strategies employed in peerto-peer(P2P) based information retrieval(IR) systems are similarity-based approaches.",
                "A multi-agent approach for <br>peer-to-peer information retrieval</br>.",
                "Multi-agent based <br>peer-to-peer information retrieval</br> systems with concurrent search sessions."
            ],
            "translated_annotated_samples": [
                "Un algoritmo de búsqueda distribuida basado en aprendizaje por refuerzo para sistemas jerárquicos de <br>recuperación de información entre pares</br>.",
                "Un enfoque multiagente para la <br>recuperación de información entre pares</br>.",
                "Sistemas de <br>recuperación de información peer-to-peer</br> basados en múltiples agentes con sesiones de búsqueda concurrentes."
            ],
            "translated_text": "Un algoritmo de búsqueda distribuida basado en aprendizaje por refuerzo para sistemas jerárquicos de <br>recuperación de información entre pares</br>. En estos enfoques, los agentes dependen de la similitud de contenido entre las consultas entrantes y sus agentes vecinos directos para dirigir las sesiones de búsqueda distribuida. Sin embargo, tal heurística es miope en el sentido de que los agentes vecinos pueden no estar conectados con agentes más relevantes. En este documento, se desarrolla un enfoque basado en aprendizaje por refuerzo en línea para aprovechar las características dinámicas en tiempo de ejecución de los sistemas de recuperación de información P2P, representadas por información sobre sesiones de búsqueda pasadas. Específicamente, los agentes mantienen estimaciones sobre las habilidades de los agentes aguas abajo para proporcionar documentos relevantes para las consultas entrantes. Estas estimaciones se actualizan gradualmente al aprender de la información de retroalimentación devuelta de sesiones de búsqueda anteriores. Basándose en esta información, los agentes derivan políticas de enrutamiento correspondientes. Posteriormente, estos agentes dirigen las consultas basándose en las políticas aprendidas y actualizan las estimaciones según las nuevas políticas de enrutamiento. Los resultados experimentales demuestran que el algoritmo de aprendizaje mejora considerablemente el rendimiento del enrutamiento en dos conjuntos de colecciones de pruebas que han sido utilizados en una variedad de estudios de recuperación de información distribuida. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagente Términos Generales Algoritmos, Rendimiento, Experimentación 1. En los últimos años ha habido un creciente interés en estudiar cómo controlar los procesos de búsqueda en sistemas de recuperación de información (IR) basados en pares (P2P) [6, 13, 14, 15]. En esta línea de investigación, uno de los problemas centrales que preocupa a los investigadores es dirigir de manera eficiente las consultas de los usuarios en la red a los agentes que poseen los documentos apropiados. En ausencia de información global, las estrategias dominantes para abordar este problema son enfoques basados en la similitud de contenido [6, 13, 14, 15]. Si bien la similitud de contenido entre consultas y nodos locales parece ser un indicador creíble para la cantidad de documentos relevantes que residen en cada nodo, estos enfoques están limitados por una serie de factores. En primer lugar, las métricas basadas en similitud pueden ser miope ya que los nodos localmente relevantes pueden no estar conectados a otros nodos relevantes. Segundo, los enfoques basados en la similitud no tienen en cuenta las características de tiempo de ejecución de los sistemas de IR P2P, incluidos los parámetros ambientales, el uso del ancho de banda y la información histórica de las sesiones de búsqueda pasadas, que proporcionan información valiosa para los algoritmos de enrutamiento de consultas. En este artículo, desarrollamos un enfoque de IR basado en aprendizaje por refuerzo para mejorar el rendimiento de los algoritmos de búsqueda de IR distribuidos. Los agentes pueden adquirir mejores estrategias de búsqueda al recopilar y analizar información de retroalimentación de sesiones de búsqueda anteriores. En particular, los agentes mantienen estimaciones, es decir, utilidad esperada, sobre las capacidades de los agentes aguas abajo de proporcionar documentos relevantes para tipos específicos de consultas entrantes. Estas estimaciones se actualizan gradualmente al aprender de la información de retroalimentación devuelta de sesiones de búsqueda anteriores. Basándose en la información actualizada de utilidad esperada, los agentes derivan políticas de enrutamiento correspondientes. Posteriormente, estos agentes dirigen las consultas basándose en las políticas aprendidas y actualizan las estimaciones sobre la utilidad esperada en función de las nuevas políticas de enrutamiento. Este proceso se lleva a cabo de manera iterativa. El objetivo del algoritmo de aprendizaje, aunque consume ancho de banda de red, es reducir el tiempo de enrutamiento para procesar más consultas por unidad de tiempo y al mismo tiempo encontrar más documentos relevantes. Esto contrasta con los enfoques basados en la similitud de contenido, donde se repiten operaciones similares para cada consulta entrante y el tiempo de procesamiento se mantiene en gran medida constante con el tiempo. Otra forma de ver este documento es que nuestro enfoque básico para la búsqueda de IR distribuida es construir una red de superposición jerárquica (organización de agentes) basada en la medida de similitud de contenido entre las colecciones de documentos de los agentes de manera ascendente. En trabajos anteriores, hemos demostrado que esta organización mejora significativamente el rendimiento de búsqueda. Sin embargo, esta estructura organizativa no tiene en cuenta los patrones de llegada de las consultas, incluyendo su frecuencia, tipos y dónde entran en el sistema, ni el ancho de banda de comunicación disponible en la red y las capacidades de procesamiento de los agentes individuales. La intención del aprendizaje por refuerzo es adaptar las decisiones de enrutamiento de los agentes a las situaciones dinámicas de la red y aprender de sesiones de búsqueda pasadas. Específicamente, las contribuciones de este artículo incluyen: (1) un enfoque basado en aprendizaje por refuerzo para que los agentes adquieran políticas de enrutamiento satisfactorias basadas en estimaciones de la contribución potencial de sus agentes vecinos; (2) dos estrategias para acelerar el proceso de aprendizaje. Según nuestro mejor conocimiento, esta es una de las primeras aplicaciones de aprendizaje por refuerzo para abordar problemas de compartición de contenido distribuido, lo cual indica algunos de los problemas al aplicar el refuerzo en una aplicación compleja. El resto de este documento está organizado de la siguiente manera: la Sección 2 revisa los sistemas jerárquicos de intercambio de contenido y el algoritmo de búsqueda de dos fases basado en dicha topología. La Sección 3 describe un enfoque basado en aprendizaje por refuerzo para dirigir el proceso de enrutamiento; la Sección 4 detalla la configuración experimental y analiza los resultados. La sección 5 discute estudios relacionados y la sección 6 concluye el artículo. BÚSQUEDA EN SISTEMAS JERÁRQUICOS DE IR P2P Esta sección revisa brevemente nuestros enfoques básicos para los sistemas jerárquicos de IR P2P. En un sistema jerárquico de recuperación de información P2P ilustrado en la Fig.1, los agentes están conectados entre sí a través de tres tipos de enlaces: enlaces ascendentes, enlaces descendentes y enlaces laterales. En las siguientes secciones, denotamos el conjunto de agentes que están directamente conectados al agente Ai como DirectConn(Ai), el cual se define como DirectConn(Ai) = NEI(Ai) ∪ PAR(Ai) ∪ CHL(Ai), donde NEI(Ai) es el conjunto de agentes vecinos conectados a Ai a través de enlaces laterales; PAR(Ai) es el conjunto de agentes a los que el agente Ai está conectado a través de enlaces ascendentes y CHL(Ai) es el conjunto de agentes a los que el agente Ai se conecta a través de enlaces descendentes. Estos enlaces se establecen a través de un proceso de agrupamiento distribuido basado en la similitud de contenido de abajo hacia arriba[15]. Estos enlaces son luego utilizados por agentes para localizar otros agentes que contienen documentos relevantes a las consultas dadas. Un agente típico Ai en nuestro sistema utiliza dos colas: una cola de búsqueda local, LSi, y una cola de reenvío de mensajes MFi. Los estados de las dos colas constituyen los estados internos de un agente. La cola de búsqueda local LSi almacena sesiones de búsqueda programadas para procesamiento local. Es una cola de prioridad y el agente Ai siempre selecciona las consultas más prometedoras para procesarlas con el fin de maximizar la utilidad global. MFi consiste en un conjunto de consultas para reenviar y se procesa de manera FIFO (primero en entrar, primero en salir). Para la primera consulta en MFi, el agente Ai determina a qué subconjunto de sus agentes vecinos reenviarla basándose en la política de enrutamiento de los agentes πi. Estas decisiones de enrutamiento determinan cómo se lleva a cabo el proceso de búsqueda en la red. En este documento, llamamos a Ai como el agente aguas arriba de Ajs y a Aj como el agente aguas abajo de Ais si A4 A5 A6 A7 A2 A3 A9 NEI(A2)={A3} PAR(A2)={A1} CHL(A2)={A4,A5} A1 A8 Figura 1: Una fracción de un sistema jerárquico P2PIR en el que un agente Ai dirige una consulta al agente Aj. El protocolo de búsqueda distribuida de nuestra organización de agentes jerárquicos está compuesto por dos pasos. En el primer paso, al recibir una consulta qk en el tiempo tl de un usuario, el agente Ai inicia una sesión de búsqueda si al sondear a sus agentes vecinos Aj ∈ NEI(Ai) con el mensaje PROBE para el valor de similitud Sim(qk, Aj) entre qk y Aj. Aquí, Ai se define como el iniciador de la consulta de la sesión de búsqueda si. En el segundo paso, Ai selecciona un grupo de agentes más prometedores para iniciar el proceso de búsqueda real con el mensaje BUSCAR. Estos mensajes de BÚSQUEDA contienen un parámetro TTL (Tiempo de Vida) además de la consulta. El valor TTL disminuye en 1 después de cada salto. En el proceso de búsqueda, los agentes descartan aquellas consultas que han sido procesadas previamente o cuyo TTL disminuye a 0, lo que evita que las consultas se repitan infinitamente en el sistema. La sesión de búsqueda termina cuando todos los agentes que reciben la consulta la descartan o el TTL disminuye a 0. Tras recibir mensajes de BÚSQUEDA para qk, los agentes programan actividades locales que incluyen búsqueda local, reenvío de qk a sus vecinos y devolución de resultados de búsqueda al iniciador de la consulta. Este proceso y los algoritmos relacionados se detallan en [15, 14]. 3. En el enfoque de búsqueda basado en aprendizaje por refuerzo, mencionado anteriormente, las decisiones de enrutamiento de un agente Ai dependen de la comparación de similitud entre las consultas entrantes y los agentes vecinos de Ai para reenviar esas consultas a agentes relevantes sin inundar la red con mensajes de consulta innecesarios. Sin embargo, esta heurística es miope porque un vecino directo relevante no necesariamente está conectado con otros agentes relevantes. En esta sección, proponemos un enfoque más general al plantear este problema como una tarea de aprendizaje por refuerzo. En busca de una mayor flexibilidad, los agentes pueden alternar entre dos modos: modo de aprendizaje y modo sin aprendizaje. En el modo de no aprendizaje, los agentes operan de la misma manera que lo hacen en los procesos de búsqueda distribuida normales descritos en [14, 15]. Por otro lado, en el modo de aprendizaje, en paralelo con las sesiones de búsqueda distribuida, los agentes también participan en un proceso de aprendizaje que se detallará en esta sección. Ten en cuenta que en el protocolo de aprendizaje, el proceso de aprendizaje no interfiere con el proceso de búsqueda distribuida. Los agentes pueden elegir iniciar y detener procesos de aprendizaje sin afectar el rendimiento del sistema. En particular, dado que el proceso de aprendizaje consume recursos de red (especialmente ancho de banda), los agentes pueden optar por iniciar el aprendizaje solo cuando la carga de la red es relativamente baja, minimizando así los costos adicionales de comunicación incurridos por el algoritmo de aprendizaje. La sección está estructurada de la siguiente manera, la Sección 3.1 describe 232 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) un modelo basado en aprendizaje por refuerzo. La sección 3.2 describe un protocolo para implementar el algoritmo de aprendizaje en la red. La sección 3.3 discute la convergencia del algoritmo de aprendizaje. 3.1 El Modelo La política de enrutamiento de un agente toma el estado de una sesión de búsqueda como entrada y produce las acciones de enrutamiento para esa consulta. En nuestro trabajo, el estado de una sesión de búsqueda sj se estipula como: QSj = (qk, ttlj) donde ttlj es el número de saltos que quedan para la sesión de búsqueda sj, qk es la consulta específica. QL es un atributo de qk que indica a qué tipo de consultas qk probablemente pertenecen. El conjunto de QL puede ser generado ejecutando un algoritmo de clasificación en línea simple en todas las consultas que han sido procesadas por los agentes, o un algoritmo fuera de línea en un conjunto de entrenamiento pre-designado. La suposición aquí es que el conjunto de tipos de consultas se aprende de antemano y pertenece al conocimiento común de los agentes en la red. El trabajo futuro incluye explorar cómo se puede lograr el aprendizaje cuando esta suposición no se cumple. Dado el conjunto de tipos de consultas establecido, una consulta entrante qi puede ser clasificada en una clase de consulta Q(qi) mediante la fórmula: Q(qi) = arg max Qj P(qi|Qj) (1) donde P(qi|Qj) indica la probabilidad de que la consulta qi sea generada por la clase de consulta Qj [8]. El conjunto de acciones de enrutamiento atómico de un agente Ai se denota como {αi}, donde {αi} se define como αi = {αi0 , αi1 , ..., αin }. Un elemento αij representa una acción para dirigir una consulta dada al agente vecino Aij ∈ DirectConn(Ai). La política de enrutamiento πi del agente Ai es estocástica y su resultado para una sesión de búsqueda con estado QSj se define como: πi(QSj) = {(αi0 , πi(QSi, αi0 )), (αi1 , πi(QSi, αi1 )), ...} (2) Nótese que el operador πi está sobrecargado para representar tanto la política probabilística para una sesión de búsqueda con estado QSj, denotada como πi(QSj); como la probabilidad de enviar la consulta a un agente vecino específico Aik ∈ DirectConn(Ai) bajo la política πi(QSj), denotada como πi(QSj, αik). Por lo tanto, la ecuación (2) significa que la probabilidad de reenviar la sesión de búsqueda al agente Ai0 es πi(QSi, αi0) y así sucesivamente. Bajo esta política estocástica, la acción de enrutamiento es no determinística. La ventaja de esta estrategia es que los mejores agentes vecinos no serán seleccionados repetidamente, mitigando así las posibles situaciones de puntos calientes. La utilidad esperada, Un i (QSj), se utiliza para estimar la ganancia potencial de utilidad al dirigir el tipo de consulta QSj al agente Ai bajo la política πn i. El superíndice n indica el valor en la n-ésima iteración en un proceso de aprendizaje iterativo. La utilidad esperada proporciona orientación de enrutamiento para futuras sesiones de búsqueda. En el proceso de búsqueda, cada agente Ai mantiene observaciones parciales de los estados de sus vecinos, como se muestra en la Figura 2. La observación parcial incluye información no local como la estimación de la utilidad potencial de su vecino Am para el estado de consulta QSj, denotado como Um(QSj), así como la información de carga, Lm. Estas observaciones son actualizadas periódicamente por los vecinos. La información de utilidad estimada se utilizará para actualizar la utilidad esperada de A en su política de enrutamiento. Cargar información de utilidad esperada para diferentes tipos de consultas de agentes vecinos... A0 A1 A3 A2 Un 0 (QS0) ... ... ... ... ...... Un 0 (QS1) Un 1 (QS1) Un 2 (QS1) Un 3 (QS1) Un 1 (QS0) Un 2 (QS0) Un 3 (QS0) Ln 0 Ln 1 Ln 2 Ln 3 ... QS0 QS1 ... Figura 2: La Observación Parcial del Agente Ais sobre sus vecinos (A0, A1...) La información de carga de Am, Lm, se define como Lm = |MFm| Cm, donde |MFm| es la longitud de la cola de mensajes en espera y Cm es la tasa de servicio de la cola de mensajes en espera del agente Am. Por lo tanto, Lm caracteriza la utilización de un canal de comunicación de agentes, y así proporciona información no local a los vecinos de Ams para ajustar los parámetros de su política de enrutamiento y evitar inundar a sus agentes aguas abajo. Ten en cuenta que, basado en las características de las consultas que ingresan al sistema y las capacidades de los agentes, la carga de trabajo de los agentes puede no ser uniforme. Después de recopilar la información de la tasa de utilización de todos sus vecinos, el agente Ai calcula Li como una medida única para evaluar la condición de carga promedio de su vecindario: Li = P k Lk |DirectConn(Ai)| Los agentes explotan el valor de Li para determinar la probabilidad de enrutamiento en su política de enrutamiento. Se debe tener en cuenta que, como se describe en la Sección 3.2, la información sobre agentes vecinos se transmite junto con el mensaje de consulta propagado entre los agentes siempre que sea posible para reducir la sobrecarga de tráfico. 3.1.1 Actualización de la Política Se introduce un proceso de actualización iterativo para que los agentes aprendan una política de enrutamiento estocástica satisfactoria. En este proceso iterativo, los agentes actualizan sus estimaciones sobre la utilidad potencial de sus políticas de enrutamiento actuales y luego propagan las estimaciones actualizadas a sus vecinos. Sus vecinos luego generan una nueva política de enrutamiento basada en la observación actualizada y a su vez calculan la utilidad esperada basada en las nuevas políticas y continúan este proceso iterativo. En particular, en el tiempo n, dado un conjunto de utilidad esperada, un agente Ai, cuyo conjunto de agentes directamente conectados es DirectConn(Ai) = {Ai0, ..., Aim}, determina su política de enrutamiento estocástico correspondiente para una sesión de búsqueda del estado QSj basada en los siguientes pasos: (1) Ai primero selecciona un subconjunto de agentes como los agentes potenciales aguas abajo del conjunto DirectConn(Ai), denotado como PDn(Ai, QSj). El tamaño del agente potencial aguas abajo se especifica como |PDn(Ai, QSj)| = min(|NEI(Ai), dn i + k)| donde k es una constante y se establece en 3 en este documento; dn i, el ancho hacia adelante, se define como el número esperado de The Sixth Intl. La fórmula especifica que el conjunto potencial de agentes aguas abajo PDn(Ai, QSj) es o bien el subconjunto de agentes vecinos con los k valores de utilidad esperada más altos para el estado QSj entre todos los agentes en DirectConn(Ai), o todos sus agentes vecinos. La k se introduce basada en la idea de una política de enrutamiento estocástica y hace que la probabilidad de reenvío del agente dn i + k más alto sea inferior al 100%. Ten en cuenta que si queremos limitar el número de agentes secundarios para la sesión de búsqueda sj a 5, la probabilidad de reenviar la consulta a todos los agentes vecinos debe sumar 5. Configurar adecuadamente el valor de dn i puede mejorar la tasa de utilización del ancho de banda de la red cuando gran parte de la red está inactiva, al mismo tiempo que mitiga la carga de tráfico cuando la red está altamente cargada. El valor de dn+1 i se actualiza en función de dn i, las observaciones anteriores y actuales sobre la situación del tráfico en el vecindario. Específicamente, la fórmula de actualización para dn+1 i es dn+1 i = dn i ∗ (1 + 1 − Li |DirectConn(Ai)| ) En esta fórmula, el ancho de avance se actualiza en función de las condiciones de tráfico del vecindario del agente Ai, es decir, Li, y su valor anterior. (2) Para cada agente Aik en el PDn(Ai, QSj), la probabilidad de reenviar la consulta a Aik se determina de la siguiente manera para asignar una mayor probabilidad de reenvío a los agentes vecinos con un valor de utilidad esperado más alto: πn+1 i (QSj, αik ) = dn+1 i |PDn(Ai, QSj)| + β ∗ ` Uik (QSj) − PDU(Ai, QSj) |PDn(Ai, QSj)| ´ (3) donde PDUn(Ai, QSj) = X o∈P Dn(Ai,QSj ) Uo(QSj) y QSj es el estado subsiguiente del agente Aik después de que el agente Ai reenvía la sesión de búsqueda con estado QSj a su agente vecino Aik; Si QSj = (qk, ttl0), entonces QSj = (qk, ttl0 − 1). En la fórmula 3, el primer término a la derecha de la ecuación, dn+1 i |P Dn(Ai,QSj )|, se utiliza para determinar la probabilidad de reenvío distribuyendo equitativamente el ancho de avance, dn+1 i, a los agentes en el conjunto PDn(Ai, QSj). El segundo término se utiliza para ajustar la probabilidad de ser elegido de manera que se favorezca a los agentes con valores de utilidad esperada más altos. β se determina de acuerdo a: β = min ` m − dn+1 i m ∗ umax − PDUn(Ai, QSj) , dn+1 i PDUn(Ai, QSj) − m ∗ umin ´ (4) donde m = |PDn(Ai, QSj)|, umax = max o∈P Dn(Ao,QSj ) Uo(QSj) y umin = min o∈P Dn(Ao,QSj ) Uo(QSj) Esta fórmula garantiza que el valor final de πn+1 i (QSj, αik ) esté bien definido, es decir, 0 ≤ πn+1 i (QSj, αik ) ≤ 1 y X i πn+1 i (QSj, αik ) = dn+1 i Sin embargo, esta solución no explora todas las posibilidades. Para equilibrar entre la explotación y la exploración, se adopta un enfoque λ-Greedy. En el enfoque λ-Greedy, además de asignar una probabilidad más alta a aquellos agentes con un valor de utilidad esperado más alto, como en la ecuación (3). Los agentes que parecen no ser buenas opciones también recibirán consultas basadas en una tasa de exploración dinámica. En particular, para agentes en el conjunto PDn(Ai, QSj), πn+1 i1 (QSj) se determina de la misma manera que arriba, con la única diferencia de que dn+1 i es reemplazado por dn+1 i ∗ (1 − λn). El ancho de banda de búsqueda restante se utiliza para el aprendizaje asignando la probabilidad λn de manera uniforme a los agentes Ai2 en el conjunto DirectConn(Ai) - PDn(Ai, QSj). πn+1 i2 (QSj, αik ) = dn+1 i ∗ λn |DirectConn(Ai) - PDn(Ai, QSj)| (5) donde PDn(Ai, QSj) ⊂ DirectConn(Ai). Se debe tener en cuenta que la tasa de exploración λ no es constante y disminuye con el tiempo. El λ se determina de acuerdo con la siguiente ecuación: λn+1 = λ0 ∗ e−c1n (6) donde λ0 es la tasa de exploración inicial, que es una constante; c1 también es una constante para ajustar la tasa de disminución de la tasa de exploración; n es la unidad de tiempo actual. 3.1.2 Actualización de la Utilidad Esperada Una vez que la política de enrutamiento en el paso n+1, πn+1 i, se determina según la fórmula anterior, el agente Ai puede actualizar su propia utilidad esperada, Un+1 i (QSi), basándose en la política de enrutamiento actualizada resultante de la fórmula 5 y los valores U actualizados de sus agentes vecinos. Bajo el supuesto de que después de que una consulta se envía a los vecinos de Ai, las sesiones de búsqueda subsiguientes son independientes, la fórmula de actualización es similar a la fórmula de actualización de Bellman en Q-Learning: Un+1 i (QSj) = (1 − θi) ∗ Un i (QSj) + θi ∗ (Rn+1 i (QSj) + X k πn+1 i (QSj, αik )Un k (QSj)) (7) donde QSj = (Qj, ttl − 1) es el siguiente estado de QSj = (Qj, ttl); Rn+1 i (QSj) es la recompensa local esperada para la clase de consulta Qk en el agente Ai bajo la política de enrutamiento πn+1 i; θi es el coeficiente para decidir cuánto peso se le da al valor antiguo durante el proceso de actualización: cuanto menor sea el valor de θi, se espera que el agente aprenda más rápido el valor real, mientras que mayor sea la volatilidad del algoritmo, y viceversa. Rn+1 (s) se actualiza de acuerdo con la siguiente ecuación: 234 The Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Rn+1 i (QSj) = Rn i (QSj) +γi ∗ (r(QSj) − Rn i (QSj)) ∗ P(qj|Qj ) (8) donde r(QSj) es la recompensa local asociada con la sesión de búsqueda. P(qj|Qj) indica qué tan relevante es la consulta qj para el tipo de consulta Qj, y γi es la tasa de aprendizaje para el agente Ai. Dependiendo de la similitud entre una consulta específica qi y su tipo de consulta correspondiente Qi, la recompensa local asociada con la sesión de búsqueda tiene un impacto diferente en la estimación de Rn i (QSj). En la fórmula anterior, este impacto se refleja en el coeficiente, el valor P(qj|Qj). Función de recompensa Después de que una sesión de búsqueda se detiene cuando sus valores TTL expiran, todos los resultados de búsqueda se devuelven al usuario y se comparan con la evaluación de relevancia. Suponiendo que el conjunto de resultados de búsqueda es SR, la recompensa Rew(SR) se define como: Rew(SR) = j 1 si |Rel(SR)| > c |Rel(SR)| c en caso contrario, donde SR es el conjunto de resultados de búsqueda devueltos, Rel(SR) es el conjunto de documentos relevantes en los resultados de búsqueda. Esta ecuación especifica que los usuarios otorgan una recompensa de 1.0 si el número de documentos relevantes devueltos alcanza un número predefinido c. De lo contrario, la recompensa es proporcional al número de documentos relevantes devueltos. La razón para establecer un valor de corte de este tipo es que la importancia de la proporción de recuperación disminuye con la abundancia de documentos relevantes en el mundo real, por lo tanto, los usuarios tienden a enfocarse solo en un número limitado de resultados buscados. Los detalles del protocolo de enrutamiento actual se introducirán en la Sección 3.2 cuando presentemos cómo se implementa el algoritmo de aprendizaje en sistemas reales. 3.2 Implementación del algoritmo de aprendizaje. Esta sección describe cómo se puede utilizar el algoritmo de aprendizaje en un proceso de búsqueda de una o dos fases. En el algoritmo de búsqueda de una sola fase, las sesiones de búsqueda comienzan desde los iniciadores de las consultas. Por el contrario, en el algoritmo de búsqueda de dos pasos, el iniciador de la consulta primero intenta buscar un punto de inicio más apropiado para la consulta introduciendo un paso exploratorio como se describe en la Sección 2. A pesar de la diferencia en la calidad de los puntos de partida, la mayor parte del proceso de aprendizaje para los dos algoritmos es en gran medida el mismo como se describe en los párrafos siguientes. Antes de que comience el aprendizaje, cada agente inicializa el valor de utilidad esperado para todos los posibles estados como 0. Posteriormente, al recibir una consulta, además de las operaciones normales descritas en la sección anterior, un agente Ai también configura un temporizador para esperar los resultados de búsqueda devueltos por sus agentes subordinados. Una vez que el temporizador expire o haya recibido respuesta de todos sus agentes aguas abajo, Ai fusiona y reenvía los resultados de búsqueda acumulados de sus agentes aguas abajo a su agente aguas arriba. Configurar el temporizador acelera el aprendizaje porque los agentes pueden evitar esperar demasiado tiempo a que los agentes aguas abajo devuelvan los resultados de búsqueda. Ten en cuenta que estos resultados detallados y la información correspondiente del agente seguirán almacenados en Ai hasta que la información de retroalimentación sea transmitida desde su agente aguas arriba y se pueda evaluar el rendimiento de sus agentes aguas abajo. La duración del temporizador está relacionada con el valor TTL. En este documento, configuramos el temporizador a ttimer = ttli ∗ 2 + tf, donde ttli ∗ 2 es la suma del tiempo de viaje de las consultas en la red, y tf es el período de tiempo esperado que los usuarios desearían esperar. Los resultados de la búsqueda serán finalmente devueltos al iniciador de la sesión de búsqueda A0. Serán comparados con la evaluación de relevancia proporcionada por los usuarios finales (como se describe en la sección del experimento, la evaluación de relevancia para el conjunto de consultas se proporciona junto con las colecciones de datos). La recompensa será calculada y propagada hacia atrás a los agentes a lo largo del camino por el que se pasaron los resultados de la búsqueda. Este es un proceso inverso de la propagación de los resultados de búsqueda. En el proceso de propagar la recompensa hacia atrás, los agentes actualizan las estimaciones de su propio valor potencial de utilidad, generan una política actualizada y transmiten sus resultados actualizados a los agentes vecinos basándose en el algoritmo descrito en la Sección 3. Tras el cambio del valor de utilidad esperado, el agente Ai envía su estimación de utilidad actualizada a sus vecinos para que puedan actuar según la utilidad esperada modificada y el estado correspondiente. Este mensaje de actualización incluye la recompensa potencial, así como el estado correspondiente QSi = (qk, ttll) del agente Ai. Cada agente vecino, Aj, reacciona a este tipo de mensaje de actualización actualizando el valor de utilidad esperado para el estado QSj(qk, ttll + 1) de acuerdo con el valor de utilidad esperado cambiado recién anunciado. Una vez que completen la actualización, los agentes informarían nuevamente a los vecinos relacionados para que actualicen sus valores. Este proceso continúa hasta que el valor TTL en el mensaje de actualización aumenta al límite de TTL. Para acelerar el proceso de aprendizaje, al actualizar los valores de utilidad esperada de un agente Ai con agentes vecinos, especificamos que Um(Qk, ttl0) >= Um(Qk, ttl1) si ttl0 > ttl1. Por lo tanto, cuando el agente Ai recibe un valor de utilidad esperada actualizado con ttl1, también actualiza los valores de utilidad esperada con cualquier ttl0 > ttl1 si Um(Qk, ttl0) < Um(Qk, ttl1) para acelerar la convergencia. Esta heurística se basa en el hecho de que la utilidad de una sesión de búsqueda es una función no decreciente del tiempo t. 3.3 Discusión Al formalizar el sistema de enrutamiento de contenido como una tarea de aprendizaje, se hacen muchas suposiciones. En sistemas reales, estas suposiciones pueden no cumplirse, por lo que el algoritmo de aprendizaje puede no converger. Dos problemas son de particular importancia, (1) Este problema de enrutamiento de contenido no tiene propiedades de Markov. A diferencia del enrutamiento de paquetes basado en el nivel de IP, la decisión de enrutamiento de cada agente para una sesión de búsqueda particular sj depende del historial de enrutamiento de sj. Por lo tanto, la suposición de que todas las sesiones de búsqueda posteriores son independientes no se cumple en la realidad. Esto puede llevar a un problema de doble conteo en el que los documentos relevantes de algunos agentes se contarán más de una vez para el estado donde el valor TTL es mayor que 1. Sin embargo, en el contexto de las organizaciones de agentes jerárquicos, dos factores mitigan estos problemas: primero, los agentes en cada grupo de contenido forman una estructura similar a un árbol. Con la ausencia de los ciclos, las estimaciones dentro del árbol estarían cerca del valor preciso. En segundo lugar, la naturaleza estocástica de la política de enrutamiento remedia parcialmente este problema. El Sexto Internacional. Otro desafío para este algoritmo de aprendizaje es que en un entorno de red real, las observaciones sobre agentes vecinos pueden no poder actualizarse a tiempo debido al retraso en la comunicación u otras situaciones. Además, cuando los agentes vecinos actualizan sus estimaciones al mismo tiempo, puede surgir oscilación durante el proceso de aprendizaje[1]. Este documento explora varios enfoques para acelerar el proceso de aprendizaje. Además de la estrategia mencionada de actualizar los valores de utilidad esperada, también empleamos una estrategia de actualización activa donde los agentes notifican a sus vecinos cada vez que se actualiza su utilidad esperada. Por lo tanto, se puede lograr una velocidad de convergencia más rápida. Esta estrategia contrasta con la actualización perezosa, donde los agentes solo repiten a sus agentes vecinos con el cambio en la utilidad esperada cuando intercambian información. El compromiso entre los dos enfoques es la carga de red versus la velocidad de aprendizaje. La ventaja de este algoritmo de aprendizaje es que una vez que se aprende una política de enrutamiento, los agentes no tienen que comparar repetidamente la similitud de las consultas siempre y cuando la topología de la red permanezca sin cambios. En cambio, el agente solo tiene que determinar adecuadamente la clasificación de la consulta y seguir las políticas aprendidas. La desventaja de este enfoque basado en el aprendizaje es que el proceso de aprendizaje debe llevarse a cabo cada vez que la estructura de la red cambia. Hay muchas posibles extensiones para este modelo de aprendizaje. Por ejemplo, actualmente se utiliza una sola medida para indicar la carga de tráfico en un vecindario de agentes. Una extensión sencilla sería llevar un registro de la carga individual de cada vecino del agente. 4. CONFIGURACIÓN DE EXPERIMENTOS Y RESULTADOS Los experimentos se llevan a cabo en el kit de simulación TRANO con dos conjuntos de datos, TREC-VLC-921 y TREC123-100. Las siguientes subsecciones presentan el banco de pruebas TRANO, los conjuntos de datos y los resultados experimentales. 4.1 Banco de pruebas TRANO TRANO (Task Routing on Agent Network Organization) es un banco de pruebas de recuperación de información basado en red de agentes múltiples. TRANO se construye sobre el Farm [4], un simulador distribuido basado en el tiempo que proporciona un marco de diseminación de datos para organizaciones de redes de agentes distribuidos a gran escala. TRANO apoya la importación y exportación de perfiles de organizaciones de agentes, incluyendo conexiones topológicas y otras características. Cada agente de TRANO está compuesto por una estructura de vista de agente y una unidad de control. En la simulación, cada agente es pulsado regularmente y el agente verifica las colas de mensajes entrantes, realiza operaciones locales y luego reenvía mensajes a otros agentes. 4.2 Configuración Experimental En nuestro experimento, utilizamos dos conjuntos de datos estándar, los conjuntos de datos TRECVLC-921 y TREC-123-100, para simular las colecciones alojadas en los agentes. Los conjuntos de datos TREC-VLC-921 y TREC123-100 fueron creados por el Instituto Nacional de Tecnología Estándar de los Estados Unidos (NIST) para sus conferencias TREC. En el ámbito de la recuperación de información distribuida, las dos colecciones de datos se dividen en 921 y 100 subcolecciones. Se observa que el conjunto de datos TREC-VLC-921 es más heterogéneo que TREC-123-100 en términos de origen, longitud de documento y distribución de documentos relevantes a partir de las estadísticas de las dos colecciones de datos enumeradas en [13]. Por lo tanto, TREC-VLC-921 está mucho más cerca de las distribuciones reales de documentos en entornos P2P. Además, TREC-123-100 se divide en dos conjuntos de 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0 500 1000 1500 2000 2500 3000 Número de consulta de ARSS frente al número de consultas entrantes para TREC-VLC-921 SSLA-921 SSNA-921 Figura 3: ARSS (Recompensa promedio por sesión de búsqueda) frente al número de sesiones de búsqueda para la búsqueda de 1 fase en TREC-VLC-921 0 0.1 0.2 0.3 0.4 0.5 0.6 0 500 1000 1500 2000 2500 3000 Número de consulta de ARSS frente al número de consultas para TREC-VLC-921 TSLA-921 TSNA-921 Figura 4: ARSS (Recompensa promedio por sesión de búsqueda) frente al número de sesiones de búsqueda para la búsqueda de 2 fases en TREC-VLC-921 subcolecciones de dos formas: aleatoriamente y por fuente. Las dos particiones se denominan TREC-123-100-Random y TREC-123-100-Source respectivamente. Los documentos en cada subcolección en el conjunto de datos TREC-123-100-Source son más coherentes que los de TREC-123-100-Random. Los dos conjuntos diferentes de particiones nos permiten observar cómo el algoritmo de aprendizaje distribuido se ve afectado por la homogeneidad de las colecciones. La organización jerárquica de agentes es generada por el algoritmo descrito en nuestro algoritmo anterior [15]. Durante el proceso de generación de la topología, la información de grado de cada agente es estimada por el algoritmo introducido por Palmer et al. [9] con parámetros α = 0.5 y β = 0.6. En nuestros experimentos, estimamos el límite superior y el límite de grado descendente utilizando factores de descuento lineales de 0.5, 0.8 y 1.0. Una vez que se construye la topología, las consultas seleccionadas al azar del conjunto de consultas 301-350 en TREC-VLC-921 y del conjunto de consultas 1-50 en TREC-123-100-Random y TREC-123-100-Source se inyectan en el sistema basado en una distribución de Poisson P(N(t) = n) = (λt)n n! e−λ 236 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 0 50 100 150 200 250 300 350 400 0 500 1000 1500 2000 2500 3000 Utilidad acumulada Número de consulta Utilidad acumulada sobre el número de consultas entrantes TSLA-921 SSNA-921 SSLA-921 TSNA-921 Figura 5: La utilidad acumulada versus el número de sesiones de búsqueda TREC-VLC-921 Además, asumimos que todos los agentes tienen la misma probabilidad de recibir consultas del entorno, es decir, λ es el mismo para cada agente. En nuestros experimentos, λ se establece en 0.0543 para que la media de las consultas entrantes del entorno a la red de agentes sea de 50 por unidad de tiempo. El tiempo de servicio para la cola de comunicación y la cola de búsqueda local, es decir, tQij y trs, se establece en 0.01 unidades de tiempo y 0.05 unidades de tiempo respectivamente. En nuestros experimentos, hay diez tipos de consultas adquiridas mediante la agrupación del conjunto de consultas 301-350 y 1-50. 4.3 Análisis y evaluación de resultados. La Figura 3 muestra el ARSS (Recompensa Promedio por Sesión de Búsqueda) versus el número de consultas entrantes a lo largo del tiempo para el Algoritmo No Aprendido de un Solo Paso (SSNA) y el Algoritmo de Aprendizaje de un Solo Paso (SSLA) para la recolección de datos TREC-VLC-921. Se muestra que la recompensa promedio para el algoritmo SSNA oscila entre 0.02 y 0.06 y su rendimiento cambia poco con el tiempo. El promedio de recompensa para el enfoque SSLA comienza en el mismo nivel que el algoritmo SSNA. Pero el rendimiento aumenta con el tiempo y la ganancia promedio de rendimiento se estabiliza en aproximadamente un 25% después del rango de consulta de 2000 a 3000. La Figura 4 muestra el ARSS (Recompensa Promedio por Sesión de Búsqueda) versus el número de consultas entrantes a lo largo del tiempo para el Algoritmo No Aprendido de Dos Pasos (TSNA) y el Algoritmo de Aprendizaje de Dos Pasos (TSLA) para la recolección de datos TREC-VLC-921. El enfoque TSNA tiene un rendimiento relativamente consistente con una recompensa promedio que oscila entre 0.05 y 0.15. El promedio de recompensa para el enfoque de TSLA, donde se explota el algoritmo de aprendizaje, comienza en el mismo nivel que el algoritmo TSNA y mejora la recompensa promedio con el tiempo hasta que se unen al sistema entre 2000 y 2500 consultas. Los resultados muestran que la ganancia promedio de rendimiento para el enfoque TSLA sobre el enfoque TNLA es del 35% después de la estabilización. La Figura 5 muestra la utilidad acumulada versus el número de consultas entrantes a lo largo del tiempo para SSNA, SSLA, TSNA y TSLA respectivamente. Se ilustra que la utilidad acumulativa de los algoritmos no basados en aprendizaje aumenta principalmente de forma lineal con el tiempo, mientras que las ganancias de los algoritmos basados en aprendizaje se aceleran cuando más consultas ingresan al sistema. Estos resultados experimentales demuestran que los enfoques basados en el aprendizaje tienen un rendimiento consistentemente mejor que los algoritmos de enrutamiento no basados en el aprendizaje. Además, el algoritmo basado en aprendizaje de dos fases es mejor que el algoritmo basado en aprendizaje de una sola fase porque la recompensa máxima que un agente puede recibir al buscar en su vecindario dentro de TTL saltos está relacionada con el número total de documentos relevantes en esa área. Por lo tanto, incluso la política de enrutamiento óptima puede hacer poco más allá de alcanzar estos documentos relevantes más rápido. Por el contrario, el algoritmo de aprendizaje basado en dos pasos puede reubicar la sesión de búsqueda en un vecindario con documentos más relevantes. El TSLA combina los méritos de ambos enfoques y los supera. La Tabla 1 enumera la utilidad acumulativa para los conjuntos de datos TREC123-100-Random y TREC-123-100-Source con organizaciones jerárquicas. Las cinco columnas muestran los resultados de cuatro enfoques diferentes. En particular, la columna TSNA-Random muestra los resultados para el conjunto de datos TREC-123-100-Random con el enfoque TSNA. La columna TSLA-Random muestra los resultados para el conjunto de datos TREC-123-100-Random con el enfoque TSLA. Hay dos números en cada celda de la columna TSLA-Random. El primer número es la utilidad acumulada real, mientras que el segundo número es el porcentaje de ganancia en términos de utilidad sobre el enfoque TSNA. Las columnas TSNA-Source y TSLA-Source muestran los resultados para el conjunto de datos TREC-123-100-Source con los enfoques TSNA y TSLA respectivamente. La Tabla 1 muestra que la mejora en el rendimiento para TREC-123-100-Random no es tan significativa como en los otros conjuntos de datos. Esto se debe a que los documentos en la subcolección de TREC-123-100-Random son seleccionados al azar, lo que hace que el modelo de colección, la firma de la colección, sea menos significativo. Dado que ambos algoritmos están diseñados basados en la suposición de que las colecciones de documentos pueden ser bien representadas por su modelo de colección, este resultado no es sorprendente. En general, las Figuras 4, 5 y la Tabla 1 demuestran que el enfoque basado en aprendizaje por refuerzo puede mejorar considerablemente el rendimiento del sistema para ambas colecciones de datos. Sin embargo, queda como trabajo futuro descubrir la correlación entre la magnitud de las mejoras en el rendimiento y el tamaño de la colección de datos y/o la extensión de la heterogeneidad entre las subcolecciones. TRABAJO RELACIONADO El problema de enrutamiento de contenido difiere del enrutamiento a nivel de red en redes de comunicación conmutadas por paquetes en que el enrutamiento basado en contenido ocurre en redes a nivel de aplicación. Además, los agentes de destino en nuestros algoritmos de enrutamiento de contenido son múltiples y las direcciones no son conocidas en el proceso de enrutamiento. Los problemas de enrutamiento a nivel de IP han sido abordados desde la perspectiva del aprendizaje por refuerzo[2, 5, 11, 12]. Estos estudios han explorado algoritmos distribuidos completamente que son capaces, sin coordinación central, de difundir conocimiento sobre la red, encontrar de manera robusta y eficiente los caminos más cortos frente a cambios en las topologías de red y costos de enlace cambiantes. Hay dos clases principales de algoritmos de enrutamiento de paquetes adaptativos y distribuidos en la literatura: algoritmos de vector de distancia y algoritmos de estado de enlace. Si bien esta línea de estudios tiene cierta similitud con nuestro trabajo, se ha centrado principalmente en redes de comunicación conmutadas por paquetes. En este dominio, el destino de un paquete es determinístico y único. Cada agente mantiene estimaciones, probabilísticamente o determinísticamente, sobre la distancia a un destino específico a través de sus vecinos. Una variante de las técnicas de Q-Learning se implementa en The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Utilidad Acumulativa para los Conjuntos de Datos TREC-123-100-Random y TREC-123-100-Source con Organización Jerárquica; Los números porcentuales en las columnas TSLA-Random y TSLA-Source demuestran la ganancia de rendimiento sobre el algoritmo sin aprendizaje Número de consulta TSNA-Random TSLA-Random TSNA-Source TSLA-Source 500 25.15 28.45 13% 24.00 21.05 -13% 1000 104.99 126.74 20% 93.95 96.44 2.6% 1250 149.79 168.40 12% 122.64 134.05 9.3% 1500 188.94 211.05 12% 155.30 189.60 22% 1750 235.49 261.60 11% 189.14 243.90 28% 2000 275.09 319.25 16% 219.0 278.80 26% para actualizar las estimaciones y converger a las distancias reales. Se ha descubierto que la propiedad de localidad es una característica importante de los sistemas de recuperación de información en estudios de modelado de usuarios[3]. En los sistemas de intercambio de contenido basados en P2P, esta propiedad se ejemplifica por el fenómeno de que los usuarios tienden a enviar consultas que representan solo un número limitado de temas y, por el contrario, los usuarios en el mismo vecindario probablemente compartan intereses comunes y envíen consultas similares [10]. El enfoque basado en el aprendizaje se percibe como más beneficioso para los sistemas reales de recuperación de información distribuida que exhiben la propiedad de localidad. Esto se debe a que los patrones de tráfico y consultas de los usuarios pueden reducir el espacio de estados y acelerar el proceso de aprendizaje. El trabajo relacionado en aprovechar esta propiedad incluye [7], donde los autores intentaron abordar este problema mediante técnicas de modelado de usuario. 6. CONCLUSIONES En este artículo, se desarrolla un enfoque basado en aprendizaje por refuerzo para mejorar el rendimiento de algoritmos de búsqueda de IR distribuidos. En particular, los agentes mantienen estimaciones, es decir, utilidad esperada, sobre la capacidad de los agentes aguas abajo para proporcionar documentos relevantes para las consultas entrantes. Estas estimaciones se actualizan gradualmente al aprender de la información de retroalimentación devuelta de sesiones de búsqueda anteriores. Basándose en la información actualizada de utilidad esperada, los agentes modifican sus políticas de enrutamiento. Posteriormente, estos agentes dirigen las consultas basándose en las políticas aprendidas y actualizan las estimaciones sobre la utilidad esperada en función de las nuevas políticas de enrutamiento. Los experimentos en dos conjuntos de datos IR distribuidos diferentes ilustran que el enfoque de aprendizaje por refuerzo mejora considerablemente la utilidad acumulativa con el tiempo. 7. REFERENCIAS [1] S. Abdallah y V. Lesser. Aprendiendo el juego de asignación de tareas. En AAMAS 06: Actas de la quinta conferencia internacional conjunta sobre agentes autónomos y sistemas multiagente, páginas 850-857, Nueva York, NY, EE. UU., 2006. ACM Press. [2] J. \n\nACM Press. [2] J. A. Boyan y M. L. Littman. Enrutamiento de paquetes en redes de cambio dinámico: Un enfoque de aprendizaje por refuerzo. En Avances en Sistemas de Procesamiento de Información Neural, volumen 6, páginas 671-678. Morgan Kaufmann Publishers, Inc., 1994. [3] J. C. French, A. L. Powell, J. P. Callan, C. L. Viles, T. Emmitt, K. J. Prey, y Y. Mou. Comparando el rendimiento de los algoritmos de selección de bases de datos. En Investigación y Desarrollo en Recuperación de Información, páginas 238-245, 1999. [4] B. Horling, R. Mailler y V. Lesser. Granja: Un entorno escalable para el desarrollo y evaluación de múltiples agentes. En Avances en Ingeniería de Software para Sistemas Multiagente, páginas 220-237, Berlín, 2004. Springer-Verlag. [5] M. Littman y J. Boyan. Un esquema distribuido de aprendizaje por refuerzo para enrutamiento de redes. En Actas del Taller Internacional sobre Aplicaciones de Redes Neuronales a las Telecomunicaciones, 1993. [6] J. Lu y J. Callan. Búsqueda federada de bibliotecas digitales basadas en texto en redes jerárquicas de pares. En ECIR05, 2005. [7] J. Lu y J. Callan. Modelado de usuario para búsqueda federada de texto completo en redes peer-to-peer. En ACM SIGIR 2006. ACM Press, 2006. [8] C. D. Manning y H. Schütze. Fundamentos del Procesamiento del Lenguaje Natural Estadístico. El MIT Press, Cambridge, Massachusetts, 1999. [9] C. R. Palmer y J. G. Steffan. Generando topologías de red que obedezcan leyes de potencia. En Actas de GLOBECOM 2000, noviembre de 2000. [10] K. Sripanidkulchai, B. Maggs y H. Zhang. Localización eficiente de contenido utilizando la localidad basada en intereses en sistemas peer-to-peer. En INFOCOM, 2003. [11] D. Subramanian, P. Druschel y J. Chen. Hormigas y aprendizaje por refuerzo: Un estudio de caso en enrutamiento en redes dinámicas. En Actas de la Decimoquinta Conferencia Internacional Conjunta sobre Inteligencia Artificial, páginas 832-839, 1997. [12] J. N. Tao y L. Weaver. Un enfoque de múltiples agentes y gradiente de políticas para el enrutamiento de redes. En Actas de la Decimoctava Conferencia Internacional sobre Aprendizaje Automático, 2001. [13] H. Zhang, W. B. Croft, B. Levine y V. Lesser. Un enfoque multiagente para la <br>recuperación de información entre pares</br>. En Actas de la Tercera Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, julio de 2004. [14] H. Zhang y V. Lesser. Sistemas de <br>recuperación de información peer-to-peer</br> basados en múltiples agentes con sesiones de búsqueda concurrentes. En Actas de la Quinta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, mayo de 2006. [15] H. Zhang y V. R. Lesser. Una organización de agentes jerárquica formada dinámicamente para un sistema distribuido de intercambio de contenido. En la Conferencia Internacional IEEE/WIC/ACM sobre Tecnología de Agentes Inteligentes (IAT 2004) del 20 al 24 de septiembre de 2004 en Beijing, China, páginas 169-175. IEEE Computer Society, 2004. 238 La Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ",
            "candidates": [],
            "error": [
                [
                    "recuperación de información entre pares",
                    "recuperación de información entre pares",
                    "recuperación de información peer-to-peer"
                ]
            ]
        },
        "multi-agent learn": {
            "translated_key": "aprendizaje multiagente",
            "is_in_text": false,
            "original_annotated_sentences": [
                "A Reinforcement Learning based Distributed Search Algorithm For Hierarchical Peer-to-Peer Information Retrieval Systems Haizheng Zhang College of Information Science and Technology Pennsylvania State University University Park, PA 16803 hzhang@ist.psu.edu Victor Lesser Department of Computer Science University Of Massachusetts Amherst, MA 01003 lesser@cs.umass.edu ABSTRACT The dominant existing routing strategies employed in peerto-peer(P2P) based information retrieval(IR) systems are similarity-based approaches.",
                "In these approaches, agents depend on the content similarity between incoming queries and their direct neighboring agents to direct the distributed search sessions.",
                "However, such a heuristic is myopic in that the neighboring agents may not be connected to more relevant agents.",
                "In this paper, an online reinforcement-learning based approach is developed to take advantage of the dynamic run-time characteristics of P2P IR systems as represented by information about past search sessions.",
                "Specifically, agents maintain estimates on the downstream agents abilities to provide relevant documents for incoming queries.",
                "These estimates are updated gradually by learning from the feedback information returned from previous search sessions.",
                "Based on this information, the agents derive corresponding routing policies.",
                "Thereafter, these agents route the queries based on the learned policies and update the estimates based on the new routing policies.",
                "Experimental results demonstrate that the learning algorithm improves considerably the routing performance on two test collection sets that have been used in a variety of distributed IR studies.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Algorithms, Performance, Experimentation 1.",
                "INTRODUCTION Over the last few years there have been increasing interests in studying how to control the search processes in peer-to-peer(P2P) based information retrieval(IR) systems [6, 13, 14, 15].",
                "In this line of research, one of the core problems that concerns researchers is to efficiently route user queries in the network to agents that are in possession of appropriate documents.",
                "In the absence of global information, the dominant strategies in addressing this problem are content-similarity based approaches [6, 13, 14, 15].",
                "While the content similarity between queries and local nodes appears to be a creditable indicator for the number of relevant documents residing on each node, these approaches are limited by a number of factors.",
                "First of all, similaritybased metrics can be myopic since locally relevant nodes may not be connected to other relevant nodes.",
                "Second, the similarity-based approaches do not take into account the run-time characteristics of the P2P IR systems, including environmental parameters, bandwidth usage, and the historical information of the past search sessions, that provide valuable information for the query routing algorithms.",
                "In this paper, we develop a reinforcement learning based IR approach for improving the performance of distributed IR search algorithms.",
                "Agents can acquire better search strategies by collecting and analyzing feedback information from previous search sessions.",
                "Particularly, agents maintain estimates, namely expected utility, on the downstream agents capabilities of providing relevant documents for specific types of incoming queries.",
                "These estimates are updated gradually by learning from the feedback information returned from previous search sessions.",
                "Based on the updated expected utility information, the agents derive corresponding routing policies.",
                "Thereafter, these agents route the queries based on the learned policies and update the estimates on the expected utility based on the new routing policies.",
                "This process is conducted in an iterative manner.",
                "The goal of the learning algorithm, even though it consumes some network bandwidth, is to shorten the routing time so that more queries are processed per time unit while at the same time finding more relevant documents.",
                "This contrasts with the content-similarity based approaches where similar operations are repeated for every incoming query and the processing time keeps largely constant over time.",
                "Another way of viewing this paper is that our basic approach to distributed IR search is to construct a hierarchical overlay network(agent organization) based on the contentsimilarity measure among agents document collections in a bottom-up fashion.",
                "In the past work, we have shown that this organization improves search performance significantly.",
                "However, this organizational structure does not take into account the arrival patterns of queries, including their frequency, types, and where they enter the system, nor the available communication bandwidth of the network and processing capabilities of individual agents.",
                "The intention of the reinforcement learning is to adapt the agents routing decisions to the dynamic network situations and learn from past search sessions.",
                "Specifically, the contributions of this paper include: (1) a reinforcement learning based approach for agents to acquire satisfactory routing policies based on estimates of the potential contribution of their neighboring agents; (2) two strategies to speed up the learning process.",
                "To our best knowledge, this is one of the first reinforcement learning applications in addressing distributed content sharing problems and it is indicative of some of the issues in applying reinforcement in a complex application.",
                "The remainder of this paper is organized as follows: Section 2 reviews the hierarchical content sharing systems and the two-phase search algorithm based on such topology.",
                "Section 3 describes a reinforcement learning based approach to direct the routing process; Section 4 details the experimental settings and analyze the results.",
                "Section 5 discusses related studies and Section 6 concludes the paper. 2.",
                "SEARCH IN HIERARCHICAL P2P IR SYSTEMS This section briefly reviews our basic approaches to hierarchical P2P IR systems.",
                "In a hierarchical P2P IR system illustrated in Fig.1, agents are connected to each other through three types of links: upward links, downward links, and lateral links.",
                "In the following sections, we denote the set of agents that are directly connected to agent Ai as DirectConn(Ai), which is defined as DirectConn(Ai) = NEI(Ai) ∪ PAR(Ai) ∪ CHL(Ai) , where NEI(Ai) is the set of neighboring agents connected to Ai through lateral links; PAR(Ai) is the set of agents whom agent Ai is connected to through upward links and CHL(Ai) is the set of agents that agent Ai connects to through downward links.",
                "These links are established through a bottom-up content-similarity based distributed clustering process[15].",
                "These links are then used by agents to locate other agents that contain documents relevant to the given queries.",
                "A typical agent Ai in our system uses two queues: a local search queue, LSi, and a message forwarding queue MFi.",
                "The states of the two queues constitute the internal states of an agent.",
                "The local search queue LSi stores search sessions that are scheduled for local processing.",
                "It is a priority queue and agent Ai always selects the most promising queries to process in order to maximize the global utility.",
                "MFi consists of a set of queries to forward on and is processed in a FIFO (first in first out) fashion.",
                "For the first query in MFi, agent Ai determines which subset of its neighboring agents to forward it to based on the agents routing policy πi.",
                "These routing decisions determine how the search process is conducted in the network.",
                "In this paper, we call Ai as Ajs upstream agent and Aj as Ais downstream agent if A4 A5 A6 A7 A2 A3 A9 NEI(A2)={A3} PAR(A2)={A1} CHL(A2)={A4,A5} A1 A8 Figure 1: A fraction of a hierarchical P2PIR system an agent Ai routes a query to agent Aj.",
                "The distributed search protocol of our hierarchical agent organization is composed of two steps.",
                "In the first step, upon receipt of a query qk at time tl from a user, agent Ai initiates a search session si by probing its neighboring agents Aj ∈ NEI(Ai) with the message PROBE for the similarity value Sim(qk, Aj) between qk and Aj.",
                "Here, Ai is defined as the query initiator of search session si.",
                "In the second step, Ai selects a group of the most promising agents to start the actual search process with the message SEARCH.",
                "These SEARCH messages contain a TTL (Time To Live) parameter in addition to the query.",
                "The TTL value decreases by 1 after each hop.",
                "In the search process, agents discard those queries that either have been previously processed or whose TTL drops to 0, which prevents queries from looping in the system forever.",
                "The search session ends when all the agents that receive the query drop it or TTL decreases to 0.",
                "Upon receipt of SEARCH messages for qk, agents schedule local activities including local searching, forwarding qk to their neighbors, and returning search results to the query initiator.",
                "This process and related algorithms are detailed in [15, 14]. 3.",
                "A BASIC REINFORCEMENTLEARNING BASED SEARCH APPROACH In the aforementioned distributed search algorithm, the routing decisions of an agent Ai rely on the similarity comparison between incoming queries and Ais neighboring agents in order to forward those queries to relevant agents without flooding the network with unnecessary query messages.",
                "However, this heuristic is myopic because a relevant direct neighbor is not necessarily connected to other relevant agents.",
                "In this section, we propose a more general approach by framing this problem as a reinforcement learning task.",
                "In pursuit of greater flexibility, agents can switch between two modes: learning mode and non-learning mode.",
                "In the non-learning mode, agents operate in the same way as they do in the normal distributed search processes described in [14, 15].",
                "On the other hand, in the learning mode, in parallel with distributed search sessions, agents also participate in a learning process which will be detailed in this section.",
                "Note that in the learning protocol, the learning process does not interfere with the distributed search process.",
                "Agents can choose to initiate and stop learning processes without affecting the system performance.",
                "In particular, since the learning process consumes network resources (especially bandwidth), agents can choose to initiate learning only when the network load is relatively low, thus minimizing the extra communication costs incurred by the learning algorithm.",
                "The section is structured as follows, Section 3.1 describes 232 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) a reinforcement learning based model.",
                "Section 3.2 describes a protocol to deploy the learning algorithm in the network.",
                "Section 3.3 discusses the convergence of the learning algorithm. 3.1 The Model An agents routing policy takes the state of a search session as input and output the routing actions for that query.",
                "In our work, the state of a search session sj is stipulated as: QSj = (qk, ttlj) where ttlj is the number of hops that remains for the search session sj , qk is the specific query.",
                "QL is an attribute of qk that indicates which type of queries qk most likely belong to.",
                "The set of QL can be generated by running a simple online classification algorithm on all the queries that have been processed by the agents, or an oﬄine algorithm on a pre-designated training set.",
                "The assumption here is that the set of query types is learned ahead of time and belongs to the common knowledge of the agents in the network.",
                "Future work includes exploring how learning can be accomplished when this assumption does not hold.",
                "Given the query types set, an incoming query qi can be classified to one query class Q(qi) by the formula: Q(qi) = arg max Qj P(qi|Qj) (1) where P(qi|Qj ) indicates the likelihood that the query qi is generated by the query class Qj [8].",
                "The set of atomic routing actions of an agent Ai is denoted as {αi}, where {αi} is defined as αi = {αi0 , αi1 , ..., αin }.",
                "An element αij represents an action to route a given query to the neighboring agent Aij ∈ DirectConn(Ai).",
                "The routing policy πi of agent Ai is stochastic and its outcome for a search session with state QSj is defined as: πi(QSj) = {(αi0 , πi(QSi, αi0 )), (αi1 , πi(QSi, αi1 )), ...} (2) Note that operator πi is overloaded to represent either the probabilistic policy for a search session with state QSj, denoted as πi(QSj); or the probability of forwarding the query to a specific neighboring agent Aik ∈ DirectConn(Ai) under the policy πi(QSj), denoted as πi(QSj, αik ).",
                "Therefore, equation (2) means that the probability of forwarding the search session to agent Ai0 is πi(QSi, αi0 ) and so on.",
                "Under this stochastic policy, the routing action is nondeterministic.",
                "The advantage of such a strategy is that the best neighboring agents will not be selected repeatedly, thereby mitigating the potential hot spots situations.",
                "The expected utility, Un i (QSj), is used to estimate the potential utility gain of routing query type QSj to agent Ai under policy πn i .",
                "The superscript n indicates the value at the nth iteration in an iterative learning process.",
                "The expected utility provides routing guidance for future search sessions.",
                "In the search process, each agent Ai maintains partial observations of its neighbors states, as shown in Fig. 2.",
                "The partial observation includes non-local information such as the potential utility estimation of its neighbor Am for query state QSj, denoted as Um(QSj), as well as the load information, Lm.",
                "These observations are updated periodically by the neighbors.",
                "The estimated utility information will be used to update Ais expected utility for its routing policy.",
                "Load Information Expected Utility For Different Query Types Neighboring Agents ... A0 A1 A3 A2 Un 0 (QS0) ... ... ... ... ...... Un 0 (QS1) Un 1 (QS1) Un 2 (QS1) Un 3 (QS1) Un 1 (QS0) Un 2 (QS0) Un 3 (QS0) Ln 0 Ln 1 Ln 2 Ln 3 ... QS0 QS1 ...",
                "Figure 2: Agent Ais Partial Observation about its neighbors(A0, A1...) The load information of Am, Lm, is defined as Lm = |MFm| Cm , where |MFm| is the length of the message-forward queue and Cm is the service rate of agent Ams message-forward queue.",
                "Therefore Lm characterizes the utilization of an agents communication channel, and thus provide non-local information for Ams neighbors to adjust the parameters of their routing policy to avoid inundating their downstream agents.",
                "Note that based on the characteristics of the queries entering the system and agents capabilities, the loading of agents may not be uniform.",
                "After collecting the utilization rate information from all its neighbors, agent Ai computes Li as a single measure for assessing the average load condition of its neighborhood: Li = P k Lk |DirectConn(Ai)| Agents exploit Li value in determining the routing probability in its routing policy.",
                "Note that, as described in Section 3.2, information about neighboring agents is piggybacked with the query message propagated among the agents whenever possible to reduce the traffic overhead. 3.1.1 Update the Policy An iterative update process is introduced for agents to learn a satisfactory stochastic routing policy.",
                "In this iterative process, agents update their estimates on the potential utility of their current routing policies and then propagate the updated estimates to their neighbors.",
                "Their neighbors then generate a new routing policy based on the updated observation and in turn they calculate the expected utility based on the new policies and continue this iterative process.",
                "In particular, at time n, given a set of expected utility, an agent Ai, whose directly connected agents set is DirectConn(Ai) = {Ai0 , ..., Aim }, determines its corresponding stochastic routing policy for a search session of state QSj based on the following steps: (1) Ai first selects a subset of agents as the potential downstream agents from set DirectConn(Ai), denoted as PDn(Ai, QSj).",
                "The size of the potential downstream agent is specified as |PDn(Ai, QSj)| = min(|NEI(Ai), dn i + k)| where k is a constant and is set to 3 in this paper; dn i , the forward width, is defined as the expected number of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 233 neighboring agents that agent Ai can forward to at time n. This formula specifies that the potential downstream agent set PDn(Ai, QSj) is either the subset of neighboring agents with dn i + k highest expected utility value for state QSj among all the agents in DirectConn(Ai), or all their neighboring agents.",
                "The k is introduced based on the idea of a stochastic routing policy and it makes the forwarding probability of the dn i +k highest agent less than 100%.",
                "Note that if we want to limit the number of downstream agents for search session sj as 5, the probability of forwarding the query to all neighboring agents should add up to 5.",
                "Setting up dn i value properly can improve the utilization rate of the network bandwidth when much of the network is idle while mitigating the traffic load when the network is highly loaded.",
                "The dn+1 i value is updated based on dn i , the previous and current observations on the traffic situation in the neighborhood.",
                "Specifically, the update formula for dn+1 i is dn+1 i = dn i ∗ (1 + 1 − Li |DirectConn(Ai)| ) In this formula, the forward width is updated based on the traffic conditions of agent Ais neighborhood, i.e Li, and its previous value. (2) For each agent Aik in the PDn(Ai, QSj), the probability of forwarding the query to Aik is determined in the following way in order to assign higher forwarding probability to the neighboring agents with higher expected utility value: πn+1 i (QSj, αik ) = dn+1 i |PDn(Ai, QSj)| + β ∗ ` Uik (QSj) − PDU(Ai, QSj) |PDn(Ai, QSj)| ´ (3) where PDUn(Ai, QSj) = X o∈P Dn(Ai,QSj ) Uo(QSj) and QSj is the subsequent state of agent Aik after agent Ai forwards the search session with state QSj to its neighboring agent Aik ; If QSj = (qk, ttl0), then QSj = (qk, ttl0 − 1).",
                "In formula 3, the first term on the right of the equation, dn+1 i |P Dn(Ai,QSj )| , is used to to determine the forwarding probability by equally distributing the forward width, dn+1 i , to the agents in PDn(Ai, QSj) set.",
                "The second term is used to adjust the probability of being chosen so that agents with higher expected utility values will be favored. β is determined according to: β = min ` m − dn+1 i m ∗ umax − PDUn(Ai, QSj) , dn+1 i PDUn(Ai, QSj) − m ∗ umin ´ (4) where m = |PDn(Ai, QSj)|, umax = max o∈P Dn(Ao,QSj ) Uo(QSj) and umin = min o∈P Dn(Ao,QSj ) Uo(QSj) This formula guarantees that the final πn+1 i (QSj, αik ) value is well defined, i.e, 0 ≤ πn+1 i (QSj, αik ) ≤ 1 and X i πn+1 i (QSj, αik ) = dn+1 i However, such a solution does not explore all the possibilities.",
                "In order to balance between exploitation and exploration, a λ-Greedy approach is taken.",
                "In the λ-Greedy approach, in addition to assigning higher probability to those agents with higher expected utility value, as in the equation (3).",
                "Agents that appear to be not-so-good choices will also be sent queries based on a dynamic exploration rate.",
                "In particular, for agents in the set PDn(Ai, QSj), πn+1 i1 (QSj) is determined in the same way as the above, with the only difference being that dn+1 i is replaced with dn+1 i ∗ (1 − λn).",
                "The remaining search bandwidth is used for learning by assigning probability λn evenly to agents Ai2 in the set DirectConn(Ai) − PDn(Ai, QSj). πn+1 i2 (QSj, αik ) = dn+1 i ∗ λn |DirectConn(Ai) − PDn(Ai, QSj)| (5) where PDn(Ai, QSj) ⊂ DirectConn(Ai).",
                "Note that the exploration rate λ is not a constant and it decreases overtime.",
                "The λ is determined according to the following equation: λn+1 = λ0 ∗ e−c1n (6) where λ0 is the initial exploration rate, which is a constant; c1 is also a constant to adjust the decreasing rate of the exploration rate; n is the current time unit. 3.1.2 Update Expected Utility Once the routing policy at step n+1, πn+1 i , is determined based on the above formula, agent Ai can update its own expected utility, Un+1 i (QSi), based on the the updated routing policy resulted from the formula 5 and the updated U values of its neighboring agents.",
                "Under the assumption that after a query is forwarded to Ais neighbors the subsequent search sessions are independent, the update formula is similar to the Bellman update formula in Q-Learning: Un+1 i (QSj) = (1 − θi) ∗ Un i (QSj) + θi ∗ (Rn+1 i (QSj) + X k πn+1 i (QSj, αik )Un k (QSj)) (7) where QSj = (Qj, ttl − 1) is the next state of QSj = (Qj, ttl); Rn+1 i (QSj) is the expected local reward for query class Qk at agent Ai under the routing policy πn+1 i ; θi is the coefficient for deciding how much weight is given to the old value during the update process: the smaller θi value is, the faster the agent is expected to learn the real value, while the greater volatility of the algorithm, and vice versa.",
                "Rn+1 (s) is updated according to the following equation: 234 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Rn+1 i (QSj) = Rn i (QSj) +γi ∗ (r(QSj) − Rn i (QSj)) ∗ P(qj|Qj ) (8) where r(QSj) is the local reward associated with the search session.",
                "P(qj|Qj ) indicates how relevant the query qj is to the query type Qj, and γi is the learning rate for agent Ai.",
                "Depending on the similarity between a specific query qi and its corresponding query type Qi, the local reward associated with the search session has different impact on the Rn i (QSj) estimation.",
                "In the above formula, this impact is reflected by the coefficient, the P(qj|Qj) value. 3.1.3 Reward function After a search session stops when its TTL values expires, all search results are returned back to the user and are compared against the relevance judgment.",
                "Assuming the set of search results is SR, the reward Rew(SR) is defined as: Rew(SR) = j 1 if |Rel(SR)| > c |Rel(SR)| c otherwise. where SR is the set of returned search results, Rel(SR) is the set of relevant documents in the search results.",
                "This equation specifies that users give 1.0 reward if the number of returned relevant documents reaches a predefined number c. Otherwise, the reward is in proportion to the number of relevant documents returned.",
                "This rationale for setting up such a cut-off value is that the importance of recall ratio decreases with the abundance of relevant documents in real world, therefore users tend to focus on only a limited number of searched results.",
                "The details of the actual routing protocol will be introduced in Section 3.2 when we introduce how the learning algorithm is deployed in real systems. 3.2 Deployment of the Learning algorithm This section describes how the learning algorithm can be used in either a single-phase or a two-phase search process.",
                "In the single-phase search algorithm, search sessions start from the initiators of the queries.",
                "In contrast, in the two-step search algorithm, the query initiator first attempts to seek a more appropriate starting point for the query by introducing an exploratory step as described in Section 2.",
                "Despite the difference in the quality of starting points, the major part of the learning process for the two algorithms is largely the same as described in the following paragraphs.",
                "Before learning starts, each agent initializes the expected utility value for all possible states as 0.",
                "Thereafter, upon receipt of a query, in addition to the normal operations described in the previous section, an agent Ai also sets up a timer to wait for the search results returned from its downstream agents.",
                "Once the timer expires or it has received response from all its downstream agents, Ai merges and forwards the search results accrued from its downstream agents to its upstream agent.",
                "Setting up the timer speeds up the learning because agents can avoid waiting too long for the downstream agents to return search results.",
                "Note that these detailed results and corresponding agent information will still be stored at Ai until the feedback information is passed from its upstream agent and the performance of its downstream agents can be evaluated.",
                "The duration of the timer is related to the TTL value.",
                "In this paper, we set the timer to ttimer = ttli ∗ 2 + tf , where ttli ∗ 2 is the sum of the travel time of the queries in the network, and tf is the expected time period that users would like to wait.",
                "The search results will eventually be returned to the search session initiator A0.",
                "They will be compared to the relevance judgment that is provided by the final users (as described in the experiment section, the relevance judgement for the query set is provided along with the data collections).",
                "The reward will be calculated and propagated backward to the agents along the way that search results were passed.",
                "This is a reverse process of the search results propagation.",
                "In the process of propagating reward backward, agents update estimates of their own potential utility value, generate an upto-dated policy and pass their updated results to the neighboring agents based on the algorithm described in Section 3.",
                "Upon change of expected utility value, agent Ai sends out its updated utility estimation to its neighbors so that they can act upon the changed expected utility and corresponding state.",
                "This update message includes the potential reward as well as the corresponding state QSi = (qk, ttll) of agent Ai.",
                "Each neighboring agent, Aj, reacts to this kind of update message by updating the expected utility value for state QSj(qk, ttll + 1) according to the newly-announced changed expected utility value.",
                "Once they complete the update, the agents would again in turn inform related neighbors to update their values.",
                "This process goes on until the TTL value in the update message increases to the TTL limit.",
                "To speed up the learning process, while updating the expected utility values of an agent Ais neighboring agents we specify that Um(Qk, ttl0) >= Um(Qk, ttl1) iff ttl0 > ttl1 Thus, when agent Ai receives an updated expected utility value with ttl1, it also updates the expected utility values with any ttl0 > ttl1 if Um(Qk, ttl0) < Um(Qk, ttl1) to speed up convergence.",
                "This heuristic is based on the fact that the utility of a search session is a non-decreasing function of time t. 3.3 Discussion In formalizing the content routing system as a learning task, many assumptions are made.",
                "In real systems, these assumptions may not hold, and thus the learning algorithm may not converge.",
                "Two problems are of particular note, (1) This content routing problem does not have Markov properties.",
                "In contrast to IP-level based packet routing, the routing decision of each agent for a particular search session sj depends on the routing history of sj.",
                "Therefore, the assumption that all subsequent search sessions are independent does not hold in reality.",
                "This may lead to double counting problem that the relevant documents of some agents will be counted more than once for the state where the TTL value is more than 1.",
                "However, in the context of the hierarchical agent organizations, two factors mitigate this problems: first, the agents in each content group form a tree-like structure.",
                "With the absense of the cycles, the estimates inside the tree would be close to the accurate value.",
                "Secondly, the stochastic nature of the routing policy partly remedies this problem.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 235 (2) Another challenge for this learning algorithm is that in a real network environment observations on neighboring agents may not be able to be updated in time due to the communication delay or other situations.",
                "In addition, when neighboring agents update their estimates at the same time, oscillation may arise during the learning process[1].",
                "This paper explores several approaches to speed up the learning process.",
                "Besides the aforementioned strategy of updating the expected utility values, we also employ an active update strategy where agents notify their neighbors whenever its expected utility is updated.",
                "Thus a faster convergence speed can be achieved.",
                "This strategy contrasts to the Lazy update, where agents only echo their neighboring agents with their expected utility change when they exchange information.",
                "The trade off between the two approaches is the network load versus learning speed.",
                "The advantage of this learning algorithm is that once a routing policy is learned, agents do not have to repeatedly compare the similarity of queries as long as the network topology remains unchanged.",
                "Instead, agent just have to determine the classification of the query properly and follow the learned policies.",
                "The disadvantage of this learning-based approach is that the learning process needs to be conducted whenever the network structure changes.",
                "There are many potential extensions for this learning model.",
                "For example, a single measure is currently used to indicate the traffic load for an agents neighborhood.",
                "A simple extension would be to keep track of individual load for each neighbor of the agent. 4.",
                "EXPERIMENTSSETTINGSAND RESULTS The experiments are conducted on TRANO simulation toolkit with two sets of datasets, TREC-VLC-921 and TREC123-100.",
                "The following sub-sections introduce the TRANO testbed, the datasets, and the experimental results. 4.1 TRANO Testbed TRANO (Task Routing on Agent Network Organization) is a multi-agent based network based information retrieval testbed.",
                "TRANO is built on top of the Farm [4], a time based distributed simulator that provides a data dissemination framework for large scale distributed agent network based organizations.",
                "TRANO supports importation and exportation of agent organization profiles including topological connections and other features.",
                "Each TRANO agent is composed of an agent view structure and a control unit.",
                "In simulation, each agent is pulsed regularly and the agent checks the incoming message queues, performs local operations and then forwards messages to other agents . 4.2 Experimental Settings In our experiment, we use two standard datasets, TRECVLC-921 and TREC-123-100 datasets, to simulate the collections hosted on agents.",
                "The TREC-VLC-921 and TREC123-100 datasets were created by the U.S. National Institute for Standard Technology(NIST) for its TREC conferences.",
                "In distributed information retrieval domain, the two data collections are split to 921 and 100 sub-collections.",
                "It is observed that dataset TREC-VLC-921 is more heterogeneous than TREC-123-100 in terms of source, document length, and relevant document distribution from the statistics of the two data collections listed in [13].",
                "Hence, TREC-VLC-921 is much closer to real document distributions in P2P environments.",
                "Furthermore, TREC-123-100 is split into two sets of 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0 500 1000 1500 2000 2500 3000 ARSS Query number ARSS versus the number of incoming queries for TREC-VLC-921 SSLA-921 SSNA-921 Figure 3: ARSS(Average reward per search session) versus the number of search sessions for 1phase search in TREC-VLC-921 0 0.1 0.2 0.3 0.4 0.5 0.6 0 500 1000 1500 2000 2500 3000 ARSS Query number ARSS versus query number for TREC-VLC-921 TSLA-921 TSNA-921 Figure 4: ARSS(Average reward per search session) versus the number of search sessions for 2phase search in TREC-VLC-921 sub-collections in two ways: randomly and by source.",
                "The two partitions are denoted as TREC-123-100-Random and TREC-123-100-Source respectively.",
                "The documents in each subcollection in dataset TREC-123-100-Source are more coherent than those in TREC-123-100-Random.",
                "The two different sets of partitions allow us to observe how the distributed learning algorithm is affected by the homogeneity of the collections.",
                "The hierarchical agent organization is generated by the algorithm described in our previous algorithm [15].",
                "During the topology generation process, degree information of each agent is estimated by the algorithm introduced by Palmer et al. [9] with parameters α = 0.5 and β = 0.6.",
                "In our experiments, we estimate the upward limit and downward degree limit using linear discount factors 0.5, 0.8 and 1.0.",
                "Once the topology is built, queries randomly selected from the query set 301−350 on TREC-VLC-921 and query set 1− 50 on TREC-123-100-Random and TREC-123-100-Source are injected to the system based on a Poisson distribution P(N(t) = n) = (λt)n n! e−λ 236 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 0 50 100 150 200 250 300 350 400 0 500 1000 1500 2000 2500 3000 Cumulativeutility Query number Cumulative utility over the number of incoming queries TSLA-921 SSNA-921 SSLA-921 TSNA-921 Figure 5: The cumulative utility versus the number of search sessions TREC-VLC-921 In addition, we assume that all agents have an equal chance of getting queries from the environment, i.e, λ is the same for every agent.",
                "In our experiments, λ is set as 0.0543 so that the mean of the incoming queries from the environment to the agent network is 50 per time unit.",
                "The service time for the communication queue and local search queue, i.e tQij and trs, is set as 0.01 time unit and 0.05 time units respectively.",
                "In our experiments, there are ten types of queries acquired by clustering the query set 301 − 350 and 1 − 50. 4.3 Results analysis and evaluation Figure 3 demonstrates the ARSS(Average Reward per Search Session) versus the number of incoming queries over time for the the Single-Step based Non-learning Algorithm (SSNA), and the Single-Step Learning Algorithm(SSLA) for data collection TREC-VLC-921.",
                "It shows that the average reward for SSNA algorithm ranges from 0.02 − 0.06 and the performance changes little over time.",
                "The average reward for SSLA approach starts at the same level with the SSNA algorithm.",
                "But the performance increases over time and the average performance gain stabilizes at about 25% after query range 2000 − 3000.",
                "Figure 4 shows the ARSS(Average Reward per Search Session) versus the number of incoming queries over time for the the Two-Step based Non-learning Algorithm(TSNA), and the Two-Step Learning Algorithm(TSLA) for data collection TREC-VLC-921.",
                "The TSNA approach has a relatively consistent performance with the average reward ranges from 0.05 − 0.15.",
                "The average reward for TSLA approach, where learning algorithm is exploited, starts at the same level with the TSNA algorithm and improves the average reward over time until 2000−2500 queries joining the system.",
                "The results show that the average performance gain for TSLA approach over TNLA approach is 35% after stabilization.",
                "Figure 5 shows the cumulative utility versus the number of incoming queries over time for SSNA, SSLA,TSNA, and TSLA respectively.",
                "It illustrates that the cumulative utility of non-learning algorithms increases largely linearly over time, while the gains of learning-based algorithms accelerate when more queries enter the system.",
                "These experimental results demonstrate that learning-based approaches consistently perform better than non-learning based routing algorithm.",
                "Moreover, two-phase learning based algorithm is better than single-phase based learning algorithm because the maximal reward an agent can receive from searching its neighborhood within TTL hops is related to the total number of the relevant documents in that area.",
                "Thus, even the optimal routing policy can do little beyond reaching these relevant documents faster.",
                "On the contrary, the two-stepbased learning algorithm can relocate the search session to a neighborhood with more relevant documents.",
                "The TSLA combines the merits of both approaches and outperforms them.",
                "Table 1 lists the cumulative utility for datasets TREC123-100-Random and TREC-123-100-Source with hierarchical organizations.",
                "The five columns show the results for four different approaches.",
                "In particular, column TSNA-Random shows the results for dataset TREC-123-100-Random with the TSNA approach.",
                "The column TSLA-Random shows the results for dataset TREC-123-100-Random with the TSLA approach.",
                "There are two numbers in each cell in the column TSLA-Random.",
                "The first number is the actual cumulative utility while the second number is the percentage gain in terms of the utility over TSNA approach.",
                "Columns TSNA-Source and TSLA-Source show the results for dataset TREC-123-100-Source with TSNA and TSLA approaches respectively.",
                "Table 1 shows that the performance improvement for TREC-123-100-Random is not as significant as the other datasets.",
                "This is because that the documents in the sub-collection of TREC-123-100-Random are selected randomly which makes the collection model, the signature of the collection, less meaningful.",
                "Since both algorithms are designed based on the assumption that document collections can be well represented by their collection model, this result is not surprising.",
                "Overall, Figures 4, 5, and Table 1 demonstrate that the reinforcement learning based approach can considerably enhance the system performance for both data collections.",
                "However, it remains as future work to discover the correlation between the magnitude of the performance gains and the size of the data collection and/or the extent of the heterogeneity between the sub-collections. 5.",
                "RELATED WORK The content routing problem differs from the networklevel routing in packet-switched communication networks in that content-based routing occurs in application-level networks.",
                "In addition, the destination agents in our contentrouting algorithms are multiple and the addresses are not known in the routing process.",
                "IP-level Routing problems have been attacked from the reinforcement learning perspective[2, 5, 11, 12].",
                "These studies have explored fully distributed algorithms that are able, without central coordination to disseminate knowledge about the network, to find the shortest paths robustly and efficiently in the face of changing network topologies and changing link costs.",
                "There are two major classes of adaptive, distributed packet routing algorithms in the literature: distance-vector algorithms and link-state algorithms.",
                "While this line of studies carry a certain similarity with our work, it has mainly focused on packet-switched communication networks.",
                "In this domain, the destination of a packet is deterministic and unique.",
                "Each agent maintains estimations, probabilistically or deterministically, on the distance to a certain destination through its neighbors.",
                "A variant of Q-Learning techniques is deployed The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 237 Table 1: Cumulative Utility for Datasets TREC-123-100-Random and TREC-123-100-Source with Hierarchical Organization; The percentage numbers in the columns TSLA-Random and TSLA-Source demonstrate the performance gain over the algorithm without learning Query number TSNA-Random TSLA-Random TSNA-Source TSLA-Source 500 25.15 28.45 13% 24.00 21.05 -13% 1000 104.99 126.74 20% 93.95 96.44 2.6% 1250 149.79 168.40 12% 122.64 134.05 9.3% 1500 188.94 211.05 12% 155.30 189.60 22% 1750 235.49 261.60 11% 189.14 243.90 28% 2000 275.09 319.25 16% 219.0 278.80 26% to update the estimations to converge to the real distances.",
                "It has been discovered that the locality property is an important feature of information retrieval systems in user modeling studies[3].",
                "In P2P based content sharing systems, this property is exemplified by the phenomenon that users tend to send queries that represent only a limited number of topics and conversely, users in the same neighborhood are likely to share common interests and send similar queries [10].",
                "The learning based approach is perceived to be more beneficial for real distributed information retrieval systems which exhibit locality property.",
                "This is because the users traffic and query patterns can reduce the state space and speed up the learning process.",
                "Related work in taking advantage of this property include [7], where the authors attempted to address this problem by user modeling techniques. 6.",
                "CONCLUSIONS In this paper, a reinforcement-learning based approach is developed to improve the performance of distributed IR search algorithms.",
                "Particularly, agents maintain estimates, namely expected utility, on the downstream agents ability to provide relevant documents for incoming queries.",
                "These estimates are updated gradually by learning from the feedback information returned from previous search sessions.",
                "Based on the updated expected utility information, the agents modify their routing policies.",
                "Thereafter, these agents route the queries based on the learned policies and update the estimates on the expected utility based on the new routing policies.",
                "The experiments on two different distributed IR datasets illustrates that the reinforcement learning approach improves considerably the cumulative utility over time. 7.",
                "REFERENCES [1] S. Abdallah and V. Lesser.",
                "Learning the task allocation game.",
                "In AAMAS 06: Proceedings of the fifth international joint conference on Autonomous agents and multiagent systems, pages 850-857, New York, NY, USA, 2006.",
                "ACM Press. [2] J.",
                "A. Boyan and M. L. Littman.",
                "Packet routing in dynamically changing networks: A reinforcement learning approach.",
                "In Advances in Neural Information Processing Systems, volume 6, pages 671-678.",
                "Morgan Kaufmann Publishers, Inc., 1994. [3] J. C. French, A. L. Powell, J. P. Callan, C. L. Viles, T. Emmitt, K. J. Prey, and Y. Mou.",
                "Comparing the performance of database selection algorithms.",
                "In Research and Development in Information Retrieval, pages 238-245, 1999. [4] B. Horling, R. Mailler, and V. Lesser.",
                "Farm: A scalable environment for multi-agent development and evaluation.",
                "In Advances in Software Engineering for Multi-Agent Systems, pages 220-237, Berlin, 2004.",
                "Springer-Verlag. [5] M. Littman and J. Boyan.",
                "A distributed reinforcement learning scheme for network routing.",
                "In Proceedings of the International Workshop on Applications of Neural Networks to Telecommunications, 1993. [6] J. Lu and J. Callan.",
                "Federated search of text-based digital libraries in hierarchical peer-to-peer networks.",
                "In In ECIR05, 2005. [7] J. Lu and J. Callan.",
                "User modeling for full-text federated search in peer-to-peer networks.",
                "In ACM SIGIR 2006.",
                "ACM Press, 2006. [8] C. D. Manning and H. Sch¨utze.",
                "Foundations of Statistical Natural Language Processing.",
                "The MIT Press, Cambridge, Massachusetts, 1999. [9] C. R. Palmer and J. G. Steffan.",
                "Generating network topologies that obey power laws.",
                "In Proceedings of GLOBECOM 2000, November 2000. [10] K. Sripanidkulchai, B. Maggs, and H. Zhang.",
                "Efficient content location using interest-based locality in peer-topeer systems.",
                "In INFOCOM, 2003. [11] D. Subramanian, P. Druschel, and J. Chen.",
                "Ants and reinforcement learning: A case study in routing in dynamic networks.",
                "In In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence, pages 832-839, 1997. [12] J. N. Tao and L. Weaver.",
                "A multi-agent, policy gradient approach to network routing.",
                "In In Proceedings of the Eighteenth International Conference on Machine Learning, 2001. [13] H. Zhang, W. B. Croft, B. Levine, and V. Lesser.",
                "A multi-agent approach for peer-to-peer information retrieval.",
                "In Proceedings of Third International Joint Conference on Autonomous Agents and Multi-Agent Systems, July 2004. [14] H. Zhang and V. Lesser.",
                "Multi-agent based peer-to-peer information retrieval systems with concurrent search sessions.",
                "In Proceedings of the Fifth International Joint Conference on Autonomous Agents and Multi-Agent Systems, May 2006. [15] H. Zhang and V. R. Lesser.",
                "A dynamically formed hierarchical agent organization for a distributed content sharing system.",
                "In 2004 IEEE/WIC/ACM International Conference on Intelligent Agent Technology (IAT 2004), 20-24 September 2004, Beijing, China, pages 169-175.",
                "IEEE Computer Society, 2004. 238 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "distribute search control": {
            "translated_key": "distribuir el control de búsqueda",
            "is_in_text": false,
            "original_annotated_sentences": [
                "A Reinforcement Learning based Distributed Search Algorithm For Hierarchical Peer-to-Peer Information Retrieval Systems Haizheng Zhang College of Information Science and Technology Pennsylvania State University University Park, PA 16803 hzhang@ist.psu.edu Victor Lesser Department of Computer Science University Of Massachusetts Amherst, MA 01003 lesser@cs.umass.edu ABSTRACT The dominant existing routing strategies employed in peerto-peer(P2P) based information retrieval(IR) systems are similarity-based approaches.",
                "In these approaches, agents depend on the content similarity between incoming queries and their direct neighboring agents to direct the distributed search sessions.",
                "However, such a heuristic is myopic in that the neighboring agents may not be connected to more relevant agents.",
                "In this paper, an online reinforcement-learning based approach is developed to take advantage of the dynamic run-time characteristics of P2P IR systems as represented by information about past search sessions.",
                "Specifically, agents maintain estimates on the downstream agents abilities to provide relevant documents for incoming queries.",
                "These estimates are updated gradually by learning from the feedback information returned from previous search sessions.",
                "Based on this information, the agents derive corresponding routing policies.",
                "Thereafter, these agents route the queries based on the learned policies and update the estimates based on the new routing policies.",
                "Experimental results demonstrate that the learning algorithm improves considerably the routing performance on two test collection sets that have been used in a variety of distributed IR studies.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Algorithms, Performance, Experimentation 1.",
                "INTRODUCTION Over the last few years there have been increasing interests in studying how to control the search processes in peer-to-peer(P2P) based information retrieval(IR) systems [6, 13, 14, 15].",
                "In this line of research, one of the core problems that concerns researchers is to efficiently route user queries in the network to agents that are in possession of appropriate documents.",
                "In the absence of global information, the dominant strategies in addressing this problem are content-similarity based approaches [6, 13, 14, 15].",
                "While the content similarity between queries and local nodes appears to be a creditable indicator for the number of relevant documents residing on each node, these approaches are limited by a number of factors.",
                "First of all, similaritybased metrics can be myopic since locally relevant nodes may not be connected to other relevant nodes.",
                "Second, the similarity-based approaches do not take into account the run-time characteristics of the P2P IR systems, including environmental parameters, bandwidth usage, and the historical information of the past search sessions, that provide valuable information for the query routing algorithms.",
                "In this paper, we develop a reinforcement learning based IR approach for improving the performance of distributed IR search algorithms.",
                "Agents can acquire better search strategies by collecting and analyzing feedback information from previous search sessions.",
                "Particularly, agents maintain estimates, namely expected utility, on the downstream agents capabilities of providing relevant documents for specific types of incoming queries.",
                "These estimates are updated gradually by learning from the feedback information returned from previous search sessions.",
                "Based on the updated expected utility information, the agents derive corresponding routing policies.",
                "Thereafter, these agents route the queries based on the learned policies and update the estimates on the expected utility based on the new routing policies.",
                "This process is conducted in an iterative manner.",
                "The goal of the learning algorithm, even though it consumes some network bandwidth, is to shorten the routing time so that more queries are processed per time unit while at the same time finding more relevant documents.",
                "This contrasts with the content-similarity based approaches where similar operations are repeated for every incoming query and the processing time keeps largely constant over time.",
                "Another way of viewing this paper is that our basic approach to distributed IR search is to construct a hierarchical overlay network(agent organization) based on the contentsimilarity measure among agents document collections in a bottom-up fashion.",
                "In the past work, we have shown that this organization improves search performance significantly.",
                "However, this organizational structure does not take into account the arrival patterns of queries, including their frequency, types, and where they enter the system, nor the available communication bandwidth of the network and processing capabilities of individual agents.",
                "The intention of the reinforcement learning is to adapt the agents routing decisions to the dynamic network situations and learn from past search sessions.",
                "Specifically, the contributions of this paper include: (1) a reinforcement learning based approach for agents to acquire satisfactory routing policies based on estimates of the potential contribution of their neighboring agents; (2) two strategies to speed up the learning process.",
                "To our best knowledge, this is one of the first reinforcement learning applications in addressing distributed content sharing problems and it is indicative of some of the issues in applying reinforcement in a complex application.",
                "The remainder of this paper is organized as follows: Section 2 reviews the hierarchical content sharing systems and the two-phase search algorithm based on such topology.",
                "Section 3 describes a reinforcement learning based approach to direct the routing process; Section 4 details the experimental settings and analyze the results.",
                "Section 5 discusses related studies and Section 6 concludes the paper. 2.",
                "SEARCH IN HIERARCHICAL P2P IR SYSTEMS This section briefly reviews our basic approaches to hierarchical P2P IR systems.",
                "In a hierarchical P2P IR system illustrated in Fig.1, agents are connected to each other through three types of links: upward links, downward links, and lateral links.",
                "In the following sections, we denote the set of agents that are directly connected to agent Ai as DirectConn(Ai), which is defined as DirectConn(Ai) = NEI(Ai) ∪ PAR(Ai) ∪ CHL(Ai) , where NEI(Ai) is the set of neighboring agents connected to Ai through lateral links; PAR(Ai) is the set of agents whom agent Ai is connected to through upward links and CHL(Ai) is the set of agents that agent Ai connects to through downward links.",
                "These links are established through a bottom-up content-similarity based distributed clustering process[15].",
                "These links are then used by agents to locate other agents that contain documents relevant to the given queries.",
                "A typical agent Ai in our system uses two queues: a local search queue, LSi, and a message forwarding queue MFi.",
                "The states of the two queues constitute the internal states of an agent.",
                "The local search queue LSi stores search sessions that are scheduled for local processing.",
                "It is a priority queue and agent Ai always selects the most promising queries to process in order to maximize the global utility.",
                "MFi consists of a set of queries to forward on and is processed in a FIFO (first in first out) fashion.",
                "For the first query in MFi, agent Ai determines which subset of its neighboring agents to forward it to based on the agents routing policy πi.",
                "These routing decisions determine how the search process is conducted in the network.",
                "In this paper, we call Ai as Ajs upstream agent and Aj as Ais downstream agent if A4 A5 A6 A7 A2 A3 A9 NEI(A2)={A3} PAR(A2)={A1} CHL(A2)={A4,A5} A1 A8 Figure 1: A fraction of a hierarchical P2PIR system an agent Ai routes a query to agent Aj.",
                "The distributed search protocol of our hierarchical agent organization is composed of two steps.",
                "In the first step, upon receipt of a query qk at time tl from a user, agent Ai initiates a search session si by probing its neighboring agents Aj ∈ NEI(Ai) with the message PROBE for the similarity value Sim(qk, Aj) between qk and Aj.",
                "Here, Ai is defined as the query initiator of search session si.",
                "In the second step, Ai selects a group of the most promising agents to start the actual search process with the message SEARCH.",
                "These SEARCH messages contain a TTL (Time To Live) parameter in addition to the query.",
                "The TTL value decreases by 1 after each hop.",
                "In the search process, agents discard those queries that either have been previously processed or whose TTL drops to 0, which prevents queries from looping in the system forever.",
                "The search session ends when all the agents that receive the query drop it or TTL decreases to 0.",
                "Upon receipt of SEARCH messages for qk, agents schedule local activities including local searching, forwarding qk to their neighbors, and returning search results to the query initiator.",
                "This process and related algorithms are detailed in [15, 14]. 3.",
                "A BASIC REINFORCEMENTLEARNING BASED SEARCH APPROACH In the aforementioned distributed search algorithm, the routing decisions of an agent Ai rely on the similarity comparison between incoming queries and Ais neighboring agents in order to forward those queries to relevant agents without flooding the network with unnecessary query messages.",
                "However, this heuristic is myopic because a relevant direct neighbor is not necessarily connected to other relevant agents.",
                "In this section, we propose a more general approach by framing this problem as a reinforcement learning task.",
                "In pursuit of greater flexibility, agents can switch between two modes: learning mode and non-learning mode.",
                "In the non-learning mode, agents operate in the same way as they do in the normal distributed search processes described in [14, 15].",
                "On the other hand, in the learning mode, in parallel with distributed search sessions, agents also participate in a learning process which will be detailed in this section.",
                "Note that in the learning protocol, the learning process does not interfere with the distributed search process.",
                "Agents can choose to initiate and stop learning processes without affecting the system performance.",
                "In particular, since the learning process consumes network resources (especially bandwidth), agents can choose to initiate learning only when the network load is relatively low, thus minimizing the extra communication costs incurred by the learning algorithm.",
                "The section is structured as follows, Section 3.1 describes 232 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) a reinforcement learning based model.",
                "Section 3.2 describes a protocol to deploy the learning algorithm in the network.",
                "Section 3.3 discusses the convergence of the learning algorithm. 3.1 The Model An agents routing policy takes the state of a search session as input and output the routing actions for that query.",
                "In our work, the state of a search session sj is stipulated as: QSj = (qk, ttlj) where ttlj is the number of hops that remains for the search session sj , qk is the specific query.",
                "QL is an attribute of qk that indicates which type of queries qk most likely belong to.",
                "The set of QL can be generated by running a simple online classification algorithm on all the queries that have been processed by the agents, or an oﬄine algorithm on a pre-designated training set.",
                "The assumption here is that the set of query types is learned ahead of time and belongs to the common knowledge of the agents in the network.",
                "Future work includes exploring how learning can be accomplished when this assumption does not hold.",
                "Given the query types set, an incoming query qi can be classified to one query class Q(qi) by the formula: Q(qi) = arg max Qj P(qi|Qj) (1) where P(qi|Qj ) indicates the likelihood that the query qi is generated by the query class Qj [8].",
                "The set of atomic routing actions of an agent Ai is denoted as {αi}, where {αi} is defined as αi = {αi0 , αi1 , ..., αin }.",
                "An element αij represents an action to route a given query to the neighboring agent Aij ∈ DirectConn(Ai).",
                "The routing policy πi of agent Ai is stochastic and its outcome for a search session with state QSj is defined as: πi(QSj) = {(αi0 , πi(QSi, αi0 )), (αi1 , πi(QSi, αi1 )), ...} (2) Note that operator πi is overloaded to represent either the probabilistic policy for a search session with state QSj, denoted as πi(QSj); or the probability of forwarding the query to a specific neighboring agent Aik ∈ DirectConn(Ai) under the policy πi(QSj), denoted as πi(QSj, αik ).",
                "Therefore, equation (2) means that the probability of forwarding the search session to agent Ai0 is πi(QSi, αi0 ) and so on.",
                "Under this stochastic policy, the routing action is nondeterministic.",
                "The advantage of such a strategy is that the best neighboring agents will not be selected repeatedly, thereby mitigating the potential hot spots situations.",
                "The expected utility, Un i (QSj), is used to estimate the potential utility gain of routing query type QSj to agent Ai under policy πn i .",
                "The superscript n indicates the value at the nth iteration in an iterative learning process.",
                "The expected utility provides routing guidance for future search sessions.",
                "In the search process, each agent Ai maintains partial observations of its neighbors states, as shown in Fig. 2.",
                "The partial observation includes non-local information such as the potential utility estimation of its neighbor Am for query state QSj, denoted as Um(QSj), as well as the load information, Lm.",
                "These observations are updated periodically by the neighbors.",
                "The estimated utility information will be used to update Ais expected utility for its routing policy.",
                "Load Information Expected Utility For Different Query Types Neighboring Agents ... A0 A1 A3 A2 Un 0 (QS0) ... ... ... ... ...... Un 0 (QS1) Un 1 (QS1) Un 2 (QS1) Un 3 (QS1) Un 1 (QS0) Un 2 (QS0) Un 3 (QS0) Ln 0 Ln 1 Ln 2 Ln 3 ... QS0 QS1 ...",
                "Figure 2: Agent Ais Partial Observation about its neighbors(A0, A1...) The load information of Am, Lm, is defined as Lm = |MFm| Cm , where |MFm| is the length of the message-forward queue and Cm is the service rate of agent Ams message-forward queue.",
                "Therefore Lm characterizes the utilization of an agents communication channel, and thus provide non-local information for Ams neighbors to adjust the parameters of their routing policy to avoid inundating their downstream agents.",
                "Note that based on the characteristics of the queries entering the system and agents capabilities, the loading of agents may not be uniform.",
                "After collecting the utilization rate information from all its neighbors, agent Ai computes Li as a single measure for assessing the average load condition of its neighborhood: Li = P k Lk |DirectConn(Ai)| Agents exploit Li value in determining the routing probability in its routing policy.",
                "Note that, as described in Section 3.2, information about neighboring agents is piggybacked with the query message propagated among the agents whenever possible to reduce the traffic overhead. 3.1.1 Update the Policy An iterative update process is introduced for agents to learn a satisfactory stochastic routing policy.",
                "In this iterative process, agents update their estimates on the potential utility of their current routing policies and then propagate the updated estimates to their neighbors.",
                "Their neighbors then generate a new routing policy based on the updated observation and in turn they calculate the expected utility based on the new policies and continue this iterative process.",
                "In particular, at time n, given a set of expected utility, an agent Ai, whose directly connected agents set is DirectConn(Ai) = {Ai0 , ..., Aim }, determines its corresponding stochastic routing policy for a search session of state QSj based on the following steps: (1) Ai first selects a subset of agents as the potential downstream agents from set DirectConn(Ai), denoted as PDn(Ai, QSj).",
                "The size of the potential downstream agent is specified as |PDn(Ai, QSj)| = min(|NEI(Ai), dn i + k)| where k is a constant and is set to 3 in this paper; dn i , the forward width, is defined as the expected number of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 233 neighboring agents that agent Ai can forward to at time n. This formula specifies that the potential downstream agent set PDn(Ai, QSj) is either the subset of neighboring agents with dn i + k highest expected utility value for state QSj among all the agents in DirectConn(Ai), or all their neighboring agents.",
                "The k is introduced based on the idea of a stochastic routing policy and it makes the forwarding probability of the dn i +k highest agent less than 100%.",
                "Note that if we want to limit the number of downstream agents for search session sj as 5, the probability of forwarding the query to all neighboring agents should add up to 5.",
                "Setting up dn i value properly can improve the utilization rate of the network bandwidth when much of the network is idle while mitigating the traffic load when the network is highly loaded.",
                "The dn+1 i value is updated based on dn i , the previous and current observations on the traffic situation in the neighborhood.",
                "Specifically, the update formula for dn+1 i is dn+1 i = dn i ∗ (1 + 1 − Li |DirectConn(Ai)| ) In this formula, the forward width is updated based on the traffic conditions of agent Ais neighborhood, i.e Li, and its previous value. (2) For each agent Aik in the PDn(Ai, QSj), the probability of forwarding the query to Aik is determined in the following way in order to assign higher forwarding probability to the neighboring agents with higher expected utility value: πn+1 i (QSj, αik ) = dn+1 i |PDn(Ai, QSj)| + β ∗ ` Uik (QSj) − PDU(Ai, QSj) |PDn(Ai, QSj)| ´ (3) where PDUn(Ai, QSj) = X o∈P Dn(Ai,QSj ) Uo(QSj) and QSj is the subsequent state of agent Aik after agent Ai forwards the search session with state QSj to its neighboring agent Aik ; If QSj = (qk, ttl0), then QSj = (qk, ttl0 − 1).",
                "In formula 3, the first term on the right of the equation, dn+1 i |P Dn(Ai,QSj )| , is used to to determine the forwarding probability by equally distributing the forward width, dn+1 i , to the agents in PDn(Ai, QSj) set.",
                "The second term is used to adjust the probability of being chosen so that agents with higher expected utility values will be favored. β is determined according to: β = min ` m − dn+1 i m ∗ umax − PDUn(Ai, QSj) , dn+1 i PDUn(Ai, QSj) − m ∗ umin ´ (4) where m = |PDn(Ai, QSj)|, umax = max o∈P Dn(Ao,QSj ) Uo(QSj) and umin = min o∈P Dn(Ao,QSj ) Uo(QSj) This formula guarantees that the final πn+1 i (QSj, αik ) value is well defined, i.e, 0 ≤ πn+1 i (QSj, αik ) ≤ 1 and X i πn+1 i (QSj, αik ) = dn+1 i However, such a solution does not explore all the possibilities.",
                "In order to balance between exploitation and exploration, a λ-Greedy approach is taken.",
                "In the λ-Greedy approach, in addition to assigning higher probability to those agents with higher expected utility value, as in the equation (3).",
                "Agents that appear to be not-so-good choices will also be sent queries based on a dynamic exploration rate.",
                "In particular, for agents in the set PDn(Ai, QSj), πn+1 i1 (QSj) is determined in the same way as the above, with the only difference being that dn+1 i is replaced with dn+1 i ∗ (1 − λn).",
                "The remaining search bandwidth is used for learning by assigning probability λn evenly to agents Ai2 in the set DirectConn(Ai) − PDn(Ai, QSj). πn+1 i2 (QSj, αik ) = dn+1 i ∗ λn |DirectConn(Ai) − PDn(Ai, QSj)| (5) where PDn(Ai, QSj) ⊂ DirectConn(Ai).",
                "Note that the exploration rate λ is not a constant and it decreases overtime.",
                "The λ is determined according to the following equation: λn+1 = λ0 ∗ e−c1n (6) where λ0 is the initial exploration rate, which is a constant; c1 is also a constant to adjust the decreasing rate of the exploration rate; n is the current time unit. 3.1.2 Update Expected Utility Once the routing policy at step n+1, πn+1 i , is determined based on the above formula, agent Ai can update its own expected utility, Un+1 i (QSi), based on the the updated routing policy resulted from the formula 5 and the updated U values of its neighboring agents.",
                "Under the assumption that after a query is forwarded to Ais neighbors the subsequent search sessions are independent, the update formula is similar to the Bellman update formula in Q-Learning: Un+1 i (QSj) = (1 − θi) ∗ Un i (QSj) + θi ∗ (Rn+1 i (QSj) + X k πn+1 i (QSj, αik )Un k (QSj)) (7) where QSj = (Qj, ttl − 1) is the next state of QSj = (Qj, ttl); Rn+1 i (QSj) is the expected local reward for query class Qk at agent Ai under the routing policy πn+1 i ; θi is the coefficient for deciding how much weight is given to the old value during the update process: the smaller θi value is, the faster the agent is expected to learn the real value, while the greater volatility of the algorithm, and vice versa.",
                "Rn+1 (s) is updated according to the following equation: 234 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Rn+1 i (QSj) = Rn i (QSj) +γi ∗ (r(QSj) − Rn i (QSj)) ∗ P(qj|Qj ) (8) where r(QSj) is the local reward associated with the search session.",
                "P(qj|Qj ) indicates how relevant the query qj is to the query type Qj, and γi is the learning rate for agent Ai.",
                "Depending on the similarity between a specific query qi and its corresponding query type Qi, the local reward associated with the search session has different impact on the Rn i (QSj) estimation.",
                "In the above formula, this impact is reflected by the coefficient, the P(qj|Qj) value. 3.1.3 Reward function After a search session stops when its TTL values expires, all search results are returned back to the user and are compared against the relevance judgment.",
                "Assuming the set of search results is SR, the reward Rew(SR) is defined as: Rew(SR) = j 1 if |Rel(SR)| > c |Rel(SR)| c otherwise. where SR is the set of returned search results, Rel(SR) is the set of relevant documents in the search results.",
                "This equation specifies that users give 1.0 reward if the number of returned relevant documents reaches a predefined number c. Otherwise, the reward is in proportion to the number of relevant documents returned.",
                "This rationale for setting up such a cut-off value is that the importance of recall ratio decreases with the abundance of relevant documents in real world, therefore users tend to focus on only a limited number of searched results.",
                "The details of the actual routing protocol will be introduced in Section 3.2 when we introduce how the learning algorithm is deployed in real systems. 3.2 Deployment of the Learning algorithm This section describes how the learning algorithm can be used in either a single-phase or a two-phase search process.",
                "In the single-phase search algorithm, search sessions start from the initiators of the queries.",
                "In contrast, in the two-step search algorithm, the query initiator first attempts to seek a more appropriate starting point for the query by introducing an exploratory step as described in Section 2.",
                "Despite the difference in the quality of starting points, the major part of the learning process for the two algorithms is largely the same as described in the following paragraphs.",
                "Before learning starts, each agent initializes the expected utility value for all possible states as 0.",
                "Thereafter, upon receipt of a query, in addition to the normal operations described in the previous section, an agent Ai also sets up a timer to wait for the search results returned from its downstream agents.",
                "Once the timer expires or it has received response from all its downstream agents, Ai merges and forwards the search results accrued from its downstream agents to its upstream agent.",
                "Setting up the timer speeds up the learning because agents can avoid waiting too long for the downstream agents to return search results.",
                "Note that these detailed results and corresponding agent information will still be stored at Ai until the feedback information is passed from its upstream agent and the performance of its downstream agents can be evaluated.",
                "The duration of the timer is related to the TTL value.",
                "In this paper, we set the timer to ttimer = ttli ∗ 2 + tf , where ttli ∗ 2 is the sum of the travel time of the queries in the network, and tf is the expected time period that users would like to wait.",
                "The search results will eventually be returned to the search session initiator A0.",
                "They will be compared to the relevance judgment that is provided by the final users (as described in the experiment section, the relevance judgement for the query set is provided along with the data collections).",
                "The reward will be calculated and propagated backward to the agents along the way that search results were passed.",
                "This is a reverse process of the search results propagation.",
                "In the process of propagating reward backward, agents update estimates of their own potential utility value, generate an upto-dated policy and pass their updated results to the neighboring agents based on the algorithm described in Section 3.",
                "Upon change of expected utility value, agent Ai sends out its updated utility estimation to its neighbors so that they can act upon the changed expected utility and corresponding state.",
                "This update message includes the potential reward as well as the corresponding state QSi = (qk, ttll) of agent Ai.",
                "Each neighboring agent, Aj, reacts to this kind of update message by updating the expected utility value for state QSj(qk, ttll + 1) according to the newly-announced changed expected utility value.",
                "Once they complete the update, the agents would again in turn inform related neighbors to update their values.",
                "This process goes on until the TTL value in the update message increases to the TTL limit.",
                "To speed up the learning process, while updating the expected utility values of an agent Ais neighboring agents we specify that Um(Qk, ttl0) >= Um(Qk, ttl1) iff ttl0 > ttl1 Thus, when agent Ai receives an updated expected utility value with ttl1, it also updates the expected utility values with any ttl0 > ttl1 if Um(Qk, ttl0) < Um(Qk, ttl1) to speed up convergence.",
                "This heuristic is based on the fact that the utility of a search session is a non-decreasing function of time t. 3.3 Discussion In formalizing the content routing system as a learning task, many assumptions are made.",
                "In real systems, these assumptions may not hold, and thus the learning algorithm may not converge.",
                "Two problems are of particular note, (1) This content routing problem does not have Markov properties.",
                "In contrast to IP-level based packet routing, the routing decision of each agent for a particular search session sj depends on the routing history of sj.",
                "Therefore, the assumption that all subsequent search sessions are independent does not hold in reality.",
                "This may lead to double counting problem that the relevant documents of some agents will be counted more than once for the state where the TTL value is more than 1.",
                "However, in the context of the hierarchical agent organizations, two factors mitigate this problems: first, the agents in each content group form a tree-like structure.",
                "With the absense of the cycles, the estimates inside the tree would be close to the accurate value.",
                "Secondly, the stochastic nature of the routing policy partly remedies this problem.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 235 (2) Another challenge for this learning algorithm is that in a real network environment observations on neighboring agents may not be able to be updated in time due to the communication delay or other situations.",
                "In addition, when neighboring agents update their estimates at the same time, oscillation may arise during the learning process[1].",
                "This paper explores several approaches to speed up the learning process.",
                "Besides the aforementioned strategy of updating the expected utility values, we also employ an active update strategy where agents notify their neighbors whenever its expected utility is updated.",
                "Thus a faster convergence speed can be achieved.",
                "This strategy contrasts to the Lazy update, where agents only echo their neighboring agents with their expected utility change when they exchange information.",
                "The trade off between the two approaches is the network load versus learning speed.",
                "The advantage of this learning algorithm is that once a routing policy is learned, agents do not have to repeatedly compare the similarity of queries as long as the network topology remains unchanged.",
                "Instead, agent just have to determine the classification of the query properly and follow the learned policies.",
                "The disadvantage of this learning-based approach is that the learning process needs to be conducted whenever the network structure changes.",
                "There are many potential extensions for this learning model.",
                "For example, a single measure is currently used to indicate the traffic load for an agents neighborhood.",
                "A simple extension would be to keep track of individual load for each neighbor of the agent. 4.",
                "EXPERIMENTSSETTINGSAND RESULTS The experiments are conducted on TRANO simulation toolkit with two sets of datasets, TREC-VLC-921 and TREC123-100.",
                "The following sub-sections introduce the TRANO testbed, the datasets, and the experimental results. 4.1 TRANO Testbed TRANO (Task Routing on Agent Network Organization) is a multi-agent based network based information retrieval testbed.",
                "TRANO is built on top of the Farm [4], a time based distributed simulator that provides a data dissemination framework for large scale distributed agent network based organizations.",
                "TRANO supports importation and exportation of agent organization profiles including topological connections and other features.",
                "Each TRANO agent is composed of an agent view structure and a control unit.",
                "In simulation, each agent is pulsed regularly and the agent checks the incoming message queues, performs local operations and then forwards messages to other agents . 4.2 Experimental Settings In our experiment, we use two standard datasets, TRECVLC-921 and TREC-123-100 datasets, to simulate the collections hosted on agents.",
                "The TREC-VLC-921 and TREC123-100 datasets were created by the U.S. National Institute for Standard Technology(NIST) for its TREC conferences.",
                "In distributed information retrieval domain, the two data collections are split to 921 and 100 sub-collections.",
                "It is observed that dataset TREC-VLC-921 is more heterogeneous than TREC-123-100 in terms of source, document length, and relevant document distribution from the statistics of the two data collections listed in [13].",
                "Hence, TREC-VLC-921 is much closer to real document distributions in P2P environments.",
                "Furthermore, TREC-123-100 is split into two sets of 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0 500 1000 1500 2000 2500 3000 ARSS Query number ARSS versus the number of incoming queries for TREC-VLC-921 SSLA-921 SSNA-921 Figure 3: ARSS(Average reward per search session) versus the number of search sessions for 1phase search in TREC-VLC-921 0 0.1 0.2 0.3 0.4 0.5 0.6 0 500 1000 1500 2000 2500 3000 ARSS Query number ARSS versus query number for TREC-VLC-921 TSLA-921 TSNA-921 Figure 4: ARSS(Average reward per search session) versus the number of search sessions for 2phase search in TREC-VLC-921 sub-collections in two ways: randomly and by source.",
                "The two partitions are denoted as TREC-123-100-Random and TREC-123-100-Source respectively.",
                "The documents in each subcollection in dataset TREC-123-100-Source are more coherent than those in TREC-123-100-Random.",
                "The two different sets of partitions allow us to observe how the distributed learning algorithm is affected by the homogeneity of the collections.",
                "The hierarchical agent organization is generated by the algorithm described in our previous algorithm [15].",
                "During the topology generation process, degree information of each agent is estimated by the algorithm introduced by Palmer et al. [9] with parameters α = 0.5 and β = 0.6.",
                "In our experiments, we estimate the upward limit and downward degree limit using linear discount factors 0.5, 0.8 and 1.0.",
                "Once the topology is built, queries randomly selected from the query set 301−350 on TREC-VLC-921 and query set 1− 50 on TREC-123-100-Random and TREC-123-100-Source are injected to the system based on a Poisson distribution P(N(t) = n) = (λt)n n! e−λ 236 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 0 50 100 150 200 250 300 350 400 0 500 1000 1500 2000 2500 3000 Cumulativeutility Query number Cumulative utility over the number of incoming queries TSLA-921 SSNA-921 SSLA-921 TSNA-921 Figure 5: The cumulative utility versus the number of search sessions TREC-VLC-921 In addition, we assume that all agents have an equal chance of getting queries from the environment, i.e, λ is the same for every agent.",
                "In our experiments, λ is set as 0.0543 so that the mean of the incoming queries from the environment to the agent network is 50 per time unit.",
                "The service time for the communication queue and local search queue, i.e tQij and trs, is set as 0.01 time unit and 0.05 time units respectively.",
                "In our experiments, there are ten types of queries acquired by clustering the query set 301 − 350 and 1 − 50. 4.3 Results analysis and evaluation Figure 3 demonstrates the ARSS(Average Reward per Search Session) versus the number of incoming queries over time for the the Single-Step based Non-learning Algorithm (SSNA), and the Single-Step Learning Algorithm(SSLA) for data collection TREC-VLC-921.",
                "It shows that the average reward for SSNA algorithm ranges from 0.02 − 0.06 and the performance changes little over time.",
                "The average reward for SSLA approach starts at the same level with the SSNA algorithm.",
                "But the performance increases over time and the average performance gain stabilizes at about 25% after query range 2000 − 3000.",
                "Figure 4 shows the ARSS(Average Reward per Search Session) versus the number of incoming queries over time for the the Two-Step based Non-learning Algorithm(TSNA), and the Two-Step Learning Algorithm(TSLA) for data collection TREC-VLC-921.",
                "The TSNA approach has a relatively consistent performance with the average reward ranges from 0.05 − 0.15.",
                "The average reward for TSLA approach, where learning algorithm is exploited, starts at the same level with the TSNA algorithm and improves the average reward over time until 2000−2500 queries joining the system.",
                "The results show that the average performance gain for TSLA approach over TNLA approach is 35% after stabilization.",
                "Figure 5 shows the cumulative utility versus the number of incoming queries over time for SSNA, SSLA,TSNA, and TSLA respectively.",
                "It illustrates that the cumulative utility of non-learning algorithms increases largely linearly over time, while the gains of learning-based algorithms accelerate when more queries enter the system.",
                "These experimental results demonstrate that learning-based approaches consistently perform better than non-learning based routing algorithm.",
                "Moreover, two-phase learning based algorithm is better than single-phase based learning algorithm because the maximal reward an agent can receive from searching its neighborhood within TTL hops is related to the total number of the relevant documents in that area.",
                "Thus, even the optimal routing policy can do little beyond reaching these relevant documents faster.",
                "On the contrary, the two-stepbased learning algorithm can relocate the search session to a neighborhood with more relevant documents.",
                "The TSLA combines the merits of both approaches and outperforms them.",
                "Table 1 lists the cumulative utility for datasets TREC123-100-Random and TREC-123-100-Source with hierarchical organizations.",
                "The five columns show the results for four different approaches.",
                "In particular, column TSNA-Random shows the results for dataset TREC-123-100-Random with the TSNA approach.",
                "The column TSLA-Random shows the results for dataset TREC-123-100-Random with the TSLA approach.",
                "There are two numbers in each cell in the column TSLA-Random.",
                "The first number is the actual cumulative utility while the second number is the percentage gain in terms of the utility over TSNA approach.",
                "Columns TSNA-Source and TSLA-Source show the results for dataset TREC-123-100-Source with TSNA and TSLA approaches respectively.",
                "Table 1 shows that the performance improvement for TREC-123-100-Random is not as significant as the other datasets.",
                "This is because that the documents in the sub-collection of TREC-123-100-Random are selected randomly which makes the collection model, the signature of the collection, less meaningful.",
                "Since both algorithms are designed based on the assumption that document collections can be well represented by their collection model, this result is not surprising.",
                "Overall, Figures 4, 5, and Table 1 demonstrate that the reinforcement learning based approach can considerably enhance the system performance for both data collections.",
                "However, it remains as future work to discover the correlation between the magnitude of the performance gains and the size of the data collection and/or the extent of the heterogeneity between the sub-collections. 5.",
                "RELATED WORK The content routing problem differs from the networklevel routing in packet-switched communication networks in that content-based routing occurs in application-level networks.",
                "In addition, the destination agents in our contentrouting algorithms are multiple and the addresses are not known in the routing process.",
                "IP-level Routing problems have been attacked from the reinforcement learning perspective[2, 5, 11, 12].",
                "These studies have explored fully distributed algorithms that are able, without central coordination to disseminate knowledge about the network, to find the shortest paths robustly and efficiently in the face of changing network topologies and changing link costs.",
                "There are two major classes of adaptive, distributed packet routing algorithms in the literature: distance-vector algorithms and link-state algorithms.",
                "While this line of studies carry a certain similarity with our work, it has mainly focused on packet-switched communication networks.",
                "In this domain, the destination of a packet is deterministic and unique.",
                "Each agent maintains estimations, probabilistically or deterministically, on the distance to a certain destination through its neighbors.",
                "A variant of Q-Learning techniques is deployed The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 237 Table 1: Cumulative Utility for Datasets TREC-123-100-Random and TREC-123-100-Source with Hierarchical Organization; The percentage numbers in the columns TSLA-Random and TSLA-Source demonstrate the performance gain over the algorithm without learning Query number TSNA-Random TSLA-Random TSNA-Source TSLA-Source 500 25.15 28.45 13% 24.00 21.05 -13% 1000 104.99 126.74 20% 93.95 96.44 2.6% 1250 149.79 168.40 12% 122.64 134.05 9.3% 1500 188.94 211.05 12% 155.30 189.60 22% 1750 235.49 261.60 11% 189.14 243.90 28% 2000 275.09 319.25 16% 219.0 278.80 26% to update the estimations to converge to the real distances.",
                "It has been discovered that the locality property is an important feature of information retrieval systems in user modeling studies[3].",
                "In P2P based content sharing systems, this property is exemplified by the phenomenon that users tend to send queries that represent only a limited number of topics and conversely, users in the same neighborhood are likely to share common interests and send similar queries [10].",
                "The learning based approach is perceived to be more beneficial for real distributed information retrieval systems which exhibit locality property.",
                "This is because the users traffic and query patterns can reduce the state space and speed up the learning process.",
                "Related work in taking advantage of this property include [7], where the authors attempted to address this problem by user modeling techniques. 6.",
                "CONCLUSIONS In this paper, a reinforcement-learning based approach is developed to improve the performance of distributed IR search algorithms.",
                "Particularly, agents maintain estimates, namely expected utility, on the downstream agents ability to provide relevant documents for incoming queries.",
                "These estimates are updated gradually by learning from the feedback information returned from previous search sessions.",
                "Based on the updated expected utility information, the agents modify their routing policies.",
                "Thereafter, these agents route the queries based on the learned policies and update the estimates on the expected utility based on the new routing policies.",
                "The experiments on two different distributed IR datasets illustrates that the reinforcement learning approach improves considerably the cumulative utility over time. 7.",
                "REFERENCES [1] S. Abdallah and V. Lesser.",
                "Learning the task allocation game.",
                "In AAMAS 06: Proceedings of the fifth international joint conference on Autonomous agents and multiagent systems, pages 850-857, New York, NY, USA, 2006.",
                "ACM Press. [2] J.",
                "A. Boyan and M. L. Littman.",
                "Packet routing in dynamically changing networks: A reinforcement learning approach.",
                "In Advances in Neural Information Processing Systems, volume 6, pages 671-678.",
                "Morgan Kaufmann Publishers, Inc., 1994. [3] J. C. French, A. L. Powell, J. P. Callan, C. L. Viles, T. Emmitt, K. J. Prey, and Y. Mou.",
                "Comparing the performance of database selection algorithms.",
                "In Research and Development in Information Retrieval, pages 238-245, 1999. [4] B. Horling, R. Mailler, and V. Lesser.",
                "Farm: A scalable environment for multi-agent development and evaluation.",
                "In Advances in Software Engineering for Multi-Agent Systems, pages 220-237, Berlin, 2004.",
                "Springer-Verlag. [5] M. Littman and J. Boyan.",
                "A distributed reinforcement learning scheme for network routing.",
                "In Proceedings of the International Workshop on Applications of Neural Networks to Telecommunications, 1993. [6] J. Lu and J. Callan.",
                "Federated search of text-based digital libraries in hierarchical peer-to-peer networks.",
                "In In ECIR05, 2005. [7] J. Lu and J. Callan.",
                "User modeling for full-text federated search in peer-to-peer networks.",
                "In ACM SIGIR 2006.",
                "ACM Press, 2006. [8] C. D. Manning and H. Sch¨utze.",
                "Foundations of Statistical Natural Language Processing.",
                "The MIT Press, Cambridge, Massachusetts, 1999. [9] C. R. Palmer and J. G. Steffan.",
                "Generating network topologies that obey power laws.",
                "In Proceedings of GLOBECOM 2000, November 2000. [10] K. Sripanidkulchai, B. Maggs, and H. Zhang.",
                "Efficient content location using interest-based locality in peer-topeer systems.",
                "In INFOCOM, 2003. [11] D. Subramanian, P. Druschel, and J. Chen.",
                "Ants and reinforcement learning: A case study in routing in dynamic networks.",
                "In In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence, pages 832-839, 1997. [12] J. N. Tao and L. Weaver.",
                "A multi-agent, policy gradient approach to network routing.",
                "In In Proceedings of the Eighteenth International Conference on Machine Learning, 2001. [13] H. Zhang, W. B. Croft, B. Levine, and V. Lesser.",
                "A multi-agent approach for peer-to-peer information retrieval.",
                "In Proceedings of Third International Joint Conference on Autonomous Agents and Multi-Agent Systems, July 2004. [14] H. Zhang and V. Lesser.",
                "Multi-agent based peer-to-peer information retrieval systems with concurrent search sessions.",
                "In Proceedings of the Fifth International Joint Conference on Autonomous Agents and Multi-Agent Systems, May 2006. [15] H. Zhang and V. R. Lesser.",
                "A dynamically formed hierarchical agent organization for a distributed content sharing system.",
                "In 2004 IEEE/WIC/ACM International Conference on Intelligent Agent Technology (IAT 2004), 20-24 September 2004, Beijing, China, pages 169-175.",
                "IEEE Computer Society, 2004. 238 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        }
    }
}