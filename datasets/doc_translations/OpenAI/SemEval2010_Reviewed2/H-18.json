{
    "id": "H-18",
    "original_text": "Topic Segmentation with Shared Topic Detection and Alignment of Multiple Documents Bingjun Sun*, Prasenjit Mitra*† , Hongyuan Zha‡ , C. Lee Giles*† , John Yen*† *Department of Computer Science and Engineering † College of Information Sciences and Technology The Pennsylvania State University University Park, PA 16802 ‡ College of Computing The Georgia Institute of Technology Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu ABSTRACT Topic detection and tracking [26] and topic segmentation [15] play an important role in capturing the local and sequential information of documents. Previous work in this area usually focuses on single documents, although similar multiple documents are available in many domains. In this paper, we introduce a novel unsupervised method for shared topic detection and topic segmentation of multiple similar documents based on mutual information (MI) and weighted mutual information (WMI) that is a combination of MI and term weights. The basic idea is that the optimal segmentation maximizes MI(or WMI). Our approach can detect shared topics among documents. It can find the optimal boundaries in a document, and align segments among documents at the same time. It also can handle single-document segmentation as a special case of the multi-document segmentation and alignment. Our methods can identify and strengthen cue terms that can be used for segmentation and partially remove stop words by using term weights based on entropy learned from multiple documents. Our experimental results show that our algorithm works well for the tasks of single-document segmentation, shared topic detection, and multi-document segmentation. Utilizing information from multiple documents can tremendously improve the performance of topic segmentation, and using WMI is even better than using MI for the multi-document segmentation. Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Clustering; H.3.1 [Information Storage and Retrieval]: Content Analysis and IndexingLinguistic processing; I.2.7 [Artificial Intelligence]: Natural Language Processing-Text analysis; I.5.3 [Pattern Recognition]: Clustering-Algorithms;Similarity measures General Terms Algorithms, Design, Experimentation 1. INTRODUCTION Many researchers have worked on topic detection and tracking (TDT) [26] and topic segmentation during the past decade. Topic segmentation intends to identify the boundaries in a document with the goal to capture the latent topical structure. Topic segmentation tasks usually fall into two categories [15]: text stream segmentation where topic transition is identified, and coherent document segmentation in which documents are split into sub-topics. The former category has applications in automatic speech recognition, while the latter one has more applications such as partial-text query of long documents in information retrieval, text summary, and quality measurement of multiple documents. Previous research in connection with TDT falls into the former category, targeted on topic tracking of broadcast speech data and newswire text, while the latter category has not been studied very well. Traditional approaches perform topic segmentation on documents one at a time [15, 25, 6]. Most of them perform badly in subtle tasks like coherent document segmentation [15]. Often, end-users seek documents that have the similar content. Search engines, like, Google, provide links to obtain similar pages. At a finer granularity, users may actually be looking to obtain sections of a document similar to a particular section that presumably discusses a topic of the users interest. Thus, the extension of topic segmentation from single documents to identifying similar segments from multiple similar documents with the same topic is a natural and necessary direction, and multi-document topic segmentation is expected to have a better performance since more information is utilized. Traditional approaches using similarity measurement based on term frequency generally have the same assumption that similar vocabulary tends to be in a coherent topic segment [15, 25, 6]. However, they usually suffer the issue of identifying stop words. For example, additional document-dependent stop words are removed together with the generic stop words in [15]. There are two reasons that we do not remove stop words directly. First, identifying stop words is another issue [12] that requires estimation in each domain. Removing common stop words may result in the loss of useful information in a specific domain. Second, even though stop words can be identified, hard classification of stop words and nonstop words cannot represent the gradually changing amount of information content of each word. We employ a soft classification using term weights. In this paper, we view the problem of topic segmentation as an optimization issue using information theoretic techniques to find the optimal boundaries of a document given the number of text segments so as to minimize the loss of mutual information (MI) (or a weighted mutual information (WMI)) after segmentation and alignment. This is equal to maximizing the MI (or WMI). The MI focuses on measuring the difference among segments whereas previous research focused on finding the similarity (e.g. cosine distance) of segments [15, 25, 6]. Topic alignment of multiple similar documents can be achieved by clustering sentences on the same topic into the same cluster. Single-document topic segmentation is just a special case of the multi-document topic segmentation and alignment problem. Terms can be co-clustered as in [10] at the same time, given the number of clusters, but our experimental results show that this method results in a worse segmentation (see Tables 1, 4, and 6). Usually, human readers can identify topic transition based on cue words, and can ignore stop words. Inspired by this, we give each term (or term cluster) a weight based on entropy among different documents and different segments of documents. Not only can this approach increase the contribution of cue words, but it can also decrease the effect of common stop words, noisy word, and document-dependent stop words. These words are common in a document. Many methods based on sentence similarity require that these words are removed before topic segmentation can be performed [15]. Our results in Figure 3 show that term weights are useful for multi-document topic segmentation and alignment. The major contribution of this paper is that it introduces a novel method for topic segmentation using MI and shows that this method performs better than previously used criteria. Also, we have addressed the problem of topic segmentation and alignment across multiple documents, whereas most existing research focused on segmentation of single documents. Multi-document segmentation and alignment can utilize information from similar documents and improves the performance of topic segmentation greatly. Obviously, our approach can handle single documents as a special case when multiple documents are unavailable. It can detect shared topics among documents to judge if they are multiple documents on the same topic. We also introduce the new criterion of WMI based on term weights learned from multiple similar documents, which can improve performance of topic segmentation further. We propose an iterative greedy algorithm based on dynamic programming and show that it works well in practice. Some of our prior work is in [24]. The rest of this paper is organized as follows: In Section 2, we review related work. Section 3 contains a formulation of the problem of topic segmentation and alignment of multiple documents with term co-clustering, a review of the criterion of MI for clustering, and finally an introduction to WMI. In Section 4, we first propose the iterative greedy algorithm of topic segmentation and alignment with term co-clustering, and then describe how the algorithm can be optimized by usFigure 1: Illustration of multi-document segmentation and alignment. ing dynamic programming. In Section 5, experiments about single-document segmentation, shared topic detection, and multi-document segmentation are described, and results are presented and discussed to evaluate the performance of our algorithm. Conclusions and some future directions of the research work are discussed in Section 6. 2. PREVIOUS WORK Generally, the existing approaches to text segmentation fall into two categories: supervised learning [19, 17, 23] and unsupervised learning [3, 27, 5, 6, 15, 25, 21]. Supervised learning usually has good performance, since it learns functions from labelled training sets. However, often getting large training sets with manual labels on document sentences is prohibitively expensive, so unsupervised approaches are desired. Some models consider dependence between sentences and sections, such as Hidden Markov Model [3, 27], Maximum Entropy Markov Model [19], and Conditional Random Fields [17], while many other approaches are based on lexical cohesion or similarity of sentences [5, 6, 15, 25, 21]. Some approaches also focus on cue words as hints of topic transitions [11]. While some existing methods only consider information in single documents [6, 15], others utilize multiple documents [16, 14]. There are not many works in the latter category, even though the performance of segmentation is expected to be better with utilization of information from multiple documents. Previous research studied methods to find shared topics [16] and topic segmentation and summarization between just a pair of documents [14]. Text classification and clustering is a related research area which categorizes documents into groups using supervised or unsupervised methods. Topical classification or clustering is an important direction in this area, especially co-clustering of documents and terms, such as LSA [9], PLSA [13], and approaches based on distances and bipartite graph partitioning [28] or maximum MI [2, 10], or maximum entropy [1, 18]. Criteria of these approaches can be utilized in the issue of topic segmentation. Some of those methods have been extended into the area of topic segmentation, such as PLSA [5] and maximum entropy [7], but to our best knowledge, using MI for topic segmentation has not been studied. 3. PROBLEM FORMULATION Our goal is to segment documents and align the segments across documents (Figure 1). Let T be the set of terms {t1, t2, ..., tl}, which appear in the unlabelled set of documents D = {d1, d2, ..., dm}. Let Sd be the set of sentences for document d ∈ D, i.e.{s1, s2, ..., snd }. We have a 3D matrix of term frequency, in which the three dimensions are random variables of D, Sd, and T. Sd actually is a random vector including a random variable for each d ∈ D. The term frequency can be used to estimate the joint probability distribution P(D, Sd, T), which is p(t, d, s) = T(t, d, s)/ND, where T(t, d, s) is the number of t in ds sentence s and ND is the total number of terms in D. ˆS represents the set of segments {ˆs1, ˆs2, ..., ˆsp} after segmentation and alignment among multiple documents, where the number of segments | ˆS| = p. A segment ˆsi of document d is a sequence of adjacent sentences in d. Since for different documents si may discuss different sub-topics, our goal is to cluster adjacent sentences in each document into segments, and align similar segments among documents, so that for different documents ˆsi is about the same sub-topic. The goal is to find the optimal topic segmentation and alignment mapping Segd(si) : {s1, s2, ..., snd } → {ˆs1, ˆs2, ..., ˆsp} and Alid(ˆsi) : {ˆs1, ˆs2, ..., ˆsp} → {ˆs1, ˆs2, ..., ˆsp}, for all d ∈ D, where ˆsi is ith segment with the constraint that only adjacent sentences can be mapped to the same segment, i.e. for d, {si, si+1, ..., sj} → {ˆsq}, where q ∈ {1, ..., p}, where p is the segment number, and if i > j, then for d, ˆsq is missing. After segmentation and alignment, random vector Sd becomes an aligned random variable ˆS. Thus, P(D, Sd, T) becomes P(D, ˆS, T). Term co-clustering is a technique that has been employed [10] to improve the accuracy of document clustering. We evaluate the effect of it for topic segmentation. A term t is mapped to exactly one term cluster. Term co-clustering involves simultaneously finding the optimal term clustering mapping Clu(t) : {t1, t2, ..., tl} → {ˆt1, ˆt2, ..., ˆtk}, where k ≤ l, l is the total number of words in all the documents, and k is the number of clusters. 4. METHODOLOGY We now describe a novel algorithm which can handle singledocument segmentation, shared topic detection, and multidocument segmentation and alignment based on MI or WMI. 4.1 Mutual Information MI I(X; Y ) is a quantity to measure the amount of information which is contained in two or more random variables [8, 10]. For the case of two random variables, we have I(X; Y ) = x∈X y∈Y p(x, y)log p(x, y) p(x)p(y) , (1) Obviously, when random variables X and Y are independent, I(X; Y ) = 0. Thus, intuitively, the value of MI depends on how random variables are dependent on each other. The optimal co-clustering is the mapping Clux : X → ˆX and Cluy : Y → ˆY that minimizes the loss: I(X; Y ) − I( ˆX; ˆY ), which is equal to maximizing I( ˆX; ˆY ). This is the criterion of MI for clustering. In the case of topic segmentation, the two random variables are the term variable T and the segment variable S, and each sample is an occurrence of a term T = t in a particular segment S = s. I(T; S) is used to measure how dependent T and S are. However, I(T; S) cannot be computed for documents before segmentation, since we do not have a set of S due to the fact that sentences of Document d, si ∈ Sd, is not aligned with other documents. Thus, instead of minimizing the loss of MI, we can maximize MI after topic segmentation, computed as: I( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS p(ˆt, ˆs)log p(ˆt, ˆs) p(ˆt)p(ˆs) , (2) where p(ˆt, ˆs) are estimated by the term frequency tf of Term Cluster ˆt and Segment ˆs in the training set D. Note that here a segment ˆs includes sentences about the the same topic among all documents. The optimal solution is the mapping Clut : T → ˆT, Segd : Sd → ˆS , and Alid : ˆS → ˆS, which maximizes I( ˆT; ˆS). 4.2 Weighted Mutual Information In topic segmentation and alignment of multiple documents, if P(D, ˆS, T) is known, based on the marginal distributions P(D|T) and P( ˆS|T) for each term t ∈ T, we can categorize terms into four types in the data set: • Common stop words are common both along the dimensions of documents and segments. • Document-dependent stop words that depends on the personal writing style are common only along the dimension of segments for some documents. • Cue words are the most important elements for segmentation. They are common along the dimension of documents only for the same segment, and they are not common along the dimensions of segments. • Noisy words are other words which are not common along both dimensions. Entropy based on P(D|T) and P( ˆS|T) can be used to identify different types of terms. To reinforce the contribution of cue words in the MI computation, and simultaneously reduce the effect of the other three types of words, similar as the idea of the tf-idf weight [22], we use entropies of each term along the dimensions of document D and segment ˆS, i.e. ED(ˆt) and EˆS(ˆt), to compute the weight. A cue word usually has a large value of ED(ˆt) but a small value of EˆS(ˆt). We introduce term weights (or term cluster weights) wˆt = ( ED(ˆt) maxˆt ∈ ˆT (ED(ˆt )) )a (1 − EˆS(ˆt) maxˆt ∈ ˆT (EˆS(ˆt )) )b , (3) where ED(ˆt) = d∈D p(d|ˆt)log|D| 1 p(d|ˆt) , EˆS(ˆt) = ˆs∈ ˆS p(ˆs|ˆt)log| ˆS| 1 p(ˆs|ˆt) , and a > 0 and b > 0 are powers to adjust term weights. Usually a = 1 and b = 1 as default, and maxˆt ∈ ˆT (ED(ˆt )) and maxˆt ∈ ˆT (EˆS(ˆt )) are used to normalize the entropy values. Term cluster weights are used to adjust p(ˆt, ˆs), pw(ˆt, ˆs) = wˆtp(ˆt, ˆs) ˆt∈ ˆT ;ˆs∈ ˆS wˆtp(ˆt, ˆs) , (4) and Iw( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS pw(ˆt, ˆs)log pw(ˆt, ˆs) pw(ˆt)pw(ˆs) , (5) where pw(ˆt) and pw(ˆs) are marginal distributions of pw(ˆt, ˆs). However, since we do not know either the term weights or P(D, ˆS, T), we need to estimate them, but wˆt depends on p(ˆs|t) and ˆS, while ˆS and p(ˆs|t) also depend on wˆt that is still unknown. Thus, an iterative algorithm is required to estimate term weights wˆt and find the best segmentation and alignment to optimize the objective function Iw concurrently. After a document is segmented into sentences Input: Joint probability distribution P(D, Sd, T), number of text segments p ∈ {2, 3, ..., max(sd)}, number of term clusters k ∈ {2, 3, ..., l} (if k = l, no term co-clustering required), and weight type w ∈ {0, 1}, indicating to use I or Iw, respectively. Output: Mapping Clu, Seg, Ali, and term weights wˆt. Initialization: 0. i = 0. Initialize Clu (0) t , Seg (0) d , and Ali (0) d ; Initialize w (0) ˆt using Equation (6) if w = 1; Stage 1: 1. If |D| = 1, k = l, and w = 0, check all sequential segmentations of d into p segments and find the best one Segd(s) = argmaxˆsI( ˆT; ˆS), and return Segd; otherwise, if w = 1 and k = l, go to 3.1; Stage 2: 2.1 If k < l, for each term t, find the best cluster ˆt as Clu(i+1)(t) = argmaxˆtI( ˆT; ˆS(i)) based on Seg(i) and Ali(i); 2.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+1) d (Seg (i+1) d (s)) = argmaxˆsI( ˆT(i+1); ˆS) based on Clu(i+1)(t) if k < l or Clu(0)(t) if k = l; 2.3 i + +. If Clu, Seg, or Ali changed, go to 2.1; otherwise, if w = 0, return Clu(i), Seg(i), and Ali(i); else j = 0, go to 3.1; Stage 3: 3.1 Update w (i+j+1) ˆt based on Seg(i+j), Ali(i+j), and Clu(i) using Equation (3); 3.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+j+1) d (Seg (i+j+1) d (s)) = argmaxˆsIw( ˆT(i); ˆS) based on Clu(i) and w (i+j+1) ˆt ; 3.3 j + +. If Iw( ˆT; ˆS) changes, go to step 6; otherwise, stop and return Clu(i), Seg(i+j), Ali(i+j), and w (i+j) ˆt ; Figure 2: Algorithm: Topic segmentation and alignment based on MI or WMI. and each sentence is segmented into words, each word is stemmed. Then the joint probability distribution P(D, Sd, T) can be estimated. Finally, this distribution can be used to compute MI in our algorithm. 4.3 Iterative Greedy Algorithm Our goal is to maximize the objective function, I( ˆT; ˆS) or Iw( ˆT; ˆS), which can measure the dependence of term occurrences in different segments. Generally, first we do not know the estimate term weights, which depend on the optimal topic segmentation and alignment, and term clusters. Moreover, this problem is NP-hard [10], even though if we know the term weights. Thus, an iterative greedy algorithm is desired to find the best solution, even though probably only local maxima are reached. We present the iterative greedy algorithm in Figure 2 to find a local maximum of I( ˆT; ˆS) or Iw( ˆT; ˆS) with simultaneous term weight estimation. This algorithm can is iterative and greedy for multi-document cases or single-document cases with term weight estimation and/or term co-clustering. Otherwise, since it is just a one step algorithm to solve the task of single-document segmentation [6, 15, 25], the global maximum of MI is guaranteed. We will show later that term co-clustering reduces the accuracy of the results and is not necessary, and for singledocument segmentation, term weights are also not required. 4.3.1 Initialization In Step 0, the initial term clustering Clut and topic segmentation and alignment Segd and Alid are important to avoid local maxima and reduce the number of iterations. First, a good guess of term weights can be made by using the distributions of term frequency along sentences for each document and averaging them to get the initial values of wˆt: wt = ( ED(t) maxt ∈T (ED(t )) )(1 − ES(t) maxt ∈T (ES(t )) ), (6) where ES(t) = 1 |Dt| d∈Dt (1 − s∈Sd p(s|t)log|Sd| 1 p(s|t) ), where Dt is the set of documents which contain Term t. Then, for the initial segmentation Seg(0) , we can simply segment documents equally by sentences. Or we can find the optimal segmentation just for each document d which maximizes the WMI, Seg (0) d = argmaxˆsIw(T; ˆS), where w = w (0) ˆt . For the initial alignment Ali(0) , we can first assume that the order of segments for each d is the same. For the initial term clustering Clu(0) , first cluster labels can be set randomly, and after the first time of Step 3, a good initial term clustering is obtained. 4.3.2 Different Cases After initialization, there are three stages for different cases. Totally there are eight cases, |D| = 1 or |D| > 1, k = l or k < l, w = 0 or w = 1. Single document segmentation without term clustering and term weight estimation (|D| = 1, k = l, w = 0) only requires Stage 1 (Step 1). If term clustering is required (k < l), Stage 2 (Step 2.1, 2.2, and 2.3) is executed iteratively. If term weight estimation is required (w = 1), Stage 3 (Step 3.1, 3.2, and 3.3) is executed iteratively. If both are required (k < l, w = 1), Stage 2 and 3 run one after the other. For multi-document segmentation without term clustering and term weight estimation (|D| > 1, k = l, w = 0), only iteration of Step 2.2 and 2.3 are required. At Stage 1, the global maximum can be found based on I( ˆT; ˆS) using dynamic programming in Section 4.4. Simultaneously finding a good term clustering and estimated term weights is impossible, since when moving a term to a new term cluster to maximize Iw( ˆT; ˆS), we do not know that the weight of this term should be the one of the new cluster or the old cluster. Thus, we first do term clustering at Stage 2, and then estimate term weights at Stage 3. At Stage 2, Step 2.1 is to find the best term clustering and Step 2.2 is to find the best segmentation. This cycle is repeated to find a local maximum based on MI I until it converges. The two steps are: (1) based on current term clustering Cluˆt, for each document d, the algorithm segments all the sentences Sd into p segments sequentially (some segments may be empty), and put them into the p segments ˆS of the whole training set D (all possible cases of different segmentation Segd and alignment Alid are checked) to find the optimal case, and (2) based on the current segmentation and alignment, for each term t, the algorithm finds the best term cluster of t based on the current segmentation Segd and alignment Alid. After finding a good term clustering, term weights are estimated if w = 1. At Stage 3, similar as Stage 2, Step 3.1 is term weight re-estimation and Step 3.2 is to find a better segmentation. They are repeated to find a local maximum based on WMI Iw until it converges. However, if the term clustering in Stage 2 is not accurate, then the term weight estimation at Stage 3 may have a bad result. Finally, at Step 3.3, this algorithm converges and return the output. This algorithm can handle both single-document and multi-document segmentation. It also can detect shared topics among documents by checking the proportion of overlapped sentences on the same topics, as described in Sec 5.2. 4.4 Algorithm Optimization In many previous works on segmentation, dynamic programming is a technique used to maximize the objective function. Similarly, at Step 1, 2.2, and 3.2 of our algorithm, we can use dynamic programming. For Stage 1, using dynamic programming can still find the global optimum, but for Stage 2 and Stage 3, we can only find the optimum for each step of topic segmentation and alignment of a document. Here we only show the dynamic programming for Step 3.2 using WMI (Step 1 and 2.2 are similar but they can use either I or Iw). There are two cases that are not shown in the algorithm in Figure 2: (a) single-document segmentation or multi-document segmentation with the same sequential order of segments, where alignment is not required, and (b) multi-document segmentation with different sequential orders of segments, where alignment is necessary. The alignment mapping function of the former case is simply just Alid(ˆsi) = ˆsi, while for the latter ones alignment mapping function Alid(ˆsi) = ˆsj, i and j may be different. The computational steps for the two cases are listed below: Case 1 (no alignment): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs) and partial pw(ˆs) without counting sentences from d. Then put sentences from i to j into Part k, and compute partial WMI PIw( ˆT; ˆsk(si, si+1, ..., sj)) ˆt∈ ˆT pw(ˆt, ˆsk)log pw(ˆt, ˆsk) pw(ˆt)pw(ˆsk) , where Alid(si, si+1, ..., sj) = k, k ∈ {1, 2, ..., p}, 1 ≤ i ≤ j ≤ nd, and Segd(sq) = ˆsk for all i ≤ q ≤ j. (2) Let M(sm, 1) = PIw( ˆT; ˆs1(s1, s2, ..., sm)). Then M(sm, L) = maxi[M(si−1, L − 1) + PIw( ˆT; ˆsL(si, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, and when i > m, no sentences are put into ˆsk when compute PIw (note PIw( ˆT; ˆs(si, ..., sm)) = 0 for single-document segmentation). (3) Finally M(snd , p) = maxi[M(si−1, p − 1)+ PIw( ˆT; ˆsp(si, ..., snd ))], where 1 ≤ i ≤ nd+1. The optimal Iw is found and the corresponding segmentation is the best. Case 2 (alignment required): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs), and partial pw(ˆs), and PIw( ˆT; ˆsk(si, si+1, ..., sj)) similarly as Case 1. (2) Let M(sm, 1, k) = PIw( ˆT; ˆsk(s1, s2, ..., sm)), where k ∈ {1, 2, ..., p}. Then M(sm, L, kL) = maxi,j[M(si−1, L − 1, kL/j) + PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, kL ∈ Set(p, L), which is the set of all p! L!(p−L)! combinations of L segments chosen from all p segments, j ∈ kL, the set of L segments chosen from all p segments, and kL/j is the combination of L − 1 segments in kL except Segment j. (3) Finally, M(snd , p, kp) = maxi,j[M(si−1, p − 1, kp/j) +PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., snd ))], where kp is just the combination of all p segments and 1 ≤ i ≤ nd + 1, which is the optimal Iw and the corresponding segmentation is the best. The steps of Case 1 and 2 are similar, except in Case 2, alignment is considered in addition to segmentation. First, basic items of probability for computing Iw are computed excluding Doc d, and then partial WMI by putting every possible sequential segment (including empty segment) of d into every segment of the set. Second, the optimal sum of PIw for L segments and the leftmost m sentences, M(sm, L), is found. Finally, the maximal WMI is found among different sums of M(sm, p − 1) and PIw for Segment p. 5. EXPERIMENTS In this section, single-document segmentation, shared topic detection, and multi-document segmentation will be tested. Different hyper parameters of our method are studied. For convenience, we refer to the method using I as MIk if w = 0, and Iw as WMIk if w = 2 or as WMIk if w = 1, where k is the number of term clusters, and if k = l, where l is the total number of terms, then no term clustering is required, i.e. MIl and WMIl. 5.1 Single-document Segmentation 5.1.1 Test Data and Evaluation The first data set we tested is a synthetic one used in previous research [6, 15, 25] and many other papers. It has 700 samples. Each is a concatenation of ten segments. Each segment is the first n sentence selected randomly from the Brown corpus, which is supposed to have a different topic from each other. Currently, the best results on this data set is achieved by Ji et.al. [15]. To compare the performance of our methods, the criterion used widely in previous research is applied, instead of the unbiased criterion introduced in [20]. It chooses a pair of words randomly. If they are in different segments (different) for the real segmentation (real), but predicted (pred) as in the same segment, it is a miss. If they are in the same segment (same), but predicted as in different segments, it is a false alarm. Thus, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, diff)p(diff|real) +p(false alarm|real, pred, same)p(same|real). 5.1.2 Experiment Results We tested the case when the number of segments is known. Table 1 shows the results of our methods with different hyper parameter values and three previous approaches, C99[25], U00[6], and ADDP03[15], on this data set when the segment number is known. In WMI for single-document segmentation, the term weights are computed as follows: wˆt = 1−EˆS(ˆt)/maxˆt ∈ ˆT (EˆS(ˆt )). For this case, our methods MIl and WMIl both outperform all the previous approaches. We compared our methods with ADDP03using one-sample one-sided t-test and p-values are shown in Table 2. From the p-values, we can see that mostly the differences are very Table 1: Average Error Rates of Single-document Segmentation Given Segment Numbers Known Range of n 3-11 3-5 6-8 9-11 Sample size 400 100 100 100 C99 12% 11% 10% 9% U00 10% 9% 7% 5% ADDP03 6.0% 6.8% 5.2% 4.3% MIl 4.68% 5.57% 2.59% 1.59% WMIl 4.94% 6.33% 2.76% 1.62% MI100 9.62% 12.92% 8.66% 6.67% Table 2: Single-document Segmentation: P-values of T-test on Error Rates Range of n 3-11 3-5 6-8 9-11 ADDP03, MIl 0.000 0.000 0.000 0.000 ADDP03, WMIl 0.000 0.099 0.000 0.000 MIl, WMIl 0.061 0.132 0.526 0.898 significant. We also compare the error rates between our two methods using two-sample two-sided t-test to check the hypothesis that they are equal. We cannot reject the hypothesis that they are equal, so the difference are not significant, even though all the error rates for MIl are smaller than WMIl. However, we can conclude that term weights contribute little in single-document segmentation. The results also show that MI using term co-clustering (k = 100) decreases the performance. We tested different number of term clusters, and found that the performance becomes better when the cluster number increases to reach l. WMIk<l has similar results that we did not show in the table. As mentioned before, using MI may be inconsistent on optimal boundaries given different numbers of segments. This situation occurs especially when the similarities among segments are quite different, i.e. some transitions are very obvious, while others are not. This is because usually a document is a hierarchical structure instead of only a sequential structure. When the segments are not at the same level, this situation may occur. Thus, a hierarchical topic segmentation approach is desired, and the structure highly depends on the number of segments for each internal node and the stop criteria of splitting. For this data set of singledocument segmentation, since it is just a synthetic set, which is just a concatenation of several segments about different topics, it is reasonable that approaches simply based on term frequency have a good performance. Usually for the tasks of segmenting coherent documents for sub-topics, the effectiveness decreases much. 5.2 Shared Topic Detection 5.2.1 Test Data and Evaluation The second data set contains 80 news articles from Google News. There are eight topics and each has 10 articles. We randomly split the set into subsets with different document numbers and each subset has all eight topics. We compare our approach MIl and WMIl with LDA [4]. LDA treats a document in the data set as a bag of words, finds its distribution on topics, and its major topic. MIl and WMIl views each sentence as a bag of words and tag it with a topic label. Then for each pair of documents, LDA determines if they are on the same topic, while MIl and Table 3: Shared Topic Detection: Average Error Rates for Different Numbers of Documents in Each Subset #Doc 10 20 40 80 LDA 8.89% 16.33% 1.35% 0.60% MIl, θ = 0.6 4.17% 1.71% 1.47% 0.0% WMIl, θ = 0.8 18.6% 3.16% 1.92% 0.0% WMIl check whether the proportion overlapped sentences on the same topic is larger than the adjustable threshold θ. That is, in MIl and WMIl, for a pair of documents d, d , if [ s∈Sd,s ∈Sd 1(topics=topics )/min(|Sd|, |Sd|)] > θ, where Sd is the set of sentences of d, and |Sd| is the number of sentences of d, then d and d have the shared topic. For a pair of documents selected randomly, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, same)p(same|real) +p(false alarm|real, pred, diff)p(diff|real), where a miss means if they have the same topic (same) for the real case (real), but predicted (pred) as on the same topic. If they are on different topics (diff), but predicted as on the same topic, it is a false alarm. 5.2.2 Experiment Results The results are shown in Table 3. If most documents have different topics, in WMIl, the estimation of term weights in Equation (3) is not correct. Thus, WMIl is not expected to have a better performance than MIl, when most documents have different topics. When there are fewer documents in a subset with the same number of topics, more documents have different topics, so WMIl is more worse than MIl. We can see that for most cases MIl has a better (or at least similar) performance than LDA. After shared topic detection, multi-document segmentation of documents with the shared topics is able to be executed. 5.3 Multi-document Segmentation 5.3.1 Test Data and Evaluation For multi-document segmentation and alignment, our goal is to identify these segments about the same topic among multiple similar documents with shared topics. Using Iw is expected to perform better than I, since without term weights the result is affected seriously by document-dependent stop words and noisy words which depends on the personal writing style. It is more likely to treat the same segments of different documents as different segments under the effect of document-dependent stop words and noisy words. Term weights can reduce the effect of document-dependent stop words and noisy words by giving cue terms more weights. The data set for multi-document segmentation and alignment has 102 samples and 2264 sentences totally. Each is the introduction part of a lab report selected from the course of Biol 240W, Pennsylvania State University. Each sample has two segments, introduction of plant hormones and the content in the lab. The length range of samples is from two to 56 sentences. Some samples only have one part and some have a reverse order the these two segments. It is not hard to identify the boundary between two segments for human. We labelled each sentence manually for evaluation. The criterion of evaluation is just using the proportion of the number of sentences with wrong predicted segment labels in the total number of sentences in the whole training Table 4: Average Error Rates of Multi-document Segmentation Given Segment Numbers Known #Doc MIl WMIl k MIk WMIk 102 3.14% 2.78% 300 4.68% 6.58% 51 4.17% 3.63% 300 17.83% 22.84% 34 5.06% 4.12% 300 18.75% 20.95% 20 7.08% 5.42% 250 20.40% 21.83% 10 10.38% 7.89% 250 21.42% 21.91% 5 15.77% 11.64% 250 21.89% 22.59% 2 25.90% 23.18% 50 25.44% 25.49% 1 23.90% 24.82% 25 25.75% 26.15% Table 5: Multi-document Segmentation: P-values of T-test on Error Rates for MIl and WMIl #Doc 51 34 20 10 5 2 P-value 0.19 0.101 0.025 0.001 0.000 0.002 set as the error rate: p(error|predicted, real) = d∈D s∈Sd 1(predicteds=reals)/ d∈D nd. In order to show the benefits of multi-document segmentation and alignment, we compared our method with different parameters on different partitions of the same training set. Except the cases that the number of documents is 102 and one (they are special cases of using the whole set and the pure single-document segmentation), we randomly divided the training set into m partitions, and each has 51, 34, 20, 10, 5, and 2 document samples. Then we applied our methods on each partition and calculated the error rate of the whole training set. Each case was repeated for 10 times for computing the average error rates. For different partitions of the training set, different k values are used, since the number of terms increases when the document number in each partition increases. 5.3.2 Experiment Results From the experiment results in Table 4, we can see the following observations: (1) When the number of documents increases, all methods have better performances. Only from one to two documents, MIl has decrease a little. We can observe this from Figure 3 at the point of document number = 2. Most curves even have the worst results at this point. There are two reasons. First, because samples vote for the best multi-document segmentation and alignment, but if only two documents are compared with each other, the one with missing segments or a totally different sequence will affect the correct segmentation and alignment of the other. Second, as noted at the beginning of this section, if two documents have more document-dependent stop words or noisy words than cue words, then the algorithm may view them as two different segments and the other segment is missing. Generally, we can only expect a better performance when the number of documents is larger than the number of segments. (2) Except single-document segmentation, WMIl is always better than MIl, and when the number of documents is reaching one or increases to a very large number, their performances become closer. Table 5 shows p-values of twosample one-sided t-test between MIl and WMIl. We also can see this trend from p-values. When document number = 5, we reached the smallest p-value and the largest difference between error rates of MIl and WMIl. For single-document Table 6: Multi-document Segmentation: Average Error Rate for Document Number = 5 in Each Subset with Different Number of Term Clusters #Cluster 75 100 150 250 l MIk 24.67% 24.54% 23.91% 22.59% 15.77% segmentation, WMIl is even a little bit worse than MIl, which is similar as the results of the single-document segmentation on the first data set. The reason is that for singledocument segmentation, we cannot estimate term weights accurately, since multiple documents are unavailable. (3) Using term clustering usually gets worse results than MIl and WMIl.(4) Using term clustering in WMIk is even worse than in MIk, since in WMIk term clusters are found first using I before using Iw. If the term clusters are not correct, then the term weights are estimated worse, which may mislead the algorithm to reach even worse results. From the results we also found that in multi-document segmentation and alignment, most documents with missing segments and a reverse order are identified correctly. Table 6 illustrates the experiment results for the case of 20 partitions (each has five document samples) of the training set and topic segmentation and alignment using MIk with different numbers of term clusters k. Notice that when the number of term clusters increases, the error rate becomes smaller. Without term clustering, we have the best result. We did not show results for WMIk with term clustering, but the results are similar. We also tested WMIl with different hyper parameters of a and b to adjust term weights. The results are presented in Figure 3. It was shown that the default case WMIl : a = 1, b = 1 gave the best results for different partitions of the training set. We can see the trend that when the document number is very small or large, the difference between MIl : a = 0, b = 0 and WMIl : a = 1, b = 1 becomes quite small. When the document number is not large (about from 2 to 10), all the cases using term weights have better performances than MIl : a = 0, b = 0 without term weights, but when the document number becomes larger, the cases WMIl : a = 1, b = 0 and WMIl : a = 2, b = 1 become worse than MIl : a = 0, b = 0. When the document number becomes very large, they are even worse than cases with small document numbers. This means that a proper way to estimate term weights for the criterion of WMI is very important. Figure 4 shows the term weights learned from the whole training set. Four types of words are categorized roughly even though the transition among them are subtle. Figure 5 illustrates the change in (weighted) mutual information for MIl and WMIl. As expected, mutual information for MIl increases monotonically with the number of steps, while WMIl does not. Finally, MIl and WMIl are scalable, with computational complexity shown in Figure 6. One advantage for our approach based on MI is that removing stop words is not required. Another important advantage is that there are no necessary hyper parameters to adjust. In single-document segmentation, the performance based on MI is even better for that based on WMI, so no extra hyper parameter is required. In multi-document segmentation, we show in the experiment, a = 1 and b = 1 is the best. Our method gives more weights to cue terms. However, usually cue terms or sentences appear at the beginning of a segment, while the end of the segment may be 1 2 5 10 20 34 51 102 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 Document Number ErrorRate MIl :a=0,b=0 WMI l :a=1,b=1 WMI l :a=1,b=0 WMI l :a=2,b=1 Figure 3: Error rates for different hyper parameters of term weights. 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Normalized Document Entropy NormalizedSegmentEntropy Noisy words Cue words Common stop words Doc−dep stop words Figure 4: Term weights learned from the whole training set. 0 100 200 300 400 500 600 0.06 0.08 0.1 0.12 0.14 0.16 0.18 Number of Steps (Weighted)MutualInformation MI l WMI l Figure 5: Change in (weighted) MI for MIl and WMIl. 0 20 40 60 80 100 120 0 200 400 600 800 1000 1200 1400 1600 1800 2000 Document Number TimetoConverge(sec) MI l WMI l Figure 6: Time to converge for MIl and WMIl. much noisy. One possible solution is giving more weights to terms at the beginning of each segment. Moreover, when the length of segments are quite different, long segments have much higher term frequencies, so they may dominate the segmentation boundaries. Normalization of term frequencies versus the segment length may be useful. 6. CONCLUSIONS AND FUTURE WORK We proposed a novel method for multi-document topic segmentation and alignment based on weighted mutual information, which can also handle single-document cases. We used dynamic programming to optimize our algorithm. Our approach outperforms all the previous methods on singledocument cases. Moreover, we also showed that doing segmentation among multiple documents can improve the performance tremendously. Our results also illustrated that using weighted mutual information can utilize the information of multiple documents to reach a better performance. We only tested our method on limited data sets. More data sets especially complicated ones should be tested. More previous methods should be compared with. Moreover, natural segmentations like paragraphs are hints that can be used to find the optimal boundaries. Supervised learning also can be considered. 7. ACKNOWLEDGMENTS The authors want to thank Xiang Ji, and Prof. J. Scott Payne for their help. 8. REFERENCES [1] A. Banerjee, I. Ghillon, J. Ghosh, S. Merugu, and D. Modha. A generalized maximum entropy approach to bregman co-clustering and matrix approximation. In Proceedings of SIGKDD, 2004. [2] R. Bekkerman, R. El-Yaniv, and A. McCallum. Multi-way distributional clustering via pairwise interactions. In Proceedings of ICML, 2005. [3] D. M. Blei and P. J. Moreno. Topic segmentation with an aspect hidden markov model. In Proceedings of SIGIR, 2001. [4] D. M. Blei, A. Ng, and M. Jordan. Latent dirichlet allocation. Journal of Machine Learning Research, 3:993-1022, 2003. [5] T. Brants, F. Chen, and I. Tsochantaridis. Topic-based document segmentation with probabilistic latent semantic analysis. In Proceedings of CIKM, 2002. [6] F. Choi. Advances in domain indepedent linear text segmentation. In Proceedings of the NAACL, 2000. [7] H. Christensen, B. Kolluru, Y. Gotoh, and S. Renals. Maximum entropy segmentation of broadcast news. In Proceedings of ICASSP, 2005. [8] T. Cover and J. Thomas. Elements of Information Theory. John Wiley and Sons, New York, USA, 1991. [9] S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and R. Harshman. Indexing by latent semantic analysis. Journal of the American Society for Information Systems, 1990. [10] I. Dhillon, S. Mallela, and D. Modha. Information-theoretic co-clustering. In Proceedings of SIGKDD, 2003. [11] M. Hajime, H. Takeo, and O. Manabu. Text segmentation with multiple surface linguistic cues. In Proceedings of COLING-ACL, 1998. [12] T. K. Ho. Stop word location and identification for adaptive text recognition. International Journal of Document Analysis and Recognition, 3(1), August 2000. [13] T. Hofmann. Probabilistic latent semantic analysis. In Proceedings of the UAI99, 1999. [14] X. Ji and H. Zha. Correlating summarization of a pair of multilingual documents. In Proceedings of RIDE, 2003. [15] X. Ji and H. Zha. Domain-independent text segmentation using anisotropic diffusion and dynamic programming. In Proceedings of SIGIR, 2003. [16] X. Ji and H. Zha. Extracting shared topics of multiple documents. In Proceedings of the 7th PAKDD, 2003. [17] J. Lafferty, A. McCallum, and F. Pereira. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML, 2001. [18] T. Li, S. Ma, and M. Ogihara. Entropy-based criterion in categorical clustering. In Proceedings of ICML, 2004. [19] A. McCallum, D. Freitag, and F. Pereira. Maximum entropy markov models for information extraction and segmentation. In Proceedings of ICML, 2000. [20] L. Pevzner and M. Hearst. A critique and improvement of an evaluation metric for text segmentation. Computational Linguistic, 28(1):19-36, 2002. [21] J. C. Reynar. Statistical models for topic segmentation. In Proceedings of ACL, 1999. [22] G. Salton and M. McGill. Introduction to Modern Information Retrieval. McGraw Hill, 1983. [23] B. Sun, Q. Tan, P. Mitra, and C. L. Giles. Extraction and search of chemical formulae in text documents on the web. In Proceedings of WWW, 2007. [24] B. Sun, D. Zhou, H. Zha, and J. Yen. Multi-task text segmentation and alignment based on weighted mutual information. In Proceedings of CIKM, 2006. [25] M. Utiyama and H. Isahara. A statistical model for domain-independent text segmentation. In Proceedings of the 39th ACL, 1999. [26] C. Wayne. Multilingual topic detection and tracking: Successful research enabled by corpora and evaluation. In Proceedings of LREC, 2000. [27] J. Yamron, I. Carp, L. Gillick, S. Lowe, and P. van Mulbregt. A hidden markov model approach to text segmentation and event tracking. In Proceedings of ICASSP, 1998. [28] H. Zha and X. Ji. Correlating multilingual documents via bipartite graph modeling. In Proceedings of SIGIR, 2002.",
    "original_translation": "La segmentación de temas con detección de temas compartidos y alineación de múltiples documentos Bingjun Sun*, Prasenjit Mitra*†, Hongyuan Zha‡, C. Lee Giles*†, John Yen*† *Departamento de Ciencias de la Computación e Ingeniería † Colegio de Ciencias de la Información y Tecnología Universidad Estatal de Pensilvania University Park, PA 16802 ‡ Colegio de Computación Instituto de Tecnología de Georgia Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu RESUMEN La detección y seguimiento de temas [26] y la segmentación de temas [15] juegan un papel importante en la captura de la información local y secuencial de los documentos. El trabajo previo en esta área suele centrarse en documentos individuales, aunque existen múltiples documentos similares en muchos dominios. En este artículo, presentamos un novedoso método no supervisado para la detección de temas compartidos y la segmentación de temas de múltiples documentos similares basado en la información mutua (MI) y la información mutua ponderada (WMI), que es una combinación de MI y pesos de términos. La idea básica es que la segmentación óptima maximiza MI (o WMI). Nuestro enfoque puede detectar temas compartidos entre documentos. Puede encontrar los límites óptimos en un documento y alinear segmentos entre documentos al mismo tiempo. También puede manejar la segmentación de un solo documento como un caso especial de la segmentación y alineación de varios documentos. Nuestros métodos pueden identificar y fortalecer términos de señal que pueden ser utilizados para la segmentación y eliminar parcialmente las palabras de parada mediante el uso de pesos de términos basados en la entropía aprendida de múltiples documentos. Nuestros resultados experimentales muestran que nuestro algoritmo funciona bien para las tareas de segmentación de un solo documento, detección de temas compartidos y segmentación de múltiples documentos. El uso de información de múltiples documentos puede mejorar enormemente el rendimiento de la segmentación de temas, y utilizar WMI es aún mejor que utilizar MI para la segmentación de múltiples documentos. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información-Agrupación; H.3.1 [Almacenamiento y Recuperación de Información]: Análisis de Contenido e Indexación-Procesamiento Lingüístico; I.2.7 [Inteligencia Artificial]: Procesamiento de Lenguaje Natural-Análisis de Texto; I.5.3 [Reconocimiento de Patrones]: Algoritmos de Agrupación-Medidas de Similitud Términos Generales Algoritmos, Diseño, Experimentación 1. INTRODUCCIÓN Muchos investigadores han trabajado en la detección y seguimiento de temas (TDT) [26] y la segmentación de temas durante la última década. La segmentación de temas tiene como objetivo identificar los límites en un documento con la meta de capturar la estructura temática latente. Las tareas de segmentación de temas generalmente se dividen en dos categorías [15]: la segmentación de flujos de texto, donde se identifica la transición de temas, y la segmentación coherente de documentos, en la que los documentos se dividen en subtemas. La primera categoría tiene aplicaciones en el reconocimiento automático del habla, mientras que la segunda tiene más aplicaciones como la consulta parcial de texto en documentos extensos en la recuperación de información, resumen de texto y medición de calidad de múltiples documentos. La investigación previa en relación con TDT se enmarca en la primera categoría, centrada en el seguimiento de temas de datos de discurso de transmisión y texto de agencia de noticias, mientras que la segunda categoría no ha sido estudiada muy bien. Los enfoques tradicionales realizan la segmentación de temas en los documentos uno a la vez [15, 25, 6]. La mayoría de ellos tienen un mal desempeño en tareas sutiles como la segmentación coherente de documentos [15]. A menudo, los usuarios finales buscan documentos que tengan un contenido similar. Los motores de búsqueda, como Google, proporcionan enlaces para obtener páginas similares. A una granularidad más fina, los usuarios pueden estar buscando obtener secciones de un documento similares a una sección en particular que presumiblemente discute un tema de interés para los usuarios. Por lo tanto, la extensión de la segmentación de temas desde documentos individuales para identificar segmentos similares de múltiples documentos similares con el mismo tema es una dirección natural y necesaria, y se espera que la segmentación de temas de múltiples documentos tenga un mejor rendimiento ya que se utiliza más información. Los enfoques tradicionales que utilizan la medición de similitud basada en la frecuencia de términos generalmente parten del mismo supuesto de que un vocabulario similar tiende a estar en un segmento de tema coherente [15, 25, 6]. Sin embargo, suelen tener el problema de identificar las palabras de parada. Por ejemplo, se eliminan las palabras de parada dependientes del documento junto con las palabras de parada genéricas en [15]. Hay dos razones por las que no eliminamos las palabras de parada directamente. Primero, identificar las palabras vacías es otro problema [12] que requiere estimación en cada dominio. Eliminar palabras comunes de parada puede resultar en la pérdida de información útil en un dominio específico. Segundo, aunque se pueden identificar las palabras de parada, la clasificación estricta de palabras de parada y palabras no de parada no puede representar el cambio gradual en la cantidad de contenido informativo de cada palabra. Empleamos una clasificación suave utilizando pesos de términos. En este documento, consideramos el problema de la segmentación de temas como un problema de optimización utilizando técnicas de teoría de la información para encontrar los límites óptimos de un documento dado el número de segmentos de texto, de manera que se minimice la pérdida de información mutua (MI) (o información mutua ponderada (WMI)) después de la segmentación y alineación. Esto es igual a maximizar el MI (o WMI). El MI se centra en medir la diferencia entre segmentos, mientras que investigaciones anteriores se enfocaban en encontrar la similitud (por ejemplo, la distancia del coseno) entre segmentos [15, 25, 6]. La alineación de temas de múltiples documentos similares se puede lograr agrupando las oraciones sobre el mismo tema en el mismo clúster. La segmentación de temas en un solo documento es solo un caso especial del problema de segmentación y alineación de temas en múltiples documentos. Los términos pueden ser co-agrupados como en [10] al mismo tiempo, dado el número de agrupaciones, pero nuestros resultados experimentales muestran que este método resulta en una peor segmentación (ver Tablas 1, 4 y 6). Por lo general, los lectores humanos pueden identificar la transición de temas basándose en palabras de señal, y pueden ignorar las palabras vacías. Inspirados por esto, asignamos a cada término (o grupo de términos) un peso basado en la entropía entre diferentes documentos y diferentes segmentos de documentos. No solo este enfoque puede aumentar la contribución de las palabras clave, sino que también puede disminuir el efecto de las palabras de parada comunes, palabras ruidosas y palabras de parada dependientes del documento. Estas palabras son comunes en un documento. Muchos métodos basados en la similitud de oraciones requieren que estas palabras sean eliminadas antes de que se pueda realizar la segmentación de temas [15]. Nuestros resultados en la Figura 3 muestran que los pesos de los términos son útiles para la segmentación y alineación de temas en múltiples documentos. La principal contribución de este artículo es que introduce un método novedoso para la segmentación de temas utilizando MI y demuestra que este método funciona mejor que los criterios previamente utilizados. Además, hemos abordado el problema de la segmentación y alineación de temas en varios documentos, mientras que la mayoría de las investigaciones existentes se centraron en la segmentación de documentos individuales. La segmentación y alineación de múltiples documentos pueden utilizar información de documentos similares y mejorar significativamente el rendimiento de la segmentación de temas. Obviamente, nuestro enfoque puede manejar documentos individuales como un caso especial cuando varios documentos no están disponibles. Puede detectar temas compartidos entre documentos para determinar si son múltiples documentos sobre el mismo tema. También introducimos el nuevo criterio de WMI basado en pesos de términos aprendidos de múltiples documentos similares, lo cual puede mejorar aún más el rendimiento de la segmentación de temas. Proponemos un algoritmo voraz iterativo basado en programación dinámica y demostramos que funciona bien en la práctica. Algunos de nuestros trabajos anteriores están en [24]. El resto de este documento está organizado de la siguiente manera: En la Sección 2, revisamos el trabajo relacionado. La sección 3 contiene una formulación del problema de segmentación de temas y alineación de múltiples documentos con co-clustering de términos, una revisión del criterio de MI para el clustering, y finalmente una introducción a WMI. En la Sección 4, primero proponemos el algoritmo voraz iterativo de segmentación y alineación de temas con co-clustering de términos, y luego describimos cómo el algoritmo puede ser optimizado mediante programación dinámica. En la Sección 5, se describen experimentos sobre la segmentación de documentos individuales, la detección de temas compartidos y la segmentación de documentos múltiples, y se presentan y discuten los resultados para evaluar el rendimiento de nuestro algoritmo. Las conclusiones y algunas direcciones futuras del trabajo de investigación se discuten en la Sección 6.2. TRABAJO PREVIO En general, los enfoques existentes para la segmentación de texto se dividen en dos categorías: aprendizaje supervisado [19, 17, 23] y aprendizaje no supervisado [3, 27, 5, 6, 15, 25, 21]. El aprendizaje supervisado suele tener un buen rendimiento, ya que aprende funciones a partir de conjuntos de entrenamiento etiquetados. Sin embargo, obtener conjuntos de entrenamiento grandes con etiquetas manuales en oraciones de documentos suele ser prohibitivamente costoso, por lo que se prefieren enfoques no supervisados. Algunos modelos consideran la dependencia entre oraciones y secciones, como el Modelo Oculto de Markov [3, 27], el Modelo de Máxima Entropía de Markov [19] y los Campos Aleatorios Condicionales [17], mientras que muchos otros enfoques se basan en la cohesión léxica o la similitud de las oraciones [5, 6, 15, 25, 21]. Algunos enfoques también se centran en las palabras clave como pistas de transiciones de tema [11]. Si bien algunos métodos existentes solo consideran información en documentos individuales [6, 15], otros utilizan varios documentos [16, 14]. No hay muchas obras en la última categoría, aunque se espera que el rendimiento de la segmentación sea mejor con la utilización de información de múltiples documentos. Investigaciones previas estudiaron métodos para encontrar temas compartidos [16] y segmentación y resumen de temas entre solo un par de documentos [14]. La clasificación y agrupación de textos es un área de investigación relacionada que categoriza documentos en grupos utilizando métodos supervisados o no supervisados. La clasificación o agrupación temática es una dirección importante en esta área, especialmente el co-agrupamiento de documentos y términos, como LSA [9], PLSA [13], y enfoques basados en distancias y particionamiento de gráficos bipartitos [28] o máxima MI [2, 10], o máxima entropía [1, 18]. Los criterios de estos enfoques pueden ser utilizados en el tema de la segmentación de temas. Algunos de esos métodos se han extendido al área de la segmentación de temas, como PLSA [5] y máxima entropía [7], pero hasta donde sabemos, el uso de MI para la segmentación de temas no ha sido estudiado. 3. FORMULACIÓN DEL PROBLEMA Nuestro objetivo es segmentar documentos y alinear los segmentos entre documentos (Figura 1). Sea T el conjunto de términos {t1, t2, ..., tl}, que aparecen en el conjunto no etiquetado de documentos D = {d1, d2, ..., dm}. Sea Sd el conjunto de oraciones para el documento d ∈ D, es decir, {s1, s2, ..., snd}. Tenemos una matriz tridimensional de frecuencia de términos, en la que las tres dimensiones son variables aleatorias de D, Sd y T. Sd en realidad es un vector aleatorio que incluye una variable aleatoria para cada d ∈ D. La frecuencia de términos se puede utilizar para estimar la distribución de probabilidad conjunta P(D, Sd, T), que es p(t, d, s) = T(t, d, s)/ND, donde T(t, d, s) es el número de t en la oración s de d y ND es el número total de términos en D. ˆS representa el conjunto de segmentos {ˆs1, ˆs2, ..., ˆsp} después de la segmentación y alineación entre varios documentos, donde el número de segmentos | ˆS| = p. Un segmento ˆsi del documento d es una secuencia de oraciones adyacentes en d. Dado que para diferentes documentos si pueden discutir diferentes subtemas, nuestro objetivo es agrupar oraciones adyacentes en cada documento en segmentos, y alinear segmentos similares entre documentos, de modo que para diferentes documentos ˆsi trate sobre el mismo subtema. El objetivo es encontrar la segmentación óptima de temas y el mapeo de alineación Segd(si) : {s1, s2, ..., snd } → {ˆs1, ˆs2, ..., ˆsp} y Alid(ˆsi) : {ˆs1, ˆs2, ..., ˆsp} → {ˆs1, ˆs2, ..., ˆsp}, para todo d ∈ D, donde ˆsi es el i-ésimo segmento con la restricción de que solo las oraciones adyacentes pueden ser asignadas al mismo segmento, es decir, para d, {si, si+1, ..., sj} → {ˆsq}, donde q ∈ {1, ..., p}, donde p es el número de segmento, y si i > j, entonces para d, ˆsq está ausente. Después de la segmentación y alineación, el vector aleatorio Sd se convierte en una variable aleatoria alineada ˆS. Por lo tanto, P(D, Sd, T) se convierte en P(D, ˆS, T). El término co-clustering es una técnica que se ha empleado [10] para mejorar la precisión del agrupamiento de documentos. Evaluamos el efecto de ello para la segmentación de temas. Un término t se asigna a exactamente un grupo de términos. El término co-clustering implica encontrar simultáneamente el mapeo óptimo de agrupación de términos Clu(t) : {t1, t2, ..., tl} → {ˆt1, ˆt2, ..., ˆtk}, donde k ≤ l, l es el número total de palabras en todos los documentos, y k es el número de clústeres. 4. METODOLOGÍA Ahora describimos un algoritmo novedoso que puede manejar la segmentación de un solo documento, la detección de temas compartidos y la segmentación y alineación de múltiples documentos basados en MI o WMI. 4.1 Información Mutua MI I(X; Y) es una cantidad que mide la cantidad de información contenida en dos o más variables aleatorias [8, 10]. Para el caso de dos variables aleatorias, tenemos I(X; Y) = x∈X y∈Y p(x, y)log p(x, y) p(x)p(y) , (1) Obviamente, cuando las variables aleatorias X e Y son independientes, I(X; Y) = 0. Por lo tanto, de manera intuitiva, el valor de MI depende de cómo las variables aleatorias están relacionadas entre sí. La co-agrupación óptima es el mapeo Clux: X → ˆX y Cluy: Y → ˆY que minimiza la pérdida: I(X; Y) - I(ˆX; ˆY), lo cual es igual a maximizar I(ˆX; ˆY). Este es el criterio de MI para la agrupación. En el caso de la segmentación de temas, las dos variables aleatorias son la variable de término T y la variable de segmento S, y cada muestra es una ocurrencia de un término T = t en un segmento particular S = s. Se utiliza I(T; S) para medir qué tan dependientes son T y S. Sin embargo, no se puede calcular I(T; S) para documentos antes de la segmentación, ya que no tenemos un conjunto de S debido a que las oraciones del Documento d, si ∈ Sd, no están alineadas con otros documentos. Así, en lugar de minimizar la pérdida de MI, podemos maximizar MI después de la segmentación de temas, calculada como: I( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS p(ˆt, ˆs)log p(ˆt, ˆs) p(ˆt)p(ˆs) , (2) donde p(ˆt, ˆs) se estiman mediante la frecuencia de términos tf del Cluster de Términos ˆt y el Segmento ˆs en el conjunto de entrenamiento D. Nótese que aquí un segmento ˆs incluye oraciones sobre el mismo tema entre todos los documentos. La solución óptima es el mapeo Clut : T → ˆT, Segd : Sd → ˆS, y Alid : ˆS → ˆS, que maximiza I( ˆT; ˆS). 4.2 Información Mutua Ponderada En la segmentación de temas y alineación de múltiples documentos, si se conoce P(D, ˆS, T), basado en las distribuciones marginales P(D|T) y P( ˆS|T) para cada término t ∈ T, podemos categorizar los términos en cuatro tipos en el conjunto de datos: • Las palabras de parada comunes son comunes a lo largo de las dimensiones de documentos y segmentos. • Las palabras de parada dependientes del documento que dependen del estilo de escritura personal son comunes solo a lo largo de la dimensión de segmentos para algunos documentos. • Las palabras clave son los elementos más importantes para la segmentación. Son comunes a lo largo de la dimensión de los documentos solo para el mismo segmento, y no son comunes a lo largo de las dimensiones de los segmentos. • Las palabras ruidosas son otras palabras que no son comunes a lo largo de ambas dimensiones. La entropía basada en P(D|T) y P( ˆS|T) se puede utilizar para identificar diferentes tipos de términos. Para reforzar la contribución de las palabras clave en el cálculo de MI, y al mismo tiempo reducir el efecto de los otros tres tipos de palabras, similar a la idea del peso tf-idf [22], utilizamos las entropías de cada término a lo largo de las dimensiones del documento D y del segmento ˆS, es decir, ED(ˆt) y EˆS(ˆt), para calcular el peso. Una palabra de señal generalmente tiene un valor alto de ED(ˆt) pero un valor bajo de EˆS(ˆt). Introducimos los pesos de términos (o pesos de grupos de términos) wˆt = ( ED(ˆt) maxˆt ∈ ˆT (ED(ˆt )) )a (1 − EˆS(ˆt) maxˆt ∈ ˆT (EˆS(ˆt )) )b , (3) donde ED(ˆt) = d∈D p(d|ˆt)log|D| 1 p(d|ˆt) , EˆS(ˆt) = ˆs∈ ˆS p(ˆs|ˆt)log| ˆS| 1 p(ˆs|ˆt) , y a > 0 y b > 0 son potencias para ajustar los pesos de términos. Normalmente a = 1 y b = 1 por defecto, y se utilizan maxˆt ∈ ˆT (ED(ˆt )) y maxˆt ∈ ˆT (EˆS(ˆt )) para normalizar los valores de entropía. Los pesos de los grupos de términos se utilizan para ajustar p(ˆt, ˆs), pw(ˆt, ˆs) = wˆtp(ˆt, ˆs) ˆt∈ ˆT ;ˆs∈ ˆS wˆtp(ˆt, ˆs) , (4) e Iw( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS pw(ˆt, ˆs)log pw(ˆt, ˆs) pw(ˆt)pw(ˆs) , (5) donde pw(ˆt) y pw(ˆs) son distribuciones marginales de pw(ˆt, ˆs). Sin embargo, dado que no conocemos ni los pesos de los términos ni P(D, ˆS, T), necesitamos estimarlos, pero wˆt depende de p(ˆs|t) y ˆS, mientras que ˆS y p(ˆs|t) también dependen de wˆt, que aún es desconocido. Por lo tanto, se requiere un algoritmo iterativo para estimar los pesos de los términos wˆt y encontrar la mejor segmentación y alineación para optimizar la función objetivo Iw de manera concurrente. Después de que un documento se segmenta en oraciones, se introduce: la distribución de probabilidad conjunta P(D, Sd, T), el número de segmentos de texto p ∈ {2, 3, ..., máx(sd)}, el número de grupos de términos k ∈ {2, 3, ..., l} (si k = l, no se requiere co-clustering de términos) y el tipo de peso w ∈ {0, 1}, indicando usar I o Iw, respectivamente. Mapeo de Clu, Seg, Ali y pesos de términos wˆt. Inicialización: 0. i = 0. Inicializar Clu (0) t, Seg (0) d y Ali (0) d; Inicializar w (0) ˆt usando la Ecuación (6) si w = 1; Etapa 1: 1. Si |D| = 1, k = l y w = 0, verifica todas las segmentaciones secuenciales de d en p segmentos y encuentra la mejor Segd(s) = argmaxˆsI( ˆT; ˆS), y devuelve Segd; de lo contrario, si w = 1 y k = l, ve a 3.1; Etapa 2: 2.1 Si k < l, para cada término t, encuentra el mejor clúster ˆt como Clu(i+1)(t) = argmaxˆtI( ˆT; ˆS(i)) basado en Seg(i) y Ali(i); 2.2 Para cada d, verifica todas las segmentaciones secuenciales de d en p segmentos con mapeo s → ˆs → ˆs, y encuentra la mejor Ali (i+1) d (Seg (i+1) d (s)) = argmaxˆsI( ˆT(i+1); ˆS) basado en Clu(i+1)(t) si k < l o Clu(0)(t) si k = l; 2.3 i + +. Si Clu, Seg o Ali cambian, ve a 2.1; de lo contrario, si w = 0, devuelve Clu(i), Seg(i) y Ali(i); de lo contrario, j = 0, ve a 3.1; Etapa 3: 3.1 Actualiza w (i+j+1) ˆt basado en Seg(i+j), Ali(i+j) y Clu(i) usando la Ecuación (3); 3.2 Para cada d, verifica todas las segmentaciones secuenciales de d en p segmentos con mapeo s → ˆs → ˆs, y encuentra la mejor Ali (i+j+1) d (Seg (i+j+1) d (s)) = argmaxˆsIw( ˆT(i); ˆS) basado en Clu(i) y w (i+j+1) ˆt; 3.3 j + +. Si Iw( ˆT; ˆS) cambia, ve al paso 6; de lo contrario, detente y devuelve Clu(i), Seg(i+j), Ali(i+j), y w (i+j) ˆt; Figura 2: Algoritmo: Segmentación y alineación de temas basada en MI o WMI. y cada oración se segmenta en palabras, cada palabra se reduce a su raíz. Entonces se puede estimar la distribución de probabilidad conjunta P(D, Sd, T). Finalmente, esta distribución se puede utilizar para calcular la MI en nuestro algoritmo. 4.3 Algoritmo Voraz Iterativo Nuestro objetivo es maximizar la función objetivo, I( ˆT; ˆS) o Iw( ˆT; ˆS), que puede medir la dependencia de las ocurrencias de términos en diferentes segmentos. Generalmente, primero no conocemos los pesos estimados de los términos, los cuales dependen de la segmentación y alineación óptimas de los temas, y los grupos de términos. Además, este problema es NP-duro [10], incluso si conocemos los pesos de los términos. Por lo tanto, se desea un algoritmo voraz iterativo para encontrar la mejor solución, aunque probablemente solo se alcancen máximos locales. Presentamos el algoritmo voraz iterativo en la Figura 2 para encontrar un máximo local de I( ˆT; ˆS) o Iw( ˆT; ˆS) con estimación simultánea de pesos de términos. Este algoritmo es iterativo y codicioso para casos de múltiples documentos o casos de un solo documento con estimación de peso de términos y/o co-clustering de términos. De lo contrario, dado que es solo un algoritmo de un paso para resolver la tarea de segmentación de un solo documento [6, 15, 25], se garantiza el máximo global de MI. Mostraremos más adelante que la coagrupación de términos reduce la precisión de los resultados y no es necesaria, y para la segmentación de un solo documento, los pesos de los términos tampoco son requeridos. 4.3.1 Inicialización En el Paso 0, la agrupación inicial de términos Clut y la segmentación y alineación de temas Segd y Alid son importantes para evitar máximos locales y reducir el número de iteraciones. Primero, se puede hacer una buena estimación de los pesos de los términos utilizando las distribuciones de frecuencia de términos a lo largo de las oraciones para cada documento y promediándolas para obtener los valores iniciales de wˆt: wt = ( ED(t) maxt ∈T (ED(t )) )(1 − ES(t) maxt ∈T (ES(t )) ), (6) donde ES(t) = 1 |Dt| d∈Dt (1 − s∈Sd p(s|t)log|Sd| 1 p(s|t) ), donde Dt es el conjunto de documentos que contienen el término t. Luego, para la segmentación inicial Seg(0), simplemente podemos segmentar los documentos de manera equitativa por oraciones. O podemos encontrar la segmentación óptima solo para cada documento d que maximice el WMI, Seg (0) d = argmaxˆsIw(T; ˆS), donde w = w (0) ˆt. Para el alineamiento inicial Ali(0), podemos primero asumir que el orden de los segmentos para cada d es el mismo. Para el agrupamiento inicial de términos Clu(0), las etiquetas de los primeros grupos pueden establecerse al azar, y después de la primera vez del Paso 3, se obtiene un buen agrupamiento inicial de términos. 4.3.2 Diferentes Casos Después de la inicialización, hay tres etapas para diferentes casos. En total hay ocho casos, |D| = 1 o |D| > 1, k = l o k < l, w = 0 o w = 1. La segmentación de un solo documento sin agrupamiento de términos y estimación de peso de términos (|D| = 1, k = l, w = 0) solo requiere la Etapa 1 (Paso 1). Si se requiere el agrupamiento de términos (k < l), la Etapa 2 (Paso 2.1, 2.2 y 2.3) se ejecuta de forma iterativa. Si se requiere la estimación del peso del término (w = 1), la Etapa 3 (Paso 3.1, 3.2 y 3.3) se ejecuta de forma iterativa. Si ambos son requeridos (k < l, w = 1), la Etapa 2 y 3 se ejecutan una tras otra. Para la segmentación de múltiples documentos sin agrupamiento de términos y estimación de peso de términos (|D| > 1, k = l, w = 0), solo se requiere la iteración de los Pasos 2.2 y 2.3. En la Etapa 1, el máximo global se puede encontrar basándose en I( ˆT; ˆS) utilizando programación dinámica en la Sección 4.4. Es imposible encontrar simultáneamente un buen agrupamiento de términos y pesos estimados de términos, ya que al mover un término a un nuevo grupo de términos para maximizar Iw( ˆT; ˆS), no sabemos si el peso de este término debería ser el del nuevo grupo o el del antiguo. Por lo tanto, primero realizamos el agrupamiento de términos en la Etapa 2, y luego estimamos los pesos de los términos en la Etapa 3. En la Etapa 2, el Paso 2.1 consiste en encontrar el mejor agrupamiento de términos y el Paso 2.2 consiste en encontrar la mejor segmentación. Este ciclo se repite para encontrar un máximo local basado en MI I hasta que converge. Los dos pasos son: (1) basados en el agrupamiento de términos actual Cluˆt, para cada documento d, el algoritmo segmenta todas las oraciones Sd en p segmentos secuencialmente (algunos segmentos pueden estar vacíos), y los coloca en los p segmentos ˆS del conjunto de entrenamiento completo D (se verifican todos los casos posibles de segmentación diferente Segd y alineación Alid) para encontrar el caso óptimo, y (2) basado en la segmentación y alineación actual, para cada término t, el algoritmo encuentra el mejor grupo de términos de t basado en la segmentación actual Segd y la alineación Alid. Después de encontrar un buen agrupamiento de términos, se estiman los pesos de los términos si w = 1. En la Etapa 3, al igual que en la Etapa 2, el Paso 3.1 es la reestimación del peso del término y el Paso 3.2 es encontrar una mejor segmentación. Se repiten para encontrar un máximo local basado en WMI Iw hasta que converja. Sin embargo, si el agrupamiento en la Etapa 2 no es preciso, entonces la estimación de peso en la Etapa 3 puede tener un mal resultado. Finalmente, en el Paso 3.3, este algoritmo converge y devuelve la salida. Este algoritmo puede manejar tanto la segmentación de un solo documento como la de múltiples documentos. También puede detectar temas compartidos entre documentos al verificar la proporción de frases superpuestas sobre los mismos temas, como se describe en la Sección 5.2. 4.4 Optimización del algoritmo. En muchos trabajos anteriores sobre segmentación, la programación dinámica es una técnica utilizada para maximizar la función objetivo. De manera similar, en los Pasos 1, 2.2 y 3.2 de nuestro algoritmo, podemos utilizar programación dinámica. Para la Etapa 1, utilizando programación dinámica aún se puede encontrar el óptimo global, pero para la Etapa 2 y la Etapa 3, solo podemos encontrar el óptimo para cada paso de segmentación de temas y alineación de un documento. Aquí solo mostramos la programación dinámica para el Paso 3.2 utilizando WMI (el Paso 1 y 2.2 son similares pero pueden usar I o Iw). Hay dos casos que no se muestran en el algoritmo de la Figura 2: (a) segmentación de un solo documento o segmentación de múltiples documentos con el mismo orden secuencial de segmentos, donde no se requiere alineación, y (b) segmentación de múltiples documentos con diferentes órdenes secuenciales de segmentos, donde la alineación es necesaria. La función de mapeo de alineación del primer caso es simplemente Alid(ˆsi) = ˆsi, mientras que para los últimos casos la función de mapeo de alineación es Alid(ˆsi) = ˆsj, donde i y j pueden ser diferentes. Los pasos computacionales para los dos casos se enumeran a continuación: Caso 1 (sin alineación): Para cada documento d: (1) Calcular pw(ˆt), pw parcial(ˆt, ˆs) y pw parcial(ˆs) sin contar las oraciones de d. Luego, colocar las oraciones de i a j en la Parte k, y calcular WMI parcial PIw( ˆT; ˆsk(si, si+1, ..., sj)) ˆt∈ ˆT pw(ˆt, ˆsk)log pw(ˆt, ˆsk) pw(ˆt)pw(ˆsk) , donde Alid(si, si+1, ..., sj) = k, k ∈ {1, 2, ..., p}, 1 ≤ i ≤ j ≤ nd, y Segd(sq) = ˆsk para todo i ≤ q ≤ j. (2) Dejar que M(sm, 1) = PIw( ˆT; ˆs1(s1, s2, ..., sm)). Entonces M(sm, L) = maxi[M(si−1, L − 1) + PIw( ˆT; ˆsL(si, ..., sm))], donde 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, y cuando i > m, no se colocan oraciones en ˆsk al calcular PIw (nota PIw( ˆT; ˆs(si, ..., sm)) = 0 para la segmentación de un solo documento). (3) Finalmente M(snd , p) = maxi[M(si−1, p − 1)+ PIw( ˆT; ˆsp(si, ..., snd ))], donde 1 ≤ i ≤ nd+1. Se encuentra el Iw óptimo y la segmentación correspondiente es la mejor. Caso 2 (alineación requerida): Para cada documento d: (1) Calcular pw(ˆt), pw parcial(ˆt, ˆs), y pw parcial(ˆs), y PIw( ˆT; ˆsk(si, si+1, ..., sj)) de manera similar al Caso 1. (2) Sea M(sm, 1, k) = PIw( ˆT; ˆsk(s1, s2, ..., sm)), donde k ∈ {1, 2, ..., p}. Entonces M(sm, L, kL) = maxi,j[M(si−1, L − 1, kL/j) + PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., sm))], donde 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, kL ∈ Set(p, L), que es el conjunto de todos los p! Hay L!(p−L)! combinaciones de L segmentos elegidos de entre todos los p segmentos, j ∈ kL, el conjunto de L segmentos elegidos de entre todos los p segmentos, y kL/j es la combinación de L − 1 segmentos en kL excepto el Segmento j. (3) Finalmente, M(snd , p, kp) = maxi,j[M(si−1, p − 1, kp/j) +PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., snd ))], donde kp es simplemente la combinación de todos los p segmentos y 1 ≤ i ≤ nd + 1, que es el óptimo Iw y la segmentación correspondiente es la mejor. Los pasos de los Casos 1 y 2 son similares, excepto que en el Caso 2, se considera el alineamiento además de la segmentación. Primero, se calculan los elementos básicos de probabilidad para computar Iw excluyendo el documento d, y luego se calcula el WMI parcial colocando cada segmento secuencial posible (incluido el segmento vacío) de d en cada segmento del conjunto. Segundo, se encuentra la suma óptima de PIw para L segmentos y las m oraciones más a la izquierda, M(sm, L). Finalmente, se encuentra el WMI máximo entre diferentes sumas de M(sm, p − 1) y PIw para el Segmento p. 5. EXPERIMENTOS En esta sección se probará la segmentación de un solo documento, la detección de temas compartidos y la segmentación de múltiples documentos. Se estudian diferentes hiperparámetros de nuestro método. Para mayor comodidad, nos referimos al método utilizando I como MIk si w = 0, e Iw como WMIk si w = 2 o como WMIk si w = 1, donde k es el número de grupos de términos, y si k = l, donde l es el número total de términos, entonces no se requiere agrupación de términos, es decir, El primer conjunto de datos que probamos es uno sintético utilizado en investigaciones anteriores [6, 15, 25] y muchos otros artículos. Tiene 700 muestras. Cada uno es una concatenación de diez segmentos. Cada segmento es la primera oración seleccionada al azar de las n primeras oraciones del corpus Brown, que se supone que tienen un tema diferente entre sí. Actualmente, los mejores resultados en este conjunto de datos son logrados por Ji et al. [15]. Para comparar el rendimiento de nuestros métodos, se aplica el criterio ampliamente utilizado en investigaciones anteriores en lugar del criterio imparcial introducido en [20]. Elige un par de palabras al azar. Si se encuentran en segmentos diferentes (diferentes) para la segmentación real (real), pero se predicen (pred) como en el mismo segmento, es un error. Si están en el mismo segmento (mismo), pero se predice que están en segmentos diferentes, es una falsa alarma. Por lo tanto, la tasa de error se calcula utilizando la siguiente ecuación: p(err|real, pred) = p(miss|real, pred, diff)p(diff|real) + p(false alarm|real, pred, same)p(same|real). 5.1.2 Resultados del Experimento Probamos el caso cuando se conoce el número de segmentos. La Tabla 1 muestra los resultados de nuestros métodos con diferentes valores de hiperparámetros y tres enfoques anteriores, C99[25], U00[6] y ADDP03[15], en este conjunto de datos cuando se conoce el número de segmento. En WMI para la segmentación de un solo documento, los pesos de los términos se calculan de la siguiente manera: wˆt = 1−EˆS(ˆt)/maxˆt ∈ ˆT (EˆS(ˆt)). Para este caso, nuestros métodos MIl y WMIl superan a todos los enfoques anteriores. Comparamos nuestros métodos con ADDP03 utilizando una prueba t de una muestra unilateral y los valores de p se muestran en la Tabla 2. A partir de los valores p, podemos ver que en su mayoría las diferencias son muy significativas. También comparamos las tasas de error entre nuestros dos métodos utilizando una prueba t de dos muestras de dos colas para verificar la hipótesis de que son iguales. No podemos rechazar la hipótesis de que son iguales, por lo que las diferencias no son significativas, a pesar de que todas las tasas de error para MIl son más pequeñas que las de WMIl. Sin embargo, podemos concluir que los pesos de los términos contribuyen poco en la segmentación de un solo documento. Los resultados también muestran que el uso de MI con co-clustering de términos (k = 100) disminuye el rendimiento. Probamos diferentes números de grupos de términos y encontramos que el rendimiento mejora cuando el número de grupos aumenta hasta llegar a l. WMIk<l tiene resultados similares que no mostramos en la tabla. Como se mencionó anteriormente, el uso de MI puede ser inconsistente en los límites óptimos dados diferentes números de segmentos. Esta situación ocurre especialmente cuando las similitudes entre los segmentos son bastante diferentes, es decir, algunas transiciones son muy obvias, mientras que otras no lo son. Esto se debe a que normalmente un documento es una estructura jerárquica en lugar de una estructura únicamente secuencial. Cuando los segmentos no están en el mismo nivel, esta situación puede ocurrir. Por lo tanto, se desea un enfoque de segmentación jerárquica de temas, y la estructura depende en gran medida del número de segmentos para cada nodo interno y del criterio de parada de la división. Para este conjunto de datos de segmentación de un solo documento, dado que es solo un conjunto sintético, que es simplemente una concatenación de varios segmentos sobre diferentes temas, es razonable que enfoques simplemente basados en la frecuencia de términos tengan un buen rendimiento. Normalmente, para las tareas de segmentación de documentos coherentes en subtemas, la efectividad disminuye mucho. 5.2 Detección de Temas Compartidos 5.2.1 Datos de Prueba y Evaluación El segundo conjunto de datos contiene 80 artículos de noticias de Google News. Hay ocho temas y cada uno tiene 10 artículos. Dividimos aleatoriamente el conjunto en subconjuntos con diferentes números de documentos y cada subconjunto tiene los ocho temas. Comparamos nuestro enfoque MIl y WMIl con LDA [4]. LDA trata un documento en el conjunto de datos como una bolsa de palabras, encuentra su distribución en temas y su tema principal. MIl y WMIl ven cada oración como un conjunto de palabras y la etiquetan con una etiqueta de tema. Luego, para cada par de documentos, LDA determina si están en el mismo tema, mientras que MIl y Table 3: Detección de Temas Compartidos: Tasas de Error Promedio para Diferentes Números de Documentos en Cada Subconjunto #Doc 10 20 40 80 LDA 8.89% 16.33% 1.35% 0.60% MIl, θ = 0.6 4.17% 1.71% 1.47% 0.0% WMIl, θ = 0.8 18.6% 3.16% 1.92% 0.0% WMIl comprueba si la proporción de oraciones superpuestas en el mismo tema es mayor que el umbral ajustable θ. Es decir, en MIl y WMIl, para un par de documentos d, d , si [ s∈Sd,s ∈Sd 1(temas=temas )/min(|Sd|, |Sd|)] > θ, donde Sd es el conjunto de oraciones de d, y |Sd| es el número de oraciones de d, entonces d y d comparten el tema. Para un par de documentos seleccionados al azar, la tasa de error se calcula utilizando la siguiente ecuación: p(err|real, pred) = p(miss|real, pred, same)p(same|real) + p(false alarm|real, pred, diff)p(diff|real), donde un error significa si tienen el mismo tema (same) para el caso real (real), pero se predice (pred) como en un tema diferente. Si están en diferentes temas (diff), pero se predicen como en el mismo tema, es una falsa alarma. 5.2.2 Resultados del Experimento Los resultados se muestran en la Tabla 3. Si la mayoría de los documentos tienen diferentes temas, en WMIl, la estimación de los pesos de términos en la Ecuación (3) no es correcta. Por lo tanto, no se espera que WMIL tenga un mejor rendimiento que MIL cuando la mayoría de los documentos tienen temas diferentes. Cuando hay menos documentos en un subconjunto con el mismo número de temas, más documentos tienen temas diferentes, por lo que WMIl es peor que MIl. Podemos ver que en la mayoría de los casos, MIl tiene un rendimiento mejor (o al menos similar) que LDA. Después de la detección de temas compartidos, se puede llevar a cabo la segmentación de documentos con los temas compartidos. 5.3 Segmentación de múltiples documentos 5.3.1 Datos de prueba y evaluación Para la segmentación y alineación de múltiples documentos, nuestro objetivo es identificar estos segmentos sobre el mismo tema entre varios documentos similares con temas compartidos. Se espera que al utilizar pesos de términos, Iw funcione mejor que I, ya que sin pesos de términos, el resultado se ve seriamente afectado por palabras de parada dependientes del documento y palabras ruidosas que dependen del estilo de escritura personal. Es más probable tratar los mismos segmentos de diferentes documentos como segmentos diferentes bajo el efecto de palabras de parada dependientes del documento y palabras ruidosas. Los pesos de términos pueden reducir el efecto de las palabras de parada dependientes del documento y las palabras ruidosas al darle más peso a los términos de señal. El conjunto de datos para la segmentación y alineación de múltiples documentos tiene un total de 102 muestras y 2264 oraciones. Cada uno es la parte de introducción de un informe de laboratorio seleccionado del curso de Biol 240W, Universidad Estatal de Pensilvania. Cada muestra tiene dos segmentos, la introducción de las hormonas vegetales y el contenido en el laboratorio. El rango de longitud de las muestras va desde dos hasta 56 oraciones. Algunas muestras solo tienen una parte y algunas tienen un orden inverso de estos dos segmentos. No es difícil identificar el límite entre dos segmentos para los humanos. Etiquetamos cada oración manualmente para evaluación. El criterio de evaluación consiste en utilizar la proporción del número de oraciones con etiquetas de segmento incorrectas predichas en el número total de oraciones en toda la Tabla de entrenamiento. Para mostrar los beneficios de la segmentación y alineación de múltiples documentos, comparamos nuestro método con diferentes parámetros en diferentes particiones del mismo conjunto de entrenamiento. Excepto los casos en los que el número de documentos es 102 y uno (que son casos especiales de uso del conjunto completo y la segmentación pura de un solo documento), dividimos aleatoriamente el conjunto de entrenamiento en m particiones, y cada una tiene 51, 34, 20, 10, 5 y 2 muestras de documentos. Luego aplicamos nuestros métodos en cada partición y calculamos la tasa de error de todo el conjunto de entrenamiento. Cada caso se repitió 10 veces para calcular las tasas de error promedio. Para diferentes particiones del conjunto de entrenamiento, se utilizan diferentes valores de k, ya que el número de términos aumenta cuando el número de documentos en cada partición aumenta. 5.3.2 Resultados del Experimento De los resultados del experimento en la Tabla 4, podemos ver las siguientes observaciones: (1) Cuando el número de documentos aumenta, todos los métodos tienen un mejor rendimiento. Solo de uno a dos documentos, el MIL ha disminuido un poco. Podemos observar esto en la Figura 3 en el punto del número de documento = 2. La mayoría de las curvas incluso tienen los peores resultados en este punto. Hay dos razones. Primero, porque las muestras votan por la mejor segmentación y alineación de múltiples documentos, pero si solo se comparan dos documentos entre sí, aquel con segmentos faltantes o una secuencia totalmente diferente afectará la correcta segmentación y alineación del otro. Segundo, como se señaló al principio de esta sección, si dos documentos tienen más palabras de parada dependientes del documento o palabras ruidosas que palabras clave, entonces el algoritmo puede considerarlos como dos segmentos diferentes y faltar el otro segmento. Generalmente, solo podemos esperar un mejor rendimiento cuando el número de documentos es mayor que el número de segmentos. (2) Excepto en la segmentación de un solo documento, WMIl siempre es mejor que MIl, y cuando el número de documentos se acerca a uno o aumenta a un número muy grande, sus rendimientos se vuelven más cercanos. La Tabla 5 muestra los valores p de la prueba t de dos muestras unilaterales entre MIl y WMIl. También podemos observar esta tendencia a partir de los valores de p. Cuando el número de documento es igual a 5, alcanzamos el valor p más pequeño y la mayor diferencia entre las tasas de error de MIl y WMIl. Para la Tabla 6 de un solo documento: Segmentación de múltiples documentos: Tasa de error promedio para el Documento Número = 5 en cada subconjunto con Diferente Número de Agrupaciones de Términos #Agrupaciones 75 100 150 250 l MIk 24.67% 24.54% 23.91% 22.59% 15.77% de segmentación, WMIl es incluso un poco peor que MIl, lo cual es similar a los resultados de la segmentación de un solo documento en el primer conjunto de datos. La razón es que para la segmentación de un solo documento, no podemos estimar con precisión los pesos de los términos, ya que no tenemos disponibles varios documentos. (3) El uso de agrupamiento de términos generalmente produce resultados peores que MIl y WMIl. (4) El uso de agrupamiento de términos en WMIk es aún peor que en MIk, ya que en WMIk los grupos de términos se encuentran primero utilizando I antes que Iw. Si los grupos de términos no son correctos, entonces los pesos de los términos se estiman peor, lo que puede llevar a que el algoritmo obtenga resultados aún peores. De los resultados también encontramos que en la segmentación y alineación de múltiples documentos, la mayoría de los documentos con segmentos faltantes y en orden inverso son identificados correctamente. La Tabla 6 ilustra los resultados del experimento para el caso de 20 particiones (cada una con cinco muestras de documentos) del conjunto de entrenamiento y la segmentación y alineación de temas utilizando MIk con diferentes números de grupos de términos k. Observa que cuando el número de grupos de términos aumenta, la tasa de error disminuye. Sin agrupamiento de términos, obtenemos el mejor resultado. No mostramos resultados para WMIk con agrupamiento de términos, pero los resultados son similares. También probamos WMIL con diferentes hiperparámetros de a y b para ajustar los pesos de los términos. Los resultados se presentan en la Figura 3. Se demostró que el caso predeterminado WMIl: a = 1, b = 1 dio los mejores resultados para diferentes particiones del conjunto de entrenamiento. Podemos observar la tendencia de que cuando el número de documentos es muy pequeño o grande, la diferencia entre MIl: a = 0, b = 0 y WMIl: a = 1, b = 1 se vuelve bastante pequeña. Cuando el número de documentos no es grande (aproximadamente de 2 a 10), todos los casos que utilizan pesos de términos tienen un mejor rendimiento que MIl: a = 0, b = 0 sin pesos de términos, pero cuando el número de documentos aumenta, los casos WMIl: a = 1, b = 0 y WMIl: a = 2, b = 1 empeoran en comparación con MIl: a = 0, b = 0. Cuando el número de documentos se vuelve muy grande, son aún peores que los casos con números de documentos pequeños. Esto significa que es muy importante encontrar una forma adecuada de estimar los pesos de los términos para el criterio de WMI. La Figura 4 muestra los pesos de los términos aprendidos de todo el conjunto de entrenamiento. Cuatro tipos de palabras se categorizan de manera aproximada aunque la transición entre ellas es sutil. La Figura 5 ilustra el cambio en la información mutua (ponderada) para MIl y WMIl. Como era de esperar, la información mutua para MIl aumenta de forma monótona con el número de pasos, mientras que WMIl no lo hace. Finalmente, MIl y WMIl son escalables, con complejidad computacional mostrada en la Figura 6. Una ventaja de nuestro enfoque basado en MI es que no es necesario eliminar las palabras vacías. Otra ventaja importante es que no hay hiperparámetros necesarios que ajustar. En la segmentación de un solo documento, el rendimiento basado en MI es aún mejor que el basado en WMI, por lo que no se requiere un hiperparámetro adicional. En la segmentación de múltiples documentos, mostramos en el experimento que a = 1 y b = 1 es lo mejor. Nuestro método otorga más peso a los términos de señal. Sin embargo, generalmente los términos o frases de señal aparecen al principio de un segmento, mientras que el final del segmento puede ser 1 2 5 10 20 34 51 102 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 Número de Documento Tasa de Error MIl :a=0,b=0 WMI l :a=1,b=1 WMI l :a=1,b=0 WMI l :a=2,b=1 Figura 3: Tasas de error para diferentes hiperparámetros de pesos de términos. 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Entropía de Documento Normalizada Entropía de Segmento Normalizada Palabras ruidosas Palabras de señal Palabras de parada comunes Palabras de parada de Doc-dep Figura 4: Pesos de términos aprendidos de todo el conjunto de entrenamiento. 0 100 200 300 400 500 600 0.06 0.08 0.1 0.12 0.14 0.16 0.18 Número de Pasos (Ponderado) Información Mutua MI l WMI l Figura 5: Cambio en la MI (ponderada) para MIl y WMIl. 0 20 40 60 80 100 120 0 200 400 600 800 1000 1200 1400 1600 1800 2000 Número de Documento Tiempo de Convergencia (seg) MI l WMI l Figura 6: Tiempo de convergencia para MIl y WMIl. muy ruidoso. Una posible solución es dar más peso a los términos al principio de cada segmento. Además, cuando la longitud de los segmentos es bastante diferente, los segmentos largos tienen frecuencias de términos mucho más altas, por lo que pueden dominar los límites de segmentación. La normalización de las frecuencias de términos en relación con la longitud del segmento puede ser útil. 6. CONCLUSIONES Y TRABAJO FUTURO Propusimos un método novedoso para la segmentación y alineación de temas en múltiples documentos basado en información mutua ponderada, que también puede manejar casos de un solo documento. Utilizamos programación dinámica para optimizar nuestro algoritmo. Nuestro enfoque supera a todos los métodos anteriores en casos de un solo documento. Además, también demostramos que realizar segmentación entre varios documentos puede mejorar el rendimiento enormemente. Nuestros resultados también demostraron que el uso de la información mutua ponderada puede aprovechar la información de varios documentos para lograr un mejor rendimiento. Solo probamos nuestro método en conjuntos de datos limitados. Se deben probar más conjuntos de datos, especialmente los complicados. Deberían compararse más métodos anteriores. Además, las segmentaciones naturales como los párrafos son pistas que se pueden utilizar para encontrar los límites óptimos. El aprendizaje supervisado también puede ser considerado. 7. AGRADECIMIENTOS Los autores desean agradecer a Xiang Ji y al Prof. J. Scott Payne por su ayuda. 8. REFERENCIAS [1] A. Banerjee, I. Ghillon, J. Ghosh, S. Merugu y D. Modha. Un enfoque generalizado de entropía máxima para la coagrupación de Bregman y la aproximación de matrices. En Actas de SIGKDD, 2004. [2] R. Bekkerman, R. El-Yaniv y A. McCallum. Agrupación distribucional de múltiples vías a través de interacciones por pares. En Actas de ICML, 2005. [3] D. M. Blei y P. J. Moreno. Segmentación de temas con un modelo oculto de Markov de aspectos. En Actas de SIGIR, 2001. [4] D. M. Blei, A. Ng y M. Jordan. Asignación latente de Dirichlet. Revista de Investigación en Aprendizaje Automático, 3:993-1022, 2003. [5] T. Brants, F. Chen e I. Tsochantaridis. Segmentación de documentos basada en temas con análisis semántico latente probabilístico. En Actas de CIKM, 2002. [6] F. Choi. Avances en la segmentación lineal de texto independiente del dominio. En Actas de la NAACL, 2000. [7] H. Christensen, B. Kolluru, Y. Gotoh y S. Renals. Segmentación de máxima entropía de noticias de transmisión. En Actas de ICASSP, 2005. [8] T. Cover y J. Thomas. Elementos de la teoría de la información. John Wiley and Sons, Nueva York, EE. UU., 1991. [9] S. Deerwester, S. Dumais, G. Furnas, T. Landauer y R. Harshman. Indexación mediante análisis semántico latente. Revista de la Sociedad Americana de Sistemas de Información, 1990. [10] I. Dhillon, S. Mallela y D. Modha. Co-clustering informativo-teórico. En Actas de SIGKDD, 2003. [11] M. Hajime, H. Takeo y O. Manabu. Segmentación de texto con múltiples señales lingüísticas superficiales. En Actas de COLING-ACL, 1998. [12] T. K. Ho. Detención de palabras y identificación para el reconocimiento de texto adaptativo. Revista Internacional de Análisis y Reconocimiento de Documentos, 3(1), agosto de 2000. [13] T. Hofmann. Análisis semántico latente probabilístico. En Actas de la UAI99, 1999. [14] X. Ji y H. Zha. Correlación de la sumarización de un par de documentos multilingües. En Actas de RIDE, 2003. [15] X. Ji y H. Zha. Segmentación de texto independiente del dominio utilizando difusión anisotrópica y programación dinámica. En Actas de SIGIR, 2003. [16] X. Ji y H. Zha. Extrayendo temas compartidos de múltiples documentos. En Actas de la 7ª PAKDD, 2003. [17] J. Lafferty, A. McCallum y F. Pereira. Campos aleatorios condicionales: Modelos probabilísticos para segmentar y etiquetar datos de secuencias. En Actas de ICML, 2001. [18] T. Li, S. Ma y M. Ogihara. Criterio basado en la entropía en la agrupación categórica. En Actas de ICML, 2004. [19] A. McCallum, D. Freitag y F. Pereira. Modelos de máxima entropía de Markov para extracción de información y segmentación. En Actas de ICML, 2000. [20] L. Pevzner y M. Hearst. Una crítica y mejora de una métrica de evaluación para la segmentación de texto. Lingüística Computacional, 28(1):19-36, 2002. [21] J. C. Reynar. Modelos estadísticos para la segmentación de temas. En Actas de ACL, 1999. [22] G. Salton y M. McGill. Introducción a la Recuperación de Información Moderna. McGraw Hill, 1983. [23] B. \n\nMcGraw Hill, 1983. [23] B. Sun, Q. Tan, P. Mitra y C. L. Giles. Extracción y búsqueda de fórmulas químicas en documentos de texto en la web. En Actas de WWW, 2007. [24] B. Sun, D. Zhou, H. Zha, y J. I'm sorry, but \"Yen\" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish? Segmentación y alineación de texto multitarea basada en información mutua ponderada. En Actas de CIKM, 2006. [25] M. Utiyama y H. Isahara. Un modelo estadístico para la segmentación de texto independiente del dominio. En Actas de la 39ª ACL, 1999. [26] C. Wayne. Detección y seguimiento de temas multilingües: Investigación exitosa habilitada por corpora y evaluación. En Actas de LREC, 2000. [27] J. Yamron, I. Carp, L. Gillick, S. Lowe y P. van Mulbregt. Un enfoque de modelo oculto de Markov para la segmentación de texto y seguimiento de eventos. En Actas de ICASSP, 1998. [28] H. Zha y X. Ji. Correlacionando documentos multilingües a través de modelado de gráficos bipartitos. En Actas de SIGIR, 2002.",
    "original_sentences": [
        "Topic Segmentation with Shared Topic Detection and Alignment of Multiple Documents Bingjun Sun*, Prasenjit Mitra*† , Hongyuan Zha‡ , C. Lee Giles*† , John Yen*† *Department of Computer Science and Engineering † College of Information Sciences and Technology The Pennsylvania State University University Park, PA 16802 ‡ College of Computing The Georgia Institute of Technology Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu ABSTRACT Topic detection and tracking [26] and topic segmentation [15] play an important role in capturing the local and sequential information of documents.",
        "Previous work in this area usually focuses on single documents, although similar multiple documents are available in many domains.",
        "In this paper, we introduce a novel unsupervised method for shared topic detection and topic segmentation of multiple similar documents based on mutual information (MI) and weighted mutual information (WMI) that is a combination of MI and term weights.",
        "The basic idea is that the optimal segmentation maximizes MI(or WMI).",
        "Our approach can detect shared topics among documents.",
        "It can find the optimal boundaries in a document, and align segments among documents at the same time.",
        "It also can handle single-document segmentation as a special case of the multi-document segmentation and alignment.",
        "Our methods can identify and strengthen cue terms that can be used for segmentation and partially remove stop words by using term weights based on entropy learned from multiple documents.",
        "Our experimental results show that our algorithm works well for the tasks of single-document segmentation, shared topic detection, and multi-document segmentation.",
        "Utilizing information from multiple documents can tremendously improve the performance of topic segmentation, and using WMI is even better than using MI for the multi-document segmentation.",
        "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Clustering; H.3.1 [Information Storage and Retrieval]: Content Analysis and IndexingLinguistic processing; I.2.7 [Artificial Intelligence]: Natural Language Processing-Text analysis; I.5.3 [Pattern Recognition]: Clustering-Algorithms;Similarity measures General Terms Algorithms, Design, Experimentation 1.",
        "INTRODUCTION Many researchers have worked on topic detection and tracking (TDT) [26] and topic segmentation during the past decade.",
        "Topic segmentation intends to identify the boundaries in a document with the goal to capture the latent topical structure.",
        "Topic segmentation tasks usually fall into two categories [15]: text stream segmentation where topic transition is identified, and coherent document segmentation in which documents are split into sub-topics.",
        "The former category has applications in automatic speech recognition, while the latter one has more applications such as partial-text query of long documents in information retrieval, text summary, and quality measurement of multiple documents.",
        "Previous research in connection with TDT falls into the former category, targeted on topic tracking of broadcast speech data and newswire text, while the latter category has not been studied very well.",
        "Traditional approaches perform topic segmentation on documents one at a time [15, 25, 6].",
        "Most of them perform badly in subtle tasks like coherent document segmentation [15].",
        "Often, end-users seek documents that have the similar content.",
        "Search engines, like, Google, provide links to obtain similar pages.",
        "At a finer granularity, users may actually be looking to obtain sections of a document similar to a particular section that presumably discusses a topic of the users interest.",
        "Thus, the extension of topic segmentation from single documents to identifying similar segments from multiple similar documents with the same topic is a natural and necessary direction, and multi-document topic segmentation is expected to have a better performance since more information is utilized.",
        "Traditional approaches using similarity measurement based on term frequency generally have the same assumption that similar vocabulary tends to be in a coherent topic segment [15, 25, 6].",
        "However, they usually suffer the issue of identifying stop words.",
        "For example, additional document-dependent stop words are removed together with the generic stop words in [15].",
        "There are two reasons that we do not remove stop words directly.",
        "First, identifying stop words is another issue [12] that requires estimation in each domain.",
        "Removing common stop words may result in the loss of useful information in a specific domain.",
        "Second, even though stop words can be identified, hard classification of stop words and nonstop words cannot represent the gradually changing amount of information content of each word.",
        "We employ a soft classification using term weights.",
        "In this paper, we view the problem of topic segmentation as an optimization issue using information theoretic techniques to find the optimal boundaries of a document given the number of text segments so as to minimize the loss of mutual information (MI) (or a weighted mutual information (WMI)) after segmentation and alignment.",
        "This is equal to maximizing the MI (or WMI).",
        "The MI focuses on measuring the difference among segments whereas previous research focused on finding the similarity (e.g. cosine distance) of segments [15, 25, 6].",
        "Topic alignment of multiple similar documents can be achieved by clustering sentences on the same topic into the same cluster.",
        "Single-document topic segmentation is just a special case of the multi-document topic segmentation and alignment problem.",
        "Terms can be co-clustered as in [10] at the same time, given the number of clusters, but our experimental results show that this method results in a worse segmentation (see Tables 1, 4, and 6).",
        "Usually, human readers can identify topic transition based on cue words, and can ignore stop words.",
        "Inspired by this, we give each term (or term cluster) a weight based on entropy among different documents and different segments of documents.",
        "Not only can this approach increase the contribution of cue words, but it can also decrease the effect of common stop words, noisy word, and document-dependent stop words.",
        "These words are common in a document.",
        "Many methods based on sentence similarity require that these words are removed before topic segmentation can be performed [15].",
        "Our results in Figure 3 show that term weights are useful for multi-document topic segmentation and alignment.",
        "The major contribution of this paper is that it introduces a novel method for topic segmentation using MI and shows that this method performs better than previously used criteria.",
        "Also, we have addressed the problem of topic segmentation and alignment across multiple documents, whereas most existing research focused on segmentation of single documents.",
        "Multi-document segmentation and alignment can utilize information from similar documents and improves the performance of topic segmentation greatly.",
        "Obviously, our approach can handle single documents as a special case when multiple documents are unavailable.",
        "It can detect shared topics among documents to judge if they are multiple documents on the same topic.",
        "We also introduce the new criterion of WMI based on term weights learned from multiple similar documents, which can improve performance of topic segmentation further.",
        "We propose an iterative greedy algorithm based on dynamic programming and show that it works well in practice.",
        "Some of our prior work is in [24].",
        "The rest of this paper is organized as follows: In Section 2, we review related work.",
        "Section 3 contains a formulation of the problem of topic segmentation and alignment of multiple documents with term co-clustering, a review of the criterion of MI for clustering, and finally an introduction to WMI.",
        "In Section 4, we first propose the iterative greedy algorithm of topic segmentation and alignment with term co-clustering, and then describe how the algorithm can be optimized by usFigure 1: Illustration of multi-document segmentation and alignment. ing dynamic programming.",
        "In Section 5, experiments about single-document segmentation, shared topic detection, and multi-document segmentation are described, and results are presented and discussed to evaluate the performance of our algorithm.",
        "Conclusions and some future directions of the research work are discussed in Section 6. 2.",
        "PREVIOUS WORK Generally, the existing approaches to text segmentation fall into two categories: supervised learning [19, 17, 23] and unsupervised learning [3, 27, 5, 6, 15, 25, 21].",
        "Supervised learning usually has good performance, since it learns functions from labelled training sets.",
        "However, often getting large training sets with manual labels on document sentences is prohibitively expensive, so unsupervised approaches are desired.",
        "Some models consider dependence between sentences and sections, such as Hidden Markov Model [3, 27], Maximum Entropy Markov Model [19], and Conditional Random Fields [17], while many other approaches are based on lexical cohesion or similarity of sentences [5, 6, 15, 25, 21].",
        "Some approaches also focus on cue words as hints of topic transitions [11].",
        "While some existing methods only consider information in single documents [6, 15], others utilize multiple documents [16, 14].",
        "There are not many works in the latter category, even though the performance of segmentation is expected to be better with utilization of information from multiple documents.",
        "Previous research studied methods to find shared topics [16] and topic segmentation and summarization between just a pair of documents [14].",
        "Text classification and clustering is a related research area which categorizes documents into groups using supervised or unsupervised methods.",
        "Topical classification or clustering is an important direction in this area, especially co-clustering of documents and terms, such as LSA [9], PLSA [13], and approaches based on distances and bipartite graph partitioning [28] or maximum MI [2, 10], or maximum entropy [1, 18].",
        "Criteria of these approaches can be utilized in the issue of topic segmentation.",
        "Some of those methods have been extended into the area of topic segmentation, such as PLSA [5] and maximum entropy [7], but to our best knowledge, using MI for topic segmentation has not been studied. 3.",
        "PROBLEM FORMULATION Our goal is to segment documents and align the segments across documents (Figure 1).",
        "Let T be the set of terms {t1, t2, ..., tl}, which appear in the unlabelled set of documents D = {d1, d2, ..., dm}.",
        "Let Sd be the set of sentences for document d ∈ D, i.e. {s1, s2, ..., snd }.",
        "We have a 3D matrix of term frequency, in which the three dimensions are random variables of D, Sd, and T. Sd actually is a random vector including a random variable for each d ∈ D. The term frequency can be used to estimate the joint probability distribution P(D, Sd, T), which is p(t, d, s) = T(t, d, s)/ND, where T(t, d, s) is the number of t in ds sentence s and ND is the total number of terms in D. ˆS represents the set of segments {ˆs1, ˆs2, ..., ˆsp} after segmentation and alignment among multiple documents, where the number of segments | ˆS| = p. A segment ˆsi of document d is a sequence of adjacent sentences in d. Since for different documents si may discuss different sub-topics, our goal is to cluster adjacent sentences in each document into segments, and align similar segments among documents, so that for different documents ˆsi is about the same sub-topic.",
        "The goal is to find the optimal topic segmentation and alignment mapping Segd(si) : {s1, s2, ..., snd } → {ˆs1, ˆs2, ..., ˆsp} and Alid(ˆsi) : {ˆs1, ˆs2, ..., ˆsp} → {ˆs1, ˆs2, ..., ˆsp}, for all d ∈ D, where ˆsi is ith segment with the constraint that only adjacent sentences can be mapped to the same segment, i.e. for d, {si, si+1, ..., sj} → {ˆsq}, where q ∈ {1, ..., p}, where p is the segment number, and if i > j, then for d, ˆsq is missing.",
        "After segmentation and alignment, random vector Sd becomes an aligned random variable ˆS.",
        "Thus, P(D, Sd, T) becomes P(D, ˆS, T).",
        "Term co-clustering is a technique that has been employed [10] to improve the accuracy of document clustering.",
        "We evaluate the effect of it for topic segmentation.",
        "A term t is mapped to exactly one term cluster.",
        "Term co-clustering involves simultaneously finding the optimal term clustering mapping Clu(t) : {t1, t2, ..., tl} → {ˆt1, ˆt2, ..., ˆtk}, where k ≤ l, l is the total number of words in all the documents, and k is the number of clusters. 4.",
        "METHODOLOGY We now describe a novel algorithm which can handle singledocument segmentation, shared topic detection, and multidocument segmentation and alignment based on MI or WMI. 4.1 Mutual Information MI I(X; Y ) is a quantity to measure the amount of information which is contained in two or more random variables [8, 10].",
        "For the case of two random variables, we have I(X; Y ) = x∈X y∈Y p(x, y)log p(x, y) p(x)p(y) , (1) Obviously, when random variables X and Y are independent, I(X; Y ) = 0.",
        "Thus, intuitively, the value of MI depends on how random variables are dependent on each other.",
        "The optimal co-clustering is the mapping Clux : X → ˆX and Cluy : Y → ˆY that minimizes the loss: I(X; Y ) − I( ˆX; ˆY ), which is equal to maximizing I( ˆX; ˆY ).",
        "This is the criterion of MI for clustering.",
        "In the case of topic segmentation, the two random variables are the term variable T and the segment variable S, and each sample is an occurrence of a term T = t in a particular segment S = s. I(T; S) is used to measure how dependent T and S are.",
        "However, I(T; S) cannot be computed for documents before segmentation, since we do not have a set of S due to the fact that sentences of Document d, si ∈ Sd, is not aligned with other documents.",
        "Thus, instead of minimizing the loss of MI, we can maximize MI after topic segmentation, computed as: I( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS p(ˆt, ˆs)log p(ˆt, ˆs) p(ˆt)p(ˆs) , (2) where p(ˆt, ˆs) are estimated by the term frequency tf of Term Cluster ˆt and Segment ˆs in the training set D. Note that here a segment ˆs includes sentences about the the same topic among all documents.",
        "The optimal solution is the mapping Clut : T → ˆT, Segd : Sd → ˆS , and Alid : ˆS → ˆS, which maximizes I( ˆT; ˆS). 4.2 Weighted Mutual Information In topic segmentation and alignment of multiple documents, if P(D, ˆS, T) is known, based on the marginal distributions P(D|T) and P( ˆS|T) for each term t ∈ T, we can categorize terms into four types in the data set: • Common stop words are common both along the dimensions of documents and segments. • Document-dependent stop words that depends on the personal writing style are common only along the dimension of segments for some documents. • Cue words are the most important elements for segmentation.",
        "They are common along the dimension of documents only for the same segment, and they are not common along the dimensions of segments. • Noisy words are other words which are not common along both dimensions.",
        "Entropy based on P(D|T) and P( ˆS|T) can be used to identify different types of terms.",
        "To reinforce the contribution of cue words in the MI computation, and simultaneously reduce the effect of the other three types of words, similar as the idea of the tf-idf weight [22], we use entropies of each term along the dimensions of document D and segment ˆS, i.e.",
        "ED(ˆt) and EˆS(ˆt), to compute the weight.",
        "A cue word usually has a large value of ED(ˆt) but a small value of EˆS(ˆt).",
        "We introduce term weights (or term cluster weights) wˆt = ( ED(ˆt) maxˆt ∈ ˆT (ED(ˆt )) )a (1 − EˆS(ˆt) maxˆt ∈ ˆT (EˆS(ˆt )) )b , (3) where ED(ˆt) = d∈D p(d|ˆt)log|D| 1 p(d|ˆt) , EˆS(ˆt) = ˆs∈ ˆS p(ˆs|ˆt)log| ˆS| 1 p(ˆs|ˆt) , and a > 0 and b > 0 are powers to adjust term weights.",
        "Usually a = 1 and b = 1 as default, and maxˆt ∈ ˆT (ED(ˆt )) and maxˆt ∈ ˆT (EˆS(ˆt )) are used to normalize the entropy values.",
        "Term cluster weights are used to adjust p(ˆt, ˆs), pw(ˆt, ˆs) = wˆtp(ˆt, ˆs) ˆt∈ ˆT ;ˆs∈ ˆS wˆtp(ˆt, ˆs) , (4) and Iw( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS pw(ˆt, ˆs)log pw(ˆt, ˆs) pw(ˆt)pw(ˆs) , (5) where pw(ˆt) and pw(ˆs) are marginal distributions of pw(ˆt, ˆs).",
        "However, since we do not know either the term weights or P(D, ˆS, T), we need to estimate them, but wˆt depends on p(ˆs|t) and ˆS, while ˆS and p(ˆs|t) also depend on wˆt that is still unknown.",
        "Thus, an iterative algorithm is required to estimate term weights wˆt and find the best segmentation and alignment to optimize the objective function Iw concurrently.",
        "After a document is segmented into sentences Input: Joint probability distribution P(D, Sd, T), number of text segments p ∈ {2, 3, ..., max(sd)}, number of term clusters k ∈ {2, 3, ..., l} (if k = l, no term co-clustering required), and weight type w ∈ {0, 1}, indicating to use I or Iw, respectively.",
        "Output: Mapping Clu, Seg, Ali, and term weights wˆt.",
        "Initialization: 0. i = 0.",
        "Initialize Clu (0) t , Seg (0) d , and Ali (0) d ; Initialize w (0) ˆt using Equation (6) if w = 1; Stage 1: 1.",
        "If |D| = 1, k = l, and w = 0, check all sequential segmentations of d into p segments and find the best one Segd(s) = argmaxˆsI( ˆT; ˆS), and return Segd; otherwise, if w = 1 and k = l, go to 3.1; Stage 2: 2.1 If k < l, for each term t, find the best cluster ˆt as Clu(i+1)(t) = argmaxˆtI( ˆT; ˆS(i)) based on Seg(i) and Ali(i); 2.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+1) d (Seg (i+1) d (s)) = argmaxˆsI( ˆT(i+1); ˆS) based on Clu(i+1)(t) if k < l or Clu(0)(t) if k = l; 2.3 i + +.",
        "If Clu, Seg, or Ali changed, go to 2.1; otherwise, if w = 0, return Clu(i), Seg(i), and Ali(i); else j = 0, go to 3.1; Stage 3: 3.1 Update w (i+j+1) ˆt based on Seg(i+j), Ali(i+j), and Clu(i) using Equation (3); 3.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+j+1) d (Seg (i+j+1) d (s)) = argmaxˆsIw( ˆT(i); ˆS) based on Clu(i) and w (i+j+1) ˆt ; 3.3 j + +.",
        "If Iw( ˆT; ˆS) changes, go to step 6; otherwise, stop and return Clu(i), Seg(i+j), Ali(i+j), and w (i+j) ˆt ; Figure 2: Algorithm: Topic segmentation and alignment based on MI or WMI. and each sentence is segmented into words, each word is stemmed.",
        "Then the joint probability distribution P(D, Sd, T) can be estimated.",
        "Finally, this distribution can be used to compute MI in our algorithm. 4.3 Iterative Greedy Algorithm Our goal is to maximize the objective function, I( ˆT; ˆS) or Iw( ˆT; ˆS), which can measure the dependence of term occurrences in different segments.",
        "Generally, first we do not know the estimate term weights, which depend on the optimal topic segmentation and alignment, and term clusters.",
        "Moreover, this problem is NP-hard [10], even though if we know the term weights.",
        "Thus, an iterative greedy algorithm is desired to find the best solution, even though probably only local maxima are reached.",
        "We present the iterative greedy algorithm in Figure 2 to find a local maximum of I( ˆT; ˆS) or Iw( ˆT; ˆS) with simultaneous term weight estimation.",
        "This algorithm can is iterative and greedy for multi-document cases or single-document cases with term weight estimation and/or term co-clustering.",
        "Otherwise, since it is just a one step algorithm to solve the task of single-document segmentation [6, 15, 25], the global maximum of MI is guaranteed.",
        "We will show later that term co-clustering reduces the accuracy of the results and is not necessary, and for singledocument segmentation, term weights are also not required. 4.3.1 Initialization In Step 0, the initial term clustering Clut and topic segmentation and alignment Segd and Alid are important to avoid local maxima and reduce the number of iterations.",
        "First, a good guess of term weights can be made by using the distributions of term frequency along sentences for each document and averaging them to get the initial values of wˆt: wt = ( ED(t) maxt ∈T (ED(t )) )(1 − ES(t) maxt ∈T (ES(t )) ), (6) where ES(t) = 1 |Dt| d∈Dt (1 − s∈Sd p(s|t)log|Sd| 1 p(s|t) ), where Dt is the set of documents which contain Term t. Then, for the initial segmentation Seg(0) , we can simply segment documents equally by sentences.",
        "Or we can find the optimal segmentation just for each document d which maximizes the WMI, Seg (0) d = argmaxˆsIw(T; ˆS), where w = w (0) ˆt .",
        "For the initial alignment Ali(0) , we can first assume that the order of segments for each d is the same.",
        "For the initial term clustering Clu(0) , first cluster labels can be set randomly, and after the first time of Step 3, a good initial term clustering is obtained. 4.3.2 Different Cases After initialization, there are three stages for different cases.",
        "Totally there are eight cases, |D| = 1 or |D| > 1, k = l or k < l, w = 0 or w = 1.",
        "Single document segmentation without term clustering and term weight estimation (|D| = 1, k = l, w = 0) only requires Stage 1 (Step 1).",
        "If term clustering is required (k < l), Stage 2 (Step 2.1, 2.2, and 2.3) is executed iteratively.",
        "If term weight estimation is required (w = 1), Stage 3 (Step 3.1, 3.2, and 3.3) is executed iteratively.",
        "If both are required (k < l, w = 1), Stage 2 and 3 run one after the other.",
        "For multi-document segmentation without term clustering and term weight estimation (|D| > 1, k = l, w = 0), only iteration of Step 2.2 and 2.3 are required.",
        "At Stage 1, the global maximum can be found based on I( ˆT; ˆS) using dynamic programming in Section 4.4.",
        "Simultaneously finding a good term clustering and estimated term weights is impossible, since when moving a term to a new term cluster to maximize Iw( ˆT; ˆS), we do not know that the weight of this term should be the one of the new cluster or the old cluster.",
        "Thus, we first do term clustering at Stage 2, and then estimate term weights at Stage 3.",
        "At Stage 2, Step 2.1 is to find the best term clustering and Step 2.2 is to find the best segmentation.",
        "This cycle is repeated to find a local maximum based on MI I until it converges.",
        "The two steps are: (1) based on current term clustering Cluˆt, for each document d, the algorithm segments all the sentences Sd into p segments sequentially (some segments may be empty), and put them into the p segments ˆS of the whole training set D (all possible cases of different segmentation Segd and alignment Alid are checked) to find the optimal case, and (2) based on the current segmentation and alignment, for each term t, the algorithm finds the best term cluster of t based on the current segmentation Segd and alignment Alid.",
        "After finding a good term clustering, term weights are estimated if w = 1.",
        "At Stage 3, similar as Stage 2, Step 3.1 is term weight re-estimation and Step 3.2 is to find a better segmentation.",
        "They are repeated to find a local maximum based on WMI Iw until it converges.",
        "However, if the term clustering in Stage 2 is not accurate, then the term weight estimation at Stage 3 may have a bad result.",
        "Finally, at Step 3.3, this algorithm converges and return the output.",
        "This algorithm can handle both single-document and multi-document segmentation.",
        "It also can detect shared topics among documents by checking the proportion of overlapped sentences on the same topics, as described in Sec 5.2. 4.4 Algorithm Optimization In many previous works on segmentation, dynamic programming is a technique used to maximize the objective function.",
        "Similarly, at Step 1, 2.2, and 3.2 of our algorithm, we can use dynamic programming.",
        "For Stage 1, using dynamic programming can still find the global optimum, but for Stage 2 and Stage 3, we can only find the optimum for each step of topic segmentation and alignment of a document.",
        "Here we only show the dynamic programming for Step 3.2 using WMI (Step 1 and 2.2 are similar but they can use either I or Iw).",
        "There are two cases that are not shown in the algorithm in Figure 2: (a) single-document segmentation or multi-document segmentation with the same sequential order of segments, where alignment is not required, and (b) multi-document segmentation with different sequential orders of segments, where alignment is necessary.",
        "The alignment mapping function of the former case is simply just Alid(ˆsi) = ˆsi, while for the latter ones alignment mapping function Alid(ˆsi) = ˆsj, i and j may be different.",
        "The computational steps for the two cases are listed below: Case 1 (no alignment): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs) and partial pw(ˆs) without counting sentences from d. Then put sentences from i to j into Part k, and compute partial WMI PIw( ˆT; ˆsk(si, si+1, ..., sj)) ˆt∈ ˆT pw(ˆt, ˆsk)log pw(ˆt, ˆsk) pw(ˆt)pw(ˆsk) , where Alid(si, si+1, ..., sj) = k, k ∈ {1, 2, ..., p}, 1 ≤ i ≤ j ≤ nd, and Segd(sq) = ˆsk for all i ≤ q ≤ j. (2) Let M(sm, 1) = PIw( ˆT; ˆs1(s1, s2, ..., sm)).",
        "Then M(sm, L) = maxi[M(si−1, L − 1) + PIw( ˆT; ˆsL(si, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, and when i > m, no sentences are put into ˆsk when compute PIw (note PIw( ˆT; ˆs(si, ..., sm)) = 0 for single-document segmentation). (3) Finally M(snd , p) = maxi[M(si−1, p − 1)+ PIw( ˆT; ˆsp(si, ..., snd ))], where 1 ≤ i ≤ nd+1.",
        "The optimal Iw is found and the corresponding segmentation is the best.",
        "Case 2 (alignment required): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs), and partial pw(ˆs), and PIw( ˆT; ˆsk(si, si+1, ..., sj)) similarly as Case 1. (2) Let M(sm, 1, k) = PIw( ˆT; ˆsk(s1, s2, ..., sm)), where k ∈ {1, 2, ..., p}.",
        "Then M(sm, L, kL) = maxi,j[M(si−1, L − 1, kL/j) + PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, kL ∈ Set(p, L), which is the set of all p!",
        "L!(p−L)! combinations of L segments chosen from all p segments, j ∈ kL, the set of L segments chosen from all p segments, and kL/j is the combination of L − 1 segments in kL except Segment j. (3) Finally, M(snd , p, kp) = maxi,j[M(si−1, p − 1, kp/j) +PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., snd ))], where kp is just the combination of all p segments and 1 ≤ i ≤ nd + 1, which is the optimal Iw and the corresponding segmentation is the best.",
        "The steps of Case 1 and 2 are similar, except in Case 2, alignment is considered in addition to segmentation.",
        "First, basic items of probability for computing Iw are computed excluding Doc d, and then partial WMI by putting every possible sequential segment (including empty segment) of d into every segment of the set.",
        "Second, the optimal sum of PIw for L segments and the leftmost m sentences, M(sm, L), is found.",
        "Finally, the maximal WMI is found among different sums of M(sm, p − 1) and PIw for Segment p. 5.",
        "EXPERIMENTS In this section, single-document segmentation, shared topic detection, and multi-document segmentation will be tested.",
        "Different hyper parameters of our method are studied.",
        "For convenience, we refer to the method using I as MIk if w = 0, and Iw as WMIk if w = 2 or as WMIk if w = 1, where k is the number of term clusters, and if k = l, where l is the total number of terms, then no term clustering is required, i.e.",
        "MIl and WMIl. 5.1 Single-document Segmentation 5.1.1 Test Data and Evaluation The first data set we tested is a synthetic one used in previous research [6, 15, 25] and many other papers.",
        "It has 700 samples.",
        "Each is a concatenation of ten segments.",
        "Each segment is the first n sentence selected randomly from the Brown corpus, which is supposed to have a different topic from each other.",
        "Currently, the best results on this data set is achieved by Ji et.al. [15].",
        "To compare the performance of our methods, the criterion used widely in previous research is applied, instead of the unbiased criterion introduced in [20].",
        "It chooses a pair of words randomly.",
        "If they are in different segments (different) for the real segmentation (real), but predicted (pred) as in the same segment, it is a miss.",
        "If they are in the same segment (same), but predicted as in different segments, it is a false alarm.",
        "Thus, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, diff)p(diff|real) +p(false alarm|real, pred, same)p(same|real). 5.1.2 Experiment Results We tested the case when the number of segments is known.",
        "Table 1 shows the results of our methods with different hyper parameter values and three previous approaches, C99[25], U00[6], and ADDP03[15], on this data set when the segment number is known.",
        "In WMI for single-document segmentation, the term weights are computed as follows: wˆt = 1−EˆS(ˆt)/maxˆt ∈ ˆT (EˆS(ˆt )).",
        "For this case, our methods MIl and WMIl both outperform all the previous approaches.",
        "We compared our methods with ADDP03using one-sample one-sided t-test and p-values are shown in Table 2.",
        "From the p-values, we can see that mostly the differences are very Table 1: Average Error Rates of Single-document Segmentation Given Segment Numbers Known Range of n 3-11 3-5 6-8 9-11 Sample size 400 100 100 100 C99 12% 11% 10% 9% U00 10% 9% 7% 5% ADDP03 6.0% 6.8% 5.2% 4.3% MIl 4.68% 5.57% 2.59% 1.59% WMIl 4.94% 6.33% 2.76% 1.62% MI100 9.62% 12.92% 8.66% 6.67% Table 2: Single-document Segmentation: P-values of T-test on Error Rates Range of n 3-11 3-5 6-8 9-11 ADDP03, MIl 0.000 0.000 0.000 0.000 ADDP03, WMIl 0.000 0.099 0.000 0.000 MIl, WMIl 0.061 0.132 0.526 0.898 significant.",
        "We also compare the error rates between our two methods using two-sample two-sided t-test to check the hypothesis that they are equal.",
        "We cannot reject the hypothesis that they are equal, so the difference are not significant, even though all the error rates for MIl are smaller than WMIl.",
        "However, we can conclude that term weights contribute little in single-document segmentation.",
        "The results also show that MI using term co-clustering (k = 100) decreases the performance.",
        "We tested different number of term clusters, and found that the performance becomes better when the cluster number increases to reach l. WMIk<l has similar results that we did not show in the table.",
        "As mentioned before, using MI may be inconsistent on optimal boundaries given different numbers of segments.",
        "This situation occurs especially when the similarities among segments are quite different, i.e. some transitions are very obvious, while others are not.",
        "This is because usually a document is a hierarchical structure instead of only a sequential structure.",
        "When the segments are not at the same level, this situation may occur.",
        "Thus, a hierarchical topic segmentation approach is desired, and the structure highly depends on the number of segments for each internal node and the stop criteria of splitting.",
        "For this data set of singledocument segmentation, since it is just a synthetic set, which is just a concatenation of several segments about different topics, it is reasonable that approaches simply based on term frequency have a good performance.",
        "Usually for the tasks of segmenting coherent documents for sub-topics, the effectiveness decreases much. 5.2 Shared Topic Detection 5.2.1 Test Data and Evaluation The second data set contains 80 news articles from Google News.",
        "There are eight topics and each has 10 articles.",
        "We randomly split the set into subsets with different document numbers and each subset has all eight topics.",
        "We compare our approach MIl and WMIl with LDA [4].",
        "LDA treats a document in the data set as a bag of words, finds its distribution on topics, and its major topic.",
        "MIl and WMIl views each sentence as a bag of words and tag it with a topic label.",
        "Then for each pair of documents, LDA determines if they are on the same topic, while MIl and Table 3: Shared Topic Detection: Average Error Rates for Different Numbers of Documents in Each Subset #Doc 10 20 40 80 LDA 8.89% 16.33% 1.35% 0.60% MIl, θ = 0.6 4.17% 1.71% 1.47% 0.0% WMIl, θ = 0.8 18.6% 3.16% 1.92% 0.0% WMIl check whether the proportion overlapped sentences on the same topic is larger than the adjustable threshold θ.",
        "That is, in MIl and WMIl, for a pair of documents d, d , if [ s∈Sd,s ∈Sd 1(topics=topics )/min(|Sd|, |Sd|)] > θ, where Sd is the set of sentences of d, and |Sd| is the number of sentences of d, then d and d have the shared topic.",
        "For a pair of documents selected randomly, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, same)p(same|real) +p(false alarm|real, pred, diff)p(diff|real), where a miss means if they have the same topic (same) for the real case (real), but predicted (pred) as on the same topic.",
        "If they are on different topics (diff), but predicted as on the same topic, it is a false alarm. 5.2.2 Experiment Results The results are shown in Table 3.",
        "If most documents have different topics, in WMIl, the estimation of term weights in Equation (3) is not correct.",
        "Thus, WMIl is not expected to have a better performance than MIl, when most documents have different topics.",
        "When there are fewer documents in a subset with the same number of topics, more documents have different topics, so WMIl is more worse than MIl.",
        "We can see that for most cases MIl has a better (or at least similar) performance than LDA.",
        "After shared topic detection, multi-document segmentation of documents with the shared topics is able to be executed. 5.3 Multi-document Segmentation 5.3.1 Test Data and Evaluation For multi-document segmentation and alignment, our goal is to identify these segments about the same topic among multiple similar documents with shared topics.",
        "Using Iw is expected to perform better than I, since without term weights the result is affected seriously by document-dependent stop words and noisy words which depends on the personal writing style.",
        "It is more likely to treat the same segments of different documents as different segments under the effect of document-dependent stop words and noisy words.",
        "Term weights can reduce the effect of document-dependent stop words and noisy words by giving cue terms more weights.",
        "The data set for multi-document segmentation and alignment has 102 samples and 2264 sentences totally.",
        "Each is the introduction part of a lab report selected from the course of Biol 240W, Pennsylvania State University.",
        "Each sample has two segments, introduction of plant hormones and the content in the lab.",
        "The length range of samples is from two to 56 sentences.",
        "Some samples only have one part and some have a reverse order the these two segments.",
        "It is not hard to identify the boundary between two segments for human.",
        "We labelled each sentence manually for evaluation.",
        "The criterion of evaluation is just using the proportion of the number of sentences with wrong predicted segment labels in the total number of sentences in the whole training Table 4: Average Error Rates of Multi-document Segmentation Given Segment Numbers Known #Doc MIl WMIl k MIk WMIk 102 3.14% 2.78% 300 4.68% 6.58% 51 4.17% 3.63% 300 17.83% 22.84% 34 5.06% 4.12% 300 18.75% 20.95% 20 7.08% 5.42% 250 20.40% 21.83% 10 10.38% 7.89% 250 21.42% 21.91% 5 15.77% 11.64% 250 21.89% 22.59% 2 25.90% 23.18% 50 25.44% 25.49% 1 23.90% 24.82% 25 25.75% 26.15% Table 5: Multi-document Segmentation: P-values of T-test on Error Rates for MIl and WMIl #Doc 51 34 20 10 5 2 P-value 0.19 0.101 0.025 0.001 0.000 0.002 set as the error rate: p(error|predicted, real) = d∈D s∈Sd 1(predicteds=reals)/ d∈D nd.",
        "In order to show the benefits of multi-document segmentation and alignment, we compared our method with different parameters on different partitions of the same training set.",
        "Except the cases that the number of documents is 102 and one (they are special cases of using the whole set and the pure single-document segmentation), we randomly divided the training set into m partitions, and each has 51, 34, 20, 10, 5, and 2 document samples.",
        "Then we applied our methods on each partition and calculated the error rate of the whole training set.",
        "Each case was repeated for 10 times for computing the average error rates.",
        "For different partitions of the training set, different k values are used, since the number of terms increases when the document number in each partition increases. 5.3.2 Experiment Results From the experiment results in Table 4, we can see the following observations: (1) When the number of documents increases, all methods have better performances.",
        "Only from one to two documents, MIl has decrease a little.",
        "We can observe this from Figure 3 at the point of document number = 2.",
        "Most curves even have the worst results at this point.",
        "There are two reasons.",
        "First, because samples vote for the best multi-document segmentation and alignment, but if only two documents are compared with each other, the one with missing segments or a totally different sequence will affect the correct segmentation and alignment of the other.",
        "Second, as noted at the beginning of this section, if two documents have more document-dependent stop words or noisy words than cue words, then the algorithm may view them as two different segments and the other segment is missing.",
        "Generally, we can only expect a better performance when the number of documents is larger than the number of segments. (2) Except single-document segmentation, WMIl is always better than MIl, and when the number of documents is reaching one or increases to a very large number, their performances become closer.",
        "Table 5 shows p-values of twosample one-sided t-test between MIl and WMIl.",
        "We also can see this trend from p-values.",
        "When document number = 5, we reached the smallest p-value and the largest difference between error rates of MIl and WMIl.",
        "For single-document Table 6: Multi-document Segmentation: Average Error Rate for Document Number = 5 in Each Subset with Different Number of Term Clusters #Cluster 75 100 150 250 l MIk 24.67% 24.54% 23.91% 22.59% 15.77% segmentation, WMIl is even a little bit worse than MIl, which is similar as the results of the single-document segmentation on the first data set.",
        "The reason is that for singledocument segmentation, we cannot estimate term weights accurately, since multiple documents are unavailable. (3) Using term clustering usually gets worse results than MIl and WMIl. (4) Using term clustering in WMIk is even worse than in MIk, since in WMIk term clusters are found first using I before using Iw.",
        "If the term clusters are not correct, then the term weights are estimated worse, which may mislead the algorithm to reach even worse results.",
        "From the results we also found that in multi-document segmentation and alignment, most documents with missing segments and a reverse order are identified correctly.",
        "Table 6 illustrates the experiment results for the case of 20 partitions (each has five document samples) of the training set and topic segmentation and alignment using MIk with different numbers of term clusters k. Notice that when the number of term clusters increases, the error rate becomes smaller.",
        "Without term clustering, we have the best result.",
        "We did not show results for WMIk with term clustering, but the results are similar.",
        "We also tested WMIl with different hyper parameters of a and b to adjust term weights.",
        "The results are presented in Figure 3.",
        "It was shown that the default case WMIl : a = 1, b = 1 gave the best results for different partitions of the training set.",
        "We can see the trend that when the document number is very small or large, the difference between MIl : a = 0, b = 0 and WMIl : a = 1, b = 1 becomes quite small.",
        "When the document number is not large (about from 2 to 10), all the cases using term weights have better performances than MIl : a = 0, b = 0 without term weights, but when the document number becomes larger, the cases WMIl : a = 1, b = 0 and WMIl : a = 2, b = 1 become worse than MIl : a = 0, b = 0.",
        "When the document number becomes very large, they are even worse than cases with small document numbers.",
        "This means that a proper way to estimate term weights for the criterion of WMI is very important.",
        "Figure 4 shows the term weights learned from the whole training set.",
        "Four types of words are categorized roughly even though the transition among them are subtle.",
        "Figure 5 illustrates the change in (weighted) mutual information for MIl and WMIl.",
        "As expected, mutual information for MIl increases monotonically with the number of steps, while WMIl does not.",
        "Finally, MIl and WMIl are scalable, with computational complexity shown in Figure 6.",
        "One advantage for our approach based on MI is that removing stop words is not required.",
        "Another important advantage is that there are no necessary hyper parameters to adjust.",
        "In single-document segmentation, the performance based on MI is even better for that based on WMI, so no extra hyper parameter is required.",
        "In multi-document segmentation, we show in the experiment, a = 1 and b = 1 is the best.",
        "Our method gives more weights to cue terms.",
        "However, usually cue terms or sentences appear at the beginning of a segment, while the end of the segment may be 1 2 5 10 20 34 51 102 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 Document Number ErrorRate MIl :a=0,b=0 WMI l :a=1,b=1 WMI l :a=1,b=0 WMI l :a=2,b=1 Figure 3: Error rates for different hyper parameters of term weights. 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Normalized Document Entropy NormalizedSegmentEntropy Noisy words Cue words Common stop words Doc−dep stop words Figure 4: Term weights learned from the whole training set. 0 100 200 300 400 500 600 0.06 0.08 0.1 0.12 0.14 0.16 0.18 Number of Steps (Weighted)MutualInformation MI l WMI l Figure 5: Change in (weighted) MI for MIl and WMIl. 0 20 40 60 80 100 120 0 200 400 600 800 1000 1200 1400 1600 1800 2000 Document Number TimetoConverge(sec) MI l WMI l Figure 6: Time to converge for MIl and WMIl. much noisy.",
        "One possible solution is giving more weights to terms at the beginning of each segment.",
        "Moreover, when the length of segments are quite different, long segments have much higher term frequencies, so they may dominate the segmentation boundaries.",
        "Normalization of term frequencies versus the segment length may be useful. 6.",
        "CONCLUSIONS AND FUTURE WORK We proposed a novel method for multi-document topic segmentation and alignment based on weighted mutual information, which can also handle single-document cases.",
        "We used dynamic programming to optimize our algorithm.",
        "Our approach outperforms all the previous methods on singledocument cases.",
        "Moreover, we also showed that doing segmentation among multiple documents can improve the performance tremendously.",
        "Our results also illustrated that using weighted mutual information can utilize the information of multiple documents to reach a better performance.",
        "We only tested our method on limited data sets.",
        "More data sets especially complicated ones should be tested.",
        "More previous methods should be compared with.",
        "Moreover, natural segmentations like paragraphs are hints that can be used to find the optimal boundaries.",
        "Supervised learning also can be considered. 7.",
        "ACKNOWLEDGMENTS The authors want to thank Xiang Ji, and Prof. J. Scott Payne for their help. 8.",
        "REFERENCES [1] A. Banerjee, I. Ghillon, J. Ghosh, S. Merugu, and D. Modha.",
        "A generalized maximum entropy approach to bregman co-clustering and matrix approximation.",
        "In Proceedings of SIGKDD, 2004. [2] R. Bekkerman, R. El-Yaniv, and A. McCallum.",
        "Multi-way distributional clustering via pairwise interactions.",
        "In Proceedings of ICML, 2005. [3] D. M. Blei and P. J. Moreno.",
        "Topic segmentation with an aspect hidden markov model.",
        "In Proceedings of SIGIR, 2001. [4] D. M. Blei, A. Ng, and M. Jordan.",
        "Latent dirichlet allocation.",
        "Journal of Machine Learning Research, 3:993-1022, 2003. [5] T. Brants, F. Chen, and I. Tsochantaridis.",
        "Topic-based document segmentation with probabilistic latent semantic analysis.",
        "In Proceedings of CIKM, 2002. [6] F. Choi.",
        "Advances in domain indepedent linear text segmentation.",
        "In Proceedings of the NAACL, 2000. [7] H. Christensen, B. Kolluru, Y. Gotoh, and S. Renals.",
        "Maximum entropy segmentation of broadcast news.",
        "In Proceedings of ICASSP, 2005. [8] T. Cover and J. Thomas.",
        "Elements of Information Theory.",
        "John Wiley and Sons, New York, USA, 1991. [9] S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and R. Harshman.",
        "Indexing by latent semantic analysis.",
        "Journal of the American Society for Information Systems, 1990. [10] I. Dhillon, S. Mallela, and D. Modha.",
        "Information-theoretic co-clustering.",
        "In Proceedings of SIGKDD, 2003. [11] M. Hajime, H. Takeo, and O. Manabu.",
        "Text segmentation with multiple surface linguistic cues.",
        "In Proceedings of COLING-ACL, 1998. [12] T. K. Ho.",
        "Stop word location and identification for adaptive text recognition.",
        "International Journal of Document Analysis and Recognition, 3(1), August 2000. [13] T. Hofmann.",
        "Probabilistic latent semantic analysis.",
        "In Proceedings of the UAI99, 1999. [14] X. Ji and H. Zha.",
        "Correlating summarization of a pair of multilingual documents.",
        "In Proceedings of RIDE, 2003. [15] X. Ji and H. Zha.",
        "Domain-independent text segmentation using anisotropic diffusion and dynamic programming.",
        "In Proceedings of SIGIR, 2003. [16] X. Ji and H. Zha.",
        "Extracting shared topics of multiple documents.",
        "In Proceedings of the 7th PAKDD, 2003. [17] J. Lafferty, A. McCallum, and F. Pereira.",
        "Conditional random fields: Probabilistic models for segmenting and labeling sequence data.",
        "In Proceedings of ICML, 2001. [18] T. Li, S. Ma, and M. Ogihara.",
        "Entropy-based criterion in categorical clustering.",
        "In Proceedings of ICML, 2004. [19] A. McCallum, D. Freitag, and F. Pereira.",
        "Maximum entropy markov models for information extraction and segmentation.",
        "In Proceedings of ICML, 2000. [20] L. Pevzner and M. Hearst.",
        "A critique and improvement of an evaluation metric for text segmentation.",
        "Computational Linguistic, 28(1):19-36, 2002. [21] J. C. Reynar.",
        "Statistical models for topic segmentation.",
        "In Proceedings of ACL, 1999. [22] G. Salton and M. McGill.",
        "Introduction to Modern Information Retrieval.",
        "McGraw Hill, 1983. [23] B.",
        "Sun, Q. Tan, P. Mitra, and C. L. Giles.",
        "Extraction and search of chemical formulae in text documents on the web.",
        "In Proceedings of WWW, 2007. [24] B.",
        "Sun, D. Zhou, H. Zha, and J.",
        "Yen.",
        "Multi-task text segmentation and alignment based on weighted mutual information.",
        "In Proceedings of CIKM, 2006. [25] M. Utiyama and H. Isahara.",
        "A statistical model for domain-independent text segmentation.",
        "In Proceedings of the 39th ACL, 1999. [26] C. Wayne.",
        "Multilingual topic detection and tracking: Successful research enabled by corpora and evaluation.",
        "In Proceedings of LREC, 2000. [27] J. Yamron, I. Carp, L. Gillick, S. Lowe, and P. van Mulbregt.",
        "A hidden markov model approach to text segmentation and event tracking.",
        "In Proceedings of ICASSP, 1998. [28] H. Zha and X. Ji.",
        "Correlating multilingual documents via bipartite graph modeling.",
        "In Proceedings of SIGIR, 2002."
    ],
    "translated_text_sentences": [
        "La segmentación de temas con detección de temas compartidos y alineación de múltiples documentos Bingjun Sun*, Prasenjit Mitra*†, Hongyuan Zha‡, C. Lee Giles*†, John Yen*† *Departamento de Ciencias de la Computación e Ingeniería † Colegio de Ciencias de la Información y Tecnología Universidad Estatal de Pensilvania University Park, PA 16802 ‡ Colegio de Computación Instituto de Tecnología de Georgia Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu RESUMEN La detección y seguimiento de temas [26] y la segmentación de temas [15] juegan un papel importante en la captura de la información local y secuencial de los documentos.",
        "El trabajo previo en esta área suele centrarse en documentos individuales, aunque existen múltiples documentos similares en muchos dominios.",
        "En este artículo, presentamos un novedoso método no supervisado para la detección de temas compartidos y la segmentación de temas de múltiples documentos similares basado en la información mutua (MI) y la información mutua ponderada (WMI), que es una combinación de MI y pesos de términos.",
        "La idea básica es que la segmentación óptima maximiza MI (o WMI).",
        "Nuestro enfoque puede detectar temas compartidos entre documentos.",
        "Puede encontrar los límites óptimos en un documento y alinear segmentos entre documentos al mismo tiempo.",
        "También puede manejar la segmentación de un solo documento como un caso especial de la segmentación y alineación de varios documentos.",
        "Nuestros métodos pueden identificar y fortalecer términos de señal que pueden ser utilizados para la segmentación y eliminar parcialmente las palabras de parada mediante el uso de pesos de términos basados en la entropía aprendida de múltiples documentos.",
        "Nuestros resultados experimentales muestran que nuestro algoritmo funciona bien para las tareas de segmentación de un solo documento, detección de temas compartidos y segmentación de múltiples documentos.",
        "El uso de información de múltiples documentos puede mejorar enormemente el rendimiento de la segmentación de temas, y utilizar WMI es aún mejor que utilizar MI para la segmentación de múltiples documentos.",
        "Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información-Agrupación; H.3.1 [Almacenamiento y Recuperación de Información]: Análisis de Contenido e Indexación-Procesamiento Lingüístico; I.2.7 [Inteligencia Artificial]: Procesamiento de Lenguaje Natural-Análisis de Texto; I.5.3 [Reconocimiento de Patrones]: Algoritmos de Agrupación-Medidas de Similitud Términos Generales Algoritmos, Diseño, Experimentación 1.",
        "INTRODUCCIÓN Muchos investigadores han trabajado en la detección y seguimiento de temas (TDT) [26] y la segmentación de temas durante la última década.",
        "La segmentación de temas tiene como objetivo identificar los límites en un documento con la meta de capturar la estructura temática latente.",
        "Las tareas de segmentación de temas generalmente se dividen en dos categorías [15]: la segmentación de flujos de texto, donde se identifica la transición de temas, y la segmentación coherente de documentos, en la que los documentos se dividen en subtemas.",
        "La primera categoría tiene aplicaciones en el reconocimiento automático del habla, mientras que la segunda tiene más aplicaciones como la consulta parcial de texto en documentos extensos en la recuperación de información, resumen de texto y medición de calidad de múltiples documentos.",
        "La investigación previa en relación con TDT se enmarca en la primera categoría, centrada en el seguimiento de temas de datos de discurso de transmisión y texto de agencia de noticias, mientras que la segunda categoría no ha sido estudiada muy bien.",
        "Los enfoques tradicionales realizan la segmentación de temas en los documentos uno a la vez [15, 25, 6].",
        "La mayoría de ellos tienen un mal desempeño en tareas sutiles como la segmentación coherente de documentos [15].",
        "A menudo, los usuarios finales buscan documentos que tengan un contenido similar.",
        "Los motores de búsqueda, como Google, proporcionan enlaces para obtener páginas similares.",
        "A una granularidad más fina, los usuarios pueden estar buscando obtener secciones de un documento similares a una sección en particular que presumiblemente discute un tema de interés para los usuarios.",
        "Por lo tanto, la extensión de la segmentación de temas desde documentos individuales para identificar segmentos similares de múltiples documentos similares con el mismo tema es una dirección natural y necesaria, y se espera que la segmentación de temas de múltiples documentos tenga un mejor rendimiento ya que se utiliza más información.",
        "Los enfoques tradicionales que utilizan la medición de similitud basada en la frecuencia de términos generalmente parten del mismo supuesto de que un vocabulario similar tiende a estar en un segmento de tema coherente [15, 25, 6].",
        "Sin embargo, suelen tener el problema de identificar las palabras de parada.",
        "Por ejemplo, se eliminan las palabras de parada dependientes del documento junto con las palabras de parada genéricas en [15].",
        "Hay dos razones por las que no eliminamos las palabras de parada directamente.",
        "Primero, identificar las palabras vacías es otro problema [12] que requiere estimación en cada dominio.",
        "Eliminar palabras comunes de parada puede resultar en la pérdida de información útil en un dominio específico.",
        "Segundo, aunque se pueden identificar las palabras de parada, la clasificación estricta de palabras de parada y palabras no de parada no puede representar el cambio gradual en la cantidad de contenido informativo de cada palabra.",
        "Empleamos una clasificación suave utilizando pesos de términos.",
        "En este documento, consideramos el problema de la segmentación de temas como un problema de optimización utilizando técnicas de teoría de la información para encontrar los límites óptimos de un documento dado el número de segmentos de texto, de manera que se minimice la pérdida de información mutua (MI) (o información mutua ponderada (WMI)) después de la segmentación y alineación.",
        "Esto es igual a maximizar el MI (o WMI).",
        "El MI se centra en medir la diferencia entre segmentos, mientras que investigaciones anteriores se enfocaban en encontrar la similitud (por ejemplo, la distancia del coseno) entre segmentos [15, 25, 6].",
        "La alineación de temas de múltiples documentos similares se puede lograr agrupando las oraciones sobre el mismo tema en el mismo clúster.",
        "La segmentación de temas en un solo documento es solo un caso especial del problema de segmentación y alineación de temas en múltiples documentos.",
        "Los términos pueden ser co-agrupados como en [10] al mismo tiempo, dado el número de agrupaciones, pero nuestros resultados experimentales muestran que este método resulta en una peor segmentación (ver Tablas 1, 4 y 6).",
        "Por lo general, los lectores humanos pueden identificar la transición de temas basándose en palabras de señal, y pueden ignorar las palabras vacías.",
        "Inspirados por esto, asignamos a cada término (o grupo de términos) un peso basado en la entropía entre diferentes documentos y diferentes segmentos de documentos.",
        "No solo este enfoque puede aumentar la contribución de las palabras clave, sino que también puede disminuir el efecto de las palabras de parada comunes, palabras ruidosas y palabras de parada dependientes del documento.",
        "Estas palabras son comunes en un documento.",
        "Muchos métodos basados en la similitud de oraciones requieren que estas palabras sean eliminadas antes de que se pueda realizar la segmentación de temas [15].",
        "Nuestros resultados en la Figura 3 muestran que los pesos de los términos son útiles para la segmentación y alineación de temas en múltiples documentos.",
        "La principal contribución de este artículo es que introduce un método novedoso para la segmentación de temas utilizando MI y demuestra que este método funciona mejor que los criterios previamente utilizados.",
        "Además, hemos abordado el problema de la segmentación y alineación de temas en varios documentos, mientras que la mayoría de las investigaciones existentes se centraron en la segmentación de documentos individuales.",
        "La segmentación y alineación de múltiples documentos pueden utilizar información de documentos similares y mejorar significativamente el rendimiento de la segmentación de temas.",
        "Obviamente, nuestro enfoque puede manejar documentos individuales como un caso especial cuando varios documentos no están disponibles.",
        "Puede detectar temas compartidos entre documentos para determinar si son múltiples documentos sobre el mismo tema.",
        "También introducimos el nuevo criterio de WMI basado en pesos de términos aprendidos de múltiples documentos similares, lo cual puede mejorar aún más el rendimiento de la segmentación de temas.",
        "Proponemos un algoritmo voraz iterativo basado en programación dinámica y demostramos que funciona bien en la práctica.",
        "Algunos de nuestros trabajos anteriores están en [24].",
        "El resto de este documento está organizado de la siguiente manera: En la Sección 2, revisamos el trabajo relacionado.",
        "La sección 3 contiene una formulación del problema de segmentación de temas y alineación de múltiples documentos con co-clustering de términos, una revisión del criterio de MI para el clustering, y finalmente una introducción a WMI.",
        "En la Sección 4, primero proponemos el algoritmo voraz iterativo de segmentación y alineación de temas con co-clustering de términos, y luego describimos cómo el algoritmo puede ser optimizado mediante programación dinámica.",
        "En la Sección 5, se describen experimentos sobre la segmentación de documentos individuales, la detección de temas compartidos y la segmentación de documentos múltiples, y se presentan y discuten los resultados para evaluar el rendimiento de nuestro algoritmo.",
        "Las conclusiones y algunas direcciones futuras del trabajo de investigación se discuten en la Sección 6.2.",
        "TRABAJO PREVIO En general, los enfoques existentes para la segmentación de texto se dividen en dos categorías: aprendizaje supervisado [19, 17, 23] y aprendizaje no supervisado [3, 27, 5, 6, 15, 25, 21].",
        "El aprendizaje supervisado suele tener un buen rendimiento, ya que aprende funciones a partir de conjuntos de entrenamiento etiquetados.",
        "Sin embargo, obtener conjuntos de entrenamiento grandes con etiquetas manuales en oraciones de documentos suele ser prohibitivamente costoso, por lo que se prefieren enfoques no supervisados.",
        "Algunos modelos consideran la dependencia entre oraciones y secciones, como el Modelo Oculto de Markov [3, 27], el Modelo de Máxima Entropía de Markov [19] y los Campos Aleatorios Condicionales [17], mientras que muchos otros enfoques se basan en la cohesión léxica o la similitud de las oraciones [5, 6, 15, 25, 21].",
        "Algunos enfoques también se centran en las palabras clave como pistas de transiciones de tema [11].",
        "Si bien algunos métodos existentes solo consideran información en documentos individuales [6, 15], otros utilizan varios documentos [16, 14].",
        "No hay muchas obras en la última categoría, aunque se espera que el rendimiento de la segmentación sea mejor con la utilización de información de múltiples documentos.",
        "Investigaciones previas estudiaron métodos para encontrar temas compartidos [16] y segmentación y resumen de temas entre solo un par de documentos [14].",
        "La clasificación y agrupación de textos es un área de investigación relacionada que categoriza documentos en grupos utilizando métodos supervisados o no supervisados.",
        "La clasificación o agrupación temática es una dirección importante en esta área, especialmente el co-agrupamiento de documentos y términos, como LSA [9], PLSA [13], y enfoques basados en distancias y particionamiento de gráficos bipartitos [28] o máxima MI [2, 10], o máxima entropía [1, 18].",
        "Los criterios de estos enfoques pueden ser utilizados en el tema de la segmentación de temas.",
        "Algunos de esos métodos se han extendido al área de la segmentación de temas, como PLSA [5] y máxima entropía [7], pero hasta donde sabemos, el uso de MI para la segmentación de temas no ha sido estudiado. 3.",
        "FORMULACIÓN DEL PROBLEMA Nuestro objetivo es segmentar documentos y alinear los segmentos entre documentos (Figura 1).",
        "Sea T el conjunto de términos {t1, t2, ..., tl}, que aparecen en el conjunto no etiquetado de documentos D = {d1, d2, ..., dm}.",
        "Sea Sd el conjunto de oraciones para el documento d ∈ D, es decir, {s1, s2, ..., snd}.",
        "Tenemos una matriz tridimensional de frecuencia de términos, en la que las tres dimensiones son variables aleatorias de D, Sd y T. Sd en realidad es un vector aleatorio que incluye una variable aleatoria para cada d ∈ D. La frecuencia de términos se puede utilizar para estimar la distribución de probabilidad conjunta P(D, Sd, T), que es p(t, d, s) = T(t, d, s)/ND, donde T(t, d, s) es el número de t en la oración s de d y ND es el número total de términos en D. ˆS representa el conjunto de segmentos {ˆs1, ˆs2, ..., ˆsp} después de la segmentación y alineación entre varios documentos, donde el número de segmentos | ˆS| = p. Un segmento ˆsi del documento d es una secuencia de oraciones adyacentes en d. Dado que para diferentes documentos si pueden discutir diferentes subtemas, nuestro objetivo es agrupar oraciones adyacentes en cada documento en segmentos, y alinear segmentos similares entre documentos, de modo que para diferentes documentos ˆsi trate sobre el mismo subtema.",
        "El objetivo es encontrar la segmentación óptima de temas y el mapeo de alineación Segd(si) : {s1, s2, ..., snd } → {ˆs1, ˆs2, ..., ˆsp} y Alid(ˆsi) : {ˆs1, ˆs2, ..., ˆsp} → {ˆs1, ˆs2, ..., ˆsp}, para todo d ∈ D, donde ˆsi es el i-ésimo segmento con la restricción de que solo las oraciones adyacentes pueden ser asignadas al mismo segmento, es decir, para d, {si, si+1, ..., sj} → {ˆsq}, donde q ∈ {1, ..., p}, donde p es el número de segmento, y si i > j, entonces para d, ˆsq está ausente.",
        "Después de la segmentación y alineación, el vector aleatorio Sd se convierte en una variable aleatoria alineada ˆS.",
        "Por lo tanto, P(D, Sd, T) se convierte en P(D, ˆS, T).",
        "El término co-clustering es una técnica que se ha empleado [10] para mejorar la precisión del agrupamiento de documentos.",
        "Evaluamos el efecto de ello para la segmentación de temas.",
        "Un término t se asigna a exactamente un grupo de términos.",
        "El término co-clustering implica encontrar simultáneamente el mapeo óptimo de agrupación de términos Clu(t) : {t1, t2, ..., tl} → {ˆt1, ˆt2, ..., ˆtk}, donde k ≤ l, l es el número total de palabras en todos los documentos, y k es el número de clústeres. 4.",
        "METODOLOGÍA Ahora describimos un algoritmo novedoso que puede manejar la segmentación de un solo documento, la detección de temas compartidos y la segmentación y alineación de múltiples documentos basados en MI o WMI. 4.1 Información Mutua MI I(X; Y) es una cantidad que mide la cantidad de información contenida en dos o más variables aleatorias [8, 10].",
        "Para el caso de dos variables aleatorias, tenemos I(X; Y) = x∈X y∈Y p(x, y)log p(x, y) p(x)p(y) , (1) Obviamente, cuando las variables aleatorias X e Y son independientes, I(X; Y) = 0.",
        "Por lo tanto, de manera intuitiva, el valor de MI depende de cómo las variables aleatorias están relacionadas entre sí.",
        "La co-agrupación óptima es el mapeo Clux: X → ˆX y Cluy: Y → ˆY que minimiza la pérdida: I(X; Y) - I(ˆX; ˆY), lo cual es igual a maximizar I(ˆX; ˆY).",
        "Este es el criterio de MI para la agrupación.",
        "En el caso de la segmentación de temas, las dos variables aleatorias son la variable de término T y la variable de segmento S, y cada muestra es una ocurrencia de un término T = t en un segmento particular S = s. Se utiliza I(T; S) para medir qué tan dependientes son T y S.",
        "Sin embargo, no se puede calcular I(T; S) para documentos antes de la segmentación, ya que no tenemos un conjunto de S debido a que las oraciones del Documento d, si ∈ Sd, no están alineadas con otros documentos.",
        "Así, en lugar de minimizar la pérdida de MI, podemos maximizar MI después de la segmentación de temas, calculada como: I( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS p(ˆt, ˆs)log p(ˆt, ˆs) p(ˆt)p(ˆs) , (2) donde p(ˆt, ˆs) se estiman mediante la frecuencia de términos tf del Cluster de Términos ˆt y el Segmento ˆs en el conjunto de entrenamiento D. Nótese que aquí un segmento ˆs incluye oraciones sobre el mismo tema entre todos los documentos.",
        "La solución óptima es el mapeo Clut : T → ˆT, Segd : Sd → ˆS, y Alid : ˆS → ˆS, que maximiza I( ˆT; ˆS). 4.2 Información Mutua Ponderada En la segmentación de temas y alineación de múltiples documentos, si se conoce P(D, ˆS, T), basado en las distribuciones marginales P(D|T) y P( ˆS|T) para cada término t ∈ T, podemos categorizar los términos en cuatro tipos en el conjunto de datos: • Las palabras de parada comunes son comunes a lo largo de las dimensiones de documentos y segmentos. • Las palabras de parada dependientes del documento que dependen del estilo de escritura personal son comunes solo a lo largo de la dimensión de segmentos para algunos documentos. • Las palabras clave son los elementos más importantes para la segmentación.",
        "Son comunes a lo largo de la dimensión de los documentos solo para el mismo segmento, y no son comunes a lo largo de las dimensiones de los segmentos. • Las palabras ruidosas son otras palabras que no son comunes a lo largo de ambas dimensiones.",
        "La entropía basada en P(D|T) y P( ˆS|T) se puede utilizar para identificar diferentes tipos de términos.",
        "Para reforzar la contribución de las palabras clave en el cálculo de MI, y al mismo tiempo reducir el efecto de los otros tres tipos de palabras, similar a la idea del peso tf-idf [22], utilizamos las entropías de cada término a lo largo de las dimensiones del documento D y del segmento ˆS, es decir,",
        "ED(ˆt) y EˆS(ˆt), para calcular el peso.",
        "Una palabra de señal generalmente tiene un valor alto de ED(ˆt) pero un valor bajo de EˆS(ˆt).",
        "Introducimos los pesos de términos (o pesos de grupos de términos) wˆt = ( ED(ˆt) maxˆt ∈ ˆT (ED(ˆt )) )a (1 − EˆS(ˆt) maxˆt ∈ ˆT (EˆS(ˆt )) )b , (3) donde ED(ˆt) = d∈D p(d|ˆt)log|D| 1 p(d|ˆt) , EˆS(ˆt) = ˆs∈ ˆS p(ˆs|ˆt)log| ˆS| 1 p(ˆs|ˆt) , y a > 0 y b > 0 son potencias para ajustar los pesos de términos.",
        "Normalmente a = 1 y b = 1 por defecto, y se utilizan maxˆt ∈ ˆT (ED(ˆt )) y maxˆt ∈ ˆT (EˆS(ˆt )) para normalizar los valores de entropía.",
        "Los pesos de los grupos de términos se utilizan para ajustar p(ˆt, ˆs), pw(ˆt, ˆs) = wˆtp(ˆt, ˆs) ˆt∈ ˆT ;ˆs∈ ˆS wˆtp(ˆt, ˆs) , (4) e Iw( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS pw(ˆt, ˆs)log pw(ˆt, ˆs) pw(ˆt)pw(ˆs) , (5) donde pw(ˆt) y pw(ˆs) son distribuciones marginales de pw(ˆt, ˆs).",
        "Sin embargo, dado que no conocemos ni los pesos de los términos ni P(D, ˆS, T), necesitamos estimarlos, pero wˆt depende de p(ˆs|t) y ˆS, mientras que ˆS y p(ˆs|t) también dependen de wˆt, que aún es desconocido.",
        "Por lo tanto, se requiere un algoritmo iterativo para estimar los pesos de los términos wˆt y encontrar la mejor segmentación y alineación para optimizar la función objetivo Iw de manera concurrente.",
        "Después de que un documento se segmenta en oraciones, se introduce: la distribución de probabilidad conjunta P(D, Sd, T), el número de segmentos de texto p ∈ {2, 3, ..., máx(sd)}, el número de grupos de términos k ∈ {2, 3, ..., l} (si k = l, no se requiere co-clustering de términos) y el tipo de peso w ∈ {0, 1}, indicando usar I o Iw, respectivamente.",
        "Mapeo de Clu, Seg, Ali y pesos de términos wˆt.",
        "Inicialización: 0. i = 0.",
        "Inicializar Clu (0) t, Seg (0) d y Ali (0) d; Inicializar w (0) ˆt usando la Ecuación (6) si w = 1; Etapa 1: 1.",
        "Si |D| = 1, k = l y w = 0, verifica todas las segmentaciones secuenciales de d en p segmentos y encuentra la mejor Segd(s) = argmaxˆsI( ˆT; ˆS), y devuelve Segd; de lo contrario, si w = 1 y k = l, ve a 3.1; Etapa 2: 2.1 Si k < l, para cada término t, encuentra el mejor clúster ˆt como Clu(i+1)(t) = argmaxˆtI( ˆT; ˆS(i)) basado en Seg(i) y Ali(i); 2.2 Para cada d, verifica todas las segmentaciones secuenciales de d en p segmentos con mapeo s → ˆs → ˆs, y encuentra la mejor Ali (i+1) d (Seg (i+1) d (s)) = argmaxˆsI( ˆT(i+1); ˆS) basado en Clu(i+1)(t) si k < l o Clu(0)(t) si k = l; 2.3 i + +.",
        "Si Clu, Seg o Ali cambian, ve a 2.1; de lo contrario, si w = 0, devuelve Clu(i), Seg(i) y Ali(i); de lo contrario, j = 0, ve a 3.1; Etapa 3: 3.1 Actualiza w (i+j+1) ˆt basado en Seg(i+j), Ali(i+j) y Clu(i) usando la Ecuación (3); 3.2 Para cada d, verifica todas las segmentaciones secuenciales de d en p segmentos con mapeo s → ˆs → ˆs, y encuentra la mejor Ali (i+j+1) d (Seg (i+j+1) d (s)) = argmaxˆsIw( ˆT(i); ˆS) basado en Clu(i) y w (i+j+1) ˆt; 3.3 j + +.",
        "Si Iw( ˆT; ˆS) cambia, ve al paso 6; de lo contrario, detente y devuelve Clu(i), Seg(i+j), Ali(i+j), y w (i+j) ˆt; Figura 2: Algoritmo: Segmentación y alineación de temas basada en MI o WMI. y cada oración se segmenta en palabras, cada palabra se reduce a su raíz.",
        "Entonces se puede estimar la distribución de probabilidad conjunta P(D, Sd, T).",
        "Finalmente, esta distribución se puede utilizar para calcular la MI en nuestro algoritmo. 4.3 Algoritmo Voraz Iterativo Nuestro objetivo es maximizar la función objetivo, I( ˆT; ˆS) o Iw( ˆT; ˆS), que puede medir la dependencia de las ocurrencias de términos en diferentes segmentos.",
        "Generalmente, primero no conocemos los pesos estimados de los términos, los cuales dependen de la segmentación y alineación óptimas de los temas, y los grupos de términos.",
        "Además, este problema es NP-duro [10], incluso si conocemos los pesos de los términos.",
        "Por lo tanto, se desea un algoritmo voraz iterativo para encontrar la mejor solución, aunque probablemente solo se alcancen máximos locales.",
        "Presentamos el algoritmo voraz iterativo en la Figura 2 para encontrar un máximo local de I( ˆT; ˆS) o Iw( ˆT; ˆS) con estimación simultánea de pesos de términos.",
        "Este algoritmo es iterativo y codicioso para casos de múltiples documentos o casos de un solo documento con estimación de peso de términos y/o co-clustering de términos.",
        "De lo contrario, dado que es solo un algoritmo de un paso para resolver la tarea de segmentación de un solo documento [6, 15, 25], se garantiza el máximo global de MI.",
        "Mostraremos más adelante que la coagrupación de términos reduce la precisión de los resultados y no es necesaria, y para la segmentación de un solo documento, los pesos de los términos tampoco son requeridos. 4.3.1 Inicialización En el Paso 0, la agrupación inicial de términos Clut y la segmentación y alineación de temas Segd y Alid son importantes para evitar máximos locales y reducir el número de iteraciones.",
        "Primero, se puede hacer una buena estimación de los pesos de los términos utilizando las distribuciones de frecuencia de términos a lo largo de las oraciones para cada documento y promediándolas para obtener los valores iniciales de wˆt: wt = ( ED(t) maxt ∈T (ED(t )) )(1 − ES(t) maxt ∈T (ES(t )) ), (6) donde ES(t) = 1 |Dt| d∈Dt (1 − s∈Sd p(s|t)log|Sd| 1 p(s|t) ), donde Dt es el conjunto de documentos que contienen el término t. Luego, para la segmentación inicial Seg(0), simplemente podemos segmentar los documentos de manera equitativa por oraciones.",
        "O podemos encontrar la segmentación óptima solo para cada documento d que maximice el WMI, Seg (0) d = argmaxˆsIw(T; ˆS), donde w = w (0) ˆt.",
        "Para el alineamiento inicial Ali(0), podemos primero asumir que el orden de los segmentos para cada d es el mismo.",
        "Para el agrupamiento inicial de términos Clu(0), las etiquetas de los primeros grupos pueden establecerse al azar, y después de la primera vez del Paso 3, se obtiene un buen agrupamiento inicial de términos. 4.3.2 Diferentes Casos Después de la inicialización, hay tres etapas para diferentes casos.",
        "En total hay ocho casos, |D| = 1 o |D| > 1, k = l o k < l, w = 0 o w = 1.",
        "La segmentación de un solo documento sin agrupamiento de términos y estimación de peso de términos (|D| = 1, k = l, w = 0) solo requiere la Etapa 1 (Paso 1).",
        "Si se requiere el agrupamiento de términos (k < l), la Etapa 2 (Paso 2.1, 2.2 y 2.3) se ejecuta de forma iterativa.",
        "Si se requiere la estimación del peso del término (w = 1), la Etapa 3 (Paso 3.1, 3.2 y 3.3) se ejecuta de forma iterativa.",
        "Si ambos son requeridos (k < l, w = 1), la Etapa 2 y 3 se ejecutan una tras otra.",
        "Para la segmentación de múltiples documentos sin agrupamiento de términos y estimación de peso de términos (|D| > 1, k = l, w = 0), solo se requiere la iteración de los Pasos 2.2 y 2.3.",
        "En la Etapa 1, el máximo global se puede encontrar basándose en I( ˆT; ˆS) utilizando programación dinámica en la Sección 4.4.",
        "Es imposible encontrar simultáneamente un buen agrupamiento de términos y pesos estimados de términos, ya que al mover un término a un nuevo grupo de términos para maximizar Iw( ˆT; ˆS), no sabemos si el peso de este término debería ser el del nuevo grupo o el del antiguo.",
        "Por lo tanto, primero realizamos el agrupamiento de términos en la Etapa 2, y luego estimamos los pesos de los términos en la Etapa 3.",
        "En la Etapa 2, el Paso 2.1 consiste en encontrar el mejor agrupamiento de términos y el Paso 2.2 consiste en encontrar la mejor segmentación.",
        "Este ciclo se repite para encontrar un máximo local basado en MI I hasta que converge.",
        "Los dos pasos son: (1) basados en el agrupamiento de términos actual Cluˆt, para cada documento d, el algoritmo segmenta todas las oraciones Sd en p segmentos secuencialmente (algunos segmentos pueden estar vacíos), y los coloca en los p segmentos ˆS del conjunto de entrenamiento completo D (se verifican todos los casos posibles de segmentación diferente Segd y alineación Alid) para encontrar el caso óptimo, y (2) basado en la segmentación y alineación actual, para cada término t, el algoritmo encuentra el mejor grupo de términos de t basado en la segmentación actual Segd y la alineación Alid.",
        "Después de encontrar un buen agrupamiento de términos, se estiman los pesos de los términos si w = 1.",
        "En la Etapa 3, al igual que en la Etapa 2, el Paso 3.1 es la reestimación del peso del término y el Paso 3.2 es encontrar una mejor segmentación.",
        "Se repiten para encontrar un máximo local basado en WMI Iw hasta que converja.",
        "Sin embargo, si el agrupamiento en la Etapa 2 no es preciso, entonces la estimación de peso en la Etapa 3 puede tener un mal resultado.",
        "Finalmente, en el Paso 3.3, este algoritmo converge y devuelve la salida.",
        "Este algoritmo puede manejar tanto la segmentación de un solo documento como la de múltiples documentos.",
        "También puede detectar temas compartidos entre documentos al verificar la proporción de frases superpuestas sobre los mismos temas, como se describe en la Sección 5.2. 4.4 Optimización del algoritmo. En muchos trabajos anteriores sobre segmentación, la programación dinámica es una técnica utilizada para maximizar la función objetivo.",
        "De manera similar, en los Pasos 1, 2.2 y 3.2 de nuestro algoritmo, podemos utilizar programación dinámica.",
        "Para la Etapa 1, utilizando programación dinámica aún se puede encontrar el óptimo global, pero para la Etapa 2 y la Etapa 3, solo podemos encontrar el óptimo para cada paso de segmentación de temas y alineación de un documento.",
        "Aquí solo mostramos la programación dinámica para el Paso 3.2 utilizando WMI (el Paso 1 y 2.2 son similares pero pueden usar I o Iw).",
        "Hay dos casos que no se muestran en el algoritmo de la Figura 2: (a) segmentación de un solo documento o segmentación de múltiples documentos con el mismo orden secuencial de segmentos, donde no se requiere alineación, y (b) segmentación de múltiples documentos con diferentes órdenes secuenciales de segmentos, donde la alineación es necesaria.",
        "La función de mapeo de alineación del primer caso es simplemente Alid(ˆsi) = ˆsi, mientras que para los últimos casos la función de mapeo de alineación es Alid(ˆsi) = ˆsj, donde i y j pueden ser diferentes.",
        "Los pasos computacionales para los dos casos se enumeran a continuación: Caso 1 (sin alineación): Para cada documento d: (1) Calcular pw(ˆt), pw parcial(ˆt, ˆs) y pw parcial(ˆs) sin contar las oraciones de d. Luego, colocar las oraciones de i a j en la Parte k, y calcular WMI parcial PIw( ˆT; ˆsk(si, si+1, ..., sj)) ˆt∈ ˆT pw(ˆt, ˆsk)log pw(ˆt, ˆsk) pw(ˆt)pw(ˆsk) , donde Alid(si, si+1, ..., sj) = k, k ∈ {1, 2, ..., p}, 1 ≤ i ≤ j ≤ nd, y Segd(sq) = ˆsk para todo i ≤ q ≤ j. (2) Dejar que M(sm, 1) = PIw( ˆT; ˆs1(s1, s2, ..., sm)).",
        "Entonces M(sm, L) = maxi[M(si−1, L − 1) + PIw( ˆT; ˆsL(si, ..., sm))], donde 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, y cuando i > m, no se colocan oraciones en ˆsk al calcular PIw (nota PIw( ˆT; ˆs(si, ..., sm)) = 0 para la segmentación de un solo documento). (3) Finalmente M(snd , p) = maxi[M(si−1, p − 1)+ PIw( ˆT; ˆsp(si, ..., snd ))], donde 1 ≤ i ≤ nd+1.",
        "Se encuentra el Iw óptimo y la segmentación correspondiente es la mejor.",
        "Caso 2 (alineación requerida): Para cada documento d: (1) Calcular pw(ˆt), pw parcial(ˆt, ˆs), y pw parcial(ˆs), y PIw( ˆT; ˆsk(si, si+1, ..., sj)) de manera similar al Caso 1. (2) Sea M(sm, 1, k) = PIw( ˆT; ˆsk(s1, s2, ..., sm)), donde k ∈ {1, 2, ..., p}.",
        "Entonces M(sm, L, kL) = maxi,j[M(si−1, L − 1, kL/j) + PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., sm))], donde 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, kL ∈ Set(p, L), que es el conjunto de todos los p!",
        "Hay L!(p−L)! combinaciones de L segmentos elegidos de entre todos los p segmentos, j ∈ kL, el conjunto de L segmentos elegidos de entre todos los p segmentos, y kL/j es la combinación de L − 1 segmentos en kL excepto el Segmento j. (3) Finalmente, M(snd , p, kp) = maxi,j[M(si−1, p − 1, kp/j) +PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., snd ))], donde kp es simplemente la combinación de todos los p segmentos y 1 ≤ i ≤ nd + 1, que es el óptimo Iw y la segmentación correspondiente es la mejor.",
        "Los pasos de los Casos 1 y 2 son similares, excepto que en el Caso 2, se considera el alineamiento además de la segmentación.",
        "Primero, se calculan los elementos básicos de probabilidad para computar Iw excluyendo el documento d, y luego se calcula el WMI parcial colocando cada segmento secuencial posible (incluido el segmento vacío) de d en cada segmento del conjunto.",
        "Segundo, se encuentra la suma óptima de PIw para L segmentos y las m oraciones más a la izquierda, M(sm, L).",
        "Finalmente, se encuentra el WMI máximo entre diferentes sumas de M(sm, p − 1) y PIw para el Segmento p. 5.",
        "EXPERIMENTOS En esta sección se probará la segmentación de un solo documento, la detección de temas compartidos y la segmentación de múltiples documentos.",
        "Se estudian diferentes hiperparámetros de nuestro método.",
        "Para mayor comodidad, nos referimos al método utilizando I como MIk si w = 0, e Iw como WMIk si w = 2 o como WMIk si w = 1, donde k es el número de grupos de términos, y si k = l, donde l es el número total de términos, entonces no se requiere agrupación de términos, es decir,",
        "El primer conjunto de datos que probamos es uno sintético utilizado en investigaciones anteriores [6, 15, 25] y muchos otros artículos.",
        "Tiene 700 muestras.",
        "Cada uno es una concatenación de diez segmentos.",
        "Cada segmento es la primera oración seleccionada al azar de las n primeras oraciones del corpus Brown, que se supone que tienen un tema diferente entre sí.",
        "Actualmente, los mejores resultados en este conjunto de datos son logrados por Ji et al. [15].",
        "Para comparar el rendimiento de nuestros métodos, se aplica el criterio ampliamente utilizado en investigaciones anteriores en lugar del criterio imparcial introducido en [20].",
        "Elige un par de palabras al azar.",
        "Si se encuentran en segmentos diferentes (diferentes) para la segmentación real (real), pero se predicen (pred) como en el mismo segmento, es un error.",
        "Si están en el mismo segmento (mismo), pero se predice que están en segmentos diferentes, es una falsa alarma.",
        "Por lo tanto, la tasa de error se calcula utilizando la siguiente ecuación: p(err|real, pred) = p(miss|real, pred, diff)p(diff|real) + p(false alarm|real, pred, same)p(same|real). 5.1.2 Resultados del Experimento Probamos el caso cuando se conoce el número de segmentos.",
        "La Tabla 1 muestra los resultados de nuestros métodos con diferentes valores de hiperparámetros y tres enfoques anteriores, C99[25], U00[6] y ADDP03[15], en este conjunto de datos cuando se conoce el número de segmento.",
        "En WMI para la segmentación de un solo documento, los pesos de los términos se calculan de la siguiente manera: wˆt = 1−EˆS(ˆt)/maxˆt ∈ ˆT (EˆS(ˆt)).",
        "Para este caso, nuestros métodos MIl y WMIl superan a todos los enfoques anteriores.",
        "Comparamos nuestros métodos con ADDP03 utilizando una prueba t de una muestra unilateral y los valores de p se muestran en la Tabla 2.",
        "A partir de los valores p, podemos ver que en su mayoría las diferencias son muy significativas.",
        "También comparamos las tasas de error entre nuestros dos métodos utilizando una prueba t de dos muestras de dos colas para verificar la hipótesis de que son iguales.",
        "No podemos rechazar la hipótesis de que son iguales, por lo que las diferencias no son significativas, a pesar de que todas las tasas de error para MIl son más pequeñas que las de WMIl.",
        "Sin embargo, podemos concluir que los pesos de los términos contribuyen poco en la segmentación de un solo documento.",
        "Los resultados también muestran que el uso de MI con co-clustering de términos (k = 100) disminuye el rendimiento.",
        "Probamos diferentes números de grupos de términos y encontramos que el rendimiento mejora cuando el número de grupos aumenta hasta llegar a l. WMIk<l tiene resultados similares que no mostramos en la tabla.",
        "Como se mencionó anteriormente, el uso de MI puede ser inconsistente en los límites óptimos dados diferentes números de segmentos.",
        "Esta situación ocurre especialmente cuando las similitudes entre los segmentos son bastante diferentes, es decir, algunas transiciones son muy obvias, mientras que otras no lo son.",
        "Esto se debe a que normalmente un documento es una estructura jerárquica en lugar de una estructura únicamente secuencial.",
        "Cuando los segmentos no están en el mismo nivel, esta situación puede ocurrir.",
        "Por lo tanto, se desea un enfoque de segmentación jerárquica de temas, y la estructura depende en gran medida del número de segmentos para cada nodo interno y del criterio de parada de la división.",
        "Para este conjunto de datos de segmentación de un solo documento, dado que es solo un conjunto sintético, que es simplemente una concatenación de varios segmentos sobre diferentes temas, es razonable que enfoques simplemente basados en la frecuencia de términos tengan un buen rendimiento.",
        "Normalmente, para las tareas de segmentación de documentos coherentes en subtemas, la efectividad disminuye mucho. 5.2 Detección de Temas Compartidos 5.2.1 Datos de Prueba y Evaluación El segundo conjunto de datos contiene 80 artículos de noticias de Google News.",
        "Hay ocho temas y cada uno tiene 10 artículos.",
        "Dividimos aleatoriamente el conjunto en subconjuntos con diferentes números de documentos y cada subconjunto tiene los ocho temas.",
        "Comparamos nuestro enfoque MIl y WMIl con LDA [4].",
        "LDA trata un documento en el conjunto de datos como una bolsa de palabras, encuentra su distribución en temas y su tema principal.",
        "MIl y WMIl ven cada oración como un conjunto de palabras y la etiquetan con una etiqueta de tema.",
        "Luego, para cada par de documentos, LDA determina si están en el mismo tema, mientras que MIl y Table 3: Detección de Temas Compartidos: Tasas de Error Promedio para Diferentes Números de Documentos en Cada Subconjunto #Doc 10 20 40 80 LDA 8.89% 16.33% 1.35% 0.60% MIl, θ = 0.6 4.17% 1.71% 1.47% 0.0% WMIl, θ = 0.8 18.6% 3.16% 1.92% 0.0% WMIl comprueba si la proporción de oraciones superpuestas en el mismo tema es mayor que el umbral ajustable θ.",
        "Es decir, en MIl y WMIl, para un par de documentos d, d , si [ s∈Sd,s ∈Sd 1(temas=temas )/min(|Sd|, |Sd|)] > θ, donde Sd es el conjunto de oraciones de d, y |Sd| es el número de oraciones de d, entonces d y d comparten el tema.",
        "Para un par de documentos seleccionados al azar, la tasa de error se calcula utilizando la siguiente ecuación: p(err|real, pred) = p(miss|real, pred, same)p(same|real) + p(false alarm|real, pred, diff)p(diff|real), donde un error significa si tienen el mismo tema (same) para el caso real (real), pero se predice (pred) como en un tema diferente.",
        "Si están en diferentes temas (diff), pero se predicen como en el mismo tema, es una falsa alarma. 5.2.2 Resultados del Experimento Los resultados se muestran en la Tabla 3.",
        "Si la mayoría de los documentos tienen diferentes temas, en WMIl, la estimación de los pesos de términos en la Ecuación (3) no es correcta.",
        "Por lo tanto, no se espera que WMIL tenga un mejor rendimiento que MIL cuando la mayoría de los documentos tienen temas diferentes.",
        "Cuando hay menos documentos en un subconjunto con el mismo número de temas, más documentos tienen temas diferentes, por lo que WMIl es peor que MIl.",
        "Podemos ver que en la mayoría de los casos, MIl tiene un rendimiento mejor (o al menos similar) que LDA.",
        "Después de la detección de temas compartidos, se puede llevar a cabo la segmentación de documentos con los temas compartidos. 5.3 Segmentación de múltiples documentos 5.3.1 Datos de prueba y evaluación Para la segmentación y alineación de múltiples documentos, nuestro objetivo es identificar estos segmentos sobre el mismo tema entre varios documentos similares con temas compartidos.",
        "Se espera que al utilizar pesos de términos, Iw funcione mejor que I, ya que sin pesos de términos, el resultado se ve seriamente afectado por palabras de parada dependientes del documento y palabras ruidosas que dependen del estilo de escritura personal.",
        "Es más probable tratar los mismos segmentos de diferentes documentos como segmentos diferentes bajo el efecto de palabras de parada dependientes del documento y palabras ruidosas.",
        "Los pesos de términos pueden reducir el efecto de las palabras de parada dependientes del documento y las palabras ruidosas al darle más peso a los términos de señal.",
        "El conjunto de datos para la segmentación y alineación de múltiples documentos tiene un total de 102 muestras y 2264 oraciones.",
        "Cada uno es la parte de introducción de un informe de laboratorio seleccionado del curso de Biol 240W, Universidad Estatal de Pensilvania.",
        "Cada muestra tiene dos segmentos, la introducción de las hormonas vegetales y el contenido en el laboratorio.",
        "El rango de longitud de las muestras va desde dos hasta 56 oraciones.",
        "Algunas muestras solo tienen una parte y algunas tienen un orden inverso de estos dos segmentos.",
        "No es difícil identificar el límite entre dos segmentos para los humanos.",
        "Etiquetamos cada oración manualmente para evaluación.",
        "El criterio de evaluación consiste en utilizar la proporción del número de oraciones con etiquetas de segmento incorrectas predichas en el número total de oraciones en toda la Tabla de entrenamiento.",
        "Para mostrar los beneficios de la segmentación y alineación de múltiples documentos, comparamos nuestro método con diferentes parámetros en diferentes particiones del mismo conjunto de entrenamiento.",
        "Excepto los casos en los que el número de documentos es 102 y uno (que son casos especiales de uso del conjunto completo y la segmentación pura de un solo documento), dividimos aleatoriamente el conjunto de entrenamiento en m particiones, y cada una tiene 51, 34, 20, 10, 5 y 2 muestras de documentos.",
        "Luego aplicamos nuestros métodos en cada partición y calculamos la tasa de error de todo el conjunto de entrenamiento.",
        "Cada caso se repitió 10 veces para calcular las tasas de error promedio.",
        "Para diferentes particiones del conjunto de entrenamiento, se utilizan diferentes valores de k, ya que el número de términos aumenta cuando el número de documentos en cada partición aumenta. 5.3.2 Resultados del Experimento De los resultados del experimento en la Tabla 4, podemos ver las siguientes observaciones: (1) Cuando el número de documentos aumenta, todos los métodos tienen un mejor rendimiento.",
        "Solo de uno a dos documentos, el MIL ha disminuido un poco.",
        "Podemos observar esto en la Figura 3 en el punto del número de documento = 2.",
        "La mayoría de las curvas incluso tienen los peores resultados en este punto.",
        "Hay dos razones.",
        "Primero, porque las muestras votan por la mejor segmentación y alineación de múltiples documentos, pero si solo se comparan dos documentos entre sí, aquel con segmentos faltantes o una secuencia totalmente diferente afectará la correcta segmentación y alineación del otro.",
        "Segundo, como se señaló al principio de esta sección, si dos documentos tienen más palabras de parada dependientes del documento o palabras ruidosas que palabras clave, entonces el algoritmo puede considerarlos como dos segmentos diferentes y faltar el otro segmento.",
        "Generalmente, solo podemos esperar un mejor rendimiento cuando el número de documentos es mayor que el número de segmentos. (2) Excepto en la segmentación de un solo documento, WMIl siempre es mejor que MIl, y cuando el número de documentos se acerca a uno o aumenta a un número muy grande, sus rendimientos se vuelven más cercanos.",
        "La Tabla 5 muestra los valores p de la prueba t de dos muestras unilaterales entre MIl y WMIl.",
        "También podemos observar esta tendencia a partir de los valores de p.",
        "Cuando el número de documento es igual a 5, alcanzamos el valor p más pequeño y la mayor diferencia entre las tasas de error de MIl y WMIl.",
        "Para la Tabla 6 de un solo documento: Segmentación de múltiples documentos: Tasa de error promedio para el Documento Número = 5 en cada subconjunto con Diferente Número de Agrupaciones de Términos #Agrupaciones 75 100 150 250 l MIk 24.67% 24.54% 23.91% 22.59% 15.77% de segmentación, WMIl es incluso un poco peor que MIl, lo cual es similar a los resultados de la segmentación de un solo documento en el primer conjunto de datos.",
        "La razón es que para la segmentación de un solo documento, no podemos estimar con precisión los pesos de los términos, ya que no tenemos disponibles varios documentos. (3) El uso de agrupamiento de términos generalmente produce resultados peores que MIl y WMIl. (4) El uso de agrupamiento de términos en WMIk es aún peor que en MIk, ya que en WMIk los grupos de términos se encuentran primero utilizando I antes que Iw.",
        "Si los grupos de términos no son correctos, entonces los pesos de los términos se estiman peor, lo que puede llevar a que el algoritmo obtenga resultados aún peores.",
        "De los resultados también encontramos que en la segmentación y alineación de múltiples documentos, la mayoría de los documentos con segmentos faltantes y en orden inverso son identificados correctamente.",
        "La Tabla 6 ilustra los resultados del experimento para el caso de 20 particiones (cada una con cinco muestras de documentos) del conjunto de entrenamiento y la segmentación y alineación de temas utilizando MIk con diferentes números de grupos de términos k. Observa que cuando el número de grupos de términos aumenta, la tasa de error disminuye.",
        "Sin agrupamiento de términos, obtenemos el mejor resultado.",
        "No mostramos resultados para WMIk con agrupamiento de términos, pero los resultados son similares.",
        "También probamos WMIL con diferentes hiperparámetros de a y b para ajustar los pesos de los términos.",
        "Los resultados se presentan en la Figura 3.",
        "Se demostró que el caso predeterminado WMIl: a = 1, b = 1 dio los mejores resultados para diferentes particiones del conjunto de entrenamiento.",
        "Podemos observar la tendencia de que cuando el número de documentos es muy pequeño o grande, la diferencia entre MIl: a = 0, b = 0 y WMIl: a = 1, b = 1 se vuelve bastante pequeña.",
        "Cuando el número de documentos no es grande (aproximadamente de 2 a 10), todos los casos que utilizan pesos de términos tienen un mejor rendimiento que MIl: a = 0, b = 0 sin pesos de términos, pero cuando el número de documentos aumenta, los casos WMIl: a = 1, b = 0 y WMIl: a = 2, b = 1 empeoran en comparación con MIl: a = 0, b = 0.",
        "Cuando el número de documentos se vuelve muy grande, son aún peores que los casos con números de documentos pequeños.",
        "Esto significa que es muy importante encontrar una forma adecuada de estimar los pesos de los términos para el criterio de WMI.",
        "La Figura 4 muestra los pesos de los términos aprendidos de todo el conjunto de entrenamiento.",
        "Cuatro tipos de palabras se categorizan de manera aproximada aunque la transición entre ellas es sutil.",
        "La Figura 5 ilustra el cambio en la información mutua (ponderada) para MIl y WMIl.",
        "Como era de esperar, la información mutua para MIl aumenta de forma monótona con el número de pasos, mientras que WMIl no lo hace.",
        "Finalmente, MIl y WMIl son escalables, con complejidad computacional mostrada en la Figura 6.",
        "Una ventaja de nuestro enfoque basado en MI es que no es necesario eliminar las palabras vacías.",
        "Otra ventaja importante es que no hay hiperparámetros necesarios que ajustar.",
        "En la segmentación de un solo documento, el rendimiento basado en MI es aún mejor que el basado en WMI, por lo que no se requiere un hiperparámetro adicional.",
        "En la segmentación de múltiples documentos, mostramos en el experimento que a = 1 y b = 1 es lo mejor.",
        "Nuestro método otorga más peso a los términos de señal.",
        "Sin embargo, generalmente los términos o frases de señal aparecen al principio de un segmento, mientras que el final del segmento puede ser 1 2 5 10 20 34 51 102 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 Número de Documento Tasa de Error MIl :a=0,b=0 WMI l :a=1,b=1 WMI l :a=1,b=0 WMI l :a=2,b=1 Figura 3: Tasas de error para diferentes hiperparámetros de pesos de términos. 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Entropía de Documento Normalizada Entropía de Segmento Normalizada Palabras ruidosas Palabras de señal Palabras de parada comunes Palabras de parada de Doc-dep Figura 4: Pesos de términos aprendidos de todo el conjunto de entrenamiento. 0 100 200 300 400 500 600 0.06 0.08 0.1 0.12 0.14 0.16 0.18 Número de Pasos (Ponderado) Información Mutua MI l WMI l Figura 5: Cambio en la MI (ponderada) para MIl y WMIl. 0 20 40 60 80 100 120 0 200 400 600 800 1000 1200 1400 1600 1800 2000 Número de Documento Tiempo de Convergencia (seg) MI l WMI l Figura 6: Tiempo de convergencia para MIl y WMIl. muy ruidoso.",
        "Una posible solución es dar más peso a los términos al principio de cada segmento.",
        "Además, cuando la longitud de los segmentos es bastante diferente, los segmentos largos tienen frecuencias de términos mucho más altas, por lo que pueden dominar los límites de segmentación.",
        "La normalización de las frecuencias de términos en relación con la longitud del segmento puede ser útil. 6.",
        "CONCLUSIONES Y TRABAJO FUTURO Propusimos un método novedoso para la segmentación y alineación de temas en múltiples documentos basado en información mutua ponderada, que también puede manejar casos de un solo documento.",
        "Utilizamos programación dinámica para optimizar nuestro algoritmo.",
        "Nuestro enfoque supera a todos los métodos anteriores en casos de un solo documento.",
        "Además, también demostramos que realizar segmentación entre varios documentos puede mejorar el rendimiento enormemente.",
        "Nuestros resultados también demostraron que el uso de la información mutua ponderada puede aprovechar la información de varios documentos para lograr un mejor rendimiento.",
        "Solo probamos nuestro método en conjuntos de datos limitados.",
        "Se deben probar más conjuntos de datos, especialmente los complicados.",
        "Deberían compararse más métodos anteriores.",
        "Además, las segmentaciones naturales como los párrafos son pistas que se pueden utilizar para encontrar los límites óptimos.",
        "El aprendizaje supervisado también puede ser considerado. 7.",
        "AGRADECIMIENTOS Los autores desean agradecer a Xiang Ji y al Prof. J. Scott Payne por su ayuda. 8.",
        "REFERENCIAS [1] A. Banerjee, I. Ghillon, J. Ghosh, S. Merugu y D. Modha.",
        "Un enfoque generalizado de entropía máxima para la coagrupación de Bregman y la aproximación de matrices.",
        "En Actas de SIGKDD, 2004. [2] R. Bekkerman, R. El-Yaniv y A. McCallum.",
        "Agrupación distribucional de múltiples vías a través de interacciones por pares.",
        "En Actas de ICML, 2005. [3] D. M. Blei y P. J. Moreno.",
        "Segmentación de temas con un modelo oculto de Markov de aspectos.",
        "En Actas de SIGIR, 2001. [4] D. M. Blei, A. Ng y M. Jordan.",
        "Asignación latente de Dirichlet.",
        "Revista de Investigación en Aprendizaje Automático, 3:993-1022, 2003. [5] T. Brants, F. Chen e I. Tsochantaridis.",
        "Segmentación de documentos basada en temas con análisis semántico latente probabilístico.",
        "En Actas de CIKM, 2002. [6] F. Choi.",
        "Avances en la segmentación lineal de texto independiente del dominio.",
        "En Actas de la NAACL, 2000. [7] H. Christensen, B. Kolluru, Y. Gotoh y S. Renals.",
        "Segmentación de máxima entropía de noticias de transmisión.",
        "En Actas de ICASSP, 2005. [8] T. Cover y J. Thomas.",
        "Elementos de la teoría de la información.",
        "John Wiley and Sons, Nueva York, EE. UU., 1991. [9] S. Deerwester, S. Dumais, G. Furnas, T. Landauer y R. Harshman.",
        "Indexación mediante análisis semántico latente.",
        "Revista de la Sociedad Americana de Sistemas de Información, 1990. [10] I. Dhillon, S. Mallela y D. Modha.",
        "Co-clustering informativo-teórico.",
        "En Actas de SIGKDD, 2003. [11] M. Hajime, H. Takeo y O. Manabu.",
        "Segmentación de texto con múltiples señales lingüísticas superficiales.",
        "En Actas de COLING-ACL, 1998. [12] T. K. Ho.",
        "Detención de palabras y identificación para el reconocimiento de texto adaptativo.",
        "Revista Internacional de Análisis y Reconocimiento de Documentos, 3(1), agosto de 2000. [13] T. Hofmann.",
        "Análisis semántico latente probabilístico.",
        "En Actas de la UAI99, 1999. [14] X. Ji y H. Zha.",
        "Correlación de la sumarización de un par de documentos multilingües.",
        "En Actas de RIDE, 2003. [15] X. Ji y H. Zha.",
        "Segmentación de texto independiente del dominio utilizando difusión anisotrópica y programación dinámica.",
        "En Actas de SIGIR, 2003. [16] X. Ji y H. Zha.",
        "Extrayendo temas compartidos de múltiples documentos.",
        "En Actas de la 7ª PAKDD, 2003. [17] J. Lafferty, A. McCallum y F. Pereira.",
        "Campos aleatorios condicionales: Modelos probabilísticos para segmentar y etiquetar datos de secuencias.",
        "En Actas de ICML, 2001. [18] T. Li, S. Ma y M. Ogihara.",
        "Criterio basado en la entropía en la agrupación categórica.",
        "En Actas de ICML, 2004. [19] A. McCallum, D. Freitag y F. Pereira.",
        "Modelos de máxima entropía de Markov para extracción de información y segmentación.",
        "En Actas de ICML, 2000. [20] L. Pevzner y M. Hearst.",
        "Una crítica y mejora de una métrica de evaluación para la segmentación de texto.",
        "Lingüística Computacional, 28(1):19-36, 2002. [21] J. C. Reynar.",
        "Modelos estadísticos para la segmentación de temas.",
        "En Actas de ACL, 1999. [22] G. Salton y M. McGill.",
        "Introducción a la Recuperación de Información Moderna.",
        "McGraw Hill, 1983. [23] B. \n\nMcGraw Hill, 1983. [23] B.",
        "Sun, Q. Tan, P. Mitra y C. L. Giles.",
        "Extracción y búsqueda de fórmulas químicas en documentos de texto en la web.",
        "En Actas de WWW, 2007. [24] B.",
        "Sun, D. Zhou, H. Zha, y J.",
        "I'm sorry, but \"Yen\" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish?",
        "Segmentación y alineación de texto multitarea basada en información mutua ponderada.",
        "En Actas de CIKM, 2006. [25] M. Utiyama y H. Isahara.",
        "Un modelo estadístico para la segmentación de texto independiente del dominio.",
        "En Actas de la 39ª ACL, 1999. [26] C. Wayne.",
        "Detección y seguimiento de temas multilingües: Investigación exitosa habilitada por corpora y evaluación.",
        "En Actas de LREC, 2000. [27] J. Yamron, I. Carp, L. Gillick, S. Lowe y P. van Mulbregt.",
        "Un enfoque de modelo oculto de Markov para la segmentación de texto y seguimiento de eventos.",
        "En Actas de ICASSP, 1998. [28] H. Zha y X. Ji.",
        "Correlacionando documentos multilingües a través de modelado de gráficos bipartitos.",
        "En Actas de SIGIR, 2002."
    ],
    "error_count": 4,
    "keys": {
        "topic detection": {
            "translated_key": "detección de temas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Topic Segmentation with Shared <br>topic detection</br> and Alignment of Multiple Documents Bingjun Sun*, Prasenjit Mitra*† , Hongyuan Zha‡ , C. Lee Giles*† , John Yen*† *Department of Computer Science and Engineering † College of Information Sciences and Technology The Pennsylvania State University University Park, PA 16802 ‡ College of Computing The Georgia Institute of Technology Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu ABSTRACT <br>topic detection</br> and tracking [26] and topic segmentation [15] play an important role in capturing the local and sequential information of documents.",
                "Previous work in this area usually focuses on single documents, although similar multiple documents are available in many domains.",
                "In this paper, we introduce a novel unsupervised method for shared <br>topic detection</br> and topic segmentation of multiple similar documents based on mutual information (MI) and weighted mutual information (WMI) that is a combination of MI and term weights.",
                "The basic idea is that the optimal segmentation maximizes MI(or WMI).",
                "Our approach can detect shared topics among documents.",
                "It can find the optimal boundaries in a document, and align segments among documents at the same time.",
                "It also can handle single-document segmentation as a special case of the multi-document segmentation and alignment.",
                "Our methods can identify and strengthen cue terms that can be used for segmentation and partially remove stop words by using term weights based on entropy learned from multiple documents.",
                "Our experimental results show that our algorithm works well for the tasks of single-document segmentation, shared <br>topic detection</br>, and multi-document segmentation.",
                "Utilizing information from multiple documents can tremendously improve the performance of topic segmentation, and using WMI is even better than using MI for the multi-document segmentation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Clustering; H.3.1 [Information Storage and Retrieval]: Content Analysis and IndexingLinguistic processing; I.2.7 [Artificial Intelligence]: Natural Language Processing-Text analysis; I.5.3 [Pattern Recognition]: Clustering-Algorithms;Similarity measures General Terms Algorithms, Design, Experimentation 1.",
                "INTRODUCTION Many researchers have worked on <br>topic detection</br> and tracking (TDT) [26] and topic segmentation during the past decade.",
                "Topic segmentation intends to identify the boundaries in a document with the goal to capture the latent topical structure.",
                "Topic segmentation tasks usually fall into two categories [15]: text stream segmentation where topic transition is identified, and coherent document segmentation in which documents are split into sub-topics.",
                "The former category has applications in automatic speech recognition, while the latter one has more applications such as partial-text query of long documents in information retrieval, text summary, and quality measurement of multiple documents.",
                "Previous research in connection with TDT falls into the former category, targeted on topic tracking of broadcast speech data and newswire text, while the latter category has not been studied very well.",
                "Traditional approaches perform topic segmentation on documents one at a time [15, 25, 6].",
                "Most of them perform badly in subtle tasks like coherent document segmentation [15].",
                "Often, end-users seek documents that have the similar content.",
                "Search engines, like, Google, provide links to obtain similar pages.",
                "At a finer granularity, users may actually be looking to obtain sections of a document similar to a particular section that presumably discusses a topic of the users interest.",
                "Thus, the extension of topic segmentation from single documents to identifying similar segments from multiple similar documents with the same topic is a natural and necessary direction, and multi-document topic segmentation is expected to have a better performance since more information is utilized.",
                "Traditional approaches using similarity measurement based on term frequency generally have the same assumption that similar vocabulary tends to be in a coherent topic segment [15, 25, 6].",
                "However, they usually suffer the issue of identifying stop words.",
                "For example, additional document-dependent stop words are removed together with the generic stop words in [15].",
                "There are two reasons that we do not remove stop words directly.",
                "First, identifying stop words is another issue [12] that requires estimation in each domain.",
                "Removing common stop words may result in the loss of useful information in a specific domain.",
                "Second, even though stop words can be identified, hard classification of stop words and nonstop words cannot represent the gradually changing amount of information content of each word.",
                "We employ a soft classification using term weights.",
                "In this paper, we view the problem of topic segmentation as an optimization issue using information theoretic techniques to find the optimal boundaries of a document given the number of text segments so as to minimize the loss of mutual information (MI) (or a weighted mutual information (WMI)) after segmentation and alignment.",
                "This is equal to maximizing the MI (or WMI).",
                "The MI focuses on measuring the difference among segments whereas previous research focused on finding the similarity (e.g. cosine distance) of segments [15, 25, 6].",
                "Topic alignment of multiple similar documents can be achieved by clustering sentences on the same topic into the same cluster.",
                "Single-document topic segmentation is just a special case of the multi-document topic segmentation and alignment problem.",
                "Terms can be co-clustered as in [10] at the same time, given the number of clusters, but our experimental results show that this method results in a worse segmentation (see Tables 1, 4, and 6).",
                "Usually, human readers can identify topic transition based on cue words, and can ignore stop words.",
                "Inspired by this, we give each term (or term cluster) a weight based on entropy among different documents and different segments of documents.",
                "Not only can this approach increase the contribution of cue words, but it can also decrease the effect of common stop words, noisy word, and document-dependent stop words.",
                "These words are common in a document.",
                "Many methods based on sentence similarity require that these words are removed before topic segmentation can be performed [15].",
                "Our results in Figure 3 show that term weights are useful for multi-document topic segmentation and alignment.",
                "The major contribution of this paper is that it introduces a novel method for topic segmentation using MI and shows that this method performs better than previously used criteria.",
                "Also, we have addressed the problem of topic segmentation and alignment across multiple documents, whereas most existing research focused on segmentation of single documents.",
                "Multi-document segmentation and alignment can utilize information from similar documents and improves the performance of topic segmentation greatly.",
                "Obviously, our approach can handle single documents as a special case when multiple documents are unavailable.",
                "It can detect shared topics among documents to judge if they are multiple documents on the same topic.",
                "We also introduce the new criterion of WMI based on term weights learned from multiple similar documents, which can improve performance of topic segmentation further.",
                "We propose an iterative greedy algorithm based on dynamic programming and show that it works well in practice.",
                "Some of our prior work is in [24].",
                "The rest of this paper is organized as follows: In Section 2, we review related work.",
                "Section 3 contains a formulation of the problem of topic segmentation and alignment of multiple documents with term co-clustering, a review of the criterion of MI for clustering, and finally an introduction to WMI.",
                "In Section 4, we first propose the iterative greedy algorithm of topic segmentation and alignment with term co-clustering, and then describe how the algorithm can be optimized by usFigure 1: Illustration of multi-document segmentation and alignment. ing dynamic programming.",
                "In Section 5, experiments about single-document segmentation, shared <br>topic detection</br>, and multi-document segmentation are described, and results are presented and discussed to evaluate the performance of our algorithm.",
                "Conclusions and some future directions of the research work are discussed in Section 6. 2.",
                "PREVIOUS WORK Generally, the existing approaches to text segmentation fall into two categories: supervised learning [19, 17, 23] and unsupervised learning [3, 27, 5, 6, 15, 25, 21].",
                "Supervised learning usually has good performance, since it learns functions from labelled training sets.",
                "However, often getting large training sets with manual labels on document sentences is prohibitively expensive, so unsupervised approaches are desired.",
                "Some models consider dependence between sentences and sections, such as Hidden Markov Model [3, 27], Maximum Entropy Markov Model [19], and Conditional Random Fields [17], while many other approaches are based on lexical cohesion or similarity of sentences [5, 6, 15, 25, 21].",
                "Some approaches also focus on cue words as hints of topic transitions [11].",
                "While some existing methods only consider information in single documents [6, 15], others utilize multiple documents [16, 14].",
                "There are not many works in the latter category, even though the performance of segmentation is expected to be better with utilization of information from multiple documents.",
                "Previous research studied methods to find shared topics [16] and topic segmentation and summarization between just a pair of documents [14].",
                "Text classification and clustering is a related research area which categorizes documents into groups using supervised or unsupervised methods.",
                "Topical classification or clustering is an important direction in this area, especially co-clustering of documents and terms, such as LSA [9], PLSA [13], and approaches based on distances and bipartite graph partitioning [28] or maximum MI [2, 10], or maximum entropy [1, 18].",
                "Criteria of these approaches can be utilized in the issue of topic segmentation.",
                "Some of those methods have been extended into the area of topic segmentation, such as PLSA [5] and maximum entropy [7], but to our best knowledge, using MI for topic segmentation has not been studied. 3.",
                "PROBLEM FORMULATION Our goal is to segment documents and align the segments across documents (Figure 1).",
                "Let T be the set of terms {t1, t2, ..., tl}, which appear in the unlabelled set of documents D = {d1, d2, ..., dm}.",
                "Let Sd be the set of sentences for document d ∈ D, i.e. {s1, s2, ..., snd }.",
                "We have a 3D matrix of term frequency, in which the three dimensions are random variables of D, Sd, and T. Sd actually is a random vector including a random variable for each d ∈ D. The term frequency can be used to estimate the joint probability distribution P(D, Sd, T), which is p(t, d, s) = T(t, d, s)/ND, where T(t, d, s) is the number of t in ds sentence s and ND is the total number of terms in D. ˆS represents the set of segments {ˆs1, ˆs2, ..., ˆsp} after segmentation and alignment among multiple documents, where the number of segments | ˆS| = p. A segment ˆsi of document d is a sequence of adjacent sentences in d. Since for different documents si may discuss different sub-topics, our goal is to cluster adjacent sentences in each document into segments, and align similar segments among documents, so that for different documents ˆsi is about the same sub-topic.",
                "The goal is to find the optimal topic segmentation and alignment mapping Segd(si) : {s1, s2, ..., snd } → {ˆs1, ˆs2, ..., ˆsp} and Alid(ˆsi) : {ˆs1, ˆs2, ..., ˆsp} → {ˆs1, ˆs2, ..., ˆsp}, for all d ∈ D, where ˆsi is ith segment with the constraint that only adjacent sentences can be mapped to the same segment, i.e. for d, {si, si+1, ..., sj} → {ˆsq}, where q ∈ {1, ..., p}, where p is the segment number, and if i > j, then for d, ˆsq is missing.",
                "After segmentation and alignment, random vector Sd becomes an aligned random variable ˆS.",
                "Thus, P(D, Sd, T) becomes P(D, ˆS, T).",
                "Term co-clustering is a technique that has been employed [10] to improve the accuracy of document clustering.",
                "We evaluate the effect of it for topic segmentation.",
                "A term t is mapped to exactly one term cluster.",
                "Term co-clustering involves simultaneously finding the optimal term clustering mapping Clu(t) : {t1, t2, ..., tl} → {ˆt1, ˆt2, ..., ˆtk}, where k ≤ l, l is the total number of words in all the documents, and k is the number of clusters. 4.",
                "METHODOLOGY We now describe a novel algorithm which can handle singledocument segmentation, shared <br>topic detection</br>, and multidocument segmentation and alignment based on MI or WMI. 4.1 Mutual Information MI I(X; Y ) is a quantity to measure the amount of information which is contained in two or more random variables [8, 10].",
                "For the case of two random variables, we have I(X; Y ) = x∈X y∈Y p(x, y)log p(x, y) p(x)p(y) , (1) Obviously, when random variables X and Y are independent, I(X; Y ) = 0.",
                "Thus, intuitively, the value of MI depends on how random variables are dependent on each other.",
                "The optimal co-clustering is the mapping Clux : X → ˆX and Cluy : Y → ˆY that minimizes the loss: I(X; Y ) − I( ˆX; ˆY ), which is equal to maximizing I( ˆX; ˆY ).",
                "This is the criterion of MI for clustering.",
                "In the case of topic segmentation, the two random variables are the term variable T and the segment variable S, and each sample is an occurrence of a term T = t in a particular segment S = s. I(T; S) is used to measure how dependent T and S are.",
                "However, I(T; S) cannot be computed for documents before segmentation, since we do not have a set of S due to the fact that sentences of Document d, si ∈ Sd, is not aligned with other documents.",
                "Thus, instead of minimizing the loss of MI, we can maximize MI after topic segmentation, computed as: I( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS p(ˆt, ˆs)log p(ˆt, ˆs) p(ˆt)p(ˆs) , (2) where p(ˆt, ˆs) are estimated by the term frequency tf of Term Cluster ˆt and Segment ˆs in the training set D. Note that here a segment ˆs includes sentences about the the same topic among all documents.",
                "The optimal solution is the mapping Clut : T → ˆT, Segd : Sd → ˆS , and Alid : ˆS → ˆS, which maximizes I( ˆT; ˆS). 4.2 Weighted Mutual Information In topic segmentation and alignment of multiple documents, if P(D, ˆS, T) is known, based on the marginal distributions P(D|T) and P( ˆS|T) for each term t ∈ T, we can categorize terms into four types in the data set: • Common stop words are common both along the dimensions of documents and segments. • Document-dependent stop words that depends on the personal writing style are common only along the dimension of segments for some documents. • Cue words are the most important elements for segmentation.",
                "They are common along the dimension of documents only for the same segment, and they are not common along the dimensions of segments. • Noisy words are other words which are not common along both dimensions.",
                "Entropy based on P(D|T) and P( ˆS|T) can be used to identify different types of terms.",
                "To reinforce the contribution of cue words in the MI computation, and simultaneously reduce the effect of the other three types of words, similar as the idea of the tf-idf weight [22], we use entropies of each term along the dimensions of document D and segment ˆS, i.e.",
                "ED(ˆt) and EˆS(ˆt), to compute the weight.",
                "A cue word usually has a large value of ED(ˆt) but a small value of EˆS(ˆt).",
                "We introduce term weights (or term cluster weights) wˆt = ( ED(ˆt) maxˆt ∈ ˆT (ED(ˆt )) )a (1 − EˆS(ˆt) maxˆt ∈ ˆT (EˆS(ˆt )) )b , (3) where ED(ˆt) = d∈D p(d|ˆt)log|D| 1 p(d|ˆt) , EˆS(ˆt) = ˆs∈ ˆS p(ˆs|ˆt)log| ˆS| 1 p(ˆs|ˆt) , and a > 0 and b > 0 are powers to adjust term weights.",
                "Usually a = 1 and b = 1 as default, and maxˆt ∈ ˆT (ED(ˆt )) and maxˆt ∈ ˆT (EˆS(ˆt )) are used to normalize the entropy values.",
                "Term cluster weights are used to adjust p(ˆt, ˆs), pw(ˆt, ˆs) = wˆtp(ˆt, ˆs) ˆt∈ ˆT ;ˆs∈ ˆS wˆtp(ˆt, ˆs) , (4) and Iw( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS pw(ˆt, ˆs)log pw(ˆt, ˆs) pw(ˆt)pw(ˆs) , (5) where pw(ˆt) and pw(ˆs) are marginal distributions of pw(ˆt, ˆs).",
                "However, since we do not know either the term weights or P(D, ˆS, T), we need to estimate them, but wˆt depends on p(ˆs|t) and ˆS, while ˆS and p(ˆs|t) also depend on wˆt that is still unknown.",
                "Thus, an iterative algorithm is required to estimate term weights wˆt and find the best segmentation and alignment to optimize the objective function Iw concurrently.",
                "After a document is segmented into sentences Input: Joint probability distribution P(D, Sd, T), number of text segments p ∈ {2, 3, ..., max(sd)}, number of term clusters k ∈ {2, 3, ..., l} (if k = l, no term co-clustering required), and weight type w ∈ {0, 1}, indicating to use I or Iw, respectively.",
                "Output: Mapping Clu, Seg, Ali, and term weights wˆt.",
                "Initialization: 0. i = 0.",
                "Initialize Clu (0) t , Seg (0) d , and Ali (0) d ; Initialize w (0) ˆt using Equation (6) if w = 1; Stage 1: 1.",
                "If |D| = 1, k = l, and w = 0, check all sequential segmentations of d into p segments and find the best one Segd(s) = argmaxˆsI( ˆT; ˆS), and return Segd; otherwise, if w = 1 and k = l, go to 3.1; Stage 2: 2.1 If k < l, for each term t, find the best cluster ˆt as Clu(i+1)(t) = argmaxˆtI( ˆT; ˆS(i)) based on Seg(i) and Ali(i); 2.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+1) d (Seg (i+1) d (s)) = argmaxˆsI( ˆT(i+1); ˆS) based on Clu(i+1)(t) if k < l or Clu(0)(t) if k = l; 2.3 i + +.",
                "If Clu, Seg, or Ali changed, go to 2.1; otherwise, if w = 0, return Clu(i), Seg(i), and Ali(i); else j = 0, go to 3.1; Stage 3: 3.1 Update w (i+j+1) ˆt based on Seg(i+j), Ali(i+j), and Clu(i) using Equation (3); 3.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+j+1) d (Seg (i+j+1) d (s)) = argmaxˆsIw( ˆT(i); ˆS) based on Clu(i) and w (i+j+1) ˆt ; 3.3 j + +.",
                "If Iw( ˆT; ˆS) changes, go to step 6; otherwise, stop and return Clu(i), Seg(i+j), Ali(i+j), and w (i+j) ˆt ; Figure 2: Algorithm: Topic segmentation and alignment based on MI or WMI. and each sentence is segmented into words, each word is stemmed.",
                "Then the joint probability distribution P(D, Sd, T) can be estimated.",
                "Finally, this distribution can be used to compute MI in our algorithm. 4.3 Iterative Greedy Algorithm Our goal is to maximize the objective function, I( ˆT; ˆS) or Iw( ˆT; ˆS), which can measure the dependence of term occurrences in different segments.",
                "Generally, first we do not know the estimate term weights, which depend on the optimal topic segmentation and alignment, and term clusters.",
                "Moreover, this problem is NP-hard [10], even though if we know the term weights.",
                "Thus, an iterative greedy algorithm is desired to find the best solution, even though probably only local maxima are reached.",
                "We present the iterative greedy algorithm in Figure 2 to find a local maximum of I( ˆT; ˆS) or Iw( ˆT; ˆS) with simultaneous term weight estimation.",
                "This algorithm can is iterative and greedy for multi-document cases or single-document cases with term weight estimation and/or term co-clustering.",
                "Otherwise, since it is just a one step algorithm to solve the task of single-document segmentation [6, 15, 25], the global maximum of MI is guaranteed.",
                "We will show later that term co-clustering reduces the accuracy of the results and is not necessary, and for singledocument segmentation, term weights are also not required. 4.3.1 Initialization In Step 0, the initial term clustering Clut and topic segmentation and alignment Segd and Alid are important to avoid local maxima and reduce the number of iterations.",
                "First, a good guess of term weights can be made by using the distributions of term frequency along sentences for each document and averaging them to get the initial values of wˆt: wt = ( ED(t) maxt ∈T (ED(t )) )(1 − ES(t) maxt ∈T (ES(t )) ), (6) where ES(t) = 1 |Dt| d∈Dt (1 − s∈Sd p(s|t)log|Sd| 1 p(s|t) ), where Dt is the set of documents which contain Term t. Then, for the initial segmentation Seg(0) , we can simply segment documents equally by sentences.",
                "Or we can find the optimal segmentation just for each document d which maximizes the WMI, Seg (0) d = argmaxˆsIw(T; ˆS), where w = w (0) ˆt .",
                "For the initial alignment Ali(0) , we can first assume that the order of segments for each d is the same.",
                "For the initial term clustering Clu(0) , first cluster labels can be set randomly, and after the first time of Step 3, a good initial term clustering is obtained. 4.3.2 Different Cases After initialization, there are three stages for different cases.",
                "Totally there are eight cases, |D| = 1 or |D| > 1, k = l or k < l, w = 0 or w = 1.",
                "Single document segmentation without term clustering and term weight estimation (|D| = 1, k = l, w = 0) only requires Stage 1 (Step 1).",
                "If term clustering is required (k < l), Stage 2 (Step 2.1, 2.2, and 2.3) is executed iteratively.",
                "If term weight estimation is required (w = 1), Stage 3 (Step 3.1, 3.2, and 3.3) is executed iteratively.",
                "If both are required (k < l, w = 1), Stage 2 and 3 run one after the other.",
                "For multi-document segmentation without term clustering and term weight estimation (|D| > 1, k = l, w = 0), only iteration of Step 2.2 and 2.3 are required.",
                "At Stage 1, the global maximum can be found based on I( ˆT; ˆS) using dynamic programming in Section 4.4.",
                "Simultaneously finding a good term clustering and estimated term weights is impossible, since when moving a term to a new term cluster to maximize Iw( ˆT; ˆS), we do not know that the weight of this term should be the one of the new cluster or the old cluster.",
                "Thus, we first do term clustering at Stage 2, and then estimate term weights at Stage 3.",
                "At Stage 2, Step 2.1 is to find the best term clustering and Step 2.2 is to find the best segmentation.",
                "This cycle is repeated to find a local maximum based on MI I until it converges.",
                "The two steps are: (1) based on current term clustering Cluˆt, for each document d, the algorithm segments all the sentences Sd into p segments sequentially (some segments may be empty), and put them into the p segments ˆS of the whole training set D (all possible cases of different segmentation Segd and alignment Alid are checked) to find the optimal case, and (2) based on the current segmentation and alignment, for each term t, the algorithm finds the best term cluster of t based on the current segmentation Segd and alignment Alid.",
                "After finding a good term clustering, term weights are estimated if w = 1.",
                "At Stage 3, similar as Stage 2, Step 3.1 is term weight re-estimation and Step 3.2 is to find a better segmentation.",
                "They are repeated to find a local maximum based on WMI Iw until it converges.",
                "However, if the term clustering in Stage 2 is not accurate, then the term weight estimation at Stage 3 may have a bad result.",
                "Finally, at Step 3.3, this algorithm converges and return the output.",
                "This algorithm can handle both single-document and multi-document segmentation.",
                "It also can detect shared topics among documents by checking the proportion of overlapped sentences on the same topics, as described in Sec 5.2. 4.4 Algorithm Optimization In many previous works on segmentation, dynamic programming is a technique used to maximize the objective function.",
                "Similarly, at Step 1, 2.2, and 3.2 of our algorithm, we can use dynamic programming.",
                "For Stage 1, using dynamic programming can still find the global optimum, but for Stage 2 and Stage 3, we can only find the optimum for each step of topic segmentation and alignment of a document.",
                "Here we only show the dynamic programming for Step 3.2 using WMI (Step 1 and 2.2 are similar but they can use either I or Iw).",
                "There are two cases that are not shown in the algorithm in Figure 2: (a) single-document segmentation or multi-document segmentation with the same sequential order of segments, where alignment is not required, and (b) multi-document segmentation with different sequential orders of segments, where alignment is necessary.",
                "The alignment mapping function of the former case is simply just Alid(ˆsi) = ˆsi, while for the latter ones alignment mapping function Alid(ˆsi) = ˆsj, i and j may be different.",
                "The computational steps for the two cases are listed below: Case 1 (no alignment): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs) and partial pw(ˆs) without counting sentences from d. Then put sentences from i to j into Part k, and compute partial WMI PIw( ˆT; ˆsk(si, si+1, ..., sj)) ˆt∈ ˆT pw(ˆt, ˆsk)log pw(ˆt, ˆsk) pw(ˆt)pw(ˆsk) , where Alid(si, si+1, ..., sj) = k, k ∈ {1, 2, ..., p}, 1 ≤ i ≤ j ≤ nd, and Segd(sq) = ˆsk for all i ≤ q ≤ j. (2) Let M(sm, 1) = PIw( ˆT; ˆs1(s1, s2, ..., sm)).",
                "Then M(sm, L) = maxi[M(si−1, L − 1) + PIw( ˆT; ˆsL(si, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, and when i > m, no sentences are put into ˆsk when compute PIw (note PIw( ˆT; ˆs(si, ..., sm)) = 0 for single-document segmentation). (3) Finally M(snd , p) = maxi[M(si−1, p − 1)+ PIw( ˆT; ˆsp(si, ..., snd ))], where 1 ≤ i ≤ nd+1.",
                "The optimal Iw is found and the corresponding segmentation is the best.",
                "Case 2 (alignment required): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs), and partial pw(ˆs), and PIw( ˆT; ˆsk(si, si+1, ..., sj)) similarly as Case 1. (2) Let M(sm, 1, k) = PIw( ˆT; ˆsk(s1, s2, ..., sm)), where k ∈ {1, 2, ..., p}.",
                "Then M(sm, L, kL) = maxi,j[M(si−1, L − 1, kL/j) + PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, kL ∈ Set(p, L), which is the set of all p!",
                "L!(p−L)! combinations of L segments chosen from all p segments, j ∈ kL, the set of L segments chosen from all p segments, and kL/j is the combination of L − 1 segments in kL except Segment j. (3) Finally, M(snd , p, kp) = maxi,j[M(si−1, p − 1, kp/j) +PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., snd ))], where kp is just the combination of all p segments and 1 ≤ i ≤ nd + 1, which is the optimal Iw and the corresponding segmentation is the best.",
                "The steps of Case 1 and 2 are similar, except in Case 2, alignment is considered in addition to segmentation.",
                "First, basic items of probability for computing Iw are computed excluding Doc d, and then partial WMI by putting every possible sequential segment (including empty segment) of d into every segment of the set.",
                "Second, the optimal sum of PIw for L segments and the leftmost m sentences, M(sm, L), is found.",
                "Finally, the maximal WMI is found among different sums of M(sm, p − 1) and PIw for Segment p. 5.",
                "EXPERIMENTS In this section, single-document segmentation, shared <br>topic detection</br>, and multi-document segmentation will be tested.",
                "Different hyper parameters of our method are studied.",
                "For convenience, we refer to the method using I as MIk if w = 0, and Iw as WMIk if w = 2 or as WMIk if w = 1, where k is the number of term clusters, and if k = l, where l is the total number of terms, then no term clustering is required, i.e.",
                "MIl and WMIl. 5.1 Single-document Segmentation 5.1.1 Test Data and Evaluation The first data set we tested is a synthetic one used in previous research [6, 15, 25] and many other papers.",
                "It has 700 samples.",
                "Each is a concatenation of ten segments.",
                "Each segment is the first n sentence selected randomly from the Brown corpus, which is supposed to have a different topic from each other.",
                "Currently, the best results on this data set is achieved by Ji et.al. [15].",
                "To compare the performance of our methods, the criterion used widely in previous research is applied, instead of the unbiased criterion introduced in [20].",
                "It chooses a pair of words randomly.",
                "If they are in different segments (different) for the real segmentation (real), but predicted (pred) as in the same segment, it is a miss.",
                "If they are in the same segment (same), but predicted as in different segments, it is a false alarm.",
                "Thus, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, diff)p(diff|real) +p(false alarm|real, pred, same)p(same|real). 5.1.2 Experiment Results We tested the case when the number of segments is known.",
                "Table 1 shows the results of our methods with different hyper parameter values and three previous approaches, C99[25], U00[6], and ADDP03[15], on this data set when the segment number is known.",
                "In WMI for single-document segmentation, the term weights are computed as follows: wˆt = 1−EˆS(ˆt)/maxˆt ∈ ˆT (EˆS(ˆt )).",
                "For this case, our methods MIl and WMIl both outperform all the previous approaches.",
                "We compared our methods with ADDP03using one-sample one-sided t-test and p-values are shown in Table 2.",
                "From the p-values, we can see that mostly the differences are very Table 1: Average Error Rates of Single-document Segmentation Given Segment Numbers Known Range of n 3-11 3-5 6-8 9-11 Sample size 400 100 100 100 C99 12% 11% 10% 9% U00 10% 9% 7% 5% ADDP03 6.0% 6.8% 5.2% 4.3% MIl 4.68% 5.57% 2.59% 1.59% WMIl 4.94% 6.33% 2.76% 1.62% MI100 9.62% 12.92% 8.66% 6.67% Table 2: Single-document Segmentation: P-values of T-test on Error Rates Range of n 3-11 3-5 6-8 9-11 ADDP03, MIl 0.000 0.000 0.000 0.000 ADDP03, WMIl 0.000 0.099 0.000 0.000 MIl, WMIl 0.061 0.132 0.526 0.898 significant.",
                "We also compare the error rates between our two methods using two-sample two-sided t-test to check the hypothesis that they are equal.",
                "We cannot reject the hypothesis that they are equal, so the difference are not significant, even though all the error rates for MIl are smaller than WMIl.",
                "However, we can conclude that term weights contribute little in single-document segmentation.",
                "The results also show that MI using term co-clustering (k = 100) decreases the performance.",
                "We tested different number of term clusters, and found that the performance becomes better when the cluster number increases to reach l. WMIk<l has similar results that we did not show in the table.",
                "As mentioned before, using MI may be inconsistent on optimal boundaries given different numbers of segments.",
                "This situation occurs especially when the similarities among segments are quite different, i.e. some transitions are very obvious, while others are not.",
                "This is because usually a document is a hierarchical structure instead of only a sequential structure.",
                "When the segments are not at the same level, this situation may occur.",
                "Thus, a hierarchical topic segmentation approach is desired, and the structure highly depends on the number of segments for each internal node and the stop criteria of splitting.",
                "For this data set of singledocument segmentation, since it is just a synthetic set, which is just a concatenation of several segments about different topics, it is reasonable that approaches simply based on term frequency have a good performance.",
                "Usually for the tasks of segmenting coherent documents for sub-topics, the effectiveness decreases much. 5.2 Shared <br>topic detection</br> 5.2.1 Test Data and Evaluation The second data set contains 80 news articles from Google News.",
                "There are eight topics and each has 10 articles.",
                "We randomly split the set into subsets with different document numbers and each subset has all eight topics.",
                "We compare our approach MIl and WMIl with LDA [4].",
                "LDA treats a document in the data set as a bag of words, finds its distribution on topics, and its major topic.",
                "MIl and WMIl views each sentence as a bag of words and tag it with a topic label.",
                "Then for each pair of documents, LDA determines if they are on the same topic, while MIl and Table 3: Shared <br>topic detection</br>: Average Error Rates for Different Numbers of Documents in Each Subset #Doc 10 20 40 80 LDA 8.89% 16.33% 1.35% 0.60% MIl, θ = 0.6 4.17% 1.71% 1.47% 0.0% WMIl, θ = 0.8 18.6% 3.16% 1.92% 0.0% WMIl check whether the proportion overlapped sentences on the same topic is larger than the adjustable threshold θ.",
                "That is, in MIl and WMIl, for a pair of documents d, d , if [ s∈Sd,s ∈Sd 1(topics=topics )/min(|Sd|, |Sd|)] > θ, where Sd is the set of sentences of d, and |Sd| is the number of sentences of d, then d and d have the shared topic.",
                "For a pair of documents selected randomly, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, same)p(same|real) +p(false alarm|real, pred, diff)p(diff|real), where a miss means if they have the same topic (same) for the real case (real), but predicted (pred) as on the same topic.",
                "If they are on different topics (diff), but predicted as on the same topic, it is a false alarm. 5.2.2 Experiment Results The results are shown in Table 3.",
                "If most documents have different topics, in WMIl, the estimation of term weights in Equation (3) is not correct.",
                "Thus, WMIl is not expected to have a better performance than MIl, when most documents have different topics.",
                "When there are fewer documents in a subset with the same number of topics, more documents have different topics, so WMIl is more worse than MIl.",
                "We can see that for most cases MIl has a better (or at least similar) performance than LDA.",
                "After shared <br>topic detection</br>, multi-document segmentation of documents with the shared topics is able to be executed. 5.3 Multi-document Segmentation 5.3.1 Test Data and Evaluation For multi-document segmentation and alignment, our goal is to identify these segments about the same topic among multiple similar documents with shared topics.",
                "Using Iw is expected to perform better than I, since without term weights the result is affected seriously by document-dependent stop words and noisy words which depends on the personal writing style.",
                "It is more likely to treat the same segments of different documents as different segments under the effect of document-dependent stop words and noisy words.",
                "Term weights can reduce the effect of document-dependent stop words and noisy words by giving cue terms more weights.",
                "The data set for multi-document segmentation and alignment has 102 samples and 2264 sentences totally.",
                "Each is the introduction part of a lab report selected from the course of Biol 240W, Pennsylvania State University.",
                "Each sample has two segments, introduction of plant hormones and the content in the lab.",
                "The length range of samples is from two to 56 sentences.",
                "Some samples only have one part and some have a reverse order the these two segments.",
                "It is not hard to identify the boundary between two segments for human.",
                "We labelled each sentence manually for evaluation.",
                "The criterion of evaluation is just using the proportion of the number of sentences with wrong predicted segment labels in the total number of sentences in the whole training Table 4: Average Error Rates of Multi-document Segmentation Given Segment Numbers Known #Doc MIl WMIl k MIk WMIk 102 3.14% 2.78% 300 4.68% 6.58% 51 4.17% 3.63% 300 17.83% 22.84% 34 5.06% 4.12% 300 18.75% 20.95% 20 7.08% 5.42% 250 20.40% 21.83% 10 10.38% 7.89% 250 21.42% 21.91% 5 15.77% 11.64% 250 21.89% 22.59% 2 25.90% 23.18% 50 25.44% 25.49% 1 23.90% 24.82% 25 25.75% 26.15% Table 5: Multi-document Segmentation: P-values of T-test on Error Rates for MIl and WMIl #Doc 51 34 20 10 5 2 P-value 0.19 0.101 0.025 0.001 0.000 0.002 set as the error rate: p(error|predicted, real) = d∈D s∈Sd 1(predicteds=reals)/ d∈D nd.",
                "In order to show the benefits of multi-document segmentation and alignment, we compared our method with different parameters on different partitions of the same training set.",
                "Except the cases that the number of documents is 102 and one (they are special cases of using the whole set and the pure single-document segmentation), we randomly divided the training set into m partitions, and each has 51, 34, 20, 10, 5, and 2 document samples.",
                "Then we applied our methods on each partition and calculated the error rate of the whole training set.",
                "Each case was repeated for 10 times for computing the average error rates.",
                "For different partitions of the training set, different k values are used, since the number of terms increases when the document number in each partition increases. 5.3.2 Experiment Results From the experiment results in Table 4, we can see the following observations: (1) When the number of documents increases, all methods have better performances.",
                "Only from one to two documents, MIl has decrease a little.",
                "We can observe this from Figure 3 at the point of document number = 2.",
                "Most curves even have the worst results at this point.",
                "There are two reasons.",
                "First, because samples vote for the best multi-document segmentation and alignment, but if only two documents are compared with each other, the one with missing segments or a totally different sequence will affect the correct segmentation and alignment of the other.",
                "Second, as noted at the beginning of this section, if two documents have more document-dependent stop words or noisy words than cue words, then the algorithm may view them as two different segments and the other segment is missing.",
                "Generally, we can only expect a better performance when the number of documents is larger than the number of segments. (2) Except single-document segmentation, WMIl is always better than MIl, and when the number of documents is reaching one or increases to a very large number, their performances become closer.",
                "Table 5 shows p-values of twosample one-sided t-test between MIl and WMIl.",
                "We also can see this trend from p-values.",
                "When document number = 5, we reached the smallest p-value and the largest difference between error rates of MIl and WMIl.",
                "For single-document Table 6: Multi-document Segmentation: Average Error Rate for Document Number = 5 in Each Subset with Different Number of Term Clusters #Cluster 75 100 150 250 l MIk 24.67% 24.54% 23.91% 22.59% 15.77% segmentation, WMIl is even a little bit worse than MIl, which is similar as the results of the single-document segmentation on the first data set.",
                "The reason is that for singledocument segmentation, we cannot estimate term weights accurately, since multiple documents are unavailable. (3) Using term clustering usually gets worse results than MIl and WMIl. (4) Using term clustering in WMIk is even worse than in MIk, since in WMIk term clusters are found first using I before using Iw.",
                "If the term clusters are not correct, then the term weights are estimated worse, which may mislead the algorithm to reach even worse results.",
                "From the results we also found that in multi-document segmentation and alignment, most documents with missing segments and a reverse order are identified correctly.",
                "Table 6 illustrates the experiment results for the case of 20 partitions (each has five document samples) of the training set and topic segmentation and alignment using MIk with different numbers of term clusters k. Notice that when the number of term clusters increases, the error rate becomes smaller.",
                "Without term clustering, we have the best result.",
                "We did not show results for WMIk with term clustering, but the results are similar.",
                "We also tested WMIl with different hyper parameters of a and b to adjust term weights.",
                "The results are presented in Figure 3.",
                "It was shown that the default case WMIl : a = 1, b = 1 gave the best results for different partitions of the training set.",
                "We can see the trend that when the document number is very small or large, the difference between MIl : a = 0, b = 0 and WMIl : a = 1, b = 1 becomes quite small.",
                "When the document number is not large (about from 2 to 10), all the cases using term weights have better performances than MIl : a = 0, b = 0 without term weights, but when the document number becomes larger, the cases WMIl : a = 1, b = 0 and WMIl : a = 2, b = 1 become worse than MIl : a = 0, b = 0.",
                "When the document number becomes very large, they are even worse than cases with small document numbers.",
                "This means that a proper way to estimate term weights for the criterion of WMI is very important.",
                "Figure 4 shows the term weights learned from the whole training set.",
                "Four types of words are categorized roughly even though the transition among them are subtle.",
                "Figure 5 illustrates the change in (weighted) mutual information for MIl and WMIl.",
                "As expected, mutual information for MIl increases monotonically with the number of steps, while WMIl does not.",
                "Finally, MIl and WMIl are scalable, with computational complexity shown in Figure 6.",
                "One advantage for our approach based on MI is that removing stop words is not required.",
                "Another important advantage is that there are no necessary hyper parameters to adjust.",
                "In single-document segmentation, the performance based on MI is even better for that based on WMI, so no extra hyper parameter is required.",
                "In multi-document segmentation, we show in the experiment, a = 1 and b = 1 is the best.",
                "Our method gives more weights to cue terms.",
                "However, usually cue terms or sentences appear at the beginning of a segment, while the end of the segment may be 1 2 5 10 20 34 51 102 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 Document Number ErrorRate MIl :a=0,b=0 WMI l :a=1,b=1 WMI l :a=1,b=0 WMI l :a=2,b=1 Figure 3: Error rates for different hyper parameters of term weights. 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Normalized Document Entropy NormalizedSegmentEntropy Noisy words Cue words Common stop words Doc−dep stop words Figure 4: Term weights learned from the whole training set. 0 100 200 300 400 500 600 0.06 0.08 0.1 0.12 0.14 0.16 0.18 Number of Steps (Weighted)MutualInformation MI l WMI l Figure 5: Change in (weighted) MI for MIl and WMIl. 0 20 40 60 80 100 120 0 200 400 600 800 1000 1200 1400 1600 1800 2000 Document Number TimetoConverge(sec) MI l WMI l Figure 6: Time to converge for MIl and WMIl. much noisy.",
                "One possible solution is giving more weights to terms at the beginning of each segment.",
                "Moreover, when the length of segments are quite different, long segments have much higher term frequencies, so they may dominate the segmentation boundaries.",
                "Normalization of term frequencies versus the segment length may be useful. 6.",
                "CONCLUSIONS AND FUTURE WORK We proposed a novel method for multi-document topic segmentation and alignment based on weighted mutual information, which can also handle single-document cases.",
                "We used dynamic programming to optimize our algorithm.",
                "Our approach outperforms all the previous methods on singledocument cases.",
                "Moreover, we also showed that doing segmentation among multiple documents can improve the performance tremendously.",
                "Our results also illustrated that using weighted mutual information can utilize the information of multiple documents to reach a better performance.",
                "We only tested our method on limited data sets.",
                "More data sets especially complicated ones should be tested.",
                "More previous methods should be compared with.",
                "Moreover, natural segmentations like paragraphs are hints that can be used to find the optimal boundaries.",
                "Supervised learning also can be considered. 7.",
                "ACKNOWLEDGMENTS The authors want to thank Xiang Ji, and Prof. J. Scott Payne for their help. 8.",
                "REFERENCES [1] A. Banerjee, I. Ghillon, J. Ghosh, S. Merugu, and D. Modha.",
                "A generalized maximum entropy approach to bregman co-clustering and matrix approximation.",
                "In Proceedings of SIGKDD, 2004. [2] R. Bekkerman, R. El-Yaniv, and A. McCallum.",
                "Multi-way distributional clustering via pairwise interactions.",
                "In Proceedings of ICML, 2005. [3] D. M. Blei and P. J. Moreno.",
                "Topic segmentation with an aspect hidden markov model.",
                "In Proceedings of SIGIR, 2001. [4] D. M. Blei, A. Ng, and M. Jordan.",
                "Latent dirichlet allocation.",
                "Journal of Machine Learning Research, 3:993-1022, 2003. [5] T. Brants, F. Chen, and I. Tsochantaridis.",
                "Topic-based document segmentation with probabilistic latent semantic analysis.",
                "In Proceedings of CIKM, 2002. [6] F. Choi.",
                "Advances in domain indepedent linear text segmentation.",
                "In Proceedings of the NAACL, 2000. [7] H. Christensen, B. Kolluru, Y. Gotoh, and S. Renals.",
                "Maximum entropy segmentation of broadcast news.",
                "In Proceedings of ICASSP, 2005. [8] T. Cover and J. Thomas.",
                "Elements of Information Theory.",
                "John Wiley and Sons, New York, USA, 1991. [9] S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and R. Harshman.",
                "Indexing by latent semantic analysis.",
                "Journal of the American Society for Information Systems, 1990. [10] I. Dhillon, S. Mallela, and D. Modha.",
                "Information-theoretic co-clustering.",
                "In Proceedings of SIGKDD, 2003. [11] M. Hajime, H. Takeo, and O. Manabu.",
                "Text segmentation with multiple surface linguistic cues.",
                "In Proceedings of COLING-ACL, 1998. [12] T. K. Ho.",
                "Stop word location and identification for adaptive text recognition.",
                "International Journal of Document Analysis and Recognition, 3(1), August 2000. [13] T. Hofmann.",
                "Probabilistic latent semantic analysis.",
                "In Proceedings of the UAI99, 1999. [14] X. Ji and H. Zha.",
                "Correlating summarization of a pair of multilingual documents.",
                "In Proceedings of RIDE, 2003. [15] X. Ji and H. Zha.",
                "Domain-independent text segmentation using anisotropic diffusion and dynamic programming.",
                "In Proceedings of SIGIR, 2003. [16] X. Ji and H. Zha.",
                "Extracting shared topics of multiple documents.",
                "In Proceedings of the 7th PAKDD, 2003. [17] J. Lafferty, A. McCallum, and F. Pereira.",
                "Conditional random fields: Probabilistic models for segmenting and labeling sequence data.",
                "In Proceedings of ICML, 2001. [18] T. Li, S. Ma, and M. Ogihara.",
                "Entropy-based criterion in categorical clustering.",
                "In Proceedings of ICML, 2004. [19] A. McCallum, D. Freitag, and F. Pereira.",
                "Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of ICML, 2000. [20] L. Pevzner and M. Hearst.",
                "A critique and improvement of an evaluation metric for text segmentation.",
                "Computational Linguistic, 28(1):19-36, 2002. [21] J. C. Reynar.",
                "Statistical models for topic segmentation.",
                "In Proceedings of ACL, 1999. [22] G. Salton and M. McGill.",
                "Introduction to Modern Information Retrieval.",
                "McGraw Hill, 1983. [23] B.",
                "Sun, Q. Tan, P. Mitra, and C. L. Giles.",
                "Extraction and search of chemical formulae in text documents on the web.",
                "In Proceedings of WWW, 2007. [24] B.",
                "Sun, D. Zhou, H. Zha, and J.",
                "Yen.",
                "Multi-task text segmentation and alignment based on weighted mutual information.",
                "In Proceedings of CIKM, 2006. [25] M. Utiyama and H. Isahara.",
                "A statistical model for domain-independent text segmentation.",
                "In Proceedings of the 39th ACL, 1999. [26] C. Wayne.",
                "Multilingual <br>topic detection</br> and tracking: Successful research enabled by corpora and evaluation.",
                "In Proceedings of LREC, 2000. [27] J. Yamron, I. Carp, L. Gillick, S. Lowe, and P. van Mulbregt.",
                "A hidden markov model approach to text segmentation and event tracking.",
                "In Proceedings of ICASSP, 1998. [28] H. Zha and X. Ji.",
                "Correlating multilingual documents via bipartite graph modeling.",
                "In Proceedings of SIGIR, 2002."
            ],
            "original_annotated_samples": [
                "Topic Segmentation with Shared <br>topic detection</br> and Alignment of Multiple Documents Bingjun Sun*, Prasenjit Mitra*† , Hongyuan Zha‡ , C. Lee Giles*† , John Yen*† *Department of Computer Science and Engineering † College of Information Sciences and Technology The Pennsylvania State University University Park, PA 16802 ‡ College of Computing The Georgia Institute of Technology Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu ABSTRACT <br>topic detection</br> and tracking [26] and topic segmentation [15] play an important role in capturing the local and sequential information of documents.",
                "In this paper, we introduce a novel unsupervised method for shared <br>topic detection</br> and topic segmentation of multiple similar documents based on mutual information (MI) and weighted mutual information (WMI) that is a combination of MI and term weights.",
                "Our experimental results show that our algorithm works well for the tasks of single-document segmentation, shared <br>topic detection</br>, and multi-document segmentation.",
                "INTRODUCTION Many researchers have worked on <br>topic detection</br> and tracking (TDT) [26] and topic segmentation during the past decade.",
                "In Section 5, experiments about single-document segmentation, shared <br>topic detection</br>, and multi-document segmentation are described, and results are presented and discussed to evaluate the performance of our algorithm."
            ],
            "translated_annotated_samples": [
                "La segmentación de temas con <br>detección de temas</br> compartidos y alineación de múltiples documentos Bingjun Sun*, Prasenjit Mitra*†, Hongyuan Zha‡, C. Lee Giles*†, John Yen*† *Departamento de Ciencias de la Computación e Ingeniería † Colegio de Ciencias de la Información y Tecnología Universidad Estatal de Pensilvania University Park, PA 16802 ‡ Colegio de Computación Instituto de Tecnología de Georgia Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu RESUMEN La detección y seguimiento de temas [26] y la segmentación de temas [15] juegan un papel importante en la captura de la información local y secuencial de los documentos.",
                "En este artículo, presentamos un novedoso método no supervisado para la <br>detección de temas</br> compartidos y la segmentación de temas de múltiples documentos similares basado en la información mutua (MI) y la información mutua ponderada (WMI), que es una combinación de MI y pesos de términos.",
                "Nuestros resultados experimentales muestran que nuestro algoritmo funciona bien para las tareas de segmentación de un solo documento, <br>detección de temas compartidos</br> y segmentación de múltiples documentos.",
                "INTRODUCCIÓN Muchos investigadores han trabajado en la <br>detección y seguimiento de temas</br> (TDT) [26] y la segmentación de temas durante la última década.",
                "En la Sección 5, se describen experimentos sobre la segmentación de documentos individuales, la <br>detección de temas compartidos</br> y la segmentación de documentos múltiples, y se presentan y discuten los resultados para evaluar el rendimiento de nuestro algoritmo."
            ],
            "translated_text": "La segmentación de temas con <br>detección de temas</br> compartidos y alineación de múltiples documentos Bingjun Sun*, Prasenjit Mitra*†, Hongyuan Zha‡, C. Lee Giles*†, John Yen*† *Departamento de Ciencias de la Computación e Ingeniería † Colegio de Ciencias de la Información y Tecnología Universidad Estatal de Pensilvania University Park, PA 16802 ‡ Colegio de Computación Instituto de Tecnología de Georgia Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu RESUMEN La detección y seguimiento de temas [26] y la segmentación de temas [15] juegan un papel importante en la captura de la información local y secuencial de los documentos. El trabajo previo en esta área suele centrarse en documentos individuales, aunque existen múltiples documentos similares en muchos dominios. En este artículo, presentamos un novedoso método no supervisado para la <br>detección de temas</br> compartidos y la segmentación de temas de múltiples documentos similares basado en la información mutua (MI) y la información mutua ponderada (WMI), que es una combinación de MI y pesos de términos. La idea básica es que la segmentación óptima maximiza MI (o WMI). Nuestro enfoque puede detectar temas compartidos entre documentos. Puede encontrar los límites óptimos en un documento y alinear segmentos entre documentos al mismo tiempo. También puede manejar la segmentación de un solo documento como un caso especial de la segmentación y alineación de varios documentos. Nuestros métodos pueden identificar y fortalecer términos de señal que pueden ser utilizados para la segmentación y eliminar parcialmente las palabras de parada mediante el uso de pesos de términos basados en la entropía aprendida de múltiples documentos. Nuestros resultados experimentales muestran que nuestro algoritmo funciona bien para las tareas de segmentación de un solo documento, <br>detección de temas compartidos</br> y segmentación de múltiples documentos. El uso de información de múltiples documentos puede mejorar enormemente el rendimiento de la segmentación de temas, y utilizar WMI es aún mejor que utilizar MI para la segmentación de múltiples documentos. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información-Agrupación; H.3.1 [Almacenamiento y Recuperación de Información]: Análisis de Contenido e Indexación-Procesamiento Lingüístico; I.2.7 [Inteligencia Artificial]: Procesamiento de Lenguaje Natural-Análisis de Texto; I.5.3 [Reconocimiento de Patrones]: Algoritmos de Agrupación-Medidas de Similitud Términos Generales Algoritmos, Diseño, Experimentación 1. INTRODUCCIÓN Muchos investigadores han trabajado en la <br>detección y seguimiento de temas</br> (TDT) [26] y la segmentación de temas durante la última década. La segmentación de temas tiene como objetivo identificar los límites en un documento con la meta de capturar la estructura temática latente. Las tareas de segmentación de temas generalmente se dividen en dos categorías [15]: la segmentación de flujos de texto, donde se identifica la transición de temas, y la segmentación coherente de documentos, en la que los documentos se dividen en subtemas. La primera categoría tiene aplicaciones en el reconocimiento automático del habla, mientras que la segunda tiene más aplicaciones como la consulta parcial de texto en documentos extensos en la recuperación de información, resumen de texto y medición de calidad de múltiples documentos. La investigación previa en relación con TDT se enmarca en la primera categoría, centrada en el seguimiento de temas de datos de discurso de transmisión y texto de agencia de noticias, mientras que la segunda categoría no ha sido estudiada muy bien. Los enfoques tradicionales realizan la segmentación de temas en los documentos uno a la vez [15, 25, 6]. La mayoría de ellos tienen un mal desempeño en tareas sutiles como la segmentación coherente de documentos [15]. A menudo, los usuarios finales buscan documentos que tengan un contenido similar. Los motores de búsqueda, como Google, proporcionan enlaces para obtener páginas similares. A una granularidad más fina, los usuarios pueden estar buscando obtener secciones de un documento similares a una sección en particular que presumiblemente discute un tema de interés para los usuarios. Por lo tanto, la extensión de la segmentación de temas desde documentos individuales para identificar segmentos similares de múltiples documentos similares con el mismo tema es una dirección natural y necesaria, y se espera que la segmentación de temas de múltiples documentos tenga un mejor rendimiento ya que se utiliza más información. Los enfoques tradicionales que utilizan la medición de similitud basada en la frecuencia de términos generalmente parten del mismo supuesto de que un vocabulario similar tiende a estar en un segmento de tema coherente [15, 25, 6]. Sin embargo, suelen tener el problema de identificar las palabras de parada. Por ejemplo, se eliminan las palabras de parada dependientes del documento junto con las palabras de parada genéricas en [15]. Hay dos razones por las que no eliminamos las palabras de parada directamente. Primero, identificar las palabras vacías es otro problema [12] que requiere estimación en cada dominio. Eliminar palabras comunes de parada puede resultar en la pérdida de información útil en un dominio específico. Segundo, aunque se pueden identificar las palabras de parada, la clasificación estricta de palabras de parada y palabras no de parada no puede representar el cambio gradual en la cantidad de contenido informativo de cada palabra. Empleamos una clasificación suave utilizando pesos de términos. En este documento, consideramos el problema de la segmentación de temas como un problema de optimización utilizando técnicas de teoría de la información para encontrar los límites óptimos de un documento dado el número de segmentos de texto, de manera que se minimice la pérdida de información mutua (MI) (o información mutua ponderada (WMI)) después de la segmentación y alineación. Esto es igual a maximizar el MI (o WMI). El MI se centra en medir la diferencia entre segmentos, mientras que investigaciones anteriores se enfocaban en encontrar la similitud (por ejemplo, la distancia del coseno) entre segmentos [15, 25, 6]. La alineación de temas de múltiples documentos similares se puede lograr agrupando las oraciones sobre el mismo tema en el mismo clúster. La segmentación de temas en un solo documento es solo un caso especial del problema de segmentación y alineación de temas en múltiples documentos. Los términos pueden ser co-agrupados como en [10] al mismo tiempo, dado el número de agrupaciones, pero nuestros resultados experimentales muestran que este método resulta en una peor segmentación (ver Tablas 1, 4 y 6). Por lo general, los lectores humanos pueden identificar la transición de temas basándose en palabras de señal, y pueden ignorar las palabras vacías. Inspirados por esto, asignamos a cada término (o grupo de términos) un peso basado en la entropía entre diferentes documentos y diferentes segmentos de documentos. No solo este enfoque puede aumentar la contribución de las palabras clave, sino que también puede disminuir el efecto de las palabras de parada comunes, palabras ruidosas y palabras de parada dependientes del documento. Estas palabras son comunes en un documento. Muchos métodos basados en la similitud de oraciones requieren que estas palabras sean eliminadas antes de que se pueda realizar la segmentación de temas [15]. Nuestros resultados en la Figura 3 muestran que los pesos de los términos son útiles para la segmentación y alineación de temas en múltiples documentos. La principal contribución de este artículo es que introduce un método novedoso para la segmentación de temas utilizando MI y demuestra que este método funciona mejor que los criterios previamente utilizados. Además, hemos abordado el problema de la segmentación y alineación de temas en varios documentos, mientras que la mayoría de las investigaciones existentes se centraron en la segmentación de documentos individuales. La segmentación y alineación de múltiples documentos pueden utilizar información de documentos similares y mejorar significativamente el rendimiento de la segmentación de temas. Obviamente, nuestro enfoque puede manejar documentos individuales como un caso especial cuando varios documentos no están disponibles. Puede detectar temas compartidos entre documentos para determinar si son múltiples documentos sobre el mismo tema. También introducimos el nuevo criterio de WMI basado en pesos de términos aprendidos de múltiples documentos similares, lo cual puede mejorar aún más el rendimiento de la segmentación de temas. Proponemos un algoritmo voraz iterativo basado en programación dinámica y demostramos que funciona bien en la práctica. Algunos de nuestros trabajos anteriores están en [24]. El resto de este documento está organizado de la siguiente manera: En la Sección 2, revisamos el trabajo relacionado. La sección 3 contiene una formulación del problema de segmentación de temas y alineación de múltiples documentos con co-clustering de términos, una revisión del criterio de MI para el clustering, y finalmente una introducción a WMI. En la Sección 4, primero proponemos el algoritmo voraz iterativo de segmentación y alineación de temas con co-clustering de términos, y luego describimos cómo el algoritmo puede ser optimizado mediante programación dinámica. En la Sección 5, se describen experimentos sobre la segmentación de documentos individuales, la <br>detección de temas compartidos</br> y la segmentación de documentos múltiples, y se presentan y discuten los resultados para evaluar el rendimiento de nuestro algoritmo. ",
            "candidates": [],
            "error": [
                [
                    "detección de temas",
                    "detección de temas",
                    "detección de temas compartidos",
                    "detección y seguimiento de temas",
                    "detección de temas compartidos"
                ]
            ]
        },
        "tracking": {
            "translated_key": "seguimiento",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Topic Segmentation with Shared Topic Detection and Alignment of Multiple Documents Bingjun Sun*, Prasenjit Mitra*† , Hongyuan Zha‡ , C. Lee Giles*† , John Yen*† *Department of Computer Science and Engineering † College of Information Sciences and Technology The Pennsylvania State University University Park, PA 16802 ‡ College of Computing The Georgia Institute of Technology Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu ABSTRACT Topic detection and <br>tracking</br> [26] and topic segmentation [15] play an important role in capturing the local and sequential information of documents.",
                "Previous work in this area usually focuses on single documents, although similar multiple documents are available in many domains.",
                "In this paper, we introduce a novel unsupervised method for shared topic detection and topic segmentation of multiple similar documents based on mutual information (MI) and weighted mutual information (WMI) that is a combination of MI and term weights.",
                "The basic idea is that the optimal segmentation maximizes MI(or WMI).",
                "Our approach can detect shared topics among documents.",
                "It can find the optimal boundaries in a document, and align segments among documents at the same time.",
                "It also can handle single-document segmentation as a special case of the multi-document segmentation and alignment.",
                "Our methods can identify and strengthen cue terms that can be used for segmentation and partially remove stop words by using term weights based on entropy learned from multiple documents.",
                "Our experimental results show that our algorithm works well for the tasks of single-document segmentation, shared topic detection, and multi-document segmentation.",
                "Utilizing information from multiple documents can tremendously improve the performance of topic segmentation, and using WMI is even better than using MI for the multi-document segmentation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Clustering; H.3.1 [Information Storage and Retrieval]: Content Analysis and IndexingLinguistic processing; I.2.7 [Artificial Intelligence]: Natural Language Processing-Text analysis; I.5.3 [Pattern Recognition]: Clustering-Algorithms;Similarity measures General Terms Algorithms, Design, Experimentation 1.",
                "INTRODUCTION Many researchers have worked on topic detection and <br>tracking</br> (TDT) [26] and topic segmentation during the past decade.",
                "Topic segmentation intends to identify the boundaries in a document with the goal to capture the latent topical structure.",
                "Topic segmentation tasks usually fall into two categories [15]: text stream segmentation where topic transition is identified, and coherent document segmentation in which documents are split into sub-topics.",
                "The former category has applications in automatic speech recognition, while the latter one has more applications such as partial-text query of long documents in information retrieval, text summary, and quality measurement of multiple documents.",
                "Previous research in connection with TDT falls into the former category, targeted on topic <br>tracking</br> of broadcast speech data and newswire text, while the latter category has not been studied very well.",
                "Traditional approaches perform topic segmentation on documents one at a time [15, 25, 6].",
                "Most of them perform badly in subtle tasks like coherent document segmentation [15].",
                "Often, end-users seek documents that have the similar content.",
                "Search engines, like, Google, provide links to obtain similar pages.",
                "At a finer granularity, users may actually be looking to obtain sections of a document similar to a particular section that presumably discusses a topic of the users interest.",
                "Thus, the extension of topic segmentation from single documents to identifying similar segments from multiple similar documents with the same topic is a natural and necessary direction, and multi-document topic segmentation is expected to have a better performance since more information is utilized.",
                "Traditional approaches using similarity measurement based on term frequency generally have the same assumption that similar vocabulary tends to be in a coherent topic segment [15, 25, 6].",
                "However, they usually suffer the issue of identifying stop words.",
                "For example, additional document-dependent stop words are removed together with the generic stop words in [15].",
                "There are two reasons that we do not remove stop words directly.",
                "First, identifying stop words is another issue [12] that requires estimation in each domain.",
                "Removing common stop words may result in the loss of useful information in a specific domain.",
                "Second, even though stop words can be identified, hard classification of stop words and nonstop words cannot represent the gradually changing amount of information content of each word.",
                "We employ a soft classification using term weights.",
                "In this paper, we view the problem of topic segmentation as an optimization issue using information theoretic techniques to find the optimal boundaries of a document given the number of text segments so as to minimize the loss of mutual information (MI) (or a weighted mutual information (WMI)) after segmentation and alignment.",
                "This is equal to maximizing the MI (or WMI).",
                "The MI focuses on measuring the difference among segments whereas previous research focused on finding the similarity (e.g. cosine distance) of segments [15, 25, 6].",
                "Topic alignment of multiple similar documents can be achieved by clustering sentences on the same topic into the same cluster.",
                "Single-document topic segmentation is just a special case of the multi-document topic segmentation and alignment problem.",
                "Terms can be co-clustered as in [10] at the same time, given the number of clusters, but our experimental results show that this method results in a worse segmentation (see Tables 1, 4, and 6).",
                "Usually, human readers can identify topic transition based on cue words, and can ignore stop words.",
                "Inspired by this, we give each term (or term cluster) a weight based on entropy among different documents and different segments of documents.",
                "Not only can this approach increase the contribution of cue words, but it can also decrease the effect of common stop words, noisy word, and document-dependent stop words.",
                "These words are common in a document.",
                "Many methods based on sentence similarity require that these words are removed before topic segmentation can be performed [15].",
                "Our results in Figure 3 show that term weights are useful for multi-document topic segmentation and alignment.",
                "The major contribution of this paper is that it introduces a novel method for topic segmentation using MI and shows that this method performs better than previously used criteria.",
                "Also, we have addressed the problem of topic segmentation and alignment across multiple documents, whereas most existing research focused on segmentation of single documents.",
                "Multi-document segmentation and alignment can utilize information from similar documents and improves the performance of topic segmentation greatly.",
                "Obviously, our approach can handle single documents as a special case when multiple documents are unavailable.",
                "It can detect shared topics among documents to judge if they are multiple documents on the same topic.",
                "We also introduce the new criterion of WMI based on term weights learned from multiple similar documents, which can improve performance of topic segmentation further.",
                "We propose an iterative greedy algorithm based on dynamic programming and show that it works well in practice.",
                "Some of our prior work is in [24].",
                "The rest of this paper is organized as follows: In Section 2, we review related work.",
                "Section 3 contains a formulation of the problem of topic segmentation and alignment of multiple documents with term co-clustering, a review of the criterion of MI for clustering, and finally an introduction to WMI.",
                "In Section 4, we first propose the iterative greedy algorithm of topic segmentation and alignment with term co-clustering, and then describe how the algorithm can be optimized by usFigure 1: Illustration of multi-document segmentation and alignment. ing dynamic programming.",
                "In Section 5, experiments about single-document segmentation, shared topic detection, and multi-document segmentation are described, and results are presented and discussed to evaluate the performance of our algorithm.",
                "Conclusions and some future directions of the research work are discussed in Section 6. 2.",
                "PREVIOUS WORK Generally, the existing approaches to text segmentation fall into two categories: supervised learning [19, 17, 23] and unsupervised learning [3, 27, 5, 6, 15, 25, 21].",
                "Supervised learning usually has good performance, since it learns functions from labelled training sets.",
                "However, often getting large training sets with manual labels on document sentences is prohibitively expensive, so unsupervised approaches are desired.",
                "Some models consider dependence between sentences and sections, such as Hidden Markov Model [3, 27], Maximum Entropy Markov Model [19], and Conditional Random Fields [17], while many other approaches are based on lexical cohesion or similarity of sentences [5, 6, 15, 25, 21].",
                "Some approaches also focus on cue words as hints of topic transitions [11].",
                "While some existing methods only consider information in single documents [6, 15], others utilize multiple documents [16, 14].",
                "There are not many works in the latter category, even though the performance of segmentation is expected to be better with utilization of information from multiple documents.",
                "Previous research studied methods to find shared topics [16] and topic segmentation and summarization between just a pair of documents [14].",
                "Text classification and clustering is a related research area which categorizes documents into groups using supervised or unsupervised methods.",
                "Topical classification or clustering is an important direction in this area, especially co-clustering of documents and terms, such as LSA [9], PLSA [13], and approaches based on distances and bipartite graph partitioning [28] or maximum MI [2, 10], or maximum entropy [1, 18].",
                "Criteria of these approaches can be utilized in the issue of topic segmentation.",
                "Some of those methods have been extended into the area of topic segmentation, such as PLSA [5] and maximum entropy [7], but to our best knowledge, using MI for topic segmentation has not been studied. 3.",
                "PROBLEM FORMULATION Our goal is to segment documents and align the segments across documents (Figure 1).",
                "Let T be the set of terms {t1, t2, ..., tl}, which appear in the unlabelled set of documents D = {d1, d2, ..., dm}.",
                "Let Sd be the set of sentences for document d ∈ D, i.e. {s1, s2, ..., snd }.",
                "We have a 3D matrix of term frequency, in which the three dimensions are random variables of D, Sd, and T. Sd actually is a random vector including a random variable for each d ∈ D. The term frequency can be used to estimate the joint probability distribution P(D, Sd, T), which is p(t, d, s) = T(t, d, s)/ND, where T(t, d, s) is the number of t in ds sentence s and ND is the total number of terms in D. ˆS represents the set of segments {ˆs1, ˆs2, ..., ˆsp} after segmentation and alignment among multiple documents, where the number of segments | ˆS| = p. A segment ˆsi of document d is a sequence of adjacent sentences in d. Since for different documents si may discuss different sub-topics, our goal is to cluster adjacent sentences in each document into segments, and align similar segments among documents, so that for different documents ˆsi is about the same sub-topic.",
                "The goal is to find the optimal topic segmentation and alignment mapping Segd(si) : {s1, s2, ..., snd } → {ˆs1, ˆs2, ..., ˆsp} and Alid(ˆsi) : {ˆs1, ˆs2, ..., ˆsp} → {ˆs1, ˆs2, ..., ˆsp}, for all d ∈ D, where ˆsi is ith segment with the constraint that only adjacent sentences can be mapped to the same segment, i.e. for d, {si, si+1, ..., sj} → {ˆsq}, where q ∈ {1, ..., p}, where p is the segment number, and if i > j, then for d, ˆsq is missing.",
                "After segmentation and alignment, random vector Sd becomes an aligned random variable ˆS.",
                "Thus, P(D, Sd, T) becomes P(D, ˆS, T).",
                "Term co-clustering is a technique that has been employed [10] to improve the accuracy of document clustering.",
                "We evaluate the effect of it for topic segmentation.",
                "A term t is mapped to exactly one term cluster.",
                "Term co-clustering involves simultaneously finding the optimal term clustering mapping Clu(t) : {t1, t2, ..., tl} → {ˆt1, ˆt2, ..., ˆtk}, where k ≤ l, l is the total number of words in all the documents, and k is the number of clusters. 4.",
                "METHODOLOGY We now describe a novel algorithm which can handle singledocument segmentation, shared topic detection, and multidocument segmentation and alignment based on MI or WMI. 4.1 Mutual Information MI I(X; Y ) is a quantity to measure the amount of information which is contained in two or more random variables [8, 10].",
                "For the case of two random variables, we have I(X; Y ) = x∈X y∈Y p(x, y)log p(x, y) p(x)p(y) , (1) Obviously, when random variables X and Y are independent, I(X; Y ) = 0.",
                "Thus, intuitively, the value of MI depends on how random variables are dependent on each other.",
                "The optimal co-clustering is the mapping Clux : X → ˆX and Cluy : Y → ˆY that minimizes the loss: I(X; Y ) − I( ˆX; ˆY ), which is equal to maximizing I( ˆX; ˆY ).",
                "This is the criterion of MI for clustering.",
                "In the case of topic segmentation, the two random variables are the term variable T and the segment variable S, and each sample is an occurrence of a term T = t in a particular segment S = s. I(T; S) is used to measure how dependent T and S are.",
                "However, I(T; S) cannot be computed for documents before segmentation, since we do not have a set of S due to the fact that sentences of Document d, si ∈ Sd, is not aligned with other documents.",
                "Thus, instead of minimizing the loss of MI, we can maximize MI after topic segmentation, computed as: I( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS p(ˆt, ˆs)log p(ˆt, ˆs) p(ˆt)p(ˆs) , (2) where p(ˆt, ˆs) are estimated by the term frequency tf of Term Cluster ˆt and Segment ˆs in the training set D. Note that here a segment ˆs includes sentences about the the same topic among all documents.",
                "The optimal solution is the mapping Clut : T → ˆT, Segd : Sd → ˆS , and Alid : ˆS → ˆS, which maximizes I( ˆT; ˆS). 4.2 Weighted Mutual Information In topic segmentation and alignment of multiple documents, if P(D, ˆS, T) is known, based on the marginal distributions P(D|T) and P( ˆS|T) for each term t ∈ T, we can categorize terms into four types in the data set: • Common stop words are common both along the dimensions of documents and segments. • Document-dependent stop words that depends on the personal writing style are common only along the dimension of segments for some documents. • Cue words are the most important elements for segmentation.",
                "They are common along the dimension of documents only for the same segment, and they are not common along the dimensions of segments. • Noisy words are other words which are not common along both dimensions.",
                "Entropy based on P(D|T) and P( ˆS|T) can be used to identify different types of terms.",
                "To reinforce the contribution of cue words in the MI computation, and simultaneously reduce the effect of the other three types of words, similar as the idea of the tf-idf weight [22], we use entropies of each term along the dimensions of document D and segment ˆS, i.e.",
                "ED(ˆt) and EˆS(ˆt), to compute the weight.",
                "A cue word usually has a large value of ED(ˆt) but a small value of EˆS(ˆt).",
                "We introduce term weights (or term cluster weights) wˆt = ( ED(ˆt) maxˆt ∈ ˆT (ED(ˆt )) )a (1 − EˆS(ˆt) maxˆt ∈ ˆT (EˆS(ˆt )) )b , (3) where ED(ˆt) = d∈D p(d|ˆt)log|D| 1 p(d|ˆt) , EˆS(ˆt) = ˆs∈ ˆS p(ˆs|ˆt)log| ˆS| 1 p(ˆs|ˆt) , and a > 0 and b > 0 are powers to adjust term weights.",
                "Usually a = 1 and b = 1 as default, and maxˆt ∈ ˆT (ED(ˆt )) and maxˆt ∈ ˆT (EˆS(ˆt )) are used to normalize the entropy values.",
                "Term cluster weights are used to adjust p(ˆt, ˆs), pw(ˆt, ˆs) = wˆtp(ˆt, ˆs) ˆt∈ ˆT ;ˆs∈ ˆS wˆtp(ˆt, ˆs) , (4) and Iw( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS pw(ˆt, ˆs)log pw(ˆt, ˆs) pw(ˆt)pw(ˆs) , (5) where pw(ˆt) and pw(ˆs) are marginal distributions of pw(ˆt, ˆs).",
                "However, since we do not know either the term weights or P(D, ˆS, T), we need to estimate them, but wˆt depends on p(ˆs|t) and ˆS, while ˆS and p(ˆs|t) also depend on wˆt that is still unknown.",
                "Thus, an iterative algorithm is required to estimate term weights wˆt and find the best segmentation and alignment to optimize the objective function Iw concurrently.",
                "After a document is segmented into sentences Input: Joint probability distribution P(D, Sd, T), number of text segments p ∈ {2, 3, ..., max(sd)}, number of term clusters k ∈ {2, 3, ..., l} (if k = l, no term co-clustering required), and weight type w ∈ {0, 1}, indicating to use I or Iw, respectively.",
                "Output: Mapping Clu, Seg, Ali, and term weights wˆt.",
                "Initialization: 0. i = 0.",
                "Initialize Clu (0) t , Seg (0) d , and Ali (0) d ; Initialize w (0) ˆt using Equation (6) if w = 1; Stage 1: 1.",
                "If |D| = 1, k = l, and w = 0, check all sequential segmentations of d into p segments and find the best one Segd(s) = argmaxˆsI( ˆT; ˆS), and return Segd; otherwise, if w = 1 and k = l, go to 3.1; Stage 2: 2.1 If k < l, for each term t, find the best cluster ˆt as Clu(i+1)(t) = argmaxˆtI( ˆT; ˆS(i)) based on Seg(i) and Ali(i); 2.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+1) d (Seg (i+1) d (s)) = argmaxˆsI( ˆT(i+1); ˆS) based on Clu(i+1)(t) if k < l or Clu(0)(t) if k = l; 2.3 i + +.",
                "If Clu, Seg, or Ali changed, go to 2.1; otherwise, if w = 0, return Clu(i), Seg(i), and Ali(i); else j = 0, go to 3.1; Stage 3: 3.1 Update w (i+j+1) ˆt based on Seg(i+j), Ali(i+j), and Clu(i) using Equation (3); 3.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+j+1) d (Seg (i+j+1) d (s)) = argmaxˆsIw( ˆT(i); ˆS) based on Clu(i) and w (i+j+1) ˆt ; 3.3 j + +.",
                "If Iw( ˆT; ˆS) changes, go to step 6; otherwise, stop and return Clu(i), Seg(i+j), Ali(i+j), and w (i+j) ˆt ; Figure 2: Algorithm: Topic segmentation and alignment based on MI or WMI. and each sentence is segmented into words, each word is stemmed.",
                "Then the joint probability distribution P(D, Sd, T) can be estimated.",
                "Finally, this distribution can be used to compute MI in our algorithm. 4.3 Iterative Greedy Algorithm Our goal is to maximize the objective function, I( ˆT; ˆS) or Iw( ˆT; ˆS), which can measure the dependence of term occurrences in different segments.",
                "Generally, first we do not know the estimate term weights, which depend on the optimal topic segmentation and alignment, and term clusters.",
                "Moreover, this problem is NP-hard [10], even though if we know the term weights.",
                "Thus, an iterative greedy algorithm is desired to find the best solution, even though probably only local maxima are reached.",
                "We present the iterative greedy algorithm in Figure 2 to find a local maximum of I( ˆT; ˆS) or Iw( ˆT; ˆS) with simultaneous term weight estimation.",
                "This algorithm can is iterative and greedy for multi-document cases or single-document cases with term weight estimation and/or term co-clustering.",
                "Otherwise, since it is just a one step algorithm to solve the task of single-document segmentation [6, 15, 25], the global maximum of MI is guaranteed.",
                "We will show later that term co-clustering reduces the accuracy of the results and is not necessary, and for singledocument segmentation, term weights are also not required. 4.3.1 Initialization In Step 0, the initial term clustering Clut and topic segmentation and alignment Segd and Alid are important to avoid local maxima and reduce the number of iterations.",
                "First, a good guess of term weights can be made by using the distributions of term frequency along sentences for each document and averaging them to get the initial values of wˆt: wt = ( ED(t) maxt ∈T (ED(t )) )(1 − ES(t) maxt ∈T (ES(t )) ), (6) where ES(t) = 1 |Dt| d∈Dt (1 − s∈Sd p(s|t)log|Sd| 1 p(s|t) ), where Dt is the set of documents which contain Term t. Then, for the initial segmentation Seg(0) , we can simply segment documents equally by sentences.",
                "Or we can find the optimal segmentation just for each document d which maximizes the WMI, Seg (0) d = argmaxˆsIw(T; ˆS), where w = w (0) ˆt .",
                "For the initial alignment Ali(0) , we can first assume that the order of segments for each d is the same.",
                "For the initial term clustering Clu(0) , first cluster labels can be set randomly, and after the first time of Step 3, a good initial term clustering is obtained. 4.3.2 Different Cases After initialization, there are three stages for different cases.",
                "Totally there are eight cases, |D| = 1 or |D| > 1, k = l or k < l, w = 0 or w = 1.",
                "Single document segmentation without term clustering and term weight estimation (|D| = 1, k = l, w = 0) only requires Stage 1 (Step 1).",
                "If term clustering is required (k < l), Stage 2 (Step 2.1, 2.2, and 2.3) is executed iteratively.",
                "If term weight estimation is required (w = 1), Stage 3 (Step 3.1, 3.2, and 3.3) is executed iteratively.",
                "If both are required (k < l, w = 1), Stage 2 and 3 run one after the other.",
                "For multi-document segmentation without term clustering and term weight estimation (|D| > 1, k = l, w = 0), only iteration of Step 2.2 and 2.3 are required.",
                "At Stage 1, the global maximum can be found based on I( ˆT; ˆS) using dynamic programming in Section 4.4.",
                "Simultaneously finding a good term clustering and estimated term weights is impossible, since when moving a term to a new term cluster to maximize Iw( ˆT; ˆS), we do not know that the weight of this term should be the one of the new cluster or the old cluster.",
                "Thus, we first do term clustering at Stage 2, and then estimate term weights at Stage 3.",
                "At Stage 2, Step 2.1 is to find the best term clustering and Step 2.2 is to find the best segmentation.",
                "This cycle is repeated to find a local maximum based on MI I until it converges.",
                "The two steps are: (1) based on current term clustering Cluˆt, for each document d, the algorithm segments all the sentences Sd into p segments sequentially (some segments may be empty), and put them into the p segments ˆS of the whole training set D (all possible cases of different segmentation Segd and alignment Alid are checked) to find the optimal case, and (2) based on the current segmentation and alignment, for each term t, the algorithm finds the best term cluster of t based on the current segmentation Segd and alignment Alid.",
                "After finding a good term clustering, term weights are estimated if w = 1.",
                "At Stage 3, similar as Stage 2, Step 3.1 is term weight re-estimation and Step 3.2 is to find a better segmentation.",
                "They are repeated to find a local maximum based on WMI Iw until it converges.",
                "However, if the term clustering in Stage 2 is not accurate, then the term weight estimation at Stage 3 may have a bad result.",
                "Finally, at Step 3.3, this algorithm converges and return the output.",
                "This algorithm can handle both single-document and multi-document segmentation.",
                "It also can detect shared topics among documents by checking the proportion of overlapped sentences on the same topics, as described in Sec 5.2. 4.4 Algorithm Optimization In many previous works on segmentation, dynamic programming is a technique used to maximize the objective function.",
                "Similarly, at Step 1, 2.2, and 3.2 of our algorithm, we can use dynamic programming.",
                "For Stage 1, using dynamic programming can still find the global optimum, but for Stage 2 and Stage 3, we can only find the optimum for each step of topic segmentation and alignment of a document.",
                "Here we only show the dynamic programming for Step 3.2 using WMI (Step 1 and 2.2 are similar but they can use either I or Iw).",
                "There are two cases that are not shown in the algorithm in Figure 2: (a) single-document segmentation or multi-document segmentation with the same sequential order of segments, where alignment is not required, and (b) multi-document segmentation with different sequential orders of segments, where alignment is necessary.",
                "The alignment mapping function of the former case is simply just Alid(ˆsi) = ˆsi, while for the latter ones alignment mapping function Alid(ˆsi) = ˆsj, i and j may be different.",
                "The computational steps for the two cases are listed below: Case 1 (no alignment): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs) and partial pw(ˆs) without counting sentences from d. Then put sentences from i to j into Part k, and compute partial WMI PIw( ˆT; ˆsk(si, si+1, ..., sj)) ˆt∈ ˆT pw(ˆt, ˆsk)log pw(ˆt, ˆsk) pw(ˆt)pw(ˆsk) , where Alid(si, si+1, ..., sj) = k, k ∈ {1, 2, ..., p}, 1 ≤ i ≤ j ≤ nd, and Segd(sq) = ˆsk for all i ≤ q ≤ j. (2) Let M(sm, 1) = PIw( ˆT; ˆs1(s1, s2, ..., sm)).",
                "Then M(sm, L) = maxi[M(si−1, L − 1) + PIw( ˆT; ˆsL(si, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, and when i > m, no sentences are put into ˆsk when compute PIw (note PIw( ˆT; ˆs(si, ..., sm)) = 0 for single-document segmentation). (3) Finally M(snd , p) = maxi[M(si−1, p − 1)+ PIw( ˆT; ˆsp(si, ..., snd ))], where 1 ≤ i ≤ nd+1.",
                "The optimal Iw is found and the corresponding segmentation is the best.",
                "Case 2 (alignment required): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs), and partial pw(ˆs), and PIw( ˆT; ˆsk(si, si+1, ..., sj)) similarly as Case 1. (2) Let M(sm, 1, k) = PIw( ˆT; ˆsk(s1, s2, ..., sm)), where k ∈ {1, 2, ..., p}.",
                "Then M(sm, L, kL) = maxi,j[M(si−1, L − 1, kL/j) + PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, kL ∈ Set(p, L), which is the set of all p!",
                "L!(p−L)! combinations of L segments chosen from all p segments, j ∈ kL, the set of L segments chosen from all p segments, and kL/j is the combination of L − 1 segments in kL except Segment j. (3) Finally, M(snd , p, kp) = maxi,j[M(si−1, p − 1, kp/j) +PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., snd ))], where kp is just the combination of all p segments and 1 ≤ i ≤ nd + 1, which is the optimal Iw and the corresponding segmentation is the best.",
                "The steps of Case 1 and 2 are similar, except in Case 2, alignment is considered in addition to segmentation.",
                "First, basic items of probability for computing Iw are computed excluding Doc d, and then partial WMI by putting every possible sequential segment (including empty segment) of d into every segment of the set.",
                "Second, the optimal sum of PIw for L segments and the leftmost m sentences, M(sm, L), is found.",
                "Finally, the maximal WMI is found among different sums of M(sm, p − 1) and PIw for Segment p. 5.",
                "EXPERIMENTS In this section, single-document segmentation, shared topic detection, and multi-document segmentation will be tested.",
                "Different hyper parameters of our method are studied.",
                "For convenience, we refer to the method using I as MIk if w = 0, and Iw as WMIk if w = 2 or as WMIk if w = 1, where k is the number of term clusters, and if k = l, where l is the total number of terms, then no term clustering is required, i.e.",
                "MIl and WMIl. 5.1 Single-document Segmentation 5.1.1 Test Data and Evaluation The first data set we tested is a synthetic one used in previous research [6, 15, 25] and many other papers.",
                "It has 700 samples.",
                "Each is a concatenation of ten segments.",
                "Each segment is the first n sentence selected randomly from the Brown corpus, which is supposed to have a different topic from each other.",
                "Currently, the best results on this data set is achieved by Ji et.al. [15].",
                "To compare the performance of our methods, the criterion used widely in previous research is applied, instead of the unbiased criterion introduced in [20].",
                "It chooses a pair of words randomly.",
                "If they are in different segments (different) for the real segmentation (real), but predicted (pred) as in the same segment, it is a miss.",
                "If they are in the same segment (same), but predicted as in different segments, it is a false alarm.",
                "Thus, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, diff)p(diff|real) +p(false alarm|real, pred, same)p(same|real). 5.1.2 Experiment Results We tested the case when the number of segments is known.",
                "Table 1 shows the results of our methods with different hyper parameter values and three previous approaches, C99[25], U00[6], and ADDP03[15], on this data set when the segment number is known.",
                "In WMI for single-document segmentation, the term weights are computed as follows: wˆt = 1−EˆS(ˆt)/maxˆt ∈ ˆT (EˆS(ˆt )).",
                "For this case, our methods MIl and WMIl both outperform all the previous approaches.",
                "We compared our methods with ADDP03using one-sample one-sided t-test and p-values are shown in Table 2.",
                "From the p-values, we can see that mostly the differences are very Table 1: Average Error Rates of Single-document Segmentation Given Segment Numbers Known Range of n 3-11 3-5 6-8 9-11 Sample size 400 100 100 100 C99 12% 11% 10% 9% U00 10% 9% 7% 5% ADDP03 6.0% 6.8% 5.2% 4.3% MIl 4.68% 5.57% 2.59% 1.59% WMIl 4.94% 6.33% 2.76% 1.62% MI100 9.62% 12.92% 8.66% 6.67% Table 2: Single-document Segmentation: P-values of T-test on Error Rates Range of n 3-11 3-5 6-8 9-11 ADDP03, MIl 0.000 0.000 0.000 0.000 ADDP03, WMIl 0.000 0.099 0.000 0.000 MIl, WMIl 0.061 0.132 0.526 0.898 significant.",
                "We also compare the error rates between our two methods using two-sample two-sided t-test to check the hypothesis that they are equal.",
                "We cannot reject the hypothesis that they are equal, so the difference are not significant, even though all the error rates for MIl are smaller than WMIl.",
                "However, we can conclude that term weights contribute little in single-document segmentation.",
                "The results also show that MI using term co-clustering (k = 100) decreases the performance.",
                "We tested different number of term clusters, and found that the performance becomes better when the cluster number increases to reach l. WMIk<l has similar results that we did not show in the table.",
                "As mentioned before, using MI may be inconsistent on optimal boundaries given different numbers of segments.",
                "This situation occurs especially when the similarities among segments are quite different, i.e. some transitions are very obvious, while others are not.",
                "This is because usually a document is a hierarchical structure instead of only a sequential structure.",
                "When the segments are not at the same level, this situation may occur.",
                "Thus, a hierarchical topic segmentation approach is desired, and the structure highly depends on the number of segments for each internal node and the stop criteria of splitting.",
                "For this data set of singledocument segmentation, since it is just a synthetic set, which is just a concatenation of several segments about different topics, it is reasonable that approaches simply based on term frequency have a good performance.",
                "Usually for the tasks of segmenting coherent documents for sub-topics, the effectiveness decreases much. 5.2 Shared Topic Detection 5.2.1 Test Data and Evaluation The second data set contains 80 news articles from Google News.",
                "There are eight topics and each has 10 articles.",
                "We randomly split the set into subsets with different document numbers and each subset has all eight topics.",
                "We compare our approach MIl and WMIl with LDA [4].",
                "LDA treats a document in the data set as a bag of words, finds its distribution on topics, and its major topic.",
                "MIl and WMIl views each sentence as a bag of words and tag it with a topic label.",
                "Then for each pair of documents, LDA determines if they are on the same topic, while MIl and Table 3: Shared Topic Detection: Average Error Rates for Different Numbers of Documents in Each Subset #Doc 10 20 40 80 LDA 8.89% 16.33% 1.35% 0.60% MIl, θ = 0.6 4.17% 1.71% 1.47% 0.0% WMIl, θ = 0.8 18.6% 3.16% 1.92% 0.0% WMIl check whether the proportion overlapped sentences on the same topic is larger than the adjustable threshold θ.",
                "That is, in MIl and WMIl, for a pair of documents d, d , if [ s∈Sd,s ∈Sd 1(topics=topics )/min(|Sd|, |Sd|)] > θ, where Sd is the set of sentences of d, and |Sd| is the number of sentences of d, then d and d have the shared topic.",
                "For a pair of documents selected randomly, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, same)p(same|real) +p(false alarm|real, pred, diff)p(diff|real), where a miss means if they have the same topic (same) for the real case (real), but predicted (pred) as on the same topic.",
                "If they are on different topics (diff), but predicted as on the same topic, it is a false alarm. 5.2.2 Experiment Results The results are shown in Table 3.",
                "If most documents have different topics, in WMIl, the estimation of term weights in Equation (3) is not correct.",
                "Thus, WMIl is not expected to have a better performance than MIl, when most documents have different topics.",
                "When there are fewer documents in a subset with the same number of topics, more documents have different topics, so WMIl is more worse than MIl.",
                "We can see that for most cases MIl has a better (or at least similar) performance than LDA.",
                "After shared topic detection, multi-document segmentation of documents with the shared topics is able to be executed. 5.3 Multi-document Segmentation 5.3.1 Test Data and Evaluation For multi-document segmentation and alignment, our goal is to identify these segments about the same topic among multiple similar documents with shared topics.",
                "Using Iw is expected to perform better than I, since without term weights the result is affected seriously by document-dependent stop words and noisy words which depends on the personal writing style.",
                "It is more likely to treat the same segments of different documents as different segments under the effect of document-dependent stop words and noisy words.",
                "Term weights can reduce the effect of document-dependent stop words and noisy words by giving cue terms more weights.",
                "The data set for multi-document segmentation and alignment has 102 samples and 2264 sentences totally.",
                "Each is the introduction part of a lab report selected from the course of Biol 240W, Pennsylvania State University.",
                "Each sample has two segments, introduction of plant hormones and the content in the lab.",
                "The length range of samples is from two to 56 sentences.",
                "Some samples only have one part and some have a reverse order the these two segments.",
                "It is not hard to identify the boundary between two segments for human.",
                "We labelled each sentence manually for evaluation.",
                "The criterion of evaluation is just using the proportion of the number of sentences with wrong predicted segment labels in the total number of sentences in the whole training Table 4: Average Error Rates of Multi-document Segmentation Given Segment Numbers Known #Doc MIl WMIl k MIk WMIk 102 3.14% 2.78% 300 4.68% 6.58% 51 4.17% 3.63% 300 17.83% 22.84% 34 5.06% 4.12% 300 18.75% 20.95% 20 7.08% 5.42% 250 20.40% 21.83% 10 10.38% 7.89% 250 21.42% 21.91% 5 15.77% 11.64% 250 21.89% 22.59% 2 25.90% 23.18% 50 25.44% 25.49% 1 23.90% 24.82% 25 25.75% 26.15% Table 5: Multi-document Segmentation: P-values of T-test on Error Rates for MIl and WMIl #Doc 51 34 20 10 5 2 P-value 0.19 0.101 0.025 0.001 0.000 0.002 set as the error rate: p(error|predicted, real) = d∈D s∈Sd 1(predicteds=reals)/ d∈D nd.",
                "In order to show the benefits of multi-document segmentation and alignment, we compared our method with different parameters on different partitions of the same training set.",
                "Except the cases that the number of documents is 102 and one (they are special cases of using the whole set and the pure single-document segmentation), we randomly divided the training set into m partitions, and each has 51, 34, 20, 10, 5, and 2 document samples.",
                "Then we applied our methods on each partition and calculated the error rate of the whole training set.",
                "Each case was repeated for 10 times for computing the average error rates.",
                "For different partitions of the training set, different k values are used, since the number of terms increases when the document number in each partition increases. 5.3.2 Experiment Results From the experiment results in Table 4, we can see the following observations: (1) When the number of documents increases, all methods have better performances.",
                "Only from one to two documents, MIl has decrease a little.",
                "We can observe this from Figure 3 at the point of document number = 2.",
                "Most curves even have the worst results at this point.",
                "There are two reasons.",
                "First, because samples vote for the best multi-document segmentation and alignment, but if only two documents are compared with each other, the one with missing segments or a totally different sequence will affect the correct segmentation and alignment of the other.",
                "Second, as noted at the beginning of this section, if two documents have more document-dependent stop words or noisy words than cue words, then the algorithm may view them as two different segments and the other segment is missing.",
                "Generally, we can only expect a better performance when the number of documents is larger than the number of segments. (2) Except single-document segmentation, WMIl is always better than MIl, and when the number of documents is reaching one or increases to a very large number, their performances become closer.",
                "Table 5 shows p-values of twosample one-sided t-test between MIl and WMIl.",
                "We also can see this trend from p-values.",
                "When document number = 5, we reached the smallest p-value and the largest difference between error rates of MIl and WMIl.",
                "For single-document Table 6: Multi-document Segmentation: Average Error Rate for Document Number = 5 in Each Subset with Different Number of Term Clusters #Cluster 75 100 150 250 l MIk 24.67% 24.54% 23.91% 22.59% 15.77% segmentation, WMIl is even a little bit worse than MIl, which is similar as the results of the single-document segmentation on the first data set.",
                "The reason is that for singledocument segmentation, we cannot estimate term weights accurately, since multiple documents are unavailable. (3) Using term clustering usually gets worse results than MIl and WMIl. (4) Using term clustering in WMIk is even worse than in MIk, since in WMIk term clusters are found first using I before using Iw.",
                "If the term clusters are not correct, then the term weights are estimated worse, which may mislead the algorithm to reach even worse results.",
                "From the results we also found that in multi-document segmentation and alignment, most documents with missing segments and a reverse order are identified correctly.",
                "Table 6 illustrates the experiment results for the case of 20 partitions (each has five document samples) of the training set and topic segmentation and alignment using MIk with different numbers of term clusters k. Notice that when the number of term clusters increases, the error rate becomes smaller.",
                "Without term clustering, we have the best result.",
                "We did not show results for WMIk with term clustering, but the results are similar.",
                "We also tested WMIl with different hyper parameters of a and b to adjust term weights.",
                "The results are presented in Figure 3.",
                "It was shown that the default case WMIl : a = 1, b = 1 gave the best results for different partitions of the training set.",
                "We can see the trend that when the document number is very small or large, the difference between MIl : a = 0, b = 0 and WMIl : a = 1, b = 1 becomes quite small.",
                "When the document number is not large (about from 2 to 10), all the cases using term weights have better performances than MIl : a = 0, b = 0 without term weights, but when the document number becomes larger, the cases WMIl : a = 1, b = 0 and WMIl : a = 2, b = 1 become worse than MIl : a = 0, b = 0.",
                "When the document number becomes very large, they are even worse than cases with small document numbers.",
                "This means that a proper way to estimate term weights for the criterion of WMI is very important.",
                "Figure 4 shows the term weights learned from the whole training set.",
                "Four types of words are categorized roughly even though the transition among them are subtle.",
                "Figure 5 illustrates the change in (weighted) mutual information for MIl and WMIl.",
                "As expected, mutual information for MIl increases monotonically with the number of steps, while WMIl does not.",
                "Finally, MIl and WMIl are scalable, with computational complexity shown in Figure 6.",
                "One advantage for our approach based on MI is that removing stop words is not required.",
                "Another important advantage is that there are no necessary hyper parameters to adjust.",
                "In single-document segmentation, the performance based on MI is even better for that based on WMI, so no extra hyper parameter is required.",
                "In multi-document segmentation, we show in the experiment, a = 1 and b = 1 is the best.",
                "Our method gives more weights to cue terms.",
                "However, usually cue terms or sentences appear at the beginning of a segment, while the end of the segment may be 1 2 5 10 20 34 51 102 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 Document Number ErrorRate MIl :a=0,b=0 WMI l :a=1,b=1 WMI l :a=1,b=0 WMI l :a=2,b=1 Figure 3: Error rates for different hyper parameters of term weights. 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Normalized Document Entropy NormalizedSegmentEntropy Noisy words Cue words Common stop words Doc−dep stop words Figure 4: Term weights learned from the whole training set. 0 100 200 300 400 500 600 0.06 0.08 0.1 0.12 0.14 0.16 0.18 Number of Steps (Weighted)MutualInformation MI l WMI l Figure 5: Change in (weighted) MI for MIl and WMIl. 0 20 40 60 80 100 120 0 200 400 600 800 1000 1200 1400 1600 1800 2000 Document Number TimetoConverge(sec) MI l WMI l Figure 6: Time to converge for MIl and WMIl. much noisy.",
                "One possible solution is giving more weights to terms at the beginning of each segment.",
                "Moreover, when the length of segments are quite different, long segments have much higher term frequencies, so they may dominate the segmentation boundaries.",
                "Normalization of term frequencies versus the segment length may be useful. 6.",
                "CONCLUSIONS AND FUTURE WORK We proposed a novel method for multi-document topic segmentation and alignment based on weighted mutual information, which can also handle single-document cases.",
                "We used dynamic programming to optimize our algorithm.",
                "Our approach outperforms all the previous methods on singledocument cases.",
                "Moreover, we also showed that doing segmentation among multiple documents can improve the performance tremendously.",
                "Our results also illustrated that using weighted mutual information can utilize the information of multiple documents to reach a better performance.",
                "We only tested our method on limited data sets.",
                "More data sets especially complicated ones should be tested.",
                "More previous methods should be compared with.",
                "Moreover, natural segmentations like paragraphs are hints that can be used to find the optimal boundaries.",
                "Supervised learning also can be considered. 7.",
                "ACKNOWLEDGMENTS The authors want to thank Xiang Ji, and Prof. J. Scott Payne for their help. 8.",
                "REFERENCES [1] A. Banerjee, I. Ghillon, J. Ghosh, S. Merugu, and D. Modha.",
                "A generalized maximum entropy approach to bregman co-clustering and matrix approximation.",
                "In Proceedings of SIGKDD, 2004. [2] R. Bekkerman, R. El-Yaniv, and A. McCallum.",
                "Multi-way distributional clustering via pairwise interactions.",
                "In Proceedings of ICML, 2005. [3] D. M. Blei and P. J. Moreno.",
                "Topic segmentation with an aspect hidden markov model.",
                "In Proceedings of SIGIR, 2001. [4] D. M. Blei, A. Ng, and M. Jordan.",
                "Latent dirichlet allocation.",
                "Journal of Machine Learning Research, 3:993-1022, 2003. [5] T. Brants, F. Chen, and I. Tsochantaridis.",
                "Topic-based document segmentation with probabilistic latent semantic analysis.",
                "In Proceedings of CIKM, 2002. [6] F. Choi.",
                "Advances in domain indepedent linear text segmentation.",
                "In Proceedings of the NAACL, 2000. [7] H. Christensen, B. Kolluru, Y. Gotoh, and S. Renals.",
                "Maximum entropy segmentation of broadcast news.",
                "In Proceedings of ICASSP, 2005. [8] T. Cover and J. Thomas.",
                "Elements of Information Theory.",
                "John Wiley and Sons, New York, USA, 1991. [9] S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and R. Harshman.",
                "Indexing by latent semantic analysis.",
                "Journal of the American Society for Information Systems, 1990. [10] I. Dhillon, S. Mallela, and D. Modha.",
                "Information-theoretic co-clustering.",
                "In Proceedings of SIGKDD, 2003. [11] M. Hajime, H. Takeo, and O. Manabu.",
                "Text segmentation with multiple surface linguistic cues.",
                "In Proceedings of COLING-ACL, 1998. [12] T. K. Ho.",
                "Stop word location and identification for adaptive text recognition.",
                "International Journal of Document Analysis and Recognition, 3(1), August 2000. [13] T. Hofmann.",
                "Probabilistic latent semantic analysis.",
                "In Proceedings of the UAI99, 1999. [14] X. Ji and H. Zha.",
                "Correlating summarization of a pair of multilingual documents.",
                "In Proceedings of RIDE, 2003. [15] X. Ji and H. Zha.",
                "Domain-independent text segmentation using anisotropic diffusion and dynamic programming.",
                "In Proceedings of SIGIR, 2003. [16] X. Ji and H. Zha.",
                "Extracting shared topics of multiple documents.",
                "In Proceedings of the 7th PAKDD, 2003. [17] J. Lafferty, A. McCallum, and F. Pereira.",
                "Conditional random fields: Probabilistic models for segmenting and labeling sequence data.",
                "In Proceedings of ICML, 2001. [18] T. Li, S. Ma, and M. Ogihara.",
                "Entropy-based criterion in categorical clustering.",
                "In Proceedings of ICML, 2004. [19] A. McCallum, D. Freitag, and F. Pereira.",
                "Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of ICML, 2000. [20] L. Pevzner and M. Hearst.",
                "A critique and improvement of an evaluation metric for text segmentation.",
                "Computational Linguistic, 28(1):19-36, 2002. [21] J. C. Reynar.",
                "Statistical models for topic segmentation.",
                "In Proceedings of ACL, 1999. [22] G. Salton and M. McGill.",
                "Introduction to Modern Information Retrieval.",
                "McGraw Hill, 1983. [23] B.",
                "Sun, Q. Tan, P. Mitra, and C. L. Giles.",
                "Extraction and search of chemical formulae in text documents on the web.",
                "In Proceedings of WWW, 2007. [24] B.",
                "Sun, D. Zhou, H. Zha, and J.",
                "Yen.",
                "Multi-task text segmentation and alignment based on weighted mutual information.",
                "In Proceedings of CIKM, 2006. [25] M. Utiyama and H. Isahara.",
                "A statistical model for domain-independent text segmentation.",
                "In Proceedings of the 39th ACL, 1999. [26] C. Wayne.",
                "Multilingual topic detection and <br>tracking</br>: Successful research enabled by corpora and evaluation.",
                "In Proceedings of LREC, 2000. [27] J. Yamron, I. Carp, L. Gillick, S. Lowe, and P. van Mulbregt.",
                "A hidden markov model approach to text segmentation and event <br>tracking</br>.",
                "In Proceedings of ICASSP, 1998. [28] H. Zha and X. Ji.",
                "Correlating multilingual documents via bipartite graph modeling.",
                "In Proceedings of SIGIR, 2002."
            ],
            "original_annotated_samples": [
                "Topic Segmentation with Shared Topic Detection and Alignment of Multiple Documents Bingjun Sun*, Prasenjit Mitra*† , Hongyuan Zha‡ , C. Lee Giles*† , John Yen*† *Department of Computer Science and Engineering † College of Information Sciences and Technology The Pennsylvania State University University Park, PA 16802 ‡ College of Computing The Georgia Institute of Technology Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu ABSTRACT Topic detection and <br>tracking</br> [26] and topic segmentation [15] play an important role in capturing the local and sequential information of documents.",
                "INTRODUCTION Many researchers have worked on topic detection and <br>tracking</br> (TDT) [26] and topic segmentation during the past decade.",
                "Previous research in connection with TDT falls into the former category, targeted on topic <br>tracking</br> of broadcast speech data and newswire text, while the latter category has not been studied very well.",
                "Multilingual topic detection and <br>tracking</br>: Successful research enabled by corpora and evaluation.",
                "A hidden markov model approach to text segmentation and event <br>tracking</br>."
            ],
            "translated_annotated_samples": [
                "La segmentación de temas con detección de temas compartidos y alineación de múltiples documentos Bingjun Sun*, Prasenjit Mitra*†, Hongyuan Zha‡, C. Lee Giles*†, John Yen*† *Departamento de Ciencias de la Computación e Ingeniería † Colegio de Ciencias de la Información y Tecnología Universidad Estatal de Pensilvania University Park, PA 16802 ‡ Colegio de Computación Instituto de Tecnología de Georgia Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu RESUMEN La detección y <br>seguimiento</br> de temas [26] y la segmentación de temas [15] juegan un papel importante en la captura de la información local y secuencial de los documentos.",
                "INTRODUCCIÓN Muchos investigadores han trabajado en la detección y <br>seguimiento</br> de temas (TDT) [26] y la segmentación de temas durante la última década.",
                "La investigación previa en relación con TDT se enmarca en la primera categoría, centrada en el <br>seguimiento</br> de temas de datos de discurso de transmisión y texto de agencia de noticias, mientras que la segunda categoría no ha sido estudiada muy bien.",
                "Detección y <br>seguimiento</br> de temas multilingües: Investigación exitosa habilitada por corpora y evaluación.",
                "Un enfoque de modelo oculto de Markov para la segmentación de texto y <br>seguimiento</br> de eventos."
            ],
            "translated_text": "La segmentación de temas con detección de temas compartidos y alineación de múltiples documentos Bingjun Sun*, Prasenjit Mitra*†, Hongyuan Zha‡, C. Lee Giles*†, John Yen*† *Departamento de Ciencias de la Computación e Ingeniería † Colegio de Ciencias de la Información y Tecnología Universidad Estatal de Pensilvania University Park, PA 16802 ‡ Colegio de Computación Instituto de Tecnología de Georgia Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu RESUMEN La detección y <br>seguimiento</br> de temas [26] y la segmentación de temas [15] juegan un papel importante en la captura de la información local y secuencial de los documentos. El trabajo previo en esta área suele centrarse en documentos individuales, aunque existen múltiples documentos similares en muchos dominios. En este artículo, presentamos un novedoso método no supervisado para la detección de temas compartidos y la segmentación de temas de múltiples documentos similares basado en la información mutua (MI) y la información mutua ponderada (WMI), que es una combinación de MI y pesos de términos. La idea básica es que la segmentación óptima maximiza MI (o WMI). Nuestro enfoque puede detectar temas compartidos entre documentos. Puede encontrar los límites óptimos en un documento y alinear segmentos entre documentos al mismo tiempo. También puede manejar la segmentación de un solo documento como un caso especial de la segmentación y alineación de varios documentos. Nuestros métodos pueden identificar y fortalecer términos de señal que pueden ser utilizados para la segmentación y eliminar parcialmente las palabras de parada mediante el uso de pesos de términos basados en la entropía aprendida de múltiples documentos. Nuestros resultados experimentales muestran que nuestro algoritmo funciona bien para las tareas de segmentación de un solo documento, detección de temas compartidos y segmentación de múltiples documentos. El uso de información de múltiples documentos puede mejorar enormemente el rendimiento de la segmentación de temas, y utilizar WMI es aún mejor que utilizar MI para la segmentación de múltiples documentos. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información-Agrupación; H.3.1 [Almacenamiento y Recuperación de Información]: Análisis de Contenido e Indexación-Procesamiento Lingüístico; I.2.7 [Inteligencia Artificial]: Procesamiento de Lenguaje Natural-Análisis de Texto; I.5.3 [Reconocimiento de Patrones]: Algoritmos de Agrupación-Medidas de Similitud Términos Generales Algoritmos, Diseño, Experimentación 1. INTRODUCCIÓN Muchos investigadores han trabajado en la detección y <br>seguimiento</br> de temas (TDT) [26] y la segmentación de temas durante la última década. La segmentación de temas tiene como objetivo identificar los límites en un documento con la meta de capturar la estructura temática latente. Las tareas de segmentación de temas generalmente se dividen en dos categorías [15]: la segmentación de flujos de texto, donde se identifica la transición de temas, y la segmentación coherente de documentos, en la que los documentos se dividen en subtemas. La primera categoría tiene aplicaciones en el reconocimiento automático del habla, mientras que la segunda tiene más aplicaciones como la consulta parcial de texto en documentos extensos en la recuperación de información, resumen de texto y medición de calidad de múltiples documentos. La investigación previa en relación con TDT se enmarca en la primera categoría, centrada en el <br>seguimiento</br> de temas de datos de discurso de transmisión y texto de agencia de noticias, mientras que la segunda categoría no ha sido estudiada muy bien. Los enfoques tradicionales realizan la segmentación de temas en los documentos uno a la vez [15, 25, 6]. La mayoría de ellos tienen un mal desempeño en tareas sutiles como la segmentación coherente de documentos [15]. A menudo, los usuarios finales buscan documentos que tengan un contenido similar. Los motores de búsqueda, como Google, proporcionan enlaces para obtener páginas similares. A una granularidad más fina, los usuarios pueden estar buscando obtener secciones de un documento similares a una sección en particular que presumiblemente discute un tema de interés para los usuarios. Por lo tanto, la extensión de la segmentación de temas desde documentos individuales para identificar segmentos similares de múltiples documentos similares con el mismo tema es una dirección natural y necesaria, y se espera que la segmentación de temas de múltiples documentos tenga un mejor rendimiento ya que se utiliza más información. Los enfoques tradicionales que utilizan la medición de similitud basada en la frecuencia de términos generalmente parten del mismo supuesto de que un vocabulario similar tiende a estar en un segmento de tema coherente [15, 25, 6]. Sin embargo, suelen tener el problema de identificar las palabras de parada. Por ejemplo, se eliminan las palabras de parada dependientes del documento junto con las palabras de parada genéricas en [15]. Hay dos razones por las que no eliminamos las palabras de parada directamente. Primero, identificar las palabras vacías es otro problema [12] que requiere estimación en cada dominio. Eliminar palabras comunes de parada puede resultar en la pérdida de información útil en un dominio específico. Segundo, aunque se pueden identificar las palabras de parada, la clasificación estricta de palabras de parada y palabras no de parada no puede representar el cambio gradual en la cantidad de contenido informativo de cada palabra. Empleamos una clasificación suave utilizando pesos de términos. En este documento, consideramos el problema de la segmentación de temas como un problema de optimización utilizando técnicas de teoría de la información para encontrar los límites óptimos de un documento dado el número de segmentos de texto, de manera que se minimice la pérdida de información mutua (MI) (o información mutua ponderada (WMI)) después de la segmentación y alineación. Esto es igual a maximizar el MI (o WMI). El MI se centra en medir la diferencia entre segmentos, mientras que investigaciones anteriores se enfocaban en encontrar la similitud (por ejemplo, la distancia del coseno) entre segmentos [15, 25, 6]. La alineación de temas de múltiples documentos similares se puede lograr agrupando las oraciones sobre el mismo tema en el mismo clúster. La segmentación de temas en un solo documento es solo un caso especial del problema de segmentación y alineación de temas en múltiples documentos. Los términos pueden ser co-agrupados como en [10] al mismo tiempo, dado el número de agrupaciones, pero nuestros resultados experimentales muestran que este método resulta en una peor segmentación (ver Tablas 1, 4 y 6). Por lo general, los lectores humanos pueden identificar la transición de temas basándose en palabras de señal, y pueden ignorar las palabras vacías. Inspirados por esto, asignamos a cada término (o grupo de términos) un peso basado en la entropía entre diferentes documentos y diferentes segmentos de documentos. No solo este enfoque puede aumentar la contribución de las palabras clave, sino que también puede disminuir el efecto de las palabras de parada comunes, palabras ruidosas y palabras de parada dependientes del documento. Estas palabras son comunes en un documento. Muchos métodos basados en la similitud de oraciones requieren que estas palabras sean eliminadas antes de que se pueda realizar la segmentación de temas [15]. Nuestros resultados en la Figura 3 muestran que los pesos de los términos son útiles para la segmentación y alineación de temas en múltiples documentos. La principal contribución de este artículo es que introduce un método novedoso para la segmentación de temas utilizando MI y demuestra que este método funciona mejor que los criterios previamente utilizados. Además, hemos abordado el problema de la segmentación y alineación de temas en varios documentos, mientras que la mayoría de las investigaciones existentes se centraron en la segmentación de documentos individuales. La segmentación y alineación de múltiples documentos pueden utilizar información de documentos similares y mejorar significativamente el rendimiento de la segmentación de temas. Obviamente, nuestro enfoque puede manejar documentos individuales como un caso especial cuando varios documentos no están disponibles. Puede detectar temas compartidos entre documentos para determinar si son múltiples documentos sobre el mismo tema. También introducimos el nuevo criterio de WMI basado en pesos de términos aprendidos de múltiples documentos similares, lo cual puede mejorar aún más el rendimiento de la segmentación de temas. Proponemos un algoritmo voraz iterativo basado en programación dinámica y demostramos que funciona bien en la práctica. Algunos de nuestros trabajos anteriores están en [24]. El resto de este documento está organizado de la siguiente manera: En la Sección 2, revisamos el trabajo relacionado. La sección 3 contiene una formulación del problema de segmentación de temas y alineación de múltiples documentos con co-clustering de términos, una revisión del criterio de MI para el clustering, y finalmente una introducción a WMI. En la Sección 4, primero proponemos el algoritmo voraz iterativo de segmentación y alineación de temas con co-clustering de términos, y luego describimos cómo el algoritmo puede ser optimizado mediante programación dinámica. En la Sección 5, se describen experimentos sobre la segmentación de documentos individuales, la detección de temas compartidos y la segmentación de documentos múltiples, y se presentan y discuten los resultados para evaluar el rendimiento de nuestro algoritmo. Las conclusiones y algunas direcciones futuras del trabajo de investigación se discuten en la Sección 6.2. TRABAJO PREVIO En general, los enfoques existentes para la segmentación de texto se dividen en dos categorías: aprendizaje supervisado [19, 17, 23] y aprendizaje no supervisado [3, 27, 5, 6, 15, 25, 21]. El aprendizaje supervisado suele tener un buen rendimiento, ya que aprende funciones a partir de conjuntos de entrenamiento etiquetados. Sin embargo, obtener conjuntos de entrenamiento grandes con etiquetas manuales en oraciones de documentos suele ser prohibitivamente costoso, por lo que se prefieren enfoques no supervisados. Algunos modelos consideran la dependencia entre oraciones y secciones, como el Modelo Oculto de Markov [3, 27], el Modelo de Máxima Entropía de Markov [19] y los Campos Aleatorios Condicionales [17], mientras que muchos otros enfoques se basan en la cohesión léxica o la similitud de las oraciones [5, 6, 15, 25, 21]. Algunos enfoques también se centran en las palabras clave como pistas de transiciones de tema [11]. Si bien algunos métodos existentes solo consideran información en documentos individuales [6, 15], otros utilizan varios documentos [16, 14]. No hay muchas obras en la última categoría, aunque se espera que el rendimiento de la segmentación sea mejor con la utilización de información de múltiples documentos. Investigaciones previas estudiaron métodos para encontrar temas compartidos [16] y segmentación y resumen de temas entre solo un par de documentos [14]. La clasificación y agrupación de textos es un área de investigación relacionada que categoriza documentos en grupos utilizando métodos supervisados o no supervisados. La clasificación o agrupación temática es una dirección importante en esta área, especialmente el co-agrupamiento de documentos y términos, como LSA [9], PLSA [13], y enfoques basados en distancias y particionamiento de gráficos bipartitos [28] o máxima MI [2, 10], o máxima entropía [1, 18]. Los criterios de estos enfoques pueden ser utilizados en el tema de la segmentación de temas. Algunos de esos métodos se han extendido al área de la segmentación de temas, como PLSA [5] y máxima entropía [7], pero hasta donde sabemos, el uso de MI para la segmentación de temas no ha sido estudiado. 3. FORMULACIÓN DEL PROBLEMA Nuestro objetivo es segmentar documentos y alinear los segmentos entre documentos (Figura 1). Sea T el conjunto de términos {t1, t2, ..., tl}, que aparecen en el conjunto no etiquetado de documentos D = {d1, d2, ..., dm}. Sea Sd el conjunto de oraciones para el documento d ∈ D, es decir, {s1, s2, ..., snd}. Tenemos una matriz tridimensional de frecuencia de términos, en la que las tres dimensiones son variables aleatorias de D, Sd y T. Sd en realidad es un vector aleatorio que incluye una variable aleatoria para cada d ∈ D. La frecuencia de términos se puede utilizar para estimar la distribución de probabilidad conjunta P(D, Sd, T), que es p(t, d, s) = T(t, d, s)/ND, donde T(t, d, s) es el número de t en la oración s de d y ND es el número total de términos en D. ˆS representa el conjunto de segmentos {ˆs1, ˆs2, ..., ˆsp} después de la segmentación y alineación entre varios documentos, donde el número de segmentos | ˆS| = p. Un segmento ˆsi del documento d es una secuencia de oraciones adyacentes en d. Dado que para diferentes documentos si pueden discutir diferentes subtemas, nuestro objetivo es agrupar oraciones adyacentes en cada documento en segmentos, y alinear segmentos similares entre documentos, de modo que para diferentes documentos ˆsi trate sobre el mismo subtema. El objetivo es encontrar la segmentación óptima de temas y el mapeo de alineación Segd(si) : {s1, s2, ..., snd } → {ˆs1, ˆs2, ..., ˆsp} y Alid(ˆsi) : {ˆs1, ˆs2, ..., ˆsp} → {ˆs1, ˆs2, ..., ˆsp}, para todo d ∈ D, donde ˆsi es el i-ésimo segmento con la restricción de que solo las oraciones adyacentes pueden ser asignadas al mismo segmento, es decir, para d, {si, si+1, ..., sj} → {ˆsq}, donde q ∈ {1, ..., p}, donde p es el número de segmento, y si i > j, entonces para d, ˆsq está ausente. Después de la segmentación y alineación, el vector aleatorio Sd se convierte en una variable aleatoria alineada ˆS. Por lo tanto, P(D, Sd, T) se convierte en P(D, ˆS, T). El término co-clustering es una técnica que se ha empleado [10] para mejorar la precisión del agrupamiento de documentos. Evaluamos el efecto de ello para la segmentación de temas. Un término t se asigna a exactamente un grupo de términos. El término co-clustering implica encontrar simultáneamente el mapeo óptimo de agrupación de términos Clu(t) : {t1, t2, ..., tl} → {ˆt1, ˆt2, ..., ˆtk}, donde k ≤ l, l es el número total de palabras en todos los documentos, y k es el número de clústeres. 4. METODOLOGÍA Ahora describimos un algoritmo novedoso que puede manejar la segmentación de un solo documento, la detección de temas compartidos y la segmentación y alineación de múltiples documentos basados en MI o WMI. 4.1 Información Mutua MI I(X; Y) es una cantidad que mide la cantidad de información contenida en dos o más variables aleatorias [8, 10]. Para el caso de dos variables aleatorias, tenemos I(X; Y) = x∈X y∈Y p(x, y)log p(x, y) p(x)p(y) , (1) Obviamente, cuando las variables aleatorias X e Y son independientes, I(X; Y) = 0. Por lo tanto, de manera intuitiva, el valor de MI depende de cómo las variables aleatorias están relacionadas entre sí. La co-agrupación óptima es el mapeo Clux: X → ˆX y Cluy: Y → ˆY que minimiza la pérdida: I(X; Y) - I(ˆX; ˆY), lo cual es igual a maximizar I(ˆX; ˆY). Este es el criterio de MI para la agrupación. En el caso de la segmentación de temas, las dos variables aleatorias son la variable de término T y la variable de segmento S, y cada muestra es una ocurrencia de un término T = t en un segmento particular S = s. Se utiliza I(T; S) para medir qué tan dependientes son T y S. Sin embargo, no se puede calcular I(T; S) para documentos antes de la segmentación, ya que no tenemos un conjunto de S debido a que las oraciones del Documento d, si ∈ Sd, no están alineadas con otros documentos. Así, en lugar de minimizar la pérdida de MI, podemos maximizar MI después de la segmentación de temas, calculada como: I( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS p(ˆt, ˆs)log p(ˆt, ˆs) p(ˆt)p(ˆs) , (2) donde p(ˆt, ˆs) se estiman mediante la frecuencia de términos tf del Cluster de Términos ˆt y el Segmento ˆs en el conjunto de entrenamiento D. Nótese que aquí un segmento ˆs incluye oraciones sobre el mismo tema entre todos los documentos. La solución óptima es el mapeo Clut : T → ˆT, Segd : Sd → ˆS, y Alid : ˆS → ˆS, que maximiza I( ˆT; ˆS). 4.2 Información Mutua Ponderada En la segmentación de temas y alineación de múltiples documentos, si se conoce P(D, ˆS, T), basado en las distribuciones marginales P(D|T) y P( ˆS|T) para cada término t ∈ T, podemos categorizar los términos en cuatro tipos en el conjunto de datos: • Las palabras de parada comunes son comunes a lo largo de las dimensiones de documentos y segmentos. • Las palabras de parada dependientes del documento que dependen del estilo de escritura personal son comunes solo a lo largo de la dimensión de segmentos para algunos documentos. • Las palabras clave son los elementos más importantes para la segmentación. Son comunes a lo largo de la dimensión de los documentos solo para el mismo segmento, y no son comunes a lo largo de las dimensiones de los segmentos. • Las palabras ruidosas son otras palabras que no son comunes a lo largo de ambas dimensiones. La entropía basada en P(D|T) y P( ˆS|T) se puede utilizar para identificar diferentes tipos de términos. Para reforzar la contribución de las palabras clave en el cálculo de MI, y al mismo tiempo reducir el efecto de los otros tres tipos de palabras, similar a la idea del peso tf-idf [22], utilizamos las entropías de cada término a lo largo de las dimensiones del documento D y del segmento ˆS, es decir, ED(ˆt) y EˆS(ˆt), para calcular el peso. Una palabra de señal generalmente tiene un valor alto de ED(ˆt) pero un valor bajo de EˆS(ˆt). Introducimos los pesos de términos (o pesos de grupos de términos) wˆt = ( ED(ˆt) maxˆt ∈ ˆT (ED(ˆt )) )a (1 − EˆS(ˆt) maxˆt ∈ ˆT (EˆS(ˆt )) )b , (3) donde ED(ˆt) = d∈D p(d|ˆt)log|D| 1 p(d|ˆt) , EˆS(ˆt) = ˆs∈ ˆS p(ˆs|ˆt)log| ˆS| 1 p(ˆs|ˆt) , y a > 0 y b > 0 son potencias para ajustar los pesos de términos. Normalmente a = 1 y b = 1 por defecto, y se utilizan maxˆt ∈ ˆT (ED(ˆt )) y maxˆt ∈ ˆT (EˆS(ˆt )) para normalizar los valores de entropía. Los pesos de los grupos de términos se utilizan para ajustar p(ˆt, ˆs), pw(ˆt, ˆs) = wˆtp(ˆt, ˆs) ˆt∈ ˆT ;ˆs∈ ˆS wˆtp(ˆt, ˆs) , (4) e Iw( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS pw(ˆt, ˆs)log pw(ˆt, ˆs) pw(ˆt)pw(ˆs) , (5) donde pw(ˆt) y pw(ˆs) son distribuciones marginales de pw(ˆt, ˆs). Sin embargo, dado que no conocemos ni los pesos de los términos ni P(D, ˆS, T), necesitamos estimarlos, pero wˆt depende de p(ˆs|t) y ˆS, mientras que ˆS y p(ˆs|t) también dependen de wˆt, que aún es desconocido. Por lo tanto, se requiere un algoritmo iterativo para estimar los pesos de los términos wˆt y encontrar la mejor segmentación y alineación para optimizar la función objetivo Iw de manera concurrente. Después de que un documento se segmenta en oraciones, se introduce: la distribución de probabilidad conjunta P(D, Sd, T), el número de segmentos de texto p ∈ {2, 3, ..., máx(sd)}, el número de grupos de términos k ∈ {2, 3, ..., l} (si k = l, no se requiere co-clustering de términos) y el tipo de peso w ∈ {0, 1}, indicando usar I o Iw, respectivamente. Mapeo de Clu, Seg, Ali y pesos de términos wˆt. Inicialización: 0. i = 0. Inicializar Clu (0) t, Seg (0) d y Ali (0) d; Inicializar w (0) ˆt usando la Ecuación (6) si w = 1; Etapa 1: 1. Si |D| = 1, k = l y w = 0, verifica todas las segmentaciones secuenciales de d en p segmentos y encuentra la mejor Segd(s) = argmaxˆsI( ˆT; ˆS), y devuelve Segd; de lo contrario, si w = 1 y k = l, ve a 3.1; Etapa 2: 2.1 Si k < l, para cada término t, encuentra el mejor clúster ˆt como Clu(i+1)(t) = argmaxˆtI( ˆT; ˆS(i)) basado en Seg(i) y Ali(i); 2.2 Para cada d, verifica todas las segmentaciones secuenciales de d en p segmentos con mapeo s → ˆs → ˆs, y encuentra la mejor Ali (i+1) d (Seg (i+1) d (s)) = argmaxˆsI( ˆT(i+1); ˆS) basado en Clu(i+1)(t) si k < l o Clu(0)(t) si k = l; 2.3 i + +. Si Clu, Seg o Ali cambian, ve a 2.1; de lo contrario, si w = 0, devuelve Clu(i), Seg(i) y Ali(i); de lo contrario, j = 0, ve a 3.1; Etapa 3: 3.1 Actualiza w (i+j+1) ˆt basado en Seg(i+j), Ali(i+j) y Clu(i) usando la Ecuación (3); 3.2 Para cada d, verifica todas las segmentaciones secuenciales de d en p segmentos con mapeo s → ˆs → ˆs, y encuentra la mejor Ali (i+j+1) d (Seg (i+j+1) d (s)) = argmaxˆsIw( ˆT(i); ˆS) basado en Clu(i) y w (i+j+1) ˆt; 3.3 j + +. Si Iw( ˆT; ˆS) cambia, ve al paso 6; de lo contrario, detente y devuelve Clu(i), Seg(i+j), Ali(i+j), y w (i+j) ˆt; Figura 2: Algoritmo: Segmentación y alineación de temas basada en MI o WMI. y cada oración se segmenta en palabras, cada palabra se reduce a su raíz. Entonces se puede estimar la distribución de probabilidad conjunta P(D, Sd, T). Finalmente, esta distribución se puede utilizar para calcular la MI en nuestro algoritmo. 4.3 Algoritmo Voraz Iterativo Nuestro objetivo es maximizar la función objetivo, I( ˆT; ˆS) o Iw( ˆT; ˆS), que puede medir la dependencia de las ocurrencias de términos en diferentes segmentos. Generalmente, primero no conocemos los pesos estimados de los términos, los cuales dependen de la segmentación y alineación óptimas de los temas, y los grupos de términos. Además, este problema es NP-duro [10], incluso si conocemos los pesos de los términos. Por lo tanto, se desea un algoritmo voraz iterativo para encontrar la mejor solución, aunque probablemente solo se alcancen máximos locales. Presentamos el algoritmo voraz iterativo en la Figura 2 para encontrar un máximo local de I( ˆT; ˆS) o Iw( ˆT; ˆS) con estimación simultánea de pesos de términos. Este algoritmo es iterativo y codicioso para casos de múltiples documentos o casos de un solo documento con estimación de peso de términos y/o co-clustering de términos. De lo contrario, dado que es solo un algoritmo de un paso para resolver la tarea de segmentación de un solo documento [6, 15, 25], se garantiza el máximo global de MI. Mostraremos más adelante que la coagrupación de términos reduce la precisión de los resultados y no es necesaria, y para la segmentación de un solo documento, los pesos de los términos tampoco son requeridos. 4.3.1 Inicialización En el Paso 0, la agrupación inicial de términos Clut y la segmentación y alineación de temas Segd y Alid son importantes para evitar máximos locales y reducir el número de iteraciones. Primero, se puede hacer una buena estimación de los pesos de los términos utilizando las distribuciones de frecuencia de términos a lo largo de las oraciones para cada documento y promediándolas para obtener los valores iniciales de wˆt: wt = ( ED(t) maxt ∈T (ED(t )) )(1 − ES(t) maxt ∈T (ES(t )) ), (6) donde ES(t) = 1 |Dt| d∈Dt (1 − s∈Sd p(s|t)log|Sd| 1 p(s|t) ), donde Dt es el conjunto de documentos que contienen el término t. Luego, para la segmentación inicial Seg(0), simplemente podemos segmentar los documentos de manera equitativa por oraciones. O podemos encontrar la segmentación óptima solo para cada documento d que maximice el WMI, Seg (0) d = argmaxˆsIw(T; ˆS), donde w = w (0) ˆt. Para el alineamiento inicial Ali(0), podemos primero asumir que el orden de los segmentos para cada d es el mismo. Para el agrupamiento inicial de términos Clu(0), las etiquetas de los primeros grupos pueden establecerse al azar, y después de la primera vez del Paso 3, se obtiene un buen agrupamiento inicial de términos. 4.3.2 Diferentes Casos Después de la inicialización, hay tres etapas para diferentes casos. En total hay ocho casos, |D| = 1 o |D| > 1, k = l o k < l, w = 0 o w = 1. La segmentación de un solo documento sin agrupamiento de términos y estimación de peso de términos (|D| = 1, k = l, w = 0) solo requiere la Etapa 1 (Paso 1). Si se requiere el agrupamiento de términos (k < l), la Etapa 2 (Paso 2.1, 2.2 y 2.3) se ejecuta de forma iterativa. Si se requiere la estimación del peso del término (w = 1), la Etapa 3 (Paso 3.1, 3.2 y 3.3) se ejecuta de forma iterativa. Si ambos son requeridos (k < l, w = 1), la Etapa 2 y 3 se ejecutan una tras otra. Para la segmentación de múltiples documentos sin agrupamiento de términos y estimación de peso de términos (|D| > 1, k = l, w = 0), solo se requiere la iteración de los Pasos 2.2 y 2.3. En la Etapa 1, el máximo global se puede encontrar basándose en I( ˆT; ˆS) utilizando programación dinámica en la Sección 4.4. Es imposible encontrar simultáneamente un buen agrupamiento de términos y pesos estimados de términos, ya que al mover un término a un nuevo grupo de términos para maximizar Iw( ˆT; ˆS), no sabemos si el peso de este término debería ser el del nuevo grupo o el del antiguo. Por lo tanto, primero realizamos el agrupamiento de términos en la Etapa 2, y luego estimamos los pesos de los términos en la Etapa 3. En la Etapa 2, el Paso 2.1 consiste en encontrar el mejor agrupamiento de términos y el Paso 2.2 consiste en encontrar la mejor segmentación. Este ciclo se repite para encontrar un máximo local basado en MI I hasta que converge. Los dos pasos son: (1) basados en el agrupamiento de términos actual Cluˆt, para cada documento d, el algoritmo segmenta todas las oraciones Sd en p segmentos secuencialmente (algunos segmentos pueden estar vacíos), y los coloca en los p segmentos ˆS del conjunto de entrenamiento completo D (se verifican todos los casos posibles de segmentación diferente Segd y alineación Alid) para encontrar el caso óptimo, y (2) basado en la segmentación y alineación actual, para cada término t, el algoritmo encuentra el mejor grupo de términos de t basado en la segmentación actual Segd y la alineación Alid. Después de encontrar un buen agrupamiento de términos, se estiman los pesos de los términos si w = 1. En la Etapa 3, al igual que en la Etapa 2, el Paso 3.1 es la reestimación del peso del término y el Paso 3.2 es encontrar una mejor segmentación. Se repiten para encontrar un máximo local basado en WMI Iw hasta que converja. Sin embargo, si el agrupamiento en la Etapa 2 no es preciso, entonces la estimación de peso en la Etapa 3 puede tener un mal resultado. Finalmente, en el Paso 3.3, este algoritmo converge y devuelve la salida. Este algoritmo puede manejar tanto la segmentación de un solo documento como la de múltiples documentos. También puede detectar temas compartidos entre documentos al verificar la proporción de frases superpuestas sobre los mismos temas, como se describe en la Sección 5.2. 4.4 Optimización del algoritmo. En muchos trabajos anteriores sobre segmentación, la programación dinámica es una técnica utilizada para maximizar la función objetivo. De manera similar, en los Pasos 1, 2.2 y 3.2 de nuestro algoritmo, podemos utilizar programación dinámica. Para la Etapa 1, utilizando programación dinámica aún se puede encontrar el óptimo global, pero para la Etapa 2 y la Etapa 3, solo podemos encontrar el óptimo para cada paso de segmentación de temas y alineación de un documento. Aquí solo mostramos la programación dinámica para el Paso 3.2 utilizando WMI (el Paso 1 y 2.2 son similares pero pueden usar I o Iw). Hay dos casos que no se muestran en el algoritmo de la Figura 2: (a) segmentación de un solo documento o segmentación de múltiples documentos con el mismo orden secuencial de segmentos, donde no se requiere alineación, y (b) segmentación de múltiples documentos con diferentes órdenes secuenciales de segmentos, donde la alineación es necesaria. La función de mapeo de alineación del primer caso es simplemente Alid(ˆsi) = ˆsi, mientras que para los últimos casos la función de mapeo de alineación es Alid(ˆsi) = ˆsj, donde i y j pueden ser diferentes. Los pasos computacionales para los dos casos se enumeran a continuación: Caso 1 (sin alineación): Para cada documento d: (1) Calcular pw(ˆt), pw parcial(ˆt, ˆs) y pw parcial(ˆs) sin contar las oraciones de d. Luego, colocar las oraciones de i a j en la Parte k, y calcular WMI parcial PIw( ˆT; ˆsk(si, si+1, ..., sj)) ˆt∈ ˆT pw(ˆt, ˆsk)log pw(ˆt, ˆsk) pw(ˆt)pw(ˆsk) , donde Alid(si, si+1, ..., sj) = k, k ∈ {1, 2, ..., p}, 1 ≤ i ≤ j ≤ nd, y Segd(sq) = ˆsk para todo i ≤ q ≤ j. (2) Dejar que M(sm, 1) = PIw( ˆT; ˆs1(s1, s2, ..., sm)). Entonces M(sm, L) = maxi[M(si−1, L − 1) + PIw( ˆT; ˆsL(si, ..., sm))], donde 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, y cuando i > m, no se colocan oraciones en ˆsk al calcular PIw (nota PIw( ˆT; ˆs(si, ..., sm)) = 0 para la segmentación de un solo documento). (3) Finalmente M(snd , p) = maxi[M(si−1, p − 1)+ PIw( ˆT; ˆsp(si, ..., snd ))], donde 1 ≤ i ≤ nd+1. Se encuentra el Iw óptimo y la segmentación correspondiente es la mejor. Caso 2 (alineación requerida): Para cada documento d: (1) Calcular pw(ˆt), pw parcial(ˆt, ˆs), y pw parcial(ˆs), y PIw( ˆT; ˆsk(si, si+1, ..., sj)) de manera similar al Caso 1. (2) Sea M(sm, 1, k) = PIw( ˆT; ˆsk(s1, s2, ..., sm)), donde k ∈ {1, 2, ..., p}. Entonces M(sm, L, kL) = maxi,j[M(si−1, L − 1, kL/j) + PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., sm))], donde 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, kL ∈ Set(p, L), que es el conjunto de todos los p! Hay L!(p−L)! combinaciones de L segmentos elegidos de entre todos los p segmentos, j ∈ kL, el conjunto de L segmentos elegidos de entre todos los p segmentos, y kL/j es la combinación de L − 1 segmentos en kL excepto el Segmento j. (3) Finalmente, M(snd , p, kp) = maxi,j[M(si−1, p − 1, kp/j) +PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., snd ))], donde kp es simplemente la combinación de todos los p segmentos y 1 ≤ i ≤ nd + 1, que es el óptimo Iw y la segmentación correspondiente es la mejor. Los pasos de los Casos 1 y 2 son similares, excepto que en el Caso 2, se considera el alineamiento además de la segmentación. Primero, se calculan los elementos básicos de probabilidad para computar Iw excluyendo el documento d, y luego se calcula el WMI parcial colocando cada segmento secuencial posible (incluido el segmento vacío) de d en cada segmento del conjunto. Segundo, se encuentra la suma óptima de PIw para L segmentos y las m oraciones más a la izquierda, M(sm, L). Finalmente, se encuentra el WMI máximo entre diferentes sumas de M(sm, p − 1) y PIw para el Segmento p. 5. EXPERIMENTOS En esta sección se probará la segmentación de un solo documento, la detección de temas compartidos y la segmentación de múltiples documentos. Se estudian diferentes hiperparámetros de nuestro método. Para mayor comodidad, nos referimos al método utilizando I como MIk si w = 0, e Iw como WMIk si w = 2 o como WMIk si w = 1, donde k es el número de grupos de términos, y si k = l, donde l es el número total de términos, entonces no se requiere agrupación de términos, es decir, El primer conjunto de datos que probamos es uno sintético utilizado en investigaciones anteriores [6, 15, 25] y muchos otros artículos. Tiene 700 muestras. Cada uno es una concatenación de diez segmentos. Cada segmento es la primera oración seleccionada al azar de las n primeras oraciones del corpus Brown, que se supone que tienen un tema diferente entre sí. Actualmente, los mejores resultados en este conjunto de datos son logrados por Ji et al. [15]. Para comparar el rendimiento de nuestros métodos, se aplica el criterio ampliamente utilizado en investigaciones anteriores en lugar del criterio imparcial introducido en [20]. Elige un par de palabras al azar. Si se encuentran en segmentos diferentes (diferentes) para la segmentación real (real), pero se predicen (pred) como en el mismo segmento, es un error. Si están en el mismo segmento (mismo), pero se predice que están en segmentos diferentes, es una falsa alarma. Por lo tanto, la tasa de error se calcula utilizando la siguiente ecuación: p(err|real, pred) = p(miss|real, pred, diff)p(diff|real) + p(false alarm|real, pred, same)p(same|real). 5.1.2 Resultados del Experimento Probamos el caso cuando se conoce el número de segmentos. La Tabla 1 muestra los resultados de nuestros métodos con diferentes valores de hiperparámetros y tres enfoques anteriores, C99[25], U00[6] y ADDP03[15], en este conjunto de datos cuando se conoce el número de segmento. En WMI para la segmentación de un solo documento, los pesos de los términos se calculan de la siguiente manera: wˆt = 1−EˆS(ˆt)/maxˆt ∈ ˆT (EˆS(ˆt)). Para este caso, nuestros métodos MIl y WMIl superan a todos los enfoques anteriores. Comparamos nuestros métodos con ADDP03 utilizando una prueba t de una muestra unilateral y los valores de p se muestran en la Tabla 2. A partir de los valores p, podemos ver que en su mayoría las diferencias son muy significativas. También comparamos las tasas de error entre nuestros dos métodos utilizando una prueba t de dos muestras de dos colas para verificar la hipótesis de que son iguales. No podemos rechazar la hipótesis de que son iguales, por lo que las diferencias no son significativas, a pesar de que todas las tasas de error para MIl son más pequeñas que las de WMIl. Sin embargo, podemos concluir que los pesos de los términos contribuyen poco en la segmentación de un solo documento. Los resultados también muestran que el uso de MI con co-clustering de términos (k = 100) disminuye el rendimiento. Probamos diferentes números de grupos de términos y encontramos que el rendimiento mejora cuando el número de grupos aumenta hasta llegar a l. WMIk<l tiene resultados similares que no mostramos en la tabla. Como se mencionó anteriormente, el uso de MI puede ser inconsistente en los límites óptimos dados diferentes números de segmentos. Esta situación ocurre especialmente cuando las similitudes entre los segmentos son bastante diferentes, es decir, algunas transiciones son muy obvias, mientras que otras no lo son. Esto se debe a que normalmente un documento es una estructura jerárquica en lugar de una estructura únicamente secuencial. Cuando los segmentos no están en el mismo nivel, esta situación puede ocurrir. Por lo tanto, se desea un enfoque de segmentación jerárquica de temas, y la estructura depende en gran medida del número de segmentos para cada nodo interno y del criterio de parada de la división. Para este conjunto de datos de segmentación de un solo documento, dado que es solo un conjunto sintético, que es simplemente una concatenación de varios segmentos sobre diferentes temas, es razonable que enfoques simplemente basados en la frecuencia de términos tengan un buen rendimiento. Normalmente, para las tareas de segmentación de documentos coherentes en subtemas, la efectividad disminuye mucho. 5.2 Detección de Temas Compartidos 5.2.1 Datos de Prueba y Evaluación El segundo conjunto de datos contiene 80 artículos de noticias de Google News. Hay ocho temas y cada uno tiene 10 artículos. Dividimos aleatoriamente el conjunto en subconjuntos con diferentes números de documentos y cada subconjunto tiene los ocho temas. Comparamos nuestro enfoque MIl y WMIl con LDA [4]. LDA trata un documento en el conjunto de datos como una bolsa de palabras, encuentra su distribución en temas y su tema principal. MIl y WMIl ven cada oración como un conjunto de palabras y la etiquetan con una etiqueta de tema. Luego, para cada par de documentos, LDA determina si están en el mismo tema, mientras que MIl y Table 3: Detección de Temas Compartidos: Tasas de Error Promedio para Diferentes Números de Documentos en Cada Subconjunto #Doc 10 20 40 80 LDA 8.89% 16.33% 1.35% 0.60% MIl, θ = 0.6 4.17% 1.71% 1.47% 0.0% WMIl, θ = 0.8 18.6% 3.16% 1.92% 0.0% WMIl comprueba si la proporción de oraciones superpuestas en el mismo tema es mayor que el umbral ajustable θ. Es decir, en MIl y WMIl, para un par de documentos d, d , si [ s∈Sd,s ∈Sd 1(temas=temas )/min(|Sd|, |Sd|)] > θ, donde Sd es el conjunto de oraciones de d, y |Sd| es el número de oraciones de d, entonces d y d comparten el tema. Para un par de documentos seleccionados al azar, la tasa de error se calcula utilizando la siguiente ecuación: p(err|real, pred) = p(miss|real, pred, same)p(same|real) + p(false alarm|real, pred, diff)p(diff|real), donde un error significa si tienen el mismo tema (same) para el caso real (real), pero se predice (pred) como en un tema diferente. Si están en diferentes temas (diff), pero se predicen como en el mismo tema, es una falsa alarma. 5.2.2 Resultados del Experimento Los resultados se muestran en la Tabla 3. Si la mayoría de los documentos tienen diferentes temas, en WMIl, la estimación de los pesos de términos en la Ecuación (3) no es correcta. Por lo tanto, no se espera que WMIL tenga un mejor rendimiento que MIL cuando la mayoría de los documentos tienen temas diferentes. Cuando hay menos documentos en un subconjunto con el mismo número de temas, más documentos tienen temas diferentes, por lo que WMIl es peor que MIl. Podemos ver que en la mayoría de los casos, MIl tiene un rendimiento mejor (o al menos similar) que LDA. Después de la detección de temas compartidos, se puede llevar a cabo la segmentación de documentos con los temas compartidos. 5.3 Segmentación de múltiples documentos 5.3.1 Datos de prueba y evaluación Para la segmentación y alineación de múltiples documentos, nuestro objetivo es identificar estos segmentos sobre el mismo tema entre varios documentos similares con temas compartidos. Se espera que al utilizar pesos de términos, Iw funcione mejor que I, ya que sin pesos de términos, el resultado se ve seriamente afectado por palabras de parada dependientes del documento y palabras ruidosas que dependen del estilo de escritura personal. Es más probable tratar los mismos segmentos de diferentes documentos como segmentos diferentes bajo el efecto de palabras de parada dependientes del documento y palabras ruidosas. Los pesos de términos pueden reducir el efecto de las palabras de parada dependientes del documento y las palabras ruidosas al darle más peso a los términos de señal. El conjunto de datos para la segmentación y alineación de múltiples documentos tiene un total de 102 muestras y 2264 oraciones. Cada uno es la parte de introducción de un informe de laboratorio seleccionado del curso de Biol 240W, Universidad Estatal de Pensilvania. Cada muestra tiene dos segmentos, la introducción de las hormonas vegetales y el contenido en el laboratorio. El rango de longitud de las muestras va desde dos hasta 56 oraciones. Algunas muestras solo tienen una parte y algunas tienen un orden inverso de estos dos segmentos. No es difícil identificar el límite entre dos segmentos para los humanos. Etiquetamos cada oración manualmente para evaluación. El criterio de evaluación consiste en utilizar la proporción del número de oraciones con etiquetas de segmento incorrectas predichas en el número total de oraciones en toda la Tabla de entrenamiento. Para mostrar los beneficios de la segmentación y alineación de múltiples documentos, comparamos nuestro método con diferentes parámetros en diferentes particiones del mismo conjunto de entrenamiento. Excepto los casos en los que el número de documentos es 102 y uno (que son casos especiales de uso del conjunto completo y la segmentación pura de un solo documento), dividimos aleatoriamente el conjunto de entrenamiento en m particiones, y cada una tiene 51, 34, 20, 10, 5 y 2 muestras de documentos. Luego aplicamos nuestros métodos en cada partición y calculamos la tasa de error de todo el conjunto de entrenamiento. Cada caso se repitió 10 veces para calcular las tasas de error promedio. Para diferentes particiones del conjunto de entrenamiento, se utilizan diferentes valores de k, ya que el número de términos aumenta cuando el número de documentos en cada partición aumenta. 5.3.2 Resultados del Experimento De los resultados del experimento en la Tabla 4, podemos ver las siguientes observaciones: (1) Cuando el número de documentos aumenta, todos los métodos tienen un mejor rendimiento. Solo de uno a dos documentos, el MIL ha disminuido un poco. Podemos observar esto en la Figura 3 en el punto del número de documento = 2. La mayoría de las curvas incluso tienen los peores resultados en este punto. Hay dos razones. Primero, porque las muestras votan por la mejor segmentación y alineación de múltiples documentos, pero si solo se comparan dos documentos entre sí, aquel con segmentos faltantes o una secuencia totalmente diferente afectará la correcta segmentación y alineación del otro. Segundo, como se señaló al principio de esta sección, si dos documentos tienen más palabras de parada dependientes del documento o palabras ruidosas que palabras clave, entonces el algoritmo puede considerarlos como dos segmentos diferentes y faltar el otro segmento. Generalmente, solo podemos esperar un mejor rendimiento cuando el número de documentos es mayor que el número de segmentos. (2) Excepto en la segmentación de un solo documento, WMIl siempre es mejor que MIl, y cuando el número de documentos se acerca a uno o aumenta a un número muy grande, sus rendimientos se vuelven más cercanos. La Tabla 5 muestra los valores p de la prueba t de dos muestras unilaterales entre MIl y WMIl. También podemos observar esta tendencia a partir de los valores de p. Cuando el número de documento es igual a 5, alcanzamos el valor p más pequeño y la mayor diferencia entre las tasas de error de MIl y WMIl. Para la Tabla 6 de un solo documento: Segmentación de múltiples documentos: Tasa de error promedio para el Documento Número = 5 en cada subconjunto con Diferente Número de Agrupaciones de Términos #Agrupaciones 75 100 150 250 l MIk 24.67% 24.54% 23.91% 22.59% 15.77% de segmentación, WMIl es incluso un poco peor que MIl, lo cual es similar a los resultados de la segmentación de un solo documento en el primer conjunto de datos. La razón es que para la segmentación de un solo documento, no podemos estimar con precisión los pesos de los términos, ya que no tenemos disponibles varios documentos. (3) El uso de agrupamiento de términos generalmente produce resultados peores que MIl y WMIl. (4) El uso de agrupamiento de términos en WMIk es aún peor que en MIk, ya que en WMIk los grupos de términos se encuentran primero utilizando I antes que Iw. Si los grupos de términos no son correctos, entonces los pesos de los términos se estiman peor, lo que puede llevar a que el algoritmo obtenga resultados aún peores. De los resultados también encontramos que en la segmentación y alineación de múltiples documentos, la mayoría de los documentos con segmentos faltantes y en orden inverso son identificados correctamente. La Tabla 6 ilustra los resultados del experimento para el caso de 20 particiones (cada una con cinco muestras de documentos) del conjunto de entrenamiento y la segmentación y alineación de temas utilizando MIk con diferentes números de grupos de términos k. Observa que cuando el número de grupos de términos aumenta, la tasa de error disminuye. Sin agrupamiento de términos, obtenemos el mejor resultado. No mostramos resultados para WMIk con agrupamiento de términos, pero los resultados son similares. También probamos WMIL con diferentes hiperparámetros de a y b para ajustar los pesos de los términos. Los resultados se presentan en la Figura 3. Se demostró que el caso predeterminado WMIl: a = 1, b = 1 dio los mejores resultados para diferentes particiones del conjunto de entrenamiento. Podemos observar la tendencia de que cuando el número de documentos es muy pequeño o grande, la diferencia entre MIl: a = 0, b = 0 y WMIl: a = 1, b = 1 se vuelve bastante pequeña. Cuando el número de documentos no es grande (aproximadamente de 2 a 10), todos los casos que utilizan pesos de términos tienen un mejor rendimiento que MIl: a = 0, b = 0 sin pesos de términos, pero cuando el número de documentos aumenta, los casos WMIl: a = 1, b = 0 y WMIl: a = 2, b = 1 empeoran en comparación con MIl: a = 0, b = 0. Cuando el número de documentos se vuelve muy grande, son aún peores que los casos con números de documentos pequeños. Esto significa que es muy importante encontrar una forma adecuada de estimar los pesos de los términos para el criterio de WMI. La Figura 4 muestra los pesos de los términos aprendidos de todo el conjunto de entrenamiento. Cuatro tipos de palabras se categorizan de manera aproximada aunque la transición entre ellas es sutil. La Figura 5 ilustra el cambio en la información mutua (ponderada) para MIl y WMIl. Como era de esperar, la información mutua para MIl aumenta de forma monótona con el número de pasos, mientras que WMIl no lo hace. Finalmente, MIl y WMIl son escalables, con complejidad computacional mostrada en la Figura 6. Una ventaja de nuestro enfoque basado en MI es que no es necesario eliminar las palabras vacías. Otra ventaja importante es que no hay hiperparámetros necesarios que ajustar. En la segmentación de un solo documento, el rendimiento basado en MI es aún mejor que el basado en WMI, por lo que no se requiere un hiperparámetro adicional. En la segmentación de múltiples documentos, mostramos en el experimento que a = 1 y b = 1 es lo mejor. Nuestro método otorga más peso a los términos de señal. Sin embargo, generalmente los términos o frases de señal aparecen al principio de un segmento, mientras que el final del segmento puede ser 1 2 5 10 20 34 51 102 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 Número de Documento Tasa de Error MIl :a=0,b=0 WMI l :a=1,b=1 WMI l :a=1,b=0 WMI l :a=2,b=1 Figura 3: Tasas de error para diferentes hiperparámetros de pesos de términos. 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Entropía de Documento Normalizada Entropía de Segmento Normalizada Palabras ruidosas Palabras de señal Palabras de parada comunes Palabras de parada de Doc-dep Figura 4: Pesos de términos aprendidos de todo el conjunto de entrenamiento. 0 100 200 300 400 500 600 0.06 0.08 0.1 0.12 0.14 0.16 0.18 Número de Pasos (Ponderado) Información Mutua MI l WMI l Figura 5: Cambio en la MI (ponderada) para MIl y WMIl. 0 20 40 60 80 100 120 0 200 400 600 800 1000 1200 1400 1600 1800 2000 Número de Documento Tiempo de Convergencia (seg) MI l WMI l Figura 6: Tiempo de convergencia para MIl y WMIl. muy ruidoso. Una posible solución es dar más peso a los términos al principio de cada segmento. Además, cuando la longitud de los segmentos es bastante diferente, los segmentos largos tienen frecuencias de términos mucho más altas, por lo que pueden dominar los límites de segmentación. La normalización de las frecuencias de términos en relación con la longitud del segmento puede ser útil. 6. CONCLUSIONES Y TRABAJO FUTURO Propusimos un método novedoso para la segmentación y alineación de temas en múltiples documentos basado en información mutua ponderada, que también puede manejar casos de un solo documento. Utilizamos programación dinámica para optimizar nuestro algoritmo. Nuestro enfoque supera a todos los métodos anteriores en casos de un solo documento. Además, también demostramos que realizar segmentación entre varios documentos puede mejorar el rendimiento enormemente. Nuestros resultados también demostraron que el uso de la información mutua ponderada puede aprovechar la información de varios documentos para lograr un mejor rendimiento. Solo probamos nuestro método en conjuntos de datos limitados. Se deben probar más conjuntos de datos, especialmente los complicados. Deberían compararse más métodos anteriores. Además, las segmentaciones naturales como los párrafos son pistas que se pueden utilizar para encontrar los límites óptimos. El aprendizaje supervisado también puede ser considerado. 7. AGRADECIMIENTOS Los autores desean agradecer a Xiang Ji y al Prof. J. Scott Payne por su ayuda. 8. REFERENCIAS [1] A. Banerjee, I. Ghillon, J. Ghosh, S. Merugu y D. Modha. Un enfoque generalizado de entropía máxima para la coagrupación de Bregman y la aproximación de matrices. En Actas de SIGKDD, 2004. [2] R. Bekkerman, R. El-Yaniv y A. McCallum. Agrupación distribucional de múltiples vías a través de interacciones por pares. En Actas de ICML, 2005. [3] D. M. Blei y P. J. Moreno. Segmentación de temas con un modelo oculto de Markov de aspectos. En Actas de SIGIR, 2001. [4] D. M. Blei, A. Ng y M. Jordan. Asignación latente de Dirichlet. Revista de Investigación en Aprendizaje Automático, 3:993-1022, 2003. [5] T. Brants, F. Chen e I. Tsochantaridis. Segmentación de documentos basada en temas con análisis semántico latente probabilístico. En Actas de CIKM, 2002. [6] F. Choi. Avances en la segmentación lineal de texto independiente del dominio. En Actas de la NAACL, 2000. [7] H. Christensen, B. Kolluru, Y. Gotoh y S. Renals. Segmentación de máxima entropía de noticias de transmisión. En Actas de ICASSP, 2005. [8] T. Cover y J. Thomas. Elementos de la teoría de la información. John Wiley and Sons, Nueva York, EE. UU., 1991. [9] S. Deerwester, S. Dumais, G. Furnas, T. Landauer y R. Harshman. Indexación mediante análisis semántico latente. Revista de la Sociedad Americana de Sistemas de Información, 1990. [10] I. Dhillon, S. Mallela y D. Modha. Co-clustering informativo-teórico. En Actas de SIGKDD, 2003. [11] M. Hajime, H. Takeo y O. Manabu. Segmentación de texto con múltiples señales lingüísticas superficiales. En Actas de COLING-ACL, 1998. [12] T. K. Ho. Detención de palabras y identificación para el reconocimiento de texto adaptativo. Revista Internacional de Análisis y Reconocimiento de Documentos, 3(1), agosto de 2000. [13] T. Hofmann. Análisis semántico latente probabilístico. En Actas de la UAI99, 1999. [14] X. Ji y H. Zha. Correlación de la sumarización de un par de documentos multilingües. En Actas de RIDE, 2003. [15] X. Ji y H. Zha. Segmentación de texto independiente del dominio utilizando difusión anisotrópica y programación dinámica. En Actas de SIGIR, 2003. [16] X. Ji y H. Zha. Extrayendo temas compartidos de múltiples documentos. En Actas de la 7ª PAKDD, 2003. [17] J. Lafferty, A. McCallum y F. Pereira. Campos aleatorios condicionales: Modelos probabilísticos para segmentar y etiquetar datos de secuencias. En Actas de ICML, 2001. [18] T. Li, S. Ma y M. Ogihara. Criterio basado en la entropía en la agrupación categórica. En Actas de ICML, 2004. [19] A. McCallum, D. Freitag y F. Pereira. Modelos de máxima entropía de Markov para extracción de información y segmentación. En Actas de ICML, 2000. [20] L. Pevzner y M. Hearst. Una crítica y mejora de una métrica de evaluación para la segmentación de texto. Lingüística Computacional, 28(1):19-36, 2002. [21] J. C. Reynar. Modelos estadísticos para la segmentación de temas. En Actas de ACL, 1999. [22] G. Salton y M. McGill. Introducción a la Recuperación de Información Moderna. McGraw Hill, 1983. [23] B. \n\nMcGraw Hill, 1983. [23] B. Sun, Q. Tan, P. Mitra y C. L. Giles. Extracción y búsqueda de fórmulas químicas en documentos de texto en la web. En Actas de WWW, 2007. [24] B. Sun, D. Zhou, H. Zha, y J. I'm sorry, but \"Yen\" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish? Segmentación y alineación de texto multitarea basada en información mutua ponderada. En Actas de CIKM, 2006. [25] M. Utiyama y H. Isahara. Un modelo estadístico para la segmentación de texto independiente del dominio. En Actas de la 39ª ACL, 1999. [26] C. Wayne. Detección y <br>seguimiento</br> de temas multilingües: Investigación exitosa habilitada por corpora y evaluación. En Actas de LREC, 2000. [27] J. Yamron, I. Carp, L. Gillick, S. Lowe y P. van Mulbregt. Un enfoque de modelo oculto de Markov para la segmentación de texto y <br>seguimiento</br> de eventos. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "topic segmentation": {
            "translated_key": "segmentación de temas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "<br>topic segmentation</br> with Shared Topic Detection and Alignment of Multiple Documents Bingjun Sun*, Prasenjit Mitra*† , Hongyuan Zha‡ , C. Lee Giles*† , John Yen*† *Department of Computer Science and Engineering † College of Information Sciences and Technology The Pennsylvania State University University Park, PA 16802 ‡ College of Computing The Georgia Institute of Technology Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu ABSTRACT Topic detection and tracking [26] and <br>topic segmentation</br> [15] play an important role in capturing the local and sequential information of documents.",
                "Previous work in this area usually focuses on single documents, although similar multiple documents are available in many domains.",
                "In this paper, we introduce a novel unsupervised method for shared topic detection and <br>topic segmentation</br> of multiple similar documents based on mutual information (MI) and weighted mutual information (WMI) that is a combination of MI and term weights.",
                "The basic idea is that the optimal segmentation maximizes MI(or WMI).",
                "Our approach can detect shared topics among documents.",
                "It can find the optimal boundaries in a document, and align segments among documents at the same time.",
                "It also can handle single-document segmentation as a special case of the multi-document segmentation and alignment.",
                "Our methods can identify and strengthen cue terms that can be used for segmentation and partially remove stop words by using term weights based on entropy learned from multiple documents.",
                "Our experimental results show that our algorithm works well for the tasks of single-document segmentation, shared topic detection, and multi-document segmentation.",
                "Utilizing information from multiple documents can tremendously improve the performance of <br>topic segmentation</br>, and using WMI is even better than using MI for the multi-document segmentation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Clustering; H.3.1 [Information Storage and Retrieval]: Content Analysis and IndexingLinguistic processing; I.2.7 [Artificial Intelligence]: Natural Language Processing-Text analysis; I.5.3 [Pattern Recognition]: Clustering-Algorithms;Similarity measures General Terms Algorithms, Design, Experimentation 1.",
                "INTRODUCTION Many researchers have worked on topic detection and tracking (TDT) [26] and <br>topic segmentation</br> during the past decade.",
                "<br>topic segmentation</br> intends to identify the boundaries in a document with the goal to capture the latent topical structure.",
                "<br>topic segmentation</br> tasks usually fall into two categories [15]: text stream segmentation where topic transition is identified, and coherent document segmentation in which documents are split into sub-topics.",
                "The former category has applications in automatic speech recognition, while the latter one has more applications such as partial-text query of long documents in information retrieval, text summary, and quality measurement of multiple documents.",
                "Previous research in connection with TDT falls into the former category, targeted on topic tracking of broadcast speech data and newswire text, while the latter category has not been studied very well.",
                "Traditional approaches perform <br>topic segmentation</br> on documents one at a time [15, 25, 6].",
                "Most of them perform badly in subtle tasks like coherent document segmentation [15].",
                "Often, end-users seek documents that have the similar content.",
                "Search engines, like, Google, provide links to obtain similar pages.",
                "At a finer granularity, users may actually be looking to obtain sections of a document similar to a particular section that presumably discusses a topic of the users interest.",
                "Thus, the extension of <br>topic segmentation</br> from single documents to identifying similar segments from multiple similar documents with the same topic is a natural and necessary direction, and multi-document <br>topic segmentation</br> is expected to have a better performance since more information is utilized.",
                "Traditional approaches using similarity measurement based on term frequency generally have the same assumption that similar vocabulary tends to be in a coherent topic segment [15, 25, 6].",
                "However, they usually suffer the issue of identifying stop words.",
                "For example, additional document-dependent stop words are removed together with the generic stop words in [15].",
                "There are two reasons that we do not remove stop words directly.",
                "First, identifying stop words is another issue [12] that requires estimation in each domain.",
                "Removing common stop words may result in the loss of useful information in a specific domain.",
                "Second, even though stop words can be identified, hard classification of stop words and nonstop words cannot represent the gradually changing amount of information content of each word.",
                "We employ a soft classification using term weights.",
                "In this paper, we view the problem of <br>topic segmentation</br> as an optimization issue using information theoretic techniques to find the optimal boundaries of a document given the number of text segments so as to minimize the loss of mutual information (MI) (or a weighted mutual information (WMI)) after segmentation and alignment.",
                "This is equal to maximizing the MI (or WMI).",
                "The MI focuses on measuring the difference among segments whereas previous research focused on finding the similarity (e.g. cosine distance) of segments [15, 25, 6].",
                "Topic alignment of multiple similar documents can be achieved by clustering sentences on the same topic into the same cluster.",
                "Single-document <br>topic segmentation</br> is just a special case of the multi-document <br>topic segmentation</br> and alignment problem.",
                "Terms can be co-clustered as in [10] at the same time, given the number of clusters, but our experimental results show that this method results in a worse segmentation (see Tables 1, 4, and 6).",
                "Usually, human readers can identify topic transition based on cue words, and can ignore stop words.",
                "Inspired by this, we give each term (or term cluster) a weight based on entropy among different documents and different segments of documents.",
                "Not only can this approach increase the contribution of cue words, but it can also decrease the effect of common stop words, noisy word, and document-dependent stop words.",
                "These words are common in a document.",
                "Many methods based on sentence similarity require that these words are removed before <br>topic segmentation</br> can be performed [15].",
                "Our results in Figure 3 show that term weights are useful for multi-document <br>topic segmentation</br> and alignment.",
                "The major contribution of this paper is that it introduces a novel method for <br>topic segmentation</br> using MI and shows that this method performs better than previously used criteria.",
                "Also, we have addressed the problem of <br>topic segmentation</br> and alignment across multiple documents, whereas most existing research focused on segmentation of single documents.",
                "Multi-document segmentation and alignment can utilize information from similar documents and improves the performance of <br>topic segmentation</br> greatly.",
                "Obviously, our approach can handle single documents as a special case when multiple documents are unavailable.",
                "It can detect shared topics among documents to judge if they are multiple documents on the same topic.",
                "We also introduce the new criterion of WMI based on term weights learned from multiple similar documents, which can improve performance of <br>topic segmentation</br> further.",
                "We propose an iterative greedy algorithm based on dynamic programming and show that it works well in practice.",
                "Some of our prior work is in [24].",
                "The rest of this paper is organized as follows: In Section 2, we review related work.",
                "Section 3 contains a formulation of the problem of <br>topic segmentation</br> and alignment of multiple documents with term co-clustering, a review of the criterion of MI for clustering, and finally an introduction to WMI.",
                "In Section 4, we first propose the iterative greedy algorithm of <br>topic segmentation</br> and alignment with term co-clustering, and then describe how the algorithm can be optimized by usFigure 1: Illustration of multi-document segmentation and alignment. ing dynamic programming.",
                "In Section 5, experiments about single-document segmentation, shared topic detection, and multi-document segmentation are described, and results are presented and discussed to evaluate the performance of our algorithm.",
                "Conclusions and some future directions of the research work are discussed in Section 6. 2.",
                "PREVIOUS WORK Generally, the existing approaches to text segmentation fall into two categories: supervised learning [19, 17, 23] and unsupervised learning [3, 27, 5, 6, 15, 25, 21].",
                "Supervised learning usually has good performance, since it learns functions from labelled training sets.",
                "However, often getting large training sets with manual labels on document sentences is prohibitively expensive, so unsupervised approaches are desired.",
                "Some models consider dependence between sentences and sections, such as Hidden Markov Model [3, 27], Maximum Entropy Markov Model [19], and Conditional Random Fields [17], while many other approaches are based on lexical cohesion or similarity of sentences [5, 6, 15, 25, 21].",
                "Some approaches also focus on cue words as hints of topic transitions [11].",
                "While some existing methods only consider information in single documents [6, 15], others utilize multiple documents [16, 14].",
                "There are not many works in the latter category, even though the performance of segmentation is expected to be better with utilization of information from multiple documents.",
                "Previous research studied methods to find shared topics [16] and <br>topic segmentation</br> and summarization between just a pair of documents [14].",
                "Text classification and clustering is a related research area which categorizes documents into groups using supervised or unsupervised methods.",
                "Topical classification or clustering is an important direction in this area, especially co-clustering of documents and terms, such as LSA [9], PLSA [13], and approaches based on distances and bipartite graph partitioning [28] or maximum MI [2, 10], or maximum entropy [1, 18].",
                "Criteria of these approaches can be utilized in the issue of <br>topic segmentation</br>.",
                "Some of those methods have been extended into the area of <br>topic segmentation</br>, such as PLSA [5] and maximum entropy [7], but to our best knowledge, using MI for <br>topic segmentation</br> has not been studied. 3.",
                "PROBLEM FORMULATION Our goal is to segment documents and align the segments across documents (Figure 1).",
                "Let T be the set of terms {t1, t2, ..., tl}, which appear in the unlabelled set of documents D = {d1, d2, ..., dm}.",
                "Let Sd be the set of sentences for document d ∈ D, i.e. {s1, s2, ..., snd }.",
                "We have a 3D matrix of term frequency, in which the three dimensions are random variables of D, Sd, and T. Sd actually is a random vector including a random variable for each d ∈ D. The term frequency can be used to estimate the joint probability distribution P(D, Sd, T), which is p(t, d, s) = T(t, d, s)/ND, where T(t, d, s) is the number of t in ds sentence s and ND is the total number of terms in D. ˆS represents the set of segments {ˆs1, ˆs2, ..., ˆsp} after segmentation and alignment among multiple documents, where the number of segments | ˆS| = p. A segment ˆsi of document d is a sequence of adjacent sentences in d. Since for different documents si may discuss different sub-topics, our goal is to cluster adjacent sentences in each document into segments, and align similar segments among documents, so that for different documents ˆsi is about the same sub-topic.",
                "The goal is to find the optimal <br>topic segmentation</br> and alignment mapping Segd(si) : {s1, s2, ..., snd } → {ˆs1, ˆs2, ..., ˆsp} and Alid(ˆsi) : {ˆs1, ˆs2, ..., ˆsp} → {ˆs1, ˆs2, ..., ˆsp}, for all d ∈ D, where ˆsi is ith segment with the constraint that only adjacent sentences can be mapped to the same segment, i.e. for d, {si, si+1, ..., sj} → {ˆsq}, where q ∈ {1, ..., p}, where p is the segment number, and if i > j, then for d, ˆsq is missing.",
                "After segmentation and alignment, random vector Sd becomes an aligned random variable ˆS.",
                "Thus, P(D, Sd, T) becomes P(D, ˆS, T).",
                "Term co-clustering is a technique that has been employed [10] to improve the accuracy of document clustering.",
                "We evaluate the effect of it for <br>topic segmentation</br>.",
                "A term t is mapped to exactly one term cluster.",
                "Term co-clustering involves simultaneously finding the optimal term clustering mapping Clu(t) : {t1, t2, ..., tl} → {ˆt1, ˆt2, ..., ˆtk}, where k ≤ l, l is the total number of words in all the documents, and k is the number of clusters. 4.",
                "METHODOLOGY We now describe a novel algorithm which can handle singledocument segmentation, shared topic detection, and multidocument segmentation and alignment based on MI or WMI. 4.1 Mutual Information MI I(X; Y ) is a quantity to measure the amount of information which is contained in two or more random variables [8, 10].",
                "For the case of two random variables, we have I(X; Y ) = x∈X y∈Y p(x, y)log p(x, y) p(x)p(y) , (1) Obviously, when random variables X and Y are independent, I(X; Y ) = 0.",
                "Thus, intuitively, the value of MI depends on how random variables are dependent on each other.",
                "The optimal co-clustering is the mapping Clux : X → ˆX and Cluy : Y → ˆY that minimizes the loss: I(X; Y ) − I( ˆX; ˆY ), which is equal to maximizing I( ˆX; ˆY ).",
                "This is the criterion of MI for clustering.",
                "In the case of <br>topic segmentation</br>, the two random variables are the term variable T and the segment variable S, and each sample is an occurrence of a term T = t in a particular segment S = s. I(T; S) is used to measure how dependent T and S are.",
                "However, I(T; S) cannot be computed for documents before segmentation, since we do not have a set of S due to the fact that sentences of Document d, si ∈ Sd, is not aligned with other documents.",
                "Thus, instead of minimizing the loss of MI, we can maximize MI after <br>topic segmentation</br>, computed as: I( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS p(ˆt, ˆs)log p(ˆt, ˆs) p(ˆt)p(ˆs) , (2) where p(ˆt, ˆs) are estimated by the term frequency tf of Term Cluster ˆt and Segment ˆs in the training set D. Note that here a segment ˆs includes sentences about the the same topic among all documents.",
                "The optimal solution is the mapping Clut : T → ˆT, Segd : Sd → ˆS , and Alid : ˆS → ˆS, which maximizes I( ˆT; ˆS). 4.2 Weighted Mutual Information In <br>topic segmentation</br> and alignment of multiple documents, if P(D, ˆS, T) is known, based on the marginal distributions P(D|T) and P( ˆS|T) for each term t ∈ T, we can categorize terms into four types in the data set: • Common stop words are common both along the dimensions of documents and segments. • Document-dependent stop words that depends on the personal writing style are common only along the dimension of segments for some documents. • Cue words are the most important elements for segmentation.",
                "They are common along the dimension of documents only for the same segment, and they are not common along the dimensions of segments. • Noisy words are other words which are not common along both dimensions.",
                "Entropy based on P(D|T) and P( ˆS|T) can be used to identify different types of terms.",
                "To reinforce the contribution of cue words in the MI computation, and simultaneously reduce the effect of the other three types of words, similar as the idea of the tf-idf weight [22], we use entropies of each term along the dimensions of document D and segment ˆS, i.e.",
                "ED(ˆt) and EˆS(ˆt), to compute the weight.",
                "A cue word usually has a large value of ED(ˆt) but a small value of EˆS(ˆt).",
                "We introduce term weights (or term cluster weights) wˆt = ( ED(ˆt) maxˆt ∈ ˆT (ED(ˆt )) )a (1 − EˆS(ˆt) maxˆt ∈ ˆT (EˆS(ˆt )) )b , (3) where ED(ˆt) = d∈D p(d|ˆt)log|D| 1 p(d|ˆt) , EˆS(ˆt) = ˆs∈ ˆS p(ˆs|ˆt)log| ˆS| 1 p(ˆs|ˆt) , and a > 0 and b > 0 are powers to adjust term weights.",
                "Usually a = 1 and b = 1 as default, and maxˆt ∈ ˆT (ED(ˆt )) and maxˆt ∈ ˆT (EˆS(ˆt )) are used to normalize the entropy values.",
                "Term cluster weights are used to adjust p(ˆt, ˆs), pw(ˆt, ˆs) = wˆtp(ˆt, ˆs) ˆt∈ ˆT ;ˆs∈ ˆS wˆtp(ˆt, ˆs) , (4) and Iw( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS pw(ˆt, ˆs)log pw(ˆt, ˆs) pw(ˆt)pw(ˆs) , (5) where pw(ˆt) and pw(ˆs) are marginal distributions of pw(ˆt, ˆs).",
                "However, since we do not know either the term weights or P(D, ˆS, T), we need to estimate them, but wˆt depends on p(ˆs|t) and ˆS, while ˆS and p(ˆs|t) also depend on wˆt that is still unknown.",
                "Thus, an iterative algorithm is required to estimate term weights wˆt and find the best segmentation and alignment to optimize the objective function Iw concurrently.",
                "After a document is segmented into sentences Input: Joint probability distribution P(D, Sd, T), number of text segments p ∈ {2, 3, ..., max(sd)}, number of term clusters k ∈ {2, 3, ..., l} (if k = l, no term co-clustering required), and weight type w ∈ {0, 1}, indicating to use I or Iw, respectively.",
                "Output: Mapping Clu, Seg, Ali, and term weights wˆt.",
                "Initialization: 0. i = 0.",
                "Initialize Clu (0) t , Seg (0) d , and Ali (0) d ; Initialize w (0) ˆt using Equation (6) if w = 1; Stage 1: 1.",
                "If |D| = 1, k = l, and w = 0, check all sequential segmentations of d into p segments and find the best one Segd(s) = argmaxˆsI( ˆT; ˆS), and return Segd; otherwise, if w = 1 and k = l, go to 3.1; Stage 2: 2.1 If k < l, for each term t, find the best cluster ˆt as Clu(i+1)(t) = argmaxˆtI( ˆT; ˆS(i)) based on Seg(i) and Ali(i); 2.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+1) d (Seg (i+1) d (s)) = argmaxˆsI( ˆT(i+1); ˆS) based on Clu(i+1)(t) if k < l or Clu(0)(t) if k = l; 2.3 i + +.",
                "If Clu, Seg, or Ali changed, go to 2.1; otherwise, if w = 0, return Clu(i), Seg(i), and Ali(i); else j = 0, go to 3.1; Stage 3: 3.1 Update w (i+j+1) ˆt based on Seg(i+j), Ali(i+j), and Clu(i) using Equation (3); 3.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+j+1) d (Seg (i+j+1) d (s)) = argmaxˆsIw( ˆT(i); ˆS) based on Clu(i) and w (i+j+1) ˆt ; 3.3 j + +.",
                "If Iw( ˆT; ˆS) changes, go to step 6; otherwise, stop and return Clu(i), Seg(i+j), Ali(i+j), and w (i+j) ˆt ; Figure 2: Algorithm: <br>topic segmentation</br> and alignment based on MI or WMI. and each sentence is segmented into words, each word is stemmed.",
                "Then the joint probability distribution P(D, Sd, T) can be estimated.",
                "Finally, this distribution can be used to compute MI in our algorithm. 4.3 Iterative Greedy Algorithm Our goal is to maximize the objective function, I( ˆT; ˆS) or Iw( ˆT; ˆS), which can measure the dependence of term occurrences in different segments.",
                "Generally, first we do not know the estimate term weights, which depend on the optimal <br>topic segmentation</br> and alignment, and term clusters.",
                "Moreover, this problem is NP-hard [10], even though if we know the term weights.",
                "Thus, an iterative greedy algorithm is desired to find the best solution, even though probably only local maxima are reached.",
                "We present the iterative greedy algorithm in Figure 2 to find a local maximum of I( ˆT; ˆS) or Iw( ˆT; ˆS) with simultaneous term weight estimation.",
                "This algorithm can is iterative and greedy for multi-document cases or single-document cases with term weight estimation and/or term co-clustering.",
                "Otherwise, since it is just a one step algorithm to solve the task of single-document segmentation [6, 15, 25], the global maximum of MI is guaranteed.",
                "We will show later that term co-clustering reduces the accuracy of the results and is not necessary, and for singledocument segmentation, term weights are also not required. 4.3.1 Initialization In Step 0, the initial term clustering Clut and <br>topic segmentation</br> and alignment Segd and Alid are important to avoid local maxima and reduce the number of iterations.",
                "First, a good guess of term weights can be made by using the distributions of term frequency along sentences for each document and averaging them to get the initial values of wˆt: wt = ( ED(t) maxt ∈T (ED(t )) )(1 − ES(t) maxt ∈T (ES(t )) ), (6) where ES(t) = 1 |Dt| d∈Dt (1 − s∈Sd p(s|t)log|Sd| 1 p(s|t) ), where Dt is the set of documents which contain Term t. Then, for the initial segmentation Seg(0) , we can simply segment documents equally by sentences.",
                "Or we can find the optimal segmentation just for each document d which maximizes the WMI, Seg (0) d = argmaxˆsIw(T; ˆS), where w = w (0) ˆt .",
                "For the initial alignment Ali(0) , we can first assume that the order of segments for each d is the same.",
                "For the initial term clustering Clu(0) , first cluster labels can be set randomly, and after the first time of Step 3, a good initial term clustering is obtained. 4.3.2 Different Cases After initialization, there are three stages for different cases.",
                "Totally there are eight cases, |D| = 1 or |D| > 1, k = l or k < l, w = 0 or w = 1.",
                "Single document segmentation without term clustering and term weight estimation (|D| = 1, k = l, w = 0) only requires Stage 1 (Step 1).",
                "If term clustering is required (k < l), Stage 2 (Step 2.1, 2.2, and 2.3) is executed iteratively.",
                "If term weight estimation is required (w = 1), Stage 3 (Step 3.1, 3.2, and 3.3) is executed iteratively.",
                "If both are required (k < l, w = 1), Stage 2 and 3 run one after the other.",
                "For multi-document segmentation without term clustering and term weight estimation (|D| > 1, k = l, w = 0), only iteration of Step 2.2 and 2.3 are required.",
                "At Stage 1, the global maximum can be found based on I( ˆT; ˆS) using dynamic programming in Section 4.4.",
                "Simultaneously finding a good term clustering and estimated term weights is impossible, since when moving a term to a new term cluster to maximize Iw( ˆT; ˆS), we do not know that the weight of this term should be the one of the new cluster or the old cluster.",
                "Thus, we first do term clustering at Stage 2, and then estimate term weights at Stage 3.",
                "At Stage 2, Step 2.1 is to find the best term clustering and Step 2.2 is to find the best segmentation.",
                "This cycle is repeated to find a local maximum based on MI I until it converges.",
                "The two steps are: (1) based on current term clustering Cluˆt, for each document d, the algorithm segments all the sentences Sd into p segments sequentially (some segments may be empty), and put them into the p segments ˆS of the whole training set D (all possible cases of different segmentation Segd and alignment Alid are checked) to find the optimal case, and (2) based on the current segmentation and alignment, for each term t, the algorithm finds the best term cluster of t based on the current segmentation Segd and alignment Alid.",
                "After finding a good term clustering, term weights are estimated if w = 1.",
                "At Stage 3, similar as Stage 2, Step 3.1 is term weight re-estimation and Step 3.2 is to find a better segmentation.",
                "They are repeated to find a local maximum based on WMI Iw until it converges.",
                "However, if the term clustering in Stage 2 is not accurate, then the term weight estimation at Stage 3 may have a bad result.",
                "Finally, at Step 3.3, this algorithm converges and return the output.",
                "This algorithm can handle both single-document and multi-document segmentation.",
                "It also can detect shared topics among documents by checking the proportion of overlapped sentences on the same topics, as described in Sec 5.2. 4.4 Algorithm Optimization In many previous works on segmentation, dynamic programming is a technique used to maximize the objective function.",
                "Similarly, at Step 1, 2.2, and 3.2 of our algorithm, we can use dynamic programming.",
                "For Stage 1, using dynamic programming can still find the global optimum, but for Stage 2 and Stage 3, we can only find the optimum for each step of <br>topic segmentation</br> and alignment of a document.",
                "Here we only show the dynamic programming for Step 3.2 using WMI (Step 1 and 2.2 are similar but they can use either I or Iw).",
                "There are two cases that are not shown in the algorithm in Figure 2: (a) single-document segmentation or multi-document segmentation with the same sequential order of segments, where alignment is not required, and (b) multi-document segmentation with different sequential orders of segments, where alignment is necessary.",
                "The alignment mapping function of the former case is simply just Alid(ˆsi) = ˆsi, while for the latter ones alignment mapping function Alid(ˆsi) = ˆsj, i and j may be different.",
                "The computational steps for the two cases are listed below: Case 1 (no alignment): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs) and partial pw(ˆs) without counting sentences from d. Then put sentences from i to j into Part k, and compute partial WMI PIw( ˆT; ˆsk(si, si+1, ..., sj)) ˆt∈ ˆT pw(ˆt, ˆsk)log pw(ˆt, ˆsk) pw(ˆt)pw(ˆsk) , where Alid(si, si+1, ..., sj) = k, k ∈ {1, 2, ..., p}, 1 ≤ i ≤ j ≤ nd, and Segd(sq) = ˆsk for all i ≤ q ≤ j. (2) Let M(sm, 1) = PIw( ˆT; ˆs1(s1, s2, ..., sm)).",
                "Then M(sm, L) = maxi[M(si−1, L − 1) + PIw( ˆT; ˆsL(si, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, and when i > m, no sentences are put into ˆsk when compute PIw (note PIw( ˆT; ˆs(si, ..., sm)) = 0 for single-document segmentation). (3) Finally M(snd , p) = maxi[M(si−1, p − 1)+ PIw( ˆT; ˆsp(si, ..., snd ))], where 1 ≤ i ≤ nd+1.",
                "The optimal Iw is found and the corresponding segmentation is the best.",
                "Case 2 (alignment required): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs), and partial pw(ˆs), and PIw( ˆT; ˆsk(si, si+1, ..., sj)) similarly as Case 1. (2) Let M(sm, 1, k) = PIw( ˆT; ˆsk(s1, s2, ..., sm)), where k ∈ {1, 2, ..., p}.",
                "Then M(sm, L, kL) = maxi,j[M(si−1, L − 1, kL/j) + PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, kL ∈ Set(p, L), which is the set of all p!",
                "L!(p−L)! combinations of L segments chosen from all p segments, j ∈ kL, the set of L segments chosen from all p segments, and kL/j is the combination of L − 1 segments in kL except Segment j. (3) Finally, M(snd , p, kp) = maxi,j[M(si−1, p − 1, kp/j) +PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., snd ))], where kp is just the combination of all p segments and 1 ≤ i ≤ nd + 1, which is the optimal Iw and the corresponding segmentation is the best.",
                "The steps of Case 1 and 2 are similar, except in Case 2, alignment is considered in addition to segmentation.",
                "First, basic items of probability for computing Iw are computed excluding Doc d, and then partial WMI by putting every possible sequential segment (including empty segment) of d into every segment of the set.",
                "Second, the optimal sum of PIw for L segments and the leftmost m sentences, M(sm, L), is found.",
                "Finally, the maximal WMI is found among different sums of M(sm, p − 1) and PIw for Segment p. 5.",
                "EXPERIMENTS In this section, single-document segmentation, shared topic detection, and multi-document segmentation will be tested.",
                "Different hyper parameters of our method are studied.",
                "For convenience, we refer to the method using I as MIk if w = 0, and Iw as WMIk if w = 2 or as WMIk if w = 1, where k is the number of term clusters, and if k = l, where l is the total number of terms, then no term clustering is required, i.e.",
                "MIl and WMIl. 5.1 Single-document Segmentation 5.1.1 Test Data and Evaluation The first data set we tested is a synthetic one used in previous research [6, 15, 25] and many other papers.",
                "It has 700 samples.",
                "Each is a concatenation of ten segments.",
                "Each segment is the first n sentence selected randomly from the Brown corpus, which is supposed to have a different topic from each other.",
                "Currently, the best results on this data set is achieved by Ji et.al. [15].",
                "To compare the performance of our methods, the criterion used widely in previous research is applied, instead of the unbiased criterion introduced in [20].",
                "It chooses a pair of words randomly.",
                "If they are in different segments (different) for the real segmentation (real), but predicted (pred) as in the same segment, it is a miss.",
                "If they are in the same segment (same), but predicted as in different segments, it is a false alarm.",
                "Thus, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, diff)p(diff|real) +p(false alarm|real, pred, same)p(same|real). 5.1.2 Experiment Results We tested the case when the number of segments is known.",
                "Table 1 shows the results of our methods with different hyper parameter values and three previous approaches, C99[25], U00[6], and ADDP03[15], on this data set when the segment number is known.",
                "In WMI for single-document segmentation, the term weights are computed as follows: wˆt = 1−EˆS(ˆt)/maxˆt ∈ ˆT (EˆS(ˆt )).",
                "For this case, our methods MIl and WMIl both outperform all the previous approaches.",
                "We compared our methods with ADDP03using one-sample one-sided t-test and p-values are shown in Table 2.",
                "From the p-values, we can see that mostly the differences are very Table 1: Average Error Rates of Single-document Segmentation Given Segment Numbers Known Range of n 3-11 3-5 6-8 9-11 Sample size 400 100 100 100 C99 12% 11% 10% 9% U00 10% 9% 7% 5% ADDP03 6.0% 6.8% 5.2% 4.3% MIl 4.68% 5.57% 2.59% 1.59% WMIl 4.94% 6.33% 2.76% 1.62% MI100 9.62% 12.92% 8.66% 6.67% Table 2: Single-document Segmentation: P-values of T-test on Error Rates Range of n 3-11 3-5 6-8 9-11 ADDP03, MIl 0.000 0.000 0.000 0.000 ADDP03, WMIl 0.000 0.099 0.000 0.000 MIl, WMIl 0.061 0.132 0.526 0.898 significant.",
                "We also compare the error rates between our two methods using two-sample two-sided t-test to check the hypothesis that they are equal.",
                "We cannot reject the hypothesis that they are equal, so the difference are not significant, even though all the error rates for MIl are smaller than WMIl.",
                "However, we can conclude that term weights contribute little in single-document segmentation.",
                "The results also show that MI using term co-clustering (k = 100) decreases the performance.",
                "We tested different number of term clusters, and found that the performance becomes better when the cluster number increases to reach l. WMIk<l has similar results that we did not show in the table.",
                "As mentioned before, using MI may be inconsistent on optimal boundaries given different numbers of segments.",
                "This situation occurs especially when the similarities among segments are quite different, i.e. some transitions are very obvious, while others are not.",
                "This is because usually a document is a hierarchical structure instead of only a sequential structure.",
                "When the segments are not at the same level, this situation may occur.",
                "Thus, a hierarchical <br>topic segmentation</br> approach is desired, and the structure highly depends on the number of segments for each internal node and the stop criteria of splitting.",
                "For this data set of singledocument segmentation, since it is just a synthetic set, which is just a concatenation of several segments about different topics, it is reasonable that approaches simply based on term frequency have a good performance.",
                "Usually for the tasks of segmenting coherent documents for sub-topics, the effectiveness decreases much. 5.2 Shared Topic Detection 5.2.1 Test Data and Evaluation The second data set contains 80 news articles from Google News.",
                "There are eight topics and each has 10 articles.",
                "We randomly split the set into subsets with different document numbers and each subset has all eight topics.",
                "We compare our approach MIl and WMIl with LDA [4].",
                "LDA treats a document in the data set as a bag of words, finds its distribution on topics, and its major topic.",
                "MIl and WMIl views each sentence as a bag of words and tag it with a topic label.",
                "Then for each pair of documents, LDA determines if they are on the same topic, while MIl and Table 3: Shared Topic Detection: Average Error Rates for Different Numbers of Documents in Each Subset #Doc 10 20 40 80 LDA 8.89% 16.33% 1.35% 0.60% MIl, θ = 0.6 4.17% 1.71% 1.47% 0.0% WMIl, θ = 0.8 18.6% 3.16% 1.92% 0.0% WMIl check whether the proportion overlapped sentences on the same topic is larger than the adjustable threshold θ.",
                "That is, in MIl and WMIl, for a pair of documents d, d , if [ s∈Sd,s ∈Sd 1(topics=topics )/min(|Sd|, |Sd|)] > θ, where Sd is the set of sentences of d, and |Sd| is the number of sentences of d, then d and d have the shared topic.",
                "For a pair of documents selected randomly, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, same)p(same|real) +p(false alarm|real, pred, diff)p(diff|real), where a miss means if they have the same topic (same) for the real case (real), but predicted (pred) as on the same topic.",
                "If they are on different topics (diff), but predicted as on the same topic, it is a false alarm. 5.2.2 Experiment Results The results are shown in Table 3.",
                "If most documents have different topics, in WMIl, the estimation of term weights in Equation (3) is not correct.",
                "Thus, WMIl is not expected to have a better performance than MIl, when most documents have different topics.",
                "When there are fewer documents in a subset with the same number of topics, more documents have different topics, so WMIl is more worse than MIl.",
                "We can see that for most cases MIl has a better (or at least similar) performance than LDA.",
                "After shared topic detection, multi-document segmentation of documents with the shared topics is able to be executed. 5.3 Multi-document Segmentation 5.3.1 Test Data and Evaluation For multi-document segmentation and alignment, our goal is to identify these segments about the same topic among multiple similar documents with shared topics.",
                "Using Iw is expected to perform better than I, since without term weights the result is affected seriously by document-dependent stop words and noisy words which depends on the personal writing style.",
                "It is more likely to treat the same segments of different documents as different segments under the effect of document-dependent stop words and noisy words.",
                "Term weights can reduce the effect of document-dependent stop words and noisy words by giving cue terms more weights.",
                "The data set for multi-document segmentation and alignment has 102 samples and 2264 sentences totally.",
                "Each is the introduction part of a lab report selected from the course of Biol 240W, Pennsylvania State University.",
                "Each sample has two segments, introduction of plant hormones and the content in the lab.",
                "The length range of samples is from two to 56 sentences.",
                "Some samples only have one part and some have a reverse order the these two segments.",
                "It is not hard to identify the boundary between two segments for human.",
                "We labelled each sentence manually for evaluation.",
                "The criterion of evaluation is just using the proportion of the number of sentences with wrong predicted segment labels in the total number of sentences in the whole training Table 4: Average Error Rates of Multi-document Segmentation Given Segment Numbers Known #Doc MIl WMIl k MIk WMIk 102 3.14% 2.78% 300 4.68% 6.58% 51 4.17% 3.63% 300 17.83% 22.84% 34 5.06% 4.12% 300 18.75% 20.95% 20 7.08% 5.42% 250 20.40% 21.83% 10 10.38% 7.89% 250 21.42% 21.91% 5 15.77% 11.64% 250 21.89% 22.59% 2 25.90% 23.18% 50 25.44% 25.49% 1 23.90% 24.82% 25 25.75% 26.15% Table 5: Multi-document Segmentation: P-values of T-test on Error Rates for MIl and WMIl #Doc 51 34 20 10 5 2 P-value 0.19 0.101 0.025 0.001 0.000 0.002 set as the error rate: p(error|predicted, real) = d∈D s∈Sd 1(predicteds=reals)/ d∈D nd.",
                "In order to show the benefits of multi-document segmentation and alignment, we compared our method with different parameters on different partitions of the same training set.",
                "Except the cases that the number of documents is 102 and one (they are special cases of using the whole set and the pure single-document segmentation), we randomly divided the training set into m partitions, and each has 51, 34, 20, 10, 5, and 2 document samples.",
                "Then we applied our methods on each partition and calculated the error rate of the whole training set.",
                "Each case was repeated for 10 times for computing the average error rates.",
                "For different partitions of the training set, different k values are used, since the number of terms increases when the document number in each partition increases. 5.3.2 Experiment Results From the experiment results in Table 4, we can see the following observations: (1) When the number of documents increases, all methods have better performances.",
                "Only from one to two documents, MIl has decrease a little.",
                "We can observe this from Figure 3 at the point of document number = 2.",
                "Most curves even have the worst results at this point.",
                "There are two reasons.",
                "First, because samples vote for the best multi-document segmentation and alignment, but if only two documents are compared with each other, the one with missing segments or a totally different sequence will affect the correct segmentation and alignment of the other.",
                "Second, as noted at the beginning of this section, if two documents have more document-dependent stop words or noisy words than cue words, then the algorithm may view them as two different segments and the other segment is missing.",
                "Generally, we can only expect a better performance when the number of documents is larger than the number of segments. (2) Except single-document segmentation, WMIl is always better than MIl, and when the number of documents is reaching one or increases to a very large number, their performances become closer.",
                "Table 5 shows p-values of twosample one-sided t-test between MIl and WMIl.",
                "We also can see this trend from p-values.",
                "When document number = 5, we reached the smallest p-value and the largest difference between error rates of MIl and WMIl.",
                "For single-document Table 6: Multi-document Segmentation: Average Error Rate for Document Number = 5 in Each Subset with Different Number of Term Clusters #Cluster 75 100 150 250 l MIk 24.67% 24.54% 23.91% 22.59% 15.77% segmentation, WMIl is even a little bit worse than MIl, which is similar as the results of the single-document segmentation on the first data set.",
                "The reason is that for singledocument segmentation, we cannot estimate term weights accurately, since multiple documents are unavailable. (3) Using term clustering usually gets worse results than MIl and WMIl. (4) Using term clustering in WMIk is even worse than in MIk, since in WMIk term clusters are found first using I before using Iw.",
                "If the term clusters are not correct, then the term weights are estimated worse, which may mislead the algorithm to reach even worse results.",
                "From the results we also found that in multi-document segmentation and alignment, most documents with missing segments and a reverse order are identified correctly.",
                "Table 6 illustrates the experiment results for the case of 20 partitions (each has five document samples) of the training set and <br>topic segmentation</br> and alignment using MIk with different numbers of term clusters k. Notice that when the number of term clusters increases, the error rate becomes smaller.",
                "Without term clustering, we have the best result.",
                "We did not show results for WMIk with term clustering, but the results are similar.",
                "We also tested WMIl with different hyper parameters of a and b to adjust term weights.",
                "The results are presented in Figure 3.",
                "It was shown that the default case WMIl : a = 1, b = 1 gave the best results for different partitions of the training set.",
                "We can see the trend that when the document number is very small or large, the difference between MIl : a = 0, b = 0 and WMIl : a = 1, b = 1 becomes quite small.",
                "When the document number is not large (about from 2 to 10), all the cases using term weights have better performances than MIl : a = 0, b = 0 without term weights, but when the document number becomes larger, the cases WMIl : a = 1, b = 0 and WMIl : a = 2, b = 1 become worse than MIl : a = 0, b = 0.",
                "When the document number becomes very large, they are even worse than cases with small document numbers.",
                "This means that a proper way to estimate term weights for the criterion of WMI is very important.",
                "Figure 4 shows the term weights learned from the whole training set.",
                "Four types of words are categorized roughly even though the transition among them are subtle.",
                "Figure 5 illustrates the change in (weighted) mutual information for MIl and WMIl.",
                "As expected, mutual information for MIl increases monotonically with the number of steps, while WMIl does not.",
                "Finally, MIl and WMIl are scalable, with computational complexity shown in Figure 6.",
                "One advantage for our approach based on MI is that removing stop words is not required.",
                "Another important advantage is that there are no necessary hyper parameters to adjust.",
                "In single-document segmentation, the performance based on MI is even better for that based on WMI, so no extra hyper parameter is required.",
                "In multi-document segmentation, we show in the experiment, a = 1 and b = 1 is the best.",
                "Our method gives more weights to cue terms.",
                "However, usually cue terms or sentences appear at the beginning of a segment, while the end of the segment may be 1 2 5 10 20 34 51 102 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 Document Number ErrorRate MIl :a=0,b=0 WMI l :a=1,b=1 WMI l :a=1,b=0 WMI l :a=2,b=1 Figure 3: Error rates for different hyper parameters of term weights. 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Normalized Document Entropy NormalizedSegmentEntropy Noisy words Cue words Common stop words Doc−dep stop words Figure 4: Term weights learned from the whole training set. 0 100 200 300 400 500 600 0.06 0.08 0.1 0.12 0.14 0.16 0.18 Number of Steps (Weighted)MutualInformation MI l WMI l Figure 5: Change in (weighted) MI for MIl and WMIl. 0 20 40 60 80 100 120 0 200 400 600 800 1000 1200 1400 1600 1800 2000 Document Number TimetoConverge(sec) MI l WMI l Figure 6: Time to converge for MIl and WMIl. much noisy.",
                "One possible solution is giving more weights to terms at the beginning of each segment.",
                "Moreover, when the length of segments are quite different, long segments have much higher term frequencies, so they may dominate the segmentation boundaries.",
                "Normalization of term frequencies versus the segment length may be useful. 6.",
                "CONCLUSIONS AND FUTURE WORK We proposed a novel method for multi-document <br>topic segmentation</br> and alignment based on weighted mutual information, which can also handle single-document cases.",
                "We used dynamic programming to optimize our algorithm.",
                "Our approach outperforms all the previous methods on singledocument cases.",
                "Moreover, we also showed that doing segmentation among multiple documents can improve the performance tremendously.",
                "Our results also illustrated that using weighted mutual information can utilize the information of multiple documents to reach a better performance.",
                "We only tested our method on limited data sets.",
                "More data sets especially complicated ones should be tested.",
                "More previous methods should be compared with.",
                "Moreover, natural segmentations like paragraphs are hints that can be used to find the optimal boundaries.",
                "Supervised learning also can be considered. 7.",
                "ACKNOWLEDGMENTS The authors want to thank Xiang Ji, and Prof. J. Scott Payne for their help. 8.",
                "REFERENCES [1] A. Banerjee, I. Ghillon, J. Ghosh, S. Merugu, and D. Modha.",
                "A generalized maximum entropy approach to bregman co-clustering and matrix approximation.",
                "In Proceedings of SIGKDD, 2004. [2] R. Bekkerman, R. El-Yaniv, and A. McCallum.",
                "Multi-way distributional clustering via pairwise interactions.",
                "In Proceedings of ICML, 2005. [3] D. M. Blei and P. J. Moreno.",
                "<br>topic segmentation</br> with an aspect hidden markov model.",
                "In Proceedings of SIGIR, 2001. [4] D. M. Blei, A. Ng, and M. Jordan.",
                "Latent dirichlet allocation.",
                "Journal of Machine Learning Research, 3:993-1022, 2003. [5] T. Brants, F. Chen, and I. Tsochantaridis.",
                "Topic-based document segmentation with probabilistic latent semantic analysis.",
                "In Proceedings of CIKM, 2002. [6] F. Choi.",
                "Advances in domain indepedent linear text segmentation.",
                "In Proceedings of the NAACL, 2000. [7] H. Christensen, B. Kolluru, Y. Gotoh, and S. Renals.",
                "Maximum entropy segmentation of broadcast news.",
                "In Proceedings of ICASSP, 2005. [8] T. Cover and J. Thomas.",
                "Elements of Information Theory.",
                "John Wiley and Sons, New York, USA, 1991. [9] S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and R. Harshman.",
                "Indexing by latent semantic analysis.",
                "Journal of the American Society for Information Systems, 1990. [10] I. Dhillon, S. Mallela, and D. Modha.",
                "Information-theoretic co-clustering.",
                "In Proceedings of SIGKDD, 2003. [11] M. Hajime, H. Takeo, and O. Manabu.",
                "Text segmentation with multiple surface linguistic cues.",
                "In Proceedings of COLING-ACL, 1998. [12] T. K. Ho.",
                "Stop word location and identification for adaptive text recognition.",
                "International Journal of Document Analysis and Recognition, 3(1), August 2000. [13] T. Hofmann.",
                "Probabilistic latent semantic analysis.",
                "In Proceedings of the UAI99, 1999. [14] X. Ji and H. Zha.",
                "Correlating summarization of a pair of multilingual documents.",
                "In Proceedings of RIDE, 2003. [15] X. Ji and H. Zha.",
                "Domain-independent text segmentation using anisotropic diffusion and dynamic programming.",
                "In Proceedings of SIGIR, 2003. [16] X. Ji and H. Zha.",
                "Extracting shared topics of multiple documents.",
                "In Proceedings of the 7th PAKDD, 2003. [17] J. Lafferty, A. McCallum, and F. Pereira.",
                "Conditional random fields: Probabilistic models for segmenting and labeling sequence data.",
                "In Proceedings of ICML, 2001. [18] T. Li, S. Ma, and M. Ogihara.",
                "Entropy-based criterion in categorical clustering.",
                "In Proceedings of ICML, 2004. [19] A. McCallum, D. Freitag, and F. Pereira.",
                "Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of ICML, 2000. [20] L. Pevzner and M. Hearst.",
                "A critique and improvement of an evaluation metric for text segmentation.",
                "Computational Linguistic, 28(1):19-36, 2002. [21] J. C. Reynar.",
                "Statistical models for <br>topic segmentation</br>.",
                "In Proceedings of ACL, 1999. [22] G. Salton and M. McGill.",
                "Introduction to Modern Information Retrieval.",
                "McGraw Hill, 1983. [23] B.",
                "Sun, Q. Tan, P. Mitra, and C. L. Giles.",
                "Extraction and search of chemical formulae in text documents on the web.",
                "In Proceedings of WWW, 2007. [24] B.",
                "Sun, D. Zhou, H. Zha, and J.",
                "Yen.",
                "Multi-task text segmentation and alignment based on weighted mutual information.",
                "In Proceedings of CIKM, 2006. [25] M. Utiyama and H. Isahara.",
                "A statistical model for domain-independent text segmentation.",
                "In Proceedings of the 39th ACL, 1999. [26] C. Wayne.",
                "Multilingual topic detection and tracking: Successful research enabled by corpora and evaluation.",
                "In Proceedings of LREC, 2000. [27] J. Yamron, I. Carp, L. Gillick, S. Lowe, and P. van Mulbregt.",
                "A hidden markov model approach to text segmentation and event tracking.",
                "In Proceedings of ICASSP, 1998. [28] H. Zha and X. Ji.",
                "Correlating multilingual documents via bipartite graph modeling.",
                "In Proceedings of SIGIR, 2002."
            ],
            "original_annotated_samples": [
                "<br>topic segmentation</br> with Shared Topic Detection and Alignment of Multiple Documents Bingjun Sun*, Prasenjit Mitra*† , Hongyuan Zha‡ , C. Lee Giles*† , John Yen*† *Department of Computer Science and Engineering † College of Information Sciences and Technology The Pennsylvania State University University Park, PA 16802 ‡ College of Computing The Georgia Institute of Technology Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu ABSTRACT Topic detection and tracking [26] and <br>topic segmentation</br> [15] play an important role in capturing the local and sequential information of documents.",
                "In this paper, we introduce a novel unsupervised method for shared topic detection and <br>topic segmentation</br> of multiple similar documents based on mutual information (MI) and weighted mutual information (WMI) that is a combination of MI and term weights.",
                "Utilizing information from multiple documents can tremendously improve the performance of <br>topic segmentation</br>, and using WMI is even better than using MI for the multi-document segmentation.",
                "INTRODUCTION Many researchers have worked on topic detection and tracking (TDT) [26] and <br>topic segmentation</br> during the past decade.",
                "<br>topic segmentation</br> intends to identify the boundaries in a document with the goal to capture the latent topical structure."
            ],
            "translated_annotated_samples": [
                "La <br>segmentación de temas</br> con detección de temas compartidos y alineación de múltiples documentos Bingjun Sun*, Prasenjit Mitra*†, Hongyuan Zha‡, C. Lee Giles*†, John Yen*† *Departamento de Ciencias de la Computación e Ingeniería † Colegio de Ciencias de la Información y Tecnología Universidad Estatal de Pensilvania University Park, PA 16802 ‡ Colegio de Computación Instituto de Tecnología de Georgia Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu RESUMEN La detección y seguimiento de temas [26] y la <br>segmentación de temas</br> [15] juegan un papel importante en la captura de la información local y secuencial de los documentos.",
                "En este artículo, presentamos un novedoso método no supervisado para la detección de temas compartidos y la <br>segmentación de temas</br> de múltiples documentos similares basado en la información mutua (MI) y la información mutua ponderada (WMI), que es una combinación de MI y pesos de términos.",
                "El uso de información de múltiples documentos puede mejorar enormemente el rendimiento de la <br>segmentación de temas</br>, y utilizar WMI es aún mejor que utilizar MI para la segmentación de múltiples documentos.",
                "INTRODUCCIÓN Muchos investigadores han trabajado en la detección y seguimiento de temas (TDT) [26] y la <br>segmentación de temas</br> durante la última década.",
                "La segmentación de temas tiene como objetivo identificar los límites en un documento con la meta de capturar la <br>estructura temática</br> latente."
            ],
            "translated_text": "La <br>segmentación de temas</br> con detección de temas compartidos y alineación de múltiples documentos Bingjun Sun*, Prasenjit Mitra*†, Hongyuan Zha‡, C. Lee Giles*†, John Yen*† *Departamento de Ciencias de la Computación e Ingeniería † Colegio de Ciencias de la Información y Tecnología Universidad Estatal de Pensilvania University Park, PA 16802 ‡ Colegio de Computación Instituto de Tecnología de Georgia Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu RESUMEN La detección y seguimiento de temas [26] y la <br>segmentación de temas</br> [15] juegan un papel importante en la captura de la información local y secuencial de los documentos. El trabajo previo en esta área suele centrarse en documentos individuales, aunque existen múltiples documentos similares en muchos dominios. En este artículo, presentamos un novedoso método no supervisado para la detección de temas compartidos y la <br>segmentación de temas</br> de múltiples documentos similares basado en la información mutua (MI) y la información mutua ponderada (WMI), que es una combinación de MI y pesos de términos. La idea básica es que la segmentación óptima maximiza MI (o WMI). Nuestro enfoque puede detectar temas compartidos entre documentos. Puede encontrar los límites óptimos en un documento y alinear segmentos entre documentos al mismo tiempo. También puede manejar la segmentación de un solo documento como un caso especial de la segmentación y alineación de varios documentos. Nuestros métodos pueden identificar y fortalecer términos de señal que pueden ser utilizados para la segmentación y eliminar parcialmente las palabras de parada mediante el uso de pesos de términos basados en la entropía aprendida de múltiples documentos. Nuestros resultados experimentales muestran que nuestro algoritmo funciona bien para las tareas de segmentación de un solo documento, detección de temas compartidos y segmentación de múltiples documentos. El uso de información de múltiples documentos puede mejorar enormemente el rendimiento de la <br>segmentación de temas</br>, y utilizar WMI es aún mejor que utilizar MI para la segmentación de múltiples documentos. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información-Agrupación; H.3.1 [Almacenamiento y Recuperación de Información]: Análisis de Contenido e Indexación-Procesamiento Lingüístico; I.2.7 [Inteligencia Artificial]: Procesamiento de Lenguaje Natural-Análisis de Texto; I.5.3 [Reconocimiento de Patrones]: Algoritmos de Agrupación-Medidas de Similitud Términos Generales Algoritmos, Diseño, Experimentación 1. INTRODUCCIÓN Muchos investigadores han trabajado en la detección y seguimiento de temas (TDT) [26] y la <br>segmentación de temas</br> durante la última década. La segmentación de temas tiene como objetivo identificar los límites en un documento con la meta de capturar la <br>estructura temática</br> latente. ",
            "candidates": [],
            "error": [
                [
                    "segmentación de temas",
                    "segmentación de temas",
                    "segmentación de temas",
                    "segmentación de temas",
                    "segmentación de temas",
                    "estructura temática"
                ]
            ]
        },
        "local and sequential information of document": {
            "translated_key": "información local y secuencial de los documentos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Topic Segmentation with Shared Topic Detection and Alignment of Multiple Documents Bingjun Sun*, Prasenjit Mitra*† , Hongyuan Zha‡ , C. Lee Giles*† , John Yen*† *Department of Computer Science and Engineering † College of Information Sciences and Technology The Pennsylvania State University University Park, PA 16802 ‡ College of Computing The Georgia Institute of Technology Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu ABSTRACT Topic detection and tracking [26] and topic segmentation [15] play an important role in capturing the <br>local and sequential information of document</br>s.",
                "Previous work in this area usually focuses on single documents, although similar multiple documents are available in many domains.",
                "In this paper, we introduce a novel unsupervised method for shared topic detection and topic segmentation of multiple similar documents based on mutual information (MI) and weighted mutual information (WMI) that is a combination of MI and term weights.",
                "The basic idea is that the optimal segmentation maximizes MI(or WMI).",
                "Our approach can detect shared topics among documents.",
                "It can find the optimal boundaries in a document, and align segments among documents at the same time.",
                "It also can handle single-document segmentation as a special case of the multi-document segmentation and alignment.",
                "Our methods can identify and strengthen cue terms that can be used for segmentation and partially remove stop words by using term weights based on entropy learned from multiple documents.",
                "Our experimental results show that our algorithm works well for the tasks of single-document segmentation, shared topic detection, and multi-document segmentation.",
                "Utilizing information from multiple documents can tremendously improve the performance of topic segmentation, and using WMI is even better than using MI for the multi-document segmentation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Clustering; H.3.1 [Information Storage and Retrieval]: Content Analysis and IndexingLinguistic processing; I.2.7 [Artificial Intelligence]: Natural Language Processing-Text analysis; I.5.3 [Pattern Recognition]: Clustering-Algorithms;Similarity measures General Terms Algorithms, Design, Experimentation 1.",
                "INTRODUCTION Many researchers have worked on topic detection and tracking (TDT) [26] and topic segmentation during the past decade.",
                "Topic segmentation intends to identify the boundaries in a document with the goal to capture the latent topical structure.",
                "Topic segmentation tasks usually fall into two categories [15]: text stream segmentation where topic transition is identified, and coherent document segmentation in which documents are split into sub-topics.",
                "The former category has applications in automatic speech recognition, while the latter one has more applications such as partial-text query of long documents in information retrieval, text summary, and quality measurement of multiple documents.",
                "Previous research in connection with TDT falls into the former category, targeted on topic tracking of broadcast speech data and newswire text, while the latter category has not been studied very well.",
                "Traditional approaches perform topic segmentation on documents one at a time [15, 25, 6].",
                "Most of them perform badly in subtle tasks like coherent document segmentation [15].",
                "Often, end-users seek documents that have the similar content.",
                "Search engines, like, Google, provide links to obtain similar pages.",
                "At a finer granularity, users may actually be looking to obtain sections of a document similar to a particular section that presumably discusses a topic of the users interest.",
                "Thus, the extension of topic segmentation from single documents to identifying similar segments from multiple similar documents with the same topic is a natural and necessary direction, and multi-document topic segmentation is expected to have a better performance since more information is utilized.",
                "Traditional approaches using similarity measurement based on term frequency generally have the same assumption that similar vocabulary tends to be in a coherent topic segment [15, 25, 6].",
                "However, they usually suffer the issue of identifying stop words.",
                "For example, additional document-dependent stop words are removed together with the generic stop words in [15].",
                "There are two reasons that we do not remove stop words directly.",
                "First, identifying stop words is another issue [12] that requires estimation in each domain.",
                "Removing common stop words may result in the loss of useful information in a specific domain.",
                "Second, even though stop words can be identified, hard classification of stop words and nonstop words cannot represent the gradually changing amount of information content of each word.",
                "We employ a soft classification using term weights.",
                "In this paper, we view the problem of topic segmentation as an optimization issue using information theoretic techniques to find the optimal boundaries of a document given the number of text segments so as to minimize the loss of mutual information (MI) (or a weighted mutual information (WMI)) after segmentation and alignment.",
                "This is equal to maximizing the MI (or WMI).",
                "The MI focuses on measuring the difference among segments whereas previous research focused on finding the similarity (e.g. cosine distance) of segments [15, 25, 6].",
                "Topic alignment of multiple similar documents can be achieved by clustering sentences on the same topic into the same cluster.",
                "Single-document topic segmentation is just a special case of the multi-document topic segmentation and alignment problem.",
                "Terms can be co-clustered as in [10] at the same time, given the number of clusters, but our experimental results show that this method results in a worse segmentation (see Tables 1, 4, and 6).",
                "Usually, human readers can identify topic transition based on cue words, and can ignore stop words.",
                "Inspired by this, we give each term (or term cluster) a weight based on entropy among different documents and different segments of documents.",
                "Not only can this approach increase the contribution of cue words, but it can also decrease the effect of common stop words, noisy word, and document-dependent stop words.",
                "These words are common in a document.",
                "Many methods based on sentence similarity require that these words are removed before topic segmentation can be performed [15].",
                "Our results in Figure 3 show that term weights are useful for multi-document topic segmentation and alignment.",
                "The major contribution of this paper is that it introduces a novel method for topic segmentation using MI and shows that this method performs better than previously used criteria.",
                "Also, we have addressed the problem of topic segmentation and alignment across multiple documents, whereas most existing research focused on segmentation of single documents.",
                "Multi-document segmentation and alignment can utilize information from similar documents and improves the performance of topic segmentation greatly.",
                "Obviously, our approach can handle single documents as a special case when multiple documents are unavailable.",
                "It can detect shared topics among documents to judge if they are multiple documents on the same topic.",
                "We also introduce the new criterion of WMI based on term weights learned from multiple similar documents, which can improve performance of topic segmentation further.",
                "We propose an iterative greedy algorithm based on dynamic programming and show that it works well in practice.",
                "Some of our prior work is in [24].",
                "The rest of this paper is organized as follows: In Section 2, we review related work.",
                "Section 3 contains a formulation of the problem of topic segmentation and alignment of multiple documents with term co-clustering, a review of the criterion of MI for clustering, and finally an introduction to WMI.",
                "In Section 4, we first propose the iterative greedy algorithm of topic segmentation and alignment with term co-clustering, and then describe how the algorithm can be optimized by usFigure 1: Illustration of multi-document segmentation and alignment. ing dynamic programming.",
                "In Section 5, experiments about single-document segmentation, shared topic detection, and multi-document segmentation are described, and results are presented and discussed to evaluate the performance of our algorithm.",
                "Conclusions and some future directions of the research work are discussed in Section 6. 2.",
                "PREVIOUS WORK Generally, the existing approaches to text segmentation fall into two categories: supervised learning [19, 17, 23] and unsupervised learning [3, 27, 5, 6, 15, 25, 21].",
                "Supervised learning usually has good performance, since it learns functions from labelled training sets.",
                "However, often getting large training sets with manual labels on document sentences is prohibitively expensive, so unsupervised approaches are desired.",
                "Some models consider dependence between sentences and sections, such as Hidden Markov Model [3, 27], Maximum Entropy Markov Model [19], and Conditional Random Fields [17], while many other approaches are based on lexical cohesion or similarity of sentences [5, 6, 15, 25, 21].",
                "Some approaches also focus on cue words as hints of topic transitions [11].",
                "While some existing methods only consider information in single documents [6, 15], others utilize multiple documents [16, 14].",
                "There are not many works in the latter category, even though the performance of segmentation is expected to be better with utilization of information from multiple documents.",
                "Previous research studied methods to find shared topics [16] and topic segmentation and summarization between just a pair of documents [14].",
                "Text classification and clustering is a related research area which categorizes documents into groups using supervised or unsupervised methods.",
                "Topical classification or clustering is an important direction in this area, especially co-clustering of documents and terms, such as LSA [9], PLSA [13], and approaches based on distances and bipartite graph partitioning [28] or maximum MI [2, 10], or maximum entropy [1, 18].",
                "Criteria of these approaches can be utilized in the issue of topic segmentation.",
                "Some of those methods have been extended into the area of topic segmentation, such as PLSA [5] and maximum entropy [7], but to our best knowledge, using MI for topic segmentation has not been studied. 3.",
                "PROBLEM FORMULATION Our goal is to segment documents and align the segments across documents (Figure 1).",
                "Let T be the set of terms {t1, t2, ..., tl}, which appear in the unlabelled set of documents D = {d1, d2, ..., dm}.",
                "Let Sd be the set of sentences for document d ∈ D, i.e. {s1, s2, ..., snd }.",
                "We have a 3D matrix of term frequency, in which the three dimensions are random variables of D, Sd, and T. Sd actually is a random vector including a random variable for each d ∈ D. The term frequency can be used to estimate the joint probability distribution P(D, Sd, T), which is p(t, d, s) = T(t, d, s)/ND, where T(t, d, s) is the number of t in ds sentence s and ND is the total number of terms in D. ˆS represents the set of segments {ˆs1, ˆs2, ..., ˆsp} after segmentation and alignment among multiple documents, where the number of segments | ˆS| = p. A segment ˆsi of document d is a sequence of adjacent sentences in d. Since for different documents si may discuss different sub-topics, our goal is to cluster adjacent sentences in each document into segments, and align similar segments among documents, so that for different documents ˆsi is about the same sub-topic.",
                "The goal is to find the optimal topic segmentation and alignment mapping Segd(si) : {s1, s2, ..., snd } → {ˆs1, ˆs2, ..., ˆsp} and Alid(ˆsi) : {ˆs1, ˆs2, ..., ˆsp} → {ˆs1, ˆs2, ..., ˆsp}, for all d ∈ D, where ˆsi is ith segment with the constraint that only adjacent sentences can be mapped to the same segment, i.e. for d, {si, si+1, ..., sj} → {ˆsq}, where q ∈ {1, ..., p}, where p is the segment number, and if i > j, then for d, ˆsq is missing.",
                "After segmentation and alignment, random vector Sd becomes an aligned random variable ˆS.",
                "Thus, P(D, Sd, T) becomes P(D, ˆS, T).",
                "Term co-clustering is a technique that has been employed [10] to improve the accuracy of document clustering.",
                "We evaluate the effect of it for topic segmentation.",
                "A term t is mapped to exactly one term cluster.",
                "Term co-clustering involves simultaneously finding the optimal term clustering mapping Clu(t) : {t1, t2, ..., tl} → {ˆt1, ˆt2, ..., ˆtk}, where k ≤ l, l is the total number of words in all the documents, and k is the number of clusters. 4.",
                "METHODOLOGY We now describe a novel algorithm which can handle singledocument segmentation, shared topic detection, and multidocument segmentation and alignment based on MI or WMI. 4.1 Mutual Information MI I(X; Y ) is a quantity to measure the amount of information which is contained in two or more random variables [8, 10].",
                "For the case of two random variables, we have I(X; Y ) = x∈X y∈Y p(x, y)log p(x, y) p(x)p(y) , (1) Obviously, when random variables X and Y are independent, I(X; Y ) = 0.",
                "Thus, intuitively, the value of MI depends on how random variables are dependent on each other.",
                "The optimal co-clustering is the mapping Clux : X → ˆX and Cluy : Y → ˆY that minimizes the loss: I(X; Y ) − I( ˆX; ˆY ), which is equal to maximizing I( ˆX; ˆY ).",
                "This is the criterion of MI for clustering.",
                "In the case of topic segmentation, the two random variables are the term variable T and the segment variable S, and each sample is an occurrence of a term T = t in a particular segment S = s. I(T; S) is used to measure how dependent T and S are.",
                "However, I(T; S) cannot be computed for documents before segmentation, since we do not have a set of S due to the fact that sentences of Document d, si ∈ Sd, is not aligned with other documents.",
                "Thus, instead of minimizing the loss of MI, we can maximize MI after topic segmentation, computed as: I( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS p(ˆt, ˆs)log p(ˆt, ˆs) p(ˆt)p(ˆs) , (2) where p(ˆt, ˆs) are estimated by the term frequency tf of Term Cluster ˆt and Segment ˆs in the training set D. Note that here a segment ˆs includes sentences about the the same topic among all documents.",
                "The optimal solution is the mapping Clut : T → ˆT, Segd : Sd → ˆS , and Alid : ˆS → ˆS, which maximizes I( ˆT; ˆS). 4.2 Weighted Mutual Information In topic segmentation and alignment of multiple documents, if P(D, ˆS, T) is known, based on the marginal distributions P(D|T) and P( ˆS|T) for each term t ∈ T, we can categorize terms into four types in the data set: • Common stop words are common both along the dimensions of documents and segments. • Document-dependent stop words that depends on the personal writing style are common only along the dimension of segments for some documents. • Cue words are the most important elements for segmentation.",
                "They are common along the dimension of documents only for the same segment, and they are not common along the dimensions of segments. • Noisy words are other words which are not common along both dimensions.",
                "Entropy based on P(D|T) and P( ˆS|T) can be used to identify different types of terms.",
                "To reinforce the contribution of cue words in the MI computation, and simultaneously reduce the effect of the other three types of words, similar as the idea of the tf-idf weight [22], we use entropies of each term along the dimensions of document D and segment ˆS, i.e.",
                "ED(ˆt) and EˆS(ˆt), to compute the weight.",
                "A cue word usually has a large value of ED(ˆt) but a small value of EˆS(ˆt).",
                "We introduce term weights (or term cluster weights) wˆt = ( ED(ˆt) maxˆt ∈ ˆT (ED(ˆt )) )a (1 − EˆS(ˆt) maxˆt ∈ ˆT (EˆS(ˆt )) )b , (3) where ED(ˆt) = d∈D p(d|ˆt)log|D| 1 p(d|ˆt) , EˆS(ˆt) = ˆs∈ ˆS p(ˆs|ˆt)log| ˆS| 1 p(ˆs|ˆt) , and a > 0 and b > 0 are powers to adjust term weights.",
                "Usually a = 1 and b = 1 as default, and maxˆt ∈ ˆT (ED(ˆt )) and maxˆt ∈ ˆT (EˆS(ˆt )) are used to normalize the entropy values.",
                "Term cluster weights are used to adjust p(ˆt, ˆs), pw(ˆt, ˆs) = wˆtp(ˆt, ˆs) ˆt∈ ˆT ;ˆs∈ ˆS wˆtp(ˆt, ˆs) , (4) and Iw( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS pw(ˆt, ˆs)log pw(ˆt, ˆs) pw(ˆt)pw(ˆs) , (5) where pw(ˆt) and pw(ˆs) are marginal distributions of pw(ˆt, ˆs).",
                "However, since we do not know either the term weights or P(D, ˆS, T), we need to estimate them, but wˆt depends on p(ˆs|t) and ˆS, while ˆS and p(ˆs|t) also depend on wˆt that is still unknown.",
                "Thus, an iterative algorithm is required to estimate term weights wˆt and find the best segmentation and alignment to optimize the objective function Iw concurrently.",
                "After a document is segmented into sentences Input: Joint probability distribution P(D, Sd, T), number of text segments p ∈ {2, 3, ..., max(sd)}, number of term clusters k ∈ {2, 3, ..., l} (if k = l, no term co-clustering required), and weight type w ∈ {0, 1}, indicating to use I or Iw, respectively.",
                "Output: Mapping Clu, Seg, Ali, and term weights wˆt.",
                "Initialization: 0. i = 0.",
                "Initialize Clu (0) t , Seg (0) d , and Ali (0) d ; Initialize w (0) ˆt using Equation (6) if w = 1; Stage 1: 1.",
                "If |D| = 1, k = l, and w = 0, check all sequential segmentations of d into p segments and find the best one Segd(s) = argmaxˆsI( ˆT; ˆS), and return Segd; otherwise, if w = 1 and k = l, go to 3.1; Stage 2: 2.1 If k < l, for each term t, find the best cluster ˆt as Clu(i+1)(t) = argmaxˆtI( ˆT; ˆS(i)) based on Seg(i) and Ali(i); 2.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+1) d (Seg (i+1) d (s)) = argmaxˆsI( ˆT(i+1); ˆS) based on Clu(i+1)(t) if k < l or Clu(0)(t) if k = l; 2.3 i + +.",
                "If Clu, Seg, or Ali changed, go to 2.1; otherwise, if w = 0, return Clu(i), Seg(i), and Ali(i); else j = 0, go to 3.1; Stage 3: 3.1 Update w (i+j+1) ˆt based on Seg(i+j), Ali(i+j), and Clu(i) using Equation (3); 3.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+j+1) d (Seg (i+j+1) d (s)) = argmaxˆsIw( ˆT(i); ˆS) based on Clu(i) and w (i+j+1) ˆt ; 3.3 j + +.",
                "If Iw( ˆT; ˆS) changes, go to step 6; otherwise, stop and return Clu(i), Seg(i+j), Ali(i+j), and w (i+j) ˆt ; Figure 2: Algorithm: Topic segmentation and alignment based on MI or WMI. and each sentence is segmented into words, each word is stemmed.",
                "Then the joint probability distribution P(D, Sd, T) can be estimated.",
                "Finally, this distribution can be used to compute MI in our algorithm. 4.3 Iterative Greedy Algorithm Our goal is to maximize the objective function, I( ˆT; ˆS) or Iw( ˆT; ˆS), which can measure the dependence of term occurrences in different segments.",
                "Generally, first we do not know the estimate term weights, which depend on the optimal topic segmentation and alignment, and term clusters.",
                "Moreover, this problem is NP-hard [10], even though if we know the term weights.",
                "Thus, an iterative greedy algorithm is desired to find the best solution, even though probably only local maxima are reached.",
                "We present the iterative greedy algorithm in Figure 2 to find a local maximum of I( ˆT; ˆS) or Iw( ˆT; ˆS) with simultaneous term weight estimation.",
                "This algorithm can is iterative and greedy for multi-document cases or single-document cases with term weight estimation and/or term co-clustering.",
                "Otherwise, since it is just a one step algorithm to solve the task of single-document segmentation [6, 15, 25], the global maximum of MI is guaranteed.",
                "We will show later that term co-clustering reduces the accuracy of the results and is not necessary, and for singledocument segmentation, term weights are also not required. 4.3.1 Initialization In Step 0, the initial term clustering Clut and topic segmentation and alignment Segd and Alid are important to avoid local maxima and reduce the number of iterations.",
                "First, a good guess of term weights can be made by using the distributions of term frequency along sentences for each document and averaging them to get the initial values of wˆt: wt = ( ED(t) maxt ∈T (ED(t )) )(1 − ES(t) maxt ∈T (ES(t )) ), (6) where ES(t) = 1 |Dt| d∈Dt (1 − s∈Sd p(s|t)log|Sd| 1 p(s|t) ), where Dt is the set of documents which contain Term t. Then, for the initial segmentation Seg(0) , we can simply segment documents equally by sentences.",
                "Or we can find the optimal segmentation just for each document d which maximizes the WMI, Seg (0) d = argmaxˆsIw(T; ˆS), where w = w (0) ˆt .",
                "For the initial alignment Ali(0) , we can first assume that the order of segments for each d is the same.",
                "For the initial term clustering Clu(0) , first cluster labels can be set randomly, and after the first time of Step 3, a good initial term clustering is obtained. 4.3.2 Different Cases After initialization, there are three stages for different cases.",
                "Totally there are eight cases, |D| = 1 or |D| > 1, k = l or k < l, w = 0 or w = 1.",
                "Single document segmentation without term clustering and term weight estimation (|D| = 1, k = l, w = 0) only requires Stage 1 (Step 1).",
                "If term clustering is required (k < l), Stage 2 (Step 2.1, 2.2, and 2.3) is executed iteratively.",
                "If term weight estimation is required (w = 1), Stage 3 (Step 3.1, 3.2, and 3.3) is executed iteratively.",
                "If both are required (k < l, w = 1), Stage 2 and 3 run one after the other.",
                "For multi-document segmentation without term clustering and term weight estimation (|D| > 1, k = l, w = 0), only iteration of Step 2.2 and 2.3 are required.",
                "At Stage 1, the global maximum can be found based on I( ˆT; ˆS) using dynamic programming in Section 4.4.",
                "Simultaneously finding a good term clustering and estimated term weights is impossible, since when moving a term to a new term cluster to maximize Iw( ˆT; ˆS), we do not know that the weight of this term should be the one of the new cluster or the old cluster.",
                "Thus, we first do term clustering at Stage 2, and then estimate term weights at Stage 3.",
                "At Stage 2, Step 2.1 is to find the best term clustering and Step 2.2 is to find the best segmentation.",
                "This cycle is repeated to find a local maximum based on MI I until it converges.",
                "The two steps are: (1) based on current term clustering Cluˆt, for each document d, the algorithm segments all the sentences Sd into p segments sequentially (some segments may be empty), and put them into the p segments ˆS of the whole training set D (all possible cases of different segmentation Segd and alignment Alid are checked) to find the optimal case, and (2) based on the current segmentation and alignment, for each term t, the algorithm finds the best term cluster of t based on the current segmentation Segd and alignment Alid.",
                "After finding a good term clustering, term weights are estimated if w = 1.",
                "At Stage 3, similar as Stage 2, Step 3.1 is term weight re-estimation and Step 3.2 is to find a better segmentation.",
                "They are repeated to find a local maximum based on WMI Iw until it converges.",
                "However, if the term clustering in Stage 2 is not accurate, then the term weight estimation at Stage 3 may have a bad result.",
                "Finally, at Step 3.3, this algorithm converges and return the output.",
                "This algorithm can handle both single-document and multi-document segmentation.",
                "It also can detect shared topics among documents by checking the proportion of overlapped sentences on the same topics, as described in Sec 5.2. 4.4 Algorithm Optimization In many previous works on segmentation, dynamic programming is a technique used to maximize the objective function.",
                "Similarly, at Step 1, 2.2, and 3.2 of our algorithm, we can use dynamic programming.",
                "For Stage 1, using dynamic programming can still find the global optimum, but for Stage 2 and Stage 3, we can only find the optimum for each step of topic segmentation and alignment of a document.",
                "Here we only show the dynamic programming for Step 3.2 using WMI (Step 1 and 2.2 are similar but they can use either I or Iw).",
                "There are two cases that are not shown in the algorithm in Figure 2: (a) single-document segmentation or multi-document segmentation with the same sequential order of segments, where alignment is not required, and (b) multi-document segmentation with different sequential orders of segments, where alignment is necessary.",
                "The alignment mapping function of the former case is simply just Alid(ˆsi) = ˆsi, while for the latter ones alignment mapping function Alid(ˆsi) = ˆsj, i and j may be different.",
                "The computational steps for the two cases are listed below: Case 1 (no alignment): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs) and partial pw(ˆs) without counting sentences from d. Then put sentences from i to j into Part k, and compute partial WMI PIw( ˆT; ˆsk(si, si+1, ..., sj)) ˆt∈ ˆT pw(ˆt, ˆsk)log pw(ˆt, ˆsk) pw(ˆt)pw(ˆsk) , where Alid(si, si+1, ..., sj) = k, k ∈ {1, 2, ..., p}, 1 ≤ i ≤ j ≤ nd, and Segd(sq) = ˆsk for all i ≤ q ≤ j. (2) Let M(sm, 1) = PIw( ˆT; ˆs1(s1, s2, ..., sm)).",
                "Then M(sm, L) = maxi[M(si−1, L − 1) + PIw( ˆT; ˆsL(si, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, and when i > m, no sentences are put into ˆsk when compute PIw (note PIw( ˆT; ˆs(si, ..., sm)) = 0 for single-document segmentation). (3) Finally M(snd , p) = maxi[M(si−1, p − 1)+ PIw( ˆT; ˆsp(si, ..., snd ))], where 1 ≤ i ≤ nd+1.",
                "The optimal Iw is found and the corresponding segmentation is the best.",
                "Case 2 (alignment required): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs), and partial pw(ˆs), and PIw( ˆT; ˆsk(si, si+1, ..., sj)) similarly as Case 1. (2) Let M(sm, 1, k) = PIw( ˆT; ˆsk(s1, s2, ..., sm)), where k ∈ {1, 2, ..., p}.",
                "Then M(sm, L, kL) = maxi,j[M(si−1, L − 1, kL/j) + PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, kL ∈ Set(p, L), which is the set of all p!",
                "L!(p−L)! combinations of L segments chosen from all p segments, j ∈ kL, the set of L segments chosen from all p segments, and kL/j is the combination of L − 1 segments in kL except Segment j. (3) Finally, M(snd , p, kp) = maxi,j[M(si−1, p − 1, kp/j) +PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., snd ))], where kp is just the combination of all p segments and 1 ≤ i ≤ nd + 1, which is the optimal Iw and the corresponding segmentation is the best.",
                "The steps of Case 1 and 2 are similar, except in Case 2, alignment is considered in addition to segmentation.",
                "First, basic items of probability for computing Iw are computed excluding Doc d, and then partial WMI by putting every possible sequential segment (including empty segment) of d into every segment of the set.",
                "Second, the optimal sum of PIw for L segments and the leftmost m sentences, M(sm, L), is found.",
                "Finally, the maximal WMI is found among different sums of M(sm, p − 1) and PIw for Segment p. 5.",
                "EXPERIMENTS In this section, single-document segmentation, shared topic detection, and multi-document segmentation will be tested.",
                "Different hyper parameters of our method are studied.",
                "For convenience, we refer to the method using I as MIk if w = 0, and Iw as WMIk if w = 2 or as WMIk if w = 1, where k is the number of term clusters, and if k = l, where l is the total number of terms, then no term clustering is required, i.e.",
                "MIl and WMIl. 5.1 Single-document Segmentation 5.1.1 Test Data and Evaluation The first data set we tested is a synthetic one used in previous research [6, 15, 25] and many other papers.",
                "It has 700 samples.",
                "Each is a concatenation of ten segments.",
                "Each segment is the first n sentence selected randomly from the Brown corpus, which is supposed to have a different topic from each other.",
                "Currently, the best results on this data set is achieved by Ji et.al. [15].",
                "To compare the performance of our methods, the criterion used widely in previous research is applied, instead of the unbiased criterion introduced in [20].",
                "It chooses a pair of words randomly.",
                "If they are in different segments (different) for the real segmentation (real), but predicted (pred) as in the same segment, it is a miss.",
                "If they are in the same segment (same), but predicted as in different segments, it is a false alarm.",
                "Thus, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, diff)p(diff|real) +p(false alarm|real, pred, same)p(same|real). 5.1.2 Experiment Results We tested the case when the number of segments is known.",
                "Table 1 shows the results of our methods with different hyper parameter values and three previous approaches, C99[25], U00[6], and ADDP03[15], on this data set when the segment number is known.",
                "In WMI for single-document segmentation, the term weights are computed as follows: wˆt = 1−EˆS(ˆt)/maxˆt ∈ ˆT (EˆS(ˆt )).",
                "For this case, our methods MIl and WMIl both outperform all the previous approaches.",
                "We compared our methods with ADDP03using one-sample one-sided t-test and p-values are shown in Table 2.",
                "From the p-values, we can see that mostly the differences are very Table 1: Average Error Rates of Single-document Segmentation Given Segment Numbers Known Range of n 3-11 3-5 6-8 9-11 Sample size 400 100 100 100 C99 12% 11% 10% 9% U00 10% 9% 7% 5% ADDP03 6.0% 6.8% 5.2% 4.3% MIl 4.68% 5.57% 2.59% 1.59% WMIl 4.94% 6.33% 2.76% 1.62% MI100 9.62% 12.92% 8.66% 6.67% Table 2: Single-document Segmentation: P-values of T-test on Error Rates Range of n 3-11 3-5 6-8 9-11 ADDP03, MIl 0.000 0.000 0.000 0.000 ADDP03, WMIl 0.000 0.099 0.000 0.000 MIl, WMIl 0.061 0.132 0.526 0.898 significant.",
                "We also compare the error rates between our two methods using two-sample two-sided t-test to check the hypothesis that they are equal.",
                "We cannot reject the hypothesis that they are equal, so the difference are not significant, even though all the error rates for MIl are smaller than WMIl.",
                "However, we can conclude that term weights contribute little in single-document segmentation.",
                "The results also show that MI using term co-clustering (k = 100) decreases the performance.",
                "We tested different number of term clusters, and found that the performance becomes better when the cluster number increases to reach l. WMIk<l has similar results that we did not show in the table.",
                "As mentioned before, using MI may be inconsistent on optimal boundaries given different numbers of segments.",
                "This situation occurs especially when the similarities among segments are quite different, i.e. some transitions are very obvious, while others are not.",
                "This is because usually a document is a hierarchical structure instead of only a sequential structure.",
                "When the segments are not at the same level, this situation may occur.",
                "Thus, a hierarchical topic segmentation approach is desired, and the structure highly depends on the number of segments for each internal node and the stop criteria of splitting.",
                "For this data set of singledocument segmentation, since it is just a synthetic set, which is just a concatenation of several segments about different topics, it is reasonable that approaches simply based on term frequency have a good performance.",
                "Usually for the tasks of segmenting coherent documents for sub-topics, the effectiveness decreases much. 5.2 Shared Topic Detection 5.2.1 Test Data and Evaluation The second data set contains 80 news articles from Google News.",
                "There are eight topics and each has 10 articles.",
                "We randomly split the set into subsets with different document numbers and each subset has all eight topics.",
                "We compare our approach MIl and WMIl with LDA [4].",
                "LDA treats a document in the data set as a bag of words, finds its distribution on topics, and its major topic.",
                "MIl and WMIl views each sentence as a bag of words and tag it with a topic label.",
                "Then for each pair of documents, LDA determines if they are on the same topic, while MIl and Table 3: Shared Topic Detection: Average Error Rates for Different Numbers of Documents in Each Subset #Doc 10 20 40 80 LDA 8.89% 16.33% 1.35% 0.60% MIl, θ = 0.6 4.17% 1.71% 1.47% 0.0% WMIl, θ = 0.8 18.6% 3.16% 1.92% 0.0% WMIl check whether the proportion overlapped sentences on the same topic is larger than the adjustable threshold θ.",
                "That is, in MIl and WMIl, for a pair of documents d, d , if [ s∈Sd,s ∈Sd 1(topics=topics )/min(|Sd|, |Sd|)] > θ, where Sd is the set of sentences of d, and |Sd| is the number of sentences of d, then d and d have the shared topic.",
                "For a pair of documents selected randomly, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, same)p(same|real) +p(false alarm|real, pred, diff)p(diff|real), where a miss means if they have the same topic (same) for the real case (real), but predicted (pred) as on the same topic.",
                "If they are on different topics (diff), but predicted as on the same topic, it is a false alarm. 5.2.2 Experiment Results The results are shown in Table 3.",
                "If most documents have different topics, in WMIl, the estimation of term weights in Equation (3) is not correct.",
                "Thus, WMIl is not expected to have a better performance than MIl, when most documents have different topics.",
                "When there are fewer documents in a subset with the same number of topics, more documents have different topics, so WMIl is more worse than MIl.",
                "We can see that for most cases MIl has a better (or at least similar) performance than LDA.",
                "After shared topic detection, multi-document segmentation of documents with the shared topics is able to be executed. 5.3 Multi-document Segmentation 5.3.1 Test Data and Evaluation For multi-document segmentation and alignment, our goal is to identify these segments about the same topic among multiple similar documents with shared topics.",
                "Using Iw is expected to perform better than I, since without term weights the result is affected seriously by document-dependent stop words and noisy words which depends on the personal writing style.",
                "It is more likely to treat the same segments of different documents as different segments under the effect of document-dependent stop words and noisy words.",
                "Term weights can reduce the effect of document-dependent stop words and noisy words by giving cue terms more weights.",
                "The data set for multi-document segmentation and alignment has 102 samples and 2264 sentences totally.",
                "Each is the introduction part of a lab report selected from the course of Biol 240W, Pennsylvania State University.",
                "Each sample has two segments, introduction of plant hormones and the content in the lab.",
                "The length range of samples is from two to 56 sentences.",
                "Some samples only have one part and some have a reverse order the these two segments.",
                "It is not hard to identify the boundary between two segments for human.",
                "We labelled each sentence manually for evaluation.",
                "The criterion of evaluation is just using the proportion of the number of sentences with wrong predicted segment labels in the total number of sentences in the whole training Table 4: Average Error Rates of Multi-document Segmentation Given Segment Numbers Known #Doc MIl WMIl k MIk WMIk 102 3.14% 2.78% 300 4.68% 6.58% 51 4.17% 3.63% 300 17.83% 22.84% 34 5.06% 4.12% 300 18.75% 20.95% 20 7.08% 5.42% 250 20.40% 21.83% 10 10.38% 7.89% 250 21.42% 21.91% 5 15.77% 11.64% 250 21.89% 22.59% 2 25.90% 23.18% 50 25.44% 25.49% 1 23.90% 24.82% 25 25.75% 26.15% Table 5: Multi-document Segmentation: P-values of T-test on Error Rates for MIl and WMIl #Doc 51 34 20 10 5 2 P-value 0.19 0.101 0.025 0.001 0.000 0.002 set as the error rate: p(error|predicted, real) = d∈D s∈Sd 1(predicteds=reals)/ d∈D nd.",
                "In order to show the benefits of multi-document segmentation and alignment, we compared our method with different parameters on different partitions of the same training set.",
                "Except the cases that the number of documents is 102 and one (they are special cases of using the whole set and the pure single-document segmentation), we randomly divided the training set into m partitions, and each has 51, 34, 20, 10, 5, and 2 document samples.",
                "Then we applied our methods on each partition and calculated the error rate of the whole training set.",
                "Each case was repeated for 10 times for computing the average error rates.",
                "For different partitions of the training set, different k values are used, since the number of terms increases when the document number in each partition increases. 5.3.2 Experiment Results From the experiment results in Table 4, we can see the following observations: (1) When the number of documents increases, all methods have better performances.",
                "Only from one to two documents, MIl has decrease a little.",
                "We can observe this from Figure 3 at the point of document number = 2.",
                "Most curves even have the worst results at this point.",
                "There are two reasons.",
                "First, because samples vote for the best multi-document segmentation and alignment, but if only two documents are compared with each other, the one with missing segments or a totally different sequence will affect the correct segmentation and alignment of the other.",
                "Second, as noted at the beginning of this section, if two documents have more document-dependent stop words or noisy words than cue words, then the algorithm may view them as two different segments and the other segment is missing.",
                "Generally, we can only expect a better performance when the number of documents is larger than the number of segments. (2) Except single-document segmentation, WMIl is always better than MIl, and when the number of documents is reaching one or increases to a very large number, their performances become closer.",
                "Table 5 shows p-values of twosample one-sided t-test between MIl and WMIl.",
                "We also can see this trend from p-values.",
                "When document number = 5, we reached the smallest p-value and the largest difference between error rates of MIl and WMIl.",
                "For single-document Table 6: Multi-document Segmentation: Average Error Rate for Document Number = 5 in Each Subset with Different Number of Term Clusters #Cluster 75 100 150 250 l MIk 24.67% 24.54% 23.91% 22.59% 15.77% segmentation, WMIl is even a little bit worse than MIl, which is similar as the results of the single-document segmentation on the first data set.",
                "The reason is that for singledocument segmentation, we cannot estimate term weights accurately, since multiple documents are unavailable. (3) Using term clustering usually gets worse results than MIl and WMIl. (4) Using term clustering in WMIk is even worse than in MIk, since in WMIk term clusters are found first using I before using Iw.",
                "If the term clusters are not correct, then the term weights are estimated worse, which may mislead the algorithm to reach even worse results.",
                "From the results we also found that in multi-document segmentation and alignment, most documents with missing segments and a reverse order are identified correctly.",
                "Table 6 illustrates the experiment results for the case of 20 partitions (each has five document samples) of the training set and topic segmentation and alignment using MIk with different numbers of term clusters k. Notice that when the number of term clusters increases, the error rate becomes smaller.",
                "Without term clustering, we have the best result.",
                "We did not show results for WMIk with term clustering, but the results are similar.",
                "We also tested WMIl with different hyper parameters of a and b to adjust term weights.",
                "The results are presented in Figure 3.",
                "It was shown that the default case WMIl : a = 1, b = 1 gave the best results for different partitions of the training set.",
                "We can see the trend that when the document number is very small or large, the difference between MIl : a = 0, b = 0 and WMIl : a = 1, b = 1 becomes quite small.",
                "When the document number is not large (about from 2 to 10), all the cases using term weights have better performances than MIl : a = 0, b = 0 without term weights, but when the document number becomes larger, the cases WMIl : a = 1, b = 0 and WMIl : a = 2, b = 1 become worse than MIl : a = 0, b = 0.",
                "When the document number becomes very large, they are even worse than cases with small document numbers.",
                "This means that a proper way to estimate term weights for the criterion of WMI is very important.",
                "Figure 4 shows the term weights learned from the whole training set.",
                "Four types of words are categorized roughly even though the transition among them are subtle.",
                "Figure 5 illustrates the change in (weighted) mutual information for MIl and WMIl.",
                "As expected, mutual information for MIl increases monotonically with the number of steps, while WMIl does not.",
                "Finally, MIl and WMIl are scalable, with computational complexity shown in Figure 6.",
                "One advantage for our approach based on MI is that removing stop words is not required.",
                "Another important advantage is that there are no necessary hyper parameters to adjust.",
                "In single-document segmentation, the performance based on MI is even better for that based on WMI, so no extra hyper parameter is required.",
                "In multi-document segmentation, we show in the experiment, a = 1 and b = 1 is the best.",
                "Our method gives more weights to cue terms.",
                "However, usually cue terms or sentences appear at the beginning of a segment, while the end of the segment may be 1 2 5 10 20 34 51 102 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 Document Number ErrorRate MIl :a=0,b=0 WMI l :a=1,b=1 WMI l :a=1,b=0 WMI l :a=2,b=1 Figure 3: Error rates for different hyper parameters of term weights. 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Normalized Document Entropy NormalizedSegmentEntropy Noisy words Cue words Common stop words Doc−dep stop words Figure 4: Term weights learned from the whole training set. 0 100 200 300 400 500 600 0.06 0.08 0.1 0.12 0.14 0.16 0.18 Number of Steps (Weighted)MutualInformation MI l WMI l Figure 5: Change in (weighted) MI for MIl and WMIl. 0 20 40 60 80 100 120 0 200 400 600 800 1000 1200 1400 1600 1800 2000 Document Number TimetoConverge(sec) MI l WMI l Figure 6: Time to converge for MIl and WMIl. much noisy.",
                "One possible solution is giving more weights to terms at the beginning of each segment.",
                "Moreover, when the length of segments are quite different, long segments have much higher term frequencies, so they may dominate the segmentation boundaries.",
                "Normalization of term frequencies versus the segment length may be useful. 6.",
                "CONCLUSIONS AND FUTURE WORK We proposed a novel method for multi-document topic segmentation and alignment based on weighted mutual information, which can also handle single-document cases.",
                "We used dynamic programming to optimize our algorithm.",
                "Our approach outperforms all the previous methods on singledocument cases.",
                "Moreover, we also showed that doing segmentation among multiple documents can improve the performance tremendously.",
                "Our results also illustrated that using weighted mutual information can utilize the information of multiple documents to reach a better performance.",
                "We only tested our method on limited data sets.",
                "More data sets especially complicated ones should be tested.",
                "More previous methods should be compared with.",
                "Moreover, natural segmentations like paragraphs are hints that can be used to find the optimal boundaries.",
                "Supervised learning also can be considered. 7.",
                "ACKNOWLEDGMENTS The authors want to thank Xiang Ji, and Prof. J. Scott Payne for their help. 8.",
                "REFERENCES [1] A. Banerjee, I. Ghillon, J. Ghosh, S. Merugu, and D. Modha.",
                "A generalized maximum entropy approach to bregman co-clustering and matrix approximation.",
                "In Proceedings of SIGKDD, 2004. [2] R. Bekkerman, R. El-Yaniv, and A. McCallum.",
                "Multi-way distributional clustering via pairwise interactions.",
                "In Proceedings of ICML, 2005. [3] D. M. Blei and P. J. Moreno.",
                "Topic segmentation with an aspect hidden markov model.",
                "In Proceedings of SIGIR, 2001. [4] D. M. Blei, A. Ng, and M. Jordan.",
                "Latent dirichlet allocation.",
                "Journal of Machine Learning Research, 3:993-1022, 2003. [5] T. Brants, F. Chen, and I. Tsochantaridis.",
                "Topic-based document segmentation with probabilistic latent semantic analysis.",
                "In Proceedings of CIKM, 2002. [6] F. Choi.",
                "Advances in domain indepedent linear text segmentation.",
                "In Proceedings of the NAACL, 2000. [7] H. Christensen, B. Kolluru, Y. Gotoh, and S. Renals.",
                "Maximum entropy segmentation of broadcast news.",
                "In Proceedings of ICASSP, 2005. [8] T. Cover and J. Thomas.",
                "Elements of Information Theory.",
                "John Wiley and Sons, New York, USA, 1991. [9] S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and R. Harshman.",
                "Indexing by latent semantic analysis.",
                "Journal of the American Society for Information Systems, 1990. [10] I. Dhillon, S. Mallela, and D. Modha.",
                "Information-theoretic co-clustering.",
                "In Proceedings of SIGKDD, 2003. [11] M. Hajime, H. Takeo, and O. Manabu.",
                "Text segmentation with multiple surface linguistic cues.",
                "In Proceedings of COLING-ACL, 1998. [12] T. K. Ho.",
                "Stop word location and identification for adaptive text recognition.",
                "International Journal of Document Analysis and Recognition, 3(1), August 2000. [13] T. Hofmann.",
                "Probabilistic latent semantic analysis.",
                "In Proceedings of the UAI99, 1999. [14] X. Ji and H. Zha.",
                "Correlating summarization of a pair of multilingual documents.",
                "In Proceedings of RIDE, 2003. [15] X. Ji and H. Zha.",
                "Domain-independent text segmentation using anisotropic diffusion and dynamic programming.",
                "In Proceedings of SIGIR, 2003. [16] X. Ji and H. Zha.",
                "Extracting shared topics of multiple documents.",
                "In Proceedings of the 7th PAKDD, 2003. [17] J. Lafferty, A. McCallum, and F. Pereira.",
                "Conditional random fields: Probabilistic models for segmenting and labeling sequence data.",
                "In Proceedings of ICML, 2001. [18] T. Li, S. Ma, and M. Ogihara.",
                "Entropy-based criterion in categorical clustering.",
                "In Proceedings of ICML, 2004. [19] A. McCallum, D. Freitag, and F. Pereira.",
                "Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of ICML, 2000. [20] L. Pevzner and M. Hearst.",
                "A critique and improvement of an evaluation metric for text segmentation.",
                "Computational Linguistic, 28(1):19-36, 2002. [21] J. C. Reynar.",
                "Statistical models for topic segmentation.",
                "In Proceedings of ACL, 1999. [22] G. Salton and M. McGill.",
                "Introduction to Modern Information Retrieval.",
                "McGraw Hill, 1983. [23] B.",
                "Sun, Q. Tan, P. Mitra, and C. L. Giles.",
                "Extraction and search of chemical formulae in text documents on the web.",
                "In Proceedings of WWW, 2007. [24] B.",
                "Sun, D. Zhou, H. Zha, and J.",
                "Yen.",
                "Multi-task text segmentation and alignment based on weighted mutual information.",
                "In Proceedings of CIKM, 2006. [25] M. Utiyama and H. Isahara.",
                "A statistical model for domain-independent text segmentation.",
                "In Proceedings of the 39th ACL, 1999. [26] C. Wayne.",
                "Multilingual topic detection and tracking: Successful research enabled by corpora and evaluation.",
                "In Proceedings of LREC, 2000. [27] J. Yamron, I. Carp, L. Gillick, S. Lowe, and P. van Mulbregt.",
                "A hidden markov model approach to text segmentation and event tracking.",
                "In Proceedings of ICASSP, 1998. [28] H. Zha and X. Ji.",
                "Correlating multilingual documents via bipartite graph modeling.",
                "In Proceedings of SIGIR, 2002."
            ],
            "original_annotated_samples": [
                "Topic Segmentation with Shared Topic Detection and Alignment of Multiple Documents Bingjun Sun*, Prasenjit Mitra*† , Hongyuan Zha‡ , C. Lee Giles*† , John Yen*† *Department of Computer Science and Engineering † College of Information Sciences and Technology The Pennsylvania State University University Park, PA 16802 ‡ College of Computing The Georgia Institute of Technology Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu ABSTRACT Topic detection and tracking [26] and topic segmentation [15] play an important role in capturing the <br>local and sequential information of document</br>s."
            ],
            "translated_annotated_samples": [
                "La segmentación de temas con detección de temas compartidos y alineación de múltiples documentos Bingjun Sun*, Prasenjit Mitra*†, Hongyuan Zha‡, C. Lee Giles*†, John Yen*† *Departamento de Ciencias de la Computación e Ingeniería † Colegio de Ciencias de la Información y Tecnología Universidad Estatal de Pensilvania University Park, PA 16802 ‡ Colegio de Computación Instituto de Tecnología de Georgia Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu RESUMEN La detección y seguimiento de temas [26] y la segmentación de temas [15] juegan un papel importante en la captura de la <br>información local y secuencial de los documentos</br>."
            ],
            "translated_text": "La segmentación de temas con detección de temas compartidos y alineación de múltiples documentos Bingjun Sun*, Prasenjit Mitra*†, Hongyuan Zha‡, C. Lee Giles*†, John Yen*† *Departamento de Ciencias de la Computación e Ingeniería † Colegio de Ciencias de la Información y Tecnología Universidad Estatal de Pensilvania University Park, PA 16802 ‡ Colegio de Computación Instituto de Tecnología de Georgia Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu RESUMEN La detección y seguimiento de temas [26] y la segmentación de temas [15] juegan un papel importante en la captura de la <br>información local y secuencial de los documentos</br>. El trabajo previo en esta área suele centrarse en documentos individuales, aunque existen múltiples documentos similares en muchos dominios. En este artículo, presentamos un novedoso método no supervisado para la detección de temas compartidos y la segmentación de temas de múltiples documentos similares basado en la información mutua (MI) y la información mutua ponderada (WMI), que es una combinación de MI y pesos de términos. La idea básica es que la segmentación óptima maximiza MI (o WMI). Nuestro enfoque puede detectar temas compartidos entre documentos. Puede encontrar los límites óptimos en un documento y alinear segmentos entre documentos al mismo tiempo. También puede manejar la segmentación de un solo documento como un caso especial de la segmentación y alineación de varios documentos. Nuestros métodos pueden identificar y fortalecer términos de señal que pueden ser utilizados para la segmentación y eliminar parcialmente las palabras de parada mediante el uso de pesos de términos basados en la entropía aprendida de múltiples documentos. Nuestros resultados experimentales muestran que nuestro algoritmo funciona bien para las tareas de segmentación de un solo documento, detección de temas compartidos y segmentación de múltiples documentos. El uso de información de múltiples documentos puede mejorar enormemente el rendimiento de la segmentación de temas, y utilizar WMI es aún mejor que utilizar MI para la segmentación de múltiples documentos. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información-Agrupación; H.3.1 [Almacenamiento y Recuperación de Información]: Análisis de Contenido e Indexación-Procesamiento Lingüístico; I.2.7 [Inteligencia Artificial]: Procesamiento de Lenguaje Natural-Análisis de Texto; I.5.3 [Reconocimiento de Patrones]: Algoritmos de Agrupación-Medidas de Similitud Términos Generales Algoritmos, Diseño, Experimentación 1. INTRODUCCIÓN Muchos investigadores han trabajado en la detección y seguimiento de temas (TDT) [26] y la segmentación de temas durante la última década. La segmentación de temas tiene como objetivo identificar los límites en un documento con la meta de capturar la estructura temática latente. Las tareas de segmentación de temas generalmente se dividen en dos categorías [15]: la segmentación de flujos de texto, donde se identifica la transición de temas, y la segmentación coherente de documentos, en la que los documentos se dividen en subtemas. La primera categoría tiene aplicaciones en el reconocimiento automático del habla, mientras que la segunda tiene más aplicaciones como la consulta parcial de texto en documentos extensos en la recuperación de información, resumen de texto y medición de calidad de múltiples documentos. La investigación previa en relación con TDT se enmarca en la primera categoría, centrada en el seguimiento de temas de datos de discurso de transmisión y texto de agencia de noticias, mientras que la segunda categoría no ha sido estudiada muy bien. Los enfoques tradicionales realizan la segmentación de temas en los documentos uno a la vez [15, 25, 6]. La mayoría de ellos tienen un mal desempeño en tareas sutiles como la segmentación coherente de documentos [15]. A menudo, los usuarios finales buscan documentos que tengan un contenido similar. Los motores de búsqueda, como Google, proporcionan enlaces para obtener páginas similares. A una granularidad más fina, los usuarios pueden estar buscando obtener secciones de un documento similares a una sección en particular que presumiblemente discute un tema de interés para los usuarios. Por lo tanto, la extensión de la segmentación de temas desde documentos individuales para identificar segmentos similares de múltiples documentos similares con el mismo tema es una dirección natural y necesaria, y se espera que la segmentación de temas de múltiples documentos tenga un mejor rendimiento ya que se utiliza más información. Los enfoques tradicionales que utilizan la medición de similitud basada en la frecuencia de términos generalmente parten del mismo supuesto de que un vocabulario similar tiende a estar en un segmento de tema coherente [15, 25, 6]. Sin embargo, suelen tener el problema de identificar las palabras de parada. Por ejemplo, se eliminan las palabras de parada dependientes del documento junto con las palabras de parada genéricas en [15]. Hay dos razones por las que no eliminamos las palabras de parada directamente. Primero, identificar las palabras vacías es otro problema [12] que requiere estimación en cada dominio. Eliminar palabras comunes de parada puede resultar en la pérdida de información útil en un dominio específico. Segundo, aunque se pueden identificar las palabras de parada, la clasificación estricta de palabras de parada y palabras no de parada no puede representar el cambio gradual en la cantidad de contenido informativo de cada palabra. Empleamos una clasificación suave utilizando pesos de términos. En este documento, consideramos el problema de la segmentación de temas como un problema de optimización utilizando técnicas de teoría de la información para encontrar los límites óptimos de un documento dado el número de segmentos de texto, de manera que se minimice la pérdida de información mutua (MI) (o información mutua ponderada (WMI)) después de la segmentación y alineación. Esto es igual a maximizar el MI (o WMI). El MI se centra en medir la diferencia entre segmentos, mientras que investigaciones anteriores se enfocaban en encontrar la similitud (por ejemplo, la distancia del coseno) entre segmentos [15, 25, 6]. La alineación de temas de múltiples documentos similares se puede lograr agrupando las oraciones sobre el mismo tema en el mismo clúster. La segmentación de temas en un solo documento es solo un caso especial del problema de segmentación y alineación de temas en múltiples documentos. Los términos pueden ser co-agrupados como en [10] al mismo tiempo, dado el número de agrupaciones, pero nuestros resultados experimentales muestran que este método resulta en una peor segmentación (ver Tablas 1, 4 y 6). Por lo general, los lectores humanos pueden identificar la transición de temas basándose en palabras de señal, y pueden ignorar las palabras vacías. Inspirados por esto, asignamos a cada término (o grupo de términos) un peso basado en la entropía entre diferentes documentos y diferentes segmentos de documentos. No solo este enfoque puede aumentar la contribución de las palabras clave, sino que también puede disminuir el efecto de las palabras de parada comunes, palabras ruidosas y palabras de parada dependientes del documento. Estas palabras son comunes en un documento. Muchos métodos basados en la similitud de oraciones requieren que estas palabras sean eliminadas antes de que se pueda realizar la segmentación de temas [15]. Nuestros resultados en la Figura 3 muestran que los pesos de los términos son útiles para la segmentación y alineación de temas en múltiples documentos. La principal contribución de este artículo es que introduce un método novedoso para la segmentación de temas utilizando MI y demuestra que este método funciona mejor que los criterios previamente utilizados. Además, hemos abordado el problema de la segmentación y alineación de temas en varios documentos, mientras que la mayoría de las investigaciones existentes se centraron en la segmentación de documentos individuales. La segmentación y alineación de múltiples documentos pueden utilizar información de documentos similares y mejorar significativamente el rendimiento de la segmentación de temas. Obviamente, nuestro enfoque puede manejar documentos individuales como un caso especial cuando varios documentos no están disponibles. Puede detectar temas compartidos entre documentos para determinar si son múltiples documentos sobre el mismo tema. También introducimos el nuevo criterio de WMI basado en pesos de términos aprendidos de múltiples documentos similares, lo cual puede mejorar aún más el rendimiento de la segmentación de temas. Proponemos un algoritmo voraz iterativo basado en programación dinámica y demostramos que funciona bien en la práctica. Algunos de nuestros trabajos anteriores están en [24]. El resto de este documento está organizado de la siguiente manera: En la Sección 2, revisamos el trabajo relacionado. La sección 3 contiene una formulación del problema de segmentación de temas y alineación de múltiples documentos con co-clustering de términos, una revisión del criterio de MI para el clustering, y finalmente una introducción a WMI. En la Sección 4, primero proponemos el algoritmo voraz iterativo de segmentación y alineación de temas con co-clustering de términos, y luego describimos cómo el algoritmo puede ser optimizado mediante programación dinámica. En la Sección 5, se describen experimentos sobre la segmentación de documentos individuales, la detección de temas compartidos y la segmentación de documentos múltiples, y se presentan y discuten los resultados para evaluar el rendimiento de nuestro algoritmo. Las conclusiones y algunas direcciones futuras del trabajo de investigación se discuten en la Sección 6.2. TRABAJO PREVIO En general, los enfoques existentes para la segmentación de texto se dividen en dos categorías: aprendizaje supervisado [19, 17, 23] y aprendizaje no supervisado [3, 27, 5, 6, 15, 25, 21]. El aprendizaje supervisado suele tener un buen rendimiento, ya que aprende funciones a partir de conjuntos de entrenamiento etiquetados. Sin embargo, obtener conjuntos de entrenamiento grandes con etiquetas manuales en oraciones de documentos suele ser prohibitivamente costoso, por lo que se prefieren enfoques no supervisados. Algunos modelos consideran la dependencia entre oraciones y secciones, como el Modelo Oculto de Markov [3, 27], el Modelo de Máxima Entropía de Markov [19] y los Campos Aleatorios Condicionales [17], mientras que muchos otros enfoques se basan en la cohesión léxica o la similitud de las oraciones [5, 6, 15, 25, 21]. Algunos enfoques también se centran en las palabras clave como pistas de transiciones de tema [11]. Si bien algunos métodos existentes solo consideran información en documentos individuales [6, 15], otros utilizan varios documentos [16, 14]. No hay muchas obras en la última categoría, aunque se espera que el rendimiento de la segmentación sea mejor con la utilización de información de múltiples documentos. Investigaciones previas estudiaron métodos para encontrar temas compartidos [16] y segmentación y resumen de temas entre solo un par de documentos [14]. La clasificación y agrupación de textos es un área de investigación relacionada que categoriza documentos en grupos utilizando métodos supervisados o no supervisados. La clasificación o agrupación temática es una dirección importante en esta área, especialmente el co-agrupamiento de documentos y términos, como LSA [9], PLSA [13], y enfoques basados en distancias y particionamiento de gráficos bipartitos [28] o máxima MI [2, 10], o máxima entropía [1, 18]. Los criterios de estos enfoques pueden ser utilizados en el tema de la segmentación de temas. Algunos de esos métodos se han extendido al área de la segmentación de temas, como PLSA [5] y máxima entropía [7], pero hasta donde sabemos, el uso de MI para la segmentación de temas no ha sido estudiado. 3. FORMULACIÓN DEL PROBLEMA Nuestro objetivo es segmentar documentos y alinear los segmentos entre documentos (Figura 1). Sea T el conjunto de términos {t1, t2, ..., tl}, que aparecen en el conjunto no etiquetado de documentos D = {d1, d2, ..., dm}. Sea Sd el conjunto de oraciones para el documento d ∈ D, es decir, {s1, s2, ..., snd}. Tenemos una matriz tridimensional de frecuencia de términos, en la que las tres dimensiones son variables aleatorias de D, Sd y T. Sd en realidad es un vector aleatorio que incluye una variable aleatoria para cada d ∈ D. La frecuencia de términos se puede utilizar para estimar la distribución de probabilidad conjunta P(D, Sd, T), que es p(t, d, s) = T(t, d, s)/ND, donde T(t, d, s) es el número de t en la oración s de d y ND es el número total de términos en D. ˆS representa el conjunto de segmentos {ˆs1, ˆs2, ..., ˆsp} después de la segmentación y alineación entre varios documentos, donde el número de segmentos | ˆS| = p. Un segmento ˆsi del documento d es una secuencia de oraciones adyacentes en d. Dado que para diferentes documentos si pueden discutir diferentes subtemas, nuestro objetivo es agrupar oraciones adyacentes en cada documento en segmentos, y alinear segmentos similares entre documentos, de modo que para diferentes documentos ˆsi trate sobre el mismo subtema. El objetivo es encontrar la segmentación óptima de temas y el mapeo de alineación Segd(si) : {s1, s2, ..., snd } → {ˆs1, ˆs2, ..., ˆsp} y Alid(ˆsi) : {ˆs1, ˆs2, ..., ˆsp} → {ˆs1, ˆs2, ..., ˆsp}, para todo d ∈ D, donde ˆsi es el i-ésimo segmento con la restricción de que solo las oraciones adyacentes pueden ser asignadas al mismo segmento, es decir, para d, {si, si+1, ..., sj} → {ˆsq}, donde q ∈ {1, ..., p}, donde p es el número de segmento, y si i > j, entonces para d, ˆsq está ausente. Después de la segmentación y alineación, el vector aleatorio Sd se convierte en una variable aleatoria alineada ˆS. Por lo tanto, P(D, Sd, T) se convierte en P(D, ˆS, T). El término co-clustering es una técnica que se ha empleado [10] para mejorar la precisión del agrupamiento de documentos. Evaluamos el efecto de ello para la segmentación de temas. Un término t se asigna a exactamente un grupo de términos. El término co-clustering implica encontrar simultáneamente el mapeo óptimo de agrupación de términos Clu(t) : {t1, t2, ..., tl} → {ˆt1, ˆt2, ..., ˆtk}, donde k ≤ l, l es el número total de palabras en todos los documentos, y k es el número de clústeres. 4. METODOLOGÍA Ahora describimos un algoritmo novedoso que puede manejar la segmentación de un solo documento, la detección de temas compartidos y la segmentación y alineación de múltiples documentos basados en MI o WMI. 4.1 Información Mutua MI I(X; Y) es una cantidad que mide la cantidad de información contenida en dos o más variables aleatorias [8, 10]. Para el caso de dos variables aleatorias, tenemos I(X; Y) = x∈X y∈Y p(x, y)log p(x, y) p(x)p(y) , (1) Obviamente, cuando las variables aleatorias X e Y son independientes, I(X; Y) = 0. Por lo tanto, de manera intuitiva, el valor de MI depende de cómo las variables aleatorias están relacionadas entre sí. La co-agrupación óptima es el mapeo Clux: X → ˆX y Cluy: Y → ˆY que minimiza la pérdida: I(X; Y) - I(ˆX; ˆY), lo cual es igual a maximizar I(ˆX; ˆY). Este es el criterio de MI para la agrupación. En el caso de la segmentación de temas, las dos variables aleatorias son la variable de término T y la variable de segmento S, y cada muestra es una ocurrencia de un término T = t en un segmento particular S = s. Se utiliza I(T; S) para medir qué tan dependientes son T y S. Sin embargo, no se puede calcular I(T; S) para documentos antes de la segmentación, ya que no tenemos un conjunto de S debido a que las oraciones del Documento d, si ∈ Sd, no están alineadas con otros documentos. Así, en lugar de minimizar la pérdida de MI, podemos maximizar MI después de la segmentación de temas, calculada como: I( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS p(ˆt, ˆs)log p(ˆt, ˆs) p(ˆt)p(ˆs) , (2) donde p(ˆt, ˆs) se estiman mediante la frecuencia de términos tf del Cluster de Términos ˆt y el Segmento ˆs en el conjunto de entrenamiento D. Nótese que aquí un segmento ˆs incluye oraciones sobre el mismo tema entre todos los documentos. La solución óptima es el mapeo Clut : T → ˆT, Segd : Sd → ˆS, y Alid : ˆS → ˆS, que maximiza I( ˆT; ˆS). 4.2 Información Mutua Ponderada En la segmentación de temas y alineación de múltiples documentos, si se conoce P(D, ˆS, T), basado en las distribuciones marginales P(D|T) y P( ˆS|T) para cada término t ∈ T, podemos categorizar los términos en cuatro tipos en el conjunto de datos: • Las palabras de parada comunes son comunes a lo largo de las dimensiones de documentos y segmentos. • Las palabras de parada dependientes del documento que dependen del estilo de escritura personal son comunes solo a lo largo de la dimensión de segmentos para algunos documentos. • Las palabras clave son los elementos más importantes para la segmentación. Son comunes a lo largo de la dimensión de los documentos solo para el mismo segmento, y no son comunes a lo largo de las dimensiones de los segmentos. • Las palabras ruidosas son otras palabras que no son comunes a lo largo de ambas dimensiones. La entropía basada en P(D|T) y P( ˆS|T) se puede utilizar para identificar diferentes tipos de términos. Para reforzar la contribución de las palabras clave en el cálculo de MI, y al mismo tiempo reducir el efecto de los otros tres tipos de palabras, similar a la idea del peso tf-idf [22], utilizamos las entropías de cada término a lo largo de las dimensiones del documento D y del segmento ˆS, es decir, ED(ˆt) y EˆS(ˆt), para calcular el peso. Una palabra de señal generalmente tiene un valor alto de ED(ˆt) pero un valor bajo de EˆS(ˆt). Introducimos los pesos de términos (o pesos de grupos de términos) wˆt = ( ED(ˆt) maxˆt ∈ ˆT (ED(ˆt )) )a (1 − EˆS(ˆt) maxˆt ∈ ˆT (EˆS(ˆt )) )b , (3) donde ED(ˆt) = d∈D p(d|ˆt)log|D| 1 p(d|ˆt) , EˆS(ˆt) = ˆs∈ ˆS p(ˆs|ˆt)log| ˆS| 1 p(ˆs|ˆt) , y a > 0 y b > 0 son potencias para ajustar los pesos de términos. Normalmente a = 1 y b = 1 por defecto, y se utilizan maxˆt ∈ ˆT (ED(ˆt )) y maxˆt ∈ ˆT (EˆS(ˆt )) para normalizar los valores de entropía. Los pesos de los grupos de términos se utilizan para ajustar p(ˆt, ˆs), pw(ˆt, ˆs) = wˆtp(ˆt, ˆs) ˆt∈ ˆT ;ˆs∈ ˆS wˆtp(ˆt, ˆs) , (4) e Iw( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS pw(ˆt, ˆs)log pw(ˆt, ˆs) pw(ˆt)pw(ˆs) , (5) donde pw(ˆt) y pw(ˆs) son distribuciones marginales de pw(ˆt, ˆs). Sin embargo, dado que no conocemos ni los pesos de los términos ni P(D, ˆS, T), necesitamos estimarlos, pero wˆt depende de p(ˆs|t) y ˆS, mientras que ˆS y p(ˆs|t) también dependen de wˆt, que aún es desconocido. Por lo tanto, se requiere un algoritmo iterativo para estimar los pesos de los términos wˆt y encontrar la mejor segmentación y alineación para optimizar la función objetivo Iw de manera concurrente. Después de que un documento se segmenta en oraciones, se introduce: la distribución de probabilidad conjunta P(D, Sd, T), el número de segmentos de texto p ∈ {2, 3, ..., máx(sd)}, el número de grupos de términos k ∈ {2, 3, ..., l} (si k = l, no se requiere co-clustering de términos) y el tipo de peso w ∈ {0, 1}, indicando usar I o Iw, respectivamente. Mapeo de Clu, Seg, Ali y pesos de términos wˆt. Inicialización: 0. i = 0. Inicializar Clu (0) t, Seg (0) d y Ali (0) d; Inicializar w (0) ˆt usando la Ecuación (6) si w = 1; Etapa 1: 1. Si |D| = 1, k = l y w = 0, verifica todas las segmentaciones secuenciales de d en p segmentos y encuentra la mejor Segd(s) = argmaxˆsI( ˆT; ˆS), y devuelve Segd; de lo contrario, si w = 1 y k = l, ve a 3.1; Etapa 2: 2.1 Si k < l, para cada término t, encuentra el mejor clúster ˆt como Clu(i+1)(t) = argmaxˆtI( ˆT; ˆS(i)) basado en Seg(i) y Ali(i); 2.2 Para cada d, verifica todas las segmentaciones secuenciales de d en p segmentos con mapeo s → ˆs → ˆs, y encuentra la mejor Ali (i+1) d (Seg (i+1) d (s)) = argmaxˆsI( ˆT(i+1); ˆS) basado en Clu(i+1)(t) si k < l o Clu(0)(t) si k = l; 2.3 i + +. Si Clu, Seg o Ali cambian, ve a 2.1; de lo contrario, si w = 0, devuelve Clu(i), Seg(i) y Ali(i); de lo contrario, j = 0, ve a 3.1; Etapa 3: 3.1 Actualiza w (i+j+1) ˆt basado en Seg(i+j), Ali(i+j) y Clu(i) usando la Ecuación (3); 3.2 Para cada d, verifica todas las segmentaciones secuenciales de d en p segmentos con mapeo s → ˆs → ˆs, y encuentra la mejor Ali (i+j+1) d (Seg (i+j+1) d (s)) = argmaxˆsIw( ˆT(i); ˆS) basado en Clu(i) y w (i+j+1) ˆt; 3.3 j + +. Si Iw( ˆT; ˆS) cambia, ve al paso 6; de lo contrario, detente y devuelve Clu(i), Seg(i+j), Ali(i+j), y w (i+j) ˆt; Figura 2: Algoritmo: Segmentación y alineación de temas basada en MI o WMI. y cada oración se segmenta en palabras, cada palabra se reduce a su raíz. Entonces se puede estimar la distribución de probabilidad conjunta P(D, Sd, T). Finalmente, esta distribución se puede utilizar para calcular la MI en nuestro algoritmo. 4.3 Algoritmo Voraz Iterativo Nuestro objetivo es maximizar la función objetivo, I( ˆT; ˆS) o Iw( ˆT; ˆS), que puede medir la dependencia de las ocurrencias de términos en diferentes segmentos. Generalmente, primero no conocemos los pesos estimados de los términos, los cuales dependen de la segmentación y alineación óptimas de los temas, y los grupos de términos. Además, este problema es NP-duro [10], incluso si conocemos los pesos de los términos. Por lo tanto, se desea un algoritmo voraz iterativo para encontrar la mejor solución, aunque probablemente solo se alcancen máximos locales. Presentamos el algoritmo voraz iterativo en la Figura 2 para encontrar un máximo local de I( ˆT; ˆS) o Iw( ˆT; ˆS) con estimación simultánea de pesos de términos. Este algoritmo es iterativo y codicioso para casos de múltiples documentos o casos de un solo documento con estimación de peso de términos y/o co-clustering de términos. De lo contrario, dado que es solo un algoritmo de un paso para resolver la tarea de segmentación de un solo documento [6, 15, 25], se garantiza el máximo global de MI. Mostraremos más adelante que la coagrupación de términos reduce la precisión de los resultados y no es necesaria, y para la segmentación de un solo documento, los pesos de los términos tampoco son requeridos. 4.3.1 Inicialización En el Paso 0, la agrupación inicial de términos Clut y la segmentación y alineación de temas Segd y Alid son importantes para evitar máximos locales y reducir el número de iteraciones. Primero, se puede hacer una buena estimación de los pesos de los términos utilizando las distribuciones de frecuencia de términos a lo largo de las oraciones para cada documento y promediándolas para obtener los valores iniciales de wˆt: wt = ( ED(t) maxt ∈T (ED(t )) )(1 − ES(t) maxt ∈T (ES(t )) ), (6) donde ES(t) = 1 |Dt| d∈Dt (1 − s∈Sd p(s|t)log|Sd| 1 p(s|t) ), donde Dt es el conjunto de documentos que contienen el término t. Luego, para la segmentación inicial Seg(0), simplemente podemos segmentar los documentos de manera equitativa por oraciones. O podemos encontrar la segmentación óptima solo para cada documento d que maximice el WMI, Seg (0) d = argmaxˆsIw(T; ˆS), donde w = w (0) ˆt. Para el alineamiento inicial Ali(0), podemos primero asumir que el orden de los segmentos para cada d es el mismo. Para el agrupamiento inicial de términos Clu(0), las etiquetas de los primeros grupos pueden establecerse al azar, y después de la primera vez del Paso 3, se obtiene un buen agrupamiento inicial de términos. 4.3.2 Diferentes Casos Después de la inicialización, hay tres etapas para diferentes casos. En total hay ocho casos, |D| = 1 o |D| > 1, k = l o k < l, w = 0 o w = 1. La segmentación de un solo documento sin agrupamiento de términos y estimación de peso de términos (|D| = 1, k = l, w = 0) solo requiere la Etapa 1 (Paso 1). Si se requiere el agrupamiento de términos (k < l), la Etapa 2 (Paso 2.1, 2.2 y 2.3) se ejecuta de forma iterativa. Si se requiere la estimación del peso del término (w = 1), la Etapa 3 (Paso 3.1, 3.2 y 3.3) se ejecuta de forma iterativa. Si ambos son requeridos (k < l, w = 1), la Etapa 2 y 3 se ejecutan una tras otra. Para la segmentación de múltiples documentos sin agrupamiento de términos y estimación de peso de términos (|D| > 1, k = l, w = 0), solo se requiere la iteración de los Pasos 2.2 y 2.3. En la Etapa 1, el máximo global se puede encontrar basándose en I( ˆT; ˆS) utilizando programación dinámica en la Sección 4.4. Es imposible encontrar simultáneamente un buen agrupamiento de términos y pesos estimados de términos, ya que al mover un término a un nuevo grupo de términos para maximizar Iw( ˆT; ˆS), no sabemos si el peso de este término debería ser el del nuevo grupo o el del antiguo. Por lo tanto, primero realizamos el agrupamiento de términos en la Etapa 2, y luego estimamos los pesos de los términos en la Etapa 3. En la Etapa 2, el Paso 2.1 consiste en encontrar el mejor agrupamiento de términos y el Paso 2.2 consiste en encontrar la mejor segmentación. Este ciclo se repite para encontrar un máximo local basado en MI I hasta que converge. Los dos pasos son: (1) basados en el agrupamiento de términos actual Cluˆt, para cada documento d, el algoritmo segmenta todas las oraciones Sd en p segmentos secuencialmente (algunos segmentos pueden estar vacíos), y los coloca en los p segmentos ˆS del conjunto de entrenamiento completo D (se verifican todos los casos posibles de segmentación diferente Segd y alineación Alid) para encontrar el caso óptimo, y (2) basado en la segmentación y alineación actual, para cada término t, el algoritmo encuentra el mejor grupo de términos de t basado en la segmentación actual Segd y la alineación Alid. Después de encontrar un buen agrupamiento de términos, se estiman los pesos de los términos si w = 1. En la Etapa 3, al igual que en la Etapa 2, el Paso 3.1 es la reestimación del peso del término y el Paso 3.2 es encontrar una mejor segmentación. Se repiten para encontrar un máximo local basado en WMI Iw hasta que converja. Sin embargo, si el agrupamiento en la Etapa 2 no es preciso, entonces la estimación de peso en la Etapa 3 puede tener un mal resultado. Finalmente, en el Paso 3.3, este algoritmo converge y devuelve la salida. Este algoritmo puede manejar tanto la segmentación de un solo documento como la de múltiples documentos. También puede detectar temas compartidos entre documentos al verificar la proporción de frases superpuestas sobre los mismos temas, como se describe en la Sección 5.2. 4.4 Optimización del algoritmo. En muchos trabajos anteriores sobre segmentación, la programación dinámica es una técnica utilizada para maximizar la función objetivo. De manera similar, en los Pasos 1, 2.2 y 3.2 de nuestro algoritmo, podemos utilizar programación dinámica. Para la Etapa 1, utilizando programación dinámica aún se puede encontrar el óptimo global, pero para la Etapa 2 y la Etapa 3, solo podemos encontrar el óptimo para cada paso de segmentación de temas y alineación de un documento. Aquí solo mostramos la programación dinámica para el Paso 3.2 utilizando WMI (el Paso 1 y 2.2 son similares pero pueden usar I o Iw). Hay dos casos que no se muestran en el algoritmo de la Figura 2: (a) segmentación de un solo documento o segmentación de múltiples documentos con el mismo orden secuencial de segmentos, donde no se requiere alineación, y (b) segmentación de múltiples documentos con diferentes órdenes secuenciales de segmentos, donde la alineación es necesaria. La función de mapeo de alineación del primer caso es simplemente Alid(ˆsi) = ˆsi, mientras que para los últimos casos la función de mapeo de alineación es Alid(ˆsi) = ˆsj, donde i y j pueden ser diferentes. Los pasos computacionales para los dos casos se enumeran a continuación: Caso 1 (sin alineación): Para cada documento d: (1) Calcular pw(ˆt), pw parcial(ˆt, ˆs) y pw parcial(ˆs) sin contar las oraciones de d. Luego, colocar las oraciones de i a j en la Parte k, y calcular WMI parcial PIw( ˆT; ˆsk(si, si+1, ..., sj)) ˆt∈ ˆT pw(ˆt, ˆsk)log pw(ˆt, ˆsk) pw(ˆt)pw(ˆsk) , donde Alid(si, si+1, ..., sj) = k, k ∈ {1, 2, ..., p}, 1 ≤ i ≤ j ≤ nd, y Segd(sq) = ˆsk para todo i ≤ q ≤ j. (2) Dejar que M(sm, 1) = PIw( ˆT; ˆs1(s1, s2, ..., sm)). Entonces M(sm, L) = maxi[M(si−1, L − 1) + PIw( ˆT; ˆsL(si, ..., sm))], donde 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, y cuando i > m, no se colocan oraciones en ˆsk al calcular PIw (nota PIw( ˆT; ˆs(si, ..., sm)) = 0 para la segmentación de un solo documento). (3) Finalmente M(snd , p) = maxi[M(si−1, p − 1)+ PIw( ˆT; ˆsp(si, ..., snd ))], donde 1 ≤ i ≤ nd+1. Se encuentra el Iw óptimo y la segmentación correspondiente es la mejor. Caso 2 (alineación requerida): Para cada documento d: (1) Calcular pw(ˆt), pw parcial(ˆt, ˆs), y pw parcial(ˆs), y PIw( ˆT; ˆsk(si, si+1, ..., sj)) de manera similar al Caso 1. (2) Sea M(sm, 1, k) = PIw( ˆT; ˆsk(s1, s2, ..., sm)), donde k ∈ {1, 2, ..., p}. Entonces M(sm, L, kL) = maxi,j[M(si−1, L − 1, kL/j) + PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., sm))], donde 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, kL ∈ Set(p, L), que es el conjunto de todos los p! Hay L!(p−L)! combinaciones de L segmentos elegidos de entre todos los p segmentos, j ∈ kL, el conjunto de L segmentos elegidos de entre todos los p segmentos, y kL/j es la combinación de L − 1 segmentos en kL excepto el Segmento j. (3) Finalmente, M(snd , p, kp) = maxi,j[M(si−1, p − 1, kp/j) +PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., snd ))], donde kp es simplemente la combinación de todos los p segmentos y 1 ≤ i ≤ nd + 1, que es el óptimo Iw y la segmentación correspondiente es la mejor. Los pasos de los Casos 1 y 2 son similares, excepto que en el Caso 2, se considera el alineamiento además de la segmentación. Primero, se calculan los elementos básicos de probabilidad para computar Iw excluyendo el documento d, y luego se calcula el WMI parcial colocando cada segmento secuencial posible (incluido el segmento vacío) de d en cada segmento del conjunto. Segundo, se encuentra la suma óptima de PIw para L segmentos y las m oraciones más a la izquierda, M(sm, L). Finalmente, se encuentra el WMI máximo entre diferentes sumas de M(sm, p − 1) y PIw para el Segmento p. 5. EXPERIMENTOS En esta sección se probará la segmentación de un solo documento, la detección de temas compartidos y la segmentación de múltiples documentos. Se estudian diferentes hiperparámetros de nuestro método. Para mayor comodidad, nos referimos al método utilizando I como MIk si w = 0, e Iw como WMIk si w = 2 o como WMIk si w = 1, donde k es el número de grupos de términos, y si k = l, donde l es el número total de términos, entonces no se requiere agrupación de términos, es decir, El primer conjunto de datos que probamos es uno sintético utilizado en investigaciones anteriores [6, 15, 25] y muchos otros artículos. Tiene 700 muestras. Cada uno es una concatenación de diez segmentos. Cada segmento es la primera oración seleccionada al azar de las n primeras oraciones del corpus Brown, que se supone que tienen un tema diferente entre sí. Actualmente, los mejores resultados en este conjunto de datos son logrados por Ji et al. [15]. Para comparar el rendimiento de nuestros métodos, se aplica el criterio ampliamente utilizado en investigaciones anteriores en lugar del criterio imparcial introducido en [20]. Elige un par de palabras al azar. Si se encuentran en segmentos diferentes (diferentes) para la segmentación real (real), pero se predicen (pred) como en el mismo segmento, es un error. Si están en el mismo segmento (mismo), pero se predice que están en segmentos diferentes, es una falsa alarma. Por lo tanto, la tasa de error se calcula utilizando la siguiente ecuación: p(err|real, pred) = p(miss|real, pred, diff)p(diff|real) + p(false alarm|real, pred, same)p(same|real). 5.1.2 Resultados del Experimento Probamos el caso cuando se conoce el número de segmentos. La Tabla 1 muestra los resultados de nuestros métodos con diferentes valores de hiperparámetros y tres enfoques anteriores, C99[25], U00[6] y ADDP03[15], en este conjunto de datos cuando se conoce el número de segmento. En WMI para la segmentación de un solo documento, los pesos de los términos se calculan de la siguiente manera: wˆt = 1−EˆS(ˆt)/maxˆt ∈ ˆT (EˆS(ˆt)). Para este caso, nuestros métodos MIl y WMIl superan a todos los enfoques anteriores. Comparamos nuestros métodos con ADDP03 utilizando una prueba t de una muestra unilateral y los valores de p se muestran en la Tabla 2. A partir de los valores p, podemos ver que en su mayoría las diferencias son muy significativas. También comparamos las tasas de error entre nuestros dos métodos utilizando una prueba t de dos muestras de dos colas para verificar la hipótesis de que son iguales. No podemos rechazar la hipótesis de que son iguales, por lo que las diferencias no son significativas, a pesar de que todas las tasas de error para MIl son más pequeñas que las de WMIl. Sin embargo, podemos concluir que los pesos de los términos contribuyen poco en la segmentación de un solo documento. Los resultados también muestran que el uso de MI con co-clustering de términos (k = 100) disminuye el rendimiento. Probamos diferentes números de grupos de términos y encontramos que el rendimiento mejora cuando el número de grupos aumenta hasta llegar a l. WMIk<l tiene resultados similares que no mostramos en la tabla. Como se mencionó anteriormente, el uso de MI puede ser inconsistente en los límites óptimos dados diferentes números de segmentos. Esta situación ocurre especialmente cuando las similitudes entre los segmentos son bastante diferentes, es decir, algunas transiciones son muy obvias, mientras que otras no lo son. Esto se debe a que normalmente un documento es una estructura jerárquica en lugar de una estructura únicamente secuencial. Cuando los segmentos no están en el mismo nivel, esta situación puede ocurrir. Por lo tanto, se desea un enfoque de segmentación jerárquica de temas, y la estructura depende en gran medida del número de segmentos para cada nodo interno y del criterio de parada de la división. Para este conjunto de datos de segmentación de un solo documento, dado que es solo un conjunto sintético, que es simplemente una concatenación de varios segmentos sobre diferentes temas, es razonable que enfoques simplemente basados en la frecuencia de términos tengan un buen rendimiento. Normalmente, para las tareas de segmentación de documentos coherentes en subtemas, la efectividad disminuye mucho. 5.2 Detección de Temas Compartidos 5.2.1 Datos de Prueba y Evaluación El segundo conjunto de datos contiene 80 artículos de noticias de Google News. Hay ocho temas y cada uno tiene 10 artículos. Dividimos aleatoriamente el conjunto en subconjuntos con diferentes números de documentos y cada subconjunto tiene los ocho temas. Comparamos nuestro enfoque MIl y WMIl con LDA [4]. LDA trata un documento en el conjunto de datos como una bolsa de palabras, encuentra su distribución en temas y su tema principal. MIl y WMIl ven cada oración como un conjunto de palabras y la etiquetan con una etiqueta de tema. Luego, para cada par de documentos, LDA determina si están en el mismo tema, mientras que MIl y Table 3: Detección de Temas Compartidos: Tasas de Error Promedio para Diferentes Números de Documentos en Cada Subconjunto #Doc 10 20 40 80 LDA 8.89% 16.33% 1.35% 0.60% MIl, θ = 0.6 4.17% 1.71% 1.47% 0.0% WMIl, θ = 0.8 18.6% 3.16% 1.92% 0.0% WMIl comprueba si la proporción de oraciones superpuestas en el mismo tema es mayor que el umbral ajustable θ. Es decir, en MIl y WMIl, para un par de documentos d, d , si [ s∈Sd,s ∈Sd 1(temas=temas )/min(|Sd|, |Sd|)] > θ, donde Sd es el conjunto de oraciones de d, y |Sd| es el número de oraciones de d, entonces d y d comparten el tema. Para un par de documentos seleccionados al azar, la tasa de error se calcula utilizando la siguiente ecuación: p(err|real, pred) = p(miss|real, pred, same)p(same|real) + p(false alarm|real, pred, diff)p(diff|real), donde un error significa si tienen el mismo tema (same) para el caso real (real), pero se predice (pred) como en un tema diferente. Si están en diferentes temas (diff), pero se predicen como en el mismo tema, es una falsa alarma. 5.2.2 Resultados del Experimento Los resultados se muestran en la Tabla 3. Si la mayoría de los documentos tienen diferentes temas, en WMIl, la estimación de los pesos de términos en la Ecuación (3) no es correcta. Por lo tanto, no se espera que WMIL tenga un mejor rendimiento que MIL cuando la mayoría de los documentos tienen temas diferentes. Cuando hay menos documentos en un subconjunto con el mismo número de temas, más documentos tienen temas diferentes, por lo que WMIl es peor que MIl. Podemos ver que en la mayoría de los casos, MIl tiene un rendimiento mejor (o al menos similar) que LDA. Después de la detección de temas compartidos, se puede llevar a cabo la segmentación de documentos con los temas compartidos. 5.3 Segmentación de múltiples documentos 5.3.1 Datos de prueba y evaluación Para la segmentación y alineación de múltiples documentos, nuestro objetivo es identificar estos segmentos sobre el mismo tema entre varios documentos similares con temas compartidos. Se espera que al utilizar pesos de términos, Iw funcione mejor que I, ya que sin pesos de términos, el resultado se ve seriamente afectado por palabras de parada dependientes del documento y palabras ruidosas que dependen del estilo de escritura personal. Es más probable tratar los mismos segmentos de diferentes documentos como segmentos diferentes bajo el efecto de palabras de parada dependientes del documento y palabras ruidosas. Los pesos de términos pueden reducir el efecto de las palabras de parada dependientes del documento y las palabras ruidosas al darle más peso a los términos de señal. El conjunto de datos para la segmentación y alineación de múltiples documentos tiene un total de 102 muestras y 2264 oraciones. Cada uno es la parte de introducción de un informe de laboratorio seleccionado del curso de Biol 240W, Universidad Estatal de Pensilvania. Cada muestra tiene dos segmentos, la introducción de las hormonas vegetales y el contenido en el laboratorio. El rango de longitud de las muestras va desde dos hasta 56 oraciones. Algunas muestras solo tienen una parte y algunas tienen un orden inverso de estos dos segmentos. No es difícil identificar el límite entre dos segmentos para los humanos. Etiquetamos cada oración manualmente para evaluación. El criterio de evaluación consiste en utilizar la proporción del número de oraciones con etiquetas de segmento incorrectas predichas en el número total de oraciones en toda la Tabla de entrenamiento. Para mostrar los beneficios de la segmentación y alineación de múltiples documentos, comparamos nuestro método con diferentes parámetros en diferentes particiones del mismo conjunto de entrenamiento. Excepto los casos en los que el número de documentos es 102 y uno (que son casos especiales de uso del conjunto completo y la segmentación pura de un solo documento), dividimos aleatoriamente el conjunto de entrenamiento en m particiones, y cada una tiene 51, 34, 20, 10, 5 y 2 muestras de documentos. Luego aplicamos nuestros métodos en cada partición y calculamos la tasa de error de todo el conjunto de entrenamiento. Cada caso se repitió 10 veces para calcular las tasas de error promedio. Para diferentes particiones del conjunto de entrenamiento, se utilizan diferentes valores de k, ya que el número de términos aumenta cuando el número de documentos en cada partición aumenta. 5.3.2 Resultados del Experimento De los resultados del experimento en la Tabla 4, podemos ver las siguientes observaciones: (1) Cuando el número de documentos aumenta, todos los métodos tienen un mejor rendimiento. Solo de uno a dos documentos, el MIL ha disminuido un poco. Podemos observar esto en la Figura 3 en el punto del número de documento = 2. La mayoría de las curvas incluso tienen los peores resultados en este punto. Hay dos razones. Primero, porque las muestras votan por la mejor segmentación y alineación de múltiples documentos, pero si solo se comparan dos documentos entre sí, aquel con segmentos faltantes o una secuencia totalmente diferente afectará la correcta segmentación y alineación del otro. Segundo, como se señaló al principio de esta sección, si dos documentos tienen más palabras de parada dependientes del documento o palabras ruidosas que palabras clave, entonces el algoritmo puede considerarlos como dos segmentos diferentes y faltar el otro segmento. Generalmente, solo podemos esperar un mejor rendimiento cuando el número de documentos es mayor que el número de segmentos. (2) Excepto en la segmentación de un solo documento, WMIl siempre es mejor que MIl, y cuando el número de documentos se acerca a uno o aumenta a un número muy grande, sus rendimientos se vuelven más cercanos. La Tabla 5 muestra los valores p de la prueba t de dos muestras unilaterales entre MIl y WMIl. También podemos observar esta tendencia a partir de los valores de p. Cuando el número de documento es igual a 5, alcanzamos el valor p más pequeño y la mayor diferencia entre las tasas de error de MIl y WMIl. Para la Tabla 6 de un solo documento: Segmentación de múltiples documentos: Tasa de error promedio para el Documento Número = 5 en cada subconjunto con Diferente Número de Agrupaciones de Términos #Agrupaciones 75 100 150 250 l MIk 24.67% 24.54% 23.91% 22.59% 15.77% de segmentación, WMIl es incluso un poco peor que MIl, lo cual es similar a los resultados de la segmentación de un solo documento en el primer conjunto de datos. La razón es que para la segmentación de un solo documento, no podemos estimar con precisión los pesos de los términos, ya que no tenemos disponibles varios documentos. (3) El uso de agrupamiento de términos generalmente produce resultados peores que MIl y WMIl. (4) El uso de agrupamiento de términos en WMIk es aún peor que en MIk, ya que en WMIk los grupos de términos se encuentran primero utilizando I antes que Iw. Si los grupos de términos no son correctos, entonces los pesos de los términos se estiman peor, lo que puede llevar a que el algoritmo obtenga resultados aún peores. De los resultados también encontramos que en la segmentación y alineación de múltiples documentos, la mayoría de los documentos con segmentos faltantes y en orden inverso son identificados correctamente. La Tabla 6 ilustra los resultados del experimento para el caso de 20 particiones (cada una con cinco muestras de documentos) del conjunto de entrenamiento y la segmentación y alineación de temas utilizando MIk con diferentes números de grupos de términos k. Observa que cuando el número de grupos de términos aumenta, la tasa de error disminuye. Sin agrupamiento de términos, obtenemos el mejor resultado. No mostramos resultados para WMIk con agrupamiento de términos, pero los resultados son similares. También probamos WMIL con diferentes hiperparámetros de a y b para ajustar los pesos de los términos. Los resultados se presentan en la Figura 3. Se demostró que el caso predeterminado WMIl: a = 1, b = 1 dio los mejores resultados para diferentes particiones del conjunto de entrenamiento. Podemos observar la tendencia de que cuando el número de documentos es muy pequeño o grande, la diferencia entre MIl: a = 0, b = 0 y WMIl: a = 1, b = 1 se vuelve bastante pequeña. Cuando el número de documentos no es grande (aproximadamente de 2 a 10), todos los casos que utilizan pesos de términos tienen un mejor rendimiento que MIl: a = 0, b = 0 sin pesos de términos, pero cuando el número de documentos aumenta, los casos WMIl: a = 1, b = 0 y WMIl: a = 2, b = 1 empeoran en comparación con MIl: a = 0, b = 0. Cuando el número de documentos se vuelve muy grande, son aún peores que los casos con números de documentos pequeños. Esto significa que es muy importante encontrar una forma adecuada de estimar los pesos de los términos para el criterio de WMI. La Figura 4 muestra los pesos de los términos aprendidos de todo el conjunto de entrenamiento. Cuatro tipos de palabras se categorizan de manera aproximada aunque la transición entre ellas es sutil. La Figura 5 ilustra el cambio en la información mutua (ponderada) para MIl y WMIl. Como era de esperar, la información mutua para MIl aumenta de forma monótona con el número de pasos, mientras que WMIl no lo hace. Finalmente, MIl y WMIl son escalables, con complejidad computacional mostrada en la Figura 6. Una ventaja de nuestro enfoque basado en MI es que no es necesario eliminar las palabras vacías. Otra ventaja importante es que no hay hiperparámetros necesarios que ajustar. En la segmentación de un solo documento, el rendimiento basado en MI es aún mejor que el basado en WMI, por lo que no se requiere un hiperparámetro adicional. En la segmentación de múltiples documentos, mostramos en el experimento que a = 1 y b = 1 es lo mejor. Nuestro método otorga más peso a los términos de señal. Sin embargo, generalmente los términos o frases de señal aparecen al principio de un segmento, mientras que el final del segmento puede ser 1 2 5 10 20 34 51 102 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 Número de Documento Tasa de Error MIl :a=0,b=0 WMI l :a=1,b=1 WMI l :a=1,b=0 WMI l :a=2,b=1 Figura 3: Tasas de error para diferentes hiperparámetros de pesos de términos. 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Entropía de Documento Normalizada Entropía de Segmento Normalizada Palabras ruidosas Palabras de señal Palabras de parada comunes Palabras de parada de Doc-dep Figura 4: Pesos de términos aprendidos de todo el conjunto de entrenamiento. 0 100 200 300 400 500 600 0.06 0.08 0.1 0.12 0.14 0.16 0.18 Número de Pasos (Ponderado) Información Mutua MI l WMI l Figura 5: Cambio en la MI (ponderada) para MIl y WMIl. 0 20 40 60 80 100 120 0 200 400 600 800 1000 1200 1400 1600 1800 2000 Número de Documento Tiempo de Convergencia (seg) MI l WMI l Figura 6: Tiempo de convergencia para MIl y WMIl. muy ruidoso. Una posible solución es dar más peso a los términos al principio de cada segmento. Además, cuando la longitud de los segmentos es bastante diferente, los segmentos largos tienen frecuencias de términos mucho más altas, por lo que pueden dominar los límites de segmentación. La normalización de las frecuencias de términos en relación con la longitud del segmento puede ser útil. 6. CONCLUSIONES Y TRABAJO FUTURO Propusimos un método novedoso para la segmentación y alineación de temas en múltiples documentos basado en información mutua ponderada, que también puede manejar casos de un solo documento. Utilizamos programación dinámica para optimizar nuestro algoritmo. Nuestro enfoque supera a todos los métodos anteriores en casos de un solo documento. Además, también demostramos que realizar segmentación entre varios documentos puede mejorar el rendimiento enormemente. Nuestros resultados también demostraron que el uso de la información mutua ponderada puede aprovechar la información de varios documentos para lograr un mejor rendimiento. Solo probamos nuestro método en conjuntos de datos limitados. Se deben probar más conjuntos de datos, especialmente los complicados. Deberían compararse más métodos anteriores. Además, las segmentaciones naturales como los párrafos son pistas que se pueden utilizar para encontrar los límites óptimos. El aprendizaje supervisado también puede ser considerado. 7. AGRADECIMIENTOS Los autores desean agradecer a Xiang Ji y al Prof. J. Scott Payne por su ayuda. 8. REFERENCIAS [1] A. Banerjee, I. Ghillon, J. Ghosh, S. Merugu y D. Modha. Un enfoque generalizado de entropía máxima para la coagrupación de Bregman y la aproximación de matrices. En Actas de SIGKDD, 2004. [2] R. Bekkerman, R. El-Yaniv y A. McCallum. Agrupación distribucional de múltiples vías a través de interacciones por pares. En Actas de ICML, 2005. [3] D. M. Blei y P. J. Moreno. Segmentación de temas con un modelo oculto de Markov de aspectos. En Actas de SIGIR, 2001. [4] D. M. Blei, A. Ng y M. Jordan. Asignación latente de Dirichlet. Revista de Investigación en Aprendizaje Automático, 3:993-1022, 2003. [5] T. Brants, F. Chen e I. Tsochantaridis. Segmentación de documentos basada en temas con análisis semántico latente probabilístico. En Actas de CIKM, 2002. [6] F. Choi. Avances en la segmentación lineal de texto independiente del dominio. En Actas de la NAACL, 2000. [7] H. Christensen, B. Kolluru, Y. Gotoh y S. Renals. Segmentación de máxima entropía de noticias de transmisión. En Actas de ICASSP, 2005. [8] T. Cover y J. Thomas. Elementos de la teoría de la información. John Wiley and Sons, Nueva York, EE. UU., 1991. [9] S. Deerwester, S. Dumais, G. Furnas, T. Landauer y R. Harshman. Indexación mediante análisis semántico latente. Revista de la Sociedad Americana de Sistemas de Información, 1990. [10] I. Dhillon, S. Mallela y D. Modha. Co-clustering informativo-teórico. En Actas de SIGKDD, 2003. [11] M. Hajime, H. Takeo y O. Manabu. Segmentación de texto con múltiples señales lingüísticas superficiales. En Actas de COLING-ACL, 1998. [12] T. K. Ho. Detención de palabras y identificación para el reconocimiento de texto adaptativo. Revista Internacional de Análisis y Reconocimiento de Documentos, 3(1), agosto de 2000. [13] T. Hofmann. Análisis semántico latente probabilístico. En Actas de la UAI99, 1999. [14] X. Ji y H. Zha. Correlación de la sumarización de un par de documentos multilingües. En Actas de RIDE, 2003. [15] X. Ji y H. Zha. Segmentación de texto independiente del dominio utilizando difusión anisotrópica y programación dinámica. En Actas de SIGIR, 2003. [16] X. Ji y H. Zha. Extrayendo temas compartidos de múltiples documentos. En Actas de la 7ª PAKDD, 2003. [17] J. Lafferty, A. McCallum y F. Pereira. Campos aleatorios condicionales: Modelos probabilísticos para segmentar y etiquetar datos de secuencias. En Actas de ICML, 2001. [18] T. Li, S. Ma y M. Ogihara. Criterio basado en la entropía en la agrupación categórica. En Actas de ICML, 2004. [19] A. McCallum, D. Freitag y F. Pereira. Modelos de máxima entropía de Markov para extracción de información y segmentación. En Actas de ICML, 2000. [20] L. Pevzner y M. Hearst. Una crítica y mejora de una métrica de evaluación para la segmentación de texto. Lingüística Computacional, 28(1):19-36, 2002. [21] J. C. Reynar. Modelos estadísticos para la segmentación de temas. En Actas de ACL, 1999. [22] G. Salton y M. McGill. Introducción a la Recuperación de Información Moderna. McGraw Hill, 1983. [23] B. \n\nMcGraw Hill, 1983. [23] B. Sun, Q. Tan, P. Mitra y C. L. Giles. Extracción y búsqueda de fórmulas químicas en documentos de texto en la web. En Actas de WWW, 2007. [24] B. Sun, D. Zhou, H. Zha, y J. I'm sorry, but \"Yen\" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish? Segmentación y alineación de texto multitarea basada en información mutua ponderada. En Actas de CIKM, 2006. [25] M. Utiyama y H. Isahara. Un modelo estadístico para la segmentación de texto independiente del dominio. En Actas de la 39ª ACL, 1999. [26] C. Wayne. Detección y seguimiento de temas multilingües: Investigación exitosa habilitada por corpora y evaluación. En Actas de LREC, 2000. [27] J. Yamron, I. Carp, L. Gillick, S. Lowe y P. van Mulbregt. Un enfoque de modelo oculto de Markov para la segmentación de texto y seguimiento de eventos. En Actas de ICASSP, 1998. [28] H. Zha y X. Ji. Correlacionando documentos multilingües a través de modelado de gráficos bipartitos. En Actas de SIGIR, 2002. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "document local and sequential information": {
            "translated_key": "documentar información local y secuencial",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Topic Segmentation with Shared Topic Detection and Alignment of Multiple Documents Bingjun Sun*, Prasenjit Mitra*† , Hongyuan Zha‡ , C. Lee Giles*† , John Yen*† *Department of Computer Science and Engineering † College of Information Sciences and Technology The Pennsylvania State University University Park, PA 16802 ‡ College of Computing The Georgia Institute of Technology Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu ABSTRACT Topic detection and tracking [26] and topic segmentation [15] play an important role in capturing the local and sequential information of documents.",
                "Previous work in this area usually focuses on single documents, although similar multiple documents are available in many domains.",
                "In this paper, we introduce a novel unsupervised method for shared topic detection and topic segmentation of multiple similar documents based on mutual information (MI) and weighted mutual information (WMI) that is a combination of MI and term weights.",
                "The basic idea is that the optimal segmentation maximizes MI(or WMI).",
                "Our approach can detect shared topics among documents.",
                "It can find the optimal boundaries in a document, and align segments among documents at the same time.",
                "It also can handle single-document segmentation as a special case of the multi-document segmentation and alignment.",
                "Our methods can identify and strengthen cue terms that can be used for segmentation and partially remove stop words by using term weights based on entropy learned from multiple documents.",
                "Our experimental results show that our algorithm works well for the tasks of single-document segmentation, shared topic detection, and multi-document segmentation.",
                "Utilizing information from multiple documents can tremendously improve the performance of topic segmentation, and using WMI is even better than using MI for the multi-document segmentation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Clustering; H.3.1 [Information Storage and Retrieval]: Content Analysis and IndexingLinguistic processing; I.2.7 [Artificial Intelligence]: Natural Language Processing-Text analysis; I.5.3 [Pattern Recognition]: Clustering-Algorithms;Similarity measures General Terms Algorithms, Design, Experimentation 1.",
                "INTRODUCTION Many researchers have worked on topic detection and tracking (TDT) [26] and topic segmentation during the past decade.",
                "Topic segmentation intends to identify the boundaries in a document with the goal to capture the latent topical structure.",
                "Topic segmentation tasks usually fall into two categories [15]: text stream segmentation where topic transition is identified, and coherent document segmentation in which documents are split into sub-topics.",
                "The former category has applications in automatic speech recognition, while the latter one has more applications such as partial-text query of long documents in information retrieval, text summary, and quality measurement of multiple documents.",
                "Previous research in connection with TDT falls into the former category, targeted on topic tracking of broadcast speech data and newswire text, while the latter category has not been studied very well.",
                "Traditional approaches perform topic segmentation on documents one at a time [15, 25, 6].",
                "Most of them perform badly in subtle tasks like coherent document segmentation [15].",
                "Often, end-users seek documents that have the similar content.",
                "Search engines, like, Google, provide links to obtain similar pages.",
                "At a finer granularity, users may actually be looking to obtain sections of a document similar to a particular section that presumably discusses a topic of the users interest.",
                "Thus, the extension of topic segmentation from single documents to identifying similar segments from multiple similar documents with the same topic is a natural and necessary direction, and multi-document topic segmentation is expected to have a better performance since more information is utilized.",
                "Traditional approaches using similarity measurement based on term frequency generally have the same assumption that similar vocabulary tends to be in a coherent topic segment [15, 25, 6].",
                "However, they usually suffer the issue of identifying stop words.",
                "For example, additional document-dependent stop words are removed together with the generic stop words in [15].",
                "There are two reasons that we do not remove stop words directly.",
                "First, identifying stop words is another issue [12] that requires estimation in each domain.",
                "Removing common stop words may result in the loss of useful information in a specific domain.",
                "Second, even though stop words can be identified, hard classification of stop words and nonstop words cannot represent the gradually changing amount of information content of each word.",
                "We employ a soft classification using term weights.",
                "In this paper, we view the problem of topic segmentation as an optimization issue using information theoretic techniques to find the optimal boundaries of a document given the number of text segments so as to minimize the loss of mutual information (MI) (or a weighted mutual information (WMI)) after segmentation and alignment.",
                "This is equal to maximizing the MI (or WMI).",
                "The MI focuses on measuring the difference among segments whereas previous research focused on finding the similarity (e.g. cosine distance) of segments [15, 25, 6].",
                "Topic alignment of multiple similar documents can be achieved by clustering sentences on the same topic into the same cluster.",
                "Single-document topic segmentation is just a special case of the multi-document topic segmentation and alignment problem.",
                "Terms can be co-clustered as in [10] at the same time, given the number of clusters, but our experimental results show that this method results in a worse segmentation (see Tables 1, 4, and 6).",
                "Usually, human readers can identify topic transition based on cue words, and can ignore stop words.",
                "Inspired by this, we give each term (or term cluster) a weight based on entropy among different documents and different segments of documents.",
                "Not only can this approach increase the contribution of cue words, but it can also decrease the effect of common stop words, noisy word, and document-dependent stop words.",
                "These words are common in a document.",
                "Many methods based on sentence similarity require that these words are removed before topic segmentation can be performed [15].",
                "Our results in Figure 3 show that term weights are useful for multi-document topic segmentation and alignment.",
                "The major contribution of this paper is that it introduces a novel method for topic segmentation using MI and shows that this method performs better than previously used criteria.",
                "Also, we have addressed the problem of topic segmentation and alignment across multiple documents, whereas most existing research focused on segmentation of single documents.",
                "Multi-document segmentation and alignment can utilize information from similar documents and improves the performance of topic segmentation greatly.",
                "Obviously, our approach can handle single documents as a special case when multiple documents are unavailable.",
                "It can detect shared topics among documents to judge if they are multiple documents on the same topic.",
                "We also introduce the new criterion of WMI based on term weights learned from multiple similar documents, which can improve performance of topic segmentation further.",
                "We propose an iterative greedy algorithm based on dynamic programming and show that it works well in practice.",
                "Some of our prior work is in [24].",
                "The rest of this paper is organized as follows: In Section 2, we review related work.",
                "Section 3 contains a formulation of the problem of topic segmentation and alignment of multiple documents with term co-clustering, a review of the criterion of MI for clustering, and finally an introduction to WMI.",
                "In Section 4, we first propose the iterative greedy algorithm of topic segmentation and alignment with term co-clustering, and then describe how the algorithm can be optimized by usFigure 1: Illustration of multi-document segmentation and alignment. ing dynamic programming.",
                "In Section 5, experiments about single-document segmentation, shared topic detection, and multi-document segmentation are described, and results are presented and discussed to evaluate the performance of our algorithm.",
                "Conclusions and some future directions of the research work are discussed in Section 6. 2.",
                "PREVIOUS WORK Generally, the existing approaches to text segmentation fall into two categories: supervised learning [19, 17, 23] and unsupervised learning [3, 27, 5, 6, 15, 25, 21].",
                "Supervised learning usually has good performance, since it learns functions from labelled training sets.",
                "However, often getting large training sets with manual labels on document sentences is prohibitively expensive, so unsupervised approaches are desired.",
                "Some models consider dependence between sentences and sections, such as Hidden Markov Model [3, 27], Maximum Entropy Markov Model [19], and Conditional Random Fields [17], while many other approaches are based on lexical cohesion or similarity of sentences [5, 6, 15, 25, 21].",
                "Some approaches also focus on cue words as hints of topic transitions [11].",
                "While some existing methods only consider information in single documents [6, 15], others utilize multiple documents [16, 14].",
                "There are not many works in the latter category, even though the performance of segmentation is expected to be better with utilization of information from multiple documents.",
                "Previous research studied methods to find shared topics [16] and topic segmentation and summarization between just a pair of documents [14].",
                "Text classification and clustering is a related research area which categorizes documents into groups using supervised or unsupervised methods.",
                "Topical classification or clustering is an important direction in this area, especially co-clustering of documents and terms, such as LSA [9], PLSA [13], and approaches based on distances and bipartite graph partitioning [28] or maximum MI [2, 10], or maximum entropy [1, 18].",
                "Criteria of these approaches can be utilized in the issue of topic segmentation.",
                "Some of those methods have been extended into the area of topic segmentation, such as PLSA [5] and maximum entropy [7], but to our best knowledge, using MI for topic segmentation has not been studied. 3.",
                "PROBLEM FORMULATION Our goal is to segment documents and align the segments across documents (Figure 1).",
                "Let T be the set of terms {t1, t2, ..., tl}, which appear in the unlabelled set of documents D = {d1, d2, ..., dm}.",
                "Let Sd be the set of sentences for document d ∈ D, i.e. {s1, s2, ..., snd }.",
                "We have a 3D matrix of term frequency, in which the three dimensions are random variables of D, Sd, and T. Sd actually is a random vector including a random variable for each d ∈ D. The term frequency can be used to estimate the joint probability distribution P(D, Sd, T), which is p(t, d, s) = T(t, d, s)/ND, where T(t, d, s) is the number of t in ds sentence s and ND is the total number of terms in D. ˆS represents the set of segments {ˆs1, ˆs2, ..., ˆsp} after segmentation and alignment among multiple documents, where the number of segments | ˆS| = p. A segment ˆsi of document d is a sequence of adjacent sentences in d. Since for different documents si may discuss different sub-topics, our goal is to cluster adjacent sentences in each document into segments, and align similar segments among documents, so that for different documents ˆsi is about the same sub-topic.",
                "The goal is to find the optimal topic segmentation and alignment mapping Segd(si) : {s1, s2, ..., snd } → {ˆs1, ˆs2, ..., ˆsp} and Alid(ˆsi) : {ˆs1, ˆs2, ..., ˆsp} → {ˆs1, ˆs2, ..., ˆsp}, for all d ∈ D, where ˆsi is ith segment with the constraint that only adjacent sentences can be mapped to the same segment, i.e. for d, {si, si+1, ..., sj} → {ˆsq}, where q ∈ {1, ..., p}, where p is the segment number, and if i > j, then for d, ˆsq is missing.",
                "After segmentation and alignment, random vector Sd becomes an aligned random variable ˆS.",
                "Thus, P(D, Sd, T) becomes P(D, ˆS, T).",
                "Term co-clustering is a technique that has been employed [10] to improve the accuracy of document clustering.",
                "We evaluate the effect of it for topic segmentation.",
                "A term t is mapped to exactly one term cluster.",
                "Term co-clustering involves simultaneously finding the optimal term clustering mapping Clu(t) : {t1, t2, ..., tl} → {ˆt1, ˆt2, ..., ˆtk}, where k ≤ l, l is the total number of words in all the documents, and k is the number of clusters. 4.",
                "METHODOLOGY We now describe a novel algorithm which can handle singledocument segmentation, shared topic detection, and multidocument segmentation and alignment based on MI or WMI. 4.1 Mutual Information MI I(X; Y ) is a quantity to measure the amount of information which is contained in two or more random variables [8, 10].",
                "For the case of two random variables, we have I(X; Y ) = x∈X y∈Y p(x, y)log p(x, y) p(x)p(y) , (1) Obviously, when random variables X and Y are independent, I(X; Y ) = 0.",
                "Thus, intuitively, the value of MI depends on how random variables are dependent on each other.",
                "The optimal co-clustering is the mapping Clux : X → ˆX and Cluy : Y → ˆY that minimizes the loss: I(X; Y ) − I( ˆX; ˆY ), which is equal to maximizing I( ˆX; ˆY ).",
                "This is the criterion of MI for clustering.",
                "In the case of topic segmentation, the two random variables are the term variable T and the segment variable S, and each sample is an occurrence of a term T = t in a particular segment S = s. I(T; S) is used to measure how dependent T and S are.",
                "However, I(T; S) cannot be computed for documents before segmentation, since we do not have a set of S due to the fact that sentences of Document d, si ∈ Sd, is not aligned with other documents.",
                "Thus, instead of minimizing the loss of MI, we can maximize MI after topic segmentation, computed as: I( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS p(ˆt, ˆs)log p(ˆt, ˆs) p(ˆt)p(ˆs) , (2) where p(ˆt, ˆs) are estimated by the term frequency tf of Term Cluster ˆt and Segment ˆs in the training set D. Note that here a segment ˆs includes sentences about the the same topic among all documents.",
                "The optimal solution is the mapping Clut : T → ˆT, Segd : Sd → ˆS , and Alid : ˆS → ˆS, which maximizes I( ˆT; ˆS). 4.2 Weighted Mutual Information In topic segmentation and alignment of multiple documents, if P(D, ˆS, T) is known, based on the marginal distributions P(D|T) and P( ˆS|T) for each term t ∈ T, we can categorize terms into four types in the data set: • Common stop words are common both along the dimensions of documents and segments. • Document-dependent stop words that depends on the personal writing style are common only along the dimension of segments for some documents. • Cue words are the most important elements for segmentation.",
                "They are common along the dimension of documents only for the same segment, and they are not common along the dimensions of segments. • Noisy words are other words which are not common along both dimensions.",
                "Entropy based on P(D|T) and P( ˆS|T) can be used to identify different types of terms.",
                "To reinforce the contribution of cue words in the MI computation, and simultaneously reduce the effect of the other three types of words, similar as the idea of the tf-idf weight [22], we use entropies of each term along the dimensions of document D and segment ˆS, i.e.",
                "ED(ˆt) and EˆS(ˆt), to compute the weight.",
                "A cue word usually has a large value of ED(ˆt) but a small value of EˆS(ˆt).",
                "We introduce term weights (or term cluster weights) wˆt = ( ED(ˆt) maxˆt ∈ ˆT (ED(ˆt )) )a (1 − EˆS(ˆt) maxˆt ∈ ˆT (EˆS(ˆt )) )b , (3) where ED(ˆt) = d∈D p(d|ˆt)log|D| 1 p(d|ˆt) , EˆS(ˆt) = ˆs∈ ˆS p(ˆs|ˆt)log| ˆS| 1 p(ˆs|ˆt) , and a > 0 and b > 0 are powers to adjust term weights.",
                "Usually a = 1 and b = 1 as default, and maxˆt ∈ ˆT (ED(ˆt )) and maxˆt ∈ ˆT (EˆS(ˆt )) are used to normalize the entropy values.",
                "Term cluster weights are used to adjust p(ˆt, ˆs), pw(ˆt, ˆs) = wˆtp(ˆt, ˆs) ˆt∈ ˆT ;ˆs∈ ˆS wˆtp(ˆt, ˆs) , (4) and Iw( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS pw(ˆt, ˆs)log pw(ˆt, ˆs) pw(ˆt)pw(ˆs) , (5) where pw(ˆt) and pw(ˆs) are marginal distributions of pw(ˆt, ˆs).",
                "However, since we do not know either the term weights or P(D, ˆS, T), we need to estimate them, but wˆt depends on p(ˆs|t) and ˆS, while ˆS and p(ˆs|t) also depend on wˆt that is still unknown.",
                "Thus, an iterative algorithm is required to estimate term weights wˆt and find the best segmentation and alignment to optimize the objective function Iw concurrently.",
                "After a document is segmented into sentences Input: Joint probability distribution P(D, Sd, T), number of text segments p ∈ {2, 3, ..., max(sd)}, number of term clusters k ∈ {2, 3, ..., l} (if k = l, no term co-clustering required), and weight type w ∈ {0, 1}, indicating to use I or Iw, respectively.",
                "Output: Mapping Clu, Seg, Ali, and term weights wˆt.",
                "Initialization: 0. i = 0.",
                "Initialize Clu (0) t , Seg (0) d , and Ali (0) d ; Initialize w (0) ˆt using Equation (6) if w = 1; Stage 1: 1.",
                "If |D| = 1, k = l, and w = 0, check all sequential segmentations of d into p segments and find the best one Segd(s) = argmaxˆsI( ˆT; ˆS), and return Segd; otherwise, if w = 1 and k = l, go to 3.1; Stage 2: 2.1 If k < l, for each term t, find the best cluster ˆt as Clu(i+1)(t) = argmaxˆtI( ˆT; ˆS(i)) based on Seg(i) and Ali(i); 2.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+1) d (Seg (i+1) d (s)) = argmaxˆsI( ˆT(i+1); ˆS) based on Clu(i+1)(t) if k < l or Clu(0)(t) if k = l; 2.3 i + +.",
                "If Clu, Seg, or Ali changed, go to 2.1; otherwise, if w = 0, return Clu(i), Seg(i), and Ali(i); else j = 0, go to 3.1; Stage 3: 3.1 Update w (i+j+1) ˆt based on Seg(i+j), Ali(i+j), and Clu(i) using Equation (3); 3.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+j+1) d (Seg (i+j+1) d (s)) = argmaxˆsIw( ˆT(i); ˆS) based on Clu(i) and w (i+j+1) ˆt ; 3.3 j + +.",
                "If Iw( ˆT; ˆS) changes, go to step 6; otherwise, stop and return Clu(i), Seg(i+j), Ali(i+j), and w (i+j) ˆt ; Figure 2: Algorithm: Topic segmentation and alignment based on MI or WMI. and each sentence is segmented into words, each word is stemmed.",
                "Then the joint probability distribution P(D, Sd, T) can be estimated.",
                "Finally, this distribution can be used to compute MI in our algorithm. 4.3 Iterative Greedy Algorithm Our goal is to maximize the objective function, I( ˆT; ˆS) or Iw( ˆT; ˆS), which can measure the dependence of term occurrences in different segments.",
                "Generally, first we do not know the estimate term weights, which depend on the optimal topic segmentation and alignment, and term clusters.",
                "Moreover, this problem is NP-hard [10], even though if we know the term weights.",
                "Thus, an iterative greedy algorithm is desired to find the best solution, even though probably only local maxima are reached.",
                "We present the iterative greedy algorithm in Figure 2 to find a local maximum of I( ˆT; ˆS) or Iw( ˆT; ˆS) with simultaneous term weight estimation.",
                "This algorithm can is iterative and greedy for multi-document cases or single-document cases with term weight estimation and/or term co-clustering.",
                "Otherwise, since it is just a one step algorithm to solve the task of single-document segmentation [6, 15, 25], the global maximum of MI is guaranteed.",
                "We will show later that term co-clustering reduces the accuracy of the results and is not necessary, and for singledocument segmentation, term weights are also not required. 4.3.1 Initialization In Step 0, the initial term clustering Clut and topic segmentation and alignment Segd and Alid are important to avoid local maxima and reduce the number of iterations.",
                "First, a good guess of term weights can be made by using the distributions of term frequency along sentences for each document and averaging them to get the initial values of wˆt: wt = ( ED(t) maxt ∈T (ED(t )) )(1 − ES(t) maxt ∈T (ES(t )) ), (6) where ES(t) = 1 |Dt| d∈Dt (1 − s∈Sd p(s|t)log|Sd| 1 p(s|t) ), where Dt is the set of documents which contain Term t. Then, for the initial segmentation Seg(0) , we can simply segment documents equally by sentences.",
                "Or we can find the optimal segmentation just for each document d which maximizes the WMI, Seg (0) d = argmaxˆsIw(T; ˆS), where w = w (0) ˆt .",
                "For the initial alignment Ali(0) , we can first assume that the order of segments for each d is the same.",
                "For the initial term clustering Clu(0) , first cluster labels can be set randomly, and after the first time of Step 3, a good initial term clustering is obtained. 4.3.2 Different Cases After initialization, there are three stages for different cases.",
                "Totally there are eight cases, |D| = 1 or |D| > 1, k = l or k < l, w = 0 or w = 1.",
                "Single document segmentation without term clustering and term weight estimation (|D| = 1, k = l, w = 0) only requires Stage 1 (Step 1).",
                "If term clustering is required (k < l), Stage 2 (Step 2.1, 2.2, and 2.3) is executed iteratively.",
                "If term weight estimation is required (w = 1), Stage 3 (Step 3.1, 3.2, and 3.3) is executed iteratively.",
                "If both are required (k < l, w = 1), Stage 2 and 3 run one after the other.",
                "For multi-document segmentation without term clustering and term weight estimation (|D| > 1, k = l, w = 0), only iteration of Step 2.2 and 2.3 are required.",
                "At Stage 1, the global maximum can be found based on I( ˆT; ˆS) using dynamic programming in Section 4.4.",
                "Simultaneously finding a good term clustering and estimated term weights is impossible, since when moving a term to a new term cluster to maximize Iw( ˆT; ˆS), we do not know that the weight of this term should be the one of the new cluster or the old cluster.",
                "Thus, we first do term clustering at Stage 2, and then estimate term weights at Stage 3.",
                "At Stage 2, Step 2.1 is to find the best term clustering and Step 2.2 is to find the best segmentation.",
                "This cycle is repeated to find a local maximum based on MI I until it converges.",
                "The two steps are: (1) based on current term clustering Cluˆt, for each document d, the algorithm segments all the sentences Sd into p segments sequentially (some segments may be empty), and put them into the p segments ˆS of the whole training set D (all possible cases of different segmentation Segd and alignment Alid are checked) to find the optimal case, and (2) based on the current segmentation and alignment, for each term t, the algorithm finds the best term cluster of t based on the current segmentation Segd and alignment Alid.",
                "After finding a good term clustering, term weights are estimated if w = 1.",
                "At Stage 3, similar as Stage 2, Step 3.1 is term weight re-estimation and Step 3.2 is to find a better segmentation.",
                "They are repeated to find a local maximum based on WMI Iw until it converges.",
                "However, if the term clustering in Stage 2 is not accurate, then the term weight estimation at Stage 3 may have a bad result.",
                "Finally, at Step 3.3, this algorithm converges and return the output.",
                "This algorithm can handle both single-document and multi-document segmentation.",
                "It also can detect shared topics among documents by checking the proportion of overlapped sentences on the same topics, as described in Sec 5.2. 4.4 Algorithm Optimization In many previous works on segmentation, dynamic programming is a technique used to maximize the objective function.",
                "Similarly, at Step 1, 2.2, and 3.2 of our algorithm, we can use dynamic programming.",
                "For Stage 1, using dynamic programming can still find the global optimum, but for Stage 2 and Stage 3, we can only find the optimum for each step of topic segmentation and alignment of a document.",
                "Here we only show the dynamic programming for Step 3.2 using WMI (Step 1 and 2.2 are similar but they can use either I or Iw).",
                "There are two cases that are not shown in the algorithm in Figure 2: (a) single-document segmentation or multi-document segmentation with the same sequential order of segments, where alignment is not required, and (b) multi-document segmentation with different sequential orders of segments, where alignment is necessary.",
                "The alignment mapping function of the former case is simply just Alid(ˆsi) = ˆsi, while for the latter ones alignment mapping function Alid(ˆsi) = ˆsj, i and j may be different.",
                "The computational steps for the two cases are listed below: Case 1 (no alignment): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs) and partial pw(ˆs) without counting sentences from d. Then put sentences from i to j into Part k, and compute partial WMI PIw( ˆT; ˆsk(si, si+1, ..., sj)) ˆt∈ ˆT pw(ˆt, ˆsk)log pw(ˆt, ˆsk) pw(ˆt)pw(ˆsk) , where Alid(si, si+1, ..., sj) = k, k ∈ {1, 2, ..., p}, 1 ≤ i ≤ j ≤ nd, and Segd(sq) = ˆsk for all i ≤ q ≤ j. (2) Let M(sm, 1) = PIw( ˆT; ˆs1(s1, s2, ..., sm)).",
                "Then M(sm, L) = maxi[M(si−1, L − 1) + PIw( ˆT; ˆsL(si, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, and when i > m, no sentences are put into ˆsk when compute PIw (note PIw( ˆT; ˆs(si, ..., sm)) = 0 for single-document segmentation). (3) Finally M(snd , p) = maxi[M(si−1, p − 1)+ PIw( ˆT; ˆsp(si, ..., snd ))], where 1 ≤ i ≤ nd+1.",
                "The optimal Iw is found and the corresponding segmentation is the best.",
                "Case 2 (alignment required): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs), and partial pw(ˆs), and PIw( ˆT; ˆsk(si, si+1, ..., sj)) similarly as Case 1. (2) Let M(sm, 1, k) = PIw( ˆT; ˆsk(s1, s2, ..., sm)), where k ∈ {1, 2, ..., p}.",
                "Then M(sm, L, kL) = maxi,j[M(si−1, L − 1, kL/j) + PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, kL ∈ Set(p, L), which is the set of all p!",
                "L!(p−L)! combinations of L segments chosen from all p segments, j ∈ kL, the set of L segments chosen from all p segments, and kL/j is the combination of L − 1 segments in kL except Segment j. (3) Finally, M(snd , p, kp) = maxi,j[M(si−1, p − 1, kp/j) +PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., snd ))], where kp is just the combination of all p segments and 1 ≤ i ≤ nd + 1, which is the optimal Iw and the corresponding segmentation is the best.",
                "The steps of Case 1 and 2 are similar, except in Case 2, alignment is considered in addition to segmentation.",
                "First, basic items of probability for computing Iw are computed excluding Doc d, and then partial WMI by putting every possible sequential segment (including empty segment) of d into every segment of the set.",
                "Second, the optimal sum of PIw for L segments and the leftmost m sentences, M(sm, L), is found.",
                "Finally, the maximal WMI is found among different sums of M(sm, p − 1) and PIw for Segment p. 5.",
                "EXPERIMENTS In this section, single-document segmentation, shared topic detection, and multi-document segmentation will be tested.",
                "Different hyper parameters of our method are studied.",
                "For convenience, we refer to the method using I as MIk if w = 0, and Iw as WMIk if w = 2 or as WMIk if w = 1, where k is the number of term clusters, and if k = l, where l is the total number of terms, then no term clustering is required, i.e.",
                "MIl and WMIl. 5.1 Single-document Segmentation 5.1.1 Test Data and Evaluation The first data set we tested is a synthetic one used in previous research [6, 15, 25] and many other papers.",
                "It has 700 samples.",
                "Each is a concatenation of ten segments.",
                "Each segment is the first n sentence selected randomly from the Brown corpus, which is supposed to have a different topic from each other.",
                "Currently, the best results on this data set is achieved by Ji et.al. [15].",
                "To compare the performance of our methods, the criterion used widely in previous research is applied, instead of the unbiased criterion introduced in [20].",
                "It chooses a pair of words randomly.",
                "If they are in different segments (different) for the real segmentation (real), but predicted (pred) as in the same segment, it is a miss.",
                "If they are in the same segment (same), but predicted as in different segments, it is a false alarm.",
                "Thus, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, diff)p(diff|real) +p(false alarm|real, pred, same)p(same|real). 5.1.2 Experiment Results We tested the case when the number of segments is known.",
                "Table 1 shows the results of our methods with different hyper parameter values and three previous approaches, C99[25], U00[6], and ADDP03[15], on this data set when the segment number is known.",
                "In WMI for single-document segmentation, the term weights are computed as follows: wˆt = 1−EˆS(ˆt)/maxˆt ∈ ˆT (EˆS(ˆt )).",
                "For this case, our methods MIl and WMIl both outperform all the previous approaches.",
                "We compared our methods with ADDP03using one-sample one-sided t-test and p-values are shown in Table 2.",
                "From the p-values, we can see that mostly the differences are very Table 1: Average Error Rates of Single-document Segmentation Given Segment Numbers Known Range of n 3-11 3-5 6-8 9-11 Sample size 400 100 100 100 C99 12% 11% 10% 9% U00 10% 9% 7% 5% ADDP03 6.0% 6.8% 5.2% 4.3% MIl 4.68% 5.57% 2.59% 1.59% WMIl 4.94% 6.33% 2.76% 1.62% MI100 9.62% 12.92% 8.66% 6.67% Table 2: Single-document Segmentation: P-values of T-test on Error Rates Range of n 3-11 3-5 6-8 9-11 ADDP03, MIl 0.000 0.000 0.000 0.000 ADDP03, WMIl 0.000 0.099 0.000 0.000 MIl, WMIl 0.061 0.132 0.526 0.898 significant.",
                "We also compare the error rates between our two methods using two-sample two-sided t-test to check the hypothesis that they are equal.",
                "We cannot reject the hypothesis that they are equal, so the difference are not significant, even though all the error rates for MIl are smaller than WMIl.",
                "However, we can conclude that term weights contribute little in single-document segmentation.",
                "The results also show that MI using term co-clustering (k = 100) decreases the performance.",
                "We tested different number of term clusters, and found that the performance becomes better when the cluster number increases to reach l. WMIk<l has similar results that we did not show in the table.",
                "As mentioned before, using MI may be inconsistent on optimal boundaries given different numbers of segments.",
                "This situation occurs especially when the similarities among segments are quite different, i.e. some transitions are very obvious, while others are not.",
                "This is because usually a document is a hierarchical structure instead of only a sequential structure.",
                "When the segments are not at the same level, this situation may occur.",
                "Thus, a hierarchical topic segmentation approach is desired, and the structure highly depends on the number of segments for each internal node and the stop criteria of splitting.",
                "For this data set of singledocument segmentation, since it is just a synthetic set, which is just a concatenation of several segments about different topics, it is reasonable that approaches simply based on term frequency have a good performance.",
                "Usually for the tasks of segmenting coherent documents for sub-topics, the effectiveness decreases much. 5.2 Shared Topic Detection 5.2.1 Test Data and Evaluation The second data set contains 80 news articles from Google News.",
                "There are eight topics and each has 10 articles.",
                "We randomly split the set into subsets with different document numbers and each subset has all eight topics.",
                "We compare our approach MIl and WMIl with LDA [4].",
                "LDA treats a document in the data set as a bag of words, finds its distribution on topics, and its major topic.",
                "MIl and WMIl views each sentence as a bag of words and tag it with a topic label.",
                "Then for each pair of documents, LDA determines if they are on the same topic, while MIl and Table 3: Shared Topic Detection: Average Error Rates for Different Numbers of Documents in Each Subset #Doc 10 20 40 80 LDA 8.89% 16.33% 1.35% 0.60% MIl, θ = 0.6 4.17% 1.71% 1.47% 0.0% WMIl, θ = 0.8 18.6% 3.16% 1.92% 0.0% WMIl check whether the proportion overlapped sentences on the same topic is larger than the adjustable threshold θ.",
                "That is, in MIl and WMIl, for a pair of documents d, d , if [ s∈Sd,s ∈Sd 1(topics=topics )/min(|Sd|, |Sd|)] > θ, where Sd is the set of sentences of d, and |Sd| is the number of sentences of d, then d and d have the shared topic.",
                "For a pair of documents selected randomly, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, same)p(same|real) +p(false alarm|real, pred, diff)p(diff|real), where a miss means if they have the same topic (same) for the real case (real), but predicted (pred) as on the same topic.",
                "If they are on different topics (diff), but predicted as on the same topic, it is a false alarm. 5.2.2 Experiment Results The results are shown in Table 3.",
                "If most documents have different topics, in WMIl, the estimation of term weights in Equation (3) is not correct.",
                "Thus, WMIl is not expected to have a better performance than MIl, when most documents have different topics.",
                "When there are fewer documents in a subset with the same number of topics, more documents have different topics, so WMIl is more worse than MIl.",
                "We can see that for most cases MIl has a better (or at least similar) performance than LDA.",
                "After shared topic detection, multi-document segmentation of documents with the shared topics is able to be executed. 5.3 Multi-document Segmentation 5.3.1 Test Data and Evaluation For multi-document segmentation and alignment, our goal is to identify these segments about the same topic among multiple similar documents with shared topics.",
                "Using Iw is expected to perform better than I, since without term weights the result is affected seriously by document-dependent stop words and noisy words which depends on the personal writing style.",
                "It is more likely to treat the same segments of different documents as different segments under the effect of document-dependent stop words and noisy words.",
                "Term weights can reduce the effect of document-dependent stop words and noisy words by giving cue terms more weights.",
                "The data set for multi-document segmentation and alignment has 102 samples and 2264 sentences totally.",
                "Each is the introduction part of a lab report selected from the course of Biol 240W, Pennsylvania State University.",
                "Each sample has two segments, introduction of plant hormones and the content in the lab.",
                "The length range of samples is from two to 56 sentences.",
                "Some samples only have one part and some have a reverse order the these two segments.",
                "It is not hard to identify the boundary between two segments for human.",
                "We labelled each sentence manually for evaluation.",
                "The criterion of evaluation is just using the proportion of the number of sentences with wrong predicted segment labels in the total number of sentences in the whole training Table 4: Average Error Rates of Multi-document Segmentation Given Segment Numbers Known #Doc MIl WMIl k MIk WMIk 102 3.14% 2.78% 300 4.68% 6.58% 51 4.17% 3.63% 300 17.83% 22.84% 34 5.06% 4.12% 300 18.75% 20.95% 20 7.08% 5.42% 250 20.40% 21.83% 10 10.38% 7.89% 250 21.42% 21.91% 5 15.77% 11.64% 250 21.89% 22.59% 2 25.90% 23.18% 50 25.44% 25.49% 1 23.90% 24.82% 25 25.75% 26.15% Table 5: Multi-document Segmentation: P-values of T-test on Error Rates for MIl and WMIl #Doc 51 34 20 10 5 2 P-value 0.19 0.101 0.025 0.001 0.000 0.002 set as the error rate: p(error|predicted, real) = d∈D s∈Sd 1(predicteds=reals)/ d∈D nd.",
                "In order to show the benefits of multi-document segmentation and alignment, we compared our method with different parameters on different partitions of the same training set.",
                "Except the cases that the number of documents is 102 and one (they are special cases of using the whole set and the pure single-document segmentation), we randomly divided the training set into m partitions, and each has 51, 34, 20, 10, 5, and 2 document samples.",
                "Then we applied our methods on each partition and calculated the error rate of the whole training set.",
                "Each case was repeated for 10 times for computing the average error rates.",
                "For different partitions of the training set, different k values are used, since the number of terms increases when the document number in each partition increases. 5.3.2 Experiment Results From the experiment results in Table 4, we can see the following observations: (1) When the number of documents increases, all methods have better performances.",
                "Only from one to two documents, MIl has decrease a little.",
                "We can observe this from Figure 3 at the point of document number = 2.",
                "Most curves even have the worst results at this point.",
                "There are two reasons.",
                "First, because samples vote for the best multi-document segmentation and alignment, but if only two documents are compared with each other, the one with missing segments or a totally different sequence will affect the correct segmentation and alignment of the other.",
                "Second, as noted at the beginning of this section, if two documents have more document-dependent stop words or noisy words than cue words, then the algorithm may view them as two different segments and the other segment is missing.",
                "Generally, we can only expect a better performance when the number of documents is larger than the number of segments. (2) Except single-document segmentation, WMIl is always better than MIl, and when the number of documents is reaching one or increases to a very large number, their performances become closer.",
                "Table 5 shows p-values of twosample one-sided t-test between MIl and WMIl.",
                "We also can see this trend from p-values.",
                "When document number = 5, we reached the smallest p-value and the largest difference between error rates of MIl and WMIl.",
                "For single-document Table 6: Multi-document Segmentation: Average Error Rate for Document Number = 5 in Each Subset with Different Number of Term Clusters #Cluster 75 100 150 250 l MIk 24.67% 24.54% 23.91% 22.59% 15.77% segmentation, WMIl is even a little bit worse than MIl, which is similar as the results of the single-document segmentation on the first data set.",
                "The reason is that for singledocument segmentation, we cannot estimate term weights accurately, since multiple documents are unavailable. (3) Using term clustering usually gets worse results than MIl and WMIl. (4) Using term clustering in WMIk is even worse than in MIk, since in WMIk term clusters are found first using I before using Iw.",
                "If the term clusters are not correct, then the term weights are estimated worse, which may mislead the algorithm to reach even worse results.",
                "From the results we also found that in multi-document segmentation and alignment, most documents with missing segments and a reverse order are identified correctly.",
                "Table 6 illustrates the experiment results for the case of 20 partitions (each has five document samples) of the training set and topic segmentation and alignment using MIk with different numbers of term clusters k. Notice that when the number of term clusters increases, the error rate becomes smaller.",
                "Without term clustering, we have the best result.",
                "We did not show results for WMIk with term clustering, but the results are similar.",
                "We also tested WMIl with different hyper parameters of a and b to adjust term weights.",
                "The results are presented in Figure 3.",
                "It was shown that the default case WMIl : a = 1, b = 1 gave the best results for different partitions of the training set.",
                "We can see the trend that when the document number is very small or large, the difference between MIl : a = 0, b = 0 and WMIl : a = 1, b = 1 becomes quite small.",
                "When the document number is not large (about from 2 to 10), all the cases using term weights have better performances than MIl : a = 0, b = 0 without term weights, but when the document number becomes larger, the cases WMIl : a = 1, b = 0 and WMIl : a = 2, b = 1 become worse than MIl : a = 0, b = 0.",
                "When the document number becomes very large, they are even worse than cases with small document numbers.",
                "This means that a proper way to estimate term weights for the criterion of WMI is very important.",
                "Figure 4 shows the term weights learned from the whole training set.",
                "Four types of words are categorized roughly even though the transition among them are subtle.",
                "Figure 5 illustrates the change in (weighted) mutual information for MIl and WMIl.",
                "As expected, mutual information for MIl increases monotonically with the number of steps, while WMIl does not.",
                "Finally, MIl and WMIl are scalable, with computational complexity shown in Figure 6.",
                "One advantage for our approach based on MI is that removing stop words is not required.",
                "Another important advantage is that there are no necessary hyper parameters to adjust.",
                "In single-document segmentation, the performance based on MI is even better for that based on WMI, so no extra hyper parameter is required.",
                "In multi-document segmentation, we show in the experiment, a = 1 and b = 1 is the best.",
                "Our method gives more weights to cue terms.",
                "However, usually cue terms or sentences appear at the beginning of a segment, while the end of the segment may be 1 2 5 10 20 34 51 102 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 Document Number ErrorRate MIl :a=0,b=0 WMI l :a=1,b=1 WMI l :a=1,b=0 WMI l :a=2,b=1 Figure 3: Error rates for different hyper parameters of term weights. 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Normalized Document Entropy NormalizedSegmentEntropy Noisy words Cue words Common stop words Doc−dep stop words Figure 4: Term weights learned from the whole training set. 0 100 200 300 400 500 600 0.06 0.08 0.1 0.12 0.14 0.16 0.18 Number of Steps (Weighted)MutualInformation MI l WMI l Figure 5: Change in (weighted) MI for MIl and WMIl. 0 20 40 60 80 100 120 0 200 400 600 800 1000 1200 1400 1600 1800 2000 Document Number TimetoConverge(sec) MI l WMI l Figure 6: Time to converge for MIl and WMIl. much noisy.",
                "One possible solution is giving more weights to terms at the beginning of each segment.",
                "Moreover, when the length of segments are quite different, long segments have much higher term frequencies, so they may dominate the segmentation boundaries.",
                "Normalization of term frequencies versus the segment length may be useful. 6.",
                "CONCLUSIONS AND FUTURE WORK We proposed a novel method for multi-document topic segmentation and alignment based on weighted mutual information, which can also handle single-document cases.",
                "We used dynamic programming to optimize our algorithm.",
                "Our approach outperforms all the previous methods on singledocument cases.",
                "Moreover, we also showed that doing segmentation among multiple documents can improve the performance tremendously.",
                "Our results also illustrated that using weighted mutual information can utilize the information of multiple documents to reach a better performance.",
                "We only tested our method on limited data sets.",
                "More data sets especially complicated ones should be tested.",
                "More previous methods should be compared with.",
                "Moreover, natural segmentations like paragraphs are hints that can be used to find the optimal boundaries.",
                "Supervised learning also can be considered. 7.",
                "ACKNOWLEDGMENTS The authors want to thank Xiang Ji, and Prof. J. Scott Payne for their help. 8.",
                "REFERENCES [1] A. Banerjee, I. Ghillon, J. Ghosh, S. Merugu, and D. Modha.",
                "A generalized maximum entropy approach to bregman co-clustering and matrix approximation.",
                "In Proceedings of SIGKDD, 2004. [2] R. Bekkerman, R. El-Yaniv, and A. McCallum.",
                "Multi-way distributional clustering via pairwise interactions.",
                "In Proceedings of ICML, 2005. [3] D. M. Blei and P. J. Moreno.",
                "Topic segmentation with an aspect hidden markov model.",
                "In Proceedings of SIGIR, 2001. [4] D. M. Blei, A. Ng, and M. Jordan.",
                "Latent dirichlet allocation.",
                "Journal of Machine Learning Research, 3:993-1022, 2003. [5] T. Brants, F. Chen, and I. Tsochantaridis.",
                "Topic-based document segmentation with probabilistic latent semantic analysis.",
                "In Proceedings of CIKM, 2002. [6] F. Choi.",
                "Advances in domain indepedent linear text segmentation.",
                "In Proceedings of the NAACL, 2000. [7] H. Christensen, B. Kolluru, Y. Gotoh, and S. Renals.",
                "Maximum entropy segmentation of broadcast news.",
                "In Proceedings of ICASSP, 2005. [8] T. Cover and J. Thomas.",
                "Elements of Information Theory.",
                "John Wiley and Sons, New York, USA, 1991. [9] S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and R. Harshman.",
                "Indexing by latent semantic analysis.",
                "Journal of the American Society for Information Systems, 1990. [10] I. Dhillon, S. Mallela, and D. Modha.",
                "Information-theoretic co-clustering.",
                "In Proceedings of SIGKDD, 2003. [11] M. Hajime, H. Takeo, and O. Manabu.",
                "Text segmentation with multiple surface linguistic cues.",
                "In Proceedings of COLING-ACL, 1998. [12] T. K. Ho.",
                "Stop word location and identification for adaptive text recognition.",
                "International Journal of Document Analysis and Recognition, 3(1), August 2000. [13] T. Hofmann.",
                "Probabilistic latent semantic analysis.",
                "In Proceedings of the UAI99, 1999. [14] X. Ji and H. Zha.",
                "Correlating summarization of a pair of multilingual documents.",
                "In Proceedings of RIDE, 2003. [15] X. Ji and H. Zha.",
                "Domain-independent text segmentation using anisotropic diffusion and dynamic programming.",
                "In Proceedings of SIGIR, 2003. [16] X. Ji and H. Zha.",
                "Extracting shared topics of multiple documents.",
                "In Proceedings of the 7th PAKDD, 2003. [17] J. Lafferty, A. McCallum, and F. Pereira.",
                "Conditional random fields: Probabilistic models for segmenting and labeling sequence data.",
                "In Proceedings of ICML, 2001. [18] T. Li, S. Ma, and M. Ogihara.",
                "Entropy-based criterion in categorical clustering.",
                "In Proceedings of ICML, 2004. [19] A. McCallum, D. Freitag, and F. Pereira.",
                "Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of ICML, 2000. [20] L. Pevzner and M. Hearst.",
                "A critique and improvement of an evaluation metric for text segmentation.",
                "Computational Linguistic, 28(1):19-36, 2002. [21] J. C. Reynar.",
                "Statistical models for topic segmentation.",
                "In Proceedings of ACL, 1999. [22] G. Salton and M. McGill.",
                "Introduction to Modern Information Retrieval.",
                "McGraw Hill, 1983. [23] B.",
                "Sun, Q. Tan, P. Mitra, and C. L. Giles.",
                "Extraction and search of chemical formulae in text documents on the web.",
                "In Proceedings of WWW, 2007. [24] B.",
                "Sun, D. Zhou, H. Zha, and J.",
                "Yen.",
                "Multi-task text segmentation and alignment based on weighted mutual information.",
                "In Proceedings of CIKM, 2006. [25] M. Utiyama and H. Isahara.",
                "A statistical model for domain-independent text segmentation.",
                "In Proceedings of the 39th ACL, 1999. [26] C. Wayne.",
                "Multilingual topic detection and tracking: Successful research enabled by corpora and evaluation.",
                "In Proceedings of LREC, 2000. [27] J. Yamron, I. Carp, L. Gillick, S. Lowe, and P. van Mulbregt.",
                "A hidden markov model approach to text segmentation and event tracking.",
                "In Proceedings of ICASSP, 1998. [28] H. Zha and X. Ji.",
                "Correlating multilingual documents via bipartite graph modeling.",
                "In Proceedings of SIGIR, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "single document": {
            "translated_key": "Documento único",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Topic Segmentation with Shared Topic Detection and Alignment of Multiple Documents Bingjun Sun*, Prasenjit Mitra*† , Hongyuan Zha‡ , C. Lee Giles*† , John Yen*† *Department of Computer Science and Engineering † College of Information Sciences and Technology The Pennsylvania State University University Park, PA 16802 ‡ College of Computing The Georgia Institute of Technology Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu ABSTRACT Topic detection and tracking [26] and topic segmentation [15] play an important role in capturing the local and sequential information of documents.",
                "Previous work in this area usually focuses on single documents, although similar multiple documents are available in many domains.",
                "In this paper, we introduce a novel unsupervised method for shared topic detection and topic segmentation of multiple similar documents based on mutual information (MI) and weighted mutual information (WMI) that is a combination of MI and term weights.",
                "The basic idea is that the optimal segmentation maximizes MI(or WMI).",
                "Our approach can detect shared topics among documents.",
                "It can find the optimal boundaries in a document, and align segments among documents at the same time.",
                "It also can handle single-document segmentation as a special case of the multi-document segmentation and alignment.",
                "Our methods can identify and strengthen cue terms that can be used for segmentation and partially remove stop words by using term weights based on entropy learned from multiple documents.",
                "Our experimental results show that our algorithm works well for the tasks of single-document segmentation, shared topic detection, and multi-document segmentation.",
                "Utilizing information from multiple documents can tremendously improve the performance of topic segmentation, and using WMI is even better than using MI for the multi-document segmentation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Clustering; H.3.1 [Information Storage and Retrieval]: Content Analysis and IndexingLinguistic processing; I.2.7 [Artificial Intelligence]: Natural Language Processing-Text analysis; I.5.3 [Pattern Recognition]: Clustering-Algorithms;Similarity measures General Terms Algorithms, Design, Experimentation 1.",
                "INTRODUCTION Many researchers have worked on topic detection and tracking (TDT) [26] and topic segmentation during the past decade.",
                "Topic segmentation intends to identify the boundaries in a document with the goal to capture the latent topical structure.",
                "Topic segmentation tasks usually fall into two categories [15]: text stream segmentation where topic transition is identified, and coherent document segmentation in which documents are split into sub-topics.",
                "The former category has applications in automatic speech recognition, while the latter one has more applications such as partial-text query of long documents in information retrieval, text summary, and quality measurement of multiple documents.",
                "Previous research in connection with TDT falls into the former category, targeted on topic tracking of broadcast speech data and newswire text, while the latter category has not been studied very well.",
                "Traditional approaches perform topic segmentation on documents one at a time [15, 25, 6].",
                "Most of them perform badly in subtle tasks like coherent document segmentation [15].",
                "Often, end-users seek documents that have the similar content.",
                "Search engines, like, Google, provide links to obtain similar pages.",
                "At a finer granularity, users may actually be looking to obtain sections of a document similar to a particular section that presumably discusses a topic of the users interest.",
                "Thus, the extension of topic segmentation from single documents to identifying similar segments from multiple similar documents with the same topic is a natural and necessary direction, and multi-document topic segmentation is expected to have a better performance since more information is utilized.",
                "Traditional approaches using similarity measurement based on term frequency generally have the same assumption that similar vocabulary tends to be in a coherent topic segment [15, 25, 6].",
                "However, they usually suffer the issue of identifying stop words.",
                "For example, additional document-dependent stop words are removed together with the generic stop words in [15].",
                "There are two reasons that we do not remove stop words directly.",
                "First, identifying stop words is another issue [12] that requires estimation in each domain.",
                "Removing common stop words may result in the loss of useful information in a specific domain.",
                "Second, even though stop words can be identified, hard classification of stop words and nonstop words cannot represent the gradually changing amount of information content of each word.",
                "We employ a soft classification using term weights.",
                "In this paper, we view the problem of topic segmentation as an optimization issue using information theoretic techniques to find the optimal boundaries of a document given the number of text segments so as to minimize the loss of mutual information (MI) (or a weighted mutual information (WMI)) after segmentation and alignment.",
                "This is equal to maximizing the MI (or WMI).",
                "The MI focuses on measuring the difference among segments whereas previous research focused on finding the similarity (e.g. cosine distance) of segments [15, 25, 6].",
                "Topic alignment of multiple similar documents can be achieved by clustering sentences on the same topic into the same cluster.",
                "Single-document topic segmentation is just a special case of the multi-document topic segmentation and alignment problem.",
                "Terms can be co-clustered as in [10] at the same time, given the number of clusters, but our experimental results show that this method results in a worse segmentation (see Tables 1, 4, and 6).",
                "Usually, human readers can identify topic transition based on cue words, and can ignore stop words.",
                "Inspired by this, we give each term (or term cluster) a weight based on entropy among different documents and different segments of documents.",
                "Not only can this approach increase the contribution of cue words, but it can also decrease the effect of common stop words, noisy word, and document-dependent stop words.",
                "These words are common in a document.",
                "Many methods based on sentence similarity require that these words are removed before topic segmentation can be performed [15].",
                "Our results in Figure 3 show that term weights are useful for multi-document topic segmentation and alignment.",
                "The major contribution of this paper is that it introduces a novel method for topic segmentation using MI and shows that this method performs better than previously used criteria.",
                "Also, we have addressed the problem of topic segmentation and alignment across multiple documents, whereas most existing research focused on segmentation of single documents.",
                "Multi-document segmentation and alignment can utilize information from similar documents and improves the performance of topic segmentation greatly.",
                "Obviously, our approach can handle single documents as a special case when multiple documents are unavailable.",
                "It can detect shared topics among documents to judge if they are multiple documents on the same topic.",
                "We also introduce the new criterion of WMI based on term weights learned from multiple similar documents, which can improve performance of topic segmentation further.",
                "We propose an iterative greedy algorithm based on dynamic programming and show that it works well in practice.",
                "Some of our prior work is in [24].",
                "The rest of this paper is organized as follows: In Section 2, we review related work.",
                "Section 3 contains a formulation of the problem of topic segmentation and alignment of multiple documents with term co-clustering, a review of the criterion of MI for clustering, and finally an introduction to WMI.",
                "In Section 4, we first propose the iterative greedy algorithm of topic segmentation and alignment with term co-clustering, and then describe how the algorithm can be optimized by usFigure 1: Illustration of multi-document segmentation and alignment. ing dynamic programming.",
                "In Section 5, experiments about single-document segmentation, shared topic detection, and multi-document segmentation are described, and results are presented and discussed to evaluate the performance of our algorithm.",
                "Conclusions and some future directions of the research work are discussed in Section 6. 2.",
                "PREVIOUS WORK Generally, the existing approaches to text segmentation fall into two categories: supervised learning [19, 17, 23] and unsupervised learning [3, 27, 5, 6, 15, 25, 21].",
                "Supervised learning usually has good performance, since it learns functions from labelled training sets.",
                "However, often getting large training sets with manual labels on document sentences is prohibitively expensive, so unsupervised approaches are desired.",
                "Some models consider dependence between sentences and sections, such as Hidden Markov Model [3, 27], Maximum Entropy Markov Model [19], and Conditional Random Fields [17], while many other approaches are based on lexical cohesion or similarity of sentences [5, 6, 15, 25, 21].",
                "Some approaches also focus on cue words as hints of topic transitions [11].",
                "While some existing methods only consider information in single documents [6, 15], others utilize multiple documents [16, 14].",
                "There are not many works in the latter category, even though the performance of segmentation is expected to be better with utilization of information from multiple documents.",
                "Previous research studied methods to find shared topics [16] and topic segmentation and summarization between just a pair of documents [14].",
                "Text classification and clustering is a related research area which categorizes documents into groups using supervised or unsupervised methods.",
                "Topical classification or clustering is an important direction in this area, especially co-clustering of documents and terms, such as LSA [9], PLSA [13], and approaches based on distances and bipartite graph partitioning [28] or maximum MI [2, 10], or maximum entropy [1, 18].",
                "Criteria of these approaches can be utilized in the issue of topic segmentation.",
                "Some of those methods have been extended into the area of topic segmentation, such as PLSA [5] and maximum entropy [7], but to our best knowledge, using MI for topic segmentation has not been studied. 3.",
                "PROBLEM FORMULATION Our goal is to segment documents and align the segments across documents (Figure 1).",
                "Let T be the set of terms {t1, t2, ..., tl}, which appear in the unlabelled set of documents D = {d1, d2, ..., dm}.",
                "Let Sd be the set of sentences for document d ∈ D, i.e. {s1, s2, ..., snd }.",
                "We have a 3D matrix of term frequency, in which the three dimensions are random variables of D, Sd, and T. Sd actually is a random vector including a random variable for each d ∈ D. The term frequency can be used to estimate the joint probability distribution P(D, Sd, T), which is p(t, d, s) = T(t, d, s)/ND, where T(t, d, s) is the number of t in ds sentence s and ND is the total number of terms in D. ˆS represents the set of segments {ˆs1, ˆs2, ..., ˆsp} after segmentation and alignment among multiple documents, where the number of segments | ˆS| = p. A segment ˆsi of document d is a sequence of adjacent sentences in d. Since for different documents si may discuss different sub-topics, our goal is to cluster adjacent sentences in each document into segments, and align similar segments among documents, so that for different documents ˆsi is about the same sub-topic.",
                "The goal is to find the optimal topic segmentation and alignment mapping Segd(si) : {s1, s2, ..., snd } → {ˆs1, ˆs2, ..., ˆsp} and Alid(ˆsi) : {ˆs1, ˆs2, ..., ˆsp} → {ˆs1, ˆs2, ..., ˆsp}, for all d ∈ D, where ˆsi is ith segment with the constraint that only adjacent sentences can be mapped to the same segment, i.e. for d, {si, si+1, ..., sj} → {ˆsq}, where q ∈ {1, ..., p}, where p is the segment number, and if i > j, then for d, ˆsq is missing.",
                "After segmentation and alignment, random vector Sd becomes an aligned random variable ˆS.",
                "Thus, P(D, Sd, T) becomes P(D, ˆS, T).",
                "Term co-clustering is a technique that has been employed [10] to improve the accuracy of document clustering.",
                "We evaluate the effect of it for topic segmentation.",
                "A term t is mapped to exactly one term cluster.",
                "Term co-clustering involves simultaneously finding the optimal term clustering mapping Clu(t) : {t1, t2, ..., tl} → {ˆt1, ˆt2, ..., ˆtk}, where k ≤ l, l is the total number of words in all the documents, and k is the number of clusters. 4.",
                "METHODOLOGY We now describe a novel algorithm which can handle singledocument segmentation, shared topic detection, and multidocument segmentation and alignment based on MI or WMI. 4.1 Mutual Information MI I(X; Y ) is a quantity to measure the amount of information which is contained in two or more random variables [8, 10].",
                "For the case of two random variables, we have I(X; Y ) = x∈X y∈Y p(x, y)log p(x, y) p(x)p(y) , (1) Obviously, when random variables X and Y are independent, I(X; Y ) = 0.",
                "Thus, intuitively, the value of MI depends on how random variables are dependent on each other.",
                "The optimal co-clustering is the mapping Clux : X → ˆX and Cluy : Y → ˆY that minimizes the loss: I(X; Y ) − I( ˆX; ˆY ), which is equal to maximizing I( ˆX; ˆY ).",
                "This is the criterion of MI for clustering.",
                "In the case of topic segmentation, the two random variables are the term variable T and the segment variable S, and each sample is an occurrence of a term T = t in a particular segment S = s. I(T; S) is used to measure how dependent T and S are.",
                "However, I(T; S) cannot be computed for documents before segmentation, since we do not have a set of S due to the fact that sentences of Document d, si ∈ Sd, is not aligned with other documents.",
                "Thus, instead of minimizing the loss of MI, we can maximize MI after topic segmentation, computed as: I( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS p(ˆt, ˆs)log p(ˆt, ˆs) p(ˆt)p(ˆs) , (2) where p(ˆt, ˆs) are estimated by the term frequency tf of Term Cluster ˆt and Segment ˆs in the training set D. Note that here a segment ˆs includes sentences about the the same topic among all documents.",
                "The optimal solution is the mapping Clut : T → ˆT, Segd : Sd → ˆS , and Alid : ˆS → ˆS, which maximizes I( ˆT; ˆS). 4.2 Weighted Mutual Information In topic segmentation and alignment of multiple documents, if P(D, ˆS, T) is known, based on the marginal distributions P(D|T) and P( ˆS|T) for each term t ∈ T, we can categorize terms into four types in the data set: • Common stop words are common both along the dimensions of documents and segments. • Document-dependent stop words that depends on the personal writing style are common only along the dimension of segments for some documents. • Cue words are the most important elements for segmentation.",
                "They are common along the dimension of documents only for the same segment, and they are not common along the dimensions of segments. • Noisy words are other words which are not common along both dimensions.",
                "Entropy based on P(D|T) and P( ˆS|T) can be used to identify different types of terms.",
                "To reinforce the contribution of cue words in the MI computation, and simultaneously reduce the effect of the other three types of words, similar as the idea of the tf-idf weight [22], we use entropies of each term along the dimensions of document D and segment ˆS, i.e.",
                "ED(ˆt) and EˆS(ˆt), to compute the weight.",
                "A cue word usually has a large value of ED(ˆt) but a small value of EˆS(ˆt).",
                "We introduce term weights (or term cluster weights) wˆt = ( ED(ˆt) maxˆt ∈ ˆT (ED(ˆt )) )a (1 − EˆS(ˆt) maxˆt ∈ ˆT (EˆS(ˆt )) )b , (3) where ED(ˆt) = d∈D p(d|ˆt)log|D| 1 p(d|ˆt) , EˆS(ˆt) = ˆs∈ ˆS p(ˆs|ˆt)log| ˆS| 1 p(ˆs|ˆt) , and a > 0 and b > 0 are powers to adjust term weights.",
                "Usually a = 1 and b = 1 as default, and maxˆt ∈ ˆT (ED(ˆt )) and maxˆt ∈ ˆT (EˆS(ˆt )) are used to normalize the entropy values.",
                "Term cluster weights are used to adjust p(ˆt, ˆs), pw(ˆt, ˆs) = wˆtp(ˆt, ˆs) ˆt∈ ˆT ;ˆs∈ ˆS wˆtp(ˆt, ˆs) , (4) and Iw( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS pw(ˆt, ˆs)log pw(ˆt, ˆs) pw(ˆt)pw(ˆs) , (5) where pw(ˆt) and pw(ˆs) are marginal distributions of pw(ˆt, ˆs).",
                "However, since we do not know either the term weights or P(D, ˆS, T), we need to estimate them, but wˆt depends on p(ˆs|t) and ˆS, while ˆS and p(ˆs|t) also depend on wˆt that is still unknown.",
                "Thus, an iterative algorithm is required to estimate term weights wˆt and find the best segmentation and alignment to optimize the objective function Iw concurrently.",
                "After a document is segmented into sentences Input: Joint probability distribution P(D, Sd, T), number of text segments p ∈ {2, 3, ..., max(sd)}, number of term clusters k ∈ {2, 3, ..., l} (if k = l, no term co-clustering required), and weight type w ∈ {0, 1}, indicating to use I or Iw, respectively.",
                "Output: Mapping Clu, Seg, Ali, and term weights wˆt.",
                "Initialization: 0. i = 0.",
                "Initialize Clu (0) t , Seg (0) d , and Ali (0) d ; Initialize w (0) ˆt using Equation (6) if w = 1; Stage 1: 1.",
                "If |D| = 1, k = l, and w = 0, check all sequential segmentations of d into p segments and find the best one Segd(s) = argmaxˆsI( ˆT; ˆS), and return Segd; otherwise, if w = 1 and k = l, go to 3.1; Stage 2: 2.1 If k < l, for each term t, find the best cluster ˆt as Clu(i+1)(t) = argmaxˆtI( ˆT; ˆS(i)) based on Seg(i) and Ali(i); 2.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+1) d (Seg (i+1) d (s)) = argmaxˆsI( ˆT(i+1); ˆS) based on Clu(i+1)(t) if k < l or Clu(0)(t) if k = l; 2.3 i + +.",
                "If Clu, Seg, or Ali changed, go to 2.1; otherwise, if w = 0, return Clu(i), Seg(i), and Ali(i); else j = 0, go to 3.1; Stage 3: 3.1 Update w (i+j+1) ˆt based on Seg(i+j), Ali(i+j), and Clu(i) using Equation (3); 3.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+j+1) d (Seg (i+j+1) d (s)) = argmaxˆsIw( ˆT(i); ˆS) based on Clu(i) and w (i+j+1) ˆt ; 3.3 j + +.",
                "If Iw( ˆT; ˆS) changes, go to step 6; otherwise, stop and return Clu(i), Seg(i+j), Ali(i+j), and w (i+j) ˆt ; Figure 2: Algorithm: Topic segmentation and alignment based on MI or WMI. and each sentence is segmented into words, each word is stemmed.",
                "Then the joint probability distribution P(D, Sd, T) can be estimated.",
                "Finally, this distribution can be used to compute MI in our algorithm. 4.3 Iterative Greedy Algorithm Our goal is to maximize the objective function, I( ˆT; ˆS) or Iw( ˆT; ˆS), which can measure the dependence of term occurrences in different segments.",
                "Generally, first we do not know the estimate term weights, which depend on the optimal topic segmentation and alignment, and term clusters.",
                "Moreover, this problem is NP-hard [10], even though if we know the term weights.",
                "Thus, an iterative greedy algorithm is desired to find the best solution, even though probably only local maxima are reached.",
                "We present the iterative greedy algorithm in Figure 2 to find a local maximum of I( ˆT; ˆS) or Iw( ˆT; ˆS) with simultaneous term weight estimation.",
                "This algorithm can is iterative and greedy for multi-document cases or single-document cases with term weight estimation and/or term co-clustering.",
                "Otherwise, since it is just a one step algorithm to solve the task of single-document segmentation [6, 15, 25], the global maximum of MI is guaranteed.",
                "We will show later that term co-clustering reduces the accuracy of the results and is not necessary, and for singledocument segmentation, term weights are also not required. 4.3.1 Initialization In Step 0, the initial term clustering Clut and topic segmentation and alignment Segd and Alid are important to avoid local maxima and reduce the number of iterations.",
                "First, a good guess of term weights can be made by using the distributions of term frequency along sentences for each document and averaging them to get the initial values of wˆt: wt = ( ED(t) maxt ∈T (ED(t )) )(1 − ES(t) maxt ∈T (ES(t )) ), (6) where ES(t) = 1 |Dt| d∈Dt (1 − s∈Sd p(s|t)log|Sd| 1 p(s|t) ), where Dt is the set of documents which contain Term t. Then, for the initial segmentation Seg(0) , we can simply segment documents equally by sentences.",
                "Or we can find the optimal segmentation just for each document d which maximizes the WMI, Seg (0) d = argmaxˆsIw(T; ˆS), where w = w (0) ˆt .",
                "For the initial alignment Ali(0) , we can first assume that the order of segments for each d is the same.",
                "For the initial term clustering Clu(0) , first cluster labels can be set randomly, and after the first time of Step 3, a good initial term clustering is obtained. 4.3.2 Different Cases After initialization, there are three stages for different cases.",
                "Totally there are eight cases, |D| = 1 or |D| > 1, k = l or k < l, w = 0 or w = 1.",
                "<br>single document</br> segmentation without term clustering and term weight estimation (|D| = 1, k = l, w = 0) only requires Stage 1 (Step 1).",
                "If term clustering is required (k < l), Stage 2 (Step 2.1, 2.2, and 2.3) is executed iteratively.",
                "If term weight estimation is required (w = 1), Stage 3 (Step 3.1, 3.2, and 3.3) is executed iteratively.",
                "If both are required (k < l, w = 1), Stage 2 and 3 run one after the other.",
                "For multi-document segmentation without term clustering and term weight estimation (|D| > 1, k = l, w = 0), only iteration of Step 2.2 and 2.3 are required.",
                "At Stage 1, the global maximum can be found based on I( ˆT; ˆS) using dynamic programming in Section 4.4.",
                "Simultaneously finding a good term clustering and estimated term weights is impossible, since when moving a term to a new term cluster to maximize Iw( ˆT; ˆS), we do not know that the weight of this term should be the one of the new cluster or the old cluster.",
                "Thus, we first do term clustering at Stage 2, and then estimate term weights at Stage 3.",
                "At Stage 2, Step 2.1 is to find the best term clustering and Step 2.2 is to find the best segmentation.",
                "This cycle is repeated to find a local maximum based on MI I until it converges.",
                "The two steps are: (1) based on current term clustering Cluˆt, for each document d, the algorithm segments all the sentences Sd into p segments sequentially (some segments may be empty), and put them into the p segments ˆS of the whole training set D (all possible cases of different segmentation Segd and alignment Alid are checked) to find the optimal case, and (2) based on the current segmentation and alignment, for each term t, the algorithm finds the best term cluster of t based on the current segmentation Segd and alignment Alid.",
                "After finding a good term clustering, term weights are estimated if w = 1.",
                "At Stage 3, similar as Stage 2, Step 3.1 is term weight re-estimation and Step 3.2 is to find a better segmentation.",
                "They are repeated to find a local maximum based on WMI Iw until it converges.",
                "However, if the term clustering in Stage 2 is not accurate, then the term weight estimation at Stage 3 may have a bad result.",
                "Finally, at Step 3.3, this algorithm converges and return the output.",
                "This algorithm can handle both single-document and multi-document segmentation.",
                "It also can detect shared topics among documents by checking the proportion of overlapped sentences on the same topics, as described in Sec 5.2. 4.4 Algorithm Optimization In many previous works on segmentation, dynamic programming is a technique used to maximize the objective function.",
                "Similarly, at Step 1, 2.2, and 3.2 of our algorithm, we can use dynamic programming.",
                "For Stage 1, using dynamic programming can still find the global optimum, but for Stage 2 and Stage 3, we can only find the optimum for each step of topic segmentation and alignment of a document.",
                "Here we only show the dynamic programming for Step 3.2 using WMI (Step 1 and 2.2 are similar but they can use either I or Iw).",
                "There are two cases that are not shown in the algorithm in Figure 2: (a) single-document segmentation or multi-document segmentation with the same sequential order of segments, where alignment is not required, and (b) multi-document segmentation with different sequential orders of segments, where alignment is necessary.",
                "The alignment mapping function of the former case is simply just Alid(ˆsi) = ˆsi, while for the latter ones alignment mapping function Alid(ˆsi) = ˆsj, i and j may be different.",
                "The computational steps for the two cases are listed below: Case 1 (no alignment): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs) and partial pw(ˆs) without counting sentences from d. Then put sentences from i to j into Part k, and compute partial WMI PIw( ˆT; ˆsk(si, si+1, ..., sj)) ˆt∈ ˆT pw(ˆt, ˆsk)log pw(ˆt, ˆsk) pw(ˆt)pw(ˆsk) , where Alid(si, si+1, ..., sj) = k, k ∈ {1, 2, ..., p}, 1 ≤ i ≤ j ≤ nd, and Segd(sq) = ˆsk for all i ≤ q ≤ j. (2) Let M(sm, 1) = PIw( ˆT; ˆs1(s1, s2, ..., sm)).",
                "Then M(sm, L) = maxi[M(si−1, L − 1) + PIw( ˆT; ˆsL(si, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, and when i > m, no sentences are put into ˆsk when compute PIw (note PIw( ˆT; ˆs(si, ..., sm)) = 0 for single-document segmentation). (3) Finally M(snd , p) = maxi[M(si−1, p − 1)+ PIw( ˆT; ˆsp(si, ..., snd ))], where 1 ≤ i ≤ nd+1.",
                "The optimal Iw is found and the corresponding segmentation is the best.",
                "Case 2 (alignment required): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs), and partial pw(ˆs), and PIw( ˆT; ˆsk(si, si+1, ..., sj)) similarly as Case 1. (2) Let M(sm, 1, k) = PIw( ˆT; ˆsk(s1, s2, ..., sm)), where k ∈ {1, 2, ..., p}.",
                "Then M(sm, L, kL) = maxi,j[M(si−1, L − 1, kL/j) + PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, kL ∈ Set(p, L), which is the set of all p!",
                "L!(p−L)! combinations of L segments chosen from all p segments, j ∈ kL, the set of L segments chosen from all p segments, and kL/j is the combination of L − 1 segments in kL except Segment j. (3) Finally, M(snd , p, kp) = maxi,j[M(si−1, p − 1, kp/j) +PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., snd ))], where kp is just the combination of all p segments and 1 ≤ i ≤ nd + 1, which is the optimal Iw and the corresponding segmentation is the best.",
                "The steps of Case 1 and 2 are similar, except in Case 2, alignment is considered in addition to segmentation.",
                "First, basic items of probability for computing Iw are computed excluding Doc d, and then partial WMI by putting every possible sequential segment (including empty segment) of d into every segment of the set.",
                "Second, the optimal sum of PIw for L segments and the leftmost m sentences, M(sm, L), is found.",
                "Finally, the maximal WMI is found among different sums of M(sm, p − 1) and PIw for Segment p. 5.",
                "EXPERIMENTS In this section, single-document segmentation, shared topic detection, and multi-document segmentation will be tested.",
                "Different hyper parameters of our method are studied.",
                "For convenience, we refer to the method using I as MIk if w = 0, and Iw as WMIk if w = 2 or as WMIk if w = 1, where k is the number of term clusters, and if k = l, where l is the total number of terms, then no term clustering is required, i.e.",
                "MIl and WMIl. 5.1 Single-document Segmentation 5.1.1 Test Data and Evaluation The first data set we tested is a synthetic one used in previous research [6, 15, 25] and many other papers.",
                "It has 700 samples.",
                "Each is a concatenation of ten segments.",
                "Each segment is the first n sentence selected randomly from the Brown corpus, which is supposed to have a different topic from each other.",
                "Currently, the best results on this data set is achieved by Ji et.al. [15].",
                "To compare the performance of our methods, the criterion used widely in previous research is applied, instead of the unbiased criterion introduced in [20].",
                "It chooses a pair of words randomly.",
                "If they are in different segments (different) for the real segmentation (real), but predicted (pred) as in the same segment, it is a miss.",
                "If they are in the same segment (same), but predicted as in different segments, it is a false alarm.",
                "Thus, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, diff)p(diff|real) +p(false alarm|real, pred, same)p(same|real). 5.1.2 Experiment Results We tested the case when the number of segments is known.",
                "Table 1 shows the results of our methods with different hyper parameter values and three previous approaches, C99[25], U00[6], and ADDP03[15], on this data set when the segment number is known.",
                "In WMI for single-document segmentation, the term weights are computed as follows: wˆt = 1−EˆS(ˆt)/maxˆt ∈ ˆT (EˆS(ˆt )).",
                "For this case, our methods MIl and WMIl both outperform all the previous approaches.",
                "We compared our methods with ADDP03using one-sample one-sided t-test and p-values are shown in Table 2.",
                "From the p-values, we can see that mostly the differences are very Table 1: Average Error Rates of Single-document Segmentation Given Segment Numbers Known Range of n 3-11 3-5 6-8 9-11 Sample size 400 100 100 100 C99 12% 11% 10% 9% U00 10% 9% 7% 5% ADDP03 6.0% 6.8% 5.2% 4.3% MIl 4.68% 5.57% 2.59% 1.59% WMIl 4.94% 6.33% 2.76% 1.62% MI100 9.62% 12.92% 8.66% 6.67% Table 2: Single-document Segmentation: P-values of T-test on Error Rates Range of n 3-11 3-5 6-8 9-11 ADDP03, MIl 0.000 0.000 0.000 0.000 ADDP03, WMIl 0.000 0.099 0.000 0.000 MIl, WMIl 0.061 0.132 0.526 0.898 significant.",
                "We also compare the error rates between our two methods using two-sample two-sided t-test to check the hypothesis that they are equal.",
                "We cannot reject the hypothesis that they are equal, so the difference are not significant, even though all the error rates for MIl are smaller than WMIl.",
                "However, we can conclude that term weights contribute little in single-document segmentation.",
                "The results also show that MI using term co-clustering (k = 100) decreases the performance.",
                "We tested different number of term clusters, and found that the performance becomes better when the cluster number increases to reach l. WMIk<l has similar results that we did not show in the table.",
                "As mentioned before, using MI may be inconsistent on optimal boundaries given different numbers of segments.",
                "This situation occurs especially when the similarities among segments are quite different, i.e. some transitions are very obvious, while others are not.",
                "This is because usually a document is a hierarchical structure instead of only a sequential structure.",
                "When the segments are not at the same level, this situation may occur.",
                "Thus, a hierarchical topic segmentation approach is desired, and the structure highly depends on the number of segments for each internal node and the stop criteria of splitting.",
                "For this data set of singledocument segmentation, since it is just a synthetic set, which is just a concatenation of several segments about different topics, it is reasonable that approaches simply based on term frequency have a good performance.",
                "Usually for the tasks of segmenting coherent documents for sub-topics, the effectiveness decreases much. 5.2 Shared Topic Detection 5.2.1 Test Data and Evaluation The second data set contains 80 news articles from Google News.",
                "There are eight topics and each has 10 articles.",
                "We randomly split the set into subsets with different document numbers and each subset has all eight topics.",
                "We compare our approach MIl and WMIl with LDA [4].",
                "LDA treats a document in the data set as a bag of words, finds its distribution on topics, and its major topic.",
                "MIl and WMIl views each sentence as a bag of words and tag it with a topic label.",
                "Then for each pair of documents, LDA determines if they are on the same topic, while MIl and Table 3: Shared Topic Detection: Average Error Rates for Different Numbers of Documents in Each Subset #Doc 10 20 40 80 LDA 8.89% 16.33% 1.35% 0.60% MIl, θ = 0.6 4.17% 1.71% 1.47% 0.0% WMIl, θ = 0.8 18.6% 3.16% 1.92% 0.0% WMIl check whether the proportion overlapped sentences on the same topic is larger than the adjustable threshold θ.",
                "That is, in MIl and WMIl, for a pair of documents d, d , if [ s∈Sd,s ∈Sd 1(topics=topics )/min(|Sd|, |Sd|)] > θ, where Sd is the set of sentences of d, and |Sd| is the number of sentences of d, then d and d have the shared topic.",
                "For a pair of documents selected randomly, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, same)p(same|real) +p(false alarm|real, pred, diff)p(diff|real), where a miss means if they have the same topic (same) for the real case (real), but predicted (pred) as on the same topic.",
                "If they are on different topics (diff), but predicted as on the same topic, it is a false alarm. 5.2.2 Experiment Results The results are shown in Table 3.",
                "If most documents have different topics, in WMIl, the estimation of term weights in Equation (3) is not correct.",
                "Thus, WMIl is not expected to have a better performance than MIl, when most documents have different topics.",
                "When there are fewer documents in a subset with the same number of topics, more documents have different topics, so WMIl is more worse than MIl.",
                "We can see that for most cases MIl has a better (or at least similar) performance than LDA.",
                "After shared topic detection, multi-document segmentation of documents with the shared topics is able to be executed. 5.3 Multi-document Segmentation 5.3.1 Test Data and Evaluation For multi-document segmentation and alignment, our goal is to identify these segments about the same topic among multiple similar documents with shared topics.",
                "Using Iw is expected to perform better than I, since without term weights the result is affected seriously by document-dependent stop words and noisy words which depends on the personal writing style.",
                "It is more likely to treat the same segments of different documents as different segments under the effect of document-dependent stop words and noisy words.",
                "Term weights can reduce the effect of document-dependent stop words and noisy words by giving cue terms more weights.",
                "The data set for multi-document segmentation and alignment has 102 samples and 2264 sentences totally.",
                "Each is the introduction part of a lab report selected from the course of Biol 240W, Pennsylvania State University.",
                "Each sample has two segments, introduction of plant hormones and the content in the lab.",
                "The length range of samples is from two to 56 sentences.",
                "Some samples only have one part and some have a reverse order the these two segments.",
                "It is not hard to identify the boundary between two segments for human.",
                "We labelled each sentence manually for evaluation.",
                "The criterion of evaluation is just using the proportion of the number of sentences with wrong predicted segment labels in the total number of sentences in the whole training Table 4: Average Error Rates of Multi-document Segmentation Given Segment Numbers Known #Doc MIl WMIl k MIk WMIk 102 3.14% 2.78% 300 4.68% 6.58% 51 4.17% 3.63% 300 17.83% 22.84% 34 5.06% 4.12% 300 18.75% 20.95% 20 7.08% 5.42% 250 20.40% 21.83% 10 10.38% 7.89% 250 21.42% 21.91% 5 15.77% 11.64% 250 21.89% 22.59% 2 25.90% 23.18% 50 25.44% 25.49% 1 23.90% 24.82% 25 25.75% 26.15% Table 5: Multi-document Segmentation: P-values of T-test on Error Rates for MIl and WMIl #Doc 51 34 20 10 5 2 P-value 0.19 0.101 0.025 0.001 0.000 0.002 set as the error rate: p(error|predicted, real) = d∈D s∈Sd 1(predicteds=reals)/ d∈D nd.",
                "In order to show the benefits of multi-document segmentation and alignment, we compared our method with different parameters on different partitions of the same training set.",
                "Except the cases that the number of documents is 102 and one (they are special cases of using the whole set and the pure single-document segmentation), we randomly divided the training set into m partitions, and each has 51, 34, 20, 10, 5, and 2 document samples.",
                "Then we applied our methods on each partition and calculated the error rate of the whole training set.",
                "Each case was repeated for 10 times for computing the average error rates.",
                "For different partitions of the training set, different k values are used, since the number of terms increases when the document number in each partition increases. 5.3.2 Experiment Results From the experiment results in Table 4, we can see the following observations: (1) When the number of documents increases, all methods have better performances.",
                "Only from one to two documents, MIl has decrease a little.",
                "We can observe this from Figure 3 at the point of document number = 2.",
                "Most curves even have the worst results at this point.",
                "There are two reasons.",
                "First, because samples vote for the best multi-document segmentation and alignment, but if only two documents are compared with each other, the one with missing segments or a totally different sequence will affect the correct segmentation and alignment of the other.",
                "Second, as noted at the beginning of this section, if two documents have more document-dependent stop words or noisy words than cue words, then the algorithm may view them as two different segments and the other segment is missing.",
                "Generally, we can only expect a better performance when the number of documents is larger than the number of segments. (2) Except single-document segmentation, WMIl is always better than MIl, and when the number of documents is reaching one or increases to a very large number, their performances become closer.",
                "Table 5 shows p-values of twosample one-sided t-test between MIl and WMIl.",
                "We also can see this trend from p-values.",
                "When document number = 5, we reached the smallest p-value and the largest difference between error rates of MIl and WMIl.",
                "For single-document Table 6: Multi-document Segmentation: Average Error Rate for Document Number = 5 in Each Subset with Different Number of Term Clusters #Cluster 75 100 150 250 l MIk 24.67% 24.54% 23.91% 22.59% 15.77% segmentation, WMIl is even a little bit worse than MIl, which is similar as the results of the single-document segmentation on the first data set.",
                "The reason is that for singledocument segmentation, we cannot estimate term weights accurately, since multiple documents are unavailable. (3) Using term clustering usually gets worse results than MIl and WMIl. (4) Using term clustering in WMIk is even worse than in MIk, since in WMIk term clusters are found first using I before using Iw.",
                "If the term clusters are not correct, then the term weights are estimated worse, which may mislead the algorithm to reach even worse results.",
                "From the results we also found that in multi-document segmentation and alignment, most documents with missing segments and a reverse order are identified correctly.",
                "Table 6 illustrates the experiment results for the case of 20 partitions (each has five document samples) of the training set and topic segmentation and alignment using MIk with different numbers of term clusters k. Notice that when the number of term clusters increases, the error rate becomes smaller.",
                "Without term clustering, we have the best result.",
                "We did not show results for WMIk with term clustering, but the results are similar.",
                "We also tested WMIl with different hyper parameters of a and b to adjust term weights.",
                "The results are presented in Figure 3.",
                "It was shown that the default case WMIl : a = 1, b = 1 gave the best results for different partitions of the training set.",
                "We can see the trend that when the document number is very small or large, the difference between MIl : a = 0, b = 0 and WMIl : a = 1, b = 1 becomes quite small.",
                "When the document number is not large (about from 2 to 10), all the cases using term weights have better performances than MIl : a = 0, b = 0 without term weights, but when the document number becomes larger, the cases WMIl : a = 1, b = 0 and WMIl : a = 2, b = 1 become worse than MIl : a = 0, b = 0.",
                "When the document number becomes very large, they are even worse than cases with small document numbers.",
                "This means that a proper way to estimate term weights for the criterion of WMI is very important.",
                "Figure 4 shows the term weights learned from the whole training set.",
                "Four types of words are categorized roughly even though the transition among them are subtle.",
                "Figure 5 illustrates the change in (weighted) mutual information for MIl and WMIl.",
                "As expected, mutual information for MIl increases monotonically with the number of steps, while WMIl does not.",
                "Finally, MIl and WMIl are scalable, with computational complexity shown in Figure 6.",
                "One advantage for our approach based on MI is that removing stop words is not required.",
                "Another important advantage is that there are no necessary hyper parameters to adjust.",
                "In single-document segmentation, the performance based on MI is even better for that based on WMI, so no extra hyper parameter is required.",
                "In multi-document segmentation, we show in the experiment, a = 1 and b = 1 is the best.",
                "Our method gives more weights to cue terms.",
                "However, usually cue terms or sentences appear at the beginning of a segment, while the end of the segment may be 1 2 5 10 20 34 51 102 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 Document Number ErrorRate MIl :a=0,b=0 WMI l :a=1,b=1 WMI l :a=1,b=0 WMI l :a=2,b=1 Figure 3: Error rates for different hyper parameters of term weights. 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Normalized Document Entropy NormalizedSegmentEntropy Noisy words Cue words Common stop words Doc−dep stop words Figure 4: Term weights learned from the whole training set. 0 100 200 300 400 500 600 0.06 0.08 0.1 0.12 0.14 0.16 0.18 Number of Steps (Weighted)MutualInformation MI l WMI l Figure 5: Change in (weighted) MI for MIl and WMIl. 0 20 40 60 80 100 120 0 200 400 600 800 1000 1200 1400 1600 1800 2000 Document Number TimetoConverge(sec) MI l WMI l Figure 6: Time to converge for MIl and WMIl. much noisy.",
                "One possible solution is giving more weights to terms at the beginning of each segment.",
                "Moreover, when the length of segments are quite different, long segments have much higher term frequencies, so they may dominate the segmentation boundaries.",
                "Normalization of term frequencies versus the segment length may be useful. 6.",
                "CONCLUSIONS AND FUTURE WORK We proposed a novel method for multi-document topic segmentation and alignment based on weighted mutual information, which can also handle single-document cases.",
                "We used dynamic programming to optimize our algorithm.",
                "Our approach outperforms all the previous methods on singledocument cases.",
                "Moreover, we also showed that doing segmentation among multiple documents can improve the performance tremendously.",
                "Our results also illustrated that using weighted mutual information can utilize the information of multiple documents to reach a better performance.",
                "We only tested our method on limited data sets.",
                "More data sets especially complicated ones should be tested.",
                "More previous methods should be compared with.",
                "Moreover, natural segmentations like paragraphs are hints that can be used to find the optimal boundaries.",
                "Supervised learning also can be considered. 7.",
                "ACKNOWLEDGMENTS The authors want to thank Xiang Ji, and Prof. J. Scott Payne for their help. 8.",
                "REFERENCES [1] A. Banerjee, I. Ghillon, J. Ghosh, S. Merugu, and D. Modha.",
                "A generalized maximum entropy approach to bregman co-clustering and matrix approximation.",
                "In Proceedings of SIGKDD, 2004. [2] R. Bekkerman, R. El-Yaniv, and A. McCallum.",
                "Multi-way distributional clustering via pairwise interactions.",
                "In Proceedings of ICML, 2005. [3] D. M. Blei and P. J. Moreno.",
                "Topic segmentation with an aspect hidden markov model.",
                "In Proceedings of SIGIR, 2001. [4] D. M. Blei, A. Ng, and M. Jordan.",
                "Latent dirichlet allocation.",
                "Journal of Machine Learning Research, 3:993-1022, 2003. [5] T. Brants, F. Chen, and I. Tsochantaridis.",
                "Topic-based document segmentation with probabilistic latent semantic analysis.",
                "In Proceedings of CIKM, 2002. [6] F. Choi.",
                "Advances in domain indepedent linear text segmentation.",
                "In Proceedings of the NAACL, 2000. [7] H. Christensen, B. Kolluru, Y. Gotoh, and S. Renals.",
                "Maximum entropy segmentation of broadcast news.",
                "In Proceedings of ICASSP, 2005. [8] T. Cover and J. Thomas.",
                "Elements of Information Theory.",
                "John Wiley and Sons, New York, USA, 1991. [9] S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and R. Harshman.",
                "Indexing by latent semantic analysis.",
                "Journal of the American Society for Information Systems, 1990. [10] I. Dhillon, S. Mallela, and D. Modha.",
                "Information-theoretic co-clustering.",
                "In Proceedings of SIGKDD, 2003. [11] M. Hajime, H. Takeo, and O. Manabu.",
                "Text segmentation with multiple surface linguistic cues.",
                "In Proceedings of COLING-ACL, 1998. [12] T. K. Ho.",
                "Stop word location and identification for adaptive text recognition.",
                "International Journal of Document Analysis and Recognition, 3(1), August 2000. [13] T. Hofmann.",
                "Probabilistic latent semantic analysis.",
                "In Proceedings of the UAI99, 1999. [14] X. Ji and H. Zha.",
                "Correlating summarization of a pair of multilingual documents.",
                "In Proceedings of RIDE, 2003. [15] X. Ji and H. Zha.",
                "Domain-independent text segmentation using anisotropic diffusion and dynamic programming.",
                "In Proceedings of SIGIR, 2003. [16] X. Ji and H. Zha.",
                "Extracting shared topics of multiple documents.",
                "In Proceedings of the 7th PAKDD, 2003. [17] J. Lafferty, A. McCallum, and F. Pereira.",
                "Conditional random fields: Probabilistic models for segmenting and labeling sequence data.",
                "In Proceedings of ICML, 2001. [18] T. Li, S. Ma, and M. Ogihara.",
                "Entropy-based criterion in categorical clustering.",
                "In Proceedings of ICML, 2004. [19] A. McCallum, D. Freitag, and F. Pereira.",
                "Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of ICML, 2000. [20] L. Pevzner and M. Hearst.",
                "A critique and improvement of an evaluation metric for text segmentation.",
                "Computational Linguistic, 28(1):19-36, 2002. [21] J. C. Reynar.",
                "Statistical models for topic segmentation.",
                "In Proceedings of ACL, 1999. [22] G. Salton and M. McGill.",
                "Introduction to Modern Information Retrieval.",
                "McGraw Hill, 1983. [23] B.",
                "Sun, Q. Tan, P. Mitra, and C. L. Giles.",
                "Extraction and search of chemical formulae in text documents on the web.",
                "In Proceedings of WWW, 2007. [24] B.",
                "Sun, D. Zhou, H. Zha, and J.",
                "Yen.",
                "Multi-task text segmentation and alignment based on weighted mutual information.",
                "In Proceedings of CIKM, 2006. [25] M. Utiyama and H. Isahara.",
                "A statistical model for domain-independent text segmentation.",
                "In Proceedings of the 39th ACL, 1999. [26] C. Wayne.",
                "Multilingual topic detection and tracking: Successful research enabled by corpora and evaluation.",
                "In Proceedings of LREC, 2000. [27] J. Yamron, I. Carp, L. Gillick, S. Lowe, and P. van Mulbregt.",
                "A hidden markov model approach to text segmentation and event tracking.",
                "In Proceedings of ICASSP, 1998. [28] H. Zha and X. Ji.",
                "Correlating multilingual documents via bipartite graph modeling.",
                "In Proceedings of SIGIR, 2002."
            ],
            "original_annotated_samples": [
                "<br>single document</br> segmentation without term clustering and term weight estimation (|D| = 1, k = l, w = 0) only requires Stage 1 (Step 1)."
            ],
            "translated_annotated_samples": [
                "La segmentación de un solo documento sin agrupamiento de términos y estimación de peso de términos (|D| = 1, k = l, w = 0) solo requiere la Etapa 1 (Paso 1)."
            ],
            "translated_text": "La segmentación de temas con detección de temas compartidos y alineación de múltiples documentos Bingjun Sun*, Prasenjit Mitra*†, Hongyuan Zha‡, C. Lee Giles*†, John Yen*† *Departamento de Ciencias de la Computación e Ingeniería † Colegio de Ciencias de la Información y Tecnología Universidad Estatal de Pensilvania University Park, PA 16802 ‡ Colegio de Computación Instituto de Tecnología de Georgia Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu RESUMEN La detección y seguimiento de temas [26] y la segmentación de temas [15] juegan un papel importante en la captura de la información local y secuencial de los documentos. El trabajo previo en esta área suele centrarse en documentos individuales, aunque existen múltiples documentos similares en muchos dominios. En este artículo, presentamos un novedoso método no supervisado para la detección de temas compartidos y la segmentación de temas de múltiples documentos similares basado en la información mutua (MI) y la información mutua ponderada (WMI), que es una combinación de MI y pesos de términos. La idea básica es que la segmentación óptima maximiza MI (o WMI). Nuestro enfoque puede detectar temas compartidos entre documentos. Puede encontrar los límites óptimos en un documento y alinear segmentos entre documentos al mismo tiempo. También puede manejar la segmentación de un solo documento como un caso especial de la segmentación y alineación de varios documentos. Nuestros métodos pueden identificar y fortalecer términos de señal que pueden ser utilizados para la segmentación y eliminar parcialmente las palabras de parada mediante el uso de pesos de términos basados en la entropía aprendida de múltiples documentos. Nuestros resultados experimentales muestran que nuestro algoritmo funciona bien para las tareas de segmentación de un solo documento, detección de temas compartidos y segmentación de múltiples documentos. El uso de información de múltiples documentos puede mejorar enormemente el rendimiento de la segmentación de temas, y utilizar WMI es aún mejor que utilizar MI para la segmentación de múltiples documentos. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información-Agrupación; H.3.1 [Almacenamiento y Recuperación de Información]: Análisis de Contenido e Indexación-Procesamiento Lingüístico; I.2.7 [Inteligencia Artificial]: Procesamiento de Lenguaje Natural-Análisis de Texto; I.5.3 [Reconocimiento de Patrones]: Algoritmos de Agrupación-Medidas de Similitud Términos Generales Algoritmos, Diseño, Experimentación 1. INTRODUCCIÓN Muchos investigadores han trabajado en la detección y seguimiento de temas (TDT) [26] y la segmentación de temas durante la última década. La segmentación de temas tiene como objetivo identificar los límites en un documento con la meta de capturar la estructura temática latente. Las tareas de segmentación de temas generalmente se dividen en dos categorías [15]: la segmentación de flujos de texto, donde se identifica la transición de temas, y la segmentación coherente de documentos, en la que los documentos se dividen en subtemas. La primera categoría tiene aplicaciones en el reconocimiento automático del habla, mientras que la segunda tiene más aplicaciones como la consulta parcial de texto en documentos extensos en la recuperación de información, resumen de texto y medición de calidad de múltiples documentos. La investigación previa en relación con TDT se enmarca en la primera categoría, centrada en el seguimiento de temas de datos de discurso de transmisión y texto de agencia de noticias, mientras que la segunda categoría no ha sido estudiada muy bien. Los enfoques tradicionales realizan la segmentación de temas en los documentos uno a la vez [15, 25, 6]. La mayoría de ellos tienen un mal desempeño en tareas sutiles como la segmentación coherente de documentos [15]. A menudo, los usuarios finales buscan documentos que tengan un contenido similar. Los motores de búsqueda, como Google, proporcionan enlaces para obtener páginas similares. A una granularidad más fina, los usuarios pueden estar buscando obtener secciones de un documento similares a una sección en particular que presumiblemente discute un tema de interés para los usuarios. Por lo tanto, la extensión de la segmentación de temas desde documentos individuales para identificar segmentos similares de múltiples documentos similares con el mismo tema es una dirección natural y necesaria, y se espera que la segmentación de temas de múltiples documentos tenga un mejor rendimiento ya que se utiliza más información. Los enfoques tradicionales que utilizan la medición de similitud basada en la frecuencia de términos generalmente parten del mismo supuesto de que un vocabulario similar tiende a estar en un segmento de tema coherente [15, 25, 6]. Sin embargo, suelen tener el problema de identificar las palabras de parada. Por ejemplo, se eliminan las palabras de parada dependientes del documento junto con las palabras de parada genéricas en [15]. Hay dos razones por las que no eliminamos las palabras de parada directamente. Primero, identificar las palabras vacías es otro problema [12] que requiere estimación en cada dominio. Eliminar palabras comunes de parada puede resultar en la pérdida de información útil en un dominio específico. Segundo, aunque se pueden identificar las palabras de parada, la clasificación estricta de palabras de parada y palabras no de parada no puede representar el cambio gradual en la cantidad de contenido informativo de cada palabra. Empleamos una clasificación suave utilizando pesos de términos. En este documento, consideramos el problema de la segmentación de temas como un problema de optimización utilizando técnicas de teoría de la información para encontrar los límites óptimos de un documento dado el número de segmentos de texto, de manera que se minimice la pérdida de información mutua (MI) (o información mutua ponderada (WMI)) después de la segmentación y alineación. Esto es igual a maximizar el MI (o WMI). El MI se centra en medir la diferencia entre segmentos, mientras que investigaciones anteriores se enfocaban en encontrar la similitud (por ejemplo, la distancia del coseno) entre segmentos [15, 25, 6]. La alineación de temas de múltiples documentos similares se puede lograr agrupando las oraciones sobre el mismo tema en el mismo clúster. La segmentación de temas en un solo documento es solo un caso especial del problema de segmentación y alineación de temas en múltiples documentos. Los términos pueden ser co-agrupados como en [10] al mismo tiempo, dado el número de agrupaciones, pero nuestros resultados experimentales muestran que este método resulta en una peor segmentación (ver Tablas 1, 4 y 6). Por lo general, los lectores humanos pueden identificar la transición de temas basándose en palabras de señal, y pueden ignorar las palabras vacías. Inspirados por esto, asignamos a cada término (o grupo de términos) un peso basado en la entropía entre diferentes documentos y diferentes segmentos de documentos. No solo este enfoque puede aumentar la contribución de las palabras clave, sino que también puede disminuir el efecto de las palabras de parada comunes, palabras ruidosas y palabras de parada dependientes del documento. Estas palabras son comunes en un documento. Muchos métodos basados en la similitud de oraciones requieren que estas palabras sean eliminadas antes de que se pueda realizar la segmentación de temas [15]. Nuestros resultados en la Figura 3 muestran que los pesos de los términos son útiles para la segmentación y alineación de temas en múltiples documentos. La principal contribución de este artículo es que introduce un método novedoso para la segmentación de temas utilizando MI y demuestra que este método funciona mejor que los criterios previamente utilizados. Además, hemos abordado el problema de la segmentación y alineación de temas en varios documentos, mientras que la mayoría de las investigaciones existentes se centraron en la segmentación de documentos individuales. La segmentación y alineación de múltiples documentos pueden utilizar información de documentos similares y mejorar significativamente el rendimiento de la segmentación de temas. Obviamente, nuestro enfoque puede manejar documentos individuales como un caso especial cuando varios documentos no están disponibles. Puede detectar temas compartidos entre documentos para determinar si son múltiples documentos sobre el mismo tema. También introducimos el nuevo criterio de WMI basado en pesos de términos aprendidos de múltiples documentos similares, lo cual puede mejorar aún más el rendimiento de la segmentación de temas. Proponemos un algoritmo voraz iterativo basado en programación dinámica y demostramos que funciona bien en la práctica. Algunos de nuestros trabajos anteriores están en [24]. El resto de este documento está organizado de la siguiente manera: En la Sección 2, revisamos el trabajo relacionado. La sección 3 contiene una formulación del problema de segmentación de temas y alineación de múltiples documentos con co-clustering de términos, una revisión del criterio de MI para el clustering, y finalmente una introducción a WMI. En la Sección 4, primero proponemos el algoritmo voraz iterativo de segmentación y alineación de temas con co-clustering de términos, y luego describimos cómo el algoritmo puede ser optimizado mediante programación dinámica. En la Sección 5, se describen experimentos sobre la segmentación de documentos individuales, la detección de temas compartidos y la segmentación de documentos múltiples, y se presentan y discuten los resultados para evaluar el rendimiento de nuestro algoritmo. Las conclusiones y algunas direcciones futuras del trabajo de investigación se discuten en la Sección 6.2. TRABAJO PREVIO En general, los enfoques existentes para la segmentación de texto se dividen en dos categorías: aprendizaje supervisado [19, 17, 23] y aprendizaje no supervisado [3, 27, 5, 6, 15, 25, 21]. El aprendizaje supervisado suele tener un buen rendimiento, ya que aprende funciones a partir de conjuntos de entrenamiento etiquetados. Sin embargo, obtener conjuntos de entrenamiento grandes con etiquetas manuales en oraciones de documentos suele ser prohibitivamente costoso, por lo que se prefieren enfoques no supervisados. Algunos modelos consideran la dependencia entre oraciones y secciones, como el Modelo Oculto de Markov [3, 27], el Modelo de Máxima Entropía de Markov [19] y los Campos Aleatorios Condicionales [17], mientras que muchos otros enfoques se basan en la cohesión léxica o la similitud de las oraciones [5, 6, 15, 25, 21]. Algunos enfoques también se centran en las palabras clave como pistas de transiciones de tema [11]. Si bien algunos métodos existentes solo consideran información en documentos individuales [6, 15], otros utilizan varios documentos [16, 14]. No hay muchas obras en la última categoría, aunque se espera que el rendimiento de la segmentación sea mejor con la utilización de información de múltiples documentos. Investigaciones previas estudiaron métodos para encontrar temas compartidos [16] y segmentación y resumen de temas entre solo un par de documentos [14]. La clasificación y agrupación de textos es un área de investigación relacionada que categoriza documentos en grupos utilizando métodos supervisados o no supervisados. La clasificación o agrupación temática es una dirección importante en esta área, especialmente el co-agrupamiento de documentos y términos, como LSA [9], PLSA [13], y enfoques basados en distancias y particionamiento de gráficos bipartitos [28] o máxima MI [2, 10], o máxima entropía [1, 18]. Los criterios de estos enfoques pueden ser utilizados en el tema de la segmentación de temas. Algunos de esos métodos se han extendido al área de la segmentación de temas, como PLSA [5] y máxima entropía [7], pero hasta donde sabemos, el uso de MI para la segmentación de temas no ha sido estudiado. 3. FORMULACIÓN DEL PROBLEMA Nuestro objetivo es segmentar documentos y alinear los segmentos entre documentos (Figura 1). Sea T el conjunto de términos {t1, t2, ..., tl}, que aparecen en el conjunto no etiquetado de documentos D = {d1, d2, ..., dm}. Sea Sd el conjunto de oraciones para el documento d ∈ D, es decir, {s1, s2, ..., snd}. Tenemos una matriz tridimensional de frecuencia de términos, en la que las tres dimensiones son variables aleatorias de D, Sd y T. Sd en realidad es un vector aleatorio que incluye una variable aleatoria para cada d ∈ D. La frecuencia de términos se puede utilizar para estimar la distribución de probabilidad conjunta P(D, Sd, T), que es p(t, d, s) = T(t, d, s)/ND, donde T(t, d, s) es el número de t en la oración s de d y ND es el número total de términos en D. ˆS representa el conjunto de segmentos {ˆs1, ˆs2, ..., ˆsp} después de la segmentación y alineación entre varios documentos, donde el número de segmentos | ˆS| = p. Un segmento ˆsi del documento d es una secuencia de oraciones adyacentes en d. Dado que para diferentes documentos si pueden discutir diferentes subtemas, nuestro objetivo es agrupar oraciones adyacentes en cada documento en segmentos, y alinear segmentos similares entre documentos, de modo que para diferentes documentos ˆsi trate sobre el mismo subtema. El objetivo es encontrar la segmentación óptima de temas y el mapeo de alineación Segd(si) : {s1, s2, ..., snd } → {ˆs1, ˆs2, ..., ˆsp} y Alid(ˆsi) : {ˆs1, ˆs2, ..., ˆsp} → {ˆs1, ˆs2, ..., ˆsp}, para todo d ∈ D, donde ˆsi es el i-ésimo segmento con la restricción de que solo las oraciones adyacentes pueden ser asignadas al mismo segmento, es decir, para d, {si, si+1, ..., sj} → {ˆsq}, donde q ∈ {1, ..., p}, donde p es el número de segmento, y si i > j, entonces para d, ˆsq está ausente. Después de la segmentación y alineación, el vector aleatorio Sd se convierte en una variable aleatoria alineada ˆS. Por lo tanto, P(D, Sd, T) se convierte en P(D, ˆS, T). El término co-clustering es una técnica que se ha empleado [10] para mejorar la precisión del agrupamiento de documentos. Evaluamos el efecto de ello para la segmentación de temas. Un término t se asigna a exactamente un grupo de términos. El término co-clustering implica encontrar simultáneamente el mapeo óptimo de agrupación de términos Clu(t) : {t1, t2, ..., tl} → {ˆt1, ˆt2, ..., ˆtk}, donde k ≤ l, l es el número total de palabras en todos los documentos, y k es el número de clústeres. 4. METODOLOGÍA Ahora describimos un algoritmo novedoso que puede manejar la segmentación de un solo documento, la detección de temas compartidos y la segmentación y alineación de múltiples documentos basados en MI o WMI. 4.1 Información Mutua MI I(X; Y) es una cantidad que mide la cantidad de información contenida en dos o más variables aleatorias [8, 10]. Para el caso de dos variables aleatorias, tenemos I(X; Y) = x∈X y∈Y p(x, y)log p(x, y) p(x)p(y) , (1) Obviamente, cuando las variables aleatorias X e Y son independientes, I(X; Y) = 0. Por lo tanto, de manera intuitiva, el valor de MI depende de cómo las variables aleatorias están relacionadas entre sí. La co-agrupación óptima es el mapeo Clux: X → ˆX y Cluy: Y → ˆY que minimiza la pérdida: I(X; Y) - I(ˆX; ˆY), lo cual es igual a maximizar I(ˆX; ˆY). Este es el criterio de MI para la agrupación. En el caso de la segmentación de temas, las dos variables aleatorias son la variable de término T y la variable de segmento S, y cada muestra es una ocurrencia de un término T = t en un segmento particular S = s. Se utiliza I(T; S) para medir qué tan dependientes son T y S. Sin embargo, no se puede calcular I(T; S) para documentos antes de la segmentación, ya que no tenemos un conjunto de S debido a que las oraciones del Documento d, si ∈ Sd, no están alineadas con otros documentos. Así, en lugar de minimizar la pérdida de MI, podemos maximizar MI después de la segmentación de temas, calculada como: I( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS p(ˆt, ˆs)log p(ˆt, ˆs) p(ˆt)p(ˆs) , (2) donde p(ˆt, ˆs) se estiman mediante la frecuencia de términos tf del Cluster de Términos ˆt y el Segmento ˆs en el conjunto de entrenamiento D. Nótese que aquí un segmento ˆs incluye oraciones sobre el mismo tema entre todos los documentos. La solución óptima es el mapeo Clut : T → ˆT, Segd : Sd → ˆS, y Alid : ˆS → ˆS, que maximiza I( ˆT; ˆS). 4.2 Información Mutua Ponderada En la segmentación de temas y alineación de múltiples documentos, si se conoce P(D, ˆS, T), basado en las distribuciones marginales P(D|T) y P( ˆS|T) para cada término t ∈ T, podemos categorizar los términos en cuatro tipos en el conjunto de datos: • Las palabras de parada comunes son comunes a lo largo de las dimensiones de documentos y segmentos. • Las palabras de parada dependientes del documento que dependen del estilo de escritura personal son comunes solo a lo largo de la dimensión de segmentos para algunos documentos. • Las palabras clave son los elementos más importantes para la segmentación. Son comunes a lo largo de la dimensión de los documentos solo para el mismo segmento, y no son comunes a lo largo de las dimensiones de los segmentos. • Las palabras ruidosas son otras palabras que no son comunes a lo largo de ambas dimensiones. La entropía basada en P(D|T) y P( ˆS|T) se puede utilizar para identificar diferentes tipos de términos. Para reforzar la contribución de las palabras clave en el cálculo de MI, y al mismo tiempo reducir el efecto de los otros tres tipos de palabras, similar a la idea del peso tf-idf [22], utilizamos las entropías de cada término a lo largo de las dimensiones del documento D y del segmento ˆS, es decir, ED(ˆt) y EˆS(ˆt), para calcular el peso. Una palabra de señal generalmente tiene un valor alto de ED(ˆt) pero un valor bajo de EˆS(ˆt). Introducimos los pesos de términos (o pesos de grupos de términos) wˆt = ( ED(ˆt) maxˆt ∈ ˆT (ED(ˆt )) )a (1 − EˆS(ˆt) maxˆt ∈ ˆT (EˆS(ˆt )) )b , (3) donde ED(ˆt) = d∈D p(d|ˆt)log|D| 1 p(d|ˆt) , EˆS(ˆt) = ˆs∈ ˆS p(ˆs|ˆt)log| ˆS| 1 p(ˆs|ˆt) , y a > 0 y b > 0 son potencias para ajustar los pesos de términos. Normalmente a = 1 y b = 1 por defecto, y se utilizan maxˆt ∈ ˆT (ED(ˆt )) y maxˆt ∈ ˆT (EˆS(ˆt )) para normalizar los valores de entropía. Los pesos de los grupos de términos se utilizan para ajustar p(ˆt, ˆs), pw(ˆt, ˆs) = wˆtp(ˆt, ˆs) ˆt∈ ˆT ;ˆs∈ ˆS wˆtp(ˆt, ˆs) , (4) e Iw( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS pw(ˆt, ˆs)log pw(ˆt, ˆs) pw(ˆt)pw(ˆs) , (5) donde pw(ˆt) y pw(ˆs) son distribuciones marginales de pw(ˆt, ˆs). Sin embargo, dado que no conocemos ni los pesos de los términos ni P(D, ˆS, T), necesitamos estimarlos, pero wˆt depende de p(ˆs|t) y ˆS, mientras que ˆS y p(ˆs|t) también dependen de wˆt, que aún es desconocido. Por lo tanto, se requiere un algoritmo iterativo para estimar los pesos de los términos wˆt y encontrar la mejor segmentación y alineación para optimizar la función objetivo Iw de manera concurrente. Después de que un documento se segmenta en oraciones, se introduce: la distribución de probabilidad conjunta P(D, Sd, T), el número de segmentos de texto p ∈ {2, 3, ..., máx(sd)}, el número de grupos de términos k ∈ {2, 3, ..., l} (si k = l, no se requiere co-clustering de términos) y el tipo de peso w ∈ {0, 1}, indicando usar I o Iw, respectivamente. Mapeo de Clu, Seg, Ali y pesos de términos wˆt. Inicialización: 0. i = 0. Inicializar Clu (0) t, Seg (0) d y Ali (0) d; Inicializar w (0) ˆt usando la Ecuación (6) si w = 1; Etapa 1: 1. Si |D| = 1, k = l y w = 0, verifica todas las segmentaciones secuenciales de d en p segmentos y encuentra la mejor Segd(s) = argmaxˆsI( ˆT; ˆS), y devuelve Segd; de lo contrario, si w = 1 y k = l, ve a 3.1; Etapa 2: 2.1 Si k < l, para cada término t, encuentra el mejor clúster ˆt como Clu(i+1)(t) = argmaxˆtI( ˆT; ˆS(i)) basado en Seg(i) y Ali(i); 2.2 Para cada d, verifica todas las segmentaciones secuenciales de d en p segmentos con mapeo s → ˆs → ˆs, y encuentra la mejor Ali (i+1) d (Seg (i+1) d (s)) = argmaxˆsI( ˆT(i+1); ˆS) basado en Clu(i+1)(t) si k < l o Clu(0)(t) si k = l; 2.3 i + +. Si Clu, Seg o Ali cambian, ve a 2.1; de lo contrario, si w = 0, devuelve Clu(i), Seg(i) y Ali(i); de lo contrario, j = 0, ve a 3.1; Etapa 3: 3.1 Actualiza w (i+j+1) ˆt basado en Seg(i+j), Ali(i+j) y Clu(i) usando la Ecuación (3); 3.2 Para cada d, verifica todas las segmentaciones secuenciales de d en p segmentos con mapeo s → ˆs → ˆs, y encuentra la mejor Ali (i+j+1) d (Seg (i+j+1) d (s)) = argmaxˆsIw( ˆT(i); ˆS) basado en Clu(i) y w (i+j+1) ˆt; 3.3 j + +. Si Iw( ˆT; ˆS) cambia, ve al paso 6; de lo contrario, detente y devuelve Clu(i), Seg(i+j), Ali(i+j), y w (i+j) ˆt; Figura 2: Algoritmo: Segmentación y alineación de temas basada en MI o WMI. y cada oración se segmenta en palabras, cada palabra se reduce a su raíz. Entonces se puede estimar la distribución de probabilidad conjunta P(D, Sd, T). Finalmente, esta distribución se puede utilizar para calcular la MI en nuestro algoritmo. 4.3 Algoritmo Voraz Iterativo Nuestro objetivo es maximizar la función objetivo, I( ˆT; ˆS) o Iw( ˆT; ˆS), que puede medir la dependencia de las ocurrencias de términos en diferentes segmentos. Generalmente, primero no conocemos los pesos estimados de los términos, los cuales dependen de la segmentación y alineación óptimas de los temas, y los grupos de términos. Además, este problema es NP-duro [10], incluso si conocemos los pesos de los términos. Por lo tanto, se desea un algoritmo voraz iterativo para encontrar la mejor solución, aunque probablemente solo se alcancen máximos locales. Presentamos el algoritmo voraz iterativo en la Figura 2 para encontrar un máximo local de I( ˆT; ˆS) o Iw( ˆT; ˆS) con estimación simultánea de pesos de términos. Este algoritmo es iterativo y codicioso para casos de múltiples documentos o casos de un solo documento con estimación de peso de términos y/o co-clustering de términos. De lo contrario, dado que es solo un algoritmo de un paso para resolver la tarea de segmentación de un solo documento [6, 15, 25], se garantiza el máximo global de MI. Mostraremos más adelante que la coagrupación de términos reduce la precisión de los resultados y no es necesaria, y para la segmentación de un solo documento, los pesos de los términos tampoco son requeridos. 4.3.1 Inicialización En el Paso 0, la agrupación inicial de términos Clut y la segmentación y alineación de temas Segd y Alid son importantes para evitar máximos locales y reducir el número de iteraciones. Primero, se puede hacer una buena estimación de los pesos de los términos utilizando las distribuciones de frecuencia de términos a lo largo de las oraciones para cada documento y promediándolas para obtener los valores iniciales de wˆt: wt = ( ED(t) maxt ∈T (ED(t )) )(1 − ES(t) maxt ∈T (ES(t )) ), (6) donde ES(t) = 1 |Dt| d∈Dt (1 − s∈Sd p(s|t)log|Sd| 1 p(s|t) ), donde Dt es el conjunto de documentos que contienen el término t. Luego, para la segmentación inicial Seg(0), simplemente podemos segmentar los documentos de manera equitativa por oraciones. O podemos encontrar la segmentación óptima solo para cada documento d que maximice el WMI, Seg (0) d = argmaxˆsIw(T; ˆS), donde w = w (0) ˆt. Para el alineamiento inicial Ali(0), podemos primero asumir que el orden de los segmentos para cada d es el mismo. Para el agrupamiento inicial de términos Clu(0), las etiquetas de los primeros grupos pueden establecerse al azar, y después de la primera vez del Paso 3, se obtiene un buen agrupamiento inicial de términos. 4.3.2 Diferentes Casos Después de la inicialización, hay tres etapas para diferentes casos. En total hay ocho casos, |D| = 1 o |D| > 1, k = l o k < l, w = 0 o w = 1. La segmentación de un solo documento sin agrupamiento de términos y estimación de peso de términos (|D| = 1, k = l, w = 0) solo requiere la Etapa 1 (Paso 1). Si se requiere el agrupamiento de términos (k < l), la Etapa 2 (Paso 2.1, 2.2 y 2.3) se ejecuta de forma iterativa. Si se requiere la estimación del peso del término (w = 1), la Etapa 3 (Paso 3.1, 3.2 y 3.3) se ejecuta de forma iterativa. Si ambos son requeridos (k < l, w = 1), la Etapa 2 y 3 se ejecutan una tras otra. Para la segmentación de múltiples documentos sin agrupamiento de términos y estimación de peso de términos (|D| > 1, k = l, w = 0), solo se requiere la iteración de los Pasos 2.2 y 2.3. En la Etapa 1, el máximo global se puede encontrar basándose en I( ˆT; ˆS) utilizando programación dinámica en la Sección 4.4. Es imposible encontrar simultáneamente un buen agrupamiento de términos y pesos estimados de términos, ya que al mover un término a un nuevo grupo de términos para maximizar Iw( ˆT; ˆS), no sabemos si el peso de este término debería ser el del nuevo grupo o el del antiguo. Por lo tanto, primero realizamos el agrupamiento de términos en la Etapa 2, y luego estimamos los pesos de los términos en la Etapa 3. En la Etapa 2, el Paso 2.1 consiste en encontrar el mejor agrupamiento de términos y el Paso 2.2 consiste en encontrar la mejor segmentación. Este ciclo se repite para encontrar un máximo local basado en MI I hasta que converge. Los dos pasos son: (1) basados en el agrupamiento de términos actual Cluˆt, para cada documento d, el algoritmo segmenta todas las oraciones Sd en p segmentos secuencialmente (algunos segmentos pueden estar vacíos), y los coloca en los p segmentos ˆS del conjunto de entrenamiento completo D (se verifican todos los casos posibles de segmentación diferente Segd y alineación Alid) para encontrar el caso óptimo, y (2) basado en la segmentación y alineación actual, para cada término t, el algoritmo encuentra el mejor grupo de términos de t basado en la segmentación actual Segd y la alineación Alid. Después de encontrar un buen agrupamiento de términos, se estiman los pesos de los términos si w = 1. En la Etapa 3, al igual que en la Etapa 2, el Paso 3.1 es la reestimación del peso del término y el Paso 3.2 es encontrar una mejor segmentación. Se repiten para encontrar un máximo local basado en WMI Iw hasta que converja. Sin embargo, si el agrupamiento en la Etapa 2 no es preciso, entonces la estimación de peso en la Etapa 3 puede tener un mal resultado. Finalmente, en el Paso 3.3, este algoritmo converge y devuelve la salida. Este algoritmo puede manejar tanto la segmentación de un solo documento como la de múltiples documentos. También puede detectar temas compartidos entre documentos al verificar la proporción de frases superpuestas sobre los mismos temas, como se describe en la Sección 5.2. 4.4 Optimización del algoritmo. En muchos trabajos anteriores sobre segmentación, la programación dinámica es una técnica utilizada para maximizar la función objetivo. De manera similar, en los Pasos 1, 2.2 y 3.2 de nuestro algoritmo, podemos utilizar programación dinámica. Para la Etapa 1, utilizando programación dinámica aún se puede encontrar el óptimo global, pero para la Etapa 2 y la Etapa 3, solo podemos encontrar el óptimo para cada paso de segmentación de temas y alineación de un documento. Aquí solo mostramos la programación dinámica para el Paso 3.2 utilizando WMI (el Paso 1 y 2.2 son similares pero pueden usar I o Iw). Hay dos casos que no se muestran en el algoritmo de la Figura 2: (a) segmentación de un solo documento o segmentación de múltiples documentos con el mismo orden secuencial de segmentos, donde no se requiere alineación, y (b) segmentación de múltiples documentos con diferentes órdenes secuenciales de segmentos, donde la alineación es necesaria. La función de mapeo de alineación del primer caso es simplemente Alid(ˆsi) = ˆsi, mientras que para los últimos casos la función de mapeo de alineación es Alid(ˆsi) = ˆsj, donde i y j pueden ser diferentes. Los pasos computacionales para los dos casos se enumeran a continuación: Caso 1 (sin alineación): Para cada documento d: (1) Calcular pw(ˆt), pw parcial(ˆt, ˆs) y pw parcial(ˆs) sin contar las oraciones de d. Luego, colocar las oraciones de i a j en la Parte k, y calcular WMI parcial PIw( ˆT; ˆsk(si, si+1, ..., sj)) ˆt∈ ˆT pw(ˆt, ˆsk)log pw(ˆt, ˆsk) pw(ˆt)pw(ˆsk) , donde Alid(si, si+1, ..., sj) = k, k ∈ {1, 2, ..., p}, 1 ≤ i ≤ j ≤ nd, y Segd(sq) = ˆsk para todo i ≤ q ≤ j. (2) Dejar que M(sm, 1) = PIw( ˆT; ˆs1(s1, s2, ..., sm)). Entonces M(sm, L) = maxi[M(si−1, L − 1) + PIw( ˆT; ˆsL(si, ..., sm))], donde 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, y cuando i > m, no se colocan oraciones en ˆsk al calcular PIw (nota PIw( ˆT; ˆs(si, ..., sm)) = 0 para la segmentación de un solo documento). (3) Finalmente M(snd , p) = maxi[M(si−1, p − 1)+ PIw( ˆT; ˆsp(si, ..., snd ))], donde 1 ≤ i ≤ nd+1. Se encuentra el Iw óptimo y la segmentación correspondiente es la mejor. Caso 2 (alineación requerida): Para cada documento d: (1) Calcular pw(ˆt), pw parcial(ˆt, ˆs), y pw parcial(ˆs), y PIw( ˆT; ˆsk(si, si+1, ..., sj)) de manera similar al Caso 1. (2) Sea M(sm, 1, k) = PIw( ˆT; ˆsk(s1, s2, ..., sm)), donde k ∈ {1, 2, ..., p}. Entonces M(sm, L, kL) = maxi,j[M(si−1, L − 1, kL/j) + PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., sm))], donde 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, kL ∈ Set(p, L), que es el conjunto de todos los p! Hay L!(p−L)! combinaciones de L segmentos elegidos de entre todos los p segmentos, j ∈ kL, el conjunto de L segmentos elegidos de entre todos los p segmentos, y kL/j es la combinación de L − 1 segmentos en kL excepto el Segmento j. (3) Finalmente, M(snd , p, kp) = maxi,j[M(si−1, p − 1, kp/j) +PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., snd ))], donde kp es simplemente la combinación de todos los p segmentos y 1 ≤ i ≤ nd + 1, que es el óptimo Iw y la segmentación correspondiente es la mejor. Los pasos de los Casos 1 y 2 son similares, excepto que en el Caso 2, se considera el alineamiento además de la segmentación. Primero, se calculan los elementos básicos de probabilidad para computar Iw excluyendo el documento d, y luego se calcula el WMI parcial colocando cada segmento secuencial posible (incluido el segmento vacío) de d en cada segmento del conjunto. Segundo, se encuentra la suma óptima de PIw para L segmentos y las m oraciones más a la izquierda, M(sm, L). Finalmente, se encuentra el WMI máximo entre diferentes sumas de M(sm, p − 1) y PIw para el Segmento p. 5. EXPERIMENTOS En esta sección se probará la segmentación de un solo documento, la detección de temas compartidos y la segmentación de múltiples documentos. Se estudian diferentes hiperparámetros de nuestro método. Para mayor comodidad, nos referimos al método utilizando I como MIk si w = 0, e Iw como WMIk si w = 2 o como WMIk si w = 1, donde k es el número de grupos de términos, y si k = l, donde l es el número total de términos, entonces no se requiere agrupación de términos, es decir, El primer conjunto de datos que probamos es uno sintético utilizado en investigaciones anteriores [6, 15, 25] y muchos otros artículos. Tiene 700 muestras. Cada uno es una concatenación de diez segmentos. Cada segmento es la primera oración seleccionada al azar de las n primeras oraciones del corpus Brown, que se supone que tienen un tema diferente entre sí. Actualmente, los mejores resultados en este conjunto de datos son logrados por Ji et al. [15]. Para comparar el rendimiento de nuestros métodos, se aplica el criterio ampliamente utilizado en investigaciones anteriores en lugar del criterio imparcial introducido en [20]. Elige un par de palabras al azar. Si se encuentran en segmentos diferentes (diferentes) para la segmentación real (real), pero se predicen (pred) como en el mismo segmento, es un error. Si están en el mismo segmento (mismo), pero se predice que están en segmentos diferentes, es una falsa alarma. Por lo tanto, la tasa de error se calcula utilizando la siguiente ecuación: p(err|real, pred) = p(miss|real, pred, diff)p(diff|real) + p(false alarm|real, pred, same)p(same|real). 5.1.2 Resultados del Experimento Probamos el caso cuando se conoce el número de segmentos. La Tabla 1 muestra los resultados de nuestros métodos con diferentes valores de hiperparámetros y tres enfoques anteriores, C99[25], U00[6] y ADDP03[15], en este conjunto de datos cuando se conoce el número de segmento. En WMI para la segmentación de un solo documento, los pesos de los términos se calculan de la siguiente manera: wˆt = 1−EˆS(ˆt)/maxˆt ∈ ˆT (EˆS(ˆt)). Para este caso, nuestros métodos MIl y WMIl superan a todos los enfoques anteriores. Comparamos nuestros métodos con ADDP03 utilizando una prueba t de una muestra unilateral y los valores de p se muestran en la Tabla 2. A partir de los valores p, podemos ver que en su mayoría las diferencias son muy significativas. También comparamos las tasas de error entre nuestros dos métodos utilizando una prueba t de dos muestras de dos colas para verificar la hipótesis de que son iguales. No podemos rechazar la hipótesis de que son iguales, por lo que las diferencias no son significativas, a pesar de que todas las tasas de error para MIl son más pequeñas que las de WMIl. Sin embargo, podemos concluir que los pesos de los términos contribuyen poco en la segmentación de un solo documento. Los resultados también muestran que el uso de MI con co-clustering de términos (k = 100) disminuye el rendimiento. Probamos diferentes números de grupos de términos y encontramos que el rendimiento mejora cuando el número de grupos aumenta hasta llegar a l. WMIk<l tiene resultados similares que no mostramos en la tabla. Como se mencionó anteriormente, el uso de MI puede ser inconsistente en los límites óptimos dados diferentes números de segmentos. Esta situación ocurre especialmente cuando las similitudes entre los segmentos son bastante diferentes, es decir, algunas transiciones son muy obvias, mientras que otras no lo son. Esto se debe a que normalmente un documento es una estructura jerárquica en lugar de una estructura únicamente secuencial. Cuando los segmentos no están en el mismo nivel, esta situación puede ocurrir. Por lo tanto, se desea un enfoque de segmentación jerárquica de temas, y la estructura depende en gran medida del número de segmentos para cada nodo interno y del criterio de parada de la división. Para este conjunto de datos de segmentación de un solo documento, dado que es solo un conjunto sintético, que es simplemente una concatenación de varios segmentos sobre diferentes temas, es razonable que enfoques simplemente basados en la frecuencia de términos tengan un buen rendimiento. Normalmente, para las tareas de segmentación de documentos coherentes en subtemas, la efectividad disminuye mucho. 5.2 Detección de Temas Compartidos 5.2.1 Datos de Prueba y Evaluación El segundo conjunto de datos contiene 80 artículos de noticias de Google News. Hay ocho temas y cada uno tiene 10 artículos. Dividimos aleatoriamente el conjunto en subconjuntos con diferentes números de documentos y cada subconjunto tiene los ocho temas. Comparamos nuestro enfoque MIl y WMIl con LDA [4]. LDA trata un documento en el conjunto de datos como una bolsa de palabras, encuentra su distribución en temas y su tema principal. MIl y WMIl ven cada oración como un conjunto de palabras y la etiquetan con una etiqueta de tema. Luego, para cada par de documentos, LDA determina si están en el mismo tema, mientras que MIl y Table 3: Detección de Temas Compartidos: Tasas de Error Promedio para Diferentes Números de Documentos en Cada Subconjunto #Doc 10 20 40 80 LDA 8.89% 16.33% 1.35% 0.60% MIl, θ = 0.6 4.17% 1.71% 1.47% 0.0% WMIl, θ = 0.8 18.6% 3.16% 1.92% 0.0% WMIl comprueba si la proporción de oraciones superpuestas en el mismo tema es mayor que el umbral ajustable θ. Es decir, en MIl y WMIl, para un par de documentos d, d , si [ s∈Sd,s ∈Sd 1(temas=temas )/min(|Sd|, |Sd|)] > θ, donde Sd es el conjunto de oraciones de d, y |Sd| es el número de oraciones de d, entonces d y d comparten el tema. Para un par de documentos seleccionados al azar, la tasa de error se calcula utilizando la siguiente ecuación: p(err|real, pred) = p(miss|real, pred, same)p(same|real) + p(false alarm|real, pred, diff)p(diff|real), donde un error significa si tienen el mismo tema (same) para el caso real (real), pero se predice (pred) como en un tema diferente. Si están en diferentes temas (diff), pero se predicen como en el mismo tema, es una falsa alarma. 5.2.2 Resultados del Experimento Los resultados se muestran en la Tabla 3. Si la mayoría de los documentos tienen diferentes temas, en WMIl, la estimación de los pesos de términos en la Ecuación (3) no es correcta. Por lo tanto, no se espera que WMIL tenga un mejor rendimiento que MIL cuando la mayoría de los documentos tienen temas diferentes. Cuando hay menos documentos en un subconjunto con el mismo número de temas, más documentos tienen temas diferentes, por lo que WMIl es peor que MIl. Podemos ver que en la mayoría de los casos, MIl tiene un rendimiento mejor (o al menos similar) que LDA. Después de la detección de temas compartidos, se puede llevar a cabo la segmentación de documentos con los temas compartidos. 5.3 Segmentación de múltiples documentos 5.3.1 Datos de prueba y evaluación Para la segmentación y alineación de múltiples documentos, nuestro objetivo es identificar estos segmentos sobre el mismo tema entre varios documentos similares con temas compartidos. Se espera que al utilizar pesos de términos, Iw funcione mejor que I, ya que sin pesos de términos, el resultado se ve seriamente afectado por palabras de parada dependientes del documento y palabras ruidosas que dependen del estilo de escritura personal. Es más probable tratar los mismos segmentos de diferentes documentos como segmentos diferentes bajo el efecto de palabras de parada dependientes del documento y palabras ruidosas. Los pesos de términos pueden reducir el efecto de las palabras de parada dependientes del documento y las palabras ruidosas al darle más peso a los términos de señal. El conjunto de datos para la segmentación y alineación de múltiples documentos tiene un total de 102 muestras y 2264 oraciones. Cada uno es la parte de introducción de un informe de laboratorio seleccionado del curso de Biol 240W, Universidad Estatal de Pensilvania. Cada muestra tiene dos segmentos, la introducción de las hormonas vegetales y el contenido en el laboratorio. El rango de longitud de las muestras va desde dos hasta 56 oraciones. Algunas muestras solo tienen una parte y algunas tienen un orden inverso de estos dos segmentos. No es difícil identificar el límite entre dos segmentos para los humanos. Etiquetamos cada oración manualmente para evaluación. El criterio de evaluación consiste en utilizar la proporción del número de oraciones con etiquetas de segmento incorrectas predichas en el número total de oraciones en toda la Tabla de entrenamiento. Para mostrar los beneficios de la segmentación y alineación de múltiples documentos, comparamos nuestro método con diferentes parámetros en diferentes particiones del mismo conjunto de entrenamiento. Excepto los casos en los que el número de documentos es 102 y uno (que son casos especiales de uso del conjunto completo y la segmentación pura de un solo documento), dividimos aleatoriamente el conjunto de entrenamiento en m particiones, y cada una tiene 51, 34, 20, 10, 5 y 2 muestras de documentos. Luego aplicamos nuestros métodos en cada partición y calculamos la tasa de error de todo el conjunto de entrenamiento. Cada caso se repitió 10 veces para calcular las tasas de error promedio. Para diferentes particiones del conjunto de entrenamiento, se utilizan diferentes valores de k, ya que el número de términos aumenta cuando el número de documentos en cada partición aumenta. 5.3.2 Resultados del Experimento De los resultados del experimento en la Tabla 4, podemos ver las siguientes observaciones: (1) Cuando el número de documentos aumenta, todos los métodos tienen un mejor rendimiento. Solo de uno a dos documentos, el MIL ha disminuido un poco. Podemos observar esto en la Figura 3 en el punto del número de documento = 2. La mayoría de las curvas incluso tienen los peores resultados en este punto. Hay dos razones. Primero, porque las muestras votan por la mejor segmentación y alineación de múltiples documentos, pero si solo se comparan dos documentos entre sí, aquel con segmentos faltantes o una secuencia totalmente diferente afectará la correcta segmentación y alineación del otro. Segundo, como se señaló al principio de esta sección, si dos documentos tienen más palabras de parada dependientes del documento o palabras ruidosas que palabras clave, entonces el algoritmo puede considerarlos como dos segmentos diferentes y faltar el otro segmento. Generalmente, solo podemos esperar un mejor rendimiento cuando el número de documentos es mayor que el número de segmentos. (2) Excepto en la segmentación de un solo documento, WMIl siempre es mejor que MIl, y cuando el número de documentos se acerca a uno o aumenta a un número muy grande, sus rendimientos se vuelven más cercanos. La Tabla 5 muestra los valores p de la prueba t de dos muestras unilaterales entre MIl y WMIl. También podemos observar esta tendencia a partir de los valores de p. Cuando el número de documento es igual a 5, alcanzamos el valor p más pequeño y la mayor diferencia entre las tasas de error de MIl y WMIl. Para la Tabla 6 de un solo documento: Segmentación de múltiples documentos: Tasa de error promedio para el Documento Número = 5 en cada subconjunto con Diferente Número de Agrupaciones de Términos #Agrupaciones 75 100 150 250 l MIk 24.67% 24.54% 23.91% 22.59% 15.77% de segmentación, WMIl es incluso un poco peor que MIl, lo cual es similar a los resultados de la segmentación de un solo documento en el primer conjunto de datos. La razón es que para la segmentación de un solo documento, no podemos estimar con precisión los pesos de los términos, ya que no tenemos disponibles varios documentos. (3) El uso de agrupamiento de términos generalmente produce resultados peores que MIl y WMIl. (4) El uso de agrupamiento de términos en WMIk es aún peor que en MIk, ya que en WMIk los grupos de términos se encuentran primero utilizando I antes que Iw. Si los grupos de términos no son correctos, entonces los pesos de los términos se estiman peor, lo que puede llevar a que el algoritmo obtenga resultados aún peores. De los resultados también encontramos que en la segmentación y alineación de múltiples documentos, la mayoría de los documentos con segmentos faltantes y en orden inverso son identificados correctamente. La Tabla 6 ilustra los resultados del experimento para el caso de 20 particiones (cada una con cinco muestras de documentos) del conjunto de entrenamiento y la segmentación y alineación de temas utilizando MIk con diferentes números de grupos de términos k. Observa que cuando el número de grupos de términos aumenta, la tasa de error disminuye. Sin agrupamiento de términos, obtenemos el mejor resultado. No mostramos resultados para WMIk con agrupamiento de términos, pero los resultados son similares. También probamos WMIL con diferentes hiperparámetros de a y b para ajustar los pesos de los términos. Los resultados se presentan en la Figura 3. Se demostró que el caso predeterminado WMIl: a = 1, b = 1 dio los mejores resultados para diferentes particiones del conjunto de entrenamiento. Podemos observar la tendencia de que cuando el número de documentos es muy pequeño o grande, la diferencia entre MIl: a = 0, b = 0 y WMIl: a = 1, b = 1 se vuelve bastante pequeña. Cuando el número de documentos no es grande (aproximadamente de 2 a 10), todos los casos que utilizan pesos de términos tienen un mejor rendimiento que MIl: a = 0, b = 0 sin pesos de términos, pero cuando el número de documentos aumenta, los casos WMIl: a = 1, b = 0 y WMIl: a = 2, b = 1 empeoran en comparación con MIl: a = 0, b = 0. Cuando el número de documentos se vuelve muy grande, son aún peores que los casos con números de documentos pequeños. Esto significa que es muy importante encontrar una forma adecuada de estimar los pesos de los términos para el criterio de WMI. La Figura 4 muestra los pesos de los términos aprendidos de todo el conjunto de entrenamiento. Cuatro tipos de palabras se categorizan de manera aproximada aunque la transición entre ellas es sutil. La Figura 5 ilustra el cambio en la información mutua (ponderada) para MIl y WMIl. Como era de esperar, la información mutua para MIl aumenta de forma monótona con el número de pasos, mientras que WMIl no lo hace. Finalmente, MIl y WMIl son escalables, con complejidad computacional mostrada en la Figura 6. Una ventaja de nuestro enfoque basado en MI es que no es necesario eliminar las palabras vacías. Otra ventaja importante es que no hay hiperparámetros necesarios que ajustar. En la segmentación de un solo documento, el rendimiento basado en MI es aún mejor que el basado en WMI, por lo que no se requiere un hiperparámetro adicional. En la segmentación de múltiples documentos, mostramos en el experimento que a = 1 y b = 1 es lo mejor. Nuestro método otorga más peso a los términos de señal. Sin embargo, generalmente los términos o frases de señal aparecen al principio de un segmento, mientras que el final del segmento puede ser 1 2 5 10 20 34 51 102 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 Número de Documento Tasa de Error MIl :a=0,b=0 WMI l :a=1,b=1 WMI l :a=1,b=0 WMI l :a=2,b=1 Figura 3: Tasas de error para diferentes hiperparámetros de pesos de términos. 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Entropía de Documento Normalizada Entropía de Segmento Normalizada Palabras ruidosas Palabras de señal Palabras de parada comunes Palabras de parada de Doc-dep Figura 4: Pesos de términos aprendidos de todo el conjunto de entrenamiento. 0 100 200 300 400 500 600 0.06 0.08 0.1 0.12 0.14 0.16 0.18 Número de Pasos (Ponderado) Información Mutua MI l WMI l Figura 5: Cambio en la MI (ponderada) para MIl y WMIl. 0 20 40 60 80 100 120 0 200 400 600 800 1000 1200 1400 1600 1800 2000 Número de Documento Tiempo de Convergencia (seg) MI l WMI l Figura 6: Tiempo de convergencia para MIl y WMIl. muy ruidoso. Una posible solución es dar más peso a los términos al principio de cada segmento. Además, cuando la longitud de los segmentos es bastante diferente, los segmentos largos tienen frecuencias de términos mucho más altas, por lo que pueden dominar los límites de segmentación. La normalización de las frecuencias de términos en relación con la longitud del segmento puede ser útil. 6. CONCLUSIONES Y TRABAJO FUTURO Propusimos un método novedoso para la segmentación y alineación de temas en múltiples documentos basado en información mutua ponderada, que también puede manejar casos de un solo documento. Utilizamos programación dinámica para optimizar nuestro algoritmo. Nuestro enfoque supera a todos los métodos anteriores en casos de un solo documento. Además, también demostramos que realizar segmentación entre varios documentos puede mejorar el rendimiento enormemente. Nuestros resultados también demostraron que el uso de la información mutua ponderada puede aprovechar la información de varios documentos para lograr un mejor rendimiento. Solo probamos nuestro método en conjuntos de datos limitados. Se deben probar más conjuntos de datos, especialmente los complicados. Deberían compararse más métodos anteriores. Además, las segmentaciones naturales como los párrafos son pistas que se pueden utilizar para encontrar los límites óptimos. El aprendizaje supervisado también puede ser considerado. 7. AGRADECIMIENTOS Los autores desean agradecer a Xiang Ji y al Prof. J. Scott Payne por su ayuda. 8. REFERENCIAS [1] A. Banerjee, I. Ghillon, J. Ghosh, S. Merugu y D. Modha. Un enfoque generalizado de entropía máxima para la coagrupación de Bregman y la aproximación de matrices. En Actas de SIGKDD, 2004. [2] R. Bekkerman, R. El-Yaniv y A. McCallum. Agrupación distribucional de múltiples vías a través de interacciones por pares. En Actas de ICML, 2005. [3] D. M. Blei y P. J. Moreno. Segmentación de temas con un modelo oculto de Markov de aspectos. En Actas de SIGIR, 2001. [4] D. M. Blei, A. Ng y M. Jordan. Asignación latente de Dirichlet. Revista de Investigación en Aprendizaje Automático, 3:993-1022, 2003. [5] T. Brants, F. Chen e I. Tsochantaridis. Segmentación de documentos basada en temas con análisis semántico latente probabilístico. En Actas de CIKM, 2002. [6] F. Choi. Avances en la segmentación lineal de texto independiente del dominio. En Actas de la NAACL, 2000. [7] H. Christensen, B. Kolluru, Y. Gotoh y S. Renals. Segmentación de máxima entropía de noticias de transmisión. En Actas de ICASSP, 2005. [8] T. Cover y J. Thomas. Elementos de la teoría de la información. John Wiley and Sons, Nueva York, EE. UU., 1991. [9] S. Deerwester, S. Dumais, G. Furnas, T. Landauer y R. Harshman. Indexación mediante análisis semántico latente. Revista de la Sociedad Americana de Sistemas de Información, 1990. [10] I. Dhillon, S. Mallela y D. Modha. Co-clustering informativo-teórico. En Actas de SIGKDD, 2003. [11] M. Hajime, H. Takeo y O. Manabu. Segmentación de texto con múltiples señales lingüísticas superficiales. En Actas de COLING-ACL, 1998. [12] T. K. Ho. Detención de palabras y identificación para el reconocimiento de texto adaptativo. Revista Internacional de Análisis y Reconocimiento de Documentos, 3(1), agosto de 2000. [13] T. Hofmann. Análisis semántico latente probabilístico. En Actas de la UAI99, 1999. [14] X. Ji y H. Zha. Correlación de la sumarización de un par de documentos multilingües. En Actas de RIDE, 2003. [15] X. Ji y H. Zha. Segmentación de texto independiente del dominio utilizando difusión anisotrópica y programación dinámica. En Actas de SIGIR, 2003. [16] X. Ji y H. Zha. Extrayendo temas compartidos de múltiples documentos. En Actas de la 7ª PAKDD, 2003. [17] J. Lafferty, A. McCallum y F. Pereira. Campos aleatorios condicionales: Modelos probabilísticos para segmentar y etiquetar datos de secuencias. En Actas de ICML, 2001. [18] T. Li, S. Ma y M. Ogihara. Criterio basado en la entropía en la agrupación categórica. En Actas de ICML, 2004. [19] A. McCallum, D. Freitag y F. Pereira. Modelos de máxima entropía de Markov para extracción de información y segmentación. En Actas de ICML, 2000. [20] L. Pevzner y M. Hearst. Una crítica y mejora de una métrica de evaluación para la segmentación de texto. Lingüística Computacional, 28(1):19-36, 2002. [21] J. C. Reynar. Modelos estadísticos para la segmentación de temas. En Actas de ACL, 1999. [22] G. Salton y M. McGill. Introducción a la Recuperación de Información Moderna. McGraw Hill, 1983. [23] B. \n\nMcGraw Hill, 1983. [23] B. Sun, Q. Tan, P. Mitra y C. L. Giles. Extracción y búsqueda de fórmulas químicas en documentos de texto en la web. En Actas de WWW, 2007. [24] B. Sun, D. Zhou, H. Zha, y J. I'm sorry, but \"Yen\" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish? Segmentación y alineación de texto multitarea basada en información mutua ponderada. En Actas de CIKM, 2006. [25] M. Utiyama y H. Isahara. Un modelo estadístico para la segmentación de texto independiente del dominio. En Actas de la 39ª ACL, 1999. [26] C. Wayne. Detección y seguimiento de temas multilingües: Investigación exitosa habilitada por corpora y evaluación. En Actas de LREC, 2000. [27] J. Yamron, I. Carp, L. Gillick, S. Lowe y P. van Mulbregt. Un enfoque de modelo oculto de Markov para la segmentación de texto y seguimiento de eventos. En Actas de ICASSP, 1998. [28] H. Zha y X. Ji. Correlacionando documentos multilingües a través de modelado de gráficos bipartitos. En Actas de SIGIR, 2002. ",
            "candidates": [],
            "error": [
                []
            ]
        },
        "multiple document": {
            "translated_key": "múltiples documentos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Topic Segmentation with Shared Topic Detection and Alignment of Multiple Documents Bingjun Sun*, Prasenjit Mitra*† , Hongyuan Zha‡ , C. Lee Giles*† , John Yen*† *Department of Computer Science and Engineering † College of Information Sciences and Technology The Pennsylvania State University University Park, PA 16802 ‡ College of Computing The Georgia Institute of Technology Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu ABSTRACT Topic detection and tracking [26] and topic segmentation [15] play an important role in capturing the local and sequential information of documents.",
                "Previous work in this area usually focuses on single documents, although similar <br>multiple document</br>s are available in many domains.",
                "In this paper, we introduce a novel unsupervised method for shared topic detection and topic segmentation of multiple similar documents based on mutual information (MI) and weighted mutual information (WMI) that is a combination of MI and term weights.",
                "The basic idea is that the optimal segmentation maximizes MI(or WMI).",
                "Our approach can detect shared topics among documents.",
                "It can find the optimal boundaries in a document, and align segments among documents at the same time.",
                "It also can handle single-document segmentation as a special case of the multi-document segmentation and alignment.",
                "Our methods can identify and strengthen cue terms that can be used for segmentation and partially remove stop words by using term weights based on entropy learned from <br>multiple document</br>s.",
                "Our experimental results show that our algorithm works well for the tasks of single-document segmentation, shared topic detection, and multi-document segmentation.",
                "Utilizing information from <br>multiple document</br>s can tremendously improve the performance of topic segmentation, and using WMI is even better than using MI for the multi-document segmentation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Clustering; H.3.1 [Information Storage and Retrieval]: Content Analysis and IndexingLinguistic processing; I.2.7 [Artificial Intelligence]: Natural Language Processing-Text analysis; I.5.3 [Pattern Recognition]: Clustering-Algorithms;Similarity measures General Terms Algorithms, Design, Experimentation 1.",
                "INTRODUCTION Many researchers have worked on topic detection and tracking (TDT) [26] and topic segmentation during the past decade.",
                "Topic segmentation intends to identify the boundaries in a document with the goal to capture the latent topical structure.",
                "Topic segmentation tasks usually fall into two categories [15]: text stream segmentation where topic transition is identified, and coherent document segmentation in which documents are split into sub-topics.",
                "The former category has applications in automatic speech recognition, while the latter one has more applications such as partial-text query of long documents in information retrieval, text summary, and quality measurement of <br>multiple document</br>s.",
                "Previous research in connection with TDT falls into the former category, targeted on topic tracking of broadcast speech data and newswire text, while the latter category has not been studied very well.",
                "Traditional approaches perform topic segmentation on documents one at a time [15, 25, 6].",
                "Most of them perform badly in subtle tasks like coherent document segmentation [15].",
                "Often, end-users seek documents that have the similar content.",
                "Search engines, like, Google, provide links to obtain similar pages.",
                "At a finer granularity, users may actually be looking to obtain sections of a document similar to a particular section that presumably discusses a topic of the users interest.",
                "Thus, the extension of topic segmentation from single documents to identifying similar segments from multiple similar documents with the same topic is a natural and necessary direction, and multi-document topic segmentation is expected to have a better performance since more information is utilized.",
                "Traditional approaches using similarity measurement based on term frequency generally have the same assumption that similar vocabulary tends to be in a coherent topic segment [15, 25, 6].",
                "However, they usually suffer the issue of identifying stop words.",
                "For example, additional document-dependent stop words are removed together with the generic stop words in [15].",
                "There are two reasons that we do not remove stop words directly.",
                "First, identifying stop words is another issue [12] that requires estimation in each domain.",
                "Removing common stop words may result in the loss of useful information in a specific domain.",
                "Second, even though stop words can be identified, hard classification of stop words and nonstop words cannot represent the gradually changing amount of information content of each word.",
                "We employ a soft classification using term weights.",
                "In this paper, we view the problem of topic segmentation as an optimization issue using information theoretic techniques to find the optimal boundaries of a document given the number of text segments so as to minimize the loss of mutual information (MI) (or a weighted mutual information (WMI)) after segmentation and alignment.",
                "This is equal to maximizing the MI (or WMI).",
                "The MI focuses on measuring the difference among segments whereas previous research focused on finding the similarity (e.g. cosine distance) of segments [15, 25, 6].",
                "Topic alignment of multiple similar documents can be achieved by clustering sentences on the same topic into the same cluster.",
                "Single-document topic segmentation is just a special case of the multi-document topic segmentation and alignment problem.",
                "Terms can be co-clustered as in [10] at the same time, given the number of clusters, but our experimental results show that this method results in a worse segmentation (see Tables 1, 4, and 6).",
                "Usually, human readers can identify topic transition based on cue words, and can ignore stop words.",
                "Inspired by this, we give each term (or term cluster) a weight based on entropy among different documents and different segments of documents.",
                "Not only can this approach increase the contribution of cue words, but it can also decrease the effect of common stop words, noisy word, and document-dependent stop words.",
                "These words are common in a document.",
                "Many methods based on sentence similarity require that these words are removed before topic segmentation can be performed [15].",
                "Our results in Figure 3 show that term weights are useful for multi-document topic segmentation and alignment.",
                "The major contribution of this paper is that it introduces a novel method for topic segmentation using MI and shows that this method performs better than previously used criteria.",
                "Also, we have addressed the problem of topic segmentation and alignment across <br>multiple document</br>s, whereas most existing research focused on segmentation of single documents.",
                "Multi-document segmentation and alignment can utilize information from similar documents and improves the performance of topic segmentation greatly.",
                "Obviously, our approach can handle single documents as a special case when <br>multiple document</br>s are unavailable.",
                "It can detect shared topics among documents to judge if they are <br>multiple document</br>s on the same topic.",
                "We also introduce the new criterion of WMI based on term weights learned from multiple similar documents, which can improve performance of topic segmentation further.",
                "We propose an iterative greedy algorithm based on dynamic programming and show that it works well in practice.",
                "Some of our prior work is in [24].",
                "The rest of this paper is organized as follows: In Section 2, we review related work.",
                "Section 3 contains a formulation of the problem of topic segmentation and alignment of <br>multiple document</br>s with term co-clustering, a review of the criterion of MI for clustering, and finally an introduction to WMI.",
                "In Section 4, we first propose the iterative greedy algorithm of topic segmentation and alignment with term co-clustering, and then describe how the algorithm can be optimized by usFigure 1: Illustration of multi-document segmentation and alignment. ing dynamic programming.",
                "In Section 5, experiments about single-document segmentation, shared topic detection, and multi-document segmentation are described, and results are presented and discussed to evaluate the performance of our algorithm.",
                "Conclusions and some future directions of the research work are discussed in Section 6. 2.",
                "PREVIOUS WORK Generally, the existing approaches to text segmentation fall into two categories: supervised learning [19, 17, 23] and unsupervised learning [3, 27, 5, 6, 15, 25, 21].",
                "Supervised learning usually has good performance, since it learns functions from labelled training sets.",
                "However, often getting large training sets with manual labels on document sentences is prohibitively expensive, so unsupervised approaches are desired.",
                "Some models consider dependence between sentences and sections, such as Hidden Markov Model [3, 27], Maximum Entropy Markov Model [19], and Conditional Random Fields [17], while many other approaches are based on lexical cohesion or similarity of sentences [5, 6, 15, 25, 21].",
                "Some approaches also focus on cue words as hints of topic transitions [11].",
                "While some existing methods only consider information in single documents [6, 15], others utilize <br>multiple document</br>s [16, 14].",
                "There are not many works in the latter category, even though the performance of segmentation is expected to be better with utilization of information from <br>multiple document</br>s.",
                "Previous research studied methods to find shared topics [16] and topic segmentation and summarization between just a pair of documents [14].",
                "Text classification and clustering is a related research area which categorizes documents into groups using supervised or unsupervised methods.",
                "Topical classification or clustering is an important direction in this area, especially co-clustering of documents and terms, such as LSA [9], PLSA [13], and approaches based on distances and bipartite graph partitioning [28] or maximum MI [2, 10], or maximum entropy [1, 18].",
                "Criteria of these approaches can be utilized in the issue of topic segmentation.",
                "Some of those methods have been extended into the area of topic segmentation, such as PLSA [5] and maximum entropy [7], but to our best knowledge, using MI for topic segmentation has not been studied. 3.",
                "PROBLEM FORMULATION Our goal is to segment documents and align the segments across documents (Figure 1).",
                "Let T be the set of terms {t1, t2, ..., tl}, which appear in the unlabelled set of documents D = {d1, d2, ..., dm}.",
                "Let Sd be the set of sentences for document d ∈ D, i.e. {s1, s2, ..., snd }.",
                "We have a 3D matrix of term frequency, in which the three dimensions are random variables of D, Sd, and T. Sd actually is a random vector including a random variable for each d ∈ D. The term frequency can be used to estimate the joint probability distribution P(D, Sd, T), which is p(t, d, s) = T(t, d, s)/ND, where T(t, d, s) is the number of t in ds sentence s and ND is the total number of terms in D. ˆS represents the set of segments {ˆs1, ˆs2, ..., ˆsp} after segmentation and alignment among <br>multiple document</br>s, where the number of segments | ˆS| = p. A segment ˆsi of document d is a sequence of adjacent sentences in d. Since for different documents si may discuss different sub-topics, our goal is to cluster adjacent sentences in each document into segments, and align similar segments among documents, so that for different documents ˆsi is about the same sub-topic.",
                "The goal is to find the optimal topic segmentation and alignment mapping Segd(si) : {s1, s2, ..., snd } → {ˆs1, ˆs2, ..., ˆsp} and Alid(ˆsi) : {ˆs1, ˆs2, ..., ˆsp} → {ˆs1, ˆs2, ..., ˆsp}, for all d ∈ D, where ˆsi is ith segment with the constraint that only adjacent sentences can be mapped to the same segment, i.e. for d, {si, si+1, ..., sj} → {ˆsq}, where q ∈ {1, ..., p}, where p is the segment number, and if i > j, then for d, ˆsq is missing.",
                "After segmentation and alignment, random vector Sd becomes an aligned random variable ˆS.",
                "Thus, P(D, Sd, T) becomes P(D, ˆS, T).",
                "Term co-clustering is a technique that has been employed [10] to improve the accuracy of document clustering.",
                "We evaluate the effect of it for topic segmentation.",
                "A term t is mapped to exactly one term cluster.",
                "Term co-clustering involves simultaneously finding the optimal term clustering mapping Clu(t) : {t1, t2, ..., tl} → {ˆt1, ˆt2, ..., ˆtk}, where k ≤ l, l is the total number of words in all the documents, and k is the number of clusters. 4.",
                "METHODOLOGY We now describe a novel algorithm which can handle singledocument segmentation, shared topic detection, and multidocument segmentation and alignment based on MI or WMI. 4.1 Mutual Information MI I(X; Y ) is a quantity to measure the amount of information which is contained in two or more random variables [8, 10].",
                "For the case of two random variables, we have I(X; Y ) = x∈X y∈Y p(x, y)log p(x, y) p(x)p(y) , (1) Obviously, when random variables X and Y are independent, I(X; Y ) = 0.",
                "Thus, intuitively, the value of MI depends on how random variables are dependent on each other.",
                "The optimal co-clustering is the mapping Clux : X → ˆX and Cluy : Y → ˆY that minimizes the loss: I(X; Y ) − I( ˆX; ˆY ), which is equal to maximizing I( ˆX; ˆY ).",
                "This is the criterion of MI for clustering.",
                "In the case of topic segmentation, the two random variables are the term variable T and the segment variable S, and each sample is an occurrence of a term T = t in a particular segment S = s. I(T; S) is used to measure how dependent T and S are.",
                "However, I(T; S) cannot be computed for documents before segmentation, since we do not have a set of S due to the fact that sentences of Document d, si ∈ Sd, is not aligned with other documents.",
                "Thus, instead of minimizing the loss of MI, we can maximize MI after topic segmentation, computed as: I( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS p(ˆt, ˆs)log p(ˆt, ˆs) p(ˆt)p(ˆs) , (2) where p(ˆt, ˆs) are estimated by the term frequency tf of Term Cluster ˆt and Segment ˆs in the training set D. Note that here a segment ˆs includes sentences about the the same topic among all documents.",
                "The optimal solution is the mapping Clut : T → ˆT, Segd : Sd → ˆS , and Alid : ˆS → ˆS, which maximizes I( ˆT; ˆS). 4.2 Weighted Mutual Information In topic segmentation and alignment of <br>multiple document</br>s, if P(D, ˆS, T) is known, based on the marginal distributions P(D|T) and P( ˆS|T) for each term t ∈ T, we can categorize terms into four types in the data set: • Common stop words are common both along the dimensions of documents and segments. • Document-dependent stop words that depends on the personal writing style are common only along the dimension of segments for some documents. • Cue words are the most important elements for segmentation.",
                "They are common along the dimension of documents only for the same segment, and they are not common along the dimensions of segments. • Noisy words are other words which are not common along both dimensions.",
                "Entropy based on P(D|T) and P( ˆS|T) can be used to identify different types of terms.",
                "To reinforce the contribution of cue words in the MI computation, and simultaneously reduce the effect of the other three types of words, similar as the idea of the tf-idf weight [22], we use entropies of each term along the dimensions of document D and segment ˆS, i.e.",
                "ED(ˆt) and EˆS(ˆt), to compute the weight.",
                "A cue word usually has a large value of ED(ˆt) but a small value of EˆS(ˆt).",
                "We introduce term weights (or term cluster weights) wˆt = ( ED(ˆt) maxˆt ∈ ˆT (ED(ˆt )) )a (1 − EˆS(ˆt) maxˆt ∈ ˆT (EˆS(ˆt )) )b , (3) where ED(ˆt) = d∈D p(d|ˆt)log|D| 1 p(d|ˆt) , EˆS(ˆt) = ˆs∈ ˆS p(ˆs|ˆt)log| ˆS| 1 p(ˆs|ˆt) , and a > 0 and b > 0 are powers to adjust term weights.",
                "Usually a = 1 and b = 1 as default, and maxˆt ∈ ˆT (ED(ˆt )) and maxˆt ∈ ˆT (EˆS(ˆt )) are used to normalize the entropy values.",
                "Term cluster weights are used to adjust p(ˆt, ˆs), pw(ˆt, ˆs) = wˆtp(ˆt, ˆs) ˆt∈ ˆT ;ˆs∈ ˆS wˆtp(ˆt, ˆs) , (4) and Iw( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS pw(ˆt, ˆs)log pw(ˆt, ˆs) pw(ˆt)pw(ˆs) , (5) where pw(ˆt) and pw(ˆs) are marginal distributions of pw(ˆt, ˆs).",
                "However, since we do not know either the term weights or P(D, ˆS, T), we need to estimate them, but wˆt depends on p(ˆs|t) and ˆS, while ˆS and p(ˆs|t) also depend on wˆt that is still unknown.",
                "Thus, an iterative algorithm is required to estimate term weights wˆt and find the best segmentation and alignment to optimize the objective function Iw concurrently.",
                "After a document is segmented into sentences Input: Joint probability distribution P(D, Sd, T), number of text segments p ∈ {2, 3, ..., max(sd)}, number of term clusters k ∈ {2, 3, ..., l} (if k = l, no term co-clustering required), and weight type w ∈ {0, 1}, indicating to use I or Iw, respectively.",
                "Output: Mapping Clu, Seg, Ali, and term weights wˆt.",
                "Initialization: 0. i = 0.",
                "Initialize Clu (0) t , Seg (0) d , and Ali (0) d ; Initialize w (0) ˆt using Equation (6) if w = 1; Stage 1: 1.",
                "If |D| = 1, k = l, and w = 0, check all sequential segmentations of d into p segments and find the best one Segd(s) = argmaxˆsI( ˆT; ˆS), and return Segd; otherwise, if w = 1 and k = l, go to 3.1; Stage 2: 2.1 If k < l, for each term t, find the best cluster ˆt as Clu(i+1)(t) = argmaxˆtI( ˆT; ˆS(i)) based on Seg(i) and Ali(i); 2.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+1) d (Seg (i+1) d (s)) = argmaxˆsI( ˆT(i+1); ˆS) based on Clu(i+1)(t) if k < l or Clu(0)(t) if k = l; 2.3 i + +.",
                "If Clu, Seg, or Ali changed, go to 2.1; otherwise, if w = 0, return Clu(i), Seg(i), and Ali(i); else j = 0, go to 3.1; Stage 3: 3.1 Update w (i+j+1) ˆt based on Seg(i+j), Ali(i+j), and Clu(i) using Equation (3); 3.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+j+1) d (Seg (i+j+1) d (s)) = argmaxˆsIw( ˆT(i); ˆS) based on Clu(i) and w (i+j+1) ˆt ; 3.3 j + +.",
                "If Iw( ˆT; ˆS) changes, go to step 6; otherwise, stop and return Clu(i), Seg(i+j), Ali(i+j), and w (i+j) ˆt ; Figure 2: Algorithm: Topic segmentation and alignment based on MI or WMI. and each sentence is segmented into words, each word is stemmed.",
                "Then the joint probability distribution P(D, Sd, T) can be estimated.",
                "Finally, this distribution can be used to compute MI in our algorithm. 4.3 Iterative Greedy Algorithm Our goal is to maximize the objective function, I( ˆT; ˆS) or Iw( ˆT; ˆS), which can measure the dependence of term occurrences in different segments.",
                "Generally, first we do not know the estimate term weights, which depend on the optimal topic segmentation and alignment, and term clusters.",
                "Moreover, this problem is NP-hard [10], even though if we know the term weights.",
                "Thus, an iterative greedy algorithm is desired to find the best solution, even though probably only local maxima are reached.",
                "We present the iterative greedy algorithm in Figure 2 to find a local maximum of I( ˆT; ˆS) or Iw( ˆT; ˆS) with simultaneous term weight estimation.",
                "This algorithm can is iterative and greedy for multi-document cases or single-document cases with term weight estimation and/or term co-clustering.",
                "Otherwise, since it is just a one step algorithm to solve the task of single-document segmentation [6, 15, 25], the global maximum of MI is guaranteed.",
                "We will show later that term co-clustering reduces the accuracy of the results and is not necessary, and for singledocument segmentation, term weights are also not required. 4.3.1 Initialization In Step 0, the initial term clustering Clut and topic segmentation and alignment Segd and Alid are important to avoid local maxima and reduce the number of iterations.",
                "First, a good guess of term weights can be made by using the distributions of term frequency along sentences for each document and averaging them to get the initial values of wˆt: wt = ( ED(t) maxt ∈T (ED(t )) )(1 − ES(t) maxt ∈T (ES(t )) ), (6) where ES(t) = 1 |Dt| d∈Dt (1 − s∈Sd p(s|t)log|Sd| 1 p(s|t) ), where Dt is the set of documents which contain Term t. Then, for the initial segmentation Seg(0) , we can simply segment documents equally by sentences.",
                "Or we can find the optimal segmentation just for each document d which maximizes the WMI, Seg (0) d = argmaxˆsIw(T; ˆS), where w = w (0) ˆt .",
                "For the initial alignment Ali(0) , we can first assume that the order of segments for each d is the same.",
                "For the initial term clustering Clu(0) , first cluster labels can be set randomly, and after the first time of Step 3, a good initial term clustering is obtained. 4.3.2 Different Cases After initialization, there are three stages for different cases.",
                "Totally there are eight cases, |D| = 1 or |D| > 1, k = l or k < l, w = 0 or w = 1.",
                "Single document segmentation without term clustering and term weight estimation (|D| = 1, k = l, w = 0) only requires Stage 1 (Step 1).",
                "If term clustering is required (k < l), Stage 2 (Step 2.1, 2.2, and 2.3) is executed iteratively.",
                "If term weight estimation is required (w = 1), Stage 3 (Step 3.1, 3.2, and 3.3) is executed iteratively.",
                "If both are required (k < l, w = 1), Stage 2 and 3 run one after the other.",
                "For multi-document segmentation without term clustering and term weight estimation (|D| > 1, k = l, w = 0), only iteration of Step 2.2 and 2.3 are required.",
                "At Stage 1, the global maximum can be found based on I( ˆT; ˆS) using dynamic programming in Section 4.4.",
                "Simultaneously finding a good term clustering and estimated term weights is impossible, since when moving a term to a new term cluster to maximize Iw( ˆT; ˆS), we do not know that the weight of this term should be the one of the new cluster or the old cluster.",
                "Thus, we first do term clustering at Stage 2, and then estimate term weights at Stage 3.",
                "At Stage 2, Step 2.1 is to find the best term clustering and Step 2.2 is to find the best segmentation.",
                "This cycle is repeated to find a local maximum based on MI I until it converges.",
                "The two steps are: (1) based on current term clustering Cluˆt, for each document d, the algorithm segments all the sentences Sd into p segments sequentially (some segments may be empty), and put them into the p segments ˆS of the whole training set D (all possible cases of different segmentation Segd and alignment Alid are checked) to find the optimal case, and (2) based on the current segmentation and alignment, for each term t, the algorithm finds the best term cluster of t based on the current segmentation Segd and alignment Alid.",
                "After finding a good term clustering, term weights are estimated if w = 1.",
                "At Stage 3, similar as Stage 2, Step 3.1 is term weight re-estimation and Step 3.2 is to find a better segmentation.",
                "They are repeated to find a local maximum based on WMI Iw until it converges.",
                "However, if the term clustering in Stage 2 is not accurate, then the term weight estimation at Stage 3 may have a bad result.",
                "Finally, at Step 3.3, this algorithm converges and return the output.",
                "This algorithm can handle both single-document and multi-document segmentation.",
                "It also can detect shared topics among documents by checking the proportion of overlapped sentences on the same topics, as described in Sec 5.2. 4.4 Algorithm Optimization In many previous works on segmentation, dynamic programming is a technique used to maximize the objective function.",
                "Similarly, at Step 1, 2.2, and 3.2 of our algorithm, we can use dynamic programming.",
                "For Stage 1, using dynamic programming can still find the global optimum, but for Stage 2 and Stage 3, we can only find the optimum for each step of topic segmentation and alignment of a document.",
                "Here we only show the dynamic programming for Step 3.2 using WMI (Step 1 and 2.2 are similar but they can use either I or Iw).",
                "There are two cases that are not shown in the algorithm in Figure 2: (a) single-document segmentation or multi-document segmentation with the same sequential order of segments, where alignment is not required, and (b) multi-document segmentation with different sequential orders of segments, where alignment is necessary.",
                "The alignment mapping function of the former case is simply just Alid(ˆsi) = ˆsi, while for the latter ones alignment mapping function Alid(ˆsi) = ˆsj, i and j may be different.",
                "The computational steps for the two cases are listed below: Case 1 (no alignment): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs) and partial pw(ˆs) without counting sentences from d. Then put sentences from i to j into Part k, and compute partial WMI PIw( ˆT; ˆsk(si, si+1, ..., sj)) ˆt∈ ˆT pw(ˆt, ˆsk)log pw(ˆt, ˆsk) pw(ˆt)pw(ˆsk) , where Alid(si, si+1, ..., sj) = k, k ∈ {1, 2, ..., p}, 1 ≤ i ≤ j ≤ nd, and Segd(sq) = ˆsk for all i ≤ q ≤ j. (2) Let M(sm, 1) = PIw( ˆT; ˆs1(s1, s2, ..., sm)).",
                "Then M(sm, L) = maxi[M(si−1, L − 1) + PIw( ˆT; ˆsL(si, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, and when i > m, no sentences are put into ˆsk when compute PIw (note PIw( ˆT; ˆs(si, ..., sm)) = 0 for single-document segmentation). (3) Finally M(snd , p) = maxi[M(si−1, p − 1)+ PIw( ˆT; ˆsp(si, ..., snd ))], where 1 ≤ i ≤ nd+1.",
                "The optimal Iw is found and the corresponding segmentation is the best.",
                "Case 2 (alignment required): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs), and partial pw(ˆs), and PIw( ˆT; ˆsk(si, si+1, ..., sj)) similarly as Case 1. (2) Let M(sm, 1, k) = PIw( ˆT; ˆsk(s1, s2, ..., sm)), where k ∈ {1, 2, ..., p}.",
                "Then M(sm, L, kL) = maxi,j[M(si−1, L − 1, kL/j) + PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, kL ∈ Set(p, L), which is the set of all p!",
                "L!(p−L)! combinations of L segments chosen from all p segments, j ∈ kL, the set of L segments chosen from all p segments, and kL/j is the combination of L − 1 segments in kL except Segment j. (3) Finally, M(snd , p, kp) = maxi,j[M(si−1, p − 1, kp/j) +PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., snd ))], where kp is just the combination of all p segments and 1 ≤ i ≤ nd + 1, which is the optimal Iw and the corresponding segmentation is the best.",
                "The steps of Case 1 and 2 are similar, except in Case 2, alignment is considered in addition to segmentation.",
                "First, basic items of probability for computing Iw are computed excluding Doc d, and then partial WMI by putting every possible sequential segment (including empty segment) of d into every segment of the set.",
                "Second, the optimal sum of PIw for L segments and the leftmost m sentences, M(sm, L), is found.",
                "Finally, the maximal WMI is found among different sums of M(sm, p − 1) and PIw for Segment p. 5.",
                "EXPERIMENTS In this section, single-document segmentation, shared topic detection, and multi-document segmentation will be tested.",
                "Different hyper parameters of our method are studied.",
                "For convenience, we refer to the method using I as MIk if w = 0, and Iw as WMIk if w = 2 or as WMIk if w = 1, where k is the number of term clusters, and if k = l, where l is the total number of terms, then no term clustering is required, i.e.",
                "MIl and WMIl. 5.1 Single-document Segmentation 5.1.1 Test Data and Evaluation The first data set we tested is a synthetic one used in previous research [6, 15, 25] and many other papers.",
                "It has 700 samples.",
                "Each is a concatenation of ten segments.",
                "Each segment is the first n sentence selected randomly from the Brown corpus, which is supposed to have a different topic from each other.",
                "Currently, the best results on this data set is achieved by Ji et.al. [15].",
                "To compare the performance of our methods, the criterion used widely in previous research is applied, instead of the unbiased criterion introduced in [20].",
                "It chooses a pair of words randomly.",
                "If they are in different segments (different) for the real segmentation (real), but predicted (pred) as in the same segment, it is a miss.",
                "If they are in the same segment (same), but predicted as in different segments, it is a false alarm.",
                "Thus, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, diff)p(diff|real) +p(false alarm|real, pred, same)p(same|real). 5.1.2 Experiment Results We tested the case when the number of segments is known.",
                "Table 1 shows the results of our methods with different hyper parameter values and three previous approaches, C99[25], U00[6], and ADDP03[15], on this data set when the segment number is known.",
                "In WMI for single-document segmentation, the term weights are computed as follows: wˆt = 1−EˆS(ˆt)/maxˆt ∈ ˆT (EˆS(ˆt )).",
                "For this case, our methods MIl and WMIl both outperform all the previous approaches.",
                "We compared our methods with ADDP03using one-sample one-sided t-test and p-values are shown in Table 2.",
                "From the p-values, we can see that mostly the differences are very Table 1: Average Error Rates of Single-document Segmentation Given Segment Numbers Known Range of n 3-11 3-5 6-8 9-11 Sample size 400 100 100 100 C99 12% 11% 10% 9% U00 10% 9% 7% 5% ADDP03 6.0% 6.8% 5.2% 4.3% MIl 4.68% 5.57% 2.59% 1.59% WMIl 4.94% 6.33% 2.76% 1.62% MI100 9.62% 12.92% 8.66% 6.67% Table 2: Single-document Segmentation: P-values of T-test on Error Rates Range of n 3-11 3-5 6-8 9-11 ADDP03, MIl 0.000 0.000 0.000 0.000 ADDP03, WMIl 0.000 0.099 0.000 0.000 MIl, WMIl 0.061 0.132 0.526 0.898 significant.",
                "We also compare the error rates between our two methods using two-sample two-sided t-test to check the hypothesis that they are equal.",
                "We cannot reject the hypothesis that they are equal, so the difference are not significant, even though all the error rates for MIl are smaller than WMIl.",
                "However, we can conclude that term weights contribute little in single-document segmentation.",
                "The results also show that MI using term co-clustering (k = 100) decreases the performance.",
                "We tested different number of term clusters, and found that the performance becomes better when the cluster number increases to reach l. WMIk<l has similar results that we did not show in the table.",
                "As mentioned before, using MI may be inconsistent on optimal boundaries given different numbers of segments.",
                "This situation occurs especially when the similarities among segments are quite different, i.e. some transitions are very obvious, while others are not.",
                "This is because usually a document is a hierarchical structure instead of only a sequential structure.",
                "When the segments are not at the same level, this situation may occur.",
                "Thus, a hierarchical topic segmentation approach is desired, and the structure highly depends on the number of segments for each internal node and the stop criteria of splitting.",
                "For this data set of singledocument segmentation, since it is just a synthetic set, which is just a concatenation of several segments about different topics, it is reasonable that approaches simply based on term frequency have a good performance.",
                "Usually for the tasks of segmenting coherent documents for sub-topics, the effectiveness decreases much. 5.2 Shared Topic Detection 5.2.1 Test Data and Evaluation The second data set contains 80 news articles from Google News.",
                "There are eight topics and each has 10 articles.",
                "We randomly split the set into subsets with different document numbers and each subset has all eight topics.",
                "We compare our approach MIl and WMIl with LDA [4].",
                "LDA treats a document in the data set as a bag of words, finds its distribution on topics, and its major topic.",
                "MIl and WMIl views each sentence as a bag of words and tag it with a topic label.",
                "Then for each pair of documents, LDA determines if they are on the same topic, while MIl and Table 3: Shared Topic Detection: Average Error Rates for Different Numbers of Documents in Each Subset #Doc 10 20 40 80 LDA 8.89% 16.33% 1.35% 0.60% MIl, θ = 0.6 4.17% 1.71% 1.47% 0.0% WMIl, θ = 0.8 18.6% 3.16% 1.92% 0.0% WMIl check whether the proportion overlapped sentences on the same topic is larger than the adjustable threshold θ.",
                "That is, in MIl and WMIl, for a pair of documents d, d , if [ s∈Sd,s ∈Sd 1(topics=topics )/min(|Sd|, |Sd|)] > θ, where Sd is the set of sentences of d, and |Sd| is the number of sentences of d, then d and d have the shared topic.",
                "For a pair of documents selected randomly, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, same)p(same|real) +p(false alarm|real, pred, diff)p(diff|real), where a miss means if they have the same topic (same) for the real case (real), but predicted (pred) as on the same topic.",
                "If they are on different topics (diff), but predicted as on the same topic, it is a false alarm. 5.2.2 Experiment Results The results are shown in Table 3.",
                "If most documents have different topics, in WMIl, the estimation of term weights in Equation (3) is not correct.",
                "Thus, WMIl is not expected to have a better performance than MIl, when most documents have different topics.",
                "When there are fewer documents in a subset with the same number of topics, more documents have different topics, so WMIl is more worse than MIl.",
                "We can see that for most cases MIl has a better (or at least similar) performance than LDA.",
                "After shared topic detection, multi-document segmentation of documents with the shared topics is able to be executed. 5.3 Multi-document Segmentation 5.3.1 Test Data and Evaluation For multi-document segmentation and alignment, our goal is to identify these segments about the same topic among multiple similar documents with shared topics.",
                "Using Iw is expected to perform better than I, since without term weights the result is affected seriously by document-dependent stop words and noisy words which depends on the personal writing style.",
                "It is more likely to treat the same segments of different documents as different segments under the effect of document-dependent stop words and noisy words.",
                "Term weights can reduce the effect of document-dependent stop words and noisy words by giving cue terms more weights.",
                "The data set for multi-document segmentation and alignment has 102 samples and 2264 sentences totally.",
                "Each is the introduction part of a lab report selected from the course of Biol 240W, Pennsylvania State University.",
                "Each sample has two segments, introduction of plant hormones and the content in the lab.",
                "The length range of samples is from two to 56 sentences.",
                "Some samples only have one part and some have a reverse order the these two segments.",
                "It is not hard to identify the boundary between two segments for human.",
                "We labelled each sentence manually for evaluation.",
                "The criterion of evaluation is just using the proportion of the number of sentences with wrong predicted segment labels in the total number of sentences in the whole training Table 4: Average Error Rates of Multi-document Segmentation Given Segment Numbers Known #Doc MIl WMIl k MIk WMIk 102 3.14% 2.78% 300 4.68% 6.58% 51 4.17% 3.63% 300 17.83% 22.84% 34 5.06% 4.12% 300 18.75% 20.95% 20 7.08% 5.42% 250 20.40% 21.83% 10 10.38% 7.89% 250 21.42% 21.91% 5 15.77% 11.64% 250 21.89% 22.59% 2 25.90% 23.18% 50 25.44% 25.49% 1 23.90% 24.82% 25 25.75% 26.15% Table 5: Multi-document Segmentation: P-values of T-test on Error Rates for MIl and WMIl #Doc 51 34 20 10 5 2 P-value 0.19 0.101 0.025 0.001 0.000 0.002 set as the error rate: p(error|predicted, real) = d∈D s∈Sd 1(predicteds=reals)/ d∈D nd.",
                "In order to show the benefits of multi-document segmentation and alignment, we compared our method with different parameters on different partitions of the same training set.",
                "Except the cases that the number of documents is 102 and one (they are special cases of using the whole set and the pure single-document segmentation), we randomly divided the training set into m partitions, and each has 51, 34, 20, 10, 5, and 2 document samples.",
                "Then we applied our methods on each partition and calculated the error rate of the whole training set.",
                "Each case was repeated for 10 times for computing the average error rates.",
                "For different partitions of the training set, different k values are used, since the number of terms increases when the document number in each partition increases. 5.3.2 Experiment Results From the experiment results in Table 4, we can see the following observations: (1) When the number of documents increases, all methods have better performances.",
                "Only from one to two documents, MIl has decrease a little.",
                "We can observe this from Figure 3 at the point of document number = 2.",
                "Most curves even have the worst results at this point.",
                "There are two reasons.",
                "First, because samples vote for the best multi-document segmentation and alignment, but if only two documents are compared with each other, the one with missing segments or a totally different sequence will affect the correct segmentation and alignment of the other.",
                "Second, as noted at the beginning of this section, if two documents have more document-dependent stop words or noisy words than cue words, then the algorithm may view them as two different segments and the other segment is missing.",
                "Generally, we can only expect a better performance when the number of documents is larger than the number of segments. (2) Except single-document segmentation, WMIl is always better than MIl, and when the number of documents is reaching one or increases to a very large number, their performances become closer.",
                "Table 5 shows p-values of twosample one-sided t-test between MIl and WMIl.",
                "We also can see this trend from p-values.",
                "When document number = 5, we reached the smallest p-value and the largest difference between error rates of MIl and WMIl.",
                "For single-document Table 6: Multi-document Segmentation: Average Error Rate for Document Number = 5 in Each Subset with Different Number of Term Clusters #Cluster 75 100 150 250 l MIk 24.67% 24.54% 23.91% 22.59% 15.77% segmentation, WMIl is even a little bit worse than MIl, which is similar as the results of the single-document segmentation on the first data set.",
                "The reason is that for singledocument segmentation, we cannot estimate term weights accurately, since <br>multiple document</br>s are unavailable. (3) Using term clustering usually gets worse results than MIl and WMIl. (4) Using term clustering in WMIk is even worse than in MIk, since in WMIk term clusters are found first using I before using Iw.",
                "If the term clusters are not correct, then the term weights are estimated worse, which may mislead the algorithm to reach even worse results.",
                "From the results we also found that in multi-document segmentation and alignment, most documents with missing segments and a reverse order are identified correctly.",
                "Table 6 illustrates the experiment results for the case of 20 partitions (each has five document samples) of the training set and topic segmentation and alignment using MIk with different numbers of term clusters k. Notice that when the number of term clusters increases, the error rate becomes smaller.",
                "Without term clustering, we have the best result.",
                "We did not show results for WMIk with term clustering, but the results are similar.",
                "We also tested WMIl with different hyper parameters of a and b to adjust term weights.",
                "The results are presented in Figure 3.",
                "It was shown that the default case WMIl : a = 1, b = 1 gave the best results for different partitions of the training set.",
                "We can see the trend that when the document number is very small or large, the difference between MIl : a = 0, b = 0 and WMIl : a = 1, b = 1 becomes quite small.",
                "When the document number is not large (about from 2 to 10), all the cases using term weights have better performances than MIl : a = 0, b = 0 without term weights, but when the document number becomes larger, the cases WMIl : a = 1, b = 0 and WMIl : a = 2, b = 1 become worse than MIl : a = 0, b = 0.",
                "When the document number becomes very large, they are even worse than cases with small document numbers.",
                "This means that a proper way to estimate term weights for the criterion of WMI is very important.",
                "Figure 4 shows the term weights learned from the whole training set.",
                "Four types of words are categorized roughly even though the transition among them are subtle.",
                "Figure 5 illustrates the change in (weighted) mutual information for MIl and WMIl.",
                "As expected, mutual information for MIl increases monotonically with the number of steps, while WMIl does not.",
                "Finally, MIl and WMIl are scalable, with computational complexity shown in Figure 6.",
                "One advantage for our approach based on MI is that removing stop words is not required.",
                "Another important advantage is that there are no necessary hyper parameters to adjust.",
                "In single-document segmentation, the performance based on MI is even better for that based on WMI, so no extra hyper parameter is required.",
                "In multi-document segmentation, we show in the experiment, a = 1 and b = 1 is the best.",
                "Our method gives more weights to cue terms.",
                "However, usually cue terms or sentences appear at the beginning of a segment, while the end of the segment may be 1 2 5 10 20 34 51 102 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 Document Number ErrorRate MIl :a=0,b=0 WMI l :a=1,b=1 WMI l :a=1,b=0 WMI l :a=2,b=1 Figure 3: Error rates for different hyper parameters of term weights. 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Normalized Document Entropy NormalizedSegmentEntropy Noisy words Cue words Common stop words Doc−dep stop words Figure 4: Term weights learned from the whole training set. 0 100 200 300 400 500 600 0.06 0.08 0.1 0.12 0.14 0.16 0.18 Number of Steps (Weighted)MutualInformation MI l WMI l Figure 5: Change in (weighted) MI for MIl and WMIl. 0 20 40 60 80 100 120 0 200 400 600 800 1000 1200 1400 1600 1800 2000 Document Number TimetoConverge(sec) MI l WMI l Figure 6: Time to converge for MIl and WMIl. much noisy.",
                "One possible solution is giving more weights to terms at the beginning of each segment.",
                "Moreover, when the length of segments are quite different, long segments have much higher term frequencies, so they may dominate the segmentation boundaries.",
                "Normalization of term frequencies versus the segment length may be useful. 6.",
                "CONCLUSIONS AND FUTURE WORK We proposed a novel method for multi-document topic segmentation and alignment based on weighted mutual information, which can also handle single-document cases.",
                "We used dynamic programming to optimize our algorithm.",
                "Our approach outperforms all the previous methods on singledocument cases.",
                "Moreover, we also showed that doing segmentation among <br>multiple document</br>s can improve the performance tremendously.",
                "Our results also illustrated that using weighted mutual information can utilize the information of <br>multiple document</br>s to reach a better performance.",
                "We only tested our method on limited data sets.",
                "More data sets especially complicated ones should be tested.",
                "More previous methods should be compared with.",
                "Moreover, natural segmentations like paragraphs are hints that can be used to find the optimal boundaries.",
                "Supervised learning also can be considered. 7.",
                "ACKNOWLEDGMENTS The authors want to thank Xiang Ji, and Prof. J. Scott Payne for their help. 8.",
                "REFERENCES [1] A. Banerjee, I. Ghillon, J. Ghosh, S. Merugu, and D. Modha.",
                "A generalized maximum entropy approach to bregman co-clustering and matrix approximation.",
                "In Proceedings of SIGKDD, 2004. [2] R. Bekkerman, R. El-Yaniv, and A. McCallum.",
                "Multi-way distributional clustering via pairwise interactions.",
                "In Proceedings of ICML, 2005. [3] D. M. Blei and P. J. Moreno.",
                "Topic segmentation with an aspect hidden markov model.",
                "In Proceedings of SIGIR, 2001. [4] D. M. Blei, A. Ng, and M. Jordan.",
                "Latent dirichlet allocation.",
                "Journal of Machine Learning Research, 3:993-1022, 2003. [5] T. Brants, F. Chen, and I. Tsochantaridis.",
                "Topic-based document segmentation with probabilistic latent semantic analysis.",
                "In Proceedings of CIKM, 2002. [6] F. Choi.",
                "Advances in domain indepedent linear text segmentation.",
                "In Proceedings of the NAACL, 2000. [7] H. Christensen, B. Kolluru, Y. Gotoh, and S. Renals.",
                "Maximum entropy segmentation of broadcast news.",
                "In Proceedings of ICASSP, 2005. [8] T. Cover and J. Thomas.",
                "Elements of Information Theory.",
                "John Wiley and Sons, New York, USA, 1991. [9] S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and R. Harshman.",
                "Indexing by latent semantic analysis.",
                "Journal of the American Society for Information Systems, 1990. [10] I. Dhillon, S. Mallela, and D. Modha.",
                "Information-theoretic co-clustering.",
                "In Proceedings of SIGKDD, 2003. [11] M. Hajime, H. Takeo, and O. Manabu.",
                "Text segmentation with multiple surface linguistic cues.",
                "In Proceedings of COLING-ACL, 1998. [12] T. K. Ho.",
                "Stop word location and identification for adaptive text recognition.",
                "International Journal of Document Analysis and Recognition, 3(1), August 2000. [13] T. Hofmann.",
                "Probabilistic latent semantic analysis.",
                "In Proceedings of the UAI99, 1999. [14] X. Ji and H. Zha.",
                "Correlating summarization of a pair of multilingual documents.",
                "In Proceedings of RIDE, 2003. [15] X. Ji and H. Zha.",
                "Domain-independent text segmentation using anisotropic diffusion and dynamic programming.",
                "In Proceedings of SIGIR, 2003. [16] X. Ji and H. Zha.",
                "Extracting shared topics of <br>multiple document</br>s.",
                "In Proceedings of the 7th PAKDD, 2003. [17] J. Lafferty, A. McCallum, and F. Pereira.",
                "Conditional random fields: Probabilistic models for segmenting and labeling sequence data.",
                "In Proceedings of ICML, 2001. [18] T. Li, S. Ma, and M. Ogihara.",
                "Entropy-based criterion in categorical clustering.",
                "In Proceedings of ICML, 2004. [19] A. McCallum, D. Freitag, and F. Pereira.",
                "Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of ICML, 2000. [20] L. Pevzner and M. Hearst.",
                "A critique and improvement of an evaluation metric for text segmentation.",
                "Computational Linguistic, 28(1):19-36, 2002. [21] J. C. Reynar.",
                "Statistical models for topic segmentation.",
                "In Proceedings of ACL, 1999. [22] G. Salton and M. McGill.",
                "Introduction to Modern Information Retrieval.",
                "McGraw Hill, 1983. [23] B.",
                "Sun, Q. Tan, P. Mitra, and C. L. Giles.",
                "Extraction and search of chemical formulae in text documents on the web.",
                "In Proceedings of WWW, 2007. [24] B.",
                "Sun, D. Zhou, H. Zha, and J.",
                "Yen.",
                "Multi-task text segmentation and alignment based on weighted mutual information.",
                "In Proceedings of CIKM, 2006. [25] M. Utiyama and H. Isahara.",
                "A statistical model for domain-independent text segmentation.",
                "In Proceedings of the 39th ACL, 1999. [26] C. Wayne.",
                "Multilingual topic detection and tracking: Successful research enabled by corpora and evaluation.",
                "In Proceedings of LREC, 2000. [27] J. Yamron, I. Carp, L. Gillick, S. Lowe, and P. van Mulbregt.",
                "A hidden markov model approach to text segmentation and event tracking.",
                "In Proceedings of ICASSP, 1998. [28] H. Zha and X. Ji.",
                "Correlating multilingual documents via bipartite graph modeling.",
                "In Proceedings of SIGIR, 2002."
            ],
            "original_annotated_samples": [
                "Previous work in this area usually focuses on single documents, although similar <br>multiple document</br>s are available in many domains.",
                "Our methods can identify and strengthen cue terms that can be used for segmentation and partially remove stop words by using term weights based on entropy learned from <br>multiple document</br>s.",
                "Utilizing information from <br>multiple document</br>s can tremendously improve the performance of topic segmentation, and using WMI is even better than using MI for the multi-document segmentation.",
                "The former category has applications in automatic speech recognition, while the latter one has more applications such as partial-text query of long documents in information retrieval, text summary, and quality measurement of <br>multiple document</br>s.",
                "Also, we have addressed the problem of topic segmentation and alignment across <br>multiple document</br>s, whereas most existing research focused on segmentation of single documents."
            ],
            "translated_annotated_samples": [
                "El trabajo previo en esta área suele centrarse en documentos individuales, aunque existen <br>múltiples documentos</br> similares en muchos dominios.",
                "Nuestros métodos pueden identificar y fortalecer términos de señal que pueden ser utilizados para la segmentación y eliminar parcialmente las palabras de parada mediante el uso de pesos de términos basados en la entropía aprendida de <br>múltiples documentos</br>.",
                "El uso de información de múltiples documentos puede mejorar enormemente el rendimiento de la segmentación de temas, y utilizar WMI es aún mejor que utilizar MI para la segmentación de múltiples documentos.",
                "La primera categoría tiene aplicaciones en el reconocimiento automático del habla, mientras que la segunda tiene más aplicaciones como la consulta parcial de texto en documentos extensos en la recuperación de información, resumen de texto y medición de calidad de <br>múltiples documentos</br>.",
                "Además, hemos abordado el problema de la segmentación y alineación de temas en <br>varios documentos</br>, mientras que la mayoría de las investigaciones existentes se centraron en la segmentación de documentos individuales."
            ],
            "translated_text": "La segmentación de temas con detección de temas compartidos y alineación de múltiples documentos Bingjun Sun*, Prasenjit Mitra*†, Hongyuan Zha‡, C. Lee Giles*†, John Yen*† *Departamento de Ciencias de la Computación e Ingeniería † Colegio de Ciencias de la Información y Tecnología Universidad Estatal de Pensilvania University Park, PA 16802 ‡ Colegio de Computación Instituto de Tecnología de Georgia Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu RESUMEN La detección y seguimiento de temas [26] y la segmentación de temas [15] juegan un papel importante en la captura de la información local y secuencial de los documentos. El trabajo previo en esta área suele centrarse en documentos individuales, aunque existen <br>múltiples documentos</br> similares en muchos dominios. En este artículo, presentamos un novedoso método no supervisado para la detección de temas compartidos y la segmentación de temas de múltiples documentos similares basado en la información mutua (MI) y la información mutua ponderada (WMI), que es una combinación de MI y pesos de términos. La idea básica es que la segmentación óptima maximiza MI (o WMI). Nuestro enfoque puede detectar temas compartidos entre documentos. Puede encontrar los límites óptimos en un documento y alinear segmentos entre documentos al mismo tiempo. También puede manejar la segmentación de un solo documento como un caso especial de la segmentación y alineación de varios documentos. Nuestros métodos pueden identificar y fortalecer términos de señal que pueden ser utilizados para la segmentación y eliminar parcialmente las palabras de parada mediante el uso de pesos de términos basados en la entropía aprendida de <br>múltiples documentos</br>. Nuestros resultados experimentales muestran que nuestro algoritmo funciona bien para las tareas de segmentación de un solo documento, detección de temas compartidos y segmentación de múltiples documentos. El uso de información de múltiples documentos puede mejorar enormemente el rendimiento de la segmentación de temas, y utilizar WMI es aún mejor que utilizar MI para la segmentación de múltiples documentos. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información-Agrupación; H.3.1 [Almacenamiento y Recuperación de Información]: Análisis de Contenido e Indexación-Procesamiento Lingüístico; I.2.7 [Inteligencia Artificial]: Procesamiento de Lenguaje Natural-Análisis de Texto; I.5.3 [Reconocimiento de Patrones]: Algoritmos de Agrupación-Medidas de Similitud Términos Generales Algoritmos, Diseño, Experimentación 1. INTRODUCCIÓN Muchos investigadores han trabajado en la detección y seguimiento de temas (TDT) [26] y la segmentación de temas durante la última década. La segmentación de temas tiene como objetivo identificar los límites en un documento con la meta de capturar la estructura temática latente. Las tareas de segmentación de temas generalmente se dividen en dos categorías [15]: la segmentación de flujos de texto, donde se identifica la transición de temas, y la segmentación coherente de documentos, en la que los documentos se dividen en subtemas. La primera categoría tiene aplicaciones en el reconocimiento automático del habla, mientras que la segunda tiene más aplicaciones como la consulta parcial de texto en documentos extensos en la recuperación de información, resumen de texto y medición de calidad de <br>múltiples documentos</br>. La investigación previa en relación con TDT se enmarca en la primera categoría, centrada en el seguimiento de temas de datos de discurso de transmisión y texto de agencia de noticias, mientras que la segunda categoría no ha sido estudiada muy bien. Los enfoques tradicionales realizan la segmentación de temas en los documentos uno a la vez [15, 25, 6]. La mayoría de ellos tienen un mal desempeño en tareas sutiles como la segmentación coherente de documentos [15]. A menudo, los usuarios finales buscan documentos que tengan un contenido similar. Los motores de búsqueda, como Google, proporcionan enlaces para obtener páginas similares. A una granularidad más fina, los usuarios pueden estar buscando obtener secciones de un documento similares a una sección en particular que presumiblemente discute un tema de interés para los usuarios. Por lo tanto, la extensión de la segmentación de temas desde documentos individuales para identificar segmentos similares de múltiples documentos similares con el mismo tema es una dirección natural y necesaria, y se espera que la segmentación de temas de múltiples documentos tenga un mejor rendimiento ya que se utiliza más información. Los enfoques tradicionales que utilizan la medición de similitud basada en la frecuencia de términos generalmente parten del mismo supuesto de que un vocabulario similar tiende a estar en un segmento de tema coherente [15, 25, 6]. Sin embargo, suelen tener el problema de identificar las palabras de parada. Por ejemplo, se eliminan las palabras de parada dependientes del documento junto con las palabras de parada genéricas en [15]. Hay dos razones por las que no eliminamos las palabras de parada directamente. Primero, identificar las palabras vacías es otro problema [12] que requiere estimación en cada dominio. Eliminar palabras comunes de parada puede resultar en la pérdida de información útil en un dominio específico. Segundo, aunque se pueden identificar las palabras de parada, la clasificación estricta de palabras de parada y palabras no de parada no puede representar el cambio gradual en la cantidad de contenido informativo de cada palabra. Empleamos una clasificación suave utilizando pesos de términos. En este documento, consideramos el problema de la segmentación de temas como un problema de optimización utilizando técnicas de teoría de la información para encontrar los límites óptimos de un documento dado el número de segmentos de texto, de manera que se minimice la pérdida de información mutua (MI) (o información mutua ponderada (WMI)) después de la segmentación y alineación. Esto es igual a maximizar el MI (o WMI). El MI se centra en medir la diferencia entre segmentos, mientras que investigaciones anteriores se enfocaban en encontrar la similitud (por ejemplo, la distancia del coseno) entre segmentos [15, 25, 6]. La alineación de temas de múltiples documentos similares se puede lograr agrupando las oraciones sobre el mismo tema en el mismo clúster. La segmentación de temas en un solo documento es solo un caso especial del problema de segmentación y alineación de temas en múltiples documentos. Los términos pueden ser co-agrupados como en [10] al mismo tiempo, dado el número de agrupaciones, pero nuestros resultados experimentales muestran que este método resulta en una peor segmentación (ver Tablas 1, 4 y 6). Por lo general, los lectores humanos pueden identificar la transición de temas basándose en palabras de señal, y pueden ignorar las palabras vacías. Inspirados por esto, asignamos a cada término (o grupo de términos) un peso basado en la entropía entre diferentes documentos y diferentes segmentos de documentos. No solo este enfoque puede aumentar la contribución de las palabras clave, sino que también puede disminuir el efecto de las palabras de parada comunes, palabras ruidosas y palabras de parada dependientes del documento. Estas palabras son comunes en un documento. Muchos métodos basados en la similitud de oraciones requieren que estas palabras sean eliminadas antes de que se pueda realizar la segmentación de temas [15]. Nuestros resultados en la Figura 3 muestran que los pesos de los términos son útiles para la segmentación y alineación de temas en múltiples documentos. La principal contribución de este artículo es que introduce un método novedoso para la segmentación de temas utilizando MI y demuestra que este método funciona mejor que los criterios previamente utilizados. Además, hemos abordado el problema de la segmentación y alineación de temas en <br>varios documentos</br>, mientras que la mayoría de las investigaciones existentes se centraron en la segmentación de documentos individuales. ",
            "candidates": [],
            "error": [
                [
                    "múltiples documentos",
                    "múltiples documentos",
                    "múltiples documentos",
                    "varios documentos"
                ]
            ]
        },
        "wmus": {
            "translated_key": "I'm sorry, but \"wmus\" does not seem to be a valid English sentence. Could you please provide a different sentence for translation?",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Topic Segmentation with Shared Topic Detection and Alignment of Multiple Documents Bingjun Sun*, Prasenjit Mitra*† , Hongyuan Zha‡ , C. Lee Giles*† , John Yen*† *Department of Computer Science and Engineering † College of Information Sciences and Technology The Pennsylvania State University University Park, PA 16802 ‡ College of Computing The Georgia Institute of Technology Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu ABSTRACT Topic detection and tracking [26] and topic segmentation [15] play an important role in capturing the local and sequential information of documents.",
                "Previous work in this area usually focuses on single documents, although similar multiple documents are available in many domains.",
                "In this paper, we introduce a novel unsupervised method for shared topic detection and topic segmentation of multiple similar documents based on mutual information (MI) and weighted mutual information (WMI) that is a combination of MI and term weights.",
                "The basic idea is that the optimal segmentation maximizes MI(or WMI).",
                "Our approach can detect shared topics among documents.",
                "It can find the optimal boundaries in a document, and align segments among documents at the same time.",
                "It also can handle single-document segmentation as a special case of the multi-document segmentation and alignment.",
                "Our methods can identify and strengthen cue terms that can be used for segmentation and partially remove stop words by using term weights based on entropy learned from multiple documents.",
                "Our experimental results show that our algorithm works well for the tasks of single-document segmentation, shared topic detection, and multi-document segmentation.",
                "Utilizing information from multiple documents can tremendously improve the performance of topic segmentation, and using WMI is even better than using MI for the multi-document segmentation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Clustering; H.3.1 [Information Storage and Retrieval]: Content Analysis and IndexingLinguistic processing; I.2.7 [Artificial Intelligence]: Natural Language Processing-Text analysis; I.5.3 [Pattern Recognition]: Clustering-Algorithms;Similarity measures General Terms Algorithms, Design, Experimentation 1.",
                "INTRODUCTION Many researchers have worked on topic detection and tracking (TDT) [26] and topic segmentation during the past decade.",
                "Topic segmentation intends to identify the boundaries in a document with the goal to capture the latent topical structure.",
                "Topic segmentation tasks usually fall into two categories [15]: text stream segmentation where topic transition is identified, and coherent document segmentation in which documents are split into sub-topics.",
                "The former category has applications in automatic speech recognition, while the latter one has more applications such as partial-text query of long documents in information retrieval, text summary, and quality measurement of multiple documents.",
                "Previous research in connection with TDT falls into the former category, targeted on topic tracking of broadcast speech data and newswire text, while the latter category has not been studied very well.",
                "Traditional approaches perform topic segmentation on documents one at a time [15, 25, 6].",
                "Most of them perform badly in subtle tasks like coherent document segmentation [15].",
                "Often, end-users seek documents that have the similar content.",
                "Search engines, like, Google, provide links to obtain similar pages.",
                "At a finer granularity, users may actually be looking to obtain sections of a document similar to a particular section that presumably discusses a topic of the users interest.",
                "Thus, the extension of topic segmentation from single documents to identifying similar segments from multiple similar documents with the same topic is a natural and necessary direction, and multi-document topic segmentation is expected to have a better performance since more information is utilized.",
                "Traditional approaches using similarity measurement based on term frequency generally have the same assumption that similar vocabulary tends to be in a coherent topic segment [15, 25, 6].",
                "However, they usually suffer the issue of identifying stop words.",
                "For example, additional document-dependent stop words are removed together with the generic stop words in [15].",
                "There are two reasons that we do not remove stop words directly.",
                "First, identifying stop words is another issue [12] that requires estimation in each domain.",
                "Removing common stop words may result in the loss of useful information in a specific domain.",
                "Second, even though stop words can be identified, hard classification of stop words and nonstop words cannot represent the gradually changing amount of information content of each word.",
                "We employ a soft classification using term weights.",
                "In this paper, we view the problem of topic segmentation as an optimization issue using information theoretic techniques to find the optimal boundaries of a document given the number of text segments so as to minimize the loss of mutual information (MI) (or a weighted mutual information (WMI)) after segmentation and alignment.",
                "This is equal to maximizing the MI (or WMI).",
                "The MI focuses on measuring the difference among segments whereas previous research focused on finding the similarity (e.g. cosine distance) of segments [15, 25, 6].",
                "Topic alignment of multiple similar documents can be achieved by clustering sentences on the same topic into the same cluster.",
                "Single-document topic segmentation is just a special case of the multi-document topic segmentation and alignment problem.",
                "Terms can be co-clustered as in [10] at the same time, given the number of clusters, but our experimental results show that this method results in a worse segmentation (see Tables 1, 4, and 6).",
                "Usually, human readers can identify topic transition based on cue words, and can ignore stop words.",
                "Inspired by this, we give each term (or term cluster) a weight based on entropy among different documents and different segments of documents.",
                "Not only can this approach increase the contribution of cue words, but it can also decrease the effect of common stop words, noisy word, and document-dependent stop words.",
                "These words are common in a document.",
                "Many methods based on sentence similarity require that these words are removed before topic segmentation can be performed [15].",
                "Our results in Figure 3 show that term weights are useful for multi-document topic segmentation and alignment.",
                "The major contribution of this paper is that it introduces a novel method for topic segmentation using MI and shows that this method performs better than previously used criteria.",
                "Also, we have addressed the problem of topic segmentation and alignment across multiple documents, whereas most existing research focused on segmentation of single documents.",
                "Multi-document segmentation and alignment can utilize information from similar documents and improves the performance of topic segmentation greatly.",
                "Obviously, our approach can handle single documents as a special case when multiple documents are unavailable.",
                "It can detect shared topics among documents to judge if they are multiple documents on the same topic.",
                "We also introduce the new criterion of WMI based on term weights learned from multiple similar documents, which can improve performance of topic segmentation further.",
                "We propose an iterative greedy algorithm based on dynamic programming and show that it works well in practice.",
                "Some of our prior work is in [24].",
                "The rest of this paper is organized as follows: In Section 2, we review related work.",
                "Section 3 contains a formulation of the problem of topic segmentation and alignment of multiple documents with term co-clustering, a review of the criterion of MI for clustering, and finally an introduction to WMI.",
                "In Section 4, we first propose the iterative greedy algorithm of topic segmentation and alignment with term co-clustering, and then describe how the algorithm can be optimized by usFigure 1: Illustration of multi-document segmentation and alignment. ing dynamic programming.",
                "In Section 5, experiments about single-document segmentation, shared topic detection, and multi-document segmentation are described, and results are presented and discussed to evaluate the performance of our algorithm.",
                "Conclusions and some future directions of the research work are discussed in Section 6. 2.",
                "PREVIOUS WORK Generally, the existing approaches to text segmentation fall into two categories: supervised learning [19, 17, 23] and unsupervised learning [3, 27, 5, 6, 15, 25, 21].",
                "Supervised learning usually has good performance, since it learns functions from labelled training sets.",
                "However, often getting large training sets with manual labels on document sentences is prohibitively expensive, so unsupervised approaches are desired.",
                "Some models consider dependence between sentences and sections, such as Hidden Markov Model [3, 27], Maximum Entropy Markov Model [19], and Conditional Random Fields [17], while many other approaches are based on lexical cohesion or similarity of sentences [5, 6, 15, 25, 21].",
                "Some approaches also focus on cue words as hints of topic transitions [11].",
                "While some existing methods only consider information in single documents [6, 15], others utilize multiple documents [16, 14].",
                "There are not many works in the latter category, even though the performance of segmentation is expected to be better with utilization of information from multiple documents.",
                "Previous research studied methods to find shared topics [16] and topic segmentation and summarization between just a pair of documents [14].",
                "Text classification and clustering is a related research area which categorizes documents into groups using supervised or unsupervised methods.",
                "Topical classification or clustering is an important direction in this area, especially co-clustering of documents and terms, such as LSA [9], PLSA [13], and approaches based on distances and bipartite graph partitioning [28] or maximum MI [2, 10], or maximum entropy [1, 18].",
                "Criteria of these approaches can be utilized in the issue of topic segmentation.",
                "Some of those methods have been extended into the area of topic segmentation, such as PLSA [5] and maximum entropy [7], but to our best knowledge, using MI for topic segmentation has not been studied. 3.",
                "PROBLEM FORMULATION Our goal is to segment documents and align the segments across documents (Figure 1).",
                "Let T be the set of terms {t1, t2, ..., tl}, which appear in the unlabelled set of documents D = {d1, d2, ..., dm}.",
                "Let Sd be the set of sentences for document d ∈ D, i.e. {s1, s2, ..., snd }.",
                "We have a 3D matrix of term frequency, in which the three dimensions are random variables of D, Sd, and T. Sd actually is a random vector including a random variable for each d ∈ D. The term frequency can be used to estimate the joint probability distribution P(D, Sd, T), which is p(t, d, s) = T(t, d, s)/ND, where T(t, d, s) is the number of t in ds sentence s and ND is the total number of terms in D. ˆS represents the set of segments {ˆs1, ˆs2, ..., ˆsp} after segmentation and alignment among multiple documents, where the number of segments | ˆS| = p. A segment ˆsi of document d is a sequence of adjacent sentences in d. Since for different documents si may discuss different sub-topics, our goal is to cluster adjacent sentences in each document into segments, and align similar segments among documents, so that for different documents ˆsi is about the same sub-topic.",
                "The goal is to find the optimal topic segmentation and alignment mapping Segd(si) : {s1, s2, ..., snd } → {ˆs1, ˆs2, ..., ˆsp} and Alid(ˆsi) : {ˆs1, ˆs2, ..., ˆsp} → {ˆs1, ˆs2, ..., ˆsp}, for all d ∈ D, where ˆsi is ith segment with the constraint that only adjacent sentences can be mapped to the same segment, i.e. for d, {si, si+1, ..., sj} → {ˆsq}, where q ∈ {1, ..., p}, where p is the segment number, and if i > j, then for d, ˆsq is missing.",
                "After segmentation and alignment, random vector Sd becomes an aligned random variable ˆS.",
                "Thus, P(D, Sd, T) becomes P(D, ˆS, T).",
                "Term co-clustering is a technique that has been employed [10] to improve the accuracy of document clustering.",
                "We evaluate the effect of it for topic segmentation.",
                "A term t is mapped to exactly one term cluster.",
                "Term co-clustering involves simultaneously finding the optimal term clustering mapping Clu(t) : {t1, t2, ..., tl} → {ˆt1, ˆt2, ..., ˆtk}, where k ≤ l, l is the total number of words in all the documents, and k is the number of clusters. 4.",
                "METHODOLOGY We now describe a novel algorithm which can handle singledocument segmentation, shared topic detection, and multidocument segmentation and alignment based on MI or WMI. 4.1 Mutual Information MI I(X; Y ) is a quantity to measure the amount of information which is contained in two or more random variables [8, 10].",
                "For the case of two random variables, we have I(X; Y ) = x∈X y∈Y p(x, y)log p(x, y) p(x)p(y) , (1) Obviously, when random variables X and Y are independent, I(X; Y ) = 0.",
                "Thus, intuitively, the value of MI depends on how random variables are dependent on each other.",
                "The optimal co-clustering is the mapping Clux : X → ˆX and Cluy : Y → ˆY that minimizes the loss: I(X; Y ) − I( ˆX; ˆY ), which is equal to maximizing I( ˆX; ˆY ).",
                "This is the criterion of MI for clustering.",
                "In the case of topic segmentation, the two random variables are the term variable T and the segment variable S, and each sample is an occurrence of a term T = t in a particular segment S = s. I(T; S) is used to measure how dependent T and S are.",
                "However, I(T; S) cannot be computed for documents before segmentation, since we do not have a set of S due to the fact that sentences of Document d, si ∈ Sd, is not aligned with other documents.",
                "Thus, instead of minimizing the loss of MI, we can maximize MI after topic segmentation, computed as: I( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS p(ˆt, ˆs)log p(ˆt, ˆs) p(ˆt)p(ˆs) , (2) where p(ˆt, ˆs) are estimated by the term frequency tf of Term Cluster ˆt and Segment ˆs in the training set D. Note that here a segment ˆs includes sentences about the the same topic among all documents.",
                "The optimal solution is the mapping Clut : T → ˆT, Segd : Sd → ˆS , and Alid : ˆS → ˆS, which maximizes I( ˆT; ˆS). 4.2 Weighted Mutual Information In topic segmentation and alignment of multiple documents, if P(D, ˆS, T) is known, based on the marginal distributions P(D|T) and P( ˆS|T) for each term t ∈ T, we can categorize terms into four types in the data set: • Common stop words are common both along the dimensions of documents and segments. • Document-dependent stop words that depends on the personal writing style are common only along the dimension of segments for some documents. • Cue words are the most important elements for segmentation.",
                "They are common along the dimension of documents only for the same segment, and they are not common along the dimensions of segments. • Noisy words are other words which are not common along both dimensions.",
                "Entropy based on P(D|T) and P( ˆS|T) can be used to identify different types of terms.",
                "To reinforce the contribution of cue words in the MI computation, and simultaneously reduce the effect of the other three types of words, similar as the idea of the tf-idf weight [22], we use entropies of each term along the dimensions of document D and segment ˆS, i.e.",
                "ED(ˆt) and EˆS(ˆt), to compute the weight.",
                "A cue word usually has a large value of ED(ˆt) but a small value of EˆS(ˆt).",
                "We introduce term weights (or term cluster weights) wˆt = ( ED(ˆt) maxˆt ∈ ˆT (ED(ˆt )) )a (1 − EˆS(ˆt) maxˆt ∈ ˆT (EˆS(ˆt )) )b , (3) where ED(ˆt) = d∈D p(d|ˆt)log|D| 1 p(d|ˆt) , EˆS(ˆt) = ˆs∈ ˆS p(ˆs|ˆt)log| ˆS| 1 p(ˆs|ˆt) , and a > 0 and b > 0 are powers to adjust term weights.",
                "Usually a = 1 and b = 1 as default, and maxˆt ∈ ˆT (ED(ˆt )) and maxˆt ∈ ˆT (EˆS(ˆt )) are used to normalize the entropy values.",
                "Term cluster weights are used to adjust p(ˆt, ˆs), pw(ˆt, ˆs) = wˆtp(ˆt, ˆs) ˆt∈ ˆT ;ˆs∈ ˆS wˆtp(ˆt, ˆs) , (4) and Iw( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS pw(ˆt, ˆs)log pw(ˆt, ˆs) pw(ˆt)pw(ˆs) , (5) where pw(ˆt) and pw(ˆs) are marginal distributions of pw(ˆt, ˆs).",
                "However, since we do not know either the term weights or P(D, ˆS, T), we need to estimate them, but wˆt depends on p(ˆs|t) and ˆS, while ˆS and p(ˆs|t) also depend on wˆt that is still unknown.",
                "Thus, an iterative algorithm is required to estimate term weights wˆt and find the best segmentation and alignment to optimize the objective function Iw concurrently.",
                "After a document is segmented into sentences Input: Joint probability distribution P(D, Sd, T), number of text segments p ∈ {2, 3, ..., max(sd)}, number of term clusters k ∈ {2, 3, ..., l} (if k = l, no term co-clustering required), and weight type w ∈ {0, 1}, indicating to use I or Iw, respectively.",
                "Output: Mapping Clu, Seg, Ali, and term weights wˆt.",
                "Initialization: 0. i = 0.",
                "Initialize Clu (0) t , Seg (0) d , and Ali (0) d ; Initialize w (0) ˆt using Equation (6) if w = 1; Stage 1: 1.",
                "If |D| = 1, k = l, and w = 0, check all sequential segmentations of d into p segments and find the best one Segd(s) = argmaxˆsI( ˆT; ˆS), and return Segd; otherwise, if w = 1 and k = l, go to 3.1; Stage 2: 2.1 If k < l, for each term t, find the best cluster ˆt as Clu(i+1)(t) = argmaxˆtI( ˆT; ˆS(i)) based on Seg(i) and Ali(i); 2.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+1) d (Seg (i+1) d (s)) = argmaxˆsI( ˆT(i+1); ˆS) based on Clu(i+1)(t) if k < l or Clu(0)(t) if k = l; 2.3 i + +.",
                "If Clu, Seg, or Ali changed, go to 2.1; otherwise, if w = 0, return Clu(i), Seg(i), and Ali(i); else j = 0, go to 3.1; Stage 3: 3.1 Update w (i+j+1) ˆt based on Seg(i+j), Ali(i+j), and Clu(i) using Equation (3); 3.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+j+1) d (Seg (i+j+1) d (s)) = argmaxˆsIw( ˆT(i); ˆS) based on Clu(i) and w (i+j+1) ˆt ; 3.3 j + +.",
                "If Iw( ˆT; ˆS) changes, go to step 6; otherwise, stop and return Clu(i), Seg(i+j), Ali(i+j), and w (i+j) ˆt ; Figure 2: Algorithm: Topic segmentation and alignment based on MI or WMI. and each sentence is segmented into words, each word is stemmed.",
                "Then the joint probability distribution P(D, Sd, T) can be estimated.",
                "Finally, this distribution can be used to compute MI in our algorithm. 4.3 Iterative Greedy Algorithm Our goal is to maximize the objective function, I( ˆT; ˆS) or Iw( ˆT; ˆS), which can measure the dependence of term occurrences in different segments.",
                "Generally, first we do not know the estimate term weights, which depend on the optimal topic segmentation and alignment, and term clusters.",
                "Moreover, this problem is NP-hard [10], even though if we know the term weights.",
                "Thus, an iterative greedy algorithm is desired to find the best solution, even though probably only local maxima are reached.",
                "We present the iterative greedy algorithm in Figure 2 to find a local maximum of I( ˆT; ˆS) or Iw( ˆT; ˆS) with simultaneous term weight estimation.",
                "This algorithm can is iterative and greedy for multi-document cases or single-document cases with term weight estimation and/or term co-clustering.",
                "Otherwise, since it is just a one step algorithm to solve the task of single-document segmentation [6, 15, 25], the global maximum of MI is guaranteed.",
                "We will show later that term co-clustering reduces the accuracy of the results and is not necessary, and for singledocument segmentation, term weights are also not required. 4.3.1 Initialization In Step 0, the initial term clustering Clut and topic segmentation and alignment Segd and Alid are important to avoid local maxima and reduce the number of iterations.",
                "First, a good guess of term weights can be made by using the distributions of term frequency along sentences for each document and averaging them to get the initial values of wˆt: wt = ( ED(t) maxt ∈T (ED(t )) )(1 − ES(t) maxt ∈T (ES(t )) ), (6) where ES(t) = 1 |Dt| d∈Dt (1 − s∈Sd p(s|t)log|Sd| 1 p(s|t) ), where Dt is the set of documents which contain Term t. Then, for the initial segmentation Seg(0) , we can simply segment documents equally by sentences.",
                "Or we can find the optimal segmentation just for each document d which maximizes the WMI, Seg (0) d = argmaxˆsIw(T; ˆS), where w = w (0) ˆt .",
                "For the initial alignment Ali(0) , we can first assume that the order of segments for each d is the same.",
                "For the initial term clustering Clu(0) , first cluster labels can be set randomly, and after the first time of Step 3, a good initial term clustering is obtained. 4.3.2 Different Cases After initialization, there are three stages for different cases.",
                "Totally there are eight cases, |D| = 1 or |D| > 1, k = l or k < l, w = 0 or w = 1.",
                "Single document segmentation without term clustering and term weight estimation (|D| = 1, k = l, w = 0) only requires Stage 1 (Step 1).",
                "If term clustering is required (k < l), Stage 2 (Step 2.1, 2.2, and 2.3) is executed iteratively.",
                "If term weight estimation is required (w = 1), Stage 3 (Step 3.1, 3.2, and 3.3) is executed iteratively.",
                "If both are required (k < l, w = 1), Stage 2 and 3 run one after the other.",
                "For multi-document segmentation without term clustering and term weight estimation (|D| > 1, k = l, w = 0), only iteration of Step 2.2 and 2.3 are required.",
                "At Stage 1, the global maximum can be found based on I( ˆT; ˆS) using dynamic programming in Section 4.4.",
                "Simultaneously finding a good term clustering and estimated term weights is impossible, since when moving a term to a new term cluster to maximize Iw( ˆT; ˆS), we do not know that the weight of this term should be the one of the new cluster or the old cluster.",
                "Thus, we first do term clustering at Stage 2, and then estimate term weights at Stage 3.",
                "At Stage 2, Step 2.1 is to find the best term clustering and Step 2.2 is to find the best segmentation.",
                "This cycle is repeated to find a local maximum based on MI I until it converges.",
                "The two steps are: (1) based on current term clustering Cluˆt, for each document d, the algorithm segments all the sentences Sd into p segments sequentially (some segments may be empty), and put them into the p segments ˆS of the whole training set D (all possible cases of different segmentation Segd and alignment Alid are checked) to find the optimal case, and (2) based on the current segmentation and alignment, for each term t, the algorithm finds the best term cluster of t based on the current segmentation Segd and alignment Alid.",
                "After finding a good term clustering, term weights are estimated if w = 1.",
                "At Stage 3, similar as Stage 2, Step 3.1 is term weight re-estimation and Step 3.2 is to find a better segmentation.",
                "They are repeated to find a local maximum based on WMI Iw until it converges.",
                "However, if the term clustering in Stage 2 is not accurate, then the term weight estimation at Stage 3 may have a bad result.",
                "Finally, at Step 3.3, this algorithm converges and return the output.",
                "This algorithm can handle both single-document and multi-document segmentation.",
                "It also can detect shared topics among documents by checking the proportion of overlapped sentences on the same topics, as described in Sec 5.2. 4.4 Algorithm Optimization In many previous works on segmentation, dynamic programming is a technique used to maximize the objective function.",
                "Similarly, at Step 1, 2.2, and 3.2 of our algorithm, we can use dynamic programming.",
                "For Stage 1, using dynamic programming can still find the global optimum, but for Stage 2 and Stage 3, we can only find the optimum for each step of topic segmentation and alignment of a document.",
                "Here we only show the dynamic programming for Step 3.2 using WMI (Step 1 and 2.2 are similar but they can use either I or Iw).",
                "There are two cases that are not shown in the algorithm in Figure 2: (a) single-document segmentation or multi-document segmentation with the same sequential order of segments, where alignment is not required, and (b) multi-document segmentation with different sequential orders of segments, where alignment is necessary.",
                "The alignment mapping function of the former case is simply just Alid(ˆsi) = ˆsi, while for the latter ones alignment mapping function Alid(ˆsi) = ˆsj, i and j may be different.",
                "The computational steps for the two cases are listed below: Case 1 (no alignment): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs) and partial pw(ˆs) without counting sentences from d. Then put sentences from i to j into Part k, and compute partial WMI PIw( ˆT; ˆsk(si, si+1, ..., sj)) ˆt∈ ˆT pw(ˆt, ˆsk)log pw(ˆt, ˆsk) pw(ˆt)pw(ˆsk) , where Alid(si, si+1, ..., sj) = k, k ∈ {1, 2, ..., p}, 1 ≤ i ≤ j ≤ nd, and Segd(sq) = ˆsk for all i ≤ q ≤ j. (2) Let M(sm, 1) = PIw( ˆT; ˆs1(s1, s2, ..., sm)).",
                "Then M(sm, L) = maxi[M(si−1, L − 1) + PIw( ˆT; ˆsL(si, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, and when i > m, no sentences are put into ˆsk when compute PIw (note PIw( ˆT; ˆs(si, ..., sm)) = 0 for single-document segmentation). (3) Finally M(snd , p) = maxi[M(si−1, p − 1)+ PIw( ˆT; ˆsp(si, ..., snd ))], where 1 ≤ i ≤ nd+1.",
                "The optimal Iw is found and the corresponding segmentation is the best.",
                "Case 2 (alignment required): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs), and partial pw(ˆs), and PIw( ˆT; ˆsk(si, si+1, ..., sj)) similarly as Case 1. (2) Let M(sm, 1, k) = PIw( ˆT; ˆsk(s1, s2, ..., sm)), where k ∈ {1, 2, ..., p}.",
                "Then M(sm, L, kL) = maxi,j[M(si−1, L − 1, kL/j) + PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, kL ∈ Set(p, L), which is the set of all p!",
                "L!(p−L)! combinations of L segments chosen from all p segments, j ∈ kL, the set of L segments chosen from all p segments, and kL/j is the combination of L − 1 segments in kL except Segment j. (3) Finally, M(snd , p, kp) = maxi,j[M(si−1, p − 1, kp/j) +PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., snd ))], where kp is just the combination of all p segments and 1 ≤ i ≤ nd + 1, which is the optimal Iw and the corresponding segmentation is the best.",
                "The steps of Case 1 and 2 are similar, except in Case 2, alignment is considered in addition to segmentation.",
                "First, basic items of probability for computing Iw are computed excluding Doc d, and then partial WMI by putting every possible sequential segment (including empty segment) of d into every segment of the set.",
                "Second, the optimal sum of PIw for L segments and the leftmost m sentences, M(sm, L), is found.",
                "Finally, the maximal WMI is found among different sums of M(sm, p − 1) and PIw for Segment p. 5.",
                "EXPERIMENTS In this section, single-document segmentation, shared topic detection, and multi-document segmentation will be tested.",
                "Different hyper parameters of our method are studied.",
                "For convenience, we refer to the method using I as MIk if w = 0, and Iw as WMIk if w = 2 or as WMIk if w = 1, where k is the number of term clusters, and if k = l, where l is the total number of terms, then no term clustering is required, i.e.",
                "MIl and WMIl. 5.1 Single-document Segmentation 5.1.1 Test Data and Evaluation The first data set we tested is a synthetic one used in previous research [6, 15, 25] and many other papers.",
                "It has 700 samples.",
                "Each is a concatenation of ten segments.",
                "Each segment is the first n sentence selected randomly from the Brown corpus, which is supposed to have a different topic from each other.",
                "Currently, the best results on this data set is achieved by Ji et.al. [15].",
                "To compare the performance of our methods, the criterion used widely in previous research is applied, instead of the unbiased criterion introduced in [20].",
                "It chooses a pair of words randomly.",
                "If they are in different segments (different) for the real segmentation (real), but predicted (pred) as in the same segment, it is a miss.",
                "If they are in the same segment (same), but predicted as in different segments, it is a false alarm.",
                "Thus, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, diff)p(diff|real) +p(false alarm|real, pred, same)p(same|real). 5.1.2 Experiment Results We tested the case when the number of segments is known.",
                "Table 1 shows the results of our methods with different hyper parameter values and three previous approaches, C99[25], U00[6], and ADDP03[15], on this data set when the segment number is known.",
                "In WMI for single-document segmentation, the term weights are computed as follows: wˆt = 1−EˆS(ˆt)/maxˆt ∈ ˆT (EˆS(ˆt )).",
                "For this case, our methods MIl and WMIl both outperform all the previous approaches.",
                "We compared our methods with ADDP03using one-sample one-sided t-test and p-values are shown in Table 2.",
                "From the p-values, we can see that mostly the differences are very Table 1: Average Error Rates of Single-document Segmentation Given Segment Numbers Known Range of n 3-11 3-5 6-8 9-11 Sample size 400 100 100 100 C99 12% 11% 10% 9% U00 10% 9% 7% 5% ADDP03 6.0% 6.8% 5.2% 4.3% MIl 4.68% 5.57% 2.59% 1.59% WMIl 4.94% 6.33% 2.76% 1.62% MI100 9.62% 12.92% 8.66% 6.67% Table 2: Single-document Segmentation: P-values of T-test on Error Rates Range of n 3-11 3-5 6-8 9-11 ADDP03, MIl 0.000 0.000 0.000 0.000 ADDP03, WMIl 0.000 0.099 0.000 0.000 MIl, WMIl 0.061 0.132 0.526 0.898 significant.",
                "We also compare the error rates between our two methods using two-sample two-sided t-test to check the hypothesis that they are equal.",
                "We cannot reject the hypothesis that they are equal, so the difference are not significant, even though all the error rates for MIl are smaller than WMIl.",
                "However, we can conclude that term weights contribute little in single-document segmentation.",
                "The results also show that MI using term co-clustering (k = 100) decreases the performance.",
                "We tested different number of term clusters, and found that the performance becomes better when the cluster number increases to reach l. WMIk<l has similar results that we did not show in the table.",
                "As mentioned before, using MI may be inconsistent on optimal boundaries given different numbers of segments.",
                "This situation occurs especially when the similarities among segments are quite different, i.e. some transitions are very obvious, while others are not.",
                "This is because usually a document is a hierarchical structure instead of only a sequential structure.",
                "When the segments are not at the same level, this situation may occur.",
                "Thus, a hierarchical topic segmentation approach is desired, and the structure highly depends on the number of segments for each internal node and the stop criteria of splitting.",
                "For this data set of singledocument segmentation, since it is just a synthetic set, which is just a concatenation of several segments about different topics, it is reasonable that approaches simply based on term frequency have a good performance.",
                "Usually for the tasks of segmenting coherent documents for sub-topics, the effectiveness decreases much. 5.2 Shared Topic Detection 5.2.1 Test Data and Evaluation The second data set contains 80 news articles from Google News.",
                "There are eight topics and each has 10 articles.",
                "We randomly split the set into subsets with different document numbers and each subset has all eight topics.",
                "We compare our approach MIl and WMIl with LDA [4].",
                "LDA treats a document in the data set as a bag of words, finds its distribution on topics, and its major topic.",
                "MIl and WMIl views each sentence as a bag of words and tag it with a topic label.",
                "Then for each pair of documents, LDA determines if they are on the same topic, while MIl and Table 3: Shared Topic Detection: Average Error Rates for Different Numbers of Documents in Each Subset #Doc 10 20 40 80 LDA 8.89% 16.33% 1.35% 0.60% MIl, θ = 0.6 4.17% 1.71% 1.47% 0.0% WMIl, θ = 0.8 18.6% 3.16% 1.92% 0.0% WMIl check whether the proportion overlapped sentences on the same topic is larger than the adjustable threshold θ.",
                "That is, in MIl and WMIl, for a pair of documents d, d , if [ s∈Sd,s ∈Sd 1(topics=topics )/min(|Sd|, |Sd|)] > θ, where Sd is the set of sentences of d, and |Sd| is the number of sentences of d, then d and d have the shared topic.",
                "For a pair of documents selected randomly, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, same)p(same|real) +p(false alarm|real, pred, diff)p(diff|real), where a miss means if they have the same topic (same) for the real case (real), but predicted (pred) as on the same topic.",
                "If they are on different topics (diff), but predicted as on the same topic, it is a false alarm. 5.2.2 Experiment Results The results are shown in Table 3.",
                "If most documents have different topics, in WMIl, the estimation of term weights in Equation (3) is not correct.",
                "Thus, WMIl is not expected to have a better performance than MIl, when most documents have different topics.",
                "When there are fewer documents in a subset with the same number of topics, more documents have different topics, so WMIl is more worse than MIl.",
                "We can see that for most cases MIl has a better (or at least similar) performance than LDA.",
                "After shared topic detection, multi-document segmentation of documents with the shared topics is able to be executed. 5.3 Multi-document Segmentation 5.3.1 Test Data and Evaluation For multi-document segmentation and alignment, our goal is to identify these segments about the same topic among multiple similar documents with shared topics.",
                "Using Iw is expected to perform better than I, since without term weights the result is affected seriously by document-dependent stop words and noisy words which depends on the personal writing style.",
                "It is more likely to treat the same segments of different documents as different segments under the effect of document-dependent stop words and noisy words.",
                "Term weights can reduce the effect of document-dependent stop words and noisy words by giving cue terms more weights.",
                "The data set for multi-document segmentation and alignment has 102 samples and 2264 sentences totally.",
                "Each is the introduction part of a lab report selected from the course of Biol 240W, Pennsylvania State University.",
                "Each sample has two segments, introduction of plant hormones and the content in the lab.",
                "The length range of samples is from two to 56 sentences.",
                "Some samples only have one part and some have a reverse order the these two segments.",
                "It is not hard to identify the boundary between two segments for human.",
                "We labelled each sentence manually for evaluation.",
                "The criterion of evaluation is just using the proportion of the number of sentences with wrong predicted segment labels in the total number of sentences in the whole training Table 4: Average Error Rates of Multi-document Segmentation Given Segment Numbers Known #Doc MIl WMIl k MIk WMIk 102 3.14% 2.78% 300 4.68% 6.58% 51 4.17% 3.63% 300 17.83% 22.84% 34 5.06% 4.12% 300 18.75% 20.95% 20 7.08% 5.42% 250 20.40% 21.83% 10 10.38% 7.89% 250 21.42% 21.91% 5 15.77% 11.64% 250 21.89% 22.59% 2 25.90% 23.18% 50 25.44% 25.49% 1 23.90% 24.82% 25 25.75% 26.15% Table 5: Multi-document Segmentation: P-values of T-test on Error Rates for MIl and WMIl #Doc 51 34 20 10 5 2 P-value 0.19 0.101 0.025 0.001 0.000 0.002 set as the error rate: p(error|predicted, real) = d∈D s∈Sd 1(predicteds=reals)/ d∈D nd.",
                "In order to show the benefits of multi-document segmentation and alignment, we compared our method with different parameters on different partitions of the same training set.",
                "Except the cases that the number of documents is 102 and one (they are special cases of using the whole set and the pure single-document segmentation), we randomly divided the training set into m partitions, and each has 51, 34, 20, 10, 5, and 2 document samples.",
                "Then we applied our methods on each partition and calculated the error rate of the whole training set.",
                "Each case was repeated for 10 times for computing the average error rates.",
                "For different partitions of the training set, different k values are used, since the number of terms increases when the document number in each partition increases. 5.3.2 Experiment Results From the experiment results in Table 4, we can see the following observations: (1) When the number of documents increases, all methods have better performances.",
                "Only from one to two documents, MIl has decrease a little.",
                "We can observe this from Figure 3 at the point of document number = 2.",
                "Most curves even have the worst results at this point.",
                "There are two reasons.",
                "First, because samples vote for the best multi-document segmentation and alignment, but if only two documents are compared with each other, the one with missing segments or a totally different sequence will affect the correct segmentation and alignment of the other.",
                "Second, as noted at the beginning of this section, if two documents have more document-dependent stop words or noisy words than cue words, then the algorithm may view them as two different segments and the other segment is missing.",
                "Generally, we can only expect a better performance when the number of documents is larger than the number of segments. (2) Except single-document segmentation, WMIl is always better than MIl, and when the number of documents is reaching one or increases to a very large number, their performances become closer.",
                "Table 5 shows p-values of twosample one-sided t-test between MIl and WMIl.",
                "We also can see this trend from p-values.",
                "When document number = 5, we reached the smallest p-value and the largest difference between error rates of MIl and WMIl.",
                "For single-document Table 6: Multi-document Segmentation: Average Error Rate for Document Number = 5 in Each Subset with Different Number of Term Clusters #Cluster 75 100 150 250 l MIk 24.67% 24.54% 23.91% 22.59% 15.77% segmentation, WMIl is even a little bit worse than MIl, which is similar as the results of the single-document segmentation on the first data set.",
                "The reason is that for singledocument segmentation, we cannot estimate term weights accurately, since multiple documents are unavailable. (3) Using term clustering usually gets worse results than MIl and WMIl. (4) Using term clustering in WMIk is even worse than in MIk, since in WMIk term clusters are found first using I before using Iw.",
                "If the term clusters are not correct, then the term weights are estimated worse, which may mislead the algorithm to reach even worse results.",
                "From the results we also found that in multi-document segmentation and alignment, most documents with missing segments and a reverse order are identified correctly.",
                "Table 6 illustrates the experiment results for the case of 20 partitions (each has five document samples) of the training set and topic segmentation and alignment using MIk with different numbers of term clusters k. Notice that when the number of term clusters increases, the error rate becomes smaller.",
                "Without term clustering, we have the best result.",
                "We did not show results for WMIk with term clustering, but the results are similar.",
                "We also tested WMIl with different hyper parameters of a and b to adjust term weights.",
                "The results are presented in Figure 3.",
                "It was shown that the default case WMIl : a = 1, b = 1 gave the best results for different partitions of the training set.",
                "We can see the trend that when the document number is very small or large, the difference between MIl : a = 0, b = 0 and WMIl : a = 1, b = 1 becomes quite small.",
                "When the document number is not large (about from 2 to 10), all the cases using term weights have better performances than MIl : a = 0, b = 0 without term weights, but when the document number becomes larger, the cases WMIl : a = 1, b = 0 and WMIl : a = 2, b = 1 become worse than MIl : a = 0, b = 0.",
                "When the document number becomes very large, they are even worse than cases with small document numbers.",
                "This means that a proper way to estimate term weights for the criterion of WMI is very important.",
                "Figure 4 shows the term weights learned from the whole training set.",
                "Four types of words are categorized roughly even though the transition among them are subtle.",
                "Figure 5 illustrates the change in (weighted) mutual information for MIl and WMIl.",
                "As expected, mutual information for MIl increases monotonically with the number of steps, while WMIl does not.",
                "Finally, MIl and WMIl are scalable, with computational complexity shown in Figure 6.",
                "One advantage for our approach based on MI is that removing stop words is not required.",
                "Another important advantage is that there are no necessary hyper parameters to adjust.",
                "In single-document segmentation, the performance based on MI is even better for that based on WMI, so no extra hyper parameter is required.",
                "In multi-document segmentation, we show in the experiment, a = 1 and b = 1 is the best.",
                "Our method gives more weights to cue terms.",
                "However, usually cue terms or sentences appear at the beginning of a segment, while the end of the segment may be 1 2 5 10 20 34 51 102 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 Document Number ErrorRate MIl :a=0,b=0 WMI l :a=1,b=1 WMI l :a=1,b=0 WMI l :a=2,b=1 Figure 3: Error rates for different hyper parameters of term weights. 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Normalized Document Entropy NormalizedSegmentEntropy Noisy words Cue words Common stop words Doc−dep stop words Figure 4: Term weights learned from the whole training set. 0 100 200 300 400 500 600 0.06 0.08 0.1 0.12 0.14 0.16 0.18 Number of Steps (Weighted)MutualInformation MI l WMI l Figure 5: Change in (weighted) MI for MIl and WMIl. 0 20 40 60 80 100 120 0 200 400 600 800 1000 1200 1400 1600 1800 2000 Document Number TimetoConverge(sec) MI l WMI l Figure 6: Time to converge for MIl and WMIl. much noisy.",
                "One possible solution is giving more weights to terms at the beginning of each segment.",
                "Moreover, when the length of segments are quite different, long segments have much higher term frequencies, so they may dominate the segmentation boundaries.",
                "Normalization of term frequencies versus the segment length may be useful. 6.",
                "CONCLUSIONS AND FUTURE WORK We proposed a novel method for multi-document topic segmentation and alignment based on weighted mutual information, which can also handle single-document cases.",
                "We used dynamic programming to optimize our algorithm.",
                "Our approach outperforms all the previous methods on singledocument cases.",
                "Moreover, we also showed that doing segmentation among multiple documents can improve the performance tremendously.",
                "Our results also illustrated that using weighted mutual information can utilize the information of multiple documents to reach a better performance.",
                "We only tested our method on limited data sets.",
                "More data sets especially complicated ones should be tested.",
                "More previous methods should be compared with.",
                "Moreover, natural segmentations like paragraphs are hints that can be used to find the optimal boundaries.",
                "Supervised learning also can be considered. 7.",
                "ACKNOWLEDGMENTS The authors want to thank Xiang Ji, and Prof. J. Scott Payne for their help. 8.",
                "REFERENCES [1] A. Banerjee, I. Ghillon, J. Ghosh, S. Merugu, and D. Modha.",
                "A generalized maximum entropy approach to bregman co-clustering and matrix approximation.",
                "In Proceedings of SIGKDD, 2004. [2] R. Bekkerman, R. El-Yaniv, and A. McCallum.",
                "Multi-way distributional clustering via pairwise interactions.",
                "In Proceedings of ICML, 2005. [3] D. M. Blei and P. J. Moreno.",
                "Topic segmentation with an aspect hidden markov model.",
                "In Proceedings of SIGIR, 2001. [4] D. M. Blei, A. Ng, and M. Jordan.",
                "Latent dirichlet allocation.",
                "Journal of Machine Learning Research, 3:993-1022, 2003. [5] T. Brants, F. Chen, and I. Tsochantaridis.",
                "Topic-based document segmentation with probabilistic latent semantic analysis.",
                "In Proceedings of CIKM, 2002. [6] F. Choi.",
                "Advances in domain indepedent linear text segmentation.",
                "In Proceedings of the NAACL, 2000. [7] H. Christensen, B. Kolluru, Y. Gotoh, and S. Renals.",
                "Maximum entropy segmentation of broadcast news.",
                "In Proceedings of ICASSP, 2005. [8] T. Cover and J. Thomas.",
                "Elements of Information Theory.",
                "John Wiley and Sons, New York, USA, 1991. [9] S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and R. Harshman.",
                "Indexing by latent semantic analysis.",
                "Journal of the American Society for Information Systems, 1990. [10] I. Dhillon, S. Mallela, and D. Modha.",
                "Information-theoretic co-clustering.",
                "In Proceedings of SIGKDD, 2003. [11] M. Hajime, H. Takeo, and O. Manabu.",
                "Text segmentation with multiple surface linguistic cues.",
                "In Proceedings of COLING-ACL, 1998. [12] T. K. Ho.",
                "Stop word location and identification for adaptive text recognition.",
                "International Journal of Document Analysis and Recognition, 3(1), August 2000. [13] T. Hofmann.",
                "Probabilistic latent semantic analysis.",
                "In Proceedings of the UAI99, 1999. [14] X. Ji and H. Zha.",
                "Correlating summarization of a pair of multilingual documents.",
                "In Proceedings of RIDE, 2003. [15] X. Ji and H. Zha.",
                "Domain-independent text segmentation using anisotropic diffusion and dynamic programming.",
                "In Proceedings of SIGIR, 2003. [16] X. Ji and H. Zha.",
                "Extracting shared topics of multiple documents.",
                "In Proceedings of the 7th PAKDD, 2003. [17] J. Lafferty, A. McCallum, and F. Pereira.",
                "Conditional random fields: Probabilistic models for segmenting and labeling sequence data.",
                "In Proceedings of ICML, 2001. [18] T. Li, S. Ma, and M. Ogihara.",
                "Entropy-based criterion in categorical clustering.",
                "In Proceedings of ICML, 2004. [19] A. McCallum, D. Freitag, and F. Pereira.",
                "Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of ICML, 2000. [20] L. Pevzner and M. Hearst.",
                "A critique and improvement of an evaluation metric for text segmentation.",
                "Computational Linguistic, 28(1):19-36, 2002. [21] J. C. Reynar.",
                "Statistical models for topic segmentation.",
                "In Proceedings of ACL, 1999. [22] G. Salton and M. McGill.",
                "Introduction to Modern Information Retrieval.",
                "McGraw Hill, 1983. [23] B.",
                "Sun, Q. Tan, P. Mitra, and C. L. Giles.",
                "Extraction and search of chemical formulae in text documents on the web.",
                "In Proceedings of WWW, 2007. [24] B.",
                "Sun, D. Zhou, H. Zha, and J.",
                "Yen.",
                "Multi-task text segmentation and alignment based on weighted mutual information.",
                "In Proceedings of CIKM, 2006. [25] M. Utiyama and H. Isahara.",
                "A statistical model for domain-independent text segmentation.",
                "In Proceedings of the 39th ACL, 1999. [26] C. Wayne.",
                "Multilingual topic detection and tracking: Successful research enabled by corpora and evaluation.",
                "In Proceedings of LREC, 2000. [27] J. Yamron, I. Carp, L. Gillick, S. Lowe, and P. van Mulbregt.",
                "A hidden markov model approach to text segmentation and event tracking.",
                "In Proceedings of ICASSP, 1998. [28] H. Zha and X. Ji.",
                "Correlating multilingual documents via bipartite graph modeling.",
                "In Proceedings of SIGIR, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "shared topic": {
            "translated_key": "temas compartidos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Topic Segmentation with <br>shared topic</br> Detection and Alignment of Multiple Documents Bingjun Sun*, Prasenjit Mitra*† , Hongyuan Zha‡ , C. Lee Giles*† , John Yen*† *Department of Computer Science and Engineering † College of Information Sciences and Technology The Pennsylvania State University University Park, PA 16802 ‡ College of Computing The Georgia Institute of Technology Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu ABSTRACT Topic detection and tracking [26] and topic segmentation [15] play an important role in capturing the local and sequential information of documents.",
                "Previous work in this area usually focuses on single documents, although similar multiple documents are available in many domains.",
                "In this paper, we introduce a novel unsupervised method for <br>shared topic</br> detection and topic segmentation of multiple similar documents based on mutual information (MI) and weighted mutual information (WMI) that is a combination of MI and term weights.",
                "The basic idea is that the optimal segmentation maximizes MI(or WMI).",
                "Our approach can detect shared topics among documents.",
                "It can find the optimal boundaries in a document, and align segments among documents at the same time.",
                "It also can handle single-document segmentation as a special case of the multi-document segmentation and alignment.",
                "Our methods can identify and strengthen cue terms that can be used for segmentation and partially remove stop words by using term weights based on entropy learned from multiple documents.",
                "Our experimental results show that our algorithm works well for the tasks of single-document segmentation, <br>shared topic</br> detection, and multi-document segmentation.",
                "Utilizing information from multiple documents can tremendously improve the performance of topic segmentation, and using WMI is even better than using MI for the multi-document segmentation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Clustering; H.3.1 [Information Storage and Retrieval]: Content Analysis and IndexingLinguistic processing; I.2.7 [Artificial Intelligence]: Natural Language Processing-Text analysis; I.5.3 [Pattern Recognition]: Clustering-Algorithms;Similarity measures General Terms Algorithms, Design, Experimentation 1.",
                "INTRODUCTION Many researchers have worked on topic detection and tracking (TDT) [26] and topic segmentation during the past decade.",
                "Topic segmentation intends to identify the boundaries in a document with the goal to capture the latent topical structure.",
                "Topic segmentation tasks usually fall into two categories [15]: text stream segmentation where topic transition is identified, and coherent document segmentation in which documents are split into sub-topics.",
                "The former category has applications in automatic speech recognition, while the latter one has more applications such as partial-text query of long documents in information retrieval, text summary, and quality measurement of multiple documents.",
                "Previous research in connection with TDT falls into the former category, targeted on topic tracking of broadcast speech data and newswire text, while the latter category has not been studied very well.",
                "Traditional approaches perform topic segmentation on documents one at a time [15, 25, 6].",
                "Most of them perform badly in subtle tasks like coherent document segmentation [15].",
                "Often, end-users seek documents that have the similar content.",
                "Search engines, like, Google, provide links to obtain similar pages.",
                "At a finer granularity, users may actually be looking to obtain sections of a document similar to a particular section that presumably discusses a topic of the users interest.",
                "Thus, the extension of topic segmentation from single documents to identifying similar segments from multiple similar documents with the same topic is a natural and necessary direction, and multi-document topic segmentation is expected to have a better performance since more information is utilized.",
                "Traditional approaches using similarity measurement based on term frequency generally have the same assumption that similar vocabulary tends to be in a coherent topic segment [15, 25, 6].",
                "However, they usually suffer the issue of identifying stop words.",
                "For example, additional document-dependent stop words are removed together with the generic stop words in [15].",
                "There are two reasons that we do not remove stop words directly.",
                "First, identifying stop words is another issue [12] that requires estimation in each domain.",
                "Removing common stop words may result in the loss of useful information in a specific domain.",
                "Second, even though stop words can be identified, hard classification of stop words and nonstop words cannot represent the gradually changing amount of information content of each word.",
                "We employ a soft classification using term weights.",
                "In this paper, we view the problem of topic segmentation as an optimization issue using information theoretic techniques to find the optimal boundaries of a document given the number of text segments so as to minimize the loss of mutual information (MI) (or a weighted mutual information (WMI)) after segmentation and alignment.",
                "This is equal to maximizing the MI (or WMI).",
                "The MI focuses on measuring the difference among segments whereas previous research focused on finding the similarity (e.g. cosine distance) of segments [15, 25, 6].",
                "Topic alignment of multiple similar documents can be achieved by clustering sentences on the same topic into the same cluster.",
                "Single-document topic segmentation is just a special case of the multi-document topic segmentation and alignment problem.",
                "Terms can be co-clustered as in [10] at the same time, given the number of clusters, but our experimental results show that this method results in a worse segmentation (see Tables 1, 4, and 6).",
                "Usually, human readers can identify topic transition based on cue words, and can ignore stop words.",
                "Inspired by this, we give each term (or term cluster) a weight based on entropy among different documents and different segments of documents.",
                "Not only can this approach increase the contribution of cue words, but it can also decrease the effect of common stop words, noisy word, and document-dependent stop words.",
                "These words are common in a document.",
                "Many methods based on sentence similarity require that these words are removed before topic segmentation can be performed [15].",
                "Our results in Figure 3 show that term weights are useful for multi-document topic segmentation and alignment.",
                "The major contribution of this paper is that it introduces a novel method for topic segmentation using MI and shows that this method performs better than previously used criteria.",
                "Also, we have addressed the problem of topic segmentation and alignment across multiple documents, whereas most existing research focused on segmentation of single documents.",
                "Multi-document segmentation and alignment can utilize information from similar documents and improves the performance of topic segmentation greatly.",
                "Obviously, our approach can handle single documents as a special case when multiple documents are unavailable.",
                "It can detect shared topics among documents to judge if they are multiple documents on the same topic.",
                "We also introduce the new criterion of WMI based on term weights learned from multiple similar documents, which can improve performance of topic segmentation further.",
                "We propose an iterative greedy algorithm based on dynamic programming and show that it works well in practice.",
                "Some of our prior work is in [24].",
                "The rest of this paper is organized as follows: In Section 2, we review related work.",
                "Section 3 contains a formulation of the problem of topic segmentation and alignment of multiple documents with term co-clustering, a review of the criterion of MI for clustering, and finally an introduction to WMI.",
                "In Section 4, we first propose the iterative greedy algorithm of topic segmentation and alignment with term co-clustering, and then describe how the algorithm can be optimized by usFigure 1: Illustration of multi-document segmentation and alignment. ing dynamic programming.",
                "In Section 5, experiments about single-document segmentation, <br>shared topic</br> detection, and multi-document segmentation are described, and results are presented and discussed to evaluate the performance of our algorithm.",
                "Conclusions and some future directions of the research work are discussed in Section 6. 2.",
                "PREVIOUS WORK Generally, the existing approaches to text segmentation fall into two categories: supervised learning [19, 17, 23] and unsupervised learning [3, 27, 5, 6, 15, 25, 21].",
                "Supervised learning usually has good performance, since it learns functions from labelled training sets.",
                "However, often getting large training sets with manual labels on document sentences is prohibitively expensive, so unsupervised approaches are desired.",
                "Some models consider dependence between sentences and sections, such as Hidden Markov Model [3, 27], Maximum Entropy Markov Model [19], and Conditional Random Fields [17], while many other approaches are based on lexical cohesion or similarity of sentences [5, 6, 15, 25, 21].",
                "Some approaches also focus on cue words as hints of topic transitions [11].",
                "While some existing methods only consider information in single documents [6, 15], others utilize multiple documents [16, 14].",
                "There are not many works in the latter category, even though the performance of segmentation is expected to be better with utilization of information from multiple documents.",
                "Previous research studied methods to find shared topics [16] and topic segmentation and summarization between just a pair of documents [14].",
                "Text classification and clustering is a related research area which categorizes documents into groups using supervised or unsupervised methods.",
                "Topical classification or clustering is an important direction in this area, especially co-clustering of documents and terms, such as LSA [9], PLSA [13], and approaches based on distances and bipartite graph partitioning [28] or maximum MI [2, 10], or maximum entropy [1, 18].",
                "Criteria of these approaches can be utilized in the issue of topic segmentation.",
                "Some of those methods have been extended into the area of topic segmentation, such as PLSA [5] and maximum entropy [7], but to our best knowledge, using MI for topic segmentation has not been studied. 3.",
                "PROBLEM FORMULATION Our goal is to segment documents and align the segments across documents (Figure 1).",
                "Let T be the set of terms {t1, t2, ..., tl}, which appear in the unlabelled set of documents D = {d1, d2, ..., dm}.",
                "Let Sd be the set of sentences for document d ∈ D, i.e. {s1, s2, ..., snd }.",
                "We have a 3D matrix of term frequency, in which the three dimensions are random variables of D, Sd, and T. Sd actually is a random vector including a random variable for each d ∈ D. The term frequency can be used to estimate the joint probability distribution P(D, Sd, T), which is p(t, d, s) = T(t, d, s)/ND, where T(t, d, s) is the number of t in ds sentence s and ND is the total number of terms in D. ˆS represents the set of segments {ˆs1, ˆs2, ..., ˆsp} after segmentation and alignment among multiple documents, where the number of segments | ˆS| = p. A segment ˆsi of document d is a sequence of adjacent sentences in d. Since for different documents si may discuss different sub-topics, our goal is to cluster adjacent sentences in each document into segments, and align similar segments among documents, so that for different documents ˆsi is about the same sub-topic.",
                "The goal is to find the optimal topic segmentation and alignment mapping Segd(si) : {s1, s2, ..., snd } → {ˆs1, ˆs2, ..., ˆsp} and Alid(ˆsi) : {ˆs1, ˆs2, ..., ˆsp} → {ˆs1, ˆs2, ..., ˆsp}, for all d ∈ D, where ˆsi is ith segment with the constraint that only adjacent sentences can be mapped to the same segment, i.e. for d, {si, si+1, ..., sj} → {ˆsq}, where q ∈ {1, ..., p}, where p is the segment number, and if i > j, then for d, ˆsq is missing.",
                "After segmentation and alignment, random vector Sd becomes an aligned random variable ˆS.",
                "Thus, P(D, Sd, T) becomes P(D, ˆS, T).",
                "Term co-clustering is a technique that has been employed [10] to improve the accuracy of document clustering.",
                "We evaluate the effect of it for topic segmentation.",
                "A term t is mapped to exactly one term cluster.",
                "Term co-clustering involves simultaneously finding the optimal term clustering mapping Clu(t) : {t1, t2, ..., tl} → {ˆt1, ˆt2, ..., ˆtk}, where k ≤ l, l is the total number of words in all the documents, and k is the number of clusters. 4.",
                "METHODOLOGY We now describe a novel algorithm which can handle singledocument segmentation, <br>shared topic</br> detection, and multidocument segmentation and alignment based on MI or WMI. 4.1 Mutual Information MI I(X; Y ) is a quantity to measure the amount of information which is contained in two or more random variables [8, 10].",
                "For the case of two random variables, we have I(X; Y ) = x∈X y∈Y p(x, y)log p(x, y) p(x)p(y) , (1) Obviously, when random variables X and Y are independent, I(X; Y ) = 0.",
                "Thus, intuitively, the value of MI depends on how random variables are dependent on each other.",
                "The optimal co-clustering is the mapping Clux : X → ˆX and Cluy : Y → ˆY that minimizes the loss: I(X; Y ) − I( ˆX; ˆY ), which is equal to maximizing I( ˆX; ˆY ).",
                "This is the criterion of MI for clustering.",
                "In the case of topic segmentation, the two random variables are the term variable T and the segment variable S, and each sample is an occurrence of a term T = t in a particular segment S = s. I(T; S) is used to measure how dependent T and S are.",
                "However, I(T; S) cannot be computed for documents before segmentation, since we do not have a set of S due to the fact that sentences of Document d, si ∈ Sd, is not aligned with other documents.",
                "Thus, instead of minimizing the loss of MI, we can maximize MI after topic segmentation, computed as: I( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS p(ˆt, ˆs)log p(ˆt, ˆs) p(ˆt)p(ˆs) , (2) where p(ˆt, ˆs) are estimated by the term frequency tf of Term Cluster ˆt and Segment ˆs in the training set D. Note that here a segment ˆs includes sentences about the the same topic among all documents.",
                "The optimal solution is the mapping Clut : T → ˆT, Segd : Sd → ˆS , and Alid : ˆS → ˆS, which maximizes I( ˆT; ˆS). 4.2 Weighted Mutual Information In topic segmentation and alignment of multiple documents, if P(D, ˆS, T) is known, based on the marginal distributions P(D|T) and P( ˆS|T) for each term t ∈ T, we can categorize terms into four types in the data set: • Common stop words are common both along the dimensions of documents and segments. • Document-dependent stop words that depends on the personal writing style are common only along the dimension of segments for some documents. • Cue words are the most important elements for segmentation.",
                "They are common along the dimension of documents only for the same segment, and they are not common along the dimensions of segments. • Noisy words are other words which are not common along both dimensions.",
                "Entropy based on P(D|T) and P( ˆS|T) can be used to identify different types of terms.",
                "To reinforce the contribution of cue words in the MI computation, and simultaneously reduce the effect of the other three types of words, similar as the idea of the tf-idf weight [22], we use entropies of each term along the dimensions of document D and segment ˆS, i.e.",
                "ED(ˆt) and EˆS(ˆt), to compute the weight.",
                "A cue word usually has a large value of ED(ˆt) but a small value of EˆS(ˆt).",
                "We introduce term weights (or term cluster weights) wˆt = ( ED(ˆt) maxˆt ∈ ˆT (ED(ˆt )) )a (1 − EˆS(ˆt) maxˆt ∈ ˆT (EˆS(ˆt )) )b , (3) where ED(ˆt) = d∈D p(d|ˆt)log|D| 1 p(d|ˆt) , EˆS(ˆt) = ˆs∈ ˆS p(ˆs|ˆt)log| ˆS| 1 p(ˆs|ˆt) , and a > 0 and b > 0 are powers to adjust term weights.",
                "Usually a = 1 and b = 1 as default, and maxˆt ∈ ˆT (ED(ˆt )) and maxˆt ∈ ˆT (EˆS(ˆt )) are used to normalize the entropy values.",
                "Term cluster weights are used to adjust p(ˆt, ˆs), pw(ˆt, ˆs) = wˆtp(ˆt, ˆs) ˆt∈ ˆT ;ˆs∈ ˆS wˆtp(ˆt, ˆs) , (4) and Iw( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS pw(ˆt, ˆs)log pw(ˆt, ˆs) pw(ˆt)pw(ˆs) , (5) where pw(ˆt) and pw(ˆs) are marginal distributions of pw(ˆt, ˆs).",
                "However, since we do not know either the term weights or P(D, ˆS, T), we need to estimate them, but wˆt depends on p(ˆs|t) and ˆS, while ˆS and p(ˆs|t) also depend on wˆt that is still unknown.",
                "Thus, an iterative algorithm is required to estimate term weights wˆt and find the best segmentation and alignment to optimize the objective function Iw concurrently.",
                "After a document is segmented into sentences Input: Joint probability distribution P(D, Sd, T), number of text segments p ∈ {2, 3, ..., max(sd)}, number of term clusters k ∈ {2, 3, ..., l} (if k = l, no term co-clustering required), and weight type w ∈ {0, 1}, indicating to use I or Iw, respectively.",
                "Output: Mapping Clu, Seg, Ali, and term weights wˆt.",
                "Initialization: 0. i = 0.",
                "Initialize Clu (0) t , Seg (0) d , and Ali (0) d ; Initialize w (0) ˆt using Equation (6) if w = 1; Stage 1: 1.",
                "If |D| = 1, k = l, and w = 0, check all sequential segmentations of d into p segments and find the best one Segd(s) = argmaxˆsI( ˆT; ˆS), and return Segd; otherwise, if w = 1 and k = l, go to 3.1; Stage 2: 2.1 If k < l, for each term t, find the best cluster ˆt as Clu(i+1)(t) = argmaxˆtI( ˆT; ˆS(i)) based on Seg(i) and Ali(i); 2.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+1) d (Seg (i+1) d (s)) = argmaxˆsI( ˆT(i+1); ˆS) based on Clu(i+1)(t) if k < l or Clu(0)(t) if k = l; 2.3 i + +.",
                "If Clu, Seg, or Ali changed, go to 2.1; otherwise, if w = 0, return Clu(i), Seg(i), and Ali(i); else j = 0, go to 3.1; Stage 3: 3.1 Update w (i+j+1) ˆt based on Seg(i+j), Ali(i+j), and Clu(i) using Equation (3); 3.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+j+1) d (Seg (i+j+1) d (s)) = argmaxˆsIw( ˆT(i); ˆS) based on Clu(i) and w (i+j+1) ˆt ; 3.3 j + +.",
                "If Iw( ˆT; ˆS) changes, go to step 6; otherwise, stop and return Clu(i), Seg(i+j), Ali(i+j), and w (i+j) ˆt ; Figure 2: Algorithm: Topic segmentation and alignment based on MI or WMI. and each sentence is segmented into words, each word is stemmed.",
                "Then the joint probability distribution P(D, Sd, T) can be estimated.",
                "Finally, this distribution can be used to compute MI in our algorithm. 4.3 Iterative Greedy Algorithm Our goal is to maximize the objective function, I( ˆT; ˆS) or Iw( ˆT; ˆS), which can measure the dependence of term occurrences in different segments.",
                "Generally, first we do not know the estimate term weights, which depend on the optimal topic segmentation and alignment, and term clusters.",
                "Moreover, this problem is NP-hard [10], even though if we know the term weights.",
                "Thus, an iterative greedy algorithm is desired to find the best solution, even though probably only local maxima are reached.",
                "We present the iterative greedy algorithm in Figure 2 to find a local maximum of I( ˆT; ˆS) or Iw( ˆT; ˆS) with simultaneous term weight estimation.",
                "This algorithm can is iterative and greedy for multi-document cases or single-document cases with term weight estimation and/or term co-clustering.",
                "Otherwise, since it is just a one step algorithm to solve the task of single-document segmentation [6, 15, 25], the global maximum of MI is guaranteed.",
                "We will show later that term co-clustering reduces the accuracy of the results and is not necessary, and for singledocument segmentation, term weights are also not required. 4.3.1 Initialization In Step 0, the initial term clustering Clut and topic segmentation and alignment Segd and Alid are important to avoid local maxima and reduce the number of iterations.",
                "First, a good guess of term weights can be made by using the distributions of term frequency along sentences for each document and averaging them to get the initial values of wˆt: wt = ( ED(t) maxt ∈T (ED(t )) )(1 − ES(t) maxt ∈T (ES(t )) ), (6) where ES(t) = 1 |Dt| d∈Dt (1 − s∈Sd p(s|t)log|Sd| 1 p(s|t) ), where Dt is the set of documents which contain Term t. Then, for the initial segmentation Seg(0) , we can simply segment documents equally by sentences.",
                "Or we can find the optimal segmentation just for each document d which maximizes the WMI, Seg (0) d = argmaxˆsIw(T; ˆS), where w = w (0) ˆt .",
                "For the initial alignment Ali(0) , we can first assume that the order of segments for each d is the same.",
                "For the initial term clustering Clu(0) , first cluster labels can be set randomly, and after the first time of Step 3, a good initial term clustering is obtained. 4.3.2 Different Cases After initialization, there are three stages for different cases.",
                "Totally there are eight cases, |D| = 1 or |D| > 1, k = l or k < l, w = 0 or w = 1.",
                "Single document segmentation without term clustering and term weight estimation (|D| = 1, k = l, w = 0) only requires Stage 1 (Step 1).",
                "If term clustering is required (k < l), Stage 2 (Step 2.1, 2.2, and 2.3) is executed iteratively.",
                "If term weight estimation is required (w = 1), Stage 3 (Step 3.1, 3.2, and 3.3) is executed iteratively.",
                "If both are required (k < l, w = 1), Stage 2 and 3 run one after the other.",
                "For multi-document segmentation without term clustering and term weight estimation (|D| > 1, k = l, w = 0), only iteration of Step 2.2 and 2.3 are required.",
                "At Stage 1, the global maximum can be found based on I( ˆT; ˆS) using dynamic programming in Section 4.4.",
                "Simultaneously finding a good term clustering and estimated term weights is impossible, since when moving a term to a new term cluster to maximize Iw( ˆT; ˆS), we do not know that the weight of this term should be the one of the new cluster or the old cluster.",
                "Thus, we first do term clustering at Stage 2, and then estimate term weights at Stage 3.",
                "At Stage 2, Step 2.1 is to find the best term clustering and Step 2.2 is to find the best segmentation.",
                "This cycle is repeated to find a local maximum based on MI I until it converges.",
                "The two steps are: (1) based on current term clustering Cluˆt, for each document d, the algorithm segments all the sentences Sd into p segments sequentially (some segments may be empty), and put them into the p segments ˆS of the whole training set D (all possible cases of different segmentation Segd and alignment Alid are checked) to find the optimal case, and (2) based on the current segmentation and alignment, for each term t, the algorithm finds the best term cluster of t based on the current segmentation Segd and alignment Alid.",
                "After finding a good term clustering, term weights are estimated if w = 1.",
                "At Stage 3, similar as Stage 2, Step 3.1 is term weight re-estimation and Step 3.2 is to find a better segmentation.",
                "They are repeated to find a local maximum based on WMI Iw until it converges.",
                "However, if the term clustering in Stage 2 is not accurate, then the term weight estimation at Stage 3 may have a bad result.",
                "Finally, at Step 3.3, this algorithm converges and return the output.",
                "This algorithm can handle both single-document and multi-document segmentation.",
                "It also can detect shared topics among documents by checking the proportion of overlapped sentences on the same topics, as described in Sec 5.2. 4.4 Algorithm Optimization In many previous works on segmentation, dynamic programming is a technique used to maximize the objective function.",
                "Similarly, at Step 1, 2.2, and 3.2 of our algorithm, we can use dynamic programming.",
                "For Stage 1, using dynamic programming can still find the global optimum, but for Stage 2 and Stage 3, we can only find the optimum for each step of topic segmentation and alignment of a document.",
                "Here we only show the dynamic programming for Step 3.2 using WMI (Step 1 and 2.2 are similar but they can use either I or Iw).",
                "There are two cases that are not shown in the algorithm in Figure 2: (a) single-document segmentation or multi-document segmentation with the same sequential order of segments, where alignment is not required, and (b) multi-document segmentation with different sequential orders of segments, where alignment is necessary.",
                "The alignment mapping function of the former case is simply just Alid(ˆsi) = ˆsi, while for the latter ones alignment mapping function Alid(ˆsi) = ˆsj, i and j may be different.",
                "The computational steps for the two cases are listed below: Case 1 (no alignment): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs) and partial pw(ˆs) without counting sentences from d. Then put sentences from i to j into Part k, and compute partial WMI PIw( ˆT; ˆsk(si, si+1, ..., sj)) ˆt∈ ˆT pw(ˆt, ˆsk)log pw(ˆt, ˆsk) pw(ˆt)pw(ˆsk) , where Alid(si, si+1, ..., sj) = k, k ∈ {1, 2, ..., p}, 1 ≤ i ≤ j ≤ nd, and Segd(sq) = ˆsk for all i ≤ q ≤ j. (2) Let M(sm, 1) = PIw( ˆT; ˆs1(s1, s2, ..., sm)).",
                "Then M(sm, L) = maxi[M(si−1, L − 1) + PIw( ˆT; ˆsL(si, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, and when i > m, no sentences are put into ˆsk when compute PIw (note PIw( ˆT; ˆs(si, ..., sm)) = 0 for single-document segmentation). (3) Finally M(snd , p) = maxi[M(si−1, p − 1)+ PIw( ˆT; ˆsp(si, ..., snd ))], where 1 ≤ i ≤ nd+1.",
                "The optimal Iw is found and the corresponding segmentation is the best.",
                "Case 2 (alignment required): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs), and partial pw(ˆs), and PIw( ˆT; ˆsk(si, si+1, ..., sj)) similarly as Case 1. (2) Let M(sm, 1, k) = PIw( ˆT; ˆsk(s1, s2, ..., sm)), where k ∈ {1, 2, ..., p}.",
                "Then M(sm, L, kL) = maxi,j[M(si−1, L − 1, kL/j) + PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, kL ∈ Set(p, L), which is the set of all p!",
                "L!(p−L)! combinations of L segments chosen from all p segments, j ∈ kL, the set of L segments chosen from all p segments, and kL/j is the combination of L − 1 segments in kL except Segment j. (3) Finally, M(snd , p, kp) = maxi,j[M(si−1, p − 1, kp/j) +PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., snd ))], where kp is just the combination of all p segments and 1 ≤ i ≤ nd + 1, which is the optimal Iw and the corresponding segmentation is the best.",
                "The steps of Case 1 and 2 are similar, except in Case 2, alignment is considered in addition to segmentation.",
                "First, basic items of probability for computing Iw are computed excluding Doc d, and then partial WMI by putting every possible sequential segment (including empty segment) of d into every segment of the set.",
                "Second, the optimal sum of PIw for L segments and the leftmost m sentences, M(sm, L), is found.",
                "Finally, the maximal WMI is found among different sums of M(sm, p − 1) and PIw for Segment p. 5.",
                "EXPERIMENTS In this section, single-document segmentation, <br>shared topic</br> detection, and multi-document segmentation will be tested.",
                "Different hyper parameters of our method are studied.",
                "For convenience, we refer to the method using I as MIk if w = 0, and Iw as WMIk if w = 2 or as WMIk if w = 1, where k is the number of term clusters, and if k = l, where l is the total number of terms, then no term clustering is required, i.e.",
                "MIl and WMIl. 5.1 Single-document Segmentation 5.1.1 Test Data and Evaluation The first data set we tested is a synthetic one used in previous research [6, 15, 25] and many other papers.",
                "It has 700 samples.",
                "Each is a concatenation of ten segments.",
                "Each segment is the first n sentence selected randomly from the Brown corpus, which is supposed to have a different topic from each other.",
                "Currently, the best results on this data set is achieved by Ji et.al. [15].",
                "To compare the performance of our methods, the criterion used widely in previous research is applied, instead of the unbiased criterion introduced in [20].",
                "It chooses a pair of words randomly.",
                "If they are in different segments (different) for the real segmentation (real), but predicted (pred) as in the same segment, it is a miss.",
                "If they are in the same segment (same), but predicted as in different segments, it is a false alarm.",
                "Thus, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, diff)p(diff|real) +p(false alarm|real, pred, same)p(same|real). 5.1.2 Experiment Results We tested the case when the number of segments is known.",
                "Table 1 shows the results of our methods with different hyper parameter values and three previous approaches, C99[25], U00[6], and ADDP03[15], on this data set when the segment number is known.",
                "In WMI for single-document segmentation, the term weights are computed as follows: wˆt = 1−EˆS(ˆt)/maxˆt ∈ ˆT (EˆS(ˆt )).",
                "For this case, our methods MIl and WMIl both outperform all the previous approaches.",
                "We compared our methods with ADDP03using one-sample one-sided t-test and p-values are shown in Table 2.",
                "From the p-values, we can see that mostly the differences are very Table 1: Average Error Rates of Single-document Segmentation Given Segment Numbers Known Range of n 3-11 3-5 6-8 9-11 Sample size 400 100 100 100 C99 12% 11% 10% 9% U00 10% 9% 7% 5% ADDP03 6.0% 6.8% 5.2% 4.3% MIl 4.68% 5.57% 2.59% 1.59% WMIl 4.94% 6.33% 2.76% 1.62% MI100 9.62% 12.92% 8.66% 6.67% Table 2: Single-document Segmentation: P-values of T-test on Error Rates Range of n 3-11 3-5 6-8 9-11 ADDP03, MIl 0.000 0.000 0.000 0.000 ADDP03, WMIl 0.000 0.099 0.000 0.000 MIl, WMIl 0.061 0.132 0.526 0.898 significant.",
                "We also compare the error rates between our two methods using two-sample two-sided t-test to check the hypothesis that they are equal.",
                "We cannot reject the hypothesis that they are equal, so the difference are not significant, even though all the error rates for MIl are smaller than WMIl.",
                "However, we can conclude that term weights contribute little in single-document segmentation.",
                "The results also show that MI using term co-clustering (k = 100) decreases the performance.",
                "We tested different number of term clusters, and found that the performance becomes better when the cluster number increases to reach l. WMIk<l has similar results that we did not show in the table.",
                "As mentioned before, using MI may be inconsistent on optimal boundaries given different numbers of segments.",
                "This situation occurs especially when the similarities among segments are quite different, i.e. some transitions are very obvious, while others are not.",
                "This is because usually a document is a hierarchical structure instead of only a sequential structure.",
                "When the segments are not at the same level, this situation may occur.",
                "Thus, a hierarchical topic segmentation approach is desired, and the structure highly depends on the number of segments for each internal node and the stop criteria of splitting.",
                "For this data set of singledocument segmentation, since it is just a synthetic set, which is just a concatenation of several segments about different topics, it is reasonable that approaches simply based on term frequency have a good performance.",
                "Usually for the tasks of segmenting coherent documents for sub-topics, the effectiveness decreases much. 5.2 <br>shared topic</br> Detection 5.2.1 Test Data and Evaluation The second data set contains 80 news articles from Google News.",
                "There are eight topics and each has 10 articles.",
                "We randomly split the set into subsets with different document numbers and each subset has all eight topics.",
                "We compare our approach MIl and WMIl with LDA [4].",
                "LDA treats a document in the data set as a bag of words, finds its distribution on topics, and its major topic.",
                "MIl and WMIl views each sentence as a bag of words and tag it with a topic label.",
                "Then for each pair of documents, LDA determines if they are on the same topic, while MIl and Table 3: <br>shared topic</br> Detection: Average Error Rates for Different Numbers of Documents in Each Subset #Doc 10 20 40 80 LDA 8.89% 16.33% 1.35% 0.60% MIl, θ = 0.6 4.17% 1.71% 1.47% 0.0% WMIl, θ = 0.8 18.6% 3.16% 1.92% 0.0% WMIl check whether the proportion overlapped sentences on the same topic is larger than the adjustable threshold θ.",
                "That is, in MIl and WMIl, for a pair of documents d, d , if [ s∈Sd,s ∈Sd 1(topics=topics )/min(|Sd|, |Sd|)] > θ, where Sd is the set of sentences of d, and |Sd| is the number of sentences of d, then d and d have the <br>shared topic</br>.",
                "For a pair of documents selected randomly, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, same)p(same|real) +p(false alarm|real, pred, diff)p(diff|real), where a miss means if they have the same topic (same) for the real case (real), but predicted (pred) as on the same topic.",
                "If they are on different topics (diff), but predicted as on the same topic, it is a false alarm. 5.2.2 Experiment Results The results are shown in Table 3.",
                "If most documents have different topics, in WMIl, the estimation of term weights in Equation (3) is not correct.",
                "Thus, WMIl is not expected to have a better performance than MIl, when most documents have different topics.",
                "When there are fewer documents in a subset with the same number of topics, more documents have different topics, so WMIl is more worse than MIl.",
                "We can see that for most cases MIl has a better (or at least similar) performance than LDA.",
                "After <br>shared topic</br> detection, multi-document segmentation of documents with the shared topics is able to be executed. 5.3 Multi-document Segmentation 5.3.1 Test Data and Evaluation For multi-document segmentation and alignment, our goal is to identify these segments about the same topic among multiple similar documents with shared topics.",
                "Using Iw is expected to perform better than I, since without term weights the result is affected seriously by document-dependent stop words and noisy words which depends on the personal writing style.",
                "It is more likely to treat the same segments of different documents as different segments under the effect of document-dependent stop words and noisy words.",
                "Term weights can reduce the effect of document-dependent stop words and noisy words by giving cue terms more weights.",
                "The data set for multi-document segmentation and alignment has 102 samples and 2264 sentences totally.",
                "Each is the introduction part of a lab report selected from the course of Biol 240W, Pennsylvania State University.",
                "Each sample has two segments, introduction of plant hormones and the content in the lab.",
                "The length range of samples is from two to 56 sentences.",
                "Some samples only have one part and some have a reverse order the these two segments.",
                "It is not hard to identify the boundary between two segments for human.",
                "We labelled each sentence manually for evaluation.",
                "The criterion of evaluation is just using the proportion of the number of sentences with wrong predicted segment labels in the total number of sentences in the whole training Table 4: Average Error Rates of Multi-document Segmentation Given Segment Numbers Known #Doc MIl WMIl k MIk WMIk 102 3.14% 2.78% 300 4.68% 6.58% 51 4.17% 3.63% 300 17.83% 22.84% 34 5.06% 4.12% 300 18.75% 20.95% 20 7.08% 5.42% 250 20.40% 21.83% 10 10.38% 7.89% 250 21.42% 21.91% 5 15.77% 11.64% 250 21.89% 22.59% 2 25.90% 23.18% 50 25.44% 25.49% 1 23.90% 24.82% 25 25.75% 26.15% Table 5: Multi-document Segmentation: P-values of T-test on Error Rates for MIl and WMIl #Doc 51 34 20 10 5 2 P-value 0.19 0.101 0.025 0.001 0.000 0.002 set as the error rate: p(error|predicted, real) = d∈D s∈Sd 1(predicteds=reals)/ d∈D nd.",
                "In order to show the benefits of multi-document segmentation and alignment, we compared our method with different parameters on different partitions of the same training set.",
                "Except the cases that the number of documents is 102 and one (they are special cases of using the whole set and the pure single-document segmentation), we randomly divided the training set into m partitions, and each has 51, 34, 20, 10, 5, and 2 document samples.",
                "Then we applied our methods on each partition and calculated the error rate of the whole training set.",
                "Each case was repeated for 10 times for computing the average error rates.",
                "For different partitions of the training set, different k values are used, since the number of terms increases when the document number in each partition increases. 5.3.2 Experiment Results From the experiment results in Table 4, we can see the following observations: (1) When the number of documents increases, all methods have better performances.",
                "Only from one to two documents, MIl has decrease a little.",
                "We can observe this from Figure 3 at the point of document number = 2.",
                "Most curves even have the worst results at this point.",
                "There are two reasons.",
                "First, because samples vote for the best multi-document segmentation and alignment, but if only two documents are compared with each other, the one with missing segments or a totally different sequence will affect the correct segmentation and alignment of the other.",
                "Second, as noted at the beginning of this section, if two documents have more document-dependent stop words or noisy words than cue words, then the algorithm may view them as two different segments and the other segment is missing.",
                "Generally, we can only expect a better performance when the number of documents is larger than the number of segments. (2) Except single-document segmentation, WMIl is always better than MIl, and when the number of documents is reaching one or increases to a very large number, their performances become closer.",
                "Table 5 shows p-values of twosample one-sided t-test between MIl and WMIl.",
                "We also can see this trend from p-values.",
                "When document number = 5, we reached the smallest p-value and the largest difference between error rates of MIl and WMIl.",
                "For single-document Table 6: Multi-document Segmentation: Average Error Rate for Document Number = 5 in Each Subset with Different Number of Term Clusters #Cluster 75 100 150 250 l MIk 24.67% 24.54% 23.91% 22.59% 15.77% segmentation, WMIl is even a little bit worse than MIl, which is similar as the results of the single-document segmentation on the first data set.",
                "The reason is that for singledocument segmentation, we cannot estimate term weights accurately, since multiple documents are unavailable. (3) Using term clustering usually gets worse results than MIl and WMIl. (4) Using term clustering in WMIk is even worse than in MIk, since in WMIk term clusters are found first using I before using Iw.",
                "If the term clusters are not correct, then the term weights are estimated worse, which may mislead the algorithm to reach even worse results.",
                "From the results we also found that in multi-document segmentation and alignment, most documents with missing segments and a reverse order are identified correctly.",
                "Table 6 illustrates the experiment results for the case of 20 partitions (each has five document samples) of the training set and topic segmentation and alignment using MIk with different numbers of term clusters k. Notice that when the number of term clusters increases, the error rate becomes smaller.",
                "Without term clustering, we have the best result.",
                "We did not show results for WMIk with term clustering, but the results are similar.",
                "We also tested WMIl with different hyper parameters of a and b to adjust term weights.",
                "The results are presented in Figure 3.",
                "It was shown that the default case WMIl : a = 1, b = 1 gave the best results for different partitions of the training set.",
                "We can see the trend that when the document number is very small or large, the difference between MIl : a = 0, b = 0 and WMIl : a = 1, b = 1 becomes quite small.",
                "When the document number is not large (about from 2 to 10), all the cases using term weights have better performances than MIl : a = 0, b = 0 without term weights, but when the document number becomes larger, the cases WMIl : a = 1, b = 0 and WMIl : a = 2, b = 1 become worse than MIl : a = 0, b = 0.",
                "When the document number becomes very large, they are even worse than cases with small document numbers.",
                "This means that a proper way to estimate term weights for the criterion of WMI is very important.",
                "Figure 4 shows the term weights learned from the whole training set.",
                "Four types of words are categorized roughly even though the transition among them are subtle.",
                "Figure 5 illustrates the change in (weighted) mutual information for MIl and WMIl.",
                "As expected, mutual information for MIl increases monotonically with the number of steps, while WMIl does not.",
                "Finally, MIl and WMIl are scalable, with computational complexity shown in Figure 6.",
                "One advantage for our approach based on MI is that removing stop words is not required.",
                "Another important advantage is that there are no necessary hyper parameters to adjust.",
                "In single-document segmentation, the performance based on MI is even better for that based on WMI, so no extra hyper parameter is required.",
                "In multi-document segmentation, we show in the experiment, a = 1 and b = 1 is the best.",
                "Our method gives more weights to cue terms.",
                "However, usually cue terms or sentences appear at the beginning of a segment, while the end of the segment may be 1 2 5 10 20 34 51 102 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 Document Number ErrorRate MIl :a=0,b=0 WMI l :a=1,b=1 WMI l :a=1,b=0 WMI l :a=2,b=1 Figure 3: Error rates for different hyper parameters of term weights. 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Normalized Document Entropy NormalizedSegmentEntropy Noisy words Cue words Common stop words Doc−dep stop words Figure 4: Term weights learned from the whole training set. 0 100 200 300 400 500 600 0.06 0.08 0.1 0.12 0.14 0.16 0.18 Number of Steps (Weighted)MutualInformation MI l WMI l Figure 5: Change in (weighted) MI for MIl and WMIl. 0 20 40 60 80 100 120 0 200 400 600 800 1000 1200 1400 1600 1800 2000 Document Number TimetoConverge(sec) MI l WMI l Figure 6: Time to converge for MIl and WMIl. much noisy.",
                "One possible solution is giving more weights to terms at the beginning of each segment.",
                "Moreover, when the length of segments are quite different, long segments have much higher term frequencies, so they may dominate the segmentation boundaries.",
                "Normalization of term frequencies versus the segment length may be useful. 6.",
                "CONCLUSIONS AND FUTURE WORK We proposed a novel method for multi-document topic segmentation and alignment based on weighted mutual information, which can also handle single-document cases.",
                "We used dynamic programming to optimize our algorithm.",
                "Our approach outperforms all the previous methods on singledocument cases.",
                "Moreover, we also showed that doing segmentation among multiple documents can improve the performance tremendously.",
                "Our results also illustrated that using weighted mutual information can utilize the information of multiple documents to reach a better performance.",
                "We only tested our method on limited data sets.",
                "More data sets especially complicated ones should be tested.",
                "More previous methods should be compared with.",
                "Moreover, natural segmentations like paragraphs are hints that can be used to find the optimal boundaries.",
                "Supervised learning also can be considered. 7.",
                "ACKNOWLEDGMENTS The authors want to thank Xiang Ji, and Prof. J. Scott Payne for their help. 8.",
                "REFERENCES [1] A. Banerjee, I. Ghillon, J. Ghosh, S. Merugu, and D. Modha.",
                "A generalized maximum entropy approach to bregman co-clustering and matrix approximation.",
                "In Proceedings of SIGKDD, 2004. [2] R. Bekkerman, R. El-Yaniv, and A. McCallum.",
                "Multi-way distributional clustering via pairwise interactions.",
                "In Proceedings of ICML, 2005. [3] D. M. Blei and P. J. Moreno.",
                "Topic segmentation with an aspect hidden markov model.",
                "In Proceedings of SIGIR, 2001. [4] D. M. Blei, A. Ng, and M. Jordan.",
                "Latent dirichlet allocation.",
                "Journal of Machine Learning Research, 3:993-1022, 2003. [5] T. Brants, F. Chen, and I. Tsochantaridis.",
                "Topic-based document segmentation with probabilistic latent semantic analysis.",
                "In Proceedings of CIKM, 2002. [6] F. Choi.",
                "Advances in domain indepedent linear text segmentation.",
                "In Proceedings of the NAACL, 2000. [7] H. Christensen, B. Kolluru, Y. Gotoh, and S. Renals.",
                "Maximum entropy segmentation of broadcast news.",
                "In Proceedings of ICASSP, 2005. [8] T. Cover and J. Thomas.",
                "Elements of Information Theory.",
                "John Wiley and Sons, New York, USA, 1991. [9] S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and R. Harshman.",
                "Indexing by latent semantic analysis.",
                "Journal of the American Society for Information Systems, 1990. [10] I. Dhillon, S. Mallela, and D. Modha.",
                "Information-theoretic co-clustering.",
                "In Proceedings of SIGKDD, 2003. [11] M. Hajime, H. Takeo, and O. Manabu.",
                "Text segmentation with multiple surface linguistic cues.",
                "In Proceedings of COLING-ACL, 1998. [12] T. K. Ho.",
                "Stop word location and identification for adaptive text recognition.",
                "International Journal of Document Analysis and Recognition, 3(1), August 2000. [13] T. Hofmann.",
                "Probabilistic latent semantic analysis.",
                "In Proceedings of the UAI99, 1999. [14] X. Ji and H. Zha.",
                "Correlating summarization of a pair of multilingual documents.",
                "In Proceedings of RIDE, 2003. [15] X. Ji and H. Zha.",
                "Domain-independent text segmentation using anisotropic diffusion and dynamic programming.",
                "In Proceedings of SIGIR, 2003. [16] X. Ji and H. Zha.",
                "Extracting shared topics of multiple documents.",
                "In Proceedings of the 7th PAKDD, 2003. [17] J. Lafferty, A. McCallum, and F. Pereira.",
                "Conditional random fields: Probabilistic models for segmenting and labeling sequence data.",
                "In Proceedings of ICML, 2001. [18] T. Li, S. Ma, and M. Ogihara.",
                "Entropy-based criterion in categorical clustering.",
                "In Proceedings of ICML, 2004. [19] A. McCallum, D. Freitag, and F. Pereira.",
                "Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of ICML, 2000. [20] L. Pevzner and M. Hearst.",
                "A critique and improvement of an evaluation metric for text segmentation.",
                "Computational Linguistic, 28(1):19-36, 2002. [21] J. C. Reynar.",
                "Statistical models for topic segmentation.",
                "In Proceedings of ACL, 1999. [22] G. Salton and M. McGill.",
                "Introduction to Modern Information Retrieval.",
                "McGraw Hill, 1983. [23] B.",
                "Sun, Q. Tan, P. Mitra, and C. L. Giles.",
                "Extraction and search of chemical formulae in text documents on the web.",
                "In Proceedings of WWW, 2007. [24] B.",
                "Sun, D. Zhou, H. Zha, and J.",
                "Yen.",
                "Multi-task text segmentation and alignment based on weighted mutual information.",
                "In Proceedings of CIKM, 2006. [25] M. Utiyama and H. Isahara.",
                "A statistical model for domain-independent text segmentation.",
                "In Proceedings of the 39th ACL, 1999. [26] C. Wayne.",
                "Multilingual topic detection and tracking: Successful research enabled by corpora and evaluation.",
                "In Proceedings of LREC, 2000. [27] J. Yamron, I. Carp, L. Gillick, S. Lowe, and P. van Mulbregt.",
                "A hidden markov model approach to text segmentation and event tracking.",
                "In Proceedings of ICASSP, 1998. [28] H. Zha and X. Ji.",
                "Correlating multilingual documents via bipartite graph modeling.",
                "In Proceedings of SIGIR, 2002."
            ],
            "original_annotated_samples": [
                "Topic Segmentation with <br>shared topic</br> Detection and Alignment of Multiple Documents Bingjun Sun*, Prasenjit Mitra*† , Hongyuan Zha‡ , C. Lee Giles*† , John Yen*† *Department of Computer Science and Engineering † College of Information Sciences and Technology The Pennsylvania State University University Park, PA 16802 ‡ College of Computing The Georgia Institute of Technology Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu ABSTRACT Topic detection and tracking [26] and topic segmentation [15] play an important role in capturing the local and sequential information of documents.",
                "In this paper, we introduce a novel unsupervised method for <br>shared topic</br> detection and topic segmentation of multiple similar documents based on mutual information (MI) and weighted mutual information (WMI) that is a combination of MI and term weights.",
                "Our experimental results show that our algorithm works well for the tasks of single-document segmentation, <br>shared topic</br> detection, and multi-document segmentation.",
                "In Section 5, experiments about single-document segmentation, <br>shared topic</br> detection, and multi-document segmentation are described, and results are presented and discussed to evaluate the performance of our algorithm.",
                "METHODOLOGY We now describe a novel algorithm which can handle singledocument segmentation, <br>shared topic</br> detection, and multidocument segmentation and alignment based on MI or WMI. 4.1 Mutual Information MI I(X; Y ) is a quantity to measure the amount of information which is contained in two or more random variables [8, 10]."
            ],
            "translated_annotated_samples": [
                "La segmentación de temas con detección de <br>temas compartidos</br> y alineación de múltiples documentos Bingjun Sun*, Prasenjit Mitra*†, Hongyuan Zha‡, C. Lee Giles*†, John Yen*† *Departamento de Ciencias de la Computación e Ingeniería † Colegio de Ciencias de la Información y Tecnología Universidad Estatal de Pensilvania University Park, PA 16802 ‡ Colegio de Computación Instituto de Tecnología de Georgia Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu RESUMEN La detección y seguimiento de temas [26] y la segmentación de temas [15] juegan un papel importante en la captura de la información local y secuencial de los documentos.",
                "En este artículo, presentamos un novedoso método no supervisado para la detección de <br>temas compartidos</br> y la segmentación de temas de múltiples documentos similares basado en la información mutua (MI) y la información mutua ponderada (WMI), que es una combinación de MI y pesos de términos.",
                "Nuestros resultados experimentales muestran que nuestro algoritmo funciona bien para las tareas de segmentación de un solo documento, detección de <br>temas compartidos</br> y segmentación de múltiples documentos.",
                "En la Sección 5, se describen experimentos sobre la segmentación de documentos individuales, la detección de <br>temas compartidos</br> y la segmentación de documentos múltiples, y se presentan y discuten los resultados para evaluar el rendimiento de nuestro algoritmo.",
                "METODOLOGÍA Ahora describimos un algoritmo novedoso que puede manejar la segmentación de un solo documento, la detección de <br>temas compartidos</br> y la segmentación y alineación de múltiples documentos basados en MI o WMI. 4.1 Información Mutua MI I(X; Y) es una cantidad que mide la cantidad de información contenida en dos o más variables aleatorias [8, 10]."
            ],
            "translated_text": "La segmentación de temas con detección de <br>temas compartidos</br> y alineación de múltiples documentos Bingjun Sun*, Prasenjit Mitra*†, Hongyuan Zha‡, C. Lee Giles*†, John Yen*† *Departamento de Ciencias de la Computación e Ingeniería † Colegio de Ciencias de la Información y Tecnología Universidad Estatal de Pensilvania University Park, PA 16802 ‡ Colegio de Computación Instituto de Tecnología de Georgia Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu RESUMEN La detección y seguimiento de temas [26] y la segmentación de temas [15] juegan un papel importante en la captura de la información local y secuencial de los documentos. El trabajo previo en esta área suele centrarse en documentos individuales, aunque existen múltiples documentos similares en muchos dominios. En este artículo, presentamos un novedoso método no supervisado para la detección de <br>temas compartidos</br> y la segmentación de temas de múltiples documentos similares basado en la información mutua (MI) y la información mutua ponderada (WMI), que es una combinación de MI y pesos de términos. La idea básica es que la segmentación óptima maximiza MI (o WMI). Nuestro enfoque puede detectar temas compartidos entre documentos. Puede encontrar los límites óptimos en un documento y alinear segmentos entre documentos al mismo tiempo. También puede manejar la segmentación de un solo documento como un caso especial de la segmentación y alineación de varios documentos. Nuestros métodos pueden identificar y fortalecer términos de señal que pueden ser utilizados para la segmentación y eliminar parcialmente las palabras de parada mediante el uso de pesos de términos basados en la entropía aprendida de múltiples documentos. Nuestros resultados experimentales muestran que nuestro algoritmo funciona bien para las tareas de segmentación de un solo documento, detección de <br>temas compartidos</br> y segmentación de múltiples documentos. El uso de información de múltiples documentos puede mejorar enormemente el rendimiento de la segmentación de temas, y utilizar WMI es aún mejor que utilizar MI para la segmentación de múltiples documentos. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información-Agrupación; H.3.1 [Almacenamiento y Recuperación de Información]: Análisis de Contenido e Indexación-Procesamiento Lingüístico; I.2.7 [Inteligencia Artificial]: Procesamiento de Lenguaje Natural-Análisis de Texto; I.5.3 [Reconocimiento de Patrones]: Algoritmos de Agrupación-Medidas de Similitud Términos Generales Algoritmos, Diseño, Experimentación 1. INTRODUCCIÓN Muchos investigadores han trabajado en la detección y seguimiento de temas (TDT) [26] y la segmentación de temas durante la última década. La segmentación de temas tiene como objetivo identificar los límites en un documento con la meta de capturar la estructura temática latente. Las tareas de segmentación de temas generalmente se dividen en dos categorías [15]: la segmentación de flujos de texto, donde se identifica la transición de temas, y la segmentación coherente de documentos, en la que los documentos se dividen en subtemas. La primera categoría tiene aplicaciones en el reconocimiento automático del habla, mientras que la segunda tiene más aplicaciones como la consulta parcial de texto en documentos extensos en la recuperación de información, resumen de texto y medición de calidad de múltiples documentos. La investigación previa en relación con TDT se enmarca en la primera categoría, centrada en el seguimiento de temas de datos de discurso de transmisión y texto de agencia de noticias, mientras que la segunda categoría no ha sido estudiada muy bien. Los enfoques tradicionales realizan la segmentación de temas en los documentos uno a la vez [15, 25, 6]. La mayoría de ellos tienen un mal desempeño en tareas sutiles como la segmentación coherente de documentos [15]. A menudo, los usuarios finales buscan documentos que tengan un contenido similar. Los motores de búsqueda, como Google, proporcionan enlaces para obtener páginas similares. A una granularidad más fina, los usuarios pueden estar buscando obtener secciones de un documento similares a una sección en particular que presumiblemente discute un tema de interés para los usuarios. Por lo tanto, la extensión de la segmentación de temas desde documentos individuales para identificar segmentos similares de múltiples documentos similares con el mismo tema es una dirección natural y necesaria, y se espera que la segmentación de temas de múltiples documentos tenga un mejor rendimiento ya que se utiliza más información. Los enfoques tradicionales que utilizan la medición de similitud basada en la frecuencia de términos generalmente parten del mismo supuesto de que un vocabulario similar tiende a estar en un segmento de tema coherente [15, 25, 6]. Sin embargo, suelen tener el problema de identificar las palabras de parada. Por ejemplo, se eliminan las palabras de parada dependientes del documento junto con las palabras de parada genéricas en [15]. Hay dos razones por las que no eliminamos las palabras de parada directamente. Primero, identificar las palabras vacías es otro problema [12] que requiere estimación en cada dominio. Eliminar palabras comunes de parada puede resultar en la pérdida de información útil en un dominio específico. Segundo, aunque se pueden identificar las palabras de parada, la clasificación estricta de palabras de parada y palabras no de parada no puede representar el cambio gradual en la cantidad de contenido informativo de cada palabra. Empleamos una clasificación suave utilizando pesos de términos. En este documento, consideramos el problema de la segmentación de temas como un problema de optimización utilizando técnicas de teoría de la información para encontrar los límites óptimos de un documento dado el número de segmentos de texto, de manera que se minimice la pérdida de información mutua (MI) (o información mutua ponderada (WMI)) después de la segmentación y alineación. Esto es igual a maximizar el MI (o WMI). El MI se centra en medir la diferencia entre segmentos, mientras que investigaciones anteriores se enfocaban en encontrar la similitud (por ejemplo, la distancia del coseno) entre segmentos [15, 25, 6]. La alineación de temas de múltiples documentos similares se puede lograr agrupando las oraciones sobre el mismo tema en el mismo clúster. La segmentación de temas en un solo documento es solo un caso especial del problema de segmentación y alineación de temas en múltiples documentos. Los términos pueden ser co-agrupados como en [10] al mismo tiempo, dado el número de agrupaciones, pero nuestros resultados experimentales muestran que este método resulta en una peor segmentación (ver Tablas 1, 4 y 6). Por lo general, los lectores humanos pueden identificar la transición de temas basándose en palabras de señal, y pueden ignorar las palabras vacías. Inspirados por esto, asignamos a cada término (o grupo de términos) un peso basado en la entropía entre diferentes documentos y diferentes segmentos de documentos. No solo este enfoque puede aumentar la contribución de las palabras clave, sino que también puede disminuir el efecto de las palabras de parada comunes, palabras ruidosas y palabras de parada dependientes del documento. Estas palabras son comunes en un documento. Muchos métodos basados en la similitud de oraciones requieren que estas palabras sean eliminadas antes de que se pueda realizar la segmentación de temas [15]. Nuestros resultados en la Figura 3 muestran que los pesos de los términos son útiles para la segmentación y alineación de temas en múltiples documentos. La principal contribución de este artículo es que introduce un método novedoso para la segmentación de temas utilizando MI y demuestra que este método funciona mejor que los criterios previamente utilizados. Además, hemos abordado el problema de la segmentación y alineación de temas en varios documentos, mientras que la mayoría de las investigaciones existentes se centraron en la segmentación de documentos individuales. La segmentación y alineación de múltiples documentos pueden utilizar información de documentos similares y mejorar significativamente el rendimiento de la segmentación de temas. Obviamente, nuestro enfoque puede manejar documentos individuales como un caso especial cuando varios documentos no están disponibles. Puede detectar temas compartidos entre documentos para determinar si son múltiples documentos sobre el mismo tema. También introducimos el nuevo criterio de WMI basado en pesos de términos aprendidos de múltiples documentos similares, lo cual puede mejorar aún más el rendimiento de la segmentación de temas. Proponemos un algoritmo voraz iterativo basado en programación dinámica y demostramos que funciona bien en la práctica. Algunos de nuestros trabajos anteriores están en [24]. El resto de este documento está organizado de la siguiente manera: En la Sección 2, revisamos el trabajo relacionado. La sección 3 contiene una formulación del problema de segmentación de temas y alineación de múltiples documentos con co-clustering de términos, una revisión del criterio de MI para el clustering, y finalmente una introducción a WMI. En la Sección 4, primero proponemos el algoritmo voraz iterativo de segmentación y alineación de temas con co-clustering de términos, y luego describimos cómo el algoritmo puede ser optimizado mediante programación dinámica. En la Sección 5, se describen experimentos sobre la segmentación de documentos individuales, la detección de <br>temas compartidos</br> y la segmentación de documentos múltiples, y se presentan y discuten los resultados para evaluar el rendimiento de nuestro algoritmo. Las conclusiones y algunas direcciones futuras del trabajo de investigación se discuten en la Sección 6.2. TRABAJO PREVIO En general, los enfoques existentes para la segmentación de texto se dividen en dos categorías: aprendizaje supervisado [19, 17, 23] y aprendizaje no supervisado [3, 27, 5, 6, 15, 25, 21]. El aprendizaje supervisado suele tener un buen rendimiento, ya que aprende funciones a partir de conjuntos de entrenamiento etiquetados. Sin embargo, obtener conjuntos de entrenamiento grandes con etiquetas manuales en oraciones de documentos suele ser prohibitivamente costoso, por lo que se prefieren enfoques no supervisados. Algunos modelos consideran la dependencia entre oraciones y secciones, como el Modelo Oculto de Markov [3, 27], el Modelo de Máxima Entropía de Markov [19] y los Campos Aleatorios Condicionales [17], mientras que muchos otros enfoques se basan en la cohesión léxica o la similitud de las oraciones [5, 6, 15, 25, 21]. Algunos enfoques también se centran en las palabras clave como pistas de transiciones de tema [11]. Si bien algunos métodos existentes solo consideran información en documentos individuales [6, 15], otros utilizan varios documentos [16, 14]. No hay muchas obras en la última categoría, aunque se espera que el rendimiento de la segmentación sea mejor con la utilización de información de múltiples documentos. Investigaciones previas estudiaron métodos para encontrar temas compartidos [16] y segmentación y resumen de temas entre solo un par de documentos [14]. La clasificación y agrupación de textos es un área de investigación relacionada que categoriza documentos en grupos utilizando métodos supervisados o no supervisados. La clasificación o agrupación temática es una dirección importante en esta área, especialmente el co-agrupamiento de documentos y términos, como LSA [9], PLSA [13], y enfoques basados en distancias y particionamiento de gráficos bipartitos [28] o máxima MI [2, 10], o máxima entropía [1, 18]. Los criterios de estos enfoques pueden ser utilizados en el tema de la segmentación de temas. Algunos de esos métodos se han extendido al área de la segmentación de temas, como PLSA [5] y máxima entropía [7], pero hasta donde sabemos, el uso de MI para la segmentación de temas no ha sido estudiado. 3. FORMULACIÓN DEL PROBLEMA Nuestro objetivo es segmentar documentos y alinear los segmentos entre documentos (Figura 1). Sea T el conjunto de términos {t1, t2, ..., tl}, que aparecen en el conjunto no etiquetado de documentos D = {d1, d2, ..., dm}. Sea Sd el conjunto de oraciones para el documento d ∈ D, es decir, {s1, s2, ..., snd}. Tenemos una matriz tridimensional de frecuencia de términos, en la que las tres dimensiones son variables aleatorias de D, Sd y T. Sd en realidad es un vector aleatorio que incluye una variable aleatoria para cada d ∈ D. La frecuencia de términos se puede utilizar para estimar la distribución de probabilidad conjunta P(D, Sd, T), que es p(t, d, s) = T(t, d, s)/ND, donde T(t, d, s) es el número de t en la oración s de d y ND es el número total de términos en D. ˆS representa el conjunto de segmentos {ˆs1, ˆs2, ..., ˆsp} después de la segmentación y alineación entre varios documentos, donde el número de segmentos | ˆS| = p. Un segmento ˆsi del documento d es una secuencia de oraciones adyacentes en d. Dado que para diferentes documentos si pueden discutir diferentes subtemas, nuestro objetivo es agrupar oraciones adyacentes en cada documento en segmentos, y alinear segmentos similares entre documentos, de modo que para diferentes documentos ˆsi trate sobre el mismo subtema. El objetivo es encontrar la segmentación óptima de temas y el mapeo de alineación Segd(si) : {s1, s2, ..., snd } → {ˆs1, ˆs2, ..., ˆsp} y Alid(ˆsi) : {ˆs1, ˆs2, ..., ˆsp} → {ˆs1, ˆs2, ..., ˆsp}, para todo d ∈ D, donde ˆsi es el i-ésimo segmento con la restricción de que solo las oraciones adyacentes pueden ser asignadas al mismo segmento, es decir, para d, {si, si+1, ..., sj} → {ˆsq}, donde q ∈ {1, ..., p}, donde p es el número de segmento, y si i > j, entonces para d, ˆsq está ausente. Después de la segmentación y alineación, el vector aleatorio Sd se convierte en una variable aleatoria alineada ˆS. Por lo tanto, P(D, Sd, T) se convierte en P(D, ˆS, T). El término co-clustering es una técnica que se ha empleado [10] para mejorar la precisión del agrupamiento de documentos. Evaluamos el efecto de ello para la segmentación de temas. Un término t se asigna a exactamente un grupo de términos. El término co-clustering implica encontrar simultáneamente el mapeo óptimo de agrupación de términos Clu(t) : {t1, t2, ..., tl} → {ˆt1, ˆt2, ..., ˆtk}, donde k ≤ l, l es el número total de palabras en todos los documentos, y k es el número de clústeres. 4. METODOLOGÍA Ahora describimos un algoritmo novedoso que puede manejar la segmentación de un solo documento, la detección de <br>temas compartidos</br> y la segmentación y alineación de múltiples documentos basados en MI o WMI. 4.1 Información Mutua MI I(X; Y) es una cantidad que mide la cantidad de información contenida en dos o más variables aleatorias [8, 10]. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "optimal boundaries": {
            "translated_key": "límites óptimos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Topic Segmentation with Shared Topic Detection and Alignment of Multiple Documents Bingjun Sun*, Prasenjit Mitra*† , Hongyuan Zha‡ , C. Lee Giles*† , John Yen*† *Department of Computer Science and Engineering † College of Information Sciences and Technology The Pennsylvania State University University Park, PA 16802 ‡ College of Computing The Georgia Institute of Technology Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu ABSTRACT Topic detection and tracking [26] and topic segmentation [15] play an important role in capturing the local and sequential information of documents.",
                "Previous work in this area usually focuses on single documents, although similar multiple documents are available in many domains.",
                "In this paper, we introduce a novel unsupervised method for shared topic detection and topic segmentation of multiple similar documents based on mutual information (MI) and weighted mutual information (WMI) that is a combination of MI and term weights.",
                "The basic idea is that the optimal segmentation maximizes MI(or WMI).",
                "Our approach can detect shared topics among documents.",
                "It can find the <br>optimal boundaries</br> in a document, and align segments among documents at the same time.",
                "It also can handle single-document segmentation as a special case of the multi-document segmentation and alignment.",
                "Our methods can identify and strengthen cue terms that can be used for segmentation and partially remove stop words by using term weights based on entropy learned from multiple documents.",
                "Our experimental results show that our algorithm works well for the tasks of single-document segmentation, shared topic detection, and multi-document segmentation.",
                "Utilizing information from multiple documents can tremendously improve the performance of topic segmentation, and using WMI is even better than using MI for the multi-document segmentation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Clustering; H.3.1 [Information Storage and Retrieval]: Content Analysis and IndexingLinguistic processing; I.2.7 [Artificial Intelligence]: Natural Language Processing-Text analysis; I.5.3 [Pattern Recognition]: Clustering-Algorithms;Similarity measures General Terms Algorithms, Design, Experimentation 1.",
                "INTRODUCTION Many researchers have worked on topic detection and tracking (TDT) [26] and topic segmentation during the past decade.",
                "Topic segmentation intends to identify the boundaries in a document with the goal to capture the latent topical structure.",
                "Topic segmentation tasks usually fall into two categories [15]: text stream segmentation where topic transition is identified, and coherent document segmentation in which documents are split into sub-topics.",
                "The former category has applications in automatic speech recognition, while the latter one has more applications such as partial-text query of long documents in information retrieval, text summary, and quality measurement of multiple documents.",
                "Previous research in connection with TDT falls into the former category, targeted on topic tracking of broadcast speech data and newswire text, while the latter category has not been studied very well.",
                "Traditional approaches perform topic segmentation on documents one at a time [15, 25, 6].",
                "Most of them perform badly in subtle tasks like coherent document segmentation [15].",
                "Often, end-users seek documents that have the similar content.",
                "Search engines, like, Google, provide links to obtain similar pages.",
                "At a finer granularity, users may actually be looking to obtain sections of a document similar to a particular section that presumably discusses a topic of the users interest.",
                "Thus, the extension of topic segmentation from single documents to identifying similar segments from multiple similar documents with the same topic is a natural and necessary direction, and multi-document topic segmentation is expected to have a better performance since more information is utilized.",
                "Traditional approaches using similarity measurement based on term frequency generally have the same assumption that similar vocabulary tends to be in a coherent topic segment [15, 25, 6].",
                "However, they usually suffer the issue of identifying stop words.",
                "For example, additional document-dependent stop words are removed together with the generic stop words in [15].",
                "There are two reasons that we do not remove stop words directly.",
                "First, identifying stop words is another issue [12] that requires estimation in each domain.",
                "Removing common stop words may result in the loss of useful information in a specific domain.",
                "Second, even though stop words can be identified, hard classification of stop words and nonstop words cannot represent the gradually changing amount of information content of each word.",
                "We employ a soft classification using term weights.",
                "In this paper, we view the problem of topic segmentation as an optimization issue using information theoretic techniques to find the <br>optimal boundaries</br> of a document given the number of text segments so as to minimize the loss of mutual information (MI) (or a weighted mutual information (WMI)) after segmentation and alignment.",
                "This is equal to maximizing the MI (or WMI).",
                "The MI focuses on measuring the difference among segments whereas previous research focused on finding the similarity (e.g. cosine distance) of segments [15, 25, 6].",
                "Topic alignment of multiple similar documents can be achieved by clustering sentences on the same topic into the same cluster.",
                "Single-document topic segmentation is just a special case of the multi-document topic segmentation and alignment problem.",
                "Terms can be co-clustered as in [10] at the same time, given the number of clusters, but our experimental results show that this method results in a worse segmentation (see Tables 1, 4, and 6).",
                "Usually, human readers can identify topic transition based on cue words, and can ignore stop words.",
                "Inspired by this, we give each term (or term cluster) a weight based on entropy among different documents and different segments of documents.",
                "Not only can this approach increase the contribution of cue words, but it can also decrease the effect of common stop words, noisy word, and document-dependent stop words.",
                "These words are common in a document.",
                "Many methods based on sentence similarity require that these words are removed before topic segmentation can be performed [15].",
                "Our results in Figure 3 show that term weights are useful for multi-document topic segmentation and alignment.",
                "The major contribution of this paper is that it introduces a novel method for topic segmentation using MI and shows that this method performs better than previously used criteria.",
                "Also, we have addressed the problem of topic segmentation and alignment across multiple documents, whereas most existing research focused on segmentation of single documents.",
                "Multi-document segmentation and alignment can utilize information from similar documents and improves the performance of topic segmentation greatly.",
                "Obviously, our approach can handle single documents as a special case when multiple documents are unavailable.",
                "It can detect shared topics among documents to judge if they are multiple documents on the same topic.",
                "We also introduce the new criterion of WMI based on term weights learned from multiple similar documents, which can improve performance of topic segmentation further.",
                "We propose an iterative greedy algorithm based on dynamic programming and show that it works well in practice.",
                "Some of our prior work is in [24].",
                "The rest of this paper is organized as follows: In Section 2, we review related work.",
                "Section 3 contains a formulation of the problem of topic segmentation and alignment of multiple documents with term co-clustering, a review of the criterion of MI for clustering, and finally an introduction to WMI.",
                "In Section 4, we first propose the iterative greedy algorithm of topic segmentation and alignment with term co-clustering, and then describe how the algorithm can be optimized by usFigure 1: Illustration of multi-document segmentation and alignment. ing dynamic programming.",
                "In Section 5, experiments about single-document segmentation, shared topic detection, and multi-document segmentation are described, and results are presented and discussed to evaluate the performance of our algorithm.",
                "Conclusions and some future directions of the research work are discussed in Section 6. 2.",
                "PREVIOUS WORK Generally, the existing approaches to text segmentation fall into two categories: supervised learning [19, 17, 23] and unsupervised learning [3, 27, 5, 6, 15, 25, 21].",
                "Supervised learning usually has good performance, since it learns functions from labelled training sets.",
                "However, often getting large training sets with manual labels on document sentences is prohibitively expensive, so unsupervised approaches are desired.",
                "Some models consider dependence between sentences and sections, such as Hidden Markov Model [3, 27], Maximum Entropy Markov Model [19], and Conditional Random Fields [17], while many other approaches are based on lexical cohesion or similarity of sentences [5, 6, 15, 25, 21].",
                "Some approaches also focus on cue words as hints of topic transitions [11].",
                "While some existing methods only consider information in single documents [6, 15], others utilize multiple documents [16, 14].",
                "There are not many works in the latter category, even though the performance of segmentation is expected to be better with utilization of information from multiple documents.",
                "Previous research studied methods to find shared topics [16] and topic segmentation and summarization between just a pair of documents [14].",
                "Text classification and clustering is a related research area which categorizes documents into groups using supervised or unsupervised methods.",
                "Topical classification or clustering is an important direction in this area, especially co-clustering of documents and terms, such as LSA [9], PLSA [13], and approaches based on distances and bipartite graph partitioning [28] or maximum MI [2, 10], or maximum entropy [1, 18].",
                "Criteria of these approaches can be utilized in the issue of topic segmentation.",
                "Some of those methods have been extended into the area of topic segmentation, such as PLSA [5] and maximum entropy [7], but to our best knowledge, using MI for topic segmentation has not been studied. 3.",
                "PROBLEM FORMULATION Our goal is to segment documents and align the segments across documents (Figure 1).",
                "Let T be the set of terms {t1, t2, ..., tl}, which appear in the unlabelled set of documents D = {d1, d2, ..., dm}.",
                "Let Sd be the set of sentences for document d ∈ D, i.e. {s1, s2, ..., snd }.",
                "We have a 3D matrix of term frequency, in which the three dimensions are random variables of D, Sd, and T. Sd actually is a random vector including a random variable for each d ∈ D. The term frequency can be used to estimate the joint probability distribution P(D, Sd, T), which is p(t, d, s) = T(t, d, s)/ND, where T(t, d, s) is the number of t in ds sentence s and ND is the total number of terms in D. ˆS represents the set of segments {ˆs1, ˆs2, ..., ˆsp} after segmentation and alignment among multiple documents, where the number of segments | ˆS| = p. A segment ˆsi of document d is a sequence of adjacent sentences in d. Since for different documents si may discuss different sub-topics, our goal is to cluster adjacent sentences in each document into segments, and align similar segments among documents, so that for different documents ˆsi is about the same sub-topic.",
                "The goal is to find the optimal topic segmentation and alignment mapping Segd(si) : {s1, s2, ..., snd } → {ˆs1, ˆs2, ..., ˆsp} and Alid(ˆsi) : {ˆs1, ˆs2, ..., ˆsp} → {ˆs1, ˆs2, ..., ˆsp}, for all d ∈ D, where ˆsi is ith segment with the constraint that only adjacent sentences can be mapped to the same segment, i.e. for d, {si, si+1, ..., sj} → {ˆsq}, where q ∈ {1, ..., p}, where p is the segment number, and if i > j, then for d, ˆsq is missing.",
                "After segmentation and alignment, random vector Sd becomes an aligned random variable ˆS.",
                "Thus, P(D, Sd, T) becomes P(D, ˆS, T).",
                "Term co-clustering is a technique that has been employed [10] to improve the accuracy of document clustering.",
                "We evaluate the effect of it for topic segmentation.",
                "A term t is mapped to exactly one term cluster.",
                "Term co-clustering involves simultaneously finding the optimal term clustering mapping Clu(t) : {t1, t2, ..., tl} → {ˆt1, ˆt2, ..., ˆtk}, where k ≤ l, l is the total number of words in all the documents, and k is the number of clusters. 4.",
                "METHODOLOGY We now describe a novel algorithm which can handle singledocument segmentation, shared topic detection, and multidocument segmentation and alignment based on MI or WMI. 4.1 Mutual Information MI I(X; Y ) is a quantity to measure the amount of information which is contained in two or more random variables [8, 10].",
                "For the case of two random variables, we have I(X; Y ) = x∈X y∈Y p(x, y)log p(x, y) p(x)p(y) , (1) Obviously, when random variables X and Y are independent, I(X; Y ) = 0.",
                "Thus, intuitively, the value of MI depends on how random variables are dependent on each other.",
                "The optimal co-clustering is the mapping Clux : X → ˆX and Cluy : Y → ˆY that minimizes the loss: I(X; Y ) − I( ˆX; ˆY ), which is equal to maximizing I( ˆX; ˆY ).",
                "This is the criterion of MI for clustering.",
                "In the case of topic segmentation, the two random variables are the term variable T and the segment variable S, and each sample is an occurrence of a term T = t in a particular segment S = s. I(T; S) is used to measure how dependent T and S are.",
                "However, I(T; S) cannot be computed for documents before segmentation, since we do not have a set of S due to the fact that sentences of Document d, si ∈ Sd, is not aligned with other documents.",
                "Thus, instead of minimizing the loss of MI, we can maximize MI after topic segmentation, computed as: I( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS p(ˆt, ˆs)log p(ˆt, ˆs) p(ˆt)p(ˆs) , (2) where p(ˆt, ˆs) are estimated by the term frequency tf of Term Cluster ˆt and Segment ˆs in the training set D. Note that here a segment ˆs includes sentences about the the same topic among all documents.",
                "The optimal solution is the mapping Clut : T → ˆT, Segd : Sd → ˆS , and Alid : ˆS → ˆS, which maximizes I( ˆT; ˆS). 4.2 Weighted Mutual Information In topic segmentation and alignment of multiple documents, if P(D, ˆS, T) is known, based on the marginal distributions P(D|T) and P( ˆS|T) for each term t ∈ T, we can categorize terms into four types in the data set: • Common stop words are common both along the dimensions of documents and segments. • Document-dependent stop words that depends on the personal writing style are common only along the dimension of segments for some documents. • Cue words are the most important elements for segmentation.",
                "They are common along the dimension of documents only for the same segment, and they are not common along the dimensions of segments. • Noisy words are other words which are not common along both dimensions.",
                "Entropy based on P(D|T) and P( ˆS|T) can be used to identify different types of terms.",
                "To reinforce the contribution of cue words in the MI computation, and simultaneously reduce the effect of the other three types of words, similar as the idea of the tf-idf weight [22], we use entropies of each term along the dimensions of document D and segment ˆS, i.e.",
                "ED(ˆt) and EˆS(ˆt), to compute the weight.",
                "A cue word usually has a large value of ED(ˆt) but a small value of EˆS(ˆt).",
                "We introduce term weights (or term cluster weights) wˆt = ( ED(ˆt) maxˆt ∈ ˆT (ED(ˆt )) )a (1 − EˆS(ˆt) maxˆt ∈ ˆT (EˆS(ˆt )) )b , (3) where ED(ˆt) = d∈D p(d|ˆt)log|D| 1 p(d|ˆt) , EˆS(ˆt) = ˆs∈ ˆS p(ˆs|ˆt)log| ˆS| 1 p(ˆs|ˆt) , and a > 0 and b > 0 are powers to adjust term weights.",
                "Usually a = 1 and b = 1 as default, and maxˆt ∈ ˆT (ED(ˆt )) and maxˆt ∈ ˆT (EˆS(ˆt )) are used to normalize the entropy values.",
                "Term cluster weights are used to adjust p(ˆt, ˆs), pw(ˆt, ˆs) = wˆtp(ˆt, ˆs) ˆt∈ ˆT ;ˆs∈ ˆS wˆtp(ˆt, ˆs) , (4) and Iw( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS pw(ˆt, ˆs)log pw(ˆt, ˆs) pw(ˆt)pw(ˆs) , (5) where pw(ˆt) and pw(ˆs) are marginal distributions of pw(ˆt, ˆs).",
                "However, since we do not know either the term weights or P(D, ˆS, T), we need to estimate them, but wˆt depends on p(ˆs|t) and ˆS, while ˆS and p(ˆs|t) also depend on wˆt that is still unknown.",
                "Thus, an iterative algorithm is required to estimate term weights wˆt and find the best segmentation and alignment to optimize the objective function Iw concurrently.",
                "After a document is segmented into sentences Input: Joint probability distribution P(D, Sd, T), number of text segments p ∈ {2, 3, ..., max(sd)}, number of term clusters k ∈ {2, 3, ..., l} (if k = l, no term co-clustering required), and weight type w ∈ {0, 1}, indicating to use I or Iw, respectively.",
                "Output: Mapping Clu, Seg, Ali, and term weights wˆt.",
                "Initialization: 0. i = 0.",
                "Initialize Clu (0) t , Seg (0) d , and Ali (0) d ; Initialize w (0) ˆt using Equation (6) if w = 1; Stage 1: 1.",
                "If |D| = 1, k = l, and w = 0, check all sequential segmentations of d into p segments and find the best one Segd(s) = argmaxˆsI( ˆT; ˆS), and return Segd; otherwise, if w = 1 and k = l, go to 3.1; Stage 2: 2.1 If k < l, for each term t, find the best cluster ˆt as Clu(i+1)(t) = argmaxˆtI( ˆT; ˆS(i)) based on Seg(i) and Ali(i); 2.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+1) d (Seg (i+1) d (s)) = argmaxˆsI( ˆT(i+1); ˆS) based on Clu(i+1)(t) if k < l or Clu(0)(t) if k = l; 2.3 i + +.",
                "If Clu, Seg, or Ali changed, go to 2.1; otherwise, if w = 0, return Clu(i), Seg(i), and Ali(i); else j = 0, go to 3.1; Stage 3: 3.1 Update w (i+j+1) ˆt based on Seg(i+j), Ali(i+j), and Clu(i) using Equation (3); 3.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+j+1) d (Seg (i+j+1) d (s)) = argmaxˆsIw( ˆT(i); ˆS) based on Clu(i) and w (i+j+1) ˆt ; 3.3 j + +.",
                "If Iw( ˆT; ˆS) changes, go to step 6; otherwise, stop and return Clu(i), Seg(i+j), Ali(i+j), and w (i+j) ˆt ; Figure 2: Algorithm: Topic segmentation and alignment based on MI or WMI. and each sentence is segmented into words, each word is stemmed.",
                "Then the joint probability distribution P(D, Sd, T) can be estimated.",
                "Finally, this distribution can be used to compute MI in our algorithm. 4.3 Iterative Greedy Algorithm Our goal is to maximize the objective function, I( ˆT; ˆS) or Iw( ˆT; ˆS), which can measure the dependence of term occurrences in different segments.",
                "Generally, first we do not know the estimate term weights, which depend on the optimal topic segmentation and alignment, and term clusters.",
                "Moreover, this problem is NP-hard [10], even though if we know the term weights.",
                "Thus, an iterative greedy algorithm is desired to find the best solution, even though probably only local maxima are reached.",
                "We present the iterative greedy algorithm in Figure 2 to find a local maximum of I( ˆT; ˆS) or Iw( ˆT; ˆS) with simultaneous term weight estimation.",
                "This algorithm can is iterative and greedy for multi-document cases or single-document cases with term weight estimation and/or term co-clustering.",
                "Otherwise, since it is just a one step algorithm to solve the task of single-document segmentation [6, 15, 25], the global maximum of MI is guaranteed.",
                "We will show later that term co-clustering reduces the accuracy of the results and is not necessary, and for singledocument segmentation, term weights are also not required. 4.3.1 Initialization In Step 0, the initial term clustering Clut and topic segmentation and alignment Segd and Alid are important to avoid local maxima and reduce the number of iterations.",
                "First, a good guess of term weights can be made by using the distributions of term frequency along sentences for each document and averaging them to get the initial values of wˆt: wt = ( ED(t) maxt ∈T (ED(t )) )(1 − ES(t) maxt ∈T (ES(t )) ), (6) where ES(t) = 1 |Dt| d∈Dt (1 − s∈Sd p(s|t)log|Sd| 1 p(s|t) ), where Dt is the set of documents which contain Term t. Then, for the initial segmentation Seg(0) , we can simply segment documents equally by sentences.",
                "Or we can find the optimal segmentation just for each document d which maximizes the WMI, Seg (0) d = argmaxˆsIw(T; ˆS), where w = w (0) ˆt .",
                "For the initial alignment Ali(0) , we can first assume that the order of segments for each d is the same.",
                "For the initial term clustering Clu(0) , first cluster labels can be set randomly, and after the first time of Step 3, a good initial term clustering is obtained. 4.3.2 Different Cases After initialization, there are three stages for different cases.",
                "Totally there are eight cases, |D| = 1 or |D| > 1, k = l or k < l, w = 0 or w = 1.",
                "Single document segmentation without term clustering and term weight estimation (|D| = 1, k = l, w = 0) only requires Stage 1 (Step 1).",
                "If term clustering is required (k < l), Stage 2 (Step 2.1, 2.2, and 2.3) is executed iteratively.",
                "If term weight estimation is required (w = 1), Stage 3 (Step 3.1, 3.2, and 3.3) is executed iteratively.",
                "If both are required (k < l, w = 1), Stage 2 and 3 run one after the other.",
                "For multi-document segmentation without term clustering and term weight estimation (|D| > 1, k = l, w = 0), only iteration of Step 2.2 and 2.3 are required.",
                "At Stage 1, the global maximum can be found based on I( ˆT; ˆS) using dynamic programming in Section 4.4.",
                "Simultaneously finding a good term clustering and estimated term weights is impossible, since when moving a term to a new term cluster to maximize Iw( ˆT; ˆS), we do not know that the weight of this term should be the one of the new cluster or the old cluster.",
                "Thus, we first do term clustering at Stage 2, and then estimate term weights at Stage 3.",
                "At Stage 2, Step 2.1 is to find the best term clustering and Step 2.2 is to find the best segmentation.",
                "This cycle is repeated to find a local maximum based on MI I until it converges.",
                "The two steps are: (1) based on current term clustering Cluˆt, for each document d, the algorithm segments all the sentences Sd into p segments sequentially (some segments may be empty), and put them into the p segments ˆS of the whole training set D (all possible cases of different segmentation Segd and alignment Alid are checked) to find the optimal case, and (2) based on the current segmentation and alignment, for each term t, the algorithm finds the best term cluster of t based on the current segmentation Segd and alignment Alid.",
                "After finding a good term clustering, term weights are estimated if w = 1.",
                "At Stage 3, similar as Stage 2, Step 3.1 is term weight re-estimation and Step 3.2 is to find a better segmentation.",
                "They are repeated to find a local maximum based on WMI Iw until it converges.",
                "However, if the term clustering in Stage 2 is not accurate, then the term weight estimation at Stage 3 may have a bad result.",
                "Finally, at Step 3.3, this algorithm converges and return the output.",
                "This algorithm can handle both single-document and multi-document segmentation.",
                "It also can detect shared topics among documents by checking the proportion of overlapped sentences on the same topics, as described in Sec 5.2. 4.4 Algorithm Optimization In many previous works on segmentation, dynamic programming is a technique used to maximize the objective function.",
                "Similarly, at Step 1, 2.2, and 3.2 of our algorithm, we can use dynamic programming.",
                "For Stage 1, using dynamic programming can still find the global optimum, but for Stage 2 and Stage 3, we can only find the optimum for each step of topic segmentation and alignment of a document.",
                "Here we only show the dynamic programming for Step 3.2 using WMI (Step 1 and 2.2 are similar but they can use either I or Iw).",
                "There are two cases that are not shown in the algorithm in Figure 2: (a) single-document segmentation or multi-document segmentation with the same sequential order of segments, where alignment is not required, and (b) multi-document segmentation with different sequential orders of segments, where alignment is necessary.",
                "The alignment mapping function of the former case is simply just Alid(ˆsi) = ˆsi, while for the latter ones alignment mapping function Alid(ˆsi) = ˆsj, i and j may be different.",
                "The computational steps for the two cases are listed below: Case 1 (no alignment): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs) and partial pw(ˆs) without counting sentences from d. Then put sentences from i to j into Part k, and compute partial WMI PIw( ˆT; ˆsk(si, si+1, ..., sj)) ˆt∈ ˆT pw(ˆt, ˆsk)log pw(ˆt, ˆsk) pw(ˆt)pw(ˆsk) , where Alid(si, si+1, ..., sj) = k, k ∈ {1, 2, ..., p}, 1 ≤ i ≤ j ≤ nd, and Segd(sq) = ˆsk for all i ≤ q ≤ j. (2) Let M(sm, 1) = PIw( ˆT; ˆs1(s1, s2, ..., sm)).",
                "Then M(sm, L) = maxi[M(si−1, L − 1) + PIw( ˆT; ˆsL(si, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, and when i > m, no sentences are put into ˆsk when compute PIw (note PIw( ˆT; ˆs(si, ..., sm)) = 0 for single-document segmentation). (3) Finally M(snd , p) = maxi[M(si−1, p − 1)+ PIw( ˆT; ˆsp(si, ..., snd ))], where 1 ≤ i ≤ nd+1.",
                "The optimal Iw is found and the corresponding segmentation is the best.",
                "Case 2 (alignment required): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs), and partial pw(ˆs), and PIw( ˆT; ˆsk(si, si+1, ..., sj)) similarly as Case 1. (2) Let M(sm, 1, k) = PIw( ˆT; ˆsk(s1, s2, ..., sm)), where k ∈ {1, 2, ..., p}.",
                "Then M(sm, L, kL) = maxi,j[M(si−1, L − 1, kL/j) + PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, kL ∈ Set(p, L), which is the set of all p!",
                "L!(p−L)! combinations of L segments chosen from all p segments, j ∈ kL, the set of L segments chosen from all p segments, and kL/j is the combination of L − 1 segments in kL except Segment j. (3) Finally, M(snd , p, kp) = maxi,j[M(si−1, p − 1, kp/j) +PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., snd ))], where kp is just the combination of all p segments and 1 ≤ i ≤ nd + 1, which is the optimal Iw and the corresponding segmentation is the best.",
                "The steps of Case 1 and 2 are similar, except in Case 2, alignment is considered in addition to segmentation.",
                "First, basic items of probability for computing Iw are computed excluding Doc d, and then partial WMI by putting every possible sequential segment (including empty segment) of d into every segment of the set.",
                "Second, the optimal sum of PIw for L segments and the leftmost m sentences, M(sm, L), is found.",
                "Finally, the maximal WMI is found among different sums of M(sm, p − 1) and PIw for Segment p. 5.",
                "EXPERIMENTS In this section, single-document segmentation, shared topic detection, and multi-document segmentation will be tested.",
                "Different hyper parameters of our method are studied.",
                "For convenience, we refer to the method using I as MIk if w = 0, and Iw as WMIk if w = 2 or as WMIk if w = 1, where k is the number of term clusters, and if k = l, where l is the total number of terms, then no term clustering is required, i.e.",
                "MIl and WMIl. 5.1 Single-document Segmentation 5.1.1 Test Data and Evaluation The first data set we tested is a synthetic one used in previous research [6, 15, 25] and many other papers.",
                "It has 700 samples.",
                "Each is a concatenation of ten segments.",
                "Each segment is the first n sentence selected randomly from the Brown corpus, which is supposed to have a different topic from each other.",
                "Currently, the best results on this data set is achieved by Ji et.al. [15].",
                "To compare the performance of our methods, the criterion used widely in previous research is applied, instead of the unbiased criterion introduced in [20].",
                "It chooses a pair of words randomly.",
                "If they are in different segments (different) for the real segmentation (real), but predicted (pred) as in the same segment, it is a miss.",
                "If they are in the same segment (same), but predicted as in different segments, it is a false alarm.",
                "Thus, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, diff)p(diff|real) +p(false alarm|real, pred, same)p(same|real). 5.1.2 Experiment Results We tested the case when the number of segments is known.",
                "Table 1 shows the results of our methods with different hyper parameter values and three previous approaches, C99[25], U00[6], and ADDP03[15], on this data set when the segment number is known.",
                "In WMI for single-document segmentation, the term weights are computed as follows: wˆt = 1−EˆS(ˆt)/maxˆt ∈ ˆT (EˆS(ˆt )).",
                "For this case, our methods MIl and WMIl both outperform all the previous approaches.",
                "We compared our methods with ADDP03using one-sample one-sided t-test and p-values are shown in Table 2.",
                "From the p-values, we can see that mostly the differences are very Table 1: Average Error Rates of Single-document Segmentation Given Segment Numbers Known Range of n 3-11 3-5 6-8 9-11 Sample size 400 100 100 100 C99 12% 11% 10% 9% U00 10% 9% 7% 5% ADDP03 6.0% 6.8% 5.2% 4.3% MIl 4.68% 5.57% 2.59% 1.59% WMIl 4.94% 6.33% 2.76% 1.62% MI100 9.62% 12.92% 8.66% 6.67% Table 2: Single-document Segmentation: P-values of T-test on Error Rates Range of n 3-11 3-5 6-8 9-11 ADDP03, MIl 0.000 0.000 0.000 0.000 ADDP03, WMIl 0.000 0.099 0.000 0.000 MIl, WMIl 0.061 0.132 0.526 0.898 significant.",
                "We also compare the error rates between our two methods using two-sample two-sided t-test to check the hypothesis that they are equal.",
                "We cannot reject the hypothesis that they are equal, so the difference are not significant, even though all the error rates for MIl are smaller than WMIl.",
                "However, we can conclude that term weights contribute little in single-document segmentation.",
                "The results also show that MI using term co-clustering (k = 100) decreases the performance.",
                "We tested different number of term clusters, and found that the performance becomes better when the cluster number increases to reach l. WMIk<l has similar results that we did not show in the table.",
                "As mentioned before, using MI may be inconsistent on <br>optimal boundaries</br> given different numbers of segments.",
                "This situation occurs especially when the similarities among segments are quite different, i.e. some transitions are very obvious, while others are not.",
                "This is because usually a document is a hierarchical structure instead of only a sequential structure.",
                "When the segments are not at the same level, this situation may occur.",
                "Thus, a hierarchical topic segmentation approach is desired, and the structure highly depends on the number of segments for each internal node and the stop criteria of splitting.",
                "For this data set of singledocument segmentation, since it is just a synthetic set, which is just a concatenation of several segments about different topics, it is reasonable that approaches simply based on term frequency have a good performance.",
                "Usually for the tasks of segmenting coherent documents for sub-topics, the effectiveness decreases much. 5.2 Shared Topic Detection 5.2.1 Test Data and Evaluation The second data set contains 80 news articles from Google News.",
                "There are eight topics and each has 10 articles.",
                "We randomly split the set into subsets with different document numbers and each subset has all eight topics.",
                "We compare our approach MIl and WMIl with LDA [4].",
                "LDA treats a document in the data set as a bag of words, finds its distribution on topics, and its major topic.",
                "MIl and WMIl views each sentence as a bag of words and tag it with a topic label.",
                "Then for each pair of documents, LDA determines if they are on the same topic, while MIl and Table 3: Shared Topic Detection: Average Error Rates for Different Numbers of Documents in Each Subset #Doc 10 20 40 80 LDA 8.89% 16.33% 1.35% 0.60% MIl, θ = 0.6 4.17% 1.71% 1.47% 0.0% WMIl, θ = 0.8 18.6% 3.16% 1.92% 0.0% WMIl check whether the proportion overlapped sentences on the same topic is larger than the adjustable threshold θ.",
                "That is, in MIl and WMIl, for a pair of documents d, d , if [ s∈Sd,s ∈Sd 1(topics=topics )/min(|Sd|, |Sd|)] > θ, where Sd is the set of sentences of d, and |Sd| is the number of sentences of d, then d and d have the shared topic.",
                "For a pair of documents selected randomly, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, same)p(same|real) +p(false alarm|real, pred, diff)p(diff|real), where a miss means if they have the same topic (same) for the real case (real), but predicted (pred) as on the same topic.",
                "If they are on different topics (diff), but predicted as on the same topic, it is a false alarm. 5.2.2 Experiment Results The results are shown in Table 3.",
                "If most documents have different topics, in WMIl, the estimation of term weights in Equation (3) is not correct.",
                "Thus, WMIl is not expected to have a better performance than MIl, when most documents have different topics.",
                "When there are fewer documents in a subset with the same number of topics, more documents have different topics, so WMIl is more worse than MIl.",
                "We can see that for most cases MIl has a better (or at least similar) performance than LDA.",
                "After shared topic detection, multi-document segmentation of documents with the shared topics is able to be executed. 5.3 Multi-document Segmentation 5.3.1 Test Data and Evaluation For multi-document segmentation and alignment, our goal is to identify these segments about the same topic among multiple similar documents with shared topics.",
                "Using Iw is expected to perform better than I, since without term weights the result is affected seriously by document-dependent stop words and noisy words which depends on the personal writing style.",
                "It is more likely to treat the same segments of different documents as different segments under the effect of document-dependent stop words and noisy words.",
                "Term weights can reduce the effect of document-dependent stop words and noisy words by giving cue terms more weights.",
                "The data set for multi-document segmentation and alignment has 102 samples and 2264 sentences totally.",
                "Each is the introduction part of a lab report selected from the course of Biol 240W, Pennsylvania State University.",
                "Each sample has two segments, introduction of plant hormones and the content in the lab.",
                "The length range of samples is from two to 56 sentences.",
                "Some samples only have one part and some have a reverse order the these two segments.",
                "It is not hard to identify the boundary between two segments for human.",
                "We labelled each sentence manually for evaluation.",
                "The criterion of evaluation is just using the proportion of the number of sentences with wrong predicted segment labels in the total number of sentences in the whole training Table 4: Average Error Rates of Multi-document Segmentation Given Segment Numbers Known #Doc MIl WMIl k MIk WMIk 102 3.14% 2.78% 300 4.68% 6.58% 51 4.17% 3.63% 300 17.83% 22.84% 34 5.06% 4.12% 300 18.75% 20.95% 20 7.08% 5.42% 250 20.40% 21.83% 10 10.38% 7.89% 250 21.42% 21.91% 5 15.77% 11.64% 250 21.89% 22.59% 2 25.90% 23.18% 50 25.44% 25.49% 1 23.90% 24.82% 25 25.75% 26.15% Table 5: Multi-document Segmentation: P-values of T-test on Error Rates for MIl and WMIl #Doc 51 34 20 10 5 2 P-value 0.19 0.101 0.025 0.001 0.000 0.002 set as the error rate: p(error|predicted, real) = d∈D s∈Sd 1(predicteds=reals)/ d∈D nd.",
                "In order to show the benefits of multi-document segmentation and alignment, we compared our method with different parameters on different partitions of the same training set.",
                "Except the cases that the number of documents is 102 and one (they are special cases of using the whole set and the pure single-document segmentation), we randomly divided the training set into m partitions, and each has 51, 34, 20, 10, 5, and 2 document samples.",
                "Then we applied our methods on each partition and calculated the error rate of the whole training set.",
                "Each case was repeated for 10 times for computing the average error rates.",
                "For different partitions of the training set, different k values are used, since the number of terms increases when the document number in each partition increases. 5.3.2 Experiment Results From the experiment results in Table 4, we can see the following observations: (1) When the number of documents increases, all methods have better performances.",
                "Only from one to two documents, MIl has decrease a little.",
                "We can observe this from Figure 3 at the point of document number = 2.",
                "Most curves even have the worst results at this point.",
                "There are two reasons.",
                "First, because samples vote for the best multi-document segmentation and alignment, but if only two documents are compared with each other, the one with missing segments or a totally different sequence will affect the correct segmentation and alignment of the other.",
                "Second, as noted at the beginning of this section, if two documents have more document-dependent stop words or noisy words than cue words, then the algorithm may view them as two different segments and the other segment is missing.",
                "Generally, we can only expect a better performance when the number of documents is larger than the number of segments. (2) Except single-document segmentation, WMIl is always better than MIl, and when the number of documents is reaching one or increases to a very large number, their performances become closer.",
                "Table 5 shows p-values of twosample one-sided t-test between MIl and WMIl.",
                "We also can see this trend from p-values.",
                "When document number = 5, we reached the smallest p-value and the largest difference between error rates of MIl and WMIl.",
                "For single-document Table 6: Multi-document Segmentation: Average Error Rate for Document Number = 5 in Each Subset with Different Number of Term Clusters #Cluster 75 100 150 250 l MIk 24.67% 24.54% 23.91% 22.59% 15.77% segmentation, WMIl is even a little bit worse than MIl, which is similar as the results of the single-document segmentation on the first data set.",
                "The reason is that for singledocument segmentation, we cannot estimate term weights accurately, since multiple documents are unavailable. (3) Using term clustering usually gets worse results than MIl and WMIl. (4) Using term clustering in WMIk is even worse than in MIk, since in WMIk term clusters are found first using I before using Iw.",
                "If the term clusters are not correct, then the term weights are estimated worse, which may mislead the algorithm to reach even worse results.",
                "From the results we also found that in multi-document segmentation and alignment, most documents with missing segments and a reverse order are identified correctly.",
                "Table 6 illustrates the experiment results for the case of 20 partitions (each has five document samples) of the training set and topic segmentation and alignment using MIk with different numbers of term clusters k. Notice that when the number of term clusters increases, the error rate becomes smaller.",
                "Without term clustering, we have the best result.",
                "We did not show results for WMIk with term clustering, but the results are similar.",
                "We also tested WMIl with different hyper parameters of a and b to adjust term weights.",
                "The results are presented in Figure 3.",
                "It was shown that the default case WMIl : a = 1, b = 1 gave the best results for different partitions of the training set.",
                "We can see the trend that when the document number is very small or large, the difference between MIl : a = 0, b = 0 and WMIl : a = 1, b = 1 becomes quite small.",
                "When the document number is not large (about from 2 to 10), all the cases using term weights have better performances than MIl : a = 0, b = 0 without term weights, but when the document number becomes larger, the cases WMIl : a = 1, b = 0 and WMIl : a = 2, b = 1 become worse than MIl : a = 0, b = 0.",
                "When the document number becomes very large, they are even worse than cases with small document numbers.",
                "This means that a proper way to estimate term weights for the criterion of WMI is very important.",
                "Figure 4 shows the term weights learned from the whole training set.",
                "Four types of words are categorized roughly even though the transition among them are subtle.",
                "Figure 5 illustrates the change in (weighted) mutual information for MIl and WMIl.",
                "As expected, mutual information for MIl increases monotonically with the number of steps, while WMIl does not.",
                "Finally, MIl and WMIl are scalable, with computational complexity shown in Figure 6.",
                "One advantage for our approach based on MI is that removing stop words is not required.",
                "Another important advantage is that there are no necessary hyper parameters to adjust.",
                "In single-document segmentation, the performance based on MI is even better for that based on WMI, so no extra hyper parameter is required.",
                "In multi-document segmentation, we show in the experiment, a = 1 and b = 1 is the best.",
                "Our method gives more weights to cue terms.",
                "However, usually cue terms or sentences appear at the beginning of a segment, while the end of the segment may be 1 2 5 10 20 34 51 102 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 Document Number ErrorRate MIl :a=0,b=0 WMI l :a=1,b=1 WMI l :a=1,b=0 WMI l :a=2,b=1 Figure 3: Error rates for different hyper parameters of term weights. 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Normalized Document Entropy NormalizedSegmentEntropy Noisy words Cue words Common stop words Doc−dep stop words Figure 4: Term weights learned from the whole training set. 0 100 200 300 400 500 600 0.06 0.08 0.1 0.12 0.14 0.16 0.18 Number of Steps (Weighted)MutualInformation MI l WMI l Figure 5: Change in (weighted) MI for MIl and WMIl. 0 20 40 60 80 100 120 0 200 400 600 800 1000 1200 1400 1600 1800 2000 Document Number TimetoConverge(sec) MI l WMI l Figure 6: Time to converge for MIl and WMIl. much noisy.",
                "One possible solution is giving more weights to terms at the beginning of each segment.",
                "Moreover, when the length of segments are quite different, long segments have much higher term frequencies, so they may dominate the segmentation boundaries.",
                "Normalization of term frequencies versus the segment length may be useful. 6.",
                "CONCLUSIONS AND FUTURE WORK We proposed a novel method for multi-document topic segmentation and alignment based on weighted mutual information, which can also handle single-document cases.",
                "We used dynamic programming to optimize our algorithm.",
                "Our approach outperforms all the previous methods on singledocument cases.",
                "Moreover, we also showed that doing segmentation among multiple documents can improve the performance tremendously.",
                "Our results also illustrated that using weighted mutual information can utilize the information of multiple documents to reach a better performance.",
                "We only tested our method on limited data sets.",
                "More data sets especially complicated ones should be tested.",
                "More previous methods should be compared with.",
                "Moreover, natural segmentations like paragraphs are hints that can be used to find the <br>optimal boundaries</br>.",
                "Supervised learning also can be considered. 7.",
                "ACKNOWLEDGMENTS The authors want to thank Xiang Ji, and Prof. J. Scott Payne for their help. 8.",
                "REFERENCES [1] A. Banerjee, I. Ghillon, J. Ghosh, S. Merugu, and D. Modha.",
                "A generalized maximum entropy approach to bregman co-clustering and matrix approximation.",
                "In Proceedings of SIGKDD, 2004. [2] R. Bekkerman, R. El-Yaniv, and A. McCallum.",
                "Multi-way distributional clustering via pairwise interactions.",
                "In Proceedings of ICML, 2005. [3] D. M. Blei and P. J. Moreno.",
                "Topic segmentation with an aspect hidden markov model.",
                "In Proceedings of SIGIR, 2001. [4] D. M. Blei, A. Ng, and M. Jordan.",
                "Latent dirichlet allocation.",
                "Journal of Machine Learning Research, 3:993-1022, 2003. [5] T. Brants, F. Chen, and I. Tsochantaridis.",
                "Topic-based document segmentation with probabilistic latent semantic analysis.",
                "In Proceedings of CIKM, 2002. [6] F. Choi.",
                "Advances in domain indepedent linear text segmentation.",
                "In Proceedings of the NAACL, 2000. [7] H. Christensen, B. Kolluru, Y. Gotoh, and S. Renals.",
                "Maximum entropy segmentation of broadcast news.",
                "In Proceedings of ICASSP, 2005. [8] T. Cover and J. Thomas.",
                "Elements of Information Theory.",
                "John Wiley and Sons, New York, USA, 1991. [9] S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and R. Harshman.",
                "Indexing by latent semantic analysis.",
                "Journal of the American Society for Information Systems, 1990. [10] I. Dhillon, S. Mallela, and D. Modha.",
                "Information-theoretic co-clustering.",
                "In Proceedings of SIGKDD, 2003. [11] M. Hajime, H. Takeo, and O. Manabu.",
                "Text segmentation with multiple surface linguistic cues.",
                "In Proceedings of COLING-ACL, 1998. [12] T. K. Ho.",
                "Stop word location and identification for adaptive text recognition.",
                "International Journal of Document Analysis and Recognition, 3(1), August 2000. [13] T. Hofmann.",
                "Probabilistic latent semantic analysis.",
                "In Proceedings of the UAI99, 1999. [14] X. Ji and H. Zha.",
                "Correlating summarization of a pair of multilingual documents.",
                "In Proceedings of RIDE, 2003. [15] X. Ji and H. Zha.",
                "Domain-independent text segmentation using anisotropic diffusion and dynamic programming.",
                "In Proceedings of SIGIR, 2003. [16] X. Ji and H. Zha.",
                "Extracting shared topics of multiple documents.",
                "In Proceedings of the 7th PAKDD, 2003. [17] J. Lafferty, A. McCallum, and F. Pereira.",
                "Conditional random fields: Probabilistic models for segmenting and labeling sequence data.",
                "In Proceedings of ICML, 2001. [18] T. Li, S. Ma, and M. Ogihara.",
                "Entropy-based criterion in categorical clustering.",
                "In Proceedings of ICML, 2004. [19] A. McCallum, D. Freitag, and F. Pereira.",
                "Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of ICML, 2000. [20] L. Pevzner and M. Hearst.",
                "A critique and improvement of an evaluation metric for text segmentation.",
                "Computational Linguistic, 28(1):19-36, 2002. [21] J. C. Reynar.",
                "Statistical models for topic segmentation.",
                "In Proceedings of ACL, 1999. [22] G. Salton and M. McGill.",
                "Introduction to Modern Information Retrieval.",
                "McGraw Hill, 1983. [23] B.",
                "Sun, Q. Tan, P. Mitra, and C. L. Giles.",
                "Extraction and search of chemical formulae in text documents on the web.",
                "In Proceedings of WWW, 2007. [24] B.",
                "Sun, D. Zhou, H. Zha, and J.",
                "Yen.",
                "Multi-task text segmentation and alignment based on weighted mutual information.",
                "In Proceedings of CIKM, 2006. [25] M. Utiyama and H. Isahara.",
                "A statistical model for domain-independent text segmentation.",
                "In Proceedings of the 39th ACL, 1999. [26] C. Wayne.",
                "Multilingual topic detection and tracking: Successful research enabled by corpora and evaluation.",
                "In Proceedings of LREC, 2000. [27] J. Yamron, I. Carp, L. Gillick, S. Lowe, and P. van Mulbregt.",
                "A hidden markov model approach to text segmentation and event tracking.",
                "In Proceedings of ICASSP, 1998. [28] H. Zha and X. Ji.",
                "Correlating multilingual documents via bipartite graph modeling.",
                "In Proceedings of SIGIR, 2002."
            ],
            "original_annotated_samples": [
                "It can find the <br>optimal boundaries</br> in a document, and align segments among documents at the same time.",
                "In this paper, we view the problem of topic segmentation as an optimization issue using information theoretic techniques to find the <br>optimal boundaries</br> of a document given the number of text segments so as to minimize the loss of mutual information (MI) (or a weighted mutual information (WMI)) after segmentation and alignment.",
                "As mentioned before, using MI may be inconsistent on <br>optimal boundaries</br> given different numbers of segments.",
                "Moreover, natural segmentations like paragraphs are hints that can be used to find the <br>optimal boundaries</br>."
            ],
            "translated_annotated_samples": [
                "Puede encontrar los <br>límites óptimos</br> en un documento y alinear segmentos entre documentos al mismo tiempo.",
                "En este documento, consideramos el problema de la segmentación de temas como un problema de optimización utilizando técnicas de teoría de la información para encontrar los <br>límites óptimos</br> de un documento dado el número de segmentos de texto, de manera que se minimice la pérdida de información mutua (MI) (o información mutua ponderada (WMI)) después de la segmentación y alineación.",
                "Como se mencionó anteriormente, el uso de MI puede ser inconsistente en los <br>límites óptimos</br> dados diferentes números de segmentos.",
                "Además, las segmentaciones naturales como los párrafos son pistas que se pueden utilizar para encontrar los <br>límites óptimos</br>."
            ],
            "translated_text": "La segmentación de temas con detección de temas compartidos y alineación de múltiples documentos Bingjun Sun*, Prasenjit Mitra*†, Hongyuan Zha‡, C. Lee Giles*†, John Yen*† *Departamento de Ciencias de la Computación e Ingeniería † Colegio de Ciencias de la Información y Tecnología Universidad Estatal de Pensilvania University Park, PA 16802 ‡ Colegio de Computación Instituto de Tecnología de Georgia Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu RESUMEN La detección y seguimiento de temas [26] y la segmentación de temas [15] juegan un papel importante en la captura de la información local y secuencial de los documentos. El trabajo previo en esta área suele centrarse en documentos individuales, aunque existen múltiples documentos similares en muchos dominios. En este artículo, presentamos un novedoso método no supervisado para la detección de temas compartidos y la segmentación de temas de múltiples documentos similares basado en la información mutua (MI) y la información mutua ponderada (WMI), que es una combinación de MI y pesos de términos. La idea básica es que la segmentación óptima maximiza MI (o WMI). Nuestro enfoque puede detectar temas compartidos entre documentos. Puede encontrar los <br>límites óptimos</br> en un documento y alinear segmentos entre documentos al mismo tiempo. También puede manejar la segmentación de un solo documento como un caso especial de la segmentación y alineación de varios documentos. Nuestros métodos pueden identificar y fortalecer términos de señal que pueden ser utilizados para la segmentación y eliminar parcialmente las palabras de parada mediante el uso de pesos de términos basados en la entropía aprendida de múltiples documentos. Nuestros resultados experimentales muestran que nuestro algoritmo funciona bien para las tareas de segmentación de un solo documento, detección de temas compartidos y segmentación de múltiples documentos. El uso de información de múltiples documentos puede mejorar enormemente el rendimiento de la segmentación de temas, y utilizar WMI es aún mejor que utilizar MI para la segmentación de múltiples documentos. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información-Agrupación; H.3.1 [Almacenamiento y Recuperación de Información]: Análisis de Contenido e Indexación-Procesamiento Lingüístico; I.2.7 [Inteligencia Artificial]: Procesamiento de Lenguaje Natural-Análisis de Texto; I.5.3 [Reconocimiento de Patrones]: Algoritmos de Agrupación-Medidas de Similitud Términos Generales Algoritmos, Diseño, Experimentación 1. INTRODUCCIÓN Muchos investigadores han trabajado en la detección y seguimiento de temas (TDT) [26] y la segmentación de temas durante la última década. La segmentación de temas tiene como objetivo identificar los límites en un documento con la meta de capturar la estructura temática latente. Las tareas de segmentación de temas generalmente se dividen en dos categorías [15]: la segmentación de flujos de texto, donde se identifica la transición de temas, y la segmentación coherente de documentos, en la que los documentos se dividen en subtemas. La primera categoría tiene aplicaciones en el reconocimiento automático del habla, mientras que la segunda tiene más aplicaciones como la consulta parcial de texto en documentos extensos en la recuperación de información, resumen de texto y medición de calidad de múltiples documentos. La investigación previa en relación con TDT se enmarca en la primera categoría, centrada en el seguimiento de temas de datos de discurso de transmisión y texto de agencia de noticias, mientras que la segunda categoría no ha sido estudiada muy bien. Los enfoques tradicionales realizan la segmentación de temas en los documentos uno a la vez [15, 25, 6]. La mayoría de ellos tienen un mal desempeño en tareas sutiles como la segmentación coherente de documentos [15]. A menudo, los usuarios finales buscan documentos que tengan un contenido similar. Los motores de búsqueda, como Google, proporcionan enlaces para obtener páginas similares. A una granularidad más fina, los usuarios pueden estar buscando obtener secciones de un documento similares a una sección en particular que presumiblemente discute un tema de interés para los usuarios. Por lo tanto, la extensión de la segmentación de temas desde documentos individuales para identificar segmentos similares de múltiples documentos similares con el mismo tema es una dirección natural y necesaria, y se espera que la segmentación de temas de múltiples documentos tenga un mejor rendimiento ya que se utiliza más información. Los enfoques tradicionales que utilizan la medición de similitud basada en la frecuencia de términos generalmente parten del mismo supuesto de que un vocabulario similar tiende a estar en un segmento de tema coherente [15, 25, 6]. Sin embargo, suelen tener el problema de identificar las palabras de parada. Por ejemplo, se eliminan las palabras de parada dependientes del documento junto con las palabras de parada genéricas en [15]. Hay dos razones por las que no eliminamos las palabras de parada directamente. Primero, identificar las palabras vacías es otro problema [12] que requiere estimación en cada dominio. Eliminar palabras comunes de parada puede resultar en la pérdida de información útil en un dominio específico. Segundo, aunque se pueden identificar las palabras de parada, la clasificación estricta de palabras de parada y palabras no de parada no puede representar el cambio gradual en la cantidad de contenido informativo de cada palabra. Empleamos una clasificación suave utilizando pesos de términos. En este documento, consideramos el problema de la segmentación de temas como un problema de optimización utilizando técnicas de teoría de la información para encontrar los <br>límites óptimos</br> de un documento dado el número de segmentos de texto, de manera que se minimice la pérdida de información mutua (MI) (o información mutua ponderada (WMI)) después de la segmentación y alineación. Esto es igual a maximizar el MI (o WMI). El MI se centra en medir la diferencia entre segmentos, mientras que investigaciones anteriores se enfocaban en encontrar la similitud (por ejemplo, la distancia del coseno) entre segmentos [15, 25, 6]. La alineación de temas de múltiples documentos similares se puede lograr agrupando las oraciones sobre el mismo tema en el mismo clúster. La segmentación de temas en un solo documento es solo un caso especial del problema de segmentación y alineación de temas en múltiples documentos. Los términos pueden ser co-agrupados como en [10] al mismo tiempo, dado el número de agrupaciones, pero nuestros resultados experimentales muestran que este método resulta en una peor segmentación (ver Tablas 1, 4 y 6). Por lo general, los lectores humanos pueden identificar la transición de temas basándose en palabras de señal, y pueden ignorar las palabras vacías. Inspirados por esto, asignamos a cada término (o grupo de términos) un peso basado en la entropía entre diferentes documentos y diferentes segmentos de documentos. No solo este enfoque puede aumentar la contribución de las palabras clave, sino que también puede disminuir el efecto de las palabras de parada comunes, palabras ruidosas y palabras de parada dependientes del documento. Estas palabras son comunes en un documento. Muchos métodos basados en la similitud de oraciones requieren que estas palabras sean eliminadas antes de que se pueda realizar la segmentación de temas [15]. Nuestros resultados en la Figura 3 muestran que los pesos de los términos son útiles para la segmentación y alineación de temas en múltiples documentos. La principal contribución de este artículo es que introduce un método novedoso para la segmentación de temas utilizando MI y demuestra que este método funciona mejor que los criterios previamente utilizados. Además, hemos abordado el problema de la segmentación y alineación de temas en varios documentos, mientras que la mayoría de las investigaciones existentes se centraron en la segmentación de documentos individuales. La segmentación y alineación de múltiples documentos pueden utilizar información de documentos similares y mejorar significativamente el rendimiento de la segmentación de temas. Obviamente, nuestro enfoque puede manejar documentos individuales como un caso especial cuando varios documentos no están disponibles. Puede detectar temas compartidos entre documentos para determinar si son múltiples documentos sobre el mismo tema. También introducimos el nuevo criterio de WMI basado en pesos de términos aprendidos de múltiples documentos similares, lo cual puede mejorar aún más el rendimiento de la segmentación de temas. Proponemos un algoritmo voraz iterativo basado en programación dinámica y demostramos que funciona bien en la práctica. Algunos de nuestros trabajos anteriores están en [24]. El resto de este documento está organizado de la siguiente manera: En la Sección 2, revisamos el trabajo relacionado. La sección 3 contiene una formulación del problema de segmentación de temas y alineación de múltiples documentos con co-clustering de términos, una revisión del criterio de MI para el clustering, y finalmente una introducción a WMI. En la Sección 4, primero proponemos el algoritmo voraz iterativo de segmentación y alineación de temas con co-clustering de términos, y luego describimos cómo el algoritmo puede ser optimizado mediante programación dinámica. En la Sección 5, se describen experimentos sobre la segmentación de documentos individuales, la detección de temas compartidos y la segmentación de documentos múltiples, y se presentan y discuten los resultados para evaluar el rendimiento de nuestro algoritmo. Las conclusiones y algunas direcciones futuras del trabajo de investigación se discuten en la Sección 6.2. TRABAJO PREVIO En general, los enfoques existentes para la segmentación de texto se dividen en dos categorías: aprendizaje supervisado [19, 17, 23] y aprendizaje no supervisado [3, 27, 5, 6, 15, 25, 21]. El aprendizaje supervisado suele tener un buen rendimiento, ya que aprende funciones a partir de conjuntos de entrenamiento etiquetados. Sin embargo, obtener conjuntos de entrenamiento grandes con etiquetas manuales en oraciones de documentos suele ser prohibitivamente costoso, por lo que se prefieren enfoques no supervisados. Algunos modelos consideran la dependencia entre oraciones y secciones, como el Modelo Oculto de Markov [3, 27], el Modelo de Máxima Entropía de Markov [19] y los Campos Aleatorios Condicionales [17], mientras que muchos otros enfoques se basan en la cohesión léxica o la similitud de las oraciones [5, 6, 15, 25, 21]. Algunos enfoques también se centran en las palabras clave como pistas de transiciones de tema [11]. Si bien algunos métodos existentes solo consideran información en documentos individuales [6, 15], otros utilizan varios documentos [16, 14]. No hay muchas obras en la última categoría, aunque se espera que el rendimiento de la segmentación sea mejor con la utilización de información de múltiples documentos. Investigaciones previas estudiaron métodos para encontrar temas compartidos [16] y segmentación y resumen de temas entre solo un par de documentos [14]. La clasificación y agrupación de textos es un área de investigación relacionada que categoriza documentos en grupos utilizando métodos supervisados o no supervisados. La clasificación o agrupación temática es una dirección importante en esta área, especialmente el co-agrupamiento de documentos y términos, como LSA [9], PLSA [13], y enfoques basados en distancias y particionamiento de gráficos bipartitos [28] o máxima MI [2, 10], o máxima entropía [1, 18]. Los criterios de estos enfoques pueden ser utilizados en el tema de la segmentación de temas. Algunos de esos métodos se han extendido al área de la segmentación de temas, como PLSA [5] y máxima entropía [7], pero hasta donde sabemos, el uso de MI para la segmentación de temas no ha sido estudiado. 3. FORMULACIÓN DEL PROBLEMA Nuestro objetivo es segmentar documentos y alinear los segmentos entre documentos (Figura 1). Sea T el conjunto de términos {t1, t2, ..., tl}, que aparecen en el conjunto no etiquetado de documentos D = {d1, d2, ..., dm}. Sea Sd el conjunto de oraciones para el documento d ∈ D, es decir, {s1, s2, ..., snd}. Tenemos una matriz tridimensional de frecuencia de términos, en la que las tres dimensiones son variables aleatorias de D, Sd y T. Sd en realidad es un vector aleatorio que incluye una variable aleatoria para cada d ∈ D. La frecuencia de términos se puede utilizar para estimar la distribución de probabilidad conjunta P(D, Sd, T), que es p(t, d, s) = T(t, d, s)/ND, donde T(t, d, s) es el número de t en la oración s de d y ND es el número total de términos en D. ˆS representa el conjunto de segmentos {ˆs1, ˆs2, ..., ˆsp} después de la segmentación y alineación entre varios documentos, donde el número de segmentos | ˆS| = p. Un segmento ˆsi del documento d es una secuencia de oraciones adyacentes en d. Dado que para diferentes documentos si pueden discutir diferentes subtemas, nuestro objetivo es agrupar oraciones adyacentes en cada documento en segmentos, y alinear segmentos similares entre documentos, de modo que para diferentes documentos ˆsi trate sobre el mismo subtema. El objetivo es encontrar la segmentación óptima de temas y el mapeo de alineación Segd(si) : {s1, s2, ..., snd } → {ˆs1, ˆs2, ..., ˆsp} y Alid(ˆsi) : {ˆs1, ˆs2, ..., ˆsp} → {ˆs1, ˆs2, ..., ˆsp}, para todo d ∈ D, donde ˆsi es el i-ésimo segmento con la restricción de que solo las oraciones adyacentes pueden ser asignadas al mismo segmento, es decir, para d, {si, si+1, ..., sj} → {ˆsq}, donde q ∈ {1, ..., p}, donde p es el número de segmento, y si i > j, entonces para d, ˆsq está ausente. Después de la segmentación y alineación, el vector aleatorio Sd se convierte en una variable aleatoria alineada ˆS. Por lo tanto, P(D, Sd, T) se convierte en P(D, ˆS, T). El término co-clustering es una técnica que se ha empleado [10] para mejorar la precisión del agrupamiento de documentos. Evaluamos el efecto de ello para la segmentación de temas. Un término t se asigna a exactamente un grupo de términos. El término co-clustering implica encontrar simultáneamente el mapeo óptimo de agrupación de términos Clu(t) : {t1, t2, ..., tl} → {ˆt1, ˆt2, ..., ˆtk}, donde k ≤ l, l es el número total de palabras en todos los documentos, y k es el número de clústeres. 4. METODOLOGÍA Ahora describimos un algoritmo novedoso que puede manejar la segmentación de un solo documento, la detección de temas compartidos y la segmentación y alineación de múltiples documentos basados en MI o WMI. 4.1 Información Mutua MI I(X; Y) es una cantidad que mide la cantidad de información contenida en dos o más variables aleatorias [8, 10]. Para el caso de dos variables aleatorias, tenemos I(X; Y) = x∈X y∈Y p(x, y)log p(x, y) p(x)p(y) , (1) Obviamente, cuando las variables aleatorias X e Y son independientes, I(X; Y) = 0. Por lo tanto, de manera intuitiva, el valor de MI depende de cómo las variables aleatorias están relacionadas entre sí. La co-agrupación óptima es el mapeo Clux: X → ˆX y Cluy: Y → ˆY que minimiza la pérdida: I(X; Y) - I(ˆX; ˆY), lo cual es igual a maximizar I(ˆX; ˆY). Este es el criterio de MI para la agrupación. En el caso de la segmentación de temas, las dos variables aleatorias son la variable de término T y la variable de segmento S, y cada muestra es una ocurrencia de un término T = t en un segmento particular S = s. Se utiliza I(T; S) para medir qué tan dependientes son T y S. Sin embargo, no se puede calcular I(T; S) para documentos antes de la segmentación, ya que no tenemos un conjunto de S debido a que las oraciones del Documento d, si ∈ Sd, no están alineadas con otros documentos. Así, en lugar de minimizar la pérdida de MI, podemos maximizar MI después de la segmentación de temas, calculada como: I( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS p(ˆt, ˆs)log p(ˆt, ˆs) p(ˆt)p(ˆs) , (2) donde p(ˆt, ˆs) se estiman mediante la frecuencia de términos tf del Cluster de Términos ˆt y el Segmento ˆs en el conjunto de entrenamiento D. Nótese que aquí un segmento ˆs incluye oraciones sobre el mismo tema entre todos los documentos. La solución óptima es el mapeo Clut : T → ˆT, Segd : Sd → ˆS, y Alid : ˆS → ˆS, que maximiza I( ˆT; ˆS). 4.2 Información Mutua Ponderada En la segmentación de temas y alineación de múltiples documentos, si se conoce P(D, ˆS, T), basado en las distribuciones marginales P(D|T) y P( ˆS|T) para cada término t ∈ T, podemos categorizar los términos en cuatro tipos en el conjunto de datos: • Las palabras de parada comunes son comunes a lo largo de las dimensiones de documentos y segmentos. • Las palabras de parada dependientes del documento que dependen del estilo de escritura personal son comunes solo a lo largo de la dimensión de segmentos para algunos documentos. • Las palabras clave son los elementos más importantes para la segmentación. Son comunes a lo largo de la dimensión de los documentos solo para el mismo segmento, y no son comunes a lo largo de las dimensiones de los segmentos. • Las palabras ruidosas son otras palabras que no son comunes a lo largo de ambas dimensiones. La entropía basada en P(D|T) y P( ˆS|T) se puede utilizar para identificar diferentes tipos de términos. Para reforzar la contribución de las palabras clave en el cálculo de MI, y al mismo tiempo reducir el efecto de los otros tres tipos de palabras, similar a la idea del peso tf-idf [22], utilizamos las entropías de cada término a lo largo de las dimensiones del documento D y del segmento ˆS, es decir, ED(ˆt) y EˆS(ˆt), para calcular el peso. Una palabra de señal generalmente tiene un valor alto de ED(ˆt) pero un valor bajo de EˆS(ˆt). Introducimos los pesos de términos (o pesos de grupos de términos) wˆt = ( ED(ˆt) maxˆt ∈ ˆT (ED(ˆt )) )a (1 − EˆS(ˆt) maxˆt ∈ ˆT (EˆS(ˆt )) )b , (3) donde ED(ˆt) = d∈D p(d|ˆt)log|D| 1 p(d|ˆt) , EˆS(ˆt) = ˆs∈ ˆS p(ˆs|ˆt)log| ˆS| 1 p(ˆs|ˆt) , y a > 0 y b > 0 son potencias para ajustar los pesos de términos. Normalmente a = 1 y b = 1 por defecto, y se utilizan maxˆt ∈ ˆT (ED(ˆt )) y maxˆt ∈ ˆT (EˆS(ˆt )) para normalizar los valores de entropía. Los pesos de los grupos de términos se utilizan para ajustar p(ˆt, ˆs), pw(ˆt, ˆs) = wˆtp(ˆt, ˆs) ˆt∈ ˆT ;ˆs∈ ˆS wˆtp(ˆt, ˆs) , (4) e Iw( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS pw(ˆt, ˆs)log pw(ˆt, ˆs) pw(ˆt)pw(ˆs) , (5) donde pw(ˆt) y pw(ˆs) son distribuciones marginales de pw(ˆt, ˆs). Sin embargo, dado que no conocemos ni los pesos de los términos ni P(D, ˆS, T), necesitamos estimarlos, pero wˆt depende de p(ˆs|t) y ˆS, mientras que ˆS y p(ˆs|t) también dependen de wˆt, que aún es desconocido. Por lo tanto, se requiere un algoritmo iterativo para estimar los pesos de los términos wˆt y encontrar la mejor segmentación y alineación para optimizar la función objetivo Iw de manera concurrente. Después de que un documento se segmenta en oraciones, se introduce: la distribución de probabilidad conjunta P(D, Sd, T), el número de segmentos de texto p ∈ {2, 3, ..., máx(sd)}, el número de grupos de términos k ∈ {2, 3, ..., l} (si k = l, no se requiere co-clustering de términos) y el tipo de peso w ∈ {0, 1}, indicando usar I o Iw, respectivamente. Mapeo de Clu, Seg, Ali y pesos de términos wˆt. Inicialización: 0. i = 0. Inicializar Clu (0) t, Seg (0) d y Ali (0) d; Inicializar w (0) ˆt usando la Ecuación (6) si w = 1; Etapa 1: 1. Si |D| = 1, k = l y w = 0, verifica todas las segmentaciones secuenciales de d en p segmentos y encuentra la mejor Segd(s) = argmaxˆsI( ˆT; ˆS), y devuelve Segd; de lo contrario, si w = 1 y k = l, ve a 3.1; Etapa 2: 2.1 Si k < l, para cada término t, encuentra el mejor clúster ˆt como Clu(i+1)(t) = argmaxˆtI( ˆT; ˆS(i)) basado en Seg(i) y Ali(i); 2.2 Para cada d, verifica todas las segmentaciones secuenciales de d en p segmentos con mapeo s → ˆs → ˆs, y encuentra la mejor Ali (i+1) d (Seg (i+1) d (s)) = argmaxˆsI( ˆT(i+1); ˆS) basado en Clu(i+1)(t) si k < l o Clu(0)(t) si k = l; 2.3 i + +. Si Clu, Seg o Ali cambian, ve a 2.1; de lo contrario, si w = 0, devuelve Clu(i), Seg(i) y Ali(i); de lo contrario, j = 0, ve a 3.1; Etapa 3: 3.1 Actualiza w (i+j+1) ˆt basado en Seg(i+j), Ali(i+j) y Clu(i) usando la Ecuación (3); 3.2 Para cada d, verifica todas las segmentaciones secuenciales de d en p segmentos con mapeo s → ˆs → ˆs, y encuentra la mejor Ali (i+j+1) d (Seg (i+j+1) d (s)) = argmaxˆsIw( ˆT(i); ˆS) basado en Clu(i) y w (i+j+1) ˆt; 3.3 j + +. Si Iw( ˆT; ˆS) cambia, ve al paso 6; de lo contrario, detente y devuelve Clu(i), Seg(i+j), Ali(i+j), y w (i+j) ˆt; Figura 2: Algoritmo: Segmentación y alineación de temas basada en MI o WMI. y cada oración se segmenta en palabras, cada palabra se reduce a su raíz. Entonces se puede estimar la distribución de probabilidad conjunta P(D, Sd, T). Finalmente, esta distribución se puede utilizar para calcular la MI en nuestro algoritmo. 4.3 Algoritmo Voraz Iterativo Nuestro objetivo es maximizar la función objetivo, I( ˆT; ˆS) o Iw( ˆT; ˆS), que puede medir la dependencia de las ocurrencias de términos en diferentes segmentos. Generalmente, primero no conocemos los pesos estimados de los términos, los cuales dependen de la segmentación y alineación óptimas de los temas, y los grupos de términos. Además, este problema es NP-duro [10], incluso si conocemos los pesos de los términos. Por lo tanto, se desea un algoritmo voraz iterativo para encontrar la mejor solución, aunque probablemente solo se alcancen máximos locales. Presentamos el algoritmo voraz iterativo en la Figura 2 para encontrar un máximo local de I( ˆT; ˆS) o Iw( ˆT; ˆS) con estimación simultánea de pesos de términos. Este algoritmo es iterativo y codicioso para casos de múltiples documentos o casos de un solo documento con estimación de peso de términos y/o co-clustering de términos. De lo contrario, dado que es solo un algoritmo de un paso para resolver la tarea de segmentación de un solo documento [6, 15, 25], se garantiza el máximo global de MI. Mostraremos más adelante que la coagrupación de términos reduce la precisión de los resultados y no es necesaria, y para la segmentación de un solo documento, los pesos de los términos tampoco son requeridos. 4.3.1 Inicialización En el Paso 0, la agrupación inicial de términos Clut y la segmentación y alineación de temas Segd y Alid son importantes para evitar máximos locales y reducir el número de iteraciones. Primero, se puede hacer una buena estimación de los pesos de los términos utilizando las distribuciones de frecuencia de términos a lo largo de las oraciones para cada documento y promediándolas para obtener los valores iniciales de wˆt: wt = ( ED(t) maxt ∈T (ED(t )) )(1 − ES(t) maxt ∈T (ES(t )) ), (6) donde ES(t) = 1 |Dt| d∈Dt (1 − s∈Sd p(s|t)log|Sd| 1 p(s|t) ), donde Dt es el conjunto de documentos que contienen el término t. Luego, para la segmentación inicial Seg(0), simplemente podemos segmentar los documentos de manera equitativa por oraciones. O podemos encontrar la segmentación óptima solo para cada documento d que maximice el WMI, Seg (0) d = argmaxˆsIw(T; ˆS), donde w = w (0) ˆt. Para el alineamiento inicial Ali(0), podemos primero asumir que el orden de los segmentos para cada d es el mismo. Para el agrupamiento inicial de términos Clu(0), las etiquetas de los primeros grupos pueden establecerse al azar, y después de la primera vez del Paso 3, se obtiene un buen agrupamiento inicial de términos. 4.3.2 Diferentes Casos Después de la inicialización, hay tres etapas para diferentes casos. En total hay ocho casos, |D| = 1 o |D| > 1, k = l o k < l, w = 0 o w = 1. La segmentación de un solo documento sin agrupamiento de términos y estimación de peso de términos (|D| = 1, k = l, w = 0) solo requiere la Etapa 1 (Paso 1). Si se requiere el agrupamiento de términos (k < l), la Etapa 2 (Paso 2.1, 2.2 y 2.3) se ejecuta de forma iterativa. Si se requiere la estimación del peso del término (w = 1), la Etapa 3 (Paso 3.1, 3.2 y 3.3) se ejecuta de forma iterativa. Si ambos son requeridos (k < l, w = 1), la Etapa 2 y 3 se ejecutan una tras otra. Para la segmentación de múltiples documentos sin agrupamiento de términos y estimación de peso de términos (|D| > 1, k = l, w = 0), solo se requiere la iteración de los Pasos 2.2 y 2.3. En la Etapa 1, el máximo global se puede encontrar basándose en I( ˆT; ˆS) utilizando programación dinámica en la Sección 4.4. Es imposible encontrar simultáneamente un buen agrupamiento de términos y pesos estimados de términos, ya que al mover un término a un nuevo grupo de términos para maximizar Iw( ˆT; ˆS), no sabemos si el peso de este término debería ser el del nuevo grupo o el del antiguo. Por lo tanto, primero realizamos el agrupamiento de términos en la Etapa 2, y luego estimamos los pesos de los términos en la Etapa 3. En la Etapa 2, el Paso 2.1 consiste en encontrar el mejor agrupamiento de términos y el Paso 2.2 consiste en encontrar la mejor segmentación. Este ciclo se repite para encontrar un máximo local basado en MI I hasta que converge. Los dos pasos son: (1) basados en el agrupamiento de términos actual Cluˆt, para cada documento d, el algoritmo segmenta todas las oraciones Sd en p segmentos secuencialmente (algunos segmentos pueden estar vacíos), y los coloca en los p segmentos ˆS del conjunto de entrenamiento completo D (se verifican todos los casos posibles de segmentación diferente Segd y alineación Alid) para encontrar el caso óptimo, y (2) basado en la segmentación y alineación actual, para cada término t, el algoritmo encuentra el mejor grupo de términos de t basado en la segmentación actual Segd y la alineación Alid. Después de encontrar un buen agrupamiento de términos, se estiman los pesos de los términos si w = 1. En la Etapa 3, al igual que en la Etapa 2, el Paso 3.1 es la reestimación del peso del término y el Paso 3.2 es encontrar una mejor segmentación. Se repiten para encontrar un máximo local basado en WMI Iw hasta que converja. Sin embargo, si el agrupamiento en la Etapa 2 no es preciso, entonces la estimación de peso en la Etapa 3 puede tener un mal resultado. Finalmente, en el Paso 3.3, este algoritmo converge y devuelve la salida. Este algoritmo puede manejar tanto la segmentación de un solo documento como la de múltiples documentos. También puede detectar temas compartidos entre documentos al verificar la proporción de frases superpuestas sobre los mismos temas, como se describe en la Sección 5.2. 4.4 Optimización del algoritmo. En muchos trabajos anteriores sobre segmentación, la programación dinámica es una técnica utilizada para maximizar la función objetivo. De manera similar, en los Pasos 1, 2.2 y 3.2 de nuestro algoritmo, podemos utilizar programación dinámica. Para la Etapa 1, utilizando programación dinámica aún se puede encontrar el óptimo global, pero para la Etapa 2 y la Etapa 3, solo podemos encontrar el óptimo para cada paso de segmentación de temas y alineación de un documento. Aquí solo mostramos la programación dinámica para el Paso 3.2 utilizando WMI (el Paso 1 y 2.2 son similares pero pueden usar I o Iw). Hay dos casos que no se muestran en el algoritmo de la Figura 2: (a) segmentación de un solo documento o segmentación de múltiples documentos con el mismo orden secuencial de segmentos, donde no se requiere alineación, y (b) segmentación de múltiples documentos con diferentes órdenes secuenciales de segmentos, donde la alineación es necesaria. La función de mapeo de alineación del primer caso es simplemente Alid(ˆsi) = ˆsi, mientras que para los últimos casos la función de mapeo de alineación es Alid(ˆsi) = ˆsj, donde i y j pueden ser diferentes. Los pasos computacionales para los dos casos se enumeran a continuación: Caso 1 (sin alineación): Para cada documento d: (1) Calcular pw(ˆt), pw parcial(ˆt, ˆs) y pw parcial(ˆs) sin contar las oraciones de d. Luego, colocar las oraciones de i a j en la Parte k, y calcular WMI parcial PIw( ˆT; ˆsk(si, si+1, ..., sj)) ˆt∈ ˆT pw(ˆt, ˆsk)log pw(ˆt, ˆsk) pw(ˆt)pw(ˆsk) , donde Alid(si, si+1, ..., sj) = k, k ∈ {1, 2, ..., p}, 1 ≤ i ≤ j ≤ nd, y Segd(sq) = ˆsk para todo i ≤ q ≤ j. (2) Dejar que M(sm, 1) = PIw( ˆT; ˆs1(s1, s2, ..., sm)). Entonces M(sm, L) = maxi[M(si−1, L − 1) + PIw( ˆT; ˆsL(si, ..., sm))], donde 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, y cuando i > m, no se colocan oraciones en ˆsk al calcular PIw (nota PIw( ˆT; ˆs(si, ..., sm)) = 0 para la segmentación de un solo documento). (3) Finalmente M(snd , p) = maxi[M(si−1, p − 1)+ PIw( ˆT; ˆsp(si, ..., snd ))], donde 1 ≤ i ≤ nd+1. Se encuentra el Iw óptimo y la segmentación correspondiente es la mejor. Caso 2 (alineación requerida): Para cada documento d: (1) Calcular pw(ˆt), pw parcial(ˆt, ˆs), y pw parcial(ˆs), y PIw( ˆT; ˆsk(si, si+1, ..., sj)) de manera similar al Caso 1. (2) Sea M(sm, 1, k) = PIw( ˆT; ˆsk(s1, s2, ..., sm)), donde k ∈ {1, 2, ..., p}. Entonces M(sm, L, kL) = maxi,j[M(si−1, L − 1, kL/j) + PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., sm))], donde 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, kL ∈ Set(p, L), que es el conjunto de todos los p! Hay L!(p−L)! combinaciones de L segmentos elegidos de entre todos los p segmentos, j ∈ kL, el conjunto de L segmentos elegidos de entre todos los p segmentos, y kL/j es la combinación de L − 1 segmentos en kL excepto el Segmento j. (3) Finalmente, M(snd , p, kp) = maxi,j[M(si−1, p − 1, kp/j) +PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., snd ))], donde kp es simplemente la combinación de todos los p segmentos y 1 ≤ i ≤ nd + 1, que es el óptimo Iw y la segmentación correspondiente es la mejor. Los pasos de los Casos 1 y 2 son similares, excepto que en el Caso 2, se considera el alineamiento además de la segmentación. Primero, se calculan los elementos básicos de probabilidad para computar Iw excluyendo el documento d, y luego se calcula el WMI parcial colocando cada segmento secuencial posible (incluido el segmento vacío) de d en cada segmento del conjunto. Segundo, se encuentra la suma óptima de PIw para L segmentos y las m oraciones más a la izquierda, M(sm, L). Finalmente, se encuentra el WMI máximo entre diferentes sumas de M(sm, p − 1) y PIw para el Segmento p. 5. EXPERIMENTOS En esta sección se probará la segmentación de un solo documento, la detección de temas compartidos y la segmentación de múltiples documentos. Se estudian diferentes hiperparámetros de nuestro método. Para mayor comodidad, nos referimos al método utilizando I como MIk si w = 0, e Iw como WMIk si w = 2 o como WMIk si w = 1, donde k es el número de grupos de términos, y si k = l, donde l es el número total de términos, entonces no se requiere agrupación de términos, es decir, El primer conjunto de datos que probamos es uno sintético utilizado en investigaciones anteriores [6, 15, 25] y muchos otros artículos. Tiene 700 muestras. Cada uno es una concatenación de diez segmentos. Cada segmento es la primera oración seleccionada al azar de las n primeras oraciones del corpus Brown, que se supone que tienen un tema diferente entre sí. Actualmente, los mejores resultados en este conjunto de datos son logrados por Ji et al. [15]. Para comparar el rendimiento de nuestros métodos, se aplica el criterio ampliamente utilizado en investigaciones anteriores en lugar del criterio imparcial introducido en [20]. Elige un par de palabras al azar. Si se encuentran en segmentos diferentes (diferentes) para la segmentación real (real), pero se predicen (pred) como en el mismo segmento, es un error. Si están en el mismo segmento (mismo), pero se predice que están en segmentos diferentes, es una falsa alarma. Por lo tanto, la tasa de error se calcula utilizando la siguiente ecuación: p(err|real, pred) = p(miss|real, pred, diff)p(diff|real) + p(false alarm|real, pred, same)p(same|real). 5.1.2 Resultados del Experimento Probamos el caso cuando se conoce el número de segmentos. La Tabla 1 muestra los resultados de nuestros métodos con diferentes valores de hiperparámetros y tres enfoques anteriores, C99[25], U00[6] y ADDP03[15], en este conjunto de datos cuando se conoce el número de segmento. En WMI para la segmentación de un solo documento, los pesos de los términos se calculan de la siguiente manera: wˆt = 1−EˆS(ˆt)/maxˆt ∈ ˆT (EˆS(ˆt)). Para este caso, nuestros métodos MIl y WMIl superan a todos los enfoques anteriores. Comparamos nuestros métodos con ADDP03 utilizando una prueba t de una muestra unilateral y los valores de p se muestran en la Tabla 2. A partir de los valores p, podemos ver que en su mayoría las diferencias son muy significativas. También comparamos las tasas de error entre nuestros dos métodos utilizando una prueba t de dos muestras de dos colas para verificar la hipótesis de que son iguales. No podemos rechazar la hipótesis de que son iguales, por lo que las diferencias no son significativas, a pesar de que todas las tasas de error para MIl son más pequeñas que las de WMIl. Sin embargo, podemos concluir que los pesos de los términos contribuyen poco en la segmentación de un solo documento. Los resultados también muestran que el uso de MI con co-clustering de términos (k = 100) disminuye el rendimiento. Probamos diferentes números de grupos de términos y encontramos que el rendimiento mejora cuando el número de grupos aumenta hasta llegar a l. WMIk<l tiene resultados similares que no mostramos en la tabla. Como se mencionó anteriormente, el uso de MI puede ser inconsistente en los <br>límites óptimos</br> dados diferentes números de segmentos. Esta situación ocurre especialmente cuando las similitudes entre los segmentos son bastante diferentes, es decir, algunas transiciones son muy obvias, mientras que otras no lo son. Esto se debe a que normalmente un documento es una estructura jerárquica en lugar de una estructura únicamente secuencial. Cuando los segmentos no están en el mismo nivel, esta situación puede ocurrir. Por lo tanto, se desea un enfoque de segmentación jerárquica de temas, y la estructura depende en gran medida del número de segmentos para cada nodo interno y del criterio de parada de la división. Para este conjunto de datos de segmentación de un solo documento, dado que es solo un conjunto sintético, que es simplemente una concatenación de varios segmentos sobre diferentes temas, es razonable que enfoques simplemente basados en la frecuencia de términos tengan un buen rendimiento. Normalmente, para las tareas de segmentación de documentos coherentes en subtemas, la efectividad disminuye mucho. 5.2 Detección de Temas Compartidos 5.2.1 Datos de Prueba y Evaluación El segundo conjunto de datos contiene 80 artículos de noticias de Google News. Hay ocho temas y cada uno tiene 10 artículos. Dividimos aleatoriamente el conjunto en subconjuntos con diferentes números de documentos y cada subconjunto tiene los ocho temas. Comparamos nuestro enfoque MIl y WMIl con LDA [4]. LDA trata un documento en el conjunto de datos como una bolsa de palabras, encuentra su distribución en temas y su tema principal. MIl y WMIl ven cada oración como un conjunto de palabras y la etiquetan con una etiqueta de tema. Luego, para cada par de documentos, LDA determina si están en el mismo tema, mientras que MIl y Table 3: Detección de Temas Compartidos: Tasas de Error Promedio para Diferentes Números de Documentos en Cada Subconjunto #Doc 10 20 40 80 LDA 8.89% 16.33% 1.35% 0.60% MIl, θ = 0.6 4.17% 1.71% 1.47% 0.0% WMIl, θ = 0.8 18.6% 3.16% 1.92% 0.0% WMIl comprueba si la proporción de oraciones superpuestas en el mismo tema es mayor que el umbral ajustable θ. Es decir, en MIl y WMIl, para un par de documentos d, d , si [ s∈Sd,s ∈Sd 1(temas=temas )/min(|Sd|, |Sd|)] > θ, donde Sd es el conjunto de oraciones de d, y |Sd| es el número de oraciones de d, entonces d y d comparten el tema. Para un par de documentos seleccionados al azar, la tasa de error se calcula utilizando la siguiente ecuación: p(err|real, pred) = p(miss|real, pred, same)p(same|real) + p(false alarm|real, pred, diff)p(diff|real), donde un error significa si tienen el mismo tema (same) para el caso real (real), pero se predice (pred) como en un tema diferente. Si están en diferentes temas (diff), pero se predicen como en el mismo tema, es una falsa alarma. 5.2.2 Resultados del Experimento Los resultados se muestran en la Tabla 3. Si la mayoría de los documentos tienen diferentes temas, en WMIl, la estimación de los pesos de términos en la Ecuación (3) no es correcta. Por lo tanto, no se espera que WMIL tenga un mejor rendimiento que MIL cuando la mayoría de los documentos tienen temas diferentes. Cuando hay menos documentos en un subconjunto con el mismo número de temas, más documentos tienen temas diferentes, por lo que WMIl es peor que MIl. Podemos ver que en la mayoría de los casos, MIl tiene un rendimiento mejor (o al menos similar) que LDA. Después de la detección de temas compartidos, se puede llevar a cabo la segmentación de documentos con los temas compartidos. 5.3 Segmentación de múltiples documentos 5.3.1 Datos de prueba y evaluación Para la segmentación y alineación de múltiples documentos, nuestro objetivo es identificar estos segmentos sobre el mismo tema entre varios documentos similares con temas compartidos. Se espera que al utilizar pesos de términos, Iw funcione mejor que I, ya que sin pesos de términos, el resultado se ve seriamente afectado por palabras de parada dependientes del documento y palabras ruidosas que dependen del estilo de escritura personal. Es más probable tratar los mismos segmentos de diferentes documentos como segmentos diferentes bajo el efecto de palabras de parada dependientes del documento y palabras ruidosas. Los pesos de términos pueden reducir el efecto de las palabras de parada dependientes del documento y las palabras ruidosas al darle más peso a los términos de señal. El conjunto de datos para la segmentación y alineación de múltiples documentos tiene un total de 102 muestras y 2264 oraciones. Cada uno es la parte de introducción de un informe de laboratorio seleccionado del curso de Biol 240W, Universidad Estatal de Pensilvania. Cada muestra tiene dos segmentos, la introducción de las hormonas vegetales y el contenido en el laboratorio. El rango de longitud de las muestras va desde dos hasta 56 oraciones. Algunas muestras solo tienen una parte y algunas tienen un orden inverso de estos dos segmentos. No es difícil identificar el límite entre dos segmentos para los humanos. Etiquetamos cada oración manualmente para evaluación. El criterio de evaluación consiste en utilizar la proporción del número de oraciones con etiquetas de segmento incorrectas predichas en el número total de oraciones en toda la Tabla de entrenamiento. Para mostrar los beneficios de la segmentación y alineación de múltiples documentos, comparamos nuestro método con diferentes parámetros en diferentes particiones del mismo conjunto de entrenamiento. Excepto los casos en los que el número de documentos es 102 y uno (que son casos especiales de uso del conjunto completo y la segmentación pura de un solo documento), dividimos aleatoriamente el conjunto de entrenamiento en m particiones, y cada una tiene 51, 34, 20, 10, 5 y 2 muestras de documentos. Luego aplicamos nuestros métodos en cada partición y calculamos la tasa de error de todo el conjunto de entrenamiento. Cada caso se repitió 10 veces para calcular las tasas de error promedio. Para diferentes particiones del conjunto de entrenamiento, se utilizan diferentes valores de k, ya que el número de términos aumenta cuando el número de documentos en cada partición aumenta. 5.3.2 Resultados del Experimento De los resultados del experimento en la Tabla 4, podemos ver las siguientes observaciones: (1) Cuando el número de documentos aumenta, todos los métodos tienen un mejor rendimiento. Solo de uno a dos documentos, el MIL ha disminuido un poco. Podemos observar esto en la Figura 3 en el punto del número de documento = 2. La mayoría de las curvas incluso tienen los peores resultados en este punto. Hay dos razones. Primero, porque las muestras votan por la mejor segmentación y alineación de múltiples documentos, pero si solo se comparan dos documentos entre sí, aquel con segmentos faltantes o una secuencia totalmente diferente afectará la correcta segmentación y alineación del otro. Segundo, como se señaló al principio de esta sección, si dos documentos tienen más palabras de parada dependientes del documento o palabras ruidosas que palabras clave, entonces el algoritmo puede considerarlos como dos segmentos diferentes y faltar el otro segmento. Generalmente, solo podemos esperar un mejor rendimiento cuando el número de documentos es mayor que el número de segmentos. (2) Excepto en la segmentación de un solo documento, WMIl siempre es mejor que MIl, y cuando el número de documentos se acerca a uno o aumenta a un número muy grande, sus rendimientos se vuelven más cercanos. La Tabla 5 muestra los valores p de la prueba t de dos muestras unilaterales entre MIl y WMIl. También podemos observar esta tendencia a partir de los valores de p. Cuando el número de documento es igual a 5, alcanzamos el valor p más pequeño y la mayor diferencia entre las tasas de error de MIl y WMIl. Para la Tabla 6 de un solo documento: Segmentación de múltiples documentos: Tasa de error promedio para el Documento Número = 5 en cada subconjunto con Diferente Número de Agrupaciones de Términos #Agrupaciones 75 100 150 250 l MIk 24.67% 24.54% 23.91% 22.59% 15.77% de segmentación, WMIl es incluso un poco peor que MIl, lo cual es similar a los resultados de la segmentación de un solo documento en el primer conjunto de datos. La razón es que para la segmentación de un solo documento, no podemos estimar con precisión los pesos de los términos, ya que no tenemos disponibles varios documentos. (3) El uso de agrupamiento de términos generalmente produce resultados peores que MIl y WMIl. (4) El uso de agrupamiento de términos en WMIk es aún peor que en MIk, ya que en WMIk los grupos de términos se encuentran primero utilizando I antes que Iw. Si los grupos de términos no son correctos, entonces los pesos de los términos se estiman peor, lo que puede llevar a que el algoritmo obtenga resultados aún peores. De los resultados también encontramos que en la segmentación y alineación de múltiples documentos, la mayoría de los documentos con segmentos faltantes y en orden inverso son identificados correctamente. La Tabla 6 ilustra los resultados del experimento para el caso de 20 particiones (cada una con cinco muestras de documentos) del conjunto de entrenamiento y la segmentación y alineación de temas utilizando MIk con diferentes números de grupos de términos k. Observa que cuando el número de grupos de términos aumenta, la tasa de error disminuye. Sin agrupamiento de términos, obtenemos el mejor resultado. No mostramos resultados para WMIk con agrupamiento de términos, pero los resultados son similares. También probamos WMIL con diferentes hiperparámetros de a y b para ajustar los pesos de los términos. Los resultados se presentan en la Figura 3. Se demostró que el caso predeterminado WMIl: a = 1, b = 1 dio los mejores resultados para diferentes particiones del conjunto de entrenamiento. Podemos observar la tendencia de que cuando el número de documentos es muy pequeño o grande, la diferencia entre MIl: a = 0, b = 0 y WMIl: a = 1, b = 1 se vuelve bastante pequeña. Cuando el número de documentos no es grande (aproximadamente de 2 a 10), todos los casos que utilizan pesos de términos tienen un mejor rendimiento que MIl: a = 0, b = 0 sin pesos de términos, pero cuando el número de documentos aumenta, los casos WMIl: a = 1, b = 0 y WMIl: a = 2, b = 1 empeoran en comparación con MIl: a = 0, b = 0. Cuando el número de documentos se vuelve muy grande, son aún peores que los casos con números de documentos pequeños. Esto significa que es muy importante encontrar una forma adecuada de estimar los pesos de los términos para el criterio de WMI. La Figura 4 muestra los pesos de los términos aprendidos de todo el conjunto de entrenamiento. Cuatro tipos de palabras se categorizan de manera aproximada aunque la transición entre ellas es sutil. La Figura 5 ilustra el cambio en la información mutua (ponderada) para MIl y WMIl. Como era de esperar, la información mutua para MIl aumenta de forma monótona con el número de pasos, mientras que WMIl no lo hace. Finalmente, MIl y WMIl son escalables, con complejidad computacional mostrada en la Figura 6. Una ventaja de nuestro enfoque basado en MI es que no es necesario eliminar las palabras vacías. Otra ventaja importante es que no hay hiperparámetros necesarios que ajustar. En la segmentación de un solo documento, el rendimiento basado en MI es aún mejor que el basado en WMI, por lo que no se requiere un hiperparámetro adicional. En la segmentación de múltiples documentos, mostramos en el experimento que a = 1 y b = 1 es lo mejor. Nuestro método otorga más peso a los términos de señal. Sin embargo, generalmente los términos o frases de señal aparecen al principio de un segmento, mientras que el final del segmento puede ser 1 2 5 10 20 34 51 102 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 Número de Documento Tasa de Error MIl :a=0,b=0 WMI l :a=1,b=1 WMI l :a=1,b=0 WMI l :a=2,b=1 Figura 3: Tasas de error para diferentes hiperparámetros de pesos de términos. 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Entropía de Documento Normalizada Entropía de Segmento Normalizada Palabras ruidosas Palabras de señal Palabras de parada comunes Palabras de parada de Doc-dep Figura 4: Pesos de términos aprendidos de todo el conjunto de entrenamiento. 0 100 200 300 400 500 600 0.06 0.08 0.1 0.12 0.14 0.16 0.18 Número de Pasos (Ponderado) Información Mutua MI l WMI l Figura 5: Cambio en la MI (ponderada) para MIl y WMIl. 0 20 40 60 80 100 120 0 200 400 600 800 1000 1200 1400 1600 1800 2000 Número de Documento Tiempo de Convergencia (seg) MI l WMI l Figura 6: Tiempo de convergencia para MIl y WMIl. muy ruidoso. Una posible solución es dar más peso a los términos al principio de cada segmento. Además, cuando la longitud de los segmentos es bastante diferente, los segmentos largos tienen frecuencias de términos mucho más altas, por lo que pueden dominar los límites de segmentación. La normalización de las frecuencias de términos en relación con la longitud del segmento puede ser útil. 6. CONCLUSIONES Y TRABAJO FUTURO Propusimos un método novedoso para la segmentación y alineación de temas en múltiples documentos basado en información mutua ponderada, que también puede manejar casos de un solo documento. Utilizamos programación dinámica para optimizar nuestro algoritmo. Nuestro enfoque supera a todos los métodos anteriores en casos de un solo documento. Además, también demostramos que realizar segmentación entre varios documentos puede mejorar el rendimiento enormemente. Nuestros resultados también demostraron que el uso de la información mutua ponderada puede aprovechar la información de varios documentos para lograr un mejor rendimiento. Solo probamos nuestro método en conjuntos de datos limitados. Se deben probar más conjuntos de datos, especialmente los complicados. Deberían compararse más métodos anteriores. Además, las segmentaciones naturales como los párrafos son pistas que se pueden utilizar para encontrar los <br>límites óptimos</br>. El aprendizaje supervisado también puede ser considerado. 7. AGRADECIMIENTOS Los autores desean agradecer a Xiang Ji y al Prof. J. Scott Payne por su ayuda. 8. REFERENCIAS [1] A. Banerjee, I. Ghillon, J. Ghosh, S. Merugu y D. Modha. Un enfoque generalizado de entropía máxima para la coagrupación de Bregman y la aproximación de matrices. En Actas de SIGKDD, 2004. [2] R. Bekkerman, R. El-Yaniv y A. McCallum. Agrupación distribucional de múltiples vías a través de interacciones por pares. En Actas de ICML, 2005. [3] D. M. Blei y P. J. Moreno. Segmentación de temas con un modelo oculto de Markov de aspectos. En Actas de SIGIR, 2001. [4] D. M. Blei, A. Ng y M. Jordan. Asignación latente de Dirichlet. Revista de Investigación en Aprendizaje Automático, 3:993-1022, 2003. [5] T. Brants, F. Chen e I. Tsochantaridis. Segmentación de documentos basada en temas con análisis semántico latente probabilístico. En Actas de CIKM, 2002. [6] F. Choi. Avances en la segmentación lineal de texto independiente del dominio. En Actas de la NAACL, 2000. [7] H. Christensen, B. Kolluru, Y. Gotoh y S. Renals. Segmentación de máxima entropía de noticias de transmisión. En Actas de ICASSP, 2005. [8] T. Cover y J. Thomas. Elementos de la teoría de la información. John Wiley and Sons, Nueva York, EE. UU., 1991. [9] S. Deerwester, S. Dumais, G. Furnas, T. Landauer y R. Harshman. Indexación mediante análisis semántico latente. Revista de la Sociedad Americana de Sistemas de Información, 1990. [10] I. Dhillon, S. Mallela y D. Modha. Co-clustering informativo-teórico. En Actas de SIGKDD, 2003. [11] M. Hajime, H. Takeo y O. Manabu. Segmentación de texto con múltiples señales lingüísticas superficiales. En Actas de COLING-ACL, 1998. [12] T. K. Ho. Detención de palabras y identificación para el reconocimiento de texto adaptativo. Revista Internacional de Análisis y Reconocimiento de Documentos, 3(1), agosto de 2000. [13] T. Hofmann. Análisis semántico latente probabilístico. En Actas de la UAI99, 1999. [14] X. Ji y H. Zha. Correlación de la sumarización de un par de documentos multilingües. En Actas de RIDE, 2003. [15] X. Ji y H. Zha. Segmentación de texto independiente del dominio utilizando difusión anisotrópica y programación dinámica. En Actas de SIGIR, 2003. [16] X. Ji y H. Zha. Extrayendo temas compartidos de múltiples documentos. En Actas de la 7ª PAKDD, 2003. [17] J. Lafferty, A. McCallum y F. Pereira. Campos aleatorios condicionales: Modelos probabilísticos para segmentar y etiquetar datos de secuencias. En Actas de ICML, 2001. [18] T. Li, S. Ma y M. Ogihara. Criterio basado en la entropía en la agrupación categórica. En Actas de ICML, 2004. [19] A. McCallum, D. Freitag y F. Pereira. Modelos de máxima entropía de Markov para extracción de información y segmentación. En Actas de ICML, 2000. [20] L. Pevzner y M. Hearst. Una crítica y mejora de una métrica de evaluación para la segmentación de texto. Lingüística Computacional, 28(1):19-36, 2002. [21] J. C. Reynar. Modelos estadísticos para la segmentación de temas. En Actas de ACL, 1999. [22] G. Salton y M. McGill. Introducción a la Recuperación de Información Moderna. McGraw Hill, 1983. [23] B. \n\nMcGraw Hill, 1983. [23] B. Sun, Q. Tan, P. Mitra y C. L. Giles. Extracción y búsqueda de fórmulas químicas en documentos de texto en la web. En Actas de WWW, 2007. [24] B. Sun, D. Zhou, H. Zha, y J. I'm sorry, but \"Yen\" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish? Segmentación y alineación de texto multitarea basada en información mutua ponderada. En Actas de CIKM, 2006. [25] M. Utiyama y H. Isahara. Un modelo estadístico para la segmentación de texto independiente del dominio. En Actas de la 39ª ACL, 1999. [26] C. Wayne. Detección y seguimiento de temas multilingües: Investigación exitosa habilitada por corpora y evaluación. En Actas de LREC, 2000. [27] J. Yamron, I. Carp, L. Gillick, S. Lowe y P. van Mulbregt. Un enfoque de modelo oculto de Markov para la segmentación de texto y seguimiento de eventos. En Actas de ICASSP, 1998. [28] H. Zha y X. Ji. Correlacionando documentos multilingües a través de modelado de gráficos bipartitos. En Actas de SIGIR, 2002. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "single-document segmentation": {
            "translated_key": "segmentación de un solo documento",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Topic Segmentation with Shared Topic Detection and Alignment of Multiple Documents Bingjun Sun*, Prasenjit Mitra*† , Hongyuan Zha‡ , C. Lee Giles*† , John Yen*† *Department of Computer Science and Engineering † College of Information Sciences and Technology The Pennsylvania State University University Park, PA 16802 ‡ College of Computing The Georgia Institute of Technology Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu ABSTRACT Topic detection and tracking [26] and topic segmentation [15] play an important role in capturing the local and sequential information of documents.",
                "Previous work in this area usually focuses on single documents, although similar multiple documents are available in many domains.",
                "In this paper, we introduce a novel unsupervised method for shared topic detection and topic segmentation of multiple similar documents based on mutual information (MI) and weighted mutual information (WMI) that is a combination of MI and term weights.",
                "The basic idea is that the optimal segmentation maximizes MI(or WMI).",
                "Our approach can detect shared topics among documents.",
                "It can find the optimal boundaries in a document, and align segments among documents at the same time.",
                "It also can handle <br>single-document segmentation</br> as a special case of the multi-document segmentation and alignment.",
                "Our methods can identify and strengthen cue terms that can be used for segmentation and partially remove stop words by using term weights based on entropy learned from multiple documents.",
                "Our experimental results show that our algorithm works well for the tasks of <br>single-document segmentation</br>, shared topic detection, and multi-document segmentation.",
                "Utilizing information from multiple documents can tremendously improve the performance of topic segmentation, and using WMI is even better than using MI for the multi-document segmentation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Clustering; H.3.1 [Information Storage and Retrieval]: Content Analysis and IndexingLinguistic processing; I.2.7 [Artificial Intelligence]: Natural Language Processing-Text analysis; I.5.3 [Pattern Recognition]: Clustering-Algorithms;Similarity measures General Terms Algorithms, Design, Experimentation 1.",
                "INTRODUCTION Many researchers have worked on topic detection and tracking (TDT) [26] and topic segmentation during the past decade.",
                "Topic segmentation intends to identify the boundaries in a document with the goal to capture the latent topical structure.",
                "Topic segmentation tasks usually fall into two categories [15]: text stream segmentation where topic transition is identified, and coherent document segmentation in which documents are split into sub-topics.",
                "The former category has applications in automatic speech recognition, while the latter one has more applications such as partial-text query of long documents in information retrieval, text summary, and quality measurement of multiple documents.",
                "Previous research in connection with TDT falls into the former category, targeted on topic tracking of broadcast speech data and newswire text, while the latter category has not been studied very well.",
                "Traditional approaches perform topic segmentation on documents one at a time [15, 25, 6].",
                "Most of them perform badly in subtle tasks like coherent document segmentation [15].",
                "Often, end-users seek documents that have the similar content.",
                "Search engines, like, Google, provide links to obtain similar pages.",
                "At a finer granularity, users may actually be looking to obtain sections of a document similar to a particular section that presumably discusses a topic of the users interest.",
                "Thus, the extension of topic segmentation from single documents to identifying similar segments from multiple similar documents with the same topic is a natural and necessary direction, and multi-document topic segmentation is expected to have a better performance since more information is utilized.",
                "Traditional approaches using similarity measurement based on term frequency generally have the same assumption that similar vocabulary tends to be in a coherent topic segment [15, 25, 6].",
                "However, they usually suffer the issue of identifying stop words.",
                "For example, additional document-dependent stop words are removed together with the generic stop words in [15].",
                "There are two reasons that we do not remove stop words directly.",
                "First, identifying stop words is another issue [12] that requires estimation in each domain.",
                "Removing common stop words may result in the loss of useful information in a specific domain.",
                "Second, even though stop words can be identified, hard classification of stop words and nonstop words cannot represent the gradually changing amount of information content of each word.",
                "We employ a soft classification using term weights.",
                "In this paper, we view the problem of topic segmentation as an optimization issue using information theoretic techniques to find the optimal boundaries of a document given the number of text segments so as to minimize the loss of mutual information (MI) (or a weighted mutual information (WMI)) after segmentation and alignment.",
                "This is equal to maximizing the MI (or WMI).",
                "The MI focuses on measuring the difference among segments whereas previous research focused on finding the similarity (e.g. cosine distance) of segments [15, 25, 6].",
                "Topic alignment of multiple similar documents can be achieved by clustering sentences on the same topic into the same cluster.",
                "Single-document topic segmentation is just a special case of the multi-document topic segmentation and alignment problem.",
                "Terms can be co-clustered as in [10] at the same time, given the number of clusters, but our experimental results show that this method results in a worse segmentation (see Tables 1, 4, and 6).",
                "Usually, human readers can identify topic transition based on cue words, and can ignore stop words.",
                "Inspired by this, we give each term (or term cluster) a weight based on entropy among different documents and different segments of documents.",
                "Not only can this approach increase the contribution of cue words, but it can also decrease the effect of common stop words, noisy word, and document-dependent stop words.",
                "These words are common in a document.",
                "Many methods based on sentence similarity require that these words are removed before topic segmentation can be performed [15].",
                "Our results in Figure 3 show that term weights are useful for multi-document topic segmentation and alignment.",
                "The major contribution of this paper is that it introduces a novel method for topic segmentation using MI and shows that this method performs better than previously used criteria.",
                "Also, we have addressed the problem of topic segmentation and alignment across multiple documents, whereas most existing research focused on segmentation of single documents.",
                "Multi-document segmentation and alignment can utilize information from similar documents and improves the performance of topic segmentation greatly.",
                "Obviously, our approach can handle single documents as a special case when multiple documents are unavailable.",
                "It can detect shared topics among documents to judge if they are multiple documents on the same topic.",
                "We also introduce the new criterion of WMI based on term weights learned from multiple similar documents, which can improve performance of topic segmentation further.",
                "We propose an iterative greedy algorithm based on dynamic programming and show that it works well in practice.",
                "Some of our prior work is in [24].",
                "The rest of this paper is organized as follows: In Section 2, we review related work.",
                "Section 3 contains a formulation of the problem of topic segmentation and alignment of multiple documents with term co-clustering, a review of the criterion of MI for clustering, and finally an introduction to WMI.",
                "In Section 4, we first propose the iterative greedy algorithm of topic segmentation and alignment with term co-clustering, and then describe how the algorithm can be optimized by usFigure 1: Illustration of multi-document segmentation and alignment. ing dynamic programming.",
                "In Section 5, experiments about <br>single-document segmentation</br>, shared topic detection, and multi-document segmentation are described, and results are presented and discussed to evaluate the performance of our algorithm.",
                "Conclusions and some future directions of the research work are discussed in Section 6. 2.",
                "PREVIOUS WORK Generally, the existing approaches to text segmentation fall into two categories: supervised learning [19, 17, 23] and unsupervised learning [3, 27, 5, 6, 15, 25, 21].",
                "Supervised learning usually has good performance, since it learns functions from labelled training sets.",
                "However, often getting large training sets with manual labels on document sentences is prohibitively expensive, so unsupervised approaches are desired.",
                "Some models consider dependence between sentences and sections, such as Hidden Markov Model [3, 27], Maximum Entropy Markov Model [19], and Conditional Random Fields [17], while many other approaches are based on lexical cohesion or similarity of sentences [5, 6, 15, 25, 21].",
                "Some approaches also focus on cue words as hints of topic transitions [11].",
                "While some existing methods only consider information in single documents [6, 15], others utilize multiple documents [16, 14].",
                "There are not many works in the latter category, even though the performance of segmentation is expected to be better with utilization of information from multiple documents.",
                "Previous research studied methods to find shared topics [16] and topic segmentation and summarization between just a pair of documents [14].",
                "Text classification and clustering is a related research area which categorizes documents into groups using supervised or unsupervised methods.",
                "Topical classification or clustering is an important direction in this area, especially co-clustering of documents and terms, such as LSA [9], PLSA [13], and approaches based on distances and bipartite graph partitioning [28] or maximum MI [2, 10], or maximum entropy [1, 18].",
                "Criteria of these approaches can be utilized in the issue of topic segmentation.",
                "Some of those methods have been extended into the area of topic segmentation, such as PLSA [5] and maximum entropy [7], but to our best knowledge, using MI for topic segmentation has not been studied. 3.",
                "PROBLEM FORMULATION Our goal is to segment documents and align the segments across documents (Figure 1).",
                "Let T be the set of terms {t1, t2, ..., tl}, which appear in the unlabelled set of documents D = {d1, d2, ..., dm}.",
                "Let Sd be the set of sentences for document d ∈ D, i.e. {s1, s2, ..., snd }.",
                "We have a 3D matrix of term frequency, in which the three dimensions are random variables of D, Sd, and T. Sd actually is a random vector including a random variable for each d ∈ D. The term frequency can be used to estimate the joint probability distribution P(D, Sd, T), which is p(t, d, s) = T(t, d, s)/ND, where T(t, d, s) is the number of t in ds sentence s and ND is the total number of terms in D. ˆS represents the set of segments {ˆs1, ˆs2, ..., ˆsp} after segmentation and alignment among multiple documents, where the number of segments | ˆS| = p. A segment ˆsi of document d is a sequence of adjacent sentences in d. Since for different documents si may discuss different sub-topics, our goal is to cluster adjacent sentences in each document into segments, and align similar segments among documents, so that for different documents ˆsi is about the same sub-topic.",
                "The goal is to find the optimal topic segmentation and alignment mapping Segd(si) : {s1, s2, ..., snd } → {ˆs1, ˆs2, ..., ˆsp} and Alid(ˆsi) : {ˆs1, ˆs2, ..., ˆsp} → {ˆs1, ˆs2, ..., ˆsp}, for all d ∈ D, where ˆsi is ith segment with the constraint that only adjacent sentences can be mapped to the same segment, i.e. for d, {si, si+1, ..., sj} → {ˆsq}, where q ∈ {1, ..., p}, where p is the segment number, and if i > j, then for d, ˆsq is missing.",
                "After segmentation and alignment, random vector Sd becomes an aligned random variable ˆS.",
                "Thus, P(D, Sd, T) becomes P(D, ˆS, T).",
                "Term co-clustering is a technique that has been employed [10] to improve the accuracy of document clustering.",
                "We evaluate the effect of it for topic segmentation.",
                "A term t is mapped to exactly one term cluster.",
                "Term co-clustering involves simultaneously finding the optimal term clustering mapping Clu(t) : {t1, t2, ..., tl} → {ˆt1, ˆt2, ..., ˆtk}, where k ≤ l, l is the total number of words in all the documents, and k is the number of clusters. 4.",
                "METHODOLOGY We now describe a novel algorithm which can handle singledocument segmentation, shared topic detection, and multidocument segmentation and alignment based on MI or WMI. 4.1 Mutual Information MI I(X; Y ) is a quantity to measure the amount of information which is contained in two or more random variables [8, 10].",
                "For the case of two random variables, we have I(X; Y ) = x∈X y∈Y p(x, y)log p(x, y) p(x)p(y) , (1) Obviously, when random variables X and Y are independent, I(X; Y ) = 0.",
                "Thus, intuitively, the value of MI depends on how random variables are dependent on each other.",
                "The optimal co-clustering is the mapping Clux : X → ˆX and Cluy : Y → ˆY that minimizes the loss: I(X; Y ) − I( ˆX; ˆY ), which is equal to maximizing I( ˆX; ˆY ).",
                "This is the criterion of MI for clustering.",
                "In the case of topic segmentation, the two random variables are the term variable T and the segment variable S, and each sample is an occurrence of a term T = t in a particular segment S = s. I(T; S) is used to measure how dependent T and S are.",
                "However, I(T; S) cannot be computed for documents before segmentation, since we do not have a set of S due to the fact that sentences of Document d, si ∈ Sd, is not aligned with other documents.",
                "Thus, instead of minimizing the loss of MI, we can maximize MI after topic segmentation, computed as: I( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS p(ˆt, ˆs)log p(ˆt, ˆs) p(ˆt)p(ˆs) , (2) where p(ˆt, ˆs) are estimated by the term frequency tf of Term Cluster ˆt and Segment ˆs in the training set D. Note that here a segment ˆs includes sentences about the the same topic among all documents.",
                "The optimal solution is the mapping Clut : T → ˆT, Segd : Sd → ˆS , and Alid : ˆS → ˆS, which maximizes I( ˆT; ˆS). 4.2 Weighted Mutual Information In topic segmentation and alignment of multiple documents, if P(D, ˆS, T) is known, based on the marginal distributions P(D|T) and P( ˆS|T) for each term t ∈ T, we can categorize terms into four types in the data set: • Common stop words are common both along the dimensions of documents and segments. • Document-dependent stop words that depends on the personal writing style are common only along the dimension of segments for some documents. • Cue words are the most important elements for segmentation.",
                "They are common along the dimension of documents only for the same segment, and they are not common along the dimensions of segments. • Noisy words are other words which are not common along both dimensions.",
                "Entropy based on P(D|T) and P( ˆS|T) can be used to identify different types of terms.",
                "To reinforce the contribution of cue words in the MI computation, and simultaneously reduce the effect of the other three types of words, similar as the idea of the tf-idf weight [22], we use entropies of each term along the dimensions of document D and segment ˆS, i.e.",
                "ED(ˆt) and EˆS(ˆt), to compute the weight.",
                "A cue word usually has a large value of ED(ˆt) but a small value of EˆS(ˆt).",
                "We introduce term weights (or term cluster weights) wˆt = ( ED(ˆt) maxˆt ∈ ˆT (ED(ˆt )) )a (1 − EˆS(ˆt) maxˆt ∈ ˆT (EˆS(ˆt )) )b , (3) where ED(ˆt) = d∈D p(d|ˆt)log|D| 1 p(d|ˆt) , EˆS(ˆt) = ˆs∈ ˆS p(ˆs|ˆt)log| ˆS| 1 p(ˆs|ˆt) , and a > 0 and b > 0 are powers to adjust term weights.",
                "Usually a = 1 and b = 1 as default, and maxˆt ∈ ˆT (ED(ˆt )) and maxˆt ∈ ˆT (EˆS(ˆt )) are used to normalize the entropy values.",
                "Term cluster weights are used to adjust p(ˆt, ˆs), pw(ˆt, ˆs) = wˆtp(ˆt, ˆs) ˆt∈ ˆT ;ˆs∈ ˆS wˆtp(ˆt, ˆs) , (4) and Iw( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS pw(ˆt, ˆs)log pw(ˆt, ˆs) pw(ˆt)pw(ˆs) , (5) where pw(ˆt) and pw(ˆs) are marginal distributions of pw(ˆt, ˆs).",
                "However, since we do not know either the term weights or P(D, ˆS, T), we need to estimate them, but wˆt depends on p(ˆs|t) and ˆS, while ˆS and p(ˆs|t) also depend on wˆt that is still unknown.",
                "Thus, an iterative algorithm is required to estimate term weights wˆt and find the best segmentation and alignment to optimize the objective function Iw concurrently.",
                "After a document is segmented into sentences Input: Joint probability distribution P(D, Sd, T), number of text segments p ∈ {2, 3, ..., max(sd)}, number of term clusters k ∈ {2, 3, ..., l} (if k = l, no term co-clustering required), and weight type w ∈ {0, 1}, indicating to use I or Iw, respectively.",
                "Output: Mapping Clu, Seg, Ali, and term weights wˆt.",
                "Initialization: 0. i = 0.",
                "Initialize Clu (0) t , Seg (0) d , and Ali (0) d ; Initialize w (0) ˆt using Equation (6) if w = 1; Stage 1: 1.",
                "If |D| = 1, k = l, and w = 0, check all sequential segmentations of d into p segments and find the best one Segd(s) = argmaxˆsI( ˆT; ˆS), and return Segd; otherwise, if w = 1 and k = l, go to 3.1; Stage 2: 2.1 If k < l, for each term t, find the best cluster ˆt as Clu(i+1)(t) = argmaxˆtI( ˆT; ˆS(i)) based on Seg(i) and Ali(i); 2.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+1) d (Seg (i+1) d (s)) = argmaxˆsI( ˆT(i+1); ˆS) based on Clu(i+1)(t) if k < l or Clu(0)(t) if k = l; 2.3 i + +.",
                "If Clu, Seg, or Ali changed, go to 2.1; otherwise, if w = 0, return Clu(i), Seg(i), and Ali(i); else j = 0, go to 3.1; Stage 3: 3.1 Update w (i+j+1) ˆt based on Seg(i+j), Ali(i+j), and Clu(i) using Equation (3); 3.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+j+1) d (Seg (i+j+1) d (s)) = argmaxˆsIw( ˆT(i); ˆS) based on Clu(i) and w (i+j+1) ˆt ; 3.3 j + +.",
                "If Iw( ˆT; ˆS) changes, go to step 6; otherwise, stop and return Clu(i), Seg(i+j), Ali(i+j), and w (i+j) ˆt ; Figure 2: Algorithm: Topic segmentation and alignment based on MI or WMI. and each sentence is segmented into words, each word is stemmed.",
                "Then the joint probability distribution P(D, Sd, T) can be estimated.",
                "Finally, this distribution can be used to compute MI in our algorithm. 4.3 Iterative Greedy Algorithm Our goal is to maximize the objective function, I( ˆT; ˆS) or Iw( ˆT; ˆS), which can measure the dependence of term occurrences in different segments.",
                "Generally, first we do not know the estimate term weights, which depend on the optimal topic segmentation and alignment, and term clusters.",
                "Moreover, this problem is NP-hard [10], even though if we know the term weights.",
                "Thus, an iterative greedy algorithm is desired to find the best solution, even though probably only local maxima are reached.",
                "We present the iterative greedy algorithm in Figure 2 to find a local maximum of I( ˆT; ˆS) or Iw( ˆT; ˆS) with simultaneous term weight estimation.",
                "This algorithm can is iterative and greedy for multi-document cases or single-document cases with term weight estimation and/or term co-clustering.",
                "Otherwise, since it is just a one step algorithm to solve the task of <br>single-document segmentation</br> [6, 15, 25], the global maximum of MI is guaranteed.",
                "We will show later that term co-clustering reduces the accuracy of the results and is not necessary, and for singledocument segmentation, term weights are also not required. 4.3.1 Initialization In Step 0, the initial term clustering Clut and topic segmentation and alignment Segd and Alid are important to avoid local maxima and reduce the number of iterations.",
                "First, a good guess of term weights can be made by using the distributions of term frequency along sentences for each document and averaging them to get the initial values of wˆt: wt = ( ED(t) maxt ∈T (ED(t )) )(1 − ES(t) maxt ∈T (ES(t )) ), (6) where ES(t) = 1 |Dt| d∈Dt (1 − s∈Sd p(s|t)log|Sd| 1 p(s|t) ), where Dt is the set of documents which contain Term t. Then, for the initial segmentation Seg(0) , we can simply segment documents equally by sentences.",
                "Or we can find the optimal segmentation just for each document d which maximizes the WMI, Seg (0) d = argmaxˆsIw(T; ˆS), where w = w (0) ˆt .",
                "For the initial alignment Ali(0) , we can first assume that the order of segments for each d is the same.",
                "For the initial term clustering Clu(0) , first cluster labels can be set randomly, and after the first time of Step 3, a good initial term clustering is obtained. 4.3.2 Different Cases After initialization, there are three stages for different cases.",
                "Totally there are eight cases, |D| = 1 or |D| > 1, k = l or k < l, w = 0 or w = 1.",
                "Single document segmentation without term clustering and term weight estimation (|D| = 1, k = l, w = 0) only requires Stage 1 (Step 1).",
                "If term clustering is required (k < l), Stage 2 (Step 2.1, 2.2, and 2.3) is executed iteratively.",
                "If term weight estimation is required (w = 1), Stage 3 (Step 3.1, 3.2, and 3.3) is executed iteratively.",
                "If both are required (k < l, w = 1), Stage 2 and 3 run one after the other.",
                "For multi-document segmentation without term clustering and term weight estimation (|D| > 1, k = l, w = 0), only iteration of Step 2.2 and 2.3 are required.",
                "At Stage 1, the global maximum can be found based on I( ˆT; ˆS) using dynamic programming in Section 4.4.",
                "Simultaneously finding a good term clustering and estimated term weights is impossible, since when moving a term to a new term cluster to maximize Iw( ˆT; ˆS), we do not know that the weight of this term should be the one of the new cluster or the old cluster.",
                "Thus, we first do term clustering at Stage 2, and then estimate term weights at Stage 3.",
                "At Stage 2, Step 2.1 is to find the best term clustering and Step 2.2 is to find the best segmentation.",
                "This cycle is repeated to find a local maximum based on MI I until it converges.",
                "The two steps are: (1) based on current term clustering Cluˆt, for each document d, the algorithm segments all the sentences Sd into p segments sequentially (some segments may be empty), and put them into the p segments ˆS of the whole training set D (all possible cases of different segmentation Segd and alignment Alid are checked) to find the optimal case, and (2) based on the current segmentation and alignment, for each term t, the algorithm finds the best term cluster of t based on the current segmentation Segd and alignment Alid.",
                "After finding a good term clustering, term weights are estimated if w = 1.",
                "At Stage 3, similar as Stage 2, Step 3.1 is term weight re-estimation and Step 3.2 is to find a better segmentation.",
                "They are repeated to find a local maximum based on WMI Iw until it converges.",
                "However, if the term clustering in Stage 2 is not accurate, then the term weight estimation at Stage 3 may have a bad result.",
                "Finally, at Step 3.3, this algorithm converges and return the output.",
                "This algorithm can handle both single-document and multi-document segmentation.",
                "It also can detect shared topics among documents by checking the proportion of overlapped sentences on the same topics, as described in Sec 5.2. 4.4 Algorithm Optimization In many previous works on segmentation, dynamic programming is a technique used to maximize the objective function.",
                "Similarly, at Step 1, 2.2, and 3.2 of our algorithm, we can use dynamic programming.",
                "For Stage 1, using dynamic programming can still find the global optimum, but for Stage 2 and Stage 3, we can only find the optimum for each step of topic segmentation and alignment of a document.",
                "Here we only show the dynamic programming for Step 3.2 using WMI (Step 1 and 2.2 are similar but they can use either I or Iw).",
                "There are two cases that are not shown in the algorithm in Figure 2: (a) <br>single-document segmentation</br> or multi-document segmentation with the same sequential order of segments, where alignment is not required, and (b) multi-document segmentation with different sequential orders of segments, where alignment is necessary.",
                "The alignment mapping function of the former case is simply just Alid(ˆsi) = ˆsi, while for the latter ones alignment mapping function Alid(ˆsi) = ˆsj, i and j may be different.",
                "The computational steps for the two cases are listed below: Case 1 (no alignment): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs) and partial pw(ˆs) without counting sentences from d. Then put sentences from i to j into Part k, and compute partial WMI PIw( ˆT; ˆsk(si, si+1, ..., sj)) ˆt∈ ˆT pw(ˆt, ˆsk)log pw(ˆt, ˆsk) pw(ˆt)pw(ˆsk) , where Alid(si, si+1, ..., sj) = k, k ∈ {1, 2, ..., p}, 1 ≤ i ≤ j ≤ nd, and Segd(sq) = ˆsk for all i ≤ q ≤ j. (2) Let M(sm, 1) = PIw( ˆT; ˆs1(s1, s2, ..., sm)).",
                "Then M(sm, L) = maxi[M(si−1, L − 1) + PIw( ˆT; ˆsL(si, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, and when i > m, no sentences are put into ˆsk when compute PIw (note PIw( ˆT; ˆs(si, ..., sm)) = 0 for <br>single-document segmentation</br>). (3) Finally M(snd , p) = maxi[M(si−1, p − 1)+ PIw( ˆT; ˆsp(si, ..., snd ))], where 1 ≤ i ≤ nd+1.",
                "The optimal Iw is found and the corresponding segmentation is the best.",
                "Case 2 (alignment required): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs), and partial pw(ˆs), and PIw( ˆT; ˆsk(si, si+1, ..., sj)) similarly as Case 1. (2) Let M(sm, 1, k) = PIw( ˆT; ˆsk(s1, s2, ..., sm)), where k ∈ {1, 2, ..., p}.",
                "Then M(sm, L, kL) = maxi,j[M(si−1, L − 1, kL/j) + PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, kL ∈ Set(p, L), which is the set of all p!",
                "L!(p−L)! combinations of L segments chosen from all p segments, j ∈ kL, the set of L segments chosen from all p segments, and kL/j is the combination of L − 1 segments in kL except Segment j. (3) Finally, M(snd , p, kp) = maxi,j[M(si−1, p − 1, kp/j) +PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., snd ))], where kp is just the combination of all p segments and 1 ≤ i ≤ nd + 1, which is the optimal Iw and the corresponding segmentation is the best.",
                "The steps of Case 1 and 2 are similar, except in Case 2, alignment is considered in addition to segmentation.",
                "First, basic items of probability for computing Iw are computed excluding Doc d, and then partial WMI by putting every possible sequential segment (including empty segment) of d into every segment of the set.",
                "Second, the optimal sum of PIw for L segments and the leftmost m sentences, M(sm, L), is found.",
                "Finally, the maximal WMI is found among different sums of M(sm, p − 1) and PIw for Segment p. 5.",
                "EXPERIMENTS In this section, <br>single-document segmentation</br>, shared topic detection, and multi-document segmentation will be tested.",
                "Different hyper parameters of our method are studied.",
                "For convenience, we refer to the method using I as MIk if w = 0, and Iw as WMIk if w = 2 or as WMIk if w = 1, where k is the number of term clusters, and if k = l, where l is the total number of terms, then no term clustering is required, i.e.",
                "MIl and WMIl. 5.1 <br>single-document segmentation</br> 5.1.1 Test Data and Evaluation The first data set we tested is a synthetic one used in previous research [6, 15, 25] and many other papers.",
                "It has 700 samples.",
                "Each is a concatenation of ten segments.",
                "Each segment is the first n sentence selected randomly from the Brown corpus, which is supposed to have a different topic from each other.",
                "Currently, the best results on this data set is achieved by Ji et.al. [15].",
                "To compare the performance of our methods, the criterion used widely in previous research is applied, instead of the unbiased criterion introduced in [20].",
                "It chooses a pair of words randomly.",
                "If they are in different segments (different) for the real segmentation (real), but predicted (pred) as in the same segment, it is a miss.",
                "If they are in the same segment (same), but predicted as in different segments, it is a false alarm.",
                "Thus, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, diff)p(diff|real) +p(false alarm|real, pred, same)p(same|real). 5.1.2 Experiment Results We tested the case when the number of segments is known.",
                "Table 1 shows the results of our methods with different hyper parameter values and three previous approaches, C99[25], U00[6], and ADDP03[15], on this data set when the segment number is known.",
                "In WMI for <br>single-document segmentation</br>, the term weights are computed as follows: wˆt = 1−EˆS(ˆt)/maxˆt ∈ ˆT (EˆS(ˆt )).",
                "For this case, our methods MIl and WMIl both outperform all the previous approaches.",
                "We compared our methods with ADDP03using one-sample one-sided t-test and p-values are shown in Table 2.",
                "From the p-values, we can see that mostly the differences are very Table 1: Average Error Rates of <br>single-document segmentation</br> Given Segment Numbers Known Range of n 3-11 3-5 6-8 9-11 Sample size 400 100 100 100 C99 12% 11% 10% 9% U00 10% 9% 7% 5% ADDP03 6.0% 6.8% 5.2% 4.3% MIl 4.68% 5.57% 2.59% 1.59% WMIl 4.94% 6.33% 2.76% 1.62% MI100 9.62% 12.92% 8.66% 6.67% Table 2: <br>single-document segmentation</br>: P-values of T-test on Error Rates Range of n 3-11 3-5 6-8 9-11 ADDP03, MIl 0.000 0.000 0.000 0.000 ADDP03, WMIl 0.000 0.099 0.000 0.000 MIl, WMIl 0.061 0.132 0.526 0.898 significant.",
                "We also compare the error rates between our two methods using two-sample two-sided t-test to check the hypothesis that they are equal.",
                "We cannot reject the hypothesis that they are equal, so the difference are not significant, even though all the error rates for MIl are smaller than WMIl.",
                "However, we can conclude that term weights contribute little in <br>single-document segmentation</br>.",
                "The results also show that MI using term co-clustering (k = 100) decreases the performance.",
                "We tested different number of term clusters, and found that the performance becomes better when the cluster number increases to reach l. WMIk<l has similar results that we did not show in the table.",
                "As mentioned before, using MI may be inconsistent on optimal boundaries given different numbers of segments.",
                "This situation occurs especially when the similarities among segments are quite different, i.e. some transitions are very obvious, while others are not.",
                "This is because usually a document is a hierarchical structure instead of only a sequential structure.",
                "When the segments are not at the same level, this situation may occur.",
                "Thus, a hierarchical topic segmentation approach is desired, and the structure highly depends on the number of segments for each internal node and the stop criteria of splitting.",
                "For this data set of singledocument segmentation, since it is just a synthetic set, which is just a concatenation of several segments about different topics, it is reasonable that approaches simply based on term frequency have a good performance.",
                "Usually for the tasks of segmenting coherent documents for sub-topics, the effectiveness decreases much. 5.2 Shared Topic Detection 5.2.1 Test Data and Evaluation The second data set contains 80 news articles from Google News.",
                "There are eight topics and each has 10 articles.",
                "We randomly split the set into subsets with different document numbers and each subset has all eight topics.",
                "We compare our approach MIl and WMIl with LDA [4].",
                "LDA treats a document in the data set as a bag of words, finds its distribution on topics, and its major topic.",
                "MIl and WMIl views each sentence as a bag of words and tag it with a topic label.",
                "Then for each pair of documents, LDA determines if they are on the same topic, while MIl and Table 3: Shared Topic Detection: Average Error Rates for Different Numbers of Documents in Each Subset #Doc 10 20 40 80 LDA 8.89% 16.33% 1.35% 0.60% MIl, θ = 0.6 4.17% 1.71% 1.47% 0.0% WMIl, θ = 0.8 18.6% 3.16% 1.92% 0.0% WMIl check whether the proportion overlapped sentences on the same topic is larger than the adjustable threshold θ.",
                "That is, in MIl and WMIl, for a pair of documents d, d , if [ s∈Sd,s ∈Sd 1(topics=topics )/min(|Sd|, |Sd|)] > θ, where Sd is the set of sentences of d, and |Sd| is the number of sentences of d, then d and d have the shared topic.",
                "For a pair of documents selected randomly, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, same)p(same|real) +p(false alarm|real, pred, diff)p(diff|real), where a miss means if they have the same topic (same) for the real case (real), but predicted (pred) as on the same topic.",
                "If they are on different topics (diff), but predicted as on the same topic, it is a false alarm. 5.2.2 Experiment Results The results are shown in Table 3.",
                "If most documents have different topics, in WMIl, the estimation of term weights in Equation (3) is not correct.",
                "Thus, WMIl is not expected to have a better performance than MIl, when most documents have different topics.",
                "When there are fewer documents in a subset with the same number of topics, more documents have different topics, so WMIl is more worse than MIl.",
                "We can see that for most cases MIl has a better (or at least similar) performance than LDA.",
                "After shared topic detection, multi-document segmentation of documents with the shared topics is able to be executed. 5.3 Multi-document Segmentation 5.3.1 Test Data and Evaluation For multi-document segmentation and alignment, our goal is to identify these segments about the same topic among multiple similar documents with shared topics.",
                "Using Iw is expected to perform better than I, since without term weights the result is affected seriously by document-dependent stop words and noisy words which depends on the personal writing style.",
                "It is more likely to treat the same segments of different documents as different segments under the effect of document-dependent stop words and noisy words.",
                "Term weights can reduce the effect of document-dependent stop words and noisy words by giving cue terms more weights.",
                "The data set for multi-document segmentation and alignment has 102 samples and 2264 sentences totally.",
                "Each is the introduction part of a lab report selected from the course of Biol 240W, Pennsylvania State University.",
                "Each sample has two segments, introduction of plant hormones and the content in the lab.",
                "The length range of samples is from two to 56 sentences.",
                "Some samples only have one part and some have a reverse order the these two segments.",
                "It is not hard to identify the boundary between two segments for human.",
                "We labelled each sentence manually for evaluation.",
                "The criterion of evaluation is just using the proportion of the number of sentences with wrong predicted segment labels in the total number of sentences in the whole training Table 4: Average Error Rates of Multi-document Segmentation Given Segment Numbers Known #Doc MIl WMIl k MIk WMIk 102 3.14% 2.78% 300 4.68% 6.58% 51 4.17% 3.63% 300 17.83% 22.84% 34 5.06% 4.12% 300 18.75% 20.95% 20 7.08% 5.42% 250 20.40% 21.83% 10 10.38% 7.89% 250 21.42% 21.91% 5 15.77% 11.64% 250 21.89% 22.59% 2 25.90% 23.18% 50 25.44% 25.49% 1 23.90% 24.82% 25 25.75% 26.15% Table 5: Multi-document Segmentation: P-values of T-test on Error Rates for MIl and WMIl #Doc 51 34 20 10 5 2 P-value 0.19 0.101 0.025 0.001 0.000 0.002 set as the error rate: p(error|predicted, real) = d∈D s∈Sd 1(predicteds=reals)/ d∈D nd.",
                "In order to show the benefits of multi-document segmentation and alignment, we compared our method with different parameters on different partitions of the same training set.",
                "Except the cases that the number of documents is 102 and one (they are special cases of using the whole set and the pure <br>single-document segmentation</br>), we randomly divided the training set into m partitions, and each has 51, 34, 20, 10, 5, and 2 document samples.",
                "Then we applied our methods on each partition and calculated the error rate of the whole training set.",
                "Each case was repeated for 10 times for computing the average error rates.",
                "For different partitions of the training set, different k values are used, since the number of terms increases when the document number in each partition increases. 5.3.2 Experiment Results From the experiment results in Table 4, we can see the following observations: (1) When the number of documents increases, all methods have better performances.",
                "Only from one to two documents, MIl has decrease a little.",
                "We can observe this from Figure 3 at the point of document number = 2.",
                "Most curves even have the worst results at this point.",
                "There are two reasons.",
                "First, because samples vote for the best multi-document segmentation and alignment, but if only two documents are compared with each other, the one with missing segments or a totally different sequence will affect the correct segmentation and alignment of the other.",
                "Second, as noted at the beginning of this section, if two documents have more document-dependent stop words or noisy words than cue words, then the algorithm may view them as two different segments and the other segment is missing.",
                "Generally, we can only expect a better performance when the number of documents is larger than the number of segments. (2) Except <br>single-document segmentation</br>, WMIl is always better than MIl, and when the number of documents is reaching one or increases to a very large number, their performances become closer.",
                "Table 5 shows p-values of twosample one-sided t-test between MIl and WMIl.",
                "We also can see this trend from p-values.",
                "When document number = 5, we reached the smallest p-value and the largest difference between error rates of MIl and WMIl.",
                "For single-document Table 6: Multi-document Segmentation: Average Error Rate for Document Number = 5 in Each Subset with Different Number of Term Clusters #Cluster 75 100 150 250 l MIk 24.67% 24.54% 23.91% 22.59% 15.77% segmentation, WMIl is even a little bit worse than MIl, which is similar as the results of the <br>single-document segmentation</br> on the first data set.",
                "The reason is that for singledocument segmentation, we cannot estimate term weights accurately, since multiple documents are unavailable. (3) Using term clustering usually gets worse results than MIl and WMIl. (4) Using term clustering in WMIk is even worse than in MIk, since in WMIk term clusters are found first using I before using Iw.",
                "If the term clusters are not correct, then the term weights are estimated worse, which may mislead the algorithm to reach even worse results.",
                "From the results we also found that in multi-document segmentation and alignment, most documents with missing segments and a reverse order are identified correctly.",
                "Table 6 illustrates the experiment results for the case of 20 partitions (each has five document samples) of the training set and topic segmentation and alignment using MIk with different numbers of term clusters k. Notice that when the number of term clusters increases, the error rate becomes smaller.",
                "Without term clustering, we have the best result.",
                "We did not show results for WMIk with term clustering, but the results are similar.",
                "We also tested WMIl with different hyper parameters of a and b to adjust term weights.",
                "The results are presented in Figure 3.",
                "It was shown that the default case WMIl : a = 1, b = 1 gave the best results for different partitions of the training set.",
                "We can see the trend that when the document number is very small or large, the difference between MIl : a = 0, b = 0 and WMIl : a = 1, b = 1 becomes quite small.",
                "When the document number is not large (about from 2 to 10), all the cases using term weights have better performances than MIl : a = 0, b = 0 without term weights, but when the document number becomes larger, the cases WMIl : a = 1, b = 0 and WMIl : a = 2, b = 1 become worse than MIl : a = 0, b = 0.",
                "When the document number becomes very large, they are even worse than cases with small document numbers.",
                "This means that a proper way to estimate term weights for the criterion of WMI is very important.",
                "Figure 4 shows the term weights learned from the whole training set.",
                "Four types of words are categorized roughly even though the transition among them are subtle.",
                "Figure 5 illustrates the change in (weighted) mutual information for MIl and WMIl.",
                "As expected, mutual information for MIl increases monotonically with the number of steps, while WMIl does not.",
                "Finally, MIl and WMIl are scalable, with computational complexity shown in Figure 6.",
                "One advantage for our approach based on MI is that removing stop words is not required.",
                "Another important advantage is that there are no necessary hyper parameters to adjust.",
                "In <br>single-document segmentation</br>, the performance based on MI is even better for that based on WMI, so no extra hyper parameter is required.",
                "In multi-document segmentation, we show in the experiment, a = 1 and b = 1 is the best.",
                "Our method gives more weights to cue terms.",
                "However, usually cue terms or sentences appear at the beginning of a segment, while the end of the segment may be 1 2 5 10 20 34 51 102 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 Document Number ErrorRate MIl :a=0,b=0 WMI l :a=1,b=1 WMI l :a=1,b=0 WMI l :a=2,b=1 Figure 3: Error rates for different hyper parameters of term weights. 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Normalized Document Entropy NormalizedSegmentEntropy Noisy words Cue words Common stop words Doc−dep stop words Figure 4: Term weights learned from the whole training set. 0 100 200 300 400 500 600 0.06 0.08 0.1 0.12 0.14 0.16 0.18 Number of Steps (Weighted)MutualInformation MI l WMI l Figure 5: Change in (weighted) MI for MIl and WMIl. 0 20 40 60 80 100 120 0 200 400 600 800 1000 1200 1400 1600 1800 2000 Document Number TimetoConverge(sec) MI l WMI l Figure 6: Time to converge for MIl and WMIl. much noisy.",
                "One possible solution is giving more weights to terms at the beginning of each segment.",
                "Moreover, when the length of segments are quite different, long segments have much higher term frequencies, so they may dominate the segmentation boundaries.",
                "Normalization of term frequencies versus the segment length may be useful. 6.",
                "CONCLUSIONS AND FUTURE WORK We proposed a novel method for multi-document topic segmentation and alignment based on weighted mutual information, which can also handle single-document cases.",
                "We used dynamic programming to optimize our algorithm.",
                "Our approach outperforms all the previous methods on singledocument cases.",
                "Moreover, we also showed that doing segmentation among multiple documents can improve the performance tremendously.",
                "Our results also illustrated that using weighted mutual information can utilize the information of multiple documents to reach a better performance.",
                "We only tested our method on limited data sets.",
                "More data sets especially complicated ones should be tested.",
                "More previous methods should be compared with.",
                "Moreover, natural segmentations like paragraphs are hints that can be used to find the optimal boundaries.",
                "Supervised learning also can be considered. 7.",
                "ACKNOWLEDGMENTS The authors want to thank Xiang Ji, and Prof. J. Scott Payne for their help. 8.",
                "REFERENCES [1] A. Banerjee, I. Ghillon, J. Ghosh, S. Merugu, and D. Modha.",
                "A generalized maximum entropy approach to bregman co-clustering and matrix approximation.",
                "In Proceedings of SIGKDD, 2004. [2] R. Bekkerman, R. El-Yaniv, and A. McCallum.",
                "Multi-way distributional clustering via pairwise interactions.",
                "In Proceedings of ICML, 2005. [3] D. M. Blei and P. J. Moreno.",
                "Topic segmentation with an aspect hidden markov model.",
                "In Proceedings of SIGIR, 2001. [4] D. M. Blei, A. Ng, and M. Jordan.",
                "Latent dirichlet allocation.",
                "Journal of Machine Learning Research, 3:993-1022, 2003. [5] T. Brants, F. Chen, and I. Tsochantaridis.",
                "Topic-based document segmentation with probabilistic latent semantic analysis.",
                "In Proceedings of CIKM, 2002. [6] F. Choi.",
                "Advances in domain indepedent linear text segmentation.",
                "In Proceedings of the NAACL, 2000. [7] H. Christensen, B. Kolluru, Y. Gotoh, and S. Renals.",
                "Maximum entropy segmentation of broadcast news.",
                "In Proceedings of ICASSP, 2005. [8] T. Cover and J. Thomas.",
                "Elements of Information Theory.",
                "John Wiley and Sons, New York, USA, 1991. [9] S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and R. Harshman.",
                "Indexing by latent semantic analysis.",
                "Journal of the American Society for Information Systems, 1990. [10] I. Dhillon, S. Mallela, and D. Modha.",
                "Information-theoretic co-clustering.",
                "In Proceedings of SIGKDD, 2003. [11] M. Hajime, H. Takeo, and O. Manabu.",
                "Text segmentation with multiple surface linguistic cues.",
                "In Proceedings of COLING-ACL, 1998. [12] T. K. Ho.",
                "Stop word location and identification for adaptive text recognition.",
                "International Journal of Document Analysis and Recognition, 3(1), August 2000. [13] T. Hofmann.",
                "Probabilistic latent semantic analysis.",
                "In Proceedings of the UAI99, 1999. [14] X. Ji and H. Zha.",
                "Correlating summarization of a pair of multilingual documents.",
                "In Proceedings of RIDE, 2003. [15] X. Ji and H. Zha.",
                "Domain-independent text segmentation using anisotropic diffusion and dynamic programming.",
                "In Proceedings of SIGIR, 2003. [16] X. Ji and H. Zha.",
                "Extracting shared topics of multiple documents.",
                "In Proceedings of the 7th PAKDD, 2003. [17] J. Lafferty, A. McCallum, and F. Pereira.",
                "Conditional random fields: Probabilistic models for segmenting and labeling sequence data.",
                "In Proceedings of ICML, 2001. [18] T. Li, S. Ma, and M. Ogihara.",
                "Entropy-based criterion in categorical clustering.",
                "In Proceedings of ICML, 2004. [19] A. McCallum, D. Freitag, and F. Pereira.",
                "Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of ICML, 2000. [20] L. Pevzner and M. Hearst.",
                "A critique and improvement of an evaluation metric for text segmentation.",
                "Computational Linguistic, 28(1):19-36, 2002. [21] J. C. Reynar.",
                "Statistical models for topic segmentation.",
                "In Proceedings of ACL, 1999. [22] G. Salton and M. McGill.",
                "Introduction to Modern Information Retrieval.",
                "McGraw Hill, 1983. [23] B.",
                "Sun, Q. Tan, P. Mitra, and C. L. Giles.",
                "Extraction and search of chemical formulae in text documents on the web.",
                "In Proceedings of WWW, 2007. [24] B.",
                "Sun, D. Zhou, H. Zha, and J.",
                "Yen.",
                "Multi-task text segmentation and alignment based on weighted mutual information.",
                "In Proceedings of CIKM, 2006. [25] M. Utiyama and H. Isahara.",
                "A statistical model for domain-independent text segmentation.",
                "In Proceedings of the 39th ACL, 1999. [26] C. Wayne.",
                "Multilingual topic detection and tracking: Successful research enabled by corpora and evaluation.",
                "In Proceedings of LREC, 2000. [27] J. Yamron, I. Carp, L. Gillick, S. Lowe, and P. van Mulbregt.",
                "A hidden markov model approach to text segmentation and event tracking.",
                "In Proceedings of ICASSP, 1998. [28] H. Zha and X. Ji.",
                "Correlating multilingual documents via bipartite graph modeling.",
                "In Proceedings of SIGIR, 2002."
            ],
            "original_annotated_samples": [
                "It also can handle <br>single-document segmentation</br> as a special case of the multi-document segmentation and alignment.",
                "Our experimental results show that our algorithm works well for the tasks of <br>single-document segmentation</br>, shared topic detection, and multi-document segmentation.",
                "In Section 5, experiments about <br>single-document segmentation</br>, shared topic detection, and multi-document segmentation are described, and results are presented and discussed to evaluate the performance of our algorithm.",
                "Otherwise, since it is just a one step algorithm to solve the task of <br>single-document segmentation</br> [6, 15, 25], the global maximum of MI is guaranteed.",
                "There are two cases that are not shown in the algorithm in Figure 2: (a) <br>single-document segmentation</br> or multi-document segmentation with the same sequential order of segments, where alignment is not required, and (b) multi-document segmentation with different sequential orders of segments, where alignment is necessary."
            ],
            "translated_annotated_samples": [
                "También puede manejar la <br>segmentación de un solo documento</br> como un caso especial de la segmentación y alineación de varios documentos.",
                "Nuestros resultados experimentales muestran que nuestro algoritmo funciona bien para las tareas de <br>segmentación de un solo documento</br>, detección de temas compartidos y segmentación de múltiples documentos.",
                "En la Sección 5, se describen experimentos sobre la <br>segmentación de documentos individuales</br>, la detección de temas compartidos y la segmentación de documentos múltiples, y se presentan y discuten los resultados para evaluar el rendimiento de nuestro algoritmo.",
                "De lo contrario, dado que es solo un algoritmo de un paso para resolver la tarea de <br>segmentación de un solo documento</br> [6, 15, 25], se garantiza el máximo global de MI.",
                "Hay dos casos que no se muestran en el algoritmo de la Figura 2: (a) <br>segmentación de un solo documento</br> o segmentación de múltiples documentos con el mismo orden secuencial de segmentos, donde no se requiere alineación, y (b) segmentación de múltiples documentos con diferentes órdenes secuenciales de segmentos, donde la alineación es necesaria."
            ],
            "translated_text": "La segmentación de temas con detección de temas compartidos y alineación de múltiples documentos Bingjun Sun*, Prasenjit Mitra*†, Hongyuan Zha‡, C. Lee Giles*†, John Yen*† *Departamento de Ciencias de la Computación e Ingeniería † Colegio de Ciencias de la Información y Tecnología Universidad Estatal de Pensilvania University Park, PA 16802 ‡ Colegio de Computación Instituto de Tecnología de Georgia Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu RESUMEN La detección y seguimiento de temas [26] y la segmentación de temas [15] juegan un papel importante en la captura de la información local y secuencial de los documentos. El trabajo previo en esta área suele centrarse en documentos individuales, aunque existen múltiples documentos similares en muchos dominios. En este artículo, presentamos un novedoso método no supervisado para la detección de temas compartidos y la segmentación de temas de múltiples documentos similares basado en la información mutua (MI) y la información mutua ponderada (WMI), que es una combinación de MI y pesos de términos. La idea básica es que la segmentación óptima maximiza MI (o WMI). Nuestro enfoque puede detectar temas compartidos entre documentos. Puede encontrar los límites óptimos en un documento y alinear segmentos entre documentos al mismo tiempo. También puede manejar la <br>segmentación de un solo documento</br> como un caso especial de la segmentación y alineación de varios documentos. Nuestros métodos pueden identificar y fortalecer términos de señal que pueden ser utilizados para la segmentación y eliminar parcialmente las palabras de parada mediante el uso de pesos de términos basados en la entropía aprendida de múltiples documentos. Nuestros resultados experimentales muestran que nuestro algoritmo funciona bien para las tareas de <br>segmentación de un solo documento</br>, detección de temas compartidos y segmentación de múltiples documentos. El uso de información de múltiples documentos puede mejorar enormemente el rendimiento de la segmentación de temas, y utilizar WMI es aún mejor que utilizar MI para la segmentación de múltiples documentos. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información-Agrupación; H.3.1 [Almacenamiento y Recuperación de Información]: Análisis de Contenido e Indexación-Procesamiento Lingüístico; I.2.7 [Inteligencia Artificial]: Procesamiento de Lenguaje Natural-Análisis de Texto; I.5.3 [Reconocimiento de Patrones]: Algoritmos de Agrupación-Medidas de Similitud Términos Generales Algoritmos, Diseño, Experimentación 1. INTRODUCCIÓN Muchos investigadores han trabajado en la detección y seguimiento de temas (TDT) [26] y la segmentación de temas durante la última década. La segmentación de temas tiene como objetivo identificar los límites en un documento con la meta de capturar la estructura temática latente. Las tareas de segmentación de temas generalmente se dividen en dos categorías [15]: la segmentación de flujos de texto, donde se identifica la transición de temas, y la segmentación coherente de documentos, en la que los documentos se dividen en subtemas. La primera categoría tiene aplicaciones en el reconocimiento automático del habla, mientras que la segunda tiene más aplicaciones como la consulta parcial de texto en documentos extensos en la recuperación de información, resumen de texto y medición de calidad de múltiples documentos. La investigación previa en relación con TDT se enmarca en la primera categoría, centrada en el seguimiento de temas de datos de discurso de transmisión y texto de agencia de noticias, mientras que la segunda categoría no ha sido estudiada muy bien. Los enfoques tradicionales realizan la segmentación de temas en los documentos uno a la vez [15, 25, 6]. La mayoría de ellos tienen un mal desempeño en tareas sutiles como la segmentación coherente de documentos [15]. A menudo, los usuarios finales buscan documentos que tengan un contenido similar. Los motores de búsqueda, como Google, proporcionan enlaces para obtener páginas similares. A una granularidad más fina, los usuarios pueden estar buscando obtener secciones de un documento similares a una sección en particular que presumiblemente discute un tema de interés para los usuarios. Por lo tanto, la extensión de la segmentación de temas desde documentos individuales para identificar segmentos similares de múltiples documentos similares con el mismo tema es una dirección natural y necesaria, y se espera que la segmentación de temas de múltiples documentos tenga un mejor rendimiento ya que se utiliza más información. Los enfoques tradicionales que utilizan la medición de similitud basada en la frecuencia de términos generalmente parten del mismo supuesto de que un vocabulario similar tiende a estar en un segmento de tema coherente [15, 25, 6]. Sin embargo, suelen tener el problema de identificar las palabras de parada. Por ejemplo, se eliminan las palabras de parada dependientes del documento junto con las palabras de parada genéricas en [15]. Hay dos razones por las que no eliminamos las palabras de parada directamente. Primero, identificar las palabras vacías es otro problema [12] que requiere estimación en cada dominio. Eliminar palabras comunes de parada puede resultar en la pérdida de información útil en un dominio específico. Segundo, aunque se pueden identificar las palabras de parada, la clasificación estricta de palabras de parada y palabras no de parada no puede representar el cambio gradual en la cantidad de contenido informativo de cada palabra. Empleamos una clasificación suave utilizando pesos de términos. En este documento, consideramos el problema de la segmentación de temas como un problema de optimización utilizando técnicas de teoría de la información para encontrar los límites óptimos de un documento dado el número de segmentos de texto, de manera que se minimice la pérdida de información mutua (MI) (o información mutua ponderada (WMI)) después de la segmentación y alineación. Esto es igual a maximizar el MI (o WMI). El MI se centra en medir la diferencia entre segmentos, mientras que investigaciones anteriores se enfocaban en encontrar la similitud (por ejemplo, la distancia del coseno) entre segmentos [15, 25, 6]. La alineación de temas de múltiples documentos similares se puede lograr agrupando las oraciones sobre el mismo tema en el mismo clúster. La segmentación de temas en un solo documento es solo un caso especial del problema de segmentación y alineación de temas en múltiples documentos. Los términos pueden ser co-agrupados como en [10] al mismo tiempo, dado el número de agrupaciones, pero nuestros resultados experimentales muestran que este método resulta en una peor segmentación (ver Tablas 1, 4 y 6). Por lo general, los lectores humanos pueden identificar la transición de temas basándose en palabras de señal, y pueden ignorar las palabras vacías. Inspirados por esto, asignamos a cada término (o grupo de términos) un peso basado en la entropía entre diferentes documentos y diferentes segmentos de documentos. No solo este enfoque puede aumentar la contribución de las palabras clave, sino que también puede disminuir el efecto de las palabras de parada comunes, palabras ruidosas y palabras de parada dependientes del documento. Estas palabras son comunes en un documento. Muchos métodos basados en la similitud de oraciones requieren que estas palabras sean eliminadas antes de que se pueda realizar la segmentación de temas [15]. Nuestros resultados en la Figura 3 muestran que los pesos de los términos son útiles para la segmentación y alineación de temas en múltiples documentos. La principal contribución de este artículo es que introduce un método novedoso para la segmentación de temas utilizando MI y demuestra que este método funciona mejor que los criterios previamente utilizados. Además, hemos abordado el problema de la segmentación y alineación de temas en varios documentos, mientras que la mayoría de las investigaciones existentes se centraron en la segmentación de documentos individuales. La segmentación y alineación de múltiples documentos pueden utilizar información de documentos similares y mejorar significativamente el rendimiento de la segmentación de temas. Obviamente, nuestro enfoque puede manejar documentos individuales como un caso especial cuando varios documentos no están disponibles. Puede detectar temas compartidos entre documentos para determinar si son múltiples documentos sobre el mismo tema. También introducimos el nuevo criterio de WMI basado en pesos de términos aprendidos de múltiples documentos similares, lo cual puede mejorar aún más el rendimiento de la segmentación de temas. Proponemos un algoritmo voraz iterativo basado en programación dinámica y demostramos que funciona bien en la práctica. Algunos de nuestros trabajos anteriores están en [24]. El resto de este documento está organizado de la siguiente manera: En la Sección 2, revisamos el trabajo relacionado. La sección 3 contiene una formulación del problema de segmentación de temas y alineación de múltiples documentos con co-clustering de términos, una revisión del criterio de MI para el clustering, y finalmente una introducción a WMI. En la Sección 4, primero proponemos el algoritmo voraz iterativo de segmentación y alineación de temas con co-clustering de términos, y luego describimos cómo el algoritmo puede ser optimizado mediante programación dinámica. En la Sección 5, se describen experimentos sobre la <br>segmentación de documentos individuales</br>, la detección de temas compartidos y la segmentación de documentos múltiples, y se presentan y discuten los resultados para evaluar el rendimiento de nuestro algoritmo. Las conclusiones y algunas direcciones futuras del trabajo de investigación se discuten en la Sección 6.2. TRABAJO PREVIO En general, los enfoques existentes para la segmentación de texto se dividen en dos categorías: aprendizaje supervisado [19, 17, 23] y aprendizaje no supervisado [3, 27, 5, 6, 15, 25, 21]. El aprendizaje supervisado suele tener un buen rendimiento, ya que aprende funciones a partir de conjuntos de entrenamiento etiquetados. Sin embargo, obtener conjuntos de entrenamiento grandes con etiquetas manuales en oraciones de documentos suele ser prohibitivamente costoso, por lo que se prefieren enfoques no supervisados. Algunos modelos consideran la dependencia entre oraciones y secciones, como el Modelo Oculto de Markov [3, 27], el Modelo de Máxima Entropía de Markov [19] y los Campos Aleatorios Condicionales [17], mientras que muchos otros enfoques se basan en la cohesión léxica o la similitud de las oraciones [5, 6, 15, 25, 21]. Algunos enfoques también se centran en las palabras clave como pistas de transiciones de tema [11]. Si bien algunos métodos existentes solo consideran información en documentos individuales [6, 15], otros utilizan varios documentos [16, 14]. No hay muchas obras en la última categoría, aunque se espera que el rendimiento de la segmentación sea mejor con la utilización de información de múltiples documentos. Investigaciones previas estudiaron métodos para encontrar temas compartidos [16] y segmentación y resumen de temas entre solo un par de documentos [14]. La clasificación y agrupación de textos es un área de investigación relacionada que categoriza documentos en grupos utilizando métodos supervisados o no supervisados. La clasificación o agrupación temática es una dirección importante en esta área, especialmente el co-agrupamiento de documentos y términos, como LSA [9], PLSA [13], y enfoques basados en distancias y particionamiento de gráficos bipartitos [28] o máxima MI [2, 10], o máxima entropía [1, 18]. Los criterios de estos enfoques pueden ser utilizados en el tema de la segmentación de temas. Algunos de esos métodos se han extendido al área de la segmentación de temas, como PLSA [5] y máxima entropía [7], pero hasta donde sabemos, el uso de MI para la segmentación de temas no ha sido estudiado. 3. FORMULACIÓN DEL PROBLEMA Nuestro objetivo es segmentar documentos y alinear los segmentos entre documentos (Figura 1). Sea T el conjunto de términos {t1, t2, ..., tl}, que aparecen en el conjunto no etiquetado de documentos D = {d1, d2, ..., dm}. Sea Sd el conjunto de oraciones para el documento d ∈ D, es decir, {s1, s2, ..., snd}. Tenemos una matriz tridimensional de frecuencia de términos, en la que las tres dimensiones son variables aleatorias de D, Sd y T. Sd en realidad es un vector aleatorio que incluye una variable aleatoria para cada d ∈ D. La frecuencia de términos se puede utilizar para estimar la distribución de probabilidad conjunta P(D, Sd, T), que es p(t, d, s) = T(t, d, s)/ND, donde T(t, d, s) es el número de t en la oración s de d y ND es el número total de términos en D. ˆS representa el conjunto de segmentos {ˆs1, ˆs2, ..., ˆsp} después de la segmentación y alineación entre varios documentos, donde el número de segmentos | ˆS| = p. Un segmento ˆsi del documento d es una secuencia de oraciones adyacentes en d. Dado que para diferentes documentos si pueden discutir diferentes subtemas, nuestro objetivo es agrupar oraciones adyacentes en cada documento en segmentos, y alinear segmentos similares entre documentos, de modo que para diferentes documentos ˆsi trate sobre el mismo subtema. El objetivo es encontrar la segmentación óptima de temas y el mapeo de alineación Segd(si) : {s1, s2, ..., snd } → {ˆs1, ˆs2, ..., ˆsp} y Alid(ˆsi) : {ˆs1, ˆs2, ..., ˆsp} → {ˆs1, ˆs2, ..., ˆsp}, para todo d ∈ D, donde ˆsi es el i-ésimo segmento con la restricción de que solo las oraciones adyacentes pueden ser asignadas al mismo segmento, es decir, para d, {si, si+1, ..., sj} → {ˆsq}, donde q ∈ {1, ..., p}, donde p es el número de segmento, y si i > j, entonces para d, ˆsq está ausente. Después de la segmentación y alineación, el vector aleatorio Sd se convierte en una variable aleatoria alineada ˆS. Por lo tanto, P(D, Sd, T) se convierte en P(D, ˆS, T). El término co-clustering es una técnica que se ha empleado [10] para mejorar la precisión del agrupamiento de documentos. Evaluamos el efecto de ello para la segmentación de temas. Un término t se asigna a exactamente un grupo de términos. El término co-clustering implica encontrar simultáneamente el mapeo óptimo de agrupación de términos Clu(t) : {t1, t2, ..., tl} → {ˆt1, ˆt2, ..., ˆtk}, donde k ≤ l, l es el número total de palabras en todos los documentos, y k es el número de clústeres. 4. METODOLOGÍA Ahora describimos un algoritmo novedoso que puede manejar la segmentación de un solo documento, la detección de temas compartidos y la segmentación y alineación de múltiples documentos basados en MI o WMI. 4.1 Información Mutua MI I(X; Y) es una cantidad que mide la cantidad de información contenida en dos o más variables aleatorias [8, 10]. Para el caso de dos variables aleatorias, tenemos I(X; Y) = x∈X y∈Y p(x, y)log p(x, y) p(x)p(y) , (1) Obviamente, cuando las variables aleatorias X e Y son independientes, I(X; Y) = 0. Por lo tanto, de manera intuitiva, el valor de MI depende de cómo las variables aleatorias están relacionadas entre sí. La co-agrupación óptima es el mapeo Clux: X → ˆX y Cluy: Y → ˆY que minimiza la pérdida: I(X; Y) - I(ˆX; ˆY), lo cual es igual a maximizar I(ˆX; ˆY). Este es el criterio de MI para la agrupación. En el caso de la segmentación de temas, las dos variables aleatorias son la variable de término T y la variable de segmento S, y cada muestra es una ocurrencia de un término T = t en un segmento particular S = s. Se utiliza I(T; S) para medir qué tan dependientes son T y S. Sin embargo, no se puede calcular I(T; S) para documentos antes de la segmentación, ya que no tenemos un conjunto de S debido a que las oraciones del Documento d, si ∈ Sd, no están alineadas con otros documentos. Así, en lugar de minimizar la pérdida de MI, podemos maximizar MI después de la segmentación de temas, calculada como: I( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS p(ˆt, ˆs)log p(ˆt, ˆs) p(ˆt)p(ˆs) , (2) donde p(ˆt, ˆs) se estiman mediante la frecuencia de términos tf del Cluster de Términos ˆt y el Segmento ˆs en el conjunto de entrenamiento D. Nótese que aquí un segmento ˆs incluye oraciones sobre el mismo tema entre todos los documentos. La solución óptima es el mapeo Clut : T → ˆT, Segd : Sd → ˆS, y Alid : ˆS → ˆS, que maximiza I( ˆT; ˆS). 4.2 Información Mutua Ponderada En la segmentación de temas y alineación de múltiples documentos, si se conoce P(D, ˆS, T), basado en las distribuciones marginales P(D|T) y P( ˆS|T) para cada término t ∈ T, podemos categorizar los términos en cuatro tipos en el conjunto de datos: • Las palabras de parada comunes son comunes a lo largo de las dimensiones de documentos y segmentos. • Las palabras de parada dependientes del documento que dependen del estilo de escritura personal son comunes solo a lo largo de la dimensión de segmentos para algunos documentos. • Las palabras clave son los elementos más importantes para la segmentación. Son comunes a lo largo de la dimensión de los documentos solo para el mismo segmento, y no son comunes a lo largo de las dimensiones de los segmentos. • Las palabras ruidosas son otras palabras que no son comunes a lo largo de ambas dimensiones. La entropía basada en P(D|T) y P( ˆS|T) se puede utilizar para identificar diferentes tipos de términos. Para reforzar la contribución de las palabras clave en el cálculo de MI, y al mismo tiempo reducir el efecto de los otros tres tipos de palabras, similar a la idea del peso tf-idf [22], utilizamos las entropías de cada término a lo largo de las dimensiones del documento D y del segmento ˆS, es decir, ED(ˆt) y EˆS(ˆt), para calcular el peso. Una palabra de señal generalmente tiene un valor alto de ED(ˆt) pero un valor bajo de EˆS(ˆt). Introducimos los pesos de términos (o pesos de grupos de términos) wˆt = ( ED(ˆt) maxˆt ∈ ˆT (ED(ˆt )) )a (1 − EˆS(ˆt) maxˆt ∈ ˆT (EˆS(ˆt )) )b , (3) donde ED(ˆt) = d∈D p(d|ˆt)log|D| 1 p(d|ˆt) , EˆS(ˆt) = ˆs∈ ˆS p(ˆs|ˆt)log| ˆS| 1 p(ˆs|ˆt) , y a > 0 y b > 0 son potencias para ajustar los pesos de términos. Normalmente a = 1 y b = 1 por defecto, y se utilizan maxˆt ∈ ˆT (ED(ˆt )) y maxˆt ∈ ˆT (EˆS(ˆt )) para normalizar los valores de entropía. Los pesos de los grupos de términos se utilizan para ajustar p(ˆt, ˆs), pw(ˆt, ˆs) = wˆtp(ˆt, ˆs) ˆt∈ ˆT ;ˆs∈ ˆS wˆtp(ˆt, ˆs) , (4) e Iw( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS pw(ˆt, ˆs)log pw(ˆt, ˆs) pw(ˆt)pw(ˆs) , (5) donde pw(ˆt) y pw(ˆs) son distribuciones marginales de pw(ˆt, ˆs). Sin embargo, dado que no conocemos ni los pesos de los términos ni P(D, ˆS, T), necesitamos estimarlos, pero wˆt depende de p(ˆs|t) y ˆS, mientras que ˆS y p(ˆs|t) también dependen de wˆt, que aún es desconocido. Por lo tanto, se requiere un algoritmo iterativo para estimar los pesos de los términos wˆt y encontrar la mejor segmentación y alineación para optimizar la función objetivo Iw de manera concurrente. Después de que un documento se segmenta en oraciones, se introduce: la distribución de probabilidad conjunta P(D, Sd, T), el número de segmentos de texto p ∈ {2, 3, ..., máx(sd)}, el número de grupos de términos k ∈ {2, 3, ..., l} (si k = l, no se requiere co-clustering de términos) y el tipo de peso w ∈ {0, 1}, indicando usar I o Iw, respectivamente. Mapeo de Clu, Seg, Ali y pesos de términos wˆt. Inicialización: 0. i = 0. Inicializar Clu (0) t, Seg (0) d y Ali (0) d; Inicializar w (0) ˆt usando la Ecuación (6) si w = 1; Etapa 1: 1. Si |D| = 1, k = l y w = 0, verifica todas las segmentaciones secuenciales de d en p segmentos y encuentra la mejor Segd(s) = argmaxˆsI( ˆT; ˆS), y devuelve Segd; de lo contrario, si w = 1 y k = l, ve a 3.1; Etapa 2: 2.1 Si k < l, para cada término t, encuentra el mejor clúster ˆt como Clu(i+1)(t) = argmaxˆtI( ˆT; ˆS(i)) basado en Seg(i) y Ali(i); 2.2 Para cada d, verifica todas las segmentaciones secuenciales de d en p segmentos con mapeo s → ˆs → ˆs, y encuentra la mejor Ali (i+1) d (Seg (i+1) d (s)) = argmaxˆsI( ˆT(i+1); ˆS) basado en Clu(i+1)(t) si k < l o Clu(0)(t) si k = l; 2.3 i + +. Si Clu, Seg o Ali cambian, ve a 2.1; de lo contrario, si w = 0, devuelve Clu(i), Seg(i) y Ali(i); de lo contrario, j = 0, ve a 3.1; Etapa 3: 3.1 Actualiza w (i+j+1) ˆt basado en Seg(i+j), Ali(i+j) y Clu(i) usando la Ecuación (3); 3.2 Para cada d, verifica todas las segmentaciones secuenciales de d en p segmentos con mapeo s → ˆs → ˆs, y encuentra la mejor Ali (i+j+1) d (Seg (i+j+1) d (s)) = argmaxˆsIw( ˆT(i); ˆS) basado en Clu(i) y w (i+j+1) ˆt; 3.3 j + +. Si Iw( ˆT; ˆS) cambia, ve al paso 6; de lo contrario, detente y devuelve Clu(i), Seg(i+j), Ali(i+j), y w (i+j) ˆt; Figura 2: Algoritmo: Segmentación y alineación de temas basada en MI o WMI. y cada oración se segmenta en palabras, cada palabra se reduce a su raíz. Entonces se puede estimar la distribución de probabilidad conjunta P(D, Sd, T). Finalmente, esta distribución se puede utilizar para calcular la MI en nuestro algoritmo. 4.3 Algoritmo Voraz Iterativo Nuestro objetivo es maximizar la función objetivo, I( ˆT; ˆS) o Iw( ˆT; ˆS), que puede medir la dependencia de las ocurrencias de términos en diferentes segmentos. Generalmente, primero no conocemos los pesos estimados de los términos, los cuales dependen de la segmentación y alineación óptimas de los temas, y los grupos de términos. Además, este problema es NP-duro [10], incluso si conocemos los pesos de los términos. Por lo tanto, se desea un algoritmo voraz iterativo para encontrar la mejor solución, aunque probablemente solo se alcancen máximos locales. Presentamos el algoritmo voraz iterativo en la Figura 2 para encontrar un máximo local de I( ˆT; ˆS) o Iw( ˆT; ˆS) con estimación simultánea de pesos de términos. Este algoritmo es iterativo y codicioso para casos de múltiples documentos o casos de un solo documento con estimación de peso de términos y/o co-clustering de términos. De lo contrario, dado que es solo un algoritmo de un paso para resolver la tarea de <br>segmentación de un solo documento</br> [6, 15, 25], se garantiza el máximo global de MI. Mostraremos más adelante que la coagrupación de términos reduce la precisión de los resultados y no es necesaria, y para la segmentación de un solo documento, los pesos de los términos tampoco son requeridos. 4.3.1 Inicialización En el Paso 0, la agrupación inicial de términos Clut y la segmentación y alineación de temas Segd y Alid son importantes para evitar máximos locales y reducir el número de iteraciones. Primero, se puede hacer una buena estimación de los pesos de los términos utilizando las distribuciones de frecuencia de términos a lo largo de las oraciones para cada documento y promediándolas para obtener los valores iniciales de wˆt: wt = ( ED(t) maxt ∈T (ED(t )) )(1 − ES(t) maxt ∈T (ES(t )) ), (6) donde ES(t) = 1 |Dt| d∈Dt (1 − s∈Sd p(s|t)log|Sd| 1 p(s|t) ), donde Dt es el conjunto de documentos que contienen el término t. Luego, para la segmentación inicial Seg(0), simplemente podemos segmentar los documentos de manera equitativa por oraciones. O podemos encontrar la segmentación óptima solo para cada documento d que maximice el WMI, Seg (0) d = argmaxˆsIw(T; ˆS), donde w = w (0) ˆt. Para el alineamiento inicial Ali(0), podemos primero asumir que el orden de los segmentos para cada d es el mismo. Para el agrupamiento inicial de términos Clu(0), las etiquetas de los primeros grupos pueden establecerse al azar, y después de la primera vez del Paso 3, se obtiene un buen agrupamiento inicial de términos. 4.3.2 Diferentes Casos Después de la inicialización, hay tres etapas para diferentes casos. En total hay ocho casos, |D| = 1 o |D| > 1, k = l o k < l, w = 0 o w = 1. La segmentación de un solo documento sin agrupamiento de términos y estimación de peso de términos (|D| = 1, k = l, w = 0) solo requiere la Etapa 1 (Paso 1). Si se requiere el agrupamiento de términos (k < l), la Etapa 2 (Paso 2.1, 2.2 y 2.3) se ejecuta de forma iterativa. Si se requiere la estimación del peso del término (w = 1), la Etapa 3 (Paso 3.1, 3.2 y 3.3) se ejecuta de forma iterativa. Si ambos son requeridos (k < l, w = 1), la Etapa 2 y 3 se ejecutan una tras otra. Para la segmentación de múltiples documentos sin agrupamiento de términos y estimación de peso de términos (|D| > 1, k = l, w = 0), solo se requiere la iteración de los Pasos 2.2 y 2.3. En la Etapa 1, el máximo global se puede encontrar basándose en I( ˆT; ˆS) utilizando programación dinámica en la Sección 4.4. Es imposible encontrar simultáneamente un buen agrupamiento de términos y pesos estimados de términos, ya que al mover un término a un nuevo grupo de términos para maximizar Iw( ˆT; ˆS), no sabemos si el peso de este término debería ser el del nuevo grupo o el del antiguo. Por lo tanto, primero realizamos el agrupamiento de términos en la Etapa 2, y luego estimamos los pesos de los términos en la Etapa 3. En la Etapa 2, el Paso 2.1 consiste en encontrar el mejor agrupamiento de términos y el Paso 2.2 consiste en encontrar la mejor segmentación. Este ciclo se repite para encontrar un máximo local basado en MI I hasta que converge. Los dos pasos son: (1) basados en el agrupamiento de términos actual Cluˆt, para cada documento d, el algoritmo segmenta todas las oraciones Sd en p segmentos secuencialmente (algunos segmentos pueden estar vacíos), y los coloca en los p segmentos ˆS del conjunto de entrenamiento completo D (se verifican todos los casos posibles de segmentación diferente Segd y alineación Alid) para encontrar el caso óptimo, y (2) basado en la segmentación y alineación actual, para cada término t, el algoritmo encuentra el mejor grupo de términos de t basado en la segmentación actual Segd y la alineación Alid. Después de encontrar un buen agrupamiento de términos, se estiman los pesos de los términos si w = 1. En la Etapa 3, al igual que en la Etapa 2, el Paso 3.1 es la reestimación del peso del término y el Paso 3.2 es encontrar una mejor segmentación. Se repiten para encontrar un máximo local basado en WMI Iw hasta que converja. Sin embargo, si el agrupamiento en la Etapa 2 no es preciso, entonces la estimación de peso en la Etapa 3 puede tener un mal resultado. Finalmente, en el Paso 3.3, este algoritmo converge y devuelve la salida. Este algoritmo puede manejar tanto la segmentación de un solo documento como la de múltiples documentos. También puede detectar temas compartidos entre documentos al verificar la proporción de frases superpuestas sobre los mismos temas, como se describe en la Sección 5.2. 4.4 Optimización del algoritmo. En muchos trabajos anteriores sobre segmentación, la programación dinámica es una técnica utilizada para maximizar la función objetivo. De manera similar, en los Pasos 1, 2.2 y 3.2 de nuestro algoritmo, podemos utilizar programación dinámica. Para la Etapa 1, utilizando programación dinámica aún se puede encontrar el óptimo global, pero para la Etapa 2 y la Etapa 3, solo podemos encontrar el óptimo para cada paso de segmentación de temas y alineación de un documento. Aquí solo mostramos la programación dinámica para el Paso 3.2 utilizando WMI (el Paso 1 y 2.2 son similares pero pueden usar I o Iw). Hay dos casos que no se muestran en el algoritmo de la Figura 2: (a) <br>segmentación de un solo documento</br> o segmentación de múltiples documentos con el mismo orden secuencial de segmentos, donde no se requiere alineación, y (b) segmentación de múltiples documentos con diferentes órdenes secuenciales de segmentos, donde la alineación es necesaria. ",
            "candidates": [],
            "error": [
                [
                    "segmentación de un solo documento",
                    "segmentación de un solo documento",
                    "segmentación de documentos individuales",
                    "segmentación de un solo documento",
                    "segmentación de un solo documento"
                ]
            ]
        },
        "multi-document segmentation": {
            "translated_key": "segmentación de múltiples documentos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Topic Segmentation with Shared Topic Detection and Alignment of Multiple Documents Bingjun Sun*, Prasenjit Mitra*† , Hongyuan Zha‡ , C. Lee Giles*† , John Yen*† *Department of Computer Science and Engineering † College of Information Sciences and Technology The Pennsylvania State University University Park, PA 16802 ‡ College of Computing The Georgia Institute of Technology Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu ABSTRACT Topic detection and tracking [26] and topic segmentation [15] play an important role in capturing the local and sequential information of documents.",
                "Previous work in this area usually focuses on single documents, although similar multiple documents are available in many domains.",
                "In this paper, we introduce a novel unsupervised method for shared topic detection and topic segmentation of multiple similar documents based on mutual information (MI) and weighted mutual information (WMI) that is a combination of MI and term weights.",
                "The basic idea is that the optimal segmentation maximizes MI(or WMI).",
                "Our approach can detect shared topics among documents.",
                "It can find the optimal boundaries in a document, and align segments among documents at the same time.",
                "It also can handle single-document segmentation as a special case of the <br>multi-document segmentation</br> and alignment.",
                "Our methods can identify and strengthen cue terms that can be used for segmentation and partially remove stop words by using term weights based on entropy learned from multiple documents.",
                "Our experimental results show that our algorithm works well for the tasks of single-document segmentation, shared topic detection, and <br>multi-document segmentation</br>.",
                "Utilizing information from multiple documents can tremendously improve the performance of topic segmentation, and using WMI is even better than using MI for the <br>multi-document segmentation</br>.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Clustering; H.3.1 [Information Storage and Retrieval]: Content Analysis and IndexingLinguistic processing; I.2.7 [Artificial Intelligence]: Natural Language Processing-Text analysis; I.5.3 [Pattern Recognition]: Clustering-Algorithms;Similarity measures General Terms Algorithms, Design, Experimentation 1.",
                "INTRODUCTION Many researchers have worked on topic detection and tracking (TDT) [26] and topic segmentation during the past decade.",
                "Topic segmentation intends to identify the boundaries in a document with the goal to capture the latent topical structure.",
                "Topic segmentation tasks usually fall into two categories [15]: text stream segmentation where topic transition is identified, and coherent document segmentation in which documents are split into sub-topics.",
                "The former category has applications in automatic speech recognition, while the latter one has more applications such as partial-text query of long documents in information retrieval, text summary, and quality measurement of multiple documents.",
                "Previous research in connection with TDT falls into the former category, targeted on topic tracking of broadcast speech data and newswire text, while the latter category has not been studied very well.",
                "Traditional approaches perform topic segmentation on documents one at a time [15, 25, 6].",
                "Most of them perform badly in subtle tasks like coherent document segmentation [15].",
                "Often, end-users seek documents that have the similar content.",
                "Search engines, like, Google, provide links to obtain similar pages.",
                "At a finer granularity, users may actually be looking to obtain sections of a document similar to a particular section that presumably discusses a topic of the users interest.",
                "Thus, the extension of topic segmentation from single documents to identifying similar segments from multiple similar documents with the same topic is a natural and necessary direction, and multi-document topic segmentation is expected to have a better performance since more information is utilized.",
                "Traditional approaches using similarity measurement based on term frequency generally have the same assumption that similar vocabulary tends to be in a coherent topic segment [15, 25, 6].",
                "However, they usually suffer the issue of identifying stop words.",
                "For example, additional document-dependent stop words are removed together with the generic stop words in [15].",
                "There are two reasons that we do not remove stop words directly.",
                "First, identifying stop words is another issue [12] that requires estimation in each domain.",
                "Removing common stop words may result in the loss of useful information in a specific domain.",
                "Second, even though stop words can be identified, hard classification of stop words and nonstop words cannot represent the gradually changing amount of information content of each word.",
                "We employ a soft classification using term weights.",
                "In this paper, we view the problem of topic segmentation as an optimization issue using information theoretic techniques to find the optimal boundaries of a document given the number of text segments so as to minimize the loss of mutual information (MI) (or a weighted mutual information (WMI)) after segmentation and alignment.",
                "This is equal to maximizing the MI (or WMI).",
                "The MI focuses on measuring the difference among segments whereas previous research focused on finding the similarity (e.g. cosine distance) of segments [15, 25, 6].",
                "Topic alignment of multiple similar documents can be achieved by clustering sentences on the same topic into the same cluster.",
                "Single-document topic segmentation is just a special case of the multi-document topic segmentation and alignment problem.",
                "Terms can be co-clustered as in [10] at the same time, given the number of clusters, but our experimental results show that this method results in a worse segmentation (see Tables 1, 4, and 6).",
                "Usually, human readers can identify topic transition based on cue words, and can ignore stop words.",
                "Inspired by this, we give each term (or term cluster) a weight based on entropy among different documents and different segments of documents.",
                "Not only can this approach increase the contribution of cue words, but it can also decrease the effect of common stop words, noisy word, and document-dependent stop words.",
                "These words are common in a document.",
                "Many methods based on sentence similarity require that these words are removed before topic segmentation can be performed [15].",
                "Our results in Figure 3 show that term weights are useful for multi-document topic segmentation and alignment.",
                "The major contribution of this paper is that it introduces a novel method for topic segmentation using MI and shows that this method performs better than previously used criteria.",
                "Also, we have addressed the problem of topic segmentation and alignment across multiple documents, whereas most existing research focused on segmentation of single documents.",
                "<br>multi-document segmentation</br> and alignment can utilize information from similar documents and improves the performance of topic segmentation greatly.",
                "Obviously, our approach can handle single documents as a special case when multiple documents are unavailable.",
                "It can detect shared topics among documents to judge if they are multiple documents on the same topic.",
                "We also introduce the new criterion of WMI based on term weights learned from multiple similar documents, which can improve performance of topic segmentation further.",
                "We propose an iterative greedy algorithm based on dynamic programming and show that it works well in practice.",
                "Some of our prior work is in [24].",
                "The rest of this paper is organized as follows: In Section 2, we review related work.",
                "Section 3 contains a formulation of the problem of topic segmentation and alignment of multiple documents with term co-clustering, a review of the criterion of MI for clustering, and finally an introduction to WMI.",
                "In Section 4, we first propose the iterative greedy algorithm of topic segmentation and alignment with term co-clustering, and then describe how the algorithm can be optimized by usFigure 1: Illustration of <br>multi-document segmentation</br> and alignment. ing dynamic programming.",
                "In Section 5, experiments about single-document segmentation, shared topic detection, and <br>multi-document segmentation</br> are described, and results are presented and discussed to evaluate the performance of our algorithm.",
                "Conclusions and some future directions of the research work are discussed in Section 6. 2.",
                "PREVIOUS WORK Generally, the existing approaches to text segmentation fall into two categories: supervised learning [19, 17, 23] and unsupervised learning [3, 27, 5, 6, 15, 25, 21].",
                "Supervised learning usually has good performance, since it learns functions from labelled training sets.",
                "However, often getting large training sets with manual labels on document sentences is prohibitively expensive, so unsupervised approaches are desired.",
                "Some models consider dependence between sentences and sections, such as Hidden Markov Model [3, 27], Maximum Entropy Markov Model [19], and Conditional Random Fields [17], while many other approaches are based on lexical cohesion or similarity of sentences [5, 6, 15, 25, 21].",
                "Some approaches also focus on cue words as hints of topic transitions [11].",
                "While some existing methods only consider information in single documents [6, 15], others utilize multiple documents [16, 14].",
                "There are not many works in the latter category, even though the performance of segmentation is expected to be better with utilization of information from multiple documents.",
                "Previous research studied methods to find shared topics [16] and topic segmentation and summarization between just a pair of documents [14].",
                "Text classification and clustering is a related research area which categorizes documents into groups using supervised or unsupervised methods.",
                "Topical classification or clustering is an important direction in this area, especially co-clustering of documents and terms, such as LSA [9], PLSA [13], and approaches based on distances and bipartite graph partitioning [28] or maximum MI [2, 10], or maximum entropy [1, 18].",
                "Criteria of these approaches can be utilized in the issue of topic segmentation.",
                "Some of those methods have been extended into the area of topic segmentation, such as PLSA [5] and maximum entropy [7], but to our best knowledge, using MI for topic segmentation has not been studied. 3.",
                "PROBLEM FORMULATION Our goal is to segment documents and align the segments across documents (Figure 1).",
                "Let T be the set of terms {t1, t2, ..., tl}, which appear in the unlabelled set of documents D = {d1, d2, ..., dm}.",
                "Let Sd be the set of sentences for document d ∈ D, i.e. {s1, s2, ..., snd }.",
                "We have a 3D matrix of term frequency, in which the three dimensions are random variables of D, Sd, and T. Sd actually is a random vector including a random variable for each d ∈ D. The term frequency can be used to estimate the joint probability distribution P(D, Sd, T), which is p(t, d, s) = T(t, d, s)/ND, where T(t, d, s) is the number of t in ds sentence s and ND is the total number of terms in D. ˆS represents the set of segments {ˆs1, ˆs2, ..., ˆsp} after segmentation and alignment among multiple documents, where the number of segments | ˆS| = p. A segment ˆsi of document d is a sequence of adjacent sentences in d. Since for different documents si may discuss different sub-topics, our goal is to cluster adjacent sentences in each document into segments, and align similar segments among documents, so that for different documents ˆsi is about the same sub-topic.",
                "The goal is to find the optimal topic segmentation and alignment mapping Segd(si) : {s1, s2, ..., snd } → {ˆs1, ˆs2, ..., ˆsp} and Alid(ˆsi) : {ˆs1, ˆs2, ..., ˆsp} → {ˆs1, ˆs2, ..., ˆsp}, for all d ∈ D, where ˆsi is ith segment with the constraint that only adjacent sentences can be mapped to the same segment, i.e. for d, {si, si+1, ..., sj} → {ˆsq}, where q ∈ {1, ..., p}, where p is the segment number, and if i > j, then for d, ˆsq is missing.",
                "After segmentation and alignment, random vector Sd becomes an aligned random variable ˆS.",
                "Thus, P(D, Sd, T) becomes P(D, ˆS, T).",
                "Term co-clustering is a technique that has been employed [10] to improve the accuracy of document clustering.",
                "We evaluate the effect of it for topic segmentation.",
                "A term t is mapped to exactly one term cluster.",
                "Term co-clustering involves simultaneously finding the optimal term clustering mapping Clu(t) : {t1, t2, ..., tl} → {ˆt1, ˆt2, ..., ˆtk}, where k ≤ l, l is the total number of words in all the documents, and k is the number of clusters. 4.",
                "METHODOLOGY We now describe a novel algorithm which can handle singledocument segmentation, shared topic detection, and multidocument segmentation and alignment based on MI or WMI. 4.1 Mutual Information MI I(X; Y ) is a quantity to measure the amount of information which is contained in two or more random variables [8, 10].",
                "For the case of two random variables, we have I(X; Y ) = x∈X y∈Y p(x, y)log p(x, y) p(x)p(y) , (1) Obviously, when random variables X and Y are independent, I(X; Y ) = 0.",
                "Thus, intuitively, the value of MI depends on how random variables are dependent on each other.",
                "The optimal co-clustering is the mapping Clux : X → ˆX and Cluy : Y → ˆY that minimizes the loss: I(X; Y ) − I( ˆX; ˆY ), which is equal to maximizing I( ˆX; ˆY ).",
                "This is the criterion of MI for clustering.",
                "In the case of topic segmentation, the two random variables are the term variable T and the segment variable S, and each sample is an occurrence of a term T = t in a particular segment S = s. I(T; S) is used to measure how dependent T and S are.",
                "However, I(T; S) cannot be computed for documents before segmentation, since we do not have a set of S due to the fact that sentences of Document d, si ∈ Sd, is not aligned with other documents.",
                "Thus, instead of minimizing the loss of MI, we can maximize MI after topic segmentation, computed as: I( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS p(ˆt, ˆs)log p(ˆt, ˆs) p(ˆt)p(ˆs) , (2) where p(ˆt, ˆs) are estimated by the term frequency tf of Term Cluster ˆt and Segment ˆs in the training set D. Note that here a segment ˆs includes sentences about the the same topic among all documents.",
                "The optimal solution is the mapping Clut : T → ˆT, Segd : Sd → ˆS , and Alid : ˆS → ˆS, which maximizes I( ˆT; ˆS). 4.2 Weighted Mutual Information In topic segmentation and alignment of multiple documents, if P(D, ˆS, T) is known, based on the marginal distributions P(D|T) and P( ˆS|T) for each term t ∈ T, we can categorize terms into four types in the data set: • Common stop words are common both along the dimensions of documents and segments. • Document-dependent stop words that depends on the personal writing style are common only along the dimension of segments for some documents. • Cue words are the most important elements for segmentation.",
                "They are common along the dimension of documents only for the same segment, and they are not common along the dimensions of segments. • Noisy words are other words which are not common along both dimensions.",
                "Entropy based on P(D|T) and P( ˆS|T) can be used to identify different types of terms.",
                "To reinforce the contribution of cue words in the MI computation, and simultaneously reduce the effect of the other three types of words, similar as the idea of the tf-idf weight [22], we use entropies of each term along the dimensions of document D and segment ˆS, i.e.",
                "ED(ˆt) and EˆS(ˆt), to compute the weight.",
                "A cue word usually has a large value of ED(ˆt) but a small value of EˆS(ˆt).",
                "We introduce term weights (or term cluster weights) wˆt = ( ED(ˆt) maxˆt ∈ ˆT (ED(ˆt )) )a (1 − EˆS(ˆt) maxˆt ∈ ˆT (EˆS(ˆt )) )b , (3) where ED(ˆt) = d∈D p(d|ˆt)log|D| 1 p(d|ˆt) , EˆS(ˆt) = ˆs∈ ˆS p(ˆs|ˆt)log| ˆS| 1 p(ˆs|ˆt) , and a > 0 and b > 0 are powers to adjust term weights.",
                "Usually a = 1 and b = 1 as default, and maxˆt ∈ ˆT (ED(ˆt )) and maxˆt ∈ ˆT (EˆS(ˆt )) are used to normalize the entropy values.",
                "Term cluster weights are used to adjust p(ˆt, ˆs), pw(ˆt, ˆs) = wˆtp(ˆt, ˆs) ˆt∈ ˆT ;ˆs∈ ˆS wˆtp(ˆt, ˆs) , (4) and Iw( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS pw(ˆt, ˆs)log pw(ˆt, ˆs) pw(ˆt)pw(ˆs) , (5) where pw(ˆt) and pw(ˆs) are marginal distributions of pw(ˆt, ˆs).",
                "However, since we do not know either the term weights or P(D, ˆS, T), we need to estimate them, but wˆt depends on p(ˆs|t) and ˆS, while ˆS and p(ˆs|t) also depend on wˆt that is still unknown.",
                "Thus, an iterative algorithm is required to estimate term weights wˆt and find the best segmentation and alignment to optimize the objective function Iw concurrently.",
                "After a document is segmented into sentences Input: Joint probability distribution P(D, Sd, T), number of text segments p ∈ {2, 3, ..., max(sd)}, number of term clusters k ∈ {2, 3, ..., l} (if k = l, no term co-clustering required), and weight type w ∈ {0, 1}, indicating to use I or Iw, respectively.",
                "Output: Mapping Clu, Seg, Ali, and term weights wˆt.",
                "Initialization: 0. i = 0.",
                "Initialize Clu (0) t , Seg (0) d , and Ali (0) d ; Initialize w (0) ˆt using Equation (6) if w = 1; Stage 1: 1.",
                "If |D| = 1, k = l, and w = 0, check all sequential segmentations of d into p segments and find the best one Segd(s) = argmaxˆsI( ˆT; ˆS), and return Segd; otherwise, if w = 1 and k = l, go to 3.1; Stage 2: 2.1 If k < l, for each term t, find the best cluster ˆt as Clu(i+1)(t) = argmaxˆtI( ˆT; ˆS(i)) based on Seg(i) and Ali(i); 2.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+1) d (Seg (i+1) d (s)) = argmaxˆsI( ˆT(i+1); ˆS) based on Clu(i+1)(t) if k < l or Clu(0)(t) if k = l; 2.3 i + +.",
                "If Clu, Seg, or Ali changed, go to 2.1; otherwise, if w = 0, return Clu(i), Seg(i), and Ali(i); else j = 0, go to 3.1; Stage 3: 3.1 Update w (i+j+1) ˆt based on Seg(i+j), Ali(i+j), and Clu(i) using Equation (3); 3.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+j+1) d (Seg (i+j+1) d (s)) = argmaxˆsIw( ˆT(i); ˆS) based on Clu(i) and w (i+j+1) ˆt ; 3.3 j + +.",
                "If Iw( ˆT; ˆS) changes, go to step 6; otherwise, stop and return Clu(i), Seg(i+j), Ali(i+j), and w (i+j) ˆt ; Figure 2: Algorithm: Topic segmentation and alignment based on MI or WMI. and each sentence is segmented into words, each word is stemmed.",
                "Then the joint probability distribution P(D, Sd, T) can be estimated.",
                "Finally, this distribution can be used to compute MI in our algorithm. 4.3 Iterative Greedy Algorithm Our goal is to maximize the objective function, I( ˆT; ˆS) or Iw( ˆT; ˆS), which can measure the dependence of term occurrences in different segments.",
                "Generally, first we do not know the estimate term weights, which depend on the optimal topic segmentation and alignment, and term clusters.",
                "Moreover, this problem is NP-hard [10], even though if we know the term weights.",
                "Thus, an iterative greedy algorithm is desired to find the best solution, even though probably only local maxima are reached.",
                "We present the iterative greedy algorithm in Figure 2 to find a local maximum of I( ˆT; ˆS) or Iw( ˆT; ˆS) with simultaneous term weight estimation.",
                "This algorithm can is iterative and greedy for multi-document cases or single-document cases with term weight estimation and/or term co-clustering.",
                "Otherwise, since it is just a one step algorithm to solve the task of single-document segmentation [6, 15, 25], the global maximum of MI is guaranteed.",
                "We will show later that term co-clustering reduces the accuracy of the results and is not necessary, and for singledocument segmentation, term weights are also not required. 4.3.1 Initialization In Step 0, the initial term clustering Clut and topic segmentation and alignment Segd and Alid are important to avoid local maxima and reduce the number of iterations.",
                "First, a good guess of term weights can be made by using the distributions of term frequency along sentences for each document and averaging them to get the initial values of wˆt: wt = ( ED(t) maxt ∈T (ED(t )) )(1 − ES(t) maxt ∈T (ES(t )) ), (6) where ES(t) = 1 |Dt| d∈Dt (1 − s∈Sd p(s|t)log|Sd| 1 p(s|t) ), where Dt is the set of documents which contain Term t. Then, for the initial segmentation Seg(0) , we can simply segment documents equally by sentences.",
                "Or we can find the optimal segmentation just for each document d which maximizes the WMI, Seg (0) d = argmaxˆsIw(T; ˆS), where w = w (0) ˆt .",
                "For the initial alignment Ali(0) , we can first assume that the order of segments for each d is the same.",
                "For the initial term clustering Clu(0) , first cluster labels can be set randomly, and after the first time of Step 3, a good initial term clustering is obtained. 4.3.2 Different Cases After initialization, there are three stages for different cases.",
                "Totally there are eight cases, |D| = 1 or |D| > 1, k = l or k < l, w = 0 or w = 1.",
                "Single document segmentation without term clustering and term weight estimation (|D| = 1, k = l, w = 0) only requires Stage 1 (Step 1).",
                "If term clustering is required (k < l), Stage 2 (Step 2.1, 2.2, and 2.3) is executed iteratively.",
                "If term weight estimation is required (w = 1), Stage 3 (Step 3.1, 3.2, and 3.3) is executed iteratively.",
                "If both are required (k < l, w = 1), Stage 2 and 3 run one after the other.",
                "For <br>multi-document segmentation</br> without term clustering and term weight estimation (|D| > 1, k = l, w = 0), only iteration of Step 2.2 and 2.3 are required.",
                "At Stage 1, the global maximum can be found based on I( ˆT; ˆS) using dynamic programming in Section 4.4.",
                "Simultaneously finding a good term clustering and estimated term weights is impossible, since when moving a term to a new term cluster to maximize Iw( ˆT; ˆS), we do not know that the weight of this term should be the one of the new cluster or the old cluster.",
                "Thus, we first do term clustering at Stage 2, and then estimate term weights at Stage 3.",
                "At Stage 2, Step 2.1 is to find the best term clustering and Step 2.2 is to find the best segmentation.",
                "This cycle is repeated to find a local maximum based on MI I until it converges.",
                "The two steps are: (1) based on current term clustering Cluˆt, for each document d, the algorithm segments all the sentences Sd into p segments sequentially (some segments may be empty), and put them into the p segments ˆS of the whole training set D (all possible cases of different segmentation Segd and alignment Alid are checked) to find the optimal case, and (2) based on the current segmentation and alignment, for each term t, the algorithm finds the best term cluster of t based on the current segmentation Segd and alignment Alid.",
                "After finding a good term clustering, term weights are estimated if w = 1.",
                "At Stage 3, similar as Stage 2, Step 3.1 is term weight re-estimation and Step 3.2 is to find a better segmentation.",
                "They are repeated to find a local maximum based on WMI Iw until it converges.",
                "However, if the term clustering in Stage 2 is not accurate, then the term weight estimation at Stage 3 may have a bad result.",
                "Finally, at Step 3.3, this algorithm converges and return the output.",
                "This algorithm can handle both single-document and <br>multi-document segmentation</br>.",
                "It also can detect shared topics among documents by checking the proportion of overlapped sentences on the same topics, as described in Sec 5.2. 4.4 Algorithm Optimization In many previous works on segmentation, dynamic programming is a technique used to maximize the objective function.",
                "Similarly, at Step 1, 2.2, and 3.2 of our algorithm, we can use dynamic programming.",
                "For Stage 1, using dynamic programming can still find the global optimum, but for Stage 2 and Stage 3, we can only find the optimum for each step of topic segmentation and alignment of a document.",
                "Here we only show the dynamic programming for Step 3.2 using WMI (Step 1 and 2.2 are similar but they can use either I or Iw).",
                "There are two cases that are not shown in the algorithm in Figure 2: (a) single-document segmentation or <br>multi-document segmentation</br> with the same sequential order of segments, where alignment is not required, and (b) <br>multi-document segmentation</br> with different sequential orders of segments, where alignment is necessary.",
                "The alignment mapping function of the former case is simply just Alid(ˆsi) = ˆsi, while for the latter ones alignment mapping function Alid(ˆsi) = ˆsj, i and j may be different.",
                "The computational steps for the two cases are listed below: Case 1 (no alignment): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs) and partial pw(ˆs) without counting sentences from d. Then put sentences from i to j into Part k, and compute partial WMI PIw( ˆT; ˆsk(si, si+1, ..., sj)) ˆt∈ ˆT pw(ˆt, ˆsk)log pw(ˆt, ˆsk) pw(ˆt)pw(ˆsk) , where Alid(si, si+1, ..., sj) = k, k ∈ {1, 2, ..., p}, 1 ≤ i ≤ j ≤ nd, and Segd(sq) = ˆsk for all i ≤ q ≤ j. (2) Let M(sm, 1) = PIw( ˆT; ˆs1(s1, s2, ..., sm)).",
                "Then M(sm, L) = maxi[M(si−1, L − 1) + PIw( ˆT; ˆsL(si, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, and when i > m, no sentences are put into ˆsk when compute PIw (note PIw( ˆT; ˆs(si, ..., sm)) = 0 for single-document segmentation). (3) Finally M(snd , p) = maxi[M(si−1, p − 1)+ PIw( ˆT; ˆsp(si, ..., snd ))], where 1 ≤ i ≤ nd+1.",
                "The optimal Iw is found and the corresponding segmentation is the best.",
                "Case 2 (alignment required): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs), and partial pw(ˆs), and PIw( ˆT; ˆsk(si, si+1, ..., sj)) similarly as Case 1. (2) Let M(sm, 1, k) = PIw( ˆT; ˆsk(s1, s2, ..., sm)), where k ∈ {1, 2, ..., p}.",
                "Then M(sm, L, kL) = maxi,j[M(si−1, L − 1, kL/j) + PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, kL ∈ Set(p, L), which is the set of all p!",
                "L!(p−L)! combinations of L segments chosen from all p segments, j ∈ kL, the set of L segments chosen from all p segments, and kL/j is the combination of L − 1 segments in kL except Segment j. (3) Finally, M(snd , p, kp) = maxi,j[M(si−1, p − 1, kp/j) +PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., snd ))], where kp is just the combination of all p segments and 1 ≤ i ≤ nd + 1, which is the optimal Iw and the corresponding segmentation is the best.",
                "The steps of Case 1 and 2 are similar, except in Case 2, alignment is considered in addition to segmentation.",
                "First, basic items of probability for computing Iw are computed excluding Doc d, and then partial WMI by putting every possible sequential segment (including empty segment) of d into every segment of the set.",
                "Second, the optimal sum of PIw for L segments and the leftmost m sentences, M(sm, L), is found.",
                "Finally, the maximal WMI is found among different sums of M(sm, p − 1) and PIw for Segment p. 5.",
                "EXPERIMENTS In this section, single-document segmentation, shared topic detection, and <br>multi-document segmentation</br> will be tested.",
                "Different hyper parameters of our method are studied.",
                "For convenience, we refer to the method using I as MIk if w = 0, and Iw as WMIk if w = 2 or as WMIk if w = 1, where k is the number of term clusters, and if k = l, where l is the total number of terms, then no term clustering is required, i.e.",
                "MIl and WMIl. 5.1 Single-document Segmentation 5.1.1 Test Data and Evaluation The first data set we tested is a synthetic one used in previous research [6, 15, 25] and many other papers.",
                "It has 700 samples.",
                "Each is a concatenation of ten segments.",
                "Each segment is the first n sentence selected randomly from the Brown corpus, which is supposed to have a different topic from each other.",
                "Currently, the best results on this data set is achieved by Ji et.al. [15].",
                "To compare the performance of our methods, the criterion used widely in previous research is applied, instead of the unbiased criterion introduced in [20].",
                "It chooses a pair of words randomly.",
                "If they are in different segments (different) for the real segmentation (real), but predicted (pred) as in the same segment, it is a miss.",
                "If they are in the same segment (same), but predicted as in different segments, it is a false alarm.",
                "Thus, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, diff)p(diff|real) +p(false alarm|real, pred, same)p(same|real). 5.1.2 Experiment Results We tested the case when the number of segments is known.",
                "Table 1 shows the results of our methods with different hyper parameter values and three previous approaches, C99[25], U00[6], and ADDP03[15], on this data set when the segment number is known.",
                "In WMI for single-document segmentation, the term weights are computed as follows: wˆt = 1−EˆS(ˆt)/maxˆt ∈ ˆT (EˆS(ˆt )).",
                "For this case, our methods MIl and WMIl both outperform all the previous approaches.",
                "We compared our methods with ADDP03using one-sample one-sided t-test and p-values are shown in Table 2.",
                "From the p-values, we can see that mostly the differences are very Table 1: Average Error Rates of Single-document Segmentation Given Segment Numbers Known Range of n 3-11 3-5 6-8 9-11 Sample size 400 100 100 100 C99 12% 11% 10% 9% U00 10% 9% 7% 5% ADDP03 6.0% 6.8% 5.2% 4.3% MIl 4.68% 5.57% 2.59% 1.59% WMIl 4.94% 6.33% 2.76% 1.62% MI100 9.62% 12.92% 8.66% 6.67% Table 2: Single-document Segmentation: P-values of T-test on Error Rates Range of n 3-11 3-5 6-8 9-11 ADDP03, MIl 0.000 0.000 0.000 0.000 ADDP03, WMIl 0.000 0.099 0.000 0.000 MIl, WMIl 0.061 0.132 0.526 0.898 significant.",
                "We also compare the error rates between our two methods using two-sample two-sided t-test to check the hypothesis that they are equal.",
                "We cannot reject the hypothesis that they are equal, so the difference are not significant, even though all the error rates for MIl are smaller than WMIl.",
                "However, we can conclude that term weights contribute little in single-document segmentation.",
                "The results also show that MI using term co-clustering (k = 100) decreases the performance.",
                "We tested different number of term clusters, and found that the performance becomes better when the cluster number increases to reach l. WMIk<l has similar results that we did not show in the table.",
                "As mentioned before, using MI may be inconsistent on optimal boundaries given different numbers of segments.",
                "This situation occurs especially when the similarities among segments are quite different, i.e. some transitions are very obvious, while others are not.",
                "This is because usually a document is a hierarchical structure instead of only a sequential structure.",
                "When the segments are not at the same level, this situation may occur.",
                "Thus, a hierarchical topic segmentation approach is desired, and the structure highly depends on the number of segments for each internal node and the stop criteria of splitting.",
                "For this data set of singledocument segmentation, since it is just a synthetic set, which is just a concatenation of several segments about different topics, it is reasonable that approaches simply based on term frequency have a good performance.",
                "Usually for the tasks of segmenting coherent documents for sub-topics, the effectiveness decreases much. 5.2 Shared Topic Detection 5.2.1 Test Data and Evaluation The second data set contains 80 news articles from Google News.",
                "There are eight topics and each has 10 articles.",
                "We randomly split the set into subsets with different document numbers and each subset has all eight topics.",
                "We compare our approach MIl and WMIl with LDA [4].",
                "LDA treats a document in the data set as a bag of words, finds its distribution on topics, and its major topic.",
                "MIl and WMIl views each sentence as a bag of words and tag it with a topic label.",
                "Then for each pair of documents, LDA determines if they are on the same topic, while MIl and Table 3: Shared Topic Detection: Average Error Rates for Different Numbers of Documents in Each Subset #Doc 10 20 40 80 LDA 8.89% 16.33% 1.35% 0.60% MIl, θ = 0.6 4.17% 1.71% 1.47% 0.0% WMIl, θ = 0.8 18.6% 3.16% 1.92% 0.0% WMIl check whether the proportion overlapped sentences on the same topic is larger than the adjustable threshold θ.",
                "That is, in MIl and WMIl, for a pair of documents d, d , if [ s∈Sd,s ∈Sd 1(topics=topics )/min(|Sd|, |Sd|)] > θ, where Sd is the set of sentences of d, and |Sd| is the number of sentences of d, then d and d have the shared topic.",
                "For a pair of documents selected randomly, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, same)p(same|real) +p(false alarm|real, pred, diff)p(diff|real), where a miss means if they have the same topic (same) for the real case (real), but predicted (pred) as on the same topic.",
                "If they are on different topics (diff), but predicted as on the same topic, it is a false alarm. 5.2.2 Experiment Results The results are shown in Table 3.",
                "If most documents have different topics, in WMIl, the estimation of term weights in Equation (3) is not correct.",
                "Thus, WMIl is not expected to have a better performance than MIl, when most documents have different topics.",
                "When there are fewer documents in a subset with the same number of topics, more documents have different topics, so WMIl is more worse than MIl.",
                "We can see that for most cases MIl has a better (or at least similar) performance than LDA.",
                "After shared topic detection, <br>multi-document segmentation</br> of documents with the shared topics is able to be executed. 5.3 <br>multi-document segmentation</br> 5.3.1 Test Data and Evaluation For multi-document segmentation and alignment, our goal is to identify these segments about the same topic among multiple similar documents with shared topics.",
                "Using Iw is expected to perform better than I, since without term weights the result is affected seriously by document-dependent stop words and noisy words which depends on the personal writing style.",
                "It is more likely to treat the same segments of different documents as different segments under the effect of document-dependent stop words and noisy words.",
                "Term weights can reduce the effect of document-dependent stop words and noisy words by giving cue terms more weights.",
                "The data set for <br>multi-document segmentation</br> and alignment has 102 samples and 2264 sentences totally.",
                "Each is the introduction part of a lab report selected from the course of Biol 240W, Pennsylvania State University.",
                "Each sample has two segments, introduction of plant hormones and the content in the lab.",
                "The length range of samples is from two to 56 sentences.",
                "Some samples only have one part and some have a reverse order the these two segments.",
                "It is not hard to identify the boundary between two segments for human.",
                "We labelled each sentence manually for evaluation.",
                "The criterion of evaluation is just using the proportion of the number of sentences with wrong predicted segment labels in the total number of sentences in the whole training Table 4: Average Error Rates of <br>multi-document segmentation</br> Given Segment Numbers Known #Doc MIl WMIl k MIk WMIk 102 3.14% 2.78% 300 4.68% 6.58% 51 4.17% 3.63% 300 17.83% 22.84% 34 5.06% 4.12% 300 18.75% 20.95% 20 7.08% 5.42% 250 20.40% 21.83% 10 10.38% 7.89% 250 21.42% 21.91% 5 15.77% 11.64% 250 21.89% 22.59% 2 25.90% 23.18% 50 25.44% 25.49% 1 23.90% 24.82% 25 25.75% 26.15% Table 5: <br>multi-document segmentation</br>: P-values of T-test on Error Rates for MIl and WMIl #Doc 51 34 20 10 5 2 P-value 0.19 0.101 0.025 0.001 0.000 0.002 set as the error rate: p(error|predicted, real) = d∈D s∈Sd 1(predicteds=reals)/ d∈D nd.",
                "In order to show the benefits of <br>multi-document segmentation</br> and alignment, we compared our method with different parameters on different partitions of the same training set.",
                "Except the cases that the number of documents is 102 and one (they are special cases of using the whole set and the pure single-document segmentation), we randomly divided the training set into m partitions, and each has 51, 34, 20, 10, 5, and 2 document samples.",
                "Then we applied our methods on each partition and calculated the error rate of the whole training set.",
                "Each case was repeated for 10 times for computing the average error rates.",
                "For different partitions of the training set, different k values are used, since the number of terms increases when the document number in each partition increases. 5.3.2 Experiment Results From the experiment results in Table 4, we can see the following observations: (1) When the number of documents increases, all methods have better performances.",
                "Only from one to two documents, MIl has decrease a little.",
                "We can observe this from Figure 3 at the point of document number = 2.",
                "Most curves even have the worst results at this point.",
                "There are two reasons.",
                "First, because samples vote for the best <br>multi-document segmentation</br> and alignment, but if only two documents are compared with each other, the one with missing segments or a totally different sequence will affect the correct segmentation and alignment of the other.",
                "Second, as noted at the beginning of this section, if two documents have more document-dependent stop words or noisy words than cue words, then the algorithm may view them as two different segments and the other segment is missing.",
                "Generally, we can only expect a better performance when the number of documents is larger than the number of segments. (2) Except single-document segmentation, WMIl is always better than MIl, and when the number of documents is reaching one or increases to a very large number, their performances become closer.",
                "Table 5 shows p-values of twosample one-sided t-test between MIl and WMIl.",
                "We also can see this trend from p-values.",
                "When document number = 5, we reached the smallest p-value and the largest difference between error rates of MIl and WMIl.",
                "For single-document Table 6: <br>multi-document segmentation</br>: Average Error Rate for Document Number = 5 in Each Subset with Different Number of Term Clusters #Cluster 75 100 150 250 l MIk 24.67% 24.54% 23.91% 22.59% 15.77% segmentation, WMIl is even a little bit worse than MIl, which is similar as the results of the single-document segmentation on the first data set.",
                "The reason is that for singledocument segmentation, we cannot estimate term weights accurately, since multiple documents are unavailable. (3) Using term clustering usually gets worse results than MIl and WMIl. (4) Using term clustering in WMIk is even worse than in MIk, since in WMIk term clusters are found first using I before using Iw.",
                "If the term clusters are not correct, then the term weights are estimated worse, which may mislead the algorithm to reach even worse results.",
                "From the results we also found that in <br>multi-document segmentation</br> and alignment, most documents with missing segments and a reverse order are identified correctly.",
                "Table 6 illustrates the experiment results for the case of 20 partitions (each has five document samples) of the training set and topic segmentation and alignment using MIk with different numbers of term clusters k. Notice that when the number of term clusters increases, the error rate becomes smaller.",
                "Without term clustering, we have the best result.",
                "We did not show results for WMIk with term clustering, but the results are similar.",
                "We also tested WMIl with different hyper parameters of a and b to adjust term weights.",
                "The results are presented in Figure 3.",
                "It was shown that the default case WMIl : a = 1, b = 1 gave the best results for different partitions of the training set.",
                "We can see the trend that when the document number is very small or large, the difference between MIl : a = 0, b = 0 and WMIl : a = 1, b = 1 becomes quite small.",
                "When the document number is not large (about from 2 to 10), all the cases using term weights have better performances than MIl : a = 0, b = 0 without term weights, but when the document number becomes larger, the cases WMIl : a = 1, b = 0 and WMIl : a = 2, b = 1 become worse than MIl : a = 0, b = 0.",
                "When the document number becomes very large, they are even worse than cases with small document numbers.",
                "This means that a proper way to estimate term weights for the criterion of WMI is very important.",
                "Figure 4 shows the term weights learned from the whole training set.",
                "Four types of words are categorized roughly even though the transition among them are subtle.",
                "Figure 5 illustrates the change in (weighted) mutual information for MIl and WMIl.",
                "As expected, mutual information for MIl increases monotonically with the number of steps, while WMIl does not.",
                "Finally, MIl and WMIl are scalable, with computational complexity shown in Figure 6.",
                "One advantage for our approach based on MI is that removing stop words is not required.",
                "Another important advantage is that there are no necessary hyper parameters to adjust.",
                "In single-document segmentation, the performance based on MI is even better for that based on WMI, so no extra hyper parameter is required.",
                "In <br>multi-document segmentation</br>, we show in the experiment, a = 1 and b = 1 is the best.",
                "Our method gives more weights to cue terms.",
                "However, usually cue terms or sentences appear at the beginning of a segment, while the end of the segment may be 1 2 5 10 20 34 51 102 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 Document Number ErrorRate MIl :a=0,b=0 WMI l :a=1,b=1 WMI l :a=1,b=0 WMI l :a=2,b=1 Figure 3: Error rates for different hyper parameters of term weights. 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Normalized Document Entropy NormalizedSegmentEntropy Noisy words Cue words Common stop words Doc−dep stop words Figure 4: Term weights learned from the whole training set. 0 100 200 300 400 500 600 0.06 0.08 0.1 0.12 0.14 0.16 0.18 Number of Steps (Weighted)MutualInformation MI l WMI l Figure 5: Change in (weighted) MI for MIl and WMIl. 0 20 40 60 80 100 120 0 200 400 600 800 1000 1200 1400 1600 1800 2000 Document Number TimetoConverge(sec) MI l WMI l Figure 6: Time to converge for MIl and WMIl. much noisy.",
                "One possible solution is giving more weights to terms at the beginning of each segment.",
                "Moreover, when the length of segments are quite different, long segments have much higher term frequencies, so they may dominate the segmentation boundaries.",
                "Normalization of term frequencies versus the segment length may be useful. 6.",
                "CONCLUSIONS AND FUTURE WORK We proposed a novel method for multi-document topic segmentation and alignment based on weighted mutual information, which can also handle single-document cases.",
                "We used dynamic programming to optimize our algorithm.",
                "Our approach outperforms all the previous methods on singledocument cases.",
                "Moreover, we also showed that doing segmentation among multiple documents can improve the performance tremendously.",
                "Our results also illustrated that using weighted mutual information can utilize the information of multiple documents to reach a better performance.",
                "We only tested our method on limited data sets.",
                "More data sets especially complicated ones should be tested.",
                "More previous methods should be compared with.",
                "Moreover, natural segmentations like paragraphs are hints that can be used to find the optimal boundaries.",
                "Supervised learning also can be considered. 7.",
                "ACKNOWLEDGMENTS The authors want to thank Xiang Ji, and Prof. J. Scott Payne for their help. 8.",
                "REFERENCES [1] A. Banerjee, I. Ghillon, J. Ghosh, S. Merugu, and D. Modha.",
                "A generalized maximum entropy approach to bregman co-clustering and matrix approximation.",
                "In Proceedings of SIGKDD, 2004. [2] R. Bekkerman, R. El-Yaniv, and A. McCallum.",
                "Multi-way distributional clustering via pairwise interactions.",
                "In Proceedings of ICML, 2005. [3] D. M. Blei and P. J. Moreno.",
                "Topic segmentation with an aspect hidden markov model.",
                "In Proceedings of SIGIR, 2001. [4] D. M. Blei, A. Ng, and M. Jordan.",
                "Latent dirichlet allocation.",
                "Journal of Machine Learning Research, 3:993-1022, 2003. [5] T. Brants, F. Chen, and I. Tsochantaridis.",
                "Topic-based document segmentation with probabilistic latent semantic analysis.",
                "In Proceedings of CIKM, 2002. [6] F. Choi.",
                "Advances in domain indepedent linear text segmentation.",
                "In Proceedings of the NAACL, 2000. [7] H. Christensen, B. Kolluru, Y. Gotoh, and S. Renals.",
                "Maximum entropy segmentation of broadcast news.",
                "In Proceedings of ICASSP, 2005. [8] T. Cover and J. Thomas.",
                "Elements of Information Theory.",
                "John Wiley and Sons, New York, USA, 1991. [9] S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and R. Harshman.",
                "Indexing by latent semantic analysis.",
                "Journal of the American Society for Information Systems, 1990. [10] I. Dhillon, S. Mallela, and D. Modha.",
                "Information-theoretic co-clustering.",
                "In Proceedings of SIGKDD, 2003. [11] M. Hajime, H. Takeo, and O. Manabu.",
                "Text segmentation with multiple surface linguistic cues.",
                "In Proceedings of COLING-ACL, 1998. [12] T. K. Ho.",
                "Stop word location and identification for adaptive text recognition.",
                "International Journal of Document Analysis and Recognition, 3(1), August 2000. [13] T. Hofmann.",
                "Probabilistic latent semantic analysis.",
                "In Proceedings of the UAI99, 1999. [14] X. Ji and H. Zha.",
                "Correlating summarization of a pair of multilingual documents.",
                "In Proceedings of RIDE, 2003. [15] X. Ji and H. Zha.",
                "Domain-independent text segmentation using anisotropic diffusion and dynamic programming.",
                "In Proceedings of SIGIR, 2003. [16] X. Ji and H. Zha.",
                "Extracting shared topics of multiple documents.",
                "In Proceedings of the 7th PAKDD, 2003. [17] J. Lafferty, A. McCallum, and F. Pereira.",
                "Conditional random fields: Probabilistic models for segmenting and labeling sequence data.",
                "In Proceedings of ICML, 2001. [18] T. Li, S. Ma, and M. Ogihara.",
                "Entropy-based criterion in categorical clustering.",
                "In Proceedings of ICML, 2004. [19] A. McCallum, D. Freitag, and F. Pereira.",
                "Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of ICML, 2000. [20] L. Pevzner and M. Hearst.",
                "A critique and improvement of an evaluation metric for text segmentation.",
                "Computational Linguistic, 28(1):19-36, 2002. [21] J. C. Reynar.",
                "Statistical models for topic segmentation.",
                "In Proceedings of ACL, 1999. [22] G. Salton and M. McGill.",
                "Introduction to Modern Information Retrieval.",
                "McGraw Hill, 1983. [23] B.",
                "Sun, Q. Tan, P. Mitra, and C. L. Giles.",
                "Extraction and search of chemical formulae in text documents on the web.",
                "In Proceedings of WWW, 2007. [24] B.",
                "Sun, D. Zhou, H. Zha, and J.",
                "Yen.",
                "Multi-task text segmentation and alignment based on weighted mutual information.",
                "In Proceedings of CIKM, 2006. [25] M. Utiyama and H. Isahara.",
                "A statistical model for domain-independent text segmentation.",
                "In Proceedings of the 39th ACL, 1999. [26] C. Wayne.",
                "Multilingual topic detection and tracking: Successful research enabled by corpora and evaluation.",
                "In Proceedings of LREC, 2000. [27] J. Yamron, I. Carp, L. Gillick, S. Lowe, and P. van Mulbregt.",
                "A hidden markov model approach to text segmentation and event tracking.",
                "In Proceedings of ICASSP, 1998. [28] H. Zha and X. Ji.",
                "Correlating multilingual documents via bipartite graph modeling.",
                "In Proceedings of SIGIR, 2002."
            ],
            "original_annotated_samples": [
                "It also can handle single-document segmentation as a special case of the <br>multi-document segmentation</br> and alignment.",
                "Our experimental results show that our algorithm works well for the tasks of single-document segmentation, shared topic detection, and <br>multi-document segmentation</br>.",
                "Utilizing information from multiple documents can tremendously improve the performance of topic segmentation, and using WMI is even better than using MI for the <br>multi-document segmentation</br>.",
                "<br>multi-document segmentation</br> and alignment can utilize information from similar documents and improves the performance of topic segmentation greatly.",
                "In Section 4, we first propose the iterative greedy algorithm of topic segmentation and alignment with term co-clustering, and then describe how the algorithm can be optimized by usFigure 1: Illustration of <br>multi-document segmentation</br> and alignment. ing dynamic programming."
            ],
            "translated_annotated_samples": [
                "También puede manejar la segmentación de un solo documento como un caso especial de la <br>segmentación y alineación de varios documentos</br>.",
                "Nuestros resultados experimentales muestran que nuestro algoritmo funciona bien para las tareas de segmentación de un solo documento, detección de temas compartidos y <br>segmentación de múltiples documentos</br>.",
                "El uso de información de múltiples documentos puede mejorar enormemente el rendimiento de la segmentación de temas, y utilizar WMI es aún mejor que utilizar MI para la <br>segmentación de múltiples documentos</br>.",
                "La <br>segmentación y alineación de múltiples documentos</br> pueden utilizar información de documentos similares y mejorar significativamente el rendimiento de la segmentación de temas.",
                "En la Sección 4, primero proponemos el algoritmo voraz iterativo de segmentación y alineación de temas con co-clustering de términos, y luego describimos cómo el algoritmo puede ser optimizado mediante programación dinámica."
            ],
            "translated_text": "La segmentación de temas con detección de temas compartidos y alineación de múltiples documentos Bingjun Sun*, Prasenjit Mitra*†, Hongyuan Zha‡, C. Lee Giles*†, John Yen*† *Departamento de Ciencias de la Computación e Ingeniería † Colegio de Ciencias de la Información y Tecnología Universidad Estatal de Pensilvania University Park, PA 16802 ‡ Colegio de Computación Instituto de Tecnología de Georgia Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu RESUMEN La detección y seguimiento de temas [26] y la segmentación de temas [15] juegan un papel importante en la captura de la información local y secuencial de los documentos. El trabajo previo en esta área suele centrarse en documentos individuales, aunque existen múltiples documentos similares en muchos dominios. En este artículo, presentamos un novedoso método no supervisado para la detección de temas compartidos y la segmentación de temas de múltiples documentos similares basado en la información mutua (MI) y la información mutua ponderada (WMI), que es una combinación de MI y pesos de términos. La idea básica es que la segmentación óptima maximiza MI (o WMI). Nuestro enfoque puede detectar temas compartidos entre documentos. Puede encontrar los límites óptimos en un documento y alinear segmentos entre documentos al mismo tiempo. También puede manejar la segmentación de un solo documento como un caso especial de la <br>segmentación y alineación de varios documentos</br>. Nuestros métodos pueden identificar y fortalecer términos de señal que pueden ser utilizados para la segmentación y eliminar parcialmente las palabras de parada mediante el uso de pesos de términos basados en la entropía aprendida de múltiples documentos. Nuestros resultados experimentales muestran que nuestro algoritmo funciona bien para las tareas de segmentación de un solo documento, detección de temas compartidos y <br>segmentación de múltiples documentos</br>. El uso de información de múltiples documentos puede mejorar enormemente el rendimiento de la segmentación de temas, y utilizar WMI es aún mejor que utilizar MI para la <br>segmentación de múltiples documentos</br>. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información-Agrupación; H.3.1 [Almacenamiento y Recuperación de Información]: Análisis de Contenido e Indexación-Procesamiento Lingüístico; I.2.7 [Inteligencia Artificial]: Procesamiento de Lenguaje Natural-Análisis de Texto; I.5.3 [Reconocimiento de Patrones]: Algoritmos de Agrupación-Medidas de Similitud Términos Generales Algoritmos, Diseño, Experimentación 1. INTRODUCCIÓN Muchos investigadores han trabajado en la detección y seguimiento de temas (TDT) [26] y la segmentación de temas durante la última década. La segmentación de temas tiene como objetivo identificar los límites en un documento con la meta de capturar la estructura temática latente. Las tareas de segmentación de temas generalmente se dividen en dos categorías [15]: la segmentación de flujos de texto, donde se identifica la transición de temas, y la segmentación coherente de documentos, en la que los documentos se dividen en subtemas. La primera categoría tiene aplicaciones en el reconocimiento automático del habla, mientras que la segunda tiene más aplicaciones como la consulta parcial de texto en documentos extensos en la recuperación de información, resumen de texto y medición de calidad de múltiples documentos. La investigación previa en relación con TDT se enmarca en la primera categoría, centrada en el seguimiento de temas de datos de discurso de transmisión y texto de agencia de noticias, mientras que la segunda categoría no ha sido estudiada muy bien. Los enfoques tradicionales realizan la segmentación de temas en los documentos uno a la vez [15, 25, 6]. La mayoría de ellos tienen un mal desempeño en tareas sutiles como la segmentación coherente de documentos [15]. A menudo, los usuarios finales buscan documentos que tengan un contenido similar. Los motores de búsqueda, como Google, proporcionan enlaces para obtener páginas similares. A una granularidad más fina, los usuarios pueden estar buscando obtener secciones de un documento similares a una sección en particular que presumiblemente discute un tema de interés para los usuarios. Por lo tanto, la extensión de la segmentación de temas desde documentos individuales para identificar segmentos similares de múltiples documentos similares con el mismo tema es una dirección natural y necesaria, y se espera que la segmentación de temas de múltiples documentos tenga un mejor rendimiento ya que se utiliza más información. Los enfoques tradicionales que utilizan la medición de similitud basada en la frecuencia de términos generalmente parten del mismo supuesto de que un vocabulario similar tiende a estar en un segmento de tema coherente [15, 25, 6]. Sin embargo, suelen tener el problema de identificar las palabras de parada. Por ejemplo, se eliminan las palabras de parada dependientes del documento junto con las palabras de parada genéricas en [15]. Hay dos razones por las que no eliminamos las palabras de parada directamente. Primero, identificar las palabras vacías es otro problema [12] que requiere estimación en cada dominio. Eliminar palabras comunes de parada puede resultar en la pérdida de información útil en un dominio específico. Segundo, aunque se pueden identificar las palabras de parada, la clasificación estricta de palabras de parada y palabras no de parada no puede representar el cambio gradual en la cantidad de contenido informativo de cada palabra. Empleamos una clasificación suave utilizando pesos de términos. En este documento, consideramos el problema de la segmentación de temas como un problema de optimización utilizando técnicas de teoría de la información para encontrar los límites óptimos de un documento dado el número de segmentos de texto, de manera que se minimice la pérdida de información mutua (MI) (o información mutua ponderada (WMI)) después de la segmentación y alineación. Esto es igual a maximizar el MI (o WMI). El MI se centra en medir la diferencia entre segmentos, mientras que investigaciones anteriores se enfocaban en encontrar la similitud (por ejemplo, la distancia del coseno) entre segmentos [15, 25, 6]. La alineación de temas de múltiples documentos similares se puede lograr agrupando las oraciones sobre el mismo tema en el mismo clúster. La segmentación de temas en un solo documento es solo un caso especial del problema de segmentación y alineación de temas en múltiples documentos. Los términos pueden ser co-agrupados como en [10] al mismo tiempo, dado el número de agrupaciones, pero nuestros resultados experimentales muestran que este método resulta en una peor segmentación (ver Tablas 1, 4 y 6). Por lo general, los lectores humanos pueden identificar la transición de temas basándose en palabras de señal, y pueden ignorar las palabras vacías. Inspirados por esto, asignamos a cada término (o grupo de términos) un peso basado en la entropía entre diferentes documentos y diferentes segmentos de documentos. No solo este enfoque puede aumentar la contribución de las palabras clave, sino que también puede disminuir el efecto de las palabras de parada comunes, palabras ruidosas y palabras de parada dependientes del documento. Estas palabras son comunes en un documento. Muchos métodos basados en la similitud de oraciones requieren que estas palabras sean eliminadas antes de que se pueda realizar la segmentación de temas [15]. Nuestros resultados en la Figura 3 muestran que los pesos de los términos son útiles para la segmentación y alineación de temas en múltiples documentos. La principal contribución de este artículo es que introduce un método novedoso para la segmentación de temas utilizando MI y demuestra que este método funciona mejor que los criterios previamente utilizados. Además, hemos abordado el problema de la segmentación y alineación de temas en varios documentos, mientras que la mayoría de las investigaciones existentes se centraron en la segmentación de documentos individuales. La <br>segmentación y alineación de múltiples documentos</br> pueden utilizar información de documentos similares y mejorar significativamente el rendimiento de la segmentación de temas. Obviamente, nuestro enfoque puede manejar documentos individuales como un caso especial cuando varios documentos no están disponibles. Puede detectar temas compartidos entre documentos para determinar si son múltiples documentos sobre el mismo tema. También introducimos el nuevo criterio de WMI basado en pesos de términos aprendidos de múltiples documentos similares, lo cual puede mejorar aún más el rendimiento de la segmentación de temas. Proponemos un algoritmo voraz iterativo basado en programación dinámica y demostramos que funciona bien en la práctica. Algunos de nuestros trabajos anteriores están en [24]. El resto de este documento está organizado de la siguiente manera: En la Sección 2, revisamos el trabajo relacionado. La sección 3 contiene una formulación del problema de segmentación de temas y alineación de múltiples documentos con co-clustering de términos, una revisión del criterio de MI para el clustering, y finalmente una introducción a WMI. En la Sección 4, primero proponemos el algoritmo voraz iterativo de segmentación y alineación de temas con co-clustering de términos, y luego describimos cómo el algoritmo puede ser optimizado mediante programación dinámica. ",
            "candidates": [],
            "error": [
                [
                    "segmentación y alineación de varios documentos",
                    "segmentación de múltiples documentos",
                    "segmentación de múltiples documentos",
                    "segmentación y alineación de múltiples documentos"
                ]
            ]
        },
        "cue term": {
            "translated_key": "términos de señal",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Topic Segmentation with Shared Topic Detection and Alignment of Multiple Documents Bingjun Sun*, Prasenjit Mitra*† , Hongyuan Zha‡ , C. Lee Giles*† , John Yen*† *Department of Computer Science and Engineering † College of Information Sciences and Technology The Pennsylvania State University University Park, PA 16802 ‡ College of Computing The Georgia Institute of Technology Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu ABSTRACT Topic detection and tracking [26] and topic segmentation [15] play an important role in capturing the local and sequential information of documents.",
                "Previous work in this area usually focuses on single documents, although similar multiple documents are available in many domains.",
                "In this paper, we introduce a novel unsupervised method for shared topic detection and topic segmentation of multiple similar documents based on mutual information (MI) and weighted mutual information (WMI) that is a combination of MI and term weights.",
                "The basic idea is that the optimal segmentation maximizes MI(or WMI).",
                "Our approach can detect shared topics among documents.",
                "It can find the optimal boundaries in a document, and align segments among documents at the same time.",
                "It also can handle single-document segmentation as a special case of the multi-document segmentation and alignment.",
                "Our methods can identify and strengthen <br>cue term</br>s that can be used for segmentation and partially remove stop words by using term weights based on entropy learned from multiple documents.",
                "Our experimental results show that our algorithm works well for the tasks of single-document segmentation, shared topic detection, and multi-document segmentation.",
                "Utilizing information from multiple documents can tremendously improve the performance of topic segmentation, and using WMI is even better than using MI for the multi-document segmentation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Clustering; H.3.1 [Information Storage and Retrieval]: Content Analysis and IndexingLinguistic processing; I.2.7 [Artificial Intelligence]: Natural Language Processing-Text analysis; I.5.3 [Pattern Recognition]: Clustering-Algorithms;Similarity measures General Terms Algorithms, Design, Experimentation 1.",
                "INTRODUCTION Many researchers have worked on topic detection and tracking (TDT) [26] and topic segmentation during the past decade.",
                "Topic segmentation intends to identify the boundaries in a document with the goal to capture the latent topical structure.",
                "Topic segmentation tasks usually fall into two categories [15]: text stream segmentation where topic transition is identified, and coherent document segmentation in which documents are split into sub-topics.",
                "The former category has applications in automatic speech recognition, while the latter one has more applications such as partial-text query of long documents in information retrieval, text summary, and quality measurement of multiple documents.",
                "Previous research in connection with TDT falls into the former category, targeted on topic tracking of broadcast speech data and newswire text, while the latter category has not been studied very well.",
                "Traditional approaches perform topic segmentation on documents one at a time [15, 25, 6].",
                "Most of them perform badly in subtle tasks like coherent document segmentation [15].",
                "Often, end-users seek documents that have the similar content.",
                "Search engines, like, Google, provide links to obtain similar pages.",
                "At a finer granularity, users may actually be looking to obtain sections of a document similar to a particular section that presumably discusses a topic of the users interest.",
                "Thus, the extension of topic segmentation from single documents to identifying similar segments from multiple similar documents with the same topic is a natural and necessary direction, and multi-document topic segmentation is expected to have a better performance since more information is utilized.",
                "Traditional approaches using similarity measurement based on term frequency generally have the same assumption that similar vocabulary tends to be in a coherent topic segment [15, 25, 6].",
                "However, they usually suffer the issue of identifying stop words.",
                "For example, additional document-dependent stop words are removed together with the generic stop words in [15].",
                "There are two reasons that we do not remove stop words directly.",
                "First, identifying stop words is another issue [12] that requires estimation in each domain.",
                "Removing common stop words may result in the loss of useful information in a specific domain.",
                "Second, even though stop words can be identified, hard classification of stop words and nonstop words cannot represent the gradually changing amount of information content of each word.",
                "We employ a soft classification using term weights.",
                "In this paper, we view the problem of topic segmentation as an optimization issue using information theoretic techniques to find the optimal boundaries of a document given the number of text segments so as to minimize the loss of mutual information (MI) (or a weighted mutual information (WMI)) after segmentation and alignment.",
                "This is equal to maximizing the MI (or WMI).",
                "The MI focuses on measuring the difference among segments whereas previous research focused on finding the similarity (e.g. cosine distance) of segments [15, 25, 6].",
                "Topic alignment of multiple similar documents can be achieved by clustering sentences on the same topic into the same cluster.",
                "Single-document topic segmentation is just a special case of the multi-document topic segmentation and alignment problem.",
                "Terms can be co-clustered as in [10] at the same time, given the number of clusters, but our experimental results show that this method results in a worse segmentation (see Tables 1, 4, and 6).",
                "Usually, human readers can identify topic transition based on cue words, and can ignore stop words.",
                "Inspired by this, we give each term (or term cluster) a weight based on entropy among different documents and different segments of documents.",
                "Not only can this approach increase the contribution of cue words, but it can also decrease the effect of common stop words, noisy word, and document-dependent stop words.",
                "These words are common in a document.",
                "Many methods based on sentence similarity require that these words are removed before topic segmentation can be performed [15].",
                "Our results in Figure 3 show that term weights are useful for multi-document topic segmentation and alignment.",
                "The major contribution of this paper is that it introduces a novel method for topic segmentation using MI and shows that this method performs better than previously used criteria.",
                "Also, we have addressed the problem of topic segmentation and alignment across multiple documents, whereas most existing research focused on segmentation of single documents.",
                "Multi-document segmentation and alignment can utilize information from similar documents and improves the performance of topic segmentation greatly.",
                "Obviously, our approach can handle single documents as a special case when multiple documents are unavailable.",
                "It can detect shared topics among documents to judge if they are multiple documents on the same topic.",
                "We also introduce the new criterion of WMI based on term weights learned from multiple similar documents, which can improve performance of topic segmentation further.",
                "We propose an iterative greedy algorithm based on dynamic programming and show that it works well in practice.",
                "Some of our prior work is in [24].",
                "The rest of this paper is organized as follows: In Section 2, we review related work.",
                "Section 3 contains a formulation of the problem of topic segmentation and alignment of multiple documents with term co-clustering, a review of the criterion of MI for clustering, and finally an introduction to WMI.",
                "In Section 4, we first propose the iterative greedy algorithm of topic segmentation and alignment with term co-clustering, and then describe how the algorithm can be optimized by usFigure 1: Illustration of multi-document segmentation and alignment. ing dynamic programming.",
                "In Section 5, experiments about single-document segmentation, shared topic detection, and multi-document segmentation are described, and results are presented and discussed to evaluate the performance of our algorithm.",
                "Conclusions and some future directions of the research work are discussed in Section 6. 2.",
                "PREVIOUS WORK Generally, the existing approaches to text segmentation fall into two categories: supervised learning [19, 17, 23] and unsupervised learning [3, 27, 5, 6, 15, 25, 21].",
                "Supervised learning usually has good performance, since it learns functions from labelled training sets.",
                "However, often getting large training sets with manual labels on document sentences is prohibitively expensive, so unsupervised approaches are desired.",
                "Some models consider dependence between sentences and sections, such as Hidden Markov Model [3, 27], Maximum Entropy Markov Model [19], and Conditional Random Fields [17], while many other approaches are based on lexical cohesion or similarity of sentences [5, 6, 15, 25, 21].",
                "Some approaches also focus on cue words as hints of topic transitions [11].",
                "While some existing methods only consider information in single documents [6, 15], others utilize multiple documents [16, 14].",
                "There are not many works in the latter category, even though the performance of segmentation is expected to be better with utilization of information from multiple documents.",
                "Previous research studied methods to find shared topics [16] and topic segmentation and summarization between just a pair of documents [14].",
                "Text classification and clustering is a related research area which categorizes documents into groups using supervised or unsupervised methods.",
                "Topical classification or clustering is an important direction in this area, especially co-clustering of documents and terms, such as LSA [9], PLSA [13], and approaches based on distances and bipartite graph partitioning [28] or maximum MI [2, 10], or maximum entropy [1, 18].",
                "Criteria of these approaches can be utilized in the issue of topic segmentation.",
                "Some of those methods have been extended into the area of topic segmentation, such as PLSA [5] and maximum entropy [7], but to our best knowledge, using MI for topic segmentation has not been studied. 3.",
                "PROBLEM FORMULATION Our goal is to segment documents and align the segments across documents (Figure 1).",
                "Let T be the set of terms {t1, t2, ..., tl}, which appear in the unlabelled set of documents D = {d1, d2, ..., dm}.",
                "Let Sd be the set of sentences for document d ∈ D, i.e. {s1, s2, ..., snd }.",
                "We have a 3D matrix of term frequency, in which the three dimensions are random variables of D, Sd, and T. Sd actually is a random vector including a random variable for each d ∈ D. The term frequency can be used to estimate the joint probability distribution P(D, Sd, T), which is p(t, d, s) = T(t, d, s)/ND, where T(t, d, s) is the number of t in ds sentence s and ND is the total number of terms in D. ˆS represents the set of segments {ˆs1, ˆs2, ..., ˆsp} after segmentation and alignment among multiple documents, where the number of segments | ˆS| = p. A segment ˆsi of document d is a sequence of adjacent sentences in d. Since for different documents si may discuss different sub-topics, our goal is to cluster adjacent sentences in each document into segments, and align similar segments among documents, so that for different documents ˆsi is about the same sub-topic.",
                "The goal is to find the optimal topic segmentation and alignment mapping Segd(si) : {s1, s2, ..., snd } → {ˆs1, ˆs2, ..., ˆsp} and Alid(ˆsi) : {ˆs1, ˆs2, ..., ˆsp} → {ˆs1, ˆs2, ..., ˆsp}, for all d ∈ D, where ˆsi is ith segment with the constraint that only adjacent sentences can be mapped to the same segment, i.e. for d, {si, si+1, ..., sj} → {ˆsq}, where q ∈ {1, ..., p}, where p is the segment number, and if i > j, then for d, ˆsq is missing.",
                "After segmentation and alignment, random vector Sd becomes an aligned random variable ˆS.",
                "Thus, P(D, Sd, T) becomes P(D, ˆS, T).",
                "Term co-clustering is a technique that has been employed [10] to improve the accuracy of document clustering.",
                "We evaluate the effect of it for topic segmentation.",
                "A term t is mapped to exactly one term cluster.",
                "Term co-clustering involves simultaneously finding the optimal term clustering mapping Clu(t) : {t1, t2, ..., tl} → {ˆt1, ˆt2, ..., ˆtk}, where k ≤ l, l is the total number of words in all the documents, and k is the number of clusters. 4.",
                "METHODOLOGY We now describe a novel algorithm which can handle singledocument segmentation, shared topic detection, and multidocument segmentation and alignment based on MI or WMI. 4.1 Mutual Information MI I(X; Y ) is a quantity to measure the amount of information which is contained in two or more random variables [8, 10].",
                "For the case of two random variables, we have I(X; Y ) = x∈X y∈Y p(x, y)log p(x, y) p(x)p(y) , (1) Obviously, when random variables X and Y are independent, I(X; Y ) = 0.",
                "Thus, intuitively, the value of MI depends on how random variables are dependent on each other.",
                "The optimal co-clustering is the mapping Clux : X → ˆX and Cluy : Y → ˆY that minimizes the loss: I(X; Y ) − I( ˆX; ˆY ), which is equal to maximizing I( ˆX; ˆY ).",
                "This is the criterion of MI for clustering.",
                "In the case of topic segmentation, the two random variables are the term variable T and the segment variable S, and each sample is an occurrence of a term T = t in a particular segment S = s. I(T; S) is used to measure how dependent T and S are.",
                "However, I(T; S) cannot be computed for documents before segmentation, since we do not have a set of S due to the fact that sentences of Document d, si ∈ Sd, is not aligned with other documents.",
                "Thus, instead of minimizing the loss of MI, we can maximize MI after topic segmentation, computed as: I( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS p(ˆt, ˆs)log p(ˆt, ˆs) p(ˆt)p(ˆs) , (2) where p(ˆt, ˆs) are estimated by the term frequency tf of Term Cluster ˆt and Segment ˆs in the training set D. Note that here a segment ˆs includes sentences about the the same topic among all documents.",
                "The optimal solution is the mapping Clut : T → ˆT, Segd : Sd → ˆS , and Alid : ˆS → ˆS, which maximizes I( ˆT; ˆS). 4.2 Weighted Mutual Information In topic segmentation and alignment of multiple documents, if P(D, ˆS, T) is known, based on the marginal distributions P(D|T) and P( ˆS|T) for each term t ∈ T, we can categorize terms into four types in the data set: • Common stop words are common both along the dimensions of documents and segments. • Document-dependent stop words that depends on the personal writing style are common only along the dimension of segments for some documents. • Cue words are the most important elements for segmentation.",
                "They are common along the dimension of documents only for the same segment, and they are not common along the dimensions of segments. • Noisy words are other words which are not common along both dimensions.",
                "Entropy based on P(D|T) and P( ˆS|T) can be used to identify different types of terms.",
                "To reinforce the contribution of cue words in the MI computation, and simultaneously reduce the effect of the other three types of words, similar as the idea of the tf-idf weight [22], we use entropies of each term along the dimensions of document D and segment ˆS, i.e.",
                "ED(ˆt) and EˆS(ˆt), to compute the weight.",
                "A cue word usually has a large value of ED(ˆt) but a small value of EˆS(ˆt).",
                "We introduce term weights (or term cluster weights) wˆt = ( ED(ˆt) maxˆt ∈ ˆT (ED(ˆt )) )a (1 − EˆS(ˆt) maxˆt ∈ ˆT (EˆS(ˆt )) )b , (3) where ED(ˆt) = d∈D p(d|ˆt)log|D| 1 p(d|ˆt) , EˆS(ˆt) = ˆs∈ ˆS p(ˆs|ˆt)log| ˆS| 1 p(ˆs|ˆt) , and a > 0 and b > 0 are powers to adjust term weights.",
                "Usually a = 1 and b = 1 as default, and maxˆt ∈ ˆT (ED(ˆt )) and maxˆt ∈ ˆT (EˆS(ˆt )) are used to normalize the entropy values.",
                "Term cluster weights are used to adjust p(ˆt, ˆs), pw(ˆt, ˆs) = wˆtp(ˆt, ˆs) ˆt∈ ˆT ;ˆs∈ ˆS wˆtp(ˆt, ˆs) , (4) and Iw( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS pw(ˆt, ˆs)log pw(ˆt, ˆs) pw(ˆt)pw(ˆs) , (5) where pw(ˆt) and pw(ˆs) are marginal distributions of pw(ˆt, ˆs).",
                "However, since we do not know either the term weights or P(D, ˆS, T), we need to estimate them, but wˆt depends on p(ˆs|t) and ˆS, while ˆS and p(ˆs|t) also depend on wˆt that is still unknown.",
                "Thus, an iterative algorithm is required to estimate term weights wˆt and find the best segmentation and alignment to optimize the objective function Iw concurrently.",
                "After a document is segmented into sentences Input: Joint probability distribution P(D, Sd, T), number of text segments p ∈ {2, 3, ..., max(sd)}, number of term clusters k ∈ {2, 3, ..., l} (if k = l, no term co-clustering required), and weight type w ∈ {0, 1}, indicating to use I or Iw, respectively.",
                "Output: Mapping Clu, Seg, Ali, and term weights wˆt.",
                "Initialization: 0. i = 0.",
                "Initialize Clu (0) t , Seg (0) d , and Ali (0) d ; Initialize w (0) ˆt using Equation (6) if w = 1; Stage 1: 1.",
                "If |D| = 1, k = l, and w = 0, check all sequential segmentations of d into p segments and find the best one Segd(s) = argmaxˆsI( ˆT; ˆS), and return Segd; otherwise, if w = 1 and k = l, go to 3.1; Stage 2: 2.1 If k < l, for each term t, find the best cluster ˆt as Clu(i+1)(t) = argmaxˆtI( ˆT; ˆS(i)) based on Seg(i) and Ali(i); 2.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+1) d (Seg (i+1) d (s)) = argmaxˆsI( ˆT(i+1); ˆS) based on Clu(i+1)(t) if k < l or Clu(0)(t) if k = l; 2.3 i + +.",
                "If Clu, Seg, or Ali changed, go to 2.1; otherwise, if w = 0, return Clu(i), Seg(i), and Ali(i); else j = 0, go to 3.1; Stage 3: 3.1 Update w (i+j+1) ˆt based on Seg(i+j), Ali(i+j), and Clu(i) using Equation (3); 3.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+j+1) d (Seg (i+j+1) d (s)) = argmaxˆsIw( ˆT(i); ˆS) based on Clu(i) and w (i+j+1) ˆt ; 3.3 j + +.",
                "If Iw( ˆT; ˆS) changes, go to step 6; otherwise, stop and return Clu(i), Seg(i+j), Ali(i+j), and w (i+j) ˆt ; Figure 2: Algorithm: Topic segmentation and alignment based on MI or WMI. and each sentence is segmented into words, each word is stemmed.",
                "Then the joint probability distribution P(D, Sd, T) can be estimated.",
                "Finally, this distribution can be used to compute MI in our algorithm. 4.3 Iterative Greedy Algorithm Our goal is to maximize the objective function, I( ˆT; ˆS) or Iw( ˆT; ˆS), which can measure the dependence of term occurrences in different segments.",
                "Generally, first we do not know the estimate term weights, which depend on the optimal topic segmentation and alignment, and term clusters.",
                "Moreover, this problem is NP-hard [10], even though if we know the term weights.",
                "Thus, an iterative greedy algorithm is desired to find the best solution, even though probably only local maxima are reached.",
                "We present the iterative greedy algorithm in Figure 2 to find a local maximum of I( ˆT; ˆS) or Iw( ˆT; ˆS) with simultaneous term weight estimation.",
                "This algorithm can is iterative and greedy for multi-document cases or single-document cases with term weight estimation and/or term co-clustering.",
                "Otherwise, since it is just a one step algorithm to solve the task of single-document segmentation [6, 15, 25], the global maximum of MI is guaranteed.",
                "We will show later that term co-clustering reduces the accuracy of the results and is not necessary, and for singledocument segmentation, term weights are also not required. 4.3.1 Initialization In Step 0, the initial term clustering Clut and topic segmentation and alignment Segd and Alid are important to avoid local maxima and reduce the number of iterations.",
                "First, a good guess of term weights can be made by using the distributions of term frequency along sentences for each document and averaging them to get the initial values of wˆt: wt = ( ED(t) maxt ∈T (ED(t )) )(1 − ES(t) maxt ∈T (ES(t )) ), (6) where ES(t) = 1 |Dt| d∈Dt (1 − s∈Sd p(s|t)log|Sd| 1 p(s|t) ), where Dt is the set of documents which contain Term t. Then, for the initial segmentation Seg(0) , we can simply segment documents equally by sentences.",
                "Or we can find the optimal segmentation just for each document d which maximizes the WMI, Seg (0) d = argmaxˆsIw(T; ˆS), where w = w (0) ˆt .",
                "For the initial alignment Ali(0) , we can first assume that the order of segments for each d is the same.",
                "For the initial term clustering Clu(0) , first cluster labels can be set randomly, and after the first time of Step 3, a good initial term clustering is obtained. 4.3.2 Different Cases After initialization, there are three stages for different cases.",
                "Totally there are eight cases, |D| = 1 or |D| > 1, k = l or k < l, w = 0 or w = 1.",
                "Single document segmentation without term clustering and term weight estimation (|D| = 1, k = l, w = 0) only requires Stage 1 (Step 1).",
                "If term clustering is required (k < l), Stage 2 (Step 2.1, 2.2, and 2.3) is executed iteratively.",
                "If term weight estimation is required (w = 1), Stage 3 (Step 3.1, 3.2, and 3.3) is executed iteratively.",
                "If both are required (k < l, w = 1), Stage 2 and 3 run one after the other.",
                "For multi-document segmentation without term clustering and term weight estimation (|D| > 1, k = l, w = 0), only iteration of Step 2.2 and 2.3 are required.",
                "At Stage 1, the global maximum can be found based on I( ˆT; ˆS) using dynamic programming in Section 4.4.",
                "Simultaneously finding a good term clustering and estimated term weights is impossible, since when moving a term to a new term cluster to maximize Iw( ˆT; ˆS), we do not know that the weight of this term should be the one of the new cluster or the old cluster.",
                "Thus, we first do term clustering at Stage 2, and then estimate term weights at Stage 3.",
                "At Stage 2, Step 2.1 is to find the best term clustering and Step 2.2 is to find the best segmentation.",
                "This cycle is repeated to find a local maximum based on MI I until it converges.",
                "The two steps are: (1) based on current term clustering Cluˆt, for each document d, the algorithm segments all the sentences Sd into p segments sequentially (some segments may be empty), and put them into the p segments ˆS of the whole training set D (all possible cases of different segmentation Segd and alignment Alid are checked) to find the optimal case, and (2) based on the current segmentation and alignment, for each term t, the algorithm finds the best term cluster of t based on the current segmentation Segd and alignment Alid.",
                "After finding a good term clustering, term weights are estimated if w = 1.",
                "At Stage 3, similar as Stage 2, Step 3.1 is term weight re-estimation and Step 3.2 is to find a better segmentation.",
                "They are repeated to find a local maximum based on WMI Iw until it converges.",
                "However, if the term clustering in Stage 2 is not accurate, then the term weight estimation at Stage 3 may have a bad result.",
                "Finally, at Step 3.3, this algorithm converges and return the output.",
                "This algorithm can handle both single-document and multi-document segmentation.",
                "It also can detect shared topics among documents by checking the proportion of overlapped sentences on the same topics, as described in Sec 5.2. 4.4 Algorithm Optimization In many previous works on segmentation, dynamic programming is a technique used to maximize the objective function.",
                "Similarly, at Step 1, 2.2, and 3.2 of our algorithm, we can use dynamic programming.",
                "For Stage 1, using dynamic programming can still find the global optimum, but for Stage 2 and Stage 3, we can only find the optimum for each step of topic segmentation and alignment of a document.",
                "Here we only show the dynamic programming for Step 3.2 using WMI (Step 1 and 2.2 are similar but they can use either I or Iw).",
                "There are two cases that are not shown in the algorithm in Figure 2: (a) single-document segmentation or multi-document segmentation with the same sequential order of segments, where alignment is not required, and (b) multi-document segmentation with different sequential orders of segments, where alignment is necessary.",
                "The alignment mapping function of the former case is simply just Alid(ˆsi) = ˆsi, while for the latter ones alignment mapping function Alid(ˆsi) = ˆsj, i and j may be different.",
                "The computational steps for the two cases are listed below: Case 1 (no alignment): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs) and partial pw(ˆs) without counting sentences from d. Then put sentences from i to j into Part k, and compute partial WMI PIw( ˆT; ˆsk(si, si+1, ..., sj)) ˆt∈ ˆT pw(ˆt, ˆsk)log pw(ˆt, ˆsk) pw(ˆt)pw(ˆsk) , where Alid(si, si+1, ..., sj) = k, k ∈ {1, 2, ..., p}, 1 ≤ i ≤ j ≤ nd, and Segd(sq) = ˆsk for all i ≤ q ≤ j. (2) Let M(sm, 1) = PIw( ˆT; ˆs1(s1, s2, ..., sm)).",
                "Then M(sm, L) = maxi[M(si−1, L − 1) + PIw( ˆT; ˆsL(si, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, and when i > m, no sentences are put into ˆsk when compute PIw (note PIw( ˆT; ˆs(si, ..., sm)) = 0 for single-document segmentation). (3) Finally M(snd , p) = maxi[M(si−1, p − 1)+ PIw( ˆT; ˆsp(si, ..., snd ))], where 1 ≤ i ≤ nd+1.",
                "The optimal Iw is found and the corresponding segmentation is the best.",
                "Case 2 (alignment required): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs), and partial pw(ˆs), and PIw( ˆT; ˆsk(si, si+1, ..., sj)) similarly as Case 1. (2) Let M(sm, 1, k) = PIw( ˆT; ˆsk(s1, s2, ..., sm)), where k ∈ {1, 2, ..., p}.",
                "Then M(sm, L, kL) = maxi,j[M(si−1, L − 1, kL/j) + PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, kL ∈ Set(p, L), which is the set of all p!",
                "L!(p−L)! combinations of L segments chosen from all p segments, j ∈ kL, the set of L segments chosen from all p segments, and kL/j is the combination of L − 1 segments in kL except Segment j. (3) Finally, M(snd , p, kp) = maxi,j[M(si−1, p − 1, kp/j) +PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., snd ))], where kp is just the combination of all p segments and 1 ≤ i ≤ nd + 1, which is the optimal Iw and the corresponding segmentation is the best.",
                "The steps of Case 1 and 2 are similar, except in Case 2, alignment is considered in addition to segmentation.",
                "First, basic items of probability for computing Iw are computed excluding Doc d, and then partial WMI by putting every possible sequential segment (including empty segment) of d into every segment of the set.",
                "Second, the optimal sum of PIw for L segments and the leftmost m sentences, M(sm, L), is found.",
                "Finally, the maximal WMI is found among different sums of M(sm, p − 1) and PIw for Segment p. 5.",
                "EXPERIMENTS In this section, single-document segmentation, shared topic detection, and multi-document segmentation will be tested.",
                "Different hyper parameters of our method are studied.",
                "For convenience, we refer to the method using I as MIk if w = 0, and Iw as WMIk if w = 2 or as WMIk if w = 1, where k is the number of term clusters, and if k = l, where l is the total number of terms, then no term clustering is required, i.e.",
                "MIl and WMIl. 5.1 Single-document Segmentation 5.1.1 Test Data and Evaluation The first data set we tested is a synthetic one used in previous research [6, 15, 25] and many other papers.",
                "It has 700 samples.",
                "Each is a concatenation of ten segments.",
                "Each segment is the first n sentence selected randomly from the Brown corpus, which is supposed to have a different topic from each other.",
                "Currently, the best results on this data set is achieved by Ji et.al. [15].",
                "To compare the performance of our methods, the criterion used widely in previous research is applied, instead of the unbiased criterion introduced in [20].",
                "It chooses a pair of words randomly.",
                "If they are in different segments (different) for the real segmentation (real), but predicted (pred) as in the same segment, it is a miss.",
                "If they are in the same segment (same), but predicted as in different segments, it is a false alarm.",
                "Thus, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, diff)p(diff|real) +p(false alarm|real, pred, same)p(same|real). 5.1.2 Experiment Results We tested the case when the number of segments is known.",
                "Table 1 shows the results of our methods with different hyper parameter values and three previous approaches, C99[25], U00[6], and ADDP03[15], on this data set when the segment number is known.",
                "In WMI for single-document segmentation, the term weights are computed as follows: wˆt = 1−EˆS(ˆt)/maxˆt ∈ ˆT (EˆS(ˆt )).",
                "For this case, our methods MIl and WMIl both outperform all the previous approaches.",
                "We compared our methods with ADDP03using one-sample one-sided t-test and p-values are shown in Table 2.",
                "From the p-values, we can see that mostly the differences are very Table 1: Average Error Rates of Single-document Segmentation Given Segment Numbers Known Range of n 3-11 3-5 6-8 9-11 Sample size 400 100 100 100 C99 12% 11% 10% 9% U00 10% 9% 7% 5% ADDP03 6.0% 6.8% 5.2% 4.3% MIl 4.68% 5.57% 2.59% 1.59% WMIl 4.94% 6.33% 2.76% 1.62% MI100 9.62% 12.92% 8.66% 6.67% Table 2: Single-document Segmentation: P-values of T-test on Error Rates Range of n 3-11 3-5 6-8 9-11 ADDP03, MIl 0.000 0.000 0.000 0.000 ADDP03, WMIl 0.000 0.099 0.000 0.000 MIl, WMIl 0.061 0.132 0.526 0.898 significant.",
                "We also compare the error rates between our two methods using two-sample two-sided t-test to check the hypothesis that they are equal.",
                "We cannot reject the hypothesis that they are equal, so the difference are not significant, even though all the error rates for MIl are smaller than WMIl.",
                "However, we can conclude that term weights contribute little in single-document segmentation.",
                "The results also show that MI using term co-clustering (k = 100) decreases the performance.",
                "We tested different number of term clusters, and found that the performance becomes better when the cluster number increases to reach l. WMIk<l has similar results that we did not show in the table.",
                "As mentioned before, using MI may be inconsistent on optimal boundaries given different numbers of segments.",
                "This situation occurs especially when the similarities among segments are quite different, i.e. some transitions are very obvious, while others are not.",
                "This is because usually a document is a hierarchical structure instead of only a sequential structure.",
                "When the segments are not at the same level, this situation may occur.",
                "Thus, a hierarchical topic segmentation approach is desired, and the structure highly depends on the number of segments for each internal node and the stop criteria of splitting.",
                "For this data set of singledocument segmentation, since it is just a synthetic set, which is just a concatenation of several segments about different topics, it is reasonable that approaches simply based on term frequency have a good performance.",
                "Usually for the tasks of segmenting coherent documents for sub-topics, the effectiveness decreases much. 5.2 Shared Topic Detection 5.2.1 Test Data and Evaluation The second data set contains 80 news articles from Google News.",
                "There are eight topics and each has 10 articles.",
                "We randomly split the set into subsets with different document numbers and each subset has all eight topics.",
                "We compare our approach MIl and WMIl with LDA [4].",
                "LDA treats a document in the data set as a bag of words, finds its distribution on topics, and its major topic.",
                "MIl and WMIl views each sentence as a bag of words and tag it with a topic label.",
                "Then for each pair of documents, LDA determines if they are on the same topic, while MIl and Table 3: Shared Topic Detection: Average Error Rates for Different Numbers of Documents in Each Subset #Doc 10 20 40 80 LDA 8.89% 16.33% 1.35% 0.60% MIl, θ = 0.6 4.17% 1.71% 1.47% 0.0% WMIl, θ = 0.8 18.6% 3.16% 1.92% 0.0% WMIl check whether the proportion overlapped sentences on the same topic is larger than the adjustable threshold θ.",
                "That is, in MIl and WMIl, for a pair of documents d, d , if [ s∈Sd,s ∈Sd 1(topics=topics )/min(|Sd|, |Sd|)] > θ, where Sd is the set of sentences of d, and |Sd| is the number of sentences of d, then d and d have the shared topic.",
                "For a pair of documents selected randomly, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, same)p(same|real) +p(false alarm|real, pred, diff)p(diff|real), where a miss means if they have the same topic (same) for the real case (real), but predicted (pred) as on the same topic.",
                "If they are on different topics (diff), but predicted as on the same topic, it is a false alarm. 5.2.2 Experiment Results The results are shown in Table 3.",
                "If most documents have different topics, in WMIl, the estimation of term weights in Equation (3) is not correct.",
                "Thus, WMIl is not expected to have a better performance than MIl, when most documents have different topics.",
                "When there are fewer documents in a subset with the same number of topics, more documents have different topics, so WMIl is more worse than MIl.",
                "We can see that for most cases MIl has a better (or at least similar) performance than LDA.",
                "After shared topic detection, multi-document segmentation of documents with the shared topics is able to be executed. 5.3 Multi-document Segmentation 5.3.1 Test Data and Evaluation For multi-document segmentation and alignment, our goal is to identify these segments about the same topic among multiple similar documents with shared topics.",
                "Using Iw is expected to perform better than I, since without term weights the result is affected seriously by document-dependent stop words and noisy words which depends on the personal writing style.",
                "It is more likely to treat the same segments of different documents as different segments under the effect of document-dependent stop words and noisy words.",
                "Term weights can reduce the effect of document-dependent stop words and noisy words by giving <br>cue term</br>s more weights.",
                "The data set for multi-document segmentation and alignment has 102 samples and 2264 sentences totally.",
                "Each is the introduction part of a lab report selected from the course of Biol 240W, Pennsylvania State University.",
                "Each sample has two segments, introduction of plant hormones and the content in the lab.",
                "The length range of samples is from two to 56 sentences.",
                "Some samples only have one part and some have a reverse order the these two segments.",
                "It is not hard to identify the boundary between two segments for human.",
                "We labelled each sentence manually for evaluation.",
                "The criterion of evaluation is just using the proportion of the number of sentences with wrong predicted segment labels in the total number of sentences in the whole training Table 4: Average Error Rates of Multi-document Segmentation Given Segment Numbers Known #Doc MIl WMIl k MIk WMIk 102 3.14% 2.78% 300 4.68% 6.58% 51 4.17% 3.63% 300 17.83% 22.84% 34 5.06% 4.12% 300 18.75% 20.95% 20 7.08% 5.42% 250 20.40% 21.83% 10 10.38% 7.89% 250 21.42% 21.91% 5 15.77% 11.64% 250 21.89% 22.59% 2 25.90% 23.18% 50 25.44% 25.49% 1 23.90% 24.82% 25 25.75% 26.15% Table 5: Multi-document Segmentation: P-values of T-test on Error Rates for MIl and WMIl #Doc 51 34 20 10 5 2 P-value 0.19 0.101 0.025 0.001 0.000 0.002 set as the error rate: p(error|predicted, real) = d∈D s∈Sd 1(predicteds=reals)/ d∈D nd.",
                "In order to show the benefits of multi-document segmentation and alignment, we compared our method with different parameters on different partitions of the same training set.",
                "Except the cases that the number of documents is 102 and one (they are special cases of using the whole set and the pure single-document segmentation), we randomly divided the training set into m partitions, and each has 51, 34, 20, 10, 5, and 2 document samples.",
                "Then we applied our methods on each partition and calculated the error rate of the whole training set.",
                "Each case was repeated for 10 times for computing the average error rates.",
                "For different partitions of the training set, different k values are used, since the number of terms increases when the document number in each partition increases. 5.3.2 Experiment Results From the experiment results in Table 4, we can see the following observations: (1) When the number of documents increases, all methods have better performances.",
                "Only from one to two documents, MIl has decrease a little.",
                "We can observe this from Figure 3 at the point of document number = 2.",
                "Most curves even have the worst results at this point.",
                "There are two reasons.",
                "First, because samples vote for the best multi-document segmentation and alignment, but if only two documents are compared with each other, the one with missing segments or a totally different sequence will affect the correct segmentation and alignment of the other.",
                "Second, as noted at the beginning of this section, if two documents have more document-dependent stop words or noisy words than cue words, then the algorithm may view them as two different segments and the other segment is missing.",
                "Generally, we can only expect a better performance when the number of documents is larger than the number of segments. (2) Except single-document segmentation, WMIl is always better than MIl, and when the number of documents is reaching one or increases to a very large number, their performances become closer.",
                "Table 5 shows p-values of twosample one-sided t-test between MIl and WMIl.",
                "We also can see this trend from p-values.",
                "When document number = 5, we reached the smallest p-value and the largest difference between error rates of MIl and WMIl.",
                "For single-document Table 6: Multi-document Segmentation: Average Error Rate for Document Number = 5 in Each Subset with Different Number of Term Clusters #Cluster 75 100 150 250 l MIk 24.67% 24.54% 23.91% 22.59% 15.77% segmentation, WMIl is even a little bit worse than MIl, which is similar as the results of the single-document segmentation on the first data set.",
                "The reason is that for singledocument segmentation, we cannot estimate term weights accurately, since multiple documents are unavailable. (3) Using term clustering usually gets worse results than MIl and WMIl. (4) Using term clustering in WMIk is even worse than in MIk, since in WMIk term clusters are found first using I before using Iw.",
                "If the term clusters are not correct, then the term weights are estimated worse, which may mislead the algorithm to reach even worse results.",
                "From the results we also found that in multi-document segmentation and alignment, most documents with missing segments and a reverse order are identified correctly.",
                "Table 6 illustrates the experiment results for the case of 20 partitions (each has five document samples) of the training set and topic segmentation and alignment using MIk with different numbers of term clusters k. Notice that when the number of term clusters increases, the error rate becomes smaller.",
                "Without term clustering, we have the best result.",
                "We did not show results for WMIk with term clustering, but the results are similar.",
                "We also tested WMIl with different hyper parameters of a and b to adjust term weights.",
                "The results are presented in Figure 3.",
                "It was shown that the default case WMIl : a = 1, b = 1 gave the best results for different partitions of the training set.",
                "We can see the trend that when the document number is very small or large, the difference between MIl : a = 0, b = 0 and WMIl : a = 1, b = 1 becomes quite small.",
                "When the document number is not large (about from 2 to 10), all the cases using term weights have better performances than MIl : a = 0, b = 0 without term weights, but when the document number becomes larger, the cases WMIl : a = 1, b = 0 and WMIl : a = 2, b = 1 become worse than MIl : a = 0, b = 0.",
                "When the document number becomes very large, they are even worse than cases with small document numbers.",
                "This means that a proper way to estimate term weights for the criterion of WMI is very important.",
                "Figure 4 shows the term weights learned from the whole training set.",
                "Four types of words are categorized roughly even though the transition among them are subtle.",
                "Figure 5 illustrates the change in (weighted) mutual information for MIl and WMIl.",
                "As expected, mutual information for MIl increases monotonically with the number of steps, while WMIl does not.",
                "Finally, MIl and WMIl are scalable, with computational complexity shown in Figure 6.",
                "One advantage for our approach based on MI is that removing stop words is not required.",
                "Another important advantage is that there are no necessary hyper parameters to adjust.",
                "In single-document segmentation, the performance based on MI is even better for that based on WMI, so no extra hyper parameter is required.",
                "In multi-document segmentation, we show in the experiment, a = 1 and b = 1 is the best.",
                "Our method gives more weights to <br>cue term</br>s.",
                "However, usually <br>cue term</br>s or sentences appear at the beginning of a segment, while the end of the segment may be 1 2 5 10 20 34 51 102 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 Document Number ErrorRate MIl :a=0,b=0 WMI l :a=1,b=1 WMI l :a=1,b=0 WMI l :a=2,b=1 Figure 3: Error rates for different hyper parameters of term weights. 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Normalized Document Entropy NormalizedSegmentEntropy Noisy words Cue words Common stop words Doc−dep stop words Figure 4: Term weights learned from the whole training set. 0 100 200 300 400 500 600 0.06 0.08 0.1 0.12 0.14 0.16 0.18 Number of Steps (Weighted)MutualInformation MI l WMI l Figure 5: Change in (weighted) MI for MIl and WMIl. 0 20 40 60 80 100 120 0 200 400 600 800 1000 1200 1400 1600 1800 2000 Document Number TimetoConverge(sec) MI l WMI l Figure 6: Time to converge for MIl and WMIl. much noisy.",
                "One possible solution is giving more weights to terms at the beginning of each segment.",
                "Moreover, when the length of segments are quite different, long segments have much higher term frequencies, so they may dominate the segmentation boundaries.",
                "Normalization of term frequencies versus the segment length may be useful. 6.",
                "CONCLUSIONS AND FUTURE WORK We proposed a novel method for multi-document topic segmentation and alignment based on weighted mutual information, which can also handle single-document cases.",
                "We used dynamic programming to optimize our algorithm.",
                "Our approach outperforms all the previous methods on singledocument cases.",
                "Moreover, we also showed that doing segmentation among multiple documents can improve the performance tremendously.",
                "Our results also illustrated that using weighted mutual information can utilize the information of multiple documents to reach a better performance.",
                "We only tested our method on limited data sets.",
                "More data sets especially complicated ones should be tested.",
                "More previous methods should be compared with.",
                "Moreover, natural segmentations like paragraphs are hints that can be used to find the optimal boundaries.",
                "Supervised learning also can be considered. 7.",
                "ACKNOWLEDGMENTS The authors want to thank Xiang Ji, and Prof. J. Scott Payne for their help. 8.",
                "REFERENCES [1] A. Banerjee, I. Ghillon, J. Ghosh, S. Merugu, and D. Modha.",
                "A generalized maximum entropy approach to bregman co-clustering and matrix approximation.",
                "In Proceedings of SIGKDD, 2004. [2] R. Bekkerman, R. El-Yaniv, and A. McCallum.",
                "Multi-way distributional clustering via pairwise interactions.",
                "In Proceedings of ICML, 2005. [3] D. M. Blei and P. J. Moreno.",
                "Topic segmentation with an aspect hidden markov model.",
                "In Proceedings of SIGIR, 2001. [4] D. M. Blei, A. Ng, and M. Jordan.",
                "Latent dirichlet allocation.",
                "Journal of Machine Learning Research, 3:993-1022, 2003. [5] T. Brants, F. Chen, and I. Tsochantaridis.",
                "Topic-based document segmentation with probabilistic latent semantic analysis.",
                "In Proceedings of CIKM, 2002. [6] F. Choi.",
                "Advances in domain indepedent linear text segmentation.",
                "In Proceedings of the NAACL, 2000. [7] H. Christensen, B. Kolluru, Y. Gotoh, and S. Renals.",
                "Maximum entropy segmentation of broadcast news.",
                "In Proceedings of ICASSP, 2005. [8] T. Cover and J. Thomas.",
                "Elements of Information Theory.",
                "John Wiley and Sons, New York, USA, 1991. [9] S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and R. Harshman.",
                "Indexing by latent semantic analysis.",
                "Journal of the American Society for Information Systems, 1990. [10] I. Dhillon, S. Mallela, and D. Modha.",
                "Information-theoretic co-clustering.",
                "In Proceedings of SIGKDD, 2003. [11] M. Hajime, H. Takeo, and O. Manabu.",
                "Text segmentation with multiple surface linguistic cues.",
                "In Proceedings of COLING-ACL, 1998. [12] T. K. Ho.",
                "Stop word location and identification for adaptive text recognition.",
                "International Journal of Document Analysis and Recognition, 3(1), August 2000. [13] T. Hofmann.",
                "Probabilistic latent semantic analysis.",
                "In Proceedings of the UAI99, 1999. [14] X. Ji and H. Zha.",
                "Correlating summarization of a pair of multilingual documents.",
                "In Proceedings of RIDE, 2003. [15] X. Ji and H. Zha.",
                "Domain-independent text segmentation using anisotropic diffusion and dynamic programming.",
                "In Proceedings of SIGIR, 2003. [16] X. Ji and H. Zha.",
                "Extracting shared topics of multiple documents.",
                "In Proceedings of the 7th PAKDD, 2003. [17] J. Lafferty, A. McCallum, and F. Pereira.",
                "Conditional random fields: Probabilistic models for segmenting and labeling sequence data.",
                "In Proceedings of ICML, 2001. [18] T. Li, S. Ma, and M. Ogihara.",
                "Entropy-based criterion in categorical clustering.",
                "In Proceedings of ICML, 2004. [19] A. McCallum, D. Freitag, and F. Pereira.",
                "Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of ICML, 2000. [20] L. Pevzner and M. Hearst.",
                "A critique and improvement of an evaluation metric for text segmentation.",
                "Computational Linguistic, 28(1):19-36, 2002. [21] J. C. Reynar.",
                "Statistical models for topic segmentation.",
                "In Proceedings of ACL, 1999. [22] G. Salton and M. McGill.",
                "Introduction to Modern Information Retrieval.",
                "McGraw Hill, 1983. [23] B.",
                "Sun, Q. Tan, P. Mitra, and C. L. Giles.",
                "Extraction and search of chemical formulae in text documents on the web.",
                "In Proceedings of WWW, 2007. [24] B.",
                "Sun, D. Zhou, H. Zha, and J.",
                "Yen.",
                "Multi-task text segmentation and alignment based on weighted mutual information.",
                "In Proceedings of CIKM, 2006. [25] M. Utiyama and H. Isahara.",
                "A statistical model for domain-independent text segmentation.",
                "In Proceedings of the 39th ACL, 1999. [26] C. Wayne.",
                "Multilingual topic detection and tracking: Successful research enabled by corpora and evaluation.",
                "In Proceedings of LREC, 2000. [27] J. Yamron, I. Carp, L. Gillick, S. Lowe, and P. van Mulbregt.",
                "A hidden markov model approach to text segmentation and event tracking.",
                "In Proceedings of ICASSP, 1998. [28] H. Zha and X. Ji.",
                "Correlating multilingual documents via bipartite graph modeling.",
                "In Proceedings of SIGIR, 2002."
            ],
            "original_annotated_samples": [
                "Our methods can identify and strengthen <br>cue term</br>s that can be used for segmentation and partially remove stop words by using term weights based on entropy learned from multiple documents.",
                "Term weights can reduce the effect of document-dependent stop words and noisy words by giving <br>cue term</br>s more weights.",
                "Our method gives more weights to <br>cue term</br>s.",
                "However, usually <br>cue term</br>s or sentences appear at the beginning of a segment, while the end of the segment may be 1 2 5 10 20 34 51 102 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 Document Number ErrorRate MIl :a=0,b=0 WMI l :a=1,b=1 WMI l :a=1,b=0 WMI l :a=2,b=1 Figure 3: Error rates for different hyper parameters of term weights. 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Normalized Document Entropy NormalizedSegmentEntropy Noisy words Cue words Common stop words Doc−dep stop words Figure 4: Term weights learned from the whole training set. 0 100 200 300 400 500 600 0.06 0.08 0.1 0.12 0.14 0.16 0.18 Number of Steps (Weighted)MutualInformation MI l WMI l Figure 5: Change in (weighted) MI for MIl and WMIl. 0 20 40 60 80 100 120 0 200 400 600 800 1000 1200 1400 1600 1800 2000 Document Number TimetoConverge(sec) MI l WMI l Figure 6: Time to converge for MIl and WMIl. much noisy."
            ],
            "translated_annotated_samples": [
                "Nuestros métodos pueden identificar y fortalecer <br>términos de señal</br> que pueden ser utilizados para la segmentación y eliminar parcialmente las palabras de parada mediante el uso de pesos de términos basados en la entropía aprendida de múltiples documentos.",
                "Los pesos de términos pueden reducir el efecto de las palabras de parada dependientes del documento y las palabras ruidosas al darle más peso a los <br>términos de señal</br>.",
                "Nuestro método otorga más peso a los <br>términos de señal</br>.",
                "Sin embargo, generalmente los términos o frases de señal aparecen al principio de un segmento, mientras que el final del segmento puede ser 1 2 5 10 20 34 51 102 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 Número de Documento Tasa de Error MIl :a=0,b=0 WMI l :a=1,b=1 WMI l :a=1,b=0 WMI l :a=2,b=1 Figura 3: Tasas de error para diferentes hiperparámetros de pesos de términos. 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Entropía de Documento Normalizada Entropía de Segmento Normalizada Palabras ruidosas Palabras de señal Palabras de parada comunes Palabras de parada de Doc-dep Figura 4: Pesos de términos aprendidos de todo el conjunto de entrenamiento. 0 100 200 300 400 500 600 0.06 0.08 0.1 0.12 0.14 0.16 0.18 Número de Pasos (Ponderado) Información Mutua MI l WMI l Figura 5: Cambio en la MI (ponderada) para MIl y WMIl. 0 20 40 60 80 100 120 0 200 400 600 800 1000 1200 1400 1600 1800 2000 Número de Documento Tiempo de Convergencia (seg) MI l WMI l Figura 6: Tiempo de convergencia para MIl y WMIl. muy ruidoso."
            ],
            "translated_text": "La segmentación de temas con detección de temas compartidos y alineación de múltiples documentos Bingjun Sun*, Prasenjit Mitra*†, Hongyuan Zha‡, C. Lee Giles*†, John Yen*† *Departamento de Ciencias de la Computación e Ingeniería † Colegio de Ciencias de la Información y Tecnología Universidad Estatal de Pensilvania University Park, PA 16802 ‡ Colegio de Computación Instituto de Tecnología de Georgia Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu RESUMEN La detección y seguimiento de temas [26] y la segmentación de temas [15] juegan un papel importante en la captura de la información local y secuencial de los documentos. El trabajo previo en esta área suele centrarse en documentos individuales, aunque existen múltiples documentos similares en muchos dominios. En este artículo, presentamos un novedoso método no supervisado para la detección de temas compartidos y la segmentación de temas de múltiples documentos similares basado en la información mutua (MI) y la información mutua ponderada (WMI), que es una combinación de MI y pesos de términos. La idea básica es que la segmentación óptima maximiza MI (o WMI). Nuestro enfoque puede detectar temas compartidos entre documentos. Puede encontrar los límites óptimos en un documento y alinear segmentos entre documentos al mismo tiempo. También puede manejar la segmentación de un solo documento como un caso especial de la segmentación y alineación de varios documentos. Nuestros métodos pueden identificar y fortalecer <br>términos de señal</br> que pueden ser utilizados para la segmentación y eliminar parcialmente las palabras de parada mediante el uso de pesos de términos basados en la entropía aprendida de múltiples documentos. Nuestros resultados experimentales muestran que nuestro algoritmo funciona bien para las tareas de segmentación de un solo documento, detección de temas compartidos y segmentación de múltiples documentos. El uso de información de múltiples documentos puede mejorar enormemente el rendimiento de la segmentación de temas, y utilizar WMI es aún mejor que utilizar MI para la segmentación de múltiples documentos. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información-Agrupación; H.3.1 [Almacenamiento y Recuperación de Información]: Análisis de Contenido e Indexación-Procesamiento Lingüístico; I.2.7 [Inteligencia Artificial]: Procesamiento de Lenguaje Natural-Análisis de Texto; I.5.3 [Reconocimiento de Patrones]: Algoritmos de Agrupación-Medidas de Similitud Términos Generales Algoritmos, Diseño, Experimentación 1. INTRODUCCIÓN Muchos investigadores han trabajado en la detección y seguimiento de temas (TDT) [26] y la segmentación de temas durante la última década. La segmentación de temas tiene como objetivo identificar los límites en un documento con la meta de capturar la estructura temática latente. Las tareas de segmentación de temas generalmente se dividen en dos categorías [15]: la segmentación de flujos de texto, donde se identifica la transición de temas, y la segmentación coherente de documentos, en la que los documentos se dividen en subtemas. La primera categoría tiene aplicaciones en el reconocimiento automático del habla, mientras que la segunda tiene más aplicaciones como la consulta parcial de texto en documentos extensos en la recuperación de información, resumen de texto y medición de calidad de múltiples documentos. La investigación previa en relación con TDT se enmarca en la primera categoría, centrada en el seguimiento de temas de datos de discurso de transmisión y texto de agencia de noticias, mientras que la segunda categoría no ha sido estudiada muy bien. Los enfoques tradicionales realizan la segmentación de temas en los documentos uno a la vez [15, 25, 6]. La mayoría de ellos tienen un mal desempeño en tareas sutiles como la segmentación coherente de documentos [15]. A menudo, los usuarios finales buscan documentos que tengan un contenido similar. Los motores de búsqueda, como Google, proporcionan enlaces para obtener páginas similares. A una granularidad más fina, los usuarios pueden estar buscando obtener secciones de un documento similares a una sección en particular que presumiblemente discute un tema de interés para los usuarios. Por lo tanto, la extensión de la segmentación de temas desde documentos individuales para identificar segmentos similares de múltiples documentos similares con el mismo tema es una dirección natural y necesaria, y se espera que la segmentación de temas de múltiples documentos tenga un mejor rendimiento ya que se utiliza más información. Los enfoques tradicionales que utilizan la medición de similitud basada en la frecuencia de términos generalmente parten del mismo supuesto de que un vocabulario similar tiende a estar en un segmento de tema coherente [15, 25, 6]. Sin embargo, suelen tener el problema de identificar las palabras de parada. Por ejemplo, se eliminan las palabras de parada dependientes del documento junto con las palabras de parada genéricas en [15]. Hay dos razones por las que no eliminamos las palabras de parada directamente. Primero, identificar las palabras vacías es otro problema [12] que requiere estimación en cada dominio. Eliminar palabras comunes de parada puede resultar en la pérdida de información útil en un dominio específico. Segundo, aunque se pueden identificar las palabras de parada, la clasificación estricta de palabras de parada y palabras no de parada no puede representar el cambio gradual en la cantidad de contenido informativo de cada palabra. Empleamos una clasificación suave utilizando pesos de términos. En este documento, consideramos el problema de la segmentación de temas como un problema de optimización utilizando técnicas de teoría de la información para encontrar los límites óptimos de un documento dado el número de segmentos de texto, de manera que se minimice la pérdida de información mutua (MI) (o información mutua ponderada (WMI)) después de la segmentación y alineación. Esto es igual a maximizar el MI (o WMI). El MI se centra en medir la diferencia entre segmentos, mientras que investigaciones anteriores se enfocaban en encontrar la similitud (por ejemplo, la distancia del coseno) entre segmentos [15, 25, 6]. La alineación de temas de múltiples documentos similares se puede lograr agrupando las oraciones sobre el mismo tema en el mismo clúster. La segmentación de temas en un solo documento es solo un caso especial del problema de segmentación y alineación de temas en múltiples documentos. Los términos pueden ser co-agrupados como en [10] al mismo tiempo, dado el número de agrupaciones, pero nuestros resultados experimentales muestran que este método resulta en una peor segmentación (ver Tablas 1, 4 y 6). Por lo general, los lectores humanos pueden identificar la transición de temas basándose en palabras de señal, y pueden ignorar las palabras vacías. Inspirados por esto, asignamos a cada término (o grupo de términos) un peso basado en la entropía entre diferentes documentos y diferentes segmentos de documentos. No solo este enfoque puede aumentar la contribución de las palabras clave, sino que también puede disminuir el efecto de las palabras de parada comunes, palabras ruidosas y palabras de parada dependientes del documento. Estas palabras son comunes en un documento. Muchos métodos basados en la similitud de oraciones requieren que estas palabras sean eliminadas antes de que se pueda realizar la segmentación de temas [15]. Nuestros resultados en la Figura 3 muestran que los pesos de los términos son útiles para la segmentación y alineación de temas en múltiples documentos. La principal contribución de este artículo es que introduce un método novedoso para la segmentación de temas utilizando MI y demuestra que este método funciona mejor que los criterios previamente utilizados. Además, hemos abordado el problema de la segmentación y alineación de temas en varios documentos, mientras que la mayoría de las investigaciones existentes se centraron en la segmentación de documentos individuales. La segmentación y alineación de múltiples documentos pueden utilizar información de documentos similares y mejorar significativamente el rendimiento de la segmentación de temas. Obviamente, nuestro enfoque puede manejar documentos individuales como un caso especial cuando varios documentos no están disponibles. Puede detectar temas compartidos entre documentos para determinar si son múltiples documentos sobre el mismo tema. También introducimos el nuevo criterio de WMI basado en pesos de términos aprendidos de múltiples documentos similares, lo cual puede mejorar aún más el rendimiento de la segmentación de temas. Proponemos un algoritmo voraz iterativo basado en programación dinámica y demostramos que funciona bien en la práctica. Algunos de nuestros trabajos anteriores están en [24]. El resto de este documento está organizado de la siguiente manera: En la Sección 2, revisamos el trabajo relacionado. La sección 3 contiene una formulación del problema de segmentación de temas y alineación de múltiples documentos con co-clustering de términos, una revisión del criterio de MI para el clustering, y finalmente una introducción a WMI. En la Sección 4, primero proponemos el algoritmo voraz iterativo de segmentación y alineación de temas con co-clustering de términos, y luego describimos cómo el algoritmo puede ser optimizado mediante programación dinámica. En la Sección 5, se describen experimentos sobre la segmentación de documentos individuales, la detección de temas compartidos y la segmentación de documentos múltiples, y se presentan y discuten los resultados para evaluar el rendimiento de nuestro algoritmo. Las conclusiones y algunas direcciones futuras del trabajo de investigación se discuten en la Sección 6.2. TRABAJO PREVIO En general, los enfoques existentes para la segmentación de texto se dividen en dos categorías: aprendizaje supervisado [19, 17, 23] y aprendizaje no supervisado [3, 27, 5, 6, 15, 25, 21]. El aprendizaje supervisado suele tener un buen rendimiento, ya que aprende funciones a partir de conjuntos de entrenamiento etiquetados. Sin embargo, obtener conjuntos de entrenamiento grandes con etiquetas manuales en oraciones de documentos suele ser prohibitivamente costoso, por lo que se prefieren enfoques no supervisados. Algunos modelos consideran la dependencia entre oraciones y secciones, como el Modelo Oculto de Markov [3, 27], el Modelo de Máxima Entropía de Markov [19] y los Campos Aleatorios Condicionales [17], mientras que muchos otros enfoques se basan en la cohesión léxica o la similitud de las oraciones [5, 6, 15, 25, 21]. Algunos enfoques también se centran en las palabras clave como pistas de transiciones de tema [11]. Si bien algunos métodos existentes solo consideran información en documentos individuales [6, 15], otros utilizan varios documentos [16, 14]. No hay muchas obras en la última categoría, aunque se espera que el rendimiento de la segmentación sea mejor con la utilización de información de múltiples documentos. Investigaciones previas estudiaron métodos para encontrar temas compartidos [16] y segmentación y resumen de temas entre solo un par de documentos [14]. La clasificación y agrupación de textos es un área de investigación relacionada que categoriza documentos en grupos utilizando métodos supervisados o no supervisados. La clasificación o agrupación temática es una dirección importante en esta área, especialmente el co-agrupamiento de documentos y términos, como LSA [9], PLSA [13], y enfoques basados en distancias y particionamiento de gráficos bipartitos [28] o máxima MI [2, 10], o máxima entropía [1, 18]. Los criterios de estos enfoques pueden ser utilizados en el tema de la segmentación de temas. Algunos de esos métodos se han extendido al área de la segmentación de temas, como PLSA [5] y máxima entropía [7], pero hasta donde sabemos, el uso de MI para la segmentación de temas no ha sido estudiado. 3. FORMULACIÓN DEL PROBLEMA Nuestro objetivo es segmentar documentos y alinear los segmentos entre documentos (Figura 1). Sea T el conjunto de términos {t1, t2, ..., tl}, que aparecen en el conjunto no etiquetado de documentos D = {d1, d2, ..., dm}. Sea Sd el conjunto de oraciones para el documento d ∈ D, es decir, {s1, s2, ..., snd}. Tenemos una matriz tridimensional de frecuencia de términos, en la que las tres dimensiones son variables aleatorias de D, Sd y T. Sd en realidad es un vector aleatorio que incluye una variable aleatoria para cada d ∈ D. La frecuencia de términos se puede utilizar para estimar la distribución de probabilidad conjunta P(D, Sd, T), que es p(t, d, s) = T(t, d, s)/ND, donde T(t, d, s) es el número de t en la oración s de d y ND es el número total de términos en D. ˆS representa el conjunto de segmentos {ˆs1, ˆs2, ..., ˆsp} después de la segmentación y alineación entre varios documentos, donde el número de segmentos | ˆS| = p. Un segmento ˆsi del documento d es una secuencia de oraciones adyacentes en d. Dado que para diferentes documentos si pueden discutir diferentes subtemas, nuestro objetivo es agrupar oraciones adyacentes en cada documento en segmentos, y alinear segmentos similares entre documentos, de modo que para diferentes documentos ˆsi trate sobre el mismo subtema. El objetivo es encontrar la segmentación óptima de temas y el mapeo de alineación Segd(si) : {s1, s2, ..., snd } → {ˆs1, ˆs2, ..., ˆsp} y Alid(ˆsi) : {ˆs1, ˆs2, ..., ˆsp} → {ˆs1, ˆs2, ..., ˆsp}, para todo d ∈ D, donde ˆsi es el i-ésimo segmento con la restricción de que solo las oraciones adyacentes pueden ser asignadas al mismo segmento, es decir, para d, {si, si+1, ..., sj} → {ˆsq}, donde q ∈ {1, ..., p}, donde p es el número de segmento, y si i > j, entonces para d, ˆsq está ausente. Después de la segmentación y alineación, el vector aleatorio Sd se convierte en una variable aleatoria alineada ˆS. Por lo tanto, P(D, Sd, T) se convierte en P(D, ˆS, T). El término co-clustering es una técnica que se ha empleado [10] para mejorar la precisión del agrupamiento de documentos. Evaluamos el efecto de ello para la segmentación de temas. Un término t se asigna a exactamente un grupo de términos. El término co-clustering implica encontrar simultáneamente el mapeo óptimo de agrupación de términos Clu(t) : {t1, t2, ..., tl} → {ˆt1, ˆt2, ..., ˆtk}, donde k ≤ l, l es el número total de palabras en todos los documentos, y k es el número de clústeres. 4. METODOLOGÍA Ahora describimos un algoritmo novedoso que puede manejar la segmentación de un solo documento, la detección de temas compartidos y la segmentación y alineación de múltiples documentos basados en MI o WMI. 4.1 Información Mutua MI I(X; Y) es una cantidad que mide la cantidad de información contenida en dos o más variables aleatorias [8, 10]. Para el caso de dos variables aleatorias, tenemos I(X; Y) = x∈X y∈Y p(x, y)log p(x, y) p(x)p(y) , (1) Obviamente, cuando las variables aleatorias X e Y son independientes, I(X; Y) = 0. Por lo tanto, de manera intuitiva, el valor de MI depende de cómo las variables aleatorias están relacionadas entre sí. La co-agrupación óptima es el mapeo Clux: X → ˆX y Cluy: Y → ˆY que minimiza la pérdida: I(X; Y) - I(ˆX; ˆY), lo cual es igual a maximizar I(ˆX; ˆY). Este es el criterio de MI para la agrupación. En el caso de la segmentación de temas, las dos variables aleatorias son la variable de término T y la variable de segmento S, y cada muestra es una ocurrencia de un término T = t en un segmento particular S = s. Se utiliza I(T; S) para medir qué tan dependientes son T y S. Sin embargo, no se puede calcular I(T; S) para documentos antes de la segmentación, ya que no tenemos un conjunto de S debido a que las oraciones del Documento d, si ∈ Sd, no están alineadas con otros documentos. Así, en lugar de minimizar la pérdida de MI, podemos maximizar MI después de la segmentación de temas, calculada como: I( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS p(ˆt, ˆs)log p(ˆt, ˆs) p(ˆt)p(ˆs) , (2) donde p(ˆt, ˆs) se estiman mediante la frecuencia de términos tf del Cluster de Términos ˆt y el Segmento ˆs en el conjunto de entrenamiento D. Nótese que aquí un segmento ˆs incluye oraciones sobre el mismo tema entre todos los documentos. La solución óptima es el mapeo Clut : T → ˆT, Segd : Sd → ˆS, y Alid : ˆS → ˆS, que maximiza I( ˆT; ˆS). 4.2 Información Mutua Ponderada En la segmentación de temas y alineación de múltiples documentos, si se conoce P(D, ˆS, T), basado en las distribuciones marginales P(D|T) y P( ˆS|T) para cada término t ∈ T, podemos categorizar los términos en cuatro tipos en el conjunto de datos: • Las palabras de parada comunes son comunes a lo largo de las dimensiones de documentos y segmentos. • Las palabras de parada dependientes del documento que dependen del estilo de escritura personal son comunes solo a lo largo de la dimensión de segmentos para algunos documentos. • Las palabras clave son los elementos más importantes para la segmentación. Son comunes a lo largo de la dimensión de los documentos solo para el mismo segmento, y no son comunes a lo largo de las dimensiones de los segmentos. • Las palabras ruidosas son otras palabras que no son comunes a lo largo de ambas dimensiones. La entropía basada en P(D|T) y P( ˆS|T) se puede utilizar para identificar diferentes tipos de términos. Para reforzar la contribución de las palabras clave en el cálculo de MI, y al mismo tiempo reducir el efecto de los otros tres tipos de palabras, similar a la idea del peso tf-idf [22], utilizamos las entropías de cada término a lo largo de las dimensiones del documento D y del segmento ˆS, es decir, ED(ˆt) y EˆS(ˆt), para calcular el peso. Una palabra de señal generalmente tiene un valor alto de ED(ˆt) pero un valor bajo de EˆS(ˆt). Introducimos los pesos de términos (o pesos de grupos de términos) wˆt = ( ED(ˆt) maxˆt ∈ ˆT (ED(ˆt )) )a (1 − EˆS(ˆt) maxˆt ∈ ˆT (EˆS(ˆt )) )b , (3) donde ED(ˆt) = d∈D p(d|ˆt)log|D| 1 p(d|ˆt) , EˆS(ˆt) = ˆs∈ ˆS p(ˆs|ˆt)log| ˆS| 1 p(ˆs|ˆt) , y a > 0 y b > 0 son potencias para ajustar los pesos de términos. Normalmente a = 1 y b = 1 por defecto, y se utilizan maxˆt ∈ ˆT (ED(ˆt )) y maxˆt ∈ ˆT (EˆS(ˆt )) para normalizar los valores de entropía. Los pesos de los grupos de términos se utilizan para ajustar p(ˆt, ˆs), pw(ˆt, ˆs) = wˆtp(ˆt, ˆs) ˆt∈ ˆT ;ˆs∈ ˆS wˆtp(ˆt, ˆs) , (4) e Iw( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS pw(ˆt, ˆs)log pw(ˆt, ˆs) pw(ˆt)pw(ˆs) , (5) donde pw(ˆt) y pw(ˆs) son distribuciones marginales de pw(ˆt, ˆs). Sin embargo, dado que no conocemos ni los pesos de los términos ni P(D, ˆS, T), necesitamos estimarlos, pero wˆt depende de p(ˆs|t) y ˆS, mientras que ˆS y p(ˆs|t) también dependen de wˆt, que aún es desconocido. Por lo tanto, se requiere un algoritmo iterativo para estimar los pesos de los términos wˆt y encontrar la mejor segmentación y alineación para optimizar la función objetivo Iw de manera concurrente. Después de que un documento se segmenta en oraciones, se introduce: la distribución de probabilidad conjunta P(D, Sd, T), el número de segmentos de texto p ∈ {2, 3, ..., máx(sd)}, el número de grupos de términos k ∈ {2, 3, ..., l} (si k = l, no se requiere co-clustering de términos) y el tipo de peso w ∈ {0, 1}, indicando usar I o Iw, respectivamente. Mapeo de Clu, Seg, Ali y pesos de términos wˆt. Inicialización: 0. i = 0. Inicializar Clu (0) t, Seg (0) d y Ali (0) d; Inicializar w (0) ˆt usando la Ecuación (6) si w = 1; Etapa 1: 1. Si |D| = 1, k = l y w = 0, verifica todas las segmentaciones secuenciales de d en p segmentos y encuentra la mejor Segd(s) = argmaxˆsI( ˆT; ˆS), y devuelve Segd; de lo contrario, si w = 1 y k = l, ve a 3.1; Etapa 2: 2.1 Si k < l, para cada término t, encuentra el mejor clúster ˆt como Clu(i+1)(t) = argmaxˆtI( ˆT; ˆS(i)) basado en Seg(i) y Ali(i); 2.2 Para cada d, verifica todas las segmentaciones secuenciales de d en p segmentos con mapeo s → ˆs → ˆs, y encuentra la mejor Ali (i+1) d (Seg (i+1) d (s)) = argmaxˆsI( ˆT(i+1); ˆS) basado en Clu(i+1)(t) si k < l o Clu(0)(t) si k = l; 2.3 i + +. Si Clu, Seg o Ali cambian, ve a 2.1; de lo contrario, si w = 0, devuelve Clu(i), Seg(i) y Ali(i); de lo contrario, j = 0, ve a 3.1; Etapa 3: 3.1 Actualiza w (i+j+1) ˆt basado en Seg(i+j), Ali(i+j) y Clu(i) usando la Ecuación (3); 3.2 Para cada d, verifica todas las segmentaciones secuenciales de d en p segmentos con mapeo s → ˆs → ˆs, y encuentra la mejor Ali (i+j+1) d (Seg (i+j+1) d (s)) = argmaxˆsIw( ˆT(i); ˆS) basado en Clu(i) y w (i+j+1) ˆt; 3.3 j + +. Si Iw( ˆT; ˆS) cambia, ve al paso 6; de lo contrario, detente y devuelve Clu(i), Seg(i+j), Ali(i+j), y w (i+j) ˆt; Figura 2: Algoritmo: Segmentación y alineación de temas basada en MI o WMI. y cada oración se segmenta en palabras, cada palabra se reduce a su raíz. Entonces se puede estimar la distribución de probabilidad conjunta P(D, Sd, T). Finalmente, esta distribución se puede utilizar para calcular la MI en nuestro algoritmo. 4.3 Algoritmo Voraz Iterativo Nuestro objetivo es maximizar la función objetivo, I( ˆT; ˆS) o Iw( ˆT; ˆS), que puede medir la dependencia de las ocurrencias de términos en diferentes segmentos. Generalmente, primero no conocemos los pesos estimados de los términos, los cuales dependen de la segmentación y alineación óptimas de los temas, y los grupos de términos. Además, este problema es NP-duro [10], incluso si conocemos los pesos de los términos. Por lo tanto, se desea un algoritmo voraz iterativo para encontrar la mejor solución, aunque probablemente solo se alcancen máximos locales. Presentamos el algoritmo voraz iterativo en la Figura 2 para encontrar un máximo local de I( ˆT; ˆS) o Iw( ˆT; ˆS) con estimación simultánea de pesos de términos. Este algoritmo es iterativo y codicioso para casos de múltiples documentos o casos de un solo documento con estimación de peso de términos y/o co-clustering de términos. De lo contrario, dado que es solo un algoritmo de un paso para resolver la tarea de segmentación de un solo documento [6, 15, 25], se garantiza el máximo global de MI. Mostraremos más adelante que la coagrupación de términos reduce la precisión de los resultados y no es necesaria, y para la segmentación de un solo documento, los pesos de los términos tampoco son requeridos. 4.3.1 Inicialización En el Paso 0, la agrupación inicial de términos Clut y la segmentación y alineación de temas Segd y Alid son importantes para evitar máximos locales y reducir el número de iteraciones. Primero, se puede hacer una buena estimación de los pesos de los términos utilizando las distribuciones de frecuencia de términos a lo largo de las oraciones para cada documento y promediándolas para obtener los valores iniciales de wˆt: wt = ( ED(t) maxt ∈T (ED(t )) )(1 − ES(t) maxt ∈T (ES(t )) ), (6) donde ES(t) = 1 |Dt| d∈Dt (1 − s∈Sd p(s|t)log|Sd| 1 p(s|t) ), donde Dt es el conjunto de documentos que contienen el término t. Luego, para la segmentación inicial Seg(0), simplemente podemos segmentar los documentos de manera equitativa por oraciones. O podemos encontrar la segmentación óptima solo para cada documento d que maximice el WMI, Seg (0) d = argmaxˆsIw(T; ˆS), donde w = w (0) ˆt. Para el alineamiento inicial Ali(0), podemos primero asumir que el orden de los segmentos para cada d es el mismo. Para el agrupamiento inicial de términos Clu(0), las etiquetas de los primeros grupos pueden establecerse al azar, y después de la primera vez del Paso 3, se obtiene un buen agrupamiento inicial de términos. 4.3.2 Diferentes Casos Después de la inicialización, hay tres etapas para diferentes casos. En total hay ocho casos, |D| = 1 o |D| > 1, k = l o k < l, w = 0 o w = 1. La segmentación de un solo documento sin agrupamiento de términos y estimación de peso de términos (|D| = 1, k = l, w = 0) solo requiere la Etapa 1 (Paso 1). Si se requiere el agrupamiento de términos (k < l), la Etapa 2 (Paso 2.1, 2.2 y 2.3) se ejecuta de forma iterativa. Si se requiere la estimación del peso del término (w = 1), la Etapa 3 (Paso 3.1, 3.2 y 3.3) se ejecuta de forma iterativa. Si ambos son requeridos (k < l, w = 1), la Etapa 2 y 3 se ejecutan una tras otra. Para la segmentación de múltiples documentos sin agrupamiento de términos y estimación de peso de términos (|D| > 1, k = l, w = 0), solo se requiere la iteración de los Pasos 2.2 y 2.3. En la Etapa 1, el máximo global se puede encontrar basándose en I( ˆT; ˆS) utilizando programación dinámica en la Sección 4.4. Es imposible encontrar simultáneamente un buen agrupamiento de términos y pesos estimados de términos, ya que al mover un término a un nuevo grupo de términos para maximizar Iw( ˆT; ˆS), no sabemos si el peso de este término debería ser el del nuevo grupo o el del antiguo. Por lo tanto, primero realizamos el agrupamiento de términos en la Etapa 2, y luego estimamos los pesos de los términos en la Etapa 3. En la Etapa 2, el Paso 2.1 consiste en encontrar el mejor agrupamiento de términos y el Paso 2.2 consiste en encontrar la mejor segmentación. Este ciclo se repite para encontrar un máximo local basado en MI I hasta que converge. Los dos pasos son: (1) basados en el agrupamiento de términos actual Cluˆt, para cada documento d, el algoritmo segmenta todas las oraciones Sd en p segmentos secuencialmente (algunos segmentos pueden estar vacíos), y los coloca en los p segmentos ˆS del conjunto de entrenamiento completo D (se verifican todos los casos posibles de segmentación diferente Segd y alineación Alid) para encontrar el caso óptimo, y (2) basado en la segmentación y alineación actual, para cada término t, el algoritmo encuentra el mejor grupo de términos de t basado en la segmentación actual Segd y la alineación Alid. Después de encontrar un buen agrupamiento de términos, se estiman los pesos de los términos si w = 1. En la Etapa 3, al igual que en la Etapa 2, el Paso 3.1 es la reestimación del peso del término y el Paso 3.2 es encontrar una mejor segmentación. Se repiten para encontrar un máximo local basado en WMI Iw hasta que converja. Sin embargo, si el agrupamiento en la Etapa 2 no es preciso, entonces la estimación de peso en la Etapa 3 puede tener un mal resultado. Finalmente, en el Paso 3.3, este algoritmo converge y devuelve la salida. Este algoritmo puede manejar tanto la segmentación de un solo documento como la de múltiples documentos. También puede detectar temas compartidos entre documentos al verificar la proporción de frases superpuestas sobre los mismos temas, como se describe en la Sección 5.2. 4.4 Optimización del algoritmo. En muchos trabajos anteriores sobre segmentación, la programación dinámica es una técnica utilizada para maximizar la función objetivo. De manera similar, en los Pasos 1, 2.2 y 3.2 de nuestro algoritmo, podemos utilizar programación dinámica. Para la Etapa 1, utilizando programación dinámica aún se puede encontrar el óptimo global, pero para la Etapa 2 y la Etapa 3, solo podemos encontrar el óptimo para cada paso de segmentación de temas y alineación de un documento. Aquí solo mostramos la programación dinámica para el Paso 3.2 utilizando WMI (el Paso 1 y 2.2 son similares pero pueden usar I o Iw). Hay dos casos que no se muestran en el algoritmo de la Figura 2: (a) segmentación de un solo documento o segmentación de múltiples documentos con el mismo orden secuencial de segmentos, donde no se requiere alineación, y (b) segmentación de múltiples documentos con diferentes órdenes secuenciales de segmentos, donde la alineación es necesaria. La función de mapeo de alineación del primer caso es simplemente Alid(ˆsi) = ˆsi, mientras que para los últimos casos la función de mapeo de alineación es Alid(ˆsi) = ˆsj, donde i y j pueden ser diferentes. Los pasos computacionales para los dos casos se enumeran a continuación: Caso 1 (sin alineación): Para cada documento d: (1) Calcular pw(ˆt), pw parcial(ˆt, ˆs) y pw parcial(ˆs) sin contar las oraciones de d. Luego, colocar las oraciones de i a j en la Parte k, y calcular WMI parcial PIw( ˆT; ˆsk(si, si+1, ..., sj)) ˆt∈ ˆT pw(ˆt, ˆsk)log pw(ˆt, ˆsk) pw(ˆt)pw(ˆsk) , donde Alid(si, si+1, ..., sj) = k, k ∈ {1, 2, ..., p}, 1 ≤ i ≤ j ≤ nd, y Segd(sq) = ˆsk para todo i ≤ q ≤ j. (2) Dejar que M(sm, 1) = PIw( ˆT; ˆs1(s1, s2, ..., sm)). Entonces M(sm, L) = maxi[M(si−1, L − 1) + PIw( ˆT; ˆsL(si, ..., sm))], donde 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, y cuando i > m, no se colocan oraciones en ˆsk al calcular PIw (nota PIw( ˆT; ˆs(si, ..., sm)) = 0 para la segmentación de un solo documento). (3) Finalmente M(snd , p) = maxi[M(si−1, p − 1)+ PIw( ˆT; ˆsp(si, ..., snd ))], donde 1 ≤ i ≤ nd+1. Se encuentra el Iw óptimo y la segmentación correspondiente es la mejor. Caso 2 (alineación requerida): Para cada documento d: (1) Calcular pw(ˆt), pw parcial(ˆt, ˆs), y pw parcial(ˆs), y PIw( ˆT; ˆsk(si, si+1, ..., sj)) de manera similar al Caso 1. (2) Sea M(sm, 1, k) = PIw( ˆT; ˆsk(s1, s2, ..., sm)), donde k ∈ {1, 2, ..., p}. Entonces M(sm, L, kL) = maxi,j[M(si−1, L − 1, kL/j) + PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., sm))], donde 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, kL ∈ Set(p, L), que es el conjunto de todos los p! Hay L!(p−L)! combinaciones de L segmentos elegidos de entre todos los p segmentos, j ∈ kL, el conjunto de L segmentos elegidos de entre todos los p segmentos, y kL/j es la combinación de L − 1 segmentos en kL excepto el Segmento j. (3) Finalmente, M(snd , p, kp) = maxi,j[M(si−1, p − 1, kp/j) +PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., snd ))], donde kp es simplemente la combinación de todos los p segmentos y 1 ≤ i ≤ nd + 1, que es el óptimo Iw y la segmentación correspondiente es la mejor. Los pasos de los Casos 1 y 2 son similares, excepto que en el Caso 2, se considera el alineamiento además de la segmentación. Primero, se calculan los elementos básicos de probabilidad para computar Iw excluyendo el documento d, y luego se calcula el WMI parcial colocando cada segmento secuencial posible (incluido el segmento vacío) de d en cada segmento del conjunto. Segundo, se encuentra la suma óptima de PIw para L segmentos y las m oraciones más a la izquierda, M(sm, L). Finalmente, se encuentra el WMI máximo entre diferentes sumas de M(sm, p − 1) y PIw para el Segmento p. 5. EXPERIMENTOS En esta sección se probará la segmentación de un solo documento, la detección de temas compartidos y la segmentación de múltiples documentos. Se estudian diferentes hiperparámetros de nuestro método. Para mayor comodidad, nos referimos al método utilizando I como MIk si w = 0, e Iw como WMIk si w = 2 o como WMIk si w = 1, donde k es el número de grupos de términos, y si k = l, donde l es el número total de términos, entonces no se requiere agrupación de términos, es decir, El primer conjunto de datos que probamos es uno sintético utilizado en investigaciones anteriores [6, 15, 25] y muchos otros artículos. Tiene 700 muestras. Cada uno es una concatenación de diez segmentos. Cada segmento es la primera oración seleccionada al azar de las n primeras oraciones del corpus Brown, que se supone que tienen un tema diferente entre sí. Actualmente, los mejores resultados en este conjunto de datos son logrados por Ji et al. [15]. Para comparar el rendimiento de nuestros métodos, se aplica el criterio ampliamente utilizado en investigaciones anteriores en lugar del criterio imparcial introducido en [20]. Elige un par de palabras al azar. Si se encuentran en segmentos diferentes (diferentes) para la segmentación real (real), pero se predicen (pred) como en el mismo segmento, es un error. Si están en el mismo segmento (mismo), pero se predice que están en segmentos diferentes, es una falsa alarma. Por lo tanto, la tasa de error se calcula utilizando la siguiente ecuación: p(err|real, pred) = p(miss|real, pred, diff)p(diff|real) + p(false alarm|real, pred, same)p(same|real). 5.1.2 Resultados del Experimento Probamos el caso cuando se conoce el número de segmentos. La Tabla 1 muestra los resultados de nuestros métodos con diferentes valores de hiperparámetros y tres enfoques anteriores, C99[25], U00[6] y ADDP03[15], en este conjunto de datos cuando se conoce el número de segmento. En WMI para la segmentación de un solo documento, los pesos de los términos se calculan de la siguiente manera: wˆt = 1−EˆS(ˆt)/maxˆt ∈ ˆT (EˆS(ˆt)). Para este caso, nuestros métodos MIl y WMIl superan a todos los enfoques anteriores. Comparamos nuestros métodos con ADDP03 utilizando una prueba t de una muestra unilateral y los valores de p se muestran en la Tabla 2. A partir de los valores p, podemos ver que en su mayoría las diferencias son muy significativas. También comparamos las tasas de error entre nuestros dos métodos utilizando una prueba t de dos muestras de dos colas para verificar la hipótesis de que son iguales. No podemos rechazar la hipótesis de que son iguales, por lo que las diferencias no son significativas, a pesar de que todas las tasas de error para MIl son más pequeñas que las de WMIl. Sin embargo, podemos concluir que los pesos de los términos contribuyen poco en la segmentación de un solo documento. Los resultados también muestran que el uso de MI con co-clustering de términos (k = 100) disminuye el rendimiento. Probamos diferentes números de grupos de términos y encontramos que el rendimiento mejora cuando el número de grupos aumenta hasta llegar a l. WMIk<l tiene resultados similares que no mostramos en la tabla. Como se mencionó anteriormente, el uso de MI puede ser inconsistente en los límites óptimos dados diferentes números de segmentos. Esta situación ocurre especialmente cuando las similitudes entre los segmentos son bastante diferentes, es decir, algunas transiciones son muy obvias, mientras que otras no lo son. Esto se debe a que normalmente un documento es una estructura jerárquica en lugar de una estructura únicamente secuencial. Cuando los segmentos no están en el mismo nivel, esta situación puede ocurrir. Por lo tanto, se desea un enfoque de segmentación jerárquica de temas, y la estructura depende en gran medida del número de segmentos para cada nodo interno y del criterio de parada de la división. Para este conjunto de datos de segmentación de un solo documento, dado que es solo un conjunto sintético, que es simplemente una concatenación de varios segmentos sobre diferentes temas, es razonable que enfoques simplemente basados en la frecuencia de términos tengan un buen rendimiento. Normalmente, para las tareas de segmentación de documentos coherentes en subtemas, la efectividad disminuye mucho. 5.2 Detección de Temas Compartidos 5.2.1 Datos de Prueba y Evaluación El segundo conjunto de datos contiene 80 artículos de noticias de Google News. Hay ocho temas y cada uno tiene 10 artículos. Dividimos aleatoriamente el conjunto en subconjuntos con diferentes números de documentos y cada subconjunto tiene los ocho temas. Comparamos nuestro enfoque MIl y WMIl con LDA [4]. LDA trata un documento en el conjunto de datos como una bolsa de palabras, encuentra su distribución en temas y su tema principal. MIl y WMIl ven cada oración como un conjunto de palabras y la etiquetan con una etiqueta de tema. Luego, para cada par de documentos, LDA determina si están en el mismo tema, mientras que MIl y Table 3: Detección de Temas Compartidos: Tasas de Error Promedio para Diferentes Números de Documentos en Cada Subconjunto #Doc 10 20 40 80 LDA 8.89% 16.33% 1.35% 0.60% MIl, θ = 0.6 4.17% 1.71% 1.47% 0.0% WMIl, θ = 0.8 18.6% 3.16% 1.92% 0.0% WMIl comprueba si la proporción de oraciones superpuestas en el mismo tema es mayor que el umbral ajustable θ. Es decir, en MIl y WMIl, para un par de documentos d, d , si [ s∈Sd,s ∈Sd 1(temas=temas )/min(|Sd|, |Sd|)] > θ, donde Sd es el conjunto de oraciones de d, y |Sd| es el número de oraciones de d, entonces d y d comparten el tema. Para un par de documentos seleccionados al azar, la tasa de error se calcula utilizando la siguiente ecuación: p(err|real, pred) = p(miss|real, pred, same)p(same|real) + p(false alarm|real, pred, diff)p(diff|real), donde un error significa si tienen el mismo tema (same) para el caso real (real), pero se predice (pred) como en un tema diferente. Si están en diferentes temas (diff), pero se predicen como en el mismo tema, es una falsa alarma. 5.2.2 Resultados del Experimento Los resultados se muestran en la Tabla 3. Si la mayoría de los documentos tienen diferentes temas, en WMIl, la estimación de los pesos de términos en la Ecuación (3) no es correcta. Por lo tanto, no se espera que WMIL tenga un mejor rendimiento que MIL cuando la mayoría de los documentos tienen temas diferentes. Cuando hay menos documentos en un subconjunto con el mismo número de temas, más documentos tienen temas diferentes, por lo que WMIl es peor que MIl. Podemos ver que en la mayoría de los casos, MIl tiene un rendimiento mejor (o al menos similar) que LDA. Después de la detección de temas compartidos, se puede llevar a cabo la segmentación de documentos con los temas compartidos. 5.3 Segmentación de múltiples documentos 5.3.1 Datos de prueba y evaluación Para la segmentación y alineación de múltiples documentos, nuestro objetivo es identificar estos segmentos sobre el mismo tema entre varios documentos similares con temas compartidos. Se espera que al utilizar pesos de términos, Iw funcione mejor que I, ya que sin pesos de términos, el resultado se ve seriamente afectado por palabras de parada dependientes del documento y palabras ruidosas que dependen del estilo de escritura personal. Es más probable tratar los mismos segmentos de diferentes documentos como segmentos diferentes bajo el efecto de palabras de parada dependientes del documento y palabras ruidosas. Los pesos de términos pueden reducir el efecto de las palabras de parada dependientes del documento y las palabras ruidosas al darle más peso a los <br>términos de señal</br>. El conjunto de datos para la segmentación y alineación de múltiples documentos tiene un total de 102 muestras y 2264 oraciones. Cada uno es la parte de introducción de un informe de laboratorio seleccionado del curso de Biol 240W, Universidad Estatal de Pensilvania. Cada muestra tiene dos segmentos, la introducción de las hormonas vegetales y el contenido en el laboratorio. El rango de longitud de las muestras va desde dos hasta 56 oraciones. Algunas muestras solo tienen una parte y algunas tienen un orden inverso de estos dos segmentos. No es difícil identificar el límite entre dos segmentos para los humanos. Etiquetamos cada oración manualmente para evaluación. El criterio de evaluación consiste en utilizar la proporción del número de oraciones con etiquetas de segmento incorrectas predichas en el número total de oraciones en toda la Tabla de entrenamiento. Para mostrar los beneficios de la segmentación y alineación de múltiples documentos, comparamos nuestro método con diferentes parámetros en diferentes particiones del mismo conjunto de entrenamiento. Excepto los casos en los que el número de documentos es 102 y uno (que son casos especiales de uso del conjunto completo y la segmentación pura de un solo documento), dividimos aleatoriamente el conjunto de entrenamiento en m particiones, y cada una tiene 51, 34, 20, 10, 5 y 2 muestras de documentos. Luego aplicamos nuestros métodos en cada partición y calculamos la tasa de error de todo el conjunto de entrenamiento. Cada caso se repitió 10 veces para calcular las tasas de error promedio. Para diferentes particiones del conjunto de entrenamiento, se utilizan diferentes valores de k, ya que el número de términos aumenta cuando el número de documentos en cada partición aumenta. 5.3.2 Resultados del Experimento De los resultados del experimento en la Tabla 4, podemos ver las siguientes observaciones: (1) Cuando el número de documentos aumenta, todos los métodos tienen un mejor rendimiento. Solo de uno a dos documentos, el MIL ha disminuido un poco. Podemos observar esto en la Figura 3 en el punto del número de documento = 2. La mayoría de las curvas incluso tienen los peores resultados en este punto. Hay dos razones. Primero, porque las muestras votan por la mejor segmentación y alineación de múltiples documentos, pero si solo se comparan dos documentos entre sí, aquel con segmentos faltantes o una secuencia totalmente diferente afectará la correcta segmentación y alineación del otro. Segundo, como se señaló al principio de esta sección, si dos documentos tienen más palabras de parada dependientes del documento o palabras ruidosas que palabras clave, entonces el algoritmo puede considerarlos como dos segmentos diferentes y faltar el otro segmento. Generalmente, solo podemos esperar un mejor rendimiento cuando el número de documentos es mayor que el número de segmentos. (2) Excepto en la segmentación de un solo documento, WMIl siempre es mejor que MIl, y cuando el número de documentos se acerca a uno o aumenta a un número muy grande, sus rendimientos se vuelven más cercanos. La Tabla 5 muestra los valores p de la prueba t de dos muestras unilaterales entre MIl y WMIl. También podemos observar esta tendencia a partir de los valores de p. Cuando el número de documento es igual a 5, alcanzamos el valor p más pequeño y la mayor diferencia entre las tasas de error de MIl y WMIl. Para la Tabla 6 de un solo documento: Segmentación de múltiples documentos: Tasa de error promedio para el Documento Número = 5 en cada subconjunto con Diferente Número de Agrupaciones de Términos #Agrupaciones 75 100 150 250 l MIk 24.67% 24.54% 23.91% 22.59% 15.77% de segmentación, WMIl es incluso un poco peor que MIl, lo cual es similar a los resultados de la segmentación de un solo documento en el primer conjunto de datos. La razón es que para la segmentación de un solo documento, no podemos estimar con precisión los pesos de los términos, ya que no tenemos disponibles varios documentos. (3) El uso de agrupamiento de términos generalmente produce resultados peores que MIl y WMIl. (4) El uso de agrupamiento de términos en WMIk es aún peor que en MIk, ya que en WMIk los grupos de términos se encuentran primero utilizando I antes que Iw. Si los grupos de términos no son correctos, entonces los pesos de los términos se estiman peor, lo que puede llevar a que el algoritmo obtenga resultados aún peores. De los resultados también encontramos que en la segmentación y alineación de múltiples documentos, la mayoría de los documentos con segmentos faltantes y en orden inverso son identificados correctamente. La Tabla 6 ilustra los resultados del experimento para el caso de 20 particiones (cada una con cinco muestras de documentos) del conjunto de entrenamiento y la segmentación y alineación de temas utilizando MIk con diferentes números de grupos de términos k. Observa que cuando el número de grupos de términos aumenta, la tasa de error disminuye. Sin agrupamiento de términos, obtenemos el mejor resultado. No mostramos resultados para WMIk con agrupamiento de términos, pero los resultados son similares. También probamos WMIL con diferentes hiperparámetros de a y b para ajustar los pesos de los términos. Los resultados se presentan en la Figura 3. Se demostró que el caso predeterminado WMIl: a = 1, b = 1 dio los mejores resultados para diferentes particiones del conjunto de entrenamiento. Podemos observar la tendencia de que cuando el número de documentos es muy pequeño o grande, la diferencia entre MIl: a = 0, b = 0 y WMIl: a = 1, b = 1 se vuelve bastante pequeña. Cuando el número de documentos no es grande (aproximadamente de 2 a 10), todos los casos que utilizan pesos de términos tienen un mejor rendimiento que MIl: a = 0, b = 0 sin pesos de términos, pero cuando el número de documentos aumenta, los casos WMIl: a = 1, b = 0 y WMIl: a = 2, b = 1 empeoran en comparación con MIl: a = 0, b = 0. Cuando el número de documentos se vuelve muy grande, son aún peores que los casos con números de documentos pequeños. Esto significa que es muy importante encontrar una forma adecuada de estimar los pesos de los términos para el criterio de WMI. La Figura 4 muestra los pesos de los términos aprendidos de todo el conjunto de entrenamiento. Cuatro tipos de palabras se categorizan de manera aproximada aunque la transición entre ellas es sutil. La Figura 5 ilustra el cambio en la información mutua (ponderada) para MIl y WMIl. Como era de esperar, la información mutua para MIl aumenta de forma monótona con el número de pasos, mientras que WMIl no lo hace. Finalmente, MIl y WMIl son escalables, con complejidad computacional mostrada en la Figura 6. Una ventaja de nuestro enfoque basado en MI es que no es necesario eliminar las palabras vacías. Otra ventaja importante es que no hay hiperparámetros necesarios que ajustar. En la segmentación de un solo documento, el rendimiento basado en MI es aún mejor que el basado en WMI, por lo que no se requiere un hiperparámetro adicional. En la segmentación de múltiples documentos, mostramos en el experimento que a = 1 y b = 1 es lo mejor. Nuestro método otorga más peso a los <br>términos de señal</br>. Sin embargo, generalmente los términos o frases de señal aparecen al principio de un segmento, mientras que el final del segmento puede ser 1 2 5 10 20 34 51 102 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 Número de Documento Tasa de Error MIl :a=0,b=0 WMI l :a=1,b=1 WMI l :a=1,b=0 WMI l :a=2,b=1 Figura 3: Tasas de error para diferentes hiperparámetros de pesos de términos. 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Entropía de Documento Normalizada Entropía de Segmento Normalizada Palabras ruidosas Palabras de señal Palabras de parada comunes Palabras de parada de Doc-dep Figura 4: Pesos de términos aprendidos de todo el conjunto de entrenamiento. 0 100 200 300 400 500 600 0.06 0.08 0.1 0.12 0.14 0.16 0.18 Número de Pasos (Ponderado) Información Mutua MI l WMI l Figura 5: Cambio en la MI (ponderada) para MIl y WMIl. 0 20 40 60 80 100 120 0 200 400 600 800 1000 1200 1400 1600 1800 2000 Número de Documento Tiempo de Convergencia (seg) MI l WMI l Figura 6: Tiempo de convergencia para MIl y WMIl. muy ruidoso. Una posible solución es dar más peso a los términos al principio de cada segmento. Además, cuando la longitud de los segmentos es bastante diferente, los segmentos largos tienen frecuencias de términos mucho más altas, por lo que pueden dominar los límites de segmentación. La normalización de las frecuencias de términos en relación con la longitud del segmento puede ser útil. 6. CONCLUSIONES Y TRABAJO FUTURO Propusimos un método novedoso para la segmentación y alineación de temas en múltiples documentos basado en información mutua ponderada, que también puede manejar casos de un solo documento. Utilizamos programación dinámica para optimizar nuestro algoritmo. Nuestro enfoque supera a todos los métodos anteriores en casos de un solo documento. Además, también demostramos que realizar segmentación entre varios documentos puede mejorar el rendimiento enormemente. Nuestros resultados también demostraron que el uso de la información mutua ponderada puede aprovechar la información de varios documentos para lograr un mejor rendimiento. Solo probamos nuestro método en conjuntos de datos limitados. Se deben probar más conjuntos de datos, especialmente los complicados. Deberían compararse más métodos anteriores. Además, las segmentaciones naturales como los párrafos son pistas que se pueden utilizar para encontrar los límites óptimos. El aprendizaje supervisado también puede ser considerado. 7. AGRADECIMIENTOS Los autores desean agradecer a Xiang Ji y al Prof. J. Scott Payne por su ayuda. 8. REFERENCIAS [1] A. Banerjee, I. Ghillon, J. Ghosh, S. Merugu y D. Modha. Un enfoque generalizado de entropía máxima para la coagrupación de Bregman y la aproximación de matrices. En Actas de SIGKDD, 2004. [2] R. Bekkerman, R. El-Yaniv y A. McCallum. Agrupación distribucional de múltiples vías a través de interacciones por pares. En Actas de ICML, 2005. [3] D. M. Blei y P. J. Moreno. Segmentación de temas con un modelo oculto de Markov de aspectos. En Actas de SIGIR, 2001. [4] D. M. Blei, A. Ng y M. Jordan. Asignación latente de Dirichlet. Revista de Investigación en Aprendizaje Automático, 3:993-1022, 2003. [5] T. Brants, F. Chen e I. Tsochantaridis. Segmentación de documentos basada en temas con análisis semántico latente probabilístico. En Actas de CIKM, 2002. [6] F. Choi. Avances en la segmentación lineal de texto independiente del dominio. En Actas de la NAACL, 2000. [7] H. Christensen, B. Kolluru, Y. Gotoh y S. Renals. Segmentación de máxima entropía de noticias de transmisión. En Actas de ICASSP, 2005. [8] T. Cover y J. Thomas. Elementos de la teoría de la información. John Wiley and Sons, Nueva York, EE. UU., 1991. [9] S. Deerwester, S. Dumais, G. Furnas, T. Landauer y R. Harshman. Indexación mediante análisis semántico latente. Revista de la Sociedad Americana de Sistemas de Información, 1990. [10] I. Dhillon, S. Mallela y D. Modha. Co-clustering informativo-teórico. En Actas de SIGKDD, 2003. [11] M. Hajime, H. Takeo y O. Manabu. Segmentación de texto con múltiples señales lingüísticas superficiales. En Actas de COLING-ACL, 1998. [12] T. K. Ho. Detención de palabras y identificación para el reconocimiento de texto adaptativo. Revista Internacional de Análisis y Reconocimiento de Documentos, 3(1), agosto de 2000. [13] T. Hofmann. Análisis semántico latente probabilístico. En Actas de la UAI99, 1999. [14] X. Ji y H. Zha. Correlación de la sumarización de un par de documentos multilingües. En Actas de RIDE, 2003. [15] X. Ji y H. Zha. Segmentación de texto independiente del dominio utilizando difusión anisotrópica y programación dinámica. En Actas de SIGIR, 2003. [16] X. Ji y H. Zha. Extrayendo temas compartidos de múltiples documentos. En Actas de la 7ª PAKDD, 2003. [17] J. Lafferty, A. McCallum y F. Pereira. Campos aleatorios condicionales: Modelos probabilísticos para segmentar y etiquetar datos de secuencias. En Actas de ICML, 2001. [18] T. Li, S. Ma y M. Ogihara. Criterio basado en la entropía en la agrupación categórica. En Actas de ICML, 2004. [19] A. McCallum, D. Freitag y F. Pereira. Modelos de máxima entropía de Markov para extracción de información y segmentación. En Actas de ICML, 2000. [20] L. Pevzner y M. Hearst. Una crítica y mejora de una métrica de evaluación para la segmentación de texto. Lingüística Computacional, 28(1):19-36, 2002. [21] J. C. Reynar. Modelos estadísticos para la segmentación de temas. En Actas de ACL, 1999. [22] G. Salton y M. McGill. Introducción a la Recuperación de Información Moderna. McGraw Hill, 1983. [23] B. \n\nMcGraw Hill, 1983. [23] B. Sun, Q. Tan, P. Mitra y C. L. Giles. Extracción y búsqueda de fórmulas químicas en documentos de texto en la web. En Actas de WWW, 2007. [24] B. Sun, D. Zhou, H. Zha, y J. I'm sorry, but \"Yen\" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish? Segmentación y alineación de texto multitarea basada en información mutua ponderada. En Actas de CIKM, 2006. [25] M. Utiyama y H. Isahara. Un modelo estadístico para la segmentación de texto independiente del dominio. En Actas de la 39ª ACL, 1999. [26] C. Wayne. Detección y seguimiento de temas multilingües: Investigación exitosa habilitada por corpora y evaluación. En Actas de LREC, 2000. [27] J. Yamron, I. Carp, L. Gillick, S. Lowe y P. van Mulbregt. Un enfoque de modelo oculto de Markov para la segmentación de texto y seguimiento de eventos. En Actas de ICASSP, 1998. [28] H. Zha y X. Ji. Correlacionando documentos multilingües a través de modelado de gráficos bipartitos. En Actas de SIGIR, 2002. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "stop word": {
            "translated_key": "stop palabra",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Topic Segmentation with Shared Topic Detection and Alignment of Multiple Documents Bingjun Sun*, Prasenjit Mitra*† , Hongyuan Zha‡ , C. Lee Giles*† , John Yen*† *Department of Computer Science and Engineering † College of Information Sciences and Technology The Pennsylvania State University University Park, PA 16802 ‡ College of Computing The Georgia Institute of Technology Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu ABSTRACT Topic detection and tracking [26] and topic segmentation [15] play an important role in capturing the local and sequential information of documents.",
                "Previous work in this area usually focuses on single documents, although similar multiple documents are available in many domains.",
                "In this paper, we introduce a novel unsupervised method for shared topic detection and topic segmentation of multiple similar documents based on mutual information (MI) and weighted mutual information (WMI) that is a combination of MI and term weights.",
                "The basic idea is that the optimal segmentation maximizes MI(or WMI).",
                "Our approach can detect shared topics among documents.",
                "It can find the optimal boundaries in a document, and align segments among documents at the same time.",
                "It also can handle single-document segmentation as a special case of the multi-document segmentation and alignment.",
                "Our methods can identify and strengthen cue terms that can be used for segmentation and partially remove stop words by using term weights based on entropy learned from multiple documents.",
                "Our experimental results show that our algorithm works well for the tasks of single-document segmentation, shared topic detection, and multi-document segmentation.",
                "Utilizing information from multiple documents can tremendously improve the performance of topic segmentation, and using WMI is even better than using MI for the multi-document segmentation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Clustering; H.3.1 [Information Storage and Retrieval]: Content Analysis and IndexingLinguistic processing; I.2.7 [Artificial Intelligence]: Natural Language Processing-Text analysis; I.5.3 [Pattern Recognition]: Clustering-Algorithms;Similarity measures General Terms Algorithms, Design, Experimentation 1.",
                "INTRODUCTION Many researchers have worked on topic detection and tracking (TDT) [26] and topic segmentation during the past decade.",
                "Topic segmentation intends to identify the boundaries in a document with the goal to capture the latent topical structure.",
                "Topic segmentation tasks usually fall into two categories [15]: text stream segmentation where topic transition is identified, and coherent document segmentation in which documents are split into sub-topics.",
                "The former category has applications in automatic speech recognition, while the latter one has more applications such as partial-text query of long documents in information retrieval, text summary, and quality measurement of multiple documents.",
                "Previous research in connection with TDT falls into the former category, targeted on topic tracking of broadcast speech data and newswire text, while the latter category has not been studied very well.",
                "Traditional approaches perform topic segmentation on documents one at a time [15, 25, 6].",
                "Most of them perform badly in subtle tasks like coherent document segmentation [15].",
                "Often, end-users seek documents that have the similar content.",
                "Search engines, like, Google, provide links to obtain similar pages.",
                "At a finer granularity, users may actually be looking to obtain sections of a document similar to a particular section that presumably discusses a topic of the users interest.",
                "Thus, the extension of topic segmentation from single documents to identifying similar segments from multiple similar documents with the same topic is a natural and necessary direction, and multi-document topic segmentation is expected to have a better performance since more information is utilized.",
                "Traditional approaches using similarity measurement based on term frequency generally have the same assumption that similar vocabulary tends to be in a coherent topic segment [15, 25, 6].",
                "However, they usually suffer the issue of identifying stop words.",
                "For example, additional document-dependent stop words are removed together with the generic stop words in [15].",
                "There are two reasons that we do not remove stop words directly.",
                "First, identifying stop words is another issue [12] that requires estimation in each domain.",
                "Removing common stop words may result in the loss of useful information in a specific domain.",
                "Second, even though stop words can be identified, hard classification of stop words and nonstop words cannot represent the gradually changing amount of information content of each word.",
                "We employ a soft classification using term weights.",
                "In this paper, we view the problem of topic segmentation as an optimization issue using information theoretic techniques to find the optimal boundaries of a document given the number of text segments so as to minimize the loss of mutual information (MI) (or a weighted mutual information (WMI)) after segmentation and alignment.",
                "This is equal to maximizing the MI (or WMI).",
                "The MI focuses on measuring the difference among segments whereas previous research focused on finding the similarity (e.g. cosine distance) of segments [15, 25, 6].",
                "Topic alignment of multiple similar documents can be achieved by clustering sentences on the same topic into the same cluster.",
                "Single-document topic segmentation is just a special case of the multi-document topic segmentation and alignment problem.",
                "Terms can be co-clustered as in [10] at the same time, given the number of clusters, but our experimental results show that this method results in a worse segmentation (see Tables 1, 4, and 6).",
                "Usually, human readers can identify topic transition based on cue words, and can ignore stop words.",
                "Inspired by this, we give each term (or term cluster) a weight based on entropy among different documents and different segments of documents.",
                "Not only can this approach increase the contribution of cue words, but it can also decrease the effect of common stop words, noisy word, and document-dependent stop words.",
                "These words are common in a document.",
                "Many methods based on sentence similarity require that these words are removed before topic segmentation can be performed [15].",
                "Our results in Figure 3 show that term weights are useful for multi-document topic segmentation and alignment.",
                "The major contribution of this paper is that it introduces a novel method for topic segmentation using MI and shows that this method performs better than previously used criteria.",
                "Also, we have addressed the problem of topic segmentation and alignment across multiple documents, whereas most existing research focused on segmentation of single documents.",
                "Multi-document segmentation and alignment can utilize information from similar documents and improves the performance of topic segmentation greatly.",
                "Obviously, our approach can handle single documents as a special case when multiple documents are unavailable.",
                "It can detect shared topics among documents to judge if they are multiple documents on the same topic.",
                "We also introduce the new criterion of WMI based on term weights learned from multiple similar documents, which can improve performance of topic segmentation further.",
                "We propose an iterative greedy algorithm based on dynamic programming and show that it works well in practice.",
                "Some of our prior work is in [24].",
                "The rest of this paper is organized as follows: In Section 2, we review related work.",
                "Section 3 contains a formulation of the problem of topic segmentation and alignment of multiple documents with term co-clustering, a review of the criterion of MI for clustering, and finally an introduction to WMI.",
                "In Section 4, we first propose the iterative greedy algorithm of topic segmentation and alignment with term co-clustering, and then describe how the algorithm can be optimized by usFigure 1: Illustration of multi-document segmentation and alignment. ing dynamic programming.",
                "In Section 5, experiments about single-document segmentation, shared topic detection, and multi-document segmentation are described, and results are presented and discussed to evaluate the performance of our algorithm.",
                "Conclusions and some future directions of the research work are discussed in Section 6. 2.",
                "PREVIOUS WORK Generally, the existing approaches to text segmentation fall into two categories: supervised learning [19, 17, 23] and unsupervised learning [3, 27, 5, 6, 15, 25, 21].",
                "Supervised learning usually has good performance, since it learns functions from labelled training sets.",
                "However, often getting large training sets with manual labels on document sentences is prohibitively expensive, so unsupervised approaches are desired.",
                "Some models consider dependence between sentences and sections, such as Hidden Markov Model [3, 27], Maximum Entropy Markov Model [19], and Conditional Random Fields [17], while many other approaches are based on lexical cohesion or similarity of sentences [5, 6, 15, 25, 21].",
                "Some approaches also focus on cue words as hints of topic transitions [11].",
                "While some existing methods only consider information in single documents [6, 15], others utilize multiple documents [16, 14].",
                "There are not many works in the latter category, even though the performance of segmentation is expected to be better with utilization of information from multiple documents.",
                "Previous research studied methods to find shared topics [16] and topic segmentation and summarization between just a pair of documents [14].",
                "Text classification and clustering is a related research area which categorizes documents into groups using supervised or unsupervised methods.",
                "Topical classification or clustering is an important direction in this area, especially co-clustering of documents and terms, such as LSA [9], PLSA [13], and approaches based on distances and bipartite graph partitioning [28] or maximum MI [2, 10], or maximum entropy [1, 18].",
                "Criteria of these approaches can be utilized in the issue of topic segmentation.",
                "Some of those methods have been extended into the area of topic segmentation, such as PLSA [5] and maximum entropy [7], but to our best knowledge, using MI for topic segmentation has not been studied. 3.",
                "PROBLEM FORMULATION Our goal is to segment documents and align the segments across documents (Figure 1).",
                "Let T be the set of terms {t1, t2, ..., tl}, which appear in the unlabelled set of documents D = {d1, d2, ..., dm}.",
                "Let Sd be the set of sentences for document d ∈ D, i.e. {s1, s2, ..., snd }.",
                "We have a 3D matrix of term frequency, in which the three dimensions are random variables of D, Sd, and T. Sd actually is a random vector including a random variable for each d ∈ D. The term frequency can be used to estimate the joint probability distribution P(D, Sd, T), which is p(t, d, s) = T(t, d, s)/ND, where T(t, d, s) is the number of t in ds sentence s and ND is the total number of terms in D. ˆS represents the set of segments {ˆs1, ˆs2, ..., ˆsp} after segmentation and alignment among multiple documents, where the number of segments | ˆS| = p. A segment ˆsi of document d is a sequence of adjacent sentences in d. Since for different documents si may discuss different sub-topics, our goal is to cluster adjacent sentences in each document into segments, and align similar segments among documents, so that for different documents ˆsi is about the same sub-topic.",
                "The goal is to find the optimal topic segmentation and alignment mapping Segd(si) : {s1, s2, ..., snd } → {ˆs1, ˆs2, ..., ˆsp} and Alid(ˆsi) : {ˆs1, ˆs2, ..., ˆsp} → {ˆs1, ˆs2, ..., ˆsp}, for all d ∈ D, where ˆsi is ith segment with the constraint that only adjacent sentences can be mapped to the same segment, i.e. for d, {si, si+1, ..., sj} → {ˆsq}, where q ∈ {1, ..., p}, where p is the segment number, and if i > j, then for d, ˆsq is missing.",
                "After segmentation and alignment, random vector Sd becomes an aligned random variable ˆS.",
                "Thus, P(D, Sd, T) becomes P(D, ˆS, T).",
                "Term co-clustering is a technique that has been employed [10] to improve the accuracy of document clustering.",
                "We evaluate the effect of it for topic segmentation.",
                "A term t is mapped to exactly one term cluster.",
                "Term co-clustering involves simultaneously finding the optimal term clustering mapping Clu(t) : {t1, t2, ..., tl} → {ˆt1, ˆt2, ..., ˆtk}, where k ≤ l, l is the total number of words in all the documents, and k is the number of clusters. 4.",
                "METHODOLOGY We now describe a novel algorithm which can handle singledocument segmentation, shared topic detection, and multidocument segmentation and alignment based on MI or WMI. 4.1 Mutual Information MI I(X; Y ) is a quantity to measure the amount of information which is contained in two or more random variables [8, 10].",
                "For the case of two random variables, we have I(X; Y ) = x∈X y∈Y p(x, y)log p(x, y) p(x)p(y) , (1) Obviously, when random variables X and Y are independent, I(X; Y ) = 0.",
                "Thus, intuitively, the value of MI depends on how random variables are dependent on each other.",
                "The optimal co-clustering is the mapping Clux : X → ˆX and Cluy : Y → ˆY that minimizes the loss: I(X; Y ) − I( ˆX; ˆY ), which is equal to maximizing I( ˆX; ˆY ).",
                "This is the criterion of MI for clustering.",
                "In the case of topic segmentation, the two random variables are the term variable T and the segment variable S, and each sample is an occurrence of a term T = t in a particular segment S = s. I(T; S) is used to measure how dependent T and S are.",
                "However, I(T; S) cannot be computed for documents before segmentation, since we do not have a set of S due to the fact that sentences of Document d, si ∈ Sd, is not aligned with other documents.",
                "Thus, instead of minimizing the loss of MI, we can maximize MI after topic segmentation, computed as: I( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS p(ˆt, ˆs)log p(ˆt, ˆs) p(ˆt)p(ˆs) , (2) where p(ˆt, ˆs) are estimated by the term frequency tf of Term Cluster ˆt and Segment ˆs in the training set D. Note that here a segment ˆs includes sentences about the the same topic among all documents.",
                "The optimal solution is the mapping Clut : T → ˆT, Segd : Sd → ˆS , and Alid : ˆS → ˆS, which maximizes I( ˆT; ˆS). 4.2 Weighted Mutual Information In topic segmentation and alignment of multiple documents, if P(D, ˆS, T) is known, based on the marginal distributions P(D|T) and P( ˆS|T) for each term t ∈ T, we can categorize terms into four types in the data set: • Common stop words are common both along the dimensions of documents and segments. • Document-dependent stop words that depends on the personal writing style are common only along the dimension of segments for some documents. • Cue words are the most important elements for segmentation.",
                "They are common along the dimension of documents only for the same segment, and they are not common along the dimensions of segments. • Noisy words are other words which are not common along both dimensions.",
                "Entropy based on P(D|T) and P( ˆS|T) can be used to identify different types of terms.",
                "To reinforce the contribution of cue words in the MI computation, and simultaneously reduce the effect of the other three types of words, similar as the idea of the tf-idf weight [22], we use entropies of each term along the dimensions of document D and segment ˆS, i.e.",
                "ED(ˆt) and EˆS(ˆt), to compute the weight.",
                "A cue word usually has a large value of ED(ˆt) but a small value of EˆS(ˆt).",
                "We introduce term weights (or term cluster weights) wˆt = ( ED(ˆt) maxˆt ∈ ˆT (ED(ˆt )) )a (1 − EˆS(ˆt) maxˆt ∈ ˆT (EˆS(ˆt )) )b , (3) where ED(ˆt) = d∈D p(d|ˆt)log|D| 1 p(d|ˆt) , EˆS(ˆt) = ˆs∈ ˆS p(ˆs|ˆt)log| ˆS| 1 p(ˆs|ˆt) , and a > 0 and b > 0 are powers to adjust term weights.",
                "Usually a = 1 and b = 1 as default, and maxˆt ∈ ˆT (ED(ˆt )) and maxˆt ∈ ˆT (EˆS(ˆt )) are used to normalize the entropy values.",
                "Term cluster weights are used to adjust p(ˆt, ˆs), pw(ˆt, ˆs) = wˆtp(ˆt, ˆs) ˆt∈ ˆT ;ˆs∈ ˆS wˆtp(ˆt, ˆs) , (4) and Iw( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS pw(ˆt, ˆs)log pw(ˆt, ˆs) pw(ˆt)pw(ˆs) , (5) where pw(ˆt) and pw(ˆs) are marginal distributions of pw(ˆt, ˆs).",
                "However, since we do not know either the term weights or P(D, ˆS, T), we need to estimate them, but wˆt depends on p(ˆs|t) and ˆS, while ˆS and p(ˆs|t) also depend on wˆt that is still unknown.",
                "Thus, an iterative algorithm is required to estimate term weights wˆt and find the best segmentation and alignment to optimize the objective function Iw concurrently.",
                "After a document is segmented into sentences Input: Joint probability distribution P(D, Sd, T), number of text segments p ∈ {2, 3, ..., max(sd)}, number of term clusters k ∈ {2, 3, ..., l} (if k = l, no term co-clustering required), and weight type w ∈ {0, 1}, indicating to use I or Iw, respectively.",
                "Output: Mapping Clu, Seg, Ali, and term weights wˆt.",
                "Initialization: 0. i = 0.",
                "Initialize Clu (0) t , Seg (0) d , and Ali (0) d ; Initialize w (0) ˆt using Equation (6) if w = 1; Stage 1: 1.",
                "If |D| = 1, k = l, and w = 0, check all sequential segmentations of d into p segments and find the best one Segd(s) = argmaxˆsI( ˆT; ˆS), and return Segd; otherwise, if w = 1 and k = l, go to 3.1; Stage 2: 2.1 If k < l, for each term t, find the best cluster ˆt as Clu(i+1)(t) = argmaxˆtI( ˆT; ˆS(i)) based on Seg(i) and Ali(i); 2.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+1) d (Seg (i+1) d (s)) = argmaxˆsI( ˆT(i+1); ˆS) based on Clu(i+1)(t) if k < l or Clu(0)(t) if k = l; 2.3 i + +.",
                "If Clu, Seg, or Ali changed, go to 2.1; otherwise, if w = 0, return Clu(i), Seg(i), and Ali(i); else j = 0, go to 3.1; Stage 3: 3.1 Update w (i+j+1) ˆt based on Seg(i+j), Ali(i+j), and Clu(i) using Equation (3); 3.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+j+1) d (Seg (i+j+1) d (s)) = argmaxˆsIw( ˆT(i); ˆS) based on Clu(i) and w (i+j+1) ˆt ; 3.3 j + +.",
                "If Iw( ˆT; ˆS) changes, go to step 6; otherwise, stop and return Clu(i), Seg(i+j), Ali(i+j), and w (i+j) ˆt ; Figure 2: Algorithm: Topic segmentation and alignment based on MI or WMI. and each sentence is segmented into words, each word is stemmed.",
                "Then the joint probability distribution P(D, Sd, T) can be estimated.",
                "Finally, this distribution can be used to compute MI in our algorithm. 4.3 Iterative Greedy Algorithm Our goal is to maximize the objective function, I( ˆT; ˆS) or Iw( ˆT; ˆS), which can measure the dependence of term occurrences in different segments.",
                "Generally, first we do not know the estimate term weights, which depend on the optimal topic segmentation and alignment, and term clusters.",
                "Moreover, this problem is NP-hard [10], even though if we know the term weights.",
                "Thus, an iterative greedy algorithm is desired to find the best solution, even though probably only local maxima are reached.",
                "We present the iterative greedy algorithm in Figure 2 to find a local maximum of I( ˆT; ˆS) or Iw( ˆT; ˆS) with simultaneous term weight estimation.",
                "This algorithm can is iterative and greedy for multi-document cases or single-document cases with term weight estimation and/or term co-clustering.",
                "Otherwise, since it is just a one step algorithm to solve the task of single-document segmentation [6, 15, 25], the global maximum of MI is guaranteed.",
                "We will show later that term co-clustering reduces the accuracy of the results and is not necessary, and for singledocument segmentation, term weights are also not required. 4.3.1 Initialization In Step 0, the initial term clustering Clut and topic segmentation and alignment Segd and Alid are important to avoid local maxima and reduce the number of iterations.",
                "First, a good guess of term weights can be made by using the distributions of term frequency along sentences for each document and averaging them to get the initial values of wˆt: wt = ( ED(t) maxt ∈T (ED(t )) )(1 − ES(t) maxt ∈T (ES(t )) ), (6) where ES(t) = 1 |Dt| d∈Dt (1 − s∈Sd p(s|t)log|Sd| 1 p(s|t) ), where Dt is the set of documents which contain Term t. Then, for the initial segmentation Seg(0) , we can simply segment documents equally by sentences.",
                "Or we can find the optimal segmentation just for each document d which maximizes the WMI, Seg (0) d = argmaxˆsIw(T; ˆS), where w = w (0) ˆt .",
                "For the initial alignment Ali(0) , we can first assume that the order of segments for each d is the same.",
                "For the initial term clustering Clu(0) , first cluster labels can be set randomly, and after the first time of Step 3, a good initial term clustering is obtained. 4.3.2 Different Cases After initialization, there are three stages for different cases.",
                "Totally there are eight cases, |D| = 1 or |D| > 1, k = l or k < l, w = 0 or w = 1.",
                "Single document segmentation without term clustering and term weight estimation (|D| = 1, k = l, w = 0) only requires Stage 1 (Step 1).",
                "If term clustering is required (k < l), Stage 2 (Step 2.1, 2.2, and 2.3) is executed iteratively.",
                "If term weight estimation is required (w = 1), Stage 3 (Step 3.1, 3.2, and 3.3) is executed iteratively.",
                "If both are required (k < l, w = 1), Stage 2 and 3 run one after the other.",
                "For multi-document segmentation without term clustering and term weight estimation (|D| > 1, k = l, w = 0), only iteration of Step 2.2 and 2.3 are required.",
                "At Stage 1, the global maximum can be found based on I( ˆT; ˆS) using dynamic programming in Section 4.4.",
                "Simultaneously finding a good term clustering and estimated term weights is impossible, since when moving a term to a new term cluster to maximize Iw( ˆT; ˆS), we do not know that the weight of this term should be the one of the new cluster or the old cluster.",
                "Thus, we first do term clustering at Stage 2, and then estimate term weights at Stage 3.",
                "At Stage 2, Step 2.1 is to find the best term clustering and Step 2.2 is to find the best segmentation.",
                "This cycle is repeated to find a local maximum based on MI I until it converges.",
                "The two steps are: (1) based on current term clustering Cluˆt, for each document d, the algorithm segments all the sentences Sd into p segments sequentially (some segments may be empty), and put them into the p segments ˆS of the whole training set D (all possible cases of different segmentation Segd and alignment Alid are checked) to find the optimal case, and (2) based on the current segmentation and alignment, for each term t, the algorithm finds the best term cluster of t based on the current segmentation Segd and alignment Alid.",
                "After finding a good term clustering, term weights are estimated if w = 1.",
                "At Stage 3, similar as Stage 2, Step 3.1 is term weight re-estimation and Step 3.2 is to find a better segmentation.",
                "They are repeated to find a local maximum based on WMI Iw until it converges.",
                "However, if the term clustering in Stage 2 is not accurate, then the term weight estimation at Stage 3 may have a bad result.",
                "Finally, at Step 3.3, this algorithm converges and return the output.",
                "This algorithm can handle both single-document and multi-document segmentation.",
                "It also can detect shared topics among documents by checking the proportion of overlapped sentences on the same topics, as described in Sec 5.2. 4.4 Algorithm Optimization In many previous works on segmentation, dynamic programming is a technique used to maximize the objective function.",
                "Similarly, at Step 1, 2.2, and 3.2 of our algorithm, we can use dynamic programming.",
                "For Stage 1, using dynamic programming can still find the global optimum, but for Stage 2 and Stage 3, we can only find the optimum for each step of topic segmentation and alignment of a document.",
                "Here we only show the dynamic programming for Step 3.2 using WMI (Step 1 and 2.2 are similar but they can use either I or Iw).",
                "There are two cases that are not shown in the algorithm in Figure 2: (a) single-document segmentation or multi-document segmentation with the same sequential order of segments, where alignment is not required, and (b) multi-document segmentation with different sequential orders of segments, where alignment is necessary.",
                "The alignment mapping function of the former case is simply just Alid(ˆsi) = ˆsi, while for the latter ones alignment mapping function Alid(ˆsi) = ˆsj, i and j may be different.",
                "The computational steps for the two cases are listed below: Case 1 (no alignment): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs) and partial pw(ˆs) without counting sentences from d. Then put sentences from i to j into Part k, and compute partial WMI PIw( ˆT; ˆsk(si, si+1, ..., sj)) ˆt∈ ˆT pw(ˆt, ˆsk)log pw(ˆt, ˆsk) pw(ˆt)pw(ˆsk) , where Alid(si, si+1, ..., sj) = k, k ∈ {1, 2, ..., p}, 1 ≤ i ≤ j ≤ nd, and Segd(sq) = ˆsk for all i ≤ q ≤ j. (2) Let M(sm, 1) = PIw( ˆT; ˆs1(s1, s2, ..., sm)).",
                "Then M(sm, L) = maxi[M(si−1, L − 1) + PIw( ˆT; ˆsL(si, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, and when i > m, no sentences are put into ˆsk when compute PIw (note PIw( ˆT; ˆs(si, ..., sm)) = 0 for single-document segmentation). (3) Finally M(snd , p) = maxi[M(si−1, p − 1)+ PIw( ˆT; ˆsp(si, ..., snd ))], where 1 ≤ i ≤ nd+1.",
                "The optimal Iw is found and the corresponding segmentation is the best.",
                "Case 2 (alignment required): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs), and partial pw(ˆs), and PIw( ˆT; ˆsk(si, si+1, ..., sj)) similarly as Case 1. (2) Let M(sm, 1, k) = PIw( ˆT; ˆsk(s1, s2, ..., sm)), where k ∈ {1, 2, ..., p}.",
                "Then M(sm, L, kL) = maxi,j[M(si−1, L − 1, kL/j) + PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, kL ∈ Set(p, L), which is the set of all p!",
                "L!(p−L)! combinations of L segments chosen from all p segments, j ∈ kL, the set of L segments chosen from all p segments, and kL/j is the combination of L − 1 segments in kL except Segment j. (3) Finally, M(snd , p, kp) = maxi,j[M(si−1, p − 1, kp/j) +PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., snd ))], where kp is just the combination of all p segments and 1 ≤ i ≤ nd + 1, which is the optimal Iw and the corresponding segmentation is the best.",
                "The steps of Case 1 and 2 are similar, except in Case 2, alignment is considered in addition to segmentation.",
                "First, basic items of probability for computing Iw are computed excluding Doc d, and then partial WMI by putting every possible sequential segment (including empty segment) of d into every segment of the set.",
                "Second, the optimal sum of PIw for L segments and the leftmost m sentences, M(sm, L), is found.",
                "Finally, the maximal WMI is found among different sums of M(sm, p − 1) and PIw for Segment p. 5.",
                "EXPERIMENTS In this section, single-document segmentation, shared topic detection, and multi-document segmentation will be tested.",
                "Different hyper parameters of our method are studied.",
                "For convenience, we refer to the method using I as MIk if w = 0, and Iw as WMIk if w = 2 or as WMIk if w = 1, where k is the number of term clusters, and if k = l, where l is the total number of terms, then no term clustering is required, i.e.",
                "MIl and WMIl. 5.1 Single-document Segmentation 5.1.1 Test Data and Evaluation The first data set we tested is a synthetic one used in previous research [6, 15, 25] and many other papers.",
                "It has 700 samples.",
                "Each is a concatenation of ten segments.",
                "Each segment is the first n sentence selected randomly from the Brown corpus, which is supposed to have a different topic from each other.",
                "Currently, the best results on this data set is achieved by Ji et.al. [15].",
                "To compare the performance of our methods, the criterion used widely in previous research is applied, instead of the unbiased criterion introduced in [20].",
                "It chooses a pair of words randomly.",
                "If they are in different segments (different) for the real segmentation (real), but predicted (pred) as in the same segment, it is a miss.",
                "If they are in the same segment (same), but predicted as in different segments, it is a false alarm.",
                "Thus, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, diff)p(diff|real) +p(false alarm|real, pred, same)p(same|real). 5.1.2 Experiment Results We tested the case when the number of segments is known.",
                "Table 1 shows the results of our methods with different hyper parameter values and three previous approaches, C99[25], U00[6], and ADDP03[15], on this data set when the segment number is known.",
                "In WMI for single-document segmentation, the term weights are computed as follows: wˆt = 1−EˆS(ˆt)/maxˆt ∈ ˆT (EˆS(ˆt )).",
                "For this case, our methods MIl and WMIl both outperform all the previous approaches.",
                "We compared our methods with ADDP03using one-sample one-sided t-test and p-values are shown in Table 2.",
                "From the p-values, we can see that mostly the differences are very Table 1: Average Error Rates of Single-document Segmentation Given Segment Numbers Known Range of n 3-11 3-5 6-8 9-11 Sample size 400 100 100 100 C99 12% 11% 10% 9% U00 10% 9% 7% 5% ADDP03 6.0% 6.8% 5.2% 4.3% MIl 4.68% 5.57% 2.59% 1.59% WMIl 4.94% 6.33% 2.76% 1.62% MI100 9.62% 12.92% 8.66% 6.67% Table 2: Single-document Segmentation: P-values of T-test on Error Rates Range of n 3-11 3-5 6-8 9-11 ADDP03, MIl 0.000 0.000 0.000 0.000 ADDP03, WMIl 0.000 0.099 0.000 0.000 MIl, WMIl 0.061 0.132 0.526 0.898 significant.",
                "We also compare the error rates between our two methods using two-sample two-sided t-test to check the hypothesis that they are equal.",
                "We cannot reject the hypothesis that they are equal, so the difference are not significant, even though all the error rates for MIl are smaller than WMIl.",
                "However, we can conclude that term weights contribute little in single-document segmentation.",
                "The results also show that MI using term co-clustering (k = 100) decreases the performance.",
                "We tested different number of term clusters, and found that the performance becomes better when the cluster number increases to reach l. WMIk<l has similar results that we did not show in the table.",
                "As mentioned before, using MI may be inconsistent on optimal boundaries given different numbers of segments.",
                "This situation occurs especially when the similarities among segments are quite different, i.e. some transitions are very obvious, while others are not.",
                "This is because usually a document is a hierarchical structure instead of only a sequential structure.",
                "When the segments are not at the same level, this situation may occur.",
                "Thus, a hierarchical topic segmentation approach is desired, and the structure highly depends on the number of segments for each internal node and the stop criteria of splitting.",
                "For this data set of singledocument segmentation, since it is just a synthetic set, which is just a concatenation of several segments about different topics, it is reasonable that approaches simply based on term frequency have a good performance.",
                "Usually for the tasks of segmenting coherent documents for sub-topics, the effectiveness decreases much. 5.2 Shared Topic Detection 5.2.1 Test Data and Evaluation The second data set contains 80 news articles from Google News.",
                "There are eight topics and each has 10 articles.",
                "We randomly split the set into subsets with different document numbers and each subset has all eight topics.",
                "We compare our approach MIl and WMIl with LDA [4].",
                "LDA treats a document in the data set as a bag of words, finds its distribution on topics, and its major topic.",
                "MIl and WMIl views each sentence as a bag of words and tag it with a topic label.",
                "Then for each pair of documents, LDA determines if they are on the same topic, while MIl and Table 3: Shared Topic Detection: Average Error Rates for Different Numbers of Documents in Each Subset #Doc 10 20 40 80 LDA 8.89% 16.33% 1.35% 0.60% MIl, θ = 0.6 4.17% 1.71% 1.47% 0.0% WMIl, θ = 0.8 18.6% 3.16% 1.92% 0.0% WMIl check whether the proportion overlapped sentences on the same topic is larger than the adjustable threshold θ.",
                "That is, in MIl and WMIl, for a pair of documents d, d , if [ s∈Sd,s ∈Sd 1(topics=topics )/min(|Sd|, |Sd|)] > θ, where Sd is the set of sentences of d, and |Sd| is the number of sentences of d, then d and d have the shared topic.",
                "For a pair of documents selected randomly, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, same)p(same|real) +p(false alarm|real, pred, diff)p(diff|real), where a miss means if they have the same topic (same) for the real case (real), but predicted (pred) as on the same topic.",
                "If they are on different topics (diff), but predicted as on the same topic, it is a false alarm. 5.2.2 Experiment Results The results are shown in Table 3.",
                "If most documents have different topics, in WMIl, the estimation of term weights in Equation (3) is not correct.",
                "Thus, WMIl is not expected to have a better performance than MIl, when most documents have different topics.",
                "When there are fewer documents in a subset with the same number of topics, more documents have different topics, so WMIl is more worse than MIl.",
                "We can see that for most cases MIl has a better (or at least similar) performance than LDA.",
                "After shared topic detection, multi-document segmentation of documents with the shared topics is able to be executed. 5.3 Multi-document Segmentation 5.3.1 Test Data and Evaluation For multi-document segmentation and alignment, our goal is to identify these segments about the same topic among multiple similar documents with shared topics.",
                "Using Iw is expected to perform better than I, since without term weights the result is affected seriously by document-dependent stop words and noisy words which depends on the personal writing style.",
                "It is more likely to treat the same segments of different documents as different segments under the effect of document-dependent stop words and noisy words.",
                "Term weights can reduce the effect of document-dependent stop words and noisy words by giving cue terms more weights.",
                "The data set for multi-document segmentation and alignment has 102 samples and 2264 sentences totally.",
                "Each is the introduction part of a lab report selected from the course of Biol 240W, Pennsylvania State University.",
                "Each sample has two segments, introduction of plant hormones and the content in the lab.",
                "The length range of samples is from two to 56 sentences.",
                "Some samples only have one part and some have a reverse order the these two segments.",
                "It is not hard to identify the boundary between two segments for human.",
                "We labelled each sentence manually for evaluation.",
                "The criterion of evaluation is just using the proportion of the number of sentences with wrong predicted segment labels in the total number of sentences in the whole training Table 4: Average Error Rates of Multi-document Segmentation Given Segment Numbers Known #Doc MIl WMIl k MIk WMIk 102 3.14% 2.78% 300 4.68% 6.58% 51 4.17% 3.63% 300 17.83% 22.84% 34 5.06% 4.12% 300 18.75% 20.95% 20 7.08% 5.42% 250 20.40% 21.83% 10 10.38% 7.89% 250 21.42% 21.91% 5 15.77% 11.64% 250 21.89% 22.59% 2 25.90% 23.18% 50 25.44% 25.49% 1 23.90% 24.82% 25 25.75% 26.15% Table 5: Multi-document Segmentation: P-values of T-test on Error Rates for MIl and WMIl #Doc 51 34 20 10 5 2 P-value 0.19 0.101 0.025 0.001 0.000 0.002 set as the error rate: p(error|predicted, real) = d∈D s∈Sd 1(predicteds=reals)/ d∈D nd.",
                "In order to show the benefits of multi-document segmentation and alignment, we compared our method with different parameters on different partitions of the same training set.",
                "Except the cases that the number of documents is 102 and one (they are special cases of using the whole set and the pure single-document segmentation), we randomly divided the training set into m partitions, and each has 51, 34, 20, 10, 5, and 2 document samples.",
                "Then we applied our methods on each partition and calculated the error rate of the whole training set.",
                "Each case was repeated for 10 times for computing the average error rates.",
                "For different partitions of the training set, different k values are used, since the number of terms increases when the document number in each partition increases. 5.3.2 Experiment Results From the experiment results in Table 4, we can see the following observations: (1) When the number of documents increases, all methods have better performances.",
                "Only from one to two documents, MIl has decrease a little.",
                "We can observe this from Figure 3 at the point of document number = 2.",
                "Most curves even have the worst results at this point.",
                "There are two reasons.",
                "First, because samples vote for the best multi-document segmentation and alignment, but if only two documents are compared with each other, the one with missing segments or a totally different sequence will affect the correct segmentation and alignment of the other.",
                "Second, as noted at the beginning of this section, if two documents have more document-dependent stop words or noisy words than cue words, then the algorithm may view them as two different segments and the other segment is missing.",
                "Generally, we can only expect a better performance when the number of documents is larger than the number of segments. (2) Except single-document segmentation, WMIl is always better than MIl, and when the number of documents is reaching one or increases to a very large number, their performances become closer.",
                "Table 5 shows p-values of twosample one-sided t-test between MIl and WMIl.",
                "We also can see this trend from p-values.",
                "When document number = 5, we reached the smallest p-value and the largest difference between error rates of MIl and WMIl.",
                "For single-document Table 6: Multi-document Segmentation: Average Error Rate for Document Number = 5 in Each Subset with Different Number of Term Clusters #Cluster 75 100 150 250 l MIk 24.67% 24.54% 23.91% 22.59% 15.77% segmentation, WMIl is even a little bit worse than MIl, which is similar as the results of the single-document segmentation on the first data set.",
                "The reason is that for singledocument segmentation, we cannot estimate term weights accurately, since multiple documents are unavailable. (3) Using term clustering usually gets worse results than MIl and WMIl. (4) Using term clustering in WMIk is even worse than in MIk, since in WMIk term clusters are found first using I before using Iw.",
                "If the term clusters are not correct, then the term weights are estimated worse, which may mislead the algorithm to reach even worse results.",
                "From the results we also found that in multi-document segmentation and alignment, most documents with missing segments and a reverse order are identified correctly.",
                "Table 6 illustrates the experiment results for the case of 20 partitions (each has five document samples) of the training set and topic segmentation and alignment using MIk with different numbers of term clusters k. Notice that when the number of term clusters increases, the error rate becomes smaller.",
                "Without term clustering, we have the best result.",
                "We did not show results for WMIk with term clustering, but the results are similar.",
                "We also tested WMIl with different hyper parameters of a and b to adjust term weights.",
                "The results are presented in Figure 3.",
                "It was shown that the default case WMIl : a = 1, b = 1 gave the best results for different partitions of the training set.",
                "We can see the trend that when the document number is very small or large, the difference between MIl : a = 0, b = 0 and WMIl : a = 1, b = 1 becomes quite small.",
                "When the document number is not large (about from 2 to 10), all the cases using term weights have better performances than MIl : a = 0, b = 0 without term weights, but when the document number becomes larger, the cases WMIl : a = 1, b = 0 and WMIl : a = 2, b = 1 become worse than MIl : a = 0, b = 0.",
                "When the document number becomes very large, they are even worse than cases with small document numbers.",
                "This means that a proper way to estimate term weights for the criterion of WMI is very important.",
                "Figure 4 shows the term weights learned from the whole training set.",
                "Four types of words are categorized roughly even though the transition among them are subtle.",
                "Figure 5 illustrates the change in (weighted) mutual information for MIl and WMIl.",
                "As expected, mutual information for MIl increases monotonically with the number of steps, while WMIl does not.",
                "Finally, MIl and WMIl are scalable, with computational complexity shown in Figure 6.",
                "One advantage for our approach based on MI is that removing stop words is not required.",
                "Another important advantage is that there are no necessary hyper parameters to adjust.",
                "In single-document segmentation, the performance based on MI is even better for that based on WMI, so no extra hyper parameter is required.",
                "In multi-document segmentation, we show in the experiment, a = 1 and b = 1 is the best.",
                "Our method gives more weights to cue terms.",
                "However, usually cue terms or sentences appear at the beginning of a segment, while the end of the segment may be 1 2 5 10 20 34 51 102 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 Document Number ErrorRate MIl :a=0,b=0 WMI l :a=1,b=1 WMI l :a=1,b=0 WMI l :a=2,b=1 Figure 3: Error rates for different hyper parameters of term weights. 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Normalized Document Entropy NormalizedSegmentEntropy Noisy words Cue words Common stop words Doc−dep stop words Figure 4: Term weights learned from the whole training set. 0 100 200 300 400 500 600 0.06 0.08 0.1 0.12 0.14 0.16 0.18 Number of Steps (Weighted)MutualInformation MI l WMI l Figure 5: Change in (weighted) MI for MIl and WMIl. 0 20 40 60 80 100 120 0 200 400 600 800 1000 1200 1400 1600 1800 2000 Document Number TimetoConverge(sec) MI l WMI l Figure 6: Time to converge for MIl and WMIl. much noisy.",
                "One possible solution is giving more weights to terms at the beginning of each segment.",
                "Moreover, when the length of segments are quite different, long segments have much higher term frequencies, so they may dominate the segmentation boundaries.",
                "Normalization of term frequencies versus the segment length may be useful. 6.",
                "CONCLUSIONS AND FUTURE WORK We proposed a novel method for multi-document topic segmentation and alignment based on weighted mutual information, which can also handle single-document cases.",
                "We used dynamic programming to optimize our algorithm.",
                "Our approach outperforms all the previous methods on singledocument cases.",
                "Moreover, we also showed that doing segmentation among multiple documents can improve the performance tremendously.",
                "Our results also illustrated that using weighted mutual information can utilize the information of multiple documents to reach a better performance.",
                "We only tested our method on limited data sets.",
                "More data sets especially complicated ones should be tested.",
                "More previous methods should be compared with.",
                "Moreover, natural segmentations like paragraphs are hints that can be used to find the optimal boundaries.",
                "Supervised learning also can be considered. 7.",
                "ACKNOWLEDGMENTS The authors want to thank Xiang Ji, and Prof. J. Scott Payne for their help. 8.",
                "REFERENCES [1] A. Banerjee, I. Ghillon, J. Ghosh, S. Merugu, and D. Modha.",
                "A generalized maximum entropy approach to bregman co-clustering and matrix approximation.",
                "In Proceedings of SIGKDD, 2004. [2] R. Bekkerman, R. El-Yaniv, and A. McCallum.",
                "Multi-way distributional clustering via pairwise interactions.",
                "In Proceedings of ICML, 2005. [3] D. M. Blei and P. J. Moreno.",
                "Topic segmentation with an aspect hidden markov model.",
                "In Proceedings of SIGIR, 2001. [4] D. M. Blei, A. Ng, and M. Jordan.",
                "Latent dirichlet allocation.",
                "Journal of Machine Learning Research, 3:993-1022, 2003. [5] T. Brants, F. Chen, and I. Tsochantaridis.",
                "Topic-based document segmentation with probabilistic latent semantic analysis.",
                "In Proceedings of CIKM, 2002. [6] F. Choi.",
                "Advances in domain indepedent linear text segmentation.",
                "In Proceedings of the NAACL, 2000. [7] H. Christensen, B. Kolluru, Y. Gotoh, and S. Renals.",
                "Maximum entropy segmentation of broadcast news.",
                "In Proceedings of ICASSP, 2005. [8] T. Cover and J. Thomas.",
                "Elements of Information Theory.",
                "John Wiley and Sons, New York, USA, 1991. [9] S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and R. Harshman.",
                "Indexing by latent semantic analysis.",
                "Journal of the American Society for Information Systems, 1990. [10] I. Dhillon, S. Mallela, and D. Modha.",
                "Information-theoretic co-clustering.",
                "In Proceedings of SIGKDD, 2003. [11] M. Hajime, H. Takeo, and O. Manabu.",
                "Text segmentation with multiple surface linguistic cues.",
                "In Proceedings of COLING-ACL, 1998. [12] T. K. Ho.",
                "<br>stop word</br> location and identification for adaptive text recognition.",
                "International Journal of Document Analysis and Recognition, 3(1), August 2000. [13] T. Hofmann.",
                "Probabilistic latent semantic analysis.",
                "In Proceedings of the UAI99, 1999. [14] X. Ji and H. Zha.",
                "Correlating summarization of a pair of multilingual documents.",
                "In Proceedings of RIDE, 2003. [15] X. Ji and H. Zha.",
                "Domain-independent text segmentation using anisotropic diffusion and dynamic programming.",
                "In Proceedings of SIGIR, 2003. [16] X. Ji and H. Zha.",
                "Extracting shared topics of multiple documents.",
                "In Proceedings of the 7th PAKDD, 2003. [17] J. Lafferty, A. McCallum, and F. Pereira.",
                "Conditional random fields: Probabilistic models for segmenting and labeling sequence data.",
                "In Proceedings of ICML, 2001. [18] T. Li, S. Ma, and M. Ogihara.",
                "Entropy-based criterion in categorical clustering.",
                "In Proceedings of ICML, 2004. [19] A. McCallum, D. Freitag, and F. Pereira.",
                "Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of ICML, 2000. [20] L. Pevzner and M. Hearst.",
                "A critique and improvement of an evaluation metric for text segmentation.",
                "Computational Linguistic, 28(1):19-36, 2002. [21] J. C. Reynar.",
                "Statistical models for topic segmentation.",
                "In Proceedings of ACL, 1999. [22] G. Salton and M. McGill.",
                "Introduction to Modern Information Retrieval.",
                "McGraw Hill, 1983. [23] B.",
                "Sun, Q. Tan, P. Mitra, and C. L. Giles.",
                "Extraction and search of chemical formulae in text documents on the web.",
                "In Proceedings of WWW, 2007. [24] B.",
                "Sun, D. Zhou, H. Zha, and J.",
                "Yen.",
                "Multi-task text segmentation and alignment based on weighted mutual information.",
                "In Proceedings of CIKM, 2006. [25] M. Utiyama and H. Isahara.",
                "A statistical model for domain-independent text segmentation.",
                "In Proceedings of the 39th ACL, 1999. [26] C. Wayne.",
                "Multilingual topic detection and tracking: Successful research enabled by corpora and evaluation.",
                "In Proceedings of LREC, 2000. [27] J. Yamron, I. Carp, L. Gillick, S. Lowe, and P. van Mulbregt.",
                "A hidden markov model approach to text segmentation and event tracking.",
                "In Proceedings of ICASSP, 1998. [28] H. Zha and X. Ji.",
                "Correlating multilingual documents via bipartite graph modeling.",
                "In Proceedings of SIGIR, 2002."
            ],
            "original_annotated_samples": [
                "<br>stop word</br> location and identification for adaptive text recognition."
            ],
            "translated_annotated_samples": [
                "Detención de palabras y identificación para el reconocimiento de texto adaptativo."
            ],
            "translated_text": "La segmentación de temas con detección de temas compartidos y alineación de múltiples documentos Bingjun Sun*, Prasenjit Mitra*†, Hongyuan Zha‡, C. Lee Giles*†, John Yen*† *Departamento de Ciencias de la Computación e Ingeniería † Colegio de Ciencias de la Información y Tecnología Universidad Estatal de Pensilvania University Park, PA 16802 ‡ Colegio de Computación Instituto de Tecnología de Georgia Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu RESUMEN La detección y seguimiento de temas [26] y la segmentación de temas [15] juegan un papel importante en la captura de la información local y secuencial de los documentos. El trabajo previo en esta área suele centrarse en documentos individuales, aunque existen múltiples documentos similares en muchos dominios. En este artículo, presentamos un novedoso método no supervisado para la detección de temas compartidos y la segmentación de temas de múltiples documentos similares basado en la información mutua (MI) y la información mutua ponderada (WMI), que es una combinación de MI y pesos de términos. La idea básica es que la segmentación óptima maximiza MI (o WMI). Nuestro enfoque puede detectar temas compartidos entre documentos. Puede encontrar los límites óptimos en un documento y alinear segmentos entre documentos al mismo tiempo. También puede manejar la segmentación de un solo documento como un caso especial de la segmentación y alineación de varios documentos. Nuestros métodos pueden identificar y fortalecer términos de señal que pueden ser utilizados para la segmentación y eliminar parcialmente las palabras de parada mediante el uso de pesos de términos basados en la entropía aprendida de múltiples documentos. Nuestros resultados experimentales muestran que nuestro algoritmo funciona bien para las tareas de segmentación de un solo documento, detección de temas compartidos y segmentación de múltiples documentos. El uso de información de múltiples documentos puede mejorar enormemente el rendimiento de la segmentación de temas, y utilizar WMI es aún mejor que utilizar MI para la segmentación de múltiples documentos. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información-Agrupación; H.3.1 [Almacenamiento y Recuperación de Información]: Análisis de Contenido e Indexación-Procesamiento Lingüístico; I.2.7 [Inteligencia Artificial]: Procesamiento de Lenguaje Natural-Análisis de Texto; I.5.3 [Reconocimiento de Patrones]: Algoritmos de Agrupación-Medidas de Similitud Términos Generales Algoritmos, Diseño, Experimentación 1. INTRODUCCIÓN Muchos investigadores han trabajado en la detección y seguimiento de temas (TDT) [26] y la segmentación de temas durante la última década. La segmentación de temas tiene como objetivo identificar los límites en un documento con la meta de capturar la estructura temática latente. Las tareas de segmentación de temas generalmente se dividen en dos categorías [15]: la segmentación de flujos de texto, donde se identifica la transición de temas, y la segmentación coherente de documentos, en la que los documentos se dividen en subtemas. La primera categoría tiene aplicaciones en el reconocimiento automático del habla, mientras que la segunda tiene más aplicaciones como la consulta parcial de texto en documentos extensos en la recuperación de información, resumen de texto y medición de calidad de múltiples documentos. La investigación previa en relación con TDT se enmarca en la primera categoría, centrada en el seguimiento de temas de datos de discurso de transmisión y texto de agencia de noticias, mientras que la segunda categoría no ha sido estudiada muy bien. Los enfoques tradicionales realizan la segmentación de temas en los documentos uno a la vez [15, 25, 6]. La mayoría de ellos tienen un mal desempeño en tareas sutiles como la segmentación coherente de documentos [15]. A menudo, los usuarios finales buscan documentos que tengan un contenido similar. Los motores de búsqueda, como Google, proporcionan enlaces para obtener páginas similares. A una granularidad más fina, los usuarios pueden estar buscando obtener secciones de un documento similares a una sección en particular que presumiblemente discute un tema de interés para los usuarios. Por lo tanto, la extensión de la segmentación de temas desde documentos individuales para identificar segmentos similares de múltiples documentos similares con el mismo tema es una dirección natural y necesaria, y se espera que la segmentación de temas de múltiples documentos tenga un mejor rendimiento ya que se utiliza más información. Los enfoques tradicionales que utilizan la medición de similitud basada en la frecuencia de términos generalmente parten del mismo supuesto de que un vocabulario similar tiende a estar en un segmento de tema coherente [15, 25, 6]. Sin embargo, suelen tener el problema de identificar las palabras de parada. Por ejemplo, se eliminan las palabras de parada dependientes del documento junto con las palabras de parada genéricas en [15]. Hay dos razones por las que no eliminamos las palabras de parada directamente. Primero, identificar las palabras vacías es otro problema [12] que requiere estimación en cada dominio. Eliminar palabras comunes de parada puede resultar en la pérdida de información útil en un dominio específico. Segundo, aunque se pueden identificar las palabras de parada, la clasificación estricta de palabras de parada y palabras no de parada no puede representar el cambio gradual en la cantidad de contenido informativo de cada palabra. Empleamos una clasificación suave utilizando pesos de términos. En este documento, consideramos el problema de la segmentación de temas como un problema de optimización utilizando técnicas de teoría de la información para encontrar los límites óptimos de un documento dado el número de segmentos de texto, de manera que se minimice la pérdida de información mutua (MI) (o información mutua ponderada (WMI)) después de la segmentación y alineación. Esto es igual a maximizar el MI (o WMI). El MI se centra en medir la diferencia entre segmentos, mientras que investigaciones anteriores se enfocaban en encontrar la similitud (por ejemplo, la distancia del coseno) entre segmentos [15, 25, 6]. La alineación de temas de múltiples documentos similares se puede lograr agrupando las oraciones sobre el mismo tema en el mismo clúster. La segmentación de temas en un solo documento es solo un caso especial del problema de segmentación y alineación de temas en múltiples documentos. Los términos pueden ser co-agrupados como en [10] al mismo tiempo, dado el número de agrupaciones, pero nuestros resultados experimentales muestran que este método resulta en una peor segmentación (ver Tablas 1, 4 y 6). Por lo general, los lectores humanos pueden identificar la transición de temas basándose en palabras de señal, y pueden ignorar las palabras vacías. Inspirados por esto, asignamos a cada término (o grupo de términos) un peso basado en la entropía entre diferentes documentos y diferentes segmentos de documentos. No solo este enfoque puede aumentar la contribución de las palabras clave, sino que también puede disminuir el efecto de las palabras de parada comunes, palabras ruidosas y palabras de parada dependientes del documento. Estas palabras son comunes en un documento. Muchos métodos basados en la similitud de oraciones requieren que estas palabras sean eliminadas antes de que se pueda realizar la segmentación de temas [15]. Nuestros resultados en la Figura 3 muestran que los pesos de los términos son útiles para la segmentación y alineación de temas en múltiples documentos. La principal contribución de este artículo es que introduce un método novedoso para la segmentación de temas utilizando MI y demuestra que este método funciona mejor que los criterios previamente utilizados. Además, hemos abordado el problema de la segmentación y alineación de temas en varios documentos, mientras que la mayoría de las investigaciones existentes se centraron en la segmentación de documentos individuales. La segmentación y alineación de múltiples documentos pueden utilizar información de documentos similares y mejorar significativamente el rendimiento de la segmentación de temas. Obviamente, nuestro enfoque puede manejar documentos individuales como un caso especial cuando varios documentos no están disponibles. Puede detectar temas compartidos entre documentos para determinar si son múltiples documentos sobre el mismo tema. También introducimos el nuevo criterio de WMI basado en pesos de términos aprendidos de múltiples documentos similares, lo cual puede mejorar aún más el rendimiento de la segmentación de temas. Proponemos un algoritmo voraz iterativo basado en programación dinámica y demostramos que funciona bien en la práctica. Algunos de nuestros trabajos anteriores están en [24]. El resto de este documento está organizado de la siguiente manera: En la Sección 2, revisamos el trabajo relacionado. La sección 3 contiene una formulación del problema de segmentación de temas y alineación de múltiples documentos con co-clustering de términos, una revisión del criterio de MI para el clustering, y finalmente una introducción a WMI. En la Sección 4, primero proponemos el algoritmo voraz iterativo de segmentación y alineación de temas con co-clustering de términos, y luego describimos cómo el algoritmo puede ser optimizado mediante programación dinámica. En la Sección 5, se describen experimentos sobre la segmentación de documentos individuales, la detección de temas compartidos y la segmentación de documentos múltiples, y se presentan y discuten los resultados para evaluar el rendimiento de nuestro algoritmo. Las conclusiones y algunas direcciones futuras del trabajo de investigación se discuten en la Sección 6.2. TRABAJO PREVIO En general, los enfoques existentes para la segmentación de texto se dividen en dos categorías: aprendizaje supervisado [19, 17, 23] y aprendizaje no supervisado [3, 27, 5, 6, 15, 25, 21]. El aprendizaje supervisado suele tener un buen rendimiento, ya que aprende funciones a partir de conjuntos de entrenamiento etiquetados. Sin embargo, obtener conjuntos de entrenamiento grandes con etiquetas manuales en oraciones de documentos suele ser prohibitivamente costoso, por lo que se prefieren enfoques no supervisados. Algunos modelos consideran la dependencia entre oraciones y secciones, como el Modelo Oculto de Markov [3, 27], el Modelo de Máxima Entropía de Markov [19] y los Campos Aleatorios Condicionales [17], mientras que muchos otros enfoques se basan en la cohesión léxica o la similitud de las oraciones [5, 6, 15, 25, 21]. Algunos enfoques también se centran en las palabras clave como pistas de transiciones de tema [11]. Si bien algunos métodos existentes solo consideran información en documentos individuales [6, 15], otros utilizan varios documentos [16, 14]. No hay muchas obras en la última categoría, aunque se espera que el rendimiento de la segmentación sea mejor con la utilización de información de múltiples documentos. Investigaciones previas estudiaron métodos para encontrar temas compartidos [16] y segmentación y resumen de temas entre solo un par de documentos [14]. La clasificación y agrupación de textos es un área de investigación relacionada que categoriza documentos en grupos utilizando métodos supervisados o no supervisados. La clasificación o agrupación temática es una dirección importante en esta área, especialmente el co-agrupamiento de documentos y términos, como LSA [9], PLSA [13], y enfoques basados en distancias y particionamiento de gráficos bipartitos [28] o máxima MI [2, 10], o máxima entropía [1, 18]. Los criterios de estos enfoques pueden ser utilizados en el tema de la segmentación de temas. Algunos de esos métodos se han extendido al área de la segmentación de temas, como PLSA [5] y máxima entropía [7], pero hasta donde sabemos, el uso de MI para la segmentación de temas no ha sido estudiado. 3. FORMULACIÓN DEL PROBLEMA Nuestro objetivo es segmentar documentos y alinear los segmentos entre documentos (Figura 1). Sea T el conjunto de términos {t1, t2, ..., tl}, que aparecen en el conjunto no etiquetado de documentos D = {d1, d2, ..., dm}. Sea Sd el conjunto de oraciones para el documento d ∈ D, es decir, {s1, s2, ..., snd}. Tenemos una matriz tridimensional de frecuencia de términos, en la que las tres dimensiones son variables aleatorias de D, Sd y T. Sd en realidad es un vector aleatorio que incluye una variable aleatoria para cada d ∈ D. La frecuencia de términos se puede utilizar para estimar la distribución de probabilidad conjunta P(D, Sd, T), que es p(t, d, s) = T(t, d, s)/ND, donde T(t, d, s) es el número de t en la oración s de d y ND es el número total de términos en D. ˆS representa el conjunto de segmentos {ˆs1, ˆs2, ..., ˆsp} después de la segmentación y alineación entre varios documentos, donde el número de segmentos | ˆS| = p. Un segmento ˆsi del documento d es una secuencia de oraciones adyacentes en d. Dado que para diferentes documentos si pueden discutir diferentes subtemas, nuestro objetivo es agrupar oraciones adyacentes en cada documento en segmentos, y alinear segmentos similares entre documentos, de modo que para diferentes documentos ˆsi trate sobre el mismo subtema. El objetivo es encontrar la segmentación óptima de temas y el mapeo de alineación Segd(si) : {s1, s2, ..., snd } → {ˆs1, ˆs2, ..., ˆsp} y Alid(ˆsi) : {ˆs1, ˆs2, ..., ˆsp} → {ˆs1, ˆs2, ..., ˆsp}, para todo d ∈ D, donde ˆsi es el i-ésimo segmento con la restricción de que solo las oraciones adyacentes pueden ser asignadas al mismo segmento, es decir, para d, {si, si+1, ..., sj} → {ˆsq}, donde q ∈ {1, ..., p}, donde p es el número de segmento, y si i > j, entonces para d, ˆsq está ausente. Después de la segmentación y alineación, el vector aleatorio Sd se convierte en una variable aleatoria alineada ˆS. Por lo tanto, P(D, Sd, T) se convierte en P(D, ˆS, T). El término co-clustering es una técnica que se ha empleado [10] para mejorar la precisión del agrupamiento de documentos. Evaluamos el efecto de ello para la segmentación de temas. Un término t se asigna a exactamente un grupo de términos. El término co-clustering implica encontrar simultáneamente el mapeo óptimo de agrupación de términos Clu(t) : {t1, t2, ..., tl} → {ˆt1, ˆt2, ..., ˆtk}, donde k ≤ l, l es el número total de palabras en todos los documentos, y k es el número de clústeres. 4. METODOLOGÍA Ahora describimos un algoritmo novedoso que puede manejar la segmentación de un solo documento, la detección de temas compartidos y la segmentación y alineación de múltiples documentos basados en MI o WMI. 4.1 Información Mutua MI I(X; Y) es una cantidad que mide la cantidad de información contenida en dos o más variables aleatorias [8, 10]. Para el caso de dos variables aleatorias, tenemos I(X; Y) = x∈X y∈Y p(x, y)log p(x, y) p(x)p(y) , (1) Obviamente, cuando las variables aleatorias X e Y son independientes, I(X; Y) = 0. Por lo tanto, de manera intuitiva, el valor de MI depende de cómo las variables aleatorias están relacionadas entre sí. La co-agrupación óptima es el mapeo Clux: X → ˆX y Cluy: Y → ˆY que minimiza la pérdida: I(X; Y) - I(ˆX; ˆY), lo cual es igual a maximizar I(ˆX; ˆY). Este es el criterio de MI para la agrupación. En el caso de la segmentación de temas, las dos variables aleatorias son la variable de término T y la variable de segmento S, y cada muestra es una ocurrencia de un término T = t en un segmento particular S = s. Se utiliza I(T; S) para medir qué tan dependientes son T y S. Sin embargo, no se puede calcular I(T; S) para documentos antes de la segmentación, ya que no tenemos un conjunto de S debido a que las oraciones del Documento d, si ∈ Sd, no están alineadas con otros documentos. Así, en lugar de minimizar la pérdida de MI, podemos maximizar MI después de la segmentación de temas, calculada como: I( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS p(ˆt, ˆs)log p(ˆt, ˆs) p(ˆt)p(ˆs) , (2) donde p(ˆt, ˆs) se estiman mediante la frecuencia de términos tf del Cluster de Términos ˆt y el Segmento ˆs en el conjunto de entrenamiento D. Nótese que aquí un segmento ˆs incluye oraciones sobre el mismo tema entre todos los documentos. La solución óptima es el mapeo Clut : T → ˆT, Segd : Sd → ˆS, y Alid : ˆS → ˆS, que maximiza I( ˆT; ˆS). 4.2 Información Mutua Ponderada En la segmentación de temas y alineación de múltiples documentos, si se conoce P(D, ˆS, T), basado en las distribuciones marginales P(D|T) y P( ˆS|T) para cada término t ∈ T, podemos categorizar los términos en cuatro tipos en el conjunto de datos: • Las palabras de parada comunes son comunes a lo largo de las dimensiones de documentos y segmentos. • Las palabras de parada dependientes del documento que dependen del estilo de escritura personal son comunes solo a lo largo de la dimensión de segmentos para algunos documentos. • Las palabras clave son los elementos más importantes para la segmentación. Son comunes a lo largo de la dimensión de los documentos solo para el mismo segmento, y no son comunes a lo largo de las dimensiones de los segmentos. • Las palabras ruidosas son otras palabras que no son comunes a lo largo de ambas dimensiones. La entropía basada en P(D|T) y P( ˆS|T) se puede utilizar para identificar diferentes tipos de términos. Para reforzar la contribución de las palabras clave en el cálculo de MI, y al mismo tiempo reducir el efecto de los otros tres tipos de palabras, similar a la idea del peso tf-idf [22], utilizamos las entropías de cada término a lo largo de las dimensiones del documento D y del segmento ˆS, es decir, ED(ˆt) y EˆS(ˆt), para calcular el peso. Una palabra de señal generalmente tiene un valor alto de ED(ˆt) pero un valor bajo de EˆS(ˆt). Introducimos los pesos de términos (o pesos de grupos de términos) wˆt = ( ED(ˆt) maxˆt ∈ ˆT (ED(ˆt )) )a (1 − EˆS(ˆt) maxˆt ∈ ˆT (EˆS(ˆt )) )b , (3) donde ED(ˆt) = d∈D p(d|ˆt)log|D| 1 p(d|ˆt) , EˆS(ˆt) = ˆs∈ ˆS p(ˆs|ˆt)log| ˆS| 1 p(ˆs|ˆt) , y a > 0 y b > 0 son potencias para ajustar los pesos de términos. Normalmente a = 1 y b = 1 por defecto, y se utilizan maxˆt ∈ ˆT (ED(ˆt )) y maxˆt ∈ ˆT (EˆS(ˆt )) para normalizar los valores de entropía. Los pesos de los grupos de términos se utilizan para ajustar p(ˆt, ˆs), pw(ˆt, ˆs) = wˆtp(ˆt, ˆs) ˆt∈ ˆT ;ˆs∈ ˆS wˆtp(ˆt, ˆs) , (4) e Iw( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS pw(ˆt, ˆs)log pw(ˆt, ˆs) pw(ˆt)pw(ˆs) , (5) donde pw(ˆt) y pw(ˆs) son distribuciones marginales de pw(ˆt, ˆs). Sin embargo, dado que no conocemos ni los pesos de los términos ni P(D, ˆS, T), necesitamos estimarlos, pero wˆt depende de p(ˆs|t) y ˆS, mientras que ˆS y p(ˆs|t) también dependen de wˆt, que aún es desconocido. Por lo tanto, se requiere un algoritmo iterativo para estimar los pesos de los términos wˆt y encontrar la mejor segmentación y alineación para optimizar la función objetivo Iw de manera concurrente. Después de que un documento se segmenta en oraciones, se introduce: la distribución de probabilidad conjunta P(D, Sd, T), el número de segmentos de texto p ∈ {2, 3, ..., máx(sd)}, el número de grupos de términos k ∈ {2, 3, ..., l} (si k = l, no se requiere co-clustering de términos) y el tipo de peso w ∈ {0, 1}, indicando usar I o Iw, respectivamente. Mapeo de Clu, Seg, Ali y pesos de términos wˆt. Inicialización: 0. i = 0. Inicializar Clu (0) t, Seg (0) d y Ali (0) d; Inicializar w (0) ˆt usando la Ecuación (6) si w = 1; Etapa 1: 1. Si |D| = 1, k = l y w = 0, verifica todas las segmentaciones secuenciales de d en p segmentos y encuentra la mejor Segd(s) = argmaxˆsI( ˆT; ˆS), y devuelve Segd; de lo contrario, si w = 1 y k = l, ve a 3.1; Etapa 2: 2.1 Si k < l, para cada término t, encuentra el mejor clúster ˆt como Clu(i+1)(t) = argmaxˆtI( ˆT; ˆS(i)) basado en Seg(i) y Ali(i); 2.2 Para cada d, verifica todas las segmentaciones secuenciales de d en p segmentos con mapeo s → ˆs → ˆs, y encuentra la mejor Ali (i+1) d (Seg (i+1) d (s)) = argmaxˆsI( ˆT(i+1); ˆS) basado en Clu(i+1)(t) si k < l o Clu(0)(t) si k = l; 2.3 i + +. Si Clu, Seg o Ali cambian, ve a 2.1; de lo contrario, si w = 0, devuelve Clu(i), Seg(i) y Ali(i); de lo contrario, j = 0, ve a 3.1; Etapa 3: 3.1 Actualiza w (i+j+1) ˆt basado en Seg(i+j), Ali(i+j) y Clu(i) usando la Ecuación (3); 3.2 Para cada d, verifica todas las segmentaciones secuenciales de d en p segmentos con mapeo s → ˆs → ˆs, y encuentra la mejor Ali (i+j+1) d (Seg (i+j+1) d (s)) = argmaxˆsIw( ˆT(i); ˆS) basado en Clu(i) y w (i+j+1) ˆt; 3.3 j + +. Si Iw( ˆT; ˆS) cambia, ve al paso 6; de lo contrario, detente y devuelve Clu(i), Seg(i+j), Ali(i+j), y w (i+j) ˆt; Figura 2: Algoritmo: Segmentación y alineación de temas basada en MI o WMI. y cada oración se segmenta en palabras, cada palabra se reduce a su raíz. Entonces se puede estimar la distribución de probabilidad conjunta P(D, Sd, T). Finalmente, esta distribución se puede utilizar para calcular la MI en nuestro algoritmo. 4.3 Algoritmo Voraz Iterativo Nuestro objetivo es maximizar la función objetivo, I( ˆT; ˆS) o Iw( ˆT; ˆS), que puede medir la dependencia de las ocurrencias de términos en diferentes segmentos. Generalmente, primero no conocemos los pesos estimados de los términos, los cuales dependen de la segmentación y alineación óptimas de los temas, y los grupos de términos. Además, este problema es NP-duro [10], incluso si conocemos los pesos de los términos. Por lo tanto, se desea un algoritmo voraz iterativo para encontrar la mejor solución, aunque probablemente solo se alcancen máximos locales. Presentamos el algoritmo voraz iterativo en la Figura 2 para encontrar un máximo local de I( ˆT; ˆS) o Iw( ˆT; ˆS) con estimación simultánea de pesos de términos. Este algoritmo es iterativo y codicioso para casos de múltiples documentos o casos de un solo documento con estimación de peso de términos y/o co-clustering de términos. De lo contrario, dado que es solo un algoritmo de un paso para resolver la tarea de segmentación de un solo documento [6, 15, 25], se garantiza el máximo global de MI. Mostraremos más adelante que la coagrupación de términos reduce la precisión de los resultados y no es necesaria, y para la segmentación de un solo documento, los pesos de los términos tampoco son requeridos. 4.3.1 Inicialización En el Paso 0, la agrupación inicial de términos Clut y la segmentación y alineación de temas Segd y Alid son importantes para evitar máximos locales y reducir el número de iteraciones. Primero, se puede hacer una buena estimación de los pesos de los términos utilizando las distribuciones de frecuencia de términos a lo largo de las oraciones para cada documento y promediándolas para obtener los valores iniciales de wˆt: wt = ( ED(t) maxt ∈T (ED(t )) )(1 − ES(t) maxt ∈T (ES(t )) ), (6) donde ES(t) = 1 |Dt| d∈Dt (1 − s∈Sd p(s|t)log|Sd| 1 p(s|t) ), donde Dt es el conjunto de documentos que contienen el término t. Luego, para la segmentación inicial Seg(0), simplemente podemos segmentar los documentos de manera equitativa por oraciones. O podemos encontrar la segmentación óptima solo para cada documento d que maximice el WMI, Seg (0) d = argmaxˆsIw(T; ˆS), donde w = w (0) ˆt. Para el alineamiento inicial Ali(0), podemos primero asumir que el orden de los segmentos para cada d es el mismo. Para el agrupamiento inicial de términos Clu(0), las etiquetas de los primeros grupos pueden establecerse al azar, y después de la primera vez del Paso 3, se obtiene un buen agrupamiento inicial de términos. 4.3.2 Diferentes Casos Después de la inicialización, hay tres etapas para diferentes casos. En total hay ocho casos, |D| = 1 o |D| > 1, k = l o k < l, w = 0 o w = 1. La segmentación de un solo documento sin agrupamiento de términos y estimación de peso de términos (|D| = 1, k = l, w = 0) solo requiere la Etapa 1 (Paso 1). Si se requiere el agrupamiento de términos (k < l), la Etapa 2 (Paso 2.1, 2.2 y 2.3) se ejecuta de forma iterativa. Si se requiere la estimación del peso del término (w = 1), la Etapa 3 (Paso 3.1, 3.2 y 3.3) se ejecuta de forma iterativa. Si ambos son requeridos (k < l, w = 1), la Etapa 2 y 3 se ejecutan una tras otra. Para la segmentación de múltiples documentos sin agrupamiento de términos y estimación de peso de términos (|D| > 1, k = l, w = 0), solo se requiere la iteración de los Pasos 2.2 y 2.3. En la Etapa 1, el máximo global se puede encontrar basándose en I( ˆT; ˆS) utilizando programación dinámica en la Sección 4.4. Es imposible encontrar simultáneamente un buen agrupamiento de términos y pesos estimados de términos, ya que al mover un término a un nuevo grupo de términos para maximizar Iw( ˆT; ˆS), no sabemos si el peso de este término debería ser el del nuevo grupo o el del antiguo. Por lo tanto, primero realizamos el agrupamiento de términos en la Etapa 2, y luego estimamos los pesos de los términos en la Etapa 3. En la Etapa 2, el Paso 2.1 consiste en encontrar el mejor agrupamiento de términos y el Paso 2.2 consiste en encontrar la mejor segmentación. Este ciclo se repite para encontrar un máximo local basado en MI I hasta que converge. Los dos pasos son: (1) basados en el agrupamiento de términos actual Cluˆt, para cada documento d, el algoritmo segmenta todas las oraciones Sd en p segmentos secuencialmente (algunos segmentos pueden estar vacíos), y los coloca en los p segmentos ˆS del conjunto de entrenamiento completo D (se verifican todos los casos posibles de segmentación diferente Segd y alineación Alid) para encontrar el caso óptimo, y (2) basado en la segmentación y alineación actual, para cada término t, el algoritmo encuentra el mejor grupo de términos de t basado en la segmentación actual Segd y la alineación Alid. Después de encontrar un buen agrupamiento de términos, se estiman los pesos de los términos si w = 1. En la Etapa 3, al igual que en la Etapa 2, el Paso 3.1 es la reestimación del peso del término y el Paso 3.2 es encontrar una mejor segmentación. Se repiten para encontrar un máximo local basado en WMI Iw hasta que converja. Sin embargo, si el agrupamiento en la Etapa 2 no es preciso, entonces la estimación de peso en la Etapa 3 puede tener un mal resultado. Finalmente, en el Paso 3.3, este algoritmo converge y devuelve la salida. Este algoritmo puede manejar tanto la segmentación de un solo documento como la de múltiples documentos. También puede detectar temas compartidos entre documentos al verificar la proporción de frases superpuestas sobre los mismos temas, como se describe en la Sección 5.2. 4.4 Optimización del algoritmo. En muchos trabajos anteriores sobre segmentación, la programación dinámica es una técnica utilizada para maximizar la función objetivo. De manera similar, en los Pasos 1, 2.2 y 3.2 de nuestro algoritmo, podemos utilizar programación dinámica. Para la Etapa 1, utilizando programación dinámica aún se puede encontrar el óptimo global, pero para la Etapa 2 y la Etapa 3, solo podemos encontrar el óptimo para cada paso de segmentación de temas y alineación de un documento. Aquí solo mostramos la programación dinámica para el Paso 3.2 utilizando WMI (el Paso 1 y 2.2 son similares pero pueden usar I o Iw). Hay dos casos que no se muestran en el algoritmo de la Figura 2: (a) segmentación de un solo documento o segmentación de múltiples documentos con el mismo orden secuencial de segmentos, donde no se requiere alineación, y (b) segmentación de múltiples documentos con diferentes órdenes secuenciales de segmentos, donde la alineación es necesaria. La función de mapeo de alineación del primer caso es simplemente Alid(ˆsi) = ˆsi, mientras que para los últimos casos la función de mapeo de alineación es Alid(ˆsi) = ˆsj, donde i y j pueden ser diferentes. Los pasos computacionales para los dos casos se enumeran a continuación: Caso 1 (sin alineación): Para cada documento d: (1) Calcular pw(ˆt), pw parcial(ˆt, ˆs) y pw parcial(ˆs) sin contar las oraciones de d. Luego, colocar las oraciones de i a j en la Parte k, y calcular WMI parcial PIw( ˆT; ˆsk(si, si+1, ..., sj)) ˆt∈ ˆT pw(ˆt, ˆsk)log pw(ˆt, ˆsk) pw(ˆt)pw(ˆsk) , donde Alid(si, si+1, ..., sj) = k, k ∈ {1, 2, ..., p}, 1 ≤ i ≤ j ≤ nd, y Segd(sq) = ˆsk para todo i ≤ q ≤ j. (2) Dejar que M(sm, 1) = PIw( ˆT; ˆs1(s1, s2, ..., sm)). Entonces M(sm, L) = maxi[M(si−1, L − 1) + PIw( ˆT; ˆsL(si, ..., sm))], donde 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, y cuando i > m, no se colocan oraciones en ˆsk al calcular PIw (nota PIw( ˆT; ˆs(si, ..., sm)) = 0 para la segmentación de un solo documento). (3) Finalmente M(snd , p) = maxi[M(si−1, p − 1)+ PIw( ˆT; ˆsp(si, ..., snd ))], donde 1 ≤ i ≤ nd+1. Se encuentra el Iw óptimo y la segmentación correspondiente es la mejor. Caso 2 (alineación requerida): Para cada documento d: (1) Calcular pw(ˆt), pw parcial(ˆt, ˆs), y pw parcial(ˆs), y PIw( ˆT; ˆsk(si, si+1, ..., sj)) de manera similar al Caso 1. (2) Sea M(sm, 1, k) = PIw( ˆT; ˆsk(s1, s2, ..., sm)), donde k ∈ {1, 2, ..., p}. Entonces M(sm, L, kL) = maxi,j[M(si−1, L − 1, kL/j) + PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., sm))], donde 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, kL ∈ Set(p, L), que es el conjunto de todos los p! Hay L!(p−L)! combinaciones de L segmentos elegidos de entre todos los p segmentos, j ∈ kL, el conjunto de L segmentos elegidos de entre todos los p segmentos, y kL/j es la combinación de L − 1 segmentos en kL excepto el Segmento j. (3) Finalmente, M(snd , p, kp) = maxi,j[M(si−1, p − 1, kp/j) +PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., snd ))], donde kp es simplemente la combinación de todos los p segmentos y 1 ≤ i ≤ nd + 1, que es el óptimo Iw y la segmentación correspondiente es la mejor. Los pasos de los Casos 1 y 2 son similares, excepto que en el Caso 2, se considera el alineamiento además de la segmentación. Primero, se calculan los elementos básicos de probabilidad para computar Iw excluyendo el documento d, y luego se calcula el WMI parcial colocando cada segmento secuencial posible (incluido el segmento vacío) de d en cada segmento del conjunto. Segundo, se encuentra la suma óptima de PIw para L segmentos y las m oraciones más a la izquierda, M(sm, L). Finalmente, se encuentra el WMI máximo entre diferentes sumas de M(sm, p − 1) y PIw para el Segmento p. 5. EXPERIMENTOS En esta sección se probará la segmentación de un solo documento, la detección de temas compartidos y la segmentación de múltiples documentos. Se estudian diferentes hiperparámetros de nuestro método. Para mayor comodidad, nos referimos al método utilizando I como MIk si w = 0, e Iw como WMIk si w = 2 o como WMIk si w = 1, donde k es el número de grupos de términos, y si k = l, donde l es el número total de términos, entonces no se requiere agrupación de términos, es decir, El primer conjunto de datos que probamos es uno sintético utilizado en investigaciones anteriores [6, 15, 25] y muchos otros artículos. Tiene 700 muestras. Cada uno es una concatenación de diez segmentos. Cada segmento es la primera oración seleccionada al azar de las n primeras oraciones del corpus Brown, que se supone que tienen un tema diferente entre sí. Actualmente, los mejores resultados en este conjunto de datos son logrados por Ji et al. [15]. Para comparar el rendimiento de nuestros métodos, se aplica el criterio ampliamente utilizado en investigaciones anteriores en lugar del criterio imparcial introducido en [20]. Elige un par de palabras al azar. Si se encuentran en segmentos diferentes (diferentes) para la segmentación real (real), pero se predicen (pred) como en el mismo segmento, es un error. Si están en el mismo segmento (mismo), pero se predice que están en segmentos diferentes, es una falsa alarma. Por lo tanto, la tasa de error se calcula utilizando la siguiente ecuación: p(err|real, pred) = p(miss|real, pred, diff)p(diff|real) + p(false alarm|real, pred, same)p(same|real). 5.1.2 Resultados del Experimento Probamos el caso cuando se conoce el número de segmentos. La Tabla 1 muestra los resultados de nuestros métodos con diferentes valores de hiperparámetros y tres enfoques anteriores, C99[25], U00[6] y ADDP03[15], en este conjunto de datos cuando se conoce el número de segmento. En WMI para la segmentación de un solo documento, los pesos de los términos se calculan de la siguiente manera: wˆt = 1−EˆS(ˆt)/maxˆt ∈ ˆT (EˆS(ˆt)). Para este caso, nuestros métodos MIl y WMIl superan a todos los enfoques anteriores. Comparamos nuestros métodos con ADDP03 utilizando una prueba t de una muestra unilateral y los valores de p se muestran en la Tabla 2. A partir de los valores p, podemos ver que en su mayoría las diferencias son muy significativas. También comparamos las tasas de error entre nuestros dos métodos utilizando una prueba t de dos muestras de dos colas para verificar la hipótesis de que son iguales. No podemos rechazar la hipótesis de que son iguales, por lo que las diferencias no son significativas, a pesar de que todas las tasas de error para MIl son más pequeñas que las de WMIl. Sin embargo, podemos concluir que los pesos de los términos contribuyen poco en la segmentación de un solo documento. Los resultados también muestran que el uso de MI con co-clustering de términos (k = 100) disminuye el rendimiento. Probamos diferentes números de grupos de términos y encontramos que el rendimiento mejora cuando el número de grupos aumenta hasta llegar a l. WMIk<l tiene resultados similares que no mostramos en la tabla. Como se mencionó anteriormente, el uso de MI puede ser inconsistente en los límites óptimos dados diferentes números de segmentos. Esta situación ocurre especialmente cuando las similitudes entre los segmentos son bastante diferentes, es decir, algunas transiciones son muy obvias, mientras que otras no lo son. Esto se debe a que normalmente un documento es una estructura jerárquica en lugar de una estructura únicamente secuencial. Cuando los segmentos no están en el mismo nivel, esta situación puede ocurrir. Por lo tanto, se desea un enfoque de segmentación jerárquica de temas, y la estructura depende en gran medida del número de segmentos para cada nodo interno y del criterio de parada de la división. Para este conjunto de datos de segmentación de un solo documento, dado que es solo un conjunto sintético, que es simplemente una concatenación de varios segmentos sobre diferentes temas, es razonable que enfoques simplemente basados en la frecuencia de términos tengan un buen rendimiento. Normalmente, para las tareas de segmentación de documentos coherentes en subtemas, la efectividad disminuye mucho. 5.2 Detección de Temas Compartidos 5.2.1 Datos de Prueba y Evaluación El segundo conjunto de datos contiene 80 artículos de noticias de Google News. Hay ocho temas y cada uno tiene 10 artículos. Dividimos aleatoriamente el conjunto en subconjuntos con diferentes números de documentos y cada subconjunto tiene los ocho temas. Comparamos nuestro enfoque MIl y WMIl con LDA [4]. LDA trata un documento en el conjunto de datos como una bolsa de palabras, encuentra su distribución en temas y su tema principal. MIl y WMIl ven cada oración como un conjunto de palabras y la etiquetan con una etiqueta de tema. Luego, para cada par de documentos, LDA determina si están en el mismo tema, mientras que MIl y Table 3: Detección de Temas Compartidos: Tasas de Error Promedio para Diferentes Números de Documentos en Cada Subconjunto #Doc 10 20 40 80 LDA 8.89% 16.33% 1.35% 0.60% MIl, θ = 0.6 4.17% 1.71% 1.47% 0.0% WMIl, θ = 0.8 18.6% 3.16% 1.92% 0.0% WMIl comprueba si la proporción de oraciones superpuestas en el mismo tema es mayor que el umbral ajustable θ. Es decir, en MIl y WMIl, para un par de documentos d, d , si [ s∈Sd,s ∈Sd 1(temas=temas )/min(|Sd|, |Sd|)] > θ, donde Sd es el conjunto de oraciones de d, y |Sd| es el número de oraciones de d, entonces d y d comparten el tema. Para un par de documentos seleccionados al azar, la tasa de error se calcula utilizando la siguiente ecuación: p(err|real, pred) = p(miss|real, pred, same)p(same|real) + p(false alarm|real, pred, diff)p(diff|real), donde un error significa si tienen el mismo tema (same) para el caso real (real), pero se predice (pred) como en un tema diferente. Si están en diferentes temas (diff), pero se predicen como en el mismo tema, es una falsa alarma. 5.2.2 Resultados del Experimento Los resultados se muestran en la Tabla 3. Si la mayoría de los documentos tienen diferentes temas, en WMIl, la estimación de los pesos de términos en la Ecuación (3) no es correcta. Por lo tanto, no se espera que WMIL tenga un mejor rendimiento que MIL cuando la mayoría de los documentos tienen temas diferentes. Cuando hay menos documentos en un subconjunto con el mismo número de temas, más documentos tienen temas diferentes, por lo que WMIl es peor que MIl. Podemos ver que en la mayoría de los casos, MIl tiene un rendimiento mejor (o al menos similar) que LDA. Después de la detección de temas compartidos, se puede llevar a cabo la segmentación de documentos con los temas compartidos. 5.3 Segmentación de múltiples documentos 5.3.1 Datos de prueba y evaluación Para la segmentación y alineación de múltiples documentos, nuestro objetivo es identificar estos segmentos sobre el mismo tema entre varios documentos similares con temas compartidos. Se espera que al utilizar pesos de términos, Iw funcione mejor que I, ya que sin pesos de términos, el resultado se ve seriamente afectado por palabras de parada dependientes del documento y palabras ruidosas que dependen del estilo de escritura personal. Es más probable tratar los mismos segmentos de diferentes documentos como segmentos diferentes bajo el efecto de palabras de parada dependientes del documento y palabras ruidosas. Los pesos de términos pueden reducir el efecto de las palabras de parada dependientes del documento y las palabras ruidosas al darle más peso a los términos de señal. El conjunto de datos para la segmentación y alineación de múltiples documentos tiene un total de 102 muestras y 2264 oraciones. Cada uno es la parte de introducción de un informe de laboratorio seleccionado del curso de Biol 240W, Universidad Estatal de Pensilvania. Cada muestra tiene dos segmentos, la introducción de las hormonas vegetales y el contenido en el laboratorio. El rango de longitud de las muestras va desde dos hasta 56 oraciones. Algunas muestras solo tienen una parte y algunas tienen un orden inverso de estos dos segmentos. No es difícil identificar el límite entre dos segmentos para los humanos. Etiquetamos cada oración manualmente para evaluación. El criterio de evaluación consiste en utilizar la proporción del número de oraciones con etiquetas de segmento incorrectas predichas en el número total de oraciones en toda la Tabla de entrenamiento. Para mostrar los beneficios de la segmentación y alineación de múltiples documentos, comparamos nuestro método con diferentes parámetros en diferentes particiones del mismo conjunto de entrenamiento. Excepto los casos en los que el número de documentos es 102 y uno (que son casos especiales de uso del conjunto completo y la segmentación pura de un solo documento), dividimos aleatoriamente el conjunto de entrenamiento en m particiones, y cada una tiene 51, 34, 20, 10, 5 y 2 muestras de documentos. Luego aplicamos nuestros métodos en cada partición y calculamos la tasa de error de todo el conjunto de entrenamiento. Cada caso se repitió 10 veces para calcular las tasas de error promedio. Para diferentes particiones del conjunto de entrenamiento, se utilizan diferentes valores de k, ya que el número de términos aumenta cuando el número de documentos en cada partición aumenta. 5.3.2 Resultados del Experimento De los resultados del experimento en la Tabla 4, podemos ver las siguientes observaciones: (1) Cuando el número de documentos aumenta, todos los métodos tienen un mejor rendimiento. Solo de uno a dos documentos, el MIL ha disminuido un poco. Podemos observar esto en la Figura 3 en el punto del número de documento = 2. La mayoría de las curvas incluso tienen los peores resultados en este punto. Hay dos razones. Primero, porque las muestras votan por la mejor segmentación y alineación de múltiples documentos, pero si solo se comparan dos documentos entre sí, aquel con segmentos faltantes o una secuencia totalmente diferente afectará la correcta segmentación y alineación del otro. Segundo, como se señaló al principio de esta sección, si dos documentos tienen más palabras de parada dependientes del documento o palabras ruidosas que palabras clave, entonces el algoritmo puede considerarlos como dos segmentos diferentes y faltar el otro segmento. Generalmente, solo podemos esperar un mejor rendimiento cuando el número de documentos es mayor que el número de segmentos. (2) Excepto en la segmentación de un solo documento, WMIl siempre es mejor que MIl, y cuando el número de documentos se acerca a uno o aumenta a un número muy grande, sus rendimientos se vuelven más cercanos. La Tabla 5 muestra los valores p de la prueba t de dos muestras unilaterales entre MIl y WMIl. También podemos observar esta tendencia a partir de los valores de p. Cuando el número de documento es igual a 5, alcanzamos el valor p más pequeño y la mayor diferencia entre las tasas de error de MIl y WMIl. Para la Tabla 6 de un solo documento: Segmentación de múltiples documentos: Tasa de error promedio para el Documento Número = 5 en cada subconjunto con Diferente Número de Agrupaciones de Términos #Agrupaciones 75 100 150 250 l MIk 24.67% 24.54% 23.91% 22.59% 15.77% de segmentación, WMIl es incluso un poco peor que MIl, lo cual es similar a los resultados de la segmentación de un solo documento en el primer conjunto de datos. La razón es que para la segmentación de un solo documento, no podemos estimar con precisión los pesos de los términos, ya que no tenemos disponibles varios documentos. (3) El uso de agrupamiento de términos generalmente produce resultados peores que MIl y WMIl. (4) El uso de agrupamiento de términos en WMIk es aún peor que en MIk, ya que en WMIk los grupos de términos se encuentran primero utilizando I antes que Iw. Si los grupos de términos no son correctos, entonces los pesos de los términos se estiman peor, lo que puede llevar a que el algoritmo obtenga resultados aún peores. De los resultados también encontramos que en la segmentación y alineación de múltiples documentos, la mayoría de los documentos con segmentos faltantes y en orden inverso son identificados correctamente. La Tabla 6 ilustra los resultados del experimento para el caso de 20 particiones (cada una con cinco muestras de documentos) del conjunto de entrenamiento y la segmentación y alineación de temas utilizando MIk con diferentes números de grupos de términos k. Observa que cuando el número de grupos de términos aumenta, la tasa de error disminuye. Sin agrupamiento de términos, obtenemos el mejor resultado. No mostramos resultados para WMIk con agrupamiento de términos, pero los resultados son similares. También probamos WMIL con diferentes hiperparámetros de a y b para ajustar los pesos de los términos. Los resultados se presentan en la Figura 3. Se demostró que el caso predeterminado WMIl: a = 1, b = 1 dio los mejores resultados para diferentes particiones del conjunto de entrenamiento. Podemos observar la tendencia de que cuando el número de documentos es muy pequeño o grande, la diferencia entre MIl: a = 0, b = 0 y WMIl: a = 1, b = 1 se vuelve bastante pequeña. Cuando el número de documentos no es grande (aproximadamente de 2 a 10), todos los casos que utilizan pesos de términos tienen un mejor rendimiento que MIl: a = 0, b = 0 sin pesos de términos, pero cuando el número de documentos aumenta, los casos WMIl: a = 1, b = 0 y WMIl: a = 2, b = 1 empeoran en comparación con MIl: a = 0, b = 0. Cuando el número de documentos se vuelve muy grande, son aún peores que los casos con números de documentos pequeños. Esto significa que es muy importante encontrar una forma adecuada de estimar los pesos de los términos para el criterio de WMI. La Figura 4 muestra los pesos de los términos aprendidos de todo el conjunto de entrenamiento. Cuatro tipos de palabras se categorizan de manera aproximada aunque la transición entre ellas es sutil. La Figura 5 ilustra el cambio en la información mutua (ponderada) para MIl y WMIl. Como era de esperar, la información mutua para MIl aumenta de forma monótona con el número de pasos, mientras que WMIl no lo hace. Finalmente, MIl y WMIl son escalables, con complejidad computacional mostrada en la Figura 6. Una ventaja de nuestro enfoque basado en MI es que no es necesario eliminar las palabras vacías. Otra ventaja importante es que no hay hiperparámetros necesarios que ajustar. En la segmentación de un solo documento, el rendimiento basado en MI es aún mejor que el basado en WMI, por lo que no se requiere un hiperparámetro adicional. En la segmentación de múltiples documentos, mostramos en el experimento que a = 1 y b = 1 es lo mejor. Nuestro método otorga más peso a los términos de señal. Sin embargo, generalmente los términos o frases de señal aparecen al principio de un segmento, mientras que el final del segmento puede ser 1 2 5 10 20 34 51 102 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 Número de Documento Tasa de Error MIl :a=0,b=0 WMI l :a=1,b=1 WMI l :a=1,b=0 WMI l :a=2,b=1 Figura 3: Tasas de error para diferentes hiperparámetros de pesos de términos. 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Entropía de Documento Normalizada Entropía de Segmento Normalizada Palabras ruidosas Palabras de señal Palabras de parada comunes Palabras de parada de Doc-dep Figura 4: Pesos de términos aprendidos de todo el conjunto de entrenamiento. 0 100 200 300 400 500 600 0.06 0.08 0.1 0.12 0.14 0.16 0.18 Número de Pasos (Ponderado) Información Mutua MI l WMI l Figura 5: Cambio en la MI (ponderada) para MIl y WMIl. 0 20 40 60 80 100 120 0 200 400 600 800 1000 1200 1400 1600 1800 2000 Número de Documento Tiempo de Convergencia (seg) MI l WMI l Figura 6: Tiempo de convergencia para MIl y WMIl. muy ruidoso. Una posible solución es dar más peso a los términos al principio de cada segmento. Además, cuando la longitud de los segmentos es bastante diferente, los segmentos largos tienen frecuencias de términos mucho más altas, por lo que pueden dominar los límites de segmentación. La normalización de las frecuencias de términos en relación con la longitud del segmento puede ser útil. 6. CONCLUSIONES Y TRABAJO FUTURO Propusimos un método novedoso para la segmentación y alineación de temas en múltiples documentos basado en información mutua ponderada, que también puede manejar casos de un solo documento. Utilizamos programación dinámica para optimizar nuestro algoritmo. Nuestro enfoque supera a todos los métodos anteriores en casos de un solo documento. Además, también demostramos que realizar segmentación entre varios documentos puede mejorar el rendimiento enormemente. Nuestros resultados también demostraron que el uso de la información mutua ponderada puede aprovechar la información de varios documentos para lograr un mejor rendimiento. Solo probamos nuestro método en conjuntos de datos limitados. Se deben probar más conjuntos de datos, especialmente los complicados. Deberían compararse más métodos anteriores. Además, las segmentaciones naturales como los párrafos son pistas que se pueden utilizar para encontrar los límites óptimos. El aprendizaje supervisado también puede ser considerado. 7. AGRADECIMIENTOS Los autores desean agradecer a Xiang Ji y al Prof. J. Scott Payne por su ayuda. 8. REFERENCIAS [1] A. Banerjee, I. Ghillon, J. Ghosh, S. Merugu y D. Modha. Un enfoque generalizado de entropía máxima para la coagrupación de Bregman y la aproximación de matrices. En Actas de SIGKDD, 2004. [2] R. Bekkerman, R. El-Yaniv y A. McCallum. Agrupación distribucional de múltiples vías a través de interacciones por pares. En Actas de ICML, 2005. [3] D. M. Blei y P. J. Moreno. Segmentación de temas con un modelo oculto de Markov de aspectos. En Actas de SIGIR, 2001. [4] D. M. Blei, A. Ng y M. Jordan. Asignación latente de Dirichlet. Revista de Investigación en Aprendizaje Automático, 3:993-1022, 2003. [5] T. Brants, F. Chen e I. Tsochantaridis. Segmentación de documentos basada en temas con análisis semántico latente probabilístico. En Actas de CIKM, 2002. [6] F. Choi. Avances en la segmentación lineal de texto independiente del dominio. En Actas de la NAACL, 2000. [7] H. Christensen, B. Kolluru, Y. Gotoh y S. Renals. Segmentación de máxima entropía de noticias de transmisión. En Actas de ICASSP, 2005. [8] T. Cover y J. Thomas. Elementos de la teoría de la información. John Wiley and Sons, Nueva York, EE. UU., 1991. [9] S. Deerwester, S. Dumais, G. Furnas, T. Landauer y R. Harshman. Indexación mediante análisis semántico latente. Revista de la Sociedad Americana de Sistemas de Información, 1990. [10] I. Dhillon, S. Mallela y D. Modha. Co-clustering informativo-teórico. En Actas de SIGKDD, 2003. [11] M. Hajime, H. Takeo y O. Manabu. Segmentación de texto con múltiples señales lingüísticas superficiales. En Actas de COLING-ACL, 1998. [12] T. K. Ho. Detención de palabras y identificación para el reconocimiento de texto adaptativo. Revista Internacional de Análisis y Reconocimiento de Documentos, 3(1), agosto de 2000. [13] T. Hofmann. Análisis semántico latente probabilístico. En Actas de la UAI99, 1999. [14] X. Ji y H. Zha. Correlación de la sumarización de un par de documentos multilingües. En Actas de RIDE, 2003. [15] X. Ji y H. Zha. Segmentación de texto independiente del dominio utilizando difusión anisotrópica y programación dinámica. En Actas de SIGIR, 2003. [16] X. Ji y H. Zha. Extrayendo temas compartidos de múltiples documentos. En Actas de la 7ª PAKDD, 2003. [17] J. Lafferty, A. McCallum y F. Pereira. Campos aleatorios condicionales: Modelos probabilísticos para segmentar y etiquetar datos de secuencias. En Actas de ICML, 2001. [18] T. Li, S. Ma y M. Ogihara. Criterio basado en la entropía en la agrupación categórica. En Actas de ICML, 2004. [19] A. McCallum, D. Freitag y F. Pereira. Modelos de máxima entropía de Markov para extracción de información y segmentación. En Actas de ICML, 2000. [20] L. Pevzner y M. Hearst. Una crítica y mejora de una métrica de evaluación para la segmentación de texto. Lingüística Computacional, 28(1):19-36, 2002. [21] J. C. Reynar. Modelos estadísticos para la segmentación de temas. En Actas de ACL, 1999. [22] G. Salton y M. McGill. Introducción a la Recuperación de Información Moderna. McGraw Hill, 1983. [23] B. \n\nMcGraw Hill, 1983. [23] B. Sun, Q. Tan, P. Mitra y C. L. Giles. Extracción y búsqueda de fórmulas químicas en documentos de texto en la web. En Actas de WWW, 2007. [24] B. Sun, D. Zhou, H. Zha, y J. I'm sorry, but \"Yen\" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish? Segmentación y alineación de texto multitarea basada en información mutua ponderada. En Actas de CIKM, 2006. [25] M. Utiyama y H. Isahara. Un modelo estadístico para la segmentación de texto independiente del dominio. En Actas de la 39ª ACL, 1999. [26] C. Wayne. Detección y seguimiento de temas multilingües: Investigación exitosa habilitada por corpora y evaluación. En Actas de LREC, 2000. [27] J. Yamron, I. Carp, L. Gillick, S. Lowe y P. van Mulbregt. Un enfoque de modelo oculto de Markov para la segmentación de texto y seguimiento de eventos. En Actas de ICASSP, 1998. [28] H. Zha y X. Ji. Correlacionando documentos multilingües a través de modelado de gráficos bipartitos. En Actas de SIGIR, 2002. ",
            "candidates": [],
            "error": [
                []
            ]
        },
        "term weight": {
            "translated_key": "peso de términos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Topic Segmentation with Shared Topic Detection and Alignment of Multiple Documents Bingjun Sun*, Prasenjit Mitra*† , Hongyuan Zha‡ , C. Lee Giles*† , John Yen*† *Department of Computer Science and Engineering † College of Information Sciences and Technology The Pennsylvania State University University Park, PA 16802 ‡ College of Computing The Georgia Institute of Technology Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu ABSTRACT Topic detection and tracking [26] and topic segmentation [15] play an important role in capturing the local and sequential information of documents.",
                "Previous work in this area usually focuses on single documents, although similar multiple documents are available in many domains.",
                "In this paper, we introduce a novel unsupervised method for shared topic detection and topic segmentation of multiple similar documents based on mutual information (MI) and weighted mutual information (WMI) that is a combination of MI and term weights.",
                "The basic idea is that the optimal segmentation maximizes MI(or WMI).",
                "Our approach can detect shared topics among documents.",
                "It can find the optimal boundaries in a document, and align segments among documents at the same time.",
                "It also can handle single-document segmentation as a special case of the multi-document segmentation and alignment.",
                "Our methods can identify and strengthen cue terms that can be used for segmentation and partially remove stop words by using term weights based on entropy learned from multiple documents.",
                "Our experimental results show that our algorithm works well for the tasks of single-document segmentation, shared topic detection, and multi-document segmentation.",
                "Utilizing information from multiple documents can tremendously improve the performance of topic segmentation, and using WMI is even better than using MI for the multi-document segmentation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Clustering; H.3.1 [Information Storage and Retrieval]: Content Analysis and IndexingLinguistic processing; I.2.7 [Artificial Intelligence]: Natural Language Processing-Text analysis; I.5.3 [Pattern Recognition]: Clustering-Algorithms;Similarity measures General Terms Algorithms, Design, Experimentation 1.",
                "INTRODUCTION Many researchers have worked on topic detection and tracking (TDT) [26] and topic segmentation during the past decade.",
                "Topic segmentation intends to identify the boundaries in a document with the goal to capture the latent topical structure.",
                "Topic segmentation tasks usually fall into two categories [15]: text stream segmentation where topic transition is identified, and coherent document segmentation in which documents are split into sub-topics.",
                "The former category has applications in automatic speech recognition, while the latter one has more applications such as partial-text query of long documents in information retrieval, text summary, and quality measurement of multiple documents.",
                "Previous research in connection with TDT falls into the former category, targeted on topic tracking of broadcast speech data and newswire text, while the latter category has not been studied very well.",
                "Traditional approaches perform topic segmentation on documents one at a time [15, 25, 6].",
                "Most of them perform badly in subtle tasks like coherent document segmentation [15].",
                "Often, end-users seek documents that have the similar content.",
                "Search engines, like, Google, provide links to obtain similar pages.",
                "At a finer granularity, users may actually be looking to obtain sections of a document similar to a particular section that presumably discusses a topic of the users interest.",
                "Thus, the extension of topic segmentation from single documents to identifying similar segments from multiple similar documents with the same topic is a natural and necessary direction, and multi-document topic segmentation is expected to have a better performance since more information is utilized.",
                "Traditional approaches using similarity measurement based on term frequency generally have the same assumption that similar vocabulary tends to be in a coherent topic segment [15, 25, 6].",
                "However, they usually suffer the issue of identifying stop words.",
                "For example, additional document-dependent stop words are removed together with the generic stop words in [15].",
                "There are two reasons that we do not remove stop words directly.",
                "First, identifying stop words is another issue [12] that requires estimation in each domain.",
                "Removing common stop words may result in the loss of useful information in a specific domain.",
                "Second, even though stop words can be identified, hard classification of stop words and nonstop words cannot represent the gradually changing amount of information content of each word.",
                "We employ a soft classification using term weights.",
                "In this paper, we view the problem of topic segmentation as an optimization issue using information theoretic techniques to find the optimal boundaries of a document given the number of text segments so as to minimize the loss of mutual information (MI) (or a weighted mutual information (WMI)) after segmentation and alignment.",
                "This is equal to maximizing the MI (or WMI).",
                "The MI focuses on measuring the difference among segments whereas previous research focused on finding the similarity (e.g. cosine distance) of segments [15, 25, 6].",
                "Topic alignment of multiple similar documents can be achieved by clustering sentences on the same topic into the same cluster.",
                "Single-document topic segmentation is just a special case of the multi-document topic segmentation and alignment problem.",
                "Terms can be co-clustered as in [10] at the same time, given the number of clusters, but our experimental results show that this method results in a worse segmentation (see Tables 1, 4, and 6).",
                "Usually, human readers can identify topic transition based on cue words, and can ignore stop words.",
                "Inspired by this, we give each term (or term cluster) a weight based on entropy among different documents and different segments of documents.",
                "Not only can this approach increase the contribution of cue words, but it can also decrease the effect of common stop words, noisy word, and document-dependent stop words.",
                "These words are common in a document.",
                "Many methods based on sentence similarity require that these words are removed before topic segmentation can be performed [15].",
                "Our results in Figure 3 show that term weights are useful for multi-document topic segmentation and alignment.",
                "The major contribution of this paper is that it introduces a novel method for topic segmentation using MI and shows that this method performs better than previously used criteria.",
                "Also, we have addressed the problem of topic segmentation and alignment across multiple documents, whereas most existing research focused on segmentation of single documents.",
                "Multi-document segmentation and alignment can utilize information from similar documents and improves the performance of topic segmentation greatly.",
                "Obviously, our approach can handle single documents as a special case when multiple documents are unavailable.",
                "It can detect shared topics among documents to judge if they are multiple documents on the same topic.",
                "We also introduce the new criterion of WMI based on term weights learned from multiple similar documents, which can improve performance of topic segmentation further.",
                "We propose an iterative greedy algorithm based on dynamic programming and show that it works well in practice.",
                "Some of our prior work is in [24].",
                "The rest of this paper is organized as follows: In Section 2, we review related work.",
                "Section 3 contains a formulation of the problem of topic segmentation and alignment of multiple documents with term co-clustering, a review of the criterion of MI for clustering, and finally an introduction to WMI.",
                "In Section 4, we first propose the iterative greedy algorithm of topic segmentation and alignment with term co-clustering, and then describe how the algorithm can be optimized by usFigure 1: Illustration of multi-document segmentation and alignment. ing dynamic programming.",
                "In Section 5, experiments about single-document segmentation, shared topic detection, and multi-document segmentation are described, and results are presented and discussed to evaluate the performance of our algorithm.",
                "Conclusions and some future directions of the research work are discussed in Section 6. 2.",
                "PREVIOUS WORK Generally, the existing approaches to text segmentation fall into two categories: supervised learning [19, 17, 23] and unsupervised learning [3, 27, 5, 6, 15, 25, 21].",
                "Supervised learning usually has good performance, since it learns functions from labelled training sets.",
                "However, often getting large training sets with manual labels on document sentences is prohibitively expensive, so unsupervised approaches are desired.",
                "Some models consider dependence between sentences and sections, such as Hidden Markov Model [3, 27], Maximum Entropy Markov Model [19], and Conditional Random Fields [17], while many other approaches are based on lexical cohesion or similarity of sentences [5, 6, 15, 25, 21].",
                "Some approaches also focus on cue words as hints of topic transitions [11].",
                "While some existing methods only consider information in single documents [6, 15], others utilize multiple documents [16, 14].",
                "There are not many works in the latter category, even though the performance of segmentation is expected to be better with utilization of information from multiple documents.",
                "Previous research studied methods to find shared topics [16] and topic segmentation and summarization between just a pair of documents [14].",
                "Text classification and clustering is a related research area which categorizes documents into groups using supervised or unsupervised methods.",
                "Topical classification or clustering is an important direction in this area, especially co-clustering of documents and terms, such as LSA [9], PLSA [13], and approaches based on distances and bipartite graph partitioning [28] or maximum MI [2, 10], or maximum entropy [1, 18].",
                "Criteria of these approaches can be utilized in the issue of topic segmentation.",
                "Some of those methods have been extended into the area of topic segmentation, such as PLSA [5] and maximum entropy [7], but to our best knowledge, using MI for topic segmentation has not been studied. 3.",
                "PROBLEM FORMULATION Our goal is to segment documents and align the segments across documents (Figure 1).",
                "Let T be the set of terms {t1, t2, ..., tl}, which appear in the unlabelled set of documents D = {d1, d2, ..., dm}.",
                "Let Sd be the set of sentences for document d ∈ D, i.e. {s1, s2, ..., snd }.",
                "We have a 3D matrix of term frequency, in which the three dimensions are random variables of D, Sd, and T. Sd actually is a random vector including a random variable for each d ∈ D. The term frequency can be used to estimate the joint probability distribution P(D, Sd, T), which is p(t, d, s) = T(t, d, s)/ND, where T(t, d, s) is the number of t in ds sentence s and ND is the total number of terms in D. ˆS represents the set of segments {ˆs1, ˆs2, ..., ˆsp} after segmentation and alignment among multiple documents, where the number of segments | ˆS| = p. A segment ˆsi of document d is a sequence of adjacent sentences in d. Since for different documents si may discuss different sub-topics, our goal is to cluster adjacent sentences in each document into segments, and align similar segments among documents, so that for different documents ˆsi is about the same sub-topic.",
                "The goal is to find the optimal topic segmentation and alignment mapping Segd(si) : {s1, s2, ..., snd } → {ˆs1, ˆs2, ..., ˆsp} and Alid(ˆsi) : {ˆs1, ˆs2, ..., ˆsp} → {ˆs1, ˆs2, ..., ˆsp}, for all d ∈ D, where ˆsi is ith segment with the constraint that only adjacent sentences can be mapped to the same segment, i.e. for d, {si, si+1, ..., sj} → {ˆsq}, where q ∈ {1, ..., p}, where p is the segment number, and if i > j, then for d, ˆsq is missing.",
                "After segmentation and alignment, random vector Sd becomes an aligned random variable ˆS.",
                "Thus, P(D, Sd, T) becomes P(D, ˆS, T).",
                "Term co-clustering is a technique that has been employed [10] to improve the accuracy of document clustering.",
                "We evaluate the effect of it for topic segmentation.",
                "A term t is mapped to exactly one term cluster.",
                "Term co-clustering involves simultaneously finding the optimal term clustering mapping Clu(t) : {t1, t2, ..., tl} → {ˆt1, ˆt2, ..., ˆtk}, where k ≤ l, l is the total number of words in all the documents, and k is the number of clusters. 4.",
                "METHODOLOGY We now describe a novel algorithm which can handle singledocument segmentation, shared topic detection, and multidocument segmentation and alignment based on MI or WMI. 4.1 Mutual Information MI I(X; Y ) is a quantity to measure the amount of information which is contained in two or more random variables [8, 10].",
                "For the case of two random variables, we have I(X; Y ) = x∈X y∈Y p(x, y)log p(x, y) p(x)p(y) , (1) Obviously, when random variables X and Y are independent, I(X; Y ) = 0.",
                "Thus, intuitively, the value of MI depends on how random variables are dependent on each other.",
                "The optimal co-clustering is the mapping Clux : X → ˆX and Cluy : Y → ˆY that minimizes the loss: I(X; Y ) − I( ˆX; ˆY ), which is equal to maximizing I( ˆX; ˆY ).",
                "This is the criterion of MI for clustering.",
                "In the case of topic segmentation, the two random variables are the term variable T and the segment variable S, and each sample is an occurrence of a term T = t in a particular segment S = s. I(T; S) is used to measure how dependent T and S are.",
                "However, I(T; S) cannot be computed for documents before segmentation, since we do not have a set of S due to the fact that sentences of Document d, si ∈ Sd, is not aligned with other documents.",
                "Thus, instead of minimizing the loss of MI, we can maximize MI after topic segmentation, computed as: I( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS p(ˆt, ˆs)log p(ˆt, ˆs) p(ˆt)p(ˆs) , (2) where p(ˆt, ˆs) are estimated by the term frequency tf of Term Cluster ˆt and Segment ˆs in the training set D. Note that here a segment ˆs includes sentences about the the same topic among all documents.",
                "The optimal solution is the mapping Clut : T → ˆT, Segd : Sd → ˆS , and Alid : ˆS → ˆS, which maximizes I( ˆT; ˆS). 4.2 Weighted Mutual Information In topic segmentation and alignment of multiple documents, if P(D, ˆS, T) is known, based on the marginal distributions P(D|T) and P( ˆS|T) for each term t ∈ T, we can categorize terms into four types in the data set: • Common stop words are common both along the dimensions of documents and segments. • Document-dependent stop words that depends on the personal writing style are common only along the dimension of segments for some documents. • Cue words are the most important elements for segmentation.",
                "They are common along the dimension of documents only for the same segment, and they are not common along the dimensions of segments. • Noisy words are other words which are not common along both dimensions.",
                "Entropy based on P(D|T) and P( ˆS|T) can be used to identify different types of terms.",
                "To reinforce the contribution of cue words in the MI computation, and simultaneously reduce the effect of the other three types of words, similar as the idea of the tf-idf weight [22], we use entropies of each term along the dimensions of document D and segment ˆS, i.e.",
                "ED(ˆt) and EˆS(ˆt), to compute the weight.",
                "A cue word usually has a large value of ED(ˆt) but a small value of EˆS(ˆt).",
                "We introduce term weights (or term cluster weights) wˆt = ( ED(ˆt) maxˆt ∈ ˆT (ED(ˆt )) )a (1 − EˆS(ˆt) maxˆt ∈ ˆT (EˆS(ˆt )) )b , (3) where ED(ˆt) = d∈D p(d|ˆt)log|D| 1 p(d|ˆt) , EˆS(ˆt) = ˆs∈ ˆS p(ˆs|ˆt)log| ˆS| 1 p(ˆs|ˆt) , and a > 0 and b > 0 are powers to adjust term weights.",
                "Usually a = 1 and b = 1 as default, and maxˆt ∈ ˆT (ED(ˆt )) and maxˆt ∈ ˆT (EˆS(ˆt )) are used to normalize the entropy values.",
                "Term cluster weights are used to adjust p(ˆt, ˆs), pw(ˆt, ˆs) = wˆtp(ˆt, ˆs) ˆt∈ ˆT ;ˆs∈ ˆS wˆtp(ˆt, ˆs) , (4) and Iw( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS pw(ˆt, ˆs)log pw(ˆt, ˆs) pw(ˆt)pw(ˆs) , (5) where pw(ˆt) and pw(ˆs) are marginal distributions of pw(ˆt, ˆs).",
                "However, since we do not know either the term weights or P(D, ˆS, T), we need to estimate them, but wˆt depends on p(ˆs|t) and ˆS, while ˆS and p(ˆs|t) also depend on wˆt that is still unknown.",
                "Thus, an iterative algorithm is required to estimate term weights wˆt and find the best segmentation and alignment to optimize the objective function Iw concurrently.",
                "After a document is segmented into sentences Input: Joint probability distribution P(D, Sd, T), number of text segments p ∈ {2, 3, ..., max(sd)}, number of term clusters k ∈ {2, 3, ..., l} (if k = l, no term co-clustering required), and weight type w ∈ {0, 1}, indicating to use I or Iw, respectively.",
                "Output: Mapping Clu, Seg, Ali, and term weights wˆt.",
                "Initialization: 0. i = 0.",
                "Initialize Clu (0) t , Seg (0) d , and Ali (0) d ; Initialize w (0) ˆt using Equation (6) if w = 1; Stage 1: 1.",
                "If |D| = 1, k = l, and w = 0, check all sequential segmentations of d into p segments and find the best one Segd(s) = argmaxˆsI( ˆT; ˆS), and return Segd; otherwise, if w = 1 and k = l, go to 3.1; Stage 2: 2.1 If k < l, for each term t, find the best cluster ˆt as Clu(i+1)(t) = argmaxˆtI( ˆT; ˆS(i)) based on Seg(i) and Ali(i); 2.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+1) d (Seg (i+1) d (s)) = argmaxˆsI( ˆT(i+1); ˆS) based on Clu(i+1)(t) if k < l or Clu(0)(t) if k = l; 2.3 i + +.",
                "If Clu, Seg, or Ali changed, go to 2.1; otherwise, if w = 0, return Clu(i), Seg(i), and Ali(i); else j = 0, go to 3.1; Stage 3: 3.1 Update w (i+j+1) ˆt based on Seg(i+j), Ali(i+j), and Clu(i) using Equation (3); 3.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+j+1) d (Seg (i+j+1) d (s)) = argmaxˆsIw( ˆT(i); ˆS) based on Clu(i) and w (i+j+1) ˆt ; 3.3 j + +.",
                "If Iw( ˆT; ˆS) changes, go to step 6; otherwise, stop and return Clu(i), Seg(i+j), Ali(i+j), and w (i+j) ˆt ; Figure 2: Algorithm: Topic segmentation and alignment based on MI or WMI. and each sentence is segmented into words, each word is stemmed.",
                "Then the joint probability distribution P(D, Sd, T) can be estimated.",
                "Finally, this distribution can be used to compute MI in our algorithm. 4.3 Iterative Greedy Algorithm Our goal is to maximize the objective function, I( ˆT; ˆS) or Iw( ˆT; ˆS), which can measure the dependence of term occurrences in different segments.",
                "Generally, first we do not know the estimate term weights, which depend on the optimal topic segmentation and alignment, and term clusters.",
                "Moreover, this problem is NP-hard [10], even though if we know the term weights.",
                "Thus, an iterative greedy algorithm is desired to find the best solution, even though probably only local maxima are reached.",
                "We present the iterative greedy algorithm in Figure 2 to find a local maximum of I( ˆT; ˆS) or Iw( ˆT; ˆS) with simultaneous <br>term weight</br> estimation.",
                "This algorithm can is iterative and greedy for multi-document cases or single-document cases with <br>term weight</br> estimation and/or term co-clustering.",
                "Otherwise, since it is just a one step algorithm to solve the task of single-document segmentation [6, 15, 25], the global maximum of MI is guaranteed.",
                "We will show later that term co-clustering reduces the accuracy of the results and is not necessary, and for singledocument segmentation, term weights are also not required. 4.3.1 Initialization In Step 0, the initial term clustering Clut and topic segmentation and alignment Segd and Alid are important to avoid local maxima and reduce the number of iterations.",
                "First, a good guess of term weights can be made by using the distributions of term frequency along sentences for each document and averaging them to get the initial values of wˆt: wt = ( ED(t) maxt ∈T (ED(t )) )(1 − ES(t) maxt ∈T (ES(t )) ), (6) where ES(t) = 1 |Dt| d∈Dt (1 − s∈Sd p(s|t)log|Sd| 1 p(s|t) ), where Dt is the set of documents which contain Term t. Then, for the initial segmentation Seg(0) , we can simply segment documents equally by sentences.",
                "Or we can find the optimal segmentation just for each document d which maximizes the WMI, Seg (0) d = argmaxˆsIw(T; ˆS), where w = w (0) ˆt .",
                "For the initial alignment Ali(0) , we can first assume that the order of segments for each d is the same.",
                "For the initial term clustering Clu(0) , first cluster labels can be set randomly, and after the first time of Step 3, a good initial term clustering is obtained. 4.3.2 Different Cases After initialization, there are three stages for different cases.",
                "Totally there are eight cases, |D| = 1 or |D| > 1, k = l or k < l, w = 0 or w = 1.",
                "Single document segmentation without term clustering and <br>term weight</br> estimation (|D| = 1, k = l, w = 0) only requires Stage 1 (Step 1).",
                "If term clustering is required (k < l), Stage 2 (Step 2.1, 2.2, and 2.3) is executed iteratively.",
                "If <br>term weight</br> estimation is required (w = 1), Stage 3 (Step 3.1, 3.2, and 3.3) is executed iteratively.",
                "If both are required (k < l, w = 1), Stage 2 and 3 run one after the other.",
                "For multi-document segmentation without term clustering and <br>term weight</br> estimation (|D| > 1, k = l, w = 0), only iteration of Step 2.2 and 2.3 are required.",
                "At Stage 1, the global maximum can be found based on I( ˆT; ˆS) using dynamic programming in Section 4.4.",
                "Simultaneously finding a good term clustering and estimated term weights is impossible, since when moving a term to a new term cluster to maximize Iw( ˆT; ˆS), we do not know that the weight of this term should be the one of the new cluster or the old cluster.",
                "Thus, we first do term clustering at Stage 2, and then estimate term weights at Stage 3.",
                "At Stage 2, Step 2.1 is to find the best term clustering and Step 2.2 is to find the best segmentation.",
                "This cycle is repeated to find a local maximum based on MI I until it converges.",
                "The two steps are: (1) based on current term clustering Cluˆt, for each document d, the algorithm segments all the sentences Sd into p segments sequentially (some segments may be empty), and put them into the p segments ˆS of the whole training set D (all possible cases of different segmentation Segd and alignment Alid are checked) to find the optimal case, and (2) based on the current segmentation and alignment, for each term t, the algorithm finds the best term cluster of t based on the current segmentation Segd and alignment Alid.",
                "After finding a good term clustering, term weights are estimated if w = 1.",
                "At Stage 3, similar as Stage 2, Step 3.1 is <br>term weight</br> re-estimation and Step 3.2 is to find a better segmentation.",
                "They are repeated to find a local maximum based on WMI Iw until it converges.",
                "However, if the term clustering in Stage 2 is not accurate, then the <br>term weight</br> estimation at Stage 3 may have a bad result.",
                "Finally, at Step 3.3, this algorithm converges and return the output.",
                "This algorithm can handle both single-document and multi-document segmentation.",
                "It also can detect shared topics among documents by checking the proportion of overlapped sentences on the same topics, as described in Sec 5.2. 4.4 Algorithm Optimization In many previous works on segmentation, dynamic programming is a technique used to maximize the objective function.",
                "Similarly, at Step 1, 2.2, and 3.2 of our algorithm, we can use dynamic programming.",
                "For Stage 1, using dynamic programming can still find the global optimum, but for Stage 2 and Stage 3, we can only find the optimum for each step of topic segmentation and alignment of a document.",
                "Here we only show the dynamic programming for Step 3.2 using WMI (Step 1 and 2.2 are similar but they can use either I or Iw).",
                "There are two cases that are not shown in the algorithm in Figure 2: (a) single-document segmentation or multi-document segmentation with the same sequential order of segments, where alignment is not required, and (b) multi-document segmentation with different sequential orders of segments, where alignment is necessary.",
                "The alignment mapping function of the former case is simply just Alid(ˆsi) = ˆsi, while for the latter ones alignment mapping function Alid(ˆsi) = ˆsj, i and j may be different.",
                "The computational steps for the two cases are listed below: Case 1 (no alignment): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs) and partial pw(ˆs) without counting sentences from d. Then put sentences from i to j into Part k, and compute partial WMI PIw( ˆT; ˆsk(si, si+1, ..., sj)) ˆt∈ ˆT pw(ˆt, ˆsk)log pw(ˆt, ˆsk) pw(ˆt)pw(ˆsk) , where Alid(si, si+1, ..., sj) = k, k ∈ {1, 2, ..., p}, 1 ≤ i ≤ j ≤ nd, and Segd(sq) = ˆsk for all i ≤ q ≤ j. (2) Let M(sm, 1) = PIw( ˆT; ˆs1(s1, s2, ..., sm)).",
                "Then M(sm, L) = maxi[M(si−1, L − 1) + PIw( ˆT; ˆsL(si, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, and when i > m, no sentences are put into ˆsk when compute PIw (note PIw( ˆT; ˆs(si, ..., sm)) = 0 for single-document segmentation). (3) Finally M(snd , p) = maxi[M(si−1, p − 1)+ PIw( ˆT; ˆsp(si, ..., snd ))], where 1 ≤ i ≤ nd+1.",
                "The optimal Iw is found and the corresponding segmentation is the best.",
                "Case 2 (alignment required): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs), and partial pw(ˆs), and PIw( ˆT; ˆsk(si, si+1, ..., sj)) similarly as Case 1. (2) Let M(sm, 1, k) = PIw( ˆT; ˆsk(s1, s2, ..., sm)), where k ∈ {1, 2, ..., p}.",
                "Then M(sm, L, kL) = maxi,j[M(si−1, L − 1, kL/j) + PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, kL ∈ Set(p, L), which is the set of all p!",
                "L!(p−L)! combinations of L segments chosen from all p segments, j ∈ kL, the set of L segments chosen from all p segments, and kL/j is the combination of L − 1 segments in kL except Segment j. (3) Finally, M(snd , p, kp) = maxi,j[M(si−1, p − 1, kp/j) +PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., snd ))], where kp is just the combination of all p segments and 1 ≤ i ≤ nd + 1, which is the optimal Iw and the corresponding segmentation is the best.",
                "The steps of Case 1 and 2 are similar, except in Case 2, alignment is considered in addition to segmentation.",
                "First, basic items of probability for computing Iw are computed excluding Doc d, and then partial WMI by putting every possible sequential segment (including empty segment) of d into every segment of the set.",
                "Second, the optimal sum of PIw for L segments and the leftmost m sentences, M(sm, L), is found.",
                "Finally, the maximal WMI is found among different sums of M(sm, p − 1) and PIw for Segment p. 5.",
                "EXPERIMENTS In this section, single-document segmentation, shared topic detection, and multi-document segmentation will be tested.",
                "Different hyper parameters of our method are studied.",
                "For convenience, we refer to the method using I as MIk if w = 0, and Iw as WMIk if w = 2 or as WMIk if w = 1, where k is the number of term clusters, and if k = l, where l is the total number of terms, then no term clustering is required, i.e.",
                "MIl and WMIl. 5.1 Single-document Segmentation 5.1.1 Test Data and Evaluation The first data set we tested is a synthetic one used in previous research [6, 15, 25] and many other papers.",
                "It has 700 samples.",
                "Each is a concatenation of ten segments.",
                "Each segment is the first n sentence selected randomly from the Brown corpus, which is supposed to have a different topic from each other.",
                "Currently, the best results on this data set is achieved by Ji et.al. [15].",
                "To compare the performance of our methods, the criterion used widely in previous research is applied, instead of the unbiased criterion introduced in [20].",
                "It chooses a pair of words randomly.",
                "If they are in different segments (different) for the real segmentation (real), but predicted (pred) as in the same segment, it is a miss.",
                "If they are in the same segment (same), but predicted as in different segments, it is a false alarm.",
                "Thus, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, diff)p(diff|real) +p(false alarm|real, pred, same)p(same|real). 5.1.2 Experiment Results We tested the case when the number of segments is known.",
                "Table 1 shows the results of our methods with different hyper parameter values and three previous approaches, C99[25], U00[6], and ADDP03[15], on this data set when the segment number is known.",
                "In WMI for single-document segmentation, the term weights are computed as follows: wˆt = 1−EˆS(ˆt)/maxˆt ∈ ˆT (EˆS(ˆt )).",
                "For this case, our methods MIl and WMIl both outperform all the previous approaches.",
                "We compared our methods with ADDP03using one-sample one-sided t-test and p-values are shown in Table 2.",
                "From the p-values, we can see that mostly the differences are very Table 1: Average Error Rates of Single-document Segmentation Given Segment Numbers Known Range of n 3-11 3-5 6-8 9-11 Sample size 400 100 100 100 C99 12% 11% 10% 9% U00 10% 9% 7% 5% ADDP03 6.0% 6.8% 5.2% 4.3% MIl 4.68% 5.57% 2.59% 1.59% WMIl 4.94% 6.33% 2.76% 1.62% MI100 9.62% 12.92% 8.66% 6.67% Table 2: Single-document Segmentation: P-values of T-test on Error Rates Range of n 3-11 3-5 6-8 9-11 ADDP03, MIl 0.000 0.000 0.000 0.000 ADDP03, WMIl 0.000 0.099 0.000 0.000 MIl, WMIl 0.061 0.132 0.526 0.898 significant.",
                "We also compare the error rates between our two methods using two-sample two-sided t-test to check the hypothesis that they are equal.",
                "We cannot reject the hypothesis that they are equal, so the difference are not significant, even though all the error rates for MIl are smaller than WMIl.",
                "However, we can conclude that term weights contribute little in single-document segmentation.",
                "The results also show that MI using term co-clustering (k = 100) decreases the performance.",
                "We tested different number of term clusters, and found that the performance becomes better when the cluster number increases to reach l. WMIk<l has similar results that we did not show in the table.",
                "As mentioned before, using MI may be inconsistent on optimal boundaries given different numbers of segments.",
                "This situation occurs especially when the similarities among segments are quite different, i.e. some transitions are very obvious, while others are not.",
                "This is because usually a document is a hierarchical structure instead of only a sequential structure.",
                "When the segments are not at the same level, this situation may occur.",
                "Thus, a hierarchical topic segmentation approach is desired, and the structure highly depends on the number of segments for each internal node and the stop criteria of splitting.",
                "For this data set of singledocument segmentation, since it is just a synthetic set, which is just a concatenation of several segments about different topics, it is reasonable that approaches simply based on term frequency have a good performance.",
                "Usually for the tasks of segmenting coherent documents for sub-topics, the effectiveness decreases much. 5.2 Shared Topic Detection 5.2.1 Test Data and Evaluation The second data set contains 80 news articles from Google News.",
                "There are eight topics and each has 10 articles.",
                "We randomly split the set into subsets with different document numbers and each subset has all eight topics.",
                "We compare our approach MIl and WMIl with LDA [4].",
                "LDA treats a document in the data set as a bag of words, finds its distribution on topics, and its major topic.",
                "MIl and WMIl views each sentence as a bag of words and tag it with a topic label.",
                "Then for each pair of documents, LDA determines if they are on the same topic, while MIl and Table 3: Shared Topic Detection: Average Error Rates for Different Numbers of Documents in Each Subset #Doc 10 20 40 80 LDA 8.89% 16.33% 1.35% 0.60% MIl, θ = 0.6 4.17% 1.71% 1.47% 0.0% WMIl, θ = 0.8 18.6% 3.16% 1.92% 0.0% WMIl check whether the proportion overlapped sentences on the same topic is larger than the adjustable threshold θ.",
                "That is, in MIl and WMIl, for a pair of documents d, d , if [ s∈Sd,s ∈Sd 1(topics=topics )/min(|Sd|, |Sd|)] > θ, where Sd is the set of sentences of d, and |Sd| is the number of sentences of d, then d and d have the shared topic.",
                "For a pair of documents selected randomly, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, same)p(same|real) +p(false alarm|real, pred, diff)p(diff|real), where a miss means if they have the same topic (same) for the real case (real), but predicted (pred) as on the same topic.",
                "If they are on different topics (diff), but predicted as on the same topic, it is a false alarm. 5.2.2 Experiment Results The results are shown in Table 3.",
                "If most documents have different topics, in WMIl, the estimation of term weights in Equation (3) is not correct.",
                "Thus, WMIl is not expected to have a better performance than MIl, when most documents have different topics.",
                "When there are fewer documents in a subset with the same number of topics, more documents have different topics, so WMIl is more worse than MIl.",
                "We can see that for most cases MIl has a better (or at least similar) performance than LDA.",
                "After shared topic detection, multi-document segmentation of documents with the shared topics is able to be executed. 5.3 Multi-document Segmentation 5.3.1 Test Data and Evaluation For multi-document segmentation and alignment, our goal is to identify these segments about the same topic among multiple similar documents with shared topics.",
                "Using Iw is expected to perform better than I, since without term weights the result is affected seriously by document-dependent stop words and noisy words which depends on the personal writing style.",
                "It is more likely to treat the same segments of different documents as different segments under the effect of document-dependent stop words and noisy words.",
                "Term weights can reduce the effect of document-dependent stop words and noisy words by giving cue terms more weights.",
                "The data set for multi-document segmentation and alignment has 102 samples and 2264 sentences totally.",
                "Each is the introduction part of a lab report selected from the course of Biol 240W, Pennsylvania State University.",
                "Each sample has two segments, introduction of plant hormones and the content in the lab.",
                "The length range of samples is from two to 56 sentences.",
                "Some samples only have one part and some have a reverse order the these two segments.",
                "It is not hard to identify the boundary between two segments for human.",
                "We labelled each sentence manually for evaluation.",
                "The criterion of evaluation is just using the proportion of the number of sentences with wrong predicted segment labels in the total number of sentences in the whole training Table 4: Average Error Rates of Multi-document Segmentation Given Segment Numbers Known #Doc MIl WMIl k MIk WMIk 102 3.14% 2.78% 300 4.68% 6.58% 51 4.17% 3.63% 300 17.83% 22.84% 34 5.06% 4.12% 300 18.75% 20.95% 20 7.08% 5.42% 250 20.40% 21.83% 10 10.38% 7.89% 250 21.42% 21.91% 5 15.77% 11.64% 250 21.89% 22.59% 2 25.90% 23.18% 50 25.44% 25.49% 1 23.90% 24.82% 25 25.75% 26.15% Table 5: Multi-document Segmentation: P-values of T-test on Error Rates for MIl and WMIl #Doc 51 34 20 10 5 2 P-value 0.19 0.101 0.025 0.001 0.000 0.002 set as the error rate: p(error|predicted, real) = d∈D s∈Sd 1(predicteds=reals)/ d∈D nd.",
                "In order to show the benefits of multi-document segmentation and alignment, we compared our method with different parameters on different partitions of the same training set.",
                "Except the cases that the number of documents is 102 and one (they are special cases of using the whole set and the pure single-document segmentation), we randomly divided the training set into m partitions, and each has 51, 34, 20, 10, 5, and 2 document samples.",
                "Then we applied our methods on each partition and calculated the error rate of the whole training set.",
                "Each case was repeated for 10 times for computing the average error rates.",
                "For different partitions of the training set, different k values are used, since the number of terms increases when the document number in each partition increases. 5.3.2 Experiment Results From the experiment results in Table 4, we can see the following observations: (1) When the number of documents increases, all methods have better performances.",
                "Only from one to two documents, MIl has decrease a little.",
                "We can observe this from Figure 3 at the point of document number = 2.",
                "Most curves even have the worst results at this point.",
                "There are two reasons.",
                "First, because samples vote for the best multi-document segmentation and alignment, but if only two documents are compared with each other, the one with missing segments or a totally different sequence will affect the correct segmentation and alignment of the other.",
                "Second, as noted at the beginning of this section, if two documents have more document-dependent stop words or noisy words than cue words, then the algorithm may view them as two different segments and the other segment is missing.",
                "Generally, we can only expect a better performance when the number of documents is larger than the number of segments. (2) Except single-document segmentation, WMIl is always better than MIl, and when the number of documents is reaching one or increases to a very large number, their performances become closer.",
                "Table 5 shows p-values of twosample one-sided t-test between MIl and WMIl.",
                "We also can see this trend from p-values.",
                "When document number = 5, we reached the smallest p-value and the largest difference between error rates of MIl and WMIl.",
                "For single-document Table 6: Multi-document Segmentation: Average Error Rate for Document Number = 5 in Each Subset with Different Number of Term Clusters #Cluster 75 100 150 250 l MIk 24.67% 24.54% 23.91% 22.59% 15.77% segmentation, WMIl is even a little bit worse than MIl, which is similar as the results of the single-document segmentation on the first data set.",
                "The reason is that for singledocument segmentation, we cannot estimate term weights accurately, since multiple documents are unavailable. (3) Using term clustering usually gets worse results than MIl and WMIl. (4) Using term clustering in WMIk is even worse than in MIk, since in WMIk term clusters are found first using I before using Iw.",
                "If the term clusters are not correct, then the term weights are estimated worse, which may mislead the algorithm to reach even worse results.",
                "From the results we also found that in multi-document segmentation and alignment, most documents with missing segments and a reverse order are identified correctly.",
                "Table 6 illustrates the experiment results for the case of 20 partitions (each has five document samples) of the training set and topic segmentation and alignment using MIk with different numbers of term clusters k. Notice that when the number of term clusters increases, the error rate becomes smaller.",
                "Without term clustering, we have the best result.",
                "We did not show results for WMIk with term clustering, but the results are similar.",
                "We also tested WMIl with different hyper parameters of a and b to adjust term weights.",
                "The results are presented in Figure 3.",
                "It was shown that the default case WMIl : a = 1, b = 1 gave the best results for different partitions of the training set.",
                "We can see the trend that when the document number is very small or large, the difference between MIl : a = 0, b = 0 and WMIl : a = 1, b = 1 becomes quite small.",
                "When the document number is not large (about from 2 to 10), all the cases using term weights have better performances than MIl : a = 0, b = 0 without term weights, but when the document number becomes larger, the cases WMIl : a = 1, b = 0 and WMIl : a = 2, b = 1 become worse than MIl : a = 0, b = 0.",
                "When the document number becomes very large, they are even worse than cases with small document numbers.",
                "This means that a proper way to estimate term weights for the criterion of WMI is very important.",
                "Figure 4 shows the term weights learned from the whole training set.",
                "Four types of words are categorized roughly even though the transition among them are subtle.",
                "Figure 5 illustrates the change in (weighted) mutual information for MIl and WMIl.",
                "As expected, mutual information for MIl increases monotonically with the number of steps, while WMIl does not.",
                "Finally, MIl and WMIl are scalable, with computational complexity shown in Figure 6.",
                "One advantage for our approach based on MI is that removing stop words is not required.",
                "Another important advantage is that there are no necessary hyper parameters to adjust.",
                "In single-document segmentation, the performance based on MI is even better for that based on WMI, so no extra hyper parameter is required.",
                "In multi-document segmentation, we show in the experiment, a = 1 and b = 1 is the best.",
                "Our method gives more weights to cue terms.",
                "However, usually cue terms or sentences appear at the beginning of a segment, while the end of the segment may be 1 2 5 10 20 34 51 102 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 Document Number ErrorRate MIl :a=0,b=0 WMI l :a=1,b=1 WMI l :a=1,b=0 WMI l :a=2,b=1 Figure 3: Error rates for different hyper parameters of term weights. 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Normalized Document Entropy NormalizedSegmentEntropy Noisy words Cue words Common stop words Doc−dep stop words Figure 4: Term weights learned from the whole training set. 0 100 200 300 400 500 600 0.06 0.08 0.1 0.12 0.14 0.16 0.18 Number of Steps (Weighted)MutualInformation MI l WMI l Figure 5: Change in (weighted) MI for MIl and WMIl. 0 20 40 60 80 100 120 0 200 400 600 800 1000 1200 1400 1600 1800 2000 Document Number TimetoConverge(sec) MI l WMI l Figure 6: Time to converge for MIl and WMIl. much noisy.",
                "One possible solution is giving more weights to terms at the beginning of each segment.",
                "Moreover, when the length of segments are quite different, long segments have much higher term frequencies, so they may dominate the segmentation boundaries.",
                "Normalization of term frequencies versus the segment length may be useful. 6.",
                "CONCLUSIONS AND FUTURE WORK We proposed a novel method for multi-document topic segmentation and alignment based on weighted mutual information, which can also handle single-document cases.",
                "We used dynamic programming to optimize our algorithm.",
                "Our approach outperforms all the previous methods on singledocument cases.",
                "Moreover, we also showed that doing segmentation among multiple documents can improve the performance tremendously.",
                "Our results also illustrated that using weighted mutual information can utilize the information of multiple documents to reach a better performance.",
                "We only tested our method on limited data sets.",
                "More data sets especially complicated ones should be tested.",
                "More previous methods should be compared with.",
                "Moreover, natural segmentations like paragraphs are hints that can be used to find the optimal boundaries.",
                "Supervised learning also can be considered. 7.",
                "ACKNOWLEDGMENTS The authors want to thank Xiang Ji, and Prof. J. Scott Payne for their help. 8.",
                "REFERENCES [1] A. Banerjee, I. Ghillon, J. Ghosh, S. Merugu, and D. Modha.",
                "A generalized maximum entropy approach to bregman co-clustering and matrix approximation.",
                "In Proceedings of SIGKDD, 2004. [2] R. Bekkerman, R. El-Yaniv, and A. McCallum.",
                "Multi-way distributional clustering via pairwise interactions.",
                "In Proceedings of ICML, 2005. [3] D. M. Blei and P. J. Moreno.",
                "Topic segmentation with an aspect hidden markov model.",
                "In Proceedings of SIGIR, 2001. [4] D. M. Blei, A. Ng, and M. Jordan.",
                "Latent dirichlet allocation.",
                "Journal of Machine Learning Research, 3:993-1022, 2003. [5] T. Brants, F. Chen, and I. Tsochantaridis.",
                "Topic-based document segmentation with probabilistic latent semantic analysis.",
                "In Proceedings of CIKM, 2002. [6] F. Choi.",
                "Advances in domain indepedent linear text segmentation.",
                "In Proceedings of the NAACL, 2000. [7] H. Christensen, B. Kolluru, Y. Gotoh, and S. Renals.",
                "Maximum entropy segmentation of broadcast news.",
                "In Proceedings of ICASSP, 2005. [8] T. Cover and J. Thomas.",
                "Elements of Information Theory.",
                "John Wiley and Sons, New York, USA, 1991. [9] S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and R. Harshman.",
                "Indexing by latent semantic analysis.",
                "Journal of the American Society for Information Systems, 1990. [10] I. Dhillon, S. Mallela, and D. Modha.",
                "Information-theoretic co-clustering.",
                "In Proceedings of SIGKDD, 2003. [11] M. Hajime, H. Takeo, and O. Manabu.",
                "Text segmentation with multiple surface linguistic cues.",
                "In Proceedings of COLING-ACL, 1998. [12] T. K. Ho.",
                "Stop word location and identification for adaptive text recognition.",
                "International Journal of Document Analysis and Recognition, 3(1), August 2000. [13] T. Hofmann.",
                "Probabilistic latent semantic analysis.",
                "In Proceedings of the UAI99, 1999. [14] X. Ji and H. Zha.",
                "Correlating summarization of a pair of multilingual documents.",
                "In Proceedings of RIDE, 2003. [15] X. Ji and H. Zha.",
                "Domain-independent text segmentation using anisotropic diffusion and dynamic programming.",
                "In Proceedings of SIGIR, 2003. [16] X. Ji and H. Zha.",
                "Extracting shared topics of multiple documents.",
                "In Proceedings of the 7th PAKDD, 2003. [17] J. Lafferty, A. McCallum, and F. Pereira.",
                "Conditional random fields: Probabilistic models for segmenting and labeling sequence data.",
                "In Proceedings of ICML, 2001. [18] T. Li, S. Ma, and M. Ogihara.",
                "Entropy-based criterion in categorical clustering.",
                "In Proceedings of ICML, 2004. [19] A. McCallum, D. Freitag, and F. Pereira.",
                "Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of ICML, 2000. [20] L. Pevzner and M. Hearst.",
                "A critique and improvement of an evaluation metric for text segmentation.",
                "Computational Linguistic, 28(1):19-36, 2002. [21] J. C. Reynar.",
                "Statistical models for topic segmentation.",
                "In Proceedings of ACL, 1999. [22] G. Salton and M. McGill.",
                "Introduction to Modern Information Retrieval.",
                "McGraw Hill, 1983. [23] B.",
                "Sun, Q. Tan, P. Mitra, and C. L. Giles.",
                "Extraction and search of chemical formulae in text documents on the web.",
                "In Proceedings of WWW, 2007. [24] B.",
                "Sun, D. Zhou, H. Zha, and J.",
                "Yen.",
                "Multi-task text segmentation and alignment based on weighted mutual information.",
                "In Proceedings of CIKM, 2006. [25] M. Utiyama and H. Isahara.",
                "A statistical model for domain-independent text segmentation.",
                "In Proceedings of the 39th ACL, 1999. [26] C. Wayne.",
                "Multilingual topic detection and tracking: Successful research enabled by corpora and evaluation.",
                "In Proceedings of LREC, 2000. [27] J. Yamron, I. Carp, L. Gillick, S. Lowe, and P. van Mulbregt.",
                "A hidden markov model approach to text segmentation and event tracking.",
                "In Proceedings of ICASSP, 1998. [28] H. Zha and X. Ji.",
                "Correlating multilingual documents via bipartite graph modeling.",
                "In Proceedings of SIGIR, 2002."
            ],
            "original_annotated_samples": [
                "We present the iterative greedy algorithm in Figure 2 to find a local maximum of I( ˆT; ˆS) or Iw( ˆT; ˆS) with simultaneous <br>term weight</br> estimation.",
                "This algorithm can is iterative and greedy for multi-document cases or single-document cases with <br>term weight</br> estimation and/or term co-clustering.",
                "Single document segmentation without term clustering and <br>term weight</br> estimation (|D| = 1, k = l, w = 0) only requires Stage 1 (Step 1).",
                "If <br>term weight</br> estimation is required (w = 1), Stage 3 (Step 3.1, 3.2, and 3.3) is executed iteratively.",
                "For multi-document segmentation without term clustering and <br>term weight</br> estimation (|D| > 1, k = l, w = 0), only iteration of Step 2.2 and 2.3 are required."
            ],
            "translated_annotated_samples": [
                "Presentamos el algoritmo voraz iterativo en la Figura 2 para encontrar un máximo local de I( ˆT; ˆS) o Iw( ˆT; ˆS) con estimación simultánea de <br>pesos de términos</br>.",
                "Este algoritmo es iterativo y codicioso para casos de múltiples documentos o casos de un solo documento con estimación de <br>peso de términos</br> y/o co-clustering de términos.",
                "La segmentación de un solo documento sin agrupamiento de términos y estimación de <br>peso de términos</br> (|D| = 1, k = l, w = 0) solo requiere la Etapa 1 (Paso 1).",
                "Si se requiere la estimación del <br>peso del término</br> (w = 1), la Etapa 3 (Paso 3.1, 3.2 y 3.3) se ejecuta de forma iterativa.",
                "Para la segmentación de múltiples documentos sin agrupamiento de términos y estimación de <br>peso de términos</br> (|D| > 1, k = l, w = 0), solo se requiere la iteración de los Pasos 2.2 y 2.3."
            ],
            "translated_text": "La segmentación de temas con detección de temas compartidos y alineación de múltiples documentos Bingjun Sun*, Prasenjit Mitra*†, Hongyuan Zha‡, C. Lee Giles*†, John Yen*† *Departamento de Ciencias de la Computación e Ingeniería † Colegio de Ciencias de la Información y Tecnología Universidad Estatal de Pensilvania University Park, PA 16802 ‡ Colegio de Computación Instituto de Tecnología de Georgia Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu RESUMEN La detección y seguimiento de temas [26] y la segmentación de temas [15] juegan un papel importante en la captura de la información local y secuencial de los documentos. El trabajo previo en esta área suele centrarse en documentos individuales, aunque existen múltiples documentos similares en muchos dominios. En este artículo, presentamos un novedoso método no supervisado para la detección de temas compartidos y la segmentación de temas de múltiples documentos similares basado en la información mutua (MI) y la información mutua ponderada (WMI), que es una combinación de MI y pesos de términos. La idea básica es que la segmentación óptima maximiza MI (o WMI). Nuestro enfoque puede detectar temas compartidos entre documentos. Puede encontrar los límites óptimos en un documento y alinear segmentos entre documentos al mismo tiempo. También puede manejar la segmentación de un solo documento como un caso especial de la segmentación y alineación de varios documentos. Nuestros métodos pueden identificar y fortalecer términos de señal que pueden ser utilizados para la segmentación y eliminar parcialmente las palabras de parada mediante el uso de pesos de términos basados en la entropía aprendida de múltiples documentos. Nuestros resultados experimentales muestran que nuestro algoritmo funciona bien para las tareas de segmentación de un solo documento, detección de temas compartidos y segmentación de múltiples documentos. El uso de información de múltiples documentos puede mejorar enormemente el rendimiento de la segmentación de temas, y utilizar WMI es aún mejor que utilizar MI para la segmentación de múltiples documentos. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información-Agrupación; H.3.1 [Almacenamiento y Recuperación de Información]: Análisis de Contenido e Indexación-Procesamiento Lingüístico; I.2.7 [Inteligencia Artificial]: Procesamiento de Lenguaje Natural-Análisis de Texto; I.5.3 [Reconocimiento de Patrones]: Algoritmos de Agrupación-Medidas de Similitud Términos Generales Algoritmos, Diseño, Experimentación 1. INTRODUCCIÓN Muchos investigadores han trabajado en la detección y seguimiento de temas (TDT) [26] y la segmentación de temas durante la última década. La segmentación de temas tiene como objetivo identificar los límites en un documento con la meta de capturar la estructura temática latente. Las tareas de segmentación de temas generalmente se dividen en dos categorías [15]: la segmentación de flujos de texto, donde se identifica la transición de temas, y la segmentación coherente de documentos, en la que los documentos se dividen en subtemas. La primera categoría tiene aplicaciones en el reconocimiento automático del habla, mientras que la segunda tiene más aplicaciones como la consulta parcial de texto en documentos extensos en la recuperación de información, resumen de texto y medición de calidad de múltiples documentos. La investigación previa en relación con TDT se enmarca en la primera categoría, centrada en el seguimiento de temas de datos de discurso de transmisión y texto de agencia de noticias, mientras que la segunda categoría no ha sido estudiada muy bien. Los enfoques tradicionales realizan la segmentación de temas en los documentos uno a la vez [15, 25, 6]. La mayoría de ellos tienen un mal desempeño en tareas sutiles como la segmentación coherente de documentos [15]. A menudo, los usuarios finales buscan documentos que tengan un contenido similar. Los motores de búsqueda, como Google, proporcionan enlaces para obtener páginas similares. A una granularidad más fina, los usuarios pueden estar buscando obtener secciones de un documento similares a una sección en particular que presumiblemente discute un tema de interés para los usuarios. Por lo tanto, la extensión de la segmentación de temas desde documentos individuales para identificar segmentos similares de múltiples documentos similares con el mismo tema es una dirección natural y necesaria, y se espera que la segmentación de temas de múltiples documentos tenga un mejor rendimiento ya que se utiliza más información. Los enfoques tradicionales que utilizan la medición de similitud basada en la frecuencia de términos generalmente parten del mismo supuesto de que un vocabulario similar tiende a estar en un segmento de tema coherente [15, 25, 6]. Sin embargo, suelen tener el problema de identificar las palabras de parada. Por ejemplo, se eliminan las palabras de parada dependientes del documento junto con las palabras de parada genéricas en [15]. Hay dos razones por las que no eliminamos las palabras de parada directamente. Primero, identificar las palabras vacías es otro problema [12] que requiere estimación en cada dominio. Eliminar palabras comunes de parada puede resultar en la pérdida de información útil en un dominio específico. Segundo, aunque se pueden identificar las palabras de parada, la clasificación estricta de palabras de parada y palabras no de parada no puede representar el cambio gradual en la cantidad de contenido informativo de cada palabra. Empleamos una clasificación suave utilizando pesos de términos. En este documento, consideramos el problema de la segmentación de temas como un problema de optimización utilizando técnicas de teoría de la información para encontrar los límites óptimos de un documento dado el número de segmentos de texto, de manera que se minimice la pérdida de información mutua (MI) (o información mutua ponderada (WMI)) después de la segmentación y alineación. Esto es igual a maximizar el MI (o WMI). El MI se centra en medir la diferencia entre segmentos, mientras que investigaciones anteriores se enfocaban en encontrar la similitud (por ejemplo, la distancia del coseno) entre segmentos [15, 25, 6]. La alineación de temas de múltiples documentos similares se puede lograr agrupando las oraciones sobre el mismo tema en el mismo clúster. La segmentación de temas en un solo documento es solo un caso especial del problema de segmentación y alineación de temas en múltiples documentos. Los términos pueden ser co-agrupados como en [10] al mismo tiempo, dado el número de agrupaciones, pero nuestros resultados experimentales muestran que este método resulta en una peor segmentación (ver Tablas 1, 4 y 6). Por lo general, los lectores humanos pueden identificar la transición de temas basándose en palabras de señal, y pueden ignorar las palabras vacías. Inspirados por esto, asignamos a cada término (o grupo de términos) un peso basado en la entropía entre diferentes documentos y diferentes segmentos de documentos. No solo este enfoque puede aumentar la contribución de las palabras clave, sino que también puede disminuir el efecto de las palabras de parada comunes, palabras ruidosas y palabras de parada dependientes del documento. Estas palabras son comunes en un documento. Muchos métodos basados en la similitud de oraciones requieren que estas palabras sean eliminadas antes de que se pueda realizar la segmentación de temas [15]. Nuestros resultados en la Figura 3 muestran que los pesos de los términos son útiles para la segmentación y alineación de temas en múltiples documentos. La principal contribución de este artículo es que introduce un método novedoso para la segmentación de temas utilizando MI y demuestra que este método funciona mejor que los criterios previamente utilizados. Además, hemos abordado el problema de la segmentación y alineación de temas en varios documentos, mientras que la mayoría de las investigaciones existentes se centraron en la segmentación de documentos individuales. La segmentación y alineación de múltiples documentos pueden utilizar información de documentos similares y mejorar significativamente el rendimiento de la segmentación de temas. Obviamente, nuestro enfoque puede manejar documentos individuales como un caso especial cuando varios documentos no están disponibles. Puede detectar temas compartidos entre documentos para determinar si son múltiples documentos sobre el mismo tema. También introducimos el nuevo criterio de WMI basado en pesos de términos aprendidos de múltiples documentos similares, lo cual puede mejorar aún más el rendimiento de la segmentación de temas. Proponemos un algoritmo voraz iterativo basado en programación dinámica y demostramos que funciona bien en la práctica. Algunos de nuestros trabajos anteriores están en [24]. El resto de este documento está organizado de la siguiente manera: En la Sección 2, revisamos el trabajo relacionado. La sección 3 contiene una formulación del problema de segmentación de temas y alineación de múltiples documentos con co-clustering de términos, una revisión del criterio de MI para el clustering, y finalmente una introducción a WMI. En la Sección 4, primero proponemos el algoritmo voraz iterativo de segmentación y alineación de temas con co-clustering de términos, y luego describimos cómo el algoritmo puede ser optimizado mediante programación dinámica. En la Sección 5, se describen experimentos sobre la segmentación de documentos individuales, la detección de temas compartidos y la segmentación de documentos múltiples, y se presentan y discuten los resultados para evaluar el rendimiento de nuestro algoritmo. Las conclusiones y algunas direcciones futuras del trabajo de investigación se discuten en la Sección 6.2. TRABAJO PREVIO En general, los enfoques existentes para la segmentación de texto se dividen en dos categorías: aprendizaje supervisado [19, 17, 23] y aprendizaje no supervisado [3, 27, 5, 6, 15, 25, 21]. El aprendizaje supervisado suele tener un buen rendimiento, ya que aprende funciones a partir de conjuntos de entrenamiento etiquetados. Sin embargo, obtener conjuntos de entrenamiento grandes con etiquetas manuales en oraciones de documentos suele ser prohibitivamente costoso, por lo que se prefieren enfoques no supervisados. Algunos modelos consideran la dependencia entre oraciones y secciones, como el Modelo Oculto de Markov [3, 27], el Modelo de Máxima Entropía de Markov [19] y los Campos Aleatorios Condicionales [17], mientras que muchos otros enfoques se basan en la cohesión léxica o la similitud de las oraciones [5, 6, 15, 25, 21]. Algunos enfoques también se centran en las palabras clave como pistas de transiciones de tema [11]. Si bien algunos métodos existentes solo consideran información en documentos individuales [6, 15], otros utilizan varios documentos [16, 14]. No hay muchas obras en la última categoría, aunque se espera que el rendimiento de la segmentación sea mejor con la utilización de información de múltiples documentos. Investigaciones previas estudiaron métodos para encontrar temas compartidos [16] y segmentación y resumen de temas entre solo un par de documentos [14]. La clasificación y agrupación de textos es un área de investigación relacionada que categoriza documentos en grupos utilizando métodos supervisados o no supervisados. La clasificación o agrupación temática es una dirección importante en esta área, especialmente el co-agrupamiento de documentos y términos, como LSA [9], PLSA [13], y enfoques basados en distancias y particionamiento de gráficos bipartitos [28] o máxima MI [2, 10], o máxima entropía [1, 18]. Los criterios de estos enfoques pueden ser utilizados en el tema de la segmentación de temas. Algunos de esos métodos se han extendido al área de la segmentación de temas, como PLSA [5] y máxima entropía [7], pero hasta donde sabemos, el uso de MI para la segmentación de temas no ha sido estudiado. 3. FORMULACIÓN DEL PROBLEMA Nuestro objetivo es segmentar documentos y alinear los segmentos entre documentos (Figura 1). Sea T el conjunto de términos {t1, t2, ..., tl}, que aparecen en el conjunto no etiquetado de documentos D = {d1, d2, ..., dm}. Sea Sd el conjunto de oraciones para el documento d ∈ D, es decir, {s1, s2, ..., snd}. Tenemos una matriz tridimensional de frecuencia de términos, en la que las tres dimensiones son variables aleatorias de D, Sd y T. Sd en realidad es un vector aleatorio que incluye una variable aleatoria para cada d ∈ D. La frecuencia de términos se puede utilizar para estimar la distribución de probabilidad conjunta P(D, Sd, T), que es p(t, d, s) = T(t, d, s)/ND, donde T(t, d, s) es el número de t en la oración s de d y ND es el número total de términos en D. ˆS representa el conjunto de segmentos {ˆs1, ˆs2, ..., ˆsp} después de la segmentación y alineación entre varios documentos, donde el número de segmentos | ˆS| = p. Un segmento ˆsi del documento d es una secuencia de oraciones adyacentes en d. Dado que para diferentes documentos si pueden discutir diferentes subtemas, nuestro objetivo es agrupar oraciones adyacentes en cada documento en segmentos, y alinear segmentos similares entre documentos, de modo que para diferentes documentos ˆsi trate sobre el mismo subtema. El objetivo es encontrar la segmentación óptima de temas y el mapeo de alineación Segd(si) : {s1, s2, ..., snd } → {ˆs1, ˆs2, ..., ˆsp} y Alid(ˆsi) : {ˆs1, ˆs2, ..., ˆsp} → {ˆs1, ˆs2, ..., ˆsp}, para todo d ∈ D, donde ˆsi es el i-ésimo segmento con la restricción de que solo las oraciones adyacentes pueden ser asignadas al mismo segmento, es decir, para d, {si, si+1, ..., sj} → {ˆsq}, donde q ∈ {1, ..., p}, donde p es el número de segmento, y si i > j, entonces para d, ˆsq está ausente. Después de la segmentación y alineación, el vector aleatorio Sd se convierte en una variable aleatoria alineada ˆS. Por lo tanto, P(D, Sd, T) se convierte en P(D, ˆS, T). El término co-clustering es una técnica que se ha empleado [10] para mejorar la precisión del agrupamiento de documentos. Evaluamos el efecto de ello para la segmentación de temas. Un término t se asigna a exactamente un grupo de términos. El término co-clustering implica encontrar simultáneamente el mapeo óptimo de agrupación de términos Clu(t) : {t1, t2, ..., tl} → {ˆt1, ˆt2, ..., ˆtk}, donde k ≤ l, l es el número total de palabras en todos los documentos, y k es el número de clústeres. 4. METODOLOGÍA Ahora describimos un algoritmo novedoso que puede manejar la segmentación de un solo documento, la detección de temas compartidos y la segmentación y alineación de múltiples documentos basados en MI o WMI. 4.1 Información Mutua MI I(X; Y) es una cantidad que mide la cantidad de información contenida en dos o más variables aleatorias [8, 10]. Para el caso de dos variables aleatorias, tenemos I(X; Y) = x∈X y∈Y p(x, y)log p(x, y) p(x)p(y) , (1) Obviamente, cuando las variables aleatorias X e Y son independientes, I(X; Y) = 0. Por lo tanto, de manera intuitiva, el valor de MI depende de cómo las variables aleatorias están relacionadas entre sí. La co-agrupación óptima es el mapeo Clux: X → ˆX y Cluy: Y → ˆY que minimiza la pérdida: I(X; Y) - I(ˆX; ˆY), lo cual es igual a maximizar I(ˆX; ˆY). Este es el criterio de MI para la agrupación. En el caso de la segmentación de temas, las dos variables aleatorias son la variable de término T y la variable de segmento S, y cada muestra es una ocurrencia de un término T = t en un segmento particular S = s. Se utiliza I(T; S) para medir qué tan dependientes son T y S. Sin embargo, no se puede calcular I(T; S) para documentos antes de la segmentación, ya que no tenemos un conjunto de S debido a que las oraciones del Documento d, si ∈ Sd, no están alineadas con otros documentos. Así, en lugar de minimizar la pérdida de MI, podemos maximizar MI después de la segmentación de temas, calculada como: I( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS p(ˆt, ˆs)log p(ˆt, ˆs) p(ˆt)p(ˆs) , (2) donde p(ˆt, ˆs) se estiman mediante la frecuencia de términos tf del Cluster de Términos ˆt y el Segmento ˆs en el conjunto de entrenamiento D. Nótese que aquí un segmento ˆs incluye oraciones sobre el mismo tema entre todos los documentos. La solución óptima es el mapeo Clut : T → ˆT, Segd : Sd → ˆS, y Alid : ˆS → ˆS, que maximiza I( ˆT; ˆS). 4.2 Información Mutua Ponderada En la segmentación de temas y alineación de múltiples documentos, si se conoce P(D, ˆS, T), basado en las distribuciones marginales P(D|T) y P( ˆS|T) para cada término t ∈ T, podemos categorizar los términos en cuatro tipos en el conjunto de datos: • Las palabras de parada comunes son comunes a lo largo de las dimensiones de documentos y segmentos. • Las palabras de parada dependientes del documento que dependen del estilo de escritura personal son comunes solo a lo largo de la dimensión de segmentos para algunos documentos. • Las palabras clave son los elementos más importantes para la segmentación. Son comunes a lo largo de la dimensión de los documentos solo para el mismo segmento, y no son comunes a lo largo de las dimensiones de los segmentos. • Las palabras ruidosas son otras palabras que no son comunes a lo largo de ambas dimensiones. La entropía basada en P(D|T) y P( ˆS|T) se puede utilizar para identificar diferentes tipos de términos. Para reforzar la contribución de las palabras clave en el cálculo de MI, y al mismo tiempo reducir el efecto de los otros tres tipos de palabras, similar a la idea del peso tf-idf [22], utilizamos las entropías de cada término a lo largo de las dimensiones del documento D y del segmento ˆS, es decir, ED(ˆt) y EˆS(ˆt), para calcular el peso. Una palabra de señal generalmente tiene un valor alto de ED(ˆt) pero un valor bajo de EˆS(ˆt). Introducimos los pesos de términos (o pesos de grupos de términos) wˆt = ( ED(ˆt) maxˆt ∈ ˆT (ED(ˆt )) )a (1 − EˆS(ˆt) maxˆt ∈ ˆT (EˆS(ˆt )) )b , (3) donde ED(ˆt) = d∈D p(d|ˆt)log|D| 1 p(d|ˆt) , EˆS(ˆt) = ˆs∈ ˆS p(ˆs|ˆt)log| ˆS| 1 p(ˆs|ˆt) , y a > 0 y b > 0 son potencias para ajustar los pesos de términos. Normalmente a = 1 y b = 1 por defecto, y se utilizan maxˆt ∈ ˆT (ED(ˆt )) y maxˆt ∈ ˆT (EˆS(ˆt )) para normalizar los valores de entropía. Los pesos de los grupos de términos se utilizan para ajustar p(ˆt, ˆs), pw(ˆt, ˆs) = wˆtp(ˆt, ˆs) ˆt∈ ˆT ;ˆs∈ ˆS wˆtp(ˆt, ˆs) , (4) e Iw( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS pw(ˆt, ˆs)log pw(ˆt, ˆs) pw(ˆt)pw(ˆs) , (5) donde pw(ˆt) y pw(ˆs) son distribuciones marginales de pw(ˆt, ˆs). Sin embargo, dado que no conocemos ni los pesos de los términos ni P(D, ˆS, T), necesitamos estimarlos, pero wˆt depende de p(ˆs|t) y ˆS, mientras que ˆS y p(ˆs|t) también dependen de wˆt, que aún es desconocido. Por lo tanto, se requiere un algoritmo iterativo para estimar los pesos de los términos wˆt y encontrar la mejor segmentación y alineación para optimizar la función objetivo Iw de manera concurrente. Después de que un documento se segmenta en oraciones, se introduce: la distribución de probabilidad conjunta P(D, Sd, T), el número de segmentos de texto p ∈ {2, 3, ..., máx(sd)}, el número de grupos de términos k ∈ {2, 3, ..., l} (si k = l, no se requiere co-clustering de términos) y el tipo de peso w ∈ {0, 1}, indicando usar I o Iw, respectivamente. Mapeo de Clu, Seg, Ali y pesos de términos wˆt. Inicialización: 0. i = 0. Inicializar Clu (0) t, Seg (0) d y Ali (0) d; Inicializar w (0) ˆt usando la Ecuación (6) si w = 1; Etapa 1: 1. Si |D| = 1, k = l y w = 0, verifica todas las segmentaciones secuenciales de d en p segmentos y encuentra la mejor Segd(s) = argmaxˆsI( ˆT; ˆS), y devuelve Segd; de lo contrario, si w = 1 y k = l, ve a 3.1; Etapa 2: 2.1 Si k < l, para cada término t, encuentra el mejor clúster ˆt como Clu(i+1)(t) = argmaxˆtI( ˆT; ˆS(i)) basado en Seg(i) y Ali(i); 2.2 Para cada d, verifica todas las segmentaciones secuenciales de d en p segmentos con mapeo s → ˆs → ˆs, y encuentra la mejor Ali (i+1) d (Seg (i+1) d (s)) = argmaxˆsI( ˆT(i+1); ˆS) basado en Clu(i+1)(t) si k < l o Clu(0)(t) si k = l; 2.3 i + +. Si Clu, Seg o Ali cambian, ve a 2.1; de lo contrario, si w = 0, devuelve Clu(i), Seg(i) y Ali(i); de lo contrario, j = 0, ve a 3.1; Etapa 3: 3.1 Actualiza w (i+j+1) ˆt basado en Seg(i+j), Ali(i+j) y Clu(i) usando la Ecuación (3); 3.2 Para cada d, verifica todas las segmentaciones secuenciales de d en p segmentos con mapeo s → ˆs → ˆs, y encuentra la mejor Ali (i+j+1) d (Seg (i+j+1) d (s)) = argmaxˆsIw( ˆT(i); ˆS) basado en Clu(i) y w (i+j+1) ˆt; 3.3 j + +. Si Iw( ˆT; ˆS) cambia, ve al paso 6; de lo contrario, detente y devuelve Clu(i), Seg(i+j), Ali(i+j), y w (i+j) ˆt; Figura 2: Algoritmo: Segmentación y alineación de temas basada en MI o WMI. y cada oración se segmenta en palabras, cada palabra se reduce a su raíz. Entonces se puede estimar la distribución de probabilidad conjunta P(D, Sd, T). Finalmente, esta distribución se puede utilizar para calcular la MI en nuestro algoritmo. 4.3 Algoritmo Voraz Iterativo Nuestro objetivo es maximizar la función objetivo, I( ˆT; ˆS) o Iw( ˆT; ˆS), que puede medir la dependencia de las ocurrencias de términos en diferentes segmentos. Generalmente, primero no conocemos los pesos estimados de los términos, los cuales dependen de la segmentación y alineación óptimas de los temas, y los grupos de términos. Además, este problema es NP-duro [10], incluso si conocemos los pesos de los términos. Por lo tanto, se desea un algoritmo voraz iterativo para encontrar la mejor solución, aunque probablemente solo se alcancen máximos locales. Presentamos el algoritmo voraz iterativo en la Figura 2 para encontrar un máximo local de I( ˆT; ˆS) o Iw( ˆT; ˆS) con estimación simultánea de <br>pesos de términos</br>. Este algoritmo es iterativo y codicioso para casos de múltiples documentos o casos de un solo documento con estimación de <br>peso de términos</br> y/o co-clustering de términos. De lo contrario, dado que es solo un algoritmo de un paso para resolver la tarea de segmentación de un solo documento [6, 15, 25], se garantiza el máximo global de MI. Mostraremos más adelante que la coagrupación de términos reduce la precisión de los resultados y no es necesaria, y para la segmentación de un solo documento, los pesos de los términos tampoco son requeridos. 4.3.1 Inicialización En el Paso 0, la agrupación inicial de términos Clut y la segmentación y alineación de temas Segd y Alid son importantes para evitar máximos locales y reducir el número de iteraciones. Primero, se puede hacer una buena estimación de los pesos de los términos utilizando las distribuciones de frecuencia de términos a lo largo de las oraciones para cada documento y promediándolas para obtener los valores iniciales de wˆt: wt = ( ED(t) maxt ∈T (ED(t )) )(1 − ES(t) maxt ∈T (ES(t )) ), (6) donde ES(t) = 1 |Dt| d∈Dt (1 − s∈Sd p(s|t)log|Sd| 1 p(s|t) ), donde Dt es el conjunto de documentos que contienen el término t. Luego, para la segmentación inicial Seg(0), simplemente podemos segmentar los documentos de manera equitativa por oraciones. O podemos encontrar la segmentación óptima solo para cada documento d que maximice el WMI, Seg (0) d = argmaxˆsIw(T; ˆS), donde w = w (0) ˆt. Para el alineamiento inicial Ali(0), podemos primero asumir que el orden de los segmentos para cada d es el mismo. Para el agrupamiento inicial de términos Clu(0), las etiquetas de los primeros grupos pueden establecerse al azar, y después de la primera vez del Paso 3, se obtiene un buen agrupamiento inicial de términos. 4.3.2 Diferentes Casos Después de la inicialización, hay tres etapas para diferentes casos. En total hay ocho casos, |D| = 1 o |D| > 1, k = l o k < l, w = 0 o w = 1. La segmentación de un solo documento sin agrupamiento de términos y estimación de <br>peso de términos</br> (|D| = 1, k = l, w = 0) solo requiere la Etapa 1 (Paso 1). Si se requiere el agrupamiento de términos (k < l), la Etapa 2 (Paso 2.1, 2.2 y 2.3) se ejecuta de forma iterativa. Si se requiere la estimación del <br>peso del término</br> (w = 1), la Etapa 3 (Paso 3.1, 3.2 y 3.3) se ejecuta de forma iterativa. Si ambos son requeridos (k < l, w = 1), la Etapa 2 y 3 se ejecutan una tras otra. Para la segmentación de múltiples documentos sin agrupamiento de términos y estimación de <br>peso de términos</br> (|D| > 1, k = l, w = 0), solo se requiere la iteración de los Pasos 2.2 y 2.3. ",
            "candidates": [],
            "error": [
                [
                    "pesos de términos",
                    "peso de términos",
                    "peso de términos",
                    "peso del término",
                    "peso de términos"
                ]
            ]
        },
        "performance of topic segmentation": {
            "translated_key": "rendimiento de la segmentación de temas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Topic Segmentation with Shared Topic Detection and Alignment of Multiple Documents Bingjun Sun*, Prasenjit Mitra*† , Hongyuan Zha‡ , C. Lee Giles*† , John Yen*† *Department of Computer Science and Engineering † College of Information Sciences and Technology The Pennsylvania State University University Park, PA 16802 ‡ College of Computing The Georgia Institute of Technology Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu ABSTRACT Topic detection and tracking [26] and topic segmentation [15] play an important role in capturing the local and sequential information of documents.",
                "Previous work in this area usually focuses on single documents, although similar multiple documents are available in many domains.",
                "In this paper, we introduce a novel unsupervised method for shared topic detection and topic segmentation of multiple similar documents based on mutual information (MI) and weighted mutual information (WMI) that is a combination of MI and term weights.",
                "The basic idea is that the optimal segmentation maximizes MI(or WMI).",
                "Our approach can detect shared topics among documents.",
                "It can find the optimal boundaries in a document, and align segments among documents at the same time.",
                "It also can handle single-document segmentation as a special case of the multi-document segmentation and alignment.",
                "Our methods can identify and strengthen cue terms that can be used for segmentation and partially remove stop words by using term weights based on entropy learned from multiple documents.",
                "Our experimental results show that our algorithm works well for the tasks of single-document segmentation, shared topic detection, and multi-document segmentation.",
                "Utilizing information from multiple documents can tremendously improve the <br>performance of topic segmentation</br>, and using WMI is even better than using MI for the multi-document segmentation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Clustering; H.3.1 [Information Storage and Retrieval]: Content Analysis and IndexingLinguistic processing; I.2.7 [Artificial Intelligence]: Natural Language Processing-Text analysis; I.5.3 [Pattern Recognition]: Clustering-Algorithms;Similarity measures General Terms Algorithms, Design, Experimentation 1.",
                "INTRODUCTION Many researchers have worked on topic detection and tracking (TDT) [26] and topic segmentation during the past decade.",
                "Topic segmentation intends to identify the boundaries in a document with the goal to capture the latent topical structure.",
                "Topic segmentation tasks usually fall into two categories [15]: text stream segmentation where topic transition is identified, and coherent document segmentation in which documents are split into sub-topics.",
                "The former category has applications in automatic speech recognition, while the latter one has more applications such as partial-text query of long documents in information retrieval, text summary, and quality measurement of multiple documents.",
                "Previous research in connection with TDT falls into the former category, targeted on topic tracking of broadcast speech data and newswire text, while the latter category has not been studied very well.",
                "Traditional approaches perform topic segmentation on documents one at a time [15, 25, 6].",
                "Most of them perform badly in subtle tasks like coherent document segmentation [15].",
                "Often, end-users seek documents that have the similar content.",
                "Search engines, like, Google, provide links to obtain similar pages.",
                "At a finer granularity, users may actually be looking to obtain sections of a document similar to a particular section that presumably discusses a topic of the users interest.",
                "Thus, the extension of topic segmentation from single documents to identifying similar segments from multiple similar documents with the same topic is a natural and necessary direction, and multi-document topic segmentation is expected to have a better performance since more information is utilized.",
                "Traditional approaches using similarity measurement based on term frequency generally have the same assumption that similar vocabulary tends to be in a coherent topic segment [15, 25, 6].",
                "However, they usually suffer the issue of identifying stop words.",
                "For example, additional document-dependent stop words are removed together with the generic stop words in [15].",
                "There are two reasons that we do not remove stop words directly.",
                "First, identifying stop words is another issue [12] that requires estimation in each domain.",
                "Removing common stop words may result in the loss of useful information in a specific domain.",
                "Second, even though stop words can be identified, hard classification of stop words and nonstop words cannot represent the gradually changing amount of information content of each word.",
                "We employ a soft classification using term weights.",
                "In this paper, we view the problem of topic segmentation as an optimization issue using information theoretic techniques to find the optimal boundaries of a document given the number of text segments so as to minimize the loss of mutual information (MI) (or a weighted mutual information (WMI)) after segmentation and alignment.",
                "This is equal to maximizing the MI (or WMI).",
                "The MI focuses on measuring the difference among segments whereas previous research focused on finding the similarity (e.g. cosine distance) of segments [15, 25, 6].",
                "Topic alignment of multiple similar documents can be achieved by clustering sentences on the same topic into the same cluster.",
                "Single-document topic segmentation is just a special case of the multi-document topic segmentation and alignment problem.",
                "Terms can be co-clustered as in [10] at the same time, given the number of clusters, but our experimental results show that this method results in a worse segmentation (see Tables 1, 4, and 6).",
                "Usually, human readers can identify topic transition based on cue words, and can ignore stop words.",
                "Inspired by this, we give each term (or term cluster) a weight based on entropy among different documents and different segments of documents.",
                "Not only can this approach increase the contribution of cue words, but it can also decrease the effect of common stop words, noisy word, and document-dependent stop words.",
                "These words are common in a document.",
                "Many methods based on sentence similarity require that these words are removed before topic segmentation can be performed [15].",
                "Our results in Figure 3 show that term weights are useful for multi-document topic segmentation and alignment.",
                "The major contribution of this paper is that it introduces a novel method for topic segmentation using MI and shows that this method performs better than previously used criteria.",
                "Also, we have addressed the problem of topic segmentation and alignment across multiple documents, whereas most existing research focused on segmentation of single documents.",
                "Multi-document segmentation and alignment can utilize information from similar documents and improves the <br>performance of topic segmentation</br> greatly.",
                "Obviously, our approach can handle single documents as a special case when multiple documents are unavailable.",
                "It can detect shared topics among documents to judge if they are multiple documents on the same topic.",
                "We also introduce the new criterion of WMI based on term weights learned from multiple similar documents, which can improve <br>performance of topic segmentation</br> further.",
                "We propose an iterative greedy algorithm based on dynamic programming and show that it works well in practice.",
                "Some of our prior work is in [24].",
                "The rest of this paper is organized as follows: In Section 2, we review related work.",
                "Section 3 contains a formulation of the problem of topic segmentation and alignment of multiple documents with term co-clustering, a review of the criterion of MI for clustering, and finally an introduction to WMI.",
                "In Section 4, we first propose the iterative greedy algorithm of topic segmentation and alignment with term co-clustering, and then describe how the algorithm can be optimized by usFigure 1: Illustration of multi-document segmentation and alignment. ing dynamic programming.",
                "In Section 5, experiments about single-document segmentation, shared topic detection, and multi-document segmentation are described, and results are presented and discussed to evaluate the performance of our algorithm.",
                "Conclusions and some future directions of the research work are discussed in Section 6. 2.",
                "PREVIOUS WORK Generally, the existing approaches to text segmentation fall into two categories: supervised learning [19, 17, 23] and unsupervised learning [3, 27, 5, 6, 15, 25, 21].",
                "Supervised learning usually has good performance, since it learns functions from labelled training sets.",
                "However, often getting large training sets with manual labels on document sentences is prohibitively expensive, so unsupervised approaches are desired.",
                "Some models consider dependence between sentences and sections, such as Hidden Markov Model [3, 27], Maximum Entropy Markov Model [19], and Conditional Random Fields [17], while many other approaches are based on lexical cohesion or similarity of sentences [5, 6, 15, 25, 21].",
                "Some approaches also focus on cue words as hints of topic transitions [11].",
                "While some existing methods only consider information in single documents [6, 15], others utilize multiple documents [16, 14].",
                "There are not many works in the latter category, even though the performance of segmentation is expected to be better with utilization of information from multiple documents.",
                "Previous research studied methods to find shared topics [16] and topic segmentation and summarization between just a pair of documents [14].",
                "Text classification and clustering is a related research area which categorizes documents into groups using supervised or unsupervised methods.",
                "Topical classification or clustering is an important direction in this area, especially co-clustering of documents and terms, such as LSA [9], PLSA [13], and approaches based on distances and bipartite graph partitioning [28] or maximum MI [2, 10], or maximum entropy [1, 18].",
                "Criteria of these approaches can be utilized in the issue of topic segmentation.",
                "Some of those methods have been extended into the area of topic segmentation, such as PLSA [5] and maximum entropy [7], but to our best knowledge, using MI for topic segmentation has not been studied. 3.",
                "PROBLEM FORMULATION Our goal is to segment documents and align the segments across documents (Figure 1).",
                "Let T be the set of terms {t1, t2, ..., tl}, which appear in the unlabelled set of documents D = {d1, d2, ..., dm}.",
                "Let Sd be the set of sentences for document d ∈ D, i.e. {s1, s2, ..., snd }.",
                "We have a 3D matrix of term frequency, in which the three dimensions are random variables of D, Sd, and T. Sd actually is a random vector including a random variable for each d ∈ D. The term frequency can be used to estimate the joint probability distribution P(D, Sd, T), which is p(t, d, s) = T(t, d, s)/ND, where T(t, d, s) is the number of t in ds sentence s and ND is the total number of terms in D. ˆS represents the set of segments {ˆs1, ˆs2, ..., ˆsp} after segmentation and alignment among multiple documents, where the number of segments | ˆS| = p. A segment ˆsi of document d is a sequence of adjacent sentences in d. Since for different documents si may discuss different sub-topics, our goal is to cluster adjacent sentences in each document into segments, and align similar segments among documents, so that for different documents ˆsi is about the same sub-topic.",
                "The goal is to find the optimal topic segmentation and alignment mapping Segd(si) : {s1, s2, ..., snd } → {ˆs1, ˆs2, ..., ˆsp} and Alid(ˆsi) : {ˆs1, ˆs2, ..., ˆsp} → {ˆs1, ˆs2, ..., ˆsp}, for all d ∈ D, where ˆsi is ith segment with the constraint that only adjacent sentences can be mapped to the same segment, i.e. for d, {si, si+1, ..., sj} → {ˆsq}, where q ∈ {1, ..., p}, where p is the segment number, and if i > j, then for d, ˆsq is missing.",
                "After segmentation and alignment, random vector Sd becomes an aligned random variable ˆS.",
                "Thus, P(D, Sd, T) becomes P(D, ˆS, T).",
                "Term co-clustering is a technique that has been employed [10] to improve the accuracy of document clustering.",
                "We evaluate the effect of it for topic segmentation.",
                "A term t is mapped to exactly one term cluster.",
                "Term co-clustering involves simultaneously finding the optimal term clustering mapping Clu(t) : {t1, t2, ..., tl} → {ˆt1, ˆt2, ..., ˆtk}, where k ≤ l, l is the total number of words in all the documents, and k is the number of clusters. 4.",
                "METHODOLOGY We now describe a novel algorithm which can handle singledocument segmentation, shared topic detection, and multidocument segmentation and alignment based on MI or WMI. 4.1 Mutual Information MI I(X; Y ) is a quantity to measure the amount of information which is contained in two or more random variables [8, 10].",
                "For the case of two random variables, we have I(X; Y ) = x∈X y∈Y p(x, y)log p(x, y) p(x)p(y) , (1) Obviously, when random variables X and Y are independent, I(X; Y ) = 0.",
                "Thus, intuitively, the value of MI depends on how random variables are dependent on each other.",
                "The optimal co-clustering is the mapping Clux : X → ˆX and Cluy : Y → ˆY that minimizes the loss: I(X; Y ) − I( ˆX; ˆY ), which is equal to maximizing I( ˆX; ˆY ).",
                "This is the criterion of MI for clustering.",
                "In the case of topic segmentation, the two random variables are the term variable T and the segment variable S, and each sample is an occurrence of a term T = t in a particular segment S = s. I(T; S) is used to measure how dependent T and S are.",
                "However, I(T; S) cannot be computed for documents before segmentation, since we do not have a set of S due to the fact that sentences of Document d, si ∈ Sd, is not aligned with other documents.",
                "Thus, instead of minimizing the loss of MI, we can maximize MI after topic segmentation, computed as: I( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS p(ˆt, ˆs)log p(ˆt, ˆs) p(ˆt)p(ˆs) , (2) where p(ˆt, ˆs) are estimated by the term frequency tf of Term Cluster ˆt and Segment ˆs in the training set D. Note that here a segment ˆs includes sentences about the the same topic among all documents.",
                "The optimal solution is the mapping Clut : T → ˆT, Segd : Sd → ˆS , and Alid : ˆS → ˆS, which maximizes I( ˆT; ˆS). 4.2 Weighted Mutual Information In topic segmentation and alignment of multiple documents, if P(D, ˆS, T) is known, based on the marginal distributions P(D|T) and P( ˆS|T) for each term t ∈ T, we can categorize terms into four types in the data set: • Common stop words are common both along the dimensions of documents and segments. • Document-dependent stop words that depends on the personal writing style are common only along the dimension of segments for some documents. • Cue words are the most important elements for segmentation.",
                "They are common along the dimension of documents only for the same segment, and they are not common along the dimensions of segments. • Noisy words are other words which are not common along both dimensions.",
                "Entropy based on P(D|T) and P( ˆS|T) can be used to identify different types of terms.",
                "To reinforce the contribution of cue words in the MI computation, and simultaneously reduce the effect of the other three types of words, similar as the idea of the tf-idf weight [22], we use entropies of each term along the dimensions of document D and segment ˆS, i.e.",
                "ED(ˆt) and EˆS(ˆt), to compute the weight.",
                "A cue word usually has a large value of ED(ˆt) but a small value of EˆS(ˆt).",
                "We introduce term weights (or term cluster weights) wˆt = ( ED(ˆt) maxˆt ∈ ˆT (ED(ˆt )) )a (1 − EˆS(ˆt) maxˆt ∈ ˆT (EˆS(ˆt )) )b , (3) where ED(ˆt) = d∈D p(d|ˆt)log|D| 1 p(d|ˆt) , EˆS(ˆt) = ˆs∈ ˆS p(ˆs|ˆt)log| ˆS| 1 p(ˆs|ˆt) , and a > 0 and b > 0 are powers to adjust term weights.",
                "Usually a = 1 and b = 1 as default, and maxˆt ∈ ˆT (ED(ˆt )) and maxˆt ∈ ˆT (EˆS(ˆt )) are used to normalize the entropy values.",
                "Term cluster weights are used to adjust p(ˆt, ˆs), pw(ˆt, ˆs) = wˆtp(ˆt, ˆs) ˆt∈ ˆT ;ˆs∈ ˆS wˆtp(ˆt, ˆs) , (4) and Iw( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS pw(ˆt, ˆs)log pw(ˆt, ˆs) pw(ˆt)pw(ˆs) , (5) where pw(ˆt) and pw(ˆs) are marginal distributions of pw(ˆt, ˆs).",
                "However, since we do not know either the term weights or P(D, ˆS, T), we need to estimate them, but wˆt depends on p(ˆs|t) and ˆS, while ˆS and p(ˆs|t) also depend on wˆt that is still unknown.",
                "Thus, an iterative algorithm is required to estimate term weights wˆt and find the best segmentation and alignment to optimize the objective function Iw concurrently.",
                "After a document is segmented into sentences Input: Joint probability distribution P(D, Sd, T), number of text segments p ∈ {2, 3, ..., max(sd)}, number of term clusters k ∈ {2, 3, ..., l} (if k = l, no term co-clustering required), and weight type w ∈ {0, 1}, indicating to use I or Iw, respectively.",
                "Output: Mapping Clu, Seg, Ali, and term weights wˆt.",
                "Initialization: 0. i = 0.",
                "Initialize Clu (0) t , Seg (0) d , and Ali (0) d ; Initialize w (0) ˆt using Equation (6) if w = 1; Stage 1: 1.",
                "If |D| = 1, k = l, and w = 0, check all sequential segmentations of d into p segments and find the best one Segd(s) = argmaxˆsI( ˆT; ˆS), and return Segd; otherwise, if w = 1 and k = l, go to 3.1; Stage 2: 2.1 If k < l, for each term t, find the best cluster ˆt as Clu(i+1)(t) = argmaxˆtI( ˆT; ˆS(i)) based on Seg(i) and Ali(i); 2.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+1) d (Seg (i+1) d (s)) = argmaxˆsI( ˆT(i+1); ˆS) based on Clu(i+1)(t) if k < l or Clu(0)(t) if k = l; 2.3 i + +.",
                "If Clu, Seg, or Ali changed, go to 2.1; otherwise, if w = 0, return Clu(i), Seg(i), and Ali(i); else j = 0, go to 3.1; Stage 3: 3.1 Update w (i+j+1) ˆt based on Seg(i+j), Ali(i+j), and Clu(i) using Equation (3); 3.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+j+1) d (Seg (i+j+1) d (s)) = argmaxˆsIw( ˆT(i); ˆS) based on Clu(i) and w (i+j+1) ˆt ; 3.3 j + +.",
                "If Iw( ˆT; ˆS) changes, go to step 6; otherwise, stop and return Clu(i), Seg(i+j), Ali(i+j), and w (i+j) ˆt ; Figure 2: Algorithm: Topic segmentation and alignment based on MI or WMI. and each sentence is segmented into words, each word is stemmed.",
                "Then the joint probability distribution P(D, Sd, T) can be estimated.",
                "Finally, this distribution can be used to compute MI in our algorithm. 4.3 Iterative Greedy Algorithm Our goal is to maximize the objective function, I( ˆT; ˆS) or Iw( ˆT; ˆS), which can measure the dependence of term occurrences in different segments.",
                "Generally, first we do not know the estimate term weights, which depend on the optimal topic segmentation and alignment, and term clusters.",
                "Moreover, this problem is NP-hard [10], even though if we know the term weights.",
                "Thus, an iterative greedy algorithm is desired to find the best solution, even though probably only local maxima are reached.",
                "We present the iterative greedy algorithm in Figure 2 to find a local maximum of I( ˆT; ˆS) or Iw( ˆT; ˆS) with simultaneous term weight estimation.",
                "This algorithm can is iterative and greedy for multi-document cases or single-document cases with term weight estimation and/or term co-clustering.",
                "Otherwise, since it is just a one step algorithm to solve the task of single-document segmentation [6, 15, 25], the global maximum of MI is guaranteed.",
                "We will show later that term co-clustering reduces the accuracy of the results and is not necessary, and for singledocument segmentation, term weights are also not required. 4.3.1 Initialization In Step 0, the initial term clustering Clut and topic segmentation and alignment Segd and Alid are important to avoid local maxima and reduce the number of iterations.",
                "First, a good guess of term weights can be made by using the distributions of term frequency along sentences for each document and averaging them to get the initial values of wˆt: wt = ( ED(t) maxt ∈T (ED(t )) )(1 − ES(t) maxt ∈T (ES(t )) ), (6) where ES(t) = 1 |Dt| d∈Dt (1 − s∈Sd p(s|t)log|Sd| 1 p(s|t) ), where Dt is the set of documents which contain Term t. Then, for the initial segmentation Seg(0) , we can simply segment documents equally by sentences.",
                "Or we can find the optimal segmentation just for each document d which maximizes the WMI, Seg (0) d = argmaxˆsIw(T; ˆS), where w = w (0) ˆt .",
                "For the initial alignment Ali(0) , we can first assume that the order of segments for each d is the same.",
                "For the initial term clustering Clu(0) , first cluster labels can be set randomly, and after the first time of Step 3, a good initial term clustering is obtained. 4.3.2 Different Cases After initialization, there are three stages for different cases.",
                "Totally there are eight cases, |D| = 1 or |D| > 1, k = l or k < l, w = 0 or w = 1.",
                "Single document segmentation without term clustering and term weight estimation (|D| = 1, k = l, w = 0) only requires Stage 1 (Step 1).",
                "If term clustering is required (k < l), Stage 2 (Step 2.1, 2.2, and 2.3) is executed iteratively.",
                "If term weight estimation is required (w = 1), Stage 3 (Step 3.1, 3.2, and 3.3) is executed iteratively.",
                "If both are required (k < l, w = 1), Stage 2 and 3 run one after the other.",
                "For multi-document segmentation without term clustering and term weight estimation (|D| > 1, k = l, w = 0), only iteration of Step 2.2 and 2.3 are required.",
                "At Stage 1, the global maximum can be found based on I( ˆT; ˆS) using dynamic programming in Section 4.4.",
                "Simultaneously finding a good term clustering and estimated term weights is impossible, since when moving a term to a new term cluster to maximize Iw( ˆT; ˆS), we do not know that the weight of this term should be the one of the new cluster or the old cluster.",
                "Thus, we first do term clustering at Stage 2, and then estimate term weights at Stage 3.",
                "At Stage 2, Step 2.1 is to find the best term clustering and Step 2.2 is to find the best segmentation.",
                "This cycle is repeated to find a local maximum based on MI I until it converges.",
                "The two steps are: (1) based on current term clustering Cluˆt, for each document d, the algorithm segments all the sentences Sd into p segments sequentially (some segments may be empty), and put them into the p segments ˆS of the whole training set D (all possible cases of different segmentation Segd and alignment Alid are checked) to find the optimal case, and (2) based on the current segmentation and alignment, for each term t, the algorithm finds the best term cluster of t based on the current segmentation Segd and alignment Alid.",
                "After finding a good term clustering, term weights are estimated if w = 1.",
                "At Stage 3, similar as Stage 2, Step 3.1 is term weight re-estimation and Step 3.2 is to find a better segmentation.",
                "They are repeated to find a local maximum based on WMI Iw until it converges.",
                "However, if the term clustering in Stage 2 is not accurate, then the term weight estimation at Stage 3 may have a bad result.",
                "Finally, at Step 3.3, this algorithm converges and return the output.",
                "This algorithm can handle both single-document and multi-document segmentation.",
                "It also can detect shared topics among documents by checking the proportion of overlapped sentences on the same topics, as described in Sec 5.2. 4.4 Algorithm Optimization In many previous works on segmentation, dynamic programming is a technique used to maximize the objective function.",
                "Similarly, at Step 1, 2.2, and 3.2 of our algorithm, we can use dynamic programming.",
                "For Stage 1, using dynamic programming can still find the global optimum, but for Stage 2 and Stage 3, we can only find the optimum for each step of topic segmentation and alignment of a document.",
                "Here we only show the dynamic programming for Step 3.2 using WMI (Step 1 and 2.2 are similar but they can use either I or Iw).",
                "There are two cases that are not shown in the algorithm in Figure 2: (a) single-document segmentation or multi-document segmentation with the same sequential order of segments, where alignment is not required, and (b) multi-document segmentation with different sequential orders of segments, where alignment is necessary.",
                "The alignment mapping function of the former case is simply just Alid(ˆsi) = ˆsi, while for the latter ones alignment mapping function Alid(ˆsi) = ˆsj, i and j may be different.",
                "The computational steps for the two cases are listed below: Case 1 (no alignment): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs) and partial pw(ˆs) without counting sentences from d. Then put sentences from i to j into Part k, and compute partial WMI PIw( ˆT; ˆsk(si, si+1, ..., sj)) ˆt∈ ˆT pw(ˆt, ˆsk)log pw(ˆt, ˆsk) pw(ˆt)pw(ˆsk) , where Alid(si, si+1, ..., sj) = k, k ∈ {1, 2, ..., p}, 1 ≤ i ≤ j ≤ nd, and Segd(sq) = ˆsk for all i ≤ q ≤ j. (2) Let M(sm, 1) = PIw( ˆT; ˆs1(s1, s2, ..., sm)).",
                "Then M(sm, L) = maxi[M(si−1, L − 1) + PIw( ˆT; ˆsL(si, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, and when i > m, no sentences are put into ˆsk when compute PIw (note PIw( ˆT; ˆs(si, ..., sm)) = 0 for single-document segmentation). (3) Finally M(snd , p) = maxi[M(si−1, p − 1)+ PIw( ˆT; ˆsp(si, ..., snd ))], where 1 ≤ i ≤ nd+1.",
                "The optimal Iw is found and the corresponding segmentation is the best.",
                "Case 2 (alignment required): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs), and partial pw(ˆs), and PIw( ˆT; ˆsk(si, si+1, ..., sj)) similarly as Case 1. (2) Let M(sm, 1, k) = PIw( ˆT; ˆsk(s1, s2, ..., sm)), where k ∈ {1, 2, ..., p}.",
                "Then M(sm, L, kL) = maxi,j[M(si−1, L − 1, kL/j) + PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, kL ∈ Set(p, L), which is the set of all p!",
                "L!(p−L)! combinations of L segments chosen from all p segments, j ∈ kL, the set of L segments chosen from all p segments, and kL/j is the combination of L − 1 segments in kL except Segment j. (3) Finally, M(snd , p, kp) = maxi,j[M(si−1, p − 1, kp/j) +PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., snd ))], where kp is just the combination of all p segments and 1 ≤ i ≤ nd + 1, which is the optimal Iw and the corresponding segmentation is the best.",
                "The steps of Case 1 and 2 are similar, except in Case 2, alignment is considered in addition to segmentation.",
                "First, basic items of probability for computing Iw are computed excluding Doc d, and then partial WMI by putting every possible sequential segment (including empty segment) of d into every segment of the set.",
                "Second, the optimal sum of PIw for L segments and the leftmost m sentences, M(sm, L), is found.",
                "Finally, the maximal WMI is found among different sums of M(sm, p − 1) and PIw for Segment p. 5.",
                "EXPERIMENTS In this section, single-document segmentation, shared topic detection, and multi-document segmentation will be tested.",
                "Different hyper parameters of our method are studied.",
                "For convenience, we refer to the method using I as MIk if w = 0, and Iw as WMIk if w = 2 or as WMIk if w = 1, where k is the number of term clusters, and if k = l, where l is the total number of terms, then no term clustering is required, i.e.",
                "MIl and WMIl. 5.1 Single-document Segmentation 5.1.1 Test Data and Evaluation The first data set we tested is a synthetic one used in previous research [6, 15, 25] and many other papers.",
                "It has 700 samples.",
                "Each is a concatenation of ten segments.",
                "Each segment is the first n sentence selected randomly from the Brown corpus, which is supposed to have a different topic from each other.",
                "Currently, the best results on this data set is achieved by Ji et.al. [15].",
                "To compare the performance of our methods, the criterion used widely in previous research is applied, instead of the unbiased criterion introduced in [20].",
                "It chooses a pair of words randomly.",
                "If they are in different segments (different) for the real segmentation (real), but predicted (pred) as in the same segment, it is a miss.",
                "If they are in the same segment (same), but predicted as in different segments, it is a false alarm.",
                "Thus, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, diff)p(diff|real) +p(false alarm|real, pred, same)p(same|real). 5.1.2 Experiment Results We tested the case when the number of segments is known.",
                "Table 1 shows the results of our methods with different hyper parameter values and three previous approaches, C99[25], U00[6], and ADDP03[15], on this data set when the segment number is known.",
                "In WMI for single-document segmentation, the term weights are computed as follows: wˆt = 1−EˆS(ˆt)/maxˆt ∈ ˆT (EˆS(ˆt )).",
                "For this case, our methods MIl and WMIl both outperform all the previous approaches.",
                "We compared our methods with ADDP03using one-sample one-sided t-test and p-values are shown in Table 2.",
                "From the p-values, we can see that mostly the differences are very Table 1: Average Error Rates of Single-document Segmentation Given Segment Numbers Known Range of n 3-11 3-5 6-8 9-11 Sample size 400 100 100 100 C99 12% 11% 10% 9% U00 10% 9% 7% 5% ADDP03 6.0% 6.8% 5.2% 4.3% MIl 4.68% 5.57% 2.59% 1.59% WMIl 4.94% 6.33% 2.76% 1.62% MI100 9.62% 12.92% 8.66% 6.67% Table 2: Single-document Segmentation: P-values of T-test on Error Rates Range of n 3-11 3-5 6-8 9-11 ADDP03, MIl 0.000 0.000 0.000 0.000 ADDP03, WMIl 0.000 0.099 0.000 0.000 MIl, WMIl 0.061 0.132 0.526 0.898 significant.",
                "We also compare the error rates between our two methods using two-sample two-sided t-test to check the hypothesis that they are equal.",
                "We cannot reject the hypothesis that they are equal, so the difference are not significant, even though all the error rates for MIl are smaller than WMIl.",
                "However, we can conclude that term weights contribute little in single-document segmentation.",
                "The results also show that MI using term co-clustering (k = 100) decreases the performance.",
                "We tested different number of term clusters, and found that the performance becomes better when the cluster number increases to reach l. WMIk<l has similar results that we did not show in the table.",
                "As mentioned before, using MI may be inconsistent on optimal boundaries given different numbers of segments.",
                "This situation occurs especially when the similarities among segments are quite different, i.e. some transitions are very obvious, while others are not.",
                "This is because usually a document is a hierarchical structure instead of only a sequential structure.",
                "When the segments are not at the same level, this situation may occur.",
                "Thus, a hierarchical topic segmentation approach is desired, and the structure highly depends on the number of segments for each internal node and the stop criteria of splitting.",
                "For this data set of singledocument segmentation, since it is just a synthetic set, which is just a concatenation of several segments about different topics, it is reasonable that approaches simply based on term frequency have a good performance.",
                "Usually for the tasks of segmenting coherent documents for sub-topics, the effectiveness decreases much. 5.2 Shared Topic Detection 5.2.1 Test Data and Evaluation The second data set contains 80 news articles from Google News.",
                "There are eight topics and each has 10 articles.",
                "We randomly split the set into subsets with different document numbers and each subset has all eight topics.",
                "We compare our approach MIl and WMIl with LDA [4].",
                "LDA treats a document in the data set as a bag of words, finds its distribution on topics, and its major topic.",
                "MIl and WMIl views each sentence as a bag of words and tag it with a topic label.",
                "Then for each pair of documents, LDA determines if they are on the same topic, while MIl and Table 3: Shared Topic Detection: Average Error Rates for Different Numbers of Documents in Each Subset #Doc 10 20 40 80 LDA 8.89% 16.33% 1.35% 0.60% MIl, θ = 0.6 4.17% 1.71% 1.47% 0.0% WMIl, θ = 0.8 18.6% 3.16% 1.92% 0.0% WMIl check whether the proportion overlapped sentences on the same topic is larger than the adjustable threshold θ.",
                "That is, in MIl and WMIl, for a pair of documents d, d , if [ s∈Sd,s ∈Sd 1(topics=topics )/min(|Sd|, |Sd|)] > θ, where Sd is the set of sentences of d, and |Sd| is the number of sentences of d, then d and d have the shared topic.",
                "For a pair of documents selected randomly, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, same)p(same|real) +p(false alarm|real, pred, diff)p(diff|real), where a miss means if they have the same topic (same) for the real case (real), but predicted (pred) as on the same topic.",
                "If they are on different topics (diff), but predicted as on the same topic, it is a false alarm. 5.2.2 Experiment Results The results are shown in Table 3.",
                "If most documents have different topics, in WMIl, the estimation of term weights in Equation (3) is not correct.",
                "Thus, WMIl is not expected to have a better performance than MIl, when most documents have different topics.",
                "When there are fewer documents in a subset with the same number of topics, more documents have different topics, so WMIl is more worse than MIl.",
                "We can see that for most cases MIl has a better (or at least similar) performance than LDA.",
                "After shared topic detection, multi-document segmentation of documents with the shared topics is able to be executed. 5.3 Multi-document Segmentation 5.3.1 Test Data and Evaluation For multi-document segmentation and alignment, our goal is to identify these segments about the same topic among multiple similar documents with shared topics.",
                "Using Iw is expected to perform better than I, since without term weights the result is affected seriously by document-dependent stop words and noisy words which depends on the personal writing style.",
                "It is more likely to treat the same segments of different documents as different segments under the effect of document-dependent stop words and noisy words.",
                "Term weights can reduce the effect of document-dependent stop words and noisy words by giving cue terms more weights.",
                "The data set for multi-document segmentation and alignment has 102 samples and 2264 sentences totally.",
                "Each is the introduction part of a lab report selected from the course of Biol 240W, Pennsylvania State University.",
                "Each sample has two segments, introduction of plant hormones and the content in the lab.",
                "The length range of samples is from two to 56 sentences.",
                "Some samples only have one part and some have a reverse order the these two segments.",
                "It is not hard to identify the boundary between two segments for human.",
                "We labelled each sentence manually for evaluation.",
                "The criterion of evaluation is just using the proportion of the number of sentences with wrong predicted segment labels in the total number of sentences in the whole training Table 4: Average Error Rates of Multi-document Segmentation Given Segment Numbers Known #Doc MIl WMIl k MIk WMIk 102 3.14% 2.78% 300 4.68% 6.58% 51 4.17% 3.63% 300 17.83% 22.84% 34 5.06% 4.12% 300 18.75% 20.95% 20 7.08% 5.42% 250 20.40% 21.83% 10 10.38% 7.89% 250 21.42% 21.91% 5 15.77% 11.64% 250 21.89% 22.59% 2 25.90% 23.18% 50 25.44% 25.49% 1 23.90% 24.82% 25 25.75% 26.15% Table 5: Multi-document Segmentation: P-values of T-test on Error Rates for MIl and WMIl #Doc 51 34 20 10 5 2 P-value 0.19 0.101 0.025 0.001 0.000 0.002 set as the error rate: p(error|predicted, real) = d∈D s∈Sd 1(predicteds=reals)/ d∈D nd.",
                "In order to show the benefits of multi-document segmentation and alignment, we compared our method with different parameters on different partitions of the same training set.",
                "Except the cases that the number of documents is 102 and one (they are special cases of using the whole set and the pure single-document segmentation), we randomly divided the training set into m partitions, and each has 51, 34, 20, 10, 5, and 2 document samples.",
                "Then we applied our methods on each partition and calculated the error rate of the whole training set.",
                "Each case was repeated for 10 times for computing the average error rates.",
                "For different partitions of the training set, different k values are used, since the number of terms increases when the document number in each partition increases. 5.3.2 Experiment Results From the experiment results in Table 4, we can see the following observations: (1) When the number of documents increases, all methods have better performances.",
                "Only from one to two documents, MIl has decrease a little.",
                "We can observe this from Figure 3 at the point of document number = 2.",
                "Most curves even have the worst results at this point.",
                "There are two reasons.",
                "First, because samples vote for the best multi-document segmentation and alignment, but if only two documents are compared with each other, the one with missing segments or a totally different sequence will affect the correct segmentation and alignment of the other.",
                "Second, as noted at the beginning of this section, if two documents have more document-dependent stop words or noisy words than cue words, then the algorithm may view them as two different segments and the other segment is missing.",
                "Generally, we can only expect a better performance when the number of documents is larger than the number of segments. (2) Except single-document segmentation, WMIl is always better than MIl, and when the number of documents is reaching one or increases to a very large number, their performances become closer.",
                "Table 5 shows p-values of twosample one-sided t-test between MIl and WMIl.",
                "We also can see this trend from p-values.",
                "When document number = 5, we reached the smallest p-value and the largest difference between error rates of MIl and WMIl.",
                "For single-document Table 6: Multi-document Segmentation: Average Error Rate for Document Number = 5 in Each Subset with Different Number of Term Clusters #Cluster 75 100 150 250 l MIk 24.67% 24.54% 23.91% 22.59% 15.77% segmentation, WMIl is even a little bit worse than MIl, which is similar as the results of the single-document segmentation on the first data set.",
                "The reason is that for singledocument segmentation, we cannot estimate term weights accurately, since multiple documents are unavailable. (3) Using term clustering usually gets worse results than MIl and WMIl. (4) Using term clustering in WMIk is even worse than in MIk, since in WMIk term clusters are found first using I before using Iw.",
                "If the term clusters are not correct, then the term weights are estimated worse, which may mislead the algorithm to reach even worse results.",
                "From the results we also found that in multi-document segmentation and alignment, most documents with missing segments and a reverse order are identified correctly.",
                "Table 6 illustrates the experiment results for the case of 20 partitions (each has five document samples) of the training set and topic segmentation and alignment using MIk with different numbers of term clusters k. Notice that when the number of term clusters increases, the error rate becomes smaller.",
                "Without term clustering, we have the best result.",
                "We did not show results for WMIk with term clustering, but the results are similar.",
                "We also tested WMIl with different hyper parameters of a and b to adjust term weights.",
                "The results are presented in Figure 3.",
                "It was shown that the default case WMIl : a = 1, b = 1 gave the best results for different partitions of the training set.",
                "We can see the trend that when the document number is very small or large, the difference between MIl : a = 0, b = 0 and WMIl : a = 1, b = 1 becomes quite small.",
                "When the document number is not large (about from 2 to 10), all the cases using term weights have better performances than MIl : a = 0, b = 0 without term weights, but when the document number becomes larger, the cases WMIl : a = 1, b = 0 and WMIl : a = 2, b = 1 become worse than MIl : a = 0, b = 0.",
                "When the document number becomes very large, they are even worse than cases with small document numbers.",
                "This means that a proper way to estimate term weights for the criterion of WMI is very important.",
                "Figure 4 shows the term weights learned from the whole training set.",
                "Four types of words are categorized roughly even though the transition among them are subtle.",
                "Figure 5 illustrates the change in (weighted) mutual information for MIl and WMIl.",
                "As expected, mutual information for MIl increases monotonically with the number of steps, while WMIl does not.",
                "Finally, MIl and WMIl are scalable, with computational complexity shown in Figure 6.",
                "One advantage for our approach based on MI is that removing stop words is not required.",
                "Another important advantage is that there are no necessary hyper parameters to adjust.",
                "In single-document segmentation, the performance based on MI is even better for that based on WMI, so no extra hyper parameter is required.",
                "In multi-document segmentation, we show in the experiment, a = 1 and b = 1 is the best.",
                "Our method gives more weights to cue terms.",
                "However, usually cue terms or sentences appear at the beginning of a segment, while the end of the segment may be 1 2 5 10 20 34 51 102 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 Document Number ErrorRate MIl :a=0,b=0 WMI l :a=1,b=1 WMI l :a=1,b=0 WMI l :a=2,b=1 Figure 3: Error rates for different hyper parameters of term weights. 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Normalized Document Entropy NormalizedSegmentEntropy Noisy words Cue words Common stop words Doc−dep stop words Figure 4: Term weights learned from the whole training set. 0 100 200 300 400 500 600 0.06 0.08 0.1 0.12 0.14 0.16 0.18 Number of Steps (Weighted)MutualInformation MI l WMI l Figure 5: Change in (weighted) MI for MIl and WMIl. 0 20 40 60 80 100 120 0 200 400 600 800 1000 1200 1400 1600 1800 2000 Document Number TimetoConverge(sec) MI l WMI l Figure 6: Time to converge for MIl and WMIl. much noisy.",
                "One possible solution is giving more weights to terms at the beginning of each segment.",
                "Moreover, when the length of segments are quite different, long segments have much higher term frequencies, so they may dominate the segmentation boundaries.",
                "Normalization of term frequencies versus the segment length may be useful. 6.",
                "CONCLUSIONS AND FUTURE WORK We proposed a novel method for multi-document topic segmentation and alignment based on weighted mutual information, which can also handle single-document cases.",
                "We used dynamic programming to optimize our algorithm.",
                "Our approach outperforms all the previous methods on singledocument cases.",
                "Moreover, we also showed that doing segmentation among multiple documents can improve the performance tremendously.",
                "Our results also illustrated that using weighted mutual information can utilize the information of multiple documents to reach a better performance.",
                "We only tested our method on limited data sets.",
                "More data sets especially complicated ones should be tested.",
                "More previous methods should be compared with.",
                "Moreover, natural segmentations like paragraphs are hints that can be used to find the optimal boundaries.",
                "Supervised learning also can be considered. 7.",
                "ACKNOWLEDGMENTS The authors want to thank Xiang Ji, and Prof. J. Scott Payne for their help. 8.",
                "REFERENCES [1] A. Banerjee, I. Ghillon, J. Ghosh, S. Merugu, and D. Modha.",
                "A generalized maximum entropy approach to bregman co-clustering and matrix approximation.",
                "In Proceedings of SIGKDD, 2004. [2] R. Bekkerman, R. El-Yaniv, and A. McCallum.",
                "Multi-way distributional clustering via pairwise interactions.",
                "In Proceedings of ICML, 2005. [3] D. M. Blei and P. J. Moreno.",
                "Topic segmentation with an aspect hidden markov model.",
                "In Proceedings of SIGIR, 2001. [4] D. M. Blei, A. Ng, and M. Jordan.",
                "Latent dirichlet allocation.",
                "Journal of Machine Learning Research, 3:993-1022, 2003. [5] T. Brants, F. Chen, and I. Tsochantaridis.",
                "Topic-based document segmentation with probabilistic latent semantic analysis.",
                "In Proceedings of CIKM, 2002. [6] F. Choi.",
                "Advances in domain indepedent linear text segmentation.",
                "In Proceedings of the NAACL, 2000. [7] H. Christensen, B. Kolluru, Y. Gotoh, and S. Renals.",
                "Maximum entropy segmentation of broadcast news.",
                "In Proceedings of ICASSP, 2005. [8] T. Cover and J. Thomas.",
                "Elements of Information Theory.",
                "John Wiley and Sons, New York, USA, 1991. [9] S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and R. Harshman.",
                "Indexing by latent semantic analysis.",
                "Journal of the American Society for Information Systems, 1990. [10] I. Dhillon, S. Mallela, and D. Modha.",
                "Information-theoretic co-clustering.",
                "In Proceedings of SIGKDD, 2003. [11] M. Hajime, H. Takeo, and O. Manabu.",
                "Text segmentation with multiple surface linguistic cues.",
                "In Proceedings of COLING-ACL, 1998. [12] T. K. Ho.",
                "Stop word location and identification for adaptive text recognition.",
                "International Journal of Document Analysis and Recognition, 3(1), August 2000. [13] T. Hofmann.",
                "Probabilistic latent semantic analysis.",
                "In Proceedings of the UAI99, 1999. [14] X. Ji and H. Zha.",
                "Correlating summarization of a pair of multilingual documents.",
                "In Proceedings of RIDE, 2003. [15] X. Ji and H. Zha.",
                "Domain-independent text segmentation using anisotropic diffusion and dynamic programming.",
                "In Proceedings of SIGIR, 2003. [16] X. Ji and H. Zha.",
                "Extracting shared topics of multiple documents.",
                "In Proceedings of the 7th PAKDD, 2003. [17] J. Lafferty, A. McCallum, and F. Pereira.",
                "Conditional random fields: Probabilistic models for segmenting and labeling sequence data.",
                "In Proceedings of ICML, 2001. [18] T. Li, S. Ma, and M. Ogihara.",
                "Entropy-based criterion in categorical clustering.",
                "In Proceedings of ICML, 2004. [19] A. McCallum, D. Freitag, and F. Pereira.",
                "Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of ICML, 2000. [20] L. Pevzner and M. Hearst.",
                "A critique and improvement of an evaluation metric for text segmentation.",
                "Computational Linguistic, 28(1):19-36, 2002. [21] J. C. Reynar.",
                "Statistical models for topic segmentation.",
                "In Proceedings of ACL, 1999. [22] G. Salton and M. McGill.",
                "Introduction to Modern Information Retrieval.",
                "McGraw Hill, 1983. [23] B.",
                "Sun, Q. Tan, P. Mitra, and C. L. Giles.",
                "Extraction and search of chemical formulae in text documents on the web.",
                "In Proceedings of WWW, 2007. [24] B.",
                "Sun, D. Zhou, H. Zha, and J.",
                "Yen.",
                "Multi-task text segmentation and alignment based on weighted mutual information.",
                "In Proceedings of CIKM, 2006. [25] M. Utiyama and H. Isahara.",
                "A statistical model for domain-independent text segmentation.",
                "In Proceedings of the 39th ACL, 1999. [26] C. Wayne.",
                "Multilingual topic detection and tracking: Successful research enabled by corpora and evaluation.",
                "In Proceedings of LREC, 2000. [27] J. Yamron, I. Carp, L. Gillick, S. Lowe, and P. van Mulbregt.",
                "A hidden markov model approach to text segmentation and event tracking.",
                "In Proceedings of ICASSP, 1998. [28] H. Zha and X. Ji.",
                "Correlating multilingual documents via bipartite graph modeling.",
                "In Proceedings of SIGIR, 2002."
            ],
            "original_annotated_samples": [
                "Utilizing information from multiple documents can tremendously improve the <br>performance of topic segmentation</br>, and using WMI is even better than using MI for the multi-document segmentation.",
                "Multi-document segmentation and alignment can utilize information from similar documents and improves the <br>performance of topic segmentation</br> greatly.",
                "We also introduce the new criterion of WMI based on term weights learned from multiple similar documents, which can improve <br>performance of topic segmentation</br> further."
            ],
            "translated_annotated_samples": [
                "El uso de información de múltiples documentos puede mejorar enormemente el <br>rendimiento de la segmentación de temas</br>, y utilizar WMI es aún mejor que utilizar MI para la segmentación de múltiples documentos.",
                "La segmentación y alineación de múltiples documentos pueden utilizar información de documentos similares y mejorar significativamente el <br>rendimiento de la segmentación de temas</br>.",
                "También introducimos el nuevo criterio de WMI basado en pesos de términos aprendidos de múltiples documentos similares, lo cual puede mejorar aún más el <br>rendimiento de la segmentación de temas</br>."
            ],
            "translated_text": "La segmentación de temas con detección de temas compartidos y alineación de múltiples documentos Bingjun Sun*, Prasenjit Mitra*†, Hongyuan Zha‡, C. Lee Giles*†, John Yen*† *Departamento de Ciencias de la Computación e Ingeniería † Colegio de Ciencias de la Información y Tecnología Universidad Estatal de Pensilvania University Park, PA 16802 ‡ Colegio de Computación Instituto de Tecnología de Georgia Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu RESUMEN La detección y seguimiento de temas [26] y la segmentación de temas [15] juegan un papel importante en la captura de la información local y secuencial de los documentos. El trabajo previo en esta área suele centrarse en documentos individuales, aunque existen múltiples documentos similares en muchos dominios. En este artículo, presentamos un novedoso método no supervisado para la detección de temas compartidos y la segmentación de temas de múltiples documentos similares basado en la información mutua (MI) y la información mutua ponderada (WMI), que es una combinación de MI y pesos de términos. La idea básica es que la segmentación óptima maximiza MI (o WMI). Nuestro enfoque puede detectar temas compartidos entre documentos. Puede encontrar los límites óptimos en un documento y alinear segmentos entre documentos al mismo tiempo. También puede manejar la segmentación de un solo documento como un caso especial de la segmentación y alineación de varios documentos. Nuestros métodos pueden identificar y fortalecer términos de señal que pueden ser utilizados para la segmentación y eliminar parcialmente las palabras de parada mediante el uso de pesos de términos basados en la entropía aprendida de múltiples documentos. Nuestros resultados experimentales muestran que nuestro algoritmo funciona bien para las tareas de segmentación de un solo documento, detección de temas compartidos y segmentación de múltiples documentos. El uso de información de múltiples documentos puede mejorar enormemente el <br>rendimiento de la segmentación de temas</br>, y utilizar WMI es aún mejor que utilizar MI para la segmentación de múltiples documentos. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información-Agrupación; H.3.1 [Almacenamiento y Recuperación de Información]: Análisis de Contenido e Indexación-Procesamiento Lingüístico; I.2.7 [Inteligencia Artificial]: Procesamiento de Lenguaje Natural-Análisis de Texto; I.5.3 [Reconocimiento de Patrones]: Algoritmos de Agrupación-Medidas de Similitud Términos Generales Algoritmos, Diseño, Experimentación 1. INTRODUCCIÓN Muchos investigadores han trabajado en la detección y seguimiento de temas (TDT) [26] y la segmentación de temas durante la última década. La segmentación de temas tiene como objetivo identificar los límites en un documento con la meta de capturar la estructura temática latente. Las tareas de segmentación de temas generalmente se dividen en dos categorías [15]: la segmentación de flujos de texto, donde se identifica la transición de temas, y la segmentación coherente de documentos, en la que los documentos se dividen en subtemas. La primera categoría tiene aplicaciones en el reconocimiento automático del habla, mientras que la segunda tiene más aplicaciones como la consulta parcial de texto en documentos extensos en la recuperación de información, resumen de texto y medición de calidad de múltiples documentos. La investigación previa en relación con TDT se enmarca en la primera categoría, centrada en el seguimiento de temas de datos de discurso de transmisión y texto de agencia de noticias, mientras que la segunda categoría no ha sido estudiada muy bien. Los enfoques tradicionales realizan la segmentación de temas en los documentos uno a la vez [15, 25, 6]. La mayoría de ellos tienen un mal desempeño en tareas sutiles como la segmentación coherente de documentos [15]. A menudo, los usuarios finales buscan documentos que tengan un contenido similar. Los motores de búsqueda, como Google, proporcionan enlaces para obtener páginas similares. A una granularidad más fina, los usuarios pueden estar buscando obtener secciones de un documento similares a una sección en particular que presumiblemente discute un tema de interés para los usuarios. Por lo tanto, la extensión de la segmentación de temas desde documentos individuales para identificar segmentos similares de múltiples documentos similares con el mismo tema es una dirección natural y necesaria, y se espera que la segmentación de temas de múltiples documentos tenga un mejor rendimiento ya que se utiliza más información. Los enfoques tradicionales que utilizan la medición de similitud basada en la frecuencia de términos generalmente parten del mismo supuesto de que un vocabulario similar tiende a estar en un segmento de tema coherente [15, 25, 6]. Sin embargo, suelen tener el problema de identificar las palabras de parada. Por ejemplo, se eliminan las palabras de parada dependientes del documento junto con las palabras de parada genéricas en [15]. Hay dos razones por las que no eliminamos las palabras de parada directamente. Primero, identificar las palabras vacías es otro problema [12] que requiere estimación en cada dominio. Eliminar palabras comunes de parada puede resultar en la pérdida de información útil en un dominio específico. Segundo, aunque se pueden identificar las palabras de parada, la clasificación estricta de palabras de parada y palabras no de parada no puede representar el cambio gradual en la cantidad de contenido informativo de cada palabra. Empleamos una clasificación suave utilizando pesos de términos. En este documento, consideramos el problema de la segmentación de temas como un problema de optimización utilizando técnicas de teoría de la información para encontrar los límites óptimos de un documento dado el número de segmentos de texto, de manera que se minimice la pérdida de información mutua (MI) (o información mutua ponderada (WMI)) después de la segmentación y alineación. Esto es igual a maximizar el MI (o WMI). El MI se centra en medir la diferencia entre segmentos, mientras que investigaciones anteriores se enfocaban en encontrar la similitud (por ejemplo, la distancia del coseno) entre segmentos [15, 25, 6]. La alineación de temas de múltiples documentos similares se puede lograr agrupando las oraciones sobre el mismo tema en el mismo clúster. La segmentación de temas en un solo documento es solo un caso especial del problema de segmentación y alineación de temas en múltiples documentos. Los términos pueden ser co-agrupados como en [10] al mismo tiempo, dado el número de agrupaciones, pero nuestros resultados experimentales muestran que este método resulta en una peor segmentación (ver Tablas 1, 4 y 6). Por lo general, los lectores humanos pueden identificar la transición de temas basándose en palabras de señal, y pueden ignorar las palabras vacías. Inspirados por esto, asignamos a cada término (o grupo de términos) un peso basado en la entropía entre diferentes documentos y diferentes segmentos de documentos. No solo este enfoque puede aumentar la contribución de las palabras clave, sino que también puede disminuir el efecto de las palabras de parada comunes, palabras ruidosas y palabras de parada dependientes del documento. Estas palabras son comunes en un documento. Muchos métodos basados en la similitud de oraciones requieren que estas palabras sean eliminadas antes de que se pueda realizar la segmentación de temas [15]. Nuestros resultados en la Figura 3 muestran que los pesos de los términos son útiles para la segmentación y alineación de temas en múltiples documentos. La principal contribución de este artículo es que introduce un método novedoso para la segmentación de temas utilizando MI y demuestra que este método funciona mejor que los criterios previamente utilizados. Además, hemos abordado el problema de la segmentación y alineación de temas en varios documentos, mientras que la mayoría de las investigaciones existentes se centraron en la segmentación de documentos individuales. La segmentación y alineación de múltiples documentos pueden utilizar información de documentos similares y mejorar significativamente el <br>rendimiento de la segmentación de temas</br>. Obviamente, nuestro enfoque puede manejar documentos individuales como un caso especial cuando varios documentos no están disponibles. Puede detectar temas compartidos entre documentos para determinar si son múltiples documentos sobre el mismo tema. También introducimos el nuevo criterio de WMI basado en pesos de términos aprendidos de múltiples documentos similares, lo cual puede mejorar aún más el <br>rendimiento de la segmentación de temas</br>. Proponemos un algoritmo voraz iterativo basado en programación dinámica y demostramos que funciona bien en la práctica. Algunos de nuestros trabajos anteriores están en [24]. El resto de este documento está organizado de la siguiente manera: En la Sección 2, revisamos el trabajo relacionado. La sección 3 contiene una formulación del problema de segmentación de temas y alineación de múltiples documentos con co-clustering de términos, una revisión del criterio de MI para el clustering, y finalmente una introducción a WMI. En la Sección 4, primero proponemos el algoritmo voraz iterativo de segmentación y alineación de temas con co-clustering de términos, y luego describimos cómo el algoritmo puede ser optimizado mediante programación dinámica. En la Sección 5, se describen experimentos sobre la segmentación de documentos individuales, la detección de temas compartidos y la segmentación de documentos múltiples, y se presentan y discuten los resultados para evaluar el rendimiento de nuestro algoritmo. Las conclusiones y algunas direcciones futuras del trabajo de investigación se discuten en la Sección 6.2. TRABAJO PREVIO En general, los enfoques existentes para la segmentación de texto se dividen en dos categorías: aprendizaje supervisado [19, 17, 23] y aprendizaje no supervisado [3, 27, 5, 6, 15, 25, 21]. El aprendizaje supervisado suele tener un buen rendimiento, ya que aprende funciones a partir de conjuntos de entrenamiento etiquetados. Sin embargo, obtener conjuntos de entrenamiento grandes con etiquetas manuales en oraciones de documentos suele ser prohibitivamente costoso, por lo que se prefieren enfoques no supervisados. Algunos modelos consideran la dependencia entre oraciones y secciones, como el Modelo Oculto de Markov [3, 27], el Modelo de Máxima Entropía de Markov [19] y los Campos Aleatorios Condicionales [17], mientras que muchos otros enfoques se basan en la cohesión léxica o la similitud de las oraciones [5, 6, 15, 25, 21]. Algunos enfoques también se centran en las palabras clave como pistas de transiciones de tema [11]. Si bien algunos métodos existentes solo consideran información en documentos individuales [6, 15], otros utilizan varios documentos [16, 14]. No hay muchas obras en la última categoría, aunque se espera que el rendimiento de la segmentación sea mejor con la utilización de información de múltiples documentos. Investigaciones previas estudiaron métodos para encontrar temas compartidos [16] y segmentación y resumen de temas entre solo un par de documentos [14]. La clasificación y agrupación de textos es un área de investigación relacionada que categoriza documentos en grupos utilizando métodos supervisados o no supervisados. La clasificación o agrupación temática es una dirección importante en esta área, especialmente el co-agrupamiento de documentos y términos, como LSA [9], PLSA [13], y enfoques basados en distancias y particionamiento de gráficos bipartitos [28] o máxima MI [2, 10], o máxima entropía [1, 18]. Los criterios de estos enfoques pueden ser utilizados en el tema de la segmentación de temas. Algunos de esos métodos se han extendido al área de la segmentación de temas, como PLSA [5] y máxima entropía [7], pero hasta donde sabemos, el uso de MI para la segmentación de temas no ha sido estudiado. 3. FORMULACIÓN DEL PROBLEMA Nuestro objetivo es segmentar documentos y alinear los segmentos entre documentos (Figura 1). Sea T el conjunto de términos {t1, t2, ..., tl}, que aparecen en el conjunto no etiquetado de documentos D = {d1, d2, ..., dm}. Sea Sd el conjunto de oraciones para el documento d ∈ D, es decir, {s1, s2, ..., snd}. Tenemos una matriz tridimensional de frecuencia de términos, en la que las tres dimensiones son variables aleatorias de D, Sd y T. Sd en realidad es un vector aleatorio que incluye una variable aleatoria para cada d ∈ D. La frecuencia de términos se puede utilizar para estimar la distribución de probabilidad conjunta P(D, Sd, T), que es p(t, d, s) = T(t, d, s)/ND, donde T(t, d, s) es el número de t en la oración s de d y ND es el número total de términos en D. ˆS representa el conjunto de segmentos {ˆs1, ˆs2, ..., ˆsp} después de la segmentación y alineación entre varios documentos, donde el número de segmentos | ˆS| = p. Un segmento ˆsi del documento d es una secuencia de oraciones adyacentes en d. Dado que para diferentes documentos si pueden discutir diferentes subtemas, nuestro objetivo es agrupar oraciones adyacentes en cada documento en segmentos, y alinear segmentos similares entre documentos, de modo que para diferentes documentos ˆsi trate sobre el mismo subtema. El objetivo es encontrar la segmentación óptima de temas y el mapeo de alineación Segd(si) : {s1, s2, ..., snd } → {ˆs1, ˆs2, ..., ˆsp} y Alid(ˆsi) : {ˆs1, ˆs2, ..., ˆsp} → {ˆs1, ˆs2, ..., ˆsp}, para todo d ∈ D, donde ˆsi es el i-ésimo segmento con la restricción de que solo las oraciones adyacentes pueden ser asignadas al mismo segmento, es decir, para d, {si, si+1, ..., sj} → {ˆsq}, donde q ∈ {1, ..., p}, donde p es el número de segmento, y si i > j, entonces para d, ˆsq está ausente. Después de la segmentación y alineación, el vector aleatorio Sd se convierte en una variable aleatoria alineada ˆS. Por lo tanto, P(D, Sd, T) se convierte en P(D, ˆS, T). El término co-clustering es una técnica que se ha empleado [10] para mejorar la precisión del agrupamiento de documentos. Evaluamos el efecto de ello para la segmentación de temas. Un término t se asigna a exactamente un grupo de términos. El término co-clustering implica encontrar simultáneamente el mapeo óptimo de agrupación de términos Clu(t) : {t1, t2, ..., tl} → {ˆt1, ˆt2, ..., ˆtk}, donde k ≤ l, l es el número total de palabras en todos los documentos, y k es el número de clústeres. 4. METODOLOGÍA Ahora describimos un algoritmo novedoso que puede manejar la segmentación de un solo documento, la detección de temas compartidos y la segmentación y alineación de múltiples documentos basados en MI o WMI. 4.1 Información Mutua MI I(X; Y) es una cantidad que mide la cantidad de información contenida en dos o más variables aleatorias [8, 10]. Para el caso de dos variables aleatorias, tenemos I(X; Y) = x∈X y∈Y p(x, y)log p(x, y) p(x)p(y) , (1) Obviamente, cuando las variables aleatorias X e Y son independientes, I(X; Y) = 0. Por lo tanto, de manera intuitiva, el valor de MI depende de cómo las variables aleatorias están relacionadas entre sí. La co-agrupación óptima es el mapeo Clux: X → ˆX y Cluy: Y → ˆY que minimiza la pérdida: I(X; Y) - I(ˆX; ˆY), lo cual es igual a maximizar I(ˆX; ˆY). Este es el criterio de MI para la agrupación. En el caso de la segmentación de temas, las dos variables aleatorias son la variable de término T y la variable de segmento S, y cada muestra es una ocurrencia de un término T = t en un segmento particular S = s. Se utiliza I(T; S) para medir qué tan dependientes son T y S. Sin embargo, no se puede calcular I(T; S) para documentos antes de la segmentación, ya que no tenemos un conjunto de S debido a que las oraciones del Documento d, si ∈ Sd, no están alineadas con otros documentos. Así, en lugar de minimizar la pérdida de MI, podemos maximizar MI después de la segmentación de temas, calculada como: I( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS p(ˆt, ˆs)log p(ˆt, ˆs) p(ˆt)p(ˆs) , (2) donde p(ˆt, ˆs) se estiman mediante la frecuencia de términos tf del Cluster de Términos ˆt y el Segmento ˆs en el conjunto de entrenamiento D. Nótese que aquí un segmento ˆs incluye oraciones sobre el mismo tema entre todos los documentos. La solución óptima es el mapeo Clut : T → ˆT, Segd : Sd → ˆS, y Alid : ˆS → ˆS, que maximiza I( ˆT; ˆS). 4.2 Información Mutua Ponderada En la segmentación de temas y alineación de múltiples documentos, si se conoce P(D, ˆS, T), basado en las distribuciones marginales P(D|T) y P( ˆS|T) para cada término t ∈ T, podemos categorizar los términos en cuatro tipos en el conjunto de datos: • Las palabras de parada comunes son comunes a lo largo de las dimensiones de documentos y segmentos. • Las palabras de parada dependientes del documento que dependen del estilo de escritura personal son comunes solo a lo largo de la dimensión de segmentos para algunos documentos. • Las palabras clave son los elementos más importantes para la segmentación. Son comunes a lo largo de la dimensión de los documentos solo para el mismo segmento, y no son comunes a lo largo de las dimensiones de los segmentos. • Las palabras ruidosas son otras palabras que no son comunes a lo largo de ambas dimensiones. La entropía basada en P(D|T) y P( ˆS|T) se puede utilizar para identificar diferentes tipos de términos. Para reforzar la contribución de las palabras clave en el cálculo de MI, y al mismo tiempo reducir el efecto de los otros tres tipos de palabras, similar a la idea del peso tf-idf [22], utilizamos las entropías de cada término a lo largo de las dimensiones del documento D y del segmento ˆS, es decir, ED(ˆt) y EˆS(ˆt), para calcular el peso. Una palabra de señal generalmente tiene un valor alto de ED(ˆt) pero un valor bajo de EˆS(ˆt). Introducimos los pesos de términos (o pesos de grupos de términos) wˆt = ( ED(ˆt) maxˆt ∈ ˆT (ED(ˆt )) )a (1 − EˆS(ˆt) maxˆt ∈ ˆT (EˆS(ˆt )) )b , (3) donde ED(ˆt) = d∈D p(d|ˆt)log|D| 1 p(d|ˆt) , EˆS(ˆt) = ˆs∈ ˆS p(ˆs|ˆt)log| ˆS| 1 p(ˆs|ˆt) , y a > 0 y b > 0 son potencias para ajustar los pesos de términos. Normalmente a = 1 y b = 1 por defecto, y se utilizan maxˆt ∈ ˆT (ED(ˆt )) y maxˆt ∈ ˆT (EˆS(ˆt )) para normalizar los valores de entropía. Los pesos de los grupos de términos se utilizan para ajustar p(ˆt, ˆs), pw(ˆt, ˆs) = wˆtp(ˆt, ˆs) ˆt∈ ˆT ;ˆs∈ ˆS wˆtp(ˆt, ˆs) , (4) e Iw( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS pw(ˆt, ˆs)log pw(ˆt, ˆs) pw(ˆt)pw(ˆs) , (5) donde pw(ˆt) y pw(ˆs) son distribuciones marginales de pw(ˆt, ˆs). Sin embargo, dado que no conocemos ni los pesos de los términos ni P(D, ˆS, T), necesitamos estimarlos, pero wˆt depende de p(ˆs|t) y ˆS, mientras que ˆS y p(ˆs|t) también dependen de wˆt, que aún es desconocido. Por lo tanto, se requiere un algoritmo iterativo para estimar los pesos de los términos wˆt y encontrar la mejor segmentación y alineación para optimizar la función objetivo Iw de manera concurrente. Después de que un documento se segmenta en oraciones, se introduce: la distribución de probabilidad conjunta P(D, Sd, T), el número de segmentos de texto p ∈ {2, 3, ..., máx(sd)}, el número de grupos de términos k ∈ {2, 3, ..., l} (si k = l, no se requiere co-clustering de términos) y el tipo de peso w ∈ {0, 1}, indicando usar I o Iw, respectivamente. Mapeo de Clu, Seg, Ali y pesos de términos wˆt. Inicialización: 0. i = 0. Inicializar Clu (0) t, Seg (0) d y Ali (0) d; Inicializar w (0) ˆt usando la Ecuación (6) si w = 1; Etapa 1: 1. Si |D| = 1, k = l y w = 0, verifica todas las segmentaciones secuenciales de d en p segmentos y encuentra la mejor Segd(s) = argmaxˆsI( ˆT; ˆS), y devuelve Segd; de lo contrario, si w = 1 y k = l, ve a 3.1; Etapa 2: 2.1 Si k < l, para cada término t, encuentra el mejor clúster ˆt como Clu(i+1)(t) = argmaxˆtI( ˆT; ˆS(i)) basado en Seg(i) y Ali(i); 2.2 Para cada d, verifica todas las segmentaciones secuenciales de d en p segmentos con mapeo s → ˆs → ˆs, y encuentra la mejor Ali (i+1) d (Seg (i+1) d (s)) = argmaxˆsI( ˆT(i+1); ˆS) basado en Clu(i+1)(t) si k < l o Clu(0)(t) si k = l; 2.3 i + +. Si Clu, Seg o Ali cambian, ve a 2.1; de lo contrario, si w = 0, devuelve Clu(i), Seg(i) y Ali(i); de lo contrario, j = 0, ve a 3.1; Etapa 3: 3.1 Actualiza w (i+j+1) ˆt basado en Seg(i+j), Ali(i+j) y Clu(i) usando la Ecuación (3); 3.2 Para cada d, verifica todas las segmentaciones secuenciales de d en p segmentos con mapeo s → ˆs → ˆs, y encuentra la mejor Ali (i+j+1) d (Seg (i+j+1) d (s)) = argmaxˆsIw( ˆT(i); ˆS) basado en Clu(i) y w (i+j+1) ˆt; 3.3 j + +. Si Iw( ˆT; ˆS) cambia, ve al paso 6; de lo contrario, detente y devuelve Clu(i), Seg(i+j), Ali(i+j), y w (i+j) ˆt; Figura 2: Algoritmo: Segmentación y alineación de temas basada en MI o WMI. y cada oración se segmenta en palabras, cada palabra se reduce a su raíz. Entonces se puede estimar la distribución de probabilidad conjunta P(D, Sd, T). Finalmente, esta distribución se puede utilizar para calcular la MI en nuestro algoritmo. 4.3 Algoritmo Voraz Iterativo Nuestro objetivo es maximizar la función objetivo, I( ˆT; ˆS) o Iw( ˆT; ˆS), que puede medir la dependencia de las ocurrencias de términos en diferentes segmentos. Generalmente, primero no conocemos los pesos estimados de los términos, los cuales dependen de la segmentación y alineación óptimas de los temas, y los grupos de términos. Además, este problema es NP-duro [10], incluso si conocemos los pesos de los términos. Por lo tanto, se desea un algoritmo voraz iterativo para encontrar la mejor solución, aunque probablemente solo se alcancen máximos locales. Presentamos el algoritmo voraz iterativo en la Figura 2 para encontrar un máximo local de I( ˆT; ˆS) o Iw( ˆT; ˆS) con estimación simultánea de pesos de términos. Este algoritmo es iterativo y codicioso para casos de múltiples documentos o casos de un solo documento con estimación de peso de términos y/o co-clustering de términos. De lo contrario, dado que es solo un algoritmo de un paso para resolver la tarea de segmentación de un solo documento [6, 15, 25], se garantiza el máximo global de MI. Mostraremos más adelante que la coagrupación de términos reduce la precisión de los resultados y no es necesaria, y para la segmentación de un solo documento, los pesos de los términos tampoco son requeridos. 4.3.1 Inicialización En el Paso 0, la agrupación inicial de términos Clut y la segmentación y alineación de temas Segd y Alid son importantes para evitar máximos locales y reducir el número de iteraciones. Primero, se puede hacer una buena estimación de los pesos de los términos utilizando las distribuciones de frecuencia de términos a lo largo de las oraciones para cada documento y promediándolas para obtener los valores iniciales de wˆt: wt = ( ED(t) maxt ∈T (ED(t )) )(1 − ES(t) maxt ∈T (ES(t )) ), (6) donde ES(t) = 1 |Dt| d∈Dt (1 − s∈Sd p(s|t)log|Sd| 1 p(s|t) ), donde Dt es el conjunto de documentos que contienen el término t. Luego, para la segmentación inicial Seg(0), simplemente podemos segmentar los documentos de manera equitativa por oraciones. O podemos encontrar la segmentación óptima solo para cada documento d que maximice el WMI, Seg (0) d = argmaxˆsIw(T; ˆS), donde w = w (0) ˆt. Para el alineamiento inicial Ali(0), podemos primero asumir que el orden de los segmentos para cada d es el mismo. Para el agrupamiento inicial de términos Clu(0), las etiquetas de los primeros grupos pueden establecerse al azar, y después de la primera vez del Paso 3, se obtiene un buen agrupamiento inicial de términos. 4.3.2 Diferentes Casos Después de la inicialización, hay tres etapas para diferentes casos. En total hay ocho casos, |D| = 1 o |D| > 1, k = l o k < l, w = 0 o w = 1. La segmentación de un solo documento sin agrupamiento de términos y estimación de peso de términos (|D| = 1, k = l, w = 0) solo requiere la Etapa 1 (Paso 1). Si se requiere el agrupamiento de términos (k < l), la Etapa 2 (Paso 2.1, 2.2 y 2.3) se ejecuta de forma iterativa. Si se requiere la estimación del peso del término (w = 1), la Etapa 3 (Paso 3.1, 3.2 y 3.3) se ejecuta de forma iterativa. Si ambos son requeridos (k < l, w = 1), la Etapa 2 y 3 se ejecutan una tras otra. Para la segmentación de múltiples documentos sin agrupamiento de términos y estimación de peso de términos (|D| > 1, k = l, w = 0), solo se requiere la iteración de los Pasos 2.2 y 2.3. En la Etapa 1, el máximo global se puede encontrar basándose en I( ˆT; ˆS) utilizando programación dinámica en la Sección 4.4. Es imposible encontrar simultáneamente un buen agrupamiento de términos y pesos estimados de términos, ya que al mover un término a un nuevo grupo de términos para maximizar Iw( ˆT; ˆS), no sabemos si el peso de este término debería ser el del nuevo grupo o el del antiguo. Por lo tanto, primero realizamos el agrupamiento de términos en la Etapa 2, y luego estimamos los pesos de los términos en la Etapa 3. En la Etapa 2, el Paso 2.1 consiste en encontrar el mejor agrupamiento de términos y el Paso 2.2 consiste en encontrar la mejor segmentación. Este ciclo se repite para encontrar un máximo local basado en MI I hasta que converge. Los dos pasos son: (1) basados en el agrupamiento de términos actual Cluˆt, para cada documento d, el algoritmo segmenta todas las oraciones Sd en p segmentos secuencialmente (algunos segmentos pueden estar vacíos), y los coloca en los p segmentos ˆS del conjunto de entrenamiento completo D (se verifican todos los casos posibles de segmentación diferente Segd y alineación Alid) para encontrar el caso óptimo, y (2) basado en la segmentación y alineación actual, para cada término t, el algoritmo encuentra el mejor grupo de términos de t basado en la segmentación actual Segd y la alineación Alid. Después de encontrar un buen agrupamiento de términos, se estiman los pesos de los términos si w = 1. En la Etapa 3, al igual que en la Etapa 2, el Paso 3.1 es la reestimación del peso del término y el Paso 3.2 es encontrar una mejor segmentación. Se repiten para encontrar un máximo local basado en WMI Iw hasta que converja. Sin embargo, si el agrupamiento en la Etapa 2 no es preciso, entonces la estimación de peso en la Etapa 3 puede tener un mal resultado. Finalmente, en el Paso 3.3, este algoritmo converge y devuelve la salida. Este algoritmo puede manejar tanto la segmentación de un solo documento como la de múltiples documentos. También puede detectar temas compartidos entre documentos al verificar la proporción de frases superpuestas sobre los mismos temas, como se describe en la Sección 5.2. 4.4 Optimización del algoritmo. En muchos trabajos anteriores sobre segmentación, la programación dinámica es una técnica utilizada para maximizar la función objetivo. De manera similar, en los Pasos 1, 2.2 y 3.2 de nuestro algoritmo, podemos utilizar programación dinámica. Para la Etapa 1, utilizando programación dinámica aún se puede encontrar el óptimo global, pero para la Etapa 2 y la Etapa 3, solo podemos encontrar el óptimo para cada paso de segmentación de temas y alineación de un documento. Aquí solo mostramos la programación dinámica para el Paso 3.2 utilizando WMI (el Paso 1 y 2.2 son similares pero pueden usar I o Iw). Hay dos casos que no se muestran en el algoritmo de la Figura 2: (a) segmentación de un solo documento o segmentación de múltiples documentos con el mismo orden secuencial de segmentos, donde no se requiere alineación, y (b) segmentación de múltiples documentos con diferentes órdenes secuenciales de segmentos, donde la alineación es necesaria. La función de mapeo de alineación del primer caso es simplemente Alid(ˆsi) = ˆsi, mientras que para los últimos casos la función de mapeo de alineación es Alid(ˆsi) = ˆsj, donde i y j pueden ser diferentes. Los pasos computacionales para los dos casos se enumeran a continuación: Caso 1 (sin alineación): Para cada documento d: (1) Calcular pw(ˆt), pw parcial(ˆt, ˆs) y pw parcial(ˆs) sin contar las oraciones de d. Luego, colocar las oraciones de i a j en la Parte k, y calcular WMI parcial PIw( ˆT; ˆsk(si, si+1, ..., sj)) ˆt∈ ˆT pw(ˆt, ˆsk)log pw(ˆt, ˆsk) pw(ˆt)pw(ˆsk) , donde Alid(si, si+1, ..., sj) = k, k ∈ {1, 2, ..., p}, 1 ≤ i ≤ j ≤ nd, y Segd(sq) = ˆsk para todo i ≤ q ≤ j. (2) Dejar que M(sm, 1) = PIw( ˆT; ˆs1(s1, s2, ..., sm)). Entonces M(sm, L) = maxi[M(si−1, L − 1) + PIw( ˆT; ˆsL(si, ..., sm))], donde 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, y cuando i > m, no se colocan oraciones en ˆsk al calcular PIw (nota PIw( ˆT; ˆs(si, ..., sm)) = 0 para la segmentación de un solo documento). (3) Finalmente M(snd , p) = maxi[M(si−1, p − 1)+ PIw( ˆT; ˆsp(si, ..., snd ))], donde 1 ≤ i ≤ nd+1. Se encuentra el Iw óptimo y la segmentación correspondiente es la mejor. Caso 2 (alineación requerida): Para cada documento d: (1) Calcular pw(ˆt), pw parcial(ˆt, ˆs), y pw parcial(ˆs), y PIw( ˆT; ˆsk(si, si+1, ..., sj)) de manera similar al Caso 1. (2) Sea M(sm, 1, k) = PIw( ˆT; ˆsk(s1, s2, ..., sm)), donde k ∈ {1, 2, ..., p}. Entonces M(sm, L, kL) = maxi,j[M(si−1, L − 1, kL/j) + PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., sm))], donde 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, kL ∈ Set(p, L), que es el conjunto de todos los p! Hay L!(p−L)! combinaciones de L segmentos elegidos de entre todos los p segmentos, j ∈ kL, el conjunto de L segmentos elegidos de entre todos los p segmentos, y kL/j es la combinación de L − 1 segmentos en kL excepto el Segmento j. (3) Finalmente, M(snd , p, kp) = maxi,j[M(si−1, p − 1, kp/j) +PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., snd ))], donde kp es simplemente la combinación de todos los p segmentos y 1 ≤ i ≤ nd + 1, que es el óptimo Iw y la segmentación correspondiente es la mejor. Los pasos de los Casos 1 y 2 son similares, excepto que en el Caso 2, se considera el alineamiento además de la segmentación. Primero, se calculan los elementos básicos de probabilidad para computar Iw excluyendo el documento d, y luego se calcula el WMI parcial colocando cada segmento secuencial posible (incluido el segmento vacío) de d en cada segmento del conjunto. Segundo, se encuentra la suma óptima de PIw para L segmentos y las m oraciones más a la izquierda, M(sm, L). Finalmente, se encuentra el WMI máximo entre diferentes sumas de M(sm, p − 1) y PIw para el Segmento p. 5. EXPERIMENTOS En esta sección se probará la segmentación de un solo documento, la detección de temas compartidos y la segmentación de múltiples documentos. Se estudian diferentes hiperparámetros de nuestro método. Para mayor comodidad, nos referimos al método utilizando I como MIk si w = 0, e Iw como WMIk si w = 2 o como WMIk si w = 1, donde k es el número de grupos de términos, y si k = l, donde l es el número total de términos, entonces no se requiere agrupación de términos, es decir, El primer conjunto de datos que probamos es uno sintético utilizado en investigaciones anteriores [6, 15, 25] y muchos otros artículos. Tiene 700 muestras. Cada uno es una concatenación de diez segmentos. Cada segmento es la primera oración seleccionada al azar de las n primeras oraciones del corpus Brown, que se supone que tienen un tema diferente entre sí. Actualmente, los mejores resultados en este conjunto de datos son logrados por Ji et al. [15]. Para comparar el rendimiento de nuestros métodos, se aplica el criterio ampliamente utilizado en investigaciones anteriores en lugar del criterio imparcial introducido en [20]. Elige un par de palabras al azar. Si se encuentran en segmentos diferentes (diferentes) para la segmentación real (real), pero se predicen (pred) como en el mismo segmento, es un error. Si están en el mismo segmento (mismo), pero se predice que están en segmentos diferentes, es una falsa alarma. Por lo tanto, la tasa de error se calcula utilizando la siguiente ecuación: p(err|real, pred) = p(miss|real, pred, diff)p(diff|real) + p(false alarm|real, pred, same)p(same|real). 5.1.2 Resultados del Experimento Probamos el caso cuando se conoce el número de segmentos. La Tabla 1 muestra los resultados de nuestros métodos con diferentes valores de hiperparámetros y tres enfoques anteriores, C99[25], U00[6] y ADDP03[15], en este conjunto de datos cuando se conoce el número de segmento. En WMI para la segmentación de un solo documento, los pesos de los términos se calculan de la siguiente manera: wˆt = 1−EˆS(ˆt)/maxˆt ∈ ˆT (EˆS(ˆt)). Para este caso, nuestros métodos MIl y WMIl superan a todos los enfoques anteriores. Comparamos nuestros métodos con ADDP03 utilizando una prueba t de una muestra unilateral y los valores de p se muestran en la Tabla 2. A partir de los valores p, podemos ver que en su mayoría las diferencias son muy significativas. También comparamos las tasas de error entre nuestros dos métodos utilizando una prueba t de dos muestras de dos colas para verificar la hipótesis de que son iguales. No podemos rechazar la hipótesis de que son iguales, por lo que las diferencias no son significativas, a pesar de que todas las tasas de error para MIl son más pequeñas que las de WMIl. Sin embargo, podemos concluir que los pesos de los términos contribuyen poco en la segmentación de un solo documento. Los resultados también muestran que el uso de MI con co-clustering de términos (k = 100) disminuye el rendimiento. Probamos diferentes números de grupos de términos y encontramos que el rendimiento mejora cuando el número de grupos aumenta hasta llegar a l. WMIk<l tiene resultados similares que no mostramos en la tabla. Como se mencionó anteriormente, el uso de MI puede ser inconsistente en los límites óptimos dados diferentes números de segmentos. Esta situación ocurre especialmente cuando las similitudes entre los segmentos son bastante diferentes, es decir, algunas transiciones son muy obvias, mientras que otras no lo son. Esto se debe a que normalmente un documento es una estructura jerárquica en lugar de una estructura únicamente secuencial. Cuando los segmentos no están en el mismo nivel, esta situación puede ocurrir. Por lo tanto, se desea un enfoque de segmentación jerárquica de temas, y la estructura depende en gran medida del número de segmentos para cada nodo interno y del criterio de parada de la división. Para este conjunto de datos de segmentación de un solo documento, dado que es solo un conjunto sintético, que es simplemente una concatenación de varios segmentos sobre diferentes temas, es razonable que enfoques simplemente basados en la frecuencia de términos tengan un buen rendimiento. Normalmente, para las tareas de segmentación de documentos coherentes en subtemas, la efectividad disminuye mucho. 5.2 Detección de Temas Compartidos 5.2.1 Datos de Prueba y Evaluación El segundo conjunto de datos contiene 80 artículos de noticias de Google News. Hay ocho temas y cada uno tiene 10 artículos. Dividimos aleatoriamente el conjunto en subconjuntos con diferentes números de documentos y cada subconjunto tiene los ocho temas. Comparamos nuestro enfoque MIl y WMIl con LDA [4]. LDA trata un documento en el conjunto de datos como una bolsa de palabras, encuentra su distribución en temas y su tema principal. MIl y WMIl ven cada oración como un conjunto de palabras y la etiquetan con una etiqueta de tema. Luego, para cada par de documentos, LDA determina si están en el mismo tema, mientras que MIl y Table 3: Detección de Temas Compartidos: Tasas de Error Promedio para Diferentes Números de Documentos en Cada Subconjunto #Doc 10 20 40 80 LDA 8.89% 16.33% 1.35% 0.60% MIl, θ = 0.6 4.17% 1.71% 1.47% 0.0% WMIl, θ = 0.8 18.6% 3.16% 1.92% 0.0% WMIl comprueba si la proporción de oraciones superpuestas en el mismo tema es mayor que el umbral ajustable θ. Es decir, en MIl y WMIl, para un par de documentos d, d , si [ s∈Sd,s ∈Sd 1(temas=temas )/min(|Sd|, |Sd|)] > θ, donde Sd es el conjunto de oraciones de d, y |Sd| es el número de oraciones de d, entonces d y d comparten el tema. Para un par de documentos seleccionados al azar, la tasa de error se calcula utilizando la siguiente ecuación: p(err|real, pred) = p(miss|real, pred, same)p(same|real) + p(false alarm|real, pred, diff)p(diff|real), donde un error significa si tienen el mismo tema (same) para el caso real (real), pero se predice (pred) como en un tema diferente. Si están en diferentes temas (diff), pero se predicen como en el mismo tema, es una falsa alarma. 5.2.2 Resultados del Experimento Los resultados se muestran en la Tabla 3. Si la mayoría de los documentos tienen diferentes temas, en WMIl, la estimación de los pesos de términos en la Ecuación (3) no es correcta. Por lo tanto, no se espera que WMIL tenga un mejor rendimiento que MIL cuando la mayoría de los documentos tienen temas diferentes. Cuando hay menos documentos en un subconjunto con el mismo número de temas, más documentos tienen temas diferentes, por lo que WMIl es peor que MIl. Podemos ver que en la mayoría de los casos, MIl tiene un rendimiento mejor (o al menos similar) que LDA. Después de la detección de temas compartidos, se puede llevar a cabo la segmentación de documentos con los temas compartidos. 5.3 Segmentación de múltiples documentos 5.3.1 Datos de prueba y evaluación Para la segmentación y alineación de múltiples documentos, nuestro objetivo es identificar estos segmentos sobre el mismo tema entre varios documentos similares con temas compartidos. Se espera que al utilizar pesos de términos, Iw funcione mejor que I, ya que sin pesos de términos, el resultado se ve seriamente afectado por palabras de parada dependientes del documento y palabras ruidosas que dependen del estilo de escritura personal. Es más probable tratar los mismos segmentos de diferentes documentos como segmentos diferentes bajo el efecto de palabras de parada dependientes del documento y palabras ruidosas. Los pesos de términos pueden reducir el efecto de las palabras de parada dependientes del documento y las palabras ruidosas al darle más peso a los términos de señal. El conjunto de datos para la segmentación y alineación de múltiples documentos tiene un total de 102 muestras y 2264 oraciones. Cada uno es la parte de introducción de un informe de laboratorio seleccionado del curso de Biol 240W, Universidad Estatal de Pensilvania. Cada muestra tiene dos segmentos, la introducción de las hormonas vegetales y el contenido en el laboratorio. El rango de longitud de las muestras va desde dos hasta 56 oraciones. Algunas muestras solo tienen una parte y algunas tienen un orden inverso de estos dos segmentos. No es difícil identificar el límite entre dos segmentos para los humanos. Etiquetamos cada oración manualmente para evaluación. El criterio de evaluación consiste en utilizar la proporción del número de oraciones con etiquetas de segmento incorrectas predichas en el número total de oraciones en toda la Tabla de entrenamiento. Para mostrar los beneficios de la segmentación y alineación de múltiples documentos, comparamos nuestro método con diferentes parámetros en diferentes particiones del mismo conjunto de entrenamiento. Excepto los casos en los que el número de documentos es 102 y uno (que son casos especiales de uso del conjunto completo y la segmentación pura de un solo documento), dividimos aleatoriamente el conjunto de entrenamiento en m particiones, y cada una tiene 51, 34, 20, 10, 5 y 2 muestras de documentos. Luego aplicamos nuestros métodos en cada partición y calculamos la tasa de error de todo el conjunto de entrenamiento. Cada caso se repitió 10 veces para calcular las tasas de error promedio. Para diferentes particiones del conjunto de entrenamiento, se utilizan diferentes valores de k, ya que el número de términos aumenta cuando el número de documentos en cada partición aumenta. 5.3.2 Resultados del Experimento De los resultados del experimento en la Tabla 4, podemos ver las siguientes observaciones: (1) Cuando el número de documentos aumenta, todos los métodos tienen un mejor rendimiento. Solo de uno a dos documentos, el MIL ha disminuido un poco. Podemos observar esto en la Figura 3 en el punto del número de documento = 2. La mayoría de las curvas incluso tienen los peores resultados en este punto. Hay dos razones. Primero, porque las muestras votan por la mejor segmentación y alineación de múltiples documentos, pero si solo se comparan dos documentos entre sí, aquel con segmentos faltantes o una secuencia totalmente diferente afectará la correcta segmentación y alineación del otro. Segundo, como se señaló al principio de esta sección, si dos documentos tienen más palabras de parada dependientes del documento o palabras ruidosas que palabras clave, entonces el algoritmo puede considerarlos como dos segmentos diferentes y faltar el otro segmento. Generalmente, solo podemos esperar un mejor rendimiento cuando el número de documentos es mayor que el número de segmentos. (2) Excepto en la segmentación de un solo documento, WMIl siempre es mejor que MIl, y cuando el número de documentos se acerca a uno o aumenta a un número muy grande, sus rendimientos se vuelven más cercanos. La Tabla 5 muestra los valores p de la prueba t de dos muestras unilaterales entre MIl y WMIl. También podemos observar esta tendencia a partir de los valores de p. Cuando el número de documento es igual a 5, alcanzamos el valor p más pequeño y la mayor diferencia entre las tasas de error de MIl y WMIl. Para la Tabla 6 de un solo documento: Segmentación de múltiples documentos: Tasa de error promedio para el Documento Número = 5 en cada subconjunto con Diferente Número de Agrupaciones de Términos #Agrupaciones 75 100 150 250 l MIk 24.67% 24.54% 23.91% 22.59% 15.77% de segmentación, WMIl es incluso un poco peor que MIl, lo cual es similar a los resultados de la segmentación de un solo documento en el primer conjunto de datos. La razón es que para la segmentación de un solo documento, no podemos estimar con precisión los pesos de los términos, ya que no tenemos disponibles varios documentos. (3) El uso de agrupamiento de términos generalmente produce resultados peores que MIl y WMIl. (4) El uso de agrupamiento de términos en WMIk es aún peor que en MIk, ya que en WMIk los grupos de términos se encuentran primero utilizando I antes que Iw. Si los grupos de términos no son correctos, entonces los pesos de los términos se estiman peor, lo que puede llevar a que el algoritmo obtenga resultados aún peores. De los resultados también encontramos que en la segmentación y alineación de múltiples documentos, la mayoría de los documentos con segmentos faltantes y en orden inverso son identificados correctamente. La Tabla 6 ilustra los resultados del experimento para el caso de 20 particiones (cada una con cinco muestras de documentos) del conjunto de entrenamiento y la segmentación y alineación de temas utilizando MIk con diferentes números de grupos de términos k. Observa que cuando el número de grupos de términos aumenta, la tasa de error disminuye. Sin agrupamiento de términos, obtenemos el mejor resultado. No mostramos resultados para WMIk con agrupamiento de términos, pero los resultados son similares. También probamos WMIL con diferentes hiperparámetros de a y b para ajustar los pesos de los términos. Los resultados se presentan en la Figura 3. Se demostró que el caso predeterminado WMIl: a = 1, b = 1 dio los mejores resultados para diferentes particiones del conjunto de entrenamiento. Podemos observar la tendencia de que cuando el número de documentos es muy pequeño o grande, la diferencia entre MIl: a = 0, b = 0 y WMIl: a = 1, b = 1 se vuelve bastante pequeña. Cuando el número de documentos no es grande (aproximadamente de 2 a 10), todos los casos que utilizan pesos de términos tienen un mejor rendimiento que MIl: a = 0, b = 0 sin pesos de términos, pero cuando el número de documentos aumenta, los casos WMIl: a = 1, b = 0 y WMIl: a = 2, b = 1 empeoran en comparación con MIl: a = 0, b = 0. Cuando el número de documentos se vuelve muy grande, son aún peores que los casos con números de documentos pequeños. Esto significa que es muy importante encontrar una forma adecuada de estimar los pesos de los términos para el criterio de WMI. La Figura 4 muestra los pesos de los términos aprendidos de todo el conjunto de entrenamiento. Cuatro tipos de palabras se categorizan de manera aproximada aunque la transición entre ellas es sutil. La Figura 5 ilustra el cambio en la información mutua (ponderada) para MIl y WMIl. Como era de esperar, la información mutua para MIl aumenta de forma monótona con el número de pasos, mientras que WMIl no lo hace. Finalmente, MIl y WMIl son escalables, con complejidad computacional mostrada en la Figura 6. Una ventaja de nuestro enfoque basado en MI es que no es necesario eliminar las palabras vacías. Otra ventaja importante es que no hay hiperparámetros necesarios que ajustar. En la segmentación de un solo documento, el rendimiento basado en MI es aún mejor que el basado en WMI, por lo que no se requiere un hiperparámetro adicional. En la segmentación de múltiples documentos, mostramos en el experimento que a = 1 y b = 1 es lo mejor. Nuestro método otorga más peso a los términos de señal. Sin embargo, generalmente los términos o frases de señal aparecen al principio de un segmento, mientras que el final del segmento puede ser 1 2 5 10 20 34 51 102 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 Número de Documento Tasa de Error MIl :a=0,b=0 WMI l :a=1,b=1 WMI l :a=1,b=0 WMI l :a=2,b=1 Figura 3: Tasas de error para diferentes hiperparámetros de pesos de términos. 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Entropía de Documento Normalizada Entropía de Segmento Normalizada Palabras ruidosas Palabras de señal Palabras de parada comunes Palabras de parada de Doc-dep Figura 4: Pesos de términos aprendidos de todo el conjunto de entrenamiento. 0 100 200 300 400 500 600 0.06 0.08 0.1 0.12 0.14 0.16 0.18 Número de Pasos (Ponderado) Información Mutua MI l WMI l Figura 5: Cambio en la MI (ponderada) para MIl y WMIl. 0 20 40 60 80 100 120 0 200 400 600 800 1000 1200 1400 1600 1800 2000 Número de Documento Tiempo de Convergencia (seg) MI l WMI l Figura 6: Tiempo de convergencia para MIl y WMIl. muy ruidoso. Una posible solución es dar más peso a los términos al principio de cada segmento. Además, cuando la longitud de los segmentos es bastante diferente, los segmentos largos tienen frecuencias de términos mucho más altas, por lo que pueden dominar los límites de segmentación. La normalización de las frecuencias de términos en relación con la longitud del segmento puede ser útil. 6. CONCLUSIONES Y TRABAJO FUTURO Propusimos un método novedoso para la segmentación y alineación de temas en múltiples documentos basado en información mutua ponderada, que también puede manejar casos de un solo documento. Utilizamos programación dinámica para optimizar nuestro algoritmo. Nuestro enfoque supera a todos los métodos anteriores en casos de un solo documento. Además, también demostramos que realizar segmentación entre varios documentos puede mejorar el rendimiento enormemente. Nuestros resultados también demostraron que el uso de la información mutua ponderada puede aprovechar la información de varios documentos para lograr un mejor rendimiento. Solo probamos nuestro método en conjuntos de datos limitados. Se deben probar más conjuntos de datos, especialmente los complicados. Deberían compararse más métodos anteriores. Además, las segmentaciones naturales como los párrafos son pistas que se pueden utilizar para encontrar los límites óptimos. El aprendizaje supervisado también puede ser considerado. 7. AGRADECIMIENTOS Los autores desean agradecer a Xiang Ji y al Prof. J. Scott Payne por su ayuda. 8. REFERENCIAS [1] A. Banerjee, I. Ghillon, J. Ghosh, S. Merugu y D. Modha. Un enfoque generalizado de entropía máxima para la coagrupación de Bregman y la aproximación de matrices. En Actas de SIGKDD, 2004. [2] R. Bekkerman, R. El-Yaniv y A. McCallum. Agrupación distribucional de múltiples vías a través de interacciones por pares. En Actas de ICML, 2005. [3] D. M. Blei y P. J. Moreno. Segmentación de temas con un modelo oculto de Markov de aspectos. En Actas de SIGIR, 2001. [4] D. M. Blei, A. Ng y M. Jordan. Asignación latente de Dirichlet. Revista de Investigación en Aprendizaje Automático, 3:993-1022, 2003. [5] T. Brants, F. Chen e I. Tsochantaridis. Segmentación de documentos basada en temas con análisis semántico latente probabilístico. En Actas de CIKM, 2002. [6] F. Choi. Avances en la segmentación lineal de texto independiente del dominio. En Actas de la NAACL, 2000. [7] H. Christensen, B. Kolluru, Y. Gotoh y S. Renals. Segmentación de máxima entropía de noticias de transmisión. En Actas de ICASSP, 2005. [8] T. Cover y J. Thomas. Elementos de la teoría de la información. John Wiley and Sons, Nueva York, EE. UU., 1991. [9] S. Deerwester, S. Dumais, G. Furnas, T. Landauer y R. Harshman. Indexación mediante análisis semántico latente. Revista de la Sociedad Americana de Sistemas de Información, 1990. [10] I. Dhillon, S. Mallela y D. Modha. Co-clustering informativo-teórico. En Actas de SIGKDD, 2003. [11] M. Hajime, H. Takeo y O. Manabu. Segmentación de texto con múltiples señales lingüísticas superficiales. En Actas de COLING-ACL, 1998. [12] T. K. Ho. Detención de palabras y identificación para el reconocimiento de texto adaptativo. Revista Internacional de Análisis y Reconocimiento de Documentos, 3(1), agosto de 2000. [13] T. Hofmann. Análisis semántico latente probabilístico. En Actas de la UAI99, 1999. [14] X. Ji y H. Zha. Correlación de la sumarización de un par de documentos multilingües. En Actas de RIDE, 2003. [15] X. Ji y H. Zha. Segmentación de texto independiente del dominio utilizando difusión anisotrópica y programación dinámica. En Actas de SIGIR, 2003. [16] X. Ji y H. Zha. Extrayendo temas compartidos de múltiples documentos. En Actas de la 7ª PAKDD, 2003. [17] J. Lafferty, A. McCallum y F. Pereira. Campos aleatorios condicionales: Modelos probabilísticos para segmentar y etiquetar datos de secuencias. En Actas de ICML, 2001. [18] T. Li, S. Ma y M. Ogihara. Criterio basado en la entropía en la agrupación categórica. En Actas de ICML, 2004. [19] A. McCallum, D. Freitag y F. Pereira. Modelos de máxima entropía de Markov para extracción de información y segmentación. En Actas de ICML, 2000. [20] L. Pevzner y M. Hearst. Una crítica y mejora de una métrica de evaluación para la segmentación de texto. Lingüística Computacional, 28(1):19-36, 2002. [21] J. C. Reynar. Modelos estadísticos para la segmentación de temas. En Actas de ACL, 1999. [22] G. Salton y M. McGill. Introducción a la Recuperación de Información Moderna. McGraw Hill, 1983. [23] B. \n\nMcGraw Hill, 1983. [23] B. Sun, Q. Tan, P. Mitra y C. L. Giles. Extracción y búsqueda de fórmulas químicas en documentos de texto en la web. En Actas de WWW, 2007. [24] B. Sun, D. Zhou, H. Zha, y J. I'm sorry, but \"Yen\" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish? Segmentación y alineación de texto multitarea basada en información mutua ponderada. En Actas de CIKM, 2006. [25] M. Utiyama y H. Isahara. Un modelo estadístico para la segmentación de texto independiente del dominio. En Actas de la 39ª ACL, 1999. [26] C. Wayne. Detección y seguimiento de temas multilingües: Investigación exitosa habilitada por corpora y evaluación. En Actas de LREC, 2000. [27] J. Yamron, I. Carp, L. Gillick, S. Lowe y P. van Mulbregt. Un enfoque de modelo oculto de Markov para la segmentación de texto y seguimiento de eventos. En Actas de ICASSP, 1998. [28] H. Zha y X. Ji. Correlacionando documentos multilingües a través de modelado de gráficos bipartitos. En Actas de SIGIR, 2002. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "topic segmentation performance": {
            "translated_key": "Rendimiento de la segmentación de temas",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Topic Segmentation with Shared Topic Detection and Alignment of Multiple Documents Bingjun Sun*, Prasenjit Mitra*† , Hongyuan Zha‡ , C. Lee Giles*† , John Yen*† *Department of Computer Science and Engineering † College of Information Sciences and Technology The Pennsylvania State University University Park, PA 16802 ‡ College of Computing The Georgia Institute of Technology Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu ABSTRACT Topic detection and tracking [26] and topic segmentation [15] play an important role in capturing the local and sequential information of documents.",
                "Previous work in this area usually focuses on single documents, although similar multiple documents are available in many domains.",
                "In this paper, we introduce a novel unsupervised method for shared topic detection and topic segmentation of multiple similar documents based on mutual information (MI) and weighted mutual information (WMI) that is a combination of MI and term weights.",
                "The basic idea is that the optimal segmentation maximizes MI(or WMI).",
                "Our approach can detect shared topics among documents.",
                "It can find the optimal boundaries in a document, and align segments among documents at the same time.",
                "It also can handle single-document segmentation as a special case of the multi-document segmentation and alignment.",
                "Our methods can identify and strengthen cue terms that can be used for segmentation and partially remove stop words by using term weights based on entropy learned from multiple documents.",
                "Our experimental results show that our algorithm works well for the tasks of single-document segmentation, shared topic detection, and multi-document segmentation.",
                "Utilizing information from multiple documents can tremendously improve the performance of topic segmentation, and using WMI is even better than using MI for the multi-document segmentation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Clustering; H.3.1 [Information Storage and Retrieval]: Content Analysis and IndexingLinguistic processing; I.2.7 [Artificial Intelligence]: Natural Language Processing-Text analysis; I.5.3 [Pattern Recognition]: Clustering-Algorithms;Similarity measures General Terms Algorithms, Design, Experimentation 1.",
                "INTRODUCTION Many researchers have worked on topic detection and tracking (TDT) [26] and topic segmentation during the past decade.",
                "Topic segmentation intends to identify the boundaries in a document with the goal to capture the latent topical structure.",
                "Topic segmentation tasks usually fall into two categories [15]: text stream segmentation where topic transition is identified, and coherent document segmentation in which documents are split into sub-topics.",
                "The former category has applications in automatic speech recognition, while the latter one has more applications such as partial-text query of long documents in information retrieval, text summary, and quality measurement of multiple documents.",
                "Previous research in connection with TDT falls into the former category, targeted on topic tracking of broadcast speech data and newswire text, while the latter category has not been studied very well.",
                "Traditional approaches perform topic segmentation on documents one at a time [15, 25, 6].",
                "Most of them perform badly in subtle tasks like coherent document segmentation [15].",
                "Often, end-users seek documents that have the similar content.",
                "Search engines, like, Google, provide links to obtain similar pages.",
                "At a finer granularity, users may actually be looking to obtain sections of a document similar to a particular section that presumably discusses a topic of the users interest.",
                "Thus, the extension of topic segmentation from single documents to identifying similar segments from multiple similar documents with the same topic is a natural and necessary direction, and multi-document topic segmentation is expected to have a better performance since more information is utilized.",
                "Traditional approaches using similarity measurement based on term frequency generally have the same assumption that similar vocabulary tends to be in a coherent topic segment [15, 25, 6].",
                "However, they usually suffer the issue of identifying stop words.",
                "For example, additional document-dependent stop words are removed together with the generic stop words in [15].",
                "There are two reasons that we do not remove stop words directly.",
                "First, identifying stop words is another issue [12] that requires estimation in each domain.",
                "Removing common stop words may result in the loss of useful information in a specific domain.",
                "Second, even though stop words can be identified, hard classification of stop words and nonstop words cannot represent the gradually changing amount of information content of each word.",
                "We employ a soft classification using term weights.",
                "In this paper, we view the problem of topic segmentation as an optimization issue using information theoretic techniques to find the optimal boundaries of a document given the number of text segments so as to minimize the loss of mutual information (MI) (or a weighted mutual information (WMI)) after segmentation and alignment.",
                "This is equal to maximizing the MI (or WMI).",
                "The MI focuses on measuring the difference among segments whereas previous research focused on finding the similarity (e.g. cosine distance) of segments [15, 25, 6].",
                "Topic alignment of multiple similar documents can be achieved by clustering sentences on the same topic into the same cluster.",
                "Single-document topic segmentation is just a special case of the multi-document topic segmentation and alignment problem.",
                "Terms can be co-clustered as in [10] at the same time, given the number of clusters, but our experimental results show that this method results in a worse segmentation (see Tables 1, 4, and 6).",
                "Usually, human readers can identify topic transition based on cue words, and can ignore stop words.",
                "Inspired by this, we give each term (or term cluster) a weight based on entropy among different documents and different segments of documents.",
                "Not only can this approach increase the contribution of cue words, but it can also decrease the effect of common stop words, noisy word, and document-dependent stop words.",
                "These words are common in a document.",
                "Many methods based on sentence similarity require that these words are removed before topic segmentation can be performed [15].",
                "Our results in Figure 3 show that term weights are useful for multi-document topic segmentation and alignment.",
                "The major contribution of this paper is that it introduces a novel method for topic segmentation using MI and shows that this method performs better than previously used criteria.",
                "Also, we have addressed the problem of topic segmentation and alignment across multiple documents, whereas most existing research focused on segmentation of single documents.",
                "Multi-document segmentation and alignment can utilize information from similar documents and improves the performance of topic segmentation greatly.",
                "Obviously, our approach can handle single documents as a special case when multiple documents are unavailable.",
                "It can detect shared topics among documents to judge if they are multiple documents on the same topic.",
                "We also introduce the new criterion of WMI based on term weights learned from multiple similar documents, which can improve performance of topic segmentation further.",
                "We propose an iterative greedy algorithm based on dynamic programming and show that it works well in practice.",
                "Some of our prior work is in [24].",
                "The rest of this paper is organized as follows: In Section 2, we review related work.",
                "Section 3 contains a formulation of the problem of topic segmentation and alignment of multiple documents with term co-clustering, a review of the criterion of MI for clustering, and finally an introduction to WMI.",
                "In Section 4, we first propose the iterative greedy algorithm of topic segmentation and alignment with term co-clustering, and then describe how the algorithm can be optimized by usFigure 1: Illustration of multi-document segmentation and alignment. ing dynamic programming.",
                "In Section 5, experiments about single-document segmentation, shared topic detection, and multi-document segmentation are described, and results are presented and discussed to evaluate the performance of our algorithm.",
                "Conclusions and some future directions of the research work are discussed in Section 6. 2.",
                "PREVIOUS WORK Generally, the existing approaches to text segmentation fall into two categories: supervised learning [19, 17, 23] and unsupervised learning [3, 27, 5, 6, 15, 25, 21].",
                "Supervised learning usually has good performance, since it learns functions from labelled training sets.",
                "However, often getting large training sets with manual labels on document sentences is prohibitively expensive, so unsupervised approaches are desired.",
                "Some models consider dependence between sentences and sections, such as Hidden Markov Model [3, 27], Maximum Entropy Markov Model [19], and Conditional Random Fields [17], while many other approaches are based on lexical cohesion or similarity of sentences [5, 6, 15, 25, 21].",
                "Some approaches also focus on cue words as hints of topic transitions [11].",
                "While some existing methods only consider information in single documents [6, 15], others utilize multiple documents [16, 14].",
                "There are not many works in the latter category, even though the performance of segmentation is expected to be better with utilization of information from multiple documents.",
                "Previous research studied methods to find shared topics [16] and topic segmentation and summarization between just a pair of documents [14].",
                "Text classification and clustering is a related research area which categorizes documents into groups using supervised or unsupervised methods.",
                "Topical classification or clustering is an important direction in this area, especially co-clustering of documents and terms, such as LSA [9], PLSA [13], and approaches based on distances and bipartite graph partitioning [28] or maximum MI [2, 10], or maximum entropy [1, 18].",
                "Criteria of these approaches can be utilized in the issue of topic segmentation.",
                "Some of those methods have been extended into the area of topic segmentation, such as PLSA [5] and maximum entropy [7], but to our best knowledge, using MI for topic segmentation has not been studied. 3.",
                "PROBLEM FORMULATION Our goal is to segment documents and align the segments across documents (Figure 1).",
                "Let T be the set of terms {t1, t2, ..., tl}, which appear in the unlabelled set of documents D = {d1, d2, ..., dm}.",
                "Let Sd be the set of sentences for document d ∈ D, i.e. {s1, s2, ..., snd }.",
                "We have a 3D matrix of term frequency, in which the three dimensions are random variables of D, Sd, and T. Sd actually is a random vector including a random variable for each d ∈ D. The term frequency can be used to estimate the joint probability distribution P(D, Sd, T), which is p(t, d, s) = T(t, d, s)/ND, where T(t, d, s) is the number of t in ds sentence s and ND is the total number of terms in D. ˆS represents the set of segments {ˆs1, ˆs2, ..., ˆsp} after segmentation and alignment among multiple documents, where the number of segments | ˆS| = p. A segment ˆsi of document d is a sequence of adjacent sentences in d. Since for different documents si may discuss different sub-topics, our goal is to cluster adjacent sentences in each document into segments, and align similar segments among documents, so that for different documents ˆsi is about the same sub-topic.",
                "The goal is to find the optimal topic segmentation and alignment mapping Segd(si) : {s1, s2, ..., snd } → {ˆs1, ˆs2, ..., ˆsp} and Alid(ˆsi) : {ˆs1, ˆs2, ..., ˆsp} → {ˆs1, ˆs2, ..., ˆsp}, for all d ∈ D, where ˆsi is ith segment with the constraint that only adjacent sentences can be mapped to the same segment, i.e. for d, {si, si+1, ..., sj} → {ˆsq}, where q ∈ {1, ..., p}, where p is the segment number, and if i > j, then for d, ˆsq is missing.",
                "After segmentation and alignment, random vector Sd becomes an aligned random variable ˆS.",
                "Thus, P(D, Sd, T) becomes P(D, ˆS, T).",
                "Term co-clustering is a technique that has been employed [10] to improve the accuracy of document clustering.",
                "We evaluate the effect of it for topic segmentation.",
                "A term t is mapped to exactly one term cluster.",
                "Term co-clustering involves simultaneously finding the optimal term clustering mapping Clu(t) : {t1, t2, ..., tl} → {ˆt1, ˆt2, ..., ˆtk}, where k ≤ l, l is the total number of words in all the documents, and k is the number of clusters. 4.",
                "METHODOLOGY We now describe a novel algorithm which can handle singledocument segmentation, shared topic detection, and multidocument segmentation and alignment based on MI or WMI. 4.1 Mutual Information MI I(X; Y ) is a quantity to measure the amount of information which is contained in two or more random variables [8, 10].",
                "For the case of two random variables, we have I(X; Y ) = x∈X y∈Y p(x, y)log p(x, y) p(x)p(y) , (1) Obviously, when random variables X and Y are independent, I(X; Y ) = 0.",
                "Thus, intuitively, the value of MI depends on how random variables are dependent on each other.",
                "The optimal co-clustering is the mapping Clux : X → ˆX and Cluy : Y → ˆY that minimizes the loss: I(X; Y ) − I( ˆX; ˆY ), which is equal to maximizing I( ˆX; ˆY ).",
                "This is the criterion of MI for clustering.",
                "In the case of topic segmentation, the two random variables are the term variable T and the segment variable S, and each sample is an occurrence of a term T = t in a particular segment S = s. I(T; S) is used to measure how dependent T and S are.",
                "However, I(T; S) cannot be computed for documents before segmentation, since we do not have a set of S due to the fact that sentences of Document d, si ∈ Sd, is not aligned with other documents.",
                "Thus, instead of minimizing the loss of MI, we can maximize MI after topic segmentation, computed as: I( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS p(ˆt, ˆs)log p(ˆt, ˆs) p(ˆt)p(ˆs) , (2) where p(ˆt, ˆs) are estimated by the term frequency tf of Term Cluster ˆt and Segment ˆs in the training set D. Note that here a segment ˆs includes sentences about the the same topic among all documents.",
                "The optimal solution is the mapping Clut : T → ˆT, Segd : Sd → ˆS , and Alid : ˆS → ˆS, which maximizes I( ˆT; ˆS). 4.2 Weighted Mutual Information In topic segmentation and alignment of multiple documents, if P(D, ˆS, T) is known, based on the marginal distributions P(D|T) and P( ˆS|T) for each term t ∈ T, we can categorize terms into four types in the data set: • Common stop words are common both along the dimensions of documents and segments. • Document-dependent stop words that depends on the personal writing style are common only along the dimension of segments for some documents. • Cue words are the most important elements for segmentation.",
                "They are common along the dimension of documents only for the same segment, and they are not common along the dimensions of segments. • Noisy words are other words which are not common along both dimensions.",
                "Entropy based on P(D|T) and P( ˆS|T) can be used to identify different types of terms.",
                "To reinforce the contribution of cue words in the MI computation, and simultaneously reduce the effect of the other three types of words, similar as the idea of the tf-idf weight [22], we use entropies of each term along the dimensions of document D and segment ˆS, i.e.",
                "ED(ˆt) and EˆS(ˆt), to compute the weight.",
                "A cue word usually has a large value of ED(ˆt) but a small value of EˆS(ˆt).",
                "We introduce term weights (or term cluster weights) wˆt = ( ED(ˆt) maxˆt ∈ ˆT (ED(ˆt )) )a (1 − EˆS(ˆt) maxˆt ∈ ˆT (EˆS(ˆt )) )b , (3) where ED(ˆt) = d∈D p(d|ˆt)log|D| 1 p(d|ˆt) , EˆS(ˆt) = ˆs∈ ˆS p(ˆs|ˆt)log| ˆS| 1 p(ˆs|ˆt) , and a > 0 and b > 0 are powers to adjust term weights.",
                "Usually a = 1 and b = 1 as default, and maxˆt ∈ ˆT (ED(ˆt )) and maxˆt ∈ ˆT (EˆS(ˆt )) are used to normalize the entropy values.",
                "Term cluster weights are used to adjust p(ˆt, ˆs), pw(ˆt, ˆs) = wˆtp(ˆt, ˆs) ˆt∈ ˆT ;ˆs∈ ˆS wˆtp(ˆt, ˆs) , (4) and Iw( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS pw(ˆt, ˆs)log pw(ˆt, ˆs) pw(ˆt)pw(ˆs) , (5) where pw(ˆt) and pw(ˆs) are marginal distributions of pw(ˆt, ˆs).",
                "However, since we do not know either the term weights or P(D, ˆS, T), we need to estimate them, but wˆt depends on p(ˆs|t) and ˆS, while ˆS and p(ˆs|t) also depend on wˆt that is still unknown.",
                "Thus, an iterative algorithm is required to estimate term weights wˆt and find the best segmentation and alignment to optimize the objective function Iw concurrently.",
                "After a document is segmented into sentences Input: Joint probability distribution P(D, Sd, T), number of text segments p ∈ {2, 3, ..., max(sd)}, number of term clusters k ∈ {2, 3, ..., l} (if k = l, no term co-clustering required), and weight type w ∈ {0, 1}, indicating to use I or Iw, respectively.",
                "Output: Mapping Clu, Seg, Ali, and term weights wˆt.",
                "Initialization: 0. i = 0.",
                "Initialize Clu (0) t , Seg (0) d , and Ali (0) d ; Initialize w (0) ˆt using Equation (6) if w = 1; Stage 1: 1.",
                "If |D| = 1, k = l, and w = 0, check all sequential segmentations of d into p segments and find the best one Segd(s) = argmaxˆsI( ˆT; ˆS), and return Segd; otherwise, if w = 1 and k = l, go to 3.1; Stage 2: 2.1 If k < l, for each term t, find the best cluster ˆt as Clu(i+1)(t) = argmaxˆtI( ˆT; ˆS(i)) based on Seg(i) and Ali(i); 2.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+1) d (Seg (i+1) d (s)) = argmaxˆsI( ˆT(i+1); ˆS) based on Clu(i+1)(t) if k < l or Clu(0)(t) if k = l; 2.3 i + +.",
                "If Clu, Seg, or Ali changed, go to 2.1; otherwise, if w = 0, return Clu(i), Seg(i), and Ali(i); else j = 0, go to 3.1; Stage 3: 3.1 Update w (i+j+1) ˆt based on Seg(i+j), Ali(i+j), and Clu(i) using Equation (3); 3.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+j+1) d (Seg (i+j+1) d (s)) = argmaxˆsIw( ˆT(i); ˆS) based on Clu(i) and w (i+j+1) ˆt ; 3.3 j + +.",
                "If Iw( ˆT; ˆS) changes, go to step 6; otherwise, stop and return Clu(i), Seg(i+j), Ali(i+j), and w (i+j) ˆt ; Figure 2: Algorithm: Topic segmentation and alignment based on MI or WMI. and each sentence is segmented into words, each word is stemmed.",
                "Then the joint probability distribution P(D, Sd, T) can be estimated.",
                "Finally, this distribution can be used to compute MI in our algorithm. 4.3 Iterative Greedy Algorithm Our goal is to maximize the objective function, I( ˆT; ˆS) or Iw( ˆT; ˆS), which can measure the dependence of term occurrences in different segments.",
                "Generally, first we do not know the estimate term weights, which depend on the optimal topic segmentation and alignment, and term clusters.",
                "Moreover, this problem is NP-hard [10], even though if we know the term weights.",
                "Thus, an iterative greedy algorithm is desired to find the best solution, even though probably only local maxima are reached.",
                "We present the iterative greedy algorithm in Figure 2 to find a local maximum of I( ˆT; ˆS) or Iw( ˆT; ˆS) with simultaneous term weight estimation.",
                "This algorithm can is iterative and greedy for multi-document cases or single-document cases with term weight estimation and/or term co-clustering.",
                "Otherwise, since it is just a one step algorithm to solve the task of single-document segmentation [6, 15, 25], the global maximum of MI is guaranteed.",
                "We will show later that term co-clustering reduces the accuracy of the results and is not necessary, and for singledocument segmentation, term weights are also not required. 4.3.1 Initialization In Step 0, the initial term clustering Clut and topic segmentation and alignment Segd and Alid are important to avoid local maxima and reduce the number of iterations.",
                "First, a good guess of term weights can be made by using the distributions of term frequency along sentences for each document and averaging them to get the initial values of wˆt: wt = ( ED(t) maxt ∈T (ED(t )) )(1 − ES(t) maxt ∈T (ES(t )) ), (6) where ES(t) = 1 |Dt| d∈Dt (1 − s∈Sd p(s|t)log|Sd| 1 p(s|t) ), where Dt is the set of documents which contain Term t. Then, for the initial segmentation Seg(0) , we can simply segment documents equally by sentences.",
                "Or we can find the optimal segmentation just for each document d which maximizes the WMI, Seg (0) d = argmaxˆsIw(T; ˆS), where w = w (0) ˆt .",
                "For the initial alignment Ali(0) , we can first assume that the order of segments for each d is the same.",
                "For the initial term clustering Clu(0) , first cluster labels can be set randomly, and after the first time of Step 3, a good initial term clustering is obtained. 4.3.2 Different Cases After initialization, there are three stages for different cases.",
                "Totally there are eight cases, |D| = 1 or |D| > 1, k = l or k < l, w = 0 or w = 1.",
                "Single document segmentation without term clustering and term weight estimation (|D| = 1, k = l, w = 0) only requires Stage 1 (Step 1).",
                "If term clustering is required (k < l), Stage 2 (Step 2.1, 2.2, and 2.3) is executed iteratively.",
                "If term weight estimation is required (w = 1), Stage 3 (Step 3.1, 3.2, and 3.3) is executed iteratively.",
                "If both are required (k < l, w = 1), Stage 2 and 3 run one after the other.",
                "For multi-document segmentation without term clustering and term weight estimation (|D| > 1, k = l, w = 0), only iteration of Step 2.2 and 2.3 are required.",
                "At Stage 1, the global maximum can be found based on I( ˆT; ˆS) using dynamic programming in Section 4.4.",
                "Simultaneously finding a good term clustering and estimated term weights is impossible, since when moving a term to a new term cluster to maximize Iw( ˆT; ˆS), we do not know that the weight of this term should be the one of the new cluster or the old cluster.",
                "Thus, we first do term clustering at Stage 2, and then estimate term weights at Stage 3.",
                "At Stage 2, Step 2.1 is to find the best term clustering and Step 2.2 is to find the best segmentation.",
                "This cycle is repeated to find a local maximum based on MI I until it converges.",
                "The two steps are: (1) based on current term clustering Cluˆt, for each document d, the algorithm segments all the sentences Sd into p segments sequentially (some segments may be empty), and put them into the p segments ˆS of the whole training set D (all possible cases of different segmentation Segd and alignment Alid are checked) to find the optimal case, and (2) based on the current segmentation and alignment, for each term t, the algorithm finds the best term cluster of t based on the current segmentation Segd and alignment Alid.",
                "After finding a good term clustering, term weights are estimated if w = 1.",
                "At Stage 3, similar as Stage 2, Step 3.1 is term weight re-estimation and Step 3.2 is to find a better segmentation.",
                "They are repeated to find a local maximum based on WMI Iw until it converges.",
                "However, if the term clustering in Stage 2 is not accurate, then the term weight estimation at Stage 3 may have a bad result.",
                "Finally, at Step 3.3, this algorithm converges and return the output.",
                "This algorithm can handle both single-document and multi-document segmentation.",
                "It also can detect shared topics among documents by checking the proportion of overlapped sentences on the same topics, as described in Sec 5.2. 4.4 Algorithm Optimization In many previous works on segmentation, dynamic programming is a technique used to maximize the objective function.",
                "Similarly, at Step 1, 2.2, and 3.2 of our algorithm, we can use dynamic programming.",
                "For Stage 1, using dynamic programming can still find the global optimum, but for Stage 2 and Stage 3, we can only find the optimum for each step of topic segmentation and alignment of a document.",
                "Here we only show the dynamic programming for Step 3.2 using WMI (Step 1 and 2.2 are similar but they can use either I or Iw).",
                "There are two cases that are not shown in the algorithm in Figure 2: (a) single-document segmentation or multi-document segmentation with the same sequential order of segments, where alignment is not required, and (b) multi-document segmentation with different sequential orders of segments, where alignment is necessary.",
                "The alignment mapping function of the former case is simply just Alid(ˆsi) = ˆsi, while for the latter ones alignment mapping function Alid(ˆsi) = ˆsj, i and j may be different.",
                "The computational steps for the two cases are listed below: Case 1 (no alignment): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs) and partial pw(ˆs) without counting sentences from d. Then put sentences from i to j into Part k, and compute partial WMI PIw( ˆT; ˆsk(si, si+1, ..., sj)) ˆt∈ ˆT pw(ˆt, ˆsk)log pw(ˆt, ˆsk) pw(ˆt)pw(ˆsk) , where Alid(si, si+1, ..., sj) = k, k ∈ {1, 2, ..., p}, 1 ≤ i ≤ j ≤ nd, and Segd(sq) = ˆsk for all i ≤ q ≤ j. (2) Let M(sm, 1) = PIw( ˆT; ˆs1(s1, s2, ..., sm)).",
                "Then M(sm, L) = maxi[M(si−1, L − 1) + PIw( ˆT; ˆsL(si, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, and when i > m, no sentences are put into ˆsk when compute PIw (note PIw( ˆT; ˆs(si, ..., sm)) = 0 for single-document segmentation). (3) Finally M(snd , p) = maxi[M(si−1, p − 1)+ PIw( ˆT; ˆsp(si, ..., snd ))], where 1 ≤ i ≤ nd+1.",
                "The optimal Iw is found and the corresponding segmentation is the best.",
                "Case 2 (alignment required): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs), and partial pw(ˆs), and PIw( ˆT; ˆsk(si, si+1, ..., sj)) similarly as Case 1. (2) Let M(sm, 1, k) = PIw( ˆT; ˆsk(s1, s2, ..., sm)), where k ∈ {1, 2, ..., p}.",
                "Then M(sm, L, kL) = maxi,j[M(si−1, L − 1, kL/j) + PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, kL ∈ Set(p, L), which is the set of all p!",
                "L!(p−L)! combinations of L segments chosen from all p segments, j ∈ kL, the set of L segments chosen from all p segments, and kL/j is the combination of L − 1 segments in kL except Segment j. (3) Finally, M(snd , p, kp) = maxi,j[M(si−1, p − 1, kp/j) +PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., snd ))], where kp is just the combination of all p segments and 1 ≤ i ≤ nd + 1, which is the optimal Iw and the corresponding segmentation is the best.",
                "The steps of Case 1 and 2 are similar, except in Case 2, alignment is considered in addition to segmentation.",
                "First, basic items of probability for computing Iw are computed excluding Doc d, and then partial WMI by putting every possible sequential segment (including empty segment) of d into every segment of the set.",
                "Second, the optimal sum of PIw for L segments and the leftmost m sentences, M(sm, L), is found.",
                "Finally, the maximal WMI is found among different sums of M(sm, p − 1) and PIw for Segment p. 5.",
                "EXPERIMENTS In this section, single-document segmentation, shared topic detection, and multi-document segmentation will be tested.",
                "Different hyper parameters of our method are studied.",
                "For convenience, we refer to the method using I as MIk if w = 0, and Iw as WMIk if w = 2 or as WMIk if w = 1, where k is the number of term clusters, and if k = l, where l is the total number of terms, then no term clustering is required, i.e.",
                "MIl and WMIl. 5.1 Single-document Segmentation 5.1.1 Test Data and Evaluation The first data set we tested is a synthetic one used in previous research [6, 15, 25] and many other papers.",
                "It has 700 samples.",
                "Each is a concatenation of ten segments.",
                "Each segment is the first n sentence selected randomly from the Brown corpus, which is supposed to have a different topic from each other.",
                "Currently, the best results on this data set is achieved by Ji et.al. [15].",
                "To compare the performance of our methods, the criterion used widely in previous research is applied, instead of the unbiased criterion introduced in [20].",
                "It chooses a pair of words randomly.",
                "If they are in different segments (different) for the real segmentation (real), but predicted (pred) as in the same segment, it is a miss.",
                "If they are in the same segment (same), but predicted as in different segments, it is a false alarm.",
                "Thus, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, diff)p(diff|real) +p(false alarm|real, pred, same)p(same|real). 5.1.2 Experiment Results We tested the case when the number of segments is known.",
                "Table 1 shows the results of our methods with different hyper parameter values and three previous approaches, C99[25], U00[6], and ADDP03[15], on this data set when the segment number is known.",
                "In WMI for single-document segmentation, the term weights are computed as follows: wˆt = 1−EˆS(ˆt)/maxˆt ∈ ˆT (EˆS(ˆt )).",
                "For this case, our methods MIl and WMIl both outperform all the previous approaches.",
                "We compared our methods with ADDP03using one-sample one-sided t-test and p-values are shown in Table 2.",
                "From the p-values, we can see that mostly the differences are very Table 1: Average Error Rates of Single-document Segmentation Given Segment Numbers Known Range of n 3-11 3-5 6-8 9-11 Sample size 400 100 100 100 C99 12% 11% 10% 9% U00 10% 9% 7% 5% ADDP03 6.0% 6.8% 5.2% 4.3% MIl 4.68% 5.57% 2.59% 1.59% WMIl 4.94% 6.33% 2.76% 1.62% MI100 9.62% 12.92% 8.66% 6.67% Table 2: Single-document Segmentation: P-values of T-test on Error Rates Range of n 3-11 3-5 6-8 9-11 ADDP03, MIl 0.000 0.000 0.000 0.000 ADDP03, WMIl 0.000 0.099 0.000 0.000 MIl, WMIl 0.061 0.132 0.526 0.898 significant.",
                "We also compare the error rates between our two methods using two-sample two-sided t-test to check the hypothesis that they are equal.",
                "We cannot reject the hypothesis that they are equal, so the difference are not significant, even though all the error rates for MIl are smaller than WMIl.",
                "However, we can conclude that term weights contribute little in single-document segmentation.",
                "The results also show that MI using term co-clustering (k = 100) decreases the performance.",
                "We tested different number of term clusters, and found that the performance becomes better when the cluster number increases to reach l. WMIk<l has similar results that we did not show in the table.",
                "As mentioned before, using MI may be inconsistent on optimal boundaries given different numbers of segments.",
                "This situation occurs especially when the similarities among segments are quite different, i.e. some transitions are very obvious, while others are not.",
                "This is because usually a document is a hierarchical structure instead of only a sequential structure.",
                "When the segments are not at the same level, this situation may occur.",
                "Thus, a hierarchical topic segmentation approach is desired, and the structure highly depends on the number of segments for each internal node and the stop criteria of splitting.",
                "For this data set of singledocument segmentation, since it is just a synthetic set, which is just a concatenation of several segments about different topics, it is reasonable that approaches simply based on term frequency have a good performance.",
                "Usually for the tasks of segmenting coherent documents for sub-topics, the effectiveness decreases much. 5.2 Shared Topic Detection 5.2.1 Test Data and Evaluation The second data set contains 80 news articles from Google News.",
                "There are eight topics and each has 10 articles.",
                "We randomly split the set into subsets with different document numbers and each subset has all eight topics.",
                "We compare our approach MIl and WMIl with LDA [4].",
                "LDA treats a document in the data set as a bag of words, finds its distribution on topics, and its major topic.",
                "MIl and WMIl views each sentence as a bag of words and tag it with a topic label.",
                "Then for each pair of documents, LDA determines if they are on the same topic, while MIl and Table 3: Shared Topic Detection: Average Error Rates for Different Numbers of Documents in Each Subset #Doc 10 20 40 80 LDA 8.89% 16.33% 1.35% 0.60% MIl, θ = 0.6 4.17% 1.71% 1.47% 0.0% WMIl, θ = 0.8 18.6% 3.16% 1.92% 0.0% WMIl check whether the proportion overlapped sentences on the same topic is larger than the adjustable threshold θ.",
                "That is, in MIl and WMIl, for a pair of documents d, d , if [ s∈Sd,s ∈Sd 1(topics=topics )/min(|Sd|, |Sd|)] > θ, where Sd is the set of sentences of d, and |Sd| is the number of sentences of d, then d and d have the shared topic.",
                "For a pair of documents selected randomly, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, same)p(same|real) +p(false alarm|real, pred, diff)p(diff|real), where a miss means if they have the same topic (same) for the real case (real), but predicted (pred) as on the same topic.",
                "If they are on different topics (diff), but predicted as on the same topic, it is a false alarm. 5.2.2 Experiment Results The results are shown in Table 3.",
                "If most documents have different topics, in WMIl, the estimation of term weights in Equation (3) is not correct.",
                "Thus, WMIl is not expected to have a better performance than MIl, when most documents have different topics.",
                "When there are fewer documents in a subset with the same number of topics, more documents have different topics, so WMIl is more worse than MIl.",
                "We can see that for most cases MIl has a better (or at least similar) performance than LDA.",
                "After shared topic detection, multi-document segmentation of documents with the shared topics is able to be executed. 5.3 Multi-document Segmentation 5.3.1 Test Data and Evaluation For multi-document segmentation and alignment, our goal is to identify these segments about the same topic among multiple similar documents with shared topics.",
                "Using Iw is expected to perform better than I, since without term weights the result is affected seriously by document-dependent stop words and noisy words which depends on the personal writing style.",
                "It is more likely to treat the same segments of different documents as different segments under the effect of document-dependent stop words and noisy words.",
                "Term weights can reduce the effect of document-dependent stop words and noisy words by giving cue terms more weights.",
                "The data set for multi-document segmentation and alignment has 102 samples and 2264 sentences totally.",
                "Each is the introduction part of a lab report selected from the course of Biol 240W, Pennsylvania State University.",
                "Each sample has two segments, introduction of plant hormones and the content in the lab.",
                "The length range of samples is from two to 56 sentences.",
                "Some samples only have one part and some have a reverse order the these two segments.",
                "It is not hard to identify the boundary between two segments for human.",
                "We labelled each sentence manually for evaluation.",
                "The criterion of evaluation is just using the proportion of the number of sentences with wrong predicted segment labels in the total number of sentences in the whole training Table 4: Average Error Rates of Multi-document Segmentation Given Segment Numbers Known #Doc MIl WMIl k MIk WMIk 102 3.14% 2.78% 300 4.68% 6.58% 51 4.17% 3.63% 300 17.83% 22.84% 34 5.06% 4.12% 300 18.75% 20.95% 20 7.08% 5.42% 250 20.40% 21.83% 10 10.38% 7.89% 250 21.42% 21.91% 5 15.77% 11.64% 250 21.89% 22.59% 2 25.90% 23.18% 50 25.44% 25.49% 1 23.90% 24.82% 25 25.75% 26.15% Table 5: Multi-document Segmentation: P-values of T-test on Error Rates for MIl and WMIl #Doc 51 34 20 10 5 2 P-value 0.19 0.101 0.025 0.001 0.000 0.002 set as the error rate: p(error|predicted, real) = d∈D s∈Sd 1(predicteds=reals)/ d∈D nd.",
                "In order to show the benefits of multi-document segmentation and alignment, we compared our method with different parameters on different partitions of the same training set.",
                "Except the cases that the number of documents is 102 and one (they are special cases of using the whole set and the pure single-document segmentation), we randomly divided the training set into m partitions, and each has 51, 34, 20, 10, 5, and 2 document samples.",
                "Then we applied our methods on each partition and calculated the error rate of the whole training set.",
                "Each case was repeated for 10 times for computing the average error rates.",
                "For different partitions of the training set, different k values are used, since the number of terms increases when the document number in each partition increases. 5.3.2 Experiment Results From the experiment results in Table 4, we can see the following observations: (1) When the number of documents increases, all methods have better performances.",
                "Only from one to two documents, MIl has decrease a little.",
                "We can observe this from Figure 3 at the point of document number = 2.",
                "Most curves even have the worst results at this point.",
                "There are two reasons.",
                "First, because samples vote for the best multi-document segmentation and alignment, but if only two documents are compared with each other, the one with missing segments or a totally different sequence will affect the correct segmentation and alignment of the other.",
                "Second, as noted at the beginning of this section, if two documents have more document-dependent stop words or noisy words than cue words, then the algorithm may view them as two different segments and the other segment is missing.",
                "Generally, we can only expect a better performance when the number of documents is larger than the number of segments. (2) Except single-document segmentation, WMIl is always better than MIl, and when the number of documents is reaching one or increases to a very large number, their performances become closer.",
                "Table 5 shows p-values of twosample one-sided t-test between MIl and WMIl.",
                "We also can see this trend from p-values.",
                "When document number = 5, we reached the smallest p-value and the largest difference between error rates of MIl and WMIl.",
                "For single-document Table 6: Multi-document Segmentation: Average Error Rate for Document Number = 5 in Each Subset with Different Number of Term Clusters #Cluster 75 100 150 250 l MIk 24.67% 24.54% 23.91% 22.59% 15.77% segmentation, WMIl is even a little bit worse than MIl, which is similar as the results of the single-document segmentation on the first data set.",
                "The reason is that for singledocument segmentation, we cannot estimate term weights accurately, since multiple documents are unavailable. (3) Using term clustering usually gets worse results than MIl and WMIl. (4) Using term clustering in WMIk is even worse than in MIk, since in WMIk term clusters are found first using I before using Iw.",
                "If the term clusters are not correct, then the term weights are estimated worse, which may mislead the algorithm to reach even worse results.",
                "From the results we also found that in multi-document segmentation and alignment, most documents with missing segments and a reverse order are identified correctly.",
                "Table 6 illustrates the experiment results for the case of 20 partitions (each has five document samples) of the training set and topic segmentation and alignment using MIk with different numbers of term clusters k. Notice that when the number of term clusters increases, the error rate becomes smaller.",
                "Without term clustering, we have the best result.",
                "We did not show results for WMIk with term clustering, but the results are similar.",
                "We also tested WMIl with different hyper parameters of a and b to adjust term weights.",
                "The results are presented in Figure 3.",
                "It was shown that the default case WMIl : a = 1, b = 1 gave the best results for different partitions of the training set.",
                "We can see the trend that when the document number is very small or large, the difference between MIl : a = 0, b = 0 and WMIl : a = 1, b = 1 becomes quite small.",
                "When the document number is not large (about from 2 to 10), all the cases using term weights have better performances than MIl : a = 0, b = 0 without term weights, but when the document number becomes larger, the cases WMIl : a = 1, b = 0 and WMIl : a = 2, b = 1 become worse than MIl : a = 0, b = 0.",
                "When the document number becomes very large, they are even worse than cases with small document numbers.",
                "This means that a proper way to estimate term weights for the criterion of WMI is very important.",
                "Figure 4 shows the term weights learned from the whole training set.",
                "Four types of words are categorized roughly even though the transition among them are subtle.",
                "Figure 5 illustrates the change in (weighted) mutual information for MIl and WMIl.",
                "As expected, mutual information for MIl increases monotonically with the number of steps, while WMIl does not.",
                "Finally, MIl and WMIl are scalable, with computational complexity shown in Figure 6.",
                "One advantage for our approach based on MI is that removing stop words is not required.",
                "Another important advantage is that there are no necessary hyper parameters to adjust.",
                "In single-document segmentation, the performance based on MI is even better for that based on WMI, so no extra hyper parameter is required.",
                "In multi-document segmentation, we show in the experiment, a = 1 and b = 1 is the best.",
                "Our method gives more weights to cue terms.",
                "However, usually cue terms or sentences appear at the beginning of a segment, while the end of the segment may be 1 2 5 10 20 34 51 102 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 Document Number ErrorRate MIl :a=0,b=0 WMI l :a=1,b=1 WMI l :a=1,b=0 WMI l :a=2,b=1 Figure 3: Error rates for different hyper parameters of term weights. 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Normalized Document Entropy NormalizedSegmentEntropy Noisy words Cue words Common stop words Doc−dep stop words Figure 4: Term weights learned from the whole training set. 0 100 200 300 400 500 600 0.06 0.08 0.1 0.12 0.14 0.16 0.18 Number of Steps (Weighted)MutualInformation MI l WMI l Figure 5: Change in (weighted) MI for MIl and WMIl. 0 20 40 60 80 100 120 0 200 400 600 800 1000 1200 1400 1600 1800 2000 Document Number TimetoConverge(sec) MI l WMI l Figure 6: Time to converge for MIl and WMIl. much noisy.",
                "One possible solution is giving more weights to terms at the beginning of each segment.",
                "Moreover, when the length of segments are quite different, long segments have much higher term frequencies, so they may dominate the segmentation boundaries.",
                "Normalization of term frequencies versus the segment length may be useful. 6.",
                "CONCLUSIONS AND FUTURE WORK We proposed a novel method for multi-document topic segmentation and alignment based on weighted mutual information, which can also handle single-document cases.",
                "We used dynamic programming to optimize our algorithm.",
                "Our approach outperforms all the previous methods on singledocument cases.",
                "Moreover, we also showed that doing segmentation among multiple documents can improve the performance tremendously.",
                "Our results also illustrated that using weighted mutual information can utilize the information of multiple documents to reach a better performance.",
                "We only tested our method on limited data sets.",
                "More data sets especially complicated ones should be tested.",
                "More previous methods should be compared with.",
                "Moreover, natural segmentations like paragraphs are hints that can be used to find the optimal boundaries.",
                "Supervised learning also can be considered. 7.",
                "ACKNOWLEDGMENTS The authors want to thank Xiang Ji, and Prof. J. Scott Payne for their help. 8.",
                "REFERENCES [1] A. Banerjee, I. Ghillon, J. Ghosh, S. Merugu, and D. Modha.",
                "A generalized maximum entropy approach to bregman co-clustering and matrix approximation.",
                "In Proceedings of SIGKDD, 2004. [2] R. Bekkerman, R. El-Yaniv, and A. McCallum.",
                "Multi-way distributional clustering via pairwise interactions.",
                "In Proceedings of ICML, 2005. [3] D. M. Blei and P. J. Moreno.",
                "Topic segmentation with an aspect hidden markov model.",
                "In Proceedings of SIGIR, 2001. [4] D. M. Blei, A. Ng, and M. Jordan.",
                "Latent dirichlet allocation.",
                "Journal of Machine Learning Research, 3:993-1022, 2003. [5] T. Brants, F. Chen, and I. Tsochantaridis.",
                "Topic-based document segmentation with probabilistic latent semantic analysis.",
                "In Proceedings of CIKM, 2002. [6] F. Choi.",
                "Advances in domain indepedent linear text segmentation.",
                "In Proceedings of the NAACL, 2000. [7] H. Christensen, B. Kolluru, Y. Gotoh, and S. Renals.",
                "Maximum entropy segmentation of broadcast news.",
                "In Proceedings of ICASSP, 2005. [8] T. Cover and J. Thomas.",
                "Elements of Information Theory.",
                "John Wiley and Sons, New York, USA, 1991. [9] S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and R. Harshman.",
                "Indexing by latent semantic analysis.",
                "Journal of the American Society for Information Systems, 1990. [10] I. Dhillon, S. Mallela, and D. Modha.",
                "Information-theoretic co-clustering.",
                "In Proceedings of SIGKDD, 2003. [11] M. Hajime, H. Takeo, and O. Manabu.",
                "Text segmentation with multiple surface linguistic cues.",
                "In Proceedings of COLING-ACL, 1998. [12] T. K. Ho.",
                "Stop word location and identification for adaptive text recognition.",
                "International Journal of Document Analysis and Recognition, 3(1), August 2000. [13] T. Hofmann.",
                "Probabilistic latent semantic analysis.",
                "In Proceedings of the UAI99, 1999. [14] X. Ji and H. Zha.",
                "Correlating summarization of a pair of multilingual documents.",
                "In Proceedings of RIDE, 2003. [15] X. Ji and H. Zha.",
                "Domain-independent text segmentation using anisotropic diffusion and dynamic programming.",
                "In Proceedings of SIGIR, 2003. [16] X. Ji and H. Zha.",
                "Extracting shared topics of multiple documents.",
                "In Proceedings of the 7th PAKDD, 2003. [17] J. Lafferty, A. McCallum, and F. Pereira.",
                "Conditional random fields: Probabilistic models for segmenting and labeling sequence data.",
                "In Proceedings of ICML, 2001. [18] T. Li, S. Ma, and M. Ogihara.",
                "Entropy-based criterion in categorical clustering.",
                "In Proceedings of ICML, 2004. [19] A. McCallum, D. Freitag, and F. Pereira.",
                "Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of ICML, 2000. [20] L. Pevzner and M. Hearst.",
                "A critique and improvement of an evaluation metric for text segmentation.",
                "Computational Linguistic, 28(1):19-36, 2002. [21] J. C. Reynar.",
                "Statistical models for topic segmentation.",
                "In Proceedings of ACL, 1999. [22] G. Salton and M. McGill.",
                "Introduction to Modern Information Retrieval.",
                "McGraw Hill, 1983. [23] B.",
                "Sun, Q. Tan, P. Mitra, and C. L. Giles.",
                "Extraction and search of chemical formulae in text documents on the web.",
                "In Proceedings of WWW, 2007. [24] B.",
                "Sun, D. Zhou, H. Zha, and J.",
                "Yen.",
                "Multi-task text segmentation and alignment based on weighted mutual information.",
                "In Proceedings of CIKM, 2006. [25] M. Utiyama and H. Isahara.",
                "A statistical model for domain-independent text segmentation.",
                "In Proceedings of the 39th ACL, 1999. [26] C. Wayne.",
                "Multilingual topic detection and tracking: Successful research enabled by corpora and evaluation.",
                "In Proceedings of LREC, 2000. [27] J. Yamron, I. Carp, L. Gillick, S. Lowe, and P. van Mulbregt.",
                "A hidden markov model approach to text segmentation and event tracking.",
                "In Proceedings of ICASSP, 1998. [28] H. Zha and X. Ji.",
                "Correlating multilingual documents via bipartite graph modeling.",
                "In Proceedings of SIGIR, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "share topic detection": {
            "translated_key": "detección de temas",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Topic Segmentation with Shared Topic Detection and Alignment of Multiple Documents Bingjun Sun*, Prasenjit Mitra*† , Hongyuan Zha‡ , C. Lee Giles*† , John Yen*† *Department of Computer Science and Engineering † College of Information Sciences and Technology The Pennsylvania State University University Park, PA 16802 ‡ College of Computing The Georgia Institute of Technology Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu ABSTRACT Topic detection and tracking [26] and topic segmentation [15] play an important role in capturing the local and sequential information of documents.",
                "Previous work in this area usually focuses on single documents, although similar multiple documents are available in many domains.",
                "In this paper, we introduce a novel unsupervised method for shared topic detection and topic segmentation of multiple similar documents based on mutual information (MI) and weighted mutual information (WMI) that is a combination of MI and term weights.",
                "The basic idea is that the optimal segmentation maximizes MI(or WMI).",
                "Our approach can detect shared topics among documents.",
                "It can find the optimal boundaries in a document, and align segments among documents at the same time.",
                "It also can handle single-document segmentation as a special case of the multi-document segmentation and alignment.",
                "Our methods can identify and strengthen cue terms that can be used for segmentation and partially remove stop words by using term weights based on entropy learned from multiple documents.",
                "Our experimental results show that our algorithm works well for the tasks of single-document segmentation, shared topic detection, and multi-document segmentation.",
                "Utilizing information from multiple documents can tremendously improve the performance of topic segmentation, and using WMI is even better than using MI for the multi-document segmentation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Clustering; H.3.1 [Information Storage and Retrieval]: Content Analysis and IndexingLinguistic processing; I.2.7 [Artificial Intelligence]: Natural Language Processing-Text analysis; I.5.3 [Pattern Recognition]: Clustering-Algorithms;Similarity measures General Terms Algorithms, Design, Experimentation 1.",
                "INTRODUCTION Many researchers have worked on topic detection and tracking (TDT) [26] and topic segmentation during the past decade.",
                "Topic segmentation intends to identify the boundaries in a document with the goal to capture the latent topical structure.",
                "Topic segmentation tasks usually fall into two categories [15]: text stream segmentation where topic transition is identified, and coherent document segmentation in which documents are split into sub-topics.",
                "The former category has applications in automatic speech recognition, while the latter one has more applications such as partial-text query of long documents in information retrieval, text summary, and quality measurement of multiple documents.",
                "Previous research in connection with TDT falls into the former category, targeted on topic tracking of broadcast speech data and newswire text, while the latter category has not been studied very well.",
                "Traditional approaches perform topic segmentation on documents one at a time [15, 25, 6].",
                "Most of them perform badly in subtle tasks like coherent document segmentation [15].",
                "Often, end-users seek documents that have the similar content.",
                "Search engines, like, Google, provide links to obtain similar pages.",
                "At a finer granularity, users may actually be looking to obtain sections of a document similar to a particular section that presumably discusses a topic of the users interest.",
                "Thus, the extension of topic segmentation from single documents to identifying similar segments from multiple similar documents with the same topic is a natural and necessary direction, and multi-document topic segmentation is expected to have a better performance since more information is utilized.",
                "Traditional approaches using similarity measurement based on term frequency generally have the same assumption that similar vocabulary tends to be in a coherent topic segment [15, 25, 6].",
                "However, they usually suffer the issue of identifying stop words.",
                "For example, additional document-dependent stop words are removed together with the generic stop words in [15].",
                "There are two reasons that we do not remove stop words directly.",
                "First, identifying stop words is another issue [12] that requires estimation in each domain.",
                "Removing common stop words may result in the loss of useful information in a specific domain.",
                "Second, even though stop words can be identified, hard classification of stop words and nonstop words cannot represent the gradually changing amount of information content of each word.",
                "We employ a soft classification using term weights.",
                "In this paper, we view the problem of topic segmentation as an optimization issue using information theoretic techniques to find the optimal boundaries of a document given the number of text segments so as to minimize the loss of mutual information (MI) (or a weighted mutual information (WMI)) after segmentation and alignment.",
                "This is equal to maximizing the MI (or WMI).",
                "The MI focuses on measuring the difference among segments whereas previous research focused on finding the similarity (e.g. cosine distance) of segments [15, 25, 6].",
                "Topic alignment of multiple similar documents can be achieved by clustering sentences on the same topic into the same cluster.",
                "Single-document topic segmentation is just a special case of the multi-document topic segmentation and alignment problem.",
                "Terms can be co-clustered as in [10] at the same time, given the number of clusters, but our experimental results show that this method results in a worse segmentation (see Tables 1, 4, and 6).",
                "Usually, human readers can identify topic transition based on cue words, and can ignore stop words.",
                "Inspired by this, we give each term (or term cluster) a weight based on entropy among different documents and different segments of documents.",
                "Not only can this approach increase the contribution of cue words, but it can also decrease the effect of common stop words, noisy word, and document-dependent stop words.",
                "These words are common in a document.",
                "Many methods based on sentence similarity require that these words are removed before topic segmentation can be performed [15].",
                "Our results in Figure 3 show that term weights are useful for multi-document topic segmentation and alignment.",
                "The major contribution of this paper is that it introduces a novel method for topic segmentation using MI and shows that this method performs better than previously used criteria.",
                "Also, we have addressed the problem of topic segmentation and alignment across multiple documents, whereas most existing research focused on segmentation of single documents.",
                "Multi-document segmentation and alignment can utilize information from similar documents and improves the performance of topic segmentation greatly.",
                "Obviously, our approach can handle single documents as a special case when multiple documents are unavailable.",
                "It can detect shared topics among documents to judge if they are multiple documents on the same topic.",
                "We also introduce the new criterion of WMI based on term weights learned from multiple similar documents, which can improve performance of topic segmentation further.",
                "We propose an iterative greedy algorithm based on dynamic programming and show that it works well in practice.",
                "Some of our prior work is in [24].",
                "The rest of this paper is organized as follows: In Section 2, we review related work.",
                "Section 3 contains a formulation of the problem of topic segmentation and alignment of multiple documents with term co-clustering, a review of the criterion of MI for clustering, and finally an introduction to WMI.",
                "In Section 4, we first propose the iterative greedy algorithm of topic segmentation and alignment with term co-clustering, and then describe how the algorithm can be optimized by usFigure 1: Illustration of multi-document segmentation and alignment. ing dynamic programming.",
                "In Section 5, experiments about single-document segmentation, shared topic detection, and multi-document segmentation are described, and results are presented and discussed to evaluate the performance of our algorithm.",
                "Conclusions and some future directions of the research work are discussed in Section 6. 2.",
                "PREVIOUS WORK Generally, the existing approaches to text segmentation fall into two categories: supervised learning [19, 17, 23] and unsupervised learning [3, 27, 5, 6, 15, 25, 21].",
                "Supervised learning usually has good performance, since it learns functions from labelled training sets.",
                "However, often getting large training sets with manual labels on document sentences is prohibitively expensive, so unsupervised approaches are desired.",
                "Some models consider dependence between sentences and sections, such as Hidden Markov Model [3, 27], Maximum Entropy Markov Model [19], and Conditional Random Fields [17], while many other approaches are based on lexical cohesion or similarity of sentences [5, 6, 15, 25, 21].",
                "Some approaches also focus on cue words as hints of topic transitions [11].",
                "While some existing methods only consider information in single documents [6, 15], others utilize multiple documents [16, 14].",
                "There are not many works in the latter category, even though the performance of segmentation is expected to be better with utilization of information from multiple documents.",
                "Previous research studied methods to find shared topics [16] and topic segmentation and summarization between just a pair of documents [14].",
                "Text classification and clustering is a related research area which categorizes documents into groups using supervised or unsupervised methods.",
                "Topical classification or clustering is an important direction in this area, especially co-clustering of documents and terms, such as LSA [9], PLSA [13], and approaches based on distances and bipartite graph partitioning [28] or maximum MI [2, 10], or maximum entropy [1, 18].",
                "Criteria of these approaches can be utilized in the issue of topic segmentation.",
                "Some of those methods have been extended into the area of topic segmentation, such as PLSA [5] and maximum entropy [7], but to our best knowledge, using MI for topic segmentation has not been studied. 3.",
                "PROBLEM FORMULATION Our goal is to segment documents and align the segments across documents (Figure 1).",
                "Let T be the set of terms {t1, t2, ..., tl}, which appear in the unlabelled set of documents D = {d1, d2, ..., dm}.",
                "Let Sd be the set of sentences for document d ∈ D, i.e. {s1, s2, ..., snd }.",
                "We have a 3D matrix of term frequency, in which the three dimensions are random variables of D, Sd, and T. Sd actually is a random vector including a random variable for each d ∈ D. The term frequency can be used to estimate the joint probability distribution P(D, Sd, T), which is p(t, d, s) = T(t, d, s)/ND, where T(t, d, s) is the number of t in ds sentence s and ND is the total number of terms in D. ˆS represents the set of segments {ˆs1, ˆs2, ..., ˆsp} after segmentation and alignment among multiple documents, where the number of segments | ˆS| = p. A segment ˆsi of document d is a sequence of adjacent sentences in d. Since for different documents si may discuss different sub-topics, our goal is to cluster adjacent sentences in each document into segments, and align similar segments among documents, so that for different documents ˆsi is about the same sub-topic.",
                "The goal is to find the optimal topic segmentation and alignment mapping Segd(si) : {s1, s2, ..., snd } → {ˆs1, ˆs2, ..., ˆsp} and Alid(ˆsi) : {ˆs1, ˆs2, ..., ˆsp} → {ˆs1, ˆs2, ..., ˆsp}, for all d ∈ D, where ˆsi is ith segment with the constraint that only adjacent sentences can be mapped to the same segment, i.e. for d, {si, si+1, ..., sj} → {ˆsq}, where q ∈ {1, ..., p}, where p is the segment number, and if i > j, then for d, ˆsq is missing.",
                "After segmentation and alignment, random vector Sd becomes an aligned random variable ˆS.",
                "Thus, P(D, Sd, T) becomes P(D, ˆS, T).",
                "Term co-clustering is a technique that has been employed [10] to improve the accuracy of document clustering.",
                "We evaluate the effect of it for topic segmentation.",
                "A term t is mapped to exactly one term cluster.",
                "Term co-clustering involves simultaneously finding the optimal term clustering mapping Clu(t) : {t1, t2, ..., tl} → {ˆt1, ˆt2, ..., ˆtk}, where k ≤ l, l is the total number of words in all the documents, and k is the number of clusters. 4.",
                "METHODOLOGY We now describe a novel algorithm which can handle singledocument segmentation, shared topic detection, and multidocument segmentation and alignment based on MI or WMI. 4.1 Mutual Information MI I(X; Y ) is a quantity to measure the amount of information which is contained in two or more random variables [8, 10].",
                "For the case of two random variables, we have I(X; Y ) = x∈X y∈Y p(x, y)log p(x, y) p(x)p(y) , (1) Obviously, when random variables X and Y are independent, I(X; Y ) = 0.",
                "Thus, intuitively, the value of MI depends on how random variables are dependent on each other.",
                "The optimal co-clustering is the mapping Clux : X → ˆX and Cluy : Y → ˆY that minimizes the loss: I(X; Y ) − I( ˆX; ˆY ), which is equal to maximizing I( ˆX; ˆY ).",
                "This is the criterion of MI for clustering.",
                "In the case of topic segmentation, the two random variables are the term variable T and the segment variable S, and each sample is an occurrence of a term T = t in a particular segment S = s. I(T; S) is used to measure how dependent T and S are.",
                "However, I(T; S) cannot be computed for documents before segmentation, since we do not have a set of S due to the fact that sentences of Document d, si ∈ Sd, is not aligned with other documents.",
                "Thus, instead of minimizing the loss of MI, we can maximize MI after topic segmentation, computed as: I( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS p(ˆt, ˆs)log p(ˆt, ˆs) p(ˆt)p(ˆs) , (2) where p(ˆt, ˆs) are estimated by the term frequency tf of Term Cluster ˆt and Segment ˆs in the training set D. Note that here a segment ˆs includes sentences about the the same topic among all documents.",
                "The optimal solution is the mapping Clut : T → ˆT, Segd : Sd → ˆS , and Alid : ˆS → ˆS, which maximizes I( ˆT; ˆS). 4.2 Weighted Mutual Information In topic segmentation and alignment of multiple documents, if P(D, ˆS, T) is known, based on the marginal distributions P(D|T) and P( ˆS|T) for each term t ∈ T, we can categorize terms into four types in the data set: • Common stop words are common both along the dimensions of documents and segments. • Document-dependent stop words that depends on the personal writing style are common only along the dimension of segments for some documents. • Cue words are the most important elements for segmentation.",
                "They are common along the dimension of documents only for the same segment, and they are not common along the dimensions of segments. • Noisy words are other words which are not common along both dimensions.",
                "Entropy based on P(D|T) and P( ˆS|T) can be used to identify different types of terms.",
                "To reinforce the contribution of cue words in the MI computation, and simultaneously reduce the effect of the other three types of words, similar as the idea of the tf-idf weight [22], we use entropies of each term along the dimensions of document D and segment ˆS, i.e.",
                "ED(ˆt) and EˆS(ˆt), to compute the weight.",
                "A cue word usually has a large value of ED(ˆt) but a small value of EˆS(ˆt).",
                "We introduce term weights (or term cluster weights) wˆt = ( ED(ˆt) maxˆt ∈ ˆT (ED(ˆt )) )a (1 − EˆS(ˆt) maxˆt ∈ ˆT (EˆS(ˆt )) )b , (3) where ED(ˆt) = d∈D p(d|ˆt)log|D| 1 p(d|ˆt) , EˆS(ˆt) = ˆs∈ ˆS p(ˆs|ˆt)log| ˆS| 1 p(ˆs|ˆt) , and a > 0 and b > 0 are powers to adjust term weights.",
                "Usually a = 1 and b = 1 as default, and maxˆt ∈ ˆT (ED(ˆt )) and maxˆt ∈ ˆT (EˆS(ˆt )) are used to normalize the entropy values.",
                "Term cluster weights are used to adjust p(ˆt, ˆs), pw(ˆt, ˆs) = wˆtp(ˆt, ˆs) ˆt∈ ˆT ;ˆs∈ ˆS wˆtp(ˆt, ˆs) , (4) and Iw( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS pw(ˆt, ˆs)log pw(ˆt, ˆs) pw(ˆt)pw(ˆs) , (5) where pw(ˆt) and pw(ˆs) are marginal distributions of pw(ˆt, ˆs).",
                "However, since we do not know either the term weights or P(D, ˆS, T), we need to estimate them, but wˆt depends on p(ˆs|t) and ˆS, while ˆS and p(ˆs|t) also depend on wˆt that is still unknown.",
                "Thus, an iterative algorithm is required to estimate term weights wˆt and find the best segmentation and alignment to optimize the objective function Iw concurrently.",
                "After a document is segmented into sentences Input: Joint probability distribution P(D, Sd, T), number of text segments p ∈ {2, 3, ..., max(sd)}, number of term clusters k ∈ {2, 3, ..., l} (if k = l, no term co-clustering required), and weight type w ∈ {0, 1}, indicating to use I or Iw, respectively.",
                "Output: Mapping Clu, Seg, Ali, and term weights wˆt.",
                "Initialization: 0. i = 0.",
                "Initialize Clu (0) t , Seg (0) d , and Ali (0) d ; Initialize w (0) ˆt using Equation (6) if w = 1; Stage 1: 1.",
                "If |D| = 1, k = l, and w = 0, check all sequential segmentations of d into p segments and find the best one Segd(s) = argmaxˆsI( ˆT; ˆS), and return Segd; otherwise, if w = 1 and k = l, go to 3.1; Stage 2: 2.1 If k < l, for each term t, find the best cluster ˆt as Clu(i+1)(t) = argmaxˆtI( ˆT; ˆS(i)) based on Seg(i) and Ali(i); 2.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+1) d (Seg (i+1) d (s)) = argmaxˆsI( ˆT(i+1); ˆS) based on Clu(i+1)(t) if k < l or Clu(0)(t) if k = l; 2.3 i + +.",
                "If Clu, Seg, or Ali changed, go to 2.1; otherwise, if w = 0, return Clu(i), Seg(i), and Ali(i); else j = 0, go to 3.1; Stage 3: 3.1 Update w (i+j+1) ˆt based on Seg(i+j), Ali(i+j), and Clu(i) using Equation (3); 3.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+j+1) d (Seg (i+j+1) d (s)) = argmaxˆsIw( ˆT(i); ˆS) based on Clu(i) and w (i+j+1) ˆt ; 3.3 j + +.",
                "If Iw( ˆT; ˆS) changes, go to step 6; otherwise, stop and return Clu(i), Seg(i+j), Ali(i+j), and w (i+j) ˆt ; Figure 2: Algorithm: Topic segmentation and alignment based on MI or WMI. and each sentence is segmented into words, each word is stemmed.",
                "Then the joint probability distribution P(D, Sd, T) can be estimated.",
                "Finally, this distribution can be used to compute MI in our algorithm. 4.3 Iterative Greedy Algorithm Our goal is to maximize the objective function, I( ˆT; ˆS) or Iw( ˆT; ˆS), which can measure the dependence of term occurrences in different segments.",
                "Generally, first we do not know the estimate term weights, which depend on the optimal topic segmentation and alignment, and term clusters.",
                "Moreover, this problem is NP-hard [10], even though if we know the term weights.",
                "Thus, an iterative greedy algorithm is desired to find the best solution, even though probably only local maxima are reached.",
                "We present the iterative greedy algorithm in Figure 2 to find a local maximum of I( ˆT; ˆS) or Iw( ˆT; ˆS) with simultaneous term weight estimation.",
                "This algorithm can is iterative and greedy for multi-document cases or single-document cases with term weight estimation and/or term co-clustering.",
                "Otherwise, since it is just a one step algorithm to solve the task of single-document segmentation [6, 15, 25], the global maximum of MI is guaranteed.",
                "We will show later that term co-clustering reduces the accuracy of the results and is not necessary, and for singledocument segmentation, term weights are also not required. 4.3.1 Initialization In Step 0, the initial term clustering Clut and topic segmentation and alignment Segd and Alid are important to avoid local maxima and reduce the number of iterations.",
                "First, a good guess of term weights can be made by using the distributions of term frequency along sentences for each document and averaging them to get the initial values of wˆt: wt = ( ED(t) maxt ∈T (ED(t )) )(1 − ES(t) maxt ∈T (ES(t )) ), (6) where ES(t) = 1 |Dt| d∈Dt (1 − s∈Sd p(s|t)log|Sd| 1 p(s|t) ), where Dt is the set of documents which contain Term t. Then, for the initial segmentation Seg(0) , we can simply segment documents equally by sentences.",
                "Or we can find the optimal segmentation just for each document d which maximizes the WMI, Seg (0) d = argmaxˆsIw(T; ˆS), where w = w (0) ˆt .",
                "For the initial alignment Ali(0) , we can first assume that the order of segments for each d is the same.",
                "For the initial term clustering Clu(0) , first cluster labels can be set randomly, and after the first time of Step 3, a good initial term clustering is obtained. 4.3.2 Different Cases After initialization, there are three stages for different cases.",
                "Totally there are eight cases, |D| = 1 or |D| > 1, k = l or k < l, w = 0 or w = 1.",
                "Single document segmentation without term clustering and term weight estimation (|D| = 1, k = l, w = 0) only requires Stage 1 (Step 1).",
                "If term clustering is required (k < l), Stage 2 (Step 2.1, 2.2, and 2.3) is executed iteratively.",
                "If term weight estimation is required (w = 1), Stage 3 (Step 3.1, 3.2, and 3.3) is executed iteratively.",
                "If both are required (k < l, w = 1), Stage 2 and 3 run one after the other.",
                "For multi-document segmentation without term clustering and term weight estimation (|D| > 1, k = l, w = 0), only iteration of Step 2.2 and 2.3 are required.",
                "At Stage 1, the global maximum can be found based on I( ˆT; ˆS) using dynamic programming in Section 4.4.",
                "Simultaneously finding a good term clustering and estimated term weights is impossible, since when moving a term to a new term cluster to maximize Iw( ˆT; ˆS), we do not know that the weight of this term should be the one of the new cluster or the old cluster.",
                "Thus, we first do term clustering at Stage 2, and then estimate term weights at Stage 3.",
                "At Stage 2, Step 2.1 is to find the best term clustering and Step 2.2 is to find the best segmentation.",
                "This cycle is repeated to find a local maximum based on MI I until it converges.",
                "The two steps are: (1) based on current term clustering Cluˆt, for each document d, the algorithm segments all the sentences Sd into p segments sequentially (some segments may be empty), and put them into the p segments ˆS of the whole training set D (all possible cases of different segmentation Segd and alignment Alid are checked) to find the optimal case, and (2) based on the current segmentation and alignment, for each term t, the algorithm finds the best term cluster of t based on the current segmentation Segd and alignment Alid.",
                "After finding a good term clustering, term weights are estimated if w = 1.",
                "At Stage 3, similar as Stage 2, Step 3.1 is term weight re-estimation and Step 3.2 is to find a better segmentation.",
                "They are repeated to find a local maximum based on WMI Iw until it converges.",
                "However, if the term clustering in Stage 2 is not accurate, then the term weight estimation at Stage 3 may have a bad result.",
                "Finally, at Step 3.3, this algorithm converges and return the output.",
                "This algorithm can handle both single-document and multi-document segmentation.",
                "It also can detect shared topics among documents by checking the proportion of overlapped sentences on the same topics, as described in Sec 5.2. 4.4 Algorithm Optimization In many previous works on segmentation, dynamic programming is a technique used to maximize the objective function.",
                "Similarly, at Step 1, 2.2, and 3.2 of our algorithm, we can use dynamic programming.",
                "For Stage 1, using dynamic programming can still find the global optimum, but for Stage 2 and Stage 3, we can only find the optimum for each step of topic segmentation and alignment of a document.",
                "Here we only show the dynamic programming for Step 3.2 using WMI (Step 1 and 2.2 are similar but they can use either I or Iw).",
                "There are two cases that are not shown in the algorithm in Figure 2: (a) single-document segmentation or multi-document segmentation with the same sequential order of segments, where alignment is not required, and (b) multi-document segmentation with different sequential orders of segments, where alignment is necessary.",
                "The alignment mapping function of the former case is simply just Alid(ˆsi) = ˆsi, while for the latter ones alignment mapping function Alid(ˆsi) = ˆsj, i and j may be different.",
                "The computational steps for the two cases are listed below: Case 1 (no alignment): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs) and partial pw(ˆs) without counting sentences from d. Then put sentences from i to j into Part k, and compute partial WMI PIw( ˆT; ˆsk(si, si+1, ..., sj)) ˆt∈ ˆT pw(ˆt, ˆsk)log pw(ˆt, ˆsk) pw(ˆt)pw(ˆsk) , where Alid(si, si+1, ..., sj) = k, k ∈ {1, 2, ..., p}, 1 ≤ i ≤ j ≤ nd, and Segd(sq) = ˆsk for all i ≤ q ≤ j. (2) Let M(sm, 1) = PIw( ˆT; ˆs1(s1, s2, ..., sm)).",
                "Then M(sm, L) = maxi[M(si−1, L − 1) + PIw( ˆT; ˆsL(si, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, and when i > m, no sentences are put into ˆsk when compute PIw (note PIw( ˆT; ˆs(si, ..., sm)) = 0 for single-document segmentation). (3) Finally M(snd , p) = maxi[M(si−1, p − 1)+ PIw( ˆT; ˆsp(si, ..., snd ))], where 1 ≤ i ≤ nd+1.",
                "The optimal Iw is found and the corresponding segmentation is the best.",
                "Case 2 (alignment required): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs), and partial pw(ˆs), and PIw( ˆT; ˆsk(si, si+1, ..., sj)) similarly as Case 1. (2) Let M(sm, 1, k) = PIw( ˆT; ˆsk(s1, s2, ..., sm)), where k ∈ {1, 2, ..., p}.",
                "Then M(sm, L, kL) = maxi,j[M(si−1, L − 1, kL/j) + PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, kL ∈ Set(p, L), which is the set of all p!",
                "L!(p−L)! combinations of L segments chosen from all p segments, j ∈ kL, the set of L segments chosen from all p segments, and kL/j is the combination of L − 1 segments in kL except Segment j. (3) Finally, M(snd , p, kp) = maxi,j[M(si−1, p − 1, kp/j) +PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., snd ))], where kp is just the combination of all p segments and 1 ≤ i ≤ nd + 1, which is the optimal Iw and the corresponding segmentation is the best.",
                "The steps of Case 1 and 2 are similar, except in Case 2, alignment is considered in addition to segmentation.",
                "First, basic items of probability for computing Iw are computed excluding Doc d, and then partial WMI by putting every possible sequential segment (including empty segment) of d into every segment of the set.",
                "Second, the optimal sum of PIw for L segments and the leftmost m sentences, M(sm, L), is found.",
                "Finally, the maximal WMI is found among different sums of M(sm, p − 1) and PIw for Segment p. 5.",
                "EXPERIMENTS In this section, single-document segmentation, shared topic detection, and multi-document segmentation will be tested.",
                "Different hyper parameters of our method are studied.",
                "For convenience, we refer to the method using I as MIk if w = 0, and Iw as WMIk if w = 2 or as WMIk if w = 1, where k is the number of term clusters, and if k = l, where l is the total number of terms, then no term clustering is required, i.e.",
                "MIl and WMIl. 5.1 Single-document Segmentation 5.1.1 Test Data and Evaluation The first data set we tested is a synthetic one used in previous research [6, 15, 25] and many other papers.",
                "It has 700 samples.",
                "Each is a concatenation of ten segments.",
                "Each segment is the first n sentence selected randomly from the Brown corpus, which is supposed to have a different topic from each other.",
                "Currently, the best results on this data set is achieved by Ji et.al. [15].",
                "To compare the performance of our methods, the criterion used widely in previous research is applied, instead of the unbiased criterion introduced in [20].",
                "It chooses a pair of words randomly.",
                "If they are in different segments (different) for the real segmentation (real), but predicted (pred) as in the same segment, it is a miss.",
                "If they are in the same segment (same), but predicted as in different segments, it is a false alarm.",
                "Thus, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, diff)p(diff|real) +p(false alarm|real, pred, same)p(same|real). 5.1.2 Experiment Results We tested the case when the number of segments is known.",
                "Table 1 shows the results of our methods with different hyper parameter values and three previous approaches, C99[25], U00[6], and ADDP03[15], on this data set when the segment number is known.",
                "In WMI for single-document segmentation, the term weights are computed as follows: wˆt = 1−EˆS(ˆt)/maxˆt ∈ ˆT (EˆS(ˆt )).",
                "For this case, our methods MIl and WMIl both outperform all the previous approaches.",
                "We compared our methods with ADDP03using one-sample one-sided t-test and p-values are shown in Table 2.",
                "From the p-values, we can see that mostly the differences are very Table 1: Average Error Rates of Single-document Segmentation Given Segment Numbers Known Range of n 3-11 3-5 6-8 9-11 Sample size 400 100 100 100 C99 12% 11% 10% 9% U00 10% 9% 7% 5% ADDP03 6.0% 6.8% 5.2% 4.3% MIl 4.68% 5.57% 2.59% 1.59% WMIl 4.94% 6.33% 2.76% 1.62% MI100 9.62% 12.92% 8.66% 6.67% Table 2: Single-document Segmentation: P-values of T-test on Error Rates Range of n 3-11 3-5 6-8 9-11 ADDP03, MIl 0.000 0.000 0.000 0.000 ADDP03, WMIl 0.000 0.099 0.000 0.000 MIl, WMIl 0.061 0.132 0.526 0.898 significant.",
                "We also compare the error rates between our two methods using two-sample two-sided t-test to check the hypothesis that they are equal.",
                "We cannot reject the hypothesis that they are equal, so the difference are not significant, even though all the error rates for MIl are smaller than WMIl.",
                "However, we can conclude that term weights contribute little in single-document segmentation.",
                "The results also show that MI using term co-clustering (k = 100) decreases the performance.",
                "We tested different number of term clusters, and found that the performance becomes better when the cluster number increases to reach l. WMIk<l has similar results that we did not show in the table.",
                "As mentioned before, using MI may be inconsistent on optimal boundaries given different numbers of segments.",
                "This situation occurs especially when the similarities among segments are quite different, i.e. some transitions are very obvious, while others are not.",
                "This is because usually a document is a hierarchical structure instead of only a sequential structure.",
                "When the segments are not at the same level, this situation may occur.",
                "Thus, a hierarchical topic segmentation approach is desired, and the structure highly depends on the number of segments for each internal node and the stop criteria of splitting.",
                "For this data set of singledocument segmentation, since it is just a synthetic set, which is just a concatenation of several segments about different topics, it is reasonable that approaches simply based on term frequency have a good performance.",
                "Usually for the tasks of segmenting coherent documents for sub-topics, the effectiveness decreases much. 5.2 Shared Topic Detection 5.2.1 Test Data and Evaluation The second data set contains 80 news articles from Google News.",
                "There are eight topics and each has 10 articles.",
                "We randomly split the set into subsets with different document numbers and each subset has all eight topics.",
                "We compare our approach MIl and WMIl with LDA [4].",
                "LDA treats a document in the data set as a bag of words, finds its distribution on topics, and its major topic.",
                "MIl and WMIl views each sentence as a bag of words and tag it with a topic label.",
                "Then for each pair of documents, LDA determines if they are on the same topic, while MIl and Table 3: Shared Topic Detection: Average Error Rates for Different Numbers of Documents in Each Subset #Doc 10 20 40 80 LDA 8.89% 16.33% 1.35% 0.60% MIl, θ = 0.6 4.17% 1.71% 1.47% 0.0% WMIl, θ = 0.8 18.6% 3.16% 1.92% 0.0% WMIl check whether the proportion overlapped sentences on the same topic is larger than the adjustable threshold θ.",
                "That is, in MIl and WMIl, for a pair of documents d, d , if [ s∈Sd,s ∈Sd 1(topics=topics )/min(|Sd|, |Sd|)] > θ, where Sd is the set of sentences of d, and |Sd| is the number of sentences of d, then d and d have the shared topic.",
                "For a pair of documents selected randomly, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, same)p(same|real) +p(false alarm|real, pred, diff)p(diff|real), where a miss means if they have the same topic (same) for the real case (real), but predicted (pred) as on the same topic.",
                "If they are on different topics (diff), but predicted as on the same topic, it is a false alarm. 5.2.2 Experiment Results The results are shown in Table 3.",
                "If most documents have different topics, in WMIl, the estimation of term weights in Equation (3) is not correct.",
                "Thus, WMIl is not expected to have a better performance than MIl, when most documents have different topics.",
                "When there are fewer documents in a subset with the same number of topics, more documents have different topics, so WMIl is more worse than MIl.",
                "We can see that for most cases MIl has a better (or at least similar) performance than LDA.",
                "After shared topic detection, multi-document segmentation of documents with the shared topics is able to be executed. 5.3 Multi-document Segmentation 5.3.1 Test Data and Evaluation For multi-document segmentation and alignment, our goal is to identify these segments about the same topic among multiple similar documents with shared topics.",
                "Using Iw is expected to perform better than I, since without term weights the result is affected seriously by document-dependent stop words and noisy words which depends on the personal writing style.",
                "It is more likely to treat the same segments of different documents as different segments under the effect of document-dependent stop words and noisy words.",
                "Term weights can reduce the effect of document-dependent stop words and noisy words by giving cue terms more weights.",
                "The data set for multi-document segmentation and alignment has 102 samples and 2264 sentences totally.",
                "Each is the introduction part of a lab report selected from the course of Biol 240W, Pennsylvania State University.",
                "Each sample has two segments, introduction of plant hormones and the content in the lab.",
                "The length range of samples is from two to 56 sentences.",
                "Some samples only have one part and some have a reverse order the these two segments.",
                "It is not hard to identify the boundary between two segments for human.",
                "We labelled each sentence manually for evaluation.",
                "The criterion of evaluation is just using the proportion of the number of sentences with wrong predicted segment labels in the total number of sentences in the whole training Table 4: Average Error Rates of Multi-document Segmentation Given Segment Numbers Known #Doc MIl WMIl k MIk WMIk 102 3.14% 2.78% 300 4.68% 6.58% 51 4.17% 3.63% 300 17.83% 22.84% 34 5.06% 4.12% 300 18.75% 20.95% 20 7.08% 5.42% 250 20.40% 21.83% 10 10.38% 7.89% 250 21.42% 21.91% 5 15.77% 11.64% 250 21.89% 22.59% 2 25.90% 23.18% 50 25.44% 25.49% 1 23.90% 24.82% 25 25.75% 26.15% Table 5: Multi-document Segmentation: P-values of T-test on Error Rates for MIl and WMIl #Doc 51 34 20 10 5 2 P-value 0.19 0.101 0.025 0.001 0.000 0.002 set as the error rate: p(error|predicted, real) = d∈D s∈Sd 1(predicteds=reals)/ d∈D nd.",
                "In order to show the benefits of multi-document segmentation and alignment, we compared our method with different parameters on different partitions of the same training set.",
                "Except the cases that the number of documents is 102 and one (they are special cases of using the whole set and the pure single-document segmentation), we randomly divided the training set into m partitions, and each has 51, 34, 20, 10, 5, and 2 document samples.",
                "Then we applied our methods on each partition and calculated the error rate of the whole training set.",
                "Each case was repeated for 10 times for computing the average error rates.",
                "For different partitions of the training set, different k values are used, since the number of terms increases when the document number in each partition increases. 5.3.2 Experiment Results From the experiment results in Table 4, we can see the following observations: (1) When the number of documents increases, all methods have better performances.",
                "Only from one to two documents, MIl has decrease a little.",
                "We can observe this from Figure 3 at the point of document number = 2.",
                "Most curves even have the worst results at this point.",
                "There are two reasons.",
                "First, because samples vote for the best multi-document segmentation and alignment, but if only two documents are compared with each other, the one with missing segments or a totally different sequence will affect the correct segmentation and alignment of the other.",
                "Second, as noted at the beginning of this section, if two documents have more document-dependent stop words or noisy words than cue words, then the algorithm may view them as two different segments and the other segment is missing.",
                "Generally, we can only expect a better performance when the number of documents is larger than the number of segments. (2) Except single-document segmentation, WMIl is always better than MIl, and when the number of documents is reaching one or increases to a very large number, their performances become closer.",
                "Table 5 shows p-values of twosample one-sided t-test between MIl and WMIl.",
                "We also can see this trend from p-values.",
                "When document number = 5, we reached the smallest p-value and the largest difference between error rates of MIl and WMIl.",
                "For single-document Table 6: Multi-document Segmentation: Average Error Rate for Document Number = 5 in Each Subset with Different Number of Term Clusters #Cluster 75 100 150 250 l MIk 24.67% 24.54% 23.91% 22.59% 15.77% segmentation, WMIl is even a little bit worse than MIl, which is similar as the results of the single-document segmentation on the first data set.",
                "The reason is that for singledocument segmentation, we cannot estimate term weights accurately, since multiple documents are unavailable. (3) Using term clustering usually gets worse results than MIl and WMIl. (4) Using term clustering in WMIk is even worse than in MIk, since in WMIk term clusters are found first using I before using Iw.",
                "If the term clusters are not correct, then the term weights are estimated worse, which may mislead the algorithm to reach even worse results.",
                "From the results we also found that in multi-document segmentation and alignment, most documents with missing segments and a reverse order are identified correctly.",
                "Table 6 illustrates the experiment results for the case of 20 partitions (each has five document samples) of the training set and topic segmentation and alignment using MIk with different numbers of term clusters k. Notice that when the number of term clusters increases, the error rate becomes smaller.",
                "Without term clustering, we have the best result.",
                "We did not show results for WMIk with term clustering, but the results are similar.",
                "We also tested WMIl with different hyper parameters of a and b to adjust term weights.",
                "The results are presented in Figure 3.",
                "It was shown that the default case WMIl : a = 1, b = 1 gave the best results for different partitions of the training set.",
                "We can see the trend that when the document number is very small or large, the difference between MIl : a = 0, b = 0 and WMIl : a = 1, b = 1 becomes quite small.",
                "When the document number is not large (about from 2 to 10), all the cases using term weights have better performances than MIl : a = 0, b = 0 without term weights, but when the document number becomes larger, the cases WMIl : a = 1, b = 0 and WMIl : a = 2, b = 1 become worse than MIl : a = 0, b = 0.",
                "When the document number becomes very large, they are even worse than cases with small document numbers.",
                "This means that a proper way to estimate term weights for the criterion of WMI is very important.",
                "Figure 4 shows the term weights learned from the whole training set.",
                "Four types of words are categorized roughly even though the transition among them are subtle.",
                "Figure 5 illustrates the change in (weighted) mutual information for MIl and WMIl.",
                "As expected, mutual information for MIl increases monotonically with the number of steps, while WMIl does not.",
                "Finally, MIl and WMIl are scalable, with computational complexity shown in Figure 6.",
                "One advantage for our approach based on MI is that removing stop words is not required.",
                "Another important advantage is that there are no necessary hyper parameters to adjust.",
                "In single-document segmentation, the performance based on MI is even better for that based on WMI, so no extra hyper parameter is required.",
                "In multi-document segmentation, we show in the experiment, a = 1 and b = 1 is the best.",
                "Our method gives more weights to cue terms.",
                "However, usually cue terms or sentences appear at the beginning of a segment, while the end of the segment may be 1 2 5 10 20 34 51 102 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 Document Number ErrorRate MIl :a=0,b=0 WMI l :a=1,b=1 WMI l :a=1,b=0 WMI l :a=2,b=1 Figure 3: Error rates for different hyper parameters of term weights. 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Normalized Document Entropy NormalizedSegmentEntropy Noisy words Cue words Common stop words Doc−dep stop words Figure 4: Term weights learned from the whole training set. 0 100 200 300 400 500 600 0.06 0.08 0.1 0.12 0.14 0.16 0.18 Number of Steps (Weighted)MutualInformation MI l WMI l Figure 5: Change in (weighted) MI for MIl and WMIl. 0 20 40 60 80 100 120 0 200 400 600 800 1000 1200 1400 1600 1800 2000 Document Number TimetoConverge(sec) MI l WMI l Figure 6: Time to converge for MIl and WMIl. much noisy.",
                "One possible solution is giving more weights to terms at the beginning of each segment.",
                "Moreover, when the length of segments are quite different, long segments have much higher term frequencies, so they may dominate the segmentation boundaries.",
                "Normalization of term frequencies versus the segment length may be useful. 6.",
                "CONCLUSIONS AND FUTURE WORK We proposed a novel method for multi-document topic segmentation and alignment based on weighted mutual information, which can also handle single-document cases.",
                "We used dynamic programming to optimize our algorithm.",
                "Our approach outperforms all the previous methods on singledocument cases.",
                "Moreover, we also showed that doing segmentation among multiple documents can improve the performance tremendously.",
                "Our results also illustrated that using weighted mutual information can utilize the information of multiple documents to reach a better performance.",
                "We only tested our method on limited data sets.",
                "More data sets especially complicated ones should be tested.",
                "More previous methods should be compared with.",
                "Moreover, natural segmentations like paragraphs are hints that can be used to find the optimal boundaries.",
                "Supervised learning also can be considered. 7.",
                "ACKNOWLEDGMENTS The authors want to thank Xiang Ji, and Prof. J. Scott Payne for their help. 8.",
                "REFERENCES [1] A. Banerjee, I. Ghillon, J. Ghosh, S. Merugu, and D. Modha.",
                "A generalized maximum entropy approach to bregman co-clustering and matrix approximation.",
                "In Proceedings of SIGKDD, 2004. [2] R. Bekkerman, R. El-Yaniv, and A. McCallum.",
                "Multi-way distributional clustering via pairwise interactions.",
                "In Proceedings of ICML, 2005. [3] D. M. Blei and P. J. Moreno.",
                "Topic segmentation with an aspect hidden markov model.",
                "In Proceedings of SIGIR, 2001. [4] D. M. Blei, A. Ng, and M. Jordan.",
                "Latent dirichlet allocation.",
                "Journal of Machine Learning Research, 3:993-1022, 2003. [5] T. Brants, F. Chen, and I. Tsochantaridis.",
                "Topic-based document segmentation with probabilistic latent semantic analysis.",
                "In Proceedings of CIKM, 2002. [6] F. Choi.",
                "Advances in domain indepedent linear text segmentation.",
                "In Proceedings of the NAACL, 2000. [7] H. Christensen, B. Kolluru, Y. Gotoh, and S. Renals.",
                "Maximum entropy segmentation of broadcast news.",
                "In Proceedings of ICASSP, 2005. [8] T. Cover and J. Thomas.",
                "Elements of Information Theory.",
                "John Wiley and Sons, New York, USA, 1991. [9] S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and R. Harshman.",
                "Indexing by latent semantic analysis.",
                "Journal of the American Society for Information Systems, 1990. [10] I. Dhillon, S. Mallela, and D. Modha.",
                "Information-theoretic co-clustering.",
                "In Proceedings of SIGKDD, 2003. [11] M. Hajime, H. Takeo, and O. Manabu.",
                "Text segmentation with multiple surface linguistic cues.",
                "In Proceedings of COLING-ACL, 1998. [12] T. K. Ho.",
                "Stop word location and identification for adaptive text recognition.",
                "International Journal of Document Analysis and Recognition, 3(1), August 2000. [13] T. Hofmann.",
                "Probabilistic latent semantic analysis.",
                "In Proceedings of the UAI99, 1999. [14] X. Ji and H. Zha.",
                "Correlating summarization of a pair of multilingual documents.",
                "In Proceedings of RIDE, 2003. [15] X. Ji and H. Zha.",
                "Domain-independent text segmentation using anisotropic diffusion and dynamic programming.",
                "In Proceedings of SIGIR, 2003. [16] X. Ji and H. Zha.",
                "Extracting shared topics of multiple documents.",
                "In Proceedings of the 7th PAKDD, 2003. [17] J. Lafferty, A. McCallum, and F. Pereira.",
                "Conditional random fields: Probabilistic models for segmenting and labeling sequence data.",
                "In Proceedings of ICML, 2001. [18] T. Li, S. Ma, and M. Ogihara.",
                "Entropy-based criterion in categorical clustering.",
                "In Proceedings of ICML, 2004. [19] A. McCallum, D. Freitag, and F. Pereira.",
                "Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of ICML, 2000. [20] L. Pevzner and M. Hearst.",
                "A critique and improvement of an evaluation metric for text segmentation.",
                "Computational Linguistic, 28(1):19-36, 2002. [21] J. C. Reynar.",
                "Statistical models for topic segmentation.",
                "In Proceedings of ACL, 1999. [22] G. Salton and M. McGill.",
                "Introduction to Modern Information Retrieval.",
                "McGraw Hill, 1983. [23] B.",
                "Sun, Q. Tan, P. Mitra, and C. L. Giles.",
                "Extraction and search of chemical formulae in text documents on the web.",
                "In Proceedings of WWW, 2007. [24] B.",
                "Sun, D. Zhou, H. Zha, and J.",
                "Yen.",
                "Multi-task text segmentation and alignment based on weighted mutual information.",
                "In Proceedings of CIKM, 2006. [25] M. Utiyama and H. Isahara.",
                "A statistical model for domain-independent text segmentation.",
                "In Proceedings of the 39th ACL, 1999. [26] C. Wayne.",
                "Multilingual topic detection and tracking: Successful research enabled by corpora and evaluation.",
                "In Proceedings of LREC, 2000. [27] J. Yamron, I. Carp, L. Gillick, S. Lowe, and P. van Mulbregt.",
                "A hidden markov model approach to text segmentation and event tracking.",
                "In Proceedings of ICASSP, 1998. [28] H. Zha and X. Ji.",
                "Correlating multilingual documents via bipartite graph modeling.",
                "In Proceedings of SIGIR, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "topic alignment": {
            "translated_key": "alineación de temas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Topic Segmentation with Shared Topic Detection and Alignment of Multiple Documents Bingjun Sun*, Prasenjit Mitra*† , Hongyuan Zha‡ , C. Lee Giles*† , John Yen*† *Department of Computer Science and Engineering † College of Information Sciences and Technology The Pennsylvania State University University Park, PA 16802 ‡ College of Computing The Georgia Institute of Technology Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu ABSTRACT Topic detection and tracking [26] and topic segmentation [15] play an important role in capturing the local and sequential information of documents.",
                "Previous work in this area usually focuses on single documents, although similar multiple documents are available in many domains.",
                "In this paper, we introduce a novel unsupervised method for shared topic detection and topic segmentation of multiple similar documents based on mutual information (MI) and weighted mutual information (WMI) that is a combination of MI and term weights.",
                "The basic idea is that the optimal segmentation maximizes MI(or WMI).",
                "Our approach can detect shared topics among documents.",
                "It can find the optimal boundaries in a document, and align segments among documents at the same time.",
                "It also can handle single-document segmentation as a special case of the multi-document segmentation and alignment.",
                "Our methods can identify and strengthen cue terms that can be used for segmentation and partially remove stop words by using term weights based on entropy learned from multiple documents.",
                "Our experimental results show that our algorithm works well for the tasks of single-document segmentation, shared topic detection, and multi-document segmentation.",
                "Utilizing information from multiple documents can tremendously improve the performance of topic segmentation, and using WMI is even better than using MI for the multi-document segmentation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Clustering; H.3.1 [Information Storage and Retrieval]: Content Analysis and IndexingLinguistic processing; I.2.7 [Artificial Intelligence]: Natural Language Processing-Text analysis; I.5.3 [Pattern Recognition]: Clustering-Algorithms;Similarity measures General Terms Algorithms, Design, Experimentation 1.",
                "INTRODUCTION Many researchers have worked on topic detection and tracking (TDT) [26] and topic segmentation during the past decade.",
                "Topic segmentation intends to identify the boundaries in a document with the goal to capture the latent topical structure.",
                "Topic segmentation tasks usually fall into two categories [15]: text stream segmentation where topic transition is identified, and coherent document segmentation in which documents are split into sub-topics.",
                "The former category has applications in automatic speech recognition, while the latter one has more applications such as partial-text query of long documents in information retrieval, text summary, and quality measurement of multiple documents.",
                "Previous research in connection with TDT falls into the former category, targeted on topic tracking of broadcast speech data and newswire text, while the latter category has not been studied very well.",
                "Traditional approaches perform topic segmentation on documents one at a time [15, 25, 6].",
                "Most of them perform badly in subtle tasks like coherent document segmentation [15].",
                "Often, end-users seek documents that have the similar content.",
                "Search engines, like, Google, provide links to obtain similar pages.",
                "At a finer granularity, users may actually be looking to obtain sections of a document similar to a particular section that presumably discusses a topic of the users interest.",
                "Thus, the extension of topic segmentation from single documents to identifying similar segments from multiple similar documents with the same topic is a natural and necessary direction, and multi-document topic segmentation is expected to have a better performance since more information is utilized.",
                "Traditional approaches using similarity measurement based on term frequency generally have the same assumption that similar vocabulary tends to be in a coherent topic segment [15, 25, 6].",
                "However, they usually suffer the issue of identifying stop words.",
                "For example, additional document-dependent stop words are removed together with the generic stop words in [15].",
                "There are two reasons that we do not remove stop words directly.",
                "First, identifying stop words is another issue [12] that requires estimation in each domain.",
                "Removing common stop words may result in the loss of useful information in a specific domain.",
                "Second, even though stop words can be identified, hard classification of stop words and nonstop words cannot represent the gradually changing amount of information content of each word.",
                "We employ a soft classification using term weights.",
                "In this paper, we view the problem of topic segmentation as an optimization issue using information theoretic techniques to find the optimal boundaries of a document given the number of text segments so as to minimize the loss of mutual information (MI) (or a weighted mutual information (WMI)) after segmentation and alignment.",
                "This is equal to maximizing the MI (or WMI).",
                "The MI focuses on measuring the difference among segments whereas previous research focused on finding the similarity (e.g. cosine distance) of segments [15, 25, 6].",
                "<br>topic alignment</br> of multiple similar documents can be achieved by clustering sentences on the same topic into the same cluster.",
                "Single-document topic segmentation is just a special case of the multi-document topic segmentation and alignment problem.",
                "Terms can be co-clustered as in [10] at the same time, given the number of clusters, but our experimental results show that this method results in a worse segmentation (see Tables 1, 4, and 6).",
                "Usually, human readers can identify topic transition based on cue words, and can ignore stop words.",
                "Inspired by this, we give each term (or term cluster) a weight based on entropy among different documents and different segments of documents.",
                "Not only can this approach increase the contribution of cue words, but it can also decrease the effect of common stop words, noisy word, and document-dependent stop words.",
                "These words are common in a document.",
                "Many methods based on sentence similarity require that these words are removed before topic segmentation can be performed [15].",
                "Our results in Figure 3 show that term weights are useful for multi-document topic segmentation and alignment.",
                "The major contribution of this paper is that it introduces a novel method for topic segmentation using MI and shows that this method performs better than previously used criteria.",
                "Also, we have addressed the problem of topic segmentation and alignment across multiple documents, whereas most existing research focused on segmentation of single documents.",
                "Multi-document segmentation and alignment can utilize information from similar documents and improves the performance of topic segmentation greatly.",
                "Obviously, our approach can handle single documents as a special case when multiple documents are unavailable.",
                "It can detect shared topics among documents to judge if they are multiple documents on the same topic.",
                "We also introduce the new criterion of WMI based on term weights learned from multiple similar documents, which can improve performance of topic segmentation further.",
                "We propose an iterative greedy algorithm based on dynamic programming and show that it works well in practice.",
                "Some of our prior work is in [24].",
                "The rest of this paper is organized as follows: In Section 2, we review related work.",
                "Section 3 contains a formulation of the problem of topic segmentation and alignment of multiple documents with term co-clustering, a review of the criterion of MI for clustering, and finally an introduction to WMI.",
                "In Section 4, we first propose the iterative greedy algorithm of topic segmentation and alignment with term co-clustering, and then describe how the algorithm can be optimized by usFigure 1: Illustration of multi-document segmentation and alignment. ing dynamic programming.",
                "In Section 5, experiments about single-document segmentation, shared topic detection, and multi-document segmentation are described, and results are presented and discussed to evaluate the performance of our algorithm.",
                "Conclusions and some future directions of the research work are discussed in Section 6. 2.",
                "PREVIOUS WORK Generally, the existing approaches to text segmentation fall into two categories: supervised learning [19, 17, 23] and unsupervised learning [3, 27, 5, 6, 15, 25, 21].",
                "Supervised learning usually has good performance, since it learns functions from labelled training sets.",
                "However, often getting large training sets with manual labels on document sentences is prohibitively expensive, so unsupervised approaches are desired.",
                "Some models consider dependence between sentences and sections, such as Hidden Markov Model [3, 27], Maximum Entropy Markov Model [19], and Conditional Random Fields [17], while many other approaches are based on lexical cohesion or similarity of sentences [5, 6, 15, 25, 21].",
                "Some approaches also focus on cue words as hints of topic transitions [11].",
                "While some existing methods only consider information in single documents [6, 15], others utilize multiple documents [16, 14].",
                "There are not many works in the latter category, even though the performance of segmentation is expected to be better with utilization of information from multiple documents.",
                "Previous research studied methods to find shared topics [16] and topic segmentation and summarization between just a pair of documents [14].",
                "Text classification and clustering is a related research area which categorizes documents into groups using supervised or unsupervised methods.",
                "Topical classification or clustering is an important direction in this area, especially co-clustering of documents and terms, such as LSA [9], PLSA [13], and approaches based on distances and bipartite graph partitioning [28] or maximum MI [2, 10], or maximum entropy [1, 18].",
                "Criteria of these approaches can be utilized in the issue of topic segmentation.",
                "Some of those methods have been extended into the area of topic segmentation, such as PLSA [5] and maximum entropy [7], but to our best knowledge, using MI for topic segmentation has not been studied. 3.",
                "PROBLEM FORMULATION Our goal is to segment documents and align the segments across documents (Figure 1).",
                "Let T be the set of terms {t1, t2, ..., tl}, which appear in the unlabelled set of documents D = {d1, d2, ..., dm}.",
                "Let Sd be the set of sentences for document d ∈ D, i.e. {s1, s2, ..., snd }.",
                "We have a 3D matrix of term frequency, in which the three dimensions are random variables of D, Sd, and T. Sd actually is a random vector including a random variable for each d ∈ D. The term frequency can be used to estimate the joint probability distribution P(D, Sd, T), which is p(t, d, s) = T(t, d, s)/ND, where T(t, d, s) is the number of t in ds sentence s and ND is the total number of terms in D. ˆS represents the set of segments {ˆs1, ˆs2, ..., ˆsp} after segmentation and alignment among multiple documents, where the number of segments | ˆS| = p. A segment ˆsi of document d is a sequence of adjacent sentences in d. Since for different documents si may discuss different sub-topics, our goal is to cluster adjacent sentences in each document into segments, and align similar segments among documents, so that for different documents ˆsi is about the same sub-topic.",
                "The goal is to find the optimal topic segmentation and alignment mapping Segd(si) : {s1, s2, ..., snd } → {ˆs1, ˆs2, ..., ˆsp} and Alid(ˆsi) : {ˆs1, ˆs2, ..., ˆsp} → {ˆs1, ˆs2, ..., ˆsp}, for all d ∈ D, where ˆsi is ith segment with the constraint that only adjacent sentences can be mapped to the same segment, i.e. for d, {si, si+1, ..., sj} → {ˆsq}, where q ∈ {1, ..., p}, where p is the segment number, and if i > j, then for d, ˆsq is missing.",
                "After segmentation and alignment, random vector Sd becomes an aligned random variable ˆS.",
                "Thus, P(D, Sd, T) becomes P(D, ˆS, T).",
                "Term co-clustering is a technique that has been employed [10] to improve the accuracy of document clustering.",
                "We evaluate the effect of it for topic segmentation.",
                "A term t is mapped to exactly one term cluster.",
                "Term co-clustering involves simultaneously finding the optimal term clustering mapping Clu(t) : {t1, t2, ..., tl} → {ˆt1, ˆt2, ..., ˆtk}, where k ≤ l, l is the total number of words in all the documents, and k is the number of clusters. 4.",
                "METHODOLOGY We now describe a novel algorithm which can handle singledocument segmentation, shared topic detection, and multidocument segmentation and alignment based on MI or WMI. 4.1 Mutual Information MI I(X; Y ) is a quantity to measure the amount of information which is contained in two or more random variables [8, 10].",
                "For the case of two random variables, we have I(X; Y ) = x∈X y∈Y p(x, y)log p(x, y) p(x)p(y) , (1) Obviously, when random variables X and Y are independent, I(X; Y ) = 0.",
                "Thus, intuitively, the value of MI depends on how random variables are dependent on each other.",
                "The optimal co-clustering is the mapping Clux : X → ˆX and Cluy : Y → ˆY that minimizes the loss: I(X; Y ) − I( ˆX; ˆY ), which is equal to maximizing I( ˆX; ˆY ).",
                "This is the criterion of MI for clustering.",
                "In the case of topic segmentation, the two random variables are the term variable T and the segment variable S, and each sample is an occurrence of a term T = t in a particular segment S = s. I(T; S) is used to measure how dependent T and S are.",
                "However, I(T; S) cannot be computed for documents before segmentation, since we do not have a set of S due to the fact that sentences of Document d, si ∈ Sd, is not aligned with other documents.",
                "Thus, instead of minimizing the loss of MI, we can maximize MI after topic segmentation, computed as: I( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS p(ˆt, ˆs)log p(ˆt, ˆs) p(ˆt)p(ˆs) , (2) where p(ˆt, ˆs) are estimated by the term frequency tf of Term Cluster ˆt and Segment ˆs in the training set D. Note that here a segment ˆs includes sentences about the the same topic among all documents.",
                "The optimal solution is the mapping Clut : T → ˆT, Segd : Sd → ˆS , and Alid : ˆS → ˆS, which maximizes I( ˆT; ˆS). 4.2 Weighted Mutual Information In topic segmentation and alignment of multiple documents, if P(D, ˆS, T) is known, based on the marginal distributions P(D|T) and P( ˆS|T) for each term t ∈ T, we can categorize terms into four types in the data set: • Common stop words are common both along the dimensions of documents and segments. • Document-dependent stop words that depends on the personal writing style are common only along the dimension of segments for some documents. • Cue words are the most important elements for segmentation.",
                "They are common along the dimension of documents only for the same segment, and they are not common along the dimensions of segments. • Noisy words are other words which are not common along both dimensions.",
                "Entropy based on P(D|T) and P( ˆS|T) can be used to identify different types of terms.",
                "To reinforce the contribution of cue words in the MI computation, and simultaneously reduce the effect of the other three types of words, similar as the idea of the tf-idf weight [22], we use entropies of each term along the dimensions of document D and segment ˆS, i.e.",
                "ED(ˆt) and EˆS(ˆt), to compute the weight.",
                "A cue word usually has a large value of ED(ˆt) but a small value of EˆS(ˆt).",
                "We introduce term weights (or term cluster weights) wˆt = ( ED(ˆt) maxˆt ∈ ˆT (ED(ˆt )) )a (1 − EˆS(ˆt) maxˆt ∈ ˆT (EˆS(ˆt )) )b , (3) where ED(ˆt) = d∈D p(d|ˆt)log|D| 1 p(d|ˆt) , EˆS(ˆt) = ˆs∈ ˆS p(ˆs|ˆt)log| ˆS| 1 p(ˆs|ˆt) , and a > 0 and b > 0 are powers to adjust term weights.",
                "Usually a = 1 and b = 1 as default, and maxˆt ∈ ˆT (ED(ˆt )) and maxˆt ∈ ˆT (EˆS(ˆt )) are used to normalize the entropy values.",
                "Term cluster weights are used to adjust p(ˆt, ˆs), pw(ˆt, ˆs) = wˆtp(ˆt, ˆs) ˆt∈ ˆT ;ˆs∈ ˆS wˆtp(ˆt, ˆs) , (4) and Iw( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS pw(ˆt, ˆs)log pw(ˆt, ˆs) pw(ˆt)pw(ˆs) , (5) where pw(ˆt) and pw(ˆs) are marginal distributions of pw(ˆt, ˆs).",
                "However, since we do not know either the term weights or P(D, ˆS, T), we need to estimate them, but wˆt depends on p(ˆs|t) and ˆS, while ˆS and p(ˆs|t) also depend on wˆt that is still unknown.",
                "Thus, an iterative algorithm is required to estimate term weights wˆt and find the best segmentation and alignment to optimize the objective function Iw concurrently.",
                "After a document is segmented into sentences Input: Joint probability distribution P(D, Sd, T), number of text segments p ∈ {2, 3, ..., max(sd)}, number of term clusters k ∈ {2, 3, ..., l} (if k = l, no term co-clustering required), and weight type w ∈ {0, 1}, indicating to use I or Iw, respectively.",
                "Output: Mapping Clu, Seg, Ali, and term weights wˆt.",
                "Initialization: 0. i = 0.",
                "Initialize Clu (0) t , Seg (0) d , and Ali (0) d ; Initialize w (0) ˆt using Equation (6) if w = 1; Stage 1: 1.",
                "If |D| = 1, k = l, and w = 0, check all sequential segmentations of d into p segments and find the best one Segd(s) = argmaxˆsI( ˆT; ˆS), and return Segd; otherwise, if w = 1 and k = l, go to 3.1; Stage 2: 2.1 If k < l, for each term t, find the best cluster ˆt as Clu(i+1)(t) = argmaxˆtI( ˆT; ˆS(i)) based on Seg(i) and Ali(i); 2.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+1) d (Seg (i+1) d (s)) = argmaxˆsI( ˆT(i+1); ˆS) based on Clu(i+1)(t) if k < l or Clu(0)(t) if k = l; 2.3 i + +.",
                "If Clu, Seg, or Ali changed, go to 2.1; otherwise, if w = 0, return Clu(i), Seg(i), and Ali(i); else j = 0, go to 3.1; Stage 3: 3.1 Update w (i+j+1) ˆt based on Seg(i+j), Ali(i+j), and Clu(i) using Equation (3); 3.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+j+1) d (Seg (i+j+1) d (s)) = argmaxˆsIw( ˆT(i); ˆS) based on Clu(i) and w (i+j+1) ˆt ; 3.3 j + +.",
                "If Iw( ˆT; ˆS) changes, go to step 6; otherwise, stop and return Clu(i), Seg(i+j), Ali(i+j), and w (i+j) ˆt ; Figure 2: Algorithm: Topic segmentation and alignment based on MI or WMI. and each sentence is segmented into words, each word is stemmed.",
                "Then the joint probability distribution P(D, Sd, T) can be estimated.",
                "Finally, this distribution can be used to compute MI in our algorithm. 4.3 Iterative Greedy Algorithm Our goal is to maximize the objective function, I( ˆT; ˆS) or Iw( ˆT; ˆS), which can measure the dependence of term occurrences in different segments.",
                "Generally, first we do not know the estimate term weights, which depend on the optimal topic segmentation and alignment, and term clusters.",
                "Moreover, this problem is NP-hard [10], even though if we know the term weights.",
                "Thus, an iterative greedy algorithm is desired to find the best solution, even though probably only local maxima are reached.",
                "We present the iterative greedy algorithm in Figure 2 to find a local maximum of I( ˆT; ˆS) or Iw( ˆT; ˆS) with simultaneous term weight estimation.",
                "This algorithm can is iterative and greedy for multi-document cases or single-document cases with term weight estimation and/or term co-clustering.",
                "Otherwise, since it is just a one step algorithm to solve the task of single-document segmentation [6, 15, 25], the global maximum of MI is guaranteed.",
                "We will show later that term co-clustering reduces the accuracy of the results and is not necessary, and for singledocument segmentation, term weights are also not required. 4.3.1 Initialization In Step 0, the initial term clustering Clut and topic segmentation and alignment Segd and Alid are important to avoid local maxima and reduce the number of iterations.",
                "First, a good guess of term weights can be made by using the distributions of term frequency along sentences for each document and averaging them to get the initial values of wˆt: wt = ( ED(t) maxt ∈T (ED(t )) )(1 − ES(t) maxt ∈T (ES(t )) ), (6) where ES(t) = 1 |Dt| d∈Dt (1 − s∈Sd p(s|t)log|Sd| 1 p(s|t) ), where Dt is the set of documents which contain Term t. Then, for the initial segmentation Seg(0) , we can simply segment documents equally by sentences.",
                "Or we can find the optimal segmentation just for each document d which maximizes the WMI, Seg (0) d = argmaxˆsIw(T; ˆS), where w = w (0) ˆt .",
                "For the initial alignment Ali(0) , we can first assume that the order of segments for each d is the same.",
                "For the initial term clustering Clu(0) , first cluster labels can be set randomly, and after the first time of Step 3, a good initial term clustering is obtained. 4.3.2 Different Cases After initialization, there are three stages for different cases.",
                "Totally there are eight cases, |D| = 1 or |D| > 1, k = l or k < l, w = 0 or w = 1.",
                "Single document segmentation without term clustering and term weight estimation (|D| = 1, k = l, w = 0) only requires Stage 1 (Step 1).",
                "If term clustering is required (k < l), Stage 2 (Step 2.1, 2.2, and 2.3) is executed iteratively.",
                "If term weight estimation is required (w = 1), Stage 3 (Step 3.1, 3.2, and 3.3) is executed iteratively.",
                "If both are required (k < l, w = 1), Stage 2 and 3 run one after the other.",
                "For multi-document segmentation without term clustering and term weight estimation (|D| > 1, k = l, w = 0), only iteration of Step 2.2 and 2.3 are required.",
                "At Stage 1, the global maximum can be found based on I( ˆT; ˆS) using dynamic programming in Section 4.4.",
                "Simultaneously finding a good term clustering and estimated term weights is impossible, since when moving a term to a new term cluster to maximize Iw( ˆT; ˆS), we do not know that the weight of this term should be the one of the new cluster or the old cluster.",
                "Thus, we first do term clustering at Stage 2, and then estimate term weights at Stage 3.",
                "At Stage 2, Step 2.1 is to find the best term clustering and Step 2.2 is to find the best segmentation.",
                "This cycle is repeated to find a local maximum based on MI I until it converges.",
                "The two steps are: (1) based on current term clustering Cluˆt, for each document d, the algorithm segments all the sentences Sd into p segments sequentially (some segments may be empty), and put them into the p segments ˆS of the whole training set D (all possible cases of different segmentation Segd and alignment Alid are checked) to find the optimal case, and (2) based on the current segmentation and alignment, for each term t, the algorithm finds the best term cluster of t based on the current segmentation Segd and alignment Alid.",
                "After finding a good term clustering, term weights are estimated if w = 1.",
                "At Stage 3, similar as Stage 2, Step 3.1 is term weight re-estimation and Step 3.2 is to find a better segmentation.",
                "They are repeated to find a local maximum based on WMI Iw until it converges.",
                "However, if the term clustering in Stage 2 is not accurate, then the term weight estimation at Stage 3 may have a bad result.",
                "Finally, at Step 3.3, this algorithm converges and return the output.",
                "This algorithm can handle both single-document and multi-document segmentation.",
                "It also can detect shared topics among documents by checking the proportion of overlapped sentences on the same topics, as described in Sec 5.2. 4.4 Algorithm Optimization In many previous works on segmentation, dynamic programming is a technique used to maximize the objective function.",
                "Similarly, at Step 1, 2.2, and 3.2 of our algorithm, we can use dynamic programming.",
                "For Stage 1, using dynamic programming can still find the global optimum, but for Stage 2 and Stage 3, we can only find the optimum for each step of topic segmentation and alignment of a document.",
                "Here we only show the dynamic programming for Step 3.2 using WMI (Step 1 and 2.2 are similar but they can use either I or Iw).",
                "There are two cases that are not shown in the algorithm in Figure 2: (a) single-document segmentation or multi-document segmentation with the same sequential order of segments, where alignment is not required, and (b) multi-document segmentation with different sequential orders of segments, where alignment is necessary.",
                "The alignment mapping function of the former case is simply just Alid(ˆsi) = ˆsi, while for the latter ones alignment mapping function Alid(ˆsi) = ˆsj, i and j may be different.",
                "The computational steps for the two cases are listed below: Case 1 (no alignment): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs) and partial pw(ˆs) without counting sentences from d. Then put sentences from i to j into Part k, and compute partial WMI PIw( ˆT; ˆsk(si, si+1, ..., sj)) ˆt∈ ˆT pw(ˆt, ˆsk)log pw(ˆt, ˆsk) pw(ˆt)pw(ˆsk) , where Alid(si, si+1, ..., sj) = k, k ∈ {1, 2, ..., p}, 1 ≤ i ≤ j ≤ nd, and Segd(sq) = ˆsk for all i ≤ q ≤ j. (2) Let M(sm, 1) = PIw( ˆT; ˆs1(s1, s2, ..., sm)).",
                "Then M(sm, L) = maxi[M(si−1, L − 1) + PIw( ˆT; ˆsL(si, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, and when i > m, no sentences are put into ˆsk when compute PIw (note PIw( ˆT; ˆs(si, ..., sm)) = 0 for single-document segmentation). (3) Finally M(snd , p) = maxi[M(si−1, p − 1)+ PIw( ˆT; ˆsp(si, ..., snd ))], where 1 ≤ i ≤ nd+1.",
                "The optimal Iw is found and the corresponding segmentation is the best.",
                "Case 2 (alignment required): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs), and partial pw(ˆs), and PIw( ˆT; ˆsk(si, si+1, ..., sj)) similarly as Case 1. (2) Let M(sm, 1, k) = PIw( ˆT; ˆsk(s1, s2, ..., sm)), where k ∈ {1, 2, ..., p}.",
                "Then M(sm, L, kL) = maxi,j[M(si−1, L − 1, kL/j) + PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, kL ∈ Set(p, L), which is the set of all p!",
                "L!(p−L)! combinations of L segments chosen from all p segments, j ∈ kL, the set of L segments chosen from all p segments, and kL/j is the combination of L − 1 segments in kL except Segment j. (3) Finally, M(snd , p, kp) = maxi,j[M(si−1, p − 1, kp/j) +PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., snd ))], where kp is just the combination of all p segments and 1 ≤ i ≤ nd + 1, which is the optimal Iw and the corresponding segmentation is the best.",
                "The steps of Case 1 and 2 are similar, except in Case 2, alignment is considered in addition to segmentation.",
                "First, basic items of probability for computing Iw are computed excluding Doc d, and then partial WMI by putting every possible sequential segment (including empty segment) of d into every segment of the set.",
                "Second, the optimal sum of PIw for L segments and the leftmost m sentences, M(sm, L), is found.",
                "Finally, the maximal WMI is found among different sums of M(sm, p − 1) and PIw for Segment p. 5.",
                "EXPERIMENTS In this section, single-document segmentation, shared topic detection, and multi-document segmentation will be tested.",
                "Different hyper parameters of our method are studied.",
                "For convenience, we refer to the method using I as MIk if w = 0, and Iw as WMIk if w = 2 or as WMIk if w = 1, where k is the number of term clusters, and if k = l, where l is the total number of terms, then no term clustering is required, i.e.",
                "MIl and WMIl. 5.1 Single-document Segmentation 5.1.1 Test Data and Evaluation The first data set we tested is a synthetic one used in previous research [6, 15, 25] and many other papers.",
                "It has 700 samples.",
                "Each is a concatenation of ten segments.",
                "Each segment is the first n sentence selected randomly from the Brown corpus, which is supposed to have a different topic from each other.",
                "Currently, the best results on this data set is achieved by Ji et.al. [15].",
                "To compare the performance of our methods, the criterion used widely in previous research is applied, instead of the unbiased criterion introduced in [20].",
                "It chooses a pair of words randomly.",
                "If they are in different segments (different) for the real segmentation (real), but predicted (pred) as in the same segment, it is a miss.",
                "If they are in the same segment (same), but predicted as in different segments, it is a false alarm.",
                "Thus, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, diff)p(diff|real) +p(false alarm|real, pred, same)p(same|real). 5.1.2 Experiment Results We tested the case when the number of segments is known.",
                "Table 1 shows the results of our methods with different hyper parameter values and three previous approaches, C99[25], U00[6], and ADDP03[15], on this data set when the segment number is known.",
                "In WMI for single-document segmentation, the term weights are computed as follows: wˆt = 1−EˆS(ˆt)/maxˆt ∈ ˆT (EˆS(ˆt )).",
                "For this case, our methods MIl and WMIl both outperform all the previous approaches.",
                "We compared our methods with ADDP03using one-sample one-sided t-test and p-values are shown in Table 2.",
                "From the p-values, we can see that mostly the differences are very Table 1: Average Error Rates of Single-document Segmentation Given Segment Numbers Known Range of n 3-11 3-5 6-8 9-11 Sample size 400 100 100 100 C99 12% 11% 10% 9% U00 10% 9% 7% 5% ADDP03 6.0% 6.8% 5.2% 4.3% MIl 4.68% 5.57% 2.59% 1.59% WMIl 4.94% 6.33% 2.76% 1.62% MI100 9.62% 12.92% 8.66% 6.67% Table 2: Single-document Segmentation: P-values of T-test on Error Rates Range of n 3-11 3-5 6-8 9-11 ADDP03, MIl 0.000 0.000 0.000 0.000 ADDP03, WMIl 0.000 0.099 0.000 0.000 MIl, WMIl 0.061 0.132 0.526 0.898 significant.",
                "We also compare the error rates between our two methods using two-sample two-sided t-test to check the hypothesis that they are equal.",
                "We cannot reject the hypothesis that they are equal, so the difference are not significant, even though all the error rates for MIl are smaller than WMIl.",
                "However, we can conclude that term weights contribute little in single-document segmentation.",
                "The results also show that MI using term co-clustering (k = 100) decreases the performance.",
                "We tested different number of term clusters, and found that the performance becomes better when the cluster number increases to reach l. WMIk<l has similar results that we did not show in the table.",
                "As mentioned before, using MI may be inconsistent on optimal boundaries given different numbers of segments.",
                "This situation occurs especially when the similarities among segments are quite different, i.e. some transitions are very obvious, while others are not.",
                "This is because usually a document is a hierarchical structure instead of only a sequential structure.",
                "When the segments are not at the same level, this situation may occur.",
                "Thus, a hierarchical topic segmentation approach is desired, and the structure highly depends on the number of segments for each internal node and the stop criteria of splitting.",
                "For this data set of singledocument segmentation, since it is just a synthetic set, which is just a concatenation of several segments about different topics, it is reasonable that approaches simply based on term frequency have a good performance.",
                "Usually for the tasks of segmenting coherent documents for sub-topics, the effectiveness decreases much. 5.2 Shared Topic Detection 5.2.1 Test Data and Evaluation The second data set contains 80 news articles from Google News.",
                "There are eight topics and each has 10 articles.",
                "We randomly split the set into subsets with different document numbers and each subset has all eight topics.",
                "We compare our approach MIl and WMIl with LDA [4].",
                "LDA treats a document in the data set as a bag of words, finds its distribution on topics, and its major topic.",
                "MIl and WMIl views each sentence as a bag of words and tag it with a topic label.",
                "Then for each pair of documents, LDA determines if they are on the same topic, while MIl and Table 3: Shared Topic Detection: Average Error Rates for Different Numbers of Documents in Each Subset #Doc 10 20 40 80 LDA 8.89% 16.33% 1.35% 0.60% MIl, θ = 0.6 4.17% 1.71% 1.47% 0.0% WMIl, θ = 0.8 18.6% 3.16% 1.92% 0.0% WMIl check whether the proportion overlapped sentences on the same topic is larger than the adjustable threshold θ.",
                "That is, in MIl and WMIl, for a pair of documents d, d , if [ s∈Sd,s ∈Sd 1(topics=topics )/min(|Sd|, |Sd|)] > θ, where Sd is the set of sentences of d, and |Sd| is the number of sentences of d, then d and d have the shared topic.",
                "For a pair of documents selected randomly, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, same)p(same|real) +p(false alarm|real, pred, diff)p(diff|real), where a miss means if they have the same topic (same) for the real case (real), but predicted (pred) as on the same topic.",
                "If they are on different topics (diff), but predicted as on the same topic, it is a false alarm. 5.2.2 Experiment Results The results are shown in Table 3.",
                "If most documents have different topics, in WMIl, the estimation of term weights in Equation (3) is not correct.",
                "Thus, WMIl is not expected to have a better performance than MIl, when most documents have different topics.",
                "When there are fewer documents in a subset with the same number of topics, more documents have different topics, so WMIl is more worse than MIl.",
                "We can see that for most cases MIl has a better (or at least similar) performance than LDA.",
                "After shared topic detection, multi-document segmentation of documents with the shared topics is able to be executed. 5.3 Multi-document Segmentation 5.3.1 Test Data and Evaluation For multi-document segmentation and alignment, our goal is to identify these segments about the same topic among multiple similar documents with shared topics.",
                "Using Iw is expected to perform better than I, since without term weights the result is affected seriously by document-dependent stop words and noisy words which depends on the personal writing style.",
                "It is more likely to treat the same segments of different documents as different segments under the effect of document-dependent stop words and noisy words.",
                "Term weights can reduce the effect of document-dependent stop words and noisy words by giving cue terms more weights.",
                "The data set for multi-document segmentation and alignment has 102 samples and 2264 sentences totally.",
                "Each is the introduction part of a lab report selected from the course of Biol 240W, Pennsylvania State University.",
                "Each sample has two segments, introduction of plant hormones and the content in the lab.",
                "The length range of samples is from two to 56 sentences.",
                "Some samples only have one part and some have a reverse order the these two segments.",
                "It is not hard to identify the boundary between two segments for human.",
                "We labelled each sentence manually for evaluation.",
                "The criterion of evaluation is just using the proportion of the number of sentences with wrong predicted segment labels in the total number of sentences in the whole training Table 4: Average Error Rates of Multi-document Segmentation Given Segment Numbers Known #Doc MIl WMIl k MIk WMIk 102 3.14% 2.78% 300 4.68% 6.58% 51 4.17% 3.63% 300 17.83% 22.84% 34 5.06% 4.12% 300 18.75% 20.95% 20 7.08% 5.42% 250 20.40% 21.83% 10 10.38% 7.89% 250 21.42% 21.91% 5 15.77% 11.64% 250 21.89% 22.59% 2 25.90% 23.18% 50 25.44% 25.49% 1 23.90% 24.82% 25 25.75% 26.15% Table 5: Multi-document Segmentation: P-values of T-test on Error Rates for MIl and WMIl #Doc 51 34 20 10 5 2 P-value 0.19 0.101 0.025 0.001 0.000 0.002 set as the error rate: p(error|predicted, real) = d∈D s∈Sd 1(predicteds=reals)/ d∈D nd.",
                "In order to show the benefits of multi-document segmentation and alignment, we compared our method with different parameters on different partitions of the same training set.",
                "Except the cases that the number of documents is 102 and one (they are special cases of using the whole set and the pure single-document segmentation), we randomly divided the training set into m partitions, and each has 51, 34, 20, 10, 5, and 2 document samples.",
                "Then we applied our methods on each partition and calculated the error rate of the whole training set.",
                "Each case was repeated for 10 times for computing the average error rates.",
                "For different partitions of the training set, different k values are used, since the number of terms increases when the document number in each partition increases. 5.3.2 Experiment Results From the experiment results in Table 4, we can see the following observations: (1) When the number of documents increases, all methods have better performances.",
                "Only from one to two documents, MIl has decrease a little.",
                "We can observe this from Figure 3 at the point of document number = 2.",
                "Most curves even have the worst results at this point.",
                "There are two reasons.",
                "First, because samples vote for the best multi-document segmentation and alignment, but if only two documents are compared with each other, the one with missing segments or a totally different sequence will affect the correct segmentation and alignment of the other.",
                "Second, as noted at the beginning of this section, if two documents have more document-dependent stop words or noisy words than cue words, then the algorithm may view them as two different segments and the other segment is missing.",
                "Generally, we can only expect a better performance when the number of documents is larger than the number of segments. (2) Except single-document segmentation, WMIl is always better than MIl, and when the number of documents is reaching one or increases to a very large number, their performances become closer.",
                "Table 5 shows p-values of twosample one-sided t-test between MIl and WMIl.",
                "We also can see this trend from p-values.",
                "When document number = 5, we reached the smallest p-value and the largest difference between error rates of MIl and WMIl.",
                "For single-document Table 6: Multi-document Segmentation: Average Error Rate for Document Number = 5 in Each Subset with Different Number of Term Clusters #Cluster 75 100 150 250 l MIk 24.67% 24.54% 23.91% 22.59% 15.77% segmentation, WMIl is even a little bit worse than MIl, which is similar as the results of the single-document segmentation on the first data set.",
                "The reason is that for singledocument segmentation, we cannot estimate term weights accurately, since multiple documents are unavailable. (3) Using term clustering usually gets worse results than MIl and WMIl. (4) Using term clustering in WMIk is even worse than in MIk, since in WMIk term clusters are found first using I before using Iw.",
                "If the term clusters are not correct, then the term weights are estimated worse, which may mislead the algorithm to reach even worse results.",
                "From the results we also found that in multi-document segmentation and alignment, most documents with missing segments and a reverse order are identified correctly.",
                "Table 6 illustrates the experiment results for the case of 20 partitions (each has five document samples) of the training set and topic segmentation and alignment using MIk with different numbers of term clusters k. Notice that when the number of term clusters increases, the error rate becomes smaller.",
                "Without term clustering, we have the best result.",
                "We did not show results for WMIk with term clustering, but the results are similar.",
                "We also tested WMIl with different hyper parameters of a and b to adjust term weights.",
                "The results are presented in Figure 3.",
                "It was shown that the default case WMIl : a = 1, b = 1 gave the best results for different partitions of the training set.",
                "We can see the trend that when the document number is very small or large, the difference between MIl : a = 0, b = 0 and WMIl : a = 1, b = 1 becomes quite small.",
                "When the document number is not large (about from 2 to 10), all the cases using term weights have better performances than MIl : a = 0, b = 0 without term weights, but when the document number becomes larger, the cases WMIl : a = 1, b = 0 and WMIl : a = 2, b = 1 become worse than MIl : a = 0, b = 0.",
                "When the document number becomes very large, they are even worse than cases with small document numbers.",
                "This means that a proper way to estimate term weights for the criterion of WMI is very important.",
                "Figure 4 shows the term weights learned from the whole training set.",
                "Four types of words are categorized roughly even though the transition among them are subtle.",
                "Figure 5 illustrates the change in (weighted) mutual information for MIl and WMIl.",
                "As expected, mutual information for MIl increases monotonically with the number of steps, while WMIl does not.",
                "Finally, MIl and WMIl are scalable, with computational complexity shown in Figure 6.",
                "One advantage for our approach based on MI is that removing stop words is not required.",
                "Another important advantage is that there are no necessary hyper parameters to adjust.",
                "In single-document segmentation, the performance based on MI is even better for that based on WMI, so no extra hyper parameter is required.",
                "In multi-document segmentation, we show in the experiment, a = 1 and b = 1 is the best.",
                "Our method gives more weights to cue terms.",
                "However, usually cue terms or sentences appear at the beginning of a segment, while the end of the segment may be 1 2 5 10 20 34 51 102 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 Document Number ErrorRate MIl :a=0,b=0 WMI l :a=1,b=1 WMI l :a=1,b=0 WMI l :a=2,b=1 Figure 3: Error rates for different hyper parameters of term weights. 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Normalized Document Entropy NormalizedSegmentEntropy Noisy words Cue words Common stop words Doc−dep stop words Figure 4: Term weights learned from the whole training set. 0 100 200 300 400 500 600 0.06 0.08 0.1 0.12 0.14 0.16 0.18 Number of Steps (Weighted)MutualInformation MI l WMI l Figure 5: Change in (weighted) MI for MIl and WMIl. 0 20 40 60 80 100 120 0 200 400 600 800 1000 1200 1400 1600 1800 2000 Document Number TimetoConverge(sec) MI l WMI l Figure 6: Time to converge for MIl and WMIl. much noisy.",
                "One possible solution is giving more weights to terms at the beginning of each segment.",
                "Moreover, when the length of segments are quite different, long segments have much higher term frequencies, so they may dominate the segmentation boundaries.",
                "Normalization of term frequencies versus the segment length may be useful. 6.",
                "CONCLUSIONS AND FUTURE WORK We proposed a novel method for multi-document topic segmentation and alignment based on weighted mutual information, which can also handle single-document cases.",
                "We used dynamic programming to optimize our algorithm.",
                "Our approach outperforms all the previous methods on singledocument cases.",
                "Moreover, we also showed that doing segmentation among multiple documents can improve the performance tremendously.",
                "Our results also illustrated that using weighted mutual information can utilize the information of multiple documents to reach a better performance.",
                "We only tested our method on limited data sets.",
                "More data sets especially complicated ones should be tested.",
                "More previous methods should be compared with.",
                "Moreover, natural segmentations like paragraphs are hints that can be used to find the optimal boundaries.",
                "Supervised learning also can be considered. 7.",
                "ACKNOWLEDGMENTS The authors want to thank Xiang Ji, and Prof. J. Scott Payne for their help. 8.",
                "REFERENCES [1] A. Banerjee, I. Ghillon, J. Ghosh, S. Merugu, and D. Modha.",
                "A generalized maximum entropy approach to bregman co-clustering and matrix approximation.",
                "In Proceedings of SIGKDD, 2004. [2] R. Bekkerman, R. El-Yaniv, and A. McCallum.",
                "Multi-way distributional clustering via pairwise interactions.",
                "In Proceedings of ICML, 2005. [3] D. M. Blei and P. J. Moreno.",
                "Topic segmentation with an aspect hidden markov model.",
                "In Proceedings of SIGIR, 2001. [4] D. M. Blei, A. Ng, and M. Jordan.",
                "Latent dirichlet allocation.",
                "Journal of Machine Learning Research, 3:993-1022, 2003. [5] T. Brants, F. Chen, and I. Tsochantaridis.",
                "Topic-based document segmentation with probabilistic latent semantic analysis.",
                "In Proceedings of CIKM, 2002. [6] F. Choi.",
                "Advances in domain indepedent linear text segmentation.",
                "In Proceedings of the NAACL, 2000. [7] H. Christensen, B. Kolluru, Y. Gotoh, and S. Renals.",
                "Maximum entropy segmentation of broadcast news.",
                "In Proceedings of ICASSP, 2005. [8] T. Cover and J. Thomas.",
                "Elements of Information Theory.",
                "John Wiley and Sons, New York, USA, 1991. [9] S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and R. Harshman.",
                "Indexing by latent semantic analysis.",
                "Journal of the American Society for Information Systems, 1990. [10] I. Dhillon, S. Mallela, and D. Modha.",
                "Information-theoretic co-clustering.",
                "In Proceedings of SIGKDD, 2003. [11] M. Hajime, H. Takeo, and O. Manabu.",
                "Text segmentation with multiple surface linguistic cues.",
                "In Proceedings of COLING-ACL, 1998. [12] T. K. Ho.",
                "Stop word location and identification for adaptive text recognition.",
                "International Journal of Document Analysis and Recognition, 3(1), August 2000. [13] T. Hofmann.",
                "Probabilistic latent semantic analysis.",
                "In Proceedings of the UAI99, 1999. [14] X. Ji and H. Zha.",
                "Correlating summarization of a pair of multilingual documents.",
                "In Proceedings of RIDE, 2003. [15] X. Ji and H. Zha.",
                "Domain-independent text segmentation using anisotropic diffusion and dynamic programming.",
                "In Proceedings of SIGIR, 2003. [16] X. Ji and H. Zha.",
                "Extracting shared topics of multiple documents.",
                "In Proceedings of the 7th PAKDD, 2003. [17] J. Lafferty, A. McCallum, and F. Pereira.",
                "Conditional random fields: Probabilistic models for segmenting and labeling sequence data.",
                "In Proceedings of ICML, 2001. [18] T. Li, S. Ma, and M. Ogihara.",
                "Entropy-based criterion in categorical clustering.",
                "In Proceedings of ICML, 2004. [19] A. McCallum, D. Freitag, and F. Pereira.",
                "Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of ICML, 2000. [20] L. Pevzner and M. Hearst.",
                "A critique and improvement of an evaluation metric for text segmentation.",
                "Computational Linguistic, 28(1):19-36, 2002. [21] J. C. Reynar.",
                "Statistical models for topic segmentation.",
                "In Proceedings of ACL, 1999. [22] G. Salton and M. McGill.",
                "Introduction to Modern Information Retrieval.",
                "McGraw Hill, 1983. [23] B.",
                "Sun, Q. Tan, P. Mitra, and C. L. Giles.",
                "Extraction and search of chemical formulae in text documents on the web.",
                "In Proceedings of WWW, 2007. [24] B.",
                "Sun, D. Zhou, H. Zha, and J.",
                "Yen.",
                "Multi-task text segmentation and alignment based on weighted mutual information.",
                "In Proceedings of CIKM, 2006. [25] M. Utiyama and H. Isahara.",
                "A statistical model for domain-independent text segmentation.",
                "In Proceedings of the 39th ACL, 1999. [26] C. Wayne.",
                "Multilingual topic detection and tracking: Successful research enabled by corpora and evaluation.",
                "In Proceedings of LREC, 2000. [27] J. Yamron, I. Carp, L. Gillick, S. Lowe, and P. van Mulbregt.",
                "A hidden markov model approach to text segmentation and event tracking.",
                "In Proceedings of ICASSP, 1998. [28] H. Zha and X. Ji.",
                "Correlating multilingual documents via bipartite graph modeling.",
                "In Proceedings of SIGIR, 2002."
            ],
            "original_annotated_samples": [
                "<br>topic alignment</br> of multiple similar documents can be achieved by clustering sentences on the same topic into the same cluster."
            ],
            "translated_annotated_samples": [
                "La <br>alineación de temas</br> de múltiples documentos similares se puede lograr agrupando las oraciones sobre el mismo tema en el mismo clúster."
            ],
            "translated_text": "La segmentación de temas con detección de temas compartidos y alineación de múltiples documentos Bingjun Sun*, Prasenjit Mitra*†, Hongyuan Zha‡, C. Lee Giles*†, John Yen*† *Departamento de Ciencias de la Computación e Ingeniería † Colegio de Ciencias de la Información y Tecnología Universidad Estatal de Pensilvania University Park, PA 16802 ‡ Colegio de Computación Instituto de Tecnología de Georgia Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu RESUMEN La detección y seguimiento de temas [26] y la segmentación de temas [15] juegan un papel importante en la captura de la información local y secuencial de los documentos. El trabajo previo en esta área suele centrarse en documentos individuales, aunque existen múltiples documentos similares en muchos dominios. En este artículo, presentamos un novedoso método no supervisado para la detección de temas compartidos y la segmentación de temas de múltiples documentos similares basado en la información mutua (MI) y la información mutua ponderada (WMI), que es una combinación de MI y pesos de términos. La idea básica es que la segmentación óptima maximiza MI (o WMI). Nuestro enfoque puede detectar temas compartidos entre documentos. Puede encontrar los límites óptimos en un documento y alinear segmentos entre documentos al mismo tiempo. También puede manejar la segmentación de un solo documento como un caso especial de la segmentación y alineación de varios documentos. Nuestros métodos pueden identificar y fortalecer términos de señal que pueden ser utilizados para la segmentación y eliminar parcialmente las palabras de parada mediante el uso de pesos de términos basados en la entropía aprendida de múltiples documentos. Nuestros resultados experimentales muestran que nuestro algoritmo funciona bien para las tareas de segmentación de un solo documento, detección de temas compartidos y segmentación de múltiples documentos. El uso de información de múltiples documentos puede mejorar enormemente el rendimiento de la segmentación de temas, y utilizar WMI es aún mejor que utilizar MI para la segmentación de múltiples documentos. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información-Agrupación; H.3.1 [Almacenamiento y Recuperación de Información]: Análisis de Contenido e Indexación-Procesamiento Lingüístico; I.2.7 [Inteligencia Artificial]: Procesamiento de Lenguaje Natural-Análisis de Texto; I.5.3 [Reconocimiento de Patrones]: Algoritmos de Agrupación-Medidas de Similitud Términos Generales Algoritmos, Diseño, Experimentación 1. INTRODUCCIÓN Muchos investigadores han trabajado en la detección y seguimiento de temas (TDT) [26] y la segmentación de temas durante la última década. La segmentación de temas tiene como objetivo identificar los límites en un documento con la meta de capturar la estructura temática latente. Las tareas de segmentación de temas generalmente se dividen en dos categorías [15]: la segmentación de flujos de texto, donde se identifica la transición de temas, y la segmentación coherente de documentos, en la que los documentos se dividen en subtemas. La primera categoría tiene aplicaciones en el reconocimiento automático del habla, mientras que la segunda tiene más aplicaciones como la consulta parcial de texto en documentos extensos en la recuperación de información, resumen de texto y medición de calidad de múltiples documentos. La investigación previa en relación con TDT se enmarca en la primera categoría, centrada en el seguimiento de temas de datos de discurso de transmisión y texto de agencia de noticias, mientras que la segunda categoría no ha sido estudiada muy bien. Los enfoques tradicionales realizan la segmentación de temas en los documentos uno a la vez [15, 25, 6]. La mayoría de ellos tienen un mal desempeño en tareas sutiles como la segmentación coherente de documentos [15]. A menudo, los usuarios finales buscan documentos que tengan un contenido similar. Los motores de búsqueda, como Google, proporcionan enlaces para obtener páginas similares. A una granularidad más fina, los usuarios pueden estar buscando obtener secciones de un documento similares a una sección en particular que presumiblemente discute un tema de interés para los usuarios. Por lo tanto, la extensión de la segmentación de temas desde documentos individuales para identificar segmentos similares de múltiples documentos similares con el mismo tema es una dirección natural y necesaria, y se espera que la segmentación de temas de múltiples documentos tenga un mejor rendimiento ya que se utiliza más información. Los enfoques tradicionales que utilizan la medición de similitud basada en la frecuencia de términos generalmente parten del mismo supuesto de que un vocabulario similar tiende a estar en un segmento de tema coherente [15, 25, 6]. Sin embargo, suelen tener el problema de identificar las palabras de parada. Por ejemplo, se eliminan las palabras de parada dependientes del documento junto con las palabras de parada genéricas en [15]. Hay dos razones por las que no eliminamos las palabras de parada directamente. Primero, identificar las palabras vacías es otro problema [12] que requiere estimación en cada dominio. Eliminar palabras comunes de parada puede resultar en la pérdida de información útil en un dominio específico. Segundo, aunque se pueden identificar las palabras de parada, la clasificación estricta de palabras de parada y palabras no de parada no puede representar el cambio gradual en la cantidad de contenido informativo de cada palabra. Empleamos una clasificación suave utilizando pesos de términos. En este documento, consideramos el problema de la segmentación de temas como un problema de optimización utilizando técnicas de teoría de la información para encontrar los límites óptimos de un documento dado el número de segmentos de texto, de manera que se minimice la pérdida de información mutua (MI) (o información mutua ponderada (WMI)) después de la segmentación y alineación. Esto es igual a maximizar el MI (o WMI). El MI se centra en medir la diferencia entre segmentos, mientras que investigaciones anteriores se enfocaban en encontrar la similitud (por ejemplo, la distancia del coseno) entre segmentos [15, 25, 6]. La <br>alineación de temas</br> de múltiples documentos similares se puede lograr agrupando las oraciones sobre el mismo tema en el mismo clúster. La segmentación de temas en un solo documento es solo un caso especial del problema de segmentación y alineación de temas en múltiples documentos. Los términos pueden ser co-agrupados como en [10] al mismo tiempo, dado el número de agrupaciones, pero nuestros resultados experimentales muestran que este método resulta en una peor segmentación (ver Tablas 1, 4 y 6). Por lo general, los lectores humanos pueden identificar la transición de temas basándose en palabras de señal, y pueden ignorar las palabras vacías. Inspirados por esto, asignamos a cada término (o grupo de términos) un peso basado en la entropía entre diferentes documentos y diferentes segmentos de documentos. No solo este enfoque puede aumentar la contribución de las palabras clave, sino que también puede disminuir el efecto de las palabras de parada comunes, palabras ruidosas y palabras de parada dependientes del documento. Estas palabras son comunes en un documento. Muchos métodos basados en la similitud de oraciones requieren que estas palabras sean eliminadas antes de que se pueda realizar la segmentación de temas [15]. Nuestros resultados en la Figura 3 muestran que los pesos de los términos son útiles para la segmentación y alineación de temas en múltiples documentos. La principal contribución de este artículo es que introduce un método novedoso para la segmentación de temas utilizando MI y demuestra que este método funciona mejor que los criterios previamente utilizados. Además, hemos abordado el problema de la segmentación y alineación de temas en varios documentos, mientras que la mayoría de las investigaciones existentes se centraron en la segmentación de documentos individuales. La segmentación y alineación de múltiples documentos pueden utilizar información de documentos similares y mejorar significativamente el rendimiento de la segmentación de temas. Obviamente, nuestro enfoque puede manejar documentos individuales como un caso especial cuando varios documentos no están disponibles. Puede detectar temas compartidos entre documentos para determinar si son múltiples documentos sobre el mismo tema. También introducimos el nuevo criterio de WMI basado en pesos de términos aprendidos de múltiples documentos similares, lo cual puede mejorar aún más el rendimiento de la segmentación de temas. Proponemos un algoritmo voraz iterativo basado en programación dinámica y demostramos que funciona bien en la práctica. Algunos de nuestros trabajos anteriores están en [24]. El resto de este documento está organizado de la siguiente manera: En la Sección 2, revisamos el trabajo relacionado. La sección 3 contiene una formulación del problema de segmentación de temas y alineación de múltiples documentos con co-clustering de términos, una revisión del criterio de MI para el clustering, y finalmente una introducción a WMI. En la Sección 4, primero proponemos el algoritmo voraz iterativo de segmentación y alineación de temas con co-clustering de términos, y luego describimos cómo el algoritmo puede ser optimizado mediante programación dinámica. En la Sección 5, se describen experimentos sobre la segmentación de documentos individuales, la detección de temas compartidos y la segmentación de documentos múltiples, y se presentan y discuten los resultados para evaluar el rendimiento de nuestro algoritmo. Las conclusiones y algunas direcciones futuras del trabajo de investigación se discuten en la Sección 6.2. TRABAJO PREVIO En general, los enfoques existentes para la segmentación de texto se dividen en dos categorías: aprendizaje supervisado [19, 17, 23] y aprendizaje no supervisado [3, 27, 5, 6, 15, 25, 21]. El aprendizaje supervisado suele tener un buen rendimiento, ya que aprende funciones a partir de conjuntos de entrenamiento etiquetados. Sin embargo, obtener conjuntos de entrenamiento grandes con etiquetas manuales en oraciones de documentos suele ser prohibitivamente costoso, por lo que se prefieren enfoques no supervisados. Algunos modelos consideran la dependencia entre oraciones y secciones, como el Modelo Oculto de Markov [3, 27], el Modelo de Máxima Entropía de Markov [19] y los Campos Aleatorios Condicionales [17], mientras que muchos otros enfoques se basan en la cohesión léxica o la similitud de las oraciones [5, 6, 15, 25, 21]. Algunos enfoques también se centran en las palabras clave como pistas de transiciones de tema [11]. Si bien algunos métodos existentes solo consideran información en documentos individuales [6, 15], otros utilizan varios documentos [16, 14]. No hay muchas obras en la última categoría, aunque se espera que el rendimiento de la segmentación sea mejor con la utilización de información de múltiples documentos. Investigaciones previas estudiaron métodos para encontrar temas compartidos [16] y segmentación y resumen de temas entre solo un par de documentos [14]. La clasificación y agrupación de textos es un área de investigación relacionada que categoriza documentos en grupos utilizando métodos supervisados o no supervisados. La clasificación o agrupación temática es una dirección importante en esta área, especialmente el co-agrupamiento de documentos y términos, como LSA [9], PLSA [13], y enfoques basados en distancias y particionamiento de gráficos bipartitos [28] o máxima MI [2, 10], o máxima entropía [1, 18]. Los criterios de estos enfoques pueden ser utilizados en el tema de la segmentación de temas. Algunos de esos métodos se han extendido al área de la segmentación de temas, como PLSA [5] y máxima entropía [7], pero hasta donde sabemos, el uso de MI para la segmentación de temas no ha sido estudiado. 3. FORMULACIÓN DEL PROBLEMA Nuestro objetivo es segmentar documentos y alinear los segmentos entre documentos (Figura 1). Sea T el conjunto de términos {t1, t2, ..., tl}, que aparecen en el conjunto no etiquetado de documentos D = {d1, d2, ..., dm}. Sea Sd el conjunto de oraciones para el documento d ∈ D, es decir, {s1, s2, ..., snd}. Tenemos una matriz tridimensional de frecuencia de términos, en la que las tres dimensiones son variables aleatorias de D, Sd y T. Sd en realidad es un vector aleatorio que incluye una variable aleatoria para cada d ∈ D. La frecuencia de términos se puede utilizar para estimar la distribución de probabilidad conjunta P(D, Sd, T), que es p(t, d, s) = T(t, d, s)/ND, donde T(t, d, s) es el número de t en la oración s de d y ND es el número total de términos en D. ˆS representa el conjunto de segmentos {ˆs1, ˆs2, ..., ˆsp} después de la segmentación y alineación entre varios documentos, donde el número de segmentos | ˆS| = p. Un segmento ˆsi del documento d es una secuencia de oraciones adyacentes en d. Dado que para diferentes documentos si pueden discutir diferentes subtemas, nuestro objetivo es agrupar oraciones adyacentes en cada documento en segmentos, y alinear segmentos similares entre documentos, de modo que para diferentes documentos ˆsi trate sobre el mismo subtema. El objetivo es encontrar la segmentación óptima de temas y el mapeo de alineación Segd(si) : {s1, s2, ..., snd } → {ˆs1, ˆs2, ..., ˆsp} y Alid(ˆsi) : {ˆs1, ˆs2, ..., ˆsp} → {ˆs1, ˆs2, ..., ˆsp}, para todo d ∈ D, donde ˆsi es el i-ésimo segmento con la restricción de que solo las oraciones adyacentes pueden ser asignadas al mismo segmento, es decir, para d, {si, si+1, ..., sj} → {ˆsq}, donde q ∈ {1, ..., p}, donde p es el número de segmento, y si i > j, entonces para d, ˆsq está ausente. Después de la segmentación y alineación, el vector aleatorio Sd se convierte en una variable aleatoria alineada ˆS. Por lo tanto, P(D, Sd, T) se convierte en P(D, ˆS, T). El término co-clustering es una técnica que se ha empleado [10] para mejorar la precisión del agrupamiento de documentos. Evaluamos el efecto de ello para la segmentación de temas. Un término t se asigna a exactamente un grupo de términos. El término co-clustering implica encontrar simultáneamente el mapeo óptimo de agrupación de términos Clu(t) : {t1, t2, ..., tl} → {ˆt1, ˆt2, ..., ˆtk}, donde k ≤ l, l es el número total de palabras en todos los documentos, y k es el número de clústeres. 4. METODOLOGÍA Ahora describimos un algoritmo novedoso que puede manejar la segmentación de un solo documento, la detección de temas compartidos y la segmentación y alineación de múltiples documentos basados en MI o WMI. 4.1 Información Mutua MI I(X; Y) es una cantidad que mide la cantidad de información contenida en dos o más variables aleatorias [8, 10]. Para el caso de dos variables aleatorias, tenemos I(X; Y) = x∈X y∈Y p(x, y)log p(x, y) p(x)p(y) , (1) Obviamente, cuando las variables aleatorias X e Y son independientes, I(X; Y) = 0. Por lo tanto, de manera intuitiva, el valor de MI depende de cómo las variables aleatorias están relacionadas entre sí. La co-agrupación óptima es el mapeo Clux: X → ˆX y Cluy: Y → ˆY que minimiza la pérdida: I(X; Y) - I(ˆX; ˆY), lo cual es igual a maximizar I(ˆX; ˆY). Este es el criterio de MI para la agrupación. En el caso de la segmentación de temas, las dos variables aleatorias son la variable de término T y la variable de segmento S, y cada muestra es una ocurrencia de un término T = t en un segmento particular S = s. Se utiliza I(T; S) para medir qué tan dependientes son T y S. Sin embargo, no se puede calcular I(T; S) para documentos antes de la segmentación, ya que no tenemos un conjunto de S debido a que las oraciones del Documento d, si ∈ Sd, no están alineadas con otros documentos. Así, en lugar de minimizar la pérdida de MI, podemos maximizar MI después de la segmentación de temas, calculada como: I( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS p(ˆt, ˆs)log p(ˆt, ˆs) p(ˆt)p(ˆs) , (2) donde p(ˆt, ˆs) se estiman mediante la frecuencia de términos tf del Cluster de Términos ˆt y el Segmento ˆs en el conjunto de entrenamiento D. Nótese que aquí un segmento ˆs incluye oraciones sobre el mismo tema entre todos los documentos. La solución óptima es el mapeo Clut : T → ˆT, Segd : Sd → ˆS, y Alid : ˆS → ˆS, que maximiza I( ˆT; ˆS). 4.2 Información Mutua Ponderada En la segmentación de temas y alineación de múltiples documentos, si se conoce P(D, ˆS, T), basado en las distribuciones marginales P(D|T) y P( ˆS|T) para cada término t ∈ T, podemos categorizar los términos en cuatro tipos en el conjunto de datos: • Las palabras de parada comunes son comunes a lo largo de las dimensiones de documentos y segmentos. • Las palabras de parada dependientes del documento que dependen del estilo de escritura personal son comunes solo a lo largo de la dimensión de segmentos para algunos documentos. • Las palabras clave son los elementos más importantes para la segmentación. Son comunes a lo largo de la dimensión de los documentos solo para el mismo segmento, y no son comunes a lo largo de las dimensiones de los segmentos. • Las palabras ruidosas son otras palabras que no son comunes a lo largo de ambas dimensiones. La entropía basada en P(D|T) y P( ˆS|T) se puede utilizar para identificar diferentes tipos de términos. Para reforzar la contribución de las palabras clave en el cálculo de MI, y al mismo tiempo reducir el efecto de los otros tres tipos de palabras, similar a la idea del peso tf-idf [22], utilizamos las entropías de cada término a lo largo de las dimensiones del documento D y del segmento ˆS, es decir, ED(ˆt) y EˆS(ˆt), para calcular el peso. Una palabra de señal generalmente tiene un valor alto de ED(ˆt) pero un valor bajo de EˆS(ˆt). Introducimos los pesos de términos (o pesos de grupos de términos) wˆt = ( ED(ˆt) maxˆt ∈ ˆT (ED(ˆt )) )a (1 − EˆS(ˆt) maxˆt ∈ ˆT (EˆS(ˆt )) )b , (3) donde ED(ˆt) = d∈D p(d|ˆt)log|D| 1 p(d|ˆt) , EˆS(ˆt) = ˆs∈ ˆS p(ˆs|ˆt)log| ˆS| 1 p(ˆs|ˆt) , y a > 0 y b > 0 son potencias para ajustar los pesos de términos. Normalmente a = 1 y b = 1 por defecto, y se utilizan maxˆt ∈ ˆT (ED(ˆt )) y maxˆt ∈ ˆT (EˆS(ˆt )) para normalizar los valores de entropía. Los pesos de los grupos de términos se utilizan para ajustar p(ˆt, ˆs), pw(ˆt, ˆs) = wˆtp(ˆt, ˆs) ˆt∈ ˆT ;ˆs∈ ˆS wˆtp(ˆt, ˆs) , (4) e Iw( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS pw(ˆt, ˆs)log pw(ˆt, ˆs) pw(ˆt)pw(ˆs) , (5) donde pw(ˆt) y pw(ˆs) son distribuciones marginales de pw(ˆt, ˆs). Sin embargo, dado que no conocemos ni los pesos de los términos ni P(D, ˆS, T), necesitamos estimarlos, pero wˆt depende de p(ˆs|t) y ˆS, mientras que ˆS y p(ˆs|t) también dependen de wˆt, que aún es desconocido. Por lo tanto, se requiere un algoritmo iterativo para estimar los pesos de los términos wˆt y encontrar la mejor segmentación y alineación para optimizar la función objetivo Iw de manera concurrente. Después de que un documento se segmenta en oraciones, se introduce: la distribución de probabilidad conjunta P(D, Sd, T), el número de segmentos de texto p ∈ {2, 3, ..., máx(sd)}, el número de grupos de términos k ∈ {2, 3, ..., l} (si k = l, no se requiere co-clustering de términos) y el tipo de peso w ∈ {0, 1}, indicando usar I o Iw, respectivamente. Mapeo de Clu, Seg, Ali y pesos de términos wˆt. Inicialización: 0. i = 0. Inicializar Clu (0) t, Seg (0) d y Ali (0) d; Inicializar w (0) ˆt usando la Ecuación (6) si w = 1; Etapa 1: 1. Si |D| = 1, k = l y w = 0, verifica todas las segmentaciones secuenciales de d en p segmentos y encuentra la mejor Segd(s) = argmaxˆsI( ˆT; ˆS), y devuelve Segd; de lo contrario, si w = 1 y k = l, ve a 3.1; Etapa 2: 2.1 Si k < l, para cada término t, encuentra el mejor clúster ˆt como Clu(i+1)(t) = argmaxˆtI( ˆT; ˆS(i)) basado en Seg(i) y Ali(i); 2.2 Para cada d, verifica todas las segmentaciones secuenciales de d en p segmentos con mapeo s → ˆs → ˆs, y encuentra la mejor Ali (i+1) d (Seg (i+1) d (s)) = argmaxˆsI( ˆT(i+1); ˆS) basado en Clu(i+1)(t) si k < l o Clu(0)(t) si k = l; 2.3 i + +. Si Clu, Seg o Ali cambian, ve a 2.1; de lo contrario, si w = 0, devuelve Clu(i), Seg(i) y Ali(i); de lo contrario, j = 0, ve a 3.1; Etapa 3: 3.1 Actualiza w (i+j+1) ˆt basado en Seg(i+j), Ali(i+j) y Clu(i) usando la Ecuación (3); 3.2 Para cada d, verifica todas las segmentaciones secuenciales de d en p segmentos con mapeo s → ˆs → ˆs, y encuentra la mejor Ali (i+j+1) d (Seg (i+j+1) d (s)) = argmaxˆsIw( ˆT(i); ˆS) basado en Clu(i) y w (i+j+1) ˆt; 3.3 j + +. Si Iw( ˆT; ˆS) cambia, ve al paso 6; de lo contrario, detente y devuelve Clu(i), Seg(i+j), Ali(i+j), y w (i+j) ˆt; Figura 2: Algoritmo: Segmentación y alineación de temas basada en MI o WMI. y cada oración se segmenta en palabras, cada palabra se reduce a su raíz. Entonces se puede estimar la distribución de probabilidad conjunta P(D, Sd, T). Finalmente, esta distribución se puede utilizar para calcular la MI en nuestro algoritmo. 4.3 Algoritmo Voraz Iterativo Nuestro objetivo es maximizar la función objetivo, I( ˆT; ˆS) o Iw( ˆT; ˆS), que puede medir la dependencia de las ocurrencias de términos en diferentes segmentos. Generalmente, primero no conocemos los pesos estimados de los términos, los cuales dependen de la segmentación y alineación óptimas de los temas, y los grupos de términos. Además, este problema es NP-duro [10], incluso si conocemos los pesos de los términos. Por lo tanto, se desea un algoritmo voraz iterativo para encontrar la mejor solución, aunque probablemente solo se alcancen máximos locales. Presentamos el algoritmo voraz iterativo en la Figura 2 para encontrar un máximo local de I( ˆT; ˆS) o Iw( ˆT; ˆS) con estimación simultánea de pesos de términos. Este algoritmo es iterativo y codicioso para casos de múltiples documentos o casos de un solo documento con estimación de peso de términos y/o co-clustering de términos. De lo contrario, dado que es solo un algoritmo de un paso para resolver la tarea de segmentación de un solo documento [6, 15, 25], se garantiza el máximo global de MI. Mostraremos más adelante que la coagrupación de términos reduce la precisión de los resultados y no es necesaria, y para la segmentación de un solo documento, los pesos de los términos tampoco son requeridos. 4.3.1 Inicialización En el Paso 0, la agrupación inicial de términos Clut y la segmentación y alineación de temas Segd y Alid son importantes para evitar máximos locales y reducir el número de iteraciones. Primero, se puede hacer una buena estimación de los pesos de los términos utilizando las distribuciones de frecuencia de términos a lo largo de las oraciones para cada documento y promediándolas para obtener los valores iniciales de wˆt: wt = ( ED(t) maxt ∈T (ED(t )) )(1 − ES(t) maxt ∈T (ES(t )) ), (6) donde ES(t) = 1 |Dt| d∈Dt (1 − s∈Sd p(s|t)log|Sd| 1 p(s|t) ), donde Dt es el conjunto de documentos que contienen el término t. Luego, para la segmentación inicial Seg(0), simplemente podemos segmentar los documentos de manera equitativa por oraciones. O podemos encontrar la segmentación óptima solo para cada documento d que maximice el WMI, Seg (0) d = argmaxˆsIw(T; ˆS), donde w = w (0) ˆt. Para el alineamiento inicial Ali(0), podemos primero asumir que el orden de los segmentos para cada d es el mismo. Para el agrupamiento inicial de términos Clu(0), las etiquetas de los primeros grupos pueden establecerse al azar, y después de la primera vez del Paso 3, se obtiene un buen agrupamiento inicial de términos. 4.3.2 Diferentes Casos Después de la inicialización, hay tres etapas para diferentes casos. En total hay ocho casos, |D| = 1 o |D| > 1, k = l o k < l, w = 0 o w = 1. La segmentación de un solo documento sin agrupamiento de términos y estimación de peso de términos (|D| = 1, k = l, w = 0) solo requiere la Etapa 1 (Paso 1). Si se requiere el agrupamiento de términos (k < l), la Etapa 2 (Paso 2.1, 2.2 y 2.3) se ejecuta de forma iterativa. Si se requiere la estimación del peso del término (w = 1), la Etapa 3 (Paso 3.1, 3.2 y 3.3) se ejecuta de forma iterativa. Si ambos son requeridos (k < l, w = 1), la Etapa 2 y 3 se ejecutan una tras otra. Para la segmentación de múltiples documentos sin agrupamiento de términos y estimación de peso de términos (|D| > 1, k = l, w = 0), solo se requiere la iteración de los Pasos 2.2 y 2.3. En la Etapa 1, el máximo global se puede encontrar basándose en I( ˆT; ˆS) utilizando programación dinámica en la Sección 4.4. Es imposible encontrar simultáneamente un buen agrupamiento de términos y pesos estimados de términos, ya que al mover un término a un nuevo grupo de términos para maximizar Iw( ˆT; ˆS), no sabemos si el peso de este término debería ser el del nuevo grupo o el del antiguo. Por lo tanto, primero realizamos el agrupamiento de términos en la Etapa 2, y luego estimamos los pesos de los términos en la Etapa 3. En la Etapa 2, el Paso 2.1 consiste en encontrar el mejor agrupamiento de términos y el Paso 2.2 consiste en encontrar la mejor segmentación. Este ciclo se repite para encontrar un máximo local basado en MI I hasta que converge. Los dos pasos son: (1) basados en el agrupamiento de términos actual Cluˆt, para cada documento d, el algoritmo segmenta todas las oraciones Sd en p segmentos secuencialmente (algunos segmentos pueden estar vacíos), y los coloca en los p segmentos ˆS del conjunto de entrenamiento completo D (se verifican todos los casos posibles de segmentación diferente Segd y alineación Alid) para encontrar el caso óptimo, y (2) basado en la segmentación y alineación actual, para cada término t, el algoritmo encuentra el mejor grupo de términos de t basado en la segmentación actual Segd y la alineación Alid. Después de encontrar un buen agrupamiento de términos, se estiman los pesos de los términos si w = 1. En la Etapa 3, al igual que en la Etapa 2, el Paso 3.1 es la reestimación del peso del término y el Paso 3.2 es encontrar una mejor segmentación. Se repiten para encontrar un máximo local basado en WMI Iw hasta que converja. Sin embargo, si el agrupamiento en la Etapa 2 no es preciso, entonces la estimación de peso en la Etapa 3 puede tener un mal resultado. Finalmente, en el Paso 3.3, este algoritmo converge y devuelve la salida. Este algoritmo puede manejar tanto la segmentación de un solo documento como la de múltiples documentos. También puede detectar temas compartidos entre documentos al verificar la proporción de frases superpuestas sobre los mismos temas, como se describe en la Sección 5.2. 4.4 Optimización del algoritmo. En muchos trabajos anteriores sobre segmentación, la programación dinámica es una técnica utilizada para maximizar la función objetivo. De manera similar, en los Pasos 1, 2.2 y 3.2 de nuestro algoritmo, podemos utilizar programación dinámica. Para la Etapa 1, utilizando programación dinámica aún se puede encontrar el óptimo global, pero para la Etapa 2 y la Etapa 3, solo podemos encontrar el óptimo para cada paso de segmentación de temas y alineación de un documento. Aquí solo mostramos la programación dinámica para el Paso 3.2 utilizando WMI (el Paso 1 y 2.2 son similares pero pueden usar I o Iw). Hay dos casos que no se muestran en el algoritmo de la Figura 2: (a) segmentación de un solo documento o segmentación de múltiples documentos con el mismo orden secuencial de segmentos, donde no se requiere alineación, y (b) segmentación de múltiples documentos con diferentes órdenes secuenciales de segmentos, donde la alineación es necesaria. La función de mapeo de alineación del primer caso es simplemente Alid(ˆsi) = ˆsi, mientras que para los últimos casos la función de mapeo de alineación es Alid(ˆsi) = ˆsj, donde i y j pueden ser diferentes. Los pasos computacionales para los dos casos se enumeran a continuación: Caso 1 (sin alineación): Para cada documento d: (1) Calcular pw(ˆt), pw parcial(ˆt, ˆs) y pw parcial(ˆs) sin contar las oraciones de d. Luego, colocar las oraciones de i a j en la Parte k, y calcular WMI parcial PIw( ˆT; ˆsk(si, si+1, ..., sj)) ˆt∈ ˆT pw(ˆt, ˆsk)log pw(ˆt, ˆsk) pw(ˆt)pw(ˆsk) , donde Alid(si, si+1, ..., sj) = k, k ∈ {1, 2, ..., p}, 1 ≤ i ≤ j ≤ nd, y Segd(sq) = ˆsk para todo i ≤ q ≤ j. (2) Dejar que M(sm, 1) = PIw( ˆT; ˆs1(s1, s2, ..., sm)). Entonces M(sm, L) = maxi[M(si−1, L − 1) + PIw( ˆT; ˆsL(si, ..., sm))], donde 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, y cuando i > m, no se colocan oraciones en ˆsk al calcular PIw (nota PIw( ˆT; ˆs(si, ..., sm)) = 0 para la segmentación de un solo documento). (3) Finalmente M(snd , p) = maxi[M(si−1, p − 1)+ PIw( ˆT; ˆsp(si, ..., snd ))], donde 1 ≤ i ≤ nd+1. Se encuentra el Iw óptimo y la segmentación correspondiente es la mejor. Caso 2 (alineación requerida): Para cada documento d: (1) Calcular pw(ˆt), pw parcial(ˆt, ˆs), y pw parcial(ˆs), y PIw( ˆT; ˆsk(si, si+1, ..., sj)) de manera similar al Caso 1. (2) Sea M(sm, 1, k) = PIw( ˆT; ˆsk(s1, s2, ..., sm)), donde k ∈ {1, 2, ..., p}. Entonces M(sm, L, kL) = maxi,j[M(si−1, L − 1, kL/j) + PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., sm))], donde 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, kL ∈ Set(p, L), que es el conjunto de todos los p! Hay L!(p−L)! combinaciones de L segmentos elegidos de entre todos los p segmentos, j ∈ kL, el conjunto de L segmentos elegidos de entre todos los p segmentos, y kL/j es la combinación de L − 1 segmentos en kL excepto el Segmento j. (3) Finalmente, M(snd , p, kp) = maxi,j[M(si−1, p − 1, kp/j) +PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., snd ))], donde kp es simplemente la combinación de todos los p segmentos y 1 ≤ i ≤ nd + 1, que es el óptimo Iw y la segmentación correspondiente es la mejor. Los pasos de los Casos 1 y 2 son similares, excepto que en el Caso 2, se considera el alineamiento además de la segmentación. Primero, se calculan los elementos básicos de probabilidad para computar Iw excluyendo el documento d, y luego se calcula el WMI parcial colocando cada segmento secuencial posible (incluido el segmento vacío) de d en cada segmento del conjunto. Segundo, se encuentra la suma óptima de PIw para L segmentos y las m oraciones más a la izquierda, M(sm, L). Finalmente, se encuentra el WMI máximo entre diferentes sumas de M(sm, p − 1) y PIw para el Segmento p. 5. EXPERIMENTOS En esta sección se probará la segmentación de un solo documento, la detección de temas compartidos y la segmentación de múltiples documentos. Se estudian diferentes hiperparámetros de nuestro método. Para mayor comodidad, nos referimos al método utilizando I como MIk si w = 0, e Iw como WMIk si w = 2 o como WMIk si w = 1, donde k es el número de grupos de términos, y si k = l, donde l es el número total de términos, entonces no se requiere agrupación de términos, es decir, El primer conjunto de datos que probamos es uno sintético utilizado en investigaciones anteriores [6, 15, 25] y muchos otros artículos. Tiene 700 muestras. Cada uno es una concatenación de diez segmentos. Cada segmento es la primera oración seleccionada al azar de las n primeras oraciones del corpus Brown, que se supone que tienen un tema diferente entre sí. Actualmente, los mejores resultados en este conjunto de datos son logrados por Ji et al. [15]. Para comparar el rendimiento de nuestros métodos, se aplica el criterio ampliamente utilizado en investigaciones anteriores en lugar del criterio imparcial introducido en [20]. Elige un par de palabras al azar. Si se encuentran en segmentos diferentes (diferentes) para la segmentación real (real), pero se predicen (pred) como en el mismo segmento, es un error. Si están en el mismo segmento (mismo), pero se predice que están en segmentos diferentes, es una falsa alarma. Por lo tanto, la tasa de error se calcula utilizando la siguiente ecuación: p(err|real, pred) = p(miss|real, pred, diff)p(diff|real) + p(false alarm|real, pred, same)p(same|real). 5.1.2 Resultados del Experimento Probamos el caso cuando se conoce el número de segmentos. La Tabla 1 muestra los resultados de nuestros métodos con diferentes valores de hiperparámetros y tres enfoques anteriores, C99[25], U00[6] y ADDP03[15], en este conjunto de datos cuando se conoce el número de segmento. En WMI para la segmentación de un solo documento, los pesos de los términos se calculan de la siguiente manera: wˆt = 1−EˆS(ˆt)/maxˆt ∈ ˆT (EˆS(ˆt)). Para este caso, nuestros métodos MIl y WMIl superan a todos los enfoques anteriores. Comparamos nuestros métodos con ADDP03 utilizando una prueba t de una muestra unilateral y los valores de p se muestran en la Tabla 2. A partir de los valores p, podemos ver que en su mayoría las diferencias son muy significativas. También comparamos las tasas de error entre nuestros dos métodos utilizando una prueba t de dos muestras de dos colas para verificar la hipótesis de que son iguales. No podemos rechazar la hipótesis de que son iguales, por lo que las diferencias no son significativas, a pesar de que todas las tasas de error para MIl son más pequeñas que las de WMIl. Sin embargo, podemos concluir que los pesos de los términos contribuyen poco en la segmentación de un solo documento. Los resultados también muestran que el uso de MI con co-clustering de términos (k = 100) disminuye el rendimiento. Probamos diferentes números de grupos de términos y encontramos que el rendimiento mejora cuando el número de grupos aumenta hasta llegar a l. WMIk<l tiene resultados similares que no mostramos en la tabla. Como se mencionó anteriormente, el uso de MI puede ser inconsistente en los límites óptimos dados diferentes números de segmentos. Esta situación ocurre especialmente cuando las similitudes entre los segmentos son bastante diferentes, es decir, algunas transiciones son muy obvias, mientras que otras no lo son. Esto se debe a que normalmente un documento es una estructura jerárquica en lugar de una estructura únicamente secuencial. Cuando los segmentos no están en el mismo nivel, esta situación puede ocurrir. Por lo tanto, se desea un enfoque de segmentación jerárquica de temas, y la estructura depende en gran medida del número de segmentos para cada nodo interno y del criterio de parada de la división. Para este conjunto de datos de segmentación de un solo documento, dado que es solo un conjunto sintético, que es simplemente una concatenación de varios segmentos sobre diferentes temas, es razonable que enfoques simplemente basados en la frecuencia de términos tengan un buen rendimiento. Normalmente, para las tareas de segmentación de documentos coherentes en subtemas, la efectividad disminuye mucho. 5.2 Detección de Temas Compartidos 5.2.1 Datos de Prueba y Evaluación El segundo conjunto de datos contiene 80 artículos de noticias de Google News. Hay ocho temas y cada uno tiene 10 artículos. Dividimos aleatoriamente el conjunto en subconjuntos con diferentes números de documentos y cada subconjunto tiene los ocho temas. Comparamos nuestro enfoque MIl y WMIl con LDA [4]. LDA trata un documento en el conjunto de datos como una bolsa de palabras, encuentra su distribución en temas y su tema principal. MIl y WMIl ven cada oración como un conjunto de palabras y la etiquetan con una etiqueta de tema. Luego, para cada par de documentos, LDA determina si están en el mismo tema, mientras que MIl y Table 3: Detección de Temas Compartidos: Tasas de Error Promedio para Diferentes Números de Documentos en Cada Subconjunto #Doc 10 20 40 80 LDA 8.89% 16.33% 1.35% 0.60% MIl, θ = 0.6 4.17% 1.71% 1.47% 0.0% WMIl, θ = 0.8 18.6% 3.16% 1.92% 0.0% WMIl comprueba si la proporción de oraciones superpuestas en el mismo tema es mayor que el umbral ajustable θ. Es decir, en MIl y WMIl, para un par de documentos d, d , si [ s∈Sd,s ∈Sd 1(temas=temas )/min(|Sd|, |Sd|)] > θ, donde Sd es el conjunto de oraciones de d, y |Sd| es el número de oraciones de d, entonces d y d comparten el tema. Para un par de documentos seleccionados al azar, la tasa de error se calcula utilizando la siguiente ecuación: p(err|real, pred) = p(miss|real, pred, same)p(same|real) + p(false alarm|real, pred, diff)p(diff|real), donde un error significa si tienen el mismo tema (same) para el caso real (real), pero se predice (pred) como en un tema diferente. Si están en diferentes temas (diff), pero se predicen como en el mismo tema, es una falsa alarma. 5.2.2 Resultados del Experimento Los resultados se muestran en la Tabla 3. Si la mayoría de los documentos tienen diferentes temas, en WMIl, la estimación de los pesos de términos en la Ecuación (3) no es correcta. Por lo tanto, no se espera que WMIL tenga un mejor rendimiento que MIL cuando la mayoría de los documentos tienen temas diferentes. Cuando hay menos documentos en un subconjunto con el mismo número de temas, más documentos tienen temas diferentes, por lo que WMIl es peor que MIl. Podemos ver que en la mayoría de los casos, MIl tiene un rendimiento mejor (o al menos similar) que LDA. Después de la detección de temas compartidos, se puede llevar a cabo la segmentación de documentos con los temas compartidos. 5.3 Segmentación de múltiples documentos 5.3.1 Datos de prueba y evaluación Para la segmentación y alineación de múltiples documentos, nuestro objetivo es identificar estos segmentos sobre el mismo tema entre varios documentos similares con temas compartidos. Se espera que al utilizar pesos de términos, Iw funcione mejor que I, ya que sin pesos de términos, el resultado se ve seriamente afectado por palabras de parada dependientes del documento y palabras ruidosas que dependen del estilo de escritura personal. Es más probable tratar los mismos segmentos de diferentes documentos como segmentos diferentes bajo el efecto de palabras de parada dependientes del documento y palabras ruidosas. Los pesos de términos pueden reducir el efecto de las palabras de parada dependientes del documento y las palabras ruidosas al darle más peso a los términos de señal. El conjunto de datos para la segmentación y alineación de múltiples documentos tiene un total de 102 muestras y 2264 oraciones. Cada uno es la parte de introducción de un informe de laboratorio seleccionado del curso de Biol 240W, Universidad Estatal de Pensilvania. Cada muestra tiene dos segmentos, la introducción de las hormonas vegetales y el contenido en el laboratorio. El rango de longitud de las muestras va desde dos hasta 56 oraciones. Algunas muestras solo tienen una parte y algunas tienen un orden inverso de estos dos segmentos. No es difícil identificar el límite entre dos segmentos para los humanos. Etiquetamos cada oración manualmente para evaluación. El criterio de evaluación consiste en utilizar la proporción del número de oraciones con etiquetas de segmento incorrectas predichas en el número total de oraciones en toda la Tabla de entrenamiento. Para mostrar los beneficios de la segmentación y alineación de múltiples documentos, comparamos nuestro método con diferentes parámetros en diferentes particiones del mismo conjunto de entrenamiento. Excepto los casos en los que el número de documentos es 102 y uno (que son casos especiales de uso del conjunto completo y la segmentación pura de un solo documento), dividimos aleatoriamente el conjunto de entrenamiento en m particiones, y cada una tiene 51, 34, 20, 10, 5 y 2 muestras de documentos. Luego aplicamos nuestros métodos en cada partición y calculamos la tasa de error de todo el conjunto de entrenamiento. Cada caso se repitió 10 veces para calcular las tasas de error promedio. Para diferentes particiones del conjunto de entrenamiento, se utilizan diferentes valores de k, ya que el número de términos aumenta cuando el número de documentos en cada partición aumenta. 5.3.2 Resultados del Experimento De los resultados del experimento en la Tabla 4, podemos ver las siguientes observaciones: (1) Cuando el número de documentos aumenta, todos los métodos tienen un mejor rendimiento. Solo de uno a dos documentos, el MIL ha disminuido un poco. Podemos observar esto en la Figura 3 en el punto del número de documento = 2. La mayoría de las curvas incluso tienen los peores resultados en este punto. Hay dos razones. Primero, porque las muestras votan por la mejor segmentación y alineación de múltiples documentos, pero si solo se comparan dos documentos entre sí, aquel con segmentos faltantes o una secuencia totalmente diferente afectará la correcta segmentación y alineación del otro. Segundo, como se señaló al principio de esta sección, si dos documentos tienen más palabras de parada dependientes del documento o palabras ruidosas que palabras clave, entonces el algoritmo puede considerarlos como dos segmentos diferentes y faltar el otro segmento. Generalmente, solo podemos esperar un mejor rendimiento cuando el número de documentos es mayor que el número de segmentos. (2) Excepto en la segmentación de un solo documento, WMIl siempre es mejor que MIl, y cuando el número de documentos se acerca a uno o aumenta a un número muy grande, sus rendimientos se vuelven más cercanos. La Tabla 5 muestra los valores p de la prueba t de dos muestras unilaterales entre MIl y WMIl. También podemos observar esta tendencia a partir de los valores de p. Cuando el número de documento es igual a 5, alcanzamos el valor p más pequeño y la mayor diferencia entre las tasas de error de MIl y WMIl. Para la Tabla 6 de un solo documento: Segmentación de múltiples documentos: Tasa de error promedio para el Documento Número = 5 en cada subconjunto con Diferente Número de Agrupaciones de Términos #Agrupaciones 75 100 150 250 l MIk 24.67% 24.54% 23.91% 22.59% 15.77% de segmentación, WMIl es incluso un poco peor que MIl, lo cual es similar a los resultados de la segmentación de un solo documento en el primer conjunto de datos. La razón es que para la segmentación de un solo documento, no podemos estimar con precisión los pesos de los términos, ya que no tenemos disponibles varios documentos. (3) El uso de agrupamiento de términos generalmente produce resultados peores que MIl y WMIl. (4) El uso de agrupamiento de términos en WMIk es aún peor que en MIk, ya que en WMIk los grupos de términos se encuentran primero utilizando I antes que Iw. Si los grupos de términos no son correctos, entonces los pesos de los términos se estiman peor, lo que puede llevar a que el algoritmo obtenga resultados aún peores. De los resultados también encontramos que en la segmentación y alineación de múltiples documentos, la mayoría de los documentos con segmentos faltantes y en orden inverso son identificados correctamente. La Tabla 6 ilustra los resultados del experimento para el caso de 20 particiones (cada una con cinco muestras de documentos) del conjunto de entrenamiento y la segmentación y alineación de temas utilizando MIk con diferentes números de grupos de términos k. Observa que cuando el número de grupos de términos aumenta, la tasa de error disminuye. Sin agrupamiento de términos, obtenemos el mejor resultado. No mostramos resultados para WMIk con agrupamiento de términos, pero los resultados son similares. También probamos WMIL con diferentes hiperparámetros de a y b para ajustar los pesos de los términos. Los resultados se presentan en la Figura 3. Se demostró que el caso predeterminado WMIl: a = 1, b = 1 dio los mejores resultados para diferentes particiones del conjunto de entrenamiento. Podemos observar la tendencia de que cuando el número de documentos es muy pequeño o grande, la diferencia entre MIl: a = 0, b = 0 y WMIl: a = 1, b = 1 se vuelve bastante pequeña. Cuando el número de documentos no es grande (aproximadamente de 2 a 10), todos los casos que utilizan pesos de términos tienen un mejor rendimiento que MIl: a = 0, b = 0 sin pesos de términos, pero cuando el número de documentos aumenta, los casos WMIl: a = 1, b = 0 y WMIl: a = 2, b = 1 empeoran en comparación con MIl: a = 0, b = 0. Cuando el número de documentos se vuelve muy grande, son aún peores que los casos con números de documentos pequeños. Esto significa que es muy importante encontrar una forma adecuada de estimar los pesos de los términos para el criterio de WMI. La Figura 4 muestra los pesos de los términos aprendidos de todo el conjunto de entrenamiento. Cuatro tipos de palabras se categorizan de manera aproximada aunque la transición entre ellas es sutil. La Figura 5 ilustra el cambio en la información mutua (ponderada) para MIl y WMIl. Como era de esperar, la información mutua para MIl aumenta de forma monótona con el número de pasos, mientras que WMIl no lo hace. Finalmente, MIl y WMIl son escalables, con complejidad computacional mostrada en la Figura 6. Una ventaja de nuestro enfoque basado en MI es que no es necesario eliminar las palabras vacías. Otra ventaja importante es que no hay hiperparámetros necesarios que ajustar. En la segmentación de un solo documento, el rendimiento basado en MI es aún mejor que el basado en WMI, por lo que no se requiere un hiperparámetro adicional. En la segmentación de múltiples documentos, mostramos en el experimento que a = 1 y b = 1 es lo mejor. Nuestro método otorga más peso a los términos de señal. Sin embargo, generalmente los términos o frases de señal aparecen al principio de un segmento, mientras que el final del segmento puede ser 1 2 5 10 20 34 51 102 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 Número de Documento Tasa de Error MIl :a=0,b=0 WMI l :a=1,b=1 WMI l :a=1,b=0 WMI l :a=2,b=1 Figura 3: Tasas de error para diferentes hiperparámetros de pesos de términos. 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Entropía de Documento Normalizada Entropía de Segmento Normalizada Palabras ruidosas Palabras de señal Palabras de parada comunes Palabras de parada de Doc-dep Figura 4: Pesos de términos aprendidos de todo el conjunto de entrenamiento. 0 100 200 300 400 500 600 0.06 0.08 0.1 0.12 0.14 0.16 0.18 Número de Pasos (Ponderado) Información Mutua MI l WMI l Figura 5: Cambio en la MI (ponderada) para MIl y WMIl. 0 20 40 60 80 100 120 0 200 400 600 800 1000 1200 1400 1600 1800 2000 Número de Documento Tiempo de Convergencia (seg) MI l WMI l Figura 6: Tiempo de convergencia para MIl y WMIl. muy ruidoso. Una posible solución es dar más peso a los términos al principio de cada segmento. Además, cuando la longitud de los segmentos es bastante diferente, los segmentos largos tienen frecuencias de términos mucho más altas, por lo que pueden dominar los límites de segmentación. La normalización de las frecuencias de términos en relación con la longitud del segmento puede ser útil. 6. CONCLUSIONES Y TRABAJO FUTURO Propusimos un método novedoso para la segmentación y alineación de temas en múltiples documentos basado en información mutua ponderada, que también puede manejar casos de un solo documento. Utilizamos programación dinámica para optimizar nuestro algoritmo. Nuestro enfoque supera a todos los métodos anteriores en casos de un solo documento. Además, también demostramos que realizar segmentación entre varios documentos puede mejorar el rendimiento enormemente. Nuestros resultados también demostraron que el uso de la información mutua ponderada puede aprovechar la información de varios documentos para lograr un mejor rendimiento. Solo probamos nuestro método en conjuntos de datos limitados. Se deben probar más conjuntos de datos, especialmente los complicados. Deberían compararse más métodos anteriores. Además, las segmentaciones naturales como los párrafos son pistas que se pueden utilizar para encontrar los límites óptimos. El aprendizaje supervisado también puede ser considerado. 7. AGRADECIMIENTOS Los autores desean agradecer a Xiang Ji y al Prof. J. Scott Payne por su ayuda. 8. REFERENCIAS [1] A. Banerjee, I. Ghillon, J. Ghosh, S. Merugu y D. Modha. Un enfoque generalizado de entropía máxima para la coagrupación de Bregman y la aproximación de matrices. En Actas de SIGKDD, 2004. [2] R. Bekkerman, R. El-Yaniv y A. McCallum. Agrupación distribucional de múltiples vías a través de interacciones por pares. En Actas de ICML, 2005. [3] D. M. Blei y P. J. Moreno. Segmentación de temas con un modelo oculto de Markov de aspectos. En Actas de SIGIR, 2001. [4] D. M. Blei, A. Ng y M. Jordan. Asignación latente de Dirichlet. Revista de Investigación en Aprendizaje Automático, 3:993-1022, 2003. [5] T. Brants, F. Chen e I. Tsochantaridis. Segmentación de documentos basada en temas con análisis semántico latente probabilístico. En Actas de CIKM, 2002. [6] F. Choi. Avances en la segmentación lineal de texto independiente del dominio. En Actas de la NAACL, 2000. [7] H. Christensen, B. Kolluru, Y. Gotoh y S. Renals. Segmentación de máxima entropía de noticias de transmisión. En Actas de ICASSP, 2005. [8] T. Cover y J. Thomas. Elementos de la teoría de la información. John Wiley and Sons, Nueva York, EE. UU., 1991. [9] S. Deerwester, S. Dumais, G. Furnas, T. Landauer y R. Harshman. Indexación mediante análisis semántico latente. Revista de la Sociedad Americana de Sistemas de Información, 1990. [10] I. Dhillon, S. Mallela y D. Modha. Co-clustering informativo-teórico. En Actas de SIGKDD, 2003. [11] M. Hajime, H. Takeo y O. Manabu. Segmentación de texto con múltiples señales lingüísticas superficiales. En Actas de COLING-ACL, 1998. [12] T. K. Ho. Detención de palabras y identificación para el reconocimiento de texto adaptativo. Revista Internacional de Análisis y Reconocimiento de Documentos, 3(1), agosto de 2000. [13] T. Hofmann. Análisis semántico latente probabilístico. En Actas de la UAI99, 1999. [14] X. Ji y H. Zha. Correlación de la sumarización de un par de documentos multilingües. En Actas de RIDE, 2003. [15] X. Ji y H. Zha. Segmentación de texto independiente del dominio utilizando difusión anisotrópica y programación dinámica. En Actas de SIGIR, 2003. [16] X. Ji y H. Zha. Extrayendo temas compartidos de múltiples documentos. En Actas de la 7ª PAKDD, 2003. [17] J. Lafferty, A. McCallum y F. Pereira. Campos aleatorios condicionales: Modelos probabilísticos para segmentar y etiquetar datos de secuencias. En Actas de ICML, 2001. [18] T. Li, S. Ma y M. Ogihara. Criterio basado en la entropía en la agrupación categórica. En Actas de ICML, 2004. [19] A. McCallum, D. Freitag y F. Pereira. Modelos de máxima entropía de Markov para extracción de información y segmentación. En Actas de ICML, 2000. [20] L. Pevzner y M. Hearst. Una crítica y mejora de una métrica de evaluación para la segmentación de texto. Lingüística Computacional, 28(1):19-36, 2002. [21] J. C. Reynar. Modelos estadísticos para la segmentación de temas. En Actas de ACL, 1999. [22] G. Salton y M. McGill. Introducción a la Recuperación de Información Moderna. McGraw Hill, 1983. [23] B. \n\nMcGraw Hill, 1983. [23] B. Sun, Q. Tan, P. Mitra y C. L. Giles. Extracción y búsqueda de fórmulas químicas en documentos de texto en la web. En Actas de WWW, 2007. [24] B. Sun, D. Zhou, H. Zha, y J. I'm sorry, but \"Yen\" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish? Segmentación y alineación de texto multitarea basada en información mutua ponderada. En Actas de CIKM, 2006. [25] M. Utiyama y H. Isahara. Un modelo estadístico para la segmentación de texto independiente del dominio. En Actas de la 39ª ACL, 1999. [26] C. Wayne. Detección y seguimiento de temas multilingües: Investigación exitosa habilitada por corpora y evaluación. En Actas de LREC, 2000. [27] J. Yamron, I. Carp, L. Gillick, S. Lowe y P. van Mulbregt. Un enfoque de modelo oculto de Markov para la segmentación de texto y seguimiento de eventos. En Actas de ICASSP, 1998. [28] H. Zha y X. Ji. Correlacionando documentos multilingües a través de modelado de gráficos bipartitos. En Actas de SIGIR, 2002. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "mutual information": {
            "translated_key": "información mutua",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Topic Segmentation with Shared Topic Detection and Alignment of Multiple Documents Bingjun Sun*, Prasenjit Mitra*† , Hongyuan Zha‡ , C. Lee Giles*† , John Yen*† *Department of Computer Science and Engineering † College of Information Sciences and Technology The Pennsylvania State University University Park, PA 16802 ‡ College of Computing The Georgia Institute of Technology Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu ABSTRACT Topic detection and tracking [26] and topic segmentation [15] play an important role in capturing the local and sequential information of documents.",
                "Previous work in this area usually focuses on single documents, although similar multiple documents are available in many domains.",
                "In this paper, we introduce a novel unsupervised method for shared topic detection and topic segmentation of multiple similar documents based on <br>mutual information</br> (MI) and weighted <br>mutual information</br> (WMI) that is a combination of MI and term weights.",
                "The basic idea is that the optimal segmentation maximizes MI(or WMI).",
                "Our approach can detect shared topics among documents.",
                "It can find the optimal boundaries in a document, and align segments among documents at the same time.",
                "It also can handle single-document segmentation as a special case of the multi-document segmentation and alignment.",
                "Our methods can identify and strengthen cue terms that can be used for segmentation and partially remove stop words by using term weights based on entropy learned from multiple documents.",
                "Our experimental results show that our algorithm works well for the tasks of single-document segmentation, shared topic detection, and multi-document segmentation.",
                "Utilizing information from multiple documents can tremendously improve the performance of topic segmentation, and using WMI is even better than using MI for the multi-document segmentation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Clustering; H.3.1 [Information Storage and Retrieval]: Content Analysis and IndexingLinguistic processing; I.2.7 [Artificial Intelligence]: Natural Language Processing-Text analysis; I.5.3 [Pattern Recognition]: Clustering-Algorithms;Similarity measures General Terms Algorithms, Design, Experimentation 1.",
                "INTRODUCTION Many researchers have worked on topic detection and tracking (TDT) [26] and topic segmentation during the past decade.",
                "Topic segmentation intends to identify the boundaries in a document with the goal to capture the latent topical structure.",
                "Topic segmentation tasks usually fall into two categories [15]: text stream segmentation where topic transition is identified, and coherent document segmentation in which documents are split into sub-topics.",
                "The former category has applications in automatic speech recognition, while the latter one has more applications such as partial-text query of long documents in information retrieval, text summary, and quality measurement of multiple documents.",
                "Previous research in connection with TDT falls into the former category, targeted on topic tracking of broadcast speech data and newswire text, while the latter category has not been studied very well.",
                "Traditional approaches perform topic segmentation on documents one at a time [15, 25, 6].",
                "Most of them perform badly in subtle tasks like coherent document segmentation [15].",
                "Often, end-users seek documents that have the similar content.",
                "Search engines, like, Google, provide links to obtain similar pages.",
                "At a finer granularity, users may actually be looking to obtain sections of a document similar to a particular section that presumably discusses a topic of the users interest.",
                "Thus, the extension of topic segmentation from single documents to identifying similar segments from multiple similar documents with the same topic is a natural and necessary direction, and multi-document topic segmentation is expected to have a better performance since more information is utilized.",
                "Traditional approaches using similarity measurement based on term frequency generally have the same assumption that similar vocabulary tends to be in a coherent topic segment [15, 25, 6].",
                "However, they usually suffer the issue of identifying stop words.",
                "For example, additional document-dependent stop words are removed together with the generic stop words in [15].",
                "There are two reasons that we do not remove stop words directly.",
                "First, identifying stop words is another issue [12] that requires estimation in each domain.",
                "Removing common stop words may result in the loss of useful information in a specific domain.",
                "Second, even though stop words can be identified, hard classification of stop words and nonstop words cannot represent the gradually changing amount of information content of each word.",
                "We employ a soft classification using term weights.",
                "In this paper, we view the problem of topic segmentation as an optimization issue using information theoretic techniques to find the optimal boundaries of a document given the number of text segments so as to minimize the loss of <br>mutual information</br> (MI) (or a weighted <br>mutual information</br> (WMI)) after segmentation and alignment.",
                "This is equal to maximizing the MI (or WMI).",
                "The MI focuses on measuring the difference among segments whereas previous research focused on finding the similarity (e.g. cosine distance) of segments [15, 25, 6].",
                "Topic alignment of multiple similar documents can be achieved by clustering sentences on the same topic into the same cluster.",
                "Single-document topic segmentation is just a special case of the multi-document topic segmentation and alignment problem.",
                "Terms can be co-clustered as in [10] at the same time, given the number of clusters, but our experimental results show that this method results in a worse segmentation (see Tables 1, 4, and 6).",
                "Usually, human readers can identify topic transition based on cue words, and can ignore stop words.",
                "Inspired by this, we give each term (or term cluster) a weight based on entropy among different documents and different segments of documents.",
                "Not only can this approach increase the contribution of cue words, but it can also decrease the effect of common stop words, noisy word, and document-dependent stop words.",
                "These words are common in a document.",
                "Many methods based on sentence similarity require that these words are removed before topic segmentation can be performed [15].",
                "Our results in Figure 3 show that term weights are useful for multi-document topic segmentation and alignment.",
                "The major contribution of this paper is that it introduces a novel method for topic segmentation using MI and shows that this method performs better than previously used criteria.",
                "Also, we have addressed the problem of topic segmentation and alignment across multiple documents, whereas most existing research focused on segmentation of single documents.",
                "Multi-document segmentation and alignment can utilize information from similar documents and improves the performance of topic segmentation greatly.",
                "Obviously, our approach can handle single documents as a special case when multiple documents are unavailable.",
                "It can detect shared topics among documents to judge if they are multiple documents on the same topic.",
                "We also introduce the new criterion of WMI based on term weights learned from multiple similar documents, which can improve performance of topic segmentation further.",
                "We propose an iterative greedy algorithm based on dynamic programming and show that it works well in practice.",
                "Some of our prior work is in [24].",
                "The rest of this paper is organized as follows: In Section 2, we review related work.",
                "Section 3 contains a formulation of the problem of topic segmentation and alignment of multiple documents with term co-clustering, a review of the criterion of MI for clustering, and finally an introduction to WMI.",
                "In Section 4, we first propose the iterative greedy algorithm of topic segmentation and alignment with term co-clustering, and then describe how the algorithm can be optimized by usFigure 1: Illustration of multi-document segmentation and alignment. ing dynamic programming.",
                "In Section 5, experiments about single-document segmentation, shared topic detection, and multi-document segmentation are described, and results are presented and discussed to evaluate the performance of our algorithm.",
                "Conclusions and some future directions of the research work are discussed in Section 6. 2.",
                "PREVIOUS WORK Generally, the existing approaches to text segmentation fall into two categories: supervised learning [19, 17, 23] and unsupervised learning [3, 27, 5, 6, 15, 25, 21].",
                "Supervised learning usually has good performance, since it learns functions from labelled training sets.",
                "However, often getting large training sets with manual labels on document sentences is prohibitively expensive, so unsupervised approaches are desired.",
                "Some models consider dependence between sentences and sections, such as Hidden Markov Model [3, 27], Maximum Entropy Markov Model [19], and Conditional Random Fields [17], while many other approaches are based on lexical cohesion or similarity of sentences [5, 6, 15, 25, 21].",
                "Some approaches also focus on cue words as hints of topic transitions [11].",
                "While some existing methods only consider information in single documents [6, 15], others utilize multiple documents [16, 14].",
                "There are not many works in the latter category, even though the performance of segmentation is expected to be better with utilization of information from multiple documents.",
                "Previous research studied methods to find shared topics [16] and topic segmentation and summarization between just a pair of documents [14].",
                "Text classification and clustering is a related research area which categorizes documents into groups using supervised or unsupervised methods.",
                "Topical classification or clustering is an important direction in this area, especially co-clustering of documents and terms, such as LSA [9], PLSA [13], and approaches based on distances and bipartite graph partitioning [28] or maximum MI [2, 10], or maximum entropy [1, 18].",
                "Criteria of these approaches can be utilized in the issue of topic segmentation.",
                "Some of those methods have been extended into the area of topic segmentation, such as PLSA [5] and maximum entropy [7], but to our best knowledge, using MI for topic segmentation has not been studied. 3.",
                "PROBLEM FORMULATION Our goal is to segment documents and align the segments across documents (Figure 1).",
                "Let T be the set of terms {t1, t2, ..., tl}, which appear in the unlabelled set of documents D = {d1, d2, ..., dm}.",
                "Let Sd be the set of sentences for document d ∈ D, i.e. {s1, s2, ..., snd }.",
                "We have a 3D matrix of term frequency, in which the three dimensions are random variables of D, Sd, and T. Sd actually is a random vector including a random variable for each d ∈ D. The term frequency can be used to estimate the joint probability distribution P(D, Sd, T), which is p(t, d, s) = T(t, d, s)/ND, where T(t, d, s) is the number of t in ds sentence s and ND is the total number of terms in D. ˆS represents the set of segments {ˆs1, ˆs2, ..., ˆsp} after segmentation and alignment among multiple documents, where the number of segments | ˆS| = p. A segment ˆsi of document d is a sequence of adjacent sentences in d. Since for different documents si may discuss different sub-topics, our goal is to cluster adjacent sentences in each document into segments, and align similar segments among documents, so that for different documents ˆsi is about the same sub-topic.",
                "The goal is to find the optimal topic segmentation and alignment mapping Segd(si) : {s1, s2, ..., snd } → {ˆs1, ˆs2, ..., ˆsp} and Alid(ˆsi) : {ˆs1, ˆs2, ..., ˆsp} → {ˆs1, ˆs2, ..., ˆsp}, for all d ∈ D, where ˆsi is ith segment with the constraint that only adjacent sentences can be mapped to the same segment, i.e. for d, {si, si+1, ..., sj} → {ˆsq}, where q ∈ {1, ..., p}, where p is the segment number, and if i > j, then for d, ˆsq is missing.",
                "After segmentation and alignment, random vector Sd becomes an aligned random variable ˆS.",
                "Thus, P(D, Sd, T) becomes P(D, ˆS, T).",
                "Term co-clustering is a technique that has been employed [10] to improve the accuracy of document clustering.",
                "We evaluate the effect of it for topic segmentation.",
                "A term t is mapped to exactly one term cluster.",
                "Term co-clustering involves simultaneously finding the optimal term clustering mapping Clu(t) : {t1, t2, ..., tl} → {ˆt1, ˆt2, ..., ˆtk}, where k ≤ l, l is the total number of words in all the documents, and k is the number of clusters. 4.",
                "METHODOLOGY We now describe a novel algorithm which can handle singledocument segmentation, shared topic detection, and multidocument segmentation and alignment based on MI or WMI. 4.1 <br>mutual information</br> MI I(X; Y ) is a quantity to measure the amount of information which is contained in two or more random variables [8, 10].",
                "For the case of two random variables, we have I(X; Y ) = x∈X y∈Y p(x, y)log p(x, y) p(x)p(y) , (1) Obviously, when random variables X and Y are independent, I(X; Y ) = 0.",
                "Thus, intuitively, the value of MI depends on how random variables are dependent on each other.",
                "The optimal co-clustering is the mapping Clux : X → ˆX and Cluy : Y → ˆY that minimizes the loss: I(X; Y ) − I( ˆX; ˆY ), which is equal to maximizing I( ˆX; ˆY ).",
                "This is the criterion of MI for clustering.",
                "In the case of topic segmentation, the two random variables are the term variable T and the segment variable S, and each sample is an occurrence of a term T = t in a particular segment S = s. I(T; S) is used to measure how dependent T and S are.",
                "However, I(T; S) cannot be computed for documents before segmentation, since we do not have a set of S due to the fact that sentences of Document d, si ∈ Sd, is not aligned with other documents.",
                "Thus, instead of minimizing the loss of MI, we can maximize MI after topic segmentation, computed as: I( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS p(ˆt, ˆs)log p(ˆt, ˆs) p(ˆt)p(ˆs) , (2) where p(ˆt, ˆs) are estimated by the term frequency tf of Term Cluster ˆt and Segment ˆs in the training set D. Note that here a segment ˆs includes sentences about the the same topic among all documents.",
                "The optimal solution is the mapping Clut : T → ˆT, Segd : Sd → ˆS , and Alid : ˆS → ˆS, which maximizes I( ˆT; ˆS). 4.2 Weighted <br>mutual information</br> In topic segmentation and alignment of multiple documents, if P(D, ˆS, T) is known, based on the marginal distributions P(D|T) and P( ˆS|T) for each term t ∈ T, we can categorize terms into four types in the data set: • Common stop words are common both along the dimensions of documents and segments. • Document-dependent stop words that depends on the personal writing style are common only along the dimension of segments for some documents. • Cue words are the most important elements for segmentation.",
                "They are common along the dimension of documents only for the same segment, and they are not common along the dimensions of segments. • Noisy words are other words which are not common along both dimensions.",
                "Entropy based on P(D|T) and P( ˆS|T) can be used to identify different types of terms.",
                "To reinforce the contribution of cue words in the MI computation, and simultaneously reduce the effect of the other three types of words, similar as the idea of the tf-idf weight [22], we use entropies of each term along the dimensions of document D and segment ˆS, i.e.",
                "ED(ˆt) and EˆS(ˆt), to compute the weight.",
                "A cue word usually has a large value of ED(ˆt) but a small value of EˆS(ˆt).",
                "We introduce term weights (or term cluster weights) wˆt = ( ED(ˆt) maxˆt ∈ ˆT (ED(ˆt )) )a (1 − EˆS(ˆt) maxˆt ∈ ˆT (EˆS(ˆt )) )b , (3) where ED(ˆt) = d∈D p(d|ˆt)log|D| 1 p(d|ˆt) , EˆS(ˆt) = ˆs∈ ˆS p(ˆs|ˆt)log| ˆS| 1 p(ˆs|ˆt) , and a > 0 and b > 0 are powers to adjust term weights.",
                "Usually a = 1 and b = 1 as default, and maxˆt ∈ ˆT (ED(ˆt )) and maxˆt ∈ ˆT (EˆS(ˆt )) are used to normalize the entropy values.",
                "Term cluster weights are used to adjust p(ˆt, ˆs), pw(ˆt, ˆs) = wˆtp(ˆt, ˆs) ˆt∈ ˆT ;ˆs∈ ˆS wˆtp(ˆt, ˆs) , (4) and Iw( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS pw(ˆt, ˆs)log pw(ˆt, ˆs) pw(ˆt)pw(ˆs) , (5) where pw(ˆt) and pw(ˆs) are marginal distributions of pw(ˆt, ˆs).",
                "However, since we do not know either the term weights or P(D, ˆS, T), we need to estimate them, but wˆt depends on p(ˆs|t) and ˆS, while ˆS and p(ˆs|t) also depend on wˆt that is still unknown.",
                "Thus, an iterative algorithm is required to estimate term weights wˆt and find the best segmentation and alignment to optimize the objective function Iw concurrently.",
                "After a document is segmented into sentences Input: Joint probability distribution P(D, Sd, T), number of text segments p ∈ {2, 3, ..., max(sd)}, number of term clusters k ∈ {2, 3, ..., l} (if k = l, no term co-clustering required), and weight type w ∈ {0, 1}, indicating to use I or Iw, respectively.",
                "Output: Mapping Clu, Seg, Ali, and term weights wˆt.",
                "Initialization: 0. i = 0.",
                "Initialize Clu (0) t , Seg (0) d , and Ali (0) d ; Initialize w (0) ˆt using Equation (6) if w = 1; Stage 1: 1.",
                "If |D| = 1, k = l, and w = 0, check all sequential segmentations of d into p segments and find the best one Segd(s) = argmaxˆsI( ˆT; ˆS), and return Segd; otherwise, if w = 1 and k = l, go to 3.1; Stage 2: 2.1 If k < l, for each term t, find the best cluster ˆt as Clu(i+1)(t) = argmaxˆtI( ˆT; ˆS(i)) based on Seg(i) and Ali(i); 2.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+1) d (Seg (i+1) d (s)) = argmaxˆsI( ˆT(i+1); ˆS) based on Clu(i+1)(t) if k < l or Clu(0)(t) if k = l; 2.3 i + +.",
                "If Clu, Seg, or Ali changed, go to 2.1; otherwise, if w = 0, return Clu(i), Seg(i), and Ali(i); else j = 0, go to 3.1; Stage 3: 3.1 Update w (i+j+1) ˆt based on Seg(i+j), Ali(i+j), and Clu(i) using Equation (3); 3.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+j+1) d (Seg (i+j+1) d (s)) = argmaxˆsIw( ˆT(i); ˆS) based on Clu(i) and w (i+j+1) ˆt ; 3.3 j + +.",
                "If Iw( ˆT; ˆS) changes, go to step 6; otherwise, stop and return Clu(i), Seg(i+j), Ali(i+j), and w (i+j) ˆt ; Figure 2: Algorithm: Topic segmentation and alignment based on MI or WMI. and each sentence is segmented into words, each word is stemmed.",
                "Then the joint probability distribution P(D, Sd, T) can be estimated.",
                "Finally, this distribution can be used to compute MI in our algorithm. 4.3 Iterative Greedy Algorithm Our goal is to maximize the objective function, I( ˆT; ˆS) or Iw( ˆT; ˆS), which can measure the dependence of term occurrences in different segments.",
                "Generally, first we do not know the estimate term weights, which depend on the optimal topic segmentation and alignment, and term clusters.",
                "Moreover, this problem is NP-hard [10], even though if we know the term weights.",
                "Thus, an iterative greedy algorithm is desired to find the best solution, even though probably only local maxima are reached.",
                "We present the iterative greedy algorithm in Figure 2 to find a local maximum of I( ˆT; ˆS) or Iw( ˆT; ˆS) with simultaneous term weight estimation.",
                "This algorithm can is iterative and greedy for multi-document cases or single-document cases with term weight estimation and/or term co-clustering.",
                "Otherwise, since it is just a one step algorithm to solve the task of single-document segmentation [6, 15, 25], the global maximum of MI is guaranteed.",
                "We will show later that term co-clustering reduces the accuracy of the results and is not necessary, and for singledocument segmentation, term weights are also not required. 4.3.1 Initialization In Step 0, the initial term clustering Clut and topic segmentation and alignment Segd and Alid are important to avoid local maxima and reduce the number of iterations.",
                "First, a good guess of term weights can be made by using the distributions of term frequency along sentences for each document and averaging them to get the initial values of wˆt: wt = ( ED(t) maxt ∈T (ED(t )) )(1 − ES(t) maxt ∈T (ES(t )) ), (6) where ES(t) = 1 |Dt| d∈Dt (1 − s∈Sd p(s|t)log|Sd| 1 p(s|t) ), where Dt is the set of documents which contain Term t. Then, for the initial segmentation Seg(0) , we can simply segment documents equally by sentences.",
                "Or we can find the optimal segmentation just for each document d which maximizes the WMI, Seg (0) d = argmaxˆsIw(T; ˆS), where w = w (0) ˆt .",
                "For the initial alignment Ali(0) , we can first assume that the order of segments for each d is the same.",
                "For the initial term clustering Clu(0) , first cluster labels can be set randomly, and after the first time of Step 3, a good initial term clustering is obtained. 4.3.2 Different Cases After initialization, there are three stages for different cases.",
                "Totally there are eight cases, |D| = 1 or |D| > 1, k = l or k < l, w = 0 or w = 1.",
                "Single document segmentation without term clustering and term weight estimation (|D| = 1, k = l, w = 0) only requires Stage 1 (Step 1).",
                "If term clustering is required (k < l), Stage 2 (Step 2.1, 2.2, and 2.3) is executed iteratively.",
                "If term weight estimation is required (w = 1), Stage 3 (Step 3.1, 3.2, and 3.3) is executed iteratively.",
                "If both are required (k < l, w = 1), Stage 2 and 3 run one after the other.",
                "For multi-document segmentation without term clustering and term weight estimation (|D| > 1, k = l, w = 0), only iteration of Step 2.2 and 2.3 are required.",
                "At Stage 1, the global maximum can be found based on I( ˆT; ˆS) using dynamic programming in Section 4.4.",
                "Simultaneously finding a good term clustering and estimated term weights is impossible, since when moving a term to a new term cluster to maximize Iw( ˆT; ˆS), we do not know that the weight of this term should be the one of the new cluster or the old cluster.",
                "Thus, we first do term clustering at Stage 2, and then estimate term weights at Stage 3.",
                "At Stage 2, Step 2.1 is to find the best term clustering and Step 2.2 is to find the best segmentation.",
                "This cycle is repeated to find a local maximum based on MI I until it converges.",
                "The two steps are: (1) based on current term clustering Cluˆt, for each document d, the algorithm segments all the sentences Sd into p segments sequentially (some segments may be empty), and put them into the p segments ˆS of the whole training set D (all possible cases of different segmentation Segd and alignment Alid are checked) to find the optimal case, and (2) based on the current segmentation and alignment, for each term t, the algorithm finds the best term cluster of t based on the current segmentation Segd and alignment Alid.",
                "After finding a good term clustering, term weights are estimated if w = 1.",
                "At Stage 3, similar as Stage 2, Step 3.1 is term weight re-estimation and Step 3.2 is to find a better segmentation.",
                "They are repeated to find a local maximum based on WMI Iw until it converges.",
                "However, if the term clustering in Stage 2 is not accurate, then the term weight estimation at Stage 3 may have a bad result.",
                "Finally, at Step 3.3, this algorithm converges and return the output.",
                "This algorithm can handle both single-document and multi-document segmentation.",
                "It also can detect shared topics among documents by checking the proportion of overlapped sentences on the same topics, as described in Sec 5.2. 4.4 Algorithm Optimization In many previous works on segmentation, dynamic programming is a technique used to maximize the objective function.",
                "Similarly, at Step 1, 2.2, and 3.2 of our algorithm, we can use dynamic programming.",
                "For Stage 1, using dynamic programming can still find the global optimum, but for Stage 2 and Stage 3, we can only find the optimum for each step of topic segmentation and alignment of a document.",
                "Here we only show the dynamic programming for Step 3.2 using WMI (Step 1 and 2.2 are similar but they can use either I or Iw).",
                "There are two cases that are not shown in the algorithm in Figure 2: (a) single-document segmentation or multi-document segmentation with the same sequential order of segments, where alignment is not required, and (b) multi-document segmentation with different sequential orders of segments, where alignment is necessary.",
                "The alignment mapping function of the former case is simply just Alid(ˆsi) = ˆsi, while for the latter ones alignment mapping function Alid(ˆsi) = ˆsj, i and j may be different.",
                "The computational steps for the two cases are listed below: Case 1 (no alignment): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs) and partial pw(ˆs) without counting sentences from d. Then put sentences from i to j into Part k, and compute partial WMI PIw( ˆT; ˆsk(si, si+1, ..., sj)) ˆt∈ ˆT pw(ˆt, ˆsk)log pw(ˆt, ˆsk) pw(ˆt)pw(ˆsk) , where Alid(si, si+1, ..., sj) = k, k ∈ {1, 2, ..., p}, 1 ≤ i ≤ j ≤ nd, and Segd(sq) = ˆsk for all i ≤ q ≤ j. (2) Let M(sm, 1) = PIw( ˆT; ˆs1(s1, s2, ..., sm)).",
                "Then M(sm, L) = maxi[M(si−1, L − 1) + PIw( ˆT; ˆsL(si, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, and when i > m, no sentences are put into ˆsk when compute PIw (note PIw( ˆT; ˆs(si, ..., sm)) = 0 for single-document segmentation). (3) Finally M(snd , p) = maxi[M(si−1, p − 1)+ PIw( ˆT; ˆsp(si, ..., snd ))], where 1 ≤ i ≤ nd+1.",
                "The optimal Iw is found and the corresponding segmentation is the best.",
                "Case 2 (alignment required): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs), and partial pw(ˆs), and PIw( ˆT; ˆsk(si, si+1, ..., sj)) similarly as Case 1. (2) Let M(sm, 1, k) = PIw( ˆT; ˆsk(s1, s2, ..., sm)), where k ∈ {1, 2, ..., p}.",
                "Then M(sm, L, kL) = maxi,j[M(si−1, L − 1, kL/j) + PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, kL ∈ Set(p, L), which is the set of all p!",
                "L!(p−L)! combinations of L segments chosen from all p segments, j ∈ kL, the set of L segments chosen from all p segments, and kL/j is the combination of L − 1 segments in kL except Segment j. (3) Finally, M(snd , p, kp) = maxi,j[M(si−1, p − 1, kp/j) +PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., snd ))], where kp is just the combination of all p segments and 1 ≤ i ≤ nd + 1, which is the optimal Iw and the corresponding segmentation is the best.",
                "The steps of Case 1 and 2 are similar, except in Case 2, alignment is considered in addition to segmentation.",
                "First, basic items of probability for computing Iw are computed excluding Doc d, and then partial WMI by putting every possible sequential segment (including empty segment) of d into every segment of the set.",
                "Second, the optimal sum of PIw for L segments and the leftmost m sentences, M(sm, L), is found.",
                "Finally, the maximal WMI is found among different sums of M(sm, p − 1) and PIw for Segment p. 5.",
                "EXPERIMENTS In this section, single-document segmentation, shared topic detection, and multi-document segmentation will be tested.",
                "Different hyper parameters of our method are studied.",
                "For convenience, we refer to the method using I as MIk if w = 0, and Iw as WMIk if w = 2 or as WMIk if w = 1, where k is the number of term clusters, and if k = l, where l is the total number of terms, then no term clustering is required, i.e.",
                "MIl and WMIl. 5.1 Single-document Segmentation 5.1.1 Test Data and Evaluation The first data set we tested is a synthetic one used in previous research [6, 15, 25] and many other papers.",
                "It has 700 samples.",
                "Each is a concatenation of ten segments.",
                "Each segment is the first n sentence selected randomly from the Brown corpus, which is supposed to have a different topic from each other.",
                "Currently, the best results on this data set is achieved by Ji et.al. [15].",
                "To compare the performance of our methods, the criterion used widely in previous research is applied, instead of the unbiased criterion introduced in [20].",
                "It chooses a pair of words randomly.",
                "If they are in different segments (different) for the real segmentation (real), but predicted (pred) as in the same segment, it is a miss.",
                "If they are in the same segment (same), but predicted as in different segments, it is a false alarm.",
                "Thus, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, diff)p(diff|real) +p(false alarm|real, pred, same)p(same|real). 5.1.2 Experiment Results We tested the case when the number of segments is known.",
                "Table 1 shows the results of our methods with different hyper parameter values and three previous approaches, C99[25], U00[6], and ADDP03[15], on this data set when the segment number is known.",
                "In WMI for single-document segmentation, the term weights are computed as follows: wˆt = 1−EˆS(ˆt)/maxˆt ∈ ˆT (EˆS(ˆt )).",
                "For this case, our methods MIl and WMIl both outperform all the previous approaches.",
                "We compared our methods with ADDP03using one-sample one-sided t-test and p-values are shown in Table 2.",
                "From the p-values, we can see that mostly the differences are very Table 1: Average Error Rates of Single-document Segmentation Given Segment Numbers Known Range of n 3-11 3-5 6-8 9-11 Sample size 400 100 100 100 C99 12% 11% 10% 9% U00 10% 9% 7% 5% ADDP03 6.0% 6.8% 5.2% 4.3% MIl 4.68% 5.57% 2.59% 1.59% WMIl 4.94% 6.33% 2.76% 1.62% MI100 9.62% 12.92% 8.66% 6.67% Table 2: Single-document Segmentation: P-values of T-test on Error Rates Range of n 3-11 3-5 6-8 9-11 ADDP03, MIl 0.000 0.000 0.000 0.000 ADDP03, WMIl 0.000 0.099 0.000 0.000 MIl, WMIl 0.061 0.132 0.526 0.898 significant.",
                "We also compare the error rates between our two methods using two-sample two-sided t-test to check the hypothesis that they are equal.",
                "We cannot reject the hypothesis that they are equal, so the difference are not significant, even though all the error rates for MIl are smaller than WMIl.",
                "However, we can conclude that term weights contribute little in single-document segmentation.",
                "The results also show that MI using term co-clustering (k = 100) decreases the performance.",
                "We tested different number of term clusters, and found that the performance becomes better when the cluster number increases to reach l. WMIk<l has similar results that we did not show in the table.",
                "As mentioned before, using MI may be inconsistent on optimal boundaries given different numbers of segments.",
                "This situation occurs especially when the similarities among segments are quite different, i.e. some transitions are very obvious, while others are not.",
                "This is because usually a document is a hierarchical structure instead of only a sequential structure.",
                "When the segments are not at the same level, this situation may occur.",
                "Thus, a hierarchical topic segmentation approach is desired, and the structure highly depends on the number of segments for each internal node and the stop criteria of splitting.",
                "For this data set of singledocument segmentation, since it is just a synthetic set, which is just a concatenation of several segments about different topics, it is reasonable that approaches simply based on term frequency have a good performance.",
                "Usually for the tasks of segmenting coherent documents for sub-topics, the effectiveness decreases much. 5.2 Shared Topic Detection 5.2.1 Test Data and Evaluation The second data set contains 80 news articles from Google News.",
                "There are eight topics and each has 10 articles.",
                "We randomly split the set into subsets with different document numbers and each subset has all eight topics.",
                "We compare our approach MIl and WMIl with LDA [4].",
                "LDA treats a document in the data set as a bag of words, finds its distribution on topics, and its major topic.",
                "MIl and WMIl views each sentence as a bag of words and tag it with a topic label.",
                "Then for each pair of documents, LDA determines if they are on the same topic, while MIl and Table 3: Shared Topic Detection: Average Error Rates for Different Numbers of Documents in Each Subset #Doc 10 20 40 80 LDA 8.89% 16.33% 1.35% 0.60% MIl, θ = 0.6 4.17% 1.71% 1.47% 0.0% WMIl, θ = 0.8 18.6% 3.16% 1.92% 0.0% WMIl check whether the proportion overlapped sentences on the same topic is larger than the adjustable threshold θ.",
                "That is, in MIl and WMIl, for a pair of documents d, d , if [ s∈Sd,s ∈Sd 1(topics=topics )/min(|Sd|, |Sd|)] > θ, where Sd is the set of sentences of d, and |Sd| is the number of sentences of d, then d and d have the shared topic.",
                "For a pair of documents selected randomly, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, same)p(same|real) +p(false alarm|real, pred, diff)p(diff|real), where a miss means if they have the same topic (same) for the real case (real), but predicted (pred) as on the same topic.",
                "If they are on different topics (diff), but predicted as on the same topic, it is a false alarm. 5.2.2 Experiment Results The results are shown in Table 3.",
                "If most documents have different topics, in WMIl, the estimation of term weights in Equation (3) is not correct.",
                "Thus, WMIl is not expected to have a better performance than MIl, when most documents have different topics.",
                "When there are fewer documents in a subset with the same number of topics, more documents have different topics, so WMIl is more worse than MIl.",
                "We can see that for most cases MIl has a better (or at least similar) performance than LDA.",
                "After shared topic detection, multi-document segmentation of documents with the shared topics is able to be executed. 5.3 Multi-document Segmentation 5.3.1 Test Data and Evaluation For multi-document segmentation and alignment, our goal is to identify these segments about the same topic among multiple similar documents with shared topics.",
                "Using Iw is expected to perform better than I, since without term weights the result is affected seriously by document-dependent stop words and noisy words which depends on the personal writing style.",
                "It is more likely to treat the same segments of different documents as different segments under the effect of document-dependent stop words and noisy words.",
                "Term weights can reduce the effect of document-dependent stop words and noisy words by giving cue terms more weights.",
                "The data set for multi-document segmentation and alignment has 102 samples and 2264 sentences totally.",
                "Each is the introduction part of a lab report selected from the course of Biol 240W, Pennsylvania State University.",
                "Each sample has two segments, introduction of plant hormones and the content in the lab.",
                "The length range of samples is from two to 56 sentences.",
                "Some samples only have one part and some have a reverse order the these two segments.",
                "It is not hard to identify the boundary between two segments for human.",
                "We labelled each sentence manually for evaluation.",
                "The criterion of evaluation is just using the proportion of the number of sentences with wrong predicted segment labels in the total number of sentences in the whole training Table 4: Average Error Rates of Multi-document Segmentation Given Segment Numbers Known #Doc MIl WMIl k MIk WMIk 102 3.14% 2.78% 300 4.68% 6.58% 51 4.17% 3.63% 300 17.83% 22.84% 34 5.06% 4.12% 300 18.75% 20.95% 20 7.08% 5.42% 250 20.40% 21.83% 10 10.38% 7.89% 250 21.42% 21.91% 5 15.77% 11.64% 250 21.89% 22.59% 2 25.90% 23.18% 50 25.44% 25.49% 1 23.90% 24.82% 25 25.75% 26.15% Table 5: Multi-document Segmentation: P-values of T-test on Error Rates for MIl and WMIl #Doc 51 34 20 10 5 2 P-value 0.19 0.101 0.025 0.001 0.000 0.002 set as the error rate: p(error|predicted, real) = d∈D s∈Sd 1(predicteds=reals)/ d∈D nd.",
                "In order to show the benefits of multi-document segmentation and alignment, we compared our method with different parameters on different partitions of the same training set.",
                "Except the cases that the number of documents is 102 and one (they are special cases of using the whole set and the pure single-document segmentation), we randomly divided the training set into m partitions, and each has 51, 34, 20, 10, 5, and 2 document samples.",
                "Then we applied our methods on each partition and calculated the error rate of the whole training set.",
                "Each case was repeated for 10 times for computing the average error rates.",
                "For different partitions of the training set, different k values are used, since the number of terms increases when the document number in each partition increases. 5.3.2 Experiment Results From the experiment results in Table 4, we can see the following observations: (1) When the number of documents increases, all methods have better performances.",
                "Only from one to two documents, MIl has decrease a little.",
                "We can observe this from Figure 3 at the point of document number = 2.",
                "Most curves even have the worst results at this point.",
                "There are two reasons.",
                "First, because samples vote for the best multi-document segmentation and alignment, but if only two documents are compared with each other, the one with missing segments or a totally different sequence will affect the correct segmentation and alignment of the other.",
                "Second, as noted at the beginning of this section, if two documents have more document-dependent stop words or noisy words than cue words, then the algorithm may view them as two different segments and the other segment is missing.",
                "Generally, we can only expect a better performance when the number of documents is larger than the number of segments. (2) Except single-document segmentation, WMIl is always better than MIl, and when the number of documents is reaching one or increases to a very large number, their performances become closer.",
                "Table 5 shows p-values of twosample one-sided t-test between MIl and WMIl.",
                "We also can see this trend from p-values.",
                "When document number = 5, we reached the smallest p-value and the largest difference between error rates of MIl and WMIl.",
                "For single-document Table 6: Multi-document Segmentation: Average Error Rate for Document Number = 5 in Each Subset with Different Number of Term Clusters #Cluster 75 100 150 250 l MIk 24.67% 24.54% 23.91% 22.59% 15.77% segmentation, WMIl is even a little bit worse than MIl, which is similar as the results of the single-document segmentation on the first data set.",
                "The reason is that for singledocument segmentation, we cannot estimate term weights accurately, since multiple documents are unavailable. (3) Using term clustering usually gets worse results than MIl and WMIl. (4) Using term clustering in WMIk is even worse than in MIk, since in WMIk term clusters are found first using I before using Iw.",
                "If the term clusters are not correct, then the term weights are estimated worse, which may mislead the algorithm to reach even worse results.",
                "From the results we also found that in multi-document segmentation and alignment, most documents with missing segments and a reverse order are identified correctly.",
                "Table 6 illustrates the experiment results for the case of 20 partitions (each has five document samples) of the training set and topic segmentation and alignment using MIk with different numbers of term clusters k. Notice that when the number of term clusters increases, the error rate becomes smaller.",
                "Without term clustering, we have the best result.",
                "We did not show results for WMIk with term clustering, but the results are similar.",
                "We also tested WMIl with different hyper parameters of a and b to adjust term weights.",
                "The results are presented in Figure 3.",
                "It was shown that the default case WMIl : a = 1, b = 1 gave the best results for different partitions of the training set.",
                "We can see the trend that when the document number is very small or large, the difference between MIl : a = 0, b = 0 and WMIl : a = 1, b = 1 becomes quite small.",
                "When the document number is not large (about from 2 to 10), all the cases using term weights have better performances than MIl : a = 0, b = 0 without term weights, but when the document number becomes larger, the cases WMIl : a = 1, b = 0 and WMIl : a = 2, b = 1 become worse than MIl : a = 0, b = 0.",
                "When the document number becomes very large, they are even worse than cases with small document numbers.",
                "This means that a proper way to estimate term weights for the criterion of WMI is very important.",
                "Figure 4 shows the term weights learned from the whole training set.",
                "Four types of words are categorized roughly even though the transition among them are subtle.",
                "Figure 5 illustrates the change in (weighted) <br>mutual information</br> for MIl and WMIl.",
                "As expected, <br>mutual information</br> for MIl increases monotonically with the number of steps, while WMIl does not.",
                "Finally, MIl and WMIl are scalable, with computational complexity shown in Figure 6.",
                "One advantage for our approach based on MI is that removing stop words is not required.",
                "Another important advantage is that there are no necessary hyper parameters to adjust.",
                "In single-document segmentation, the performance based on MI is even better for that based on WMI, so no extra hyper parameter is required.",
                "In multi-document segmentation, we show in the experiment, a = 1 and b = 1 is the best.",
                "Our method gives more weights to cue terms.",
                "However, usually cue terms or sentences appear at the beginning of a segment, while the end of the segment may be 1 2 5 10 20 34 51 102 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 Document Number ErrorRate MIl :a=0,b=0 WMI l :a=1,b=1 WMI l :a=1,b=0 WMI l :a=2,b=1 Figure 3: Error rates for different hyper parameters of term weights. 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Normalized Document Entropy NormalizedSegmentEntropy Noisy words Cue words Common stop words Doc−dep stop words Figure 4: Term weights learned from the whole training set. 0 100 200 300 400 500 600 0.06 0.08 0.1 0.12 0.14 0.16 0.18 Number of Steps (Weighted)MutualInformation MI l WMI l Figure 5: Change in (weighted) MI for MIl and WMIl. 0 20 40 60 80 100 120 0 200 400 600 800 1000 1200 1400 1600 1800 2000 Document Number TimetoConverge(sec) MI l WMI l Figure 6: Time to converge for MIl and WMIl. much noisy.",
                "One possible solution is giving more weights to terms at the beginning of each segment.",
                "Moreover, when the length of segments are quite different, long segments have much higher term frequencies, so they may dominate the segmentation boundaries.",
                "Normalization of term frequencies versus the segment length may be useful. 6.",
                "CONCLUSIONS AND FUTURE WORK We proposed a novel method for multi-document topic segmentation and alignment based on weighted <br>mutual information</br>, which can also handle single-document cases.",
                "We used dynamic programming to optimize our algorithm.",
                "Our approach outperforms all the previous methods on singledocument cases.",
                "Moreover, we also showed that doing segmentation among multiple documents can improve the performance tremendously.",
                "Our results also illustrated that using weighted <br>mutual information</br> can utilize the information of multiple documents to reach a better performance.",
                "We only tested our method on limited data sets.",
                "More data sets especially complicated ones should be tested.",
                "More previous methods should be compared with.",
                "Moreover, natural segmentations like paragraphs are hints that can be used to find the optimal boundaries.",
                "Supervised learning also can be considered. 7.",
                "ACKNOWLEDGMENTS The authors want to thank Xiang Ji, and Prof. J. Scott Payne for their help. 8.",
                "REFERENCES [1] A. Banerjee, I. Ghillon, J. Ghosh, S. Merugu, and D. Modha.",
                "A generalized maximum entropy approach to bregman co-clustering and matrix approximation.",
                "In Proceedings of SIGKDD, 2004. [2] R. Bekkerman, R. El-Yaniv, and A. McCallum.",
                "Multi-way distributional clustering via pairwise interactions.",
                "In Proceedings of ICML, 2005. [3] D. M. Blei and P. J. Moreno.",
                "Topic segmentation with an aspect hidden markov model.",
                "In Proceedings of SIGIR, 2001. [4] D. M. Blei, A. Ng, and M. Jordan.",
                "Latent dirichlet allocation.",
                "Journal of Machine Learning Research, 3:993-1022, 2003. [5] T. Brants, F. Chen, and I. Tsochantaridis.",
                "Topic-based document segmentation with probabilistic latent semantic analysis.",
                "In Proceedings of CIKM, 2002. [6] F. Choi.",
                "Advances in domain indepedent linear text segmentation.",
                "In Proceedings of the NAACL, 2000. [7] H. Christensen, B. Kolluru, Y. Gotoh, and S. Renals.",
                "Maximum entropy segmentation of broadcast news.",
                "In Proceedings of ICASSP, 2005. [8] T. Cover and J. Thomas.",
                "Elements of Information Theory.",
                "John Wiley and Sons, New York, USA, 1991. [9] S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and R. Harshman.",
                "Indexing by latent semantic analysis.",
                "Journal of the American Society for Information Systems, 1990. [10] I. Dhillon, S. Mallela, and D. Modha.",
                "Information-theoretic co-clustering.",
                "In Proceedings of SIGKDD, 2003. [11] M. Hajime, H. Takeo, and O. Manabu.",
                "Text segmentation with multiple surface linguistic cues.",
                "In Proceedings of COLING-ACL, 1998. [12] T. K. Ho.",
                "Stop word location and identification for adaptive text recognition.",
                "International Journal of Document Analysis and Recognition, 3(1), August 2000. [13] T. Hofmann.",
                "Probabilistic latent semantic analysis.",
                "In Proceedings of the UAI99, 1999. [14] X. Ji and H. Zha.",
                "Correlating summarization of a pair of multilingual documents.",
                "In Proceedings of RIDE, 2003. [15] X. Ji and H. Zha.",
                "Domain-independent text segmentation using anisotropic diffusion and dynamic programming.",
                "In Proceedings of SIGIR, 2003. [16] X. Ji and H. Zha.",
                "Extracting shared topics of multiple documents.",
                "In Proceedings of the 7th PAKDD, 2003. [17] J. Lafferty, A. McCallum, and F. Pereira.",
                "Conditional random fields: Probabilistic models for segmenting and labeling sequence data.",
                "In Proceedings of ICML, 2001. [18] T. Li, S. Ma, and M. Ogihara.",
                "Entropy-based criterion in categorical clustering.",
                "In Proceedings of ICML, 2004. [19] A. McCallum, D. Freitag, and F. Pereira.",
                "Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of ICML, 2000. [20] L. Pevzner and M. Hearst.",
                "A critique and improvement of an evaluation metric for text segmentation.",
                "Computational Linguistic, 28(1):19-36, 2002. [21] J. C. Reynar.",
                "Statistical models for topic segmentation.",
                "In Proceedings of ACL, 1999. [22] G. Salton and M. McGill.",
                "Introduction to Modern Information Retrieval.",
                "McGraw Hill, 1983. [23] B.",
                "Sun, Q. Tan, P. Mitra, and C. L. Giles.",
                "Extraction and search of chemical formulae in text documents on the web.",
                "In Proceedings of WWW, 2007. [24] B.",
                "Sun, D. Zhou, H. Zha, and J.",
                "Yen.",
                "Multi-task text segmentation and alignment based on weighted <br>mutual information</br>.",
                "In Proceedings of CIKM, 2006. [25] M. Utiyama and H. Isahara.",
                "A statistical model for domain-independent text segmentation.",
                "In Proceedings of the 39th ACL, 1999. [26] C. Wayne.",
                "Multilingual topic detection and tracking: Successful research enabled by corpora and evaluation.",
                "In Proceedings of LREC, 2000. [27] J. Yamron, I. Carp, L. Gillick, S. Lowe, and P. van Mulbregt.",
                "A hidden markov model approach to text segmentation and event tracking.",
                "In Proceedings of ICASSP, 1998. [28] H. Zha and X. Ji.",
                "Correlating multilingual documents via bipartite graph modeling.",
                "In Proceedings of SIGIR, 2002."
            ],
            "original_annotated_samples": [
                "In this paper, we introduce a novel unsupervised method for shared topic detection and topic segmentation of multiple similar documents based on <br>mutual information</br> (MI) and weighted <br>mutual information</br> (WMI) that is a combination of MI and term weights.",
                "In this paper, we view the problem of topic segmentation as an optimization issue using information theoretic techniques to find the optimal boundaries of a document given the number of text segments so as to minimize the loss of <br>mutual information</br> (MI) (or a weighted <br>mutual information</br> (WMI)) after segmentation and alignment.",
                "METHODOLOGY We now describe a novel algorithm which can handle singledocument segmentation, shared topic detection, and multidocument segmentation and alignment based on MI or WMI. 4.1 <br>mutual information</br> MI I(X; Y ) is a quantity to measure the amount of information which is contained in two or more random variables [8, 10].",
                "The optimal solution is the mapping Clut : T → ˆT, Segd : Sd → ˆS , and Alid : ˆS → ˆS, which maximizes I( ˆT; ˆS). 4.2 Weighted <br>mutual information</br> In topic segmentation and alignment of multiple documents, if P(D, ˆS, T) is known, based on the marginal distributions P(D|T) and P( ˆS|T) for each term t ∈ T, we can categorize terms into four types in the data set: • Common stop words are common both along the dimensions of documents and segments. • Document-dependent stop words that depends on the personal writing style are common only along the dimension of segments for some documents. • Cue words are the most important elements for segmentation.",
                "Figure 5 illustrates the change in (weighted) <br>mutual information</br> for MIl and WMIl."
            ],
            "translated_annotated_samples": [
                "En este artículo, presentamos un novedoso método no supervisado para la detección de temas compartidos y la segmentación de temas de múltiples documentos similares basado en la <br>información mutua</br> (MI) y la <br>información mutua</br> ponderada (WMI), que es una combinación de MI y pesos de términos.",
                "En este documento, consideramos el problema de la segmentación de temas como un problema de optimización utilizando técnicas de teoría de la información para encontrar los límites óptimos de un documento dado el número de segmentos de texto, de manera que se minimice la pérdida de <br>información mutua</br> (MI) (o <br>información mutua</br> ponderada (WMI)) después de la segmentación y alineación.",
                "METODOLOGÍA Ahora describimos un algoritmo novedoso que puede manejar la segmentación de un solo documento, la detección de temas compartidos y la segmentación y alineación de múltiples documentos basados en MI o WMI. 4.1 Información Mutua MI I(X; Y) es una cantidad que mide la cantidad de información contenida en dos o más variables aleatorias [8, 10].",
                "La solución óptima es el mapeo Clut : T → ˆT, Segd : Sd → ˆS, y Alid : ˆS → ˆS, que maximiza I( ˆT; ˆS). 4.2 Información Mutua Ponderada En la segmentación de temas y alineación de múltiples documentos, si se conoce P(D, ˆS, T), basado en las distribuciones marginales P(D|T) y P( ˆS|T) para cada término t ∈ T, podemos categorizar los términos en cuatro tipos en el conjunto de datos: • Las palabras de parada comunes son comunes a lo largo de las dimensiones de documentos y segmentos. • Las palabras de parada dependientes del documento que dependen del estilo de escritura personal son comunes solo a lo largo de la dimensión de segmentos para algunos documentos. • Las palabras clave son los elementos más importantes para la segmentación.",
                "La Figura 5 ilustra el cambio en la <br>información mutua</br> (ponderada) para MIl y WMIl."
            ],
            "translated_text": "La segmentación de temas con detección de temas compartidos y alineación de múltiples documentos Bingjun Sun*, Prasenjit Mitra*†, Hongyuan Zha‡, C. Lee Giles*†, John Yen*† *Departamento de Ciencias de la Computación e Ingeniería † Colegio de Ciencias de la Información y Tecnología Universidad Estatal de Pensilvania University Park, PA 16802 ‡ Colegio de Computación Instituto de Tecnología de Georgia Atlanta, GA 30332 *bsun@cse.psu.edu, † {pmitra,giles,jyen}@ist.psu.edu, ‡ zha@cc.gatech.edu RESUMEN La detección y seguimiento de temas [26] y la segmentación de temas [15] juegan un papel importante en la captura de la información local y secuencial de los documentos. El trabajo previo en esta área suele centrarse en documentos individuales, aunque existen múltiples documentos similares en muchos dominios. En este artículo, presentamos un novedoso método no supervisado para la detección de temas compartidos y la segmentación de temas de múltiples documentos similares basado en la <br>información mutua</br> (MI) y la <br>información mutua</br> ponderada (WMI), que es una combinación de MI y pesos de términos. La idea básica es que la segmentación óptima maximiza MI (o WMI). Nuestro enfoque puede detectar temas compartidos entre documentos. Puede encontrar los límites óptimos en un documento y alinear segmentos entre documentos al mismo tiempo. También puede manejar la segmentación de un solo documento como un caso especial de la segmentación y alineación de varios documentos. Nuestros métodos pueden identificar y fortalecer términos de señal que pueden ser utilizados para la segmentación y eliminar parcialmente las palabras de parada mediante el uso de pesos de términos basados en la entropía aprendida de múltiples documentos. Nuestros resultados experimentales muestran que nuestro algoritmo funciona bien para las tareas de segmentación de un solo documento, detección de temas compartidos y segmentación de múltiples documentos. El uso de información de múltiples documentos puede mejorar enormemente el rendimiento de la segmentación de temas, y utilizar WMI es aún mejor que utilizar MI para la segmentación de múltiples documentos. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información-Agrupación; H.3.1 [Almacenamiento y Recuperación de Información]: Análisis de Contenido e Indexación-Procesamiento Lingüístico; I.2.7 [Inteligencia Artificial]: Procesamiento de Lenguaje Natural-Análisis de Texto; I.5.3 [Reconocimiento de Patrones]: Algoritmos de Agrupación-Medidas de Similitud Términos Generales Algoritmos, Diseño, Experimentación 1. INTRODUCCIÓN Muchos investigadores han trabajado en la detección y seguimiento de temas (TDT) [26] y la segmentación de temas durante la última década. La segmentación de temas tiene como objetivo identificar los límites en un documento con la meta de capturar la estructura temática latente. Las tareas de segmentación de temas generalmente se dividen en dos categorías [15]: la segmentación de flujos de texto, donde se identifica la transición de temas, y la segmentación coherente de documentos, en la que los documentos se dividen en subtemas. La primera categoría tiene aplicaciones en el reconocimiento automático del habla, mientras que la segunda tiene más aplicaciones como la consulta parcial de texto en documentos extensos en la recuperación de información, resumen de texto y medición de calidad de múltiples documentos. La investigación previa en relación con TDT se enmarca en la primera categoría, centrada en el seguimiento de temas de datos de discurso de transmisión y texto de agencia de noticias, mientras que la segunda categoría no ha sido estudiada muy bien. Los enfoques tradicionales realizan la segmentación de temas en los documentos uno a la vez [15, 25, 6]. La mayoría de ellos tienen un mal desempeño en tareas sutiles como la segmentación coherente de documentos [15]. A menudo, los usuarios finales buscan documentos que tengan un contenido similar. Los motores de búsqueda, como Google, proporcionan enlaces para obtener páginas similares. A una granularidad más fina, los usuarios pueden estar buscando obtener secciones de un documento similares a una sección en particular que presumiblemente discute un tema de interés para los usuarios. Por lo tanto, la extensión de la segmentación de temas desde documentos individuales para identificar segmentos similares de múltiples documentos similares con el mismo tema es una dirección natural y necesaria, y se espera que la segmentación de temas de múltiples documentos tenga un mejor rendimiento ya que se utiliza más información. Los enfoques tradicionales que utilizan la medición de similitud basada en la frecuencia de términos generalmente parten del mismo supuesto de que un vocabulario similar tiende a estar en un segmento de tema coherente [15, 25, 6]. Sin embargo, suelen tener el problema de identificar las palabras de parada. Por ejemplo, se eliminan las palabras de parada dependientes del documento junto con las palabras de parada genéricas en [15]. Hay dos razones por las que no eliminamos las palabras de parada directamente. Primero, identificar las palabras vacías es otro problema [12] que requiere estimación en cada dominio. Eliminar palabras comunes de parada puede resultar en la pérdida de información útil en un dominio específico. Segundo, aunque se pueden identificar las palabras de parada, la clasificación estricta de palabras de parada y palabras no de parada no puede representar el cambio gradual en la cantidad de contenido informativo de cada palabra. Empleamos una clasificación suave utilizando pesos de términos. En este documento, consideramos el problema de la segmentación de temas como un problema de optimización utilizando técnicas de teoría de la información para encontrar los límites óptimos de un documento dado el número de segmentos de texto, de manera que se minimice la pérdida de <br>información mutua</br> (MI) (o <br>información mutua</br> ponderada (WMI)) después de la segmentación y alineación. Esto es igual a maximizar el MI (o WMI). El MI se centra en medir la diferencia entre segmentos, mientras que investigaciones anteriores se enfocaban en encontrar la similitud (por ejemplo, la distancia del coseno) entre segmentos [15, 25, 6]. La alineación de temas de múltiples documentos similares se puede lograr agrupando las oraciones sobre el mismo tema en el mismo clúster. La segmentación de temas en un solo documento es solo un caso especial del problema de segmentación y alineación de temas en múltiples documentos. Los términos pueden ser co-agrupados como en [10] al mismo tiempo, dado el número de agrupaciones, pero nuestros resultados experimentales muestran que este método resulta en una peor segmentación (ver Tablas 1, 4 y 6). Por lo general, los lectores humanos pueden identificar la transición de temas basándose en palabras de señal, y pueden ignorar las palabras vacías. Inspirados por esto, asignamos a cada término (o grupo de términos) un peso basado en la entropía entre diferentes documentos y diferentes segmentos de documentos. No solo este enfoque puede aumentar la contribución de las palabras clave, sino que también puede disminuir el efecto de las palabras de parada comunes, palabras ruidosas y palabras de parada dependientes del documento. Estas palabras son comunes en un documento. Muchos métodos basados en la similitud de oraciones requieren que estas palabras sean eliminadas antes de que se pueda realizar la segmentación de temas [15]. Nuestros resultados en la Figura 3 muestran que los pesos de los términos son útiles para la segmentación y alineación de temas en múltiples documentos. La principal contribución de este artículo es que introduce un método novedoso para la segmentación de temas utilizando MI y demuestra que este método funciona mejor que los criterios previamente utilizados. Además, hemos abordado el problema de la segmentación y alineación de temas en varios documentos, mientras que la mayoría de las investigaciones existentes se centraron en la segmentación de documentos individuales. La segmentación y alineación de múltiples documentos pueden utilizar información de documentos similares y mejorar significativamente el rendimiento de la segmentación de temas. Obviamente, nuestro enfoque puede manejar documentos individuales como un caso especial cuando varios documentos no están disponibles. Puede detectar temas compartidos entre documentos para determinar si son múltiples documentos sobre el mismo tema. También introducimos el nuevo criterio de WMI basado en pesos de términos aprendidos de múltiples documentos similares, lo cual puede mejorar aún más el rendimiento de la segmentación de temas. Proponemos un algoritmo voraz iterativo basado en programación dinámica y demostramos que funciona bien en la práctica. Algunos de nuestros trabajos anteriores están en [24]. El resto de este documento está organizado de la siguiente manera: En la Sección 2, revisamos el trabajo relacionado. La sección 3 contiene una formulación del problema de segmentación de temas y alineación de múltiples documentos con co-clustering de términos, una revisión del criterio de MI para el clustering, y finalmente una introducción a WMI. En la Sección 4, primero proponemos el algoritmo voraz iterativo de segmentación y alineación de temas con co-clustering de términos, y luego describimos cómo el algoritmo puede ser optimizado mediante programación dinámica. En la Sección 5, se describen experimentos sobre la segmentación de documentos individuales, la detección de temas compartidos y la segmentación de documentos múltiples, y se presentan y discuten los resultados para evaluar el rendimiento de nuestro algoritmo. Las conclusiones y algunas direcciones futuras del trabajo de investigación se discuten en la Sección 6.2. TRABAJO PREVIO En general, los enfoques existentes para la segmentación de texto se dividen en dos categorías: aprendizaje supervisado [19, 17, 23] y aprendizaje no supervisado [3, 27, 5, 6, 15, 25, 21]. El aprendizaje supervisado suele tener un buen rendimiento, ya que aprende funciones a partir de conjuntos de entrenamiento etiquetados. Sin embargo, obtener conjuntos de entrenamiento grandes con etiquetas manuales en oraciones de documentos suele ser prohibitivamente costoso, por lo que se prefieren enfoques no supervisados. Algunos modelos consideran la dependencia entre oraciones y secciones, como el Modelo Oculto de Markov [3, 27], el Modelo de Máxima Entropía de Markov [19] y los Campos Aleatorios Condicionales [17], mientras que muchos otros enfoques se basan en la cohesión léxica o la similitud de las oraciones [5, 6, 15, 25, 21]. Algunos enfoques también se centran en las palabras clave como pistas de transiciones de tema [11]. Si bien algunos métodos existentes solo consideran información en documentos individuales [6, 15], otros utilizan varios documentos [16, 14]. No hay muchas obras en la última categoría, aunque se espera que el rendimiento de la segmentación sea mejor con la utilización de información de múltiples documentos. Investigaciones previas estudiaron métodos para encontrar temas compartidos [16] y segmentación y resumen de temas entre solo un par de documentos [14]. La clasificación y agrupación de textos es un área de investigación relacionada que categoriza documentos en grupos utilizando métodos supervisados o no supervisados. La clasificación o agrupación temática es una dirección importante en esta área, especialmente el co-agrupamiento de documentos y términos, como LSA [9], PLSA [13], y enfoques basados en distancias y particionamiento de gráficos bipartitos [28] o máxima MI [2, 10], o máxima entropía [1, 18]. Los criterios de estos enfoques pueden ser utilizados en el tema de la segmentación de temas. Algunos de esos métodos se han extendido al área de la segmentación de temas, como PLSA [5] y máxima entropía [7], pero hasta donde sabemos, el uso de MI para la segmentación de temas no ha sido estudiado. 3. FORMULACIÓN DEL PROBLEMA Nuestro objetivo es segmentar documentos y alinear los segmentos entre documentos (Figura 1). Sea T el conjunto de términos {t1, t2, ..., tl}, que aparecen en el conjunto no etiquetado de documentos D = {d1, d2, ..., dm}. Sea Sd el conjunto de oraciones para el documento d ∈ D, es decir, {s1, s2, ..., snd}. Tenemos una matriz tridimensional de frecuencia de términos, en la que las tres dimensiones son variables aleatorias de D, Sd y T. Sd en realidad es un vector aleatorio que incluye una variable aleatoria para cada d ∈ D. La frecuencia de términos se puede utilizar para estimar la distribución de probabilidad conjunta P(D, Sd, T), que es p(t, d, s) = T(t, d, s)/ND, donde T(t, d, s) es el número de t en la oración s de d y ND es el número total de términos en D. ˆS representa el conjunto de segmentos {ˆs1, ˆs2, ..., ˆsp} después de la segmentación y alineación entre varios documentos, donde el número de segmentos | ˆS| = p. Un segmento ˆsi del documento d es una secuencia de oraciones adyacentes en d. Dado que para diferentes documentos si pueden discutir diferentes subtemas, nuestro objetivo es agrupar oraciones adyacentes en cada documento en segmentos, y alinear segmentos similares entre documentos, de modo que para diferentes documentos ˆsi trate sobre el mismo subtema. El objetivo es encontrar la segmentación óptima de temas y el mapeo de alineación Segd(si) : {s1, s2, ..., snd } → {ˆs1, ˆs2, ..., ˆsp} y Alid(ˆsi) : {ˆs1, ˆs2, ..., ˆsp} → {ˆs1, ˆs2, ..., ˆsp}, para todo d ∈ D, donde ˆsi es el i-ésimo segmento con la restricción de que solo las oraciones adyacentes pueden ser asignadas al mismo segmento, es decir, para d, {si, si+1, ..., sj} → {ˆsq}, donde q ∈ {1, ..., p}, donde p es el número de segmento, y si i > j, entonces para d, ˆsq está ausente. Después de la segmentación y alineación, el vector aleatorio Sd se convierte en una variable aleatoria alineada ˆS. Por lo tanto, P(D, Sd, T) se convierte en P(D, ˆS, T). El término co-clustering es una técnica que se ha empleado [10] para mejorar la precisión del agrupamiento de documentos. Evaluamos el efecto de ello para la segmentación de temas. Un término t se asigna a exactamente un grupo de términos. El término co-clustering implica encontrar simultáneamente el mapeo óptimo de agrupación de términos Clu(t) : {t1, t2, ..., tl} → {ˆt1, ˆt2, ..., ˆtk}, donde k ≤ l, l es el número total de palabras en todos los documentos, y k es el número de clústeres. 4. METODOLOGÍA Ahora describimos un algoritmo novedoso que puede manejar la segmentación de un solo documento, la detección de temas compartidos y la segmentación y alineación de múltiples documentos basados en MI o WMI. 4.1 Información Mutua MI I(X; Y) es una cantidad que mide la cantidad de información contenida en dos o más variables aleatorias [8, 10]. Para el caso de dos variables aleatorias, tenemos I(X; Y) = x∈X y∈Y p(x, y)log p(x, y) p(x)p(y) , (1) Obviamente, cuando las variables aleatorias X e Y son independientes, I(X; Y) = 0. Por lo tanto, de manera intuitiva, el valor de MI depende de cómo las variables aleatorias están relacionadas entre sí. La co-agrupación óptima es el mapeo Clux: X → ˆX y Cluy: Y → ˆY que minimiza la pérdida: I(X; Y) - I(ˆX; ˆY), lo cual es igual a maximizar I(ˆX; ˆY). Este es el criterio de MI para la agrupación. En el caso de la segmentación de temas, las dos variables aleatorias son la variable de término T y la variable de segmento S, y cada muestra es una ocurrencia de un término T = t en un segmento particular S = s. Se utiliza I(T; S) para medir qué tan dependientes son T y S. Sin embargo, no se puede calcular I(T; S) para documentos antes de la segmentación, ya que no tenemos un conjunto de S debido a que las oraciones del Documento d, si ∈ Sd, no están alineadas con otros documentos. Así, en lugar de minimizar la pérdida de MI, podemos maximizar MI después de la segmentación de temas, calculada como: I( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS p(ˆt, ˆs)log p(ˆt, ˆs) p(ˆt)p(ˆs) , (2) donde p(ˆt, ˆs) se estiman mediante la frecuencia de términos tf del Cluster de Términos ˆt y el Segmento ˆs en el conjunto de entrenamiento D. Nótese que aquí un segmento ˆs incluye oraciones sobre el mismo tema entre todos los documentos. La solución óptima es el mapeo Clut : T → ˆT, Segd : Sd → ˆS, y Alid : ˆS → ˆS, que maximiza I( ˆT; ˆS). 4.2 Información Mutua Ponderada En la segmentación de temas y alineación de múltiples documentos, si se conoce P(D, ˆS, T), basado en las distribuciones marginales P(D|T) y P( ˆS|T) para cada término t ∈ T, podemos categorizar los términos en cuatro tipos en el conjunto de datos: • Las palabras de parada comunes son comunes a lo largo de las dimensiones de documentos y segmentos. • Las palabras de parada dependientes del documento que dependen del estilo de escritura personal son comunes solo a lo largo de la dimensión de segmentos para algunos documentos. • Las palabras clave son los elementos más importantes para la segmentación. Son comunes a lo largo de la dimensión de los documentos solo para el mismo segmento, y no son comunes a lo largo de las dimensiones de los segmentos. • Las palabras ruidosas son otras palabras que no son comunes a lo largo de ambas dimensiones. La entropía basada en P(D|T) y P( ˆS|T) se puede utilizar para identificar diferentes tipos de términos. Para reforzar la contribución de las palabras clave en el cálculo de MI, y al mismo tiempo reducir el efecto de los otros tres tipos de palabras, similar a la idea del peso tf-idf [22], utilizamos las entropías de cada término a lo largo de las dimensiones del documento D y del segmento ˆS, es decir, ED(ˆt) y EˆS(ˆt), para calcular el peso. Una palabra de señal generalmente tiene un valor alto de ED(ˆt) pero un valor bajo de EˆS(ˆt). Introducimos los pesos de términos (o pesos de grupos de términos) wˆt = ( ED(ˆt) maxˆt ∈ ˆT (ED(ˆt )) )a (1 − EˆS(ˆt) maxˆt ∈ ˆT (EˆS(ˆt )) )b , (3) donde ED(ˆt) = d∈D p(d|ˆt)log|D| 1 p(d|ˆt) , EˆS(ˆt) = ˆs∈ ˆS p(ˆs|ˆt)log| ˆS| 1 p(ˆs|ˆt) , y a > 0 y b > 0 son potencias para ajustar los pesos de términos. Normalmente a = 1 y b = 1 por defecto, y se utilizan maxˆt ∈ ˆT (ED(ˆt )) y maxˆt ∈ ˆT (EˆS(ˆt )) para normalizar los valores de entropía. Los pesos de los grupos de términos se utilizan para ajustar p(ˆt, ˆs), pw(ˆt, ˆs) = wˆtp(ˆt, ˆs) ˆt∈ ˆT ;ˆs∈ ˆS wˆtp(ˆt, ˆs) , (4) e Iw( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS pw(ˆt, ˆs)log pw(ˆt, ˆs) pw(ˆt)pw(ˆs) , (5) donde pw(ˆt) y pw(ˆs) son distribuciones marginales de pw(ˆt, ˆs). Sin embargo, dado que no conocemos ni los pesos de los términos ni P(D, ˆS, T), necesitamos estimarlos, pero wˆt depende de p(ˆs|t) y ˆS, mientras que ˆS y p(ˆs|t) también dependen de wˆt, que aún es desconocido. Por lo tanto, se requiere un algoritmo iterativo para estimar los pesos de los términos wˆt y encontrar la mejor segmentación y alineación para optimizar la función objetivo Iw de manera concurrente. Después de que un documento se segmenta en oraciones, se introduce: la distribución de probabilidad conjunta P(D, Sd, T), el número de segmentos de texto p ∈ {2, 3, ..., máx(sd)}, el número de grupos de términos k ∈ {2, 3, ..., l} (si k = l, no se requiere co-clustering de términos) y el tipo de peso w ∈ {0, 1}, indicando usar I o Iw, respectivamente. Mapeo de Clu, Seg, Ali y pesos de términos wˆt. Inicialización: 0. i = 0. Inicializar Clu (0) t, Seg (0) d y Ali (0) d; Inicializar w (0) ˆt usando la Ecuación (6) si w = 1; Etapa 1: 1. Si |D| = 1, k = l y w = 0, verifica todas las segmentaciones secuenciales de d en p segmentos y encuentra la mejor Segd(s) = argmaxˆsI( ˆT; ˆS), y devuelve Segd; de lo contrario, si w = 1 y k = l, ve a 3.1; Etapa 2: 2.1 Si k < l, para cada término t, encuentra el mejor clúster ˆt como Clu(i+1)(t) = argmaxˆtI( ˆT; ˆS(i)) basado en Seg(i) y Ali(i); 2.2 Para cada d, verifica todas las segmentaciones secuenciales de d en p segmentos con mapeo s → ˆs → ˆs, y encuentra la mejor Ali (i+1) d (Seg (i+1) d (s)) = argmaxˆsI( ˆT(i+1); ˆS) basado en Clu(i+1)(t) si k < l o Clu(0)(t) si k = l; 2.3 i + +. Si Clu, Seg o Ali cambian, ve a 2.1; de lo contrario, si w = 0, devuelve Clu(i), Seg(i) y Ali(i); de lo contrario, j = 0, ve a 3.1; Etapa 3: 3.1 Actualiza w (i+j+1) ˆt basado en Seg(i+j), Ali(i+j) y Clu(i) usando la Ecuación (3); 3.2 Para cada d, verifica todas las segmentaciones secuenciales de d en p segmentos con mapeo s → ˆs → ˆs, y encuentra la mejor Ali (i+j+1) d (Seg (i+j+1) d (s)) = argmaxˆsIw( ˆT(i); ˆS) basado en Clu(i) y w (i+j+1) ˆt; 3.3 j + +. Si Iw( ˆT; ˆS) cambia, ve al paso 6; de lo contrario, detente y devuelve Clu(i), Seg(i+j), Ali(i+j), y w (i+j) ˆt; Figura 2: Algoritmo: Segmentación y alineación de temas basada en MI o WMI. y cada oración se segmenta en palabras, cada palabra se reduce a su raíz. Entonces se puede estimar la distribución de probabilidad conjunta P(D, Sd, T). Finalmente, esta distribución se puede utilizar para calcular la MI en nuestro algoritmo. 4.3 Algoritmo Voraz Iterativo Nuestro objetivo es maximizar la función objetivo, I( ˆT; ˆS) o Iw( ˆT; ˆS), que puede medir la dependencia de las ocurrencias de términos en diferentes segmentos. Generalmente, primero no conocemos los pesos estimados de los términos, los cuales dependen de la segmentación y alineación óptimas de los temas, y los grupos de términos. Además, este problema es NP-duro [10], incluso si conocemos los pesos de los términos. Por lo tanto, se desea un algoritmo voraz iterativo para encontrar la mejor solución, aunque probablemente solo se alcancen máximos locales. Presentamos el algoritmo voraz iterativo en la Figura 2 para encontrar un máximo local de I( ˆT; ˆS) o Iw( ˆT; ˆS) con estimación simultánea de pesos de términos. Este algoritmo es iterativo y codicioso para casos de múltiples documentos o casos de un solo documento con estimación de peso de términos y/o co-clustering de términos. De lo contrario, dado que es solo un algoritmo de un paso para resolver la tarea de segmentación de un solo documento [6, 15, 25], se garantiza el máximo global de MI. Mostraremos más adelante que la coagrupación de términos reduce la precisión de los resultados y no es necesaria, y para la segmentación de un solo documento, los pesos de los términos tampoco son requeridos. 4.3.1 Inicialización En el Paso 0, la agrupación inicial de términos Clut y la segmentación y alineación de temas Segd y Alid son importantes para evitar máximos locales y reducir el número de iteraciones. Primero, se puede hacer una buena estimación de los pesos de los términos utilizando las distribuciones de frecuencia de términos a lo largo de las oraciones para cada documento y promediándolas para obtener los valores iniciales de wˆt: wt = ( ED(t) maxt ∈T (ED(t )) )(1 − ES(t) maxt ∈T (ES(t )) ), (6) donde ES(t) = 1 |Dt| d∈Dt (1 − s∈Sd p(s|t)log|Sd| 1 p(s|t) ), donde Dt es el conjunto de documentos que contienen el término t. Luego, para la segmentación inicial Seg(0), simplemente podemos segmentar los documentos de manera equitativa por oraciones. O podemos encontrar la segmentación óptima solo para cada documento d que maximice el WMI, Seg (0) d = argmaxˆsIw(T; ˆS), donde w = w (0) ˆt. Para el alineamiento inicial Ali(0), podemos primero asumir que el orden de los segmentos para cada d es el mismo. Para el agrupamiento inicial de términos Clu(0), las etiquetas de los primeros grupos pueden establecerse al azar, y después de la primera vez del Paso 3, se obtiene un buen agrupamiento inicial de términos. 4.3.2 Diferentes Casos Después de la inicialización, hay tres etapas para diferentes casos. En total hay ocho casos, |D| = 1 o |D| > 1, k = l o k < l, w = 0 o w = 1. La segmentación de un solo documento sin agrupamiento de términos y estimación de peso de términos (|D| = 1, k = l, w = 0) solo requiere la Etapa 1 (Paso 1). Si se requiere el agrupamiento de términos (k < l), la Etapa 2 (Paso 2.1, 2.2 y 2.3) se ejecuta de forma iterativa. Si se requiere la estimación del peso del término (w = 1), la Etapa 3 (Paso 3.1, 3.2 y 3.3) se ejecuta de forma iterativa. Si ambos son requeridos (k < l, w = 1), la Etapa 2 y 3 se ejecutan una tras otra. Para la segmentación de múltiples documentos sin agrupamiento de términos y estimación de peso de términos (|D| > 1, k = l, w = 0), solo se requiere la iteración de los Pasos 2.2 y 2.3. En la Etapa 1, el máximo global se puede encontrar basándose en I( ˆT; ˆS) utilizando programación dinámica en la Sección 4.4. Es imposible encontrar simultáneamente un buen agrupamiento de términos y pesos estimados de términos, ya que al mover un término a un nuevo grupo de términos para maximizar Iw( ˆT; ˆS), no sabemos si el peso de este término debería ser el del nuevo grupo o el del antiguo. Por lo tanto, primero realizamos el agrupamiento de términos en la Etapa 2, y luego estimamos los pesos de los términos en la Etapa 3. En la Etapa 2, el Paso 2.1 consiste en encontrar el mejor agrupamiento de términos y el Paso 2.2 consiste en encontrar la mejor segmentación. Este ciclo se repite para encontrar un máximo local basado en MI I hasta que converge. Los dos pasos son: (1) basados en el agrupamiento de términos actual Cluˆt, para cada documento d, el algoritmo segmenta todas las oraciones Sd en p segmentos secuencialmente (algunos segmentos pueden estar vacíos), y los coloca en los p segmentos ˆS del conjunto de entrenamiento completo D (se verifican todos los casos posibles de segmentación diferente Segd y alineación Alid) para encontrar el caso óptimo, y (2) basado en la segmentación y alineación actual, para cada término t, el algoritmo encuentra el mejor grupo de términos de t basado en la segmentación actual Segd y la alineación Alid. Después de encontrar un buen agrupamiento de términos, se estiman los pesos de los términos si w = 1. En la Etapa 3, al igual que en la Etapa 2, el Paso 3.1 es la reestimación del peso del término y el Paso 3.2 es encontrar una mejor segmentación. Se repiten para encontrar un máximo local basado en WMI Iw hasta que converja. Sin embargo, si el agrupamiento en la Etapa 2 no es preciso, entonces la estimación de peso en la Etapa 3 puede tener un mal resultado. Finalmente, en el Paso 3.3, este algoritmo converge y devuelve la salida. Este algoritmo puede manejar tanto la segmentación de un solo documento como la de múltiples documentos. También puede detectar temas compartidos entre documentos al verificar la proporción de frases superpuestas sobre los mismos temas, como se describe en la Sección 5.2. 4.4 Optimización del algoritmo. En muchos trabajos anteriores sobre segmentación, la programación dinámica es una técnica utilizada para maximizar la función objetivo. De manera similar, en los Pasos 1, 2.2 y 3.2 de nuestro algoritmo, podemos utilizar programación dinámica. Para la Etapa 1, utilizando programación dinámica aún se puede encontrar el óptimo global, pero para la Etapa 2 y la Etapa 3, solo podemos encontrar el óptimo para cada paso de segmentación de temas y alineación de un documento. Aquí solo mostramos la programación dinámica para el Paso 3.2 utilizando WMI (el Paso 1 y 2.2 son similares pero pueden usar I o Iw). Hay dos casos que no se muestran en el algoritmo de la Figura 2: (a) segmentación de un solo documento o segmentación de múltiples documentos con el mismo orden secuencial de segmentos, donde no se requiere alineación, y (b) segmentación de múltiples documentos con diferentes órdenes secuenciales de segmentos, donde la alineación es necesaria. La función de mapeo de alineación del primer caso es simplemente Alid(ˆsi) = ˆsi, mientras que para los últimos casos la función de mapeo de alineación es Alid(ˆsi) = ˆsj, donde i y j pueden ser diferentes. Los pasos computacionales para los dos casos se enumeran a continuación: Caso 1 (sin alineación): Para cada documento d: (1) Calcular pw(ˆt), pw parcial(ˆt, ˆs) y pw parcial(ˆs) sin contar las oraciones de d. Luego, colocar las oraciones de i a j en la Parte k, y calcular WMI parcial PIw( ˆT; ˆsk(si, si+1, ..., sj)) ˆt∈ ˆT pw(ˆt, ˆsk)log pw(ˆt, ˆsk) pw(ˆt)pw(ˆsk) , donde Alid(si, si+1, ..., sj) = k, k ∈ {1, 2, ..., p}, 1 ≤ i ≤ j ≤ nd, y Segd(sq) = ˆsk para todo i ≤ q ≤ j. (2) Dejar que M(sm, 1) = PIw( ˆT; ˆs1(s1, s2, ..., sm)). Entonces M(sm, L) = maxi[M(si−1, L − 1) + PIw( ˆT; ˆsL(si, ..., sm))], donde 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, y cuando i > m, no se colocan oraciones en ˆsk al calcular PIw (nota PIw( ˆT; ˆs(si, ..., sm)) = 0 para la segmentación de un solo documento). (3) Finalmente M(snd , p) = maxi[M(si−1, p − 1)+ PIw( ˆT; ˆsp(si, ..., snd ))], donde 1 ≤ i ≤ nd+1. Se encuentra el Iw óptimo y la segmentación correspondiente es la mejor. Caso 2 (alineación requerida): Para cada documento d: (1) Calcular pw(ˆt), pw parcial(ˆt, ˆs), y pw parcial(ˆs), y PIw( ˆT; ˆsk(si, si+1, ..., sj)) de manera similar al Caso 1. (2) Sea M(sm, 1, k) = PIw( ˆT; ˆsk(s1, s2, ..., sm)), donde k ∈ {1, 2, ..., p}. Entonces M(sm, L, kL) = maxi,j[M(si−1, L − 1, kL/j) + PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., sm))], donde 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, kL ∈ Set(p, L), que es el conjunto de todos los p! Hay L!(p−L)! combinaciones de L segmentos elegidos de entre todos los p segmentos, j ∈ kL, el conjunto de L segmentos elegidos de entre todos los p segmentos, y kL/j es la combinación de L − 1 segmentos en kL excepto el Segmento j. (3) Finalmente, M(snd , p, kp) = maxi,j[M(si−1, p − 1, kp/j) +PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., snd ))], donde kp es simplemente la combinación de todos los p segmentos y 1 ≤ i ≤ nd + 1, que es el óptimo Iw y la segmentación correspondiente es la mejor. Los pasos de los Casos 1 y 2 son similares, excepto que en el Caso 2, se considera el alineamiento además de la segmentación. Primero, se calculan los elementos básicos de probabilidad para computar Iw excluyendo el documento d, y luego se calcula el WMI parcial colocando cada segmento secuencial posible (incluido el segmento vacío) de d en cada segmento del conjunto. Segundo, se encuentra la suma óptima de PIw para L segmentos y las m oraciones más a la izquierda, M(sm, L). Finalmente, se encuentra el WMI máximo entre diferentes sumas de M(sm, p − 1) y PIw para el Segmento p. 5. EXPERIMENTOS En esta sección se probará la segmentación de un solo documento, la detección de temas compartidos y la segmentación de múltiples documentos. Se estudian diferentes hiperparámetros de nuestro método. Para mayor comodidad, nos referimos al método utilizando I como MIk si w = 0, e Iw como WMIk si w = 2 o como WMIk si w = 1, donde k es el número de grupos de términos, y si k = l, donde l es el número total de términos, entonces no se requiere agrupación de términos, es decir, El primer conjunto de datos que probamos es uno sintético utilizado en investigaciones anteriores [6, 15, 25] y muchos otros artículos. Tiene 700 muestras. Cada uno es una concatenación de diez segmentos. Cada segmento es la primera oración seleccionada al azar de las n primeras oraciones del corpus Brown, que se supone que tienen un tema diferente entre sí. Actualmente, los mejores resultados en este conjunto de datos son logrados por Ji et al. [15]. Para comparar el rendimiento de nuestros métodos, se aplica el criterio ampliamente utilizado en investigaciones anteriores en lugar del criterio imparcial introducido en [20]. Elige un par de palabras al azar. Si se encuentran en segmentos diferentes (diferentes) para la segmentación real (real), pero se predicen (pred) como en el mismo segmento, es un error. Si están en el mismo segmento (mismo), pero se predice que están en segmentos diferentes, es una falsa alarma. Por lo tanto, la tasa de error se calcula utilizando la siguiente ecuación: p(err|real, pred) = p(miss|real, pred, diff)p(diff|real) + p(false alarm|real, pred, same)p(same|real). 5.1.2 Resultados del Experimento Probamos el caso cuando se conoce el número de segmentos. La Tabla 1 muestra los resultados de nuestros métodos con diferentes valores de hiperparámetros y tres enfoques anteriores, C99[25], U00[6] y ADDP03[15], en este conjunto de datos cuando se conoce el número de segmento. En WMI para la segmentación de un solo documento, los pesos de los términos se calculan de la siguiente manera: wˆt = 1−EˆS(ˆt)/maxˆt ∈ ˆT (EˆS(ˆt)). Para este caso, nuestros métodos MIl y WMIl superan a todos los enfoques anteriores. Comparamos nuestros métodos con ADDP03 utilizando una prueba t de una muestra unilateral y los valores de p se muestran en la Tabla 2. A partir de los valores p, podemos ver que en su mayoría las diferencias son muy significativas. También comparamos las tasas de error entre nuestros dos métodos utilizando una prueba t de dos muestras de dos colas para verificar la hipótesis de que son iguales. No podemos rechazar la hipótesis de que son iguales, por lo que las diferencias no son significativas, a pesar de que todas las tasas de error para MIl son más pequeñas que las de WMIl. Sin embargo, podemos concluir que los pesos de los términos contribuyen poco en la segmentación de un solo documento. Los resultados también muestran que el uso de MI con co-clustering de términos (k = 100) disminuye el rendimiento. Probamos diferentes números de grupos de términos y encontramos que el rendimiento mejora cuando el número de grupos aumenta hasta llegar a l. WMIk<l tiene resultados similares que no mostramos en la tabla. Como se mencionó anteriormente, el uso de MI puede ser inconsistente en los límites óptimos dados diferentes números de segmentos. Esta situación ocurre especialmente cuando las similitudes entre los segmentos son bastante diferentes, es decir, algunas transiciones son muy obvias, mientras que otras no lo son. Esto se debe a que normalmente un documento es una estructura jerárquica en lugar de una estructura únicamente secuencial. Cuando los segmentos no están en el mismo nivel, esta situación puede ocurrir. Por lo tanto, se desea un enfoque de segmentación jerárquica de temas, y la estructura depende en gran medida del número de segmentos para cada nodo interno y del criterio de parada de la división. Para este conjunto de datos de segmentación de un solo documento, dado que es solo un conjunto sintético, que es simplemente una concatenación de varios segmentos sobre diferentes temas, es razonable que enfoques simplemente basados en la frecuencia de términos tengan un buen rendimiento. Normalmente, para las tareas de segmentación de documentos coherentes en subtemas, la efectividad disminuye mucho. 5.2 Detección de Temas Compartidos 5.2.1 Datos de Prueba y Evaluación El segundo conjunto de datos contiene 80 artículos de noticias de Google News. Hay ocho temas y cada uno tiene 10 artículos. Dividimos aleatoriamente el conjunto en subconjuntos con diferentes números de documentos y cada subconjunto tiene los ocho temas. Comparamos nuestro enfoque MIl y WMIl con LDA [4]. LDA trata un documento en el conjunto de datos como una bolsa de palabras, encuentra su distribución en temas y su tema principal. MIl y WMIl ven cada oración como un conjunto de palabras y la etiquetan con una etiqueta de tema. Luego, para cada par de documentos, LDA determina si están en el mismo tema, mientras que MIl y Table 3: Detección de Temas Compartidos: Tasas de Error Promedio para Diferentes Números de Documentos en Cada Subconjunto #Doc 10 20 40 80 LDA 8.89% 16.33% 1.35% 0.60% MIl, θ = 0.6 4.17% 1.71% 1.47% 0.0% WMIl, θ = 0.8 18.6% 3.16% 1.92% 0.0% WMIl comprueba si la proporción de oraciones superpuestas en el mismo tema es mayor que el umbral ajustable θ. Es decir, en MIl y WMIl, para un par de documentos d, d , si [ s∈Sd,s ∈Sd 1(temas=temas )/min(|Sd|, |Sd|)] > θ, donde Sd es el conjunto de oraciones de d, y |Sd| es el número de oraciones de d, entonces d y d comparten el tema. Para un par de documentos seleccionados al azar, la tasa de error se calcula utilizando la siguiente ecuación: p(err|real, pred) = p(miss|real, pred, same)p(same|real) + p(false alarm|real, pred, diff)p(diff|real), donde un error significa si tienen el mismo tema (same) para el caso real (real), pero se predice (pred) como en un tema diferente. Si están en diferentes temas (diff), pero se predicen como en el mismo tema, es una falsa alarma. 5.2.2 Resultados del Experimento Los resultados se muestran en la Tabla 3. Si la mayoría de los documentos tienen diferentes temas, en WMIl, la estimación de los pesos de términos en la Ecuación (3) no es correcta. Por lo tanto, no se espera que WMIL tenga un mejor rendimiento que MIL cuando la mayoría de los documentos tienen temas diferentes. Cuando hay menos documentos en un subconjunto con el mismo número de temas, más documentos tienen temas diferentes, por lo que WMIl es peor que MIl. Podemos ver que en la mayoría de los casos, MIl tiene un rendimiento mejor (o al menos similar) que LDA. Después de la detección de temas compartidos, se puede llevar a cabo la segmentación de documentos con los temas compartidos. 5.3 Segmentación de múltiples documentos 5.3.1 Datos de prueba y evaluación Para la segmentación y alineación de múltiples documentos, nuestro objetivo es identificar estos segmentos sobre el mismo tema entre varios documentos similares con temas compartidos. Se espera que al utilizar pesos de términos, Iw funcione mejor que I, ya que sin pesos de términos, el resultado se ve seriamente afectado por palabras de parada dependientes del documento y palabras ruidosas que dependen del estilo de escritura personal. Es más probable tratar los mismos segmentos de diferentes documentos como segmentos diferentes bajo el efecto de palabras de parada dependientes del documento y palabras ruidosas. Los pesos de términos pueden reducir el efecto de las palabras de parada dependientes del documento y las palabras ruidosas al darle más peso a los términos de señal. El conjunto de datos para la segmentación y alineación de múltiples documentos tiene un total de 102 muestras y 2264 oraciones. Cada uno es la parte de introducción de un informe de laboratorio seleccionado del curso de Biol 240W, Universidad Estatal de Pensilvania. Cada muestra tiene dos segmentos, la introducción de las hormonas vegetales y el contenido en el laboratorio. El rango de longitud de las muestras va desde dos hasta 56 oraciones. Algunas muestras solo tienen una parte y algunas tienen un orden inverso de estos dos segmentos. No es difícil identificar el límite entre dos segmentos para los humanos. Etiquetamos cada oración manualmente para evaluación. El criterio de evaluación consiste en utilizar la proporción del número de oraciones con etiquetas de segmento incorrectas predichas en el número total de oraciones en toda la Tabla de entrenamiento. Para mostrar los beneficios de la segmentación y alineación de múltiples documentos, comparamos nuestro método con diferentes parámetros en diferentes particiones del mismo conjunto de entrenamiento. Excepto los casos en los que el número de documentos es 102 y uno (que son casos especiales de uso del conjunto completo y la segmentación pura de un solo documento), dividimos aleatoriamente el conjunto de entrenamiento en m particiones, y cada una tiene 51, 34, 20, 10, 5 y 2 muestras de documentos. Luego aplicamos nuestros métodos en cada partición y calculamos la tasa de error de todo el conjunto de entrenamiento. Cada caso se repitió 10 veces para calcular las tasas de error promedio. Para diferentes particiones del conjunto de entrenamiento, se utilizan diferentes valores de k, ya que el número de términos aumenta cuando el número de documentos en cada partición aumenta. 5.3.2 Resultados del Experimento De los resultados del experimento en la Tabla 4, podemos ver las siguientes observaciones: (1) Cuando el número de documentos aumenta, todos los métodos tienen un mejor rendimiento. Solo de uno a dos documentos, el MIL ha disminuido un poco. Podemos observar esto en la Figura 3 en el punto del número de documento = 2. La mayoría de las curvas incluso tienen los peores resultados en este punto. Hay dos razones. Primero, porque las muestras votan por la mejor segmentación y alineación de múltiples documentos, pero si solo se comparan dos documentos entre sí, aquel con segmentos faltantes o una secuencia totalmente diferente afectará la correcta segmentación y alineación del otro. Segundo, como se señaló al principio de esta sección, si dos documentos tienen más palabras de parada dependientes del documento o palabras ruidosas que palabras clave, entonces el algoritmo puede considerarlos como dos segmentos diferentes y faltar el otro segmento. Generalmente, solo podemos esperar un mejor rendimiento cuando el número de documentos es mayor que el número de segmentos. (2) Excepto en la segmentación de un solo documento, WMIl siempre es mejor que MIl, y cuando el número de documentos se acerca a uno o aumenta a un número muy grande, sus rendimientos se vuelven más cercanos. La Tabla 5 muestra los valores p de la prueba t de dos muestras unilaterales entre MIl y WMIl. También podemos observar esta tendencia a partir de los valores de p. Cuando el número de documento es igual a 5, alcanzamos el valor p más pequeño y la mayor diferencia entre las tasas de error de MIl y WMIl. Para la Tabla 6 de un solo documento: Segmentación de múltiples documentos: Tasa de error promedio para el Documento Número = 5 en cada subconjunto con Diferente Número de Agrupaciones de Términos #Agrupaciones 75 100 150 250 l MIk 24.67% 24.54% 23.91% 22.59% 15.77% de segmentación, WMIl es incluso un poco peor que MIl, lo cual es similar a los resultados de la segmentación de un solo documento en el primer conjunto de datos. La razón es que para la segmentación de un solo documento, no podemos estimar con precisión los pesos de los términos, ya que no tenemos disponibles varios documentos. (3) El uso de agrupamiento de términos generalmente produce resultados peores que MIl y WMIl. (4) El uso de agrupamiento de términos en WMIk es aún peor que en MIk, ya que en WMIk los grupos de términos se encuentran primero utilizando I antes que Iw. Si los grupos de términos no son correctos, entonces los pesos de los términos se estiman peor, lo que puede llevar a que el algoritmo obtenga resultados aún peores. De los resultados también encontramos que en la segmentación y alineación de múltiples documentos, la mayoría de los documentos con segmentos faltantes y en orden inverso son identificados correctamente. La Tabla 6 ilustra los resultados del experimento para el caso de 20 particiones (cada una con cinco muestras de documentos) del conjunto de entrenamiento y la segmentación y alineación de temas utilizando MIk con diferentes números de grupos de términos k. Observa que cuando el número de grupos de términos aumenta, la tasa de error disminuye. Sin agrupamiento de términos, obtenemos el mejor resultado. No mostramos resultados para WMIk con agrupamiento de términos, pero los resultados son similares. También probamos WMIL con diferentes hiperparámetros de a y b para ajustar los pesos de los términos. Los resultados se presentan en la Figura 3. Se demostró que el caso predeterminado WMIl: a = 1, b = 1 dio los mejores resultados para diferentes particiones del conjunto de entrenamiento. Podemos observar la tendencia de que cuando el número de documentos es muy pequeño o grande, la diferencia entre MIl: a = 0, b = 0 y WMIl: a = 1, b = 1 se vuelve bastante pequeña. Cuando el número de documentos no es grande (aproximadamente de 2 a 10), todos los casos que utilizan pesos de términos tienen un mejor rendimiento que MIl: a = 0, b = 0 sin pesos de términos, pero cuando el número de documentos aumenta, los casos WMIl: a = 1, b = 0 y WMIl: a = 2, b = 1 empeoran en comparación con MIl: a = 0, b = 0. Cuando el número de documentos se vuelve muy grande, son aún peores que los casos con números de documentos pequeños. Esto significa que es muy importante encontrar una forma adecuada de estimar los pesos de los términos para el criterio de WMI. La Figura 4 muestra los pesos de los términos aprendidos de todo el conjunto de entrenamiento. Cuatro tipos de palabras se categorizan de manera aproximada aunque la transición entre ellas es sutil. La Figura 5 ilustra el cambio en la <br>información mutua</br> (ponderada) para MIl y WMIl. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        }
    }
}