{
    "id": "H-5",
    "original_text": "Utility-based Information Distillation Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst. Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst. Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst. Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst. Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst. Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst. Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu ABSTRACT This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework. It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant passage ranking with respect to long-lasting information needs (tasks with multiple queries). Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings. For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task. Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses. We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty. Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components. Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation. 1. INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval. Adaptive filtering (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics. Based on the initial query and a few positive examples (if available), an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user. The incremental learning nature of AF systems makes them more powerful than standard search engines that support ad-hoc retrieval (e.g. Google and Yahoo) in terms of finding relevant information with respect to long-lasting topics of interest, and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs, without having to modify their queries manually. A variety of supervised learning algorithms (Rocchio-style classifiers, Exponential-Gaussian models, local regression and logistic regression approaches) have been studied for adaptive settings, examined with explicit and implicit relevance feedback, and evaluated with respect to utility optimization on large benchmark data collections in TREC (Text Retrieval Conferences) and TDT (Topic Detection and Tracking) forums [1, 4, 7, 15, 16, 20, 24, 23]. Regularized logistic regression [21] has been found representative for the state-of-the-art approaches, and highly efficient for frequent model adaptations over large document collections such as the TREC-10 corpus (over 800,000 documents and 84 topics). Despite substantial achievements in recent adaptive filtering research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently. Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications: 1. User has a rather passive role in the conventional adaptive filtering setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision. A more active alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents (or passages) per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists. The latter form of user interaction has been highly effective in standard retrieval for ad-hoc queries. How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2. The unit for receiving a relevance judgment (yes or no) is restricted to the document level in conventional AF. However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant. Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system. For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3. System-selected documents are often highly redundant. A major news event, for example, would be reported by multiple sources repeatedly for a while, making most of the information content in those articles redundant with each other. A conventional AF system would select all these redundant news stories for user feedback, wasting the users time while offering little gain. Clearly, techniques for novelty detection can help in principle [25, 2, 22] for improving the utility of the AF systems. However, the effectiveness of such techniques at passage level to detect novelty with respect to users (fine-grained) feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user. To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and novelty detection for removal of redundancy from user interactions with the system. We call the new process utility-based information distillation. Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach. Therefore, we extended a benchmark corpus - the TDT4 collection of news stories and TV broadcasts - with task definitions, multiple queries per task, and answer keys per query. We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 . To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses. Moreover, we propose an extension of NDCG (Normalized Discounted Cumulated Gain) [9] for assessing the utility of ranked passages as a function of both relevance and novelty. The rest of this paper is organized as follows. Section 2 outlines the information distillation process with a concrete example. Section 3 describes the technical cores of our system called CAFÂ´E - CMU Adaptive Filtering Engine. Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme. Section 5 describes the extended TDT4 corpus. Section 6 presents our experiments and results. Section 7 concludes the study and gives future perspectives. 2. A SAMPLE TASK Consider a news event - the escape of seven convicts from a Texas prison in December 2000 and their capture a month later. Assuming a user were interested in this event since its early stage, the information need could be: Find information about the escape of convicts from Texas prison, and information related to their recapture. The associated lower-level questions could be: 1. How many prisoners escaped? 2. Where and when were they sighted? 3. Who are their known contacts inside and outside the prison? 4. How are they armed? 5. Do they have any vehicles? 6. What steps have been taken so far? We call such an information need a task, and the associated questions as the queries in this task. A distillation system is supposed to monitor the incoming documents, process them chunk by chunk in a temporal order, select potentially relevant and novel passages from each chunk with respect to each query, and present a ranked list of passages to the user. Passage ranking here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list. When presented with a list of passages, the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant. These spans of text are taken as positive examples in the adaptation of the query profile, and also added to the users history. Passages not marked by the user are taken as negative examples. As soon as the query profile is updated, the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low, based on user preference. For example, if the user highlights ...officials have posted a $100,000 reward for their capture... as relevant answer to the query What steps have been taken so far?, then the highlighted piece is used as an additional positive training example in the adaptation of the query profile. This piece of feedback is also added to the user history as a seen example, so that in future, the system will not place another passage mentioning $100,000 reward at the top of the ranked list. However, an article mentioning ...officials have doubled the reward money to $200,000... might be ranked high since it is both relevant to the (updated) query profile and novel with respect to the (updated) user history. The user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly. Clearly, novelty detection is very important for the utility of such a system because of the iterative search. Without novelty detection, the old relevant passages would be shown to the user repeatedly in each ranked list. Through the above example, we can see the main properties of our new framework for utility-based information distillation over temporally ordered documents. Our framework combines and extends the power of adaptive filtering (AF), ad-hoc retrieval (IR) and novelty detection (ND). Compared to standard IR, our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task. Compared to conventional AF, it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system. Compared to past work, this is the first evaluation of novelty detection integrated with adaptive filtering for sequenced queries that allows flexible user feedback over ranked passages. The combination of AF, IR and ND with the new extensions raises an important research question regarding evaluation methodology: how can we measure the utility of such an information distillation system? Existing metrics in standard IR, AF and ND are insufficient, and new solutions must be explored, as we will discuss in Section 4, after describing the technical cores of our system in the next section. 3. TECHNICAL CORES The core components of CAFÂ´E are - 1) AF for incremental learning of query profiles, 2) IR for estimating relevance of passages with respect to query profiles, 3) ND for assessing novelty of passages with respect to users history, and 4) anti-redundancy component to remove redundancy from ranked lists. 3.1 Adaptive Filtering Component We use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for AF [21]. Logistic regression (LR) is a supervised learning algorithm for statistical classification. Based on a training set of labeled instances, it learns a class model which can then by used to predict the labels of unseen instances. Its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required, as is the case in adaptive filtering, where the system must learn from each new feedback provided by the user. (See [21] and [23] for computational complexity and implementation issues). In adaptive filtering, each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query. For training the model, we use the query itself as the initial positive training example of the class, and the user-highlighted pieces of text (marked as Relevant or Not-relevant) during feedback as additional training examples. To address the cold start issue in the early stage before any user feedback is obtained, the system uses a small sample from a retrospective corpus as the initial negative examples in the training set. The details of using logistic regression for adaptive filtering (assigning different weights to positive and negative training instances, and regularizing the objective function to prevent over-fitting on training data) are presented in [21]. The class model wâ learned by Logistic Regression, or the query profile, is a vector whose dimensions are individual terms and whose elements are the regression coefficients, indicating how influential each term is in the query profile. The query profile is updated whenever a new piece of user feedback is received. A temporally decaying weight can be applied to each training example, as an option, to emphasize the most recent user feedback. 3.2 Passage Retrieval Component We use standard IR techniques in this part of our system. Incoming documents are processed in chunks, where each chunk can be defined as a fixed span of time or as a fixed number of documents, as preferred by the user. For each incoming document, corpus statistics like the IDF (Inverted Document Frequency) of each term are updated. We use a state-of-the-art named entity identifier and tracker [8, 12] to identify person and location names, and merge them with co-referent named entities seen in the past. Then the documents are segmented into passages, which can be a whole document, a paragraph, a sentence, or any other continuous span of text, as preferred. Each passage is represented using a vector of TF-IDF (Term FrequencyInverse Document Frequency) weights, where term can be a word or a named entity. Given a query profile, i.e. the logistic regression solution wâ as described in Section 3.1, the system computes the posterior probability of relevance for each passage x as fRL(x) â¡ P(y = 1|x, wâ ) = 1 (1 + eâwâÂ·x) (1) Passages are ordered by their relevance scores, and the ones with scores above a threshold (tuned on a training set) comprise the relevance list that is passed on to the novelty detection step. 3.3 Novelty Detection Component CAFÂ´E maintains a user history H(t), which contains all the spans of text hi that the user highlighted (as feedback) during his or her past interactions with the system, up to the current time t. Denoting the history as H(t) = n h1, h2, ..., ht o , (2) the novelty score of a new candidate passage x is computed as: fND(x) = 1 â max iâ1..t {cos(x, hi)} (3) where both candidate passage x and highlighted spans of text hi are represented as TF-IDF vectors. The novelty score of each passage is compared to a prespecified threshold (also tuned on a training set), and any passage with a score below this threshold is removed from the relevance list. 3.4 Anti-redundant Ranking Component Although the novelty detection component ensures that only novel (previously unseen) information remains in the relevance list, this list might still contain the same novel information at multiple positions in the ranked list. Suppose, for example, that the user has already read about a $100,000 reward for information about the escaped convicts. A new piece of news that the award has been increased to $200,000 is novel since the user hasnt read about it yet. However, multiple news sources would report this news and we might end up showing (redundant) articles from all these sources in a ranked list. Hence, a ranked list should also be made non-redundant with respect to its own contents. We use a simplified version of the Maximal Marginal Relevance method [5], originally developed for combining relevance and novelty in text retrieval and summarization. Our procedure starts with the current list of passages sorted by relevance (section 3.2), filtered by Novelty Detection component (section 3.3), and generates a new non-redundant list as follows: 1. Take the top passage in the current list as the top one in the new list. 2. Add the next passage x in the current list to the new list only if fAR(x) > t where fAR(x) = 1 â max piâLnew {cos(x, pi)} and Lnew is the set of passages already selected in the new list. 3. Repeat step 2 until all the passages in the current list have been examined. After applying the above-mentioned algorithm, each passage in the new list is sufficiently dissimilar to others, thus favoring diversity rather than redundancy in the new ranked list. The anti-redundancy threshold t is tuned on a training set. 4. EVALUATION METHODOLOGY The approach we proposed above for information distillation raises important issues regarding evaluation methodology. Firstly, since our framework allows the output to be passages at different levels of granularity (e.g. k-sentence windows where k may vary) instead of a fixed length, it is not possible to have pre-annotated relevance judgments at all such granularity levels. Secondly, since we wish to measure the utility of the system output as a combination of both relevance and novelty, traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time. Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists. None of the existing measures in ad-hoc retrieval, adaptive filtering, novelty detection or other related areas (text summarization and question answering) have desirable properties in all the three aspects. Therefore, we must develop a new evaluation methodology. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers. Answer keys define what should be present in a system response to receive credit, and are comprised of a collection of information nuggets, i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them. Defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping [19], since system-returned passages can contain the same information expressed in many different ways. Hence, QA evaluations have relied on human assessors for the mapping between various expressions, making the process costly, time consuming, and not scalable to large query and document collections, and extensive system evaluations with various parameter settings. 4.1.1 Automating Evaluation based on Answer Keys Automatic evaluation methods would allow for faster system building and tuning, as well as provide an objective and affordable way of comparing various systems. Recently, such methods have been proposed, more or less, based on the idea of n-gram co-occurrences. Pourpre [10] assigns a fractional recall score to a system response based on its unigram overlap with a given nuggets description. For example, a system response A B C has recall 3/4 with respect to a nugget with description A B C D. However, such an approach is unfair to systems that present the same information but using words other than A, B, C, and D. Another open issue is how to weight individual words in measuring the closeness of a match. For example, consider the question How many prisoners escaped?. In the nugget Seven prisoners escaped from a Texas prison, there is no indication that seven is the keyword, and that it must be matched to get any relevance credit. Using IDF values does not help, since seven will generally not have a higher IDF than words like texas and prison. Also, redefining the nugget as just seven does not solve the problem since now it might spuriously match any mention of seven out of context. Nuggeteer [13] works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold. However, it is also plagued by spurious relevance since not all words contained in the nugget description (or known correct responses) are central to the nugget. 4.1.2 Nugget-Matching Rules We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on nugget-matching rules, which are generated using a semiautomatic procedure explained below. These rules are essentially Boolean queries that will only match against snippets that contain the nugget. For instance, a candidate rule for matching answers to How many prisoners escaped? is (Texas AND seven AND escape AND (convicts OR prisoners)), possibly with other synonyms and variants in the rule. For a corpus of news articles, which usually follow a typical formal prose, it is fairly easy to write such simple rules to match expected answers using a bootstrap approach, as described below. We propose a two-stage approach, inspired by Autoslog [14], that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents. In the first stage, human subjects annotated (using a highlighting tool) portions of ontopic documents that contained answers to each nugget 2 . In the second stage, subjects used our rule generation tool to create rules that would match the annotations for each nugget. The tool allows users to enter a Boolean rule as a disjunction of conjunctions (e.g. ((a AND b) OR (a AND c AND d) OR (e))). Given a candidate rule, our tool uses it as a Boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match. Hence, the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision. We observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions (adding more ANDs), and improving the recall by adding more conjunctions to the disjunction (adding more ORs). As an example, lets try to create a rule for the nugget which says that seven prisoners escaped from the Texas prison. We start with a simple rule - (seven). When we input this into the rule generation tool, we realize that this rule matches many spurious occurrences of seven (e.g. ...seven states...) and thus gets a low precision score. We can further qualify our rule - Texas AND seven AND convicts. Next, by looking at the missed annotations, we realize that some news articles mentioned ...seven prisoners escaped.... We then replace convicts with the disjunction (convicts OR prisoners). We continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the (small number of) misses and false alarms can be safely ignored. Thus we can create nugget-matching rules that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses. Human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably. 4.2 Evaluating the Utility of a Sequence of Ranked Lists The utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information, and how much the user lost in terms of time and energy. We calculate this utility from the utilities of individual passages as follows. After reading each passage returned by the system, the user derives some gain depending on the presence of relevant and novel information, and incurs a loss in terms of the time and energy spent in going through the passage. However, the likelihood that the user would actually read a passage depends on its position in the ranked list. Hence, for a query q, the expected utility 2 LDC [18] already provides relevance judgments for 100 topics on the TDT4 corpus. We further ensured that these judgments are exhaustive on the entire corpus using pooling. of a passage pi at rank i can be defined as U(pi, q) = P(i) â (Gain(pi, q) â Loss(pi, q)) (4) where P(i) is the probability that the user would go through a passage at rank i. The expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage: U(q) = nX i=1 P(i) â (Gain(pi, q) â Loss(pi, q)) (5) Note that if we ignore the loss term and define P(i) as P(i) â 1/ logb(b + i â 1) (6) then we get the recently popularized metric called Discounted Cumulated Gain (DCG) [9], where Gain(pi, q) is defined as the graded relevance of passage pi. However, without the loss term, DCG is a purely recall-oriented metric and not suitable for an adaptive filtering setting, where the systems utility depends in part on its ability to limit the number of items shown to the user. Although P(i) could be defined based on empirical studies of user behavior, for simplicity, we use P(i) exactly as defined in equation 6. The gain G(pi, q) of passage pi with respect to the query q is a function of - 1) the number of relevant nuggets present in pi, and 2) the novelty of each of these nuggets. We combine these two factors as follows. For each nugget Nj, we assign an initial weight wj, and also keep a count nj of the number of times this nugget has been seen by the user in the past. The gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor Î³. Thus, G(pi, q) is defined as G(pi, q) = X Nj âC(pi,q) wj â Î³nj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the answer key of query q. The initial weights wj are all set of be 1.0 in our experiments, but can also be set based on a pyramid approach [11]. The choice of dampening factor Î³ determines the users tolerance for redundancy. When Î³ = 0, a nugget will only receive credit for its first occurrence i.e. when nj is zero3 . For 0 < Î³ < 1, a nugget receives smaller credit for each successive occurrence. When Î³ = 1, no dampening occurs and repeated occurrences of a nugget receive the same credit. Note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system, since the users are expected to remember what the system showed them in the past. We define the loss L(pi, q) as a constant cost c (we use 0.1) incurred when reading a system-returned passage. Thus, our metric can be re-written as U(q) = nX i=1 Gain(pi, q) logb(b + i â 1) â L(n) (8) where L(n) is the loss associated with a ranked list of length n: L(n) = c Â· nX i=1 1 logb(b + i â 1) (9) 3 Note that 00 = 1 Due to the similarity with Discounted Cumulated Gain (DCG), we call our metric Discounted Cumulated Utility (DCU). The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) â¤ 0 i.e. when the gain is less than or equal to the cost of reading the passage. 5. DATA TDT4 was the benchmark corpus used in TDT2002 and TDT2003 evaluations. The corpus consists of over 90, 000 news articles from multiple sources (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) published between October 2000 and January 2001, in the languages of Arabic, English, and Mandarin. Speech-recognized and machine-translated versions of the non-English articles were provided as well. LDC [18] has annotated the corpus with 100 topics, that correspond to various news events in this time period. Out of these, we selected a subset of 12 actionable events, and defined corresponding tasks for them4 . For each task, we manually defined a profile consisting of an initial set of (5 to 10) queries, a free-text description of the user history, i.e., what the user already knows about the event, and a list of known on-topic and off-topic documents (if available) as training examples. For each query, we generated answer keys and corresponding nugget matching rules using the procedure described in section 4.1.2, and produced a total of 120 queries, with an average of 7 nuggets per query. 6. EXPERIMENTS AND RESULTS 6.1 Baselines We used Indri [17], a popular language-model based retrieval engine, as a baseline for comparison with CAFÂ´E. Indri supports standard search engine functionality, including pseudo-relevance feedback (PRF) [3, 6], and is representative of a typical query-based retrieval system. Indri does not support any kind of novelty detection. We compare Indri with PRF turned on and off, against CAFÂ´E with user feedback, novelty detection and antiredundant ranking turned on and off. 6.2 Experimental Setup We divided the TDT4 corpus spanning 4 months into 10 chunks, each defined as a period of 12 consecutive days. At any given point of time in the distillation process, each system accessed the past data up to the current point, and returned a ranked list of up 50 passages per query. The 12 tasks defined on the corpus were divided into a training and test set with 6 tasks each. Each system was allowed to use the training set to tune its parameters for optimizing NDCU (equation 8), including the relevance threshold for both Indri and CAFÂ´E, and the novelty and antiredundancy thresholds for CAFÂ´E. The NDCU for each system run is calculated automatically. User feedback was also simulated - relevance judgments for each system-returned passage (as determined by the nugget matching rules described in section 4.1.2) were 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figure 1: Performance of Indri across chunks Figure 2: Performance of CAFÂ´E across chunks used as user feedback in the adaptation of query profiles and user histories. 6.3 Results In Table 1, we show the NDCU scores of the two systems under various settings. These scores are averaged over the six tasks in the test set, and are calculated with two dampening factors (see section 4.2): Î³ = 0 and 0.1, to simulate no tolerance and small tolerance for redundancy, respectively. Using Î³ = 0 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information. Hence, the improvement obtained from enabling user feedback is smaller with Î³ = 0 than the improvement obtained from feedback with Î³ = 0.1. This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information. However, the user does not benefit from seeing such redundant passages, and is usually interested in other passages containing related information. It is informative to evaluate retrieval systems using our utility measure (with Î³ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback. Sometimes, however, users might indeed be interested in seeing the same information from multiple sources, as an Table 1: NDCU Scores of Indri and CAFÂ´E for two dampening factors (Î³), and various settings (F: Feedback, N: Novelty Detection, A: Anti-Redundant Ranking) Indri CAFÂ´E Î³ Base +PRF Base +F +F+N +F+A +F+N+A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.35 0.36 0.36 indicator of its importance or reliability. In such a case, we can simply choose a higher value for Î³ which corresponds to a higher tolerance for redundancy, and hence let the system tune its parameters accordingly. Since documents were processed chunk by chunk, it would be interesting to see how the performance of systems improves over time. Figures 1 and 2 show the performance trends for both the systems across chunks. While the performance with and without feedback on the first few chunks is expected to be close, for subsequent chunks, the performance curve with feedback enabled rises above the one with the no-feedback setting. The performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks, making some queries easier than others in certain chunks. Moreover, since Indri uses pseudo-relevance feedback while CAFÂ´E uses feedback based on actual relevance judgments, the improvement in case of Indri is less dramatic than that of CAFÂ´E. 7. CONCLUDING REMARKS This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages. Our system, called CAFÂ´E, combines adaptive filtering, novelty detection and antiredundant passage ranking in a unified framework for utility optimization. We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses. We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty. Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off. 8. ACKNOWLEDGMENTS We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments. This work is supported in parts by the National Science Foundation (NSF) under grant IIS0434035, and the Defense Advanced Research Project Agency (DARPA) under contracts NBCHD030010 and W0550432. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. 9. ADDITIONAL AUTHORS Jian Zhang (jianzhan@stat.purdue.edu)â , Jaime Carbonell (jgc@cs.cmu.edu)â  , Peter Brusilovsky (peterb+@pitt.edu)â¡ , Daqing He(dah44@pitt.edu)â¡ 10. REFERENCES [1] J. Allan. Incremental Relevance Feedback for Information Filtering. Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 270-278, 1996. [2] J. Allan, C. Wade, and A. Bolivar. Retrieval and Novelty Detection at the Sentence Level. Proceedings of the ACM SIGIR conference on research and development in information retrieval, 2003. [3] C. Buckley, G. Salton, and J. Allan. Automatic Retrieval with Locality Information using SMART. NIST special publication, (500207):59-72, 1993. [4] J. Callan. Learning While Filtering Documents. Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 224-231, 1998. [5] J. Carbonell and J. Goldstein. The use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries. Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335-336, 1998. [6] E. Efthimiadis. Query Expansion. Annual Review of Information Science and Technology (ARIST), 31:p121-87, 1996. [7] J. Fiscus and G. Duddington. Topic Detection and Tracking Overview. Topic Detection and Tracking: Event-based Information Organization, pages 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos. A Statistical Model for Multilingual Entity Detection and Tracking. NAACL/HLT, 2004. [9] K. JÂ¨arvelin and J. KekÂ¨alÂ¨ainen. Cumulated Gain-based Evaluation of IR Techniques. ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin and D. Demner-Fushman. Automatically Evaluating Answers to Definition Questions. Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), 2005. â Statistics Dept., Purdue University, West Lafayette, USA â  Language Technologies Inst., Carnegie Mellon University, Pittsburgh, USA â¡ School of Information Sciences, Univ. of Pittsburgh, Pittsburgh, USA [11] J. Lin and D. Demner-Fushman. Will Pyramids Built of nUggets Topple Over. Proceedings of HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos. A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree. Proc. of ACL, 4:136-143, 2004. [13] G. Marton. Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments. HLT/NAACL, 2006. [14] E. Riloff. Automatically Constructing a Dictionary for Information Extraction Tasks. Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, 1993. [15] S. Robertson and S. Walker. Microsoft Cambridge at TREC-9: Filtering track. The Ninth Text REtrieval Conference (TREC-9), pages 361-368. [16] R. Schapire, Y. Singer, and A. Singhal. Boosting and Rocchio Applied to Text Filtering. Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle, and W. Croft. Indri: A Language Model-based Search Engine for Complex Queries. Proceedings of the International Conference on Intelligence Analysis, 2004. [18] The Linguistic Data Consortium. http://www.ldc.upenn.edu/. [19] E. Voorhees. Overview of the TREC 2003 Question Answering Track. Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), 2003. [20] Y. Yang and B. Kisiel. Margin-based Local Regression for Adaptive Filtering. Proceedings of the twelfth international conference on Information and knowledge management, pages 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel. Robustness of Adaptive Filtering Methods in a Cross-benchmark Evaluation. Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 98-105, 2005. [22] C. Zhai, W. Cohen, and J. Lafferty. Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval. Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-17, 2003. [23] J. Zhang and Y. Yang. Robustness of Regularized Linear Classification Methods in Text Categorization. Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 190-197, 2003. [24] Y. Zhang. Using Bayesian Priors to Combine Classifiers for Adaptive Filtering. Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 345-352, 2004. [25] Y. Zhang, J. Callan, and T. Minka. Novelty and Redundancy Detection in Adaptive Filtering. Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002.",
    "original_translation": "DestilaciÃ³n de informaciÃ³n basada en utilidad sobre documentos secuenciados temporalmente en el Instituto de TecnologÃ­as del Lenguaje de Yiming Yang. Universidad Carnegie Mellon, Pittsburgh, EE. UU. yiming@cs.cmu.edu Abhimanyu Lad Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. alad@cs.cmu.edu Instituto de TecnologÃ­as del Lenguaje Ni Lao. Universidad Carnegie Mellon, Pittsburgh, EE. UU. nlao@cs.cmu.edu Abhay Harpale Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. aharpale@cs.cmu.edu Bryan Kisiel Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. bkisiel@cs.cmu.edu MÃ³nica Rogati Instituto de TecnologÃ­as del Lenguaje. La Universidad Carnegie Mellon en Pittsburgh, EE. UU. mrogati@cs.cmu.edu RESUMEN Este artÃ­culo examina un nuevo enfoque para la destilaciÃ³n de informaciÃ³n en documentos ordenados temporalmente, y propone un esquema de evaluaciÃ³n novedoso para dicho marco. Combina las fortalezas y se extiende mÃ¡s allÃ¡ del filtrado adaptativo convencional, la detecciÃ³n de novedades y la clasificaciÃ³n de pasajes no redundantes con respecto a las necesidades de informaciÃ³n duraderas (tareas con mÃºltiples consultas). Nuestro enfoque respalda la retroalimentaciÃ³n detallada del usuario a travÃ©s del resaltado de fragmentos arbitrarios de texto, y aprovecha dicha informaciÃ³n para la optimizaciÃ³n de la utilidad en entornos adaptativos. Para nuestros experimentos, definimos tareas hipotÃ©ticas basadas en eventos de noticias en el corpus TDT4, con mÃºltiples consultas por tarea. Se generaron claves de respuestas (nuggets) para cada consulta y se utilizÃ³ un procedimiento semiautomÃ¡tico para adquirir reglas que permiten emparejar automÃ¡ticamente los nuggets con las respuestas del sistema. TambiÃ©n proponemos una extensiÃ³n de la mÃ©trica NDCG para evaluar la utilidad de los pasajes clasificados como una combinaciÃ³n de relevancia y novedad. Nuestros resultados muestran mejoras alentadoras en la utilidad utilizando el nuevo enfoque, en comparaciÃ³n con los sistemas base sin aprendizaje incremental o componentes de detecciÃ³n de novedades. CategorÃ­as y Descriptores de Asignaturas H.3.3 [BÃºsqueda y RecuperaciÃ³n de InformaciÃ³n]: Filtrado de informaciÃ³n, RetroalimentaciÃ³n de relevancia, Modelos de recuperaciÃ³n, Proceso de selecciÃ³n; I.5.2 TÃ©rminos Generales DiseÃ±o, MediciÃ³n, Rendimiento, ExperimentaciÃ³n. 1. INTRODUCCIÃN Seguir informaciÃ³n nueva y relevante de flujos de datos temporales para usuarios con necesidades duraderas ha sido un tema de investigaciÃ³n desafiante en la recuperaciÃ³n de informaciÃ³n. El filtrado adaptativo (AF) es una tarea de predicciÃ³n en lÃ­nea de la relevancia de cada nuevo documento con respecto a temas predefinidos. BasÃ¡ndose en la consulta inicial y algunos ejemplos positivos (si estÃ¡n disponibles), un sistema de AF mantiene un perfil para cada tema de interÃ©s, y lo actualiza constantemente segÃºn la retroalimentaciÃ³n del usuario. La naturaleza de aprendizaje incremental de los sistemas de AF los hace mÃ¡s poderosos que los motores de bÃºsqueda estÃ¡ndar que admiten recuperaciÃ³n ad-hoc (por ejemplo. Google y Yahoo) en tÃ©rminos de encontrar informaciÃ³n relevante con respecto a temas de interÃ©s duraderos, y mÃ¡s atractivo para los usuarios que estÃ¡n dispuestos a proporcionar retroalimentaciÃ³n para adaptar el sistema a sus necesidades de informaciÃ³n especÃ­ficas, sin tener que modificar sus consultas manualmente. Se han estudiado una variedad de algoritmos de aprendizaje supervisado (clasificadores de estilo Rocchio, modelos exponenciales-gaussianos, regresiÃ³n local y enfoques de regresiÃ³n logÃ­stica) para entornos adaptativos, examinados con retroalimentaciÃ³n de relevancia explÃ­cita e implÃ­cita, y evaluados con respecto a la optimizaciÃ³n de la utilidad en grandes colecciones de datos de referencia en los foros de TREC (Conferencias de RecuperaciÃ³n de Texto) y TDT (DetecciÃ³n y Seguimiento de Temas) [1, 4, 7, 15, 16, 20, 24, 23]. La regresiÃ³n logÃ­stica regularizada [21] se ha encontrado representativa para los enfoques de vanguardia, y altamente eficiente para adaptaciones frecuentes del modelo en grandes colecciones de documentos como el corpus TREC-10 (mÃ¡s de 800,000 documentos y 84 temas). A pesar de los logros sustanciales en la investigaciÃ³n reciente de filtrado adaptativo, aÃºn quedan problemas significativos sin resolver sobre cÃ³mo aprovechar de manera efectiva y eficiente la retroalimentaciÃ³n del usuario. EspecÃ­ficamente, los siguientes problemas pueden limitar seriamente la verdadera utilidad de los sistemas de AF en aplicaciones del mundo real: 1. El usuario tiene un papel bastante pasivo en la configuraciÃ³n convencional de filtrado adaptativo: solo reacciona ante el sistema cuando este toma una decisiÃ³n afirmativa sobre un documento, confirmando o rechazando esa decisiÃ³n. Una alternativa mÃ¡s activa serÃ­a permitir al usuario emitir mÃºltiples consultas sobre un tema, revisar una lista clasificada de documentos candidatos (o pasajes) por consulta, y proporcionar retroalimentaciÃ³n sobre la lista clasificada, refinando asÃ­ su necesidad de informaciÃ³n y solicitando listas clasificadas actualizadas. La Ãºltima forma de interacciÃ³n del usuario ha sido altamente efectiva en la recuperaciÃ³n estÃ¡ndar para consultas ad-hoc. CÃ³mo implementar una estrategia para necesidades de informaciÃ³n duraderas en entornos de FA es una pregunta abierta para la investigaciÃ³n. 2. La unidad para recibir un juicio de relevancia (sÃ­ o no) estÃ¡ restringida al nivel del documento en AF convencional. Sin embargo, un usuario real puede estar dispuesto a proporcionar comentarios mÃ¡s informativos y detallados resaltando algunas partes de texto en un documento recuperado como relevantes, en lugar de etiquetar todo el documento como relevante. Aprovechar de manera efectiva este tipo de retroalimentaciÃ³n detallada podrÃ­a mejorar sustancialmente la calidad de un sistema de AF. Para esto, necesitamos habilitar el aprendizaje supervisado a partir de fragmentos de texto etiquetados de longitud arbitraria en lugar de solo permitir documentos etiquetados. 3. Los documentos seleccionados por el sistema suelen ser altamente redundantes. Un evento importante de noticias, por ejemplo, serÃ­a reportado por mÃºltiples fuentes repetidamente durante un tiempo, haciendo que la mayor parte del contenido informativo en esos artÃ­culos sea redundante entre sÃ­. Un sistema de AF convencional seleccionarÃ­a todas estas historias de noticias redundantes para la retroalimentaciÃ³n del usuario, desperdiciando el tiempo de los usuarios mientras ofrece poco beneficio. Claramente, las tÃ©cnicas de detecciÃ³n de novedades pueden ayudar en principio [25, 2, 22] a mejorar la utilidad de los sistemas AF. Sin embargo, la efectividad de tales tÃ©cnicas a nivel de pasaje para detectar novedad con respecto a la retroalimentaciÃ³n de los usuarios a nivel detallado y para detectar redundancia en listas clasificadas aÃºn debe ser evaluada utilizando una medida de utilidad que imite las necesidades de un usuario real. Para abordar las limitaciones actuales de los sistemas de AF mencionadas anteriormente, proponemos y examinamos un nuevo enfoque en este artÃ­culo, que combina las fortalezas del AF convencional (aprendizaje incremental de modelos de temas), recuperaciÃ³n de pasajes multipase para consultas de larga duraciÃ³n condicionadas al tema, y detecciÃ³n de novedades para eliminar la redundancia de las interacciones del usuario con el sistema. Llamamos al nuevo proceso destilaciÃ³n de informaciÃ³n basada en utilidad. Ten en cuenta que los corpus de referencia convencionales para evaluaciones de AF, que tienen juicios de relevancia a nivel de documento y no definen tareas con mÃºltiples consultas, son insuficientes para evaluar el nuevo enfoque. Por lo tanto, ampliamos un corpus de referencia, la colecciÃ³n TDT4 de noticias y transmisiones de televisiÃ³n, con definiciones de tareas, mÃºltiples consultas por tarea y claves de respuestas por consulta. Hemos realizado nuestros experimentos en este corpus extendido TDT4 y hemos puesto a disposiciÃ³n del pÃºblico los datos generados adicionalmente para futuras evaluaciones comparativas. Para evaluar automÃ¡ticamente los fragmentos de texto arbitrarios devueltos por el sistema utilizando nuestras claves de respuestas, desarrollamos un esquema de evaluaciÃ³n con un procedimiento semiautomÃ¡tico para 1 URL: http://nyc.lti.cs.cmu.edu/downloads adquiriendo reglas que pueden comparar fragmentos con las respuestas del sistema. AdemÃ¡s, proponemos una extensiÃ³n de NDCG (Normalized Discounted Cumulated Gain) [9] para evaluar la utilidad de los pasajes clasificados como una funciÃ³n tanto de la relevancia como de la novedad. El resto de este documento estÃ¡ organizado de la siguiente manera. La secciÃ³n 2 describe el proceso de destilaciÃ³n de informaciÃ³n con un ejemplo concreto. La secciÃ³n 3 describe los nÃºcleos tÃ©cnicos de nuestro sistema llamado CAFÂ´E - CMU Adaptive Filtering Engine. La secciÃ³n 4 discute problemas con respecto a la metodologÃ­a de evaluaciÃ³n y propone un nuevo esquema. La secciÃ³n 5 describe el corpus extendido TDT4. La secciÃ³n 6 presenta nuestros experimentos y resultados. La secciÃ³n 7 concluye el estudio y ofrece perspectivas futuras. 2. Una TAREA DE EJEMPLO Considera un evento noticioso: la fuga de siete convictos de una prisiÃ³n de Texas en diciembre de 2000 y su captura un mes despuÃ©s. Suponiendo que un usuario estuviera interesado en este evento desde sus primeras etapas, la necesidad de informaciÃ³n podrÃ­a ser: Encontrar informaciÃ³n sobre la fuga de convictos de una prisiÃ³n de Texas, y la informaciÃ³n relacionada con su recaptura. Las preguntas de nivel inferior asociadas podrÃ­an ser: 1. Â¿CuÃ¡ntos prisioneros escaparon? 2. Â¿DÃ³nde y cuÃ¡ndo fueron avistados? 3. Â¿QuiÃ©nes son sus contactos conocidos dentro y fuera de la prisiÃ³n? 4. Â¿CÃ³mo estÃ¡n armados? 5. Â¿Tienen algÃºn vehÃ­culo? 6. Â¿QuÃ© pasos se han tomado hasta ahora? Llamamos a esta necesidad de informaciÃ³n una tarea, y a las preguntas asociadas como las consultas en esta tarea. Un sistema de destilaciÃ³n debe monitorear los documentos entrantes, procesarlos por fragmentos en un orden temporal, seleccionar pasajes potencialmente relevantes y novedosos de cada fragmento con respecto a cada consulta, y presentar una lista clasificada de pasajes al usuario. La clasificaciÃ³n de pasajes aquÃ­ se basa en cuÃ¡n relevante es un pasaje con respecto a la consulta actual, cuÃ¡n novedoso es con respecto al historial actual del usuario (de sus interacciones con el sistema) y cuÃ¡n redundante es en comparaciÃ³n con otros pasajes con un rango mÃ¡s alto en la lista. Cuando se le presenta una lista de pasajes, el usuario puede proporcionar retroalimentaciÃ³n resaltando tramos de texto arbitrarios que considere relevantes. Estos fragmentos de texto se toman como ejemplos positivos en la adaptaciÃ³n del perfil de consulta, y tambiÃ©n se aÃ±aden al historial de los usuarios. Los pasajes no marcados por el usuario se consideran ejemplos negativos. Tan pronto como se actualiza el perfil de consulta, el sistema vuelve a realizar una bÃºsqueda y devuelve otra lista clasificada de pasajes donde los pasajes previamente vistos son eliminados o clasificados como bajos, segÃºn la preferencia del usuario. Por ejemplo, si el usuario resalta ...los funcionarios han ofrecido una recompensa de $100,000 por su captura... como respuesta relevante a la consulta Â¿QuÃ© pasos se han tomado hasta ahora?, entonces la pieza resaltada se utiliza como un ejemplo de entrenamiento positivo adicional en la adaptaciÃ³n del perfil de la consulta. Este comentario tambiÃ©n se aÃ±ade al historial del usuario como un ejemplo visto, para que en el futuro, el sistema no coloque otro pasaje mencionando una recompensa de $100,000 en la parte superior de la lista clasificada. Sin embargo, un artÃ­culo que mencione que los funcionarios han duplicado el dinero de recompensa a $200,000 podrÃ­a clasificarse alto ya que es relevante tanto para el perfil de consulta (actualizado) como novedoso con respecto a la historia del usuario (actualizada). El usuario puede modificar las consultas originales o agregar una nueva consulta durante el proceso; los perfiles de consulta se cambiarÃ¡n en consecuencia. Claramente, la detecciÃ³n de novedades es muy importante para la utilidad de dicho sistema debido a la bÃºsqueda iterativa. Sin detecciÃ³n de novedad, los antiguos pasajes relevantes se mostrarÃ­an al usuario repetidamente en cada lista clasificada. A travÃ©s del ejemplo anterior, podemos ver las principales propiedades de nuestro nuevo marco de trabajo para la destilaciÃ³n de informaciÃ³n basada en utilidad sobre documentos ordenados temporalmente. Nuestro marco de trabajo combina y amplÃ­a el poder del filtrado adaptativo (AF), la recuperaciÃ³n ad-hoc (IR) y la detecciÃ³n de novedades (ND). En comparaciÃ³n con la IR estÃ¡ndar, nuestro enfoque tiene la capacidad de aprender de forma incremental las necesidades de informaciÃ³n a largo plazo y modelar una secuencia de consultas dentro de una tarea. En comparaciÃ³n con el AF convencional, permite un papel mÃ¡s activo del usuario en refinar sus necesidades de informaciÃ³n y solicitar nuevos resultados al permitir comentarios sobre relevancia y novedad mediante el resaltado de fragmentos arbitrarios de texto en los pasajes devueltos por el sistema. Comparado con trabajos anteriores, esta es la primera evaluaciÃ³n de detecciÃ³n de novedad integrada con filtrado adaptativo para consultas secuenciadas que permite retroalimentaciÃ³n flexible del usuario sobre pasajes clasificados. La combinaciÃ³n de AF, IR y ND con las nuevas extensiones plantea una importante pregunta de investigaciÃ³n sobre la metodologÃ­a de evaluaciÃ³n: Â¿cÃ³mo podemos medir la utilidad de un sistema de destilaciÃ³n de informaciÃ³n como este? Las mÃ©tricas existentes en IR estÃ¡ndar, AF y ND son insuficientes, y se deben explorar nuevas soluciones, como discutiremos en la SecciÃ³n 4, despuÃ©s de describir los nÃºcleos tÃ©cnicos de nuestro sistema en la prÃ³xima secciÃ³n. 3. NÃCLEOS TÃCNICOS Los componentes principales de CAFÃ son: 1) AF para el aprendizaje incremental de perfiles de consulta, 2) IR para estimar la relevancia de pasajes con respecto a los perfiles de consulta, 3) ND para evaluar la novedad de pasajes con respecto al historial de usuarios, y 4) un componente anti-redundancia para eliminar la redundancia de las listas clasificadas. 3.1 Componente de Filtrado Adaptativo Utilizamos un algoritmo de vanguardia en el campo, el mÃ©todo de regresiÃ³n logÃ­stica regularizada que obtuvo los mejores resultados en varios corpus de evaluaciÃ³n de referencia para AF [21]. La regresiÃ³n logÃ­stica (LR) es un algoritmo de aprendizaje supervisado para clasificaciÃ³n estadÃ­stica. BasÃ¡ndose en un conjunto de entrenamiento de instancias etiquetadas, aprende un modelo de clase que luego puede ser utilizado para predecir las etiquetas de instancias no vistas. Su rendimiento, asÃ­ como su eficiencia en tÃ©rminos de tiempo de entrenamiento, lo convierten en un buen candidato cuando se requieren actualizaciones frecuentes del modelo de clase, como es el caso en el filtrado adaptativo, donde el sistema debe aprender de cada nueva retroalimentaciÃ³n proporcionada por el usuario. (Consulte [21] y [23] para complejidad computacional y problemas de implementaciÃ³n). En el filtrado adaptativo, cada consulta se considera como una clase y la probabilidad de que un pasaje pertenezca a esta clase corresponde al grado de relevancia del pasaje con respecto a la consulta. Para entrenar el modelo, utilizamos la propia consulta como ejemplo inicial de entrenamiento positivo de la clase, y las piezas de texto resaltadas por el usuario (marcadas como Relevante o No relevante) durante la retroalimentaciÃ³n como ejemplos de entrenamiento adicionales. Para abordar el problema de inicio en frÃ­o en la etapa inicial antes de obtener cualquier retroalimentaciÃ³n del usuario, el sistema utiliza una pequeÃ±a muestra de un corpus retrospectivo como ejemplos negativos iniciales en el conjunto de entrenamiento. Los detalles de cÃ³mo utilizar la regresiÃ³n logÃ­stica para el filtrado adaptativo (asignando diferentes pesos a las instancias de entrenamiento positivas y negativas, y regularizando la funciÃ³n objetivo para prevenir el sobreajuste en los datos de entrenamiento) se presentan en [21]. El modelo de clase wâ aprendido por RegresiÃ³n LogÃ­stica, o el perfil de consulta, es un vector cuyas dimensiones son tÃ©rminos individuales y cuyos elementos son los coeficientes de regresiÃ³n, indicando cuÃ¡n influyente es cada tÃ©rmino en el perfil de consulta. El perfil de la consulta se actualiza cada vez que se recibe una nueva pieza de retroalimentaciÃ³n del usuario. Un peso decaÃ­do temporalmente se puede aplicar a cada ejemplo de entrenamiento, como una opciÃ³n, para enfatizar la retroalimentaciÃ³n mÃ¡s reciente del usuario. 3.2 Componente de RecuperaciÃ³n de Pasajes Utilizamos tÃ©cnicas estÃ¡ndar de IR en esta parte de nuestro sistema. Los documentos entrantes se procesan por bloques, donde cada bloque puede definirse como un intervalo de tiempo fijo o como un nÃºmero fijo de documentos, segÃºn lo prefiera el usuario. Para cada documento entrante, se actualizan las estadÃ­sticas del corpus como el IDF (Frecuencia Inversa de Documento) de cada tÃ©rmino. Utilizamos un identificador y rastreador de entidades con tecnologÃ­a de punta [8, 12] para identificar nombres de personas y lugares, y fusionarlos con entidades con referencia cruzada vistas en el pasado. Luego, los documentos se dividen en pasajes, que pueden ser un documento completo, un pÃ¡rrafo, una oraciÃ³n, o cualquier otro fragmento continuo de texto, segÃºn se prefiera. Cada pasaje se representa utilizando un vector de pesos TF-IDF (Frecuencia de TÃ©rmino - Frecuencia Inversa de Documento), donde el tÃ©rmino puede ser una palabra o una entidad nombrada. Dado un perfil de consulta, es decir, la soluciÃ³n de regresiÃ³n logÃ­stica wâ como se describe en la SecciÃ³n 3.1, el sistema calcula la probabilidad posterior de relevancia para cada pasaje x como fRL(x) â¡ P(y = 1|x, wâ ) = 1 (1 + eâwâÂ·x) (1). Los pasajes se ordenan segÃºn sus puntajes de relevancia, y aquellos con puntajes por encima de un umbral (ajustado en un conjunto de entrenamiento) conforman la lista de relevancia que se pasa al paso de detecciÃ³n de novedad. 3.3 Componente de DetecciÃ³n de Novedad CAFÂ´E mantiene un historial de usuario H(t), que contiene todos los fragmentos de texto hi que el usuario resaltÃ³ (como retroalimentaciÃ³n) durante sus interacciones pasadas con el sistema, hasta el tiempo actual t. Denotando el historial como H(t) = n h1, h2, ..., ht o , (2), el puntaje de novedad de un nuevo pasaje candidato x se calcula como: fND(x) = 1 â max iâ1..t {cos(x, hi)} (3), donde tanto el pasaje candidato x como los fragmentos de texto resaltados hi se representan como vectores TF-IDF. El puntaje de novedad de cada pasaje se compara con un umbral predefinido (ajustado tambiÃ©n en un conjunto de entrenamiento), y cualquier pasaje con un puntaje por debajo de este umbral se elimina de la lista de relevancia. Componente de clasificaciÃ³n anti-redundante Aunque el componente de detecciÃ³n de novedad garantiza que solo la informaciÃ³n novedosa (no vista previamente) permanezca en la lista de relevancia, esta lista aÃºn podrÃ­a contener la misma informaciÃ³n novedosa en mÃºltiples posiciones en la lista clasificada. Supongamos, por ejemplo, que el usuario ya ha leÃ­do sobre una recompensa de $100,000 por informaciÃ³n sobre los convictos fugados. Una nueva noticia de que el premio ha sido aumentado a $200,000 es novedosa ya que el usuario aÃºn no la ha leÃ­do. Sin embargo, mÃºltiples fuentes de noticias podrÃ­an informar sobre esta noticia y podrÃ­amos terminar mostrando artÃ­culos (redundantes) de todas estas fuentes en una lista clasificada. Por lo tanto, una lista clasificada tambiÃ©n debe ser no redundante con respecto a su propio contenido. Utilizamos una versiÃ³n simplificada del mÃ©todo de MÃ¡xima Relevancia Marginal [5], originalmente desarrollado para combinar relevancia y novedad en la recuperaciÃ³n de texto y resÃºmenes. Nuestro procedimiento comienza con la lista actual de pasajes ordenados por relevancia (secciÃ³n 3.2), filtrados por el componente de DetecciÃ³n de Novedades (secciÃ³n 3.3), y genera una nueva lista no redundante de la siguiente manera: 1. Toma el pasaje superior en la lista actual como el primero en la nueva lista. 2. Agrega el siguiente pasaje x en la lista actual a la nueva lista solo si fAR(x) > t donde fAR(x) = 1 â max piâLnew {cos(x, pi)} y Lnew es el conjunto de pasajes ya seleccionados en la nueva lista. Repetir el paso 2 hasta que se hayan examinado todos los pasajes en la lista actual. DespuÃ©s de aplicar el algoritmo mencionado anteriormente, cada pasaje en la nueva lista es lo suficientemente diferente a los demÃ¡s, favoreciendo asÃ­ la diversidad en lugar de la redundancia en la nueva lista clasificada. El umbral anti-redundancia t se ajusta en un conjunto de entrenamiento. 4. METODOLOGÃA DE EVALUACIÃN El enfoque que propusimos anteriormente para la destilaciÃ³n de informaciÃ³n plantea importantes cuestiones sobre la metodologÃ­a de evaluaciÃ³n. En primer lugar, dado que nuestro marco de trabajo permite que la salida sean fragmentos en diferentes niveles de granularidad (por ejemplo, ventanas de k oraciones donde k puede variar) en lugar de una longitud fija, no es posible tener juicios de relevancia preanotados en todos esos niveles de granularidad. En segundo lugar, dado que deseamos medir la utilidad de la salida del sistema como una combinaciÃ³n de relevancia y novedad, las medidas tradicionales basadas Ãºnicamente en la relevancia deben ser reemplazadas por medidas que penalicen la repeticiÃ³n de la misma informaciÃ³n en la salida del sistema a lo largo del tiempo. En tercer lugar, dado que la salida del sistema son listas clasificadas, debemos recompensar a aquellos sistemas que presenten informaciÃ³n Ãºtil (tanto relevante como no vista previamente) utilizando listas clasificadas mÃ¡s cortas, y penalizar a aquellos que presenten la misma informaciÃ³n utilizando listas clasificadas mÃ¡s largas. Ninguna de las medidas existentes en la recuperaciÃ³n ad-hoc, filtrado adaptativo, detecciÃ³n de novedades u otras Ã¡reas relacionadas (resumen de texto y respuesta a preguntas) tiene propiedades deseables en los tres aspectos. Por lo tanto, debemos desarrollar una nueva metodologÃ­a de evaluaciÃ³n. 4.1 Claves de Respuesta Para permitir la evaluaciÃ³n de un sistema cuya salida consiste en pasajes de longitud arbitraria, tomamos prestado el concepto de claves de respuesta de la comunidad de Preguntas y Respuestas (QA), donde los sistemas pueden devolver fragmentos de texto arbitrarios como respuestas. Las claves de respuestas definen lo que debe estar presente en la respuesta de un sistema para recibir crÃ©dito, y estÃ¡n compuestas por una colecciÃ³n de \"nuggets\" de informaciÃ³n, es decir, unidades de datos sobre las cuales los evaluadores humanos pueden tomar decisiones binarias sobre si una respuesta del sistema las contiene o no. La definiciÃ³n de claves de respuestas y la toma de decisiones binarias asociadas son tareas conceptuales que requieren un mapeo semÃ¡ntico [19], ya que los pasajes devueltos por el sistema pueden contener la misma informaciÃ³n expresada de muchas formas diferentes. Por lo tanto, las evaluaciones de QA han dependido de evaluadores humanos para la correspondencia entre diversas expresiones, lo que hace que el proceso sea costoso, consume mucho tiempo y no sea escalable para grandes colecciones de consultas y documentos, y evaluaciones extensas de sistemas con diversos ajustes de parÃ¡metros. 4.1.1 AutomatizaciÃ³n de la evaluaciÃ³n basada en claves de respuestas Los mÃ©todos de evaluaciÃ³n automÃ¡tica permitirÃ­an una construcciÃ³n y ajuste mÃ¡s rÃ¡pidos del sistema, asÃ­ como proporcionar una forma objetiva y asequible de comparar diversos sistemas. Recientemente, se han propuesto mÃ©todos basados mÃ¡s o menos en la idea de las co-ocurrencias de n-gramos. Pourpre [10] asigna una puntuaciÃ³n de recuerdo fraccional a una respuesta del sistema basada en su superposiciÃ³n de unigramas con una descripciÃ³n de pepitas dada. Por ejemplo, una respuesta del sistema A B C tiene una recuperaciÃ³n de 3/4 con respecto a un fragmento con la descripciÃ³n A B C D. Sin embargo, este enfoque es injusto para los sistemas que presentan la misma informaciÃ³n pero utilizando palabras distintas a A, B, C y D. Otro problema abierto es cÃ³mo ponderar las palabras individuales al medir la cercanÃ­a de una coincidencia. Por ejemplo, considera la pregunta Â¿CuÃ¡ntos prisioneros escaparon? En el fragmento Siete prisioneros escaparon de una prisiÃ³n de Texas, no hay indicaciÃ³n de que siete sea la palabra clave y que debe coincidir para obtener algÃºn crÃ©dito de relevancia. El uso de los valores de IDF no ayuda, ya que la palabra \"seven\" generalmente no tendrÃ¡ un IDF mÃ¡s alto que palabras como \"texas\" y \"prison\". AdemÃ¡s, redefinir el fragmento como simplemente siete no resuelve el problema, ya que ahora podrÃ­a coincidir errÃ³neamente con cualquier menciÃ³n de siete fuera de contexto. Nuggeteer [13] trabaja sobre principios similares pero toma decisiones binarias sobre si hay un fragmento en una respuesta de sistema dada ajustando un umbral. Sin embargo, tambiÃ©n estÃ¡ plagado de relevancia espuria ya que no todas las palabras contenidas en la descripciÃ³n de la pepita (o respuestas correctas conocidas) son centrales para la pepita. 4.1.2 Reglas de Coincidencia de Pepitas Proponemos un mÃ©todo automÃ¡tico confiable para determinar si un fragmento de texto contiene una pepita dada, basado en reglas de coincidencia de pepitas, que se generan utilizando un procedimiento semiautomÃ¡tico explicado a continuaciÃ³n. Estas reglas son bÃ¡sicamente consultas booleanas que solo coincidirÃ¡n con fragmentos que contengan la pepita. Por ejemplo, una regla candidata para emparejar respuestas a Â¿CuÃ¡ntos prisioneros escaparon? es (Texas Y siete Y escapar Y (convictos O prisioneros)), posiblemente con otros sinÃ³nimos y variantes en la regla. Para un corpus de artÃ­culos de noticias, que suelen seguir una prosa formal tÃ­pica, es bastante fÃ¡cil escribir reglas simples para coincidir con respuestas esperadas utilizando un enfoque de arranque, como se describe a continuaciÃ³n. Proponemos un enfoque de dos etapas, inspirado en Autoslog [14], que combina la capacidad de los humanos para identificar expresiones semÃ¡nticamente equivalentes y la capacidad del sistema para recopilar evidencia estadÃ­stica de un corpus de documentos anotados por humanos. En la primera etapa, los sujetos humanos anotaron (utilizando una herramienta de resaltado) porciones de documentos relevantes que contenÃ­an respuestas a cada fragmento 2. En la segunda etapa, los sujetos utilizaron nuestra herramienta de generaciÃ³n de reglas para crear reglas que coincidieran con las anotaciones de cada fragmento. La herramienta permite a los usuarios ingresar una regla booleana como una disyunciÃ³n de conjunciones (por ejemplo, ((a Y b) O (a Y c Y d) O (e))). Dada una regla candidata, nuestra herramienta la utiliza como una consulta booleana sobre el conjunto completo de documentos relevantes y calcula su recall y precisiÃ³n con respecto a las anotaciones que se espera que coincida. Por lo tanto, los sujetos pueden comenzar con una regla simple y refinarla iterativamente hasta que estÃ©n satisfechos con su precisiÃ³n y recuperaciÃ³n. Observamos que era muy fÃ¡cil para los humanos mejorar la precisiÃ³n de una regla ajustando sus conjunciones existentes (agregando mÃ¡s Y) y mejorar la recuperaciÃ³n agregando mÃ¡s conjunciones a la disyunciÃ³n (agregando mÃ¡s O). Como ejemplo, intentemos crear una regla para el acertijo que dice que siete prisioneros escaparon de la prisiÃ³n de Texas. Empezamos con una regla simple - (siete). Cuando introducimos esto en la herramienta de generaciÃ³n de reglas, nos damos cuenta de que esta regla coincide con muchas ocurrencias espurias de siete (por ejemplo, ...siete estados...) y, por lo tanto, obtiene una puntuaciÃ³n baja de precisiÃ³n. Podemos calificar aÃºn mÃ¡s nuestra regla - Texas Y siete Y convictos. A continuaciÃ³n, al revisar las anotaciones omitidas, nos damos cuenta de que algunos artÃ­culos de noticias mencionaban ...siete prisioneros escaparon.... Luego reemplazamos convictos por la disyunciÃ³n (convictos O prisioneros). Continuamos ajustando la regla de esta manera hasta lograr un nivel de recordaciÃ³n y precisiÃ³n suficientemente alto, es decir, que los (pocos) errores y alarmas falsas se puedan ignorar de forma segura. AsÃ­ podemos crear reglas de coincidencia de pepitas que capturen de manera sucinta varias formas de expresar una pepita, evitando al mismo tiempo emparejar respuestas incorrectas (o fuera de contexto). La participaciÃ³n humana en el proceso de creaciÃ³n de reglas garantiza reglas genÃ©ricas de alta calidad que luego se pueden utilizar para evaluar de manera confiable las respuestas del sistema arbitrario. 4.2 EvaluaciÃ³n de la Utilidad de una Secuencia de Listas Clasificadas La utilidad de un sistema de recuperaciÃ³n se puede definir como la diferencia entre cuÃ¡nta informaciÃ³n Ãºtil obtuvo el usuario y cuÃ¡nto tiempo y energÃ­a perdiÃ³. Calculamos esta utilidad a partir de las utilidades de los pasajes individuales de la siguiente manera. DespuÃ©s de leer cada pasaje devuelto por el sistema, el usuario obtiene cierta ganancia dependiendo de la presencia de informaciÃ³n relevante y novedosa, y incurre en una pÃ©rdida en tÃ©rminos del tiempo y la energÃ­a invertidos en revisar el pasaje. Sin embargo, la probabilidad de que el usuario realmente lea un pasaje depende de su posiciÃ³n en la lista clasificada. Por lo tanto, para una consulta q, la utilidad esperada 2 LDC [18] ya proporciona juicios de relevancia para 100 temas en el corpus TDT4. AdemÃ¡s, nos aseguramos de que estos juicios sean exhaustivos en todo el corpus utilizando la agrupaciÃ³n. El valor de un pasaje pi en el rango i se puede definir como U(pi, q) = P(i) â (Ganancia(pi, q) - PÃ©rdida(pi, q)) (4) donde P(i) es la probabilidad de que el usuario pase por un pasaje en el rango i. La utilidad esperada para una lista clasificada completa de longitud n se puede calcular simplemente sumando la utilidad esperada de cada pasaje: U(q) = Î£ i=1 P(i) â (Ganancia(pi, q) â PÃ©rdida(pi, q)) (5) NÃ³tese que si ignoramos el tÃ©rmino de pÃ©rdida y definimos P(i) como P(i) â 1/ logb(b + i â 1) (6) entonces obtenemos la mÃ©trica recientemente popularizada llamada Ganancia Acumulada Descontada (DCG) [9], donde Ganancia(pi, q) se define como la relevancia graduada del pasaje pi. Sin embargo, sin el tÃ©rmino de pÃ©rdida, DCG es una mÃ©trica puramente orientada al recuerdo y no es adecuada para un entorno de filtrado adaptativo, donde la utilidad del sistema depende en parte de su capacidad para limitar el nÃºmero de elementos mostrados al usuario. Aunque P(i) podrÃ­a definirse en base a estudios empÃ­ricos del comportamiento del usuario, por simplicidad, utilizamos P(i) tal como se define exactamente en la ecuaciÃ³n 6. La ganancia G(pi, q) del pasaje pi con respecto a la consulta q es una funciÃ³n de - 1) el nÃºmero de pepitas relevantes presentes en pi, y 2) la novedad de cada una de estas pepitas. Combinamos estos dos factores de la siguiente manera. Para cada pepita Nj, asignamos un peso inicial wj, y tambiÃ©n mantenemos un conteo nj del nÃºmero de veces que esta pepita ha sido vista por el usuario en el pasado. La ganancia derivada de cada ocurrencia posterior del mismo hallazgo se asume que se reduce por un factor de amortiguamiento Î³. Por lo tanto, G(pi, q) se define como G(pi, q) = X Nj âC(pi,q) wj â Î³nj (7) donde C(pi, q) es el conjunto de todas las pepitas que aparecen en el pasaje pi y tambiÃ©n pertenecen a la clave de respuesta de la consulta q. Los pesos iniciales wj se establecen todos en 1.0 en nuestros experimentos, pero tambiÃ©n pueden establecerse segÃºn un enfoque de pirÃ¡mide [11]. La elecciÃ³n del factor de amortiguaciÃ³n Î³ determina la tolerancia del usuario a la redundancia. Cuando Î³ = 0, un pepita solo recibirÃ¡ crÃ©dito por su primera ocurrencia, es decir, cuando nj sea cero. Para 0 < Î³ < 1, un nugget recibe menos crÃ©dito por cada ocurrencia sucesiva. Cuando Î³ = 1, no se produce amortiguamiento y las repeticiones de un valor reciben el mismo crÃ©dito. Se debe tener en cuenta que los recuentos de ocurrencia de las pepitas se conservan entre la evaluaciÃ³n de las listas clasificadas sucesivas devueltas por el sistema, ya que se espera que los usuarios recuerden lo que el sistema les mostrÃ³ en el pasado. Definimos la pÃ©rdida L(pi, q) como un costo constante c (usamos 0.1) incurrido al leer un pasaje devuelto por el sistema. Por lo tanto, nuestra mÃ©trica puede ser reescrita como U(q) = nX i=1 Ganancia(pi, q) logb(b + i â 1) â L(n) (8) donde L(n) es la pÃ©rdida asociada con una lista clasificada de longitud n: L(n) = c Â· nX i=1 1 logb(b + i â 1) (9) 3 NÃ³tese que 00 = 1 Debido a la similitud con la Ganancia Acumulada Descontada (DCG), llamamos a nuestra mÃ©trica Utilidad Acumulada Descontada (DCU). El puntaje de DCU obtenido por el sistema se convierte en un puntaje de DCU normalizado (NDCU) dividiÃ©ndolo por el puntaje de DCU de la lista ideal clasificada, que se crea ordenando los pasajes por sus puntajes de utilidad decrecientes U(pi, q) y deteniÃ©ndose cuando U(pi, q) â¤ 0, es decir, cuando la ganancia es menor o igual al costo de leer el pasaje. El conjunto de datos TDT4 fue el corpus de referencia utilizado en las evaluaciones TDT2002 y TDT2003. El corpus consiste en mÃ¡s de 90,000 artÃ­culos de noticias de mÃºltiples fuentes (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) publicados entre octubre de 2000 y enero de 2001, en los idiomas Ã¡rabe, inglÃ©s y mandarÃ­n. Las versiones de los artÃ­culos en otros idiomas reconocidas por voz y traducidas por mÃ¡quina tambiÃ©n fueron proporcionadas. LDC [18] ha anotado el corpus con 100 temas, que corresponden a varios eventos de noticias en este perÃ­odo de tiempo. De entre estos, seleccionamos un subconjunto de 12 eventos accionables y definimos tareas correspondientes para ellos. Para cada tarea, definimos manualmente un perfil que consiste en un conjunto inicial de (5 a 10) consultas, una descripciÃ³n de texto libre del historial del usuario, es decir, lo que el usuario ya sabe sobre el evento, y una lista de documentos conocidos relevantes y no relevantes (si estÃ¡n disponibles) como ejemplos de entrenamiento. Para cada consulta, generamos claves de respuestas y reglas de coincidencia de pepitas correspondientes utilizando el procedimiento descrito en la secciÃ³n 4.1.2, y produjimos un total de 120 consultas, con un promedio de 7 pepitas por consulta. EXPERIMENTOS Y RESULTADOS 6.1 Baselines Utilizamos Indri [17], un motor de bÃºsqueda basado en modelos de lenguaje popular, como referencia para comparar con CAFÃ. Indri admite la funcionalidad estÃ¡ndar de los motores de bÃºsqueda, incluido el feedback de pseudo-relevancia (PRF) [3, 6], y es representativo de un sistema de recuperaciÃ³n basado en consultas tÃ­pico. Indri no admite ningÃºn tipo de detecciÃ³n de novedades. Comparamos Indri con PRF activado y desactivado, frente a CAFÃ con retroalimentaciÃ³n del usuario, detecciÃ³n de novedades y clasificaciÃ³n antirredundante activados y desactivados. 6.2 ConfiguraciÃ³n Experimental Dividimos el corpus TDT4 que abarca 4 meses en 10 fragmentos, cada uno definido como un perÃ­odo de 12 dÃ­as consecutivos. En cualquier momento dado del proceso de destilaciÃ³n, cada sistema accedÃ­a a los datos pasados hasta el punto actual, y devolvÃ­a una lista clasificada de hasta 50 pasajes por consulta. Las 12 tareas definidas en el corpus se dividieron en un conjunto de entrenamiento y prueba con 6 tareas cada uno. Cada sistema pudo utilizar el conjunto de entrenamiento para ajustar sus parÃ¡metros con el fin de optimizar NDCU (ecuaciÃ³n 8), incluyendo el umbral de relevancia tanto para Indri como para CAFÂ´E, y los umbrales de novedad y antirredundancia para CAFÂ´E. La NDCU para cada ejecuciÃ³n del sistema se calcula automÃ¡ticamente. TambiÃ©n se simulÃ³ la retroalimentaciÃ³n de los usuarios: los juicios de relevancia para cada pasaje devuelto por el sistema (segÃºn las reglas de coincidencia de pepitas descritas en la secciÃ³n 4.1.2) fueron utilizados como retroalimentaciÃ³n de los usuarios en la adaptaciÃ³n de perfiles de consulta e historiales de usuario. En la Tabla 1, mostramos los puntajes NDCU de los dos sistemas en diferentes configuraciones. Estas puntuaciones se promedian en las seis tareas del conjunto de pruebas, y se calculan con dos factores de amortiguaciÃ³n (ver secciÃ³n 4.2): Î³ = 0 y 0.1, para simular ninguna tolerancia y poca tolerancia para la redundancia, respectivamente. Usar Î³ = 0 crea una mÃ©trica mucho mÃ¡s estricta ya que no otorga ningÃºn crÃ©dito a un pasaje que contenga informaciÃ³n relevante pero redundante. Por lo tanto, la mejora obtenida al habilitar la retroalimentaciÃ³n del usuario es menor con Î³ = 0 que la mejora obtenida de la retroalimentaciÃ³n con Î³ = 0.1. Esto revela una deficiencia de los sistemas de recuperaciÃ³n contemporÃ¡neos: cuando el usuario da retroalimentaciÃ³n positiva sobre un pasaje, los sistemas otorgan mayor peso a los tÃ©rminos presentes en ese pasaje y tienden a recuperar otros pasajes que contienen los mismos tÃ©rminos, y por lo tanto, generalmente la misma informaciÃ³n. Sin embargo, el usuario no se beneficia al ver pasajes redundantes y generalmente estÃ¡ interesado en otros pasajes que contengan informaciÃ³n relacionada. Es informativo evaluar los sistemas de recuperaciÃ³n utilizando nuestra medida de utilidad (con Î³ = 0) que tiene en cuenta la novedad y, por lo tanto, ofrece una imagen mÃ¡s realista de cuÃ¡n bien un sistema puede generalizar a partir de la retroalimentaciÃ³n del usuario, en lugar de utilizar medidas tradicionales de RI como la recuperaciÃ³n y la precisiÃ³n, que ofrecen una imagen incompleta de la mejora obtenida a partir de la retroalimentaciÃ³n del usuario. A veces, sin embargo, los usuarios podrÃ­an estar interesados en ver la misma informaciÃ³n de mÃºltiples fuentes, como un indicador de su importancia o fiabilidad. En tal caso, simplemente podemos elegir un valor mÃ¡s alto para Î³ que corresponda a una mayor tolerancia para la redundancia, y asÃ­ permitir que el sistema ajuste sus parÃ¡metros en consecuencia. Dado que los documentos fueron procesados por fragmentos, serÃ­a interesante ver cÃ³mo mejora el rendimiento de los sistemas con el tiempo. Las figuras 1 y 2 muestran las tendencias de rendimiento de ambos sistemas a lo largo de los fragmentos. Si bien se espera que el rendimiento con y sin retroalimentaciÃ³n en los primeros fragmentos sea similar, para los fragmentos posteriores, la curva de rendimiento con la retroalimentaciÃ³n habilitada supera a la del ajuste sin retroalimentaciÃ³n. Las tendencias de rendimiento no son consistentes en todos los fragmentos porque los documentos relevantes no estÃ¡n distribuidos uniformemente en todos los fragmentos, lo que hace que algunas consultas sean mÃ¡s fÃ¡ciles que otras en ciertos fragmentos. AdemÃ¡s, dado que Indri utiliza retroalimentaciÃ³n de pseudo-relevancia mientras que CAFÂ´E utiliza retroalimentaciÃ³n basada en juicios de relevancia reales, la mejora en el caso de Indri es menos dramÃ¡tica que la de CAFÂ´E. 7. CONCLUSIONES FINALES Este artÃ­culo presenta la primera investigaciÃ³n sobre la destilaciÃ³n de informaciÃ³n basada en la utilidad con un sistema que aprende las necesidades de informaciÃ³n duraderas a partir de la retroalimentaciÃ³n detallada del usuario sobre una secuencia de pasajes clasificados. Nuestro sistema, llamado CAFÂ´E, combina filtrado adaptativo, detecciÃ³n de novedades y clasificaciÃ³n de pasajes antirredundantes en un marco unificado para la optimizaciÃ³n de la utilidad. Desarrollamos un nuevo esquema para la evaluaciÃ³n y retroalimentaciÃ³n automatizadas basado en un procedimiento semiautomÃ¡tico para adquirir reglas que permitan emparejar automÃ¡ticamente fragmentos con las respuestas del sistema. TambiÃ©n propusimos una extensiÃ³n de la mÃ©trica NDCG para evaluar la utilidad de los pasajes clasificados como una combinaciÃ³n ponderada de relevancia y novedad. Nuestros experimentos en el reciÃ©n anotado corpus de referencia TDT4 muestran un alentador aumento de utilidad sobre Indri, y tambiÃ©n sobre nuestro propio sistema con aprendizaje incremental y detecciÃ³n de novedades desactivados. 8. AGRADECIMIENTOS Nos gustarÃ­a agradecer a Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng y al Programa de AnÃ¡lisis de Datos Cualitativos de la Universidad de Pittsburgh liderado por el Dr. Stuart Shulman por su ayuda en la recolecciÃ³n y procesamiento de las anotaciones extendidas de TDT4 utilizadas en nuestros experimentos. Este trabajo estÃ¡ parcialmente respaldado por la FundaciÃ³n Nacional de Ciencias (NSF) bajo la subvenciÃ³n IIS0434035, y la Agencia de Proyectos de InvestigaciÃ³n Avanzada de Defensa (DARPA) bajo los contratos NBCHD030010 y W0550432. Cualquier opiniÃ³n, hallazgo, conclusiÃ³n o recomendaciÃ³n expresada en este material son responsabilidad de los autores y no reflejan necesariamente los puntos de vista de los patrocinadores. AUTORES ADICIONALES Jian Zhang (jianzhan@stat.purdue.edu)â , Jaime Carbonell (jgc@cs.cmu.edu)â  , Peter Brusilovsky (peterb+@pitt.edu)â¡ , Daqing He(dah44@pitt.edu)â¡ 10. REFERENCIAS [1] J. Allan. RetroalimentaciÃ³n incremental de relevancia para la filtraciÃ³n de informaciÃ³n. Actas de la 19Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 270-278, 1996. [2] J. Allan, C. Wade y A. Bolivar. RecuperaciÃ³n y DetecciÃ³n de Novedad a Nivel de OraciÃ³n. Actas de la conferencia ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, 2003. [3] C. Buckley, G. Salton y J. Allan. RecuperaciÃ³n automÃ¡tica con informaciÃ³n de localidad utilizando SMART. PublicaciÃ³n especial del NIST, (500207):59-72, 1993. [4] J. Callan. Aprendiendo mientras se filtran documentos. Actas de la 21Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 224-231, 1998. [5] J. Carbonell y J. Goldstein. El uso de MMR, Reordenamiento basado en Diversidad para Reorganizar Documentos y Producir ResÃºmenes. Actas de la 21Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 335-336, 1998. [6] E. Efthimiadis. ExpansiÃ³n de consulta. RevisiÃ³n Anual de Ciencia de la InformaciÃ³n y TecnologÃ­a (ARIST), 31:p121-87, 1996. [7] J. Fiscus y G. Duddington. Resumen de DetecciÃ³n y Seguimiento de Temas. DetecciÃ³n y seguimiento de temas: OrganizaciÃ³n de la informaciÃ³n basada en eventos, pÃ¡ginas 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov y S. Roukos. Un modelo estadÃ­stico para la detecciÃ³n y seguimiento de entidades multilingÃ¼es. NAACL/HLT, 2004. [9] K. JÃ¤rvelin y J. KekÃ¤lÃ¤inen. EvaluaciÃ³n basada en la Ganancia Acumulada de TÃ©cnicas de RecuperaciÃ³n de InformaciÃ³n. ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin y D. Demner-Fushman. EvaluaciÃ³n automÃ¡tica de respuestas a preguntas de definiciÃ³n. Actas de la Conferencia de TecnologÃ­a del Lenguaje Humano y Conferencia sobre MÃ©todos EmpÃ­ricos en Procesamiento del Lenguaje Natural (HLT/EMNLP 2005), 2005. â Departamento de EstadÃ­stica, Universidad de Purdue, West Lafayette, EE. UU. â  Instituto de TecnologÃ­as del Lenguaje, Universidad Carnegie Mellon, Pittsburgh, EE. UU. â¡ Escuela de Ciencias de la InformaciÃ³n, Universidad de Pittsburgh, Pittsburgh, EE. UU. [11] J. Lin y D. Demner-Fushman. Â¿Se derrumbarÃ¡n las pirÃ¡mides construidas con pepitas? Actas de HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla y S. Roukos. Un algoritmo de resoluciÃ³n de correferencia sÃ­ncrona basado en el Ã¡rbol de Bell. Actas de ACL, 4:136-143, 2004. [13] G. Marton. Nuggeteer: EvaluaciÃ³n AutomÃ¡tica Basada en Pepitas Utilizando Descripciones y Juicios. HLT/NAACL, 2006. [14] E. Riloff. ConstrucciÃ³n automÃ¡tica de un diccionario para tareas de extracciÃ³n de informaciÃ³n. Actas de la UndÃ©cima Conferencia Nacional sobre Inteligencia Artificial, pÃ¡ginas 811-816, 1993. [15] S. Robertson y S. Walker. Microsoft Cambridge en TREC-9: Pista de filtrado. La Novena Conferencia de RecuperaciÃ³n de Texto (TREC-9), pÃ¡ginas 361-368. [16] R. Schapire, Y. Cantante, y A. Singhal. Boosting y Rocchio aplicados a la filtraciÃ³n de texto. Actas de la 21Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle y W. Croft. Indri: Un motor de bÃºsqueda basado en modelos de lenguaje para consultas complejas. Actas de la Conferencia Internacional sobre AnÃ¡lisis de Inteligencia, 2004. [18] El Consorcio de Datos LingÃ¼Ã­sticos. http://www.ldc.upenn.edu/. [19] E. Voorhees. Resumen de la pista de preguntas y respuestas TREC 2003. Actas de la DuodÃ©cima Conferencia de RecuperaciÃ³n de Texto (TREC 2003), 2003. [20] Y. Yang y B. Kisiel. RegresiÃ³n local basada en mÃ¡rgenes para filtrado adaptativo. Actas de la duodÃ©cima conferencia internacional sobre gestiÃ³n de la informaciÃ³n y el conocimiento, pÃ¡ginas 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang y B. Kisiel. Robustez de los MÃ©todos de Filtrado Adaptativo en una EvaluaciÃ³n de Referencia Cruzada. Actas de la 28Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 98-105, 2005. [22] C. Zhai, W. Cohen y J. Lafferty. MÃ¡s allÃ¡ de la relevancia independiente: MÃ©todos y mÃ©tricas de evaluaciÃ³n para la recuperaciÃ³n de subtemas. Actas de la 26Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 10-17, 2003. [23] J. Zhang y Y. Yang. Robustez de los MÃ©todos de ClasificaciÃ³n Lineal Regularizados en la CategorizaciÃ³n de Textos. Actas de la 26Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 190-197, 2003. [24] Y. Zhang. Utilizando priors bayesianos para combinar clasificadores para filtrado adaptativo. Actas de la 27Âª conferencia internacional anual sobre InvestigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 345-352, 2004. [25] Y. Zhang, J. Callan y T. Minka. DetecciÃ³n de novedad y redundancia en el filtrado adaptativo. Actas de la 25Âª Conferencia Internacional Anual de ACM SIGIR sobre InvestigaciÃ³n y Desarrollo en RecuperaciÃ³n de InformaciÃ³n, 2002.",
    "original_sentences": [
        "Utility-based Information Distillation Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst.",
        "Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst.",
        "Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst.",
        "Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst.",
        "Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst.",
        "Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst.",
        "Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu ABSTRACT This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework.",
        "It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant passage ranking with respect to long-lasting information needs (tasks with multiple queries).",
        "Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings.",
        "For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task.",
        "Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses.",
        "We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty.",
        "Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components.",
        "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation. 1.",
        "INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval.",
        "Adaptive filtering (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics.",
        "Based on the initial query and a few positive examples (if available), an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user.",
        "The incremental learning nature of AF systems makes them more powerful than standard search engines that support ad-hoc retrieval (e.g.",
        "Google and Yahoo) in terms of finding relevant information with respect to long-lasting topics of interest, and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs, without having to modify their queries manually.",
        "A variety of supervised learning algorithms (Rocchio-style classifiers, Exponential-Gaussian models, local regression and logistic regression approaches) have been studied for adaptive settings, examined with explicit and implicit relevance feedback, and evaluated with respect to utility optimization on large benchmark data collections in TREC (Text Retrieval Conferences) and TDT (Topic Detection and Tracking) forums [1, 4, 7, 15, 16, 20, 24, 23].",
        "Regularized logistic regression [21] has been found representative for the state-of-the-art approaches, and highly efficient for frequent model adaptations over large document collections such as the TREC-10 corpus (over 800,000 documents and 84 topics).",
        "Despite substantial achievements in recent adaptive filtering research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently.",
        "Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications: 1.",
        "User has a rather passive role in the conventional adaptive filtering setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision.",
        "A more active alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents (or passages) per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists.",
        "The latter form of user interaction has been highly effective in standard retrieval for ad-hoc queries.",
        "How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2.",
        "The unit for receiving a relevance judgment (yes or no) is restricted to the document level in conventional AF.",
        "However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant.",
        "Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system.",
        "For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3.",
        "System-selected documents are often highly redundant.",
        "A major news event, for example, would be reported by multiple sources repeatedly for a while, making most of the information content in those articles redundant with each other.",
        "A conventional AF system would select all these redundant news stories for user feedback, wasting the users time while offering little gain.",
        "Clearly, techniques for novelty detection can help in principle [25, 2, 22] for improving the utility of the AF systems.",
        "However, the effectiveness of such techniques at passage level to detect novelty with respect to users (fine-grained) feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user.",
        "To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and novelty detection for removal of redundancy from user interactions with the system.",
        "We call the new process utility-based information distillation.",
        "Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach.",
        "Therefore, we extended a benchmark corpus - the TDT4 collection of news stories and TV broadcasts - with task definitions, multiple queries per task, and answer keys per query.",
        "We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 .",
        "To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses.",
        "Moreover, we propose an extension of NDCG (Normalized Discounted Cumulated Gain) [9] for assessing the utility of ranked passages as a function of both relevance and novelty.",
        "The rest of this paper is organized as follows.",
        "Section 2 outlines the information distillation process with a concrete example.",
        "Section 3 describes the technical cores of our system called CAFÂ´E - CMU Adaptive Filtering Engine.",
        "Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme.",
        "Section 5 describes the extended TDT4 corpus.",
        "Section 6 presents our experiments and results.",
        "Section 7 concludes the study and gives future perspectives. 2.",
        "A SAMPLE TASK Consider a news event - the escape of seven convicts from a Texas prison in December 2000 and their capture a month later.",
        "Assuming a user were interested in this event since its early stage, the information need could be: Find information about the escape of convicts from Texas prison, and information related to their recapture.",
        "The associated lower-level questions could be: 1.",
        "How many prisoners escaped? 2.",
        "Where and when were they sighted? 3. Who are their known contacts inside and outside the prison? 4.",
        "How are they armed? 5.",
        "Do they have any vehicles? 6.",
        "What steps have been taken so far?",
        "We call such an information need a task, and the associated questions as the queries in this task.",
        "A distillation system is supposed to monitor the incoming documents, process them chunk by chunk in a temporal order, select potentially relevant and novel passages from each chunk with respect to each query, and present a ranked list of passages to the user.",
        "Passage ranking here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list.",
        "When presented with a list of passages, the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant.",
        "These spans of text are taken as positive examples in the adaptation of the query profile, and also added to the users history.",
        "Passages not marked by the user are taken as negative examples.",
        "As soon as the query profile is updated, the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low, based on user preference.",
        "For example, if the user highlights ...officials have posted a $100,000 reward for their capture... as relevant answer to the query What steps have been taken so far?, then the highlighted piece is used as an additional positive training example in the adaptation of the query profile.",
        "This piece of feedback is also added to the user history as a seen example, so that in future, the system will not place another passage mentioning $100,000 reward at the top of the ranked list.",
        "However, an article mentioning ...officials have doubled the reward money to $200,000... might be ranked high since it is both relevant to the (updated) query profile and novel with respect to the (updated) user history.",
        "The user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly.",
        "Clearly, novelty detection is very important for the utility of such a system because of the iterative search.",
        "Without novelty detection, the old relevant passages would be shown to the user repeatedly in each ranked list.",
        "Through the above example, we can see the main properties of our new framework for utility-based information distillation over temporally ordered documents.",
        "Our framework combines and extends the power of adaptive filtering (AF), ad-hoc retrieval (IR) and novelty detection (ND).",
        "Compared to standard IR, our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task.",
        "Compared to conventional AF, it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system.",
        "Compared to past work, this is the first evaluation of novelty detection integrated with adaptive filtering for sequenced queries that allows flexible user feedback over ranked passages.",
        "The combination of AF, IR and ND with the new extensions raises an important research question regarding evaluation methodology: how can we measure the utility of such an information distillation system?",
        "Existing metrics in standard IR, AF and ND are insufficient, and new solutions must be explored, as we will discuss in Section 4, after describing the technical cores of our system in the next section. 3.",
        "TECHNICAL CORES The core components of CAFÂ´E are - 1) AF for incremental learning of query profiles, 2) IR for estimating relevance of passages with respect to query profiles, 3) ND for assessing novelty of passages with respect to users history, and 4) anti-redundancy component to remove redundancy from ranked lists. 3.1 Adaptive Filtering Component We use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for AF [21].",
        "Logistic regression (LR) is a supervised learning algorithm for statistical classification.",
        "Based on a training set of labeled instances, it learns a class model which can then by used to predict the labels of unseen instances.",
        "Its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required, as is the case in adaptive filtering, where the system must learn from each new feedback provided by the user. (See [21] and [23] for computational complexity and implementation issues).",
        "In adaptive filtering, each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query.",
        "For training the model, we use the query itself as the initial positive training example of the class, and the user-highlighted pieces of text (marked as Relevant or Not-relevant) during feedback as additional training examples.",
        "To address the cold start issue in the early stage before any user feedback is obtained, the system uses a small sample from a retrospective corpus as the initial negative examples in the training set.",
        "The details of using logistic regression for adaptive filtering (assigning different weights to positive and negative training instances, and regularizing the objective function to prevent over-fitting on training data) are presented in [21].",
        "The class model wâ learned by Logistic Regression, or the query profile, is a vector whose dimensions are individual terms and whose elements are the regression coefficients, indicating how influential each term is in the query profile.",
        "The query profile is updated whenever a new piece of user feedback is received.",
        "A temporally decaying weight can be applied to each training example, as an option, to emphasize the most recent user feedback. 3.2 Passage Retrieval Component We use standard IR techniques in this part of our system.",
        "Incoming documents are processed in chunks, where each chunk can be defined as a fixed span of time or as a fixed number of documents, as preferred by the user.",
        "For each incoming document, corpus statistics like the IDF (Inverted Document Frequency) of each term are updated.",
        "We use a state-of-the-art named entity identifier and tracker [8, 12] to identify person and location names, and merge them with co-referent named entities seen in the past.",
        "Then the documents are segmented into passages, which can be a whole document, a paragraph, a sentence, or any other continuous span of text, as preferred.",
        "Each passage is represented using a vector of TF-IDF (Term FrequencyInverse Document Frequency) weights, where term can be a word or a named entity.",
        "Given a query profile, i.e. the logistic regression solution wâ as described in Section 3.1, the system computes the posterior probability of relevance for each passage x as fRL(x) â¡ P(y = 1|x, wâ ) = 1 (1 + eâwâÂ·x) (1) Passages are ordered by their relevance scores, and the ones with scores above a threshold (tuned on a training set) comprise the relevance list that is passed on to the novelty detection step. 3.3 Novelty Detection Component CAFÂ´E maintains a user history H(t), which contains all the spans of text hi that the user highlighted (as feedback) during his or her past interactions with the system, up to the current time t. Denoting the history as H(t) = n h1, h2, ..., ht o , (2) the novelty score of a new candidate passage x is computed as: fND(x) = 1 â max iâ1..t {cos(x, hi)} (3) where both candidate passage x and highlighted spans of text hi are represented as TF-IDF vectors.",
        "The novelty score of each passage is compared to a prespecified threshold (also tuned on a training set), and any passage with a score below this threshold is removed from the relevance list. 3.4 Anti-redundant Ranking Component Although the novelty detection component ensures that only novel (previously unseen) information remains in the relevance list, this list might still contain the same novel information at multiple positions in the ranked list.",
        "Suppose, for example, that the user has already read about a $100,000 reward for information about the escaped convicts.",
        "A new piece of news that the award has been increased to $200,000 is novel since the user hasnt read about it yet.",
        "However, multiple news sources would report this news and we might end up showing (redundant) articles from all these sources in a ranked list.",
        "Hence, a ranked list should also be made non-redundant with respect to its own contents.",
        "We use a simplified version of the Maximal Marginal Relevance method [5], originally developed for combining relevance and novelty in text retrieval and summarization.",
        "Our procedure starts with the current list of passages sorted by relevance (section 3.2), filtered by Novelty Detection component (section 3.3), and generates a new non-redundant list as follows: 1.",
        "Take the top passage in the current list as the top one in the new list. 2.",
        "Add the next passage x in the current list to the new list only if fAR(x) > t where fAR(x) = 1 â max piâLnew {cos(x, pi)} and Lnew is the set of passages already selected in the new list. 3.",
        "Repeat step 2 until all the passages in the current list have been examined.",
        "After applying the above-mentioned algorithm, each passage in the new list is sufficiently dissimilar to others, thus favoring diversity rather than redundancy in the new ranked list.",
        "The anti-redundancy threshold t is tuned on a training set. 4.",
        "EVALUATION METHODOLOGY The approach we proposed above for information distillation raises important issues regarding evaluation methodology.",
        "Firstly, since our framework allows the output to be passages at different levels of granularity (e.g. k-sentence windows where k may vary) instead of a fixed length, it is not possible to have pre-annotated relevance judgments at all such granularity levels.",
        "Secondly, since we wish to measure the utility of the system output as a combination of both relevance and novelty, traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time.",
        "Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists.",
        "None of the existing measures in ad-hoc retrieval, adaptive filtering, novelty detection or other related areas (text summarization and question answering) have desirable properties in all the three aspects.",
        "Therefore, we must develop a new evaluation methodology. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers.",
        "Answer keys define what should be present in a system response to receive credit, and are comprised of a collection of information nuggets, i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them.",
        "Defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping [19], since system-returned passages can contain the same information expressed in many different ways.",
        "Hence, QA evaluations have relied on human assessors for the mapping between various expressions, making the process costly, time consuming, and not scalable to large query and document collections, and extensive system evaluations with various parameter settings. 4.1.1 Automating Evaluation based on Answer Keys Automatic evaluation methods would allow for faster system building and tuning, as well as provide an objective and affordable way of comparing various systems.",
        "Recently, such methods have been proposed, more or less, based on the idea of n-gram co-occurrences.",
        "Pourpre [10] assigns a fractional recall score to a system response based on its unigram overlap with a given nuggets description.",
        "For example, a system response A B C has recall 3/4 with respect to a nugget with description A B C D. However, such an approach is unfair to systems that present the same information but using words other than A, B, C, and D. Another open issue is how to weight individual words in measuring the closeness of a match.",
        "For example, consider the question How many prisoners escaped?.",
        "In the nugget Seven prisoners escaped from a Texas prison, there is no indication that seven is the keyword, and that it must be matched to get any relevance credit.",
        "Using IDF values does not help, since seven will generally not have a higher IDF than words like texas and prison.",
        "Also, redefining the nugget as just seven does not solve the problem since now it might spuriously match any mention of seven out of context.",
        "Nuggeteer [13] works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold.",
        "However, it is also plagued by spurious relevance since not all words contained in the nugget description (or known correct responses) are central to the nugget. 4.1.2 Nugget-Matching Rules We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on nugget-matching rules, which are generated using a semiautomatic procedure explained below.",
        "These rules are essentially Boolean queries that will only match against snippets that contain the nugget.",
        "For instance, a candidate rule for matching answers to How many prisoners escaped? is (Texas AND seven AND escape AND (convicts OR prisoners)), possibly with other synonyms and variants in the rule.",
        "For a corpus of news articles, which usually follow a typical formal prose, it is fairly easy to write such simple rules to match expected answers using a bootstrap approach, as described below.",
        "We propose a two-stage approach, inspired by Autoslog [14], that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents.",
        "In the first stage, human subjects annotated (using a highlighting tool) portions of ontopic documents that contained answers to each nugget 2 .",
        "In the second stage, subjects used our rule generation tool to create rules that would match the annotations for each nugget.",
        "The tool allows users to enter a Boolean rule as a disjunction of conjunctions (e.g. ((a AND b) OR (a AND c AND d) OR (e))).",
        "Given a candidate rule, our tool uses it as a Boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match.",
        "Hence, the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision.",
        "We observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions (adding more ANDs), and improving the recall by adding more conjunctions to the disjunction (adding more ORs).",
        "As an example, lets try to create a rule for the nugget which says that seven prisoners escaped from the Texas prison.",
        "We start with a simple rule - (seven).",
        "When we input this into the rule generation tool, we realize that this rule matches many spurious occurrences of seven (e.g. ...seven states...) and thus gets a low precision score.",
        "We can further qualify our rule - Texas AND seven AND convicts.",
        "Next, by looking at the missed annotations, we realize that some news articles mentioned ...seven prisoners escaped.... We then replace convicts with the disjunction (convicts OR prisoners).",
        "We continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the (small number of) misses and false alarms can be safely ignored.",
        "Thus we can create nugget-matching rules that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses.",
        "Human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably. 4.2 Evaluating the Utility of a Sequence of Ranked Lists The utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information, and how much the user lost in terms of time and energy.",
        "We calculate this utility from the utilities of individual passages as follows.",
        "After reading each passage returned by the system, the user derives some gain depending on the presence of relevant and novel information, and incurs a loss in terms of the time and energy spent in going through the passage.",
        "However, the likelihood that the user would actually read a passage depends on its position in the ranked list.",
        "Hence, for a query q, the expected utility 2 LDC [18] already provides relevance judgments for 100 topics on the TDT4 corpus.",
        "We further ensured that these judgments are exhaustive on the entire corpus using pooling. of a passage pi at rank i can be defined as U(pi, q) = P(i) â (Gain(pi, q) â Loss(pi, q)) (4) where P(i) is the probability that the user would go through a passage at rank i.",
        "The expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage: U(q) = nX i=1 P(i) â (Gain(pi, q) â Loss(pi, q)) (5) Note that if we ignore the loss term and define P(i) as P(i) â 1/ logb(b + i â 1) (6) then we get the recently popularized metric called Discounted Cumulated Gain (DCG) [9], where Gain(pi, q) is defined as the graded relevance of passage pi.",
        "However, without the loss term, DCG is a purely recall-oriented metric and not suitable for an adaptive filtering setting, where the systems utility depends in part on its ability to limit the number of items shown to the user.",
        "Although P(i) could be defined based on empirical studies of user behavior, for simplicity, we use P(i) exactly as defined in equation 6.",
        "The gain G(pi, q) of passage pi with respect to the query q is a function of - 1) the number of relevant nuggets present in pi, and 2) the novelty of each of these nuggets.",
        "We combine these two factors as follows.",
        "For each nugget Nj, we assign an initial weight wj, and also keep a count nj of the number of times this nugget has been seen by the user in the past.",
        "The gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor Î³.",
        "Thus, G(pi, q) is defined as G(pi, q) = X Nj âC(pi,q) wj â Î³nj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the answer key of query q.",
        "The initial weights wj are all set of be 1.0 in our experiments, but can also be set based on a pyramid approach [11].",
        "The choice of dampening factor Î³ determines the users tolerance for redundancy.",
        "When Î³ = 0, a nugget will only receive credit for its first occurrence i.e. when nj is zero3 .",
        "For 0 < Î³ < 1, a nugget receives smaller credit for each successive occurrence.",
        "When Î³ = 1, no dampening occurs and repeated occurrences of a nugget receive the same credit.",
        "Note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system, since the users are expected to remember what the system showed them in the past.",
        "We define the loss L(pi, q) as a constant cost c (we use 0.1) incurred when reading a system-returned passage.",
        "Thus, our metric can be re-written as U(q) = nX i=1 Gain(pi, q) logb(b + i â 1) â L(n) (8) where L(n) is the loss associated with a ranked list of length n: L(n) = c Â· nX i=1 1 logb(b + i â 1) (9) 3 Note that 00 = 1 Due to the similarity with Discounted Cumulated Gain (DCG), we call our metric Discounted Cumulated Utility (DCU).",
        "The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) â¤ 0 i.e. when the gain is less than or equal to the cost of reading the passage. 5.",
        "DATA TDT4 was the benchmark corpus used in TDT2002 and TDT2003 evaluations.",
        "The corpus consists of over 90, 000 news articles from multiple sources (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) published between October 2000 and January 2001, in the languages of Arabic, English, and Mandarin.",
        "Speech-recognized and machine-translated versions of the non-English articles were provided as well.",
        "LDC [18] has annotated the corpus with 100 topics, that correspond to various news events in this time period.",
        "Out of these, we selected a subset of 12 actionable events, and defined corresponding tasks for them4 .",
        "For each task, we manually defined a profile consisting of an initial set of (5 to 10) queries, a free-text description of the user history, i.e., what the user already knows about the event, and a list of known on-topic and off-topic documents (if available) as training examples.",
        "For each query, we generated answer keys and corresponding nugget matching rules using the procedure described in section 4.1.2, and produced a total of 120 queries, with an average of 7 nuggets per query. 6.",
        "EXPERIMENTS AND RESULTS 6.1 Baselines We used Indri [17], a popular language-model based retrieval engine, as a baseline for comparison with CAFÂ´E.",
        "Indri supports standard search engine functionality, including pseudo-relevance feedback (PRF) [3, 6], and is representative of a typical query-based retrieval system.",
        "Indri does not support any kind of novelty detection.",
        "We compare Indri with PRF turned on and off, against CAFÂ´E with user feedback, novelty detection and antiredundant ranking turned on and off. 6.2 Experimental Setup We divided the TDT4 corpus spanning 4 months into 10 chunks, each defined as a period of 12 consecutive days.",
        "At any given point of time in the distillation process, each system accessed the past data up to the current point, and returned a ranked list of up 50 passages per query.",
        "The 12 tasks defined on the corpus were divided into a training and test set with 6 tasks each.",
        "Each system was allowed to use the training set to tune its parameters for optimizing NDCU (equation 8), including the relevance threshold for both Indri and CAFÂ´E, and the novelty and antiredundancy thresholds for CAFÂ´E.",
        "The NDCU for each system run is calculated automatically.",
        "User feedback was also simulated - relevance judgments for each system-returned passage (as determined by the nugget matching rules described in section 4.1.2) were 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figure 1: Performance of Indri across chunks Figure 2: Performance of CAFÂ´E across chunks used as user feedback in the adaptation of query profiles and user histories. 6.3 Results In Table 1, we show the NDCU scores of the two systems under various settings.",
        "These scores are averaged over the six tasks in the test set, and are calculated with two dampening factors (see section 4.2): Î³ = 0 and 0.1, to simulate no tolerance and small tolerance for redundancy, respectively.",
        "Using Î³ = 0 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information.",
        "Hence, the improvement obtained from enabling user feedback is smaller with Î³ = 0 than the improvement obtained from feedback with Î³ = 0.1.",
        "This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information.",
        "However, the user does not benefit from seeing such redundant passages, and is usually interested in other passages containing related information.",
        "It is informative to evaluate retrieval systems using our utility measure (with Î³ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.",
        "Sometimes, however, users might indeed be interested in seeing the same information from multiple sources, as an Table 1: NDCU Scores of Indri and CAFÂ´E for two dampening factors (Î³), and various settings (F: Feedback, N: Novelty Detection, A: Anti-Redundant Ranking) Indri CAFÂ´E Î³ Base +PRF Base +F +F+N +F+A +F+N+A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.35 0.36 0.36 indicator of its importance or reliability.",
        "In such a case, we can simply choose a higher value for Î³ which corresponds to a higher tolerance for redundancy, and hence let the system tune its parameters accordingly.",
        "Since documents were processed chunk by chunk, it would be interesting to see how the performance of systems improves over time.",
        "Figures 1 and 2 show the performance trends for both the systems across chunks.",
        "While the performance with and without feedback on the first few chunks is expected to be close, for subsequent chunks, the performance curve with feedback enabled rises above the one with the no-feedback setting.",
        "The performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks, making some queries easier than others in certain chunks.",
        "Moreover, since Indri uses pseudo-relevance feedback while CAFÂ´E uses feedback based on actual relevance judgments, the improvement in case of Indri is less dramatic than that of CAFÂ´E. 7.",
        "CONCLUDING REMARKS This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages.",
        "Our system, called CAFÂ´E, combines adaptive filtering, novelty detection and antiredundant passage ranking in a unified framework for utility optimization.",
        "We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses.",
        "We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty.",
        "Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off. 8.",
        "ACKNOWLEDGMENTS We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments.",
        "This work is supported in parts by the National Science Foundation (NSF) under grant IIS0434035, and the Defense Advanced Research Project Agency (DARPA) under contracts NBCHD030010 and W0550432.",
        "Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. 9.",
        "ADDITIONAL AUTHORS Jian Zhang (jianzhan@stat.purdue.edu)â , Jaime Carbonell (jgc@cs.cmu.edu)â  , Peter Brusilovsky (peterb+@pitt.edu)â¡ , Daqing He(dah44@pitt.edu)â¡ 10.",
        "REFERENCES [1] J. Allan.",
        "Incremental Relevance Feedback for Information Filtering.",
        "Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 270-278, 1996. [2] J. Allan, C. Wade, and A. Bolivar.",
        "Retrieval and Novelty Detection at the Sentence Level.",
        "Proceedings of the ACM SIGIR conference on research and development in information retrieval, 2003. [3] C. Buckley, G. Salton, and J. Allan.",
        "Automatic Retrieval with Locality Information using SMART.",
        "NIST special publication, (500207):59-72, 1993. [4] J. Callan.",
        "Learning While Filtering Documents.",
        "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 224-231, 1998. [5] J. Carbonell and J. Goldstein.",
        "The use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries.",
        "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335-336, 1998. [6] E. Efthimiadis.",
        "Query Expansion.",
        "Annual Review of Information Science and Technology (ARIST), 31:p121-87, 1996. [7] J. Fiscus and G. Duddington.",
        "Topic Detection and Tracking Overview.",
        "Topic Detection and Tracking: Event-based Information Organization, pages 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.",
        "A Statistical Model for Multilingual Entity Detection and Tracking.",
        "NAACL/HLT, 2004. [9] K. JÂ¨arvelin and J. KekÂ¨alÂ¨ainen.",
        "Cumulated Gain-based Evaluation of IR Techniques.",
        "ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin and D. Demner-Fushman.",
        "Automatically Evaluating Answers to Definition Questions.",
        "Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), 2005. â Statistics Dept., Purdue University, West Lafayette, USA â  Language Technologies Inst., Carnegie Mellon University, Pittsburgh, USA â¡ School of Information Sciences, Univ. of Pittsburgh, Pittsburgh, USA [11] J. Lin and D. Demner-Fushman.",
        "Will Pyramids Built of nUggets Topple Over.",
        "Proceedings of HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.",
        "A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree.",
        "Proc. of ACL, 4:136-143, 2004. [13] G. Marton.",
        "Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments.",
        "HLT/NAACL, 2006. [14] E. Riloff.",
        "Automatically Constructing a Dictionary for Information Extraction Tasks.",
        "Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, 1993. [15] S. Robertson and S. Walker.",
        "Microsoft Cambridge at TREC-9: Filtering track.",
        "The Ninth Text REtrieval Conference (TREC-9), pages 361-368. [16] R. Schapire, Y.",
        "Singer, and A. Singhal.",
        "Boosting and Rocchio Applied to Text Filtering.",
        "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle, and W. Croft.",
        "Indri: A Language Model-based Search Engine for Complex Queries.",
        "Proceedings of the International Conference on Intelligence Analysis, 2004. [18] The Linguistic Data Consortium. http://www.ldc.upenn.edu/. [19] E. Voorhees.",
        "Overview of the TREC 2003 Question Answering Track.",
        "Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), 2003. [20] Y. Yang and B. Kisiel.",
        "Margin-based Local Regression for Adaptive Filtering.",
        "Proceedings of the twelfth international conference on Information and knowledge management, pages 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
        "Robustness of Adaptive Filtering Methods in a Cross-benchmark Evaluation.",
        "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 98-105, 2005. [22] C. Zhai, W. Cohen, and J. Lafferty.",
        "Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval.",
        "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-17, 2003. [23] J. Zhang and Y. Yang.",
        "Robustness of Regularized Linear Classification Methods in Text Categorization.",
        "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 190-197, 2003. [24] Y. Zhang.",
        "Using Bayesian Priors to Combine Classifiers for Adaptive Filtering.",
        "Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 345-352, 2004. [25] Y. Zhang, J. Callan, and T. Minka.",
        "Novelty and Redundancy Detection in Adaptive Filtering.",
        "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002."
    ],
    "translated_text_sentences": [
        "DestilaciÃ³n de informaciÃ³n basada en utilidad sobre documentos secuenciados temporalmente en el Instituto de TecnologÃ­as del Lenguaje de Yiming Yang.",
        "Universidad Carnegie Mellon, Pittsburgh, EE. UU. yiming@cs.cmu.edu Abhimanyu Lad Instituto de TecnologÃ­as del Lenguaje.",
        "Universidad Carnegie Mellon, Pittsburgh, EE. UU. alad@cs.cmu.edu Instituto de TecnologÃ­as del Lenguaje Ni Lao.",
        "Universidad Carnegie Mellon, Pittsburgh, EE. UU. nlao@cs.cmu.edu Abhay Harpale Instituto de TecnologÃ­as del Lenguaje.",
        "Universidad Carnegie Mellon, Pittsburgh, EE. UU. aharpale@cs.cmu.edu Bryan Kisiel Instituto de TecnologÃ­as del Lenguaje.",
        "Universidad Carnegie Mellon, Pittsburgh, EE. UU. bkisiel@cs.cmu.edu MÃ³nica Rogati Instituto de TecnologÃ­as del Lenguaje.",
        "La Universidad Carnegie Mellon en Pittsburgh, EE. UU. mrogati@cs.cmu.edu RESUMEN Este artÃ­culo examina un nuevo enfoque para la destilaciÃ³n de informaciÃ³n en documentos ordenados temporalmente, y propone un esquema de evaluaciÃ³n novedoso para dicho marco.",
        "Combina las fortalezas y se extiende mÃ¡s allÃ¡ del filtrado adaptativo convencional, la detecciÃ³n de novedades y la clasificaciÃ³n de pasajes no redundantes con respecto a las necesidades de informaciÃ³n duraderas (tareas con mÃºltiples consultas).",
        "Nuestro enfoque respalda la retroalimentaciÃ³n detallada del usuario a travÃ©s del resaltado de fragmentos arbitrarios de texto, y aprovecha dicha informaciÃ³n para la optimizaciÃ³n de la utilidad en entornos adaptativos.",
        "Para nuestros experimentos, definimos tareas hipotÃ©ticas basadas en eventos de noticias en el corpus TDT4, con mÃºltiples consultas por tarea.",
        "Se generaron claves de respuestas (nuggets) para cada consulta y se utilizÃ³ un procedimiento semiautomÃ¡tico para adquirir reglas que permiten emparejar automÃ¡ticamente los nuggets con las respuestas del sistema.",
        "TambiÃ©n proponemos una extensiÃ³n de la mÃ©trica NDCG para evaluar la utilidad de los pasajes clasificados como una combinaciÃ³n de relevancia y novedad.",
        "Nuestros resultados muestran mejoras alentadoras en la utilidad utilizando el nuevo enfoque, en comparaciÃ³n con los sistemas base sin aprendizaje incremental o componentes de detecciÃ³n de novedades.",
        "CategorÃ­as y Descriptores de Asignaturas H.3.3 [BÃºsqueda y RecuperaciÃ³n de InformaciÃ³n]: Filtrado de informaciÃ³n, RetroalimentaciÃ³n de relevancia, Modelos de recuperaciÃ³n, Proceso de selecciÃ³n; I.5.2 TÃ©rminos Generales DiseÃ±o, MediciÃ³n, Rendimiento, ExperimentaciÃ³n. 1.",
        "INTRODUCCIÃN Seguir informaciÃ³n nueva y relevante de flujos de datos temporales para usuarios con necesidades duraderas ha sido un tema de investigaciÃ³n desafiante en la recuperaciÃ³n de informaciÃ³n.",
        "El filtrado adaptativo (AF) es una tarea de predicciÃ³n en lÃ­nea de la relevancia de cada nuevo documento con respecto a temas predefinidos.",
        "BasÃ¡ndose en la consulta inicial y algunos ejemplos positivos (si estÃ¡n disponibles), un sistema de AF mantiene un perfil para cada tema de interÃ©s, y lo actualiza constantemente segÃºn la retroalimentaciÃ³n del usuario.",
        "La naturaleza de aprendizaje incremental de los sistemas de AF los hace mÃ¡s poderosos que los motores de bÃºsqueda estÃ¡ndar que admiten recuperaciÃ³n ad-hoc (por ejemplo.",
        "Google y Yahoo) en tÃ©rminos de encontrar informaciÃ³n relevante con respecto a temas de interÃ©s duraderos, y mÃ¡s atractivo para los usuarios que estÃ¡n dispuestos a proporcionar retroalimentaciÃ³n para adaptar el sistema a sus necesidades de informaciÃ³n especÃ­ficas, sin tener que modificar sus consultas manualmente.",
        "Se han estudiado una variedad de algoritmos de aprendizaje supervisado (clasificadores de estilo Rocchio, modelos exponenciales-gaussianos, regresiÃ³n local y enfoques de regresiÃ³n logÃ­stica) para entornos adaptativos, examinados con retroalimentaciÃ³n de relevancia explÃ­cita e implÃ­cita, y evaluados con respecto a la optimizaciÃ³n de la utilidad en grandes colecciones de datos de referencia en los foros de TREC (Conferencias de RecuperaciÃ³n de Texto) y TDT (DetecciÃ³n y Seguimiento de Temas) [1, 4, 7, 15, 16, 20, 24, 23].",
        "La regresiÃ³n logÃ­stica regularizada [21] se ha encontrado representativa para los enfoques de vanguardia, y altamente eficiente para adaptaciones frecuentes del modelo en grandes colecciones de documentos como el corpus TREC-10 (mÃ¡s de 800,000 documentos y 84 temas).",
        "A pesar de los logros sustanciales en la investigaciÃ³n reciente de filtrado adaptativo, aÃºn quedan problemas significativos sin resolver sobre cÃ³mo aprovechar de manera efectiva y eficiente la retroalimentaciÃ³n del usuario.",
        "EspecÃ­ficamente, los siguientes problemas pueden limitar seriamente la verdadera utilidad de los sistemas de AF en aplicaciones del mundo real: 1.",
        "El usuario tiene un papel bastante pasivo en la configuraciÃ³n convencional de filtrado adaptativo: solo reacciona ante el sistema cuando este toma una decisiÃ³n afirmativa sobre un documento, confirmando o rechazando esa decisiÃ³n.",
        "Una alternativa mÃ¡s activa serÃ­a permitir al usuario emitir mÃºltiples consultas sobre un tema, revisar una lista clasificada de documentos candidatos (o pasajes) por consulta, y proporcionar retroalimentaciÃ³n sobre la lista clasificada, refinando asÃ­ su necesidad de informaciÃ³n y solicitando listas clasificadas actualizadas.",
        "La Ãºltima forma de interacciÃ³n del usuario ha sido altamente efectiva en la recuperaciÃ³n estÃ¡ndar para consultas ad-hoc.",
        "CÃ³mo implementar una estrategia para necesidades de informaciÃ³n duraderas en entornos de FA es una pregunta abierta para la investigaciÃ³n. 2.",
        "La unidad para recibir un juicio de relevancia (sÃ­ o no) estÃ¡ restringida al nivel del documento en AF convencional.",
        "Sin embargo, un usuario real puede estar dispuesto a proporcionar comentarios mÃ¡s informativos y detallados resaltando algunas partes de texto en un documento recuperado como relevantes, en lugar de etiquetar todo el documento como relevante.",
        "Aprovechar de manera efectiva este tipo de retroalimentaciÃ³n detallada podrÃ­a mejorar sustancialmente la calidad de un sistema de AF.",
        "Para esto, necesitamos habilitar el aprendizaje supervisado a partir de fragmentos de texto etiquetados de longitud arbitraria en lugar de solo permitir documentos etiquetados. 3.",
        "Los documentos seleccionados por el sistema suelen ser altamente redundantes.",
        "Un evento importante de noticias, por ejemplo, serÃ­a reportado por mÃºltiples fuentes repetidamente durante un tiempo, haciendo que la mayor parte del contenido informativo en esos artÃ­culos sea redundante entre sÃ­.",
        "Un sistema de AF convencional seleccionarÃ­a todas estas historias de noticias redundantes para la retroalimentaciÃ³n del usuario, desperdiciando el tiempo de los usuarios mientras ofrece poco beneficio.",
        "Claramente, las tÃ©cnicas de detecciÃ³n de novedades pueden ayudar en principio [25, 2, 22] a mejorar la utilidad de los sistemas AF.",
        "Sin embargo, la efectividad de tales tÃ©cnicas a nivel de pasaje para detectar novedad con respecto a la retroalimentaciÃ³n de los usuarios a nivel detallado y para detectar redundancia en listas clasificadas aÃºn debe ser evaluada utilizando una medida de utilidad que imite las necesidades de un usuario real.",
        "Para abordar las limitaciones actuales de los sistemas de AF mencionadas anteriormente, proponemos y examinamos un nuevo enfoque en este artÃ­culo, que combina las fortalezas del AF convencional (aprendizaje incremental de modelos de temas), recuperaciÃ³n de pasajes multipase para consultas de larga duraciÃ³n condicionadas al tema, y detecciÃ³n de novedades para eliminar la redundancia de las interacciones del usuario con el sistema.",
        "Llamamos al nuevo proceso destilaciÃ³n de informaciÃ³n basada en utilidad.",
        "Ten en cuenta que los corpus de referencia convencionales para evaluaciones de AF, que tienen juicios de relevancia a nivel de documento y no definen tareas con mÃºltiples consultas, son insuficientes para evaluar el nuevo enfoque.",
        "Por lo tanto, ampliamos un corpus de referencia, la colecciÃ³n TDT4 de noticias y transmisiones de televisiÃ³n, con definiciones de tareas, mÃºltiples consultas por tarea y claves de respuestas por consulta.",
        "Hemos realizado nuestros experimentos en este corpus extendido TDT4 y hemos puesto a disposiciÃ³n del pÃºblico los datos generados adicionalmente para futuras evaluaciones comparativas.",
        "Para evaluar automÃ¡ticamente los fragmentos de texto arbitrarios devueltos por el sistema utilizando nuestras claves de respuestas, desarrollamos un esquema de evaluaciÃ³n con un procedimiento semiautomÃ¡tico para 1 URL: http://nyc.lti.cs.cmu.edu/downloads adquiriendo reglas que pueden comparar fragmentos con las respuestas del sistema.",
        "AdemÃ¡s, proponemos una extensiÃ³n de NDCG (Normalized Discounted Cumulated Gain) [9] para evaluar la utilidad de los pasajes clasificados como una funciÃ³n tanto de la relevancia como de la novedad.",
        "El resto de este documento estÃ¡ organizado de la siguiente manera.",
        "La secciÃ³n 2 describe el proceso de destilaciÃ³n de informaciÃ³n con un ejemplo concreto.",
        "La secciÃ³n 3 describe los nÃºcleos tÃ©cnicos de nuestro sistema llamado CAFÂ´E - CMU Adaptive Filtering Engine.",
        "La secciÃ³n 4 discute problemas con respecto a la metodologÃ­a de evaluaciÃ³n y propone un nuevo esquema.",
        "La secciÃ³n 5 describe el corpus extendido TDT4.",
        "La secciÃ³n 6 presenta nuestros experimentos y resultados.",
        "La secciÃ³n 7 concluye el estudio y ofrece perspectivas futuras. 2.",
        "Una TAREA DE EJEMPLO Considera un evento noticioso: la fuga de siete convictos de una prisiÃ³n de Texas en diciembre de 2000 y su captura un mes despuÃ©s.",
        "Suponiendo que un usuario estuviera interesado en este evento desde sus primeras etapas, la necesidad de informaciÃ³n podrÃ­a ser: Encontrar informaciÃ³n sobre la fuga de convictos de una prisiÃ³n de Texas, y la informaciÃ³n relacionada con su recaptura.",
        "Las preguntas de nivel inferior asociadas podrÃ­an ser: 1.",
        "Â¿CuÃ¡ntos prisioneros escaparon? 2.",
        "Â¿DÃ³nde y cuÃ¡ndo fueron avistados? 3. Â¿QuiÃ©nes son sus contactos conocidos dentro y fuera de la prisiÃ³n? 4.",
        "Â¿CÃ³mo estÃ¡n armados? 5.",
        "Â¿Tienen algÃºn vehÃ­culo? 6.",
        "Â¿QuÃ© pasos se han tomado hasta ahora?",
        "Llamamos a esta necesidad de informaciÃ³n una tarea, y a las preguntas asociadas como las consultas en esta tarea.",
        "Un sistema de destilaciÃ³n debe monitorear los documentos entrantes, procesarlos por fragmentos en un orden temporal, seleccionar pasajes potencialmente relevantes y novedosos de cada fragmento con respecto a cada consulta, y presentar una lista clasificada de pasajes al usuario.",
        "La clasificaciÃ³n de pasajes aquÃ­ se basa en cuÃ¡n relevante es un pasaje con respecto a la consulta actual, cuÃ¡n novedoso es con respecto al historial actual del usuario (de sus interacciones con el sistema) y cuÃ¡n redundante es en comparaciÃ³n con otros pasajes con un rango mÃ¡s alto en la lista.",
        "Cuando se le presenta una lista de pasajes, el usuario puede proporcionar retroalimentaciÃ³n resaltando tramos de texto arbitrarios que considere relevantes.",
        "Estos fragmentos de texto se toman como ejemplos positivos en la adaptaciÃ³n del perfil de consulta, y tambiÃ©n se aÃ±aden al historial de los usuarios.",
        "Los pasajes no marcados por el usuario se consideran ejemplos negativos.",
        "Tan pronto como se actualiza el perfil de consulta, el sistema vuelve a realizar una bÃºsqueda y devuelve otra lista clasificada de pasajes donde los pasajes previamente vistos son eliminados o clasificados como bajos, segÃºn la preferencia del usuario.",
        "Por ejemplo, si el usuario resalta ...los funcionarios han ofrecido una recompensa de $100,000 por su captura... como respuesta relevante a la consulta Â¿QuÃ© pasos se han tomado hasta ahora?, entonces la pieza resaltada se utiliza como un ejemplo de entrenamiento positivo adicional en la adaptaciÃ³n del perfil de la consulta.",
        "Este comentario tambiÃ©n se aÃ±ade al historial del usuario como un ejemplo visto, para que en el futuro, el sistema no coloque otro pasaje mencionando una recompensa de $100,000 en la parte superior de la lista clasificada.",
        "Sin embargo, un artÃ­culo que mencione que los funcionarios han duplicado el dinero de recompensa a $200,000 podrÃ­a clasificarse alto ya que es relevante tanto para el perfil de consulta (actualizado) como novedoso con respecto a la historia del usuario (actualizada).",
        "El usuario puede modificar las consultas originales o agregar una nueva consulta durante el proceso; los perfiles de consulta se cambiarÃ¡n en consecuencia.",
        "Claramente, la detecciÃ³n de novedades es muy importante para la utilidad de dicho sistema debido a la bÃºsqueda iterativa.",
        "Sin detecciÃ³n de novedad, los antiguos pasajes relevantes se mostrarÃ­an al usuario repetidamente en cada lista clasificada.",
        "A travÃ©s del ejemplo anterior, podemos ver las principales propiedades de nuestro nuevo marco de trabajo para la destilaciÃ³n de informaciÃ³n basada en utilidad sobre documentos ordenados temporalmente.",
        "Nuestro marco de trabajo combina y amplÃ­a el poder del filtrado adaptativo (AF), la recuperaciÃ³n ad-hoc (IR) y la detecciÃ³n de novedades (ND).",
        "En comparaciÃ³n con la IR estÃ¡ndar, nuestro enfoque tiene la capacidad de aprender de forma incremental las necesidades de informaciÃ³n a largo plazo y modelar una secuencia de consultas dentro de una tarea.",
        "En comparaciÃ³n con el AF convencional, permite un papel mÃ¡s activo del usuario en refinar sus necesidades de informaciÃ³n y solicitar nuevos resultados al permitir comentarios sobre relevancia y novedad mediante el resaltado de fragmentos arbitrarios de texto en los pasajes devueltos por el sistema.",
        "Comparado con trabajos anteriores, esta es la primera evaluaciÃ³n de detecciÃ³n de novedad integrada con filtrado adaptativo para consultas secuenciadas que permite retroalimentaciÃ³n flexible del usuario sobre pasajes clasificados.",
        "La combinaciÃ³n de AF, IR y ND con las nuevas extensiones plantea una importante pregunta de investigaciÃ³n sobre la metodologÃ­a de evaluaciÃ³n: Â¿cÃ³mo podemos medir la utilidad de un sistema de destilaciÃ³n de informaciÃ³n como este?",
        "Las mÃ©tricas existentes en IR estÃ¡ndar, AF y ND son insuficientes, y se deben explorar nuevas soluciones, como discutiremos en la SecciÃ³n 4, despuÃ©s de describir los nÃºcleos tÃ©cnicos de nuestro sistema en la prÃ³xima secciÃ³n. 3.",
        "NÃCLEOS TÃCNICOS Los componentes principales de CAFÃ son: 1) AF para el aprendizaje incremental de perfiles de consulta, 2) IR para estimar la relevancia de pasajes con respecto a los perfiles de consulta, 3) ND para evaluar la novedad de pasajes con respecto al historial de usuarios, y 4) un componente anti-redundancia para eliminar la redundancia de las listas clasificadas. 3.1 Componente de Filtrado Adaptativo Utilizamos un algoritmo de vanguardia en el campo, el mÃ©todo de regresiÃ³n logÃ­stica regularizada que obtuvo los mejores resultados en varios corpus de evaluaciÃ³n de referencia para AF [21].",
        "La regresiÃ³n logÃ­stica (LR) es un algoritmo de aprendizaje supervisado para clasificaciÃ³n estadÃ­stica.",
        "BasÃ¡ndose en un conjunto de entrenamiento de instancias etiquetadas, aprende un modelo de clase que luego puede ser utilizado para predecir las etiquetas de instancias no vistas.",
        "Su rendimiento, asÃ­ como su eficiencia en tÃ©rminos de tiempo de entrenamiento, lo convierten en un buen candidato cuando se requieren actualizaciones frecuentes del modelo de clase, como es el caso en el filtrado adaptativo, donde el sistema debe aprender de cada nueva retroalimentaciÃ³n proporcionada por el usuario. (Consulte [21] y [23] para complejidad computacional y problemas de implementaciÃ³n).",
        "En el filtrado adaptativo, cada consulta se considera como una clase y la probabilidad de que un pasaje pertenezca a esta clase corresponde al grado de relevancia del pasaje con respecto a la consulta.",
        "Para entrenar el modelo, utilizamos la propia consulta como ejemplo inicial de entrenamiento positivo de la clase, y las piezas de texto resaltadas por el usuario (marcadas como Relevante o No relevante) durante la retroalimentaciÃ³n como ejemplos de entrenamiento adicionales.",
        "Para abordar el problema de inicio en frÃ­o en la etapa inicial antes de obtener cualquier retroalimentaciÃ³n del usuario, el sistema utiliza una pequeÃ±a muestra de un corpus retrospectivo como ejemplos negativos iniciales en el conjunto de entrenamiento.",
        "Los detalles de cÃ³mo utilizar la regresiÃ³n logÃ­stica para el filtrado adaptativo (asignando diferentes pesos a las instancias de entrenamiento positivas y negativas, y regularizando la funciÃ³n objetivo para prevenir el sobreajuste en los datos de entrenamiento) se presentan en [21].",
        "El modelo de clase wâ aprendido por RegresiÃ³n LogÃ­stica, o el perfil de consulta, es un vector cuyas dimensiones son tÃ©rminos individuales y cuyos elementos son los coeficientes de regresiÃ³n, indicando cuÃ¡n influyente es cada tÃ©rmino en el perfil de consulta.",
        "El perfil de la consulta se actualiza cada vez que se recibe una nueva pieza de retroalimentaciÃ³n del usuario.",
        "Un peso decaÃ­do temporalmente se puede aplicar a cada ejemplo de entrenamiento, como una opciÃ³n, para enfatizar la retroalimentaciÃ³n mÃ¡s reciente del usuario. 3.2 Componente de RecuperaciÃ³n de Pasajes Utilizamos tÃ©cnicas estÃ¡ndar de IR en esta parte de nuestro sistema.",
        "Los documentos entrantes se procesan por bloques, donde cada bloque puede definirse como un intervalo de tiempo fijo o como un nÃºmero fijo de documentos, segÃºn lo prefiera el usuario.",
        "Para cada documento entrante, se actualizan las estadÃ­sticas del corpus como el IDF (Frecuencia Inversa de Documento) de cada tÃ©rmino.",
        "Utilizamos un identificador y rastreador de entidades con tecnologÃ­a de punta [8, 12] para identificar nombres de personas y lugares, y fusionarlos con entidades con referencia cruzada vistas en el pasado.",
        "Luego, los documentos se dividen en pasajes, que pueden ser un documento completo, un pÃ¡rrafo, una oraciÃ³n, o cualquier otro fragmento continuo de texto, segÃºn se prefiera.",
        "Cada pasaje se representa utilizando un vector de pesos TF-IDF (Frecuencia de TÃ©rmino - Frecuencia Inversa de Documento), donde el tÃ©rmino puede ser una palabra o una entidad nombrada.",
        "Dado un perfil de consulta, es decir, la soluciÃ³n de regresiÃ³n logÃ­stica wâ como se describe en la SecciÃ³n 3.1, el sistema calcula la probabilidad posterior de relevancia para cada pasaje x como fRL(x) â¡ P(y = 1|x, wâ ) = 1 (1 + eâwâÂ·x) (1). Los pasajes se ordenan segÃºn sus puntajes de relevancia, y aquellos con puntajes por encima de un umbral (ajustado en un conjunto de entrenamiento) conforman la lista de relevancia que se pasa al paso de detecciÃ³n de novedad. 3.3 Componente de DetecciÃ³n de Novedad CAFÂ´E mantiene un historial de usuario H(t), que contiene todos los fragmentos de texto hi que el usuario resaltÃ³ (como retroalimentaciÃ³n) durante sus interacciones pasadas con el sistema, hasta el tiempo actual t. Denotando el historial como H(t) = n h1, h2, ..., ht o , (2), el puntaje de novedad de un nuevo pasaje candidato x se calcula como: fND(x) = 1 â max iâ1..t {cos(x, hi)} (3), donde tanto el pasaje candidato x como los fragmentos de texto resaltados hi se representan como vectores TF-IDF.",
        "El puntaje de novedad de cada pasaje se compara con un umbral predefinido (ajustado tambiÃ©n en un conjunto de entrenamiento), y cualquier pasaje con un puntaje por debajo de este umbral se elimina de la lista de relevancia. Componente de clasificaciÃ³n anti-redundante Aunque el componente de detecciÃ³n de novedad garantiza que solo la informaciÃ³n novedosa (no vista previamente) permanezca en la lista de relevancia, esta lista aÃºn podrÃ­a contener la misma informaciÃ³n novedosa en mÃºltiples posiciones en la lista clasificada.",
        "Supongamos, por ejemplo, que el usuario ya ha leÃ­do sobre una recompensa de $100,000 por informaciÃ³n sobre los convictos fugados.",
        "Una nueva noticia de que el premio ha sido aumentado a $200,000 es novedosa ya que el usuario aÃºn no la ha leÃ­do.",
        "Sin embargo, mÃºltiples fuentes de noticias podrÃ­an informar sobre esta noticia y podrÃ­amos terminar mostrando artÃ­culos (redundantes) de todas estas fuentes en una lista clasificada.",
        "Por lo tanto, una lista clasificada tambiÃ©n debe ser no redundante con respecto a su propio contenido.",
        "Utilizamos una versiÃ³n simplificada del mÃ©todo de MÃ¡xima Relevancia Marginal [5], originalmente desarrollado para combinar relevancia y novedad en la recuperaciÃ³n de texto y resÃºmenes.",
        "Nuestro procedimiento comienza con la lista actual de pasajes ordenados por relevancia (secciÃ³n 3.2), filtrados por el componente de DetecciÃ³n de Novedades (secciÃ³n 3.3), y genera una nueva lista no redundante de la siguiente manera: 1.",
        "Toma el pasaje superior en la lista actual como el primero en la nueva lista. 2.",
        "Agrega el siguiente pasaje x en la lista actual a la nueva lista solo si fAR(x) > t donde fAR(x) = 1 â max piâLnew {cos(x, pi)} y Lnew es el conjunto de pasajes ya seleccionados en la nueva lista.",
        "Repetir el paso 2 hasta que se hayan examinado todos los pasajes en la lista actual.",
        "DespuÃ©s de aplicar el algoritmo mencionado anteriormente, cada pasaje en la nueva lista es lo suficientemente diferente a los demÃ¡s, favoreciendo asÃ­ la diversidad en lugar de la redundancia en la nueva lista clasificada.",
        "El umbral anti-redundancia t se ajusta en un conjunto de entrenamiento. 4.",
        "METODOLOGÃA DE EVALUACIÃN El enfoque que propusimos anteriormente para la destilaciÃ³n de informaciÃ³n plantea importantes cuestiones sobre la metodologÃ­a de evaluaciÃ³n.",
        "En primer lugar, dado que nuestro marco de trabajo permite que la salida sean fragmentos en diferentes niveles de granularidad (por ejemplo, ventanas de k oraciones donde k puede variar) en lugar de una longitud fija, no es posible tener juicios de relevancia preanotados en todos esos niveles de granularidad.",
        "En segundo lugar, dado que deseamos medir la utilidad de la salida del sistema como una combinaciÃ³n de relevancia y novedad, las medidas tradicionales basadas Ãºnicamente en la relevancia deben ser reemplazadas por medidas que penalicen la repeticiÃ³n de la misma informaciÃ³n en la salida del sistema a lo largo del tiempo.",
        "En tercer lugar, dado que la salida del sistema son listas clasificadas, debemos recompensar a aquellos sistemas que presenten informaciÃ³n Ãºtil (tanto relevante como no vista previamente) utilizando listas clasificadas mÃ¡s cortas, y penalizar a aquellos que presenten la misma informaciÃ³n utilizando listas clasificadas mÃ¡s largas.",
        "Ninguna de las medidas existentes en la recuperaciÃ³n ad-hoc, filtrado adaptativo, detecciÃ³n de novedades u otras Ã¡reas relacionadas (resumen de texto y respuesta a preguntas) tiene propiedades deseables en los tres aspectos.",
        "Por lo tanto, debemos desarrollar una nueva metodologÃ­a de evaluaciÃ³n. 4.1 Claves de Respuesta Para permitir la evaluaciÃ³n de un sistema cuya salida consiste en pasajes de longitud arbitraria, tomamos prestado el concepto de claves de respuesta de la comunidad de Preguntas y Respuestas (QA), donde los sistemas pueden devolver fragmentos de texto arbitrarios como respuestas.",
        "Las claves de respuestas definen lo que debe estar presente en la respuesta de un sistema para recibir crÃ©dito, y estÃ¡n compuestas por una colecciÃ³n de \"nuggets\" de informaciÃ³n, es decir, unidades de datos sobre las cuales los evaluadores humanos pueden tomar decisiones binarias sobre si una respuesta del sistema las contiene o no.",
        "La definiciÃ³n de claves de respuestas y la toma de decisiones binarias asociadas son tareas conceptuales que requieren un mapeo semÃ¡ntico [19], ya que los pasajes devueltos por el sistema pueden contener la misma informaciÃ³n expresada de muchas formas diferentes.",
        "Por lo tanto, las evaluaciones de QA han dependido de evaluadores humanos para la correspondencia entre diversas expresiones, lo que hace que el proceso sea costoso, consume mucho tiempo y no sea escalable para grandes colecciones de consultas y documentos, y evaluaciones extensas de sistemas con diversos ajustes de parÃ¡metros. 4.1.1 AutomatizaciÃ³n de la evaluaciÃ³n basada en claves de respuestas Los mÃ©todos de evaluaciÃ³n automÃ¡tica permitirÃ­an una construcciÃ³n y ajuste mÃ¡s rÃ¡pidos del sistema, asÃ­ como proporcionar una forma objetiva y asequible de comparar diversos sistemas.",
        "Recientemente, se han propuesto mÃ©todos basados mÃ¡s o menos en la idea de las co-ocurrencias de n-gramos.",
        "Pourpre [10] asigna una puntuaciÃ³n de recuerdo fraccional a una respuesta del sistema basada en su superposiciÃ³n de unigramas con una descripciÃ³n de pepitas dada.",
        "Por ejemplo, una respuesta del sistema A B C tiene una recuperaciÃ³n de 3/4 con respecto a un fragmento con la descripciÃ³n A B C D. Sin embargo, este enfoque es injusto para los sistemas que presentan la misma informaciÃ³n pero utilizando palabras distintas a A, B, C y D. Otro problema abierto es cÃ³mo ponderar las palabras individuales al medir la cercanÃ­a de una coincidencia.",
        "Por ejemplo, considera la pregunta Â¿CuÃ¡ntos prisioneros escaparon?",
        "En el fragmento Siete prisioneros escaparon de una prisiÃ³n de Texas, no hay indicaciÃ³n de que siete sea la palabra clave y que debe coincidir para obtener algÃºn crÃ©dito de relevancia.",
        "El uso de los valores de IDF no ayuda, ya que la palabra \"seven\" generalmente no tendrÃ¡ un IDF mÃ¡s alto que palabras como \"texas\" y \"prison\".",
        "AdemÃ¡s, redefinir el fragmento como simplemente siete no resuelve el problema, ya que ahora podrÃ­a coincidir errÃ³neamente con cualquier menciÃ³n de siete fuera de contexto.",
        "Nuggeteer [13] trabaja sobre principios similares pero toma decisiones binarias sobre si hay un fragmento en una respuesta de sistema dada ajustando un umbral.",
        "Sin embargo, tambiÃ©n estÃ¡ plagado de relevancia espuria ya que no todas las palabras contenidas en la descripciÃ³n de la pepita (o respuestas correctas conocidas) son centrales para la pepita. 4.1.2 Reglas de Coincidencia de Pepitas Proponemos un mÃ©todo automÃ¡tico confiable para determinar si un fragmento de texto contiene una pepita dada, basado en reglas de coincidencia de pepitas, que se generan utilizando un procedimiento semiautomÃ¡tico explicado a continuaciÃ³n.",
        "Estas reglas son bÃ¡sicamente consultas booleanas que solo coincidirÃ¡n con fragmentos que contengan la pepita.",
        "Por ejemplo, una regla candidata para emparejar respuestas a Â¿CuÃ¡ntos prisioneros escaparon? es (Texas Y siete Y escapar Y (convictos O prisioneros)), posiblemente con otros sinÃ³nimos y variantes en la regla.",
        "Para un corpus de artÃ­culos de noticias, que suelen seguir una prosa formal tÃ­pica, es bastante fÃ¡cil escribir reglas simples para coincidir con respuestas esperadas utilizando un enfoque de arranque, como se describe a continuaciÃ³n.",
        "Proponemos un enfoque de dos etapas, inspirado en Autoslog [14], que combina la capacidad de los humanos para identificar expresiones semÃ¡nticamente equivalentes y la capacidad del sistema para recopilar evidencia estadÃ­stica de un corpus de documentos anotados por humanos.",
        "En la primera etapa, los sujetos humanos anotaron (utilizando una herramienta de resaltado) porciones de documentos relevantes que contenÃ­an respuestas a cada fragmento 2.",
        "En la segunda etapa, los sujetos utilizaron nuestra herramienta de generaciÃ³n de reglas para crear reglas que coincidieran con las anotaciones de cada fragmento.",
        "La herramienta permite a los usuarios ingresar una regla booleana como una disyunciÃ³n de conjunciones (por ejemplo, ((a Y b) O (a Y c Y d) O (e))).",
        "Dada una regla candidata, nuestra herramienta la utiliza como una consulta booleana sobre el conjunto completo de documentos relevantes y calcula su recall y precisiÃ³n con respecto a las anotaciones que se espera que coincida.",
        "Por lo tanto, los sujetos pueden comenzar con una regla simple y refinarla iterativamente hasta que estÃ©n satisfechos con su precisiÃ³n y recuperaciÃ³n.",
        "Observamos que era muy fÃ¡cil para los humanos mejorar la precisiÃ³n de una regla ajustando sus conjunciones existentes (agregando mÃ¡s Y) y mejorar la recuperaciÃ³n agregando mÃ¡s conjunciones a la disyunciÃ³n (agregando mÃ¡s O).",
        "Como ejemplo, intentemos crear una regla para el acertijo que dice que siete prisioneros escaparon de la prisiÃ³n de Texas.",
        "Empezamos con una regla simple - (siete).",
        "Cuando introducimos esto en la herramienta de generaciÃ³n de reglas, nos damos cuenta de que esta regla coincide con muchas ocurrencias espurias de siete (por ejemplo, ...siete estados...) y, por lo tanto, obtiene una puntuaciÃ³n baja de precisiÃ³n.",
        "Podemos calificar aÃºn mÃ¡s nuestra regla - Texas Y siete Y convictos.",
        "A continuaciÃ³n, al revisar las anotaciones omitidas, nos damos cuenta de que algunos artÃ­culos de noticias mencionaban ...siete prisioneros escaparon.... Luego reemplazamos convictos por la disyunciÃ³n (convictos O prisioneros).",
        "Continuamos ajustando la regla de esta manera hasta lograr un nivel de recordaciÃ³n y precisiÃ³n suficientemente alto, es decir, que los (pocos) errores y alarmas falsas se puedan ignorar de forma segura.",
        "AsÃ­ podemos crear reglas de coincidencia de pepitas que capturen de manera sucinta varias formas de expresar una pepita, evitando al mismo tiempo emparejar respuestas incorrectas (o fuera de contexto).",
        "La participaciÃ³n humana en el proceso de creaciÃ³n de reglas garantiza reglas genÃ©ricas de alta calidad que luego se pueden utilizar para evaluar de manera confiable las respuestas del sistema arbitrario. 4.2 EvaluaciÃ³n de la Utilidad de una Secuencia de Listas Clasificadas La utilidad de un sistema de recuperaciÃ³n se puede definir como la diferencia entre cuÃ¡nta informaciÃ³n Ãºtil obtuvo el usuario y cuÃ¡nto tiempo y energÃ­a perdiÃ³.",
        "Calculamos esta utilidad a partir de las utilidades de los pasajes individuales de la siguiente manera.",
        "DespuÃ©s de leer cada pasaje devuelto por el sistema, el usuario obtiene cierta ganancia dependiendo de la presencia de informaciÃ³n relevante y novedosa, y incurre en una pÃ©rdida en tÃ©rminos del tiempo y la energÃ­a invertidos en revisar el pasaje.",
        "Sin embargo, la probabilidad de que el usuario realmente lea un pasaje depende de su posiciÃ³n en la lista clasificada.",
        "Por lo tanto, para una consulta q, la utilidad esperada 2 LDC [18] ya proporciona juicios de relevancia para 100 temas en el corpus TDT4.",
        "AdemÃ¡s, nos aseguramos de que estos juicios sean exhaustivos en todo el corpus utilizando la agrupaciÃ³n. El valor de un pasaje pi en el rango i se puede definir como U(pi, q) = P(i) â (Ganancia(pi, q) - PÃ©rdida(pi, q)) (4) donde P(i) es la probabilidad de que el usuario pase por un pasaje en el rango i.",
        "La utilidad esperada para una lista clasificada completa de longitud n se puede calcular simplemente sumando la utilidad esperada de cada pasaje: U(q) = Î£ i=1 P(i) â (Ganancia(pi, q) â PÃ©rdida(pi, q)) (5) NÃ³tese que si ignoramos el tÃ©rmino de pÃ©rdida y definimos P(i) como P(i) â 1/ logb(b + i â 1) (6) entonces obtenemos la mÃ©trica recientemente popularizada llamada Ganancia Acumulada Descontada (DCG) [9], donde Ganancia(pi, q) se define como la relevancia graduada del pasaje pi.",
        "Sin embargo, sin el tÃ©rmino de pÃ©rdida, DCG es una mÃ©trica puramente orientada al recuerdo y no es adecuada para un entorno de filtrado adaptativo, donde la utilidad del sistema depende en parte de su capacidad para limitar el nÃºmero de elementos mostrados al usuario.",
        "Aunque P(i) podrÃ­a definirse en base a estudios empÃ­ricos del comportamiento del usuario, por simplicidad, utilizamos P(i) tal como se define exactamente en la ecuaciÃ³n 6.",
        "La ganancia G(pi, q) del pasaje pi con respecto a la consulta q es una funciÃ³n de - 1) el nÃºmero de pepitas relevantes presentes en pi, y 2) la novedad de cada una de estas pepitas.",
        "Combinamos estos dos factores de la siguiente manera.",
        "Para cada pepita Nj, asignamos un peso inicial wj, y tambiÃ©n mantenemos un conteo nj del nÃºmero de veces que esta pepita ha sido vista por el usuario en el pasado.",
        "La ganancia derivada de cada ocurrencia posterior del mismo hallazgo se asume que se reduce por un factor de amortiguamiento Î³.",
        "Por lo tanto, G(pi, q) se define como G(pi, q) = X Nj âC(pi,q) wj â Î³nj (7) donde C(pi, q) es el conjunto de todas las pepitas que aparecen en el pasaje pi y tambiÃ©n pertenecen a la clave de respuesta de la consulta q.",
        "Los pesos iniciales wj se establecen todos en 1.0 en nuestros experimentos, pero tambiÃ©n pueden establecerse segÃºn un enfoque de pirÃ¡mide [11].",
        "La elecciÃ³n del factor de amortiguaciÃ³n Î³ determina la tolerancia del usuario a la redundancia.",
        "Cuando Î³ = 0, un pepita solo recibirÃ¡ crÃ©dito por su primera ocurrencia, es decir, cuando nj sea cero.",
        "Para 0 < Î³ < 1, un nugget recibe menos crÃ©dito por cada ocurrencia sucesiva.",
        "Cuando Î³ = 1, no se produce amortiguamiento y las repeticiones de un valor reciben el mismo crÃ©dito.",
        "Se debe tener en cuenta que los recuentos de ocurrencia de las pepitas se conservan entre la evaluaciÃ³n de las listas clasificadas sucesivas devueltas por el sistema, ya que se espera que los usuarios recuerden lo que el sistema les mostrÃ³ en el pasado.",
        "Definimos la pÃ©rdida L(pi, q) como un costo constante c (usamos 0.1) incurrido al leer un pasaje devuelto por el sistema.",
        "Por lo tanto, nuestra mÃ©trica puede ser reescrita como U(q) = nX i=1 Ganancia(pi, q) logb(b + i â 1) â L(n) (8) donde L(n) es la pÃ©rdida asociada con una lista clasificada de longitud n: L(n) = c Â· nX i=1 1 logb(b + i â 1) (9) 3 NÃ³tese que 00 = 1 Debido a la similitud con la Ganancia Acumulada Descontada (DCG), llamamos a nuestra mÃ©trica Utilidad Acumulada Descontada (DCU).",
        "El puntaje de DCU obtenido por el sistema se convierte en un puntaje de DCU normalizado (NDCU) dividiÃ©ndolo por el puntaje de DCU de la lista ideal clasificada, que se crea ordenando los pasajes por sus puntajes de utilidad decrecientes U(pi, q) y deteniÃ©ndose cuando U(pi, q) â¤ 0, es decir, cuando la ganancia es menor o igual al costo de leer el pasaje.",
        "El conjunto de datos TDT4 fue el corpus de referencia utilizado en las evaluaciones TDT2002 y TDT2003.",
        "El corpus consiste en mÃ¡s de 90,000 artÃ­culos de noticias de mÃºltiples fuentes (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) publicados entre octubre de 2000 y enero de 2001, en los idiomas Ã¡rabe, inglÃ©s y mandarÃ­n.",
        "Las versiones de los artÃ­culos en otros idiomas reconocidas por voz y traducidas por mÃ¡quina tambiÃ©n fueron proporcionadas.",
        "LDC [18] ha anotado el corpus con 100 temas, que corresponden a varios eventos de noticias en este perÃ­odo de tiempo.",
        "De entre estos, seleccionamos un subconjunto de 12 eventos accionables y definimos tareas correspondientes para ellos.",
        "Para cada tarea, definimos manualmente un perfil que consiste en un conjunto inicial de (5 a 10) consultas, una descripciÃ³n de texto libre del historial del usuario, es decir, lo que el usuario ya sabe sobre el evento, y una lista de documentos conocidos relevantes y no relevantes (si estÃ¡n disponibles) como ejemplos de entrenamiento.",
        "Para cada consulta, generamos claves de respuestas y reglas de coincidencia de pepitas correspondientes utilizando el procedimiento descrito en la secciÃ³n 4.1.2, y produjimos un total de 120 consultas, con un promedio de 7 pepitas por consulta.",
        "EXPERIMENTOS Y RESULTADOS 6.1 Baselines Utilizamos Indri [17], un motor de bÃºsqueda basado en modelos de lenguaje popular, como referencia para comparar con CAFÃ.",
        "Indri admite la funcionalidad estÃ¡ndar de los motores de bÃºsqueda, incluido el feedback de pseudo-relevancia (PRF) [3, 6], y es representativo de un sistema de recuperaciÃ³n basado en consultas tÃ­pico.",
        "Indri no admite ningÃºn tipo de detecciÃ³n de novedades.",
        "Comparamos Indri con PRF activado y desactivado, frente a CAFÃ con retroalimentaciÃ³n del usuario, detecciÃ³n de novedades y clasificaciÃ³n antirredundante activados y desactivados. 6.2 ConfiguraciÃ³n Experimental Dividimos el corpus TDT4 que abarca 4 meses en 10 fragmentos, cada uno definido como un perÃ­odo de 12 dÃ­as consecutivos.",
        "En cualquier momento dado del proceso de destilaciÃ³n, cada sistema accedÃ­a a los datos pasados hasta el punto actual, y devolvÃ­a una lista clasificada de hasta 50 pasajes por consulta.",
        "Las 12 tareas definidas en el corpus se dividieron en un conjunto de entrenamiento y prueba con 6 tareas cada uno.",
        "Cada sistema pudo utilizar el conjunto de entrenamiento para ajustar sus parÃ¡metros con el fin de optimizar NDCU (ecuaciÃ³n 8), incluyendo el umbral de relevancia tanto para Indri como para CAFÂ´E, y los umbrales de novedad y antirredundancia para CAFÂ´E.",
        "La NDCU para cada ejecuciÃ³n del sistema se calcula automÃ¡ticamente.",
        "TambiÃ©n se simulÃ³ la retroalimentaciÃ³n de los usuarios: los juicios de relevancia para cada pasaje devuelto por el sistema (segÃºn las reglas de coincidencia de pepitas descritas en la secciÃ³n 4.1.2) fueron utilizados como retroalimentaciÃ³n de los usuarios en la adaptaciÃ³n de perfiles de consulta e historiales de usuario. En la Tabla 1, mostramos los puntajes NDCU de los dos sistemas en diferentes configuraciones.",
        "Estas puntuaciones se promedian en las seis tareas del conjunto de pruebas, y se calculan con dos factores de amortiguaciÃ³n (ver secciÃ³n 4.2): Î³ = 0 y 0.1, para simular ninguna tolerancia y poca tolerancia para la redundancia, respectivamente.",
        "Usar Î³ = 0 crea una mÃ©trica mucho mÃ¡s estricta ya que no otorga ningÃºn crÃ©dito a un pasaje que contenga informaciÃ³n relevante pero redundante.",
        "Por lo tanto, la mejora obtenida al habilitar la retroalimentaciÃ³n del usuario es menor con Î³ = 0 que la mejora obtenida de la retroalimentaciÃ³n con Î³ = 0.1.",
        "Esto revela una deficiencia de los sistemas de recuperaciÃ³n contemporÃ¡neos: cuando el usuario da retroalimentaciÃ³n positiva sobre un pasaje, los sistemas otorgan mayor peso a los tÃ©rminos presentes en ese pasaje y tienden a recuperar otros pasajes que contienen los mismos tÃ©rminos, y por lo tanto, generalmente la misma informaciÃ³n.",
        "Sin embargo, el usuario no se beneficia al ver pasajes redundantes y generalmente estÃ¡ interesado en otros pasajes que contengan informaciÃ³n relacionada.",
        "Es informativo evaluar los sistemas de recuperaciÃ³n utilizando nuestra medida de utilidad (con Î³ = 0) que tiene en cuenta la novedad y, por lo tanto, ofrece una imagen mÃ¡s realista de cuÃ¡n bien un sistema puede generalizar a partir de la retroalimentaciÃ³n del usuario, en lugar de utilizar medidas tradicionales de RI como la recuperaciÃ³n y la precisiÃ³n, que ofrecen una imagen incompleta de la mejora obtenida a partir de la retroalimentaciÃ³n del usuario.",
        "A veces, sin embargo, los usuarios podrÃ­an estar interesados en ver la misma informaciÃ³n de mÃºltiples fuentes, como un indicador de su importancia o fiabilidad.",
        "En tal caso, simplemente podemos elegir un valor mÃ¡s alto para Î³ que corresponda a una mayor tolerancia para la redundancia, y asÃ­ permitir que el sistema ajuste sus parÃ¡metros en consecuencia.",
        "Dado que los documentos fueron procesados por fragmentos, serÃ­a interesante ver cÃ³mo mejora el rendimiento de los sistemas con el tiempo.",
        "Las figuras 1 y 2 muestran las tendencias de rendimiento de ambos sistemas a lo largo de los fragmentos.",
        "Si bien se espera que el rendimiento con y sin retroalimentaciÃ³n en los primeros fragmentos sea similar, para los fragmentos posteriores, la curva de rendimiento con la retroalimentaciÃ³n habilitada supera a la del ajuste sin retroalimentaciÃ³n.",
        "Las tendencias de rendimiento no son consistentes en todos los fragmentos porque los documentos relevantes no estÃ¡n distribuidos uniformemente en todos los fragmentos, lo que hace que algunas consultas sean mÃ¡s fÃ¡ciles que otras en ciertos fragmentos.",
        "AdemÃ¡s, dado que Indri utiliza retroalimentaciÃ³n de pseudo-relevancia mientras que CAFÂ´E utiliza retroalimentaciÃ³n basada en juicios de relevancia reales, la mejora en el caso de Indri es menos dramÃ¡tica que la de CAFÂ´E. 7.",
        "CONCLUSIONES FINALES Este artÃ­culo presenta la primera investigaciÃ³n sobre la destilaciÃ³n de informaciÃ³n basada en la utilidad con un sistema que aprende las necesidades de informaciÃ³n duraderas a partir de la retroalimentaciÃ³n detallada del usuario sobre una secuencia de pasajes clasificados.",
        "Nuestro sistema, llamado CAFÂ´E, combina filtrado adaptativo, detecciÃ³n de novedades y clasificaciÃ³n de pasajes antirredundantes en un marco unificado para la optimizaciÃ³n de la utilidad.",
        "Desarrollamos un nuevo esquema para la evaluaciÃ³n y retroalimentaciÃ³n automatizadas basado en un procedimiento semiautomÃ¡tico para adquirir reglas que permitan emparejar automÃ¡ticamente fragmentos con las respuestas del sistema.",
        "TambiÃ©n propusimos una extensiÃ³n de la mÃ©trica NDCG para evaluar la utilidad de los pasajes clasificados como una combinaciÃ³n ponderada de relevancia y novedad.",
        "Nuestros experimentos en el reciÃ©n anotado corpus de referencia TDT4 muestran un alentador aumento de utilidad sobre Indri, y tambiÃ©n sobre nuestro propio sistema con aprendizaje incremental y detecciÃ³n de novedades desactivados. 8.",
        "AGRADECIMIENTOS Nos gustarÃ­a agradecer a Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng y al Programa de AnÃ¡lisis de Datos Cualitativos de la Universidad de Pittsburgh liderado por el Dr. Stuart Shulman por su ayuda en la recolecciÃ³n y procesamiento de las anotaciones extendidas de TDT4 utilizadas en nuestros experimentos.",
        "Este trabajo estÃ¡ parcialmente respaldado por la FundaciÃ³n Nacional de Ciencias (NSF) bajo la subvenciÃ³n IIS0434035, y la Agencia de Proyectos de InvestigaciÃ³n Avanzada de Defensa (DARPA) bajo los contratos NBCHD030010 y W0550432.",
        "Cualquier opiniÃ³n, hallazgo, conclusiÃ³n o recomendaciÃ³n expresada en este material son responsabilidad de los autores y no reflejan necesariamente los puntos de vista de los patrocinadores.",
        "AUTORES ADICIONALES Jian Zhang (jianzhan@stat.purdue.edu)â , Jaime Carbonell (jgc@cs.cmu.edu)â  , Peter Brusilovsky (peterb+@pitt.edu)â¡ , Daqing He(dah44@pitt.edu)â¡ 10.",
        "REFERENCIAS [1] J. Allan.",
        "RetroalimentaciÃ³n incremental de relevancia para la filtraciÃ³n de informaciÃ³n.",
        "Actas de la 19Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 270-278, 1996. [2] J. Allan, C. Wade y A. Bolivar.",
        "RecuperaciÃ³n y DetecciÃ³n de Novedad a Nivel de OraciÃ³n.",
        "Actas de la conferencia ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, 2003. [3] C. Buckley, G. Salton y J. Allan.",
        "RecuperaciÃ³n automÃ¡tica con informaciÃ³n de localidad utilizando SMART.",
        "PublicaciÃ³n especial del NIST, (500207):59-72, 1993. [4] J. Callan.",
        "Aprendiendo mientras se filtran documentos.",
        "Actas de la 21Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 224-231, 1998. [5] J. Carbonell y J. Goldstein.",
        "El uso de MMR, Reordenamiento basado en Diversidad para Reorganizar Documentos y Producir ResÃºmenes.",
        "Actas de la 21Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 335-336, 1998. [6] E. Efthimiadis.",
        "ExpansiÃ³n de consulta.",
        "RevisiÃ³n Anual de Ciencia de la InformaciÃ³n y TecnologÃ­a (ARIST), 31:p121-87, 1996. [7] J. Fiscus y G. Duddington.",
        "Resumen de DetecciÃ³n y Seguimiento de Temas.",
        "DetecciÃ³n y seguimiento de temas: OrganizaciÃ³n de la informaciÃ³n basada en eventos, pÃ¡ginas 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov y S. Roukos.",
        "Un modelo estadÃ­stico para la detecciÃ³n y seguimiento de entidades multilingÃ¼es.",
        "NAACL/HLT, 2004. [9] K. JÃ¤rvelin y J. KekÃ¤lÃ¤inen.",
        "EvaluaciÃ³n basada en la Ganancia Acumulada de TÃ©cnicas de RecuperaciÃ³n de InformaciÃ³n.",
        "ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin y D. Demner-Fushman.",
        "EvaluaciÃ³n automÃ¡tica de respuestas a preguntas de definiciÃ³n.",
        "Actas de la Conferencia de TecnologÃ­a del Lenguaje Humano y Conferencia sobre MÃ©todos EmpÃ­ricos en Procesamiento del Lenguaje Natural (HLT/EMNLP 2005), 2005. â Departamento de EstadÃ­stica, Universidad de Purdue, West Lafayette, EE. UU. â  Instituto de TecnologÃ­as del Lenguaje, Universidad Carnegie Mellon, Pittsburgh, EE. UU. â¡ Escuela de Ciencias de la InformaciÃ³n, Universidad de Pittsburgh, Pittsburgh, EE. UU. [11] J. Lin y D. Demner-Fushman.",
        "Â¿Se derrumbarÃ¡n las pirÃ¡mides construidas con pepitas?",
        "Actas de HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla y S. Roukos.",
        "Un algoritmo de resoluciÃ³n de correferencia sÃ­ncrona basado en el Ã¡rbol de Bell.",
        "Actas de ACL, 4:136-143, 2004. [13] G. Marton.",
        "Nuggeteer: EvaluaciÃ³n AutomÃ¡tica Basada en Pepitas Utilizando Descripciones y Juicios.",
        "HLT/NAACL, 2006. [14] E. Riloff.",
        "ConstrucciÃ³n automÃ¡tica de un diccionario para tareas de extracciÃ³n de informaciÃ³n.",
        "Actas de la UndÃ©cima Conferencia Nacional sobre Inteligencia Artificial, pÃ¡ginas 811-816, 1993. [15] S. Robertson y S. Walker.",
        "Microsoft Cambridge en TREC-9: Pista de filtrado.",
        "La Novena Conferencia de RecuperaciÃ³n de Texto (TREC-9), pÃ¡ginas 361-368. [16] R. Schapire, Y.",
        "Cantante, y A. Singhal.",
        "Boosting y Rocchio aplicados a la filtraciÃ³n de texto.",
        "Actas de la 21Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle y W. Croft.",
        "Indri: Un motor de bÃºsqueda basado en modelos de lenguaje para consultas complejas.",
        "Actas de la Conferencia Internacional sobre AnÃ¡lisis de Inteligencia, 2004. [18] El Consorcio de Datos LingÃ¼Ã­sticos. http://www.ldc.upenn.edu/. [19] E. Voorhees.",
        "Resumen de la pista de preguntas y respuestas TREC 2003.",
        "Actas de la DuodÃ©cima Conferencia de RecuperaciÃ³n de Texto (TREC 2003), 2003. [20] Y. Yang y B. Kisiel.",
        "RegresiÃ³n local basada en mÃ¡rgenes para filtrado adaptativo.",
        "Actas de la duodÃ©cima conferencia internacional sobre gestiÃ³n de la informaciÃ³n y el conocimiento, pÃ¡ginas 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang y B. Kisiel.",
        "Robustez de los MÃ©todos de Filtrado Adaptativo en una EvaluaciÃ³n de Referencia Cruzada.",
        "Actas de la 28Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 98-105, 2005. [22] C. Zhai, W. Cohen y J. Lafferty.",
        "MÃ¡s allÃ¡ de la relevancia independiente: MÃ©todos y mÃ©tricas de evaluaciÃ³n para la recuperaciÃ³n de subtemas.",
        "Actas de la 26Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 10-17, 2003. [23] J. Zhang y Y. Yang.",
        "Robustez de los MÃ©todos de ClasificaciÃ³n Lineal Regularizados en la CategorizaciÃ³n de Textos.",
        "Actas de la 26Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 190-197, 2003. [24] Y. Zhang.",
        "Utilizando priors bayesianos para combinar clasificadores para filtrado adaptativo.",
        "Actas de la 27Âª conferencia internacional anual sobre InvestigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 345-352, 2004. [25] Y. Zhang, J. Callan y T. Minka.",
        "DetecciÃ³n de novedad y redundancia en el filtrado adaptativo.",
        "Actas de la 25Âª Conferencia Internacional Anual de ACM SIGIR sobre InvestigaciÃ³n y Desarrollo en RecuperaciÃ³n de InformaciÃ³n, 2002."
    ],
    "error_count": 3,
    "keys": {
        "utility-based information distillation": {
            "translated_key": "destilaciÃ³n de informaciÃ³n basada en utilidad",
            "is_in_text": true,
            "original_annotated_sentences": [
                "<br>utility-based information distillation</br> Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu ABSTRACT This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework.",
                "It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant passage ranking with respect to long-lasting information needs (tasks with multiple queries).",
                "Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings.",
                "For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task.",
                "Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty.",
                "Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval.",
                "Adaptive filtering (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics.",
                "Based on the initial query and a few positive examples (if available), an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user.",
                "The incremental learning nature of AF systems makes them more powerful than standard search engines that support ad-hoc retrieval (e.g.",
                "Google and Yahoo) in terms of finding relevant information with respect to long-lasting topics of interest, and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs, without having to modify their queries manually.",
                "A variety of supervised learning algorithms (Rocchio-style classifiers, Exponential-Gaussian models, local regression and logistic regression approaches) have been studied for adaptive settings, examined with explicit and implicit relevance feedback, and evaluated with respect to utility optimization on large benchmark data collections in TREC (Text Retrieval Conferences) and TDT (Topic Detection and Tracking) forums [1, 4, 7, 15, 16, 20, 24, 23].",
                "Regularized logistic regression [21] has been found representative for the state-of-the-art approaches, and highly efficient for frequent model adaptations over large document collections such as the TREC-10 corpus (over 800,000 documents and 84 topics).",
                "Despite substantial achievements in recent adaptive filtering research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently.",
                "Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications: 1.",
                "User has a rather passive role in the conventional adaptive filtering setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision.",
                "A more active alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents (or passages) per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists.",
                "The latter form of user interaction has been highly effective in standard retrieval for ad-hoc queries.",
                "How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2.",
                "The unit for receiving a relevance judgment (yes or no) is restricted to the document level in conventional AF.",
                "However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant.",
                "Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system.",
                "For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3.",
                "System-selected documents are often highly redundant.",
                "A major news event, for example, would be reported by multiple sources repeatedly for a while, making most of the information content in those articles redundant with each other.",
                "A conventional AF system would select all these redundant news stories for user feedback, wasting the users time while offering little gain.",
                "Clearly, techniques for novelty detection can help in principle [25, 2, 22] for improving the utility of the AF systems.",
                "However, the effectiveness of such techniques at passage level to detect novelty with respect to users (fine-grained) feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user.",
                "To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and novelty detection for removal of redundancy from user interactions with the system.",
                "We call the new process <br>utility-based information distillation</br>.",
                "Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach.",
                "Therefore, we extended a benchmark corpus - the TDT4 collection of news stories and TV broadcasts - with task definitions, multiple queries per task, and answer keys per query.",
                "We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 .",
                "To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses.",
                "Moreover, we propose an extension of NDCG (Normalized Discounted Cumulated Gain) [9] for assessing the utility of ranked passages as a function of both relevance and novelty.",
                "The rest of this paper is organized as follows.",
                "Section 2 outlines the information distillation process with a concrete example.",
                "Section 3 describes the technical cores of our system called CAFÂ´E - CMU Adaptive Filtering Engine.",
                "Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme.",
                "Section 5 describes the extended TDT4 corpus.",
                "Section 6 presents our experiments and results.",
                "Section 7 concludes the study and gives future perspectives. 2.",
                "A SAMPLE TASK Consider a news event - the escape of seven convicts from a Texas prison in December 2000 and their capture a month later.",
                "Assuming a user were interested in this event since its early stage, the information need could be: Find information about the escape of convicts from Texas prison, and information related to their recapture.",
                "The associated lower-level questions could be: 1.",
                "How many prisoners escaped? 2.",
                "Where and when were they sighted? 3. Who are their known contacts inside and outside the prison? 4.",
                "How are they armed? 5.",
                "Do they have any vehicles? 6.",
                "What steps have been taken so far?",
                "We call such an information need a task, and the associated questions as the queries in this task.",
                "A distillation system is supposed to monitor the incoming documents, process them chunk by chunk in a temporal order, select potentially relevant and novel passages from each chunk with respect to each query, and present a ranked list of passages to the user.",
                "Passage ranking here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list.",
                "When presented with a list of passages, the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant.",
                "These spans of text are taken as positive examples in the adaptation of the query profile, and also added to the users history.",
                "Passages not marked by the user are taken as negative examples.",
                "As soon as the query profile is updated, the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low, based on user preference.",
                "For example, if the user highlights ...officials have posted a $100,000 reward for their capture... as relevant answer to the query What steps have been taken so far?, then the highlighted piece is used as an additional positive training example in the adaptation of the query profile.",
                "This piece of feedback is also added to the user history as a seen example, so that in future, the system will not place another passage mentioning $100,000 reward at the top of the ranked list.",
                "However, an article mentioning ...officials have doubled the reward money to $200,000... might be ranked high since it is both relevant to the (updated) query profile and novel with respect to the (updated) user history.",
                "The user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly.",
                "Clearly, novelty detection is very important for the utility of such a system because of the iterative search.",
                "Without novelty detection, the old relevant passages would be shown to the user repeatedly in each ranked list.",
                "Through the above example, we can see the main properties of our new framework for <br>utility-based information distillation</br> over temporally ordered documents.",
                "Our framework combines and extends the power of adaptive filtering (AF), ad-hoc retrieval (IR) and novelty detection (ND).",
                "Compared to standard IR, our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task.",
                "Compared to conventional AF, it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system.",
                "Compared to past work, this is the first evaluation of novelty detection integrated with adaptive filtering for sequenced queries that allows flexible user feedback over ranked passages.",
                "The combination of AF, IR and ND with the new extensions raises an important research question regarding evaluation methodology: how can we measure the utility of such an information distillation system?",
                "Existing metrics in standard IR, AF and ND are insufficient, and new solutions must be explored, as we will discuss in Section 4, after describing the technical cores of our system in the next section. 3.",
                "TECHNICAL CORES The core components of CAFÂ´E are - 1) AF for incremental learning of query profiles, 2) IR for estimating relevance of passages with respect to query profiles, 3) ND for assessing novelty of passages with respect to users history, and 4) anti-redundancy component to remove redundancy from ranked lists. 3.1 Adaptive Filtering Component We use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for AF [21].",
                "Logistic regression (LR) is a supervised learning algorithm for statistical classification.",
                "Based on a training set of labeled instances, it learns a class model which can then by used to predict the labels of unseen instances.",
                "Its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required, as is the case in adaptive filtering, where the system must learn from each new feedback provided by the user. (See [21] and [23] for computational complexity and implementation issues).",
                "In adaptive filtering, each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query.",
                "For training the model, we use the query itself as the initial positive training example of the class, and the user-highlighted pieces of text (marked as Relevant or Not-relevant) during feedback as additional training examples.",
                "To address the cold start issue in the early stage before any user feedback is obtained, the system uses a small sample from a retrospective corpus as the initial negative examples in the training set.",
                "The details of using logistic regression for adaptive filtering (assigning different weights to positive and negative training instances, and regularizing the objective function to prevent over-fitting on training data) are presented in [21].",
                "The class model wâ learned by Logistic Regression, or the query profile, is a vector whose dimensions are individual terms and whose elements are the regression coefficients, indicating how influential each term is in the query profile.",
                "The query profile is updated whenever a new piece of user feedback is received.",
                "A temporally decaying weight can be applied to each training example, as an option, to emphasize the most recent user feedback. 3.2 Passage Retrieval Component We use standard IR techniques in this part of our system.",
                "Incoming documents are processed in chunks, where each chunk can be defined as a fixed span of time or as a fixed number of documents, as preferred by the user.",
                "For each incoming document, corpus statistics like the IDF (Inverted Document Frequency) of each term are updated.",
                "We use a state-of-the-art named entity identifier and tracker [8, 12] to identify person and location names, and merge them with co-referent named entities seen in the past.",
                "Then the documents are segmented into passages, which can be a whole document, a paragraph, a sentence, or any other continuous span of text, as preferred.",
                "Each passage is represented using a vector of TF-IDF (Term FrequencyInverse Document Frequency) weights, where term can be a word or a named entity.",
                "Given a query profile, i.e. the logistic regression solution wâ as described in Section 3.1, the system computes the posterior probability of relevance for each passage x as fRL(x) â¡ P(y = 1|x, wâ ) = 1 (1 + eâwâÂ·x) (1) Passages are ordered by their relevance scores, and the ones with scores above a threshold (tuned on a training set) comprise the relevance list that is passed on to the novelty detection step. 3.3 Novelty Detection Component CAFÂ´E maintains a user history H(t), which contains all the spans of text hi that the user highlighted (as feedback) during his or her past interactions with the system, up to the current time t. Denoting the history as H(t) = n h1, h2, ..., ht o , (2) the novelty score of a new candidate passage x is computed as: fND(x) = 1 â max iâ1..t {cos(x, hi)} (3) where both candidate passage x and highlighted spans of text hi are represented as TF-IDF vectors.",
                "The novelty score of each passage is compared to a prespecified threshold (also tuned on a training set), and any passage with a score below this threshold is removed from the relevance list. 3.4 Anti-redundant Ranking Component Although the novelty detection component ensures that only novel (previously unseen) information remains in the relevance list, this list might still contain the same novel information at multiple positions in the ranked list.",
                "Suppose, for example, that the user has already read about a $100,000 reward for information about the escaped convicts.",
                "A new piece of news that the award has been increased to $200,000 is novel since the user hasnt read about it yet.",
                "However, multiple news sources would report this news and we might end up showing (redundant) articles from all these sources in a ranked list.",
                "Hence, a ranked list should also be made non-redundant with respect to its own contents.",
                "We use a simplified version of the Maximal Marginal Relevance method [5], originally developed for combining relevance and novelty in text retrieval and summarization.",
                "Our procedure starts with the current list of passages sorted by relevance (section 3.2), filtered by Novelty Detection component (section 3.3), and generates a new non-redundant list as follows: 1.",
                "Take the top passage in the current list as the top one in the new list. 2.",
                "Add the next passage x in the current list to the new list only if fAR(x) > t where fAR(x) = 1 â max piâLnew {cos(x, pi)} and Lnew is the set of passages already selected in the new list. 3.",
                "Repeat step 2 until all the passages in the current list have been examined.",
                "After applying the above-mentioned algorithm, each passage in the new list is sufficiently dissimilar to others, thus favoring diversity rather than redundancy in the new ranked list.",
                "The anti-redundancy threshold t is tuned on a training set. 4.",
                "EVALUATION METHODOLOGY The approach we proposed above for information distillation raises important issues regarding evaluation methodology.",
                "Firstly, since our framework allows the output to be passages at different levels of granularity (e.g. k-sentence windows where k may vary) instead of a fixed length, it is not possible to have pre-annotated relevance judgments at all such granularity levels.",
                "Secondly, since we wish to measure the utility of the system output as a combination of both relevance and novelty, traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time.",
                "Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists.",
                "None of the existing measures in ad-hoc retrieval, adaptive filtering, novelty detection or other related areas (text summarization and question answering) have desirable properties in all the three aspects.",
                "Therefore, we must develop a new evaluation methodology. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers.",
                "Answer keys define what should be present in a system response to receive credit, and are comprised of a collection of information nuggets, i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them.",
                "Defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping [19], since system-returned passages can contain the same information expressed in many different ways.",
                "Hence, QA evaluations have relied on human assessors for the mapping between various expressions, making the process costly, time consuming, and not scalable to large query and document collections, and extensive system evaluations with various parameter settings. 4.1.1 Automating Evaluation based on Answer Keys Automatic evaluation methods would allow for faster system building and tuning, as well as provide an objective and affordable way of comparing various systems.",
                "Recently, such methods have been proposed, more or less, based on the idea of n-gram co-occurrences.",
                "Pourpre [10] assigns a fractional recall score to a system response based on its unigram overlap with a given nuggets description.",
                "For example, a system response A B C has recall 3/4 with respect to a nugget with description A B C D. However, such an approach is unfair to systems that present the same information but using words other than A, B, C, and D. Another open issue is how to weight individual words in measuring the closeness of a match.",
                "For example, consider the question How many prisoners escaped?.",
                "In the nugget Seven prisoners escaped from a Texas prison, there is no indication that seven is the keyword, and that it must be matched to get any relevance credit.",
                "Using IDF values does not help, since seven will generally not have a higher IDF than words like texas and prison.",
                "Also, redefining the nugget as just seven does not solve the problem since now it might spuriously match any mention of seven out of context.",
                "Nuggeteer [13] works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold.",
                "However, it is also plagued by spurious relevance since not all words contained in the nugget description (or known correct responses) are central to the nugget. 4.1.2 Nugget-Matching Rules We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on nugget-matching rules, which are generated using a semiautomatic procedure explained below.",
                "These rules are essentially Boolean queries that will only match against snippets that contain the nugget.",
                "For instance, a candidate rule for matching answers to How many prisoners escaped? is (Texas AND seven AND escape AND (convicts OR prisoners)), possibly with other synonyms and variants in the rule.",
                "For a corpus of news articles, which usually follow a typical formal prose, it is fairly easy to write such simple rules to match expected answers using a bootstrap approach, as described below.",
                "We propose a two-stage approach, inspired by Autoslog [14], that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents.",
                "In the first stage, human subjects annotated (using a highlighting tool) portions of ontopic documents that contained answers to each nugget 2 .",
                "In the second stage, subjects used our rule generation tool to create rules that would match the annotations for each nugget.",
                "The tool allows users to enter a Boolean rule as a disjunction of conjunctions (e.g. ((a AND b) OR (a AND c AND d) OR (e))).",
                "Given a candidate rule, our tool uses it as a Boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match.",
                "Hence, the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision.",
                "We observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions (adding more ANDs), and improving the recall by adding more conjunctions to the disjunction (adding more ORs).",
                "As an example, lets try to create a rule for the nugget which says that seven prisoners escaped from the Texas prison.",
                "We start with a simple rule - (seven).",
                "When we input this into the rule generation tool, we realize that this rule matches many spurious occurrences of seven (e.g. ...seven states...) and thus gets a low precision score.",
                "We can further qualify our rule - Texas AND seven AND convicts.",
                "Next, by looking at the missed annotations, we realize that some news articles mentioned ...seven prisoners escaped.... We then replace convicts with the disjunction (convicts OR prisoners).",
                "We continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the (small number of) misses and false alarms can be safely ignored.",
                "Thus we can create nugget-matching rules that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses.",
                "Human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably. 4.2 Evaluating the Utility of a Sequence of Ranked Lists The utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information, and how much the user lost in terms of time and energy.",
                "We calculate this utility from the utilities of individual passages as follows.",
                "After reading each passage returned by the system, the user derives some gain depending on the presence of relevant and novel information, and incurs a loss in terms of the time and energy spent in going through the passage.",
                "However, the likelihood that the user would actually read a passage depends on its position in the ranked list.",
                "Hence, for a query q, the expected utility 2 LDC [18] already provides relevance judgments for 100 topics on the TDT4 corpus.",
                "We further ensured that these judgments are exhaustive on the entire corpus using pooling. of a passage pi at rank i can be defined as U(pi, q) = P(i) â (Gain(pi, q) â Loss(pi, q)) (4) where P(i) is the probability that the user would go through a passage at rank i.",
                "The expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage: U(q) = nX i=1 P(i) â (Gain(pi, q) â Loss(pi, q)) (5) Note that if we ignore the loss term and define P(i) as P(i) â 1/ logb(b + i â 1) (6) then we get the recently popularized metric called Discounted Cumulated Gain (DCG) [9], where Gain(pi, q) is defined as the graded relevance of passage pi.",
                "However, without the loss term, DCG is a purely recall-oriented metric and not suitable for an adaptive filtering setting, where the systems utility depends in part on its ability to limit the number of items shown to the user.",
                "Although P(i) could be defined based on empirical studies of user behavior, for simplicity, we use P(i) exactly as defined in equation 6.",
                "The gain G(pi, q) of passage pi with respect to the query q is a function of - 1) the number of relevant nuggets present in pi, and 2) the novelty of each of these nuggets.",
                "We combine these two factors as follows.",
                "For each nugget Nj, we assign an initial weight wj, and also keep a count nj of the number of times this nugget has been seen by the user in the past.",
                "The gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor Î³.",
                "Thus, G(pi, q) is defined as G(pi, q) = X Nj âC(pi,q) wj â Î³nj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the answer key of query q.",
                "The initial weights wj are all set of be 1.0 in our experiments, but can also be set based on a pyramid approach [11].",
                "The choice of dampening factor Î³ determines the users tolerance for redundancy.",
                "When Î³ = 0, a nugget will only receive credit for its first occurrence i.e. when nj is zero3 .",
                "For 0 < Î³ < 1, a nugget receives smaller credit for each successive occurrence.",
                "When Î³ = 1, no dampening occurs and repeated occurrences of a nugget receive the same credit.",
                "Note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system, since the users are expected to remember what the system showed them in the past.",
                "We define the loss L(pi, q) as a constant cost c (we use 0.1) incurred when reading a system-returned passage.",
                "Thus, our metric can be re-written as U(q) = nX i=1 Gain(pi, q) logb(b + i â 1) â L(n) (8) where L(n) is the loss associated with a ranked list of length n: L(n) = c Â· nX i=1 1 logb(b + i â 1) (9) 3 Note that 00 = 1 Due to the similarity with Discounted Cumulated Gain (DCG), we call our metric Discounted Cumulated Utility (DCU).",
                "The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) â¤ 0 i.e. when the gain is less than or equal to the cost of reading the passage. 5.",
                "DATA TDT4 was the benchmark corpus used in TDT2002 and TDT2003 evaluations.",
                "The corpus consists of over 90, 000 news articles from multiple sources (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) published between October 2000 and January 2001, in the languages of Arabic, English, and Mandarin.",
                "Speech-recognized and machine-translated versions of the non-English articles were provided as well.",
                "LDC [18] has annotated the corpus with 100 topics, that correspond to various news events in this time period.",
                "Out of these, we selected a subset of 12 actionable events, and defined corresponding tasks for them4 .",
                "For each task, we manually defined a profile consisting of an initial set of (5 to 10) queries, a free-text description of the user history, i.e., what the user already knows about the event, and a list of known on-topic and off-topic documents (if available) as training examples.",
                "For each query, we generated answer keys and corresponding nugget matching rules using the procedure described in section 4.1.2, and produced a total of 120 queries, with an average of 7 nuggets per query. 6.",
                "EXPERIMENTS AND RESULTS 6.1 Baselines We used Indri [17], a popular language-model based retrieval engine, as a baseline for comparison with CAFÂ´E.",
                "Indri supports standard search engine functionality, including pseudo-relevance feedback (PRF) [3, 6], and is representative of a typical query-based retrieval system.",
                "Indri does not support any kind of novelty detection.",
                "We compare Indri with PRF turned on and off, against CAFÂ´E with user feedback, novelty detection and antiredundant ranking turned on and off. 6.2 Experimental Setup We divided the TDT4 corpus spanning 4 months into 10 chunks, each defined as a period of 12 consecutive days.",
                "At any given point of time in the distillation process, each system accessed the past data up to the current point, and returned a ranked list of up 50 passages per query.",
                "The 12 tasks defined on the corpus were divided into a training and test set with 6 tasks each.",
                "Each system was allowed to use the training set to tune its parameters for optimizing NDCU (equation 8), including the relevance threshold for both Indri and CAFÂ´E, and the novelty and antiredundancy thresholds for CAFÂ´E.",
                "The NDCU for each system run is calculated automatically.",
                "User feedback was also simulated - relevance judgments for each system-returned passage (as determined by the nugget matching rules described in section 4.1.2) were 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figure 1: Performance of Indri across chunks Figure 2: Performance of CAFÂ´E across chunks used as user feedback in the adaptation of query profiles and user histories. 6.3 Results In Table 1, we show the NDCU scores of the two systems under various settings.",
                "These scores are averaged over the six tasks in the test set, and are calculated with two dampening factors (see section 4.2): Î³ = 0 and 0.1, to simulate no tolerance and small tolerance for redundancy, respectively.",
                "Using Î³ = 0 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information.",
                "Hence, the improvement obtained from enabling user feedback is smaller with Î³ = 0 than the improvement obtained from feedback with Î³ = 0.1.",
                "This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information.",
                "However, the user does not benefit from seeing such redundant passages, and is usually interested in other passages containing related information.",
                "It is informative to evaluate retrieval systems using our utility measure (with Î³ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.",
                "Sometimes, however, users might indeed be interested in seeing the same information from multiple sources, as an Table 1: NDCU Scores of Indri and CAFÂ´E for two dampening factors (Î³), and various settings (F: Feedback, N: Novelty Detection, A: Anti-Redundant Ranking) Indri CAFÂ´E Î³ Base +PRF Base +F +F+N +F+A +F+N+A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.35 0.36 0.36 indicator of its importance or reliability.",
                "In such a case, we can simply choose a higher value for Î³ which corresponds to a higher tolerance for redundancy, and hence let the system tune its parameters accordingly.",
                "Since documents were processed chunk by chunk, it would be interesting to see how the performance of systems improves over time.",
                "Figures 1 and 2 show the performance trends for both the systems across chunks.",
                "While the performance with and without feedback on the first few chunks is expected to be close, for subsequent chunks, the performance curve with feedback enabled rises above the one with the no-feedback setting.",
                "The performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks, making some queries easier than others in certain chunks.",
                "Moreover, since Indri uses pseudo-relevance feedback while CAFÂ´E uses feedback based on actual relevance judgments, the improvement in case of Indri is less dramatic than that of CAFÂ´E. 7.",
                "CONCLUDING REMARKS This paper presents the first investigation on <br>utility-based information distillation</br> with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages.",
                "Our system, called CAFÂ´E, combines adaptive filtering, novelty detection and antiredundant passage ranking in a unified framework for utility optimization.",
                "We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty.",
                "Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off. 8.",
                "ACKNOWLEDGMENTS We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments.",
                "This work is supported in parts by the National Science Foundation (NSF) under grant IIS0434035, and the Defense Advanced Research Project Agency (DARPA) under contracts NBCHD030010 and W0550432.",
                "Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. 9.",
                "ADDITIONAL AUTHORS Jian Zhang (jianzhan@stat.purdue.edu)â , Jaime Carbonell (jgc@cs.cmu.edu)â  , Peter Brusilovsky (peterb+@pitt.edu)â¡ , Daqing He(dah44@pitt.edu)â¡ 10.",
                "REFERENCES [1] J. Allan.",
                "Incremental Relevance Feedback for Information Filtering.",
                "Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 270-278, 1996. [2] J. Allan, C. Wade, and A. Bolivar.",
                "Retrieval and Novelty Detection at the Sentence Level.",
                "Proceedings of the ACM SIGIR conference on research and development in information retrieval, 2003. [3] C. Buckley, G. Salton, and J. Allan.",
                "Automatic Retrieval with Locality Information using SMART.",
                "NIST special publication, (500207):59-72, 1993. [4] J. Callan.",
                "Learning While Filtering Documents.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 224-231, 1998. [5] J. Carbonell and J. Goldstein.",
                "The use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335-336, 1998. [6] E. Efthimiadis.",
                "Query Expansion.",
                "Annual Review of Information Science and Technology (ARIST), 31:p121-87, 1996. [7] J. Fiscus and G. Duddington.",
                "Topic Detection and Tracking Overview.",
                "Topic Detection and Tracking: Event-based Information Organization, pages 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.",
                "A Statistical Model for Multilingual Entity Detection and Tracking.",
                "NAACL/HLT, 2004. [9] K. JÂ¨arvelin and J. KekÂ¨alÂ¨ainen.",
                "Cumulated Gain-based Evaluation of IR Techniques.",
                "ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin and D. Demner-Fushman.",
                "Automatically Evaluating Answers to Definition Questions.",
                "Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), 2005. â Statistics Dept., Purdue University, West Lafayette, USA â  Language Technologies Inst., Carnegie Mellon University, Pittsburgh, USA â¡ School of Information Sciences, Univ. of Pittsburgh, Pittsburgh, USA [11] J. Lin and D. Demner-Fushman.",
                "Will Pyramids Built of nUggets Topple Over.",
                "Proceedings of HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.",
                "A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree.",
                "Proc. of ACL, 4:136-143, 2004. [13] G. Marton.",
                "Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments.",
                "HLT/NAACL, 2006. [14] E. Riloff.",
                "Automatically Constructing a Dictionary for Information Extraction Tasks.",
                "Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, 1993. [15] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9: Filtering track.",
                "The Ninth Text REtrieval Conference (TREC-9), pages 361-368. [16] R. Schapire, Y.",
                "Singer, and A. Singhal.",
                "Boosting and Rocchio Applied to Text Filtering.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle, and W. Croft.",
                "Indri: A Language Model-based Search Engine for Complex Queries.",
                "Proceedings of the International Conference on Intelligence Analysis, 2004. [18] The Linguistic Data Consortium. http://www.ldc.upenn.edu/. [19] E. Voorhees.",
                "Overview of the TREC 2003 Question Answering Track.",
                "Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), 2003. [20] Y. Yang and B. Kisiel.",
                "Margin-based Local Regression for Adaptive Filtering.",
                "Proceedings of the twelfth international conference on Information and knowledge management, pages 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of Adaptive Filtering Methods in a Cross-benchmark Evaluation.",
                "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 98-105, 2005. [22] C. Zhai, W. Cohen, and J. Lafferty.",
                "Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-17, 2003. [23] J. Zhang and Y. Yang.",
                "Robustness of Regularized Linear Classification Methods in Text Categorization.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 190-197, 2003. [24] Y. Zhang.",
                "Using Bayesian Priors to Combine Classifiers for Adaptive Filtering.",
                "Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 345-352, 2004. [25] Y. Zhang, J. Callan, and T. Minka.",
                "Novelty and Redundancy Detection in Adaptive Filtering.",
                "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002."
            ],
            "original_annotated_samples": [
                "<br>utility-based information distillation</br> Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst.",
                "We call the new process <br>utility-based information distillation</br>.",
                "Through the above example, we can see the main properties of our new framework for <br>utility-based information distillation</br> over temporally ordered documents.",
                "CONCLUDING REMARKS This paper presents the first investigation on <br>utility-based information distillation</br> with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages."
            ],
            "translated_annotated_samples": [
                "DestilaciÃ³n de informaciÃ³n basada en utilidad sobre documentos secuenciados temporalmente en el Instituto de TecnologÃ­as del Lenguaje de Yiming Yang.",
                "Llamamos al nuevo proceso <br>destilaciÃ³n de informaciÃ³n basada en utilidad</br>.",
                "A travÃ©s del ejemplo anterior, podemos ver las principales propiedades de nuestro nuevo marco de trabajo para la <br>destilaciÃ³n de informaciÃ³n basada en utilidad</br> sobre documentos ordenados temporalmente.",
                "CONCLUSIONES FINALES Este artÃ­culo presenta la primera investigaciÃ³n sobre la <br>destilaciÃ³n de informaciÃ³n basada en la utilidad</br> con un sistema que aprende las necesidades de informaciÃ³n duraderas a partir de la retroalimentaciÃ³n detallada del usuario sobre una secuencia de pasajes clasificados."
            ],
            "translated_text": "DestilaciÃ³n de informaciÃ³n basada en utilidad sobre documentos secuenciados temporalmente en el Instituto de TecnologÃ­as del Lenguaje de Yiming Yang. Universidad Carnegie Mellon, Pittsburgh, EE. UU. yiming@cs.cmu.edu Abhimanyu Lad Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. alad@cs.cmu.edu Instituto de TecnologÃ­as del Lenguaje Ni Lao. Universidad Carnegie Mellon, Pittsburgh, EE. UU. nlao@cs.cmu.edu Abhay Harpale Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. aharpale@cs.cmu.edu Bryan Kisiel Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. bkisiel@cs.cmu.edu MÃ³nica Rogati Instituto de TecnologÃ­as del Lenguaje. La Universidad Carnegie Mellon en Pittsburgh, EE. UU. mrogati@cs.cmu.edu RESUMEN Este artÃ­culo examina un nuevo enfoque para la destilaciÃ³n de informaciÃ³n en documentos ordenados temporalmente, y propone un esquema de evaluaciÃ³n novedoso para dicho marco. Combina las fortalezas y se extiende mÃ¡s allÃ¡ del filtrado adaptativo convencional, la detecciÃ³n de novedades y la clasificaciÃ³n de pasajes no redundantes con respecto a las necesidades de informaciÃ³n duraderas (tareas con mÃºltiples consultas). Nuestro enfoque respalda la retroalimentaciÃ³n detallada del usuario a travÃ©s del resaltado de fragmentos arbitrarios de texto, y aprovecha dicha informaciÃ³n para la optimizaciÃ³n de la utilidad en entornos adaptativos. Para nuestros experimentos, definimos tareas hipotÃ©ticas basadas en eventos de noticias en el corpus TDT4, con mÃºltiples consultas por tarea. Se generaron claves de respuestas (nuggets) para cada consulta y se utilizÃ³ un procedimiento semiautomÃ¡tico para adquirir reglas que permiten emparejar automÃ¡ticamente los nuggets con las respuestas del sistema. TambiÃ©n proponemos una extensiÃ³n de la mÃ©trica NDCG para evaluar la utilidad de los pasajes clasificados como una combinaciÃ³n de relevancia y novedad. Nuestros resultados muestran mejoras alentadoras en la utilidad utilizando el nuevo enfoque, en comparaciÃ³n con los sistemas base sin aprendizaje incremental o componentes de detecciÃ³n de novedades. CategorÃ­as y Descriptores de Asignaturas H.3.3 [BÃºsqueda y RecuperaciÃ³n de InformaciÃ³n]: Filtrado de informaciÃ³n, RetroalimentaciÃ³n de relevancia, Modelos de recuperaciÃ³n, Proceso de selecciÃ³n; I.5.2 TÃ©rminos Generales DiseÃ±o, MediciÃ³n, Rendimiento, ExperimentaciÃ³n. 1. INTRODUCCIÃN Seguir informaciÃ³n nueva y relevante de flujos de datos temporales para usuarios con necesidades duraderas ha sido un tema de investigaciÃ³n desafiante en la recuperaciÃ³n de informaciÃ³n. El filtrado adaptativo (AF) es una tarea de predicciÃ³n en lÃ­nea de la relevancia de cada nuevo documento con respecto a temas predefinidos. BasÃ¡ndose en la consulta inicial y algunos ejemplos positivos (si estÃ¡n disponibles), un sistema de AF mantiene un perfil para cada tema de interÃ©s, y lo actualiza constantemente segÃºn la retroalimentaciÃ³n del usuario. La naturaleza de aprendizaje incremental de los sistemas de AF los hace mÃ¡s poderosos que los motores de bÃºsqueda estÃ¡ndar que admiten recuperaciÃ³n ad-hoc (por ejemplo. Google y Yahoo) en tÃ©rminos de encontrar informaciÃ³n relevante con respecto a temas de interÃ©s duraderos, y mÃ¡s atractivo para los usuarios que estÃ¡n dispuestos a proporcionar retroalimentaciÃ³n para adaptar el sistema a sus necesidades de informaciÃ³n especÃ­ficas, sin tener que modificar sus consultas manualmente. Se han estudiado una variedad de algoritmos de aprendizaje supervisado (clasificadores de estilo Rocchio, modelos exponenciales-gaussianos, regresiÃ³n local y enfoques de regresiÃ³n logÃ­stica) para entornos adaptativos, examinados con retroalimentaciÃ³n de relevancia explÃ­cita e implÃ­cita, y evaluados con respecto a la optimizaciÃ³n de la utilidad en grandes colecciones de datos de referencia en los foros de TREC (Conferencias de RecuperaciÃ³n de Texto) y TDT (DetecciÃ³n y Seguimiento de Temas) [1, 4, 7, 15, 16, 20, 24, 23]. La regresiÃ³n logÃ­stica regularizada [21] se ha encontrado representativa para los enfoques de vanguardia, y altamente eficiente para adaptaciones frecuentes del modelo en grandes colecciones de documentos como el corpus TREC-10 (mÃ¡s de 800,000 documentos y 84 temas). A pesar de los logros sustanciales en la investigaciÃ³n reciente de filtrado adaptativo, aÃºn quedan problemas significativos sin resolver sobre cÃ³mo aprovechar de manera efectiva y eficiente la retroalimentaciÃ³n del usuario. EspecÃ­ficamente, los siguientes problemas pueden limitar seriamente la verdadera utilidad de los sistemas de AF en aplicaciones del mundo real: 1. El usuario tiene un papel bastante pasivo en la configuraciÃ³n convencional de filtrado adaptativo: solo reacciona ante el sistema cuando este toma una decisiÃ³n afirmativa sobre un documento, confirmando o rechazando esa decisiÃ³n. Una alternativa mÃ¡s activa serÃ­a permitir al usuario emitir mÃºltiples consultas sobre un tema, revisar una lista clasificada de documentos candidatos (o pasajes) por consulta, y proporcionar retroalimentaciÃ³n sobre la lista clasificada, refinando asÃ­ su necesidad de informaciÃ³n y solicitando listas clasificadas actualizadas. La Ãºltima forma de interacciÃ³n del usuario ha sido altamente efectiva en la recuperaciÃ³n estÃ¡ndar para consultas ad-hoc. CÃ³mo implementar una estrategia para necesidades de informaciÃ³n duraderas en entornos de FA es una pregunta abierta para la investigaciÃ³n. 2. La unidad para recibir un juicio de relevancia (sÃ­ o no) estÃ¡ restringida al nivel del documento en AF convencional. Sin embargo, un usuario real puede estar dispuesto a proporcionar comentarios mÃ¡s informativos y detallados resaltando algunas partes de texto en un documento recuperado como relevantes, en lugar de etiquetar todo el documento como relevante. Aprovechar de manera efectiva este tipo de retroalimentaciÃ³n detallada podrÃ­a mejorar sustancialmente la calidad de un sistema de AF. Para esto, necesitamos habilitar el aprendizaje supervisado a partir de fragmentos de texto etiquetados de longitud arbitraria en lugar de solo permitir documentos etiquetados. 3. Los documentos seleccionados por el sistema suelen ser altamente redundantes. Un evento importante de noticias, por ejemplo, serÃ­a reportado por mÃºltiples fuentes repetidamente durante un tiempo, haciendo que la mayor parte del contenido informativo en esos artÃ­culos sea redundante entre sÃ­. Un sistema de AF convencional seleccionarÃ­a todas estas historias de noticias redundantes para la retroalimentaciÃ³n del usuario, desperdiciando el tiempo de los usuarios mientras ofrece poco beneficio. Claramente, las tÃ©cnicas de detecciÃ³n de novedades pueden ayudar en principio [25, 2, 22] a mejorar la utilidad de los sistemas AF. Sin embargo, la efectividad de tales tÃ©cnicas a nivel de pasaje para detectar novedad con respecto a la retroalimentaciÃ³n de los usuarios a nivel detallado y para detectar redundancia en listas clasificadas aÃºn debe ser evaluada utilizando una medida de utilidad que imite las necesidades de un usuario real. Para abordar las limitaciones actuales de los sistemas de AF mencionadas anteriormente, proponemos y examinamos un nuevo enfoque en este artÃ­culo, que combina las fortalezas del AF convencional (aprendizaje incremental de modelos de temas), recuperaciÃ³n de pasajes multipase para consultas de larga duraciÃ³n condicionadas al tema, y detecciÃ³n de novedades para eliminar la redundancia de las interacciones del usuario con el sistema. Llamamos al nuevo proceso <br>destilaciÃ³n de informaciÃ³n basada en utilidad</br>. Ten en cuenta que los corpus de referencia convencionales para evaluaciones de AF, que tienen juicios de relevancia a nivel de documento y no definen tareas con mÃºltiples consultas, son insuficientes para evaluar el nuevo enfoque. Por lo tanto, ampliamos un corpus de referencia, la colecciÃ³n TDT4 de noticias y transmisiones de televisiÃ³n, con definiciones de tareas, mÃºltiples consultas por tarea y claves de respuestas por consulta. Hemos realizado nuestros experimentos en este corpus extendido TDT4 y hemos puesto a disposiciÃ³n del pÃºblico los datos generados adicionalmente para futuras evaluaciones comparativas. Para evaluar automÃ¡ticamente los fragmentos de texto arbitrarios devueltos por el sistema utilizando nuestras claves de respuestas, desarrollamos un esquema de evaluaciÃ³n con un procedimiento semiautomÃ¡tico para 1 URL: http://nyc.lti.cs.cmu.edu/downloads adquiriendo reglas que pueden comparar fragmentos con las respuestas del sistema. AdemÃ¡s, proponemos una extensiÃ³n de NDCG (Normalized Discounted Cumulated Gain) [9] para evaluar la utilidad de los pasajes clasificados como una funciÃ³n tanto de la relevancia como de la novedad. El resto de este documento estÃ¡ organizado de la siguiente manera. La secciÃ³n 2 describe el proceso de destilaciÃ³n de informaciÃ³n con un ejemplo concreto. La secciÃ³n 3 describe los nÃºcleos tÃ©cnicos de nuestro sistema llamado CAFÂ´E - CMU Adaptive Filtering Engine. La secciÃ³n 4 discute problemas con respecto a la metodologÃ­a de evaluaciÃ³n y propone un nuevo esquema. La secciÃ³n 5 describe el corpus extendido TDT4. La secciÃ³n 6 presenta nuestros experimentos y resultados. La secciÃ³n 7 concluye el estudio y ofrece perspectivas futuras. 2. Una TAREA DE EJEMPLO Considera un evento noticioso: la fuga de siete convictos de una prisiÃ³n de Texas en diciembre de 2000 y su captura un mes despuÃ©s. Suponiendo que un usuario estuviera interesado en este evento desde sus primeras etapas, la necesidad de informaciÃ³n podrÃ­a ser: Encontrar informaciÃ³n sobre la fuga de convictos de una prisiÃ³n de Texas, y la informaciÃ³n relacionada con su recaptura. Las preguntas de nivel inferior asociadas podrÃ­an ser: 1. Â¿CuÃ¡ntos prisioneros escaparon? 2. Â¿DÃ³nde y cuÃ¡ndo fueron avistados? 3. Â¿QuiÃ©nes son sus contactos conocidos dentro y fuera de la prisiÃ³n? 4. Â¿CÃ³mo estÃ¡n armados? 5. Â¿Tienen algÃºn vehÃ­culo? 6. Â¿QuÃ© pasos se han tomado hasta ahora? Llamamos a esta necesidad de informaciÃ³n una tarea, y a las preguntas asociadas como las consultas en esta tarea. Un sistema de destilaciÃ³n debe monitorear los documentos entrantes, procesarlos por fragmentos en un orden temporal, seleccionar pasajes potencialmente relevantes y novedosos de cada fragmento con respecto a cada consulta, y presentar una lista clasificada de pasajes al usuario. La clasificaciÃ³n de pasajes aquÃ­ se basa en cuÃ¡n relevante es un pasaje con respecto a la consulta actual, cuÃ¡n novedoso es con respecto al historial actual del usuario (de sus interacciones con el sistema) y cuÃ¡n redundante es en comparaciÃ³n con otros pasajes con un rango mÃ¡s alto en la lista. Cuando se le presenta una lista de pasajes, el usuario puede proporcionar retroalimentaciÃ³n resaltando tramos de texto arbitrarios que considere relevantes. Estos fragmentos de texto se toman como ejemplos positivos en la adaptaciÃ³n del perfil de consulta, y tambiÃ©n se aÃ±aden al historial de los usuarios. Los pasajes no marcados por el usuario se consideran ejemplos negativos. Tan pronto como se actualiza el perfil de consulta, el sistema vuelve a realizar una bÃºsqueda y devuelve otra lista clasificada de pasajes donde los pasajes previamente vistos son eliminados o clasificados como bajos, segÃºn la preferencia del usuario. Por ejemplo, si el usuario resalta ...los funcionarios han ofrecido una recompensa de $100,000 por su captura... como respuesta relevante a la consulta Â¿QuÃ© pasos se han tomado hasta ahora?, entonces la pieza resaltada se utiliza como un ejemplo de entrenamiento positivo adicional en la adaptaciÃ³n del perfil de la consulta. Este comentario tambiÃ©n se aÃ±ade al historial del usuario como un ejemplo visto, para que en el futuro, el sistema no coloque otro pasaje mencionando una recompensa de $100,000 en la parte superior de la lista clasificada. Sin embargo, un artÃ­culo que mencione que los funcionarios han duplicado el dinero de recompensa a $200,000 podrÃ­a clasificarse alto ya que es relevante tanto para el perfil de consulta (actualizado) como novedoso con respecto a la historia del usuario (actualizada). El usuario puede modificar las consultas originales o agregar una nueva consulta durante el proceso; los perfiles de consulta se cambiarÃ¡n en consecuencia. Claramente, la detecciÃ³n de novedades es muy importante para la utilidad de dicho sistema debido a la bÃºsqueda iterativa. Sin detecciÃ³n de novedad, los antiguos pasajes relevantes se mostrarÃ­an al usuario repetidamente en cada lista clasificada. A travÃ©s del ejemplo anterior, podemos ver las principales propiedades de nuestro nuevo marco de trabajo para la <br>destilaciÃ³n de informaciÃ³n basada en utilidad</br> sobre documentos ordenados temporalmente. Nuestro marco de trabajo combina y amplÃ­a el poder del filtrado adaptativo (AF), la recuperaciÃ³n ad-hoc (IR) y la detecciÃ³n de novedades (ND). En comparaciÃ³n con la IR estÃ¡ndar, nuestro enfoque tiene la capacidad de aprender de forma incremental las necesidades de informaciÃ³n a largo plazo y modelar una secuencia de consultas dentro de una tarea. En comparaciÃ³n con el AF convencional, permite un papel mÃ¡s activo del usuario en refinar sus necesidades de informaciÃ³n y solicitar nuevos resultados al permitir comentarios sobre relevancia y novedad mediante el resaltado de fragmentos arbitrarios de texto en los pasajes devueltos por el sistema. Comparado con trabajos anteriores, esta es la primera evaluaciÃ³n de detecciÃ³n de novedad integrada con filtrado adaptativo para consultas secuenciadas que permite retroalimentaciÃ³n flexible del usuario sobre pasajes clasificados. La combinaciÃ³n de AF, IR y ND con las nuevas extensiones plantea una importante pregunta de investigaciÃ³n sobre la metodologÃ­a de evaluaciÃ³n: Â¿cÃ³mo podemos medir la utilidad de un sistema de destilaciÃ³n de informaciÃ³n como este? Las mÃ©tricas existentes en IR estÃ¡ndar, AF y ND son insuficientes, y se deben explorar nuevas soluciones, como discutiremos en la SecciÃ³n 4, despuÃ©s de describir los nÃºcleos tÃ©cnicos de nuestro sistema en la prÃ³xima secciÃ³n. 3. NÃCLEOS TÃCNICOS Los componentes principales de CAFÃ son: 1) AF para el aprendizaje incremental de perfiles de consulta, 2) IR para estimar la relevancia de pasajes con respecto a los perfiles de consulta, 3) ND para evaluar la novedad de pasajes con respecto al historial de usuarios, y 4) un componente anti-redundancia para eliminar la redundancia de las listas clasificadas. 3.1 Componente de Filtrado Adaptativo Utilizamos un algoritmo de vanguardia en el campo, el mÃ©todo de regresiÃ³n logÃ­stica regularizada que obtuvo los mejores resultados en varios corpus de evaluaciÃ³n de referencia para AF [21]. La regresiÃ³n logÃ­stica (LR) es un algoritmo de aprendizaje supervisado para clasificaciÃ³n estadÃ­stica. BasÃ¡ndose en un conjunto de entrenamiento de instancias etiquetadas, aprende un modelo de clase que luego puede ser utilizado para predecir las etiquetas de instancias no vistas. Su rendimiento, asÃ­ como su eficiencia en tÃ©rminos de tiempo de entrenamiento, lo convierten en un buen candidato cuando se requieren actualizaciones frecuentes del modelo de clase, como es el caso en el filtrado adaptativo, donde el sistema debe aprender de cada nueva retroalimentaciÃ³n proporcionada por el usuario. (Consulte [21] y [23] para complejidad computacional y problemas de implementaciÃ³n). En el filtrado adaptativo, cada consulta se considera como una clase y la probabilidad de que un pasaje pertenezca a esta clase corresponde al grado de relevancia del pasaje con respecto a la consulta. Para entrenar el modelo, utilizamos la propia consulta como ejemplo inicial de entrenamiento positivo de la clase, y las piezas de texto resaltadas por el usuario (marcadas como Relevante o No relevante) durante la retroalimentaciÃ³n como ejemplos de entrenamiento adicionales. Para abordar el problema de inicio en frÃ­o en la etapa inicial antes de obtener cualquier retroalimentaciÃ³n del usuario, el sistema utiliza una pequeÃ±a muestra de un corpus retrospectivo como ejemplos negativos iniciales en el conjunto de entrenamiento. Los detalles de cÃ³mo utilizar la regresiÃ³n logÃ­stica para el filtrado adaptativo (asignando diferentes pesos a las instancias de entrenamiento positivas y negativas, y regularizando la funciÃ³n objetivo para prevenir el sobreajuste en los datos de entrenamiento) se presentan en [21]. El modelo de clase wâ aprendido por RegresiÃ³n LogÃ­stica, o el perfil de consulta, es un vector cuyas dimensiones son tÃ©rminos individuales y cuyos elementos son los coeficientes de regresiÃ³n, indicando cuÃ¡n influyente es cada tÃ©rmino en el perfil de consulta. El perfil de la consulta se actualiza cada vez que se recibe una nueva pieza de retroalimentaciÃ³n del usuario. Un peso decaÃ­do temporalmente se puede aplicar a cada ejemplo de entrenamiento, como una opciÃ³n, para enfatizar la retroalimentaciÃ³n mÃ¡s reciente del usuario. 3.2 Componente de RecuperaciÃ³n de Pasajes Utilizamos tÃ©cnicas estÃ¡ndar de IR en esta parte de nuestro sistema. Los documentos entrantes se procesan por bloques, donde cada bloque puede definirse como un intervalo de tiempo fijo o como un nÃºmero fijo de documentos, segÃºn lo prefiera el usuario. Para cada documento entrante, se actualizan las estadÃ­sticas del corpus como el IDF (Frecuencia Inversa de Documento) de cada tÃ©rmino. Utilizamos un identificador y rastreador de entidades con tecnologÃ­a de punta [8, 12] para identificar nombres de personas y lugares, y fusionarlos con entidades con referencia cruzada vistas en el pasado. Luego, los documentos se dividen en pasajes, que pueden ser un documento completo, un pÃ¡rrafo, una oraciÃ³n, o cualquier otro fragmento continuo de texto, segÃºn se prefiera. Cada pasaje se representa utilizando un vector de pesos TF-IDF (Frecuencia de TÃ©rmino - Frecuencia Inversa de Documento), donde el tÃ©rmino puede ser una palabra o una entidad nombrada. Dado un perfil de consulta, es decir, la soluciÃ³n de regresiÃ³n logÃ­stica wâ como se describe en la SecciÃ³n 3.1, el sistema calcula la probabilidad posterior de relevancia para cada pasaje x como fRL(x) â¡ P(y = 1|x, wâ ) = 1 (1 + eâwâÂ·x) (1). Los pasajes se ordenan segÃºn sus puntajes de relevancia, y aquellos con puntajes por encima de un umbral (ajustado en un conjunto de entrenamiento) conforman la lista de relevancia que se pasa al paso de detecciÃ³n de novedad. 3.3 Componente de DetecciÃ³n de Novedad CAFÂ´E mantiene un historial de usuario H(t), que contiene todos los fragmentos de texto hi que el usuario resaltÃ³ (como retroalimentaciÃ³n) durante sus interacciones pasadas con el sistema, hasta el tiempo actual t. Denotando el historial como H(t) = n h1, h2, ..., ht o , (2), el puntaje de novedad de un nuevo pasaje candidato x se calcula como: fND(x) = 1 â max iâ1..t {cos(x, hi)} (3), donde tanto el pasaje candidato x como los fragmentos de texto resaltados hi se representan como vectores TF-IDF. El puntaje de novedad de cada pasaje se compara con un umbral predefinido (ajustado tambiÃ©n en un conjunto de entrenamiento), y cualquier pasaje con un puntaje por debajo de este umbral se elimina de la lista de relevancia. Componente de clasificaciÃ³n anti-redundante Aunque el componente de detecciÃ³n de novedad garantiza que solo la informaciÃ³n novedosa (no vista previamente) permanezca en la lista de relevancia, esta lista aÃºn podrÃ­a contener la misma informaciÃ³n novedosa en mÃºltiples posiciones en la lista clasificada. Supongamos, por ejemplo, que el usuario ya ha leÃ­do sobre una recompensa de $100,000 por informaciÃ³n sobre los convictos fugados. Una nueva noticia de que el premio ha sido aumentado a $200,000 es novedosa ya que el usuario aÃºn no la ha leÃ­do. Sin embargo, mÃºltiples fuentes de noticias podrÃ­an informar sobre esta noticia y podrÃ­amos terminar mostrando artÃ­culos (redundantes) de todas estas fuentes en una lista clasificada. Por lo tanto, una lista clasificada tambiÃ©n debe ser no redundante con respecto a su propio contenido. Utilizamos una versiÃ³n simplificada del mÃ©todo de MÃ¡xima Relevancia Marginal [5], originalmente desarrollado para combinar relevancia y novedad en la recuperaciÃ³n de texto y resÃºmenes. Nuestro procedimiento comienza con la lista actual de pasajes ordenados por relevancia (secciÃ³n 3.2), filtrados por el componente de DetecciÃ³n de Novedades (secciÃ³n 3.3), y genera una nueva lista no redundante de la siguiente manera: 1. Toma el pasaje superior en la lista actual como el primero en la nueva lista. 2. Agrega el siguiente pasaje x en la lista actual a la nueva lista solo si fAR(x) > t donde fAR(x) = 1 â max piâLnew {cos(x, pi)} y Lnew es el conjunto de pasajes ya seleccionados en la nueva lista. Repetir el paso 2 hasta que se hayan examinado todos los pasajes en la lista actual. DespuÃ©s de aplicar el algoritmo mencionado anteriormente, cada pasaje en la nueva lista es lo suficientemente diferente a los demÃ¡s, favoreciendo asÃ­ la diversidad en lugar de la redundancia en la nueva lista clasificada. El umbral anti-redundancia t se ajusta en un conjunto de entrenamiento. 4. METODOLOGÃA DE EVALUACIÃN El enfoque que propusimos anteriormente para la destilaciÃ³n de informaciÃ³n plantea importantes cuestiones sobre la metodologÃ­a de evaluaciÃ³n. En primer lugar, dado que nuestro marco de trabajo permite que la salida sean fragmentos en diferentes niveles de granularidad (por ejemplo, ventanas de k oraciones donde k puede variar) en lugar de una longitud fija, no es posible tener juicios de relevancia preanotados en todos esos niveles de granularidad. En segundo lugar, dado que deseamos medir la utilidad de la salida del sistema como una combinaciÃ³n de relevancia y novedad, las medidas tradicionales basadas Ãºnicamente en la relevancia deben ser reemplazadas por medidas que penalicen la repeticiÃ³n de la misma informaciÃ³n en la salida del sistema a lo largo del tiempo. En tercer lugar, dado que la salida del sistema son listas clasificadas, debemos recompensar a aquellos sistemas que presenten informaciÃ³n Ãºtil (tanto relevante como no vista previamente) utilizando listas clasificadas mÃ¡s cortas, y penalizar a aquellos que presenten la misma informaciÃ³n utilizando listas clasificadas mÃ¡s largas. Ninguna de las medidas existentes en la recuperaciÃ³n ad-hoc, filtrado adaptativo, detecciÃ³n de novedades u otras Ã¡reas relacionadas (resumen de texto y respuesta a preguntas) tiene propiedades deseables en los tres aspectos. Por lo tanto, debemos desarrollar una nueva metodologÃ­a de evaluaciÃ³n. 4.1 Claves de Respuesta Para permitir la evaluaciÃ³n de un sistema cuya salida consiste en pasajes de longitud arbitraria, tomamos prestado el concepto de claves de respuesta de la comunidad de Preguntas y Respuestas (QA), donde los sistemas pueden devolver fragmentos de texto arbitrarios como respuestas. Las claves de respuestas definen lo que debe estar presente en la respuesta de un sistema para recibir crÃ©dito, y estÃ¡n compuestas por una colecciÃ³n de \"nuggets\" de informaciÃ³n, es decir, unidades de datos sobre las cuales los evaluadores humanos pueden tomar decisiones binarias sobre si una respuesta del sistema las contiene o no. La definiciÃ³n de claves de respuestas y la toma de decisiones binarias asociadas son tareas conceptuales que requieren un mapeo semÃ¡ntico [19], ya que los pasajes devueltos por el sistema pueden contener la misma informaciÃ³n expresada de muchas formas diferentes. Por lo tanto, las evaluaciones de QA han dependido de evaluadores humanos para la correspondencia entre diversas expresiones, lo que hace que el proceso sea costoso, consume mucho tiempo y no sea escalable para grandes colecciones de consultas y documentos, y evaluaciones extensas de sistemas con diversos ajustes de parÃ¡metros. 4.1.1 AutomatizaciÃ³n de la evaluaciÃ³n basada en claves de respuestas Los mÃ©todos de evaluaciÃ³n automÃ¡tica permitirÃ­an una construcciÃ³n y ajuste mÃ¡s rÃ¡pidos del sistema, asÃ­ como proporcionar una forma objetiva y asequible de comparar diversos sistemas. Recientemente, se han propuesto mÃ©todos basados mÃ¡s o menos en la idea de las co-ocurrencias de n-gramos. Pourpre [10] asigna una puntuaciÃ³n de recuerdo fraccional a una respuesta del sistema basada en su superposiciÃ³n de unigramas con una descripciÃ³n de pepitas dada. Por ejemplo, una respuesta del sistema A B C tiene una recuperaciÃ³n de 3/4 con respecto a un fragmento con la descripciÃ³n A B C D. Sin embargo, este enfoque es injusto para los sistemas que presentan la misma informaciÃ³n pero utilizando palabras distintas a A, B, C y D. Otro problema abierto es cÃ³mo ponderar las palabras individuales al medir la cercanÃ­a de una coincidencia. Por ejemplo, considera la pregunta Â¿CuÃ¡ntos prisioneros escaparon? En el fragmento Siete prisioneros escaparon de una prisiÃ³n de Texas, no hay indicaciÃ³n de que siete sea la palabra clave y que debe coincidir para obtener algÃºn crÃ©dito de relevancia. El uso de los valores de IDF no ayuda, ya que la palabra \"seven\" generalmente no tendrÃ¡ un IDF mÃ¡s alto que palabras como \"texas\" y \"prison\". AdemÃ¡s, redefinir el fragmento como simplemente siete no resuelve el problema, ya que ahora podrÃ­a coincidir errÃ³neamente con cualquier menciÃ³n de siete fuera de contexto. Nuggeteer [13] trabaja sobre principios similares pero toma decisiones binarias sobre si hay un fragmento en una respuesta de sistema dada ajustando un umbral. Sin embargo, tambiÃ©n estÃ¡ plagado de relevancia espuria ya que no todas las palabras contenidas en la descripciÃ³n de la pepita (o respuestas correctas conocidas) son centrales para la pepita. 4.1.2 Reglas de Coincidencia de Pepitas Proponemos un mÃ©todo automÃ¡tico confiable para determinar si un fragmento de texto contiene una pepita dada, basado en reglas de coincidencia de pepitas, que se generan utilizando un procedimiento semiautomÃ¡tico explicado a continuaciÃ³n. Estas reglas son bÃ¡sicamente consultas booleanas que solo coincidirÃ¡n con fragmentos que contengan la pepita. Por ejemplo, una regla candidata para emparejar respuestas a Â¿CuÃ¡ntos prisioneros escaparon? es (Texas Y siete Y escapar Y (convictos O prisioneros)), posiblemente con otros sinÃ³nimos y variantes en la regla. Para un corpus de artÃ­culos de noticias, que suelen seguir una prosa formal tÃ­pica, es bastante fÃ¡cil escribir reglas simples para coincidir con respuestas esperadas utilizando un enfoque de arranque, como se describe a continuaciÃ³n. Proponemos un enfoque de dos etapas, inspirado en Autoslog [14], que combina la capacidad de los humanos para identificar expresiones semÃ¡nticamente equivalentes y la capacidad del sistema para recopilar evidencia estadÃ­stica de un corpus de documentos anotados por humanos. En la primera etapa, los sujetos humanos anotaron (utilizando una herramienta de resaltado) porciones de documentos relevantes que contenÃ­an respuestas a cada fragmento 2. En la segunda etapa, los sujetos utilizaron nuestra herramienta de generaciÃ³n de reglas para crear reglas que coincidieran con las anotaciones de cada fragmento. La herramienta permite a los usuarios ingresar una regla booleana como una disyunciÃ³n de conjunciones (por ejemplo, ((a Y b) O (a Y c Y d) O (e))). Dada una regla candidata, nuestra herramienta la utiliza como una consulta booleana sobre el conjunto completo de documentos relevantes y calcula su recall y precisiÃ³n con respecto a las anotaciones que se espera que coincida. Por lo tanto, los sujetos pueden comenzar con una regla simple y refinarla iterativamente hasta que estÃ©n satisfechos con su precisiÃ³n y recuperaciÃ³n. Observamos que era muy fÃ¡cil para los humanos mejorar la precisiÃ³n de una regla ajustando sus conjunciones existentes (agregando mÃ¡s Y) y mejorar la recuperaciÃ³n agregando mÃ¡s conjunciones a la disyunciÃ³n (agregando mÃ¡s O). Como ejemplo, intentemos crear una regla para el acertijo que dice que siete prisioneros escaparon de la prisiÃ³n de Texas. Empezamos con una regla simple - (siete). Cuando introducimos esto en la herramienta de generaciÃ³n de reglas, nos damos cuenta de que esta regla coincide con muchas ocurrencias espurias de siete (por ejemplo, ...siete estados...) y, por lo tanto, obtiene una puntuaciÃ³n baja de precisiÃ³n. Podemos calificar aÃºn mÃ¡s nuestra regla - Texas Y siete Y convictos. A continuaciÃ³n, al revisar las anotaciones omitidas, nos damos cuenta de que algunos artÃ­culos de noticias mencionaban ...siete prisioneros escaparon.... Luego reemplazamos convictos por la disyunciÃ³n (convictos O prisioneros). Continuamos ajustando la regla de esta manera hasta lograr un nivel de recordaciÃ³n y precisiÃ³n suficientemente alto, es decir, que los (pocos) errores y alarmas falsas se puedan ignorar de forma segura. AsÃ­ podemos crear reglas de coincidencia de pepitas que capturen de manera sucinta varias formas de expresar una pepita, evitando al mismo tiempo emparejar respuestas incorrectas (o fuera de contexto). La participaciÃ³n humana en el proceso de creaciÃ³n de reglas garantiza reglas genÃ©ricas de alta calidad que luego se pueden utilizar para evaluar de manera confiable las respuestas del sistema arbitrario. 4.2 EvaluaciÃ³n de la Utilidad de una Secuencia de Listas Clasificadas La utilidad de un sistema de recuperaciÃ³n se puede definir como la diferencia entre cuÃ¡nta informaciÃ³n Ãºtil obtuvo el usuario y cuÃ¡nto tiempo y energÃ­a perdiÃ³. Calculamos esta utilidad a partir de las utilidades de los pasajes individuales de la siguiente manera. DespuÃ©s de leer cada pasaje devuelto por el sistema, el usuario obtiene cierta ganancia dependiendo de la presencia de informaciÃ³n relevante y novedosa, y incurre en una pÃ©rdida en tÃ©rminos del tiempo y la energÃ­a invertidos en revisar el pasaje. Sin embargo, la probabilidad de que el usuario realmente lea un pasaje depende de su posiciÃ³n en la lista clasificada. Por lo tanto, para una consulta q, la utilidad esperada 2 LDC [18] ya proporciona juicios de relevancia para 100 temas en el corpus TDT4. AdemÃ¡s, nos aseguramos de que estos juicios sean exhaustivos en todo el corpus utilizando la agrupaciÃ³n. El valor de un pasaje pi en el rango i se puede definir como U(pi, q) = P(i) â (Ganancia(pi, q) - PÃ©rdida(pi, q)) (4) donde P(i) es la probabilidad de que el usuario pase por un pasaje en el rango i. La utilidad esperada para una lista clasificada completa de longitud n se puede calcular simplemente sumando la utilidad esperada de cada pasaje: U(q) = Î£ i=1 P(i) â (Ganancia(pi, q) â PÃ©rdida(pi, q)) (5) NÃ³tese que si ignoramos el tÃ©rmino de pÃ©rdida y definimos P(i) como P(i) â 1/ logb(b + i â 1) (6) entonces obtenemos la mÃ©trica recientemente popularizada llamada Ganancia Acumulada Descontada (DCG) [9], donde Ganancia(pi, q) se define como la relevancia graduada del pasaje pi. Sin embargo, sin el tÃ©rmino de pÃ©rdida, DCG es una mÃ©trica puramente orientada al recuerdo y no es adecuada para un entorno de filtrado adaptativo, donde la utilidad del sistema depende en parte de su capacidad para limitar el nÃºmero de elementos mostrados al usuario. Aunque P(i) podrÃ­a definirse en base a estudios empÃ­ricos del comportamiento del usuario, por simplicidad, utilizamos P(i) tal como se define exactamente en la ecuaciÃ³n 6. La ganancia G(pi, q) del pasaje pi con respecto a la consulta q es una funciÃ³n de - 1) el nÃºmero de pepitas relevantes presentes en pi, y 2) la novedad de cada una de estas pepitas. Combinamos estos dos factores de la siguiente manera. Para cada pepita Nj, asignamos un peso inicial wj, y tambiÃ©n mantenemos un conteo nj del nÃºmero de veces que esta pepita ha sido vista por el usuario en el pasado. La ganancia derivada de cada ocurrencia posterior del mismo hallazgo se asume que se reduce por un factor de amortiguamiento Î³. Por lo tanto, G(pi, q) se define como G(pi, q) = X Nj âC(pi,q) wj â Î³nj (7) donde C(pi, q) es el conjunto de todas las pepitas que aparecen en el pasaje pi y tambiÃ©n pertenecen a la clave de respuesta de la consulta q. Los pesos iniciales wj se establecen todos en 1.0 en nuestros experimentos, pero tambiÃ©n pueden establecerse segÃºn un enfoque de pirÃ¡mide [11]. La elecciÃ³n del factor de amortiguaciÃ³n Î³ determina la tolerancia del usuario a la redundancia. Cuando Î³ = 0, un pepita solo recibirÃ¡ crÃ©dito por su primera ocurrencia, es decir, cuando nj sea cero. Para 0 < Î³ < 1, un nugget recibe menos crÃ©dito por cada ocurrencia sucesiva. Cuando Î³ = 1, no se produce amortiguamiento y las repeticiones de un valor reciben el mismo crÃ©dito. Se debe tener en cuenta que los recuentos de ocurrencia de las pepitas se conservan entre la evaluaciÃ³n de las listas clasificadas sucesivas devueltas por el sistema, ya que se espera que los usuarios recuerden lo que el sistema les mostrÃ³ en el pasado. Definimos la pÃ©rdida L(pi, q) como un costo constante c (usamos 0.1) incurrido al leer un pasaje devuelto por el sistema. Por lo tanto, nuestra mÃ©trica puede ser reescrita como U(q) = nX i=1 Ganancia(pi, q) logb(b + i â 1) â L(n) (8) donde L(n) es la pÃ©rdida asociada con una lista clasificada de longitud n: L(n) = c Â· nX i=1 1 logb(b + i â 1) (9) 3 NÃ³tese que 00 = 1 Debido a la similitud con la Ganancia Acumulada Descontada (DCG), llamamos a nuestra mÃ©trica Utilidad Acumulada Descontada (DCU). El puntaje de DCU obtenido por el sistema se convierte en un puntaje de DCU normalizado (NDCU) dividiÃ©ndolo por el puntaje de DCU de la lista ideal clasificada, que se crea ordenando los pasajes por sus puntajes de utilidad decrecientes U(pi, q) y deteniÃ©ndose cuando U(pi, q) â¤ 0, es decir, cuando la ganancia es menor o igual al costo de leer el pasaje. El conjunto de datos TDT4 fue el corpus de referencia utilizado en las evaluaciones TDT2002 y TDT2003. El corpus consiste en mÃ¡s de 90,000 artÃ­culos de noticias de mÃºltiples fuentes (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) publicados entre octubre de 2000 y enero de 2001, en los idiomas Ã¡rabe, inglÃ©s y mandarÃ­n. Las versiones de los artÃ­culos en otros idiomas reconocidas por voz y traducidas por mÃ¡quina tambiÃ©n fueron proporcionadas. LDC [18] ha anotado el corpus con 100 temas, que corresponden a varios eventos de noticias en este perÃ­odo de tiempo. De entre estos, seleccionamos un subconjunto de 12 eventos accionables y definimos tareas correspondientes para ellos. Para cada tarea, definimos manualmente un perfil que consiste en un conjunto inicial de (5 a 10) consultas, una descripciÃ³n de texto libre del historial del usuario, es decir, lo que el usuario ya sabe sobre el evento, y una lista de documentos conocidos relevantes y no relevantes (si estÃ¡n disponibles) como ejemplos de entrenamiento. Para cada consulta, generamos claves de respuestas y reglas de coincidencia de pepitas correspondientes utilizando el procedimiento descrito en la secciÃ³n 4.1.2, y produjimos un total de 120 consultas, con un promedio de 7 pepitas por consulta. EXPERIMENTOS Y RESULTADOS 6.1 Baselines Utilizamos Indri [17], un motor de bÃºsqueda basado en modelos de lenguaje popular, como referencia para comparar con CAFÃ. Indri admite la funcionalidad estÃ¡ndar de los motores de bÃºsqueda, incluido el feedback de pseudo-relevancia (PRF) [3, 6], y es representativo de un sistema de recuperaciÃ³n basado en consultas tÃ­pico. Indri no admite ningÃºn tipo de detecciÃ³n de novedades. Comparamos Indri con PRF activado y desactivado, frente a CAFÃ con retroalimentaciÃ³n del usuario, detecciÃ³n de novedades y clasificaciÃ³n antirredundante activados y desactivados. 6.2 ConfiguraciÃ³n Experimental Dividimos el corpus TDT4 que abarca 4 meses en 10 fragmentos, cada uno definido como un perÃ­odo de 12 dÃ­as consecutivos. En cualquier momento dado del proceso de destilaciÃ³n, cada sistema accedÃ­a a los datos pasados hasta el punto actual, y devolvÃ­a una lista clasificada de hasta 50 pasajes por consulta. Las 12 tareas definidas en el corpus se dividieron en un conjunto de entrenamiento y prueba con 6 tareas cada uno. Cada sistema pudo utilizar el conjunto de entrenamiento para ajustar sus parÃ¡metros con el fin de optimizar NDCU (ecuaciÃ³n 8), incluyendo el umbral de relevancia tanto para Indri como para CAFÂ´E, y los umbrales de novedad y antirredundancia para CAFÂ´E. La NDCU para cada ejecuciÃ³n del sistema se calcula automÃ¡ticamente. TambiÃ©n se simulÃ³ la retroalimentaciÃ³n de los usuarios: los juicios de relevancia para cada pasaje devuelto por el sistema (segÃºn las reglas de coincidencia de pepitas descritas en la secciÃ³n 4.1.2) fueron utilizados como retroalimentaciÃ³n de los usuarios en la adaptaciÃ³n de perfiles de consulta e historiales de usuario. En la Tabla 1, mostramos los puntajes NDCU de los dos sistemas en diferentes configuraciones. Estas puntuaciones se promedian en las seis tareas del conjunto de pruebas, y se calculan con dos factores de amortiguaciÃ³n (ver secciÃ³n 4.2): Î³ = 0 y 0.1, para simular ninguna tolerancia y poca tolerancia para la redundancia, respectivamente. Usar Î³ = 0 crea una mÃ©trica mucho mÃ¡s estricta ya que no otorga ningÃºn crÃ©dito a un pasaje que contenga informaciÃ³n relevante pero redundante. Por lo tanto, la mejora obtenida al habilitar la retroalimentaciÃ³n del usuario es menor con Î³ = 0 que la mejora obtenida de la retroalimentaciÃ³n con Î³ = 0.1. Esto revela una deficiencia de los sistemas de recuperaciÃ³n contemporÃ¡neos: cuando el usuario da retroalimentaciÃ³n positiva sobre un pasaje, los sistemas otorgan mayor peso a los tÃ©rminos presentes en ese pasaje y tienden a recuperar otros pasajes que contienen los mismos tÃ©rminos, y por lo tanto, generalmente la misma informaciÃ³n. Sin embargo, el usuario no se beneficia al ver pasajes redundantes y generalmente estÃ¡ interesado en otros pasajes que contengan informaciÃ³n relacionada. Es informativo evaluar los sistemas de recuperaciÃ³n utilizando nuestra medida de utilidad (con Î³ = 0) que tiene en cuenta la novedad y, por lo tanto, ofrece una imagen mÃ¡s realista de cuÃ¡n bien un sistema puede generalizar a partir de la retroalimentaciÃ³n del usuario, en lugar de utilizar medidas tradicionales de RI como la recuperaciÃ³n y la precisiÃ³n, que ofrecen una imagen incompleta de la mejora obtenida a partir de la retroalimentaciÃ³n del usuario. A veces, sin embargo, los usuarios podrÃ­an estar interesados en ver la misma informaciÃ³n de mÃºltiples fuentes, como un indicador de su importancia o fiabilidad. En tal caso, simplemente podemos elegir un valor mÃ¡s alto para Î³ que corresponda a una mayor tolerancia para la redundancia, y asÃ­ permitir que el sistema ajuste sus parÃ¡metros en consecuencia. Dado que los documentos fueron procesados por fragmentos, serÃ­a interesante ver cÃ³mo mejora el rendimiento de los sistemas con el tiempo. Las figuras 1 y 2 muestran las tendencias de rendimiento de ambos sistemas a lo largo de los fragmentos. Si bien se espera que el rendimiento con y sin retroalimentaciÃ³n en los primeros fragmentos sea similar, para los fragmentos posteriores, la curva de rendimiento con la retroalimentaciÃ³n habilitada supera a la del ajuste sin retroalimentaciÃ³n. Las tendencias de rendimiento no son consistentes en todos los fragmentos porque los documentos relevantes no estÃ¡n distribuidos uniformemente en todos los fragmentos, lo que hace que algunas consultas sean mÃ¡s fÃ¡ciles que otras en ciertos fragmentos. AdemÃ¡s, dado que Indri utiliza retroalimentaciÃ³n de pseudo-relevancia mientras que CAFÂ´E utiliza retroalimentaciÃ³n basada en juicios de relevancia reales, la mejora en el caso de Indri es menos dramÃ¡tica que la de CAFÂ´E. 7. CONCLUSIONES FINALES Este artÃ­culo presenta la primera investigaciÃ³n sobre la <br>destilaciÃ³n de informaciÃ³n basada en la utilidad</br> con un sistema que aprende las necesidades de informaciÃ³n duraderas a partir de la retroalimentaciÃ³n detallada del usuario sobre una secuencia de pasajes clasificados. Nuestro sistema, llamado CAFÂ´E, combina filtrado adaptativo, detecciÃ³n de novedades y clasificaciÃ³n de pasajes antirredundantes en un marco unificado para la optimizaciÃ³n de la utilidad. Desarrollamos un nuevo esquema para la evaluaciÃ³n y retroalimentaciÃ³n automatizadas basado en un procedimiento semiautomÃ¡tico para adquirir reglas que permitan emparejar automÃ¡ticamente fragmentos con las respuestas del sistema. TambiÃ©n propusimos una extensiÃ³n de la mÃ©trica NDCG para evaluar la utilidad de los pasajes clasificados como una combinaciÃ³n ponderada de relevancia y novedad. Nuestros experimentos en el reciÃ©n anotado corpus de referencia TDT4 muestran un alentador aumento de utilidad sobre Indri, y tambiÃ©n sobre nuestro propio sistema con aprendizaje incremental y detecciÃ³n de novedades desactivados. 8. AGRADECIMIENTOS Nos gustarÃ­a agradecer a Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng y al Programa de AnÃ¡lisis de Datos Cualitativos de la Universidad de Pittsburgh liderado por el Dr. Stuart Shulman por su ayuda en la recolecciÃ³n y procesamiento de las anotaciones extendidas de TDT4 utilizadas en nuestros experimentos. Este trabajo estÃ¡ parcialmente respaldado por la FundaciÃ³n Nacional de Ciencias (NSF) bajo la subvenciÃ³n IIS0434035, y la Agencia de Proyectos de InvestigaciÃ³n Avanzada de Defensa (DARPA) bajo los contratos NBCHD030010 y W0550432. Cualquier opiniÃ³n, hallazgo, conclusiÃ³n o recomendaciÃ³n expresada en este material son responsabilidad de los autores y no reflejan necesariamente los puntos de vista de los patrocinadores. AUTORES ADICIONALES Jian Zhang (jianzhan@stat.purdue.edu)â , Jaime Carbonell (jgc@cs.cmu.edu)â  , Peter Brusilovsky (peterb+@pitt.edu)â¡ , Daqing He(dah44@pitt.edu)â¡ 10. REFERENCIAS [1] J. Allan. RetroalimentaciÃ³n incremental de relevancia para la filtraciÃ³n de informaciÃ³n. Actas de la 19Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 270-278, 1996. [2] J. Allan, C. Wade y A. Bolivar. RecuperaciÃ³n y DetecciÃ³n de Novedad a Nivel de OraciÃ³n. Actas de la conferencia ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, 2003. [3] C. Buckley, G. Salton y J. Allan. RecuperaciÃ³n automÃ¡tica con informaciÃ³n de localidad utilizando SMART. PublicaciÃ³n especial del NIST, (500207):59-72, 1993. [4] J. Callan. Aprendiendo mientras se filtran documentos. Actas de la 21Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 224-231, 1998. [5] J. Carbonell y J. Goldstein. El uso de MMR, Reordenamiento basado en Diversidad para Reorganizar Documentos y Producir ResÃºmenes. Actas de la 21Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 335-336, 1998. [6] E. Efthimiadis. ExpansiÃ³n de consulta. RevisiÃ³n Anual de Ciencia de la InformaciÃ³n y TecnologÃ­a (ARIST), 31:p121-87, 1996. [7] J. Fiscus y G. Duddington. Resumen de DetecciÃ³n y Seguimiento de Temas. DetecciÃ³n y seguimiento de temas: OrganizaciÃ³n de la informaciÃ³n basada en eventos, pÃ¡ginas 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov y S. Roukos. Un modelo estadÃ­stico para la detecciÃ³n y seguimiento de entidades multilingÃ¼es. NAACL/HLT, 2004. [9] K. JÃ¤rvelin y J. KekÃ¤lÃ¤inen. EvaluaciÃ³n basada en la Ganancia Acumulada de TÃ©cnicas de RecuperaciÃ³n de InformaciÃ³n. ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin y D. Demner-Fushman. EvaluaciÃ³n automÃ¡tica de respuestas a preguntas de definiciÃ³n. Actas de la Conferencia de TecnologÃ­a del Lenguaje Humano y Conferencia sobre MÃ©todos EmpÃ­ricos en Procesamiento del Lenguaje Natural (HLT/EMNLP 2005), 2005. â Departamento de EstadÃ­stica, Universidad de Purdue, West Lafayette, EE. UU. â  Instituto de TecnologÃ­as del Lenguaje, Universidad Carnegie Mellon, Pittsburgh, EE. UU. â¡ Escuela de Ciencias de la InformaciÃ³n, Universidad de Pittsburgh, Pittsburgh, EE. UU. [11] J. Lin y D. Demner-Fushman. Â¿Se derrumbarÃ¡n las pirÃ¡mides construidas con pepitas? Actas de HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla y S. Roukos. Un algoritmo de resoluciÃ³n de correferencia sÃ­ncrona basado en el Ã¡rbol de Bell. Actas de ACL, 4:136-143, 2004. [13] G. Marton. Nuggeteer: EvaluaciÃ³n AutomÃ¡tica Basada en Pepitas Utilizando Descripciones y Juicios. HLT/NAACL, 2006. [14] E. Riloff. ConstrucciÃ³n automÃ¡tica de un diccionario para tareas de extracciÃ³n de informaciÃ³n. Actas de la UndÃ©cima Conferencia Nacional sobre Inteligencia Artificial, pÃ¡ginas 811-816, 1993. [15] S. Robertson y S. Walker. Microsoft Cambridge en TREC-9: Pista de filtrado. La Novena Conferencia de RecuperaciÃ³n de Texto (TREC-9), pÃ¡ginas 361-368. [16] R. Schapire, Y. Cantante, y A. Singhal. Boosting y Rocchio aplicados a la filtraciÃ³n de texto. Actas de la 21Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle y W. Croft. Indri: Un motor de bÃºsqueda basado en modelos de lenguaje para consultas complejas. Actas de la Conferencia Internacional sobre AnÃ¡lisis de Inteligencia, 2004. [18] El Consorcio de Datos LingÃ¼Ã­sticos. http://www.ldc.upenn.edu/. [19] E. Voorhees. Resumen de la pista de preguntas y respuestas TREC 2003. Actas de la DuodÃ©cima Conferencia de RecuperaciÃ³n de Texto (TREC 2003), 2003. [20] Y. Yang y B. Kisiel. RegresiÃ³n local basada en mÃ¡rgenes para filtrado adaptativo. Actas de la duodÃ©cima conferencia internacional sobre gestiÃ³n de la informaciÃ³n y el conocimiento, pÃ¡ginas 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang y B. Kisiel. Robustez de los MÃ©todos de Filtrado Adaptativo en una EvaluaciÃ³n de Referencia Cruzada. Actas de la 28Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 98-105, 2005. [22] C. Zhai, W. Cohen y J. Lafferty. MÃ¡s allÃ¡ de la relevancia independiente: MÃ©todos y mÃ©tricas de evaluaciÃ³n para la recuperaciÃ³n de subtemas. Actas de la 26Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 10-17, 2003. [23] J. Zhang y Y. Yang. Robustez de los MÃ©todos de ClasificaciÃ³n Lineal Regularizados en la CategorizaciÃ³n de Textos. Actas de la 26Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 190-197, 2003. [24] Y. Zhang. Utilizando priors bayesianos para combinar clasificadores para filtrado adaptativo. Actas de la 27Âª conferencia internacional anual sobre InvestigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 345-352, 2004. [25] Y. Zhang, J. Callan y T. Minka. DetecciÃ³n de novedad y redundancia en el filtrado adaptativo. Actas de la 25Âª Conferencia Internacional Anual de ACM SIGIR sobre InvestigaciÃ³n y Desarrollo en RecuperaciÃ³n de InformaciÃ³n, 2002. ",
            "candidates": [],
            "error": [
                [
                    "destilaciÃ³n de informaciÃ³n basada en utilidad",
                    "destilaciÃ³n de informaciÃ³n basada en utilidad",
                    "destilaciÃ³n de informaciÃ³n basada en la utilidad"
                ]
            ]
        },
        "temporally ordered document": {
            "translated_key": "documentos ordenados temporalmente",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Utility-based Information Distillation Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu ABSTRACT This paper examines a new approach to information distillation over <br>temporally ordered document</br>s, and proposes a novel evaluation scheme for such a framework.",
                "It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant passage ranking with respect to long-lasting information needs (tasks with multiple queries).",
                "Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings.",
                "For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task.",
                "Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty.",
                "Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval.",
                "Adaptive filtering (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics.",
                "Based on the initial query and a few positive examples (if available), an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user.",
                "The incremental learning nature of AF systems makes them more powerful than standard search engines that support ad-hoc retrieval (e.g.",
                "Google and Yahoo) in terms of finding relevant information with respect to long-lasting topics of interest, and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs, without having to modify their queries manually.",
                "A variety of supervised learning algorithms (Rocchio-style classifiers, Exponential-Gaussian models, local regression and logistic regression approaches) have been studied for adaptive settings, examined with explicit and implicit relevance feedback, and evaluated with respect to utility optimization on large benchmark data collections in TREC (Text Retrieval Conferences) and TDT (Topic Detection and Tracking) forums [1, 4, 7, 15, 16, 20, 24, 23].",
                "Regularized logistic regression [21] has been found representative for the state-of-the-art approaches, and highly efficient for frequent model adaptations over large document collections such as the TREC-10 corpus (over 800,000 documents and 84 topics).",
                "Despite substantial achievements in recent adaptive filtering research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently.",
                "Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications: 1.",
                "User has a rather passive role in the conventional adaptive filtering setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision.",
                "A more active alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents (or passages) per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists.",
                "The latter form of user interaction has been highly effective in standard retrieval for ad-hoc queries.",
                "How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2.",
                "The unit for receiving a relevance judgment (yes or no) is restricted to the document level in conventional AF.",
                "However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant.",
                "Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system.",
                "For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3.",
                "System-selected documents are often highly redundant.",
                "A major news event, for example, would be reported by multiple sources repeatedly for a while, making most of the information content in those articles redundant with each other.",
                "A conventional AF system would select all these redundant news stories for user feedback, wasting the users time while offering little gain.",
                "Clearly, techniques for novelty detection can help in principle [25, 2, 22] for improving the utility of the AF systems.",
                "However, the effectiveness of such techniques at passage level to detect novelty with respect to users (fine-grained) feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user.",
                "To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and novelty detection for removal of redundancy from user interactions with the system.",
                "We call the new process utility-based information distillation.",
                "Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach.",
                "Therefore, we extended a benchmark corpus - the TDT4 collection of news stories and TV broadcasts - with task definitions, multiple queries per task, and answer keys per query.",
                "We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 .",
                "To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses.",
                "Moreover, we propose an extension of NDCG (Normalized Discounted Cumulated Gain) [9] for assessing the utility of ranked passages as a function of both relevance and novelty.",
                "The rest of this paper is organized as follows.",
                "Section 2 outlines the information distillation process with a concrete example.",
                "Section 3 describes the technical cores of our system called CAFÂ´E - CMU Adaptive Filtering Engine.",
                "Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme.",
                "Section 5 describes the extended TDT4 corpus.",
                "Section 6 presents our experiments and results.",
                "Section 7 concludes the study and gives future perspectives. 2.",
                "A SAMPLE TASK Consider a news event - the escape of seven convicts from a Texas prison in December 2000 and their capture a month later.",
                "Assuming a user were interested in this event since its early stage, the information need could be: Find information about the escape of convicts from Texas prison, and information related to their recapture.",
                "The associated lower-level questions could be: 1.",
                "How many prisoners escaped? 2.",
                "Where and when were they sighted? 3. Who are their known contacts inside and outside the prison? 4.",
                "How are they armed? 5.",
                "Do they have any vehicles? 6.",
                "What steps have been taken so far?",
                "We call such an information need a task, and the associated questions as the queries in this task.",
                "A distillation system is supposed to monitor the incoming documents, process them chunk by chunk in a temporal order, select potentially relevant and novel passages from each chunk with respect to each query, and present a ranked list of passages to the user.",
                "Passage ranking here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list.",
                "When presented with a list of passages, the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant.",
                "These spans of text are taken as positive examples in the adaptation of the query profile, and also added to the users history.",
                "Passages not marked by the user are taken as negative examples.",
                "As soon as the query profile is updated, the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low, based on user preference.",
                "For example, if the user highlights ...officials have posted a $100,000 reward for their capture... as relevant answer to the query What steps have been taken so far?, then the highlighted piece is used as an additional positive training example in the adaptation of the query profile.",
                "This piece of feedback is also added to the user history as a seen example, so that in future, the system will not place another passage mentioning $100,000 reward at the top of the ranked list.",
                "However, an article mentioning ...officials have doubled the reward money to $200,000... might be ranked high since it is both relevant to the (updated) query profile and novel with respect to the (updated) user history.",
                "The user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly.",
                "Clearly, novelty detection is very important for the utility of such a system because of the iterative search.",
                "Without novelty detection, the old relevant passages would be shown to the user repeatedly in each ranked list.",
                "Through the above example, we can see the main properties of our new framework for utility-based information distillation over <br>temporally ordered document</br>s.",
                "Our framework combines and extends the power of adaptive filtering (AF), ad-hoc retrieval (IR) and novelty detection (ND).",
                "Compared to standard IR, our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task.",
                "Compared to conventional AF, it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system.",
                "Compared to past work, this is the first evaluation of novelty detection integrated with adaptive filtering for sequenced queries that allows flexible user feedback over ranked passages.",
                "The combination of AF, IR and ND with the new extensions raises an important research question regarding evaluation methodology: how can we measure the utility of such an information distillation system?",
                "Existing metrics in standard IR, AF and ND are insufficient, and new solutions must be explored, as we will discuss in Section 4, after describing the technical cores of our system in the next section. 3.",
                "TECHNICAL CORES The core components of CAFÂ´E are - 1) AF for incremental learning of query profiles, 2) IR for estimating relevance of passages with respect to query profiles, 3) ND for assessing novelty of passages with respect to users history, and 4) anti-redundancy component to remove redundancy from ranked lists. 3.1 Adaptive Filtering Component We use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for AF [21].",
                "Logistic regression (LR) is a supervised learning algorithm for statistical classification.",
                "Based on a training set of labeled instances, it learns a class model which can then by used to predict the labels of unseen instances.",
                "Its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required, as is the case in adaptive filtering, where the system must learn from each new feedback provided by the user. (See [21] and [23] for computational complexity and implementation issues).",
                "In adaptive filtering, each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query.",
                "For training the model, we use the query itself as the initial positive training example of the class, and the user-highlighted pieces of text (marked as Relevant or Not-relevant) during feedback as additional training examples.",
                "To address the cold start issue in the early stage before any user feedback is obtained, the system uses a small sample from a retrospective corpus as the initial negative examples in the training set.",
                "The details of using logistic regression for adaptive filtering (assigning different weights to positive and negative training instances, and regularizing the objective function to prevent over-fitting on training data) are presented in [21].",
                "The class model wâ learned by Logistic Regression, or the query profile, is a vector whose dimensions are individual terms and whose elements are the regression coefficients, indicating how influential each term is in the query profile.",
                "The query profile is updated whenever a new piece of user feedback is received.",
                "A temporally decaying weight can be applied to each training example, as an option, to emphasize the most recent user feedback. 3.2 Passage Retrieval Component We use standard IR techniques in this part of our system.",
                "Incoming documents are processed in chunks, where each chunk can be defined as a fixed span of time or as a fixed number of documents, as preferred by the user.",
                "For each incoming document, corpus statistics like the IDF (Inverted Document Frequency) of each term are updated.",
                "We use a state-of-the-art named entity identifier and tracker [8, 12] to identify person and location names, and merge them with co-referent named entities seen in the past.",
                "Then the documents are segmented into passages, which can be a whole document, a paragraph, a sentence, or any other continuous span of text, as preferred.",
                "Each passage is represented using a vector of TF-IDF (Term FrequencyInverse Document Frequency) weights, where term can be a word or a named entity.",
                "Given a query profile, i.e. the logistic regression solution wâ as described in Section 3.1, the system computes the posterior probability of relevance for each passage x as fRL(x) â¡ P(y = 1|x, wâ ) = 1 (1 + eâwâÂ·x) (1) Passages are ordered by their relevance scores, and the ones with scores above a threshold (tuned on a training set) comprise the relevance list that is passed on to the novelty detection step. 3.3 Novelty Detection Component CAFÂ´E maintains a user history H(t), which contains all the spans of text hi that the user highlighted (as feedback) during his or her past interactions with the system, up to the current time t. Denoting the history as H(t) = n h1, h2, ..., ht o , (2) the novelty score of a new candidate passage x is computed as: fND(x) = 1 â max iâ1..t {cos(x, hi)} (3) where both candidate passage x and highlighted spans of text hi are represented as TF-IDF vectors.",
                "The novelty score of each passage is compared to a prespecified threshold (also tuned on a training set), and any passage with a score below this threshold is removed from the relevance list. 3.4 Anti-redundant Ranking Component Although the novelty detection component ensures that only novel (previously unseen) information remains in the relevance list, this list might still contain the same novel information at multiple positions in the ranked list.",
                "Suppose, for example, that the user has already read about a $100,000 reward for information about the escaped convicts.",
                "A new piece of news that the award has been increased to $200,000 is novel since the user hasnt read about it yet.",
                "However, multiple news sources would report this news and we might end up showing (redundant) articles from all these sources in a ranked list.",
                "Hence, a ranked list should also be made non-redundant with respect to its own contents.",
                "We use a simplified version of the Maximal Marginal Relevance method [5], originally developed for combining relevance and novelty in text retrieval and summarization.",
                "Our procedure starts with the current list of passages sorted by relevance (section 3.2), filtered by Novelty Detection component (section 3.3), and generates a new non-redundant list as follows: 1.",
                "Take the top passage in the current list as the top one in the new list. 2.",
                "Add the next passage x in the current list to the new list only if fAR(x) > t where fAR(x) = 1 â max piâLnew {cos(x, pi)} and Lnew is the set of passages already selected in the new list. 3.",
                "Repeat step 2 until all the passages in the current list have been examined.",
                "After applying the above-mentioned algorithm, each passage in the new list is sufficiently dissimilar to others, thus favoring diversity rather than redundancy in the new ranked list.",
                "The anti-redundancy threshold t is tuned on a training set. 4.",
                "EVALUATION METHODOLOGY The approach we proposed above for information distillation raises important issues regarding evaluation methodology.",
                "Firstly, since our framework allows the output to be passages at different levels of granularity (e.g. k-sentence windows where k may vary) instead of a fixed length, it is not possible to have pre-annotated relevance judgments at all such granularity levels.",
                "Secondly, since we wish to measure the utility of the system output as a combination of both relevance and novelty, traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time.",
                "Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists.",
                "None of the existing measures in ad-hoc retrieval, adaptive filtering, novelty detection or other related areas (text summarization and question answering) have desirable properties in all the three aspects.",
                "Therefore, we must develop a new evaluation methodology. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers.",
                "Answer keys define what should be present in a system response to receive credit, and are comprised of a collection of information nuggets, i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them.",
                "Defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping [19], since system-returned passages can contain the same information expressed in many different ways.",
                "Hence, QA evaluations have relied on human assessors for the mapping between various expressions, making the process costly, time consuming, and not scalable to large query and document collections, and extensive system evaluations with various parameter settings. 4.1.1 Automating Evaluation based on Answer Keys Automatic evaluation methods would allow for faster system building and tuning, as well as provide an objective and affordable way of comparing various systems.",
                "Recently, such methods have been proposed, more or less, based on the idea of n-gram co-occurrences.",
                "Pourpre [10] assigns a fractional recall score to a system response based on its unigram overlap with a given nuggets description.",
                "For example, a system response A B C has recall 3/4 with respect to a nugget with description A B C D. However, such an approach is unfair to systems that present the same information but using words other than A, B, C, and D. Another open issue is how to weight individual words in measuring the closeness of a match.",
                "For example, consider the question How many prisoners escaped?.",
                "In the nugget Seven prisoners escaped from a Texas prison, there is no indication that seven is the keyword, and that it must be matched to get any relevance credit.",
                "Using IDF values does not help, since seven will generally not have a higher IDF than words like texas and prison.",
                "Also, redefining the nugget as just seven does not solve the problem since now it might spuriously match any mention of seven out of context.",
                "Nuggeteer [13] works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold.",
                "However, it is also plagued by spurious relevance since not all words contained in the nugget description (or known correct responses) are central to the nugget. 4.1.2 Nugget-Matching Rules We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on nugget-matching rules, which are generated using a semiautomatic procedure explained below.",
                "These rules are essentially Boolean queries that will only match against snippets that contain the nugget.",
                "For instance, a candidate rule for matching answers to How many prisoners escaped? is (Texas AND seven AND escape AND (convicts OR prisoners)), possibly with other synonyms and variants in the rule.",
                "For a corpus of news articles, which usually follow a typical formal prose, it is fairly easy to write such simple rules to match expected answers using a bootstrap approach, as described below.",
                "We propose a two-stage approach, inspired by Autoslog [14], that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents.",
                "In the first stage, human subjects annotated (using a highlighting tool) portions of ontopic documents that contained answers to each nugget 2 .",
                "In the second stage, subjects used our rule generation tool to create rules that would match the annotations for each nugget.",
                "The tool allows users to enter a Boolean rule as a disjunction of conjunctions (e.g. ((a AND b) OR (a AND c AND d) OR (e))).",
                "Given a candidate rule, our tool uses it as a Boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match.",
                "Hence, the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision.",
                "We observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions (adding more ANDs), and improving the recall by adding more conjunctions to the disjunction (adding more ORs).",
                "As an example, lets try to create a rule for the nugget which says that seven prisoners escaped from the Texas prison.",
                "We start with a simple rule - (seven).",
                "When we input this into the rule generation tool, we realize that this rule matches many spurious occurrences of seven (e.g. ...seven states...) and thus gets a low precision score.",
                "We can further qualify our rule - Texas AND seven AND convicts.",
                "Next, by looking at the missed annotations, we realize that some news articles mentioned ...seven prisoners escaped.... We then replace convicts with the disjunction (convicts OR prisoners).",
                "We continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the (small number of) misses and false alarms can be safely ignored.",
                "Thus we can create nugget-matching rules that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses.",
                "Human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably. 4.2 Evaluating the Utility of a Sequence of Ranked Lists The utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information, and how much the user lost in terms of time and energy.",
                "We calculate this utility from the utilities of individual passages as follows.",
                "After reading each passage returned by the system, the user derives some gain depending on the presence of relevant and novel information, and incurs a loss in terms of the time and energy spent in going through the passage.",
                "However, the likelihood that the user would actually read a passage depends on its position in the ranked list.",
                "Hence, for a query q, the expected utility 2 LDC [18] already provides relevance judgments for 100 topics on the TDT4 corpus.",
                "We further ensured that these judgments are exhaustive on the entire corpus using pooling. of a passage pi at rank i can be defined as U(pi, q) = P(i) â (Gain(pi, q) â Loss(pi, q)) (4) where P(i) is the probability that the user would go through a passage at rank i.",
                "The expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage: U(q) = nX i=1 P(i) â (Gain(pi, q) â Loss(pi, q)) (5) Note that if we ignore the loss term and define P(i) as P(i) â 1/ logb(b + i â 1) (6) then we get the recently popularized metric called Discounted Cumulated Gain (DCG) [9], where Gain(pi, q) is defined as the graded relevance of passage pi.",
                "However, without the loss term, DCG is a purely recall-oriented metric and not suitable for an adaptive filtering setting, where the systems utility depends in part on its ability to limit the number of items shown to the user.",
                "Although P(i) could be defined based on empirical studies of user behavior, for simplicity, we use P(i) exactly as defined in equation 6.",
                "The gain G(pi, q) of passage pi with respect to the query q is a function of - 1) the number of relevant nuggets present in pi, and 2) the novelty of each of these nuggets.",
                "We combine these two factors as follows.",
                "For each nugget Nj, we assign an initial weight wj, and also keep a count nj of the number of times this nugget has been seen by the user in the past.",
                "The gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor Î³.",
                "Thus, G(pi, q) is defined as G(pi, q) = X Nj âC(pi,q) wj â Î³nj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the answer key of query q.",
                "The initial weights wj are all set of be 1.0 in our experiments, but can also be set based on a pyramid approach [11].",
                "The choice of dampening factor Î³ determines the users tolerance for redundancy.",
                "When Î³ = 0, a nugget will only receive credit for its first occurrence i.e. when nj is zero3 .",
                "For 0 < Î³ < 1, a nugget receives smaller credit for each successive occurrence.",
                "When Î³ = 1, no dampening occurs and repeated occurrences of a nugget receive the same credit.",
                "Note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system, since the users are expected to remember what the system showed them in the past.",
                "We define the loss L(pi, q) as a constant cost c (we use 0.1) incurred when reading a system-returned passage.",
                "Thus, our metric can be re-written as U(q) = nX i=1 Gain(pi, q) logb(b + i â 1) â L(n) (8) where L(n) is the loss associated with a ranked list of length n: L(n) = c Â· nX i=1 1 logb(b + i â 1) (9) 3 Note that 00 = 1 Due to the similarity with Discounted Cumulated Gain (DCG), we call our metric Discounted Cumulated Utility (DCU).",
                "The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) â¤ 0 i.e. when the gain is less than or equal to the cost of reading the passage. 5.",
                "DATA TDT4 was the benchmark corpus used in TDT2002 and TDT2003 evaluations.",
                "The corpus consists of over 90, 000 news articles from multiple sources (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) published between October 2000 and January 2001, in the languages of Arabic, English, and Mandarin.",
                "Speech-recognized and machine-translated versions of the non-English articles were provided as well.",
                "LDC [18] has annotated the corpus with 100 topics, that correspond to various news events in this time period.",
                "Out of these, we selected a subset of 12 actionable events, and defined corresponding tasks for them4 .",
                "For each task, we manually defined a profile consisting of an initial set of (5 to 10) queries, a free-text description of the user history, i.e., what the user already knows about the event, and a list of known on-topic and off-topic documents (if available) as training examples.",
                "For each query, we generated answer keys and corresponding nugget matching rules using the procedure described in section 4.1.2, and produced a total of 120 queries, with an average of 7 nuggets per query. 6.",
                "EXPERIMENTS AND RESULTS 6.1 Baselines We used Indri [17], a popular language-model based retrieval engine, as a baseline for comparison with CAFÂ´E.",
                "Indri supports standard search engine functionality, including pseudo-relevance feedback (PRF) [3, 6], and is representative of a typical query-based retrieval system.",
                "Indri does not support any kind of novelty detection.",
                "We compare Indri with PRF turned on and off, against CAFÂ´E with user feedback, novelty detection and antiredundant ranking turned on and off. 6.2 Experimental Setup We divided the TDT4 corpus spanning 4 months into 10 chunks, each defined as a period of 12 consecutive days.",
                "At any given point of time in the distillation process, each system accessed the past data up to the current point, and returned a ranked list of up 50 passages per query.",
                "The 12 tasks defined on the corpus were divided into a training and test set with 6 tasks each.",
                "Each system was allowed to use the training set to tune its parameters for optimizing NDCU (equation 8), including the relevance threshold for both Indri and CAFÂ´E, and the novelty and antiredundancy thresholds for CAFÂ´E.",
                "The NDCU for each system run is calculated automatically.",
                "User feedback was also simulated - relevance judgments for each system-returned passage (as determined by the nugget matching rules described in section 4.1.2) were 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figure 1: Performance of Indri across chunks Figure 2: Performance of CAFÂ´E across chunks used as user feedback in the adaptation of query profiles and user histories. 6.3 Results In Table 1, we show the NDCU scores of the two systems under various settings.",
                "These scores are averaged over the six tasks in the test set, and are calculated with two dampening factors (see section 4.2): Î³ = 0 and 0.1, to simulate no tolerance and small tolerance for redundancy, respectively.",
                "Using Î³ = 0 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information.",
                "Hence, the improvement obtained from enabling user feedback is smaller with Î³ = 0 than the improvement obtained from feedback with Î³ = 0.1.",
                "This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information.",
                "However, the user does not benefit from seeing such redundant passages, and is usually interested in other passages containing related information.",
                "It is informative to evaluate retrieval systems using our utility measure (with Î³ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.",
                "Sometimes, however, users might indeed be interested in seeing the same information from multiple sources, as an Table 1: NDCU Scores of Indri and CAFÂ´E for two dampening factors (Î³), and various settings (F: Feedback, N: Novelty Detection, A: Anti-Redundant Ranking) Indri CAFÂ´E Î³ Base +PRF Base +F +F+N +F+A +F+N+A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.35 0.36 0.36 indicator of its importance or reliability.",
                "In such a case, we can simply choose a higher value for Î³ which corresponds to a higher tolerance for redundancy, and hence let the system tune its parameters accordingly.",
                "Since documents were processed chunk by chunk, it would be interesting to see how the performance of systems improves over time.",
                "Figures 1 and 2 show the performance trends for both the systems across chunks.",
                "While the performance with and without feedback on the first few chunks is expected to be close, for subsequent chunks, the performance curve with feedback enabled rises above the one with the no-feedback setting.",
                "The performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks, making some queries easier than others in certain chunks.",
                "Moreover, since Indri uses pseudo-relevance feedback while CAFÂ´E uses feedback based on actual relevance judgments, the improvement in case of Indri is less dramatic than that of CAFÂ´E. 7.",
                "CONCLUDING REMARKS This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages.",
                "Our system, called CAFÂ´E, combines adaptive filtering, novelty detection and antiredundant passage ranking in a unified framework for utility optimization.",
                "We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty.",
                "Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off. 8.",
                "ACKNOWLEDGMENTS We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments.",
                "This work is supported in parts by the National Science Foundation (NSF) under grant IIS0434035, and the Defense Advanced Research Project Agency (DARPA) under contracts NBCHD030010 and W0550432.",
                "Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. 9.",
                "ADDITIONAL AUTHORS Jian Zhang (jianzhan@stat.purdue.edu)â , Jaime Carbonell (jgc@cs.cmu.edu)â  , Peter Brusilovsky (peterb+@pitt.edu)â¡ , Daqing He(dah44@pitt.edu)â¡ 10.",
                "REFERENCES [1] J. Allan.",
                "Incremental Relevance Feedback for Information Filtering.",
                "Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 270-278, 1996. [2] J. Allan, C. Wade, and A. Bolivar.",
                "Retrieval and Novelty Detection at the Sentence Level.",
                "Proceedings of the ACM SIGIR conference on research and development in information retrieval, 2003. [3] C. Buckley, G. Salton, and J. Allan.",
                "Automatic Retrieval with Locality Information using SMART.",
                "NIST special publication, (500207):59-72, 1993. [4] J. Callan.",
                "Learning While Filtering Documents.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 224-231, 1998. [5] J. Carbonell and J. Goldstein.",
                "The use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335-336, 1998. [6] E. Efthimiadis.",
                "Query Expansion.",
                "Annual Review of Information Science and Technology (ARIST), 31:p121-87, 1996. [7] J. Fiscus and G. Duddington.",
                "Topic Detection and Tracking Overview.",
                "Topic Detection and Tracking: Event-based Information Organization, pages 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.",
                "A Statistical Model for Multilingual Entity Detection and Tracking.",
                "NAACL/HLT, 2004. [9] K. JÂ¨arvelin and J. KekÂ¨alÂ¨ainen.",
                "Cumulated Gain-based Evaluation of IR Techniques.",
                "ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin and D. Demner-Fushman.",
                "Automatically Evaluating Answers to Definition Questions.",
                "Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), 2005. â Statistics Dept., Purdue University, West Lafayette, USA â  Language Technologies Inst., Carnegie Mellon University, Pittsburgh, USA â¡ School of Information Sciences, Univ. of Pittsburgh, Pittsburgh, USA [11] J. Lin and D. Demner-Fushman.",
                "Will Pyramids Built of nUggets Topple Over.",
                "Proceedings of HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.",
                "A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree.",
                "Proc. of ACL, 4:136-143, 2004. [13] G. Marton.",
                "Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments.",
                "HLT/NAACL, 2006. [14] E. Riloff.",
                "Automatically Constructing a Dictionary for Information Extraction Tasks.",
                "Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, 1993. [15] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9: Filtering track.",
                "The Ninth Text REtrieval Conference (TREC-9), pages 361-368. [16] R. Schapire, Y.",
                "Singer, and A. Singhal.",
                "Boosting and Rocchio Applied to Text Filtering.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle, and W. Croft.",
                "Indri: A Language Model-based Search Engine for Complex Queries.",
                "Proceedings of the International Conference on Intelligence Analysis, 2004. [18] The Linguistic Data Consortium. http://www.ldc.upenn.edu/. [19] E. Voorhees.",
                "Overview of the TREC 2003 Question Answering Track.",
                "Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), 2003. [20] Y. Yang and B. Kisiel.",
                "Margin-based Local Regression for Adaptive Filtering.",
                "Proceedings of the twelfth international conference on Information and knowledge management, pages 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of Adaptive Filtering Methods in a Cross-benchmark Evaluation.",
                "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 98-105, 2005. [22] C. Zhai, W. Cohen, and J. Lafferty.",
                "Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-17, 2003. [23] J. Zhang and Y. Yang.",
                "Robustness of Regularized Linear Classification Methods in Text Categorization.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 190-197, 2003. [24] Y. Zhang.",
                "Using Bayesian Priors to Combine Classifiers for Adaptive Filtering.",
                "Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 345-352, 2004. [25] Y. Zhang, J. Callan, and T. Minka.",
                "Novelty and Redundancy Detection in Adaptive Filtering.",
                "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002."
            ],
            "original_annotated_samples": [
                "Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu ABSTRACT This paper examines a new approach to information distillation over <br>temporally ordered document</br>s, and proposes a novel evaluation scheme for such a framework.",
                "Through the above example, we can see the main properties of our new framework for utility-based information distillation over <br>temporally ordered document</br>s."
            ],
            "translated_annotated_samples": [
                "La Universidad Carnegie Mellon en Pittsburgh, EE. UU. mrogati@cs.cmu.edu RESUMEN Este artÃ­culo examina un nuevo enfoque para la destilaciÃ³n de informaciÃ³n en <br>documentos ordenados temporalmente</br>, y propone un esquema de evaluaciÃ³n novedoso para dicho marco.",
                "A travÃ©s del ejemplo anterior, podemos ver las principales propiedades de nuestro nuevo marco de trabajo para la destilaciÃ³n de informaciÃ³n basada en utilidad sobre <br>documentos ordenados temporalmente</br>."
            ],
            "translated_text": "DestilaciÃ³n de informaciÃ³n basada en utilidad sobre documentos secuenciados temporalmente en el Instituto de TecnologÃ­as del Lenguaje de Yiming Yang. Universidad Carnegie Mellon, Pittsburgh, EE. UU. yiming@cs.cmu.edu Abhimanyu Lad Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. alad@cs.cmu.edu Instituto de TecnologÃ­as del Lenguaje Ni Lao. Universidad Carnegie Mellon, Pittsburgh, EE. UU. nlao@cs.cmu.edu Abhay Harpale Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. aharpale@cs.cmu.edu Bryan Kisiel Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. bkisiel@cs.cmu.edu MÃ³nica Rogati Instituto de TecnologÃ­as del Lenguaje. La Universidad Carnegie Mellon en Pittsburgh, EE. UU. mrogati@cs.cmu.edu RESUMEN Este artÃ­culo examina un nuevo enfoque para la destilaciÃ³n de informaciÃ³n en <br>documentos ordenados temporalmente</br>, y propone un esquema de evaluaciÃ³n novedoso para dicho marco. Combina las fortalezas y se extiende mÃ¡s allÃ¡ del filtrado adaptativo convencional, la detecciÃ³n de novedades y la clasificaciÃ³n de pasajes no redundantes con respecto a las necesidades de informaciÃ³n duraderas (tareas con mÃºltiples consultas). Nuestro enfoque respalda la retroalimentaciÃ³n detallada del usuario a travÃ©s del resaltado de fragmentos arbitrarios de texto, y aprovecha dicha informaciÃ³n para la optimizaciÃ³n de la utilidad en entornos adaptativos. Para nuestros experimentos, definimos tareas hipotÃ©ticas basadas en eventos de noticias en el corpus TDT4, con mÃºltiples consultas por tarea. Se generaron claves de respuestas (nuggets) para cada consulta y se utilizÃ³ un procedimiento semiautomÃ¡tico para adquirir reglas que permiten emparejar automÃ¡ticamente los nuggets con las respuestas del sistema. TambiÃ©n proponemos una extensiÃ³n de la mÃ©trica NDCG para evaluar la utilidad de los pasajes clasificados como una combinaciÃ³n de relevancia y novedad. Nuestros resultados muestran mejoras alentadoras en la utilidad utilizando el nuevo enfoque, en comparaciÃ³n con los sistemas base sin aprendizaje incremental o componentes de detecciÃ³n de novedades. CategorÃ­as y Descriptores de Asignaturas H.3.3 [BÃºsqueda y RecuperaciÃ³n de InformaciÃ³n]: Filtrado de informaciÃ³n, RetroalimentaciÃ³n de relevancia, Modelos de recuperaciÃ³n, Proceso de selecciÃ³n; I.5.2 TÃ©rminos Generales DiseÃ±o, MediciÃ³n, Rendimiento, ExperimentaciÃ³n. 1. INTRODUCCIÃN Seguir informaciÃ³n nueva y relevante de flujos de datos temporales para usuarios con necesidades duraderas ha sido un tema de investigaciÃ³n desafiante en la recuperaciÃ³n de informaciÃ³n. El filtrado adaptativo (AF) es una tarea de predicciÃ³n en lÃ­nea de la relevancia de cada nuevo documento con respecto a temas predefinidos. BasÃ¡ndose en la consulta inicial y algunos ejemplos positivos (si estÃ¡n disponibles), un sistema de AF mantiene un perfil para cada tema de interÃ©s, y lo actualiza constantemente segÃºn la retroalimentaciÃ³n del usuario. La naturaleza de aprendizaje incremental de los sistemas de AF los hace mÃ¡s poderosos que los motores de bÃºsqueda estÃ¡ndar que admiten recuperaciÃ³n ad-hoc (por ejemplo. Google y Yahoo) en tÃ©rminos de encontrar informaciÃ³n relevante con respecto a temas de interÃ©s duraderos, y mÃ¡s atractivo para los usuarios que estÃ¡n dispuestos a proporcionar retroalimentaciÃ³n para adaptar el sistema a sus necesidades de informaciÃ³n especÃ­ficas, sin tener que modificar sus consultas manualmente. Se han estudiado una variedad de algoritmos de aprendizaje supervisado (clasificadores de estilo Rocchio, modelos exponenciales-gaussianos, regresiÃ³n local y enfoques de regresiÃ³n logÃ­stica) para entornos adaptativos, examinados con retroalimentaciÃ³n de relevancia explÃ­cita e implÃ­cita, y evaluados con respecto a la optimizaciÃ³n de la utilidad en grandes colecciones de datos de referencia en los foros de TREC (Conferencias de RecuperaciÃ³n de Texto) y TDT (DetecciÃ³n y Seguimiento de Temas) [1, 4, 7, 15, 16, 20, 24, 23]. La regresiÃ³n logÃ­stica regularizada [21] se ha encontrado representativa para los enfoques de vanguardia, y altamente eficiente para adaptaciones frecuentes del modelo en grandes colecciones de documentos como el corpus TREC-10 (mÃ¡s de 800,000 documentos y 84 temas). A pesar de los logros sustanciales en la investigaciÃ³n reciente de filtrado adaptativo, aÃºn quedan problemas significativos sin resolver sobre cÃ³mo aprovechar de manera efectiva y eficiente la retroalimentaciÃ³n del usuario. EspecÃ­ficamente, los siguientes problemas pueden limitar seriamente la verdadera utilidad de los sistemas de AF en aplicaciones del mundo real: 1. El usuario tiene un papel bastante pasivo en la configuraciÃ³n convencional de filtrado adaptativo: solo reacciona ante el sistema cuando este toma una decisiÃ³n afirmativa sobre un documento, confirmando o rechazando esa decisiÃ³n. Una alternativa mÃ¡s activa serÃ­a permitir al usuario emitir mÃºltiples consultas sobre un tema, revisar una lista clasificada de documentos candidatos (o pasajes) por consulta, y proporcionar retroalimentaciÃ³n sobre la lista clasificada, refinando asÃ­ su necesidad de informaciÃ³n y solicitando listas clasificadas actualizadas. La Ãºltima forma de interacciÃ³n del usuario ha sido altamente efectiva en la recuperaciÃ³n estÃ¡ndar para consultas ad-hoc. CÃ³mo implementar una estrategia para necesidades de informaciÃ³n duraderas en entornos de FA es una pregunta abierta para la investigaciÃ³n. 2. La unidad para recibir un juicio de relevancia (sÃ­ o no) estÃ¡ restringida al nivel del documento en AF convencional. Sin embargo, un usuario real puede estar dispuesto a proporcionar comentarios mÃ¡s informativos y detallados resaltando algunas partes de texto en un documento recuperado como relevantes, en lugar de etiquetar todo el documento como relevante. Aprovechar de manera efectiva este tipo de retroalimentaciÃ³n detallada podrÃ­a mejorar sustancialmente la calidad de un sistema de AF. Para esto, necesitamos habilitar el aprendizaje supervisado a partir de fragmentos de texto etiquetados de longitud arbitraria en lugar de solo permitir documentos etiquetados. 3. Los documentos seleccionados por el sistema suelen ser altamente redundantes. Un evento importante de noticias, por ejemplo, serÃ­a reportado por mÃºltiples fuentes repetidamente durante un tiempo, haciendo que la mayor parte del contenido informativo en esos artÃ­culos sea redundante entre sÃ­. Un sistema de AF convencional seleccionarÃ­a todas estas historias de noticias redundantes para la retroalimentaciÃ³n del usuario, desperdiciando el tiempo de los usuarios mientras ofrece poco beneficio. Claramente, las tÃ©cnicas de detecciÃ³n de novedades pueden ayudar en principio [25, 2, 22] a mejorar la utilidad de los sistemas AF. Sin embargo, la efectividad de tales tÃ©cnicas a nivel de pasaje para detectar novedad con respecto a la retroalimentaciÃ³n de los usuarios a nivel detallado y para detectar redundancia en listas clasificadas aÃºn debe ser evaluada utilizando una medida de utilidad que imite las necesidades de un usuario real. Para abordar las limitaciones actuales de los sistemas de AF mencionadas anteriormente, proponemos y examinamos un nuevo enfoque en este artÃ­culo, que combina las fortalezas del AF convencional (aprendizaje incremental de modelos de temas), recuperaciÃ³n de pasajes multipase para consultas de larga duraciÃ³n condicionadas al tema, y detecciÃ³n de novedades para eliminar la redundancia de las interacciones del usuario con el sistema. Llamamos al nuevo proceso destilaciÃ³n de informaciÃ³n basada en utilidad. Ten en cuenta que los corpus de referencia convencionales para evaluaciones de AF, que tienen juicios de relevancia a nivel de documento y no definen tareas con mÃºltiples consultas, son insuficientes para evaluar el nuevo enfoque. Por lo tanto, ampliamos un corpus de referencia, la colecciÃ³n TDT4 de noticias y transmisiones de televisiÃ³n, con definiciones de tareas, mÃºltiples consultas por tarea y claves de respuestas por consulta. Hemos realizado nuestros experimentos en este corpus extendido TDT4 y hemos puesto a disposiciÃ³n del pÃºblico los datos generados adicionalmente para futuras evaluaciones comparativas. Para evaluar automÃ¡ticamente los fragmentos de texto arbitrarios devueltos por el sistema utilizando nuestras claves de respuestas, desarrollamos un esquema de evaluaciÃ³n con un procedimiento semiautomÃ¡tico para 1 URL: http://nyc.lti.cs.cmu.edu/downloads adquiriendo reglas que pueden comparar fragmentos con las respuestas del sistema. AdemÃ¡s, proponemos una extensiÃ³n de NDCG (Normalized Discounted Cumulated Gain) [9] para evaluar la utilidad de los pasajes clasificados como una funciÃ³n tanto de la relevancia como de la novedad. El resto de este documento estÃ¡ organizado de la siguiente manera. La secciÃ³n 2 describe el proceso de destilaciÃ³n de informaciÃ³n con un ejemplo concreto. La secciÃ³n 3 describe los nÃºcleos tÃ©cnicos de nuestro sistema llamado CAFÂ´E - CMU Adaptive Filtering Engine. La secciÃ³n 4 discute problemas con respecto a la metodologÃ­a de evaluaciÃ³n y propone un nuevo esquema. La secciÃ³n 5 describe el corpus extendido TDT4. La secciÃ³n 6 presenta nuestros experimentos y resultados. La secciÃ³n 7 concluye el estudio y ofrece perspectivas futuras. 2. Una TAREA DE EJEMPLO Considera un evento noticioso: la fuga de siete convictos de una prisiÃ³n de Texas en diciembre de 2000 y su captura un mes despuÃ©s. Suponiendo que un usuario estuviera interesado en este evento desde sus primeras etapas, la necesidad de informaciÃ³n podrÃ­a ser: Encontrar informaciÃ³n sobre la fuga de convictos de una prisiÃ³n de Texas, y la informaciÃ³n relacionada con su recaptura. Las preguntas de nivel inferior asociadas podrÃ­an ser: 1. Â¿CuÃ¡ntos prisioneros escaparon? 2. Â¿DÃ³nde y cuÃ¡ndo fueron avistados? 3. Â¿QuiÃ©nes son sus contactos conocidos dentro y fuera de la prisiÃ³n? 4. Â¿CÃ³mo estÃ¡n armados? 5. Â¿Tienen algÃºn vehÃ­culo? 6. Â¿QuÃ© pasos se han tomado hasta ahora? Llamamos a esta necesidad de informaciÃ³n una tarea, y a las preguntas asociadas como las consultas en esta tarea. Un sistema de destilaciÃ³n debe monitorear los documentos entrantes, procesarlos por fragmentos en un orden temporal, seleccionar pasajes potencialmente relevantes y novedosos de cada fragmento con respecto a cada consulta, y presentar una lista clasificada de pasajes al usuario. La clasificaciÃ³n de pasajes aquÃ­ se basa en cuÃ¡n relevante es un pasaje con respecto a la consulta actual, cuÃ¡n novedoso es con respecto al historial actual del usuario (de sus interacciones con el sistema) y cuÃ¡n redundante es en comparaciÃ³n con otros pasajes con un rango mÃ¡s alto en la lista. Cuando se le presenta una lista de pasajes, el usuario puede proporcionar retroalimentaciÃ³n resaltando tramos de texto arbitrarios que considere relevantes. Estos fragmentos de texto se toman como ejemplos positivos en la adaptaciÃ³n del perfil de consulta, y tambiÃ©n se aÃ±aden al historial de los usuarios. Los pasajes no marcados por el usuario se consideran ejemplos negativos. Tan pronto como se actualiza el perfil de consulta, el sistema vuelve a realizar una bÃºsqueda y devuelve otra lista clasificada de pasajes donde los pasajes previamente vistos son eliminados o clasificados como bajos, segÃºn la preferencia del usuario. Por ejemplo, si el usuario resalta ...los funcionarios han ofrecido una recompensa de $100,000 por su captura... como respuesta relevante a la consulta Â¿QuÃ© pasos se han tomado hasta ahora?, entonces la pieza resaltada se utiliza como un ejemplo de entrenamiento positivo adicional en la adaptaciÃ³n del perfil de la consulta. Este comentario tambiÃ©n se aÃ±ade al historial del usuario como un ejemplo visto, para que en el futuro, el sistema no coloque otro pasaje mencionando una recompensa de $100,000 en la parte superior de la lista clasificada. Sin embargo, un artÃ­culo que mencione que los funcionarios han duplicado el dinero de recompensa a $200,000 podrÃ­a clasificarse alto ya que es relevante tanto para el perfil de consulta (actualizado) como novedoso con respecto a la historia del usuario (actualizada). El usuario puede modificar las consultas originales o agregar una nueva consulta durante el proceso; los perfiles de consulta se cambiarÃ¡n en consecuencia. Claramente, la detecciÃ³n de novedades es muy importante para la utilidad de dicho sistema debido a la bÃºsqueda iterativa. Sin detecciÃ³n de novedad, los antiguos pasajes relevantes se mostrarÃ­an al usuario repetidamente en cada lista clasificada. A travÃ©s del ejemplo anterior, podemos ver las principales propiedades de nuestro nuevo marco de trabajo para la destilaciÃ³n de informaciÃ³n basada en utilidad sobre <br>documentos ordenados temporalmente</br>. Nuestro marco de trabajo combina y amplÃ­a el poder del filtrado adaptativo (AF), la recuperaciÃ³n ad-hoc (IR) y la detecciÃ³n de novedades (ND). En comparaciÃ³n con la IR estÃ¡ndar, nuestro enfoque tiene la capacidad de aprender de forma incremental las necesidades de informaciÃ³n a largo plazo y modelar una secuencia de consultas dentro de una tarea. En comparaciÃ³n con el AF convencional, permite un papel mÃ¡s activo del usuario en refinar sus necesidades de informaciÃ³n y solicitar nuevos resultados al permitir comentarios sobre relevancia y novedad mediante el resaltado de fragmentos arbitrarios de texto en los pasajes devueltos por el sistema. Comparado con trabajos anteriores, esta es la primera evaluaciÃ³n de detecciÃ³n de novedad integrada con filtrado adaptativo para consultas secuenciadas que permite retroalimentaciÃ³n flexible del usuario sobre pasajes clasificados. La combinaciÃ³n de AF, IR y ND con las nuevas extensiones plantea una importante pregunta de investigaciÃ³n sobre la metodologÃ­a de evaluaciÃ³n: Â¿cÃ³mo podemos medir la utilidad de un sistema de destilaciÃ³n de informaciÃ³n como este? Las mÃ©tricas existentes en IR estÃ¡ndar, AF y ND son insuficientes, y se deben explorar nuevas soluciones, como discutiremos en la SecciÃ³n 4, despuÃ©s de describir los nÃºcleos tÃ©cnicos de nuestro sistema en la prÃ³xima secciÃ³n. 3. NÃCLEOS TÃCNICOS Los componentes principales de CAFÃ son: 1) AF para el aprendizaje incremental de perfiles de consulta, 2) IR para estimar la relevancia de pasajes con respecto a los perfiles de consulta, 3) ND para evaluar la novedad de pasajes con respecto al historial de usuarios, y 4) un componente anti-redundancia para eliminar la redundancia de las listas clasificadas. 3.1 Componente de Filtrado Adaptativo Utilizamos un algoritmo de vanguardia en el campo, el mÃ©todo de regresiÃ³n logÃ­stica regularizada que obtuvo los mejores resultados en varios corpus de evaluaciÃ³n de referencia para AF [21]. La regresiÃ³n logÃ­stica (LR) es un algoritmo de aprendizaje supervisado para clasificaciÃ³n estadÃ­stica. BasÃ¡ndose en un conjunto de entrenamiento de instancias etiquetadas, aprende un modelo de clase que luego puede ser utilizado para predecir las etiquetas de instancias no vistas. Su rendimiento, asÃ­ como su eficiencia en tÃ©rminos de tiempo de entrenamiento, lo convierten en un buen candidato cuando se requieren actualizaciones frecuentes del modelo de clase, como es el caso en el filtrado adaptativo, donde el sistema debe aprender de cada nueva retroalimentaciÃ³n proporcionada por el usuario. (Consulte [21] y [23] para complejidad computacional y problemas de implementaciÃ³n). En el filtrado adaptativo, cada consulta se considera como una clase y la probabilidad de que un pasaje pertenezca a esta clase corresponde al grado de relevancia del pasaje con respecto a la consulta. Para entrenar el modelo, utilizamos la propia consulta como ejemplo inicial de entrenamiento positivo de la clase, y las piezas de texto resaltadas por el usuario (marcadas como Relevante o No relevante) durante la retroalimentaciÃ³n como ejemplos de entrenamiento adicionales. Para abordar el problema de inicio en frÃ­o en la etapa inicial antes de obtener cualquier retroalimentaciÃ³n del usuario, el sistema utiliza una pequeÃ±a muestra de un corpus retrospectivo como ejemplos negativos iniciales en el conjunto de entrenamiento. Los detalles de cÃ³mo utilizar la regresiÃ³n logÃ­stica para el filtrado adaptativo (asignando diferentes pesos a las instancias de entrenamiento positivas y negativas, y regularizando la funciÃ³n objetivo para prevenir el sobreajuste en los datos de entrenamiento) se presentan en [21]. El modelo de clase wâ aprendido por RegresiÃ³n LogÃ­stica, o el perfil de consulta, es un vector cuyas dimensiones son tÃ©rminos individuales y cuyos elementos son los coeficientes de regresiÃ³n, indicando cuÃ¡n influyente es cada tÃ©rmino en el perfil de consulta. El perfil de la consulta se actualiza cada vez que se recibe una nueva pieza de retroalimentaciÃ³n del usuario. Un peso decaÃ­do temporalmente se puede aplicar a cada ejemplo de entrenamiento, como una opciÃ³n, para enfatizar la retroalimentaciÃ³n mÃ¡s reciente del usuario. 3.2 Componente de RecuperaciÃ³n de Pasajes Utilizamos tÃ©cnicas estÃ¡ndar de IR en esta parte de nuestro sistema. Los documentos entrantes se procesan por bloques, donde cada bloque puede definirse como un intervalo de tiempo fijo o como un nÃºmero fijo de documentos, segÃºn lo prefiera el usuario. Para cada documento entrante, se actualizan las estadÃ­sticas del corpus como el IDF (Frecuencia Inversa de Documento) de cada tÃ©rmino. Utilizamos un identificador y rastreador de entidades con tecnologÃ­a de punta [8, 12] para identificar nombres de personas y lugares, y fusionarlos con entidades con referencia cruzada vistas en el pasado. Luego, los documentos se dividen en pasajes, que pueden ser un documento completo, un pÃ¡rrafo, una oraciÃ³n, o cualquier otro fragmento continuo de texto, segÃºn se prefiera. Cada pasaje se representa utilizando un vector de pesos TF-IDF (Frecuencia de TÃ©rmino - Frecuencia Inversa de Documento), donde el tÃ©rmino puede ser una palabra o una entidad nombrada. Dado un perfil de consulta, es decir, la soluciÃ³n de regresiÃ³n logÃ­stica wâ como se describe en la SecciÃ³n 3.1, el sistema calcula la probabilidad posterior de relevancia para cada pasaje x como fRL(x) â¡ P(y = 1|x, wâ ) = 1 (1 + eâwâÂ·x) (1). Los pasajes se ordenan segÃºn sus puntajes de relevancia, y aquellos con puntajes por encima de un umbral (ajustado en un conjunto de entrenamiento) conforman la lista de relevancia que se pasa al paso de detecciÃ³n de novedad. 3.3 Componente de DetecciÃ³n de Novedad CAFÂ´E mantiene un historial de usuario H(t), que contiene todos los fragmentos de texto hi que el usuario resaltÃ³ (como retroalimentaciÃ³n) durante sus interacciones pasadas con el sistema, hasta el tiempo actual t. Denotando el historial como H(t) = n h1, h2, ..., ht o , (2), el puntaje de novedad de un nuevo pasaje candidato x se calcula como: fND(x) = 1 â max iâ1..t {cos(x, hi)} (3), donde tanto el pasaje candidato x como los fragmentos de texto resaltados hi se representan como vectores TF-IDF. El puntaje de novedad de cada pasaje se compara con un umbral predefinido (ajustado tambiÃ©n en un conjunto de entrenamiento), y cualquier pasaje con un puntaje por debajo de este umbral se elimina de la lista de relevancia. Componente de clasificaciÃ³n anti-redundante Aunque el componente de detecciÃ³n de novedad garantiza que solo la informaciÃ³n novedosa (no vista previamente) permanezca en la lista de relevancia, esta lista aÃºn podrÃ­a contener la misma informaciÃ³n novedosa en mÃºltiples posiciones en la lista clasificada. Supongamos, por ejemplo, que el usuario ya ha leÃ­do sobre una recompensa de $100,000 por informaciÃ³n sobre los convictos fugados. Una nueva noticia de que el premio ha sido aumentado a $200,000 es novedosa ya que el usuario aÃºn no la ha leÃ­do. Sin embargo, mÃºltiples fuentes de noticias podrÃ­an informar sobre esta noticia y podrÃ­amos terminar mostrando artÃ­culos (redundantes) de todas estas fuentes en una lista clasificada. Por lo tanto, una lista clasificada tambiÃ©n debe ser no redundante con respecto a su propio contenido. Utilizamos una versiÃ³n simplificada del mÃ©todo de MÃ¡xima Relevancia Marginal [5], originalmente desarrollado para combinar relevancia y novedad en la recuperaciÃ³n de texto y resÃºmenes. Nuestro procedimiento comienza con la lista actual de pasajes ordenados por relevancia (secciÃ³n 3.2), filtrados por el componente de DetecciÃ³n de Novedades (secciÃ³n 3.3), y genera una nueva lista no redundante de la siguiente manera: 1. Toma el pasaje superior en la lista actual como el primero en la nueva lista. 2. Agrega el siguiente pasaje x en la lista actual a la nueva lista solo si fAR(x) > t donde fAR(x) = 1 â max piâLnew {cos(x, pi)} y Lnew es el conjunto de pasajes ya seleccionados en la nueva lista. Repetir el paso 2 hasta que se hayan examinado todos los pasajes en la lista actual. DespuÃ©s de aplicar el algoritmo mencionado anteriormente, cada pasaje en la nueva lista es lo suficientemente diferente a los demÃ¡s, favoreciendo asÃ­ la diversidad en lugar de la redundancia en la nueva lista clasificada. El umbral anti-redundancia t se ajusta en un conjunto de entrenamiento. 4. METODOLOGÃA DE EVALUACIÃN El enfoque que propusimos anteriormente para la destilaciÃ³n de informaciÃ³n plantea importantes cuestiones sobre la metodologÃ­a de evaluaciÃ³n. En primer lugar, dado que nuestro marco de trabajo permite que la salida sean fragmentos en diferentes niveles de granularidad (por ejemplo, ventanas de k oraciones donde k puede variar) en lugar de una longitud fija, no es posible tener juicios de relevancia preanotados en todos esos niveles de granularidad. En segundo lugar, dado que deseamos medir la utilidad de la salida del sistema como una combinaciÃ³n de relevancia y novedad, las medidas tradicionales basadas Ãºnicamente en la relevancia deben ser reemplazadas por medidas que penalicen la repeticiÃ³n de la misma informaciÃ³n en la salida del sistema a lo largo del tiempo. En tercer lugar, dado que la salida del sistema son listas clasificadas, debemos recompensar a aquellos sistemas que presenten informaciÃ³n Ãºtil (tanto relevante como no vista previamente) utilizando listas clasificadas mÃ¡s cortas, y penalizar a aquellos que presenten la misma informaciÃ³n utilizando listas clasificadas mÃ¡s largas. Ninguna de las medidas existentes en la recuperaciÃ³n ad-hoc, filtrado adaptativo, detecciÃ³n de novedades u otras Ã¡reas relacionadas (resumen de texto y respuesta a preguntas) tiene propiedades deseables en los tres aspectos. Por lo tanto, debemos desarrollar una nueva metodologÃ­a de evaluaciÃ³n. 4.1 Claves de Respuesta Para permitir la evaluaciÃ³n de un sistema cuya salida consiste en pasajes de longitud arbitraria, tomamos prestado el concepto de claves de respuesta de la comunidad de Preguntas y Respuestas (QA), donde los sistemas pueden devolver fragmentos de texto arbitrarios como respuestas. Las claves de respuestas definen lo que debe estar presente en la respuesta de un sistema para recibir crÃ©dito, y estÃ¡n compuestas por una colecciÃ³n de \"nuggets\" de informaciÃ³n, es decir, unidades de datos sobre las cuales los evaluadores humanos pueden tomar decisiones binarias sobre si una respuesta del sistema las contiene o no. La definiciÃ³n de claves de respuestas y la toma de decisiones binarias asociadas son tareas conceptuales que requieren un mapeo semÃ¡ntico [19], ya que los pasajes devueltos por el sistema pueden contener la misma informaciÃ³n expresada de muchas formas diferentes. Por lo tanto, las evaluaciones de QA han dependido de evaluadores humanos para la correspondencia entre diversas expresiones, lo que hace que el proceso sea costoso, consume mucho tiempo y no sea escalable para grandes colecciones de consultas y documentos, y evaluaciones extensas de sistemas con diversos ajustes de parÃ¡metros. 4.1.1 AutomatizaciÃ³n de la evaluaciÃ³n basada en claves de respuestas Los mÃ©todos de evaluaciÃ³n automÃ¡tica permitirÃ­an una construcciÃ³n y ajuste mÃ¡s rÃ¡pidos del sistema, asÃ­ como proporcionar una forma objetiva y asequible de comparar diversos sistemas. Recientemente, se han propuesto mÃ©todos basados mÃ¡s o menos en la idea de las co-ocurrencias de n-gramos. Pourpre [10] asigna una puntuaciÃ³n de recuerdo fraccional a una respuesta del sistema basada en su superposiciÃ³n de unigramas con una descripciÃ³n de pepitas dada. Por ejemplo, una respuesta del sistema A B C tiene una recuperaciÃ³n de 3/4 con respecto a un fragmento con la descripciÃ³n A B C D. Sin embargo, este enfoque es injusto para los sistemas que presentan la misma informaciÃ³n pero utilizando palabras distintas a A, B, C y D. Otro problema abierto es cÃ³mo ponderar las palabras individuales al medir la cercanÃ­a de una coincidencia. Por ejemplo, considera la pregunta Â¿CuÃ¡ntos prisioneros escaparon? En el fragmento Siete prisioneros escaparon de una prisiÃ³n de Texas, no hay indicaciÃ³n de que siete sea la palabra clave y que debe coincidir para obtener algÃºn crÃ©dito de relevancia. El uso de los valores de IDF no ayuda, ya que la palabra \"seven\" generalmente no tendrÃ¡ un IDF mÃ¡s alto que palabras como \"texas\" y \"prison\". AdemÃ¡s, redefinir el fragmento como simplemente siete no resuelve el problema, ya que ahora podrÃ­a coincidir errÃ³neamente con cualquier menciÃ³n de siete fuera de contexto. Nuggeteer [13] trabaja sobre principios similares pero toma decisiones binarias sobre si hay un fragmento en una respuesta de sistema dada ajustando un umbral. Sin embargo, tambiÃ©n estÃ¡ plagado de relevancia espuria ya que no todas las palabras contenidas en la descripciÃ³n de la pepita (o respuestas correctas conocidas) son centrales para la pepita. 4.1.2 Reglas de Coincidencia de Pepitas Proponemos un mÃ©todo automÃ¡tico confiable para determinar si un fragmento de texto contiene una pepita dada, basado en reglas de coincidencia de pepitas, que se generan utilizando un procedimiento semiautomÃ¡tico explicado a continuaciÃ³n. Estas reglas son bÃ¡sicamente consultas booleanas que solo coincidirÃ¡n con fragmentos que contengan la pepita. Por ejemplo, una regla candidata para emparejar respuestas a Â¿CuÃ¡ntos prisioneros escaparon? es (Texas Y siete Y escapar Y (convictos O prisioneros)), posiblemente con otros sinÃ³nimos y variantes en la regla. Para un corpus de artÃ­culos de noticias, que suelen seguir una prosa formal tÃ­pica, es bastante fÃ¡cil escribir reglas simples para coincidir con respuestas esperadas utilizando un enfoque de arranque, como se describe a continuaciÃ³n. Proponemos un enfoque de dos etapas, inspirado en Autoslog [14], que combina la capacidad de los humanos para identificar expresiones semÃ¡nticamente equivalentes y la capacidad del sistema para recopilar evidencia estadÃ­stica de un corpus de documentos anotados por humanos. En la primera etapa, los sujetos humanos anotaron (utilizando una herramienta de resaltado) porciones de documentos relevantes que contenÃ­an respuestas a cada fragmento 2. En la segunda etapa, los sujetos utilizaron nuestra herramienta de generaciÃ³n de reglas para crear reglas que coincidieran con las anotaciones de cada fragmento. La herramienta permite a los usuarios ingresar una regla booleana como una disyunciÃ³n de conjunciones (por ejemplo, ((a Y b) O (a Y c Y d) O (e))). Dada una regla candidata, nuestra herramienta la utiliza como una consulta booleana sobre el conjunto completo de documentos relevantes y calcula su recall y precisiÃ³n con respecto a las anotaciones que se espera que coincida. Por lo tanto, los sujetos pueden comenzar con una regla simple y refinarla iterativamente hasta que estÃ©n satisfechos con su precisiÃ³n y recuperaciÃ³n. Observamos que era muy fÃ¡cil para los humanos mejorar la precisiÃ³n de una regla ajustando sus conjunciones existentes (agregando mÃ¡s Y) y mejorar la recuperaciÃ³n agregando mÃ¡s conjunciones a la disyunciÃ³n (agregando mÃ¡s O). Como ejemplo, intentemos crear una regla para el acertijo que dice que siete prisioneros escaparon de la prisiÃ³n de Texas. Empezamos con una regla simple - (siete). Cuando introducimos esto en la herramienta de generaciÃ³n de reglas, nos damos cuenta de que esta regla coincide con muchas ocurrencias espurias de siete (por ejemplo, ...siete estados...) y, por lo tanto, obtiene una puntuaciÃ³n baja de precisiÃ³n. Podemos calificar aÃºn mÃ¡s nuestra regla - Texas Y siete Y convictos. A continuaciÃ³n, al revisar las anotaciones omitidas, nos damos cuenta de que algunos artÃ­culos de noticias mencionaban ...siete prisioneros escaparon.... Luego reemplazamos convictos por la disyunciÃ³n (convictos O prisioneros). Continuamos ajustando la regla de esta manera hasta lograr un nivel de recordaciÃ³n y precisiÃ³n suficientemente alto, es decir, que los (pocos) errores y alarmas falsas se puedan ignorar de forma segura. AsÃ­ podemos crear reglas de coincidencia de pepitas que capturen de manera sucinta varias formas de expresar una pepita, evitando al mismo tiempo emparejar respuestas incorrectas (o fuera de contexto). La participaciÃ³n humana en el proceso de creaciÃ³n de reglas garantiza reglas genÃ©ricas de alta calidad que luego se pueden utilizar para evaluar de manera confiable las respuestas del sistema arbitrario. 4.2 EvaluaciÃ³n de la Utilidad de una Secuencia de Listas Clasificadas La utilidad de un sistema de recuperaciÃ³n se puede definir como la diferencia entre cuÃ¡nta informaciÃ³n Ãºtil obtuvo el usuario y cuÃ¡nto tiempo y energÃ­a perdiÃ³. Calculamos esta utilidad a partir de las utilidades de los pasajes individuales de la siguiente manera. DespuÃ©s de leer cada pasaje devuelto por el sistema, el usuario obtiene cierta ganancia dependiendo de la presencia de informaciÃ³n relevante y novedosa, y incurre en una pÃ©rdida en tÃ©rminos del tiempo y la energÃ­a invertidos en revisar el pasaje. Sin embargo, la probabilidad de que el usuario realmente lea un pasaje depende de su posiciÃ³n en la lista clasificada. Por lo tanto, para una consulta q, la utilidad esperada 2 LDC [18] ya proporciona juicios de relevancia para 100 temas en el corpus TDT4. AdemÃ¡s, nos aseguramos de que estos juicios sean exhaustivos en todo el corpus utilizando la agrupaciÃ³n. El valor de un pasaje pi en el rango i se puede definir como U(pi, q) = P(i) â (Ganancia(pi, q) - PÃ©rdida(pi, q)) (4) donde P(i) es la probabilidad de que el usuario pase por un pasaje en el rango i. La utilidad esperada para una lista clasificada completa de longitud n se puede calcular simplemente sumando la utilidad esperada de cada pasaje: U(q) = Î£ i=1 P(i) â (Ganancia(pi, q) â PÃ©rdida(pi, q)) (5) NÃ³tese que si ignoramos el tÃ©rmino de pÃ©rdida y definimos P(i) como P(i) â 1/ logb(b + i â 1) (6) entonces obtenemos la mÃ©trica recientemente popularizada llamada Ganancia Acumulada Descontada (DCG) [9], donde Ganancia(pi, q) se define como la relevancia graduada del pasaje pi. Sin embargo, sin el tÃ©rmino de pÃ©rdida, DCG es una mÃ©trica puramente orientada al recuerdo y no es adecuada para un entorno de filtrado adaptativo, donde la utilidad del sistema depende en parte de su capacidad para limitar el nÃºmero de elementos mostrados al usuario. Aunque P(i) podrÃ­a definirse en base a estudios empÃ­ricos del comportamiento del usuario, por simplicidad, utilizamos P(i) tal como se define exactamente en la ecuaciÃ³n 6. La ganancia G(pi, q) del pasaje pi con respecto a la consulta q es una funciÃ³n de - 1) el nÃºmero de pepitas relevantes presentes en pi, y 2) la novedad de cada una de estas pepitas. Combinamos estos dos factores de la siguiente manera. Para cada pepita Nj, asignamos un peso inicial wj, y tambiÃ©n mantenemos un conteo nj del nÃºmero de veces que esta pepita ha sido vista por el usuario en el pasado. La ganancia derivada de cada ocurrencia posterior del mismo hallazgo se asume que se reduce por un factor de amortiguamiento Î³. Por lo tanto, G(pi, q) se define como G(pi, q) = X Nj âC(pi,q) wj â Î³nj (7) donde C(pi, q) es el conjunto de todas las pepitas que aparecen en el pasaje pi y tambiÃ©n pertenecen a la clave de respuesta de la consulta q. Los pesos iniciales wj se establecen todos en 1.0 en nuestros experimentos, pero tambiÃ©n pueden establecerse segÃºn un enfoque de pirÃ¡mide [11]. La elecciÃ³n del factor de amortiguaciÃ³n Î³ determina la tolerancia del usuario a la redundancia. Cuando Î³ = 0, un pepita solo recibirÃ¡ crÃ©dito por su primera ocurrencia, es decir, cuando nj sea cero. Para 0 < Î³ < 1, un nugget recibe menos crÃ©dito por cada ocurrencia sucesiva. Cuando Î³ = 1, no se produce amortiguamiento y las repeticiones de un valor reciben el mismo crÃ©dito. Se debe tener en cuenta que los recuentos de ocurrencia de las pepitas se conservan entre la evaluaciÃ³n de las listas clasificadas sucesivas devueltas por el sistema, ya que se espera que los usuarios recuerden lo que el sistema les mostrÃ³ en el pasado. Definimos la pÃ©rdida L(pi, q) como un costo constante c (usamos 0.1) incurrido al leer un pasaje devuelto por el sistema. Por lo tanto, nuestra mÃ©trica puede ser reescrita como U(q) = nX i=1 Ganancia(pi, q) logb(b + i â 1) â L(n) (8) donde L(n) es la pÃ©rdida asociada con una lista clasificada de longitud n: L(n) = c Â· nX i=1 1 logb(b + i â 1) (9) 3 NÃ³tese que 00 = 1 Debido a la similitud con la Ganancia Acumulada Descontada (DCG), llamamos a nuestra mÃ©trica Utilidad Acumulada Descontada (DCU). El puntaje de DCU obtenido por el sistema se convierte en un puntaje de DCU normalizado (NDCU) dividiÃ©ndolo por el puntaje de DCU de la lista ideal clasificada, que se crea ordenando los pasajes por sus puntajes de utilidad decrecientes U(pi, q) y deteniÃ©ndose cuando U(pi, q) â¤ 0, es decir, cuando la ganancia es menor o igual al costo de leer el pasaje. El conjunto de datos TDT4 fue el corpus de referencia utilizado en las evaluaciones TDT2002 y TDT2003. El corpus consiste en mÃ¡s de 90,000 artÃ­culos de noticias de mÃºltiples fuentes (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) publicados entre octubre de 2000 y enero de 2001, en los idiomas Ã¡rabe, inglÃ©s y mandarÃ­n. Las versiones de los artÃ­culos en otros idiomas reconocidas por voz y traducidas por mÃ¡quina tambiÃ©n fueron proporcionadas. LDC [18] ha anotado el corpus con 100 temas, que corresponden a varios eventos de noticias en este perÃ­odo de tiempo. De entre estos, seleccionamos un subconjunto de 12 eventos accionables y definimos tareas correspondientes para ellos. Para cada tarea, definimos manualmente un perfil que consiste en un conjunto inicial de (5 a 10) consultas, una descripciÃ³n de texto libre del historial del usuario, es decir, lo que el usuario ya sabe sobre el evento, y una lista de documentos conocidos relevantes y no relevantes (si estÃ¡n disponibles) como ejemplos de entrenamiento. Para cada consulta, generamos claves de respuestas y reglas de coincidencia de pepitas correspondientes utilizando el procedimiento descrito en la secciÃ³n 4.1.2, y produjimos un total de 120 consultas, con un promedio de 7 pepitas por consulta. EXPERIMENTOS Y RESULTADOS 6.1 Baselines Utilizamos Indri [17], un motor de bÃºsqueda basado en modelos de lenguaje popular, como referencia para comparar con CAFÃ. Indri admite la funcionalidad estÃ¡ndar de los motores de bÃºsqueda, incluido el feedback de pseudo-relevancia (PRF) [3, 6], y es representativo de un sistema de recuperaciÃ³n basado en consultas tÃ­pico. Indri no admite ningÃºn tipo de detecciÃ³n de novedades. Comparamos Indri con PRF activado y desactivado, frente a CAFÃ con retroalimentaciÃ³n del usuario, detecciÃ³n de novedades y clasificaciÃ³n antirredundante activados y desactivados. 6.2 ConfiguraciÃ³n Experimental Dividimos el corpus TDT4 que abarca 4 meses en 10 fragmentos, cada uno definido como un perÃ­odo de 12 dÃ­as consecutivos. En cualquier momento dado del proceso de destilaciÃ³n, cada sistema accedÃ­a a los datos pasados hasta el punto actual, y devolvÃ­a una lista clasificada de hasta 50 pasajes por consulta. Las 12 tareas definidas en el corpus se dividieron en un conjunto de entrenamiento y prueba con 6 tareas cada uno. Cada sistema pudo utilizar el conjunto de entrenamiento para ajustar sus parÃ¡metros con el fin de optimizar NDCU (ecuaciÃ³n 8), incluyendo el umbral de relevancia tanto para Indri como para CAFÂ´E, y los umbrales de novedad y antirredundancia para CAFÂ´E. La NDCU para cada ejecuciÃ³n del sistema se calcula automÃ¡ticamente. TambiÃ©n se simulÃ³ la retroalimentaciÃ³n de los usuarios: los juicios de relevancia para cada pasaje devuelto por el sistema (segÃºn las reglas de coincidencia de pepitas descritas en la secciÃ³n 4.1.2) fueron utilizados como retroalimentaciÃ³n de los usuarios en la adaptaciÃ³n de perfiles de consulta e historiales de usuario. En la Tabla 1, mostramos los puntajes NDCU de los dos sistemas en diferentes configuraciones. Estas puntuaciones se promedian en las seis tareas del conjunto de pruebas, y se calculan con dos factores de amortiguaciÃ³n (ver secciÃ³n 4.2): Î³ = 0 y 0.1, para simular ninguna tolerancia y poca tolerancia para la redundancia, respectivamente. Usar Î³ = 0 crea una mÃ©trica mucho mÃ¡s estricta ya que no otorga ningÃºn crÃ©dito a un pasaje que contenga informaciÃ³n relevante pero redundante. Por lo tanto, la mejora obtenida al habilitar la retroalimentaciÃ³n del usuario es menor con Î³ = 0 que la mejora obtenida de la retroalimentaciÃ³n con Î³ = 0.1. Esto revela una deficiencia de los sistemas de recuperaciÃ³n contemporÃ¡neos: cuando el usuario da retroalimentaciÃ³n positiva sobre un pasaje, los sistemas otorgan mayor peso a los tÃ©rminos presentes en ese pasaje y tienden a recuperar otros pasajes que contienen los mismos tÃ©rminos, y por lo tanto, generalmente la misma informaciÃ³n. Sin embargo, el usuario no se beneficia al ver pasajes redundantes y generalmente estÃ¡ interesado en otros pasajes que contengan informaciÃ³n relacionada. Es informativo evaluar los sistemas de recuperaciÃ³n utilizando nuestra medida de utilidad (con Î³ = 0) que tiene en cuenta la novedad y, por lo tanto, ofrece una imagen mÃ¡s realista de cuÃ¡n bien un sistema puede generalizar a partir de la retroalimentaciÃ³n del usuario, en lugar de utilizar medidas tradicionales de RI como la recuperaciÃ³n y la precisiÃ³n, que ofrecen una imagen incompleta de la mejora obtenida a partir de la retroalimentaciÃ³n del usuario. A veces, sin embargo, los usuarios podrÃ­an estar interesados en ver la misma informaciÃ³n de mÃºltiples fuentes, como un indicador de su importancia o fiabilidad. En tal caso, simplemente podemos elegir un valor mÃ¡s alto para Î³ que corresponda a una mayor tolerancia para la redundancia, y asÃ­ permitir que el sistema ajuste sus parÃ¡metros en consecuencia. Dado que los documentos fueron procesados por fragmentos, serÃ­a interesante ver cÃ³mo mejora el rendimiento de los sistemas con el tiempo. Las figuras 1 y 2 muestran las tendencias de rendimiento de ambos sistemas a lo largo de los fragmentos. Si bien se espera que el rendimiento con y sin retroalimentaciÃ³n en los primeros fragmentos sea similar, para los fragmentos posteriores, la curva de rendimiento con la retroalimentaciÃ³n habilitada supera a la del ajuste sin retroalimentaciÃ³n. Las tendencias de rendimiento no son consistentes en todos los fragmentos porque los documentos relevantes no estÃ¡n distribuidos uniformemente en todos los fragmentos, lo que hace que algunas consultas sean mÃ¡s fÃ¡ciles que otras en ciertos fragmentos. AdemÃ¡s, dado que Indri utiliza retroalimentaciÃ³n de pseudo-relevancia mientras que CAFÂ´E utiliza retroalimentaciÃ³n basada en juicios de relevancia reales, la mejora en el caso de Indri es menos dramÃ¡tica que la de CAFÂ´E. 7. CONCLUSIONES FINALES Este artÃ­culo presenta la primera investigaciÃ³n sobre la destilaciÃ³n de informaciÃ³n basada en la utilidad con un sistema que aprende las necesidades de informaciÃ³n duraderas a partir de la retroalimentaciÃ³n detallada del usuario sobre una secuencia de pasajes clasificados. Nuestro sistema, llamado CAFÂ´E, combina filtrado adaptativo, detecciÃ³n de novedades y clasificaciÃ³n de pasajes antirredundantes en un marco unificado para la optimizaciÃ³n de la utilidad. Desarrollamos un nuevo esquema para la evaluaciÃ³n y retroalimentaciÃ³n automatizadas basado en un procedimiento semiautomÃ¡tico para adquirir reglas que permitan emparejar automÃ¡ticamente fragmentos con las respuestas del sistema. TambiÃ©n propusimos una extensiÃ³n de la mÃ©trica NDCG para evaluar la utilidad de los pasajes clasificados como una combinaciÃ³n ponderada de relevancia y novedad. Nuestros experimentos en el reciÃ©n anotado corpus de referencia TDT4 muestran un alentador aumento de utilidad sobre Indri, y tambiÃ©n sobre nuestro propio sistema con aprendizaje incremental y detecciÃ³n de novedades desactivados. 8. AGRADECIMIENTOS Nos gustarÃ­a agradecer a Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng y al Programa de AnÃ¡lisis de Datos Cualitativos de la Universidad de Pittsburgh liderado por el Dr. Stuart Shulman por su ayuda en la recolecciÃ³n y procesamiento de las anotaciones extendidas de TDT4 utilizadas en nuestros experimentos. Este trabajo estÃ¡ parcialmente respaldado por la FundaciÃ³n Nacional de Ciencias (NSF) bajo la subvenciÃ³n IIS0434035, y la Agencia de Proyectos de InvestigaciÃ³n Avanzada de Defensa (DARPA) bajo los contratos NBCHD030010 y W0550432. Cualquier opiniÃ³n, hallazgo, conclusiÃ³n o recomendaciÃ³n expresada en este material son responsabilidad de los autores y no reflejan necesariamente los puntos de vista de los patrocinadores. AUTORES ADICIONALES Jian Zhang (jianzhan@stat.purdue.edu)â , Jaime Carbonell (jgc@cs.cmu.edu)â  , Peter Brusilovsky (peterb+@pitt.edu)â¡ , Daqing He(dah44@pitt.edu)â¡ 10. REFERENCIAS [1] J. Allan. RetroalimentaciÃ³n incremental de relevancia para la filtraciÃ³n de informaciÃ³n. Actas de la 19Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 270-278, 1996. [2] J. Allan, C. Wade y A. Bolivar. RecuperaciÃ³n y DetecciÃ³n de Novedad a Nivel de OraciÃ³n. Actas de la conferencia ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, 2003. [3] C. Buckley, G. Salton y J. Allan. RecuperaciÃ³n automÃ¡tica con informaciÃ³n de localidad utilizando SMART. PublicaciÃ³n especial del NIST, (500207):59-72, 1993. [4] J. Callan. Aprendiendo mientras se filtran documentos. Actas de la 21Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 224-231, 1998. [5] J. Carbonell y J. Goldstein. El uso de MMR, Reordenamiento basado en Diversidad para Reorganizar Documentos y Producir ResÃºmenes. Actas de la 21Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 335-336, 1998. [6] E. Efthimiadis. ExpansiÃ³n de consulta. RevisiÃ³n Anual de Ciencia de la InformaciÃ³n y TecnologÃ­a (ARIST), 31:p121-87, 1996. [7] J. Fiscus y G. Duddington. Resumen de DetecciÃ³n y Seguimiento de Temas. DetecciÃ³n y seguimiento de temas: OrganizaciÃ³n de la informaciÃ³n basada en eventos, pÃ¡ginas 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov y S. Roukos. Un modelo estadÃ­stico para la detecciÃ³n y seguimiento de entidades multilingÃ¼es. NAACL/HLT, 2004. [9] K. JÃ¤rvelin y J. KekÃ¤lÃ¤inen. EvaluaciÃ³n basada en la Ganancia Acumulada de TÃ©cnicas de RecuperaciÃ³n de InformaciÃ³n. ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin y D. Demner-Fushman. EvaluaciÃ³n automÃ¡tica de respuestas a preguntas de definiciÃ³n. Actas de la Conferencia de TecnologÃ­a del Lenguaje Humano y Conferencia sobre MÃ©todos EmpÃ­ricos en Procesamiento del Lenguaje Natural (HLT/EMNLP 2005), 2005. â Departamento de EstadÃ­stica, Universidad de Purdue, West Lafayette, EE. UU. â  Instituto de TecnologÃ­as del Lenguaje, Universidad Carnegie Mellon, Pittsburgh, EE. UU. â¡ Escuela de Ciencias de la InformaciÃ³n, Universidad de Pittsburgh, Pittsburgh, EE. UU. [11] J. Lin y D. Demner-Fushman. Â¿Se derrumbarÃ¡n las pirÃ¡mides construidas con pepitas? Actas de HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla y S. Roukos. Un algoritmo de resoluciÃ³n de correferencia sÃ­ncrona basado en el Ã¡rbol de Bell. Actas de ACL, 4:136-143, 2004. [13] G. Marton. Nuggeteer: EvaluaciÃ³n AutomÃ¡tica Basada en Pepitas Utilizando Descripciones y Juicios. HLT/NAACL, 2006. [14] E. Riloff. ConstrucciÃ³n automÃ¡tica de un diccionario para tareas de extracciÃ³n de informaciÃ³n. Actas de la UndÃ©cima Conferencia Nacional sobre Inteligencia Artificial, pÃ¡ginas 811-816, 1993. [15] S. Robertson y S. Walker. Microsoft Cambridge en TREC-9: Pista de filtrado. La Novena Conferencia de RecuperaciÃ³n de Texto (TREC-9), pÃ¡ginas 361-368. [16] R. Schapire, Y. Cantante, y A. Singhal. Boosting y Rocchio aplicados a la filtraciÃ³n de texto. Actas de la 21Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle y W. Croft. Indri: Un motor de bÃºsqueda basado en modelos de lenguaje para consultas complejas. Actas de la Conferencia Internacional sobre AnÃ¡lisis de Inteligencia, 2004. [18] El Consorcio de Datos LingÃ¼Ã­sticos. http://www.ldc.upenn.edu/. [19] E. Voorhees. Resumen de la pista de preguntas y respuestas TREC 2003. Actas de la DuodÃ©cima Conferencia de RecuperaciÃ³n de Texto (TREC 2003), 2003. [20] Y. Yang y B. Kisiel. RegresiÃ³n local basada en mÃ¡rgenes para filtrado adaptativo. Actas de la duodÃ©cima conferencia internacional sobre gestiÃ³n de la informaciÃ³n y el conocimiento, pÃ¡ginas 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang y B. Kisiel. Robustez de los MÃ©todos de Filtrado Adaptativo en una EvaluaciÃ³n de Referencia Cruzada. Actas de la 28Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 98-105, 2005. [22] C. Zhai, W. Cohen y J. Lafferty. MÃ¡s allÃ¡ de la relevancia independiente: MÃ©todos y mÃ©tricas de evaluaciÃ³n para la recuperaciÃ³n de subtemas. Actas de la 26Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 10-17, 2003. [23] J. Zhang y Y. Yang. Robustez de los MÃ©todos de ClasificaciÃ³n Lineal Regularizados en la CategorizaciÃ³n de Textos. Actas de la 26Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 190-197, 2003. [24] Y. Zhang. Utilizando priors bayesianos para combinar clasificadores para filtrado adaptativo. Actas de la 27Âª conferencia internacional anual sobre InvestigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 345-352, 2004. [25] Y. Zhang, J. Callan y T. Minka. DetecciÃ³n de novedad y redundancia en el filtrado adaptativo. Actas de la 25Âª Conferencia Internacional Anual de ACM SIGIR sobre InvestigaciÃ³n y Desarrollo en RecuperaciÃ³n de InformaciÃ³n, 2002. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "passage ranking": {
            "translated_key": "clasificaciÃ³n de pasajes",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Utility-based Information Distillation Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu ABSTRACT This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework.",
                "It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant <br>passage ranking</br> with respect to long-lasting information needs (tasks with multiple queries).",
                "Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings.",
                "For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task.",
                "Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty.",
                "Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval.",
                "Adaptive filtering (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics.",
                "Based on the initial query and a few positive examples (if available), an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user.",
                "The incremental learning nature of AF systems makes them more powerful than standard search engines that support ad-hoc retrieval (e.g.",
                "Google and Yahoo) in terms of finding relevant information with respect to long-lasting topics of interest, and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs, without having to modify their queries manually.",
                "A variety of supervised learning algorithms (Rocchio-style classifiers, Exponential-Gaussian models, local regression and logistic regression approaches) have been studied for adaptive settings, examined with explicit and implicit relevance feedback, and evaluated with respect to utility optimization on large benchmark data collections in TREC (Text Retrieval Conferences) and TDT (Topic Detection and Tracking) forums [1, 4, 7, 15, 16, 20, 24, 23].",
                "Regularized logistic regression [21] has been found representative for the state-of-the-art approaches, and highly efficient for frequent model adaptations over large document collections such as the TREC-10 corpus (over 800,000 documents and 84 topics).",
                "Despite substantial achievements in recent adaptive filtering research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently.",
                "Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications: 1.",
                "User has a rather passive role in the conventional adaptive filtering setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision.",
                "A more active alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents (or passages) per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists.",
                "The latter form of user interaction has been highly effective in standard retrieval for ad-hoc queries.",
                "How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2.",
                "The unit for receiving a relevance judgment (yes or no) is restricted to the document level in conventional AF.",
                "However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant.",
                "Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system.",
                "For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3.",
                "System-selected documents are often highly redundant.",
                "A major news event, for example, would be reported by multiple sources repeatedly for a while, making most of the information content in those articles redundant with each other.",
                "A conventional AF system would select all these redundant news stories for user feedback, wasting the users time while offering little gain.",
                "Clearly, techniques for novelty detection can help in principle [25, 2, 22] for improving the utility of the AF systems.",
                "However, the effectiveness of such techniques at passage level to detect novelty with respect to users (fine-grained) feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user.",
                "To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and novelty detection for removal of redundancy from user interactions with the system.",
                "We call the new process utility-based information distillation.",
                "Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach.",
                "Therefore, we extended a benchmark corpus - the TDT4 collection of news stories and TV broadcasts - with task definitions, multiple queries per task, and answer keys per query.",
                "We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 .",
                "To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses.",
                "Moreover, we propose an extension of NDCG (Normalized Discounted Cumulated Gain) [9] for assessing the utility of ranked passages as a function of both relevance and novelty.",
                "The rest of this paper is organized as follows.",
                "Section 2 outlines the information distillation process with a concrete example.",
                "Section 3 describes the technical cores of our system called CAFÂ´E - CMU Adaptive Filtering Engine.",
                "Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme.",
                "Section 5 describes the extended TDT4 corpus.",
                "Section 6 presents our experiments and results.",
                "Section 7 concludes the study and gives future perspectives. 2.",
                "A SAMPLE TASK Consider a news event - the escape of seven convicts from a Texas prison in December 2000 and their capture a month later.",
                "Assuming a user were interested in this event since its early stage, the information need could be: Find information about the escape of convicts from Texas prison, and information related to their recapture.",
                "The associated lower-level questions could be: 1.",
                "How many prisoners escaped? 2.",
                "Where and when were they sighted? 3. Who are their known contacts inside and outside the prison? 4.",
                "How are they armed? 5.",
                "Do they have any vehicles? 6.",
                "What steps have been taken so far?",
                "We call such an information need a task, and the associated questions as the queries in this task.",
                "A distillation system is supposed to monitor the incoming documents, process them chunk by chunk in a temporal order, select potentially relevant and novel passages from each chunk with respect to each query, and present a ranked list of passages to the user.",
                "<br>passage ranking</br> here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list.",
                "When presented with a list of passages, the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant.",
                "These spans of text are taken as positive examples in the adaptation of the query profile, and also added to the users history.",
                "Passages not marked by the user are taken as negative examples.",
                "As soon as the query profile is updated, the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low, based on user preference.",
                "For example, if the user highlights ...officials have posted a $100,000 reward for their capture... as relevant answer to the query What steps have been taken so far?, then the highlighted piece is used as an additional positive training example in the adaptation of the query profile.",
                "This piece of feedback is also added to the user history as a seen example, so that in future, the system will not place another passage mentioning $100,000 reward at the top of the ranked list.",
                "However, an article mentioning ...officials have doubled the reward money to $200,000... might be ranked high since it is both relevant to the (updated) query profile and novel with respect to the (updated) user history.",
                "The user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly.",
                "Clearly, novelty detection is very important for the utility of such a system because of the iterative search.",
                "Without novelty detection, the old relevant passages would be shown to the user repeatedly in each ranked list.",
                "Through the above example, we can see the main properties of our new framework for utility-based information distillation over temporally ordered documents.",
                "Our framework combines and extends the power of adaptive filtering (AF), ad-hoc retrieval (IR) and novelty detection (ND).",
                "Compared to standard IR, our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task.",
                "Compared to conventional AF, it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system.",
                "Compared to past work, this is the first evaluation of novelty detection integrated with adaptive filtering for sequenced queries that allows flexible user feedback over ranked passages.",
                "The combination of AF, IR and ND with the new extensions raises an important research question regarding evaluation methodology: how can we measure the utility of such an information distillation system?",
                "Existing metrics in standard IR, AF and ND are insufficient, and new solutions must be explored, as we will discuss in Section 4, after describing the technical cores of our system in the next section. 3.",
                "TECHNICAL CORES The core components of CAFÂ´E are - 1) AF for incremental learning of query profiles, 2) IR for estimating relevance of passages with respect to query profiles, 3) ND for assessing novelty of passages with respect to users history, and 4) anti-redundancy component to remove redundancy from ranked lists. 3.1 Adaptive Filtering Component We use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for AF [21].",
                "Logistic regression (LR) is a supervised learning algorithm for statistical classification.",
                "Based on a training set of labeled instances, it learns a class model which can then by used to predict the labels of unseen instances.",
                "Its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required, as is the case in adaptive filtering, where the system must learn from each new feedback provided by the user. (See [21] and [23] for computational complexity and implementation issues).",
                "In adaptive filtering, each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query.",
                "For training the model, we use the query itself as the initial positive training example of the class, and the user-highlighted pieces of text (marked as Relevant or Not-relevant) during feedback as additional training examples.",
                "To address the cold start issue in the early stage before any user feedback is obtained, the system uses a small sample from a retrospective corpus as the initial negative examples in the training set.",
                "The details of using logistic regression for adaptive filtering (assigning different weights to positive and negative training instances, and regularizing the objective function to prevent over-fitting on training data) are presented in [21].",
                "The class model wâ learned by Logistic Regression, or the query profile, is a vector whose dimensions are individual terms and whose elements are the regression coefficients, indicating how influential each term is in the query profile.",
                "The query profile is updated whenever a new piece of user feedback is received.",
                "A temporally decaying weight can be applied to each training example, as an option, to emphasize the most recent user feedback. 3.2 Passage Retrieval Component We use standard IR techniques in this part of our system.",
                "Incoming documents are processed in chunks, where each chunk can be defined as a fixed span of time or as a fixed number of documents, as preferred by the user.",
                "For each incoming document, corpus statistics like the IDF (Inverted Document Frequency) of each term are updated.",
                "We use a state-of-the-art named entity identifier and tracker [8, 12] to identify person and location names, and merge them with co-referent named entities seen in the past.",
                "Then the documents are segmented into passages, which can be a whole document, a paragraph, a sentence, or any other continuous span of text, as preferred.",
                "Each passage is represented using a vector of TF-IDF (Term FrequencyInverse Document Frequency) weights, where term can be a word or a named entity.",
                "Given a query profile, i.e. the logistic regression solution wâ as described in Section 3.1, the system computes the posterior probability of relevance for each passage x as fRL(x) â¡ P(y = 1|x, wâ ) = 1 (1 + eâwâÂ·x) (1) Passages are ordered by their relevance scores, and the ones with scores above a threshold (tuned on a training set) comprise the relevance list that is passed on to the novelty detection step. 3.3 Novelty Detection Component CAFÂ´E maintains a user history H(t), which contains all the spans of text hi that the user highlighted (as feedback) during his or her past interactions with the system, up to the current time t. Denoting the history as H(t) = n h1, h2, ..., ht o , (2) the novelty score of a new candidate passage x is computed as: fND(x) = 1 â max iâ1..t {cos(x, hi)} (3) where both candidate passage x and highlighted spans of text hi are represented as TF-IDF vectors.",
                "The novelty score of each passage is compared to a prespecified threshold (also tuned on a training set), and any passage with a score below this threshold is removed from the relevance list. 3.4 Anti-redundant Ranking Component Although the novelty detection component ensures that only novel (previously unseen) information remains in the relevance list, this list might still contain the same novel information at multiple positions in the ranked list.",
                "Suppose, for example, that the user has already read about a $100,000 reward for information about the escaped convicts.",
                "A new piece of news that the award has been increased to $200,000 is novel since the user hasnt read about it yet.",
                "However, multiple news sources would report this news and we might end up showing (redundant) articles from all these sources in a ranked list.",
                "Hence, a ranked list should also be made non-redundant with respect to its own contents.",
                "We use a simplified version of the Maximal Marginal Relevance method [5], originally developed for combining relevance and novelty in text retrieval and summarization.",
                "Our procedure starts with the current list of passages sorted by relevance (section 3.2), filtered by Novelty Detection component (section 3.3), and generates a new non-redundant list as follows: 1.",
                "Take the top passage in the current list as the top one in the new list. 2.",
                "Add the next passage x in the current list to the new list only if fAR(x) > t where fAR(x) = 1 â max piâLnew {cos(x, pi)} and Lnew is the set of passages already selected in the new list. 3.",
                "Repeat step 2 until all the passages in the current list have been examined.",
                "After applying the above-mentioned algorithm, each passage in the new list is sufficiently dissimilar to others, thus favoring diversity rather than redundancy in the new ranked list.",
                "The anti-redundancy threshold t is tuned on a training set. 4.",
                "EVALUATION METHODOLOGY The approach we proposed above for information distillation raises important issues regarding evaluation methodology.",
                "Firstly, since our framework allows the output to be passages at different levels of granularity (e.g. k-sentence windows where k may vary) instead of a fixed length, it is not possible to have pre-annotated relevance judgments at all such granularity levels.",
                "Secondly, since we wish to measure the utility of the system output as a combination of both relevance and novelty, traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time.",
                "Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists.",
                "None of the existing measures in ad-hoc retrieval, adaptive filtering, novelty detection or other related areas (text summarization and question answering) have desirable properties in all the three aspects.",
                "Therefore, we must develop a new evaluation methodology. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers.",
                "Answer keys define what should be present in a system response to receive credit, and are comprised of a collection of information nuggets, i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them.",
                "Defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping [19], since system-returned passages can contain the same information expressed in many different ways.",
                "Hence, QA evaluations have relied on human assessors for the mapping between various expressions, making the process costly, time consuming, and not scalable to large query and document collections, and extensive system evaluations with various parameter settings. 4.1.1 Automating Evaluation based on Answer Keys Automatic evaluation methods would allow for faster system building and tuning, as well as provide an objective and affordable way of comparing various systems.",
                "Recently, such methods have been proposed, more or less, based on the idea of n-gram co-occurrences.",
                "Pourpre [10] assigns a fractional recall score to a system response based on its unigram overlap with a given nuggets description.",
                "For example, a system response A B C has recall 3/4 with respect to a nugget with description A B C D. However, such an approach is unfair to systems that present the same information but using words other than A, B, C, and D. Another open issue is how to weight individual words in measuring the closeness of a match.",
                "For example, consider the question How many prisoners escaped?.",
                "In the nugget Seven prisoners escaped from a Texas prison, there is no indication that seven is the keyword, and that it must be matched to get any relevance credit.",
                "Using IDF values does not help, since seven will generally not have a higher IDF than words like texas and prison.",
                "Also, redefining the nugget as just seven does not solve the problem since now it might spuriously match any mention of seven out of context.",
                "Nuggeteer [13] works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold.",
                "However, it is also plagued by spurious relevance since not all words contained in the nugget description (or known correct responses) are central to the nugget. 4.1.2 Nugget-Matching Rules We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on nugget-matching rules, which are generated using a semiautomatic procedure explained below.",
                "These rules are essentially Boolean queries that will only match against snippets that contain the nugget.",
                "For instance, a candidate rule for matching answers to How many prisoners escaped? is (Texas AND seven AND escape AND (convicts OR prisoners)), possibly with other synonyms and variants in the rule.",
                "For a corpus of news articles, which usually follow a typical formal prose, it is fairly easy to write such simple rules to match expected answers using a bootstrap approach, as described below.",
                "We propose a two-stage approach, inspired by Autoslog [14], that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents.",
                "In the first stage, human subjects annotated (using a highlighting tool) portions of ontopic documents that contained answers to each nugget 2 .",
                "In the second stage, subjects used our rule generation tool to create rules that would match the annotations for each nugget.",
                "The tool allows users to enter a Boolean rule as a disjunction of conjunctions (e.g. ((a AND b) OR (a AND c AND d) OR (e))).",
                "Given a candidate rule, our tool uses it as a Boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match.",
                "Hence, the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision.",
                "We observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions (adding more ANDs), and improving the recall by adding more conjunctions to the disjunction (adding more ORs).",
                "As an example, lets try to create a rule for the nugget which says that seven prisoners escaped from the Texas prison.",
                "We start with a simple rule - (seven).",
                "When we input this into the rule generation tool, we realize that this rule matches many spurious occurrences of seven (e.g. ...seven states...) and thus gets a low precision score.",
                "We can further qualify our rule - Texas AND seven AND convicts.",
                "Next, by looking at the missed annotations, we realize that some news articles mentioned ...seven prisoners escaped.... We then replace convicts with the disjunction (convicts OR prisoners).",
                "We continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the (small number of) misses and false alarms can be safely ignored.",
                "Thus we can create nugget-matching rules that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses.",
                "Human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably. 4.2 Evaluating the Utility of a Sequence of Ranked Lists The utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information, and how much the user lost in terms of time and energy.",
                "We calculate this utility from the utilities of individual passages as follows.",
                "After reading each passage returned by the system, the user derives some gain depending on the presence of relevant and novel information, and incurs a loss in terms of the time and energy spent in going through the passage.",
                "However, the likelihood that the user would actually read a passage depends on its position in the ranked list.",
                "Hence, for a query q, the expected utility 2 LDC [18] already provides relevance judgments for 100 topics on the TDT4 corpus.",
                "We further ensured that these judgments are exhaustive on the entire corpus using pooling. of a passage pi at rank i can be defined as U(pi, q) = P(i) â (Gain(pi, q) â Loss(pi, q)) (4) where P(i) is the probability that the user would go through a passage at rank i.",
                "The expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage: U(q) = nX i=1 P(i) â (Gain(pi, q) â Loss(pi, q)) (5) Note that if we ignore the loss term and define P(i) as P(i) â 1/ logb(b + i â 1) (6) then we get the recently popularized metric called Discounted Cumulated Gain (DCG) [9], where Gain(pi, q) is defined as the graded relevance of passage pi.",
                "However, without the loss term, DCG is a purely recall-oriented metric and not suitable for an adaptive filtering setting, where the systems utility depends in part on its ability to limit the number of items shown to the user.",
                "Although P(i) could be defined based on empirical studies of user behavior, for simplicity, we use P(i) exactly as defined in equation 6.",
                "The gain G(pi, q) of passage pi with respect to the query q is a function of - 1) the number of relevant nuggets present in pi, and 2) the novelty of each of these nuggets.",
                "We combine these two factors as follows.",
                "For each nugget Nj, we assign an initial weight wj, and also keep a count nj of the number of times this nugget has been seen by the user in the past.",
                "The gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor Î³.",
                "Thus, G(pi, q) is defined as G(pi, q) = X Nj âC(pi,q) wj â Î³nj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the answer key of query q.",
                "The initial weights wj are all set of be 1.0 in our experiments, but can also be set based on a pyramid approach [11].",
                "The choice of dampening factor Î³ determines the users tolerance for redundancy.",
                "When Î³ = 0, a nugget will only receive credit for its first occurrence i.e. when nj is zero3 .",
                "For 0 < Î³ < 1, a nugget receives smaller credit for each successive occurrence.",
                "When Î³ = 1, no dampening occurs and repeated occurrences of a nugget receive the same credit.",
                "Note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system, since the users are expected to remember what the system showed them in the past.",
                "We define the loss L(pi, q) as a constant cost c (we use 0.1) incurred when reading a system-returned passage.",
                "Thus, our metric can be re-written as U(q) = nX i=1 Gain(pi, q) logb(b + i â 1) â L(n) (8) where L(n) is the loss associated with a ranked list of length n: L(n) = c Â· nX i=1 1 logb(b + i â 1) (9) 3 Note that 00 = 1 Due to the similarity with Discounted Cumulated Gain (DCG), we call our metric Discounted Cumulated Utility (DCU).",
                "The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) â¤ 0 i.e. when the gain is less than or equal to the cost of reading the passage. 5.",
                "DATA TDT4 was the benchmark corpus used in TDT2002 and TDT2003 evaluations.",
                "The corpus consists of over 90, 000 news articles from multiple sources (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) published between October 2000 and January 2001, in the languages of Arabic, English, and Mandarin.",
                "Speech-recognized and machine-translated versions of the non-English articles were provided as well.",
                "LDC [18] has annotated the corpus with 100 topics, that correspond to various news events in this time period.",
                "Out of these, we selected a subset of 12 actionable events, and defined corresponding tasks for them4 .",
                "For each task, we manually defined a profile consisting of an initial set of (5 to 10) queries, a free-text description of the user history, i.e., what the user already knows about the event, and a list of known on-topic and off-topic documents (if available) as training examples.",
                "For each query, we generated answer keys and corresponding nugget matching rules using the procedure described in section 4.1.2, and produced a total of 120 queries, with an average of 7 nuggets per query. 6.",
                "EXPERIMENTS AND RESULTS 6.1 Baselines We used Indri [17], a popular language-model based retrieval engine, as a baseline for comparison with CAFÂ´E.",
                "Indri supports standard search engine functionality, including pseudo-relevance feedback (PRF) [3, 6], and is representative of a typical query-based retrieval system.",
                "Indri does not support any kind of novelty detection.",
                "We compare Indri with PRF turned on and off, against CAFÂ´E with user feedback, novelty detection and antiredundant ranking turned on and off. 6.2 Experimental Setup We divided the TDT4 corpus spanning 4 months into 10 chunks, each defined as a period of 12 consecutive days.",
                "At any given point of time in the distillation process, each system accessed the past data up to the current point, and returned a ranked list of up 50 passages per query.",
                "The 12 tasks defined on the corpus were divided into a training and test set with 6 tasks each.",
                "Each system was allowed to use the training set to tune its parameters for optimizing NDCU (equation 8), including the relevance threshold for both Indri and CAFÂ´E, and the novelty and antiredundancy thresholds for CAFÂ´E.",
                "The NDCU for each system run is calculated automatically.",
                "User feedback was also simulated - relevance judgments for each system-returned passage (as determined by the nugget matching rules described in section 4.1.2) were 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figure 1: Performance of Indri across chunks Figure 2: Performance of CAFÂ´E across chunks used as user feedback in the adaptation of query profiles and user histories. 6.3 Results In Table 1, we show the NDCU scores of the two systems under various settings.",
                "These scores are averaged over the six tasks in the test set, and are calculated with two dampening factors (see section 4.2): Î³ = 0 and 0.1, to simulate no tolerance and small tolerance for redundancy, respectively.",
                "Using Î³ = 0 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information.",
                "Hence, the improvement obtained from enabling user feedback is smaller with Î³ = 0 than the improvement obtained from feedback with Î³ = 0.1.",
                "This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information.",
                "However, the user does not benefit from seeing such redundant passages, and is usually interested in other passages containing related information.",
                "It is informative to evaluate retrieval systems using our utility measure (with Î³ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.",
                "Sometimes, however, users might indeed be interested in seeing the same information from multiple sources, as an Table 1: NDCU Scores of Indri and CAFÂ´E for two dampening factors (Î³), and various settings (F: Feedback, N: Novelty Detection, A: Anti-Redundant Ranking) Indri CAFÂ´E Î³ Base +PRF Base +F +F+N +F+A +F+N+A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.35 0.36 0.36 indicator of its importance or reliability.",
                "In such a case, we can simply choose a higher value for Î³ which corresponds to a higher tolerance for redundancy, and hence let the system tune its parameters accordingly.",
                "Since documents were processed chunk by chunk, it would be interesting to see how the performance of systems improves over time.",
                "Figures 1 and 2 show the performance trends for both the systems across chunks.",
                "While the performance with and without feedback on the first few chunks is expected to be close, for subsequent chunks, the performance curve with feedback enabled rises above the one with the no-feedback setting.",
                "The performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks, making some queries easier than others in certain chunks.",
                "Moreover, since Indri uses pseudo-relevance feedback while CAFÂ´E uses feedback based on actual relevance judgments, the improvement in case of Indri is less dramatic than that of CAFÂ´E. 7.",
                "CONCLUDING REMARKS This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages.",
                "Our system, called CAFÂ´E, combines adaptive filtering, novelty detection and antiredundant <br>passage ranking</br> in a unified framework for utility optimization.",
                "We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty.",
                "Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off. 8.",
                "ACKNOWLEDGMENTS We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments.",
                "This work is supported in parts by the National Science Foundation (NSF) under grant IIS0434035, and the Defense Advanced Research Project Agency (DARPA) under contracts NBCHD030010 and W0550432.",
                "Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. 9.",
                "ADDITIONAL AUTHORS Jian Zhang (jianzhan@stat.purdue.edu)â , Jaime Carbonell (jgc@cs.cmu.edu)â  , Peter Brusilovsky (peterb+@pitt.edu)â¡ , Daqing He(dah44@pitt.edu)â¡ 10.",
                "REFERENCES [1] J. Allan.",
                "Incremental Relevance Feedback for Information Filtering.",
                "Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 270-278, 1996. [2] J. Allan, C. Wade, and A. Bolivar.",
                "Retrieval and Novelty Detection at the Sentence Level.",
                "Proceedings of the ACM SIGIR conference on research and development in information retrieval, 2003. [3] C. Buckley, G. Salton, and J. Allan.",
                "Automatic Retrieval with Locality Information using SMART.",
                "NIST special publication, (500207):59-72, 1993. [4] J. Callan.",
                "Learning While Filtering Documents.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 224-231, 1998. [5] J. Carbonell and J. Goldstein.",
                "The use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335-336, 1998. [6] E. Efthimiadis.",
                "Query Expansion.",
                "Annual Review of Information Science and Technology (ARIST), 31:p121-87, 1996. [7] J. Fiscus and G. Duddington.",
                "Topic Detection and Tracking Overview.",
                "Topic Detection and Tracking: Event-based Information Organization, pages 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.",
                "A Statistical Model for Multilingual Entity Detection and Tracking.",
                "NAACL/HLT, 2004. [9] K. JÂ¨arvelin and J. KekÂ¨alÂ¨ainen.",
                "Cumulated Gain-based Evaluation of IR Techniques.",
                "ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin and D. Demner-Fushman.",
                "Automatically Evaluating Answers to Definition Questions.",
                "Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), 2005. â Statistics Dept., Purdue University, West Lafayette, USA â  Language Technologies Inst., Carnegie Mellon University, Pittsburgh, USA â¡ School of Information Sciences, Univ. of Pittsburgh, Pittsburgh, USA [11] J. Lin and D. Demner-Fushman.",
                "Will Pyramids Built of nUggets Topple Over.",
                "Proceedings of HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.",
                "A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree.",
                "Proc. of ACL, 4:136-143, 2004. [13] G. Marton.",
                "Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments.",
                "HLT/NAACL, 2006. [14] E. Riloff.",
                "Automatically Constructing a Dictionary for Information Extraction Tasks.",
                "Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, 1993. [15] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9: Filtering track.",
                "The Ninth Text REtrieval Conference (TREC-9), pages 361-368. [16] R. Schapire, Y.",
                "Singer, and A. Singhal.",
                "Boosting and Rocchio Applied to Text Filtering.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle, and W. Croft.",
                "Indri: A Language Model-based Search Engine for Complex Queries.",
                "Proceedings of the International Conference on Intelligence Analysis, 2004. [18] The Linguistic Data Consortium. http://www.ldc.upenn.edu/. [19] E. Voorhees.",
                "Overview of the TREC 2003 Question Answering Track.",
                "Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), 2003. [20] Y. Yang and B. Kisiel.",
                "Margin-based Local Regression for Adaptive Filtering.",
                "Proceedings of the twelfth international conference on Information and knowledge management, pages 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of Adaptive Filtering Methods in a Cross-benchmark Evaluation.",
                "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 98-105, 2005. [22] C. Zhai, W. Cohen, and J. Lafferty.",
                "Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-17, 2003. [23] J. Zhang and Y. Yang.",
                "Robustness of Regularized Linear Classification Methods in Text Categorization.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 190-197, 2003. [24] Y. Zhang.",
                "Using Bayesian Priors to Combine Classifiers for Adaptive Filtering.",
                "Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 345-352, 2004. [25] Y. Zhang, J. Callan, and T. Minka.",
                "Novelty and Redundancy Detection in Adaptive Filtering.",
                "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002."
            ],
            "original_annotated_samples": [
                "It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant <br>passage ranking</br> with respect to long-lasting information needs (tasks with multiple queries).",
                "<br>passage ranking</br> here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list.",
                "Our system, called CAFÂ´E, combines adaptive filtering, novelty detection and antiredundant <br>passage ranking</br> in a unified framework for utility optimization."
            ],
            "translated_annotated_samples": [
                "Combina las fortalezas y se extiende mÃ¡s allÃ¡ del filtrado adaptativo convencional, la detecciÃ³n de novedades y la <br>clasificaciÃ³n de pasajes</br> no redundantes con respecto a las necesidades de informaciÃ³n duraderas (tareas con mÃºltiples consultas).",
                "La <br>clasificaciÃ³n de pasajes</br> aquÃ­ se basa en cuÃ¡n relevante es un pasaje con respecto a la consulta actual, cuÃ¡n novedoso es con respecto al historial actual del usuario (de sus interacciones con el sistema) y cuÃ¡n redundante es en comparaciÃ³n con otros pasajes con un rango mÃ¡s alto en la lista.",
                "Nuestro sistema, llamado CAFÂ´E, combina filtrado adaptativo, detecciÃ³n de novedades y <br>clasificaciÃ³n de pasajes</br> antirredundantes en un marco unificado para la optimizaciÃ³n de la utilidad."
            ],
            "translated_text": "DestilaciÃ³n de informaciÃ³n basada en utilidad sobre documentos secuenciados temporalmente en el Instituto de TecnologÃ­as del Lenguaje de Yiming Yang. Universidad Carnegie Mellon, Pittsburgh, EE. UU. yiming@cs.cmu.edu Abhimanyu Lad Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. alad@cs.cmu.edu Instituto de TecnologÃ­as del Lenguaje Ni Lao. Universidad Carnegie Mellon, Pittsburgh, EE. UU. nlao@cs.cmu.edu Abhay Harpale Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. aharpale@cs.cmu.edu Bryan Kisiel Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. bkisiel@cs.cmu.edu MÃ³nica Rogati Instituto de TecnologÃ­as del Lenguaje. La Universidad Carnegie Mellon en Pittsburgh, EE. UU. mrogati@cs.cmu.edu RESUMEN Este artÃ­culo examina un nuevo enfoque para la destilaciÃ³n de informaciÃ³n en documentos ordenados temporalmente, y propone un esquema de evaluaciÃ³n novedoso para dicho marco. Combina las fortalezas y se extiende mÃ¡s allÃ¡ del filtrado adaptativo convencional, la detecciÃ³n de novedades y la <br>clasificaciÃ³n de pasajes</br> no redundantes con respecto a las necesidades de informaciÃ³n duraderas (tareas con mÃºltiples consultas). Nuestro enfoque respalda la retroalimentaciÃ³n detallada del usuario a travÃ©s del resaltado de fragmentos arbitrarios de texto, y aprovecha dicha informaciÃ³n para la optimizaciÃ³n de la utilidad en entornos adaptativos. Para nuestros experimentos, definimos tareas hipotÃ©ticas basadas en eventos de noticias en el corpus TDT4, con mÃºltiples consultas por tarea. Se generaron claves de respuestas (nuggets) para cada consulta y se utilizÃ³ un procedimiento semiautomÃ¡tico para adquirir reglas que permiten emparejar automÃ¡ticamente los nuggets con las respuestas del sistema. TambiÃ©n proponemos una extensiÃ³n de la mÃ©trica NDCG para evaluar la utilidad de los pasajes clasificados como una combinaciÃ³n de relevancia y novedad. Nuestros resultados muestran mejoras alentadoras en la utilidad utilizando el nuevo enfoque, en comparaciÃ³n con los sistemas base sin aprendizaje incremental o componentes de detecciÃ³n de novedades. CategorÃ­as y Descriptores de Asignaturas H.3.3 [BÃºsqueda y RecuperaciÃ³n de InformaciÃ³n]: Filtrado de informaciÃ³n, RetroalimentaciÃ³n de relevancia, Modelos de recuperaciÃ³n, Proceso de selecciÃ³n; I.5.2 TÃ©rminos Generales DiseÃ±o, MediciÃ³n, Rendimiento, ExperimentaciÃ³n. 1. INTRODUCCIÃN Seguir informaciÃ³n nueva y relevante de flujos de datos temporales para usuarios con necesidades duraderas ha sido un tema de investigaciÃ³n desafiante en la recuperaciÃ³n de informaciÃ³n. El filtrado adaptativo (AF) es una tarea de predicciÃ³n en lÃ­nea de la relevancia de cada nuevo documento con respecto a temas predefinidos. BasÃ¡ndose en la consulta inicial y algunos ejemplos positivos (si estÃ¡n disponibles), un sistema de AF mantiene un perfil para cada tema de interÃ©s, y lo actualiza constantemente segÃºn la retroalimentaciÃ³n del usuario. La naturaleza de aprendizaje incremental de los sistemas de AF los hace mÃ¡s poderosos que los motores de bÃºsqueda estÃ¡ndar que admiten recuperaciÃ³n ad-hoc (por ejemplo. Google y Yahoo) en tÃ©rminos de encontrar informaciÃ³n relevante con respecto a temas de interÃ©s duraderos, y mÃ¡s atractivo para los usuarios que estÃ¡n dispuestos a proporcionar retroalimentaciÃ³n para adaptar el sistema a sus necesidades de informaciÃ³n especÃ­ficas, sin tener que modificar sus consultas manualmente. Se han estudiado una variedad de algoritmos de aprendizaje supervisado (clasificadores de estilo Rocchio, modelos exponenciales-gaussianos, regresiÃ³n local y enfoques de regresiÃ³n logÃ­stica) para entornos adaptativos, examinados con retroalimentaciÃ³n de relevancia explÃ­cita e implÃ­cita, y evaluados con respecto a la optimizaciÃ³n de la utilidad en grandes colecciones de datos de referencia en los foros de TREC (Conferencias de RecuperaciÃ³n de Texto) y TDT (DetecciÃ³n y Seguimiento de Temas) [1, 4, 7, 15, 16, 20, 24, 23]. La regresiÃ³n logÃ­stica regularizada [21] se ha encontrado representativa para los enfoques de vanguardia, y altamente eficiente para adaptaciones frecuentes del modelo en grandes colecciones de documentos como el corpus TREC-10 (mÃ¡s de 800,000 documentos y 84 temas). A pesar de los logros sustanciales en la investigaciÃ³n reciente de filtrado adaptativo, aÃºn quedan problemas significativos sin resolver sobre cÃ³mo aprovechar de manera efectiva y eficiente la retroalimentaciÃ³n del usuario. EspecÃ­ficamente, los siguientes problemas pueden limitar seriamente la verdadera utilidad de los sistemas de AF en aplicaciones del mundo real: 1. El usuario tiene un papel bastante pasivo en la configuraciÃ³n convencional de filtrado adaptativo: solo reacciona ante el sistema cuando este toma una decisiÃ³n afirmativa sobre un documento, confirmando o rechazando esa decisiÃ³n. Una alternativa mÃ¡s activa serÃ­a permitir al usuario emitir mÃºltiples consultas sobre un tema, revisar una lista clasificada de documentos candidatos (o pasajes) por consulta, y proporcionar retroalimentaciÃ³n sobre la lista clasificada, refinando asÃ­ su necesidad de informaciÃ³n y solicitando listas clasificadas actualizadas. La Ãºltima forma de interacciÃ³n del usuario ha sido altamente efectiva en la recuperaciÃ³n estÃ¡ndar para consultas ad-hoc. CÃ³mo implementar una estrategia para necesidades de informaciÃ³n duraderas en entornos de FA es una pregunta abierta para la investigaciÃ³n. 2. La unidad para recibir un juicio de relevancia (sÃ­ o no) estÃ¡ restringida al nivel del documento en AF convencional. Sin embargo, un usuario real puede estar dispuesto a proporcionar comentarios mÃ¡s informativos y detallados resaltando algunas partes de texto en un documento recuperado como relevantes, en lugar de etiquetar todo el documento como relevante. Aprovechar de manera efectiva este tipo de retroalimentaciÃ³n detallada podrÃ­a mejorar sustancialmente la calidad de un sistema de AF. Para esto, necesitamos habilitar el aprendizaje supervisado a partir de fragmentos de texto etiquetados de longitud arbitraria en lugar de solo permitir documentos etiquetados. 3. Los documentos seleccionados por el sistema suelen ser altamente redundantes. Un evento importante de noticias, por ejemplo, serÃ­a reportado por mÃºltiples fuentes repetidamente durante un tiempo, haciendo que la mayor parte del contenido informativo en esos artÃ­culos sea redundante entre sÃ­. Un sistema de AF convencional seleccionarÃ­a todas estas historias de noticias redundantes para la retroalimentaciÃ³n del usuario, desperdiciando el tiempo de los usuarios mientras ofrece poco beneficio. Claramente, las tÃ©cnicas de detecciÃ³n de novedades pueden ayudar en principio [25, 2, 22] a mejorar la utilidad de los sistemas AF. Sin embargo, la efectividad de tales tÃ©cnicas a nivel de pasaje para detectar novedad con respecto a la retroalimentaciÃ³n de los usuarios a nivel detallado y para detectar redundancia en listas clasificadas aÃºn debe ser evaluada utilizando una medida de utilidad que imite las necesidades de un usuario real. Para abordar las limitaciones actuales de los sistemas de AF mencionadas anteriormente, proponemos y examinamos un nuevo enfoque en este artÃ­culo, que combina las fortalezas del AF convencional (aprendizaje incremental de modelos de temas), recuperaciÃ³n de pasajes multipase para consultas de larga duraciÃ³n condicionadas al tema, y detecciÃ³n de novedades para eliminar la redundancia de las interacciones del usuario con el sistema. Llamamos al nuevo proceso destilaciÃ³n de informaciÃ³n basada en utilidad. Ten en cuenta que los corpus de referencia convencionales para evaluaciones de AF, que tienen juicios de relevancia a nivel de documento y no definen tareas con mÃºltiples consultas, son insuficientes para evaluar el nuevo enfoque. Por lo tanto, ampliamos un corpus de referencia, la colecciÃ³n TDT4 de noticias y transmisiones de televisiÃ³n, con definiciones de tareas, mÃºltiples consultas por tarea y claves de respuestas por consulta. Hemos realizado nuestros experimentos en este corpus extendido TDT4 y hemos puesto a disposiciÃ³n del pÃºblico los datos generados adicionalmente para futuras evaluaciones comparativas. Para evaluar automÃ¡ticamente los fragmentos de texto arbitrarios devueltos por el sistema utilizando nuestras claves de respuestas, desarrollamos un esquema de evaluaciÃ³n con un procedimiento semiautomÃ¡tico para 1 URL: http://nyc.lti.cs.cmu.edu/downloads adquiriendo reglas que pueden comparar fragmentos con las respuestas del sistema. AdemÃ¡s, proponemos una extensiÃ³n de NDCG (Normalized Discounted Cumulated Gain) [9] para evaluar la utilidad de los pasajes clasificados como una funciÃ³n tanto de la relevancia como de la novedad. El resto de este documento estÃ¡ organizado de la siguiente manera. La secciÃ³n 2 describe el proceso de destilaciÃ³n de informaciÃ³n con un ejemplo concreto. La secciÃ³n 3 describe los nÃºcleos tÃ©cnicos de nuestro sistema llamado CAFÂ´E - CMU Adaptive Filtering Engine. La secciÃ³n 4 discute problemas con respecto a la metodologÃ­a de evaluaciÃ³n y propone un nuevo esquema. La secciÃ³n 5 describe el corpus extendido TDT4. La secciÃ³n 6 presenta nuestros experimentos y resultados. La secciÃ³n 7 concluye el estudio y ofrece perspectivas futuras. 2. Una TAREA DE EJEMPLO Considera un evento noticioso: la fuga de siete convictos de una prisiÃ³n de Texas en diciembre de 2000 y su captura un mes despuÃ©s. Suponiendo que un usuario estuviera interesado en este evento desde sus primeras etapas, la necesidad de informaciÃ³n podrÃ­a ser: Encontrar informaciÃ³n sobre la fuga de convictos de una prisiÃ³n de Texas, y la informaciÃ³n relacionada con su recaptura. Las preguntas de nivel inferior asociadas podrÃ­an ser: 1. Â¿CuÃ¡ntos prisioneros escaparon? 2. Â¿DÃ³nde y cuÃ¡ndo fueron avistados? 3. Â¿QuiÃ©nes son sus contactos conocidos dentro y fuera de la prisiÃ³n? 4. Â¿CÃ³mo estÃ¡n armados? 5. Â¿Tienen algÃºn vehÃ­culo? 6. Â¿QuÃ© pasos se han tomado hasta ahora? Llamamos a esta necesidad de informaciÃ³n una tarea, y a las preguntas asociadas como las consultas en esta tarea. Un sistema de destilaciÃ³n debe monitorear los documentos entrantes, procesarlos por fragmentos en un orden temporal, seleccionar pasajes potencialmente relevantes y novedosos de cada fragmento con respecto a cada consulta, y presentar una lista clasificada de pasajes al usuario. La <br>clasificaciÃ³n de pasajes</br> aquÃ­ se basa en cuÃ¡n relevante es un pasaje con respecto a la consulta actual, cuÃ¡n novedoso es con respecto al historial actual del usuario (de sus interacciones con el sistema) y cuÃ¡n redundante es en comparaciÃ³n con otros pasajes con un rango mÃ¡s alto en la lista. Cuando se le presenta una lista de pasajes, el usuario puede proporcionar retroalimentaciÃ³n resaltando tramos de texto arbitrarios que considere relevantes. Estos fragmentos de texto se toman como ejemplos positivos en la adaptaciÃ³n del perfil de consulta, y tambiÃ©n se aÃ±aden al historial de los usuarios. Los pasajes no marcados por el usuario se consideran ejemplos negativos. Tan pronto como se actualiza el perfil de consulta, el sistema vuelve a realizar una bÃºsqueda y devuelve otra lista clasificada de pasajes donde los pasajes previamente vistos son eliminados o clasificados como bajos, segÃºn la preferencia del usuario. Por ejemplo, si el usuario resalta ...los funcionarios han ofrecido una recompensa de $100,000 por su captura... como respuesta relevante a la consulta Â¿QuÃ© pasos se han tomado hasta ahora?, entonces la pieza resaltada se utiliza como un ejemplo de entrenamiento positivo adicional en la adaptaciÃ³n del perfil de la consulta. Este comentario tambiÃ©n se aÃ±ade al historial del usuario como un ejemplo visto, para que en el futuro, el sistema no coloque otro pasaje mencionando una recompensa de $100,000 en la parte superior de la lista clasificada. Sin embargo, un artÃ­culo que mencione que los funcionarios han duplicado el dinero de recompensa a $200,000 podrÃ­a clasificarse alto ya que es relevante tanto para el perfil de consulta (actualizado) como novedoso con respecto a la historia del usuario (actualizada). El usuario puede modificar las consultas originales o agregar una nueva consulta durante el proceso; los perfiles de consulta se cambiarÃ¡n en consecuencia. Claramente, la detecciÃ³n de novedades es muy importante para la utilidad de dicho sistema debido a la bÃºsqueda iterativa. Sin detecciÃ³n de novedad, los antiguos pasajes relevantes se mostrarÃ­an al usuario repetidamente en cada lista clasificada. A travÃ©s del ejemplo anterior, podemos ver las principales propiedades de nuestro nuevo marco de trabajo para la destilaciÃ³n de informaciÃ³n basada en utilidad sobre documentos ordenados temporalmente. Nuestro marco de trabajo combina y amplÃ­a el poder del filtrado adaptativo (AF), la recuperaciÃ³n ad-hoc (IR) y la detecciÃ³n de novedades (ND). En comparaciÃ³n con la IR estÃ¡ndar, nuestro enfoque tiene la capacidad de aprender de forma incremental las necesidades de informaciÃ³n a largo plazo y modelar una secuencia de consultas dentro de una tarea. En comparaciÃ³n con el AF convencional, permite un papel mÃ¡s activo del usuario en refinar sus necesidades de informaciÃ³n y solicitar nuevos resultados al permitir comentarios sobre relevancia y novedad mediante el resaltado de fragmentos arbitrarios de texto en los pasajes devueltos por el sistema. Comparado con trabajos anteriores, esta es la primera evaluaciÃ³n de detecciÃ³n de novedad integrada con filtrado adaptativo para consultas secuenciadas que permite retroalimentaciÃ³n flexible del usuario sobre pasajes clasificados. La combinaciÃ³n de AF, IR y ND con las nuevas extensiones plantea una importante pregunta de investigaciÃ³n sobre la metodologÃ­a de evaluaciÃ³n: Â¿cÃ³mo podemos medir la utilidad de un sistema de destilaciÃ³n de informaciÃ³n como este? Las mÃ©tricas existentes en IR estÃ¡ndar, AF y ND son insuficientes, y se deben explorar nuevas soluciones, como discutiremos en la SecciÃ³n 4, despuÃ©s de describir los nÃºcleos tÃ©cnicos de nuestro sistema en la prÃ³xima secciÃ³n. 3. NÃCLEOS TÃCNICOS Los componentes principales de CAFÃ son: 1) AF para el aprendizaje incremental de perfiles de consulta, 2) IR para estimar la relevancia de pasajes con respecto a los perfiles de consulta, 3) ND para evaluar la novedad de pasajes con respecto al historial de usuarios, y 4) un componente anti-redundancia para eliminar la redundancia de las listas clasificadas. 3.1 Componente de Filtrado Adaptativo Utilizamos un algoritmo de vanguardia en el campo, el mÃ©todo de regresiÃ³n logÃ­stica regularizada que obtuvo los mejores resultados en varios corpus de evaluaciÃ³n de referencia para AF [21]. La regresiÃ³n logÃ­stica (LR) es un algoritmo de aprendizaje supervisado para clasificaciÃ³n estadÃ­stica. BasÃ¡ndose en un conjunto de entrenamiento de instancias etiquetadas, aprende un modelo de clase que luego puede ser utilizado para predecir las etiquetas de instancias no vistas. Su rendimiento, asÃ­ como su eficiencia en tÃ©rminos de tiempo de entrenamiento, lo convierten en un buen candidato cuando se requieren actualizaciones frecuentes del modelo de clase, como es el caso en el filtrado adaptativo, donde el sistema debe aprender de cada nueva retroalimentaciÃ³n proporcionada por el usuario. (Consulte [21] y [23] para complejidad computacional y problemas de implementaciÃ³n). En el filtrado adaptativo, cada consulta se considera como una clase y la probabilidad de que un pasaje pertenezca a esta clase corresponde al grado de relevancia del pasaje con respecto a la consulta. Para entrenar el modelo, utilizamos la propia consulta como ejemplo inicial de entrenamiento positivo de la clase, y las piezas de texto resaltadas por el usuario (marcadas como Relevante o No relevante) durante la retroalimentaciÃ³n como ejemplos de entrenamiento adicionales. Para abordar el problema de inicio en frÃ­o en la etapa inicial antes de obtener cualquier retroalimentaciÃ³n del usuario, el sistema utiliza una pequeÃ±a muestra de un corpus retrospectivo como ejemplos negativos iniciales en el conjunto de entrenamiento. Los detalles de cÃ³mo utilizar la regresiÃ³n logÃ­stica para el filtrado adaptativo (asignando diferentes pesos a las instancias de entrenamiento positivas y negativas, y regularizando la funciÃ³n objetivo para prevenir el sobreajuste en los datos de entrenamiento) se presentan en [21]. El modelo de clase wâ aprendido por RegresiÃ³n LogÃ­stica, o el perfil de consulta, es un vector cuyas dimensiones son tÃ©rminos individuales y cuyos elementos son los coeficientes de regresiÃ³n, indicando cuÃ¡n influyente es cada tÃ©rmino en el perfil de consulta. El perfil de la consulta se actualiza cada vez que se recibe una nueva pieza de retroalimentaciÃ³n del usuario. Un peso decaÃ­do temporalmente se puede aplicar a cada ejemplo de entrenamiento, como una opciÃ³n, para enfatizar la retroalimentaciÃ³n mÃ¡s reciente del usuario. 3.2 Componente de RecuperaciÃ³n de Pasajes Utilizamos tÃ©cnicas estÃ¡ndar de IR en esta parte de nuestro sistema. Los documentos entrantes se procesan por bloques, donde cada bloque puede definirse como un intervalo de tiempo fijo o como un nÃºmero fijo de documentos, segÃºn lo prefiera el usuario. Para cada documento entrante, se actualizan las estadÃ­sticas del corpus como el IDF (Frecuencia Inversa de Documento) de cada tÃ©rmino. Utilizamos un identificador y rastreador de entidades con tecnologÃ­a de punta [8, 12] para identificar nombres de personas y lugares, y fusionarlos con entidades con referencia cruzada vistas en el pasado. Luego, los documentos se dividen en pasajes, que pueden ser un documento completo, un pÃ¡rrafo, una oraciÃ³n, o cualquier otro fragmento continuo de texto, segÃºn se prefiera. Cada pasaje se representa utilizando un vector de pesos TF-IDF (Frecuencia de TÃ©rmino - Frecuencia Inversa de Documento), donde el tÃ©rmino puede ser una palabra o una entidad nombrada. Dado un perfil de consulta, es decir, la soluciÃ³n de regresiÃ³n logÃ­stica wâ como se describe en la SecciÃ³n 3.1, el sistema calcula la probabilidad posterior de relevancia para cada pasaje x como fRL(x) â¡ P(y = 1|x, wâ ) = 1 (1 + eâwâÂ·x) (1). Los pasajes se ordenan segÃºn sus puntajes de relevancia, y aquellos con puntajes por encima de un umbral (ajustado en un conjunto de entrenamiento) conforman la lista de relevancia que se pasa al paso de detecciÃ³n de novedad. 3.3 Componente de DetecciÃ³n de Novedad CAFÂ´E mantiene un historial de usuario H(t), que contiene todos los fragmentos de texto hi que el usuario resaltÃ³ (como retroalimentaciÃ³n) durante sus interacciones pasadas con el sistema, hasta el tiempo actual t. Denotando el historial como H(t) = n h1, h2, ..., ht o , (2), el puntaje de novedad de un nuevo pasaje candidato x se calcula como: fND(x) = 1 â max iâ1..t {cos(x, hi)} (3), donde tanto el pasaje candidato x como los fragmentos de texto resaltados hi se representan como vectores TF-IDF. El puntaje de novedad de cada pasaje se compara con un umbral predefinido (ajustado tambiÃ©n en un conjunto de entrenamiento), y cualquier pasaje con un puntaje por debajo de este umbral se elimina de la lista de relevancia. Componente de clasificaciÃ³n anti-redundante Aunque el componente de detecciÃ³n de novedad garantiza que solo la informaciÃ³n novedosa (no vista previamente) permanezca en la lista de relevancia, esta lista aÃºn podrÃ­a contener la misma informaciÃ³n novedosa en mÃºltiples posiciones en la lista clasificada. Supongamos, por ejemplo, que el usuario ya ha leÃ­do sobre una recompensa de $100,000 por informaciÃ³n sobre los convictos fugados. Una nueva noticia de que el premio ha sido aumentado a $200,000 es novedosa ya que el usuario aÃºn no la ha leÃ­do. Sin embargo, mÃºltiples fuentes de noticias podrÃ­an informar sobre esta noticia y podrÃ­amos terminar mostrando artÃ­culos (redundantes) de todas estas fuentes en una lista clasificada. Por lo tanto, una lista clasificada tambiÃ©n debe ser no redundante con respecto a su propio contenido. Utilizamos una versiÃ³n simplificada del mÃ©todo de MÃ¡xima Relevancia Marginal [5], originalmente desarrollado para combinar relevancia y novedad en la recuperaciÃ³n de texto y resÃºmenes. Nuestro procedimiento comienza con la lista actual de pasajes ordenados por relevancia (secciÃ³n 3.2), filtrados por el componente de DetecciÃ³n de Novedades (secciÃ³n 3.3), y genera una nueva lista no redundante de la siguiente manera: 1. Toma el pasaje superior en la lista actual como el primero en la nueva lista. 2. Agrega el siguiente pasaje x en la lista actual a la nueva lista solo si fAR(x) > t donde fAR(x) = 1 â max piâLnew {cos(x, pi)} y Lnew es el conjunto de pasajes ya seleccionados en la nueva lista. Repetir el paso 2 hasta que se hayan examinado todos los pasajes en la lista actual. DespuÃ©s de aplicar el algoritmo mencionado anteriormente, cada pasaje en la nueva lista es lo suficientemente diferente a los demÃ¡s, favoreciendo asÃ­ la diversidad en lugar de la redundancia en la nueva lista clasificada. El umbral anti-redundancia t se ajusta en un conjunto de entrenamiento. 4. METODOLOGÃA DE EVALUACIÃN El enfoque que propusimos anteriormente para la destilaciÃ³n de informaciÃ³n plantea importantes cuestiones sobre la metodologÃ­a de evaluaciÃ³n. En primer lugar, dado que nuestro marco de trabajo permite que la salida sean fragmentos en diferentes niveles de granularidad (por ejemplo, ventanas de k oraciones donde k puede variar) en lugar de una longitud fija, no es posible tener juicios de relevancia preanotados en todos esos niveles de granularidad. En segundo lugar, dado que deseamos medir la utilidad de la salida del sistema como una combinaciÃ³n de relevancia y novedad, las medidas tradicionales basadas Ãºnicamente en la relevancia deben ser reemplazadas por medidas que penalicen la repeticiÃ³n de la misma informaciÃ³n en la salida del sistema a lo largo del tiempo. En tercer lugar, dado que la salida del sistema son listas clasificadas, debemos recompensar a aquellos sistemas que presenten informaciÃ³n Ãºtil (tanto relevante como no vista previamente) utilizando listas clasificadas mÃ¡s cortas, y penalizar a aquellos que presenten la misma informaciÃ³n utilizando listas clasificadas mÃ¡s largas. Ninguna de las medidas existentes en la recuperaciÃ³n ad-hoc, filtrado adaptativo, detecciÃ³n de novedades u otras Ã¡reas relacionadas (resumen de texto y respuesta a preguntas) tiene propiedades deseables en los tres aspectos. Por lo tanto, debemos desarrollar una nueva metodologÃ­a de evaluaciÃ³n. 4.1 Claves de Respuesta Para permitir la evaluaciÃ³n de un sistema cuya salida consiste en pasajes de longitud arbitraria, tomamos prestado el concepto de claves de respuesta de la comunidad de Preguntas y Respuestas (QA), donde los sistemas pueden devolver fragmentos de texto arbitrarios como respuestas. Las claves de respuestas definen lo que debe estar presente en la respuesta de un sistema para recibir crÃ©dito, y estÃ¡n compuestas por una colecciÃ³n de \"nuggets\" de informaciÃ³n, es decir, unidades de datos sobre las cuales los evaluadores humanos pueden tomar decisiones binarias sobre si una respuesta del sistema las contiene o no. La definiciÃ³n de claves de respuestas y la toma de decisiones binarias asociadas son tareas conceptuales que requieren un mapeo semÃ¡ntico [19], ya que los pasajes devueltos por el sistema pueden contener la misma informaciÃ³n expresada de muchas formas diferentes. Por lo tanto, las evaluaciones de QA han dependido de evaluadores humanos para la correspondencia entre diversas expresiones, lo que hace que el proceso sea costoso, consume mucho tiempo y no sea escalable para grandes colecciones de consultas y documentos, y evaluaciones extensas de sistemas con diversos ajustes de parÃ¡metros. 4.1.1 AutomatizaciÃ³n de la evaluaciÃ³n basada en claves de respuestas Los mÃ©todos de evaluaciÃ³n automÃ¡tica permitirÃ­an una construcciÃ³n y ajuste mÃ¡s rÃ¡pidos del sistema, asÃ­ como proporcionar una forma objetiva y asequible de comparar diversos sistemas. Recientemente, se han propuesto mÃ©todos basados mÃ¡s o menos en la idea de las co-ocurrencias de n-gramos. Pourpre [10] asigna una puntuaciÃ³n de recuerdo fraccional a una respuesta del sistema basada en su superposiciÃ³n de unigramas con una descripciÃ³n de pepitas dada. Por ejemplo, una respuesta del sistema A B C tiene una recuperaciÃ³n de 3/4 con respecto a un fragmento con la descripciÃ³n A B C D. Sin embargo, este enfoque es injusto para los sistemas que presentan la misma informaciÃ³n pero utilizando palabras distintas a A, B, C y D. Otro problema abierto es cÃ³mo ponderar las palabras individuales al medir la cercanÃ­a de una coincidencia. Por ejemplo, considera la pregunta Â¿CuÃ¡ntos prisioneros escaparon? En el fragmento Siete prisioneros escaparon de una prisiÃ³n de Texas, no hay indicaciÃ³n de que siete sea la palabra clave y que debe coincidir para obtener algÃºn crÃ©dito de relevancia. El uso de los valores de IDF no ayuda, ya que la palabra \"seven\" generalmente no tendrÃ¡ un IDF mÃ¡s alto que palabras como \"texas\" y \"prison\". AdemÃ¡s, redefinir el fragmento como simplemente siete no resuelve el problema, ya que ahora podrÃ­a coincidir errÃ³neamente con cualquier menciÃ³n de siete fuera de contexto. Nuggeteer [13] trabaja sobre principios similares pero toma decisiones binarias sobre si hay un fragmento en una respuesta de sistema dada ajustando un umbral. Sin embargo, tambiÃ©n estÃ¡ plagado de relevancia espuria ya que no todas las palabras contenidas en la descripciÃ³n de la pepita (o respuestas correctas conocidas) son centrales para la pepita. 4.1.2 Reglas de Coincidencia de Pepitas Proponemos un mÃ©todo automÃ¡tico confiable para determinar si un fragmento de texto contiene una pepita dada, basado en reglas de coincidencia de pepitas, que se generan utilizando un procedimiento semiautomÃ¡tico explicado a continuaciÃ³n. Estas reglas son bÃ¡sicamente consultas booleanas que solo coincidirÃ¡n con fragmentos que contengan la pepita. Por ejemplo, una regla candidata para emparejar respuestas a Â¿CuÃ¡ntos prisioneros escaparon? es (Texas Y siete Y escapar Y (convictos O prisioneros)), posiblemente con otros sinÃ³nimos y variantes en la regla. Para un corpus de artÃ­culos de noticias, que suelen seguir una prosa formal tÃ­pica, es bastante fÃ¡cil escribir reglas simples para coincidir con respuestas esperadas utilizando un enfoque de arranque, como se describe a continuaciÃ³n. Proponemos un enfoque de dos etapas, inspirado en Autoslog [14], que combina la capacidad de los humanos para identificar expresiones semÃ¡nticamente equivalentes y la capacidad del sistema para recopilar evidencia estadÃ­stica de un corpus de documentos anotados por humanos. En la primera etapa, los sujetos humanos anotaron (utilizando una herramienta de resaltado) porciones de documentos relevantes que contenÃ­an respuestas a cada fragmento 2. En la segunda etapa, los sujetos utilizaron nuestra herramienta de generaciÃ³n de reglas para crear reglas que coincidieran con las anotaciones de cada fragmento. La herramienta permite a los usuarios ingresar una regla booleana como una disyunciÃ³n de conjunciones (por ejemplo, ((a Y b) O (a Y c Y d) O (e))). Dada una regla candidata, nuestra herramienta la utiliza como una consulta booleana sobre el conjunto completo de documentos relevantes y calcula su recall y precisiÃ³n con respecto a las anotaciones que se espera que coincida. Por lo tanto, los sujetos pueden comenzar con una regla simple y refinarla iterativamente hasta que estÃ©n satisfechos con su precisiÃ³n y recuperaciÃ³n. Observamos que era muy fÃ¡cil para los humanos mejorar la precisiÃ³n de una regla ajustando sus conjunciones existentes (agregando mÃ¡s Y) y mejorar la recuperaciÃ³n agregando mÃ¡s conjunciones a la disyunciÃ³n (agregando mÃ¡s O). Como ejemplo, intentemos crear una regla para el acertijo que dice que siete prisioneros escaparon de la prisiÃ³n de Texas. Empezamos con una regla simple - (siete). Cuando introducimos esto en la herramienta de generaciÃ³n de reglas, nos damos cuenta de que esta regla coincide con muchas ocurrencias espurias de siete (por ejemplo, ...siete estados...) y, por lo tanto, obtiene una puntuaciÃ³n baja de precisiÃ³n. Podemos calificar aÃºn mÃ¡s nuestra regla - Texas Y siete Y convictos. A continuaciÃ³n, al revisar las anotaciones omitidas, nos damos cuenta de que algunos artÃ­culos de noticias mencionaban ...siete prisioneros escaparon.... Luego reemplazamos convictos por la disyunciÃ³n (convictos O prisioneros). Continuamos ajustando la regla de esta manera hasta lograr un nivel de recordaciÃ³n y precisiÃ³n suficientemente alto, es decir, que los (pocos) errores y alarmas falsas se puedan ignorar de forma segura. AsÃ­ podemos crear reglas de coincidencia de pepitas que capturen de manera sucinta varias formas de expresar una pepita, evitando al mismo tiempo emparejar respuestas incorrectas (o fuera de contexto). La participaciÃ³n humana en el proceso de creaciÃ³n de reglas garantiza reglas genÃ©ricas de alta calidad que luego se pueden utilizar para evaluar de manera confiable las respuestas del sistema arbitrario. 4.2 EvaluaciÃ³n de la Utilidad de una Secuencia de Listas Clasificadas La utilidad de un sistema de recuperaciÃ³n se puede definir como la diferencia entre cuÃ¡nta informaciÃ³n Ãºtil obtuvo el usuario y cuÃ¡nto tiempo y energÃ­a perdiÃ³. Calculamos esta utilidad a partir de las utilidades de los pasajes individuales de la siguiente manera. DespuÃ©s de leer cada pasaje devuelto por el sistema, el usuario obtiene cierta ganancia dependiendo de la presencia de informaciÃ³n relevante y novedosa, y incurre en una pÃ©rdida en tÃ©rminos del tiempo y la energÃ­a invertidos en revisar el pasaje. Sin embargo, la probabilidad de que el usuario realmente lea un pasaje depende de su posiciÃ³n en la lista clasificada. Por lo tanto, para una consulta q, la utilidad esperada 2 LDC [18] ya proporciona juicios de relevancia para 100 temas en el corpus TDT4. AdemÃ¡s, nos aseguramos de que estos juicios sean exhaustivos en todo el corpus utilizando la agrupaciÃ³n. El valor de un pasaje pi en el rango i se puede definir como U(pi, q) = P(i) â (Ganancia(pi, q) - PÃ©rdida(pi, q)) (4) donde P(i) es la probabilidad de que el usuario pase por un pasaje en el rango i. La utilidad esperada para una lista clasificada completa de longitud n se puede calcular simplemente sumando la utilidad esperada de cada pasaje: U(q) = Î£ i=1 P(i) â (Ganancia(pi, q) â PÃ©rdida(pi, q)) (5) NÃ³tese que si ignoramos el tÃ©rmino de pÃ©rdida y definimos P(i) como P(i) â 1/ logb(b + i â 1) (6) entonces obtenemos la mÃ©trica recientemente popularizada llamada Ganancia Acumulada Descontada (DCG) [9], donde Ganancia(pi, q) se define como la relevancia graduada del pasaje pi. Sin embargo, sin el tÃ©rmino de pÃ©rdida, DCG es una mÃ©trica puramente orientada al recuerdo y no es adecuada para un entorno de filtrado adaptativo, donde la utilidad del sistema depende en parte de su capacidad para limitar el nÃºmero de elementos mostrados al usuario. Aunque P(i) podrÃ­a definirse en base a estudios empÃ­ricos del comportamiento del usuario, por simplicidad, utilizamos P(i) tal como se define exactamente en la ecuaciÃ³n 6. La ganancia G(pi, q) del pasaje pi con respecto a la consulta q es una funciÃ³n de - 1) el nÃºmero de pepitas relevantes presentes en pi, y 2) la novedad de cada una de estas pepitas. Combinamos estos dos factores de la siguiente manera. Para cada pepita Nj, asignamos un peso inicial wj, y tambiÃ©n mantenemos un conteo nj del nÃºmero de veces que esta pepita ha sido vista por el usuario en el pasado. La ganancia derivada de cada ocurrencia posterior del mismo hallazgo se asume que se reduce por un factor de amortiguamiento Î³. Por lo tanto, G(pi, q) se define como G(pi, q) = X Nj âC(pi,q) wj â Î³nj (7) donde C(pi, q) es el conjunto de todas las pepitas que aparecen en el pasaje pi y tambiÃ©n pertenecen a la clave de respuesta de la consulta q. Los pesos iniciales wj se establecen todos en 1.0 en nuestros experimentos, pero tambiÃ©n pueden establecerse segÃºn un enfoque de pirÃ¡mide [11]. La elecciÃ³n del factor de amortiguaciÃ³n Î³ determina la tolerancia del usuario a la redundancia. Cuando Î³ = 0, un pepita solo recibirÃ¡ crÃ©dito por su primera ocurrencia, es decir, cuando nj sea cero. Para 0 < Î³ < 1, un nugget recibe menos crÃ©dito por cada ocurrencia sucesiva. Cuando Î³ = 1, no se produce amortiguamiento y las repeticiones de un valor reciben el mismo crÃ©dito. Se debe tener en cuenta que los recuentos de ocurrencia de las pepitas se conservan entre la evaluaciÃ³n de las listas clasificadas sucesivas devueltas por el sistema, ya que se espera que los usuarios recuerden lo que el sistema les mostrÃ³ en el pasado. Definimos la pÃ©rdida L(pi, q) como un costo constante c (usamos 0.1) incurrido al leer un pasaje devuelto por el sistema. Por lo tanto, nuestra mÃ©trica puede ser reescrita como U(q) = nX i=1 Ganancia(pi, q) logb(b + i â 1) â L(n) (8) donde L(n) es la pÃ©rdida asociada con una lista clasificada de longitud n: L(n) = c Â· nX i=1 1 logb(b + i â 1) (9) 3 NÃ³tese que 00 = 1 Debido a la similitud con la Ganancia Acumulada Descontada (DCG), llamamos a nuestra mÃ©trica Utilidad Acumulada Descontada (DCU). El puntaje de DCU obtenido por el sistema se convierte en un puntaje de DCU normalizado (NDCU) dividiÃ©ndolo por el puntaje de DCU de la lista ideal clasificada, que se crea ordenando los pasajes por sus puntajes de utilidad decrecientes U(pi, q) y deteniÃ©ndose cuando U(pi, q) â¤ 0, es decir, cuando la ganancia es menor o igual al costo de leer el pasaje. El conjunto de datos TDT4 fue el corpus de referencia utilizado en las evaluaciones TDT2002 y TDT2003. El corpus consiste en mÃ¡s de 90,000 artÃ­culos de noticias de mÃºltiples fuentes (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) publicados entre octubre de 2000 y enero de 2001, en los idiomas Ã¡rabe, inglÃ©s y mandarÃ­n. Las versiones de los artÃ­culos en otros idiomas reconocidas por voz y traducidas por mÃ¡quina tambiÃ©n fueron proporcionadas. LDC [18] ha anotado el corpus con 100 temas, que corresponden a varios eventos de noticias en este perÃ­odo de tiempo. De entre estos, seleccionamos un subconjunto de 12 eventos accionables y definimos tareas correspondientes para ellos. Para cada tarea, definimos manualmente un perfil que consiste en un conjunto inicial de (5 a 10) consultas, una descripciÃ³n de texto libre del historial del usuario, es decir, lo que el usuario ya sabe sobre el evento, y una lista de documentos conocidos relevantes y no relevantes (si estÃ¡n disponibles) como ejemplos de entrenamiento. Para cada consulta, generamos claves de respuestas y reglas de coincidencia de pepitas correspondientes utilizando el procedimiento descrito en la secciÃ³n 4.1.2, y produjimos un total de 120 consultas, con un promedio de 7 pepitas por consulta. EXPERIMENTOS Y RESULTADOS 6.1 Baselines Utilizamos Indri [17], un motor de bÃºsqueda basado en modelos de lenguaje popular, como referencia para comparar con CAFÃ. Indri admite la funcionalidad estÃ¡ndar de los motores de bÃºsqueda, incluido el feedback de pseudo-relevancia (PRF) [3, 6], y es representativo de un sistema de recuperaciÃ³n basado en consultas tÃ­pico. Indri no admite ningÃºn tipo de detecciÃ³n de novedades. Comparamos Indri con PRF activado y desactivado, frente a CAFÃ con retroalimentaciÃ³n del usuario, detecciÃ³n de novedades y clasificaciÃ³n antirredundante activados y desactivados. 6.2 ConfiguraciÃ³n Experimental Dividimos el corpus TDT4 que abarca 4 meses en 10 fragmentos, cada uno definido como un perÃ­odo de 12 dÃ­as consecutivos. En cualquier momento dado del proceso de destilaciÃ³n, cada sistema accedÃ­a a los datos pasados hasta el punto actual, y devolvÃ­a una lista clasificada de hasta 50 pasajes por consulta. Las 12 tareas definidas en el corpus se dividieron en un conjunto de entrenamiento y prueba con 6 tareas cada uno. Cada sistema pudo utilizar el conjunto de entrenamiento para ajustar sus parÃ¡metros con el fin de optimizar NDCU (ecuaciÃ³n 8), incluyendo el umbral de relevancia tanto para Indri como para CAFÂ´E, y los umbrales de novedad y antirredundancia para CAFÂ´E. La NDCU para cada ejecuciÃ³n del sistema se calcula automÃ¡ticamente. TambiÃ©n se simulÃ³ la retroalimentaciÃ³n de los usuarios: los juicios de relevancia para cada pasaje devuelto por el sistema (segÃºn las reglas de coincidencia de pepitas descritas en la secciÃ³n 4.1.2) fueron utilizados como retroalimentaciÃ³n de los usuarios en la adaptaciÃ³n de perfiles de consulta e historiales de usuario. En la Tabla 1, mostramos los puntajes NDCU de los dos sistemas en diferentes configuraciones. Estas puntuaciones se promedian en las seis tareas del conjunto de pruebas, y se calculan con dos factores de amortiguaciÃ³n (ver secciÃ³n 4.2): Î³ = 0 y 0.1, para simular ninguna tolerancia y poca tolerancia para la redundancia, respectivamente. Usar Î³ = 0 crea una mÃ©trica mucho mÃ¡s estricta ya que no otorga ningÃºn crÃ©dito a un pasaje que contenga informaciÃ³n relevante pero redundante. Por lo tanto, la mejora obtenida al habilitar la retroalimentaciÃ³n del usuario es menor con Î³ = 0 que la mejora obtenida de la retroalimentaciÃ³n con Î³ = 0.1. Esto revela una deficiencia de los sistemas de recuperaciÃ³n contemporÃ¡neos: cuando el usuario da retroalimentaciÃ³n positiva sobre un pasaje, los sistemas otorgan mayor peso a los tÃ©rminos presentes en ese pasaje y tienden a recuperar otros pasajes que contienen los mismos tÃ©rminos, y por lo tanto, generalmente la misma informaciÃ³n. Sin embargo, el usuario no se beneficia al ver pasajes redundantes y generalmente estÃ¡ interesado en otros pasajes que contengan informaciÃ³n relacionada. Es informativo evaluar los sistemas de recuperaciÃ³n utilizando nuestra medida de utilidad (con Î³ = 0) que tiene en cuenta la novedad y, por lo tanto, ofrece una imagen mÃ¡s realista de cuÃ¡n bien un sistema puede generalizar a partir de la retroalimentaciÃ³n del usuario, en lugar de utilizar medidas tradicionales de RI como la recuperaciÃ³n y la precisiÃ³n, que ofrecen una imagen incompleta de la mejora obtenida a partir de la retroalimentaciÃ³n del usuario. A veces, sin embargo, los usuarios podrÃ­an estar interesados en ver la misma informaciÃ³n de mÃºltiples fuentes, como un indicador de su importancia o fiabilidad. En tal caso, simplemente podemos elegir un valor mÃ¡s alto para Î³ que corresponda a una mayor tolerancia para la redundancia, y asÃ­ permitir que el sistema ajuste sus parÃ¡metros en consecuencia. Dado que los documentos fueron procesados por fragmentos, serÃ­a interesante ver cÃ³mo mejora el rendimiento de los sistemas con el tiempo. Las figuras 1 y 2 muestran las tendencias de rendimiento de ambos sistemas a lo largo de los fragmentos. Si bien se espera que el rendimiento con y sin retroalimentaciÃ³n en los primeros fragmentos sea similar, para los fragmentos posteriores, la curva de rendimiento con la retroalimentaciÃ³n habilitada supera a la del ajuste sin retroalimentaciÃ³n. Las tendencias de rendimiento no son consistentes en todos los fragmentos porque los documentos relevantes no estÃ¡n distribuidos uniformemente en todos los fragmentos, lo que hace que algunas consultas sean mÃ¡s fÃ¡ciles que otras en ciertos fragmentos. AdemÃ¡s, dado que Indri utiliza retroalimentaciÃ³n de pseudo-relevancia mientras que CAFÂ´E utiliza retroalimentaciÃ³n basada en juicios de relevancia reales, la mejora en el caso de Indri es menos dramÃ¡tica que la de CAFÂ´E. 7. CONCLUSIONES FINALES Este artÃ­culo presenta la primera investigaciÃ³n sobre la destilaciÃ³n de informaciÃ³n basada en la utilidad con un sistema que aprende las necesidades de informaciÃ³n duraderas a partir de la retroalimentaciÃ³n detallada del usuario sobre una secuencia de pasajes clasificados. Nuestro sistema, llamado CAFÂ´E, combina filtrado adaptativo, detecciÃ³n de novedades y <br>clasificaciÃ³n de pasajes</br> antirredundantes en un marco unificado para la optimizaciÃ³n de la utilidad. Desarrollamos un nuevo esquema para la evaluaciÃ³n y retroalimentaciÃ³n automatizadas basado en un procedimiento semiautomÃ¡tico para adquirir reglas que permitan emparejar automÃ¡ticamente fragmentos con las respuestas del sistema. TambiÃ©n propusimos una extensiÃ³n de la mÃ©trica NDCG para evaluar la utilidad de los pasajes clasificados como una combinaciÃ³n ponderada de relevancia y novedad. Nuestros experimentos en el reciÃ©n anotado corpus de referencia TDT4 muestran un alentador aumento de utilidad sobre Indri, y tambiÃ©n sobre nuestro propio sistema con aprendizaje incremental y detecciÃ³n de novedades desactivados. 8. AGRADECIMIENTOS Nos gustarÃ­a agradecer a Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng y al Programa de AnÃ¡lisis de Datos Cualitativos de la Universidad de Pittsburgh liderado por el Dr. Stuart Shulman por su ayuda en la recolecciÃ³n y procesamiento de las anotaciones extendidas de TDT4 utilizadas en nuestros experimentos. Este trabajo estÃ¡ parcialmente respaldado por la FundaciÃ³n Nacional de Ciencias (NSF) bajo la subvenciÃ³n IIS0434035, y la Agencia de Proyectos de InvestigaciÃ³n Avanzada de Defensa (DARPA) bajo los contratos NBCHD030010 y W0550432. Cualquier opiniÃ³n, hallazgo, conclusiÃ³n o recomendaciÃ³n expresada en este material son responsabilidad de los autores y no reflejan necesariamente los puntos de vista de los patrocinadores. AUTORES ADICIONALES Jian Zhang (jianzhan@stat.purdue.edu)â , Jaime Carbonell (jgc@cs.cmu.edu)â  , Peter Brusilovsky (peterb+@pitt.edu)â¡ , Daqing He(dah44@pitt.edu)â¡ 10. REFERENCIAS [1] J. Allan. RetroalimentaciÃ³n incremental de relevancia para la filtraciÃ³n de informaciÃ³n. Actas de la 19Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 270-278, 1996. [2] J. Allan, C. Wade y A. Bolivar. RecuperaciÃ³n y DetecciÃ³n de Novedad a Nivel de OraciÃ³n. Actas de la conferencia ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, 2003. [3] C. Buckley, G. Salton y J. Allan. RecuperaciÃ³n automÃ¡tica con informaciÃ³n de localidad utilizando SMART. PublicaciÃ³n especial del NIST, (500207):59-72, 1993. [4] J. Callan. Aprendiendo mientras se filtran documentos. Actas de la 21Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 224-231, 1998. [5] J. Carbonell y J. Goldstein. El uso de MMR, Reordenamiento basado en Diversidad para Reorganizar Documentos y Producir ResÃºmenes. Actas de la 21Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 335-336, 1998. [6] E. Efthimiadis. ExpansiÃ³n de consulta. RevisiÃ³n Anual de Ciencia de la InformaciÃ³n y TecnologÃ­a (ARIST), 31:p121-87, 1996. [7] J. Fiscus y G. Duddington. Resumen de DetecciÃ³n y Seguimiento de Temas. DetecciÃ³n y seguimiento de temas: OrganizaciÃ³n de la informaciÃ³n basada en eventos, pÃ¡ginas 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov y S. Roukos. Un modelo estadÃ­stico para la detecciÃ³n y seguimiento de entidades multilingÃ¼es. NAACL/HLT, 2004. [9] K. JÃ¤rvelin y J. KekÃ¤lÃ¤inen. EvaluaciÃ³n basada en la Ganancia Acumulada de TÃ©cnicas de RecuperaciÃ³n de InformaciÃ³n. ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin y D. Demner-Fushman. EvaluaciÃ³n automÃ¡tica de respuestas a preguntas de definiciÃ³n. Actas de la Conferencia de TecnologÃ­a del Lenguaje Humano y Conferencia sobre MÃ©todos EmpÃ­ricos en Procesamiento del Lenguaje Natural (HLT/EMNLP 2005), 2005. â Departamento de EstadÃ­stica, Universidad de Purdue, West Lafayette, EE. UU. â  Instituto de TecnologÃ­as del Lenguaje, Universidad Carnegie Mellon, Pittsburgh, EE. UU. â¡ Escuela de Ciencias de la InformaciÃ³n, Universidad de Pittsburgh, Pittsburgh, EE. UU. [11] J. Lin y D. Demner-Fushman. Â¿Se derrumbarÃ¡n las pirÃ¡mides construidas con pepitas? Actas de HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla y S. Roukos. Un algoritmo de resoluciÃ³n de correferencia sÃ­ncrona basado en el Ã¡rbol de Bell. Actas de ACL, 4:136-143, 2004. [13] G. Marton. Nuggeteer: EvaluaciÃ³n AutomÃ¡tica Basada en Pepitas Utilizando Descripciones y Juicios. HLT/NAACL, 2006. [14] E. Riloff. ConstrucciÃ³n automÃ¡tica de un diccionario para tareas de extracciÃ³n de informaciÃ³n. Actas de la UndÃ©cima Conferencia Nacional sobre Inteligencia Artificial, pÃ¡ginas 811-816, 1993. [15] S. Robertson y S. Walker. Microsoft Cambridge en TREC-9: Pista de filtrado. La Novena Conferencia de RecuperaciÃ³n de Texto (TREC-9), pÃ¡ginas 361-368. [16] R. Schapire, Y. Cantante, y A. Singhal. Boosting y Rocchio aplicados a la filtraciÃ³n de texto. Actas de la 21Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle y W. Croft. Indri: Un motor de bÃºsqueda basado en modelos de lenguaje para consultas complejas. Actas de la Conferencia Internacional sobre AnÃ¡lisis de Inteligencia, 2004. [18] El Consorcio de Datos LingÃ¼Ã­sticos. http://www.ldc.upenn.edu/. [19] E. Voorhees. Resumen de la pista de preguntas y respuestas TREC 2003. Actas de la DuodÃ©cima Conferencia de RecuperaciÃ³n de Texto (TREC 2003), 2003. [20] Y. Yang y B. Kisiel. RegresiÃ³n local basada en mÃ¡rgenes para filtrado adaptativo. Actas de la duodÃ©cima conferencia internacional sobre gestiÃ³n de la informaciÃ³n y el conocimiento, pÃ¡ginas 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang y B. Kisiel. Robustez de los MÃ©todos de Filtrado Adaptativo en una EvaluaciÃ³n de Referencia Cruzada. Actas de la 28Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 98-105, 2005. [22] C. Zhai, W. Cohen y J. Lafferty. MÃ¡s allÃ¡ de la relevancia independiente: MÃ©todos y mÃ©tricas de evaluaciÃ³n para la recuperaciÃ³n de subtemas. Actas de la 26Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 10-17, 2003. [23] J. Zhang y Y. Yang. Robustez de los MÃ©todos de ClasificaciÃ³n Lineal Regularizados en la CategorizaciÃ³n de Textos. Actas de la 26Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 190-197, 2003. [24] Y. Zhang. Utilizando priors bayesianos para combinar clasificadores para filtrado adaptativo. Actas de la 27Âª conferencia internacional anual sobre InvestigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 345-352, 2004. [25] Y. Zhang, J. Callan y T. Minka. DetecciÃ³n de novedad y redundancia en el filtrado adaptativo. Actas de la 25Âª Conferencia Internacional Anual de ACM SIGIR sobre InvestigaciÃ³n y Desarrollo en RecuperaciÃ³n de InformaciÃ³n, 2002. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "adaptive filtering": {
            "translated_key": "filtrado adaptativo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Utility-based Information Distillation Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu ABSTRACT This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework.",
                "It combines the strengths of and extends beyond conventional <br>adaptive filtering</br>, novelty detection and non-redundant passage ranking with respect to long-lasting information needs (tasks with multiple queries).",
                "Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings.",
                "For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task.",
                "Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty.",
                "Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval.",
                "<br>adaptive filtering</br> (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics.",
                "Based on the initial query and a few positive examples (if available), an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user.",
                "The incremental learning nature of AF systems makes them more powerful than standard search engines that support ad-hoc retrieval (e.g.",
                "Google and Yahoo) in terms of finding relevant information with respect to long-lasting topics of interest, and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs, without having to modify their queries manually.",
                "A variety of supervised learning algorithms (Rocchio-style classifiers, Exponential-Gaussian models, local regression and logistic regression approaches) have been studied for adaptive settings, examined with explicit and implicit relevance feedback, and evaluated with respect to utility optimization on large benchmark data collections in TREC (Text Retrieval Conferences) and TDT (Topic Detection and Tracking) forums [1, 4, 7, 15, 16, 20, 24, 23].",
                "Regularized logistic regression [21] has been found representative for the state-of-the-art approaches, and highly efficient for frequent model adaptations over large document collections such as the TREC-10 corpus (over 800,000 documents and 84 topics).",
                "Despite substantial achievements in recent <br>adaptive filtering</br> research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently.",
                "Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications: 1.",
                "User has a rather passive role in the conventional <br>adaptive filtering</br> setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision.",
                "A more active alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents (or passages) per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists.",
                "The latter form of user interaction has been highly effective in standard retrieval for ad-hoc queries.",
                "How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2.",
                "The unit for receiving a relevance judgment (yes or no) is restricted to the document level in conventional AF.",
                "However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant.",
                "Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system.",
                "For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3.",
                "System-selected documents are often highly redundant.",
                "A major news event, for example, would be reported by multiple sources repeatedly for a while, making most of the information content in those articles redundant with each other.",
                "A conventional AF system would select all these redundant news stories for user feedback, wasting the users time while offering little gain.",
                "Clearly, techniques for novelty detection can help in principle [25, 2, 22] for improving the utility of the AF systems.",
                "However, the effectiveness of such techniques at passage level to detect novelty with respect to users (fine-grained) feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user.",
                "To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and novelty detection for removal of redundancy from user interactions with the system.",
                "We call the new process utility-based information distillation.",
                "Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach.",
                "Therefore, we extended a benchmark corpus - the TDT4 collection of news stories and TV broadcasts - with task definitions, multiple queries per task, and answer keys per query.",
                "We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 .",
                "To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses.",
                "Moreover, we propose an extension of NDCG (Normalized Discounted Cumulated Gain) [9] for assessing the utility of ranked passages as a function of both relevance and novelty.",
                "The rest of this paper is organized as follows.",
                "Section 2 outlines the information distillation process with a concrete example.",
                "Section 3 describes the technical cores of our system called CAFÂ´E - CMU <br>adaptive filtering</br> Engine.",
                "Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme.",
                "Section 5 describes the extended TDT4 corpus.",
                "Section 6 presents our experiments and results.",
                "Section 7 concludes the study and gives future perspectives. 2.",
                "A SAMPLE TASK Consider a news event - the escape of seven convicts from a Texas prison in December 2000 and their capture a month later.",
                "Assuming a user were interested in this event since its early stage, the information need could be: Find information about the escape of convicts from Texas prison, and information related to their recapture.",
                "The associated lower-level questions could be: 1.",
                "How many prisoners escaped? 2.",
                "Where and when were they sighted? 3. Who are their known contacts inside and outside the prison? 4.",
                "How are they armed? 5.",
                "Do they have any vehicles? 6.",
                "What steps have been taken so far?",
                "We call such an information need a task, and the associated questions as the queries in this task.",
                "A distillation system is supposed to monitor the incoming documents, process them chunk by chunk in a temporal order, select potentially relevant and novel passages from each chunk with respect to each query, and present a ranked list of passages to the user.",
                "Passage ranking here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list.",
                "When presented with a list of passages, the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant.",
                "These spans of text are taken as positive examples in the adaptation of the query profile, and also added to the users history.",
                "Passages not marked by the user are taken as negative examples.",
                "As soon as the query profile is updated, the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low, based on user preference.",
                "For example, if the user highlights ...officials have posted a $100,000 reward for their capture... as relevant answer to the query What steps have been taken so far?, then the highlighted piece is used as an additional positive training example in the adaptation of the query profile.",
                "This piece of feedback is also added to the user history as a seen example, so that in future, the system will not place another passage mentioning $100,000 reward at the top of the ranked list.",
                "However, an article mentioning ...officials have doubled the reward money to $200,000... might be ranked high since it is both relevant to the (updated) query profile and novel with respect to the (updated) user history.",
                "The user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly.",
                "Clearly, novelty detection is very important for the utility of such a system because of the iterative search.",
                "Without novelty detection, the old relevant passages would be shown to the user repeatedly in each ranked list.",
                "Through the above example, we can see the main properties of our new framework for utility-based information distillation over temporally ordered documents.",
                "Our framework combines and extends the power of <br>adaptive filtering</br> (AF), ad-hoc retrieval (IR) and novelty detection (ND).",
                "Compared to standard IR, our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task.",
                "Compared to conventional AF, it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system.",
                "Compared to past work, this is the first evaluation of novelty detection integrated with <br>adaptive filtering</br> for sequenced queries that allows flexible user feedback over ranked passages.",
                "The combination of AF, IR and ND with the new extensions raises an important research question regarding evaluation methodology: how can we measure the utility of such an information distillation system?",
                "Existing metrics in standard IR, AF and ND are insufficient, and new solutions must be explored, as we will discuss in Section 4, after describing the technical cores of our system in the next section. 3.",
                "TECHNICAL CORES The core components of CAFÂ´E are - 1) AF for incremental learning of query profiles, 2) IR for estimating relevance of passages with respect to query profiles, 3) ND for assessing novelty of passages with respect to users history, and 4) anti-redundancy component to remove redundancy from ranked lists. 3.1 <br>adaptive filtering</br> Component We use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for AF [21].",
                "Logistic regression (LR) is a supervised learning algorithm for statistical classification.",
                "Based on a training set of labeled instances, it learns a class model which can then by used to predict the labels of unseen instances.",
                "Its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required, as is the case in <br>adaptive filtering</br>, where the system must learn from each new feedback provided by the user. (See [21] and [23] for computational complexity and implementation issues).",
                "In <br>adaptive filtering</br>, each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query.",
                "For training the model, we use the query itself as the initial positive training example of the class, and the user-highlighted pieces of text (marked as Relevant or Not-relevant) during feedback as additional training examples.",
                "To address the cold start issue in the early stage before any user feedback is obtained, the system uses a small sample from a retrospective corpus as the initial negative examples in the training set.",
                "The details of using logistic regression for <br>adaptive filtering</br> (assigning different weights to positive and negative training instances, and regularizing the objective function to prevent over-fitting on training data) are presented in [21].",
                "The class model wâ learned by Logistic Regression, or the query profile, is a vector whose dimensions are individual terms and whose elements are the regression coefficients, indicating how influential each term is in the query profile.",
                "The query profile is updated whenever a new piece of user feedback is received.",
                "A temporally decaying weight can be applied to each training example, as an option, to emphasize the most recent user feedback. 3.2 Passage Retrieval Component We use standard IR techniques in this part of our system.",
                "Incoming documents are processed in chunks, where each chunk can be defined as a fixed span of time or as a fixed number of documents, as preferred by the user.",
                "For each incoming document, corpus statistics like the IDF (Inverted Document Frequency) of each term are updated.",
                "We use a state-of-the-art named entity identifier and tracker [8, 12] to identify person and location names, and merge them with co-referent named entities seen in the past.",
                "Then the documents are segmented into passages, which can be a whole document, a paragraph, a sentence, or any other continuous span of text, as preferred.",
                "Each passage is represented using a vector of TF-IDF (Term FrequencyInverse Document Frequency) weights, where term can be a word or a named entity.",
                "Given a query profile, i.e. the logistic regression solution wâ as described in Section 3.1, the system computes the posterior probability of relevance for each passage x as fRL(x) â¡ P(y = 1|x, wâ ) = 1 (1 + eâwâÂ·x) (1) Passages are ordered by their relevance scores, and the ones with scores above a threshold (tuned on a training set) comprise the relevance list that is passed on to the novelty detection step. 3.3 Novelty Detection Component CAFÂ´E maintains a user history H(t), which contains all the spans of text hi that the user highlighted (as feedback) during his or her past interactions with the system, up to the current time t. Denoting the history as H(t) = n h1, h2, ..., ht o , (2) the novelty score of a new candidate passage x is computed as: fND(x) = 1 â max iâ1..t {cos(x, hi)} (3) where both candidate passage x and highlighted spans of text hi are represented as TF-IDF vectors.",
                "The novelty score of each passage is compared to a prespecified threshold (also tuned on a training set), and any passage with a score below this threshold is removed from the relevance list. 3.4 Anti-redundant Ranking Component Although the novelty detection component ensures that only novel (previously unseen) information remains in the relevance list, this list might still contain the same novel information at multiple positions in the ranked list.",
                "Suppose, for example, that the user has already read about a $100,000 reward for information about the escaped convicts.",
                "A new piece of news that the award has been increased to $200,000 is novel since the user hasnt read about it yet.",
                "However, multiple news sources would report this news and we might end up showing (redundant) articles from all these sources in a ranked list.",
                "Hence, a ranked list should also be made non-redundant with respect to its own contents.",
                "We use a simplified version of the Maximal Marginal Relevance method [5], originally developed for combining relevance and novelty in text retrieval and summarization.",
                "Our procedure starts with the current list of passages sorted by relevance (section 3.2), filtered by Novelty Detection component (section 3.3), and generates a new non-redundant list as follows: 1.",
                "Take the top passage in the current list as the top one in the new list. 2.",
                "Add the next passage x in the current list to the new list only if fAR(x) > t where fAR(x) = 1 â max piâLnew {cos(x, pi)} and Lnew is the set of passages already selected in the new list. 3.",
                "Repeat step 2 until all the passages in the current list have been examined.",
                "After applying the above-mentioned algorithm, each passage in the new list is sufficiently dissimilar to others, thus favoring diversity rather than redundancy in the new ranked list.",
                "The anti-redundancy threshold t is tuned on a training set. 4.",
                "EVALUATION METHODOLOGY The approach we proposed above for information distillation raises important issues regarding evaluation methodology.",
                "Firstly, since our framework allows the output to be passages at different levels of granularity (e.g. k-sentence windows where k may vary) instead of a fixed length, it is not possible to have pre-annotated relevance judgments at all such granularity levels.",
                "Secondly, since we wish to measure the utility of the system output as a combination of both relevance and novelty, traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time.",
                "Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists.",
                "None of the existing measures in ad-hoc retrieval, <br>adaptive filtering</br>, novelty detection or other related areas (text summarization and question answering) have desirable properties in all the three aspects.",
                "Therefore, we must develop a new evaluation methodology. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers.",
                "Answer keys define what should be present in a system response to receive credit, and are comprised of a collection of information nuggets, i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them.",
                "Defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping [19], since system-returned passages can contain the same information expressed in many different ways.",
                "Hence, QA evaluations have relied on human assessors for the mapping between various expressions, making the process costly, time consuming, and not scalable to large query and document collections, and extensive system evaluations with various parameter settings. 4.1.1 Automating Evaluation based on Answer Keys Automatic evaluation methods would allow for faster system building and tuning, as well as provide an objective and affordable way of comparing various systems.",
                "Recently, such methods have been proposed, more or less, based on the idea of n-gram co-occurrences.",
                "Pourpre [10] assigns a fractional recall score to a system response based on its unigram overlap with a given nuggets description.",
                "For example, a system response A B C has recall 3/4 with respect to a nugget with description A B C D. However, such an approach is unfair to systems that present the same information but using words other than A, B, C, and D. Another open issue is how to weight individual words in measuring the closeness of a match.",
                "For example, consider the question How many prisoners escaped?.",
                "In the nugget Seven prisoners escaped from a Texas prison, there is no indication that seven is the keyword, and that it must be matched to get any relevance credit.",
                "Using IDF values does not help, since seven will generally not have a higher IDF than words like texas and prison.",
                "Also, redefining the nugget as just seven does not solve the problem since now it might spuriously match any mention of seven out of context.",
                "Nuggeteer [13] works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold.",
                "However, it is also plagued by spurious relevance since not all words contained in the nugget description (or known correct responses) are central to the nugget. 4.1.2 Nugget-Matching Rules We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on nugget-matching rules, which are generated using a semiautomatic procedure explained below.",
                "These rules are essentially Boolean queries that will only match against snippets that contain the nugget.",
                "For instance, a candidate rule for matching answers to How many prisoners escaped? is (Texas AND seven AND escape AND (convicts OR prisoners)), possibly with other synonyms and variants in the rule.",
                "For a corpus of news articles, which usually follow a typical formal prose, it is fairly easy to write such simple rules to match expected answers using a bootstrap approach, as described below.",
                "We propose a two-stage approach, inspired by Autoslog [14], that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents.",
                "In the first stage, human subjects annotated (using a highlighting tool) portions of ontopic documents that contained answers to each nugget 2 .",
                "In the second stage, subjects used our rule generation tool to create rules that would match the annotations for each nugget.",
                "The tool allows users to enter a Boolean rule as a disjunction of conjunctions (e.g. ((a AND b) OR (a AND c AND d) OR (e))).",
                "Given a candidate rule, our tool uses it as a Boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match.",
                "Hence, the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision.",
                "We observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions (adding more ANDs), and improving the recall by adding more conjunctions to the disjunction (adding more ORs).",
                "As an example, lets try to create a rule for the nugget which says that seven prisoners escaped from the Texas prison.",
                "We start with a simple rule - (seven).",
                "When we input this into the rule generation tool, we realize that this rule matches many spurious occurrences of seven (e.g. ...seven states...) and thus gets a low precision score.",
                "We can further qualify our rule - Texas AND seven AND convicts.",
                "Next, by looking at the missed annotations, we realize that some news articles mentioned ...seven prisoners escaped.... We then replace convicts with the disjunction (convicts OR prisoners).",
                "We continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the (small number of) misses and false alarms can be safely ignored.",
                "Thus we can create nugget-matching rules that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses.",
                "Human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably. 4.2 Evaluating the Utility of a Sequence of Ranked Lists The utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information, and how much the user lost in terms of time and energy.",
                "We calculate this utility from the utilities of individual passages as follows.",
                "After reading each passage returned by the system, the user derives some gain depending on the presence of relevant and novel information, and incurs a loss in terms of the time and energy spent in going through the passage.",
                "However, the likelihood that the user would actually read a passage depends on its position in the ranked list.",
                "Hence, for a query q, the expected utility 2 LDC [18] already provides relevance judgments for 100 topics on the TDT4 corpus.",
                "We further ensured that these judgments are exhaustive on the entire corpus using pooling. of a passage pi at rank i can be defined as U(pi, q) = P(i) â (Gain(pi, q) â Loss(pi, q)) (4) where P(i) is the probability that the user would go through a passage at rank i.",
                "The expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage: U(q) = nX i=1 P(i) â (Gain(pi, q) â Loss(pi, q)) (5) Note that if we ignore the loss term and define P(i) as P(i) â 1/ logb(b + i â 1) (6) then we get the recently popularized metric called Discounted Cumulated Gain (DCG) [9], where Gain(pi, q) is defined as the graded relevance of passage pi.",
                "However, without the loss term, DCG is a purely recall-oriented metric and not suitable for an <br>adaptive filtering</br> setting, where the systems utility depends in part on its ability to limit the number of items shown to the user.",
                "Although P(i) could be defined based on empirical studies of user behavior, for simplicity, we use P(i) exactly as defined in equation 6.",
                "The gain G(pi, q) of passage pi with respect to the query q is a function of - 1) the number of relevant nuggets present in pi, and 2) the novelty of each of these nuggets.",
                "We combine these two factors as follows.",
                "For each nugget Nj, we assign an initial weight wj, and also keep a count nj of the number of times this nugget has been seen by the user in the past.",
                "The gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor Î³.",
                "Thus, G(pi, q) is defined as G(pi, q) = X Nj âC(pi,q) wj â Î³nj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the answer key of query q.",
                "The initial weights wj are all set of be 1.0 in our experiments, but can also be set based on a pyramid approach [11].",
                "The choice of dampening factor Î³ determines the users tolerance for redundancy.",
                "When Î³ = 0, a nugget will only receive credit for its first occurrence i.e. when nj is zero3 .",
                "For 0 < Î³ < 1, a nugget receives smaller credit for each successive occurrence.",
                "When Î³ = 1, no dampening occurs and repeated occurrences of a nugget receive the same credit.",
                "Note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system, since the users are expected to remember what the system showed them in the past.",
                "We define the loss L(pi, q) as a constant cost c (we use 0.1) incurred when reading a system-returned passage.",
                "Thus, our metric can be re-written as U(q) = nX i=1 Gain(pi, q) logb(b + i â 1) â L(n) (8) where L(n) is the loss associated with a ranked list of length n: L(n) = c Â· nX i=1 1 logb(b + i â 1) (9) 3 Note that 00 = 1 Due to the similarity with Discounted Cumulated Gain (DCG), we call our metric Discounted Cumulated Utility (DCU).",
                "The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) â¤ 0 i.e. when the gain is less than or equal to the cost of reading the passage. 5.",
                "DATA TDT4 was the benchmark corpus used in TDT2002 and TDT2003 evaluations.",
                "The corpus consists of over 90, 000 news articles from multiple sources (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) published between October 2000 and January 2001, in the languages of Arabic, English, and Mandarin.",
                "Speech-recognized and machine-translated versions of the non-English articles were provided as well.",
                "LDC [18] has annotated the corpus with 100 topics, that correspond to various news events in this time period.",
                "Out of these, we selected a subset of 12 actionable events, and defined corresponding tasks for them4 .",
                "For each task, we manually defined a profile consisting of an initial set of (5 to 10) queries, a free-text description of the user history, i.e., what the user already knows about the event, and a list of known on-topic and off-topic documents (if available) as training examples.",
                "For each query, we generated answer keys and corresponding nugget matching rules using the procedure described in section 4.1.2, and produced a total of 120 queries, with an average of 7 nuggets per query. 6.",
                "EXPERIMENTS AND RESULTS 6.1 Baselines We used Indri [17], a popular language-model based retrieval engine, as a baseline for comparison with CAFÂ´E.",
                "Indri supports standard search engine functionality, including pseudo-relevance feedback (PRF) [3, 6], and is representative of a typical query-based retrieval system.",
                "Indri does not support any kind of novelty detection.",
                "We compare Indri with PRF turned on and off, against CAFÂ´E with user feedback, novelty detection and antiredundant ranking turned on and off. 6.2 Experimental Setup We divided the TDT4 corpus spanning 4 months into 10 chunks, each defined as a period of 12 consecutive days.",
                "At any given point of time in the distillation process, each system accessed the past data up to the current point, and returned a ranked list of up 50 passages per query.",
                "The 12 tasks defined on the corpus were divided into a training and test set with 6 tasks each.",
                "Each system was allowed to use the training set to tune its parameters for optimizing NDCU (equation 8), including the relevance threshold for both Indri and CAFÂ´E, and the novelty and antiredundancy thresholds for CAFÂ´E.",
                "The NDCU for each system run is calculated automatically.",
                "User feedback was also simulated - relevance judgments for each system-returned passage (as determined by the nugget matching rules described in section 4.1.2) were 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figure 1: Performance of Indri across chunks Figure 2: Performance of CAFÂ´E across chunks used as user feedback in the adaptation of query profiles and user histories. 6.3 Results In Table 1, we show the NDCU scores of the two systems under various settings.",
                "These scores are averaged over the six tasks in the test set, and are calculated with two dampening factors (see section 4.2): Î³ = 0 and 0.1, to simulate no tolerance and small tolerance for redundancy, respectively.",
                "Using Î³ = 0 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information.",
                "Hence, the improvement obtained from enabling user feedback is smaller with Î³ = 0 than the improvement obtained from feedback with Î³ = 0.1.",
                "This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information.",
                "However, the user does not benefit from seeing such redundant passages, and is usually interested in other passages containing related information.",
                "It is informative to evaluate retrieval systems using our utility measure (with Î³ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.",
                "Sometimes, however, users might indeed be interested in seeing the same information from multiple sources, as an Table 1: NDCU Scores of Indri and CAFÂ´E for two dampening factors (Î³), and various settings (F: Feedback, N: Novelty Detection, A: Anti-Redundant Ranking) Indri CAFÂ´E Î³ Base +PRF Base +F +F+N +F+A +F+N+A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.35 0.36 0.36 indicator of its importance or reliability.",
                "In such a case, we can simply choose a higher value for Î³ which corresponds to a higher tolerance for redundancy, and hence let the system tune its parameters accordingly.",
                "Since documents were processed chunk by chunk, it would be interesting to see how the performance of systems improves over time.",
                "Figures 1 and 2 show the performance trends for both the systems across chunks.",
                "While the performance with and without feedback on the first few chunks is expected to be close, for subsequent chunks, the performance curve with feedback enabled rises above the one with the no-feedback setting.",
                "The performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks, making some queries easier than others in certain chunks.",
                "Moreover, since Indri uses pseudo-relevance feedback while CAFÂ´E uses feedback based on actual relevance judgments, the improvement in case of Indri is less dramatic than that of CAFÂ´E. 7.",
                "CONCLUDING REMARKS This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages.",
                "Our system, called CAFÂ´E, combines <br>adaptive filtering</br>, novelty detection and antiredundant passage ranking in a unified framework for utility optimization.",
                "We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty.",
                "Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off. 8.",
                "ACKNOWLEDGMENTS We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments.",
                "This work is supported in parts by the National Science Foundation (NSF) under grant IIS0434035, and the Defense Advanced Research Project Agency (DARPA) under contracts NBCHD030010 and W0550432.",
                "Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. 9.",
                "ADDITIONAL AUTHORS Jian Zhang (jianzhan@stat.purdue.edu)â , Jaime Carbonell (jgc@cs.cmu.edu)â  , Peter Brusilovsky (peterb+@pitt.edu)â¡ , Daqing He(dah44@pitt.edu)â¡ 10.",
                "REFERENCES [1] J. Allan.",
                "Incremental Relevance Feedback for Information Filtering.",
                "Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 270-278, 1996. [2] J. Allan, C. Wade, and A. Bolivar.",
                "Retrieval and Novelty Detection at the Sentence Level.",
                "Proceedings of the ACM SIGIR conference on research and development in information retrieval, 2003. [3] C. Buckley, G. Salton, and J. Allan.",
                "Automatic Retrieval with Locality Information using SMART.",
                "NIST special publication, (500207):59-72, 1993. [4] J. Callan.",
                "Learning While Filtering Documents.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 224-231, 1998. [5] J. Carbonell and J. Goldstein.",
                "The use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335-336, 1998. [6] E. Efthimiadis.",
                "Query Expansion.",
                "Annual Review of Information Science and Technology (ARIST), 31:p121-87, 1996. [7] J. Fiscus and G. Duddington.",
                "Topic Detection and Tracking Overview.",
                "Topic Detection and Tracking: Event-based Information Organization, pages 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.",
                "A Statistical Model for Multilingual Entity Detection and Tracking.",
                "NAACL/HLT, 2004. [9] K. JÂ¨arvelin and J. KekÂ¨alÂ¨ainen.",
                "Cumulated Gain-based Evaluation of IR Techniques.",
                "ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin and D. Demner-Fushman.",
                "Automatically Evaluating Answers to Definition Questions.",
                "Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), 2005. â Statistics Dept., Purdue University, West Lafayette, USA â  Language Technologies Inst., Carnegie Mellon University, Pittsburgh, USA â¡ School of Information Sciences, Univ. of Pittsburgh, Pittsburgh, USA [11] J. Lin and D. Demner-Fushman.",
                "Will Pyramids Built of nUggets Topple Over.",
                "Proceedings of HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.",
                "A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree.",
                "Proc. of ACL, 4:136-143, 2004. [13] G. Marton.",
                "Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments.",
                "HLT/NAACL, 2006. [14] E. Riloff.",
                "Automatically Constructing a Dictionary for Information Extraction Tasks.",
                "Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, 1993. [15] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9: Filtering track.",
                "The Ninth Text REtrieval Conference (TREC-9), pages 361-368. [16] R. Schapire, Y.",
                "Singer, and A. Singhal.",
                "Boosting and Rocchio Applied to Text Filtering.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle, and W. Croft.",
                "Indri: A Language Model-based Search Engine for Complex Queries.",
                "Proceedings of the International Conference on Intelligence Analysis, 2004. [18] The Linguistic Data Consortium. http://www.ldc.upenn.edu/. [19] E. Voorhees.",
                "Overview of the TREC 2003 Question Answering Track.",
                "Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), 2003. [20] Y. Yang and B. Kisiel.",
                "Margin-based Local Regression for <br>adaptive filtering</br>.",
                "Proceedings of the twelfth international conference on Information and knowledge management, pages 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of <br>adaptive filtering</br> Methods in a Cross-benchmark Evaluation.",
                "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 98-105, 2005. [22] C. Zhai, W. Cohen, and J. Lafferty.",
                "Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-17, 2003. [23] J. Zhang and Y. Yang.",
                "Robustness of Regularized Linear Classification Methods in Text Categorization.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 190-197, 2003. [24] Y. Zhang.",
                "Using Bayesian Priors to Combine Classifiers for <br>adaptive filtering</br>.",
                "Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 345-352, 2004. [25] Y. Zhang, J. Callan, and T. Minka.",
                "Novelty and Redundancy Detection in <br>adaptive filtering</br>.",
                "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002."
            ],
            "original_annotated_samples": [
                "It combines the strengths of and extends beyond conventional <br>adaptive filtering</br>, novelty detection and non-redundant passage ranking with respect to long-lasting information needs (tasks with multiple queries).",
                "<br>adaptive filtering</br> (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics.",
                "Despite substantial achievements in recent <br>adaptive filtering</br> research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently.",
                "User has a rather passive role in the conventional <br>adaptive filtering</br> setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision.",
                "Section 3 describes the technical cores of our system called CAFÂ´E - CMU <br>adaptive filtering</br> Engine."
            ],
            "translated_annotated_samples": [
                "Combina las fortalezas y se extiende mÃ¡s allÃ¡ del <br>filtrado adaptativo</br> convencional, la detecciÃ³n de novedades y la clasificaciÃ³n de pasajes no redundantes con respecto a las necesidades de informaciÃ³n duraderas (tareas con mÃºltiples consultas).",
                "El <br>filtrado adaptativo</br> (AF) es una tarea de predicciÃ³n en lÃ­nea de la relevancia de cada nuevo documento con respecto a temas predefinidos.",
                "A pesar de los logros sustanciales en la investigaciÃ³n reciente de <br>filtrado adaptativo</br>, aÃºn quedan problemas significativos sin resolver sobre cÃ³mo aprovechar de manera efectiva y eficiente la retroalimentaciÃ³n del usuario.",
                "El usuario tiene un papel bastante pasivo en la configuraciÃ³n convencional de <br>filtrado adaptativo</br>: solo reacciona ante el sistema cuando este toma una decisiÃ³n afirmativa sobre un documento, confirmando o rechazando esa decisiÃ³n.",
                "La secciÃ³n 3 describe los nÃºcleos tÃ©cnicos de nuestro sistema llamado CAFÂ´E - CMU Adaptive Filtering Engine."
            ],
            "translated_text": "DestilaciÃ³n de informaciÃ³n basada en utilidad sobre documentos secuenciados temporalmente en el Instituto de TecnologÃ­as del Lenguaje de Yiming Yang. Universidad Carnegie Mellon, Pittsburgh, EE. UU. yiming@cs.cmu.edu Abhimanyu Lad Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. alad@cs.cmu.edu Instituto de TecnologÃ­as del Lenguaje Ni Lao. Universidad Carnegie Mellon, Pittsburgh, EE. UU. nlao@cs.cmu.edu Abhay Harpale Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. aharpale@cs.cmu.edu Bryan Kisiel Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. bkisiel@cs.cmu.edu MÃ³nica Rogati Instituto de TecnologÃ­as del Lenguaje. La Universidad Carnegie Mellon en Pittsburgh, EE. UU. mrogati@cs.cmu.edu RESUMEN Este artÃ­culo examina un nuevo enfoque para la destilaciÃ³n de informaciÃ³n en documentos ordenados temporalmente, y propone un esquema de evaluaciÃ³n novedoso para dicho marco. Combina las fortalezas y se extiende mÃ¡s allÃ¡ del <br>filtrado adaptativo</br> convencional, la detecciÃ³n de novedades y la clasificaciÃ³n de pasajes no redundantes con respecto a las necesidades de informaciÃ³n duraderas (tareas con mÃºltiples consultas). Nuestro enfoque respalda la retroalimentaciÃ³n detallada del usuario a travÃ©s del resaltado de fragmentos arbitrarios de texto, y aprovecha dicha informaciÃ³n para la optimizaciÃ³n de la utilidad en entornos adaptativos. Para nuestros experimentos, definimos tareas hipotÃ©ticas basadas en eventos de noticias en el corpus TDT4, con mÃºltiples consultas por tarea. Se generaron claves de respuestas (nuggets) para cada consulta y se utilizÃ³ un procedimiento semiautomÃ¡tico para adquirir reglas que permiten emparejar automÃ¡ticamente los nuggets con las respuestas del sistema. TambiÃ©n proponemos una extensiÃ³n de la mÃ©trica NDCG para evaluar la utilidad de los pasajes clasificados como una combinaciÃ³n de relevancia y novedad. Nuestros resultados muestran mejoras alentadoras en la utilidad utilizando el nuevo enfoque, en comparaciÃ³n con los sistemas base sin aprendizaje incremental o componentes de detecciÃ³n de novedades. CategorÃ­as y Descriptores de Asignaturas H.3.3 [BÃºsqueda y RecuperaciÃ³n de InformaciÃ³n]: Filtrado de informaciÃ³n, RetroalimentaciÃ³n de relevancia, Modelos de recuperaciÃ³n, Proceso de selecciÃ³n; I.5.2 TÃ©rminos Generales DiseÃ±o, MediciÃ³n, Rendimiento, ExperimentaciÃ³n. 1. INTRODUCCIÃN Seguir informaciÃ³n nueva y relevante de flujos de datos temporales para usuarios con necesidades duraderas ha sido un tema de investigaciÃ³n desafiante en la recuperaciÃ³n de informaciÃ³n. El <br>filtrado adaptativo</br> (AF) es una tarea de predicciÃ³n en lÃ­nea de la relevancia de cada nuevo documento con respecto a temas predefinidos. BasÃ¡ndose en la consulta inicial y algunos ejemplos positivos (si estÃ¡n disponibles), un sistema de AF mantiene un perfil para cada tema de interÃ©s, y lo actualiza constantemente segÃºn la retroalimentaciÃ³n del usuario. La naturaleza de aprendizaje incremental de los sistemas de AF los hace mÃ¡s poderosos que los motores de bÃºsqueda estÃ¡ndar que admiten recuperaciÃ³n ad-hoc (por ejemplo. Google y Yahoo) en tÃ©rminos de encontrar informaciÃ³n relevante con respecto a temas de interÃ©s duraderos, y mÃ¡s atractivo para los usuarios que estÃ¡n dispuestos a proporcionar retroalimentaciÃ³n para adaptar el sistema a sus necesidades de informaciÃ³n especÃ­ficas, sin tener que modificar sus consultas manualmente. Se han estudiado una variedad de algoritmos de aprendizaje supervisado (clasificadores de estilo Rocchio, modelos exponenciales-gaussianos, regresiÃ³n local y enfoques de regresiÃ³n logÃ­stica) para entornos adaptativos, examinados con retroalimentaciÃ³n de relevancia explÃ­cita e implÃ­cita, y evaluados con respecto a la optimizaciÃ³n de la utilidad en grandes colecciones de datos de referencia en los foros de TREC (Conferencias de RecuperaciÃ³n de Texto) y TDT (DetecciÃ³n y Seguimiento de Temas) [1, 4, 7, 15, 16, 20, 24, 23]. La regresiÃ³n logÃ­stica regularizada [21] se ha encontrado representativa para los enfoques de vanguardia, y altamente eficiente para adaptaciones frecuentes del modelo en grandes colecciones de documentos como el corpus TREC-10 (mÃ¡s de 800,000 documentos y 84 temas). A pesar de los logros sustanciales en la investigaciÃ³n reciente de <br>filtrado adaptativo</br>, aÃºn quedan problemas significativos sin resolver sobre cÃ³mo aprovechar de manera efectiva y eficiente la retroalimentaciÃ³n del usuario. EspecÃ­ficamente, los siguientes problemas pueden limitar seriamente la verdadera utilidad de los sistemas de AF en aplicaciones del mundo real: 1. El usuario tiene un papel bastante pasivo en la configuraciÃ³n convencional de <br>filtrado adaptativo</br>: solo reacciona ante el sistema cuando este toma una decisiÃ³n afirmativa sobre un documento, confirmando o rechazando esa decisiÃ³n. Una alternativa mÃ¡s activa serÃ­a permitir al usuario emitir mÃºltiples consultas sobre un tema, revisar una lista clasificada de documentos candidatos (o pasajes) por consulta, y proporcionar retroalimentaciÃ³n sobre la lista clasificada, refinando asÃ­ su necesidad de informaciÃ³n y solicitando listas clasificadas actualizadas. La Ãºltima forma de interacciÃ³n del usuario ha sido altamente efectiva en la recuperaciÃ³n estÃ¡ndar para consultas ad-hoc. CÃ³mo implementar una estrategia para necesidades de informaciÃ³n duraderas en entornos de FA es una pregunta abierta para la investigaciÃ³n. 2. La unidad para recibir un juicio de relevancia (sÃ­ o no) estÃ¡ restringida al nivel del documento en AF convencional. Sin embargo, un usuario real puede estar dispuesto a proporcionar comentarios mÃ¡s informativos y detallados resaltando algunas partes de texto en un documento recuperado como relevantes, en lugar de etiquetar todo el documento como relevante. Aprovechar de manera efectiva este tipo de retroalimentaciÃ³n detallada podrÃ­a mejorar sustancialmente la calidad de un sistema de AF. Para esto, necesitamos habilitar el aprendizaje supervisado a partir de fragmentos de texto etiquetados de longitud arbitraria en lugar de solo permitir documentos etiquetados. 3. Los documentos seleccionados por el sistema suelen ser altamente redundantes. Un evento importante de noticias, por ejemplo, serÃ­a reportado por mÃºltiples fuentes repetidamente durante un tiempo, haciendo que la mayor parte del contenido informativo en esos artÃ­culos sea redundante entre sÃ­. Un sistema de AF convencional seleccionarÃ­a todas estas historias de noticias redundantes para la retroalimentaciÃ³n del usuario, desperdiciando el tiempo de los usuarios mientras ofrece poco beneficio. Claramente, las tÃ©cnicas de detecciÃ³n de novedades pueden ayudar en principio [25, 2, 22] a mejorar la utilidad de los sistemas AF. Sin embargo, la efectividad de tales tÃ©cnicas a nivel de pasaje para detectar novedad con respecto a la retroalimentaciÃ³n de los usuarios a nivel detallado y para detectar redundancia en listas clasificadas aÃºn debe ser evaluada utilizando una medida de utilidad que imite las necesidades de un usuario real. Para abordar las limitaciones actuales de los sistemas de AF mencionadas anteriormente, proponemos y examinamos un nuevo enfoque en este artÃ­culo, que combina las fortalezas del AF convencional (aprendizaje incremental de modelos de temas), recuperaciÃ³n de pasajes multipase para consultas de larga duraciÃ³n condicionadas al tema, y detecciÃ³n de novedades para eliminar la redundancia de las interacciones del usuario con el sistema. Llamamos al nuevo proceso destilaciÃ³n de informaciÃ³n basada en utilidad. Ten en cuenta que los corpus de referencia convencionales para evaluaciones de AF, que tienen juicios de relevancia a nivel de documento y no definen tareas con mÃºltiples consultas, son insuficientes para evaluar el nuevo enfoque. Por lo tanto, ampliamos un corpus de referencia, la colecciÃ³n TDT4 de noticias y transmisiones de televisiÃ³n, con definiciones de tareas, mÃºltiples consultas por tarea y claves de respuestas por consulta. Hemos realizado nuestros experimentos en este corpus extendido TDT4 y hemos puesto a disposiciÃ³n del pÃºblico los datos generados adicionalmente para futuras evaluaciones comparativas. Para evaluar automÃ¡ticamente los fragmentos de texto arbitrarios devueltos por el sistema utilizando nuestras claves de respuestas, desarrollamos un esquema de evaluaciÃ³n con un procedimiento semiautomÃ¡tico para 1 URL: http://nyc.lti.cs.cmu.edu/downloads adquiriendo reglas que pueden comparar fragmentos con las respuestas del sistema. AdemÃ¡s, proponemos una extensiÃ³n de NDCG (Normalized Discounted Cumulated Gain) [9] para evaluar la utilidad de los pasajes clasificados como una funciÃ³n tanto de la relevancia como de la novedad. El resto de este documento estÃ¡ organizado de la siguiente manera. La secciÃ³n 2 describe el proceso de destilaciÃ³n de informaciÃ³n con un ejemplo concreto. La secciÃ³n 3 describe los nÃºcleos tÃ©cnicos de nuestro sistema llamado CAFÂ´E - CMU Adaptive Filtering Engine. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "ad-hoc retrieval": {
            "translated_key": "recuperaciÃ³n ad-hoc",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Utility-based Information Distillation Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu ABSTRACT This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework.",
                "It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant passage ranking with respect to long-lasting information needs (tasks with multiple queries).",
                "Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings.",
                "For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task.",
                "Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty.",
                "Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval.",
                "Adaptive filtering (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics.",
                "Based on the initial query and a few positive examples (if available), an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user.",
                "The incremental learning nature of AF systems makes them more powerful than standard search engines that support <br>ad-hoc retrieval</br> (e.g.",
                "Google and Yahoo) in terms of finding relevant information with respect to long-lasting topics of interest, and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs, without having to modify their queries manually.",
                "A variety of supervised learning algorithms (Rocchio-style classifiers, Exponential-Gaussian models, local regression and logistic regression approaches) have been studied for adaptive settings, examined with explicit and implicit relevance feedback, and evaluated with respect to utility optimization on large benchmark data collections in TREC (Text Retrieval Conferences) and TDT (Topic Detection and Tracking) forums [1, 4, 7, 15, 16, 20, 24, 23].",
                "Regularized logistic regression [21] has been found representative for the state-of-the-art approaches, and highly efficient for frequent model adaptations over large document collections such as the TREC-10 corpus (over 800,000 documents and 84 topics).",
                "Despite substantial achievements in recent adaptive filtering research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently.",
                "Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications: 1.",
                "User has a rather passive role in the conventional adaptive filtering setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision.",
                "A more active alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents (or passages) per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists.",
                "The latter form of user interaction has been highly effective in standard retrieval for ad-hoc queries.",
                "How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2.",
                "The unit for receiving a relevance judgment (yes or no) is restricted to the document level in conventional AF.",
                "However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant.",
                "Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system.",
                "For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3.",
                "System-selected documents are often highly redundant.",
                "A major news event, for example, would be reported by multiple sources repeatedly for a while, making most of the information content in those articles redundant with each other.",
                "A conventional AF system would select all these redundant news stories for user feedback, wasting the users time while offering little gain.",
                "Clearly, techniques for novelty detection can help in principle [25, 2, 22] for improving the utility of the AF systems.",
                "However, the effectiveness of such techniques at passage level to detect novelty with respect to users (fine-grained) feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user.",
                "To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and novelty detection for removal of redundancy from user interactions with the system.",
                "We call the new process utility-based information distillation.",
                "Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach.",
                "Therefore, we extended a benchmark corpus - the TDT4 collection of news stories and TV broadcasts - with task definitions, multiple queries per task, and answer keys per query.",
                "We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 .",
                "To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses.",
                "Moreover, we propose an extension of NDCG (Normalized Discounted Cumulated Gain) [9] for assessing the utility of ranked passages as a function of both relevance and novelty.",
                "The rest of this paper is organized as follows.",
                "Section 2 outlines the information distillation process with a concrete example.",
                "Section 3 describes the technical cores of our system called CAFÂ´E - CMU Adaptive Filtering Engine.",
                "Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme.",
                "Section 5 describes the extended TDT4 corpus.",
                "Section 6 presents our experiments and results.",
                "Section 7 concludes the study and gives future perspectives. 2.",
                "A SAMPLE TASK Consider a news event - the escape of seven convicts from a Texas prison in December 2000 and their capture a month later.",
                "Assuming a user were interested in this event since its early stage, the information need could be: Find information about the escape of convicts from Texas prison, and information related to their recapture.",
                "The associated lower-level questions could be: 1.",
                "How many prisoners escaped? 2.",
                "Where and when were they sighted? 3. Who are their known contacts inside and outside the prison? 4.",
                "How are they armed? 5.",
                "Do they have any vehicles? 6.",
                "What steps have been taken so far?",
                "We call such an information need a task, and the associated questions as the queries in this task.",
                "A distillation system is supposed to monitor the incoming documents, process them chunk by chunk in a temporal order, select potentially relevant and novel passages from each chunk with respect to each query, and present a ranked list of passages to the user.",
                "Passage ranking here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list.",
                "When presented with a list of passages, the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant.",
                "These spans of text are taken as positive examples in the adaptation of the query profile, and also added to the users history.",
                "Passages not marked by the user are taken as negative examples.",
                "As soon as the query profile is updated, the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low, based on user preference.",
                "For example, if the user highlights ...officials have posted a $100,000 reward for their capture... as relevant answer to the query What steps have been taken so far?, then the highlighted piece is used as an additional positive training example in the adaptation of the query profile.",
                "This piece of feedback is also added to the user history as a seen example, so that in future, the system will not place another passage mentioning $100,000 reward at the top of the ranked list.",
                "However, an article mentioning ...officials have doubled the reward money to $200,000... might be ranked high since it is both relevant to the (updated) query profile and novel with respect to the (updated) user history.",
                "The user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly.",
                "Clearly, novelty detection is very important for the utility of such a system because of the iterative search.",
                "Without novelty detection, the old relevant passages would be shown to the user repeatedly in each ranked list.",
                "Through the above example, we can see the main properties of our new framework for utility-based information distillation over temporally ordered documents.",
                "Our framework combines and extends the power of adaptive filtering (AF), <br>ad-hoc retrieval</br> (IR) and novelty detection (ND).",
                "Compared to standard IR, our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task.",
                "Compared to conventional AF, it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system.",
                "Compared to past work, this is the first evaluation of novelty detection integrated with adaptive filtering for sequenced queries that allows flexible user feedback over ranked passages.",
                "The combination of AF, IR and ND with the new extensions raises an important research question regarding evaluation methodology: how can we measure the utility of such an information distillation system?",
                "Existing metrics in standard IR, AF and ND are insufficient, and new solutions must be explored, as we will discuss in Section 4, after describing the technical cores of our system in the next section. 3.",
                "TECHNICAL CORES The core components of CAFÂ´E are - 1) AF for incremental learning of query profiles, 2) IR for estimating relevance of passages with respect to query profiles, 3) ND for assessing novelty of passages with respect to users history, and 4) anti-redundancy component to remove redundancy from ranked lists. 3.1 Adaptive Filtering Component We use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for AF [21].",
                "Logistic regression (LR) is a supervised learning algorithm for statistical classification.",
                "Based on a training set of labeled instances, it learns a class model which can then by used to predict the labels of unseen instances.",
                "Its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required, as is the case in adaptive filtering, where the system must learn from each new feedback provided by the user. (See [21] and [23] for computational complexity and implementation issues).",
                "In adaptive filtering, each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query.",
                "For training the model, we use the query itself as the initial positive training example of the class, and the user-highlighted pieces of text (marked as Relevant or Not-relevant) during feedback as additional training examples.",
                "To address the cold start issue in the early stage before any user feedback is obtained, the system uses a small sample from a retrospective corpus as the initial negative examples in the training set.",
                "The details of using logistic regression for adaptive filtering (assigning different weights to positive and negative training instances, and regularizing the objective function to prevent over-fitting on training data) are presented in [21].",
                "The class model wâ learned by Logistic Regression, or the query profile, is a vector whose dimensions are individual terms and whose elements are the regression coefficients, indicating how influential each term is in the query profile.",
                "The query profile is updated whenever a new piece of user feedback is received.",
                "A temporally decaying weight can be applied to each training example, as an option, to emphasize the most recent user feedback. 3.2 Passage Retrieval Component We use standard IR techniques in this part of our system.",
                "Incoming documents are processed in chunks, where each chunk can be defined as a fixed span of time or as a fixed number of documents, as preferred by the user.",
                "For each incoming document, corpus statistics like the IDF (Inverted Document Frequency) of each term are updated.",
                "We use a state-of-the-art named entity identifier and tracker [8, 12] to identify person and location names, and merge them with co-referent named entities seen in the past.",
                "Then the documents are segmented into passages, which can be a whole document, a paragraph, a sentence, or any other continuous span of text, as preferred.",
                "Each passage is represented using a vector of TF-IDF (Term FrequencyInverse Document Frequency) weights, where term can be a word or a named entity.",
                "Given a query profile, i.e. the logistic regression solution wâ as described in Section 3.1, the system computes the posterior probability of relevance for each passage x as fRL(x) â¡ P(y = 1|x, wâ ) = 1 (1 + eâwâÂ·x) (1) Passages are ordered by their relevance scores, and the ones with scores above a threshold (tuned on a training set) comprise the relevance list that is passed on to the novelty detection step. 3.3 Novelty Detection Component CAFÂ´E maintains a user history H(t), which contains all the spans of text hi that the user highlighted (as feedback) during his or her past interactions with the system, up to the current time t. Denoting the history as H(t) = n h1, h2, ..., ht o , (2) the novelty score of a new candidate passage x is computed as: fND(x) = 1 â max iâ1..t {cos(x, hi)} (3) where both candidate passage x and highlighted spans of text hi are represented as TF-IDF vectors.",
                "The novelty score of each passage is compared to a prespecified threshold (also tuned on a training set), and any passage with a score below this threshold is removed from the relevance list. 3.4 Anti-redundant Ranking Component Although the novelty detection component ensures that only novel (previously unseen) information remains in the relevance list, this list might still contain the same novel information at multiple positions in the ranked list.",
                "Suppose, for example, that the user has already read about a $100,000 reward for information about the escaped convicts.",
                "A new piece of news that the award has been increased to $200,000 is novel since the user hasnt read about it yet.",
                "However, multiple news sources would report this news and we might end up showing (redundant) articles from all these sources in a ranked list.",
                "Hence, a ranked list should also be made non-redundant with respect to its own contents.",
                "We use a simplified version of the Maximal Marginal Relevance method [5], originally developed for combining relevance and novelty in text retrieval and summarization.",
                "Our procedure starts with the current list of passages sorted by relevance (section 3.2), filtered by Novelty Detection component (section 3.3), and generates a new non-redundant list as follows: 1.",
                "Take the top passage in the current list as the top one in the new list. 2.",
                "Add the next passage x in the current list to the new list only if fAR(x) > t where fAR(x) = 1 â max piâLnew {cos(x, pi)} and Lnew is the set of passages already selected in the new list. 3.",
                "Repeat step 2 until all the passages in the current list have been examined.",
                "After applying the above-mentioned algorithm, each passage in the new list is sufficiently dissimilar to others, thus favoring diversity rather than redundancy in the new ranked list.",
                "The anti-redundancy threshold t is tuned on a training set. 4.",
                "EVALUATION METHODOLOGY The approach we proposed above for information distillation raises important issues regarding evaluation methodology.",
                "Firstly, since our framework allows the output to be passages at different levels of granularity (e.g. k-sentence windows where k may vary) instead of a fixed length, it is not possible to have pre-annotated relevance judgments at all such granularity levels.",
                "Secondly, since we wish to measure the utility of the system output as a combination of both relevance and novelty, traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time.",
                "Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists.",
                "None of the existing measures in <br>ad-hoc retrieval</br>, adaptive filtering, novelty detection or other related areas (text summarization and question answering) have desirable properties in all the three aspects.",
                "Therefore, we must develop a new evaluation methodology. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers.",
                "Answer keys define what should be present in a system response to receive credit, and are comprised of a collection of information nuggets, i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them.",
                "Defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping [19], since system-returned passages can contain the same information expressed in many different ways.",
                "Hence, QA evaluations have relied on human assessors for the mapping between various expressions, making the process costly, time consuming, and not scalable to large query and document collections, and extensive system evaluations with various parameter settings. 4.1.1 Automating Evaluation based on Answer Keys Automatic evaluation methods would allow for faster system building and tuning, as well as provide an objective and affordable way of comparing various systems.",
                "Recently, such methods have been proposed, more or less, based on the idea of n-gram co-occurrences.",
                "Pourpre [10] assigns a fractional recall score to a system response based on its unigram overlap with a given nuggets description.",
                "For example, a system response A B C has recall 3/4 with respect to a nugget with description A B C D. However, such an approach is unfair to systems that present the same information but using words other than A, B, C, and D. Another open issue is how to weight individual words in measuring the closeness of a match.",
                "For example, consider the question How many prisoners escaped?.",
                "In the nugget Seven prisoners escaped from a Texas prison, there is no indication that seven is the keyword, and that it must be matched to get any relevance credit.",
                "Using IDF values does not help, since seven will generally not have a higher IDF than words like texas and prison.",
                "Also, redefining the nugget as just seven does not solve the problem since now it might spuriously match any mention of seven out of context.",
                "Nuggeteer [13] works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold.",
                "However, it is also plagued by spurious relevance since not all words contained in the nugget description (or known correct responses) are central to the nugget. 4.1.2 Nugget-Matching Rules We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on nugget-matching rules, which are generated using a semiautomatic procedure explained below.",
                "These rules are essentially Boolean queries that will only match against snippets that contain the nugget.",
                "For instance, a candidate rule for matching answers to How many prisoners escaped? is (Texas AND seven AND escape AND (convicts OR prisoners)), possibly with other synonyms and variants in the rule.",
                "For a corpus of news articles, which usually follow a typical formal prose, it is fairly easy to write such simple rules to match expected answers using a bootstrap approach, as described below.",
                "We propose a two-stage approach, inspired by Autoslog [14], that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents.",
                "In the first stage, human subjects annotated (using a highlighting tool) portions of ontopic documents that contained answers to each nugget 2 .",
                "In the second stage, subjects used our rule generation tool to create rules that would match the annotations for each nugget.",
                "The tool allows users to enter a Boolean rule as a disjunction of conjunctions (e.g. ((a AND b) OR (a AND c AND d) OR (e))).",
                "Given a candidate rule, our tool uses it as a Boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match.",
                "Hence, the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision.",
                "We observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions (adding more ANDs), and improving the recall by adding more conjunctions to the disjunction (adding more ORs).",
                "As an example, lets try to create a rule for the nugget which says that seven prisoners escaped from the Texas prison.",
                "We start with a simple rule - (seven).",
                "When we input this into the rule generation tool, we realize that this rule matches many spurious occurrences of seven (e.g. ...seven states...) and thus gets a low precision score.",
                "We can further qualify our rule - Texas AND seven AND convicts.",
                "Next, by looking at the missed annotations, we realize that some news articles mentioned ...seven prisoners escaped.... We then replace convicts with the disjunction (convicts OR prisoners).",
                "We continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the (small number of) misses and false alarms can be safely ignored.",
                "Thus we can create nugget-matching rules that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses.",
                "Human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably. 4.2 Evaluating the Utility of a Sequence of Ranked Lists The utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information, and how much the user lost in terms of time and energy.",
                "We calculate this utility from the utilities of individual passages as follows.",
                "After reading each passage returned by the system, the user derives some gain depending on the presence of relevant and novel information, and incurs a loss in terms of the time and energy spent in going through the passage.",
                "However, the likelihood that the user would actually read a passage depends on its position in the ranked list.",
                "Hence, for a query q, the expected utility 2 LDC [18] already provides relevance judgments for 100 topics on the TDT4 corpus.",
                "We further ensured that these judgments are exhaustive on the entire corpus using pooling. of a passage pi at rank i can be defined as U(pi, q) = P(i) â (Gain(pi, q) â Loss(pi, q)) (4) where P(i) is the probability that the user would go through a passage at rank i.",
                "The expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage: U(q) = nX i=1 P(i) â (Gain(pi, q) â Loss(pi, q)) (5) Note that if we ignore the loss term and define P(i) as P(i) â 1/ logb(b + i â 1) (6) then we get the recently popularized metric called Discounted Cumulated Gain (DCG) [9], where Gain(pi, q) is defined as the graded relevance of passage pi.",
                "However, without the loss term, DCG is a purely recall-oriented metric and not suitable for an adaptive filtering setting, where the systems utility depends in part on its ability to limit the number of items shown to the user.",
                "Although P(i) could be defined based on empirical studies of user behavior, for simplicity, we use P(i) exactly as defined in equation 6.",
                "The gain G(pi, q) of passage pi with respect to the query q is a function of - 1) the number of relevant nuggets present in pi, and 2) the novelty of each of these nuggets.",
                "We combine these two factors as follows.",
                "For each nugget Nj, we assign an initial weight wj, and also keep a count nj of the number of times this nugget has been seen by the user in the past.",
                "The gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor Î³.",
                "Thus, G(pi, q) is defined as G(pi, q) = X Nj âC(pi,q) wj â Î³nj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the answer key of query q.",
                "The initial weights wj are all set of be 1.0 in our experiments, but can also be set based on a pyramid approach [11].",
                "The choice of dampening factor Î³ determines the users tolerance for redundancy.",
                "When Î³ = 0, a nugget will only receive credit for its first occurrence i.e. when nj is zero3 .",
                "For 0 < Î³ < 1, a nugget receives smaller credit for each successive occurrence.",
                "When Î³ = 1, no dampening occurs and repeated occurrences of a nugget receive the same credit.",
                "Note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system, since the users are expected to remember what the system showed them in the past.",
                "We define the loss L(pi, q) as a constant cost c (we use 0.1) incurred when reading a system-returned passage.",
                "Thus, our metric can be re-written as U(q) = nX i=1 Gain(pi, q) logb(b + i â 1) â L(n) (8) where L(n) is the loss associated with a ranked list of length n: L(n) = c Â· nX i=1 1 logb(b + i â 1) (9) 3 Note that 00 = 1 Due to the similarity with Discounted Cumulated Gain (DCG), we call our metric Discounted Cumulated Utility (DCU).",
                "The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) â¤ 0 i.e. when the gain is less than or equal to the cost of reading the passage. 5.",
                "DATA TDT4 was the benchmark corpus used in TDT2002 and TDT2003 evaluations.",
                "The corpus consists of over 90, 000 news articles from multiple sources (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) published between October 2000 and January 2001, in the languages of Arabic, English, and Mandarin.",
                "Speech-recognized and machine-translated versions of the non-English articles were provided as well.",
                "LDC [18] has annotated the corpus with 100 topics, that correspond to various news events in this time period.",
                "Out of these, we selected a subset of 12 actionable events, and defined corresponding tasks for them4 .",
                "For each task, we manually defined a profile consisting of an initial set of (5 to 10) queries, a free-text description of the user history, i.e., what the user already knows about the event, and a list of known on-topic and off-topic documents (if available) as training examples.",
                "For each query, we generated answer keys and corresponding nugget matching rules using the procedure described in section 4.1.2, and produced a total of 120 queries, with an average of 7 nuggets per query. 6.",
                "EXPERIMENTS AND RESULTS 6.1 Baselines We used Indri [17], a popular language-model based retrieval engine, as a baseline for comparison with CAFÂ´E.",
                "Indri supports standard search engine functionality, including pseudo-relevance feedback (PRF) [3, 6], and is representative of a typical query-based retrieval system.",
                "Indri does not support any kind of novelty detection.",
                "We compare Indri with PRF turned on and off, against CAFÂ´E with user feedback, novelty detection and antiredundant ranking turned on and off. 6.2 Experimental Setup We divided the TDT4 corpus spanning 4 months into 10 chunks, each defined as a period of 12 consecutive days.",
                "At any given point of time in the distillation process, each system accessed the past data up to the current point, and returned a ranked list of up 50 passages per query.",
                "The 12 tasks defined on the corpus were divided into a training and test set with 6 tasks each.",
                "Each system was allowed to use the training set to tune its parameters for optimizing NDCU (equation 8), including the relevance threshold for both Indri and CAFÂ´E, and the novelty and antiredundancy thresholds for CAFÂ´E.",
                "The NDCU for each system run is calculated automatically.",
                "User feedback was also simulated - relevance judgments for each system-returned passage (as determined by the nugget matching rules described in section 4.1.2) were 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figure 1: Performance of Indri across chunks Figure 2: Performance of CAFÂ´E across chunks used as user feedback in the adaptation of query profiles and user histories. 6.3 Results In Table 1, we show the NDCU scores of the two systems under various settings.",
                "These scores are averaged over the six tasks in the test set, and are calculated with two dampening factors (see section 4.2): Î³ = 0 and 0.1, to simulate no tolerance and small tolerance for redundancy, respectively.",
                "Using Î³ = 0 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information.",
                "Hence, the improvement obtained from enabling user feedback is smaller with Î³ = 0 than the improvement obtained from feedback with Î³ = 0.1.",
                "This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information.",
                "However, the user does not benefit from seeing such redundant passages, and is usually interested in other passages containing related information.",
                "It is informative to evaluate retrieval systems using our utility measure (with Î³ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.",
                "Sometimes, however, users might indeed be interested in seeing the same information from multiple sources, as an Table 1: NDCU Scores of Indri and CAFÂ´E for two dampening factors (Î³), and various settings (F: Feedback, N: Novelty Detection, A: Anti-Redundant Ranking) Indri CAFÂ´E Î³ Base +PRF Base +F +F+N +F+A +F+N+A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.35 0.36 0.36 indicator of its importance or reliability.",
                "In such a case, we can simply choose a higher value for Î³ which corresponds to a higher tolerance for redundancy, and hence let the system tune its parameters accordingly.",
                "Since documents were processed chunk by chunk, it would be interesting to see how the performance of systems improves over time.",
                "Figures 1 and 2 show the performance trends for both the systems across chunks.",
                "While the performance with and without feedback on the first few chunks is expected to be close, for subsequent chunks, the performance curve with feedback enabled rises above the one with the no-feedback setting.",
                "The performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks, making some queries easier than others in certain chunks.",
                "Moreover, since Indri uses pseudo-relevance feedback while CAFÂ´E uses feedback based on actual relevance judgments, the improvement in case of Indri is less dramatic than that of CAFÂ´E. 7.",
                "CONCLUDING REMARKS This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages.",
                "Our system, called CAFÂ´E, combines adaptive filtering, novelty detection and antiredundant passage ranking in a unified framework for utility optimization.",
                "We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty.",
                "Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off. 8.",
                "ACKNOWLEDGMENTS We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments.",
                "This work is supported in parts by the National Science Foundation (NSF) under grant IIS0434035, and the Defense Advanced Research Project Agency (DARPA) under contracts NBCHD030010 and W0550432.",
                "Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. 9.",
                "ADDITIONAL AUTHORS Jian Zhang (jianzhan@stat.purdue.edu)â , Jaime Carbonell (jgc@cs.cmu.edu)â  , Peter Brusilovsky (peterb+@pitt.edu)â¡ , Daqing He(dah44@pitt.edu)â¡ 10.",
                "REFERENCES [1] J. Allan.",
                "Incremental Relevance Feedback for Information Filtering.",
                "Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 270-278, 1996. [2] J. Allan, C. Wade, and A. Bolivar.",
                "Retrieval and Novelty Detection at the Sentence Level.",
                "Proceedings of the ACM SIGIR conference on research and development in information retrieval, 2003. [3] C. Buckley, G. Salton, and J. Allan.",
                "Automatic Retrieval with Locality Information using SMART.",
                "NIST special publication, (500207):59-72, 1993. [4] J. Callan.",
                "Learning While Filtering Documents.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 224-231, 1998. [5] J. Carbonell and J. Goldstein.",
                "The use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335-336, 1998. [6] E. Efthimiadis.",
                "Query Expansion.",
                "Annual Review of Information Science and Technology (ARIST), 31:p121-87, 1996. [7] J. Fiscus and G. Duddington.",
                "Topic Detection and Tracking Overview.",
                "Topic Detection and Tracking: Event-based Information Organization, pages 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.",
                "A Statistical Model for Multilingual Entity Detection and Tracking.",
                "NAACL/HLT, 2004. [9] K. JÂ¨arvelin and J. KekÂ¨alÂ¨ainen.",
                "Cumulated Gain-based Evaluation of IR Techniques.",
                "ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin and D. Demner-Fushman.",
                "Automatically Evaluating Answers to Definition Questions.",
                "Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), 2005. â Statistics Dept., Purdue University, West Lafayette, USA â  Language Technologies Inst., Carnegie Mellon University, Pittsburgh, USA â¡ School of Information Sciences, Univ. of Pittsburgh, Pittsburgh, USA [11] J. Lin and D. Demner-Fushman.",
                "Will Pyramids Built of nUggets Topple Over.",
                "Proceedings of HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.",
                "A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree.",
                "Proc. of ACL, 4:136-143, 2004. [13] G. Marton.",
                "Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments.",
                "HLT/NAACL, 2006. [14] E. Riloff.",
                "Automatically Constructing a Dictionary for Information Extraction Tasks.",
                "Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, 1993. [15] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9: Filtering track.",
                "The Ninth Text REtrieval Conference (TREC-9), pages 361-368. [16] R. Schapire, Y.",
                "Singer, and A. Singhal.",
                "Boosting and Rocchio Applied to Text Filtering.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle, and W. Croft.",
                "Indri: A Language Model-based Search Engine for Complex Queries.",
                "Proceedings of the International Conference on Intelligence Analysis, 2004. [18] The Linguistic Data Consortium. http://www.ldc.upenn.edu/. [19] E. Voorhees.",
                "Overview of the TREC 2003 Question Answering Track.",
                "Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), 2003. [20] Y. Yang and B. Kisiel.",
                "Margin-based Local Regression for Adaptive Filtering.",
                "Proceedings of the twelfth international conference on Information and knowledge management, pages 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of Adaptive Filtering Methods in a Cross-benchmark Evaluation.",
                "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 98-105, 2005. [22] C. Zhai, W. Cohen, and J. Lafferty.",
                "Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-17, 2003. [23] J. Zhang and Y. Yang.",
                "Robustness of Regularized Linear Classification Methods in Text Categorization.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 190-197, 2003. [24] Y. Zhang.",
                "Using Bayesian Priors to Combine Classifiers for Adaptive Filtering.",
                "Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 345-352, 2004. [25] Y. Zhang, J. Callan, and T. Minka.",
                "Novelty and Redundancy Detection in Adaptive Filtering.",
                "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002."
            ],
            "original_annotated_samples": [
                "The incremental learning nature of AF systems makes them more powerful than standard search engines that support <br>ad-hoc retrieval</br> (e.g.",
                "Our framework combines and extends the power of adaptive filtering (AF), <br>ad-hoc retrieval</br> (IR) and novelty detection (ND).",
                "None of the existing measures in <br>ad-hoc retrieval</br>, adaptive filtering, novelty detection or other related areas (text summarization and question answering) have desirable properties in all the three aspects."
            ],
            "translated_annotated_samples": [
                "La naturaleza de aprendizaje incremental de los sistemas de AF los hace mÃ¡s poderosos que los motores de bÃºsqueda estÃ¡ndar que admiten <br>recuperaciÃ³n ad-hoc</br> (por ejemplo.",
                "Nuestro marco de trabajo combina y amplÃ­a el poder del filtrado adaptativo (AF), la <br>recuperaciÃ³n ad-hoc</br> (IR) y la detecciÃ³n de novedades (ND).",
                "Ninguna de las medidas existentes en la <br>recuperaciÃ³n ad-hoc</br>, filtrado adaptativo, detecciÃ³n de novedades u otras Ã¡reas relacionadas (resumen de texto y respuesta a preguntas) tiene propiedades deseables en los tres aspectos."
            ],
            "translated_text": "DestilaciÃ³n de informaciÃ³n basada en utilidad sobre documentos secuenciados temporalmente en el Instituto de TecnologÃ­as del Lenguaje de Yiming Yang. Universidad Carnegie Mellon, Pittsburgh, EE. UU. yiming@cs.cmu.edu Abhimanyu Lad Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. alad@cs.cmu.edu Instituto de TecnologÃ­as del Lenguaje Ni Lao. Universidad Carnegie Mellon, Pittsburgh, EE. UU. nlao@cs.cmu.edu Abhay Harpale Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. aharpale@cs.cmu.edu Bryan Kisiel Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. bkisiel@cs.cmu.edu MÃ³nica Rogati Instituto de TecnologÃ­as del Lenguaje. La Universidad Carnegie Mellon en Pittsburgh, EE. UU. mrogati@cs.cmu.edu RESUMEN Este artÃ­culo examina un nuevo enfoque para la destilaciÃ³n de informaciÃ³n en documentos ordenados temporalmente, y propone un esquema de evaluaciÃ³n novedoso para dicho marco. Combina las fortalezas y se extiende mÃ¡s allÃ¡ del filtrado adaptativo convencional, la detecciÃ³n de novedades y la clasificaciÃ³n de pasajes no redundantes con respecto a las necesidades de informaciÃ³n duraderas (tareas con mÃºltiples consultas). Nuestro enfoque respalda la retroalimentaciÃ³n detallada del usuario a travÃ©s del resaltado de fragmentos arbitrarios de texto, y aprovecha dicha informaciÃ³n para la optimizaciÃ³n de la utilidad en entornos adaptativos. Para nuestros experimentos, definimos tareas hipotÃ©ticas basadas en eventos de noticias en el corpus TDT4, con mÃºltiples consultas por tarea. Se generaron claves de respuestas (nuggets) para cada consulta y se utilizÃ³ un procedimiento semiautomÃ¡tico para adquirir reglas que permiten emparejar automÃ¡ticamente los nuggets con las respuestas del sistema. TambiÃ©n proponemos una extensiÃ³n de la mÃ©trica NDCG para evaluar la utilidad de los pasajes clasificados como una combinaciÃ³n de relevancia y novedad. Nuestros resultados muestran mejoras alentadoras en la utilidad utilizando el nuevo enfoque, en comparaciÃ³n con los sistemas base sin aprendizaje incremental o componentes de detecciÃ³n de novedades. CategorÃ­as y Descriptores de Asignaturas H.3.3 [BÃºsqueda y RecuperaciÃ³n de InformaciÃ³n]: Filtrado de informaciÃ³n, RetroalimentaciÃ³n de relevancia, Modelos de recuperaciÃ³n, Proceso de selecciÃ³n; I.5.2 TÃ©rminos Generales DiseÃ±o, MediciÃ³n, Rendimiento, ExperimentaciÃ³n. 1. INTRODUCCIÃN Seguir informaciÃ³n nueva y relevante de flujos de datos temporales para usuarios con necesidades duraderas ha sido un tema de investigaciÃ³n desafiante en la recuperaciÃ³n de informaciÃ³n. El filtrado adaptativo (AF) es una tarea de predicciÃ³n en lÃ­nea de la relevancia de cada nuevo documento con respecto a temas predefinidos. BasÃ¡ndose en la consulta inicial y algunos ejemplos positivos (si estÃ¡n disponibles), un sistema de AF mantiene un perfil para cada tema de interÃ©s, y lo actualiza constantemente segÃºn la retroalimentaciÃ³n del usuario. La naturaleza de aprendizaje incremental de los sistemas de AF los hace mÃ¡s poderosos que los motores de bÃºsqueda estÃ¡ndar que admiten <br>recuperaciÃ³n ad-hoc</br> (por ejemplo. Google y Yahoo) en tÃ©rminos de encontrar informaciÃ³n relevante con respecto a temas de interÃ©s duraderos, y mÃ¡s atractivo para los usuarios que estÃ¡n dispuestos a proporcionar retroalimentaciÃ³n para adaptar el sistema a sus necesidades de informaciÃ³n especÃ­ficas, sin tener que modificar sus consultas manualmente. Se han estudiado una variedad de algoritmos de aprendizaje supervisado (clasificadores de estilo Rocchio, modelos exponenciales-gaussianos, regresiÃ³n local y enfoques de regresiÃ³n logÃ­stica) para entornos adaptativos, examinados con retroalimentaciÃ³n de relevancia explÃ­cita e implÃ­cita, y evaluados con respecto a la optimizaciÃ³n de la utilidad en grandes colecciones de datos de referencia en los foros de TREC (Conferencias de RecuperaciÃ³n de Texto) y TDT (DetecciÃ³n y Seguimiento de Temas) [1, 4, 7, 15, 16, 20, 24, 23]. La regresiÃ³n logÃ­stica regularizada [21] se ha encontrado representativa para los enfoques de vanguardia, y altamente eficiente para adaptaciones frecuentes del modelo en grandes colecciones de documentos como el corpus TREC-10 (mÃ¡s de 800,000 documentos y 84 temas). A pesar de los logros sustanciales en la investigaciÃ³n reciente de filtrado adaptativo, aÃºn quedan problemas significativos sin resolver sobre cÃ³mo aprovechar de manera efectiva y eficiente la retroalimentaciÃ³n del usuario. EspecÃ­ficamente, los siguientes problemas pueden limitar seriamente la verdadera utilidad de los sistemas de AF en aplicaciones del mundo real: 1. El usuario tiene un papel bastante pasivo en la configuraciÃ³n convencional de filtrado adaptativo: solo reacciona ante el sistema cuando este toma una decisiÃ³n afirmativa sobre un documento, confirmando o rechazando esa decisiÃ³n. Una alternativa mÃ¡s activa serÃ­a permitir al usuario emitir mÃºltiples consultas sobre un tema, revisar una lista clasificada de documentos candidatos (o pasajes) por consulta, y proporcionar retroalimentaciÃ³n sobre la lista clasificada, refinando asÃ­ su necesidad de informaciÃ³n y solicitando listas clasificadas actualizadas. La Ãºltima forma de interacciÃ³n del usuario ha sido altamente efectiva en la recuperaciÃ³n estÃ¡ndar para consultas ad-hoc. CÃ³mo implementar una estrategia para necesidades de informaciÃ³n duraderas en entornos de FA es una pregunta abierta para la investigaciÃ³n. 2. La unidad para recibir un juicio de relevancia (sÃ­ o no) estÃ¡ restringida al nivel del documento en AF convencional. Sin embargo, un usuario real puede estar dispuesto a proporcionar comentarios mÃ¡s informativos y detallados resaltando algunas partes de texto en un documento recuperado como relevantes, en lugar de etiquetar todo el documento como relevante. Aprovechar de manera efectiva este tipo de retroalimentaciÃ³n detallada podrÃ­a mejorar sustancialmente la calidad de un sistema de AF. Para esto, necesitamos habilitar el aprendizaje supervisado a partir de fragmentos de texto etiquetados de longitud arbitraria en lugar de solo permitir documentos etiquetados. 3. Los documentos seleccionados por el sistema suelen ser altamente redundantes. Un evento importante de noticias, por ejemplo, serÃ­a reportado por mÃºltiples fuentes repetidamente durante un tiempo, haciendo que la mayor parte del contenido informativo en esos artÃ­culos sea redundante entre sÃ­. Un sistema de AF convencional seleccionarÃ­a todas estas historias de noticias redundantes para la retroalimentaciÃ³n del usuario, desperdiciando el tiempo de los usuarios mientras ofrece poco beneficio. Claramente, las tÃ©cnicas de detecciÃ³n de novedades pueden ayudar en principio [25, 2, 22] a mejorar la utilidad de los sistemas AF. Sin embargo, la efectividad de tales tÃ©cnicas a nivel de pasaje para detectar novedad con respecto a la retroalimentaciÃ³n de los usuarios a nivel detallado y para detectar redundancia en listas clasificadas aÃºn debe ser evaluada utilizando una medida de utilidad que imite las necesidades de un usuario real. Para abordar las limitaciones actuales de los sistemas de AF mencionadas anteriormente, proponemos y examinamos un nuevo enfoque en este artÃ­culo, que combina las fortalezas del AF convencional (aprendizaje incremental de modelos de temas), recuperaciÃ³n de pasajes multipase para consultas de larga duraciÃ³n condicionadas al tema, y detecciÃ³n de novedades para eliminar la redundancia de las interacciones del usuario con el sistema. Llamamos al nuevo proceso destilaciÃ³n de informaciÃ³n basada en utilidad. Ten en cuenta que los corpus de referencia convencionales para evaluaciones de AF, que tienen juicios de relevancia a nivel de documento y no definen tareas con mÃºltiples consultas, son insuficientes para evaluar el nuevo enfoque. Por lo tanto, ampliamos un corpus de referencia, la colecciÃ³n TDT4 de noticias y transmisiones de televisiÃ³n, con definiciones de tareas, mÃºltiples consultas por tarea y claves de respuestas por consulta. Hemos realizado nuestros experimentos en este corpus extendido TDT4 y hemos puesto a disposiciÃ³n del pÃºblico los datos generados adicionalmente para futuras evaluaciones comparativas. Para evaluar automÃ¡ticamente los fragmentos de texto arbitrarios devueltos por el sistema utilizando nuestras claves de respuestas, desarrollamos un esquema de evaluaciÃ³n con un procedimiento semiautomÃ¡tico para 1 URL: http://nyc.lti.cs.cmu.edu/downloads adquiriendo reglas que pueden comparar fragmentos con las respuestas del sistema. AdemÃ¡s, proponemos una extensiÃ³n de NDCG (Normalized Discounted Cumulated Gain) [9] para evaluar la utilidad de los pasajes clasificados como una funciÃ³n tanto de la relevancia como de la novedad. El resto de este documento estÃ¡ organizado de la siguiente manera. La secciÃ³n 2 describe el proceso de destilaciÃ³n de informaciÃ³n con un ejemplo concreto. La secciÃ³n 3 describe los nÃºcleos tÃ©cnicos de nuestro sistema llamado CAFÂ´E - CMU Adaptive Filtering Engine. La secciÃ³n 4 discute problemas con respecto a la metodologÃ­a de evaluaciÃ³n y propone un nuevo esquema. La secciÃ³n 5 describe el corpus extendido TDT4. La secciÃ³n 6 presenta nuestros experimentos y resultados. La secciÃ³n 7 concluye el estudio y ofrece perspectivas futuras. 2. Una TAREA DE EJEMPLO Considera un evento noticioso: la fuga de siete convictos de una prisiÃ³n de Texas en diciembre de 2000 y su captura un mes despuÃ©s. Suponiendo que un usuario estuviera interesado en este evento desde sus primeras etapas, la necesidad de informaciÃ³n podrÃ­a ser: Encontrar informaciÃ³n sobre la fuga de convictos de una prisiÃ³n de Texas, y la informaciÃ³n relacionada con su recaptura. Las preguntas de nivel inferior asociadas podrÃ­an ser: 1. Â¿CuÃ¡ntos prisioneros escaparon? 2. Â¿DÃ³nde y cuÃ¡ndo fueron avistados? 3. Â¿QuiÃ©nes son sus contactos conocidos dentro y fuera de la prisiÃ³n? 4. Â¿CÃ³mo estÃ¡n armados? 5. Â¿Tienen algÃºn vehÃ­culo? 6. Â¿QuÃ© pasos se han tomado hasta ahora? Llamamos a esta necesidad de informaciÃ³n una tarea, y a las preguntas asociadas como las consultas en esta tarea. Un sistema de destilaciÃ³n debe monitorear los documentos entrantes, procesarlos por fragmentos en un orden temporal, seleccionar pasajes potencialmente relevantes y novedosos de cada fragmento con respecto a cada consulta, y presentar una lista clasificada de pasajes al usuario. La clasificaciÃ³n de pasajes aquÃ­ se basa en cuÃ¡n relevante es un pasaje con respecto a la consulta actual, cuÃ¡n novedoso es con respecto al historial actual del usuario (de sus interacciones con el sistema) y cuÃ¡n redundante es en comparaciÃ³n con otros pasajes con un rango mÃ¡s alto en la lista. Cuando se le presenta una lista de pasajes, el usuario puede proporcionar retroalimentaciÃ³n resaltando tramos de texto arbitrarios que considere relevantes. Estos fragmentos de texto se toman como ejemplos positivos en la adaptaciÃ³n del perfil de consulta, y tambiÃ©n se aÃ±aden al historial de los usuarios. Los pasajes no marcados por el usuario se consideran ejemplos negativos. Tan pronto como se actualiza el perfil de consulta, el sistema vuelve a realizar una bÃºsqueda y devuelve otra lista clasificada de pasajes donde los pasajes previamente vistos son eliminados o clasificados como bajos, segÃºn la preferencia del usuario. Por ejemplo, si el usuario resalta ...los funcionarios han ofrecido una recompensa de $100,000 por su captura... como respuesta relevante a la consulta Â¿QuÃ© pasos se han tomado hasta ahora?, entonces la pieza resaltada se utiliza como un ejemplo de entrenamiento positivo adicional en la adaptaciÃ³n del perfil de la consulta. Este comentario tambiÃ©n se aÃ±ade al historial del usuario como un ejemplo visto, para que en el futuro, el sistema no coloque otro pasaje mencionando una recompensa de $100,000 en la parte superior de la lista clasificada. Sin embargo, un artÃ­culo que mencione que los funcionarios han duplicado el dinero de recompensa a $200,000 podrÃ­a clasificarse alto ya que es relevante tanto para el perfil de consulta (actualizado) como novedoso con respecto a la historia del usuario (actualizada). El usuario puede modificar las consultas originales o agregar una nueva consulta durante el proceso; los perfiles de consulta se cambiarÃ¡n en consecuencia. Claramente, la detecciÃ³n de novedades es muy importante para la utilidad de dicho sistema debido a la bÃºsqueda iterativa. Sin detecciÃ³n de novedad, los antiguos pasajes relevantes se mostrarÃ­an al usuario repetidamente en cada lista clasificada. A travÃ©s del ejemplo anterior, podemos ver las principales propiedades de nuestro nuevo marco de trabajo para la destilaciÃ³n de informaciÃ³n basada en utilidad sobre documentos ordenados temporalmente. Nuestro marco de trabajo combina y amplÃ­a el poder del filtrado adaptativo (AF), la <br>recuperaciÃ³n ad-hoc</br> (IR) y la detecciÃ³n de novedades (ND). En comparaciÃ³n con la IR estÃ¡ndar, nuestro enfoque tiene la capacidad de aprender de forma incremental las necesidades de informaciÃ³n a largo plazo y modelar una secuencia de consultas dentro de una tarea. En comparaciÃ³n con el AF convencional, permite un papel mÃ¡s activo del usuario en refinar sus necesidades de informaciÃ³n y solicitar nuevos resultados al permitir comentarios sobre relevancia y novedad mediante el resaltado de fragmentos arbitrarios de texto en los pasajes devueltos por el sistema. Comparado con trabajos anteriores, esta es la primera evaluaciÃ³n de detecciÃ³n de novedad integrada con filtrado adaptativo para consultas secuenciadas que permite retroalimentaciÃ³n flexible del usuario sobre pasajes clasificados. La combinaciÃ³n de AF, IR y ND con las nuevas extensiones plantea una importante pregunta de investigaciÃ³n sobre la metodologÃ­a de evaluaciÃ³n: Â¿cÃ³mo podemos medir la utilidad de un sistema de destilaciÃ³n de informaciÃ³n como este? Las mÃ©tricas existentes en IR estÃ¡ndar, AF y ND son insuficientes, y se deben explorar nuevas soluciones, como discutiremos en la SecciÃ³n 4, despuÃ©s de describir los nÃºcleos tÃ©cnicos de nuestro sistema en la prÃ³xima secciÃ³n. 3. NÃCLEOS TÃCNICOS Los componentes principales de CAFÃ son: 1) AF para el aprendizaje incremental de perfiles de consulta, 2) IR para estimar la relevancia de pasajes con respecto a los perfiles de consulta, 3) ND para evaluar la novedad de pasajes con respecto al historial de usuarios, y 4) un componente anti-redundancia para eliminar la redundancia de las listas clasificadas. 3.1 Componente de Filtrado Adaptativo Utilizamos un algoritmo de vanguardia en el campo, el mÃ©todo de regresiÃ³n logÃ­stica regularizada que obtuvo los mejores resultados en varios corpus de evaluaciÃ³n de referencia para AF [21]. La regresiÃ³n logÃ­stica (LR) es un algoritmo de aprendizaje supervisado para clasificaciÃ³n estadÃ­stica. BasÃ¡ndose en un conjunto de entrenamiento de instancias etiquetadas, aprende un modelo de clase que luego puede ser utilizado para predecir las etiquetas de instancias no vistas. Su rendimiento, asÃ­ como su eficiencia en tÃ©rminos de tiempo de entrenamiento, lo convierten en un buen candidato cuando se requieren actualizaciones frecuentes del modelo de clase, como es el caso en el filtrado adaptativo, donde el sistema debe aprender de cada nueva retroalimentaciÃ³n proporcionada por el usuario. (Consulte [21] y [23] para complejidad computacional y problemas de implementaciÃ³n). En el filtrado adaptativo, cada consulta se considera como una clase y la probabilidad de que un pasaje pertenezca a esta clase corresponde al grado de relevancia del pasaje con respecto a la consulta. Para entrenar el modelo, utilizamos la propia consulta como ejemplo inicial de entrenamiento positivo de la clase, y las piezas de texto resaltadas por el usuario (marcadas como Relevante o No relevante) durante la retroalimentaciÃ³n como ejemplos de entrenamiento adicionales. Para abordar el problema de inicio en frÃ­o en la etapa inicial antes de obtener cualquier retroalimentaciÃ³n del usuario, el sistema utiliza una pequeÃ±a muestra de un corpus retrospectivo como ejemplos negativos iniciales en el conjunto de entrenamiento. Los detalles de cÃ³mo utilizar la regresiÃ³n logÃ­stica para el filtrado adaptativo (asignando diferentes pesos a las instancias de entrenamiento positivas y negativas, y regularizando la funciÃ³n objetivo para prevenir el sobreajuste en los datos de entrenamiento) se presentan en [21]. El modelo de clase wâ aprendido por RegresiÃ³n LogÃ­stica, o el perfil de consulta, es un vector cuyas dimensiones son tÃ©rminos individuales y cuyos elementos son los coeficientes de regresiÃ³n, indicando cuÃ¡n influyente es cada tÃ©rmino en el perfil de consulta. El perfil de la consulta se actualiza cada vez que se recibe una nueva pieza de retroalimentaciÃ³n del usuario. Un peso decaÃ­do temporalmente se puede aplicar a cada ejemplo de entrenamiento, como una opciÃ³n, para enfatizar la retroalimentaciÃ³n mÃ¡s reciente del usuario. 3.2 Componente de RecuperaciÃ³n de Pasajes Utilizamos tÃ©cnicas estÃ¡ndar de IR en esta parte de nuestro sistema. Los documentos entrantes se procesan por bloques, donde cada bloque puede definirse como un intervalo de tiempo fijo o como un nÃºmero fijo de documentos, segÃºn lo prefiera el usuario. Para cada documento entrante, se actualizan las estadÃ­sticas del corpus como el IDF (Frecuencia Inversa de Documento) de cada tÃ©rmino. Utilizamos un identificador y rastreador de entidades con tecnologÃ­a de punta [8, 12] para identificar nombres de personas y lugares, y fusionarlos con entidades con referencia cruzada vistas en el pasado. Luego, los documentos se dividen en pasajes, que pueden ser un documento completo, un pÃ¡rrafo, una oraciÃ³n, o cualquier otro fragmento continuo de texto, segÃºn se prefiera. Cada pasaje se representa utilizando un vector de pesos TF-IDF (Frecuencia de TÃ©rmino - Frecuencia Inversa de Documento), donde el tÃ©rmino puede ser una palabra o una entidad nombrada. Dado un perfil de consulta, es decir, la soluciÃ³n de regresiÃ³n logÃ­stica wâ como se describe en la SecciÃ³n 3.1, el sistema calcula la probabilidad posterior de relevancia para cada pasaje x como fRL(x) â¡ P(y = 1|x, wâ ) = 1 (1 + eâwâÂ·x) (1). Los pasajes se ordenan segÃºn sus puntajes de relevancia, y aquellos con puntajes por encima de un umbral (ajustado en un conjunto de entrenamiento) conforman la lista de relevancia que se pasa al paso de detecciÃ³n de novedad. 3.3 Componente de DetecciÃ³n de Novedad CAFÂ´E mantiene un historial de usuario H(t), que contiene todos los fragmentos de texto hi que el usuario resaltÃ³ (como retroalimentaciÃ³n) durante sus interacciones pasadas con el sistema, hasta el tiempo actual t. Denotando el historial como H(t) = n h1, h2, ..., ht o , (2), el puntaje de novedad de un nuevo pasaje candidato x se calcula como: fND(x) = 1 â max iâ1..t {cos(x, hi)} (3), donde tanto el pasaje candidato x como los fragmentos de texto resaltados hi se representan como vectores TF-IDF. El puntaje de novedad de cada pasaje se compara con un umbral predefinido (ajustado tambiÃ©n en un conjunto de entrenamiento), y cualquier pasaje con un puntaje por debajo de este umbral se elimina de la lista de relevancia. Componente de clasificaciÃ³n anti-redundante Aunque el componente de detecciÃ³n de novedad garantiza que solo la informaciÃ³n novedosa (no vista previamente) permanezca en la lista de relevancia, esta lista aÃºn podrÃ­a contener la misma informaciÃ³n novedosa en mÃºltiples posiciones en la lista clasificada. Supongamos, por ejemplo, que el usuario ya ha leÃ­do sobre una recompensa de $100,000 por informaciÃ³n sobre los convictos fugados. Una nueva noticia de que el premio ha sido aumentado a $200,000 es novedosa ya que el usuario aÃºn no la ha leÃ­do. Sin embargo, mÃºltiples fuentes de noticias podrÃ­an informar sobre esta noticia y podrÃ­amos terminar mostrando artÃ­culos (redundantes) de todas estas fuentes en una lista clasificada. Por lo tanto, una lista clasificada tambiÃ©n debe ser no redundante con respecto a su propio contenido. Utilizamos una versiÃ³n simplificada del mÃ©todo de MÃ¡xima Relevancia Marginal [5], originalmente desarrollado para combinar relevancia y novedad en la recuperaciÃ³n de texto y resÃºmenes. Nuestro procedimiento comienza con la lista actual de pasajes ordenados por relevancia (secciÃ³n 3.2), filtrados por el componente de DetecciÃ³n de Novedades (secciÃ³n 3.3), y genera una nueva lista no redundante de la siguiente manera: 1. Toma el pasaje superior en la lista actual como el primero en la nueva lista. 2. Agrega el siguiente pasaje x en la lista actual a la nueva lista solo si fAR(x) > t donde fAR(x) = 1 â max piâLnew {cos(x, pi)} y Lnew es el conjunto de pasajes ya seleccionados en la nueva lista. Repetir el paso 2 hasta que se hayan examinado todos los pasajes en la lista actual. DespuÃ©s de aplicar el algoritmo mencionado anteriormente, cada pasaje en la nueva lista es lo suficientemente diferente a los demÃ¡s, favoreciendo asÃ­ la diversidad en lugar de la redundancia en la nueva lista clasificada. El umbral anti-redundancia t se ajusta en un conjunto de entrenamiento. 4. METODOLOGÃA DE EVALUACIÃN El enfoque que propusimos anteriormente para la destilaciÃ³n de informaciÃ³n plantea importantes cuestiones sobre la metodologÃ­a de evaluaciÃ³n. En primer lugar, dado que nuestro marco de trabajo permite que la salida sean fragmentos en diferentes niveles de granularidad (por ejemplo, ventanas de k oraciones donde k puede variar) en lugar de una longitud fija, no es posible tener juicios de relevancia preanotados en todos esos niveles de granularidad. En segundo lugar, dado que deseamos medir la utilidad de la salida del sistema como una combinaciÃ³n de relevancia y novedad, las medidas tradicionales basadas Ãºnicamente en la relevancia deben ser reemplazadas por medidas que penalicen la repeticiÃ³n de la misma informaciÃ³n en la salida del sistema a lo largo del tiempo. En tercer lugar, dado que la salida del sistema son listas clasificadas, debemos recompensar a aquellos sistemas que presenten informaciÃ³n Ãºtil (tanto relevante como no vista previamente) utilizando listas clasificadas mÃ¡s cortas, y penalizar a aquellos que presenten la misma informaciÃ³n utilizando listas clasificadas mÃ¡s largas. Ninguna de las medidas existentes en la <br>recuperaciÃ³n ad-hoc</br>, filtrado adaptativo, detecciÃ³n de novedades u otras Ã¡reas relacionadas (resumen de texto y respuesta a preguntas) tiene propiedades deseables en los tres aspectos. Por lo tanto, debemos desarrollar una nueva metodologÃ­a de evaluaciÃ³n. 4.1 Claves de Respuesta Para permitir la evaluaciÃ³n de un sistema cuya salida consiste en pasajes de longitud arbitraria, tomamos prestado el concepto de claves de respuesta de la comunidad de Preguntas y Respuestas (QA), donde los sistemas pueden devolver fragmentos de texto arbitrarios como respuestas. Las claves de respuestas definen lo que debe estar presente en la respuesta de un sistema para recibir crÃ©dito, y estÃ¡n compuestas por una colecciÃ³n de \"nuggets\" de informaciÃ³n, es decir, unidades de datos sobre las cuales los evaluadores humanos pueden tomar decisiones binarias sobre si una respuesta del sistema las contiene o no. La definiciÃ³n de claves de respuestas y la toma de decisiones binarias asociadas son tareas conceptuales que requieren un mapeo semÃ¡ntico [19], ya que los pasajes devueltos por el sistema pueden contener la misma informaciÃ³n expresada de muchas formas diferentes. Por lo tanto, las evaluaciones de QA han dependido de evaluadores humanos para la correspondencia entre diversas expresiones, lo que hace que el proceso sea costoso, consume mucho tiempo y no sea escalable para grandes colecciones de consultas y documentos, y evaluaciones extensas de sistemas con diversos ajustes de parÃ¡metros. 4.1.1 AutomatizaciÃ³n de la evaluaciÃ³n basada en claves de respuestas Los mÃ©todos de evaluaciÃ³n automÃ¡tica permitirÃ­an una construcciÃ³n y ajuste mÃ¡s rÃ¡pidos del sistema, asÃ­ como proporcionar una forma objetiva y asequible de comparar diversos sistemas. Recientemente, se han propuesto mÃ©todos basados mÃ¡s o menos en la idea de las co-ocurrencias de n-gramos. Pourpre [10] asigna una puntuaciÃ³n de recuerdo fraccional a una respuesta del sistema basada en su superposiciÃ³n de unigramas con una descripciÃ³n de pepitas dada. Por ejemplo, una respuesta del sistema A B C tiene una recuperaciÃ³n de 3/4 con respecto a un fragmento con la descripciÃ³n A B C D. Sin embargo, este enfoque es injusto para los sistemas que presentan la misma informaciÃ³n pero utilizando palabras distintas a A, B, C y D. Otro problema abierto es cÃ³mo ponderar las palabras individuales al medir la cercanÃ­a de una coincidencia. Por ejemplo, considera la pregunta Â¿CuÃ¡ntos prisioneros escaparon? En el fragmento Siete prisioneros escaparon de una prisiÃ³n de Texas, no hay indicaciÃ³n de que siete sea la palabra clave y que debe coincidir para obtener algÃºn crÃ©dito de relevancia. El uso de los valores de IDF no ayuda, ya que la palabra \"seven\" generalmente no tendrÃ¡ un IDF mÃ¡s alto que palabras como \"texas\" y \"prison\". AdemÃ¡s, redefinir el fragmento como simplemente siete no resuelve el problema, ya que ahora podrÃ­a coincidir errÃ³neamente con cualquier menciÃ³n de siete fuera de contexto. Nuggeteer [13] trabaja sobre principios similares pero toma decisiones binarias sobre si hay un fragmento en una respuesta de sistema dada ajustando un umbral. Sin embargo, tambiÃ©n estÃ¡ plagado de relevancia espuria ya que no todas las palabras contenidas en la descripciÃ³n de la pepita (o respuestas correctas conocidas) son centrales para la pepita. 4.1.2 Reglas de Coincidencia de Pepitas Proponemos un mÃ©todo automÃ¡tico confiable para determinar si un fragmento de texto contiene una pepita dada, basado en reglas de coincidencia de pepitas, que se generan utilizando un procedimiento semiautomÃ¡tico explicado a continuaciÃ³n. Estas reglas son bÃ¡sicamente consultas booleanas que solo coincidirÃ¡n con fragmentos que contengan la pepita. Por ejemplo, una regla candidata para emparejar respuestas a Â¿CuÃ¡ntos prisioneros escaparon? es (Texas Y siete Y escapar Y (convictos O prisioneros)), posiblemente con otros sinÃ³nimos y variantes en la regla. Para un corpus de artÃ­culos de noticias, que suelen seguir una prosa formal tÃ­pica, es bastante fÃ¡cil escribir reglas simples para coincidir con respuestas esperadas utilizando un enfoque de arranque, como se describe a continuaciÃ³n. Proponemos un enfoque de dos etapas, inspirado en Autoslog [14], que combina la capacidad de los humanos para identificar expresiones semÃ¡nticamente equivalentes y la capacidad del sistema para recopilar evidencia estadÃ­stica de un corpus de documentos anotados por humanos. En la primera etapa, los sujetos humanos anotaron (utilizando una herramienta de resaltado) porciones de documentos relevantes que contenÃ­an respuestas a cada fragmento 2. En la segunda etapa, los sujetos utilizaron nuestra herramienta de generaciÃ³n de reglas para crear reglas que coincidieran con las anotaciones de cada fragmento. La herramienta permite a los usuarios ingresar una regla booleana como una disyunciÃ³n de conjunciones (por ejemplo, ((a Y b) O (a Y c Y d) O (e))). Dada una regla candidata, nuestra herramienta la utiliza como una consulta booleana sobre el conjunto completo de documentos relevantes y calcula su recall y precisiÃ³n con respecto a las anotaciones que se espera que coincida. Por lo tanto, los sujetos pueden comenzar con una regla simple y refinarla iterativamente hasta que estÃ©n satisfechos con su precisiÃ³n y recuperaciÃ³n. Observamos que era muy fÃ¡cil para los humanos mejorar la precisiÃ³n de una regla ajustando sus conjunciones existentes (agregando mÃ¡s Y) y mejorar la recuperaciÃ³n agregando mÃ¡s conjunciones a la disyunciÃ³n (agregando mÃ¡s O). Como ejemplo, intentemos crear una regla para el acertijo que dice que siete prisioneros escaparon de la prisiÃ³n de Texas. Empezamos con una regla simple - (siete). Cuando introducimos esto en la herramienta de generaciÃ³n de reglas, nos damos cuenta de que esta regla coincide con muchas ocurrencias espurias de siete (por ejemplo, ...siete estados...) y, por lo tanto, obtiene una puntuaciÃ³n baja de precisiÃ³n. Podemos calificar aÃºn mÃ¡s nuestra regla - Texas Y siete Y convictos. A continuaciÃ³n, al revisar las anotaciones omitidas, nos damos cuenta de que algunos artÃ­culos de noticias mencionaban ...siete prisioneros escaparon.... Luego reemplazamos convictos por la disyunciÃ³n (convictos O prisioneros). Continuamos ajustando la regla de esta manera hasta lograr un nivel de recordaciÃ³n y precisiÃ³n suficientemente alto, es decir, que los (pocos) errores y alarmas falsas se puedan ignorar de forma segura. AsÃ­ podemos crear reglas de coincidencia de pepitas que capturen de manera sucinta varias formas de expresar una pepita, evitando al mismo tiempo emparejar respuestas incorrectas (o fuera de contexto). La participaciÃ³n humana en el proceso de creaciÃ³n de reglas garantiza reglas genÃ©ricas de alta calidad que luego se pueden utilizar para evaluar de manera confiable las respuestas del sistema arbitrario. 4.2 EvaluaciÃ³n de la Utilidad de una Secuencia de Listas Clasificadas La utilidad de un sistema de recuperaciÃ³n se puede definir como la diferencia entre cuÃ¡nta informaciÃ³n Ãºtil obtuvo el usuario y cuÃ¡nto tiempo y energÃ­a perdiÃ³. Calculamos esta utilidad a partir de las utilidades de los pasajes individuales de la siguiente manera. DespuÃ©s de leer cada pasaje devuelto por el sistema, el usuario obtiene cierta ganancia dependiendo de la presencia de informaciÃ³n relevante y novedosa, y incurre en una pÃ©rdida en tÃ©rminos del tiempo y la energÃ­a invertidos en revisar el pasaje. Sin embargo, la probabilidad de que el usuario realmente lea un pasaje depende de su posiciÃ³n en la lista clasificada. Por lo tanto, para una consulta q, la utilidad esperada 2 LDC [18] ya proporciona juicios de relevancia para 100 temas en el corpus TDT4. AdemÃ¡s, nos aseguramos de que estos juicios sean exhaustivos en todo el corpus utilizando la agrupaciÃ³n. El valor de un pasaje pi en el rango i se puede definir como U(pi, q) = P(i) â (Ganancia(pi, q) - PÃ©rdida(pi, q)) (4) donde P(i) es la probabilidad de que el usuario pase por un pasaje en el rango i. La utilidad esperada para una lista clasificada completa de longitud n se puede calcular simplemente sumando la utilidad esperada de cada pasaje: U(q) = Î£ i=1 P(i) â (Ganancia(pi, q) â PÃ©rdida(pi, q)) (5) NÃ³tese que si ignoramos el tÃ©rmino de pÃ©rdida y definimos P(i) como P(i) â 1/ logb(b + i â 1) (6) entonces obtenemos la mÃ©trica recientemente popularizada llamada Ganancia Acumulada Descontada (DCG) [9], donde Ganancia(pi, q) se define como la relevancia graduada del pasaje pi. Sin embargo, sin el tÃ©rmino de pÃ©rdida, DCG es una mÃ©trica puramente orientada al recuerdo y no es adecuada para un entorno de filtrado adaptativo, donde la utilidad del sistema depende en parte de su capacidad para limitar el nÃºmero de elementos mostrados al usuario. Aunque P(i) podrÃ­a definirse en base a estudios empÃ­ricos del comportamiento del usuario, por simplicidad, utilizamos P(i) tal como se define exactamente en la ecuaciÃ³n 6. La ganancia G(pi, q) del pasaje pi con respecto a la consulta q es una funciÃ³n de - 1) el nÃºmero de pepitas relevantes presentes en pi, y 2) la novedad de cada una de estas pepitas. Combinamos estos dos factores de la siguiente manera. Para cada pepita Nj, asignamos un peso inicial wj, y tambiÃ©n mantenemos un conteo nj del nÃºmero de veces que esta pepita ha sido vista por el usuario en el pasado. La ganancia derivada de cada ocurrencia posterior del mismo hallazgo se asume que se reduce por un factor de amortiguamiento Î³. Por lo tanto, G(pi, q) se define como G(pi, q) = X Nj âC(pi,q) wj â Î³nj (7) donde C(pi, q) es el conjunto de todas las pepitas que aparecen en el pasaje pi y tambiÃ©n pertenecen a la clave de respuesta de la consulta q. Los pesos iniciales wj se establecen todos en 1.0 en nuestros experimentos, pero tambiÃ©n pueden establecerse segÃºn un enfoque de pirÃ¡mide [11]. La elecciÃ³n del factor de amortiguaciÃ³n Î³ determina la tolerancia del usuario a la redundancia. Cuando Î³ = 0, un pepita solo recibirÃ¡ crÃ©dito por su primera ocurrencia, es decir, cuando nj sea cero. Para 0 < Î³ < 1, un nugget recibe menos crÃ©dito por cada ocurrencia sucesiva. Cuando Î³ = 1, no se produce amortiguamiento y las repeticiones de un valor reciben el mismo crÃ©dito. Se debe tener en cuenta que los recuentos de ocurrencia de las pepitas se conservan entre la evaluaciÃ³n de las listas clasificadas sucesivas devueltas por el sistema, ya que se espera que los usuarios recuerden lo que el sistema les mostrÃ³ en el pasado. Definimos la pÃ©rdida L(pi, q) como un costo constante c (usamos 0.1) incurrido al leer un pasaje devuelto por el sistema. Por lo tanto, nuestra mÃ©trica puede ser reescrita como U(q) = nX i=1 Ganancia(pi, q) logb(b + i â 1) â L(n) (8) donde L(n) es la pÃ©rdida asociada con una lista clasificada de longitud n: L(n) = c Â· nX i=1 1 logb(b + i â 1) (9) 3 NÃ³tese que 00 = 1 Debido a la similitud con la Ganancia Acumulada Descontada (DCG), llamamos a nuestra mÃ©trica Utilidad Acumulada Descontada (DCU). El puntaje de DCU obtenido por el sistema se convierte en un puntaje de DCU normalizado (NDCU) dividiÃ©ndolo por el puntaje de DCU de la lista ideal clasificada, que se crea ordenando los pasajes por sus puntajes de utilidad decrecientes U(pi, q) y deteniÃ©ndose cuando U(pi, q) â¤ 0, es decir, cuando la ganancia es menor o igual al costo de leer el pasaje. El conjunto de datos TDT4 fue el corpus de referencia utilizado en las evaluaciones TDT2002 y TDT2003. El corpus consiste en mÃ¡s de 90,000 artÃ­culos de noticias de mÃºltiples fuentes (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) publicados entre octubre de 2000 y enero de 2001, en los idiomas Ã¡rabe, inglÃ©s y mandarÃ­n. Las versiones de los artÃ­culos en otros idiomas reconocidas por voz y traducidas por mÃ¡quina tambiÃ©n fueron proporcionadas. LDC [18] ha anotado el corpus con 100 temas, que corresponden a varios eventos de noticias en este perÃ­odo de tiempo. De entre estos, seleccionamos un subconjunto de 12 eventos accionables y definimos tareas correspondientes para ellos. Para cada tarea, definimos manualmente un perfil que consiste en un conjunto inicial de (5 a 10) consultas, una descripciÃ³n de texto libre del historial del usuario, es decir, lo que el usuario ya sabe sobre el evento, y una lista de documentos conocidos relevantes y no relevantes (si estÃ¡n disponibles) como ejemplos de entrenamiento. Para cada consulta, generamos claves de respuestas y reglas de coincidencia de pepitas correspondientes utilizando el procedimiento descrito en la secciÃ³n 4.1.2, y produjimos un total de 120 consultas, con un promedio de 7 pepitas por consulta. EXPERIMENTOS Y RESULTADOS 6.1 Baselines Utilizamos Indri [17], un motor de bÃºsqueda basado en modelos de lenguaje popular, como referencia para comparar con CAFÃ. Indri admite la funcionalidad estÃ¡ndar de los motores de bÃºsqueda, incluido el feedback de pseudo-relevancia (PRF) [3, 6], y es representativo de un sistema de recuperaciÃ³n basado en consultas tÃ­pico. Indri no admite ningÃºn tipo de detecciÃ³n de novedades. Comparamos Indri con PRF activado y desactivado, frente a CAFÃ con retroalimentaciÃ³n del usuario, detecciÃ³n de novedades y clasificaciÃ³n antirredundante activados y desactivados. 6.2 ConfiguraciÃ³n Experimental Dividimos el corpus TDT4 que abarca 4 meses en 10 fragmentos, cada uno definido como un perÃ­odo de 12 dÃ­as consecutivos. En cualquier momento dado del proceso de destilaciÃ³n, cada sistema accedÃ­a a los datos pasados hasta el punto actual, y devolvÃ­a una lista clasificada de hasta 50 pasajes por consulta. Las 12 tareas definidas en el corpus se dividieron en un conjunto de entrenamiento y prueba con 6 tareas cada uno. Cada sistema pudo utilizar el conjunto de entrenamiento para ajustar sus parÃ¡metros con el fin de optimizar NDCU (ecuaciÃ³n 8), incluyendo el umbral de relevancia tanto para Indri como para CAFÂ´E, y los umbrales de novedad y antirredundancia para CAFÂ´E. La NDCU para cada ejecuciÃ³n del sistema se calcula automÃ¡ticamente. TambiÃ©n se simulÃ³ la retroalimentaciÃ³n de los usuarios: los juicios de relevancia para cada pasaje devuelto por el sistema (segÃºn las reglas de coincidencia de pepitas descritas en la secciÃ³n 4.1.2) fueron utilizados como retroalimentaciÃ³n de los usuarios en la adaptaciÃ³n de perfiles de consulta e historiales de usuario. En la Tabla 1, mostramos los puntajes NDCU de los dos sistemas en diferentes configuraciones. Estas puntuaciones se promedian en las seis tareas del conjunto de pruebas, y se calculan con dos factores de amortiguaciÃ³n (ver secciÃ³n 4.2): Î³ = 0 y 0.1, para simular ninguna tolerancia y poca tolerancia para la redundancia, respectivamente. Usar Î³ = 0 crea una mÃ©trica mucho mÃ¡s estricta ya que no otorga ningÃºn crÃ©dito a un pasaje que contenga informaciÃ³n relevante pero redundante. Por lo tanto, la mejora obtenida al habilitar la retroalimentaciÃ³n del usuario es menor con Î³ = 0 que la mejora obtenida de la retroalimentaciÃ³n con Î³ = 0.1. Esto revela una deficiencia de los sistemas de recuperaciÃ³n contemporÃ¡neos: cuando el usuario da retroalimentaciÃ³n positiva sobre un pasaje, los sistemas otorgan mayor peso a los tÃ©rminos presentes en ese pasaje y tienden a recuperar otros pasajes que contienen los mismos tÃ©rminos, y por lo tanto, generalmente la misma informaciÃ³n. Sin embargo, el usuario no se beneficia al ver pasajes redundantes y generalmente estÃ¡ interesado en otros pasajes que contengan informaciÃ³n relacionada. Es informativo evaluar los sistemas de recuperaciÃ³n utilizando nuestra medida de utilidad (con Î³ = 0) que tiene en cuenta la novedad y, por lo tanto, ofrece una imagen mÃ¡s realista de cuÃ¡n bien un sistema puede generalizar a partir de la retroalimentaciÃ³n del usuario, en lugar de utilizar medidas tradicionales de RI como la recuperaciÃ³n y la precisiÃ³n, que ofrecen una imagen incompleta de la mejora obtenida a partir de la retroalimentaciÃ³n del usuario. A veces, sin embargo, los usuarios podrÃ­an estar interesados en ver la misma informaciÃ³n de mÃºltiples fuentes, como un indicador de su importancia o fiabilidad. En tal caso, simplemente podemos elegir un valor mÃ¡s alto para Î³ que corresponda a una mayor tolerancia para la redundancia, y asÃ­ permitir que el sistema ajuste sus parÃ¡metros en consecuencia. Dado que los documentos fueron procesados por fragmentos, serÃ­a interesante ver cÃ³mo mejora el rendimiento de los sistemas con el tiempo. Las figuras 1 y 2 muestran las tendencias de rendimiento de ambos sistemas a lo largo de los fragmentos. Si bien se espera que el rendimiento con y sin retroalimentaciÃ³n en los primeros fragmentos sea similar, para los fragmentos posteriores, la curva de rendimiento con la retroalimentaciÃ³n habilitada supera a la del ajuste sin retroalimentaciÃ³n. Las tendencias de rendimiento no son consistentes en todos los fragmentos porque los documentos relevantes no estÃ¡n distribuidos uniformemente en todos los fragmentos, lo que hace que algunas consultas sean mÃ¡s fÃ¡ciles que otras en ciertos fragmentos. AdemÃ¡s, dado que Indri utiliza retroalimentaciÃ³n de pseudo-relevancia mientras que CAFÂ´E utiliza retroalimentaciÃ³n basada en juicios de relevancia reales, la mejora en el caso de Indri es menos dramÃ¡tica que la de CAFÂ´E. 7. CONCLUSIONES FINALES Este artÃ­culo presenta la primera investigaciÃ³n sobre la destilaciÃ³n de informaciÃ³n basada en la utilidad con un sistema que aprende las necesidades de informaciÃ³n duraderas a partir de la retroalimentaciÃ³n detallada del usuario sobre una secuencia de pasajes clasificados. Nuestro sistema, llamado CAFÂ´E, combina filtrado adaptativo, detecciÃ³n de novedades y clasificaciÃ³n de pasajes antirredundantes en un marco unificado para la optimizaciÃ³n de la utilidad. Desarrollamos un nuevo esquema para la evaluaciÃ³n y retroalimentaciÃ³n automatizadas basado en un procedimiento semiautomÃ¡tico para adquirir reglas que permitan emparejar automÃ¡ticamente fragmentos con las respuestas del sistema. TambiÃ©n propusimos una extensiÃ³n de la mÃ©trica NDCG para evaluar la utilidad de los pasajes clasificados como una combinaciÃ³n ponderada de relevancia y novedad. Nuestros experimentos en el reciÃ©n anotado corpus de referencia TDT4 muestran un alentador aumento de utilidad sobre Indri, y tambiÃ©n sobre nuestro propio sistema con aprendizaje incremental y detecciÃ³n de novedades desactivados. 8. AGRADECIMIENTOS Nos gustarÃ­a agradecer a Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng y al Programa de AnÃ¡lisis de Datos Cualitativos de la Universidad de Pittsburgh liderado por el Dr. Stuart Shulman por su ayuda en la recolecciÃ³n y procesamiento de las anotaciones extendidas de TDT4 utilizadas en nuestros experimentos. Este trabajo estÃ¡ parcialmente respaldado por la FundaciÃ³n Nacional de Ciencias (NSF) bajo la subvenciÃ³n IIS0434035, y la Agencia de Proyectos de InvestigaciÃ³n Avanzada de Defensa (DARPA) bajo los contratos NBCHD030010 y W0550432. Cualquier opiniÃ³n, hallazgo, conclusiÃ³n o recomendaciÃ³n expresada en este material son responsabilidad de los autores y no reflejan necesariamente los puntos de vista de los patrocinadores. AUTORES ADICIONALES Jian Zhang (jianzhan@stat.purdue.edu)â , Jaime Carbonell (jgc@cs.cmu.edu)â  , Peter Brusilovsky (peterb+@pitt.edu)â¡ , Daqing He(dah44@pitt.edu)â¡ 10. REFERENCIAS [1] J. Allan. RetroalimentaciÃ³n incremental de relevancia para la filtraciÃ³n de informaciÃ³n. Actas de la 19Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 270-278, 1996. [2] J. Allan, C. Wade y A. Bolivar. RecuperaciÃ³n y DetecciÃ³n de Novedad a Nivel de OraciÃ³n. Actas de la conferencia ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, 2003. [3] C. Buckley, G. Salton y J. Allan. RecuperaciÃ³n automÃ¡tica con informaciÃ³n de localidad utilizando SMART. PublicaciÃ³n especial del NIST, (500207):59-72, 1993. [4] J. Callan. Aprendiendo mientras se filtran documentos. Actas de la 21Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 224-231, 1998. [5] J. Carbonell y J. Goldstein. El uso de MMR, Reordenamiento basado en Diversidad para Reorganizar Documentos y Producir ResÃºmenes. Actas de la 21Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 335-336, 1998. [6] E. Efthimiadis. ExpansiÃ³n de consulta. RevisiÃ³n Anual de Ciencia de la InformaciÃ³n y TecnologÃ­a (ARIST), 31:p121-87, 1996. [7] J. Fiscus y G. Duddington. Resumen de DetecciÃ³n y Seguimiento de Temas. DetecciÃ³n y seguimiento de temas: OrganizaciÃ³n de la informaciÃ³n basada en eventos, pÃ¡ginas 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov y S. Roukos. Un modelo estadÃ­stico para la detecciÃ³n y seguimiento de entidades multilingÃ¼es. NAACL/HLT, 2004. [9] K. JÃ¤rvelin y J. KekÃ¤lÃ¤inen. EvaluaciÃ³n basada en la Ganancia Acumulada de TÃ©cnicas de RecuperaciÃ³n de InformaciÃ³n. ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin y D. Demner-Fushman. EvaluaciÃ³n automÃ¡tica de respuestas a preguntas de definiciÃ³n. Actas de la Conferencia de TecnologÃ­a del Lenguaje Humano y Conferencia sobre MÃ©todos EmpÃ­ricos en Procesamiento del Lenguaje Natural (HLT/EMNLP 2005), 2005. â Departamento de EstadÃ­stica, Universidad de Purdue, West Lafayette, EE. UU. â  Instituto de TecnologÃ­as del Lenguaje, Universidad Carnegie Mellon, Pittsburgh, EE. UU. â¡ Escuela de Ciencias de la InformaciÃ³n, Universidad de Pittsburgh, Pittsburgh, EE. UU. [11] J. Lin y D. Demner-Fushman. Â¿Se derrumbarÃ¡n las pirÃ¡mides construidas con pepitas? Actas de HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla y S. Roukos. Un algoritmo de resoluciÃ³n de correferencia sÃ­ncrona basado en el Ã¡rbol de Bell. Actas de ACL, 4:136-143, 2004. [13] G. Marton. Nuggeteer: EvaluaciÃ³n AutomÃ¡tica Basada en Pepitas Utilizando Descripciones y Juicios. HLT/NAACL, 2006. [14] E. Riloff. ConstrucciÃ³n automÃ¡tica de un diccionario para tareas de extracciÃ³n de informaciÃ³n. Actas de la UndÃ©cima Conferencia Nacional sobre Inteligencia Artificial, pÃ¡ginas 811-816, 1993. [15] S. Robertson y S. Walker. Microsoft Cambridge en TREC-9: Pista de filtrado. La Novena Conferencia de RecuperaciÃ³n de Texto (TREC-9), pÃ¡ginas 361-368. [16] R. Schapire, Y. Cantante, y A. Singhal. Boosting y Rocchio aplicados a la filtraciÃ³n de texto. Actas de la 21Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle y W. Croft. Indri: Un motor de bÃºsqueda basado en modelos de lenguaje para consultas complejas. Actas de la Conferencia Internacional sobre AnÃ¡lisis de Inteligencia, 2004. [18] El Consorcio de Datos LingÃ¼Ã­sticos. http://www.ldc.upenn.edu/. [19] E. Voorhees. Resumen de la pista de preguntas y respuestas TREC 2003. Actas de la DuodÃ©cima Conferencia de RecuperaciÃ³n de Texto (TREC 2003), 2003. [20] Y. Yang y B. Kisiel. RegresiÃ³n local basada en mÃ¡rgenes para filtrado adaptativo. Actas de la duodÃ©cima conferencia internacional sobre gestiÃ³n de la informaciÃ³n y el conocimiento, pÃ¡ginas 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang y B. Kisiel. Robustez de los MÃ©todos de Filtrado Adaptativo en una EvaluaciÃ³n de Referencia Cruzada. Actas de la 28Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 98-105, 2005. [22] C. Zhai, W. Cohen y J. Lafferty. MÃ¡s allÃ¡ de la relevancia independiente: MÃ©todos y mÃ©tricas de evaluaciÃ³n para la recuperaciÃ³n de subtemas. Actas de la 26Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 10-17, 2003. [23] J. Zhang y Y. Yang. Robustez de los MÃ©todos de ClasificaciÃ³n Lineal Regularizados en la CategorizaciÃ³n de Textos. Actas de la 26Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 190-197, 2003. [24] Y. Zhang. Utilizando priors bayesianos para combinar clasificadores para filtrado adaptativo. Actas de la 27Âª conferencia internacional anual sobre InvestigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 345-352, 2004. [25] Y. Zhang, J. Callan y T. Minka. DetecciÃ³n de novedad y redundancia en el filtrado adaptativo. Actas de la 25Âª Conferencia Internacional Anual de ACM SIGIR sobre InvestigaciÃ³n y Desarrollo en RecuperaciÃ³n de InformaciÃ³n, 2002. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "novelty detection": {
            "translated_key": "detecciÃ³n de novedades",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Utility-based Information Distillation Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu ABSTRACT This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework.",
                "It combines the strengths of and extends beyond conventional adaptive filtering, <br>novelty detection</br> and non-redundant passage ranking with respect to long-lasting information needs (tasks with multiple queries).",
                "Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings.",
                "For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task.",
                "Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty.",
                "Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the <br>novelty detection</br> components.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval.",
                "Adaptive filtering (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics.",
                "Based on the initial query and a few positive examples (if available), an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user.",
                "The incremental learning nature of AF systems makes them more powerful than standard search engines that support ad-hoc retrieval (e.g.",
                "Google and Yahoo) in terms of finding relevant information with respect to long-lasting topics of interest, and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs, without having to modify their queries manually.",
                "A variety of supervised learning algorithms (Rocchio-style classifiers, Exponential-Gaussian models, local regression and logistic regression approaches) have been studied for adaptive settings, examined with explicit and implicit relevance feedback, and evaluated with respect to utility optimization on large benchmark data collections in TREC (Text Retrieval Conferences) and TDT (Topic Detection and Tracking) forums [1, 4, 7, 15, 16, 20, 24, 23].",
                "Regularized logistic regression [21] has been found representative for the state-of-the-art approaches, and highly efficient for frequent model adaptations over large document collections such as the TREC-10 corpus (over 800,000 documents and 84 topics).",
                "Despite substantial achievements in recent adaptive filtering research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently.",
                "Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications: 1.",
                "User has a rather passive role in the conventional adaptive filtering setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision.",
                "A more active alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents (or passages) per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists.",
                "The latter form of user interaction has been highly effective in standard retrieval for ad-hoc queries.",
                "How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2.",
                "The unit for receiving a relevance judgment (yes or no) is restricted to the document level in conventional AF.",
                "However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant.",
                "Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system.",
                "For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3.",
                "System-selected documents are often highly redundant.",
                "A major news event, for example, would be reported by multiple sources repeatedly for a while, making most of the information content in those articles redundant with each other.",
                "A conventional AF system would select all these redundant news stories for user feedback, wasting the users time while offering little gain.",
                "Clearly, techniques for <br>novelty detection</br> can help in principle [25, 2, 22] for improving the utility of the AF systems.",
                "However, the effectiveness of such techniques at passage level to detect novelty with respect to users (fine-grained) feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user.",
                "To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and <br>novelty detection</br> for removal of redundancy from user interactions with the system.",
                "We call the new process utility-based information distillation.",
                "Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach.",
                "Therefore, we extended a benchmark corpus - the TDT4 collection of news stories and TV broadcasts - with task definitions, multiple queries per task, and answer keys per query.",
                "We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 .",
                "To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses.",
                "Moreover, we propose an extension of NDCG (Normalized Discounted Cumulated Gain) [9] for assessing the utility of ranked passages as a function of both relevance and novelty.",
                "The rest of this paper is organized as follows.",
                "Section 2 outlines the information distillation process with a concrete example.",
                "Section 3 describes the technical cores of our system called CAFÂ´E - CMU Adaptive Filtering Engine.",
                "Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme.",
                "Section 5 describes the extended TDT4 corpus.",
                "Section 6 presents our experiments and results.",
                "Section 7 concludes the study and gives future perspectives. 2.",
                "A SAMPLE TASK Consider a news event - the escape of seven convicts from a Texas prison in December 2000 and their capture a month later.",
                "Assuming a user were interested in this event since its early stage, the information need could be: Find information about the escape of convicts from Texas prison, and information related to their recapture.",
                "The associated lower-level questions could be: 1.",
                "How many prisoners escaped? 2.",
                "Where and when were they sighted? 3. Who are their known contacts inside and outside the prison? 4.",
                "How are they armed? 5.",
                "Do they have any vehicles? 6.",
                "What steps have been taken so far?",
                "We call such an information need a task, and the associated questions as the queries in this task.",
                "A distillation system is supposed to monitor the incoming documents, process them chunk by chunk in a temporal order, select potentially relevant and novel passages from each chunk with respect to each query, and present a ranked list of passages to the user.",
                "Passage ranking here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list.",
                "When presented with a list of passages, the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant.",
                "These spans of text are taken as positive examples in the adaptation of the query profile, and also added to the users history.",
                "Passages not marked by the user are taken as negative examples.",
                "As soon as the query profile is updated, the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low, based on user preference.",
                "For example, if the user highlights ...officials have posted a $100,000 reward for their capture... as relevant answer to the query What steps have been taken so far?, then the highlighted piece is used as an additional positive training example in the adaptation of the query profile.",
                "This piece of feedback is also added to the user history as a seen example, so that in future, the system will not place another passage mentioning $100,000 reward at the top of the ranked list.",
                "However, an article mentioning ...officials have doubled the reward money to $200,000... might be ranked high since it is both relevant to the (updated) query profile and novel with respect to the (updated) user history.",
                "The user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly.",
                "Clearly, <br>novelty detection</br> is very important for the utility of such a system because of the iterative search.",
                "Without <br>novelty detection</br>, the old relevant passages would be shown to the user repeatedly in each ranked list.",
                "Through the above example, we can see the main properties of our new framework for utility-based information distillation over temporally ordered documents.",
                "Our framework combines and extends the power of adaptive filtering (AF), ad-hoc retrieval (IR) and <br>novelty detection</br> (ND).",
                "Compared to standard IR, our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task.",
                "Compared to conventional AF, it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system.",
                "Compared to past work, this is the first evaluation of <br>novelty detection</br> integrated with adaptive filtering for sequenced queries that allows flexible user feedback over ranked passages.",
                "The combination of AF, IR and ND with the new extensions raises an important research question regarding evaluation methodology: how can we measure the utility of such an information distillation system?",
                "Existing metrics in standard IR, AF and ND are insufficient, and new solutions must be explored, as we will discuss in Section 4, after describing the technical cores of our system in the next section. 3.",
                "TECHNICAL CORES The core components of CAFÂ´E are - 1) AF for incremental learning of query profiles, 2) IR for estimating relevance of passages with respect to query profiles, 3) ND for assessing novelty of passages with respect to users history, and 4) anti-redundancy component to remove redundancy from ranked lists. 3.1 Adaptive Filtering Component We use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for AF [21].",
                "Logistic regression (LR) is a supervised learning algorithm for statistical classification.",
                "Based on a training set of labeled instances, it learns a class model which can then by used to predict the labels of unseen instances.",
                "Its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required, as is the case in adaptive filtering, where the system must learn from each new feedback provided by the user. (See [21] and [23] for computational complexity and implementation issues).",
                "In adaptive filtering, each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query.",
                "For training the model, we use the query itself as the initial positive training example of the class, and the user-highlighted pieces of text (marked as Relevant or Not-relevant) during feedback as additional training examples.",
                "To address the cold start issue in the early stage before any user feedback is obtained, the system uses a small sample from a retrospective corpus as the initial negative examples in the training set.",
                "The details of using logistic regression for adaptive filtering (assigning different weights to positive and negative training instances, and regularizing the objective function to prevent over-fitting on training data) are presented in [21].",
                "The class model wâ learned by Logistic Regression, or the query profile, is a vector whose dimensions are individual terms and whose elements are the regression coefficients, indicating how influential each term is in the query profile.",
                "The query profile is updated whenever a new piece of user feedback is received.",
                "A temporally decaying weight can be applied to each training example, as an option, to emphasize the most recent user feedback. 3.2 Passage Retrieval Component We use standard IR techniques in this part of our system.",
                "Incoming documents are processed in chunks, where each chunk can be defined as a fixed span of time or as a fixed number of documents, as preferred by the user.",
                "For each incoming document, corpus statistics like the IDF (Inverted Document Frequency) of each term are updated.",
                "We use a state-of-the-art named entity identifier and tracker [8, 12] to identify person and location names, and merge them with co-referent named entities seen in the past.",
                "Then the documents are segmented into passages, which can be a whole document, a paragraph, a sentence, or any other continuous span of text, as preferred.",
                "Each passage is represented using a vector of TF-IDF (Term FrequencyInverse Document Frequency) weights, where term can be a word or a named entity.",
                "Given a query profile, i.e. the logistic regression solution wâ as described in Section 3.1, the system computes the posterior probability of relevance for each passage x as fRL(x) â¡ P(y = 1|x, wâ ) = 1 (1 + eâwâÂ·x) (1) Passages are ordered by their relevance scores, and the ones with scores above a threshold (tuned on a training set) comprise the relevance list that is passed on to the <br>novelty detection</br> step. 3.3 <br>novelty detection</br> Component CAFÂ´E maintains a user history H(t), which contains all the spans of text hi that the user highlighted (as feedback) during his or her past interactions with the system, up to the current time t. Denoting the history as H(t) = n h1, h2, ..., ht o , (2) the novelty score of a new candidate passage x is computed as: fND(x) = 1 â max iâ1..t {cos(x, hi)} (3) where both candidate passage x and highlighted spans of text hi are represented as TF-IDF vectors.",
                "The novelty score of each passage is compared to a prespecified threshold (also tuned on a training set), and any passage with a score below this threshold is removed from the relevance list. 3.4 Anti-redundant Ranking Component Although the <br>novelty detection</br> component ensures that only novel (previously unseen) information remains in the relevance list, this list might still contain the same novel information at multiple positions in the ranked list.",
                "Suppose, for example, that the user has already read about a $100,000 reward for information about the escaped convicts.",
                "A new piece of news that the award has been increased to $200,000 is novel since the user hasnt read about it yet.",
                "However, multiple news sources would report this news and we might end up showing (redundant) articles from all these sources in a ranked list.",
                "Hence, a ranked list should also be made non-redundant with respect to its own contents.",
                "We use a simplified version of the Maximal Marginal Relevance method [5], originally developed for combining relevance and novelty in text retrieval and summarization.",
                "Our procedure starts with the current list of passages sorted by relevance (section 3.2), filtered by <br>novelty detection</br> component (section 3.3), and generates a new non-redundant list as follows: 1.",
                "Take the top passage in the current list as the top one in the new list. 2.",
                "Add the next passage x in the current list to the new list only if fAR(x) > t where fAR(x) = 1 â max piâLnew {cos(x, pi)} and Lnew is the set of passages already selected in the new list. 3.",
                "Repeat step 2 until all the passages in the current list have been examined.",
                "After applying the above-mentioned algorithm, each passage in the new list is sufficiently dissimilar to others, thus favoring diversity rather than redundancy in the new ranked list.",
                "The anti-redundancy threshold t is tuned on a training set. 4.",
                "EVALUATION METHODOLOGY The approach we proposed above for information distillation raises important issues regarding evaluation methodology.",
                "Firstly, since our framework allows the output to be passages at different levels of granularity (e.g. k-sentence windows where k may vary) instead of a fixed length, it is not possible to have pre-annotated relevance judgments at all such granularity levels.",
                "Secondly, since we wish to measure the utility of the system output as a combination of both relevance and novelty, traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time.",
                "Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists.",
                "None of the existing measures in ad-hoc retrieval, adaptive filtering, <br>novelty detection</br> or other related areas (text summarization and question answering) have desirable properties in all the three aspects.",
                "Therefore, we must develop a new evaluation methodology. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers.",
                "Answer keys define what should be present in a system response to receive credit, and are comprised of a collection of information nuggets, i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them.",
                "Defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping [19], since system-returned passages can contain the same information expressed in many different ways.",
                "Hence, QA evaluations have relied on human assessors for the mapping between various expressions, making the process costly, time consuming, and not scalable to large query and document collections, and extensive system evaluations with various parameter settings. 4.1.1 Automating Evaluation based on Answer Keys Automatic evaluation methods would allow for faster system building and tuning, as well as provide an objective and affordable way of comparing various systems.",
                "Recently, such methods have been proposed, more or less, based on the idea of n-gram co-occurrences.",
                "Pourpre [10] assigns a fractional recall score to a system response based on its unigram overlap with a given nuggets description.",
                "For example, a system response A B C has recall 3/4 with respect to a nugget with description A B C D. However, such an approach is unfair to systems that present the same information but using words other than A, B, C, and D. Another open issue is how to weight individual words in measuring the closeness of a match.",
                "For example, consider the question How many prisoners escaped?.",
                "In the nugget Seven prisoners escaped from a Texas prison, there is no indication that seven is the keyword, and that it must be matched to get any relevance credit.",
                "Using IDF values does not help, since seven will generally not have a higher IDF than words like texas and prison.",
                "Also, redefining the nugget as just seven does not solve the problem since now it might spuriously match any mention of seven out of context.",
                "Nuggeteer [13] works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold.",
                "However, it is also plagued by spurious relevance since not all words contained in the nugget description (or known correct responses) are central to the nugget. 4.1.2 Nugget-Matching Rules We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on nugget-matching rules, which are generated using a semiautomatic procedure explained below.",
                "These rules are essentially Boolean queries that will only match against snippets that contain the nugget.",
                "For instance, a candidate rule for matching answers to How many prisoners escaped? is (Texas AND seven AND escape AND (convicts OR prisoners)), possibly with other synonyms and variants in the rule.",
                "For a corpus of news articles, which usually follow a typical formal prose, it is fairly easy to write such simple rules to match expected answers using a bootstrap approach, as described below.",
                "We propose a two-stage approach, inspired by Autoslog [14], that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents.",
                "In the first stage, human subjects annotated (using a highlighting tool) portions of ontopic documents that contained answers to each nugget 2 .",
                "In the second stage, subjects used our rule generation tool to create rules that would match the annotations for each nugget.",
                "The tool allows users to enter a Boolean rule as a disjunction of conjunctions (e.g. ((a AND b) OR (a AND c AND d) OR (e))).",
                "Given a candidate rule, our tool uses it as a Boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match.",
                "Hence, the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision.",
                "We observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions (adding more ANDs), and improving the recall by adding more conjunctions to the disjunction (adding more ORs).",
                "As an example, lets try to create a rule for the nugget which says that seven prisoners escaped from the Texas prison.",
                "We start with a simple rule - (seven).",
                "When we input this into the rule generation tool, we realize that this rule matches many spurious occurrences of seven (e.g. ...seven states...) and thus gets a low precision score.",
                "We can further qualify our rule - Texas AND seven AND convicts.",
                "Next, by looking at the missed annotations, we realize that some news articles mentioned ...seven prisoners escaped.... We then replace convicts with the disjunction (convicts OR prisoners).",
                "We continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the (small number of) misses and false alarms can be safely ignored.",
                "Thus we can create nugget-matching rules that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses.",
                "Human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably. 4.2 Evaluating the Utility of a Sequence of Ranked Lists The utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information, and how much the user lost in terms of time and energy.",
                "We calculate this utility from the utilities of individual passages as follows.",
                "After reading each passage returned by the system, the user derives some gain depending on the presence of relevant and novel information, and incurs a loss in terms of the time and energy spent in going through the passage.",
                "However, the likelihood that the user would actually read a passage depends on its position in the ranked list.",
                "Hence, for a query q, the expected utility 2 LDC [18] already provides relevance judgments for 100 topics on the TDT4 corpus.",
                "We further ensured that these judgments are exhaustive on the entire corpus using pooling. of a passage pi at rank i can be defined as U(pi, q) = P(i) â (Gain(pi, q) â Loss(pi, q)) (4) where P(i) is the probability that the user would go through a passage at rank i.",
                "The expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage: U(q) = nX i=1 P(i) â (Gain(pi, q) â Loss(pi, q)) (5) Note that if we ignore the loss term and define P(i) as P(i) â 1/ logb(b + i â 1) (6) then we get the recently popularized metric called Discounted Cumulated Gain (DCG) [9], where Gain(pi, q) is defined as the graded relevance of passage pi.",
                "However, without the loss term, DCG is a purely recall-oriented metric and not suitable for an adaptive filtering setting, where the systems utility depends in part on its ability to limit the number of items shown to the user.",
                "Although P(i) could be defined based on empirical studies of user behavior, for simplicity, we use P(i) exactly as defined in equation 6.",
                "The gain G(pi, q) of passage pi with respect to the query q is a function of - 1) the number of relevant nuggets present in pi, and 2) the novelty of each of these nuggets.",
                "We combine these two factors as follows.",
                "For each nugget Nj, we assign an initial weight wj, and also keep a count nj of the number of times this nugget has been seen by the user in the past.",
                "The gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor Î³.",
                "Thus, G(pi, q) is defined as G(pi, q) = X Nj âC(pi,q) wj â Î³nj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the answer key of query q.",
                "The initial weights wj are all set of be 1.0 in our experiments, but can also be set based on a pyramid approach [11].",
                "The choice of dampening factor Î³ determines the users tolerance for redundancy.",
                "When Î³ = 0, a nugget will only receive credit for its first occurrence i.e. when nj is zero3 .",
                "For 0 < Î³ < 1, a nugget receives smaller credit for each successive occurrence.",
                "When Î³ = 1, no dampening occurs and repeated occurrences of a nugget receive the same credit.",
                "Note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system, since the users are expected to remember what the system showed them in the past.",
                "We define the loss L(pi, q) as a constant cost c (we use 0.1) incurred when reading a system-returned passage.",
                "Thus, our metric can be re-written as U(q) = nX i=1 Gain(pi, q) logb(b + i â 1) â L(n) (8) where L(n) is the loss associated with a ranked list of length n: L(n) = c Â· nX i=1 1 logb(b + i â 1) (9) 3 Note that 00 = 1 Due to the similarity with Discounted Cumulated Gain (DCG), we call our metric Discounted Cumulated Utility (DCU).",
                "The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) â¤ 0 i.e. when the gain is less than or equal to the cost of reading the passage. 5.",
                "DATA TDT4 was the benchmark corpus used in TDT2002 and TDT2003 evaluations.",
                "The corpus consists of over 90, 000 news articles from multiple sources (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) published between October 2000 and January 2001, in the languages of Arabic, English, and Mandarin.",
                "Speech-recognized and machine-translated versions of the non-English articles were provided as well.",
                "LDC [18] has annotated the corpus with 100 topics, that correspond to various news events in this time period.",
                "Out of these, we selected a subset of 12 actionable events, and defined corresponding tasks for them4 .",
                "For each task, we manually defined a profile consisting of an initial set of (5 to 10) queries, a free-text description of the user history, i.e., what the user already knows about the event, and a list of known on-topic and off-topic documents (if available) as training examples.",
                "For each query, we generated answer keys and corresponding nugget matching rules using the procedure described in section 4.1.2, and produced a total of 120 queries, with an average of 7 nuggets per query. 6.",
                "EXPERIMENTS AND RESULTS 6.1 Baselines We used Indri [17], a popular language-model based retrieval engine, as a baseline for comparison with CAFÂ´E.",
                "Indri supports standard search engine functionality, including pseudo-relevance feedback (PRF) [3, 6], and is representative of a typical query-based retrieval system.",
                "Indri does not support any kind of <br>novelty detection</br>.",
                "We compare Indri with PRF turned on and off, against CAFÂ´E with user feedback, <br>novelty detection</br> and antiredundant ranking turned on and off. 6.2 Experimental Setup We divided the TDT4 corpus spanning 4 months into 10 chunks, each defined as a period of 12 consecutive days.",
                "At any given point of time in the distillation process, each system accessed the past data up to the current point, and returned a ranked list of up 50 passages per query.",
                "The 12 tasks defined on the corpus were divided into a training and test set with 6 tasks each.",
                "Each system was allowed to use the training set to tune its parameters for optimizing NDCU (equation 8), including the relevance threshold for both Indri and CAFÂ´E, and the novelty and antiredundancy thresholds for CAFÂ´E.",
                "The NDCU for each system run is calculated automatically.",
                "User feedback was also simulated - relevance judgments for each system-returned passage (as determined by the nugget matching rules described in section 4.1.2) were 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figure 1: Performance of Indri across chunks Figure 2: Performance of CAFÂ´E across chunks used as user feedback in the adaptation of query profiles and user histories. 6.3 Results In Table 1, we show the NDCU scores of the two systems under various settings.",
                "These scores are averaged over the six tasks in the test set, and are calculated with two dampening factors (see section 4.2): Î³ = 0 and 0.1, to simulate no tolerance and small tolerance for redundancy, respectively.",
                "Using Î³ = 0 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information.",
                "Hence, the improvement obtained from enabling user feedback is smaller with Î³ = 0 than the improvement obtained from feedback with Î³ = 0.1.",
                "This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information.",
                "However, the user does not benefit from seeing such redundant passages, and is usually interested in other passages containing related information.",
                "It is informative to evaluate retrieval systems using our utility measure (with Î³ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.",
                "Sometimes, however, users might indeed be interested in seeing the same information from multiple sources, as an Table 1: NDCU Scores of Indri and CAFÂ´E for two dampening factors (Î³), and various settings (F: Feedback, N: <br>novelty detection</br>, A: Anti-Redundant Ranking) Indri CAFÂ´E Î³ Base +PRF Base +F +F+N +F+A +F+N+A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.35 0.36 0.36 indicator of its importance or reliability.",
                "In such a case, we can simply choose a higher value for Î³ which corresponds to a higher tolerance for redundancy, and hence let the system tune its parameters accordingly.",
                "Since documents were processed chunk by chunk, it would be interesting to see how the performance of systems improves over time.",
                "Figures 1 and 2 show the performance trends for both the systems across chunks.",
                "While the performance with and without feedback on the first few chunks is expected to be close, for subsequent chunks, the performance curve with feedback enabled rises above the one with the no-feedback setting.",
                "The performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks, making some queries easier than others in certain chunks.",
                "Moreover, since Indri uses pseudo-relevance feedback while CAFÂ´E uses feedback based on actual relevance judgments, the improvement in case of Indri is less dramatic than that of CAFÂ´E. 7.",
                "CONCLUDING REMARKS This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages.",
                "Our system, called CAFÂ´E, combines adaptive filtering, <br>novelty detection</br> and antiredundant passage ranking in a unified framework for utility optimization.",
                "We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty.",
                "Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and <br>novelty detection</br> turned off. 8.",
                "ACKNOWLEDGMENTS We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments.",
                "This work is supported in parts by the National Science Foundation (NSF) under grant IIS0434035, and the Defense Advanced Research Project Agency (DARPA) under contracts NBCHD030010 and W0550432.",
                "Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. 9.",
                "ADDITIONAL AUTHORS Jian Zhang (jianzhan@stat.purdue.edu)â , Jaime Carbonell (jgc@cs.cmu.edu)â  , Peter Brusilovsky (peterb+@pitt.edu)â¡ , Daqing He(dah44@pitt.edu)â¡ 10.",
                "REFERENCES [1] J. Allan.",
                "Incremental Relevance Feedback for Information Filtering.",
                "Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 270-278, 1996. [2] J. Allan, C. Wade, and A. Bolivar.",
                "Retrieval and <br>novelty detection</br> at the Sentence Level.",
                "Proceedings of the ACM SIGIR conference on research and development in information retrieval, 2003. [3] C. Buckley, G. Salton, and J. Allan.",
                "Automatic Retrieval with Locality Information using SMART.",
                "NIST special publication, (500207):59-72, 1993. [4] J. Callan.",
                "Learning While Filtering Documents.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 224-231, 1998. [5] J. Carbonell and J. Goldstein.",
                "The use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335-336, 1998. [6] E. Efthimiadis.",
                "Query Expansion.",
                "Annual Review of Information Science and Technology (ARIST), 31:p121-87, 1996. [7] J. Fiscus and G. Duddington.",
                "Topic Detection and Tracking Overview.",
                "Topic Detection and Tracking: Event-based Information Organization, pages 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.",
                "A Statistical Model for Multilingual Entity Detection and Tracking.",
                "NAACL/HLT, 2004. [9] K. JÂ¨arvelin and J. KekÂ¨alÂ¨ainen.",
                "Cumulated Gain-based Evaluation of IR Techniques.",
                "ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin and D. Demner-Fushman.",
                "Automatically Evaluating Answers to Definition Questions.",
                "Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), 2005. â Statistics Dept., Purdue University, West Lafayette, USA â  Language Technologies Inst., Carnegie Mellon University, Pittsburgh, USA â¡ School of Information Sciences, Univ. of Pittsburgh, Pittsburgh, USA [11] J. Lin and D. Demner-Fushman.",
                "Will Pyramids Built of nUggets Topple Over.",
                "Proceedings of HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.",
                "A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree.",
                "Proc. of ACL, 4:136-143, 2004. [13] G. Marton.",
                "Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments.",
                "HLT/NAACL, 2006. [14] E. Riloff.",
                "Automatically Constructing a Dictionary for Information Extraction Tasks.",
                "Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, 1993. [15] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9: Filtering track.",
                "The Ninth Text REtrieval Conference (TREC-9), pages 361-368. [16] R. Schapire, Y.",
                "Singer, and A. Singhal.",
                "Boosting and Rocchio Applied to Text Filtering.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle, and W. Croft.",
                "Indri: A Language Model-based Search Engine for Complex Queries.",
                "Proceedings of the International Conference on Intelligence Analysis, 2004. [18] The Linguistic Data Consortium. http://www.ldc.upenn.edu/. [19] E. Voorhees.",
                "Overview of the TREC 2003 Question Answering Track.",
                "Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), 2003. [20] Y. Yang and B. Kisiel.",
                "Margin-based Local Regression for Adaptive Filtering.",
                "Proceedings of the twelfth international conference on Information and knowledge management, pages 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of Adaptive Filtering Methods in a Cross-benchmark Evaluation.",
                "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 98-105, 2005. [22] C. Zhai, W. Cohen, and J. Lafferty.",
                "Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-17, 2003. [23] J. Zhang and Y. Yang.",
                "Robustness of Regularized Linear Classification Methods in Text Categorization.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 190-197, 2003. [24] Y. Zhang.",
                "Using Bayesian Priors to Combine Classifiers for Adaptive Filtering.",
                "Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 345-352, 2004. [25] Y. Zhang, J. Callan, and T. Minka.",
                "Novelty and Redundancy Detection in Adaptive Filtering.",
                "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002."
            ],
            "original_annotated_samples": [
                "It combines the strengths of and extends beyond conventional adaptive filtering, <br>novelty detection</br> and non-redundant passage ranking with respect to long-lasting information needs (tasks with multiple queries).",
                "Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the <br>novelty detection</br> components.",
                "Clearly, techniques for <br>novelty detection</br> can help in principle [25, 2, 22] for improving the utility of the AF systems.",
                "To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and <br>novelty detection</br> for removal of redundancy from user interactions with the system.",
                "Clearly, <br>novelty detection</br> is very important for the utility of such a system because of the iterative search."
            ],
            "translated_annotated_samples": [
                "Combina las fortalezas y se extiende mÃ¡s allÃ¡ del filtrado adaptativo convencional, la <br>detecciÃ³n de novedades</br> y la clasificaciÃ³n de pasajes no redundantes con respecto a las necesidades de informaciÃ³n duraderas (tareas con mÃºltiples consultas).",
                "Nuestros resultados muestran mejoras alentadoras en la utilidad utilizando el nuevo enfoque, en comparaciÃ³n con los sistemas base sin aprendizaje incremental o componentes de <br>detecciÃ³n de novedades</br>.",
                "Claramente, las tÃ©cnicas de <br>detecciÃ³n de novedades</br> pueden ayudar en principio [25, 2, 22] a mejorar la utilidad de los sistemas AF.",
                "Para abordar las limitaciones actuales de los sistemas de AF mencionadas anteriormente, proponemos y examinamos un nuevo enfoque en este artÃ­culo, que combina las fortalezas del AF convencional (aprendizaje incremental de modelos de temas), recuperaciÃ³n de pasajes multipase para consultas de larga duraciÃ³n condicionadas al tema, y <br>detecciÃ³n de novedades</br> para eliminar la redundancia de las interacciones del usuario con el sistema.",
                "Claramente, la <br>detecciÃ³n de novedades</br> es muy importante para la utilidad de dicho sistema debido a la bÃºsqueda iterativa."
            ],
            "translated_text": "DestilaciÃ³n de informaciÃ³n basada en utilidad sobre documentos secuenciados temporalmente en el Instituto de TecnologÃ­as del Lenguaje de Yiming Yang. Universidad Carnegie Mellon, Pittsburgh, EE. UU. yiming@cs.cmu.edu Abhimanyu Lad Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. alad@cs.cmu.edu Instituto de TecnologÃ­as del Lenguaje Ni Lao. Universidad Carnegie Mellon, Pittsburgh, EE. UU. nlao@cs.cmu.edu Abhay Harpale Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. aharpale@cs.cmu.edu Bryan Kisiel Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. bkisiel@cs.cmu.edu MÃ³nica Rogati Instituto de TecnologÃ­as del Lenguaje. La Universidad Carnegie Mellon en Pittsburgh, EE. UU. mrogati@cs.cmu.edu RESUMEN Este artÃ­culo examina un nuevo enfoque para la destilaciÃ³n de informaciÃ³n en documentos ordenados temporalmente, y propone un esquema de evaluaciÃ³n novedoso para dicho marco. Combina las fortalezas y se extiende mÃ¡s allÃ¡ del filtrado adaptativo convencional, la <br>detecciÃ³n de novedades</br> y la clasificaciÃ³n de pasajes no redundantes con respecto a las necesidades de informaciÃ³n duraderas (tareas con mÃºltiples consultas). Nuestro enfoque respalda la retroalimentaciÃ³n detallada del usuario a travÃ©s del resaltado de fragmentos arbitrarios de texto, y aprovecha dicha informaciÃ³n para la optimizaciÃ³n de la utilidad en entornos adaptativos. Para nuestros experimentos, definimos tareas hipotÃ©ticas basadas en eventos de noticias en el corpus TDT4, con mÃºltiples consultas por tarea. Se generaron claves de respuestas (nuggets) para cada consulta y se utilizÃ³ un procedimiento semiautomÃ¡tico para adquirir reglas que permiten emparejar automÃ¡ticamente los nuggets con las respuestas del sistema. TambiÃ©n proponemos una extensiÃ³n de la mÃ©trica NDCG para evaluar la utilidad de los pasajes clasificados como una combinaciÃ³n de relevancia y novedad. Nuestros resultados muestran mejoras alentadoras en la utilidad utilizando el nuevo enfoque, en comparaciÃ³n con los sistemas base sin aprendizaje incremental o componentes de <br>detecciÃ³n de novedades</br>. CategorÃ­as y Descriptores de Asignaturas H.3.3 [BÃºsqueda y RecuperaciÃ³n de InformaciÃ³n]: Filtrado de informaciÃ³n, RetroalimentaciÃ³n de relevancia, Modelos de recuperaciÃ³n, Proceso de selecciÃ³n; I.5.2 TÃ©rminos Generales DiseÃ±o, MediciÃ³n, Rendimiento, ExperimentaciÃ³n. 1. INTRODUCCIÃN Seguir informaciÃ³n nueva y relevante de flujos de datos temporales para usuarios con necesidades duraderas ha sido un tema de investigaciÃ³n desafiante en la recuperaciÃ³n de informaciÃ³n. El filtrado adaptativo (AF) es una tarea de predicciÃ³n en lÃ­nea de la relevancia de cada nuevo documento con respecto a temas predefinidos. BasÃ¡ndose en la consulta inicial y algunos ejemplos positivos (si estÃ¡n disponibles), un sistema de AF mantiene un perfil para cada tema de interÃ©s, y lo actualiza constantemente segÃºn la retroalimentaciÃ³n del usuario. La naturaleza de aprendizaje incremental de los sistemas de AF los hace mÃ¡s poderosos que los motores de bÃºsqueda estÃ¡ndar que admiten recuperaciÃ³n ad-hoc (por ejemplo. Google y Yahoo) en tÃ©rminos de encontrar informaciÃ³n relevante con respecto a temas de interÃ©s duraderos, y mÃ¡s atractivo para los usuarios que estÃ¡n dispuestos a proporcionar retroalimentaciÃ³n para adaptar el sistema a sus necesidades de informaciÃ³n especÃ­ficas, sin tener que modificar sus consultas manualmente. Se han estudiado una variedad de algoritmos de aprendizaje supervisado (clasificadores de estilo Rocchio, modelos exponenciales-gaussianos, regresiÃ³n local y enfoques de regresiÃ³n logÃ­stica) para entornos adaptativos, examinados con retroalimentaciÃ³n de relevancia explÃ­cita e implÃ­cita, y evaluados con respecto a la optimizaciÃ³n de la utilidad en grandes colecciones de datos de referencia en los foros de TREC (Conferencias de RecuperaciÃ³n de Texto) y TDT (DetecciÃ³n y Seguimiento de Temas) [1, 4, 7, 15, 16, 20, 24, 23]. La regresiÃ³n logÃ­stica regularizada [21] se ha encontrado representativa para los enfoques de vanguardia, y altamente eficiente para adaptaciones frecuentes del modelo en grandes colecciones de documentos como el corpus TREC-10 (mÃ¡s de 800,000 documentos y 84 temas). A pesar de los logros sustanciales en la investigaciÃ³n reciente de filtrado adaptativo, aÃºn quedan problemas significativos sin resolver sobre cÃ³mo aprovechar de manera efectiva y eficiente la retroalimentaciÃ³n del usuario. EspecÃ­ficamente, los siguientes problemas pueden limitar seriamente la verdadera utilidad de los sistemas de AF en aplicaciones del mundo real: 1. El usuario tiene un papel bastante pasivo en la configuraciÃ³n convencional de filtrado adaptativo: solo reacciona ante el sistema cuando este toma una decisiÃ³n afirmativa sobre un documento, confirmando o rechazando esa decisiÃ³n. Una alternativa mÃ¡s activa serÃ­a permitir al usuario emitir mÃºltiples consultas sobre un tema, revisar una lista clasificada de documentos candidatos (o pasajes) por consulta, y proporcionar retroalimentaciÃ³n sobre la lista clasificada, refinando asÃ­ su necesidad de informaciÃ³n y solicitando listas clasificadas actualizadas. La Ãºltima forma de interacciÃ³n del usuario ha sido altamente efectiva en la recuperaciÃ³n estÃ¡ndar para consultas ad-hoc. CÃ³mo implementar una estrategia para necesidades de informaciÃ³n duraderas en entornos de FA es una pregunta abierta para la investigaciÃ³n. 2. La unidad para recibir un juicio de relevancia (sÃ­ o no) estÃ¡ restringida al nivel del documento en AF convencional. Sin embargo, un usuario real puede estar dispuesto a proporcionar comentarios mÃ¡s informativos y detallados resaltando algunas partes de texto en un documento recuperado como relevantes, en lugar de etiquetar todo el documento como relevante. Aprovechar de manera efectiva este tipo de retroalimentaciÃ³n detallada podrÃ­a mejorar sustancialmente la calidad de un sistema de AF. Para esto, necesitamos habilitar el aprendizaje supervisado a partir de fragmentos de texto etiquetados de longitud arbitraria en lugar de solo permitir documentos etiquetados. 3. Los documentos seleccionados por el sistema suelen ser altamente redundantes. Un evento importante de noticias, por ejemplo, serÃ­a reportado por mÃºltiples fuentes repetidamente durante un tiempo, haciendo que la mayor parte del contenido informativo en esos artÃ­culos sea redundante entre sÃ­. Un sistema de AF convencional seleccionarÃ­a todas estas historias de noticias redundantes para la retroalimentaciÃ³n del usuario, desperdiciando el tiempo de los usuarios mientras ofrece poco beneficio. Claramente, las tÃ©cnicas de <br>detecciÃ³n de novedades</br> pueden ayudar en principio [25, 2, 22] a mejorar la utilidad de los sistemas AF. Sin embargo, la efectividad de tales tÃ©cnicas a nivel de pasaje para detectar novedad con respecto a la retroalimentaciÃ³n de los usuarios a nivel detallado y para detectar redundancia en listas clasificadas aÃºn debe ser evaluada utilizando una medida de utilidad que imite las necesidades de un usuario real. Para abordar las limitaciones actuales de los sistemas de AF mencionadas anteriormente, proponemos y examinamos un nuevo enfoque en este artÃ­culo, que combina las fortalezas del AF convencional (aprendizaje incremental de modelos de temas), recuperaciÃ³n de pasajes multipase para consultas de larga duraciÃ³n condicionadas al tema, y <br>detecciÃ³n de novedades</br> para eliminar la redundancia de las interacciones del usuario con el sistema. Llamamos al nuevo proceso destilaciÃ³n de informaciÃ³n basada en utilidad. Ten en cuenta que los corpus de referencia convencionales para evaluaciones de AF, que tienen juicios de relevancia a nivel de documento y no definen tareas con mÃºltiples consultas, son insuficientes para evaluar el nuevo enfoque. Por lo tanto, ampliamos un corpus de referencia, la colecciÃ³n TDT4 de noticias y transmisiones de televisiÃ³n, con definiciones de tareas, mÃºltiples consultas por tarea y claves de respuestas por consulta. Hemos realizado nuestros experimentos en este corpus extendido TDT4 y hemos puesto a disposiciÃ³n del pÃºblico los datos generados adicionalmente para futuras evaluaciones comparativas. Para evaluar automÃ¡ticamente los fragmentos de texto arbitrarios devueltos por el sistema utilizando nuestras claves de respuestas, desarrollamos un esquema de evaluaciÃ³n con un procedimiento semiautomÃ¡tico para 1 URL: http://nyc.lti.cs.cmu.edu/downloads adquiriendo reglas que pueden comparar fragmentos con las respuestas del sistema. AdemÃ¡s, proponemos una extensiÃ³n de NDCG (Normalized Discounted Cumulated Gain) [9] para evaluar la utilidad de los pasajes clasificados como una funciÃ³n tanto de la relevancia como de la novedad. El resto de este documento estÃ¡ organizado de la siguiente manera. La secciÃ³n 2 describe el proceso de destilaciÃ³n de informaciÃ³n con un ejemplo concreto. La secciÃ³n 3 describe los nÃºcleos tÃ©cnicos de nuestro sistema llamado CAFÂ´E - CMU Adaptive Filtering Engine. La secciÃ³n 4 discute problemas con respecto a la metodologÃ­a de evaluaciÃ³n y propone un nuevo esquema. La secciÃ³n 5 describe el corpus extendido TDT4. La secciÃ³n 6 presenta nuestros experimentos y resultados. La secciÃ³n 7 concluye el estudio y ofrece perspectivas futuras. 2. Una TAREA DE EJEMPLO Considera un evento noticioso: la fuga de siete convictos de una prisiÃ³n de Texas en diciembre de 2000 y su captura un mes despuÃ©s. Suponiendo que un usuario estuviera interesado en este evento desde sus primeras etapas, la necesidad de informaciÃ³n podrÃ­a ser: Encontrar informaciÃ³n sobre la fuga de convictos de una prisiÃ³n de Texas, y la informaciÃ³n relacionada con su recaptura. Las preguntas de nivel inferior asociadas podrÃ­an ser: 1. Â¿CuÃ¡ntos prisioneros escaparon? 2. Â¿DÃ³nde y cuÃ¡ndo fueron avistados? 3. Â¿QuiÃ©nes son sus contactos conocidos dentro y fuera de la prisiÃ³n? 4. Â¿CÃ³mo estÃ¡n armados? 5. Â¿Tienen algÃºn vehÃ­culo? 6. Â¿QuÃ© pasos se han tomado hasta ahora? Llamamos a esta necesidad de informaciÃ³n una tarea, y a las preguntas asociadas como las consultas en esta tarea. Un sistema de destilaciÃ³n debe monitorear los documentos entrantes, procesarlos por fragmentos en un orden temporal, seleccionar pasajes potencialmente relevantes y novedosos de cada fragmento con respecto a cada consulta, y presentar una lista clasificada de pasajes al usuario. La clasificaciÃ³n de pasajes aquÃ­ se basa en cuÃ¡n relevante es un pasaje con respecto a la consulta actual, cuÃ¡n novedoso es con respecto al historial actual del usuario (de sus interacciones con el sistema) y cuÃ¡n redundante es en comparaciÃ³n con otros pasajes con un rango mÃ¡s alto en la lista. Cuando se le presenta una lista de pasajes, el usuario puede proporcionar retroalimentaciÃ³n resaltando tramos de texto arbitrarios que considere relevantes. Estos fragmentos de texto se toman como ejemplos positivos en la adaptaciÃ³n del perfil de consulta, y tambiÃ©n se aÃ±aden al historial de los usuarios. Los pasajes no marcados por el usuario se consideran ejemplos negativos. Tan pronto como se actualiza el perfil de consulta, el sistema vuelve a realizar una bÃºsqueda y devuelve otra lista clasificada de pasajes donde los pasajes previamente vistos son eliminados o clasificados como bajos, segÃºn la preferencia del usuario. Por ejemplo, si el usuario resalta ...los funcionarios han ofrecido una recompensa de $100,000 por su captura... como respuesta relevante a la consulta Â¿QuÃ© pasos se han tomado hasta ahora?, entonces la pieza resaltada se utiliza como un ejemplo de entrenamiento positivo adicional en la adaptaciÃ³n del perfil de la consulta. Este comentario tambiÃ©n se aÃ±ade al historial del usuario como un ejemplo visto, para que en el futuro, el sistema no coloque otro pasaje mencionando una recompensa de $100,000 en la parte superior de la lista clasificada. Sin embargo, un artÃ­culo que mencione que los funcionarios han duplicado el dinero de recompensa a $200,000 podrÃ­a clasificarse alto ya que es relevante tanto para el perfil de consulta (actualizado) como novedoso con respecto a la historia del usuario (actualizada). El usuario puede modificar las consultas originales o agregar una nueva consulta durante el proceso; los perfiles de consulta se cambiarÃ¡n en consecuencia. Claramente, la <br>detecciÃ³n de novedades</br> es muy importante para la utilidad de dicho sistema debido a la bÃºsqueda iterativa. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "new evaluation methodology": {
            "translated_key": "nueva metodologÃ­a de evaluaciÃ³n",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Utility-based Information Distillation Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu ABSTRACT This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework.",
                "It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant passage ranking with respect to long-lasting information needs (tasks with multiple queries).",
                "Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings.",
                "For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task.",
                "Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty.",
                "Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval.",
                "Adaptive filtering (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics.",
                "Based on the initial query and a few positive examples (if available), an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user.",
                "The incremental learning nature of AF systems makes them more powerful than standard search engines that support ad-hoc retrieval (e.g.",
                "Google and Yahoo) in terms of finding relevant information with respect to long-lasting topics of interest, and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs, without having to modify their queries manually.",
                "A variety of supervised learning algorithms (Rocchio-style classifiers, Exponential-Gaussian models, local regression and logistic regression approaches) have been studied for adaptive settings, examined with explicit and implicit relevance feedback, and evaluated with respect to utility optimization on large benchmark data collections in TREC (Text Retrieval Conferences) and TDT (Topic Detection and Tracking) forums [1, 4, 7, 15, 16, 20, 24, 23].",
                "Regularized logistic regression [21] has been found representative for the state-of-the-art approaches, and highly efficient for frequent model adaptations over large document collections such as the TREC-10 corpus (over 800,000 documents and 84 topics).",
                "Despite substantial achievements in recent adaptive filtering research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently.",
                "Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications: 1.",
                "User has a rather passive role in the conventional adaptive filtering setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision.",
                "A more active alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents (or passages) per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists.",
                "The latter form of user interaction has been highly effective in standard retrieval for ad-hoc queries.",
                "How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2.",
                "The unit for receiving a relevance judgment (yes or no) is restricted to the document level in conventional AF.",
                "However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant.",
                "Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system.",
                "For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3.",
                "System-selected documents are often highly redundant.",
                "A major news event, for example, would be reported by multiple sources repeatedly for a while, making most of the information content in those articles redundant with each other.",
                "A conventional AF system would select all these redundant news stories for user feedback, wasting the users time while offering little gain.",
                "Clearly, techniques for novelty detection can help in principle [25, 2, 22] for improving the utility of the AF systems.",
                "However, the effectiveness of such techniques at passage level to detect novelty with respect to users (fine-grained) feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user.",
                "To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and novelty detection for removal of redundancy from user interactions with the system.",
                "We call the new process utility-based information distillation.",
                "Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach.",
                "Therefore, we extended a benchmark corpus - the TDT4 collection of news stories and TV broadcasts - with task definitions, multiple queries per task, and answer keys per query.",
                "We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 .",
                "To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses.",
                "Moreover, we propose an extension of NDCG (Normalized Discounted Cumulated Gain) [9] for assessing the utility of ranked passages as a function of both relevance and novelty.",
                "The rest of this paper is organized as follows.",
                "Section 2 outlines the information distillation process with a concrete example.",
                "Section 3 describes the technical cores of our system called CAFÂ´E - CMU Adaptive Filtering Engine.",
                "Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme.",
                "Section 5 describes the extended TDT4 corpus.",
                "Section 6 presents our experiments and results.",
                "Section 7 concludes the study and gives future perspectives. 2.",
                "A SAMPLE TASK Consider a news event - the escape of seven convicts from a Texas prison in December 2000 and their capture a month later.",
                "Assuming a user were interested in this event since its early stage, the information need could be: Find information about the escape of convicts from Texas prison, and information related to their recapture.",
                "The associated lower-level questions could be: 1.",
                "How many prisoners escaped? 2.",
                "Where and when were they sighted? 3. Who are their known contacts inside and outside the prison? 4.",
                "How are they armed? 5.",
                "Do they have any vehicles? 6.",
                "What steps have been taken so far?",
                "We call such an information need a task, and the associated questions as the queries in this task.",
                "A distillation system is supposed to monitor the incoming documents, process them chunk by chunk in a temporal order, select potentially relevant and novel passages from each chunk with respect to each query, and present a ranked list of passages to the user.",
                "Passage ranking here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list.",
                "When presented with a list of passages, the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant.",
                "These spans of text are taken as positive examples in the adaptation of the query profile, and also added to the users history.",
                "Passages not marked by the user are taken as negative examples.",
                "As soon as the query profile is updated, the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low, based on user preference.",
                "For example, if the user highlights ...officials have posted a $100,000 reward for their capture... as relevant answer to the query What steps have been taken so far?, then the highlighted piece is used as an additional positive training example in the adaptation of the query profile.",
                "This piece of feedback is also added to the user history as a seen example, so that in future, the system will not place another passage mentioning $100,000 reward at the top of the ranked list.",
                "However, an article mentioning ...officials have doubled the reward money to $200,000... might be ranked high since it is both relevant to the (updated) query profile and novel with respect to the (updated) user history.",
                "The user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly.",
                "Clearly, novelty detection is very important for the utility of such a system because of the iterative search.",
                "Without novelty detection, the old relevant passages would be shown to the user repeatedly in each ranked list.",
                "Through the above example, we can see the main properties of our new framework for utility-based information distillation over temporally ordered documents.",
                "Our framework combines and extends the power of adaptive filtering (AF), ad-hoc retrieval (IR) and novelty detection (ND).",
                "Compared to standard IR, our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task.",
                "Compared to conventional AF, it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system.",
                "Compared to past work, this is the first evaluation of novelty detection integrated with adaptive filtering for sequenced queries that allows flexible user feedback over ranked passages.",
                "The combination of AF, IR and ND with the new extensions raises an important research question regarding evaluation methodology: how can we measure the utility of such an information distillation system?",
                "Existing metrics in standard IR, AF and ND are insufficient, and new solutions must be explored, as we will discuss in Section 4, after describing the technical cores of our system in the next section. 3.",
                "TECHNICAL CORES The core components of CAFÂ´E are - 1) AF for incremental learning of query profiles, 2) IR for estimating relevance of passages with respect to query profiles, 3) ND for assessing novelty of passages with respect to users history, and 4) anti-redundancy component to remove redundancy from ranked lists. 3.1 Adaptive Filtering Component We use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for AF [21].",
                "Logistic regression (LR) is a supervised learning algorithm for statistical classification.",
                "Based on a training set of labeled instances, it learns a class model which can then by used to predict the labels of unseen instances.",
                "Its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required, as is the case in adaptive filtering, where the system must learn from each new feedback provided by the user. (See [21] and [23] for computational complexity and implementation issues).",
                "In adaptive filtering, each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query.",
                "For training the model, we use the query itself as the initial positive training example of the class, and the user-highlighted pieces of text (marked as Relevant or Not-relevant) during feedback as additional training examples.",
                "To address the cold start issue in the early stage before any user feedback is obtained, the system uses a small sample from a retrospective corpus as the initial negative examples in the training set.",
                "The details of using logistic regression for adaptive filtering (assigning different weights to positive and negative training instances, and regularizing the objective function to prevent over-fitting on training data) are presented in [21].",
                "The class model wâ learned by Logistic Regression, or the query profile, is a vector whose dimensions are individual terms and whose elements are the regression coefficients, indicating how influential each term is in the query profile.",
                "The query profile is updated whenever a new piece of user feedback is received.",
                "A temporally decaying weight can be applied to each training example, as an option, to emphasize the most recent user feedback. 3.2 Passage Retrieval Component We use standard IR techniques in this part of our system.",
                "Incoming documents are processed in chunks, where each chunk can be defined as a fixed span of time or as a fixed number of documents, as preferred by the user.",
                "For each incoming document, corpus statistics like the IDF (Inverted Document Frequency) of each term are updated.",
                "We use a state-of-the-art named entity identifier and tracker [8, 12] to identify person and location names, and merge them with co-referent named entities seen in the past.",
                "Then the documents are segmented into passages, which can be a whole document, a paragraph, a sentence, or any other continuous span of text, as preferred.",
                "Each passage is represented using a vector of TF-IDF (Term FrequencyInverse Document Frequency) weights, where term can be a word or a named entity.",
                "Given a query profile, i.e. the logistic regression solution wâ as described in Section 3.1, the system computes the posterior probability of relevance for each passage x as fRL(x) â¡ P(y = 1|x, wâ ) = 1 (1 + eâwâÂ·x) (1) Passages are ordered by their relevance scores, and the ones with scores above a threshold (tuned on a training set) comprise the relevance list that is passed on to the novelty detection step. 3.3 Novelty Detection Component CAFÂ´E maintains a user history H(t), which contains all the spans of text hi that the user highlighted (as feedback) during his or her past interactions with the system, up to the current time t. Denoting the history as H(t) = n h1, h2, ..., ht o , (2) the novelty score of a new candidate passage x is computed as: fND(x) = 1 â max iâ1..t {cos(x, hi)} (3) where both candidate passage x and highlighted spans of text hi are represented as TF-IDF vectors.",
                "The novelty score of each passage is compared to a prespecified threshold (also tuned on a training set), and any passage with a score below this threshold is removed from the relevance list. 3.4 Anti-redundant Ranking Component Although the novelty detection component ensures that only novel (previously unseen) information remains in the relevance list, this list might still contain the same novel information at multiple positions in the ranked list.",
                "Suppose, for example, that the user has already read about a $100,000 reward for information about the escaped convicts.",
                "A new piece of news that the award has been increased to $200,000 is novel since the user hasnt read about it yet.",
                "However, multiple news sources would report this news and we might end up showing (redundant) articles from all these sources in a ranked list.",
                "Hence, a ranked list should also be made non-redundant with respect to its own contents.",
                "We use a simplified version of the Maximal Marginal Relevance method [5], originally developed for combining relevance and novelty in text retrieval and summarization.",
                "Our procedure starts with the current list of passages sorted by relevance (section 3.2), filtered by Novelty Detection component (section 3.3), and generates a new non-redundant list as follows: 1.",
                "Take the top passage in the current list as the top one in the new list. 2.",
                "Add the next passage x in the current list to the new list only if fAR(x) > t where fAR(x) = 1 â max piâLnew {cos(x, pi)} and Lnew is the set of passages already selected in the new list. 3.",
                "Repeat step 2 until all the passages in the current list have been examined.",
                "After applying the above-mentioned algorithm, each passage in the new list is sufficiently dissimilar to others, thus favoring diversity rather than redundancy in the new ranked list.",
                "The anti-redundancy threshold t is tuned on a training set. 4.",
                "EVALUATION METHODOLOGY The approach we proposed above for information distillation raises important issues regarding evaluation methodology.",
                "Firstly, since our framework allows the output to be passages at different levels of granularity (e.g. k-sentence windows where k may vary) instead of a fixed length, it is not possible to have pre-annotated relevance judgments at all such granularity levels.",
                "Secondly, since we wish to measure the utility of the system output as a combination of both relevance and novelty, traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time.",
                "Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists.",
                "None of the existing measures in ad-hoc retrieval, adaptive filtering, novelty detection or other related areas (text summarization and question answering) have desirable properties in all the three aspects.",
                "Therefore, we must develop a <br>new evaluation methodology</br>. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers.",
                "Answer keys define what should be present in a system response to receive credit, and are comprised of a collection of information nuggets, i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them.",
                "Defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping [19], since system-returned passages can contain the same information expressed in many different ways.",
                "Hence, QA evaluations have relied on human assessors for the mapping between various expressions, making the process costly, time consuming, and not scalable to large query and document collections, and extensive system evaluations with various parameter settings. 4.1.1 Automating Evaluation based on Answer Keys Automatic evaluation methods would allow for faster system building and tuning, as well as provide an objective and affordable way of comparing various systems.",
                "Recently, such methods have been proposed, more or less, based on the idea of n-gram co-occurrences.",
                "Pourpre [10] assigns a fractional recall score to a system response based on its unigram overlap with a given nuggets description.",
                "For example, a system response A B C has recall 3/4 with respect to a nugget with description A B C D. However, such an approach is unfair to systems that present the same information but using words other than A, B, C, and D. Another open issue is how to weight individual words in measuring the closeness of a match.",
                "For example, consider the question How many prisoners escaped?.",
                "In the nugget Seven prisoners escaped from a Texas prison, there is no indication that seven is the keyword, and that it must be matched to get any relevance credit.",
                "Using IDF values does not help, since seven will generally not have a higher IDF than words like texas and prison.",
                "Also, redefining the nugget as just seven does not solve the problem since now it might spuriously match any mention of seven out of context.",
                "Nuggeteer [13] works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold.",
                "However, it is also plagued by spurious relevance since not all words contained in the nugget description (or known correct responses) are central to the nugget. 4.1.2 Nugget-Matching Rules We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on nugget-matching rules, which are generated using a semiautomatic procedure explained below.",
                "These rules are essentially Boolean queries that will only match against snippets that contain the nugget.",
                "For instance, a candidate rule for matching answers to How many prisoners escaped? is (Texas AND seven AND escape AND (convicts OR prisoners)), possibly with other synonyms and variants in the rule.",
                "For a corpus of news articles, which usually follow a typical formal prose, it is fairly easy to write such simple rules to match expected answers using a bootstrap approach, as described below.",
                "We propose a two-stage approach, inspired by Autoslog [14], that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents.",
                "In the first stage, human subjects annotated (using a highlighting tool) portions of ontopic documents that contained answers to each nugget 2 .",
                "In the second stage, subjects used our rule generation tool to create rules that would match the annotations for each nugget.",
                "The tool allows users to enter a Boolean rule as a disjunction of conjunctions (e.g. ((a AND b) OR (a AND c AND d) OR (e))).",
                "Given a candidate rule, our tool uses it as a Boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match.",
                "Hence, the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision.",
                "We observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions (adding more ANDs), and improving the recall by adding more conjunctions to the disjunction (adding more ORs).",
                "As an example, lets try to create a rule for the nugget which says that seven prisoners escaped from the Texas prison.",
                "We start with a simple rule - (seven).",
                "When we input this into the rule generation tool, we realize that this rule matches many spurious occurrences of seven (e.g. ...seven states...) and thus gets a low precision score.",
                "We can further qualify our rule - Texas AND seven AND convicts.",
                "Next, by looking at the missed annotations, we realize that some news articles mentioned ...seven prisoners escaped.... We then replace convicts with the disjunction (convicts OR prisoners).",
                "We continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the (small number of) misses and false alarms can be safely ignored.",
                "Thus we can create nugget-matching rules that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses.",
                "Human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably. 4.2 Evaluating the Utility of a Sequence of Ranked Lists The utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information, and how much the user lost in terms of time and energy.",
                "We calculate this utility from the utilities of individual passages as follows.",
                "After reading each passage returned by the system, the user derives some gain depending on the presence of relevant and novel information, and incurs a loss in terms of the time and energy spent in going through the passage.",
                "However, the likelihood that the user would actually read a passage depends on its position in the ranked list.",
                "Hence, for a query q, the expected utility 2 LDC [18] already provides relevance judgments for 100 topics on the TDT4 corpus.",
                "We further ensured that these judgments are exhaustive on the entire corpus using pooling. of a passage pi at rank i can be defined as U(pi, q) = P(i) â (Gain(pi, q) â Loss(pi, q)) (4) where P(i) is the probability that the user would go through a passage at rank i.",
                "The expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage: U(q) = nX i=1 P(i) â (Gain(pi, q) â Loss(pi, q)) (5) Note that if we ignore the loss term and define P(i) as P(i) â 1/ logb(b + i â 1) (6) then we get the recently popularized metric called Discounted Cumulated Gain (DCG) [9], where Gain(pi, q) is defined as the graded relevance of passage pi.",
                "However, without the loss term, DCG is a purely recall-oriented metric and not suitable for an adaptive filtering setting, where the systems utility depends in part on its ability to limit the number of items shown to the user.",
                "Although P(i) could be defined based on empirical studies of user behavior, for simplicity, we use P(i) exactly as defined in equation 6.",
                "The gain G(pi, q) of passage pi with respect to the query q is a function of - 1) the number of relevant nuggets present in pi, and 2) the novelty of each of these nuggets.",
                "We combine these two factors as follows.",
                "For each nugget Nj, we assign an initial weight wj, and also keep a count nj of the number of times this nugget has been seen by the user in the past.",
                "The gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor Î³.",
                "Thus, G(pi, q) is defined as G(pi, q) = X Nj âC(pi,q) wj â Î³nj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the answer key of query q.",
                "The initial weights wj are all set of be 1.0 in our experiments, but can also be set based on a pyramid approach [11].",
                "The choice of dampening factor Î³ determines the users tolerance for redundancy.",
                "When Î³ = 0, a nugget will only receive credit for its first occurrence i.e. when nj is zero3 .",
                "For 0 < Î³ < 1, a nugget receives smaller credit for each successive occurrence.",
                "When Î³ = 1, no dampening occurs and repeated occurrences of a nugget receive the same credit.",
                "Note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system, since the users are expected to remember what the system showed them in the past.",
                "We define the loss L(pi, q) as a constant cost c (we use 0.1) incurred when reading a system-returned passage.",
                "Thus, our metric can be re-written as U(q) = nX i=1 Gain(pi, q) logb(b + i â 1) â L(n) (8) where L(n) is the loss associated with a ranked list of length n: L(n) = c Â· nX i=1 1 logb(b + i â 1) (9) 3 Note that 00 = 1 Due to the similarity with Discounted Cumulated Gain (DCG), we call our metric Discounted Cumulated Utility (DCU).",
                "The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) â¤ 0 i.e. when the gain is less than or equal to the cost of reading the passage. 5.",
                "DATA TDT4 was the benchmark corpus used in TDT2002 and TDT2003 evaluations.",
                "The corpus consists of over 90, 000 news articles from multiple sources (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) published between October 2000 and January 2001, in the languages of Arabic, English, and Mandarin.",
                "Speech-recognized and machine-translated versions of the non-English articles were provided as well.",
                "LDC [18] has annotated the corpus with 100 topics, that correspond to various news events in this time period.",
                "Out of these, we selected a subset of 12 actionable events, and defined corresponding tasks for them4 .",
                "For each task, we manually defined a profile consisting of an initial set of (5 to 10) queries, a free-text description of the user history, i.e., what the user already knows about the event, and a list of known on-topic and off-topic documents (if available) as training examples.",
                "For each query, we generated answer keys and corresponding nugget matching rules using the procedure described in section 4.1.2, and produced a total of 120 queries, with an average of 7 nuggets per query. 6.",
                "EXPERIMENTS AND RESULTS 6.1 Baselines We used Indri [17], a popular language-model based retrieval engine, as a baseline for comparison with CAFÂ´E.",
                "Indri supports standard search engine functionality, including pseudo-relevance feedback (PRF) [3, 6], and is representative of a typical query-based retrieval system.",
                "Indri does not support any kind of novelty detection.",
                "We compare Indri with PRF turned on and off, against CAFÂ´E with user feedback, novelty detection and antiredundant ranking turned on and off. 6.2 Experimental Setup We divided the TDT4 corpus spanning 4 months into 10 chunks, each defined as a period of 12 consecutive days.",
                "At any given point of time in the distillation process, each system accessed the past data up to the current point, and returned a ranked list of up 50 passages per query.",
                "The 12 tasks defined on the corpus were divided into a training and test set with 6 tasks each.",
                "Each system was allowed to use the training set to tune its parameters for optimizing NDCU (equation 8), including the relevance threshold for both Indri and CAFÂ´E, and the novelty and antiredundancy thresholds for CAFÂ´E.",
                "The NDCU for each system run is calculated automatically.",
                "User feedback was also simulated - relevance judgments for each system-returned passage (as determined by the nugget matching rules described in section 4.1.2) were 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figure 1: Performance of Indri across chunks Figure 2: Performance of CAFÂ´E across chunks used as user feedback in the adaptation of query profiles and user histories. 6.3 Results In Table 1, we show the NDCU scores of the two systems under various settings.",
                "These scores are averaged over the six tasks in the test set, and are calculated with two dampening factors (see section 4.2): Î³ = 0 and 0.1, to simulate no tolerance and small tolerance for redundancy, respectively.",
                "Using Î³ = 0 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information.",
                "Hence, the improvement obtained from enabling user feedback is smaller with Î³ = 0 than the improvement obtained from feedback with Î³ = 0.1.",
                "This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information.",
                "However, the user does not benefit from seeing such redundant passages, and is usually interested in other passages containing related information.",
                "It is informative to evaluate retrieval systems using our utility measure (with Î³ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.",
                "Sometimes, however, users might indeed be interested in seeing the same information from multiple sources, as an Table 1: NDCU Scores of Indri and CAFÂ´E for two dampening factors (Î³), and various settings (F: Feedback, N: Novelty Detection, A: Anti-Redundant Ranking) Indri CAFÂ´E Î³ Base +PRF Base +F +F+N +F+A +F+N+A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.35 0.36 0.36 indicator of its importance or reliability.",
                "In such a case, we can simply choose a higher value for Î³ which corresponds to a higher tolerance for redundancy, and hence let the system tune its parameters accordingly.",
                "Since documents were processed chunk by chunk, it would be interesting to see how the performance of systems improves over time.",
                "Figures 1 and 2 show the performance trends for both the systems across chunks.",
                "While the performance with and without feedback on the first few chunks is expected to be close, for subsequent chunks, the performance curve with feedback enabled rises above the one with the no-feedback setting.",
                "The performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks, making some queries easier than others in certain chunks.",
                "Moreover, since Indri uses pseudo-relevance feedback while CAFÂ´E uses feedback based on actual relevance judgments, the improvement in case of Indri is less dramatic than that of CAFÂ´E. 7.",
                "CONCLUDING REMARKS This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages.",
                "Our system, called CAFÂ´E, combines adaptive filtering, novelty detection and antiredundant passage ranking in a unified framework for utility optimization.",
                "We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty.",
                "Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off. 8.",
                "ACKNOWLEDGMENTS We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments.",
                "This work is supported in parts by the National Science Foundation (NSF) under grant IIS0434035, and the Defense Advanced Research Project Agency (DARPA) under contracts NBCHD030010 and W0550432.",
                "Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. 9.",
                "ADDITIONAL AUTHORS Jian Zhang (jianzhan@stat.purdue.edu)â , Jaime Carbonell (jgc@cs.cmu.edu)â  , Peter Brusilovsky (peterb+@pitt.edu)â¡ , Daqing He(dah44@pitt.edu)â¡ 10.",
                "REFERENCES [1] J. Allan.",
                "Incremental Relevance Feedback for Information Filtering.",
                "Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 270-278, 1996. [2] J. Allan, C. Wade, and A. Bolivar.",
                "Retrieval and Novelty Detection at the Sentence Level.",
                "Proceedings of the ACM SIGIR conference on research and development in information retrieval, 2003. [3] C. Buckley, G. Salton, and J. Allan.",
                "Automatic Retrieval with Locality Information using SMART.",
                "NIST special publication, (500207):59-72, 1993. [4] J. Callan.",
                "Learning While Filtering Documents.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 224-231, 1998. [5] J. Carbonell and J. Goldstein.",
                "The use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335-336, 1998. [6] E. Efthimiadis.",
                "Query Expansion.",
                "Annual Review of Information Science and Technology (ARIST), 31:p121-87, 1996. [7] J. Fiscus and G. Duddington.",
                "Topic Detection and Tracking Overview.",
                "Topic Detection and Tracking: Event-based Information Organization, pages 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.",
                "A Statistical Model for Multilingual Entity Detection and Tracking.",
                "NAACL/HLT, 2004. [9] K. JÂ¨arvelin and J. KekÂ¨alÂ¨ainen.",
                "Cumulated Gain-based Evaluation of IR Techniques.",
                "ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin and D. Demner-Fushman.",
                "Automatically Evaluating Answers to Definition Questions.",
                "Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), 2005. â Statistics Dept., Purdue University, West Lafayette, USA â  Language Technologies Inst., Carnegie Mellon University, Pittsburgh, USA â¡ School of Information Sciences, Univ. of Pittsburgh, Pittsburgh, USA [11] J. Lin and D. Demner-Fushman.",
                "Will Pyramids Built of nUggets Topple Over.",
                "Proceedings of HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.",
                "A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree.",
                "Proc. of ACL, 4:136-143, 2004. [13] G. Marton.",
                "Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments.",
                "HLT/NAACL, 2006. [14] E. Riloff.",
                "Automatically Constructing a Dictionary for Information Extraction Tasks.",
                "Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, 1993. [15] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9: Filtering track.",
                "The Ninth Text REtrieval Conference (TREC-9), pages 361-368. [16] R. Schapire, Y.",
                "Singer, and A. Singhal.",
                "Boosting and Rocchio Applied to Text Filtering.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle, and W. Croft.",
                "Indri: A Language Model-based Search Engine for Complex Queries.",
                "Proceedings of the International Conference on Intelligence Analysis, 2004. [18] The Linguistic Data Consortium. http://www.ldc.upenn.edu/. [19] E. Voorhees.",
                "Overview of the TREC 2003 Question Answering Track.",
                "Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), 2003. [20] Y. Yang and B. Kisiel.",
                "Margin-based Local Regression for Adaptive Filtering.",
                "Proceedings of the twelfth international conference on Information and knowledge management, pages 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of Adaptive Filtering Methods in a Cross-benchmark Evaluation.",
                "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 98-105, 2005. [22] C. Zhai, W. Cohen, and J. Lafferty.",
                "Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-17, 2003. [23] J. Zhang and Y. Yang.",
                "Robustness of Regularized Linear Classification Methods in Text Categorization.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 190-197, 2003. [24] Y. Zhang.",
                "Using Bayesian Priors to Combine Classifiers for Adaptive Filtering.",
                "Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 345-352, 2004. [25] Y. Zhang, J. Callan, and T. Minka.",
                "Novelty and Redundancy Detection in Adaptive Filtering.",
                "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002."
            ],
            "original_annotated_samples": [
                "Therefore, we must develop a <br>new evaluation methodology</br>. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers."
            ],
            "translated_annotated_samples": [
                "Por lo tanto, debemos desarrollar una <br>nueva metodologÃ­a de evaluaciÃ³n</br>. 4.1 Claves de Respuesta Para permitir la evaluaciÃ³n de un sistema cuya salida consiste en pasajes de longitud arbitraria, tomamos prestado el concepto de claves de respuesta de la comunidad de Preguntas y Respuestas (QA), donde los sistemas pueden devolver fragmentos de texto arbitrarios como respuestas."
            ],
            "translated_text": "DestilaciÃ³n de informaciÃ³n basada en utilidad sobre documentos secuenciados temporalmente en el Instituto de TecnologÃ­as del Lenguaje de Yiming Yang. Universidad Carnegie Mellon, Pittsburgh, EE. UU. yiming@cs.cmu.edu Abhimanyu Lad Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. alad@cs.cmu.edu Instituto de TecnologÃ­as del Lenguaje Ni Lao. Universidad Carnegie Mellon, Pittsburgh, EE. UU. nlao@cs.cmu.edu Abhay Harpale Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. aharpale@cs.cmu.edu Bryan Kisiel Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. bkisiel@cs.cmu.edu MÃ³nica Rogati Instituto de TecnologÃ­as del Lenguaje. La Universidad Carnegie Mellon en Pittsburgh, EE. UU. mrogati@cs.cmu.edu RESUMEN Este artÃ­culo examina un nuevo enfoque para la destilaciÃ³n de informaciÃ³n en documentos ordenados temporalmente, y propone un esquema de evaluaciÃ³n novedoso para dicho marco. Combina las fortalezas y se extiende mÃ¡s allÃ¡ del filtrado adaptativo convencional, la detecciÃ³n de novedades y la clasificaciÃ³n de pasajes no redundantes con respecto a las necesidades de informaciÃ³n duraderas (tareas con mÃºltiples consultas). Nuestro enfoque respalda la retroalimentaciÃ³n detallada del usuario a travÃ©s del resaltado de fragmentos arbitrarios de texto, y aprovecha dicha informaciÃ³n para la optimizaciÃ³n de la utilidad en entornos adaptativos. Para nuestros experimentos, definimos tareas hipotÃ©ticas basadas en eventos de noticias en el corpus TDT4, con mÃºltiples consultas por tarea. Se generaron claves de respuestas (nuggets) para cada consulta y se utilizÃ³ un procedimiento semiautomÃ¡tico para adquirir reglas que permiten emparejar automÃ¡ticamente los nuggets con las respuestas del sistema. TambiÃ©n proponemos una extensiÃ³n de la mÃ©trica NDCG para evaluar la utilidad de los pasajes clasificados como una combinaciÃ³n de relevancia y novedad. Nuestros resultados muestran mejoras alentadoras en la utilidad utilizando el nuevo enfoque, en comparaciÃ³n con los sistemas base sin aprendizaje incremental o componentes de detecciÃ³n de novedades. CategorÃ­as y Descriptores de Asignaturas H.3.3 [BÃºsqueda y RecuperaciÃ³n de InformaciÃ³n]: Filtrado de informaciÃ³n, RetroalimentaciÃ³n de relevancia, Modelos de recuperaciÃ³n, Proceso de selecciÃ³n; I.5.2 TÃ©rminos Generales DiseÃ±o, MediciÃ³n, Rendimiento, ExperimentaciÃ³n. 1. INTRODUCCIÃN Seguir informaciÃ³n nueva y relevante de flujos de datos temporales para usuarios con necesidades duraderas ha sido un tema de investigaciÃ³n desafiante en la recuperaciÃ³n de informaciÃ³n. El filtrado adaptativo (AF) es una tarea de predicciÃ³n en lÃ­nea de la relevancia de cada nuevo documento con respecto a temas predefinidos. BasÃ¡ndose en la consulta inicial y algunos ejemplos positivos (si estÃ¡n disponibles), un sistema de AF mantiene un perfil para cada tema de interÃ©s, y lo actualiza constantemente segÃºn la retroalimentaciÃ³n del usuario. La naturaleza de aprendizaje incremental de los sistemas de AF los hace mÃ¡s poderosos que los motores de bÃºsqueda estÃ¡ndar que admiten recuperaciÃ³n ad-hoc (por ejemplo. Google y Yahoo) en tÃ©rminos de encontrar informaciÃ³n relevante con respecto a temas de interÃ©s duraderos, y mÃ¡s atractivo para los usuarios que estÃ¡n dispuestos a proporcionar retroalimentaciÃ³n para adaptar el sistema a sus necesidades de informaciÃ³n especÃ­ficas, sin tener que modificar sus consultas manualmente. Se han estudiado una variedad de algoritmos de aprendizaje supervisado (clasificadores de estilo Rocchio, modelos exponenciales-gaussianos, regresiÃ³n local y enfoques de regresiÃ³n logÃ­stica) para entornos adaptativos, examinados con retroalimentaciÃ³n de relevancia explÃ­cita e implÃ­cita, y evaluados con respecto a la optimizaciÃ³n de la utilidad en grandes colecciones de datos de referencia en los foros de TREC (Conferencias de RecuperaciÃ³n de Texto) y TDT (DetecciÃ³n y Seguimiento de Temas) [1, 4, 7, 15, 16, 20, 24, 23]. La regresiÃ³n logÃ­stica regularizada [21] se ha encontrado representativa para los enfoques de vanguardia, y altamente eficiente para adaptaciones frecuentes del modelo en grandes colecciones de documentos como el corpus TREC-10 (mÃ¡s de 800,000 documentos y 84 temas). A pesar de los logros sustanciales en la investigaciÃ³n reciente de filtrado adaptativo, aÃºn quedan problemas significativos sin resolver sobre cÃ³mo aprovechar de manera efectiva y eficiente la retroalimentaciÃ³n del usuario. EspecÃ­ficamente, los siguientes problemas pueden limitar seriamente la verdadera utilidad de los sistemas de AF en aplicaciones del mundo real: 1. El usuario tiene un papel bastante pasivo en la configuraciÃ³n convencional de filtrado adaptativo: solo reacciona ante el sistema cuando este toma una decisiÃ³n afirmativa sobre un documento, confirmando o rechazando esa decisiÃ³n. Una alternativa mÃ¡s activa serÃ­a permitir al usuario emitir mÃºltiples consultas sobre un tema, revisar una lista clasificada de documentos candidatos (o pasajes) por consulta, y proporcionar retroalimentaciÃ³n sobre la lista clasificada, refinando asÃ­ su necesidad de informaciÃ³n y solicitando listas clasificadas actualizadas. La Ãºltima forma de interacciÃ³n del usuario ha sido altamente efectiva en la recuperaciÃ³n estÃ¡ndar para consultas ad-hoc. CÃ³mo implementar una estrategia para necesidades de informaciÃ³n duraderas en entornos de FA es una pregunta abierta para la investigaciÃ³n. 2. La unidad para recibir un juicio de relevancia (sÃ­ o no) estÃ¡ restringida al nivel del documento en AF convencional. Sin embargo, un usuario real puede estar dispuesto a proporcionar comentarios mÃ¡s informativos y detallados resaltando algunas partes de texto en un documento recuperado como relevantes, en lugar de etiquetar todo el documento como relevante. Aprovechar de manera efectiva este tipo de retroalimentaciÃ³n detallada podrÃ­a mejorar sustancialmente la calidad de un sistema de AF. Para esto, necesitamos habilitar el aprendizaje supervisado a partir de fragmentos de texto etiquetados de longitud arbitraria en lugar de solo permitir documentos etiquetados. 3. Los documentos seleccionados por el sistema suelen ser altamente redundantes. Un evento importante de noticias, por ejemplo, serÃ­a reportado por mÃºltiples fuentes repetidamente durante un tiempo, haciendo que la mayor parte del contenido informativo en esos artÃ­culos sea redundante entre sÃ­. Un sistema de AF convencional seleccionarÃ­a todas estas historias de noticias redundantes para la retroalimentaciÃ³n del usuario, desperdiciando el tiempo de los usuarios mientras ofrece poco beneficio. Claramente, las tÃ©cnicas de detecciÃ³n de novedades pueden ayudar en principio [25, 2, 22] a mejorar la utilidad de los sistemas AF. Sin embargo, la efectividad de tales tÃ©cnicas a nivel de pasaje para detectar novedad con respecto a la retroalimentaciÃ³n de los usuarios a nivel detallado y para detectar redundancia en listas clasificadas aÃºn debe ser evaluada utilizando una medida de utilidad que imite las necesidades de un usuario real. Para abordar las limitaciones actuales de los sistemas de AF mencionadas anteriormente, proponemos y examinamos un nuevo enfoque en este artÃ­culo, que combina las fortalezas del AF convencional (aprendizaje incremental de modelos de temas), recuperaciÃ³n de pasajes multipase para consultas de larga duraciÃ³n condicionadas al tema, y detecciÃ³n de novedades para eliminar la redundancia de las interacciones del usuario con el sistema. Llamamos al nuevo proceso destilaciÃ³n de informaciÃ³n basada en utilidad. Ten en cuenta que los corpus de referencia convencionales para evaluaciones de AF, que tienen juicios de relevancia a nivel de documento y no definen tareas con mÃºltiples consultas, son insuficientes para evaluar el nuevo enfoque. Por lo tanto, ampliamos un corpus de referencia, la colecciÃ³n TDT4 de noticias y transmisiones de televisiÃ³n, con definiciones de tareas, mÃºltiples consultas por tarea y claves de respuestas por consulta. Hemos realizado nuestros experimentos en este corpus extendido TDT4 y hemos puesto a disposiciÃ³n del pÃºblico los datos generados adicionalmente para futuras evaluaciones comparativas. Para evaluar automÃ¡ticamente los fragmentos de texto arbitrarios devueltos por el sistema utilizando nuestras claves de respuestas, desarrollamos un esquema de evaluaciÃ³n con un procedimiento semiautomÃ¡tico para 1 URL: http://nyc.lti.cs.cmu.edu/downloads adquiriendo reglas que pueden comparar fragmentos con las respuestas del sistema. AdemÃ¡s, proponemos una extensiÃ³n de NDCG (Normalized Discounted Cumulated Gain) [9] para evaluar la utilidad de los pasajes clasificados como una funciÃ³n tanto de la relevancia como de la novedad. El resto de este documento estÃ¡ organizado de la siguiente manera. La secciÃ³n 2 describe el proceso de destilaciÃ³n de informaciÃ³n con un ejemplo concreto. La secciÃ³n 3 describe los nÃºcleos tÃ©cnicos de nuestro sistema llamado CAFÂ´E - CMU Adaptive Filtering Engine. La secciÃ³n 4 discute problemas con respecto a la metodologÃ­a de evaluaciÃ³n y propone un nuevo esquema. La secciÃ³n 5 describe el corpus extendido TDT4. La secciÃ³n 6 presenta nuestros experimentos y resultados. La secciÃ³n 7 concluye el estudio y ofrece perspectivas futuras. 2. Una TAREA DE EJEMPLO Considera un evento noticioso: la fuga de siete convictos de una prisiÃ³n de Texas en diciembre de 2000 y su captura un mes despuÃ©s. Suponiendo que un usuario estuviera interesado en este evento desde sus primeras etapas, la necesidad de informaciÃ³n podrÃ­a ser: Encontrar informaciÃ³n sobre la fuga de convictos de una prisiÃ³n de Texas, y la informaciÃ³n relacionada con su recaptura. Las preguntas de nivel inferior asociadas podrÃ­an ser: 1. Â¿CuÃ¡ntos prisioneros escaparon? 2. Â¿DÃ³nde y cuÃ¡ndo fueron avistados? 3. Â¿QuiÃ©nes son sus contactos conocidos dentro y fuera de la prisiÃ³n? 4. Â¿CÃ³mo estÃ¡n armados? 5. Â¿Tienen algÃºn vehÃ­culo? 6. Â¿QuÃ© pasos se han tomado hasta ahora? Llamamos a esta necesidad de informaciÃ³n una tarea, y a las preguntas asociadas como las consultas en esta tarea. Un sistema de destilaciÃ³n debe monitorear los documentos entrantes, procesarlos por fragmentos en un orden temporal, seleccionar pasajes potencialmente relevantes y novedosos de cada fragmento con respecto a cada consulta, y presentar una lista clasificada de pasajes al usuario. La clasificaciÃ³n de pasajes aquÃ­ se basa en cuÃ¡n relevante es un pasaje con respecto a la consulta actual, cuÃ¡n novedoso es con respecto al historial actual del usuario (de sus interacciones con el sistema) y cuÃ¡n redundante es en comparaciÃ³n con otros pasajes con un rango mÃ¡s alto en la lista. Cuando se le presenta una lista de pasajes, el usuario puede proporcionar retroalimentaciÃ³n resaltando tramos de texto arbitrarios que considere relevantes. Estos fragmentos de texto se toman como ejemplos positivos en la adaptaciÃ³n del perfil de consulta, y tambiÃ©n se aÃ±aden al historial de los usuarios. Los pasajes no marcados por el usuario se consideran ejemplos negativos. Tan pronto como se actualiza el perfil de consulta, el sistema vuelve a realizar una bÃºsqueda y devuelve otra lista clasificada de pasajes donde los pasajes previamente vistos son eliminados o clasificados como bajos, segÃºn la preferencia del usuario. Por ejemplo, si el usuario resalta ...los funcionarios han ofrecido una recompensa de $100,000 por su captura... como respuesta relevante a la consulta Â¿QuÃ© pasos se han tomado hasta ahora?, entonces la pieza resaltada se utiliza como un ejemplo de entrenamiento positivo adicional en la adaptaciÃ³n del perfil de la consulta. Este comentario tambiÃ©n se aÃ±ade al historial del usuario como un ejemplo visto, para que en el futuro, el sistema no coloque otro pasaje mencionando una recompensa de $100,000 en la parte superior de la lista clasificada. Sin embargo, un artÃ­culo que mencione que los funcionarios han duplicado el dinero de recompensa a $200,000 podrÃ­a clasificarse alto ya que es relevante tanto para el perfil de consulta (actualizado) como novedoso con respecto a la historia del usuario (actualizada). El usuario puede modificar las consultas originales o agregar una nueva consulta durante el proceso; los perfiles de consulta se cambiarÃ¡n en consecuencia. Claramente, la detecciÃ³n de novedades es muy importante para la utilidad de dicho sistema debido a la bÃºsqueda iterativa. Sin detecciÃ³n de novedad, los antiguos pasajes relevantes se mostrarÃ­an al usuario repetidamente en cada lista clasificada. A travÃ©s del ejemplo anterior, podemos ver las principales propiedades de nuestro nuevo marco de trabajo para la destilaciÃ³n de informaciÃ³n basada en utilidad sobre documentos ordenados temporalmente. Nuestro marco de trabajo combina y amplÃ­a el poder del filtrado adaptativo (AF), la recuperaciÃ³n ad-hoc (IR) y la detecciÃ³n de novedades (ND). En comparaciÃ³n con la IR estÃ¡ndar, nuestro enfoque tiene la capacidad de aprender de forma incremental las necesidades de informaciÃ³n a largo plazo y modelar una secuencia de consultas dentro de una tarea. En comparaciÃ³n con el AF convencional, permite un papel mÃ¡s activo del usuario en refinar sus necesidades de informaciÃ³n y solicitar nuevos resultados al permitir comentarios sobre relevancia y novedad mediante el resaltado de fragmentos arbitrarios de texto en los pasajes devueltos por el sistema. Comparado con trabajos anteriores, esta es la primera evaluaciÃ³n de detecciÃ³n de novedad integrada con filtrado adaptativo para consultas secuenciadas que permite retroalimentaciÃ³n flexible del usuario sobre pasajes clasificados. La combinaciÃ³n de AF, IR y ND con las nuevas extensiones plantea una importante pregunta de investigaciÃ³n sobre la metodologÃ­a de evaluaciÃ³n: Â¿cÃ³mo podemos medir la utilidad de un sistema de destilaciÃ³n de informaciÃ³n como este? Las mÃ©tricas existentes en IR estÃ¡ndar, AF y ND son insuficientes, y se deben explorar nuevas soluciones, como discutiremos en la SecciÃ³n 4, despuÃ©s de describir los nÃºcleos tÃ©cnicos de nuestro sistema en la prÃ³xima secciÃ³n. 3. NÃCLEOS TÃCNICOS Los componentes principales de CAFÃ son: 1) AF para el aprendizaje incremental de perfiles de consulta, 2) IR para estimar la relevancia de pasajes con respecto a los perfiles de consulta, 3) ND para evaluar la novedad de pasajes con respecto al historial de usuarios, y 4) un componente anti-redundancia para eliminar la redundancia de las listas clasificadas. 3.1 Componente de Filtrado Adaptativo Utilizamos un algoritmo de vanguardia en el campo, el mÃ©todo de regresiÃ³n logÃ­stica regularizada que obtuvo los mejores resultados en varios corpus de evaluaciÃ³n de referencia para AF [21]. La regresiÃ³n logÃ­stica (LR) es un algoritmo de aprendizaje supervisado para clasificaciÃ³n estadÃ­stica. BasÃ¡ndose en un conjunto de entrenamiento de instancias etiquetadas, aprende un modelo de clase que luego puede ser utilizado para predecir las etiquetas de instancias no vistas. Su rendimiento, asÃ­ como su eficiencia en tÃ©rminos de tiempo de entrenamiento, lo convierten en un buen candidato cuando se requieren actualizaciones frecuentes del modelo de clase, como es el caso en el filtrado adaptativo, donde el sistema debe aprender de cada nueva retroalimentaciÃ³n proporcionada por el usuario. (Consulte [21] y [23] para complejidad computacional y problemas de implementaciÃ³n). En el filtrado adaptativo, cada consulta se considera como una clase y la probabilidad de que un pasaje pertenezca a esta clase corresponde al grado de relevancia del pasaje con respecto a la consulta. Para entrenar el modelo, utilizamos la propia consulta como ejemplo inicial de entrenamiento positivo de la clase, y las piezas de texto resaltadas por el usuario (marcadas como Relevante o No relevante) durante la retroalimentaciÃ³n como ejemplos de entrenamiento adicionales. Para abordar el problema de inicio en frÃ­o en la etapa inicial antes de obtener cualquier retroalimentaciÃ³n del usuario, el sistema utiliza una pequeÃ±a muestra de un corpus retrospectivo como ejemplos negativos iniciales en el conjunto de entrenamiento. Los detalles de cÃ³mo utilizar la regresiÃ³n logÃ­stica para el filtrado adaptativo (asignando diferentes pesos a las instancias de entrenamiento positivas y negativas, y regularizando la funciÃ³n objetivo para prevenir el sobreajuste en los datos de entrenamiento) se presentan en [21]. El modelo de clase wâ aprendido por RegresiÃ³n LogÃ­stica, o el perfil de consulta, es un vector cuyas dimensiones son tÃ©rminos individuales y cuyos elementos son los coeficientes de regresiÃ³n, indicando cuÃ¡n influyente es cada tÃ©rmino en el perfil de consulta. El perfil de la consulta se actualiza cada vez que se recibe una nueva pieza de retroalimentaciÃ³n del usuario. Un peso decaÃ­do temporalmente se puede aplicar a cada ejemplo de entrenamiento, como una opciÃ³n, para enfatizar la retroalimentaciÃ³n mÃ¡s reciente del usuario. 3.2 Componente de RecuperaciÃ³n de Pasajes Utilizamos tÃ©cnicas estÃ¡ndar de IR en esta parte de nuestro sistema. Los documentos entrantes se procesan por bloques, donde cada bloque puede definirse como un intervalo de tiempo fijo o como un nÃºmero fijo de documentos, segÃºn lo prefiera el usuario. Para cada documento entrante, se actualizan las estadÃ­sticas del corpus como el IDF (Frecuencia Inversa de Documento) de cada tÃ©rmino. Utilizamos un identificador y rastreador de entidades con tecnologÃ­a de punta [8, 12] para identificar nombres de personas y lugares, y fusionarlos con entidades con referencia cruzada vistas en el pasado. Luego, los documentos se dividen en pasajes, que pueden ser un documento completo, un pÃ¡rrafo, una oraciÃ³n, o cualquier otro fragmento continuo de texto, segÃºn se prefiera. Cada pasaje se representa utilizando un vector de pesos TF-IDF (Frecuencia de TÃ©rmino - Frecuencia Inversa de Documento), donde el tÃ©rmino puede ser una palabra o una entidad nombrada. Dado un perfil de consulta, es decir, la soluciÃ³n de regresiÃ³n logÃ­stica wâ como se describe en la SecciÃ³n 3.1, el sistema calcula la probabilidad posterior de relevancia para cada pasaje x como fRL(x) â¡ P(y = 1|x, wâ ) = 1 (1 + eâwâÂ·x) (1). Los pasajes se ordenan segÃºn sus puntajes de relevancia, y aquellos con puntajes por encima de un umbral (ajustado en un conjunto de entrenamiento) conforman la lista de relevancia que se pasa al paso de detecciÃ³n de novedad. 3.3 Componente de DetecciÃ³n de Novedad CAFÂ´E mantiene un historial de usuario H(t), que contiene todos los fragmentos de texto hi que el usuario resaltÃ³ (como retroalimentaciÃ³n) durante sus interacciones pasadas con el sistema, hasta el tiempo actual t. Denotando el historial como H(t) = n h1, h2, ..., ht o , (2), el puntaje de novedad de un nuevo pasaje candidato x se calcula como: fND(x) = 1 â max iâ1..t {cos(x, hi)} (3), donde tanto el pasaje candidato x como los fragmentos de texto resaltados hi se representan como vectores TF-IDF. El puntaje de novedad de cada pasaje se compara con un umbral predefinido (ajustado tambiÃ©n en un conjunto de entrenamiento), y cualquier pasaje con un puntaje por debajo de este umbral se elimina de la lista de relevancia. Componente de clasificaciÃ³n anti-redundante Aunque el componente de detecciÃ³n de novedad garantiza que solo la informaciÃ³n novedosa (no vista previamente) permanezca en la lista de relevancia, esta lista aÃºn podrÃ­a contener la misma informaciÃ³n novedosa en mÃºltiples posiciones en la lista clasificada. Supongamos, por ejemplo, que el usuario ya ha leÃ­do sobre una recompensa de $100,000 por informaciÃ³n sobre los convictos fugados. Una nueva noticia de que el premio ha sido aumentado a $200,000 es novedosa ya que el usuario aÃºn no la ha leÃ­do. Sin embargo, mÃºltiples fuentes de noticias podrÃ­an informar sobre esta noticia y podrÃ­amos terminar mostrando artÃ­culos (redundantes) de todas estas fuentes en una lista clasificada. Por lo tanto, una lista clasificada tambiÃ©n debe ser no redundante con respecto a su propio contenido. Utilizamos una versiÃ³n simplificada del mÃ©todo de MÃ¡xima Relevancia Marginal [5], originalmente desarrollado para combinar relevancia y novedad en la recuperaciÃ³n de texto y resÃºmenes. Nuestro procedimiento comienza con la lista actual de pasajes ordenados por relevancia (secciÃ³n 3.2), filtrados por el componente de DetecciÃ³n de Novedades (secciÃ³n 3.3), y genera una nueva lista no redundante de la siguiente manera: 1. Toma el pasaje superior en la lista actual como el primero en la nueva lista. 2. Agrega el siguiente pasaje x en la lista actual a la nueva lista solo si fAR(x) > t donde fAR(x) = 1 â max piâLnew {cos(x, pi)} y Lnew es el conjunto de pasajes ya seleccionados en la nueva lista. Repetir el paso 2 hasta que se hayan examinado todos los pasajes en la lista actual. DespuÃ©s de aplicar el algoritmo mencionado anteriormente, cada pasaje en la nueva lista es lo suficientemente diferente a los demÃ¡s, favoreciendo asÃ­ la diversidad en lugar de la redundancia en la nueva lista clasificada. El umbral anti-redundancia t se ajusta en un conjunto de entrenamiento. 4. METODOLOGÃA DE EVALUACIÃN El enfoque que propusimos anteriormente para la destilaciÃ³n de informaciÃ³n plantea importantes cuestiones sobre la metodologÃ­a de evaluaciÃ³n. En primer lugar, dado que nuestro marco de trabajo permite que la salida sean fragmentos en diferentes niveles de granularidad (por ejemplo, ventanas de k oraciones donde k puede variar) en lugar de una longitud fija, no es posible tener juicios de relevancia preanotados en todos esos niveles de granularidad. En segundo lugar, dado que deseamos medir la utilidad de la salida del sistema como una combinaciÃ³n de relevancia y novedad, las medidas tradicionales basadas Ãºnicamente en la relevancia deben ser reemplazadas por medidas que penalicen la repeticiÃ³n de la misma informaciÃ³n en la salida del sistema a lo largo del tiempo. En tercer lugar, dado que la salida del sistema son listas clasificadas, debemos recompensar a aquellos sistemas que presenten informaciÃ³n Ãºtil (tanto relevante como no vista previamente) utilizando listas clasificadas mÃ¡s cortas, y penalizar a aquellos que presenten la misma informaciÃ³n utilizando listas clasificadas mÃ¡s largas. Ninguna de las medidas existentes en la recuperaciÃ³n ad-hoc, filtrado adaptativo, detecciÃ³n de novedades u otras Ã¡reas relacionadas (resumen de texto y respuesta a preguntas) tiene propiedades deseables en los tres aspectos. Por lo tanto, debemos desarrollar una <br>nueva metodologÃ­a de evaluaciÃ³n</br>. 4.1 Claves de Respuesta Para permitir la evaluaciÃ³n de un sistema cuya salida consiste en pasajes de longitud arbitraria, tomamos prestado el concepto de claves de respuesta de la comunidad de Preguntas y Respuestas (QA), donde los sistemas pueden devolver fragmentos de texto arbitrarios como respuestas. Las claves de respuestas definen lo que debe estar presente en la respuesta de un sistema para recibir crÃ©dito, y estÃ¡n compuestas por una colecciÃ³n de \"nuggets\" de informaciÃ³n, es decir, unidades de datos sobre las cuales los evaluadores humanos pueden tomar decisiones binarias sobre si una respuesta del sistema las contiene o no. La definiciÃ³n de claves de respuestas y la toma de decisiones binarias asociadas son tareas conceptuales que requieren un mapeo semÃ¡ntico [19], ya que los pasajes devueltos por el sistema pueden contener la misma informaciÃ³n expresada de muchas formas diferentes. Por lo tanto, las evaluaciones de QA han dependido de evaluadores humanos para la correspondencia entre diversas expresiones, lo que hace que el proceso sea costoso, consume mucho tiempo y no sea escalable para grandes colecciones de consultas y documentos, y evaluaciones extensas de sistemas con diversos ajustes de parÃ¡metros. 4.1.1 AutomatizaciÃ³n de la evaluaciÃ³n basada en claves de respuestas Los mÃ©todos de evaluaciÃ³n automÃ¡tica permitirÃ­an una construcciÃ³n y ajuste mÃ¡s rÃ¡pidos del sistema, asÃ­ como proporcionar una forma objetiva y asequible de comparar diversos sistemas. Recientemente, se han propuesto mÃ©todos basados mÃ¡s o menos en la idea de las co-ocurrencias de n-gramos. Pourpre [10] asigna una puntuaciÃ³n de recuerdo fraccional a una respuesta del sistema basada en su superposiciÃ³n de unigramas con una descripciÃ³n de pepitas dada. Por ejemplo, una respuesta del sistema A B C tiene una recuperaciÃ³n de 3/4 con respecto a un fragmento con la descripciÃ³n A B C D. Sin embargo, este enfoque es injusto para los sistemas que presentan la misma informaciÃ³n pero utilizando palabras distintas a A, B, C y D. Otro problema abierto es cÃ³mo ponderar las palabras individuales al medir la cercanÃ­a de una coincidencia. Por ejemplo, considera la pregunta Â¿CuÃ¡ntos prisioneros escaparon? En el fragmento Siete prisioneros escaparon de una prisiÃ³n de Texas, no hay indicaciÃ³n de que siete sea la palabra clave y que debe coincidir para obtener algÃºn crÃ©dito de relevancia. El uso de los valores de IDF no ayuda, ya que la palabra \"seven\" generalmente no tendrÃ¡ un IDF mÃ¡s alto que palabras como \"texas\" y \"prison\". AdemÃ¡s, redefinir el fragmento como simplemente siete no resuelve el problema, ya que ahora podrÃ­a coincidir errÃ³neamente con cualquier menciÃ³n de siete fuera de contexto. Nuggeteer [13] trabaja sobre principios similares pero toma decisiones binarias sobre si hay un fragmento en una respuesta de sistema dada ajustando un umbral. Sin embargo, tambiÃ©n estÃ¡ plagado de relevancia espuria ya que no todas las palabras contenidas en la descripciÃ³n de la pepita (o respuestas correctas conocidas) son centrales para la pepita. 4.1.2 Reglas de Coincidencia de Pepitas Proponemos un mÃ©todo automÃ¡tico confiable para determinar si un fragmento de texto contiene una pepita dada, basado en reglas de coincidencia de pepitas, que se generan utilizando un procedimiento semiautomÃ¡tico explicado a continuaciÃ³n. Estas reglas son bÃ¡sicamente consultas booleanas que solo coincidirÃ¡n con fragmentos que contengan la pepita. Por ejemplo, una regla candidata para emparejar respuestas a Â¿CuÃ¡ntos prisioneros escaparon? es (Texas Y siete Y escapar Y (convictos O prisioneros)), posiblemente con otros sinÃ³nimos y variantes en la regla. Para un corpus de artÃ­culos de noticias, que suelen seguir una prosa formal tÃ­pica, es bastante fÃ¡cil escribir reglas simples para coincidir con respuestas esperadas utilizando un enfoque de arranque, como se describe a continuaciÃ³n. Proponemos un enfoque de dos etapas, inspirado en Autoslog [14], que combina la capacidad de los humanos para identificar expresiones semÃ¡nticamente equivalentes y la capacidad del sistema para recopilar evidencia estadÃ­stica de un corpus de documentos anotados por humanos. En la primera etapa, los sujetos humanos anotaron (utilizando una herramienta de resaltado) porciones de documentos relevantes que contenÃ­an respuestas a cada fragmento 2. En la segunda etapa, los sujetos utilizaron nuestra herramienta de generaciÃ³n de reglas para crear reglas que coincidieran con las anotaciones de cada fragmento. La herramienta permite a los usuarios ingresar una regla booleana como una disyunciÃ³n de conjunciones (por ejemplo, ((a Y b) O (a Y c Y d) O (e))). Dada una regla candidata, nuestra herramienta la utiliza como una consulta booleana sobre el conjunto completo de documentos relevantes y calcula su recall y precisiÃ³n con respecto a las anotaciones que se espera que coincida. Por lo tanto, los sujetos pueden comenzar con una regla simple y refinarla iterativamente hasta que estÃ©n satisfechos con su precisiÃ³n y recuperaciÃ³n. Observamos que era muy fÃ¡cil para los humanos mejorar la precisiÃ³n de una regla ajustando sus conjunciones existentes (agregando mÃ¡s Y) y mejorar la recuperaciÃ³n agregando mÃ¡s conjunciones a la disyunciÃ³n (agregando mÃ¡s O). Como ejemplo, intentemos crear una regla para el acertijo que dice que siete prisioneros escaparon de la prisiÃ³n de Texas. Empezamos con una regla simple - (siete). Cuando introducimos esto en la herramienta de generaciÃ³n de reglas, nos damos cuenta de que esta regla coincide con muchas ocurrencias espurias de siete (por ejemplo, ...siete estados...) y, por lo tanto, obtiene una puntuaciÃ³n baja de precisiÃ³n. Podemos calificar aÃºn mÃ¡s nuestra regla - Texas Y siete Y convictos. A continuaciÃ³n, al revisar las anotaciones omitidas, nos damos cuenta de que algunos artÃ­culos de noticias mencionaban ...siete prisioneros escaparon.... Luego reemplazamos convictos por la disyunciÃ³n (convictos O prisioneros). Continuamos ajustando la regla de esta manera hasta lograr un nivel de recordaciÃ³n y precisiÃ³n suficientemente alto, es decir, que los (pocos) errores y alarmas falsas se puedan ignorar de forma segura. AsÃ­ podemos crear reglas de coincidencia de pepitas que capturen de manera sucinta varias formas de expresar una pepita, evitando al mismo tiempo emparejar respuestas incorrectas (o fuera de contexto). La participaciÃ³n humana en el proceso de creaciÃ³n de reglas garantiza reglas genÃ©ricas de alta calidad que luego se pueden utilizar para evaluar de manera confiable las respuestas del sistema arbitrario. 4.2 EvaluaciÃ³n de la Utilidad de una Secuencia de Listas Clasificadas La utilidad de un sistema de recuperaciÃ³n se puede definir como la diferencia entre cuÃ¡nta informaciÃ³n Ãºtil obtuvo el usuario y cuÃ¡nto tiempo y energÃ­a perdiÃ³. Calculamos esta utilidad a partir de las utilidades de los pasajes individuales de la siguiente manera. DespuÃ©s de leer cada pasaje devuelto por el sistema, el usuario obtiene cierta ganancia dependiendo de la presencia de informaciÃ³n relevante y novedosa, y incurre en una pÃ©rdida en tÃ©rminos del tiempo y la energÃ­a invertidos en revisar el pasaje. Sin embargo, la probabilidad de que el usuario realmente lea un pasaje depende de su posiciÃ³n en la lista clasificada. Por lo tanto, para una consulta q, la utilidad esperada 2 LDC [18] ya proporciona juicios de relevancia para 100 temas en el corpus TDT4. AdemÃ¡s, nos aseguramos de que estos juicios sean exhaustivos en todo el corpus utilizando la agrupaciÃ³n. El valor de un pasaje pi en el rango i se puede definir como U(pi, q) = P(i) â (Ganancia(pi, q) - PÃ©rdida(pi, q)) (4) donde P(i) es la probabilidad de que el usuario pase por un pasaje en el rango i. La utilidad esperada para una lista clasificada completa de longitud n se puede calcular simplemente sumando la utilidad esperada de cada pasaje: U(q) = Î£ i=1 P(i) â (Ganancia(pi, q) â PÃ©rdida(pi, q)) (5) NÃ³tese que si ignoramos el tÃ©rmino de pÃ©rdida y definimos P(i) como P(i) â 1/ logb(b + i â 1) (6) entonces obtenemos la mÃ©trica recientemente popularizada llamada Ganancia Acumulada Descontada (DCG) [9], donde Ganancia(pi, q) se define como la relevancia graduada del pasaje pi. Sin embargo, sin el tÃ©rmino de pÃ©rdida, DCG es una mÃ©trica puramente orientada al recuerdo y no es adecuada para un entorno de filtrado adaptativo, donde la utilidad del sistema depende en parte de su capacidad para limitar el nÃºmero de elementos mostrados al usuario. Aunque P(i) podrÃ­a definirse en base a estudios empÃ­ricos del comportamiento del usuario, por simplicidad, utilizamos P(i) tal como se define exactamente en la ecuaciÃ³n 6. La ganancia G(pi, q) del pasaje pi con respecto a la consulta q es una funciÃ³n de - 1) el nÃºmero de pepitas relevantes presentes en pi, y 2) la novedad de cada una de estas pepitas. Combinamos estos dos factores de la siguiente manera. Para cada pepita Nj, asignamos un peso inicial wj, y tambiÃ©n mantenemos un conteo nj del nÃºmero de veces que esta pepita ha sido vista por el usuario en el pasado. La ganancia derivada de cada ocurrencia posterior del mismo hallazgo se asume que se reduce por un factor de amortiguamiento Î³. Por lo tanto, G(pi, q) se define como G(pi, q) = X Nj âC(pi,q) wj â Î³nj (7) donde C(pi, q) es el conjunto de todas las pepitas que aparecen en el pasaje pi y tambiÃ©n pertenecen a la clave de respuesta de la consulta q. Los pesos iniciales wj se establecen todos en 1.0 en nuestros experimentos, pero tambiÃ©n pueden establecerse segÃºn un enfoque de pirÃ¡mide [11]. La elecciÃ³n del factor de amortiguaciÃ³n Î³ determina la tolerancia del usuario a la redundancia. Cuando Î³ = 0, un pepita solo recibirÃ¡ crÃ©dito por su primera ocurrencia, es decir, cuando nj sea cero. Para 0 < Î³ < 1, un nugget recibe menos crÃ©dito por cada ocurrencia sucesiva. Cuando Î³ = 1, no se produce amortiguamiento y las repeticiones de un valor reciben el mismo crÃ©dito. Se debe tener en cuenta que los recuentos de ocurrencia de las pepitas se conservan entre la evaluaciÃ³n de las listas clasificadas sucesivas devueltas por el sistema, ya que se espera que los usuarios recuerden lo que el sistema les mostrÃ³ en el pasado. Definimos la pÃ©rdida L(pi, q) como un costo constante c (usamos 0.1) incurrido al leer un pasaje devuelto por el sistema. Por lo tanto, nuestra mÃ©trica puede ser reescrita como U(q) = nX i=1 Ganancia(pi, q) logb(b + i â 1) â L(n) (8) donde L(n) es la pÃ©rdida asociada con una lista clasificada de longitud n: L(n) = c Â· nX i=1 1 logb(b + i â 1) (9) 3 NÃ³tese que 00 = 1 Debido a la similitud con la Ganancia Acumulada Descontada (DCG), llamamos a nuestra mÃ©trica Utilidad Acumulada Descontada (DCU). El puntaje de DCU obtenido por el sistema se convierte en un puntaje de DCU normalizado (NDCU) dividiÃ©ndolo por el puntaje de DCU de la lista ideal clasificada, que se crea ordenando los pasajes por sus puntajes de utilidad decrecientes U(pi, q) y deteniÃ©ndose cuando U(pi, q) â¤ 0, es decir, cuando la ganancia es menor o igual al costo de leer el pasaje. El conjunto de datos TDT4 fue el corpus de referencia utilizado en las evaluaciones TDT2002 y TDT2003. El corpus consiste en mÃ¡s de 90,000 artÃ­culos de noticias de mÃºltiples fuentes (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) publicados entre octubre de 2000 y enero de 2001, en los idiomas Ã¡rabe, inglÃ©s y mandarÃ­n. Las versiones de los artÃ­culos en otros idiomas reconocidas por voz y traducidas por mÃ¡quina tambiÃ©n fueron proporcionadas. LDC [18] ha anotado el corpus con 100 temas, que corresponden a varios eventos de noticias en este perÃ­odo de tiempo. De entre estos, seleccionamos un subconjunto de 12 eventos accionables y definimos tareas correspondientes para ellos. Para cada tarea, definimos manualmente un perfil que consiste en un conjunto inicial de (5 a 10) consultas, una descripciÃ³n de texto libre del historial del usuario, es decir, lo que el usuario ya sabe sobre el evento, y una lista de documentos conocidos relevantes y no relevantes (si estÃ¡n disponibles) como ejemplos de entrenamiento. Para cada consulta, generamos claves de respuestas y reglas de coincidencia de pepitas correspondientes utilizando el procedimiento descrito en la secciÃ³n 4.1.2, y produjimos un total de 120 consultas, con un promedio de 7 pepitas por consulta. EXPERIMENTOS Y RESULTADOS 6.1 Baselines Utilizamos Indri [17], un motor de bÃºsqueda basado en modelos de lenguaje popular, como referencia para comparar con CAFÃ. Indri admite la funcionalidad estÃ¡ndar de los motores de bÃºsqueda, incluido el feedback de pseudo-relevancia (PRF) [3, 6], y es representativo de un sistema de recuperaciÃ³n basado en consultas tÃ­pico. Indri no admite ningÃºn tipo de detecciÃ³n de novedades. Comparamos Indri con PRF activado y desactivado, frente a CAFÃ con retroalimentaciÃ³n del usuario, detecciÃ³n de novedades y clasificaciÃ³n antirredundante activados y desactivados. 6.2 ConfiguraciÃ³n Experimental Dividimos el corpus TDT4 que abarca 4 meses en 10 fragmentos, cada uno definido como un perÃ­odo de 12 dÃ­as consecutivos. En cualquier momento dado del proceso de destilaciÃ³n, cada sistema accedÃ­a a los datos pasados hasta el punto actual, y devolvÃ­a una lista clasificada de hasta 50 pasajes por consulta. Las 12 tareas definidas en el corpus se dividieron en un conjunto de entrenamiento y prueba con 6 tareas cada uno. Cada sistema pudo utilizar el conjunto de entrenamiento para ajustar sus parÃ¡metros con el fin de optimizar NDCU (ecuaciÃ³n 8), incluyendo el umbral de relevancia tanto para Indri como para CAFÂ´E, y los umbrales de novedad y antirredundancia para CAFÂ´E. La NDCU para cada ejecuciÃ³n del sistema se calcula automÃ¡ticamente. TambiÃ©n se simulÃ³ la retroalimentaciÃ³n de los usuarios: los juicios de relevancia para cada pasaje devuelto por el sistema (segÃºn las reglas de coincidencia de pepitas descritas en la secciÃ³n 4.1.2) fueron utilizados como retroalimentaciÃ³n de los usuarios en la adaptaciÃ³n de perfiles de consulta e historiales de usuario. En la Tabla 1, mostramos los puntajes NDCU de los dos sistemas en diferentes configuraciones. Estas puntuaciones se promedian en las seis tareas del conjunto de pruebas, y se calculan con dos factores de amortiguaciÃ³n (ver secciÃ³n 4.2): Î³ = 0 y 0.1, para simular ninguna tolerancia y poca tolerancia para la redundancia, respectivamente. Usar Î³ = 0 crea una mÃ©trica mucho mÃ¡s estricta ya que no otorga ningÃºn crÃ©dito a un pasaje que contenga informaciÃ³n relevante pero redundante. Por lo tanto, la mejora obtenida al habilitar la retroalimentaciÃ³n del usuario es menor con Î³ = 0 que la mejora obtenida de la retroalimentaciÃ³n con Î³ = 0.1. Esto revela una deficiencia de los sistemas de recuperaciÃ³n contemporÃ¡neos: cuando el usuario da retroalimentaciÃ³n positiva sobre un pasaje, los sistemas otorgan mayor peso a los tÃ©rminos presentes en ese pasaje y tienden a recuperar otros pasajes que contienen los mismos tÃ©rminos, y por lo tanto, generalmente la misma informaciÃ³n. Sin embargo, el usuario no se beneficia al ver pasajes redundantes y generalmente estÃ¡ interesado en otros pasajes que contengan informaciÃ³n relacionada. Es informativo evaluar los sistemas de recuperaciÃ³n utilizando nuestra medida de utilidad (con Î³ = 0) que tiene en cuenta la novedad y, por lo tanto, ofrece una imagen mÃ¡s realista de cuÃ¡n bien un sistema puede generalizar a partir de la retroalimentaciÃ³n del usuario, en lugar de utilizar medidas tradicionales de RI como la recuperaciÃ³n y la precisiÃ³n, que ofrecen una imagen incompleta de la mejora obtenida a partir de la retroalimentaciÃ³n del usuario. A veces, sin embargo, los usuarios podrÃ­an estar interesados en ver la misma informaciÃ³n de mÃºltiples fuentes, como un indicador de su importancia o fiabilidad. En tal caso, simplemente podemos elegir un valor mÃ¡s alto para Î³ que corresponda a una mayor tolerancia para la redundancia, y asÃ­ permitir que el sistema ajuste sus parÃ¡metros en consecuencia. Dado que los documentos fueron procesados por fragmentos, serÃ­a interesante ver cÃ³mo mejora el rendimiento de los sistemas con el tiempo. Las figuras 1 y 2 muestran las tendencias de rendimiento de ambos sistemas a lo largo de los fragmentos. Si bien se espera que el rendimiento con y sin retroalimentaciÃ³n en los primeros fragmentos sea similar, para los fragmentos posteriores, la curva de rendimiento con la retroalimentaciÃ³n habilitada supera a la del ajuste sin retroalimentaciÃ³n. Las tendencias de rendimiento no son consistentes en todos los fragmentos porque los documentos relevantes no estÃ¡n distribuidos uniformemente en todos los fragmentos, lo que hace que algunas consultas sean mÃ¡s fÃ¡ciles que otras en ciertos fragmentos. AdemÃ¡s, dado que Indri utiliza retroalimentaciÃ³n de pseudo-relevancia mientras que CAFÂ´E utiliza retroalimentaciÃ³n basada en juicios de relevancia reales, la mejora en el caso de Indri es menos dramÃ¡tica que la de CAFÂ´E. 7. CONCLUSIONES FINALES Este artÃ­culo presenta la primera investigaciÃ³n sobre la destilaciÃ³n de informaciÃ³n basada en la utilidad con un sistema que aprende las necesidades de informaciÃ³n duraderas a partir de la retroalimentaciÃ³n detallada del usuario sobre una secuencia de pasajes clasificados. Nuestro sistema, llamado CAFÂ´E, combina filtrado adaptativo, detecciÃ³n de novedades y clasificaciÃ³n de pasajes antirredundantes en un marco unificado para la optimizaciÃ³n de la utilidad. Desarrollamos un nuevo esquema para la evaluaciÃ³n y retroalimentaciÃ³n automatizadas basado en un procedimiento semiautomÃ¡tico para adquirir reglas que permitan emparejar automÃ¡ticamente fragmentos con las respuestas del sistema. TambiÃ©n propusimos una extensiÃ³n de la mÃ©trica NDCG para evaluar la utilidad de los pasajes clasificados como una combinaciÃ³n ponderada de relevancia y novedad. Nuestros experimentos en el reciÃ©n anotado corpus de referencia TDT4 muestran un alentador aumento de utilidad sobre Indri, y tambiÃ©n sobre nuestro propio sistema con aprendizaje incremental y detecciÃ³n de novedades desactivados. 8. AGRADECIMIENTOS Nos gustarÃ­a agradecer a Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng y al Programa de AnÃ¡lisis de Datos Cualitativos de la Universidad de Pittsburgh liderado por el Dr. Stuart Shulman por su ayuda en la recolecciÃ³n y procesamiento de las anotaciones extendidas de TDT4 utilizadas en nuestros experimentos. Este trabajo estÃ¡ parcialmente respaldado por la FundaciÃ³n Nacional de Ciencias (NSF) bajo la subvenciÃ³n IIS0434035, y la Agencia de Proyectos de InvestigaciÃ³n Avanzada de Defensa (DARPA) bajo los contratos NBCHD030010 y W0550432. Cualquier opiniÃ³n, hallazgo, conclusiÃ³n o recomendaciÃ³n expresada en este material son responsabilidad de los autores y no reflejan necesariamente los puntos de vista de los patrocinadores. AUTORES ADICIONALES Jian Zhang (jianzhan@stat.purdue.edu)â , Jaime Carbonell (jgc@cs.cmu.edu)â  , Peter Brusilovsky (peterb+@pitt.edu)â¡ , Daqing He(dah44@pitt.edu)â¡ 10. REFERENCIAS [1] J. Allan. RetroalimentaciÃ³n incremental de relevancia para la filtraciÃ³n de informaciÃ³n. Actas de la 19Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 270-278, 1996. [2] J. Allan, C. Wade y A. Bolivar. RecuperaciÃ³n y DetecciÃ³n de Novedad a Nivel de OraciÃ³n. Actas de la conferencia ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, 2003. [3] C. Buckley, G. Salton y J. Allan. RecuperaciÃ³n automÃ¡tica con informaciÃ³n de localidad utilizando SMART. PublicaciÃ³n especial del NIST, (500207):59-72, 1993. [4] J. Callan. Aprendiendo mientras se filtran documentos. Actas de la 21Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 224-231, 1998. [5] J. Carbonell y J. Goldstein. El uso de MMR, Reordenamiento basado en Diversidad para Reorganizar Documentos y Producir ResÃºmenes. Actas de la 21Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 335-336, 1998. [6] E. Efthimiadis. ExpansiÃ³n de consulta. RevisiÃ³n Anual de Ciencia de la InformaciÃ³n y TecnologÃ­a (ARIST), 31:p121-87, 1996. [7] J. Fiscus y G. Duddington. Resumen de DetecciÃ³n y Seguimiento de Temas. DetecciÃ³n y seguimiento de temas: OrganizaciÃ³n de la informaciÃ³n basada en eventos, pÃ¡ginas 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov y S. Roukos. Un modelo estadÃ­stico para la detecciÃ³n y seguimiento de entidades multilingÃ¼es. NAACL/HLT, 2004. [9] K. JÃ¤rvelin y J. KekÃ¤lÃ¤inen. EvaluaciÃ³n basada en la Ganancia Acumulada de TÃ©cnicas de RecuperaciÃ³n de InformaciÃ³n. ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin y D. Demner-Fushman. EvaluaciÃ³n automÃ¡tica de respuestas a preguntas de definiciÃ³n. Actas de la Conferencia de TecnologÃ­a del Lenguaje Humano y Conferencia sobre MÃ©todos EmpÃ­ricos en Procesamiento del Lenguaje Natural (HLT/EMNLP 2005), 2005. â Departamento de EstadÃ­stica, Universidad de Purdue, West Lafayette, EE. UU. â  Instituto de TecnologÃ­as del Lenguaje, Universidad Carnegie Mellon, Pittsburgh, EE. UU. â¡ Escuela de Ciencias de la InformaciÃ³n, Universidad de Pittsburgh, Pittsburgh, EE. UU. [11] J. Lin y D. Demner-Fushman. Â¿Se derrumbarÃ¡n las pirÃ¡mides construidas con pepitas? Actas de HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla y S. Roukos. Un algoritmo de resoluciÃ³n de correferencia sÃ­ncrona basado en el Ã¡rbol de Bell. Actas de ACL, 4:136-143, 2004. [13] G. Marton. Nuggeteer: EvaluaciÃ³n AutomÃ¡tica Basada en Pepitas Utilizando Descripciones y Juicios. HLT/NAACL, 2006. [14] E. Riloff. ConstrucciÃ³n automÃ¡tica de un diccionario para tareas de extracciÃ³n de informaciÃ³n. Actas de la UndÃ©cima Conferencia Nacional sobre Inteligencia Artificial, pÃ¡ginas 811-816, 1993. [15] S. Robertson y S. Walker. Microsoft Cambridge en TREC-9: Pista de filtrado. La Novena Conferencia de RecuperaciÃ³n de Texto (TREC-9), pÃ¡ginas 361-368. [16] R. Schapire, Y. Cantante, y A. Singhal. Boosting y Rocchio aplicados a la filtraciÃ³n de texto. Actas de la 21Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle y W. Croft. Indri: Un motor de bÃºsqueda basado en modelos de lenguaje para consultas complejas. Actas de la Conferencia Internacional sobre AnÃ¡lisis de Inteligencia, 2004. [18] El Consorcio de Datos LingÃ¼Ã­sticos. http://www.ldc.upenn.edu/. [19] E. Voorhees. Resumen de la pista de preguntas y respuestas TREC 2003. Actas de la DuodÃ©cima Conferencia de RecuperaciÃ³n de Texto (TREC 2003), 2003. [20] Y. Yang y B. Kisiel. RegresiÃ³n local basada en mÃ¡rgenes para filtrado adaptativo. Actas de la duodÃ©cima conferencia internacional sobre gestiÃ³n de la informaciÃ³n y el conocimiento, pÃ¡ginas 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang y B. Kisiel. Robustez de los MÃ©todos de Filtrado Adaptativo en una EvaluaciÃ³n de Referencia Cruzada. Actas de la 28Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 98-105, 2005. [22] C. Zhai, W. Cohen y J. Lafferty. MÃ¡s allÃ¡ de la relevancia independiente: MÃ©todos y mÃ©tricas de evaluaciÃ³n para la recuperaciÃ³n de subtemas. Actas de la 26Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 10-17, 2003. [23] J. Zhang y Y. Yang. Robustez de los MÃ©todos de ClasificaciÃ³n Lineal Regularizados en la CategorizaciÃ³n de Textos. Actas de la 26Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 190-197, 2003. [24] Y. Zhang. Utilizando priors bayesianos para combinar clasificadores para filtrado adaptativo. Actas de la 27Âª conferencia internacional anual sobre InvestigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 345-352, 2004. [25] Y. Zhang, J. Callan y T. Minka. DetecciÃ³n de novedad y redundancia en el filtrado adaptativo. Actas de la 25Âª Conferencia Internacional Anual de ACM SIGIR sobre InvestigaciÃ³n y Desarrollo en RecuperaciÃ³n de InformaciÃ³n, 2002. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "answer key": {
            "translated_key": "clave de respuesta",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Utility-based Information Distillation Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu ABSTRACT This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework.",
                "It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant passage ranking with respect to long-lasting information needs (tasks with multiple queries).",
                "Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings.",
                "For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task.",
                "Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty.",
                "Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval.",
                "Adaptive filtering (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics.",
                "Based on the initial query and a few positive examples (if available), an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user.",
                "The incremental learning nature of AF systems makes them more powerful than standard search engines that support ad-hoc retrieval (e.g.",
                "Google and Yahoo) in terms of finding relevant information with respect to long-lasting topics of interest, and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs, without having to modify their queries manually.",
                "A variety of supervised learning algorithms (Rocchio-style classifiers, Exponential-Gaussian models, local regression and logistic regression approaches) have been studied for adaptive settings, examined with explicit and implicit relevance feedback, and evaluated with respect to utility optimization on large benchmark data collections in TREC (Text Retrieval Conferences) and TDT (Topic Detection and Tracking) forums [1, 4, 7, 15, 16, 20, 24, 23].",
                "Regularized logistic regression [21] has been found representative for the state-of-the-art approaches, and highly efficient for frequent model adaptations over large document collections such as the TREC-10 corpus (over 800,000 documents and 84 topics).",
                "Despite substantial achievements in recent adaptive filtering research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently.",
                "Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications: 1.",
                "User has a rather passive role in the conventional adaptive filtering setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision.",
                "A more active alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents (or passages) per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists.",
                "The latter form of user interaction has been highly effective in standard retrieval for ad-hoc queries.",
                "How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2.",
                "The unit for receiving a relevance judgment (yes or no) is restricted to the document level in conventional AF.",
                "However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant.",
                "Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system.",
                "For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3.",
                "System-selected documents are often highly redundant.",
                "A major news event, for example, would be reported by multiple sources repeatedly for a while, making most of the information content in those articles redundant with each other.",
                "A conventional AF system would select all these redundant news stories for user feedback, wasting the users time while offering little gain.",
                "Clearly, techniques for novelty detection can help in principle [25, 2, 22] for improving the utility of the AF systems.",
                "However, the effectiveness of such techniques at passage level to detect novelty with respect to users (fine-grained) feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user.",
                "To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and novelty detection for removal of redundancy from user interactions with the system.",
                "We call the new process utility-based information distillation.",
                "Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach.",
                "Therefore, we extended a benchmark corpus - the TDT4 collection of news stories and TV broadcasts - with task definitions, multiple queries per task, and answer keys per query.",
                "We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 .",
                "To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses.",
                "Moreover, we propose an extension of NDCG (Normalized Discounted Cumulated Gain) [9] for assessing the utility of ranked passages as a function of both relevance and novelty.",
                "The rest of this paper is organized as follows.",
                "Section 2 outlines the information distillation process with a concrete example.",
                "Section 3 describes the technical cores of our system called CAFÂ´E - CMU Adaptive Filtering Engine.",
                "Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme.",
                "Section 5 describes the extended TDT4 corpus.",
                "Section 6 presents our experiments and results.",
                "Section 7 concludes the study and gives future perspectives. 2.",
                "A SAMPLE TASK Consider a news event - the escape of seven convicts from a Texas prison in December 2000 and their capture a month later.",
                "Assuming a user were interested in this event since its early stage, the information need could be: Find information about the escape of convicts from Texas prison, and information related to their recapture.",
                "The associated lower-level questions could be: 1.",
                "How many prisoners escaped? 2.",
                "Where and when were they sighted? 3. Who are their known contacts inside and outside the prison? 4.",
                "How are they armed? 5.",
                "Do they have any vehicles? 6.",
                "What steps have been taken so far?",
                "We call such an information need a task, and the associated questions as the queries in this task.",
                "A distillation system is supposed to monitor the incoming documents, process them chunk by chunk in a temporal order, select potentially relevant and novel passages from each chunk with respect to each query, and present a ranked list of passages to the user.",
                "Passage ranking here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list.",
                "When presented with a list of passages, the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant.",
                "These spans of text are taken as positive examples in the adaptation of the query profile, and also added to the users history.",
                "Passages not marked by the user are taken as negative examples.",
                "As soon as the query profile is updated, the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low, based on user preference.",
                "For example, if the user highlights ...officials have posted a $100,000 reward for their capture... as relevant answer to the query What steps have been taken so far?, then the highlighted piece is used as an additional positive training example in the adaptation of the query profile.",
                "This piece of feedback is also added to the user history as a seen example, so that in future, the system will not place another passage mentioning $100,000 reward at the top of the ranked list.",
                "However, an article mentioning ...officials have doubled the reward money to $200,000... might be ranked high since it is both relevant to the (updated) query profile and novel with respect to the (updated) user history.",
                "The user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly.",
                "Clearly, novelty detection is very important for the utility of such a system because of the iterative search.",
                "Without novelty detection, the old relevant passages would be shown to the user repeatedly in each ranked list.",
                "Through the above example, we can see the main properties of our new framework for utility-based information distillation over temporally ordered documents.",
                "Our framework combines and extends the power of adaptive filtering (AF), ad-hoc retrieval (IR) and novelty detection (ND).",
                "Compared to standard IR, our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task.",
                "Compared to conventional AF, it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system.",
                "Compared to past work, this is the first evaluation of novelty detection integrated with adaptive filtering for sequenced queries that allows flexible user feedback over ranked passages.",
                "The combination of AF, IR and ND with the new extensions raises an important research question regarding evaluation methodology: how can we measure the utility of such an information distillation system?",
                "Existing metrics in standard IR, AF and ND are insufficient, and new solutions must be explored, as we will discuss in Section 4, after describing the technical cores of our system in the next section. 3.",
                "TECHNICAL CORES The core components of CAFÂ´E are - 1) AF for incremental learning of query profiles, 2) IR for estimating relevance of passages with respect to query profiles, 3) ND for assessing novelty of passages with respect to users history, and 4) anti-redundancy component to remove redundancy from ranked lists. 3.1 Adaptive Filtering Component We use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for AF [21].",
                "Logistic regression (LR) is a supervised learning algorithm for statistical classification.",
                "Based on a training set of labeled instances, it learns a class model which can then by used to predict the labels of unseen instances.",
                "Its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required, as is the case in adaptive filtering, where the system must learn from each new feedback provided by the user. (See [21] and [23] for computational complexity and implementation issues).",
                "In adaptive filtering, each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query.",
                "For training the model, we use the query itself as the initial positive training example of the class, and the user-highlighted pieces of text (marked as Relevant or Not-relevant) during feedback as additional training examples.",
                "To address the cold start issue in the early stage before any user feedback is obtained, the system uses a small sample from a retrospective corpus as the initial negative examples in the training set.",
                "The details of using logistic regression for adaptive filtering (assigning different weights to positive and negative training instances, and regularizing the objective function to prevent over-fitting on training data) are presented in [21].",
                "The class model wâ learned by Logistic Regression, or the query profile, is a vector whose dimensions are individual terms and whose elements are the regression coefficients, indicating how influential each term is in the query profile.",
                "The query profile is updated whenever a new piece of user feedback is received.",
                "A temporally decaying weight can be applied to each training example, as an option, to emphasize the most recent user feedback. 3.2 Passage Retrieval Component We use standard IR techniques in this part of our system.",
                "Incoming documents are processed in chunks, where each chunk can be defined as a fixed span of time or as a fixed number of documents, as preferred by the user.",
                "For each incoming document, corpus statistics like the IDF (Inverted Document Frequency) of each term are updated.",
                "We use a state-of-the-art named entity identifier and tracker [8, 12] to identify person and location names, and merge them with co-referent named entities seen in the past.",
                "Then the documents are segmented into passages, which can be a whole document, a paragraph, a sentence, or any other continuous span of text, as preferred.",
                "Each passage is represented using a vector of TF-IDF (Term FrequencyInverse Document Frequency) weights, where term can be a word or a named entity.",
                "Given a query profile, i.e. the logistic regression solution wâ as described in Section 3.1, the system computes the posterior probability of relevance for each passage x as fRL(x) â¡ P(y = 1|x, wâ ) = 1 (1 + eâwâÂ·x) (1) Passages are ordered by their relevance scores, and the ones with scores above a threshold (tuned on a training set) comprise the relevance list that is passed on to the novelty detection step. 3.3 Novelty Detection Component CAFÂ´E maintains a user history H(t), which contains all the spans of text hi that the user highlighted (as feedback) during his or her past interactions with the system, up to the current time t. Denoting the history as H(t) = n h1, h2, ..., ht o , (2) the novelty score of a new candidate passage x is computed as: fND(x) = 1 â max iâ1..t {cos(x, hi)} (3) where both candidate passage x and highlighted spans of text hi are represented as TF-IDF vectors.",
                "The novelty score of each passage is compared to a prespecified threshold (also tuned on a training set), and any passage with a score below this threshold is removed from the relevance list. 3.4 Anti-redundant Ranking Component Although the novelty detection component ensures that only novel (previously unseen) information remains in the relevance list, this list might still contain the same novel information at multiple positions in the ranked list.",
                "Suppose, for example, that the user has already read about a $100,000 reward for information about the escaped convicts.",
                "A new piece of news that the award has been increased to $200,000 is novel since the user hasnt read about it yet.",
                "However, multiple news sources would report this news and we might end up showing (redundant) articles from all these sources in a ranked list.",
                "Hence, a ranked list should also be made non-redundant with respect to its own contents.",
                "We use a simplified version of the Maximal Marginal Relevance method [5], originally developed for combining relevance and novelty in text retrieval and summarization.",
                "Our procedure starts with the current list of passages sorted by relevance (section 3.2), filtered by Novelty Detection component (section 3.3), and generates a new non-redundant list as follows: 1.",
                "Take the top passage in the current list as the top one in the new list. 2.",
                "Add the next passage x in the current list to the new list only if fAR(x) > t where fAR(x) = 1 â max piâLnew {cos(x, pi)} and Lnew is the set of passages already selected in the new list. 3.",
                "Repeat step 2 until all the passages in the current list have been examined.",
                "After applying the above-mentioned algorithm, each passage in the new list is sufficiently dissimilar to others, thus favoring diversity rather than redundancy in the new ranked list.",
                "The anti-redundancy threshold t is tuned on a training set. 4.",
                "EVALUATION METHODOLOGY The approach we proposed above for information distillation raises important issues regarding evaluation methodology.",
                "Firstly, since our framework allows the output to be passages at different levels of granularity (e.g. k-sentence windows where k may vary) instead of a fixed length, it is not possible to have pre-annotated relevance judgments at all such granularity levels.",
                "Secondly, since we wish to measure the utility of the system output as a combination of both relevance and novelty, traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time.",
                "Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists.",
                "None of the existing measures in ad-hoc retrieval, adaptive filtering, novelty detection or other related areas (text summarization and question answering) have desirable properties in all the three aspects.",
                "Therefore, we must develop a new evaluation methodology. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers.",
                "Answer keys define what should be present in a system response to receive credit, and are comprised of a collection of information nuggets, i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them.",
                "Defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping [19], since system-returned passages can contain the same information expressed in many different ways.",
                "Hence, QA evaluations have relied on human assessors for the mapping between various expressions, making the process costly, time consuming, and not scalable to large query and document collections, and extensive system evaluations with various parameter settings. 4.1.1 Automating Evaluation based on Answer Keys Automatic evaluation methods would allow for faster system building and tuning, as well as provide an objective and affordable way of comparing various systems.",
                "Recently, such methods have been proposed, more or less, based on the idea of n-gram co-occurrences.",
                "Pourpre [10] assigns a fractional recall score to a system response based on its unigram overlap with a given nuggets description.",
                "For example, a system response A B C has recall 3/4 with respect to a nugget with description A B C D. However, such an approach is unfair to systems that present the same information but using words other than A, B, C, and D. Another open issue is how to weight individual words in measuring the closeness of a match.",
                "For example, consider the question How many prisoners escaped?.",
                "In the nugget Seven prisoners escaped from a Texas prison, there is no indication that seven is the keyword, and that it must be matched to get any relevance credit.",
                "Using IDF values does not help, since seven will generally not have a higher IDF than words like texas and prison.",
                "Also, redefining the nugget as just seven does not solve the problem since now it might spuriously match any mention of seven out of context.",
                "Nuggeteer [13] works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold.",
                "However, it is also plagued by spurious relevance since not all words contained in the nugget description (or known correct responses) are central to the nugget. 4.1.2 Nugget-Matching Rules We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on nugget-matching rules, which are generated using a semiautomatic procedure explained below.",
                "These rules are essentially Boolean queries that will only match against snippets that contain the nugget.",
                "For instance, a candidate rule for matching answers to How many prisoners escaped? is (Texas AND seven AND escape AND (convicts OR prisoners)), possibly with other synonyms and variants in the rule.",
                "For a corpus of news articles, which usually follow a typical formal prose, it is fairly easy to write such simple rules to match expected answers using a bootstrap approach, as described below.",
                "We propose a two-stage approach, inspired by Autoslog [14], that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents.",
                "In the first stage, human subjects annotated (using a highlighting tool) portions of ontopic documents that contained answers to each nugget 2 .",
                "In the second stage, subjects used our rule generation tool to create rules that would match the annotations for each nugget.",
                "The tool allows users to enter a Boolean rule as a disjunction of conjunctions (e.g. ((a AND b) OR (a AND c AND d) OR (e))).",
                "Given a candidate rule, our tool uses it as a Boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match.",
                "Hence, the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision.",
                "We observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions (adding more ANDs), and improving the recall by adding more conjunctions to the disjunction (adding more ORs).",
                "As an example, lets try to create a rule for the nugget which says that seven prisoners escaped from the Texas prison.",
                "We start with a simple rule - (seven).",
                "When we input this into the rule generation tool, we realize that this rule matches many spurious occurrences of seven (e.g. ...seven states...) and thus gets a low precision score.",
                "We can further qualify our rule - Texas AND seven AND convicts.",
                "Next, by looking at the missed annotations, we realize that some news articles mentioned ...seven prisoners escaped.... We then replace convicts with the disjunction (convicts OR prisoners).",
                "We continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the (small number of) misses and false alarms can be safely ignored.",
                "Thus we can create nugget-matching rules that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses.",
                "Human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably. 4.2 Evaluating the Utility of a Sequence of Ranked Lists The utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information, and how much the user lost in terms of time and energy.",
                "We calculate this utility from the utilities of individual passages as follows.",
                "After reading each passage returned by the system, the user derives some gain depending on the presence of relevant and novel information, and incurs a loss in terms of the time and energy spent in going through the passage.",
                "However, the likelihood that the user would actually read a passage depends on its position in the ranked list.",
                "Hence, for a query q, the expected utility 2 LDC [18] already provides relevance judgments for 100 topics on the TDT4 corpus.",
                "We further ensured that these judgments are exhaustive on the entire corpus using pooling. of a passage pi at rank i can be defined as U(pi, q) = P(i) â (Gain(pi, q) â Loss(pi, q)) (4) where P(i) is the probability that the user would go through a passage at rank i.",
                "The expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage: U(q) = nX i=1 P(i) â (Gain(pi, q) â Loss(pi, q)) (5) Note that if we ignore the loss term and define P(i) as P(i) â 1/ logb(b + i â 1) (6) then we get the recently popularized metric called Discounted Cumulated Gain (DCG) [9], where Gain(pi, q) is defined as the graded relevance of passage pi.",
                "However, without the loss term, DCG is a purely recall-oriented metric and not suitable for an adaptive filtering setting, where the systems utility depends in part on its ability to limit the number of items shown to the user.",
                "Although P(i) could be defined based on empirical studies of user behavior, for simplicity, we use P(i) exactly as defined in equation 6.",
                "The gain G(pi, q) of passage pi with respect to the query q is a function of - 1) the number of relevant nuggets present in pi, and 2) the novelty of each of these nuggets.",
                "We combine these two factors as follows.",
                "For each nugget Nj, we assign an initial weight wj, and also keep a count nj of the number of times this nugget has been seen by the user in the past.",
                "The gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor Î³.",
                "Thus, G(pi, q) is defined as G(pi, q) = X Nj âC(pi,q) wj â Î³nj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the <br>answer key</br> of query q.",
                "The initial weights wj are all set of be 1.0 in our experiments, but can also be set based on a pyramid approach [11].",
                "The choice of dampening factor Î³ determines the users tolerance for redundancy.",
                "When Î³ = 0, a nugget will only receive credit for its first occurrence i.e. when nj is zero3 .",
                "For 0 < Î³ < 1, a nugget receives smaller credit for each successive occurrence.",
                "When Î³ = 1, no dampening occurs and repeated occurrences of a nugget receive the same credit.",
                "Note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system, since the users are expected to remember what the system showed them in the past.",
                "We define the loss L(pi, q) as a constant cost c (we use 0.1) incurred when reading a system-returned passage.",
                "Thus, our metric can be re-written as U(q) = nX i=1 Gain(pi, q) logb(b + i â 1) â L(n) (8) where L(n) is the loss associated with a ranked list of length n: L(n) = c Â· nX i=1 1 logb(b + i â 1) (9) 3 Note that 00 = 1 Due to the similarity with Discounted Cumulated Gain (DCG), we call our metric Discounted Cumulated Utility (DCU).",
                "The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) â¤ 0 i.e. when the gain is less than or equal to the cost of reading the passage. 5.",
                "DATA TDT4 was the benchmark corpus used in TDT2002 and TDT2003 evaluations.",
                "The corpus consists of over 90, 000 news articles from multiple sources (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) published between October 2000 and January 2001, in the languages of Arabic, English, and Mandarin.",
                "Speech-recognized and machine-translated versions of the non-English articles were provided as well.",
                "LDC [18] has annotated the corpus with 100 topics, that correspond to various news events in this time period.",
                "Out of these, we selected a subset of 12 actionable events, and defined corresponding tasks for them4 .",
                "For each task, we manually defined a profile consisting of an initial set of (5 to 10) queries, a free-text description of the user history, i.e., what the user already knows about the event, and a list of known on-topic and off-topic documents (if available) as training examples.",
                "For each query, we generated answer keys and corresponding nugget matching rules using the procedure described in section 4.1.2, and produced a total of 120 queries, with an average of 7 nuggets per query. 6.",
                "EXPERIMENTS AND RESULTS 6.1 Baselines We used Indri [17], a popular language-model based retrieval engine, as a baseline for comparison with CAFÂ´E.",
                "Indri supports standard search engine functionality, including pseudo-relevance feedback (PRF) [3, 6], and is representative of a typical query-based retrieval system.",
                "Indri does not support any kind of novelty detection.",
                "We compare Indri with PRF turned on and off, against CAFÂ´E with user feedback, novelty detection and antiredundant ranking turned on and off. 6.2 Experimental Setup We divided the TDT4 corpus spanning 4 months into 10 chunks, each defined as a period of 12 consecutive days.",
                "At any given point of time in the distillation process, each system accessed the past data up to the current point, and returned a ranked list of up 50 passages per query.",
                "The 12 tasks defined on the corpus were divided into a training and test set with 6 tasks each.",
                "Each system was allowed to use the training set to tune its parameters for optimizing NDCU (equation 8), including the relevance threshold for both Indri and CAFÂ´E, and the novelty and antiredundancy thresholds for CAFÂ´E.",
                "The NDCU for each system run is calculated automatically.",
                "User feedback was also simulated - relevance judgments for each system-returned passage (as determined by the nugget matching rules described in section 4.1.2) were 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figure 1: Performance of Indri across chunks Figure 2: Performance of CAFÂ´E across chunks used as user feedback in the adaptation of query profiles and user histories. 6.3 Results In Table 1, we show the NDCU scores of the two systems under various settings.",
                "These scores are averaged over the six tasks in the test set, and are calculated with two dampening factors (see section 4.2): Î³ = 0 and 0.1, to simulate no tolerance and small tolerance for redundancy, respectively.",
                "Using Î³ = 0 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information.",
                "Hence, the improvement obtained from enabling user feedback is smaller with Î³ = 0 than the improvement obtained from feedback with Î³ = 0.1.",
                "This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information.",
                "However, the user does not benefit from seeing such redundant passages, and is usually interested in other passages containing related information.",
                "It is informative to evaluate retrieval systems using our utility measure (with Î³ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.",
                "Sometimes, however, users might indeed be interested in seeing the same information from multiple sources, as an Table 1: NDCU Scores of Indri and CAFÂ´E for two dampening factors (Î³), and various settings (F: Feedback, N: Novelty Detection, A: Anti-Redundant Ranking) Indri CAFÂ´E Î³ Base +PRF Base +F +F+N +F+A +F+N+A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.35 0.36 0.36 indicator of its importance or reliability.",
                "In such a case, we can simply choose a higher value for Î³ which corresponds to a higher tolerance for redundancy, and hence let the system tune its parameters accordingly.",
                "Since documents were processed chunk by chunk, it would be interesting to see how the performance of systems improves over time.",
                "Figures 1 and 2 show the performance trends for both the systems across chunks.",
                "While the performance with and without feedback on the first few chunks is expected to be close, for subsequent chunks, the performance curve with feedback enabled rises above the one with the no-feedback setting.",
                "The performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks, making some queries easier than others in certain chunks.",
                "Moreover, since Indri uses pseudo-relevance feedback while CAFÂ´E uses feedback based on actual relevance judgments, the improvement in case of Indri is less dramatic than that of CAFÂ´E. 7.",
                "CONCLUDING REMARKS This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages.",
                "Our system, called CAFÂ´E, combines adaptive filtering, novelty detection and antiredundant passage ranking in a unified framework for utility optimization.",
                "We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty.",
                "Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off. 8.",
                "ACKNOWLEDGMENTS We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments.",
                "This work is supported in parts by the National Science Foundation (NSF) under grant IIS0434035, and the Defense Advanced Research Project Agency (DARPA) under contracts NBCHD030010 and W0550432.",
                "Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. 9.",
                "ADDITIONAL AUTHORS Jian Zhang (jianzhan@stat.purdue.edu)â , Jaime Carbonell (jgc@cs.cmu.edu)â  , Peter Brusilovsky (peterb+@pitt.edu)â¡ , Daqing He(dah44@pitt.edu)â¡ 10.",
                "REFERENCES [1] J. Allan.",
                "Incremental Relevance Feedback for Information Filtering.",
                "Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 270-278, 1996. [2] J. Allan, C. Wade, and A. Bolivar.",
                "Retrieval and Novelty Detection at the Sentence Level.",
                "Proceedings of the ACM SIGIR conference on research and development in information retrieval, 2003. [3] C. Buckley, G. Salton, and J. Allan.",
                "Automatic Retrieval with Locality Information using SMART.",
                "NIST special publication, (500207):59-72, 1993. [4] J. Callan.",
                "Learning While Filtering Documents.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 224-231, 1998. [5] J. Carbonell and J. Goldstein.",
                "The use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335-336, 1998. [6] E. Efthimiadis.",
                "Query Expansion.",
                "Annual Review of Information Science and Technology (ARIST), 31:p121-87, 1996. [7] J. Fiscus and G. Duddington.",
                "Topic Detection and Tracking Overview.",
                "Topic Detection and Tracking: Event-based Information Organization, pages 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.",
                "A Statistical Model for Multilingual Entity Detection and Tracking.",
                "NAACL/HLT, 2004. [9] K. JÂ¨arvelin and J. KekÂ¨alÂ¨ainen.",
                "Cumulated Gain-based Evaluation of IR Techniques.",
                "ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin and D. Demner-Fushman.",
                "Automatically Evaluating Answers to Definition Questions.",
                "Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), 2005. â Statistics Dept., Purdue University, West Lafayette, USA â  Language Technologies Inst., Carnegie Mellon University, Pittsburgh, USA â¡ School of Information Sciences, Univ. of Pittsburgh, Pittsburgh, USA [11] J. Lin and D. Demner-Fushman.",
                "Will Pyramids Built of nUggets Topple Over.",
                "Proceedings of HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.",
                "A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree.",
                "Proc. of ACL, 4:136-143, 2004. [13] G. Marton.",
                "Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments.",
                "HLT/NAACL, 2006. [14] E. Riloff.",
                "Automatically Constructing a Dictionary for Information Extraction Tasks.",
                "Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, 1993. [15] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9: Filtering track.",
                "The Ninth Text REtrieval Conference (TREC-9), pages 361-368. [16] R. Schapire, Y.",
                "Singer, and A. Singhal.",
                "Boosting and Rocchio Applied to Text Filtering.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle, and W. Croft.",
                "Indri: A Language Model-based Search Engine for Complex Queries.",
                "Proceedings of the International Conference on Intelligence Analysis, 2004. [18] The Linguistic Data Consortium. http://www.ldc.upenn.edu/. [19] E. Voorhees.",
                "Overview of the TREC 2003 Question Answering Track.",
                "Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), 2003. [20] Y. Yang and B. Kisiel.",
                "Margin-based Local Regression for Adaptive Filtering.",
                "Proceedings of the twelfth international conference on Information and knowledge management, pages 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of Adaptive Filtering Methods in a Cross-benchmark Evaluation.",
                "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 98-105, 2005. [22] C. Zhai, W. Cohen, and J. Lafferty.",
                "Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-17, 2003. [23] J. Zhang and Y. Yang.",
                "Robustness of Regularized Linear Classification Methods in Text Categorization.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 190-197, 2003. [24] Y. Zhang.",
                "Using Bayesian Priors to Combine Classifiers for Adaptive Filtering.",
                "Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 345-352, 2004. [25] Y. Zhang, J. Callan, and T. Minka.",
                "Novelty and Redundancy Detection in Adaptive Filtering.",
                "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002."
            ],
            "original_annotated_samples": [
                "Thus, G(pi, q) is defined as G(pi, q) = X Nj âC(pi,q) wj â Î³nj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the <br>answer key</br> of query q."
            ],
            "translated_annotated_samples": [
                "Por lo tanto, G(pi, q) se define como G(pi, q) = X Nj âC(pi,q) wj â Î³nj (7) donde C(pi, q) es el conjunto de todas las pepitas que aparecen en el pasaje pi y tambiÃ©n pertenecen a la <br>clave de respuesta</br> de la consulta q."
            ],
            "translated_text": "DestilaciÃ³n de informaciÃ³n basada en utilidad sobre documentos secuenciados temporalmente en el Instituto de TecnologÃ­as del Lenguaje de Yiming Yang. Universidad Carnegie Mellon, Pittsburgh, EE. UU. yiming@cs.cmu.edu Abhimanyu Lad Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. alad@cs.cmu.edu Instituto de TecnologÃ­as del Lenguaje Ni Lao. Universidad Carnegie Mellon, Pittsburgh, EE. UU. nlao@cs.cmu.edu Abhay Harpale Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. aharpale@cs.cmu.edu Bryan Kisiel Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. bkisiel@cs.cmu.edu MÃ³nica Rogati Instituto de TecnologÃ­as del Lenguaje. La Universidad Carnegie Mellon en Pittsburgh, EE. UU. mrogati@cs.cmu.edu RESUMEN Este artÃ­culo examina un nuevo enfoque para la destilaciÃ³n de informaciÃ³n en documentos ordenados temporalmente, y propone un esquema de evaluaciÃ³n novedoso para dicho marco. Combina las fortalezas y se extiende mÃ¡s allÃ¡ del filtrado adaptativo convencional, la detecciÃ³n de novedades y la clasificaciÃ³n de pasajes no redundantes con respecto a las necesidades de informaciÃ³n duraderas (tareas con mÃºltiples consultas). Nuestro enfoque respalda la retroalimentaciÃ³n detallada del usuario a travÃ©s del resaltado de fragmentos arbitrarios de texto, y aprovecha dicha informaciÃ³n para la optimizaciÃ³n de la utilidad en entornos adaptativos. Para nuestros experimentos, definimos tareas hipotÃ©ticas basadas en eventos de noticias en el corpus TDT4, con mÃºltiples consultas por tarea. Se generaron claves de respuestas (nuggets) para cada consulta y se utilizÃ³ un procedimiento semiautomÃ¡tico para adquirir reglas que permiten emparejar automÃ¡ticamente los nuggets con las respuestas del sistema. TambiÃ©n proponemos una extensiÃ³n de la mÃ©trica NDCG para evaluar la utilidad de los pasajes clasificados como una combinaciÃ³n de relevancia y novedad. Nuestros resultados muestran mejoras alentadoras en la utilidad utilizando el nuevo enfoque, en comparaciÃ³n con los sistemas base sin aprendizaje incremental o componentes de detecciÃ³n de novedades. CategorÃ­as y Descriptores de Asignaturas H.3.3 [BÃºsqueda y RecuperaciÃ³n de InformaciÃ³n]: Filtrado de informaciÃ³n, RetroalimentaciÃ³n de relevancia, Modelos de recuperaciÃ³n, Proceso de selecciÃ³n; I.5.2 TÃ©rminos Generales DiseÃ±o, MediciÃ³n, Rendimiento, ExperimentaciÃ³n. 1. INTRODUCCIÃN Seguir informaciÃ³n nueva y relevante de flujos de datos temporales para usuarios con necesidades duraderas ha sido un tema de investigaciÃ³n desafiante en la recuperaciÃ³n de informaciÃ³n. El filtrado adaptativo (AF) es una tarea de predicciÃ³n en lÃ­nea de la relevancia de cada nuevo documento con respecto a temas predefinidos. BasÃ¡ndose en la consulta inicial y algunos ejemplos positivos (si estÃ¡n disponibles), un sistema de AF mantiene un perfil para cada tema de interÃ©s, y lo actualiza constantemente segÃºn la retroalimentaciÃ³n del usuario. La naturaleza de aprendizaje incremental de los sistemas de AF los hace mÃ¡s poderosos que los motores de bÃºsqueda estÃ¡ndar que admiten recuperaciÃ³n ad-hoc (por ejemplo. Google y Yahoo) en tÃ©rminos de encontrar informaciÃ³n relevante con respecto a temas de interÃ©s duraderos, y mÃ¡s atractivo para los usuarios que estÃ¡n dispuestos a proporcionar retroalimentaciÃ³n para adaptar el sistema a sus necesidades de informaciÃ³n especÃ­ficas, sin tener que modificar sus consultas manualmente. Se han estudiado una variedad de algoritmos de aprendizaje supervisado (clasificadores de estilo Rocchio, modelos exponenciales-gaussianos, regresiÃ³n local y enfoques de regresiÃ³n logÃ­stica) para entornos adaptativos, examinados con retroalimentaciÃ³n de relevancia explÃ­cita e implÃ­cita, y evaluados con respecto a la optimizaciÃ³n de la utilidad en grandes colecciones de datos de referencia en los foros de TREC (Conferencias de RecuperaciÃ³n de Texto) y TDT (DetecciÃ³n y Seguimiento de Temas) [1, 4, 7, 15, 16, 20, 24, 23]. La regresiÃ³n logÃ­stica regularizada [21] se ha encontrado representativa para los enfoques de vanguardia, y altamente eficiente para adaptaciones frecuentes del modelo en grandes colecciones de documentos como el corpus TREC-10 (mÃ¡s de 800,000 documentos y 84 temas). A pesar de los logros sustanciales en la investigaciÃ³n reciente de filtrado adaptativo, aÃºn quedan problemas significativos sin resolver sobre cÃ³mo aprovechar de manera efectiva y eficiente la retroalimentaciÃ³n del usuario. EspecÃ­ficamente, los siguientes problemas pueden limitar seriamente la verdadera utilidad de los sistemas de AF en aplicaciones del mundo real: 1. El usuario tiene un papel bastante pasivo en la configuraciÃ³n convencional de filtrado adaptativo: solo reacciona ante el sistema cuando este toma una decisiÃ³n afirmativa sobre un documento, confirmando o rechazando esa decisiÃ³n. Una alternativa mÃ¡s activa serÃ­a permitir al usuario emitir mÃºltiples consultas sobre un tema, revisar una lista clasificada de documentos candidatos (o pasajes) por consulta, y proporcionar retroalimentaciÃ³n sobre la lista clasificada, refinando asÃ­ su necesidad de informaciÃ³n y solicitando listas clasificadas actualizadas. La Ãºltima forma de interacciÃ³n del usuario ha sido altamente efectiva en la recuperaciÃ³n estÃ¡ndar para consultas ad-hoc. CÃ³mo implementar una estrategia para necesidades de informaciÃ³n duraderas en entornos de FA es una pregunta abierta para la investigaciÃ³n. 2. La unidad para recibir un juicio de relevancia (sÃ­ o no) estÃ¡ restringida al nivel del documento en AF convencional. Sin embargo, un usuario real puede estar dispuesto a proporcionar comentarios mÃ¡s informativos y detallados resaltando algunas partes de texto en un documento recuperado como relevantes, en lugar de etiquetar todo el documento como relevante. Aprovechar de manera efectiva este tipo de retroalimentaciÃ³n detallada podrÃ­a mejorar sustancialmente la calidad de un sistema de AF. Para esto, necesitamos habilitar el aprendizaje supervisado a partir de fragmentos de texto etiquetados de longitud arbitraria en lugar de solo permitir documentos etiquetados. 3. Los documentos seleccionados por el sistema suelen ser altamente redundantes. Un evento importante de noticias, por ejemplo, serÃ­a reportado por mÃºltiples fuentes repetidamente durante un tiempo, haciendo que la mayor parte del contenido informativo en esos artÃ­culos sea redundante entre sÃ­. Un sistema de AF convencional seleccionarÃ­a todas estas historias de noticias redundantes para la retroalimentaciÃ³n del usuario, desperdiciando el tiempo de los usuarios mientras ofrece poco beneficio. Claramente, las tÃ©cnicas de detecciÃ³n de novedades pueden ayudar en principio [25, 2, 22] a mejorar la utilidad de los sistemas AF. Sin embargo, la efectividad de tales tÃ©cnicas a nivel de pasaje para detectar novedad con respecto a la retroalimentaciÃ³n de los usuarios a nivel detallado y para detectar redundancia en listas clasificadas aÃºn debe ser evaluada utilizando una medida de utilidad que imite las necesidades de un usuario real. Para abordar las limitaciones actuales de los sistemas de AF mencionadas anteriormente, proponemos y examinamos un nuevo enfoque en este artÃ­culo, que combina las fortalezas del AF convencional (aprendizaje incremental de modelos de temas), recuperaciÃ³n de pasajes multipase para consultas de larga duraciÃ³n condicionadas al tema, y detecciÃ³n de novedades para eliminar la redundancia de las interacciones del usuario con el sistema. Llamamos al nuevo proceso destilaciÃ³n de informaciÃ³n basada en utilidad. Ten en cuenta que los corpus de referencia convencionales para evaluaciones de AF, que tienen juicios de relevancia a nivel de documento y no definen tareas con mÃºltiples consultas, son insuficientes para evaluar el nuevo enfoque. Por lo tanto, ampliamos un corpus de referencia, la colecciÃ³n TDT4 de noticias y transmisiones de televisiÃ³n, con definiciones de tareas, mÃºltiples consultas por tarea y claves de respuestas por consulta. Hemos realizado nuestros experimentos en este corpus extendido TDT4 y hemos puesto a disposiciÃ³n del pÃºblico los datos generados adicionalmente para futuras evaluaciones comparativas. Para evaluar automÃ¡ticamente los fragmentos de texto arbitrarios devueltos por el sistema utilizando nuestras claves de respuestas, desarrollamos un esquema de evaluaciÃ³n con un procedimiento semiautomÃ¡tico para 1 URL: http://nyc.lti.cs.cmu.edu/downloads adquiriendo reglas que pueden comparar fragmentos con las respuestas del sistema. AdemÃ¡s, proponemos una extensiÃ³n de NDCG (Normalized Discounted Cumulated Gain) [9] para evaluar la utilidad de los pasajes clasificados como una funciÃ³n tanto de la relevancia como de la novedad. El resto de este documento estÃ¡ organizado de la siguiente manera. La secciÃ³n 2 describe el proceso de destilaciÃ³n de informaciÃ³n con un ejemplo concreto. La secciÃ³n 3 describe los nÃºcleos tÃ©cnicos de nuestro sistema llamado CAFÂ´E - CMU Adaptive Filtering Engine. La secciÃ³n 4 discute problemas con respecto a la metodologÃ­a de evaluaciÃ³n y propone un nuevo esquema. La secciÃ³n 5 describe el corpus extendido TDT4. La secciÃ³n 6 presenta nuestros experimentos y resultados. La secciÃ³n 7 concluye el estudio y ofrece perspectivas futuras. 2. Una TAREA DE EJEMPLO Considera un evento noticioso: la fuga de siete convictos de una prisiÃ³n de Texas en diciembre de 2000 y su captura un mes despuÃ©s. Suponiendo que un usuario estuviera interesado en este evento desde sus primeras etapas, la necesidad de informaciÃ³n podrÃ­a ser: Encontrar informaciÃ³n sobre la fuga de convictos de una prisiÃ³n de Texas, y la informaciÃ³n relacionada con su recaptura. Las preguntas de nivel inferior asociadas podrÃ­an ser: 1. Â¿CuÃ¡ntos prisioneros escaparon? 2. Â¿DÃ³nde y cuÃ¡ndo fueron avistados? 3. Â¿QuiÃ©nes son sus contactos conocidos dentro y fuera de la prisiÃ³n? 4. Â¿CÃ³mo estÃ¡n armados? 5. Â¿Tienen algÃºn vehÃ­culo? 6. Â¿QuÃ© pasos se han tomado hasta ahora? Llamamos a esta necesidad de informaciÃ³n una tarea, y a las preguntas asociadas como las consultas en esta tarea. Un sistema de destilaciÃ³n debe monitorear los documentos entrantes, procesarlos por fragmentos en un orden temporal, seleccionar pasajes potencialmente relevantes y novedosos de cada fragmento con respecto a cada consulta, y presentar una lista clasificada de pasajes al usuario. La clasificaciÃ³n de pasajes aquÃ­ se basa en cuÃ¡n relevante es un pasaje con respecto a la consulta actual, cuÃ¡n novedoso es con respecto al historial actual del usuario (de sus interacciones con el sistema) y cuÃ¡n redundante es en comparaciÃ³n con otros pasajes con un rango mÃ¡s alto en la lista. Cuando se le presenta una lista de pasajes, el usuario puede proporcionar retroalimentaciÃ³n resaltando tramos de texto arbitrarios que considere relevantes. Estos fragmentos de texto se toman como ejemplos positivos en la adaptaciÃ³n del perfil de consulta, y tambiÃ©n se aÃ±aden al historial de los usuarios. Los pasajes no marcados por el usuario se consideran ejemplos negativos. Tan pronto como se actualiza el perfil de consulta, el sistema vuelve a realizar una bÃºsqueda y devuelve otra lista clasificada de pasajes donde los pasajes previamente vistos son eliminados o clasificados como bajos, segÃºn la preferencia del usuario. Por ejemplo, si el usuario resalta ...los funcionarios han ofrecido una recompensa de $100,000 por su captura... como respuesta relevante a la consulta Â¿QuÃ© pasos se han tomado hasta ahora?, entonces la pieza resaltada se utiliza como un ejemplo de entrenamiento positivo adicional en la adaptaciÃ³n del perfil de la consulta. Este comentario tambiÃ©n se aÃ±ade al historial del usuario como un ejemplo visto, para que en el futuro, el sistema no coloque otro pasaje mencionando una recompensa de $100,000 en la parte superior de la lista clasificada. Sin embargo, un artÃ­culo que mencione que los funcionarios han duplicado el dinero de recompensa a $200,000 podrÃ­a clasificarse alto ya que es relevante tanto para el perfil de consulta (actualizado) como novedoso con respecto a la historia del usuario (actualizada). El usuario puede modificar las consultas originales o agregar una nueva consulta durante el proceso; los perfiles de consulta se cambiarÃ¡n en consecuencia. Claramente, la detecciÃ³n de novedades es muy importante para la utilidad de dicho sistema debido a la bÃºsqueda iterativa. Sin detecciÃ³n de novedad, los antiguos pasajes relevantes se mostrarÃ­an al usuario repetidamente en cada lista clasificada. A travÃ©s del ejemplo anterior, podemos ver las principales propiedades de nuestro nuevo marco de trabajo para la destilaciÃ³n de informaciÃ³n basada en utilidad sobre documentos ordenados temporalmente. Nuestro marco de trabajo combina y amplÃ­a el poder del filtrado adaptativo (AF), la recuperaciÃ³n ad-hoc (IR) y la detecciÃ³n de novedades (ND). En comparaciÃ³n con la IR estÃ¡ndar, nuestro enfoque tiene la capacidad de aprender de forma incremental las necesidades de informaciÃ³n a largo plazo y modelar una secuencia de consultas dentro de una tarea. En comparaciÃ³n con el AF convencional, permite un papel mÃ¡s activo del usuario en refinar sus necesidades de informaciÃ³n y solicitar nuevos resultados al permitir comentarios sobre relevancia y novedad mediante el resaltado de fragmentos arbitrarios de texto en los pasajes devueltos por el sistema. Comparado con trabajos anteriores, esta es la primera evaluaciÃ³n de detecciÃ³n de novedad integrada con filtrado adaptativo para consultas secuenciadas que permite retroalimentaciÃ³n flexible del usuario sobre pasajes clasificados. La combinaciÃ³n de AF, IR y ND con las nuevas extensiones plantea una importante pregunta de investigaciÃ³n sobre la metodologÃ­a de evaluaciÃ³n: Â¿cÃ³mo podemos medir la utilidad de un sistema de destilaciÃ³n de informaciÃ³n como este? Las mÃ©tricas existentes en IR estÃ¡ndar, AF y ND son insuficientes, y se deben explorar nuevas soluciones, como discutiremos en la SecciÃ³n 4, despuÃ©s de describir los nÃºcleos tÃ©cnicos de nuestro sistema en la prÃ³xima secciÃ³n. 3. NÃCLEOS TÃCNICOS Los componentes principales de CAFÃ son: 1) AF para el aprendizaje incremental de perfiles de consulta, 2) IR para estimar la relevancia de pasajes con respecto a los perfiles de consulta, 3) ND para evaluar la novedad de pasajes con respecto al historial de usuarios, y 4) un componente anti-redundancia para eliminar la redundancia de las listas clasificadas. 3.1 Componente de Filtrado Adaptativo Utilizamos un algoritmo de vanguardia en el campo, el mÃ©todo de regresiÃ³n logÃ­stica regularizada que obtuvo los mejores resultados en varios corpus de evaluaciÃ³n de referencia para AF [21]. La regresiÃ³n logÃ­stica (LR) es un algoritmo de aprendizaje supervisado para clasificaciÃ³n estadÃ­stica. BasÃ¡ndose en un conjunto de entrenamiento de instancias etiquetadas, aprende un modelo de clase que luego puede ser utilizado para predecir las etiquetas de instancias no vistas. Su rendimiento, asÃ­ como su eficiencia en tÃ©rminos de tiempo de entrenamiento, lo convierten en un buen candidato cuando se requieren actualizaciones frecuentes del modelo de clase, como es el caso en el filtrado adaptativo, donde el sistema debe aprender de cada nueva retroalimentaciÃ³n proporcionada por el usuario. (Consulte [21] y [23] para complejidad computacional y problemas de implementaciÃ³n). En el filtrado adaptativo, cada consulta se considera como una clase y la probabilidad de que un pasaje pertenezca a esta clase corresponde al grado de relevancia del pasaje con respecto a la consulta. Para entrenar el modelo, utilizamos la propia consulta como ejemplo inicial de entrenamiento positivo de la clase, y las piezas de texto resaltadas por el usuario (marcadas como Relevante o No relevante) durante la retroalimentaciÃ³n como ejemplos de entrenamiento adicionales. Para abordar el problema de inicio en frÃ­o en la etapa inicial antes de obtener cualquier retroalimentaciÃ³n del usuario, el sistema utiliza una pequeÃ±a muestra de un corpus retrospectivo como ejemplos negativos iniciales en el conjunto de entrenamiento. Los detalles de cÃ³mo utilizar la regresiÃ³n logÃ­stica para el filtrado adaptativo (asignando diferentes pesos a las instancias de entrenamiento positivas y negativas, y regularizando la funciÃ³n objetivo para prevenir el sobreajuste en los datos de entrenamiento) se presentan en [21]. El modelo de clase wâ aprendido por RegresiÃ³n LogÃ­stica, o el perfil de consulta, es un vector cuyas dimensiones son tÃ©rminos individuales y cuyos elementos son los coeficientes de regresiÃ³n, indicando cuÃ¡n influyente es cada tÃ©rmino en el perfil de consulta. El perfil de la consulta se actualiza cada vez que se recibe una nueva pieza de retroalimentaciÃ³n del usuario. Un peso decaÃ­do temporalmente se puede aplicar a cada ejemplo de entrenamiento, como una opciÃ³n, para enfatizar la retroalimentaciÃ³n mÃ¡s reciente del usuario. 3.2 Componente de RecuperaciÃ³n de Pasajes Utilizamos tÃ©cnicas estÃ¡ndar de IR en esta parte de nuestro sistema. Los documentos entrantes se procesan por bloques, donde cada bloque puede definirse como un intervalo de tiempo fijo o como un nÃºmero fijo de documentos, segÃºn lo prefiera el usuario. Para cada documento entrante, se actualizan las estadÃ­sticas del corpus como el IDF (Frecuencia Inversa de Documento) de cada tÃ©rmino. Utilizamos un identificador y rastreador de entidades con tecnologÃ­a de punta [8, 12] para identificar nombres de personas y lugares, y fusionarlos con entidades con referencia cruzada vistas en el pasado. Luego, los documentos se dividen en pasajes, que pueden ser un documento completo, un pÃ¡rrafo, una oraciÃ³n, o cualquier otro fragmento continuo de texto, segÃºn se prefiera. Cada pasaje se representa utilizando un vector de pesos TF-IDF (Frecuencia de TÃ©rmino - Frecuencia Inversa de Documento), donde el tÃ©rmino puede ser una palabra o una entidad nombrada. Dado un perfil de consulta, es decir, la soluciÃ³n de regresiÃ³n logÃ­stica wâ como se describe en la SecciÃ³n 3.1, el sistema calcula la probabilidad posterior de relevancia para cada pasaje x como fRL(x) â¡ P(y = 1|x, wâ ) = 1 (1 + eâwâÂ·x) (1). Los pasajes se ordenan segÃºn sus puntajes de relevancia, y aquellos con puntajes por encima de un umbral (ajustado en un conjunto de entrenamiento) conforman la lista de relevancia que se pasa al paso de detecciÃ³n de novedad. 3.3 Componente de DetecciÃ³n de Novedad CAFÂ´E mantiene un historial de usuario H(t), que contiene todos los fragmentos de texto hi que el usuario resaltÃ³ (como retroalimentaciÃ³n) durante sus interacciones pasadas con el sistema, hasta el tiempo actual t. Denotando el historial como H(t) = n h1, h2, ..., ht o , (2), el puntaje de novedad de un nuevo pasaje candidato x se calcula como: fND(x) = 1 â max iâ1..t {cos(x, hi)} (3), donde tanto el pasaje candidato x como los fragmentos de texto resaltados hi se representan como vectores TF-IDF. El puntaje de novedad de cada pasaje se compara con un umbral predefinido (ajustado tambiÃ©n en un conjunto de entrenamiento), y cualquier pasaje con un puntaje por debajo de este umbral se elimina de la lista de relevancia. Componente de clasificaciÃ³n anti-redundante Aunque el componente de detecciÃ³n de novedad garantiza que solo la informaciÃ³n novedosa (no vista previamente) permanezca en la lista de relevancia, esta lista aÃºn podrÃ­a contener la misma informaciÃ³n novedosa en mÃºltiples posiciones en la lista clasificada. Supongamos, por ejemplo, que el usuario ya ha leÃ­do sobre una recompensa de $100,000 por informaciÃ³n sobre los convictos fugados. Una nueva noticia de que el premio ha sido aumentado a $200,000 es novedosa ya que el usuario aÃºn no la ha leÃ­do. Sin embargo, mÃºltiples fuentes de noticias podrÃ­an informar sobre esta noticia y podrÃ­amos terminar mostrando artÃ­culos (redundantes) de todas estas fuentes en una lista clasificada. Por lo tanto, una lista clasificada tambiÃ©n debe ser no redundante con respecto a su propio contenido. Utilizamos una versiÃ³n simplificada del mÃ©todo de MÃ¡xima Relevancia Marginal [5], originalmente desarrollado para combinar relevancia y novedad en la recuperaciÃ³n de texto y resÃºmenes. Nuestro procedimiento comienza con la lista actual de pasajes ordenados por relevancia (secciÃ³n 3.2), filtrados por el componente de DetecciÃ³n de Novedades (secciÃ³n 3.3), y genera una nueva lista no redundante de la siguiente manera: 1. Toma el pasaje superior en la lista actual como el primero en la nueva lista. 2. Agrega el siguiente pasaje x en la lista actual a la nueva lista solo si fAR(x) > t donde fAR(x) = 1 â max piâLnew {cos(x, pi)} y Lnew es el conjunto de pasajes ya seleccionados en la nueva lista. Repetir el paso 2 hasta que se hayan examinado todos los pasajes en la lista actual. DespuÃ©s de aplicar el algoritmo mencionado anteriormente, cada pasaje en la nueva lista es lo suficientemente diferente a los demÃ¡s, favoreciendo asÃ­ la diversidad en lugar de la redundancia en la nueva lista clasificada. El umbral anti-redundancia t se ajusta en un conjunto de entrenamiento. 4. METODOLOGÃA DE EVALUACIÃN El enfoque que propusimos anteriormente para la destilaciÃ³n de informaciÃ³n plantea importantes cuestiones sobre la metodologÃ­a de evaluaciÃ³n. En primer lugar, dado que nuestro marco de trabajo permite que la salida sean fragmentos en diferentes niveles de granularidad (por ejemplo, ventanas de k oraciones donde k puede variar) en lugar de una longitud fija, no es posible tener juicios de relevancia preanotados en todos esos niveles de granularidad. En segundo lugar, dado que deseamos medir la utilidad de la salida del sistema como una combinaciÃ³n de relevancia y novedad, las medidas tradicionales basadas Ãºnicamente en la relevancia deben ser reemplazadas por medidas que penalicen la repeticiÃ³n de la misma informaciÃ³n en la salida del sistema a lo largo del tiempo. En tercer lugar, dado que la salida del sistema son listas clasificadas, debemos recompensar a aquellos sistemas que presenten informaciÃ³n Ãºtil (tanto relevante como no vista previamente) utilizando listas clasificadas mÃ¡s cortas, y penalizar a aquellos que presenten la misma informaciÃ³n utilizando listas clasificadas mÃ¡s largas. Ninguna de las medidas existentes en la recuperaciÃ³n ad-hoc, filtrado adaptativo, detecciÃ³n de novedades u otras Ã¡reas relacionadas (resumen de texto y respuesta a preguntas) tiene propiedades deseables en los tres aspectos. Por lo tanto, debemos desarrollar una nueva metodologÃ­a de evaluaciÃ³n. 4.1 Claves de Respuesta Para permitir la evaluaciÃ³n de un sistema cuya salida consiste en pasajes de longitud arbitraria, tomamos prestado el concepto de claves de respuesta de la comunidad de Preguntas y Respuestas (QA), donde los sistemas pueden devolver fragmentos de texto arbitrarios como respuestas. Las claves de respuestas definen lo que debe estar presente en la respuesta de un sistema para recibir crÃ©dito, y estÃ¡n compuestas por una colecciÃ³n de \"nuggets\" de informaciÃ³n, es decir, unidades de datos sobre las cuales los evaluadores humanos pueden tomar decisiones binarias sobre si una respuesta del sistema las contiene o no. La definiciÃ³n de claves de respuestas y la toma de decisiones binarias asociadas son tareas conceptuales que requieren un mapeo semÃ¡ntico [19], ya que los pasajes devueltos por el sistema pueden contener la misma informaciÃ³n expresada de muchas formas diferentes. Por lo tanto, las evaluaciones de QA han dependido de evaluadores humanos para la correspondencia entre diversas expresiones, lo que hace que el proceso sea costoso, consume mucho tiempo y no sea escalable para grandes colecciones de consultas y documentos, y evaluaciones extensas de sistemas con diversos ajustes de parÃ¡metros. 4.1.1 AutomatizaciÃ³n de la evaluaciÃ³n basada en claves de respuestas Los mÃ©todos de evaluaciÃ³n automÃ¡tica permitirÃ­an una construcciÃ³n y ajuste mÃ¡s rÃ¡pidos del sistema, asÃ­ como proporcionar una forma objetiva y asequible de comparar diversos sistemas. Recientemente, se han propuesto mÃ©todos basados mÃ¡s o menos en la idea de las co-ocurrencias de n-gramos. Pourpre [10] asigna una puntuaciÃ³n de recuerdo fraccional a una respuesta del sistema basada en su superposiciÃ³n de unigramas con una descripciÃ³n de pepitas dada. Por ejemplo, una respuesta del sistema A B C tiene una recuperaciÃ³n de 3/4 con respecto a un fragmento con la descripciÃ³n A B C D. Sin embargo, este enfoque es injusto para los sistemas que presentan la misma informaciÃ³n pero utilizando palabras distintas a A, B, C y D. Otro problema abierto es cÃ³mo ponderar las palabras individuales al medir la cercanÃ­a de una coincidencia. Por ejemplo, considera la pregunta Â¿CuÃ¡ntos prisioneros escaparon? En el fragmento Siete prisioneros escaparon de una prisiÃ³n de Texas, no hay indicaciÃ³n de que siete sea la palabra clave y que debe coincidir para obtener algÃºn crÃ©dito de relevancia. El uso de los valores de IDF no ayuda, ya que la palabra \"seven\" generalmente no tendrÃ¡ un IDF mÃ¡s alto que palabras como \"texas\" y \"prison\". AdemÃ¡s, redefinir el fragmento como simplemente siete no resuelve el problema, ya que ahora podrÃ­a coincidir errÃ³neamente con cualquier menciÃ³n de siete fuera de contexto. Nuggeteer [13] trabaja sobre principios similares pero toma decisiones binarias sobre si hay un fragmento en una respuesta de sistema dada ajustando un umbral. Sin embargo, tambiÃ©n estÃ¡ plagado de relevancia espuria ya que no todas las palabras contenidas en la descripciÃ³n de la pepita (o respuestas correctas conocidas) son centrales para la pepita. 4.1.2 Reglas de Coincidencia de Pepitas Proponemos un mÃ©todo automÃ¡tico confiable para determinar si un fragmento de texto contiene una pepita dada, basado en reglas de coincidencia de pepitas, que se generan utilizando un procedimiento semiautomÃ¡tico explicado a continuaciÃ³n. Estas reglas son bÃ¡sicamente consultas booleanas que solo coincidirÃ¡n con fragmentos que contengan la pepita. Por ejemplo, una regla candidata para emparejar respuestas a Â¿CuÃ¡ntos prisioneros escaparon? es (Texas Y siete Y escapar Y (convictos O prisioneros)), posiblemente con otros sinÃ³nimos y variantes en la regla. Para un corpus de artÃ­culos de noticias, que suelen seguir una prosa formal tÃ­pica, es bastante fÃ¡cil escribir reglas simples para coincidir con respuestas esperadas utilizando un enfoque de arranque, como se describe a continuaciÃ³n. Proponemos un enfoque de dos etapas, inspirado en Autoslog [14], que combina la capacidad de los humanos para identificar expresiones semÃ¡nticamente equivalentes y la capacidad del sistema para recopilar evidencia estadÃ­stica de un corpus de documentos anotados por humanos. En la primera etapa, los sujetos humanos anotaron (utilizando una herramienta de resaltado) porciones de documentos relevantes que contenÃ­an respuestas a cada fragmento 2. En la segunda etapa, los sujetos utilizaron nuestra herramienta de generaciÃ³n de reglas para crear reglas que coincidieran con las anotaciones de cada fragmento. La herramienta permite a los usuarios ingresar una regla booleana como una disyunciÃ³n de conjunciones (por ejemplo, ((a Y b) O (a Y c Y d) O (e))). Dada una regla candidata, nuestra herramienta la utiliza como una consulta booleana sobre el conjunto completo de documentos relevantes y calcula su recall y precisiÃ³n con respecto a las anotaciones que se espera que coincida. Por lo tanto, los sujetos pueden comenzar con una regla simple y refinarla iterativamente hasta que estÃ©n satisfechos con su precisiÃ³n y recuperaciÃ³n. Observamos que era muy fÃ¡cil para los humanos mejorar la precisiÃ³n de una regla ajustando sus conjunciones existentes (agregando mÃ¡s Y) y mejorar la recuperaciÃ³n agregando mÃ¡s conjunciones a la disyunciÃ³n (agregando mÃ¡s O). Como ejemplo, intentemos crear una regla para el acertijo que dice que siete prisioneros escaparon de la prisiÃ³n de Texas. Empezamos con una regla simple - (siete). Cuando introducimos esto en la herramienta de generaciÃ³n de reglas, nos damos cuenta de que esta regla coincide con muchas ocurrencias espurias de siete (por ejemplo, ...siete estados...) y, por lo tanto, obtiene una puntuaciÃ³n baja de precisiÃ³n. Podemos calificar aÃºn mÃ¡s nuestra regla - Texas Y siete Y convictos. A continuaciÃ³n, al revisar las anotaciones omitidas, nos damos cuenta de que algunos artÃ­culos de noticias mencionaban ...siete prisioneros escaparon.... Luego reemplazamos convictos por la disyunciÃ³n (convictos O prisioneros). Continuamos ajustando la regla de esta manera hasta lograr un nivel de recordaciÃ³n y precisiÃ³n suficientemente alto, es decir, que los (pocos) errores y alarmas falsas se puedan ignorar de forma segura. AsÃ­ podemos crear reglas de coincidencia de pepitas que capturen de manera sucinta varias formas de expresar una pepita, evitando al mismo tiempo emparejar respuestas incorrectas (o fuera de contexto). La participaciÃ³n humana en el proceso de creaciÃ³n de reglas garantiza reglas genÃ©ricas de alta calidad que luego se pueden utilizar para evaluar de manera confiable las respuestas del sistema arbitrario. 4.2 EvaluaciÃ³n de la Utilidad de una Secuencia de Listas Clasificadas La utilidad de un sistema de recuperaciÃ³n se puede definir como la diferencia entre cuÃ¡nta informaciÃ³n Ãºtil obtuvo el usuario y cuÃ¡nto tiempo y energÃ­a perdiÃ³. Calculamos esta utilidad a partir de las utilidades de los pasajes individuales de la siguiente manera. DespuÃ©s de leer cada pasaje devuelto por el sistema, el usuario obtiene cierta ganancia dependiendo de la presencia de informaciÃ³n relevante y novedosa, y incurre en una pÃ©rdida en tÃ©rminos del tiempo y la energÃ­a invertidos en revisar el pasaje. Sin embargo, la probabilidad de que el usuario realmente lea un pasaje depende de su posiciÃ³n en la lista clasificada. Por lo tanto, para una consulta q, la utilidad esperada 2 LDC [18] ya proporciona juicios de relevancia para 100 temas en el corpus TDT4. AdemÃ¡s, nos aseguramos de que estos juicios sean exhaustivos en todo el corpus utilizando la agrupaciÃ³n. El valor de un pasaje pi en el rango i se puede definir como U(pi, q) = P(i) â (Ganancia(pi, q) - PÃ©rdida(pi, q)) (4) donde P(i) es la probabilidad de que el usuario pase por un pasaje en el rango i. La utilidad esperada para una lista clasificada completa de longitud n se puede calcular simplemente sumando la utilidad esperada de cada pasaje: U(q) = Î£ i=1 P(i) â (Ganancia(pi, q) â PÃ©rdida(pi, q)) (5) NÃ³tese que si ignoramos el tÃ©rmino de pÃ©rdida y definimos P(i) como P(i) â 1/ logb(b + i â 1) (6) entonces obtenemos la mÃ©trica recientemente popularizada llamada Ganancia Acumulada Descontada (DCG) [9], donde Ganancia(pi, q) se define como la relevancia graduada del pasaje pi. Sin embargo, sin el tÃ©rmino de pÃ©rdida, DCG es una mÃ©trica puramente orientada al recuerdo y no es adecuada para un entorno de filtrado adaptativo, donde la utilidad del sistema depende en parte de su capacidad para limitar el nÃºmero de elementos mostrados al usuario. Aunque P(i) podrÃ­a definirse en base a estudios empÃ­ricos del comportamiento del usuario, por simplicidad, utilizamos P(i) tal como se define exactamente en la ecuaciÃ³n 6. La ganancia G(pi, q) del pasaje pi con respecto a la consulta q es una funciÃ³n de - 1) el nÃºmero de pepitas relevantes presentes en pi, y 2) la novedad de cada una de estas pepitas. Combinamos estos dos factores de la siguiente manera. Para cada pepita Nj, asignamos un peso inicial wj, y tambiÃ©n mantenemos un conteo nj del nÃºmero de veces que esta pepita ha sido vista por el usuario en el pasado. La ganancia derivada de cada ocurrencia posterior del mismo hallazgo se asume que se reduce por un factor de amortiguamiento Î³. Por lo tanto, G(pi, q) se define como G(pi, q) = X Nj âC(pi,q) wj â Î³nj (7) donde C(pi, q) es el conjunto de todas las pepitas que aparecen en el pasaje pi y tambiÃ©n pertenecen a la <br>clave de respuesta</br> de la consulta q. Los pesos iniciales wj se establecen todos en 1.0 en nuestros experimentos, pero tambiÃ©n pueden establecerse segÃºn un enfoque de pirÃ¡mide [11]. La elecciÃ³n del factor de amortiguaciÃ³n Î³ determina la tolerancia del usuario a la redundancia. Cuando Î³ = 0, un pepita solo recibirÃ¡ crÃ©dito por su primera ocurrencia, es decir, cuando nj sea cero. Para 0 < Î³ < 1, un nugget recibe menos crÃ©dito por cada ocurrencia sucesiva. Cuando Î³ = 1, no se produce amortiguamiento y las repeticiones de un valor reciben el mismo crÃ©dito. Se debe tener en cuenta que los recuentos de ocurrencia de las pepitas se conservan entre la evaluaciÃ³n de las listas clasificadas sucesivas devueltas por el sistema, ya que se espera que los usuarios recuerden lo que el sistema les mostrÃ³ en el pasado. Definimos la pÃ©rdida L(pi, q) como un costo constante c (usamos 0.1) incurrido al leer un pasaje devuelto por el sistema. Por lo tanto, nuestra mÃ©trica puede ser reescrita como U(q) = nX i=1 Ganancia(pi, q) logb(b + i â 1) â L(n) (8) donde L(n) es la pÃ©rdida asociada con una lista clasificada de longitud n: L(n) = c Â· nX i=1 1 logb(b + i â 1) (9) 3 NÃ³tese que 00 = 1 Debido a la similitud con la Ganancia Acumulada Descontada (DCG), llamamos a nuestra mÃ©trica Utilidad Acumulada Descontada (DCU). El puntaje de DCU obtenido por el sistema se convierte en un puntaje de DCU normalizado (NDCU) dividiÃ©ndolo por el puntaje de DCU de la lista ideal clasificada, que se crea ordenando los pasajes por sus puntajes de utilidad decrecientes U(pi, q) y deteniÃ©ndose cuando U(pi, q) â¤ 0, es decir, cuando la ganancia es menor o igual al costo de leer el pasaje. El conjunto de datos TDT4 fue el corpus de referencia utilizado en las evaluaciones TDT2002 y TDT2003. El corpus consiste en mÃ¡s de 90,000 artÃ­culos de noticias de mÃºltiples fuentes (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) publicados entre octubre de 2000 y enero de 2001, en los idiomas Ã¡rabe, inglÃ©s y mandarÃ­n. Las versiones de los artÃ­culos en otros idiomas reconocidas por voz y traducidas por mÃ¡quina tambiÃ©n fueron proporcionadas. LDC [18] ha anotado el corpus con 100 temas, que corresponden a varios eventos de noticias en este perÃ­odo de tiempo. De entre estos, seleccionamos un subconjunto de 12 eventos accionables y definimos tareas correspondientes para ellos. Para cada tarea, definimos manualmente un perfil que consiste en un conjunto inicial de (5 a 10) consultas, una descripciÃ³n de texto libre del historial del usuario, es decir, lo que el usuario ya sabe sobre el evento, y una lista de documentos conocidos relevantes y no relevantes (si estÃ¡n disponibles) como ejemplos de entrenamiento. Para cada consulta, generamos claves de respuestas y reglas de coincidencia de pepitas correspondientes utilizando el procedimiento descrito en la secciÃ³n 4.1.2, y produjimos un total de 120 consultas, con un promedio de 7 pepitas por consulta. EXPERIMENTOS Y RESULTADOS 6.1 Baselines Utilizamos Indri [17], un motor de bÃºsqueda basado en modelos de lenguaje popular, como referencia para comparar con CAFÃ. Indri admite la funcionalidad estÃ¡ndar de los motores de bÃºsqueda, incluido el feedback de pseudo-relevancia (PRF) [3, 6], y es representativo de un sistema de recuperaciÃ³n basado en consultas tÃ­pico. Indri no admite ningÃºn tipo de detecciÃ³n de novedades. Comparamos Indri con PRF activado y desactivado, frente a CAFÃ con retroalimentaciÃ³n del usuario, detecciÃ³n de novedades y clasificaciÃ³n antirredundante activados y desactivados. 6.2 ConfiguraciÃ³n Experimental Dividimos el corpus TDT4 que abarca 4 meses en 10 fragmentos, cada uno definido como un perÃ­odo de 12 dÃ­as consecutivos. En cualquier momento dado del proceso de destilaciÃ³n, cada sistema accedÃ­a a los datos pasados hasta el punto actual, y devolvÃ­a una lista clasificada de hasta 50 pasajes por consulta. Las 12 tareas definidas en el corpus se dividieron en un conjunto de entrenamiento y prueba con 6 tareas cada uno. Cada sistema pudo utilizar el conjunto de entrenamiento para ajustar sus parÃ¡metros con el fin de optimizar NDCU (ecuaciÃ³n 8), incluyendo el umbral de relevancia tanto para Indri como para CAFÂ´E, y los umbrales de novedad y antirredundancia para CAFÂ´E. La NDCU para cada ejecuciÃ³n del sistema se calcula automÃ¡ticamente. TambiÃ©n se simulÃ³ la retroalimentaciÃ³n de los usuarios: los juicios de relevancia para cada pasaje devuelto por el sistema (segÃºn las reglas de coincidencia de pepitas descritas en la secciÃ³n 4.1.2) fueron utilizados como retroalimentaciÃ³n de los usuarios en la adaptaciÃ³n de perfiles de consulta e historiales de usuario. En la Tabla 1, mostramos los puntajes NDCU de los dos sistemas en diferentes configuraciones. Estas puntuaciones se promedian en las seis tareas del conjunto de pruebas, y se calculan con dos factores de amortiguaciÃ³n (ver secciÃ³n 4.2): Î³ = 0 y 0.1, para simular ninguna tolerancia y poca tolerancia para la redundancia, respectivamente. Usar Î³ = 0 crea una mÃ©trica mucho mÃ¡s estricta ya que no otorga ningÃºn crÃ©dito a un pasaje que contenga informaciÃ³n relevante pero redundante. Por lo tanto, la mejora obtenida al habilitar la retroalimentaciÃ³n del usuario es menor con Î³ = 0 que la mejora obtenida de la retroalimentaciÃ³n con Î³ = 0.1. Esto revela una deficiencia de los sistemas de recuperaciÃ³n contemporÃ¡neos: cuando el usuario da retroalimentaciÃ³n positiva sobre un pasaje, los sistemas otorgan mayor peso a los tÃ©rminos presentes en ese pasaje y tienden a recuperar otros pasajes que contienen los mismos tÃ©rminos, y por lo tanto, generalmente la misma informaciÃ³n. Sin embargo, el usuario no se beneficia al ver pasajes redundantes y generalmente estÃ¡ interesado en otros pasajes que contengan informaciÃ³n relacionada. Es informativo evaluar los sistemas de recuperaciÃ³n utilizando nuestra medida de utilidad (con Î³ = 0) que tiene en cuenta la novedad y, por lo tanto, ofrece una imagen mÃ¡s realista de cuÃ¡n bien un sistema puede generalizar a partir de la retroalimentaciÃ³n del usuario, en lugar de utilizar medidas tradicionales de RI como la recuperaciÃ³n y la precisiÃ³n, que ofrecen una imagen incompleta de la mejora obtenida a partir de la retroalimentaciÃ³n del usuario. A veces, sin embargo, los usuarios podrÃ­an estar interesados en ver la misma informaciÃ³n de mÃºltiples fuentes, como un indicador de su importancia o fiabilidad. En tal caso, simplemente podemos elegir un valor mÃ¡s alto para Î³ que corresponda a una mayor tolerancia para la redundancia, y asÃ­ permitir que el sistema ajuste sus parÃ¡metros en consecuencia. Dado que los documentos fueron procesados por fragmentos, serÃ­a interesante ver cÃ³mo mejora el rendimiento de los sistemas con el tiempo. Las figuras 1 y 2 muestran las tendencias de rendimiento de ambos sistemas a lo largo de los fragmentos. Si bien se espera que el rendimiento con y sin retroalimentaciÃ³n en los primeros fragmentos sea similar, para los fragmentos posteriores, la curva de rendimiento con la retroalimentaciÃ³n habilitada supera a la del ajuste sin retroalimentaciÃ³n. Las tendencias de rendimiento no son consistentes en todos los fragmentos porque los documentos relevantes no estÃ¡n distribuidos uniformemente en todos los fragmentos, lo que hace que algunas consultas sean mÃ¡s fÃ¡ciles que otras en ciertos fragmentos. AdemÃ¡s, dado que Indri utiliza retroalimentaciÃ³n de pseudo-relevancia mientras que CAFÂ´E utiliza retroalimentaciÃ³n basada en juicios de relevancia reales, la mejora en el caso de Indri es menos dramÃ¡tica que la de CAFÂ´E. 7. CONCLUSIONES FINALES Este artÃ­culo presenta la primera investigaciÃ³n sobre la destilaciÃ³n de informaciÃ³n basada en la utilidad con un sistema que aprende las necesidades de informaciÃ³n duraderas a partir de la retroalimentaciÃ³n detallada del usuario sobre una secuencia de pasajes clasificados. Nuestro sistema, llamado CAFÂ´E, combina filtrado adaptativo, detecciÃ³n de novedades y clasificaciÃ³n de pasajes antirredundantes en un marco unificado para la optimizaciÃ³n de la utilidad. Desarrollamos un nuevo esquema para la evaluaciÃ³n y retroalimentaciÃ³n automatizadas basado en un procedimiento semiautomÃ¡tico para adquirir reglas que permitan emparejar automÃ¡ticamente fragmentos con las respuestas del sistema. TambiÃ©n propusimos una extensiÃ³n de la mÃ©trica NDCG para evaluar la utilidad de los pasajes clasificados como una combinaciÃ³n ponderada de relevancia y novedad. Nuestros experimentos en el reciÃ©n anotado corpus de referencia TDT4 muestran un alentador aumento de utilidad sobre Indri, y tambiÃ©n sobre nuestro propio sistema con aprendizaje incremental y detecciÃ³n de novedades desactivados. 8. AGRADECIMIENTOS Nos gustarÃ­a agradecer a Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng y al Programa de AnÃ¡lisis de Datos Cualitativos de la Universidad de Pittsburgh liderado por el Dr. Stuart Shulman por su ayuda en la recolecciÃ³n y procesamiento de las anotaciones extendidas de TDT4 utilizadas en nuestros experimentos. Este trabajo estÃ¡ parcialmente respaldado por la FundaciÃ³n Nacional de Ciencias (NSF) bajo la subvenciÃ³n IIS0434035, y la Agencia de Proyectos de InvestigaciÃ³n Avanzada de Defensa (DARPA) bajo los contratos NBCHD030010 y W0550432. Cualquier opiniÃ³n, hallazgo, conclusiÃ³n o recomendaciÃ³n expresada en este material son responsabilidad de los autores y no reflejan necesariamente los puntos de vista de los patrocinadores. AUTORES ADICIONALES Jian Zhang (jianzhan@stat.purdue.edu)â , Jaime Carbonell (jgc@cs.cmu.edu)â  , Peter Brusilovsky (peterb+@pitt.edu)â¡ , Daqing He(dah44@pitt.edu)â¡ 10. REFERENCIAS [1] J. Allan. RetroalimentaciÃ³n incremental de relevancia para la filtraciÃ³n de informaciÃ³n. Actas de la 19Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 270-278, 1996. [2] J. Allan, C. Wade y A. Bolivar. RecuperaciÃ³n y DetecciÃ³n de Novedad a Nivel de OraciÃ³n. Actas de la conferencia ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, 2003. [3] C. Buckley, G. Salton y J. Allan. RecuperaciÃ³n automÃ¡tica con informaciÃ³n de localidad utilizando SMART. PublicaciÃ³n especial del NIST, (500207):59-72, 1993. [4] J. Callan. Aprendiendo mientras se filtran documentos. Actas de la 21Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 224-231, 1998. [5] J. Carbonell y J. Goldstein. El uso de MMR, Reordenamiento basado en Diversidad para Reorganizar Documentos y Producir ResÃºmenes. Actas de la 21Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 335-336, 1998. [6] E. Efthimiadis. ExpansiÃ³n de consulta. RevisiÃ³n Anual de Ciencia de la InformaciÃ³n y TecnologÃ­a (ARIST), 31:p121-87, 1996. [7] J. Fiscus y G. Duddington. Resumen de DetecciÃ³n y Seguimiento de Temas. DetecciÃ³n y seguimiento de temas: OrganizaciÃ³n de la informaciÃ³n basada en eventos, pÃ¡ginas 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov y S. Roukos. Un modelo estadÃ­stico para la detecciÃ³n y seguimiento de entidades multilingÃ¼es. NAACL/HLT, 2004. [9] K. JÃ¤rvelin y J. KekÃ¤lÃ¤inen. EvaluaciÃ³n basada en la Ganancia Acumulada de TÃ©cnicas de RecuperaciÃ³n de InformaciÃ³n. ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin y D. Demner-Fushman. EvaluaciÃ³n automÃ¡tica de respuestas a preguntas de definiciÃ³n. Actas de la Conferencia de TecnologÃ­a del Lenguaje Humano y Conferencia sobre MÃ©todos EmpÃ­ricos en Procesamiento del Lenguaje Natural (HLT/EMNLP 2005), 2005. â Departamento de EstadÃ­stica, Universidad de Purdue, West Lafayette, EE. UU. â  Instituto de TecnologÃ­as del Lenguaje, Universidad Carnegie Mellon, Pittsburgh, EE. UU. â¡ Escuela de Ciencias de la InformaciÃ³n, Universidad de Pittsburgh, Pittsburgh, EE. UU. [11] J. Lin y D. Demner-Fushman. Â¿Se derrumbarÃ¡n las pirÃ¡mides construidas con pepitas? Actas de HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla y S. Roukos. Un algoritmo de resoluciÃ³n de correferencia sÃ­ncrona basado en el Ã¡rbol de Bell. Actas de ACL, 4:136-143, 2004. [13] G. Marton. Nuggeteer: EvaluaciÃ³n AutomÃ¡tica Basada en Pepitas Utilizando Descripciones y Juicios. HLT/NAACL, 2006. [14] E. Riloff. ConstrucciÃ³n automÃ¡tica de un diccionario para tareas de extracciÃ³n de informaciÃ³n. Actas de la UndÃ©cima Conferencia Nacional sobre Inteligencia Artificial, pÃ¡ginas 811-816, 1993. [15] S. Robertson y S. Walker. Microsoft Cambridge en TREC-9: Pista de filtrado. La Novena Conferencia de RecuperaciÃ³n de Texto (TREC-9), pÃ¡ginas 361-368. [16] R. Schapire, Y. Cantante, y A. Singhal. Boosting y Rocchio aplicados a la filtraciÃ³n de texto. Actas de la 21Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle y W. Croft. Indri: Un motor de bÃºsqueda basado en modelos de lenguaje para consultas complejas. Actas de la Conferencia Internacional sobre AnÃ¡lisis de Inteligencia, 2004. [18] El Consorcio de Datos LingÃ¼Ã­sticos. http://www.ldc.upenn.edu/. [19] E. Voorhees. Resumen de la pista de preguntas y respuestas TREC 2003. Actas de la DuodÃ©cima Conferencia de RecuperaciÃ³n de Texto (TREC 2003), 2003. [20] Y. Yang y B. Kisiel. RegresiÃ³n local basada en mÃ¡rgenes para filtrado adaptativo. Actas de la duodÃ©cima conferencia internacional sobre gestiÃ³n de la informaciÃ³n y el conocimiento, pÃ¡ginas 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang y B. Kisiel. Robustez de los MÃ©todos de Filtrado Adaptativo en una EvaluaciÃ³n de Referencia Cruzada. Actas de la 28Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 98-105, 2005. [22] C. Zhai, W. Cohen y J. Lafferty. MÃ¡s allÃ¡ de la relevancia independiente: MÃ©todos y mÃ©tricas de evaluaciÃ³n para la recuperaciÃ³n de subtemas. Actas de la 26Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 10-17, 2003. [23] J. Zhang y Y. Yang. Robustez de los MÃ©todos de ClasificaciÃ³n Lineal Regularizados en la CategorizaciÃ³n de Textos. Actas de la 26Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 190-197, 2003. [24] Y. Zhang. Utilizando priors bayesianos para combinar clasificadores para filtrado adaptativo. Actas de la 27Âª conferencia internacional anual sobre InvestigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 345-352, 2004. [25] Y. Zhang, J. Callan y T. Minka. DetecciÃ³n de novedad y redundancia en el filtrado adaptativo. Actas de la 25Âª Conferencia Internacional Anual de ACM SIGIR sobre InvestigaciÃ³n y Desarrollo en RecuperaciÃ³n de InformaciÃ³n, 2002. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "nugget-matching rule": {
            "translated_key": "reglas de coincidencia de pepitas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Utility-based Information Distillation Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu ABSTRACT This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework.",
                "It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant passage ranking with respect to long-lasting information needs (tasks with multiple queries).",
                "Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings.",
                "For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task.",
                "Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty.",
                "Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval.",
                "Adaptive filtering (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics.",
                "Based on the initial query and a few positive examples (if available), an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user.",
                "The incremental learning nature of AF systems makes them more powerful than standard search engines that support ad-hoc retrieval (e.g.",
                "Google and Yahoo) in terms of finding relevant information with respect to long-lasting topics of interest, and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs, without having to modify their queries manually.",
                "A variety of supervised learning algorithms (Rocchio-style classifiers, Exponential-Gaussian models, local regression and logistic regression approaches) have been studied for adaptive settings, examined with explicit and implicit relevance feedback, and evaluated with respect to utility optimization on large benchmark data collections in TREC (Text Retrieval Conferences) and TDT (Topic Detection and Tracking) forums [1, 4, 7, 15, 16, 20, 24, 23].",
                "Regularized logistic regression [21] has been found representative for the state-of-the-art approaches, and highly efficient for frequent model adaptations over large document collections such as the TREC-10 corpus (over 800,000 documents and 84 topics).",
                "Despite substantial achievements in recent adaptive filtering research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently.",
                "Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications: 1.",
                "User has a rather passive role in the conventional adaptive filtering setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision.",
                "A more active alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents (or passages) per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists.",
                "The latter form of user interaction has been highly effective in standard retrieval for ad-hoc queries.",
                "How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2.",
                "The unit for receiving a relevance judgment (yes or no) is restricted to the document level in conventional AF.",
                "However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant.",
                "Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system.",
                "For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3.",
                "System-selected documents are often highly redundant.",
                "A major news event, for example, would be reported by multiple sources repeatedly for a while, making most of the information content in those articles redundant with each other.",
                "A conventional AF system would select all these redundant news stories for user feedback, wasting the users time while offering little gain.",
                "Clearly, techniques for novelty detection can help in principle [25, 2, 22] for improving the utility of the AF systems.",
                "However, the effectiveness of such techniques at passage level to detect novelty with respect to users (fine-grained) feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user.",
                "To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and novelty detection for removal of redundancy from user interactions with the system.",
                "We call the new process utility-based information distillation.",
                "Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach.",
                "Therefore, we extended a benchmark corpus - the TDT4 collection of news stories and TV broadcasts - with task definitions, multiple queries per task, and answer keys per query.",
                "We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 .",
                "To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses.",
                "Moreover, we propose an extension of NDCG (Normalized Discounted Cumulated Gain) [9] for assessing the utility of ranked passages as a function of both relevance and novelty.",
                "The rest of this paper is organized as follows.",
                "Section 2 outlines the information distillation process with a concrete example.",
                "Section 3 describes the technical cores of our system called CAFÂ´E - CMU Adaptive Filtering Engine.",
                "Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme.",
                "Section 5 describes the extended TDT4 corpus.",
                "Section 6 presents our experiments and results.",
                "Section 7 concludes the study and gives future perspectives. 2.",
                "A SAMPLE TASK Consider a news event - the escape of seven convicts from a Texas prison in December 2000 and their capture a month later.",
                "Assuming a user were interested in this event since its early stage, the information need could be: Find information about the escape of convicts from Texas prison, and information related to their recapture.",
                "The associated lower-level questions could be: 1.",
                "How many prisoners escaped? 2.",
                "Where and when were they sighted? 3. Who are their known contacts inside and outside the prison? 4.",
                "How are they armed? 5.",
                "Do they have any vehicles? 6.",
                "What steps have been taken so far?",
                "We call such an information need a task, and the associated questions as the queries in this task.",
                "A distillation system is supposed to monitor the incoming documents, process them chunk by chunk in a temporal order, select potentially relevant and novel passages from each chunk with respect to each query, and present a ranked list of passages to the user.",
                "Passage ranking here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list.",
                "When presented with a list of passages, the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant.",
                "These spans of text are taken as positive examples in the adaptation of the query profile, and also added to the users history.",
                "Passages not marked by the user are taken as negative examples.",
                "As soon as the query profile is updated, the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low, based on user preference.",
                "For example, if the user highlights ...officials have posted a $100,000 reward for their capture... as relevant answer to the query What steps have been taken so far?, then the highlighted piece is used as an additional positive training example in the adaptation of the query profile.",
                "This piece of feedback is also added to the user history as a seen example, so that in future, the system will not place another passage mentioning $100,000 reward at the top of the ranked list.",
                "However, an article mentioning ...officials have doubled the reward money to $200,000... might be ranked high since it is both relevant to the (updated) query profile and novel with respect to the (updated) user history.",
                "The user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly.",
                "Clearly, novelty detection is very important for the utility of such a system because of the iterative search.",
                "Without novelty detection, the old relevant passages would be shown to the user repeatedly in each ranked list.",
                "Through the above example, we can see the main properties of our new framework for utility-based information distillation over temporally ordered documents.",
                "Our framework combines and extends the power of adaptive filtering (AF), ad-hoc retrieval (IR) and novelty detection (ND).",
                "Compared to standard IR, our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task.",
                "Compared to conventional AF, it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system.",
                "Compared to past work, this is the first evaluation of novelty detection integrated with adaptive filtering for sequenced queries that allows flexible user feedback over ranked passages.",
                "The combination of AF, IR and ND with the new extensions raises an important research question regarding evaluation methodology: how can we measure the utility of such an information distillation system?",
                "Existing metrics in standard IR, AF and ND are insufficient, and new solutions must be explored, as we will discuss in Section 4, after describing the technical cores of our system in the next section. 3.",
                "TECHNICAL CORES The core components of CAFÂ´E are - 1) AF for incremental learning of query profiles, 2) IR for estimating relevance of passages with respect to query profiles, 3) ND for assessing novelty of passages with respect to users history, and 4) anti-redundancy component to remove redundancy from ranked lists. 3.1 Adaptive Filtering Component We use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for AF [21].",
                "Logistic regression (LR) is a supervised learning algorithm for statistical classification.",
                "Based on a training set of labeled instances, it learns a class model which can then by used to predict the labels of unseen instances.",
                "Its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required, as is the case in adaptive filtering, where the system must learn from each new feedback provided by the user. (See [21] and [23] for computational complexity and implementation issues).",
                "In adaptive filtering, each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query.",
                "For training the model, we use the query itself as the initial positive training example of the class, and the user-highlighted pieces of text (marked as Relevant or Not-relevant) during feedback as additional training examples.",
                "To address the cold start issue in the early stage before any user feedback is obtained, the system uses a small sample from a retrospective corpus as the initial negative examples in the training set.",
                "The details of using logistic regression for adaptive filtering (assigning different weights to positive and negative training instances, and regularizing the objective function to prevent over-fitting on training data) are presented in [21].",
                "The class model wâ learned by Logistic Regression, or the query profile, is a vector whose dimensions are individual terms and whose elements are the regression coefficients, indicating how influential each term is in the query profile.",
                "The query profile is updated whenever a new piece of user feedback is received.",
                "A temporally decaying weight can be applied to each training example, as an option, to emphasize the most recent user feedback. 3.2 Passage Retrieval Component We use standard IR techniques in this part of our system.",
                "Incoming documents are processed in chunks, where each chunk can be defined as a fixed span of time or as a fixed number of documents, as preferred by the user.",
                "For each incoming document, corpus statistics like the IDF (Inverted Document Frequency) of each term are updated.",
                "We use a state-of-the-art named entity identifier and tracker [8, 12] to identify person and location names, and merge them with co-referent named entities seen in the past.",
                "Then the documents are segmented into passages, which can be a whole document, a paragraph, a sentence, or any other continuous span of text, as preferred.",
                "Each passage is represented using a vector of TF-IDF (Term FrequencyInverse Document Frequency) weights, where term can be a word or a named entity.",
                "Given a query profile, i.e. the logistic regression solution wâ as described in Section 3.1, the system computes the posterior probability of relevance for each passage x as fRL(x) â¡ P(y = 1|x, wâ ) = 1 (1 + eâwâÂ·x) (1) Passages are ordered by their relevance scores, and the ones with scores above a threshold (tuned on a training set) comprise the relevance list that is passed on to the novelty detection step. 3.3 Novelty Detection Component CAFÂ´E maintains a user history H(t), which contains all the spans of text hi that the user highlighted (as feedback) during his or her past interactions with the system, up to the current time t. Denoting the history as H(t) = n h1, h2, ..., ht o , (2) the novelty score of a new candidate passage x is computed as: fND(x) = 1 â max iâ1..t {cos(x, hi)} (3) where both candidate passage x and highlighted spans of text hi are represented as TF-IDF vectors.",
                "The novelty score of each passage is compared to a prespecified threshold (also tuned on a training set), and any passage with a score below this threshold is removed from the relevance list. 3.4 Anti-redundant Ranking Component Although the novelty detection component ensures that only novel (previously unseen) information remains in the relevance list, this list might still contain the same novel information at multiple positions in the ranked list.",
                "Suppose, for example, that the user has already read about a $100,000 reward for information about the escaped convicts.",
                "A new piece of news that the award has been increased to $200,000 is novel since the user hasnt read about it yet.",
                "However, multiple news sources would report this news and we might end up showing (redundant) articles from all these sources in a ranked list.",
                "Hence, a ranked list should also be made non-redundant with respect to its own contents.",
                "We use a simplified version of the Maximal Marginal Relevance method [5], originally developed for combining relevance and novelty in text retrieval and summarization.",
                "Our procedure starts with the current list of passages sorted by relevance (section 3.2), filtered by Novelty Detection component (section 3.3), and generates a new non-redundant list as follows: 1.",
                "Take the top passage in the current list as the top one in the new list. 2.",
                "Add the next passage x in the current list to the new list only if fAR(x) > t where fAR(x) = 1 â max piâLnew {cos(x, pi)} and Lnew is the set of passages already selected in the new list. 3.",
                "Repeat step 2 until all the passages in the current list have been examined.",
                "After applying the above-mentioned algorithm, each passage in the new list is sufficiently dissimilar to others, thus favoring diversity rather than redundancy in the new ranked list.",
                "The anti-redundancy threshold t is tuned on a training set. 4.",
                "EVALUATION METHODOLOGY The approach we proposed above for information distillation raises important issues regarding evaluation methodology.",
                "Firstly, since our framework allows the output to be passages at different levels of granularity (e.g. k-sentence windows where k may vary) instead of a fixed length, it is not possible to have pre-annotated relevance judgments at all such granularity levels.",
                "Secondly, since we wish to measure the utility of the system output as a combination of both relevance and novelty, traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time.",
                "Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists.",
                "None of the existing measures in ad-hoc retrieval, adaptive filtering, novelty detection or other related areas (text summarization and question answering) have desirable properties in all the three aspects.",
                "Therefore, we must develop a new evaluation methodology. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers.",
                "Answer keys define what should be present in a system response to receive credit, and are comprised of a collection of information nuggets, i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them.",
                "Defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping [19], since system-returned passages can contain the same information expressed in many different ways.",
                "Hence, QA evaluations have relied on human assessors for the mapping between various expressions, making the process costly, time consuming, and not scalable to large query and document collections, and extensive system evaluations with various parameter settings. 4.1.1 Automating Evaluation based on Answer Keys Automatic evaluation methods would allow for faster system building and tuning, as well as provide an objective and affordable way of comparing various systems.",
                "Recently, such methods have been proposed, more or less, based on the idea of n-gram co-occurrences.",
                "Pourpre [10] assigns a fractional recall score to a system response based on its unigram overlap with a given nuggets description.",
                "For example, a system response A B C has recall 3/4 with respect to a nugget with description A B C D. However, such an approach is unfair to systems that present the same information but using words other than A, B, C, and D. Another open issue is how to weight individual words in measuring the closeness of a match.",
                "For example, consider the question How many prisoners escaped?.",
                "In the nugget Seven prisoners escaped from a Texas prison, there is no indication that seven is the keyword, and that it must be matched to get any relevance credit.",
                "Using IDF values does not help, since seven will generally not have a higher IDF than words like texas and prison.",
                "Also, redefining the nugget as just seven does not solve the problem since now it might spuriously match any mention of seven out of context.",
                "Nuggeteer [13] works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold.",
                "However, it is also plagued by spurious relevance since not all words contained in the nugget description (or known correct responses) are central to the nugget. 4.1.2 Nugget-Matching Rules We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on <br>nugget-matching rule</br>s, which are generated using a semiautomatic procedure explained below.",
                "These rules are essentially Boolean queries that will only match against snippets that contain the nugget.",
                "For instance, a candidate rule for matching answers to How many prisoners escaped? is (Texas AND seven AND escape AND (convicts OR prisoners)), possibly with other synonyms and variants in the rule.",
                "For a corpus of news articles, which usually follow a typical formal prose, it is fairly easy to write such simple rules to match expected answers using a bootstrap approach, as described below.",
                "We propose a two-stage approach, inspired by Autoslog [14], that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents.",
                "In the first stage, human subjects annotated (using a highlighting tool) portions of ontopic documents that contained answers to each nugget 2 .",
                "In the second stage, subjects used our rule generation tool to create rules that would match the annotations for each nugget.",
                "The tool allows users to enter a Boolean rule as a disjunction of conjunctions (e.g. ((a AND b) OR (a AND c AND d) OR (e))).",
                "Given a candidate rule, our tool uses it as a Boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match.",
                "Hence, the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision.",
                "We observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions (adding more ANDs), and improving the recall by adding more conjunctions to the disjunction (adding more ORs).",
                "As an example, lets try to create a rule for the nugget which says that seven prisoners escaped from the Texas prison.",
                "We start with a simple rule - (seven).",
                "When we input this into the rule generation tool, we realize that this rule matches many spurious occurrences of seven (e.g. ...seven states...) and thus gets a low precision score.",
                "We can further qualify our rule - Texas AND seven AND convicts.",
                "Next, by looking at the missed annotations, we realize that some news articles mentioned ...seven prisoners escaped.... We then replace convicts with the disjunction (convicts OR prisoners).",
                "We continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the (small number of) misses and false alarms can be safely ignored.",
                "Thus we can create <br>nugget-matching rule</br>s that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses.",
                "Human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably. 4.2 Evaluating the Utility of a Sequence of Ranked Lists The utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information, and how much the user lost in terms of time and energy.",
                "We calculate this utility from the utilities of individual passages as follows.",
                "After reading each passage returned by the system, the user derives some gain depending on the presence of relevant and novel information, and incurs a loss in terms of the time and energy spent in going through the passage.",
                "However, the likelihood that the user would actually read a passage depends on its position in the ranked list.",
                "Hence, for a query q, the expected utility 2 LDC [18] already provides relevance judgments for 100 topics on the TDT4 corpus.",
                "We further ensured that these judgments are exhaustive on the entire corpus using pooling. of a passage pi at rank i can be defined as U(pi, q) = P(i) â (Gain(pi, q) â Loss(pi, q)) (4) where P(i) is the probability that the user would go through a passage at rank i.",
                "The expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage: U(q) = nX i=1 P(i) â (Gain(pi, q) â Loss(pi, q)) (5) Note that if we ignore the loss term and define P(i) as P(i) â 1/ logb(b + i â 1) (6) then we get the recently popularized metric called Discounted Cumulated Gain (DCG) [9], where Gain(pi, q) is defined as the graded relevance of passage pi.",
                "However, without the loss term, DCG is a purely recall-oriented metric and not suitable for an adaptive filtering setting, where the systems utility depends in part on its ability to limit the number of items shown to the user.",
                "Although P(i) could be defined based on empirical studies of user behavior, for simplicity, we use P(i) exactly as defined in equation 6.",
                "The gain G(pi, q) of passage pi with respect to the query q is a function of - 1) the number of relevant nuggets present in pi, and 2) the novelty of each of these nuggets.",
                "We combine these two factors as follows.",
                "For each nugget Nj, we assign an initial weight wj, and also keep a count nj of the number of times this nugget has been seen by the user in the past.",
                "The gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor Î³.",
                "Thus, G(pi, q) is defined as G(pi, q) = X Nj âC(pi,q) wj â Î³nj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the answer key of query q.",
                "The initial weights wj are all set of be 1.0 in our experiments, but can also be set based on a pyramid approach [11].",
                "The choice of dampening factor Î³ determines the users tolerance for redundancy.",
                "When Î³ = 0, a nugget will only receive credit for its first occurrence i.e. when nj is zero3 .",
                "For 0 < Î³ < 1, a nugget receives smaller credit for each successive occurrence.",
                "When Î³ = 1, no dampening occurs and repeated occurrences of a nugget receive the same credit.",
                "Note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system, since the users are expected to remember what the system showed them in the past.",
                "We define the loss L(pi, q) as a constant cost c (we use 0.1) incurred when reading a system-returned passage.",
                "Thus, our metric can be re-written as U(q) = nX i=1 Gain(pi, q) logb(b + i â 1) â L(n) (8) where L(n) is the loss associated with a ranked list of length n: L(n) = c Â· nX i=1 1 logb(b + i â 1) (9) 3 Note that 00 = 1 Due to the similarity with Discounted Cumulated Gain (DCG), we call our metric Discounted Cumulated Utility (DCU).",
                "The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) â¤ 0 i.e. when the gain is less than or equal to the cost of reading the passage. 5.",
                "DATA TDT4 was the benchmark corpus used in TDT2002 and TDT2003 evaluations.",
                "The corpus consists of over 90, 000 news articles from multiple sources (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) published between October 2000 and January 2001, in the languages of Arabic, English, and Mandarin.",
                "Speech-recognized and machine-translated versions of the non-English articles were provided as well.",
                "LDC [18] has annotated the corpus with 100 topics, that correspond to various news events in this time period.",
                "Out of these, we selected a subset of 12 actionable events, and defined corresponding tasks for them4 .",
                "For each task, we manually defined a profile consisting of an initial set of (5 to 10) queries, a free-text description of the user history, i.e., what the user already knows about the event, and a list of known on-topic and off-topic documents (if available) as training examples.",
                "For each query, we generated answer keys and corresponding nugget matching rules using the procedure described in section 4.1.2, and produced a total of 120 queries, with an average of 7 nuggets per query. 6.",
                "EXPERIMENTS AND RESULTS 6.1 Baselines We used Indri [17], a popular language-model based retrieval engine, as a baseline for comparison with CAFÂ´E.",
                "Indri supports standard search engine functionality, including pseudo-relevance feedback (PRF) [3, 6], and is representative of a typical query-based retrieval system.",
                "Indri does not support any kind of novelty detection.",
                "We compare Indri with PRF turned on and off, against CAFÂ´E with user feedback, novelty detection and antiredundant ranking turned on and off. 6.2 Experimental Setup We divided the TDT4 corpus spanning 4 months into 10 chunks, each defined as a period of 12 consecutive days.",
                "At any given point of time in the distillation process, each system accessed the past data up to the current point, and returned a ranked list of up 50 passages per query.",
                "The 12 tasks defined on the corpus were divided into a training and test set with 6 tasks each.",
                "Each system was allowed to use the training set to tune its parameters for optimizing NDCU (equation 8), including the relevance threshold for both Indri and CAFÂ´E, and the novelty and antiredundancy thresholds for CAFÂ´E.",
                "The NDCU for each system run is calculated automatically.",
                "User feedback was also simulated - relevance judgments for each system-returned passage (as determined by the nugget matching rules described in section 4.1.2) were 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figure 1: Performance of Indri across chunks Figure 2: Performance of CAFÂ´E across chunks used as user feedback in the adaptation of query profiles and user histories. 6.3 Results In Table 1, we show the NDCU scores of the two systems under various settings.",
                "These scores are averaged over the six tasks in the test set, and are calculated with two dampening factors (see section 4.2): Î³ = 0 and 0.1, to simulate no tolerance and small tolerance for redundancy, respectively.",
                "Using Î³ = 0 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information.",
                "Hence, the improvement obtained from enabling user feedback is smaller with Î³ = 0 than the improvement obtained from feedback with Î³ = 0.1.",
                "This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information.",
                "However, the user does not benefit from seeing such redundant passages, and is usually interested in other passages containing related information.",
                "It is informative to evaluate retrieval systems using our utility measure (with Î³ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.",
                "Sometimes, however, users might indeed be interested in seeing the same information from multiple sources, as an Table 1: NDCU Scores of Indri and CAFÂ´E for two dampening factors (Î³), and various settings (F: Feedback, N: Novelty Detection, A: Anti-Redundant Ranking) Indri CAFÂ´E Î³ Base +PRF Base +F +F+N +F+A +F+N+A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.35 0.36 0.36 indicator of its importance or reliability.",
                "In such a case, we can simply choose a higher value for Î³ which corresponds to a higher tolerance for redundancy, and hence let the system tune its parameters accordingly.",
                "Since documents were processed chunk by chunk, it would be interesting to see how the performance of systems improves over time.",
                "Figures 1 and 2 show the performance trends for both the systems across chunks.",
                "While the performance with and without feedback on the first few chunks is expected to be close, for subsequent chunks, the performance curve with feedback enabled rises above the one with the no-feedback setting.",
                "The performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks, making some queries easier than others in certain chunks.",
                "Moreover, since Indri uses pseudo-relevance feedback while CAFÂ´E uses feedback based on actual relevance judgments, the improvement in case of Indri is less dramatic than that of CAFÂ´E. 7.",
                "CONCLUDING REMARKS This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages.",
                "Our system, called CAFÂ´E, combines adaptive filtering, novelty detection and antiredundant passage ranking in a unified framework for utility optimization.",
                "We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty.",
                "Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off. 8.",
                "ACKNOWLEDGMENTS We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments.",
                "This work is supported in parts by the National Science Foundation (NSF) under grant IIS0434035, and the Defense Advanced Research Project Agency (DARPA) under contracts NBCHD030010 and W0550432.",
                "Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. 9.",
                "ADDITIONAL AUTHORS Jian Zhang (jianzhan@stat.purdue.edu)â , Jaime Carbonell (jgc@cs.cmu.edu)â  , Peter Brusilovsky (peterb+@pitt.edu)â¡ , Daqing He(dah44@pitt.edu)â¡ 10.",
                "REFERENCES [1] J. Allan.",
                "Incremental Relevance Feedback for Information Filtering.",
                "Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 270-278, 1996. [2] J. Allan, C. Wade, and A. Bolivar.",
                "Retrieval and Novelty Detection at the Sentence Level.",
                "Proceedings of the ACM SIGIR conference on research and development in information retrieval, 2003. [3] C. Buckley, G. Salton, and J. Allan.",
                "Automatic Retrieval with Locality Information using SMART.",
                "NIST special publication, (500207):59-72, 1993. [4] J. Callan.",
                "Learning While Filtering Documents.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 224-231, 1998. [5] J. Carbonell and J. Goldstein.",
                "The use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335-336, 1998. [6] E. Efthimiadis.",
                "Query Expansion.",
                "Annual Review of Information Science and Technology (ARIST), 31:p121-87, 1996. [7] J. Fiscus and G. Duddington.",
                "Topic Detection and Tracking Overview.",
                "Topic Detection and Tracking: Event-based Information Organization, pages 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.",
                "A Statistical Model for Multilingual Entity Detection and Tracking.",
                "NAACL/HLT, 2004. [9] K. JÂ¨arvelin and J. KekÂ¨alÂ¨ainen.",
                "Cumulated Gain-based Evaluation of IR Techniques.",
                "ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin and D. Demner-Fushman.",
                "Automatically Evaluating Answers to Definition Questions.",
                "Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), 2005. â Statistics Dept., Purdue University, West Lafayette, USA â  Language Technologies Inst., Carnegie Mellon University, Pittsburgh, USA â¡ School of Information Sciences, Univ. of Pittsburgh, Pittsburgh, USA [11] J. Lin and D. Demner-Fushman.",
                "Will Pyramids Built of nUggets Topple Over.",
                "Proceedings of HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.",
                "A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree.",
                "Proc. of ACL, 4:136-143, 2004. [13] G. Marton.",
                "Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments.",
                "HLT/NAACL, 2006. [14] E. Riloff.",
                "Automatically Constructing a Dictionary for Information Extraction Tasks.",
                "Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, 1993. [15] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9: Filtering track.",
                "The Ninth Text REtrieval Conference (TREC-9), pages 361-368. [16] R. Schapire, Y.",
                "Singer, and A. Singhal.",
                "Boosting and Rocchio Applied to Text Filtering.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle, and W. Croft.",
                "Indri: A Language Model-based Search Engine for Complex Queries.",
                "Proceedings of the International Conference on Intelligence Analysis, 2004. [18] The Linguistic Data Consortium. http://www.ldc.upenn.edu/. [19] E. Voorhees.",
                "Overview of the TREC 2003 Question Answering Track.",
                "Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), 2003. [20] Y. Yang and B. Kisiel.",
                "Margin-based Local Regression for Adaptive Filtering.",
                "Proceedings of the twelfth international conference on Information and knowledge management, pages 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of Adaptive Filtering Methods in a Cross-benchmark Evaluation.",
                "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 98-105, 2005. [22] C. Zhai, W. Cohen, and J. Lafferty.",
                "Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-17, 2003. [23] J. Zhang and Y. Yang.",
                "Robustness of Regularized Linear Classification Methods in Text Categorization.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 190-197, 2003. [24] Y. Zhang.",
                "Using Bayesian Priors to Combine Classifiers for Adaptive Filtering.",
                "Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 345-352, 2004. [25] Y. Zhang, J. Callan, and T. Minka.",
                "Novelty and Redundancy Detection in Adaptive Filtering.",
                "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002."
            ],
            "original_annotated_samples": [
                "However, it is also plagued by spurious relevance since not all words contained in the nugget description (or known correct responses) are central to the nugget. 4.1.2 Nugget-Matching Rules We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on <br>nugget-matching rule</br>s, which are generated using a semiautomatic procedure explained below.",
                "Thus we can create <br>nugget-matching rule</br>s that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses."
            ],
            "translated_annotated_samples": [
                "Sin embargo, tambiÃ©n estÃ¡ plagado de relevancia espuria ya que no todas las palabras contenidas en la descripciÃ³n de la pepita (o respuestas correctas conocidas) son centrales para la pepita. 4.1.2 Reglas de Coincidencia de Pepitas Proponemos un mÃ©todo automÃ¡tico confiable para determinar si un fragmento de texto contiene una pepita dada, basado en <br>reglas de coincidencia de pepitas</br>, que se generan utilizando un procedimiento semiautomÃ¡tico explicado a continuaciÃ³n.",
                "AsÃ­ podemos crear <br>reglas de coincidencia de pepitas</br> que capturen de manera sucinta varias formas de expresar una pepita, evitando al mismo tiempo emparejar respuestas incorrectas (o fuera de contexto)."
            ],
            "translated_text": "DestilaciÃ³n de informaciÃ³n basada en utilidad sobre documentos secuenciados temporalmente en el Instituto de TecnologÃ­as del Lenguaje de Yiming Yang. Universidad Carnegie Mellon, Pittsburgh, EE. UU. yiming@cs.cmu.edu Abhimanyu Lad Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. alad@cs.cmu.edu Instituto de TecnologÃ­as del Lenguaje Ni Lao. Universidad Carnegie Mellon, Pittsburgh, EE. UU. nlao@cs.cmu.edu Abhay Harpale Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. aharpale@cs.cmu.edu Bryan Kisiel Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. bkisiel@cs.cmu.edu MÃ³nica Rogati Instituto de TecnologÃ­as del Lenguaje. La Universidad Carnegie Mellon en Pittsburgh, EE. UU. mrogati@cs.cmu.edu RESUMEN Este artÃ­culo examina un nuevo enfoque para la destilaciÃ³n de informaciÃ³n en documentos ordenados temporalmente, y propone un esquema de evaluaciÃ³n novedoso para dicho marco. Combina las fortalezas y se extiende mÃ¡s allÃ¡ del filtrado adaptativo convencional, la detecciÃ³n de novedades y la clasificaciÃ³n de pasajes no redundantes con respecto a las necesidades de informaciÃ³n duraderas (tareas con mÃºltiples consultas). Nuestro enfoque respalda la retroalimentaciÃ³n detallada del usuario a travÃ©s del resaltado de fragmentos arbitrarios de texto, y aprovecha dicha informaciÃ³n para la optimizaciÃ³n de la utilidad en entornos adaptativos. Para nuestros experimentos, definimos tareas hipotÃ©ticas basadas en eventos de noticias en el corpus TDT4, con mÃºltiples consultas por tarea. Se generaron claves de respuestas (nuggets) para cada consulta y se utilizÃ³ un procedimiento semiautomÃ¡tico para adquirir reglas que permiten emparejar automÃ¡ticamente los nuggets con las respuestas del sistema. TambiÃ©n proponemos una extensiÃ³n de la mÃ©trica NDCG para evaluar la utilidad de los pasajes clasificados como una combinaciÃ³n de relevancia y novedad. Nuestros resultados muestran mejoras alentadoras en la utilidad utilizando el nuevo enfoque, en comparaciÃ³n con los sistemas base sin aprendizaje incremental o componentes de detecciÃ³n de novedades. CategorÃ­as y Descriptores de Asignaturas H.3.3 [BÃºsqueda y RecuperaciÃ³n de InformaciÃ³n]: Filtrado de informaciÃ³n, RetroalimentaciÃ³n de relevancia, Modelos de recuperaciÃ³n, Proceso de selecciÃ³n; I.5.2 TÃ©rminos Generales DiseÃ±o, MediciÃ³n, Rendimiento, ExperimentaciÃ³n. 1. INTRODUCCIÃN Seguir informaciÃ³n nueva y relevante de flujos de datos temporales para usuarios con necesidades duraderas ha sido un tema de investigaciÃ³n desafiante en la recuperaciÃ³n de informaciÃ³n. El filtrado adaptativo (AF) es una tarea de predicciÃ³n en lÃ­nea de la relevancia de cada nuevo documento con respecto a temas predefinidos. BasÃ¡ndose en la consulta inicial y algunos ejemplos positivos (si estÃ¡n disponibles), un sistema de AF mantiene un perfil para cada tema de interÃ©s, y lo actualiza constantemente segÃºn la retroalimentaciÃ³n del usuario. La naturaleza de aprendizaje incremental de los sistemas de AF los hace mÃ¡s poderosos que los motores de bÃºsqueda estÃ¡ndar que admiten recuperaciÃ³n ad-hoc (por ejemplo. Google y Yahoo) en tÃ©rminos de encontrar informaciÃ³n relevante con respecto a temas de interÃ©s duraderos, y mÃ¡s atractivo para los usuarios que estÃ¡n dispuestos a proporcionar retroalimentaciÃ³n para adaptar el sistema a sus necesidades de informaciÃ³n especÃ­ficas, sin tener que modificar sus consultas manualmente. Se han estudiado una variedad de algoritmos de aprendizaje supervisado (clasificadores de estilo Rocchio, modelos exponenciales-gaussianos, regresiÃ³n local y enfoques de regresiÃ³n logÃ­stica) para entornos adaptativos, examinados con retroalimentaciÃ³n de relevancia explÃ­cita e implÃ­cita, y evaluados con respecto a la optimizaciÃ³n de la utilidad en grandes colecciones de datos de referencia en los foros de TREC (Conferencias de RecuperaciÃ³n de Texto) y TDT (DetecciÃ³n y Seguimiento de Temas) [1, 4, 7, 15, 16, 20, 24, 23]. La regresiÃ³n logÃ­stica regularizada [21] se ha encontrado representativa para los enfoques de vanguardia, y altamente eficiente para adaptaciones frecuentes del modelo en grandes colecciones de documentos como el corpus TREC-10 (mÃ¡s de 800,000 documentos y 84 temas). A pesar de los logros sustanciales en la investigaciÃ³n reciente de filtrado adaptativo, aÃºn quedan problemas significativos sin resolver sobre cÃ³mo aprovechar de manera efectiva y eficiente la retroalimentaciÃ³n del usuario. EspecÃ­ficamente, los siguientes problemas pueden limitar seriamente la verdadera utilidad de los sistemas de AF en aplicaciones del mundo real: 1. El usuario tiene un papel bastante pasivo en la configuraciÃ³n convencional de filtrado adaptativo: solo reacciona ante el sistema cuando este toma una decisiÃ³n afirmativa sobre un documento, confirmando o rechazando esa decisiÃ³n. Una alternativa mÃ¡s activa serÃ­a permitir al usuario emitir mÃºltiples consultas sobre un tema, revisar una lista clasificada de documentos candidatos (o pasajes) por consulta, y proporcionar retroalimentaciÃ³n sobre la lista clasificada, refinando asÃ­ su necesidad de informaciÃ³n y solicitando listas clasificadas actualizadas. La Ãºltima forma de interacciÃ³n del usuario ha sido altamente efectiva en la recuperaciÃ³n estÃ¡ndar para consultas ad-hoc. CÃ³mo implementar una estrategia para necesidades de informaciÃ³n duraderas en entornos de FA es una pregunta abierta para la investigaciÃ³n. 2. La unidad para recibir un juicio de relevancia (sÃ­ o no) estÃ¡ restringida al nivel del documento en AF convencional. Sin embargo, un usuario real puede estar dispuesto a proporcionar comentarios mÃ¡s informativos y detallados resaltando algunas partes de texto en un documento recuperado como relevantes, en lugar de etiquetar todo el documento como relevante. Aprovechar de manera efectiva este tipo de retroalimentaciÃ³n detallada podrÃ­a mejorar sustancialmente la calidad de un sistema de AF. Para esto, necesitamos habilitar el aprendizaje supervisado a partir de fragmentos de texto etiquetados de longitud arbitraria en lugar de solo permitir documentos etiquetados. 3. Los documentos seleccionados por el sistema suelen ser altamente redundantes. Un evento importante de noticias, por ejemplo, serÃ­a reportado por mÃºltiples fuentes repetidamente durante un tiempo, haciendo que la mayor parte del contenido informativo en esos artÃ­culos sea redundante entre sÃ­. Un sistema de AF convencional seleccionarÃ­a todas estas historias de noticias redundantes para la retroalimentaciÃ³n del usuario, desperdiciando el tiempo de los usuarios mientras ofrece poco beneficio. Claramente, las tÃ©cnicas de detecciÃ³n de novedades pueden ayudar en principio [25, 2, 22] a mejorar la utilidad de los sistemas AF. Sin embargo, la efectividad de tales tÃ©cnicas a nivel de pasaje para detectar novedad con respecto a la retroalimentaciÃ³n de los usuarios a nivel detallado y para detectar redundancia en listas clasificadas aÃºn debe ser evaluada utilizando una medida de utilidad que imite las necesidades de un usuario real. Para abordar las limitaciones actuales de los sistemas de AF mencionadas anteriormente, proponemos y examinamos un nuevo enfoque en este artÃ­culo, que combina las fortalezas del AF convencional (aprendizaje incremental de modelos de temas), recuperaciÃ³n de pasajes multipase para consultas de larga duraciÃ³n condicionadas al tema, y detecciÃ³n de novedades para eliminar la redundancia de las interacciones del usuario con el sistema. Llamamos al nuevo proceso destilaciÃ³n de informaciÃ³n basada en utilidad. Ten en cuenta que los corpus de referencia convencionales para evaluaciones de AF, que tienen juicios de relevancia a nivel de documento y no definen tareas con mÃºltiples consultas, son insuficientes para evaluar el nuevo enfoque. Por lo tanto, ampliamos un corpus de referencia, la colecciÃ³n TDT4 de noticias y transmisiones de televisiÃ³n, con definiciones de tareas, mÃºltiples consultas por tarea y claves de respuestas por consulta. Hemos realizado nuestros experimentos en este corpus extendido TDT4 y hemos puesto a disposiciÃ³n del pÃºblico los datos generados adicionalmente para futuras evaluaciones comparativas. Para evaluar automÃ¡ticamente los fragmentos de texto arbitrarios devueltos por el sistema utilizando nuestras claves de respuestas, desarrollamos un esquema de evaluaciÃ³n con un procedimiento semiautomÃ¡tico para 1 URL: http://nyc.lti.cs.cmu.edu/downloads adquiriendo reglas que pueden comparar fragmentos con las respuestas del sistema. AdemÃ¡s, proponemos una extensiÃ³n de NDCG (Normalized Discounted Cumulated Gain) [9] para evaluar la utilidad de los pasajes clasificados como una funciÃ³n tanto de la relevancia como de la novedad. El resto de este documento estÃ¡ organizado de la siguiente manera. La secciÃ³n 2 describe el proceso de destilaciÃ³n de informaciÃ³n con un ejemplo concreto. La secciÃ³n 3 describe los nÃºcleos tÃ©cnicos de nuestro sistema llamado CAFÂ´E - CMU Adaptive Filtering Engine. La secciÃ³n 4 discute problemas con respecto a la metodologÃ­a de evaluaciÃ³n y propone un nuevo esquema. La secciÃ³n 5 describe el corpus extendido TDT4. La secciÃ³n 6 presenta nuestros experimentos y resultados. La secciÃ³n 7 concluye el estudio y ofrece perspectivas futuras. 2. Una TAREA DE EJEMPLO Considera un evento noticioso: la fuga de siete convictos de una prisiÃ³n de Texas en diciembre de 2000 y su captura un mes despuÃ©s. Suponiendo que un usuario estuviera interesado en este evento desde sus primeras etapas, la necesidad de informaciÃ³n podrÃ­a ser: Encontrar informaciÃ³n sobre la fuga de convictos de una prisiÃ³n de Texas, y la informaciÃ³n relacionada con su recaptura. Las preguntas de nivel inferior asociadas podrÃ­an ser: 1. Â¿CuÃ¡ntos prisioneros escaparon? 2. Â¿DÃ³nde y cuÃ¡ndo fueron avistados? 3. Â¿QuiÃ©nes son sus contactos conocidos dentro y fuera de la prisiÃ³n? 4. Â¿CÃ³mo estÃ¡n armados? 5. Â¿Tienen algÃºn vehÃ­culo? 6. Â¿QuÃ© pasos se han tomado hasta ahora? Llamamos a esta necesidad de informaciÃ³n una tarea, y a las preguntas asociadas como las consultas en esta tarea. Un sistema de destilaciÃ³n debe monitorear los documentos entrantes, procesarlos por fragmentos en un orden temporal, seleccionar pasajes potencialmente relevantes y novedosos de cada fragmento con respecto a cada consulta, y presentar una lista clasificada de pasajes al usuario. La clasificaciÃ³n de pasajes aquÃ­ se basa en cuÃ¡n relevante es un pasaje con respecto a la consulta actual, cuÃ¡n novedoso es con respecto al historial actual del usuario (de sus interacciones con el sistema) y cuÃ¡n redundante es en comparaciÃ³n con otros pasajes con un rango mÃ¡s alto en la lista. Cuando se le presenta una lista de pasajes, el usuario puede proporcionar retroalimentaciÃ³n resaltando tramos de texto arbitrarios que considere relevantes. Estos fragmentos de texto se toman como ejemplos positivos en la adaptaciÃ³n del perfil de consulta, y tambiÃ©n se aÃ±aden al historial de los usuarios. Los pasajes no marcados por el usuario se consideran ejemplos negativos. Tan pronto como se actualiza el perfil de consulta, el sistema vuelve a realizar una bÃºsqueda y devuelve otra lista clasificada de pasajes donde los pasajes previamente vistos son eliminados o clasificados como bajos, segÃºn la preferencia del usuario. Por ejemplo, si el usuario resalta ...los funcionarios han ofrecido una recompensa de $100,000 por su captura... como respuesta relevante a la consulta Â¿QuÃ© pasos se han tomado hasta ahora?, entonces la pieza resaltada se utiliza como un ejemplo de entrenamiento positivo adicional en la adaptaciÃ³n del perfil de la consulta. Este comentario tambiÃ©n se aÃ±ade al historial del usuario como un ejemplo visto, para que en el futuro, el sistema no coloque otro pasaje mencionando una recompensa de $100,000 en la parte superior de la lista clasificada. Sin embargo, un artÃ­culo que mencione que los funcionarios han duplicado el dinero de recompensa a $200,000 podrÃ­a clasificarse alto ya que es relevante tanto para el perfil de consulta (actualizado) como novedoso con respecto a la historia del usuario (actualizada). El usuario puede modificar las consultas originales o agregar una nueva consulta durante el proceso; los perfiles de consulta se cambiarÃ¡n en consecuencia. Claramente, la detecciÃ³n de novedades es muy importante para la utilidad de dicho sistema debido a la bÃºsqueda iterativa. Sin detecciÃ³n de novedad, los antiguos pasajes relevantes se mostrarÃ­an al usuario repetidamente en cada lista clasificada. A travÃ©s del ejemplo anterior, podemos ver las principales propiedades de nuestro nuevo marco de trabajo para la destilaciÃ³n de informaciÃ³n basada en utilidad sobre documentos ordenados temporalmente. Nuestro marco de trabajo combina y amplÃ­a el poder del filtrado adaptativo (AF), la recuperaciÃ³n ad-hoc (IR) y la detecciÃ³n de novedades (ND). En comparaciÃ³n con la IR estÃ¡ndar, nuestro enfoque tiene la capacidad de aprender de forma incremental las necesidades de informaciÃ³n a largo plazo y modelar una secuencia de consultas dentro de una tarea. En comparaciÃ³n con el AF convencional, permite un papel mÃ¡s activo del usuario en refinar sus necesidades de informaciÃ³n y solicitar nuevos resultados al permitir comentarios sobre relevancia y novedad mediante el resaltado de fragmentos arbitrarios de texto en los pasajes devueltos por el sistema. Comparado con trabajos anteriores, esta es la primera evaluaciÃ³n de detecciÃ³n de novedad integrada con filtrado adaptativo para consultas secuenciadas que permite retroalimentaciÃ³n flexible del usuario sobre pasajes clasificados. La combinaciÃ³n de AF, IR y ND con las nuevas extensiones plantea una importante pregunta de investigaciÃ³n sobre la metodologÃ­a de evaluaciÃ³n: Â¿cÃ³mo podemos medir la utilidad de un sistema de destilaciÃ³n de informaciÃ³n como este? Las mÃ©tricas existentes en IR estÃ¡ndar, AF y ND son insuficientes, y se deben explorar nuevas soluciones, como discutiremos en la SecciÃ³n 4, despuÃ©s de describir los nÃºcleos tÃ©cnicos de nuestro sistema en la prÃ³xima secciÃ³n. 3. NÃCLEOS TÃCNICOS Los componentes principales de CAFÃ son: 1) AF para el aprendizaje incremental de perfiles de consulta, 2) IR para estimar la relevancia de pasajes con respecto a los perfiles de consulta, 3) ND para evaluar la novedad de pasajes con respecto al historial de usuarios, y 4) un componente anti-redundancia para eliminar la redundancia de las listas clasificadas. 3.1 Componente de Filtrado Adaptativo Utilizamos un algoritmo de vanguardia en el campo, el mÃ©todo de regresiÃ³n logÃ­stica regularizada que obtuvo los mejores resultados en varios corpus de evaluaciÃ³n de referencia para AF [21]. La regresiÃ³n logÃ­stica (LR) es un algoritmo de aprendizaje supervisado para clasificaciÃ³n estadÃ­stica. BasÃ¡ndose en un conjunto de entrenamiento de instancias etiquetadas, aprende un modelo de clase que luego puede ser utilizado para predecir las etiquetas de instancias no vistas. Su rendimiento, asÃ­ como su eficiencia en tÃ©rminos de tiempo de entrenamiento, lo convierten en un buen candidato cuando se requieren actualizaciones frecuentes del modelo de clase, como es el caso en el filtrado adaptativo, donde el sistema debe aprender de cada nueva retroalimentaciÃ³n proporcionada por el usuario. (Consulte [21] y [23] para complejidad computacional y problemas de implementaciÃ³n). En el filtrado adaptativo, cada consulta se considera como una clase y la probabilidad de que un pasaje pertenezca a esta clase corresponde al grado de relevancia del pasaje con respecto a la consulta. Para entrenar el modelo, utilizamos la propia consulta como ejemplo inicial de entrenamiento positivo de la clase, y las piezas de texto resaltadas por el usuario (marcadas como Relevante o No relevante) durante la retroalimentaciÃ³n como ejemplos de entrenamiento adicionales. Para abordar el problema de inicio en frÃ­o en la etapa inicial antes de obtener cualquier retroalimentaciÃ³n del usuario, el sistema utiliza una pequeÃ±a muestra de un corpus retrospectivo como ejemplos negativos iniciales en el conjunto de entrenamiento. Los detalles de cÃ³mo utilizar la regresiÃ³n logÃ­stica para el filtrado adaptativo (asignando diferentes pesos a las instancias de entrenamiento positivas y negativas, y regularizando la funciÃ³n objetivo para prevenir el sobreajuste en los datos de entrenamiento) se presentan en [21]. El modelo de clase wâ aprendido por RegresiÃ³n LogÃ­stica, o el perfil de consulta, es un vector cuyas dimensiones son tÃ©rminos individuales y cuyos elementos son los coeficientes de regresiÃ³n, indicando cuÃ¡n influyente es cada tÃ©rmino en el perfil de consulta. El perfil de la consulta se actualiza cada vez que se recibe una nueva pieza de retroalimentaciÃ³n del usuario. Un peso decaÃ­do temporalmente se puede aplicar a cada ejemplo de entrenamiento, como una opciÃ³n, para enfatizar la retroalimentaciÃ³n mÃ¡s reciente del usuario. 3.2 Componente de RecuperaciÃ³n de Pasajes Utilizamos tÃ©cnicas estÃ¡ndar de IR en esta parte de nuestro sistema. Los documentos entrantes se procesan por bloques, donde cada bloque puede definirse como un intervalo de tiempo fijo o como un nÃºmero fijo de documentos, segÃºn lo prefiera el usuario. Para cada documento entrante, se actualizan las estadÃ­sticas del corpus como el IDF (Frecuencia Inversa de Documento) de cada tÃ©rmino. Utilizamos un identificador y rastreador de entidades con tecnologÃ­a de punta [8, 12] para identificar nombres de personas y lugares, y fusionarlos con entidades con referencia cruzada vistas en el pasado. Luego, los documentos se dividen en pasajes, que pueden ser un documento completo, un pÃ¡rrafo, una oraciÃ³n, o cualquier otro fragmento continuo de texto, segÃºn se prefiera. Cada pasaje se representa utilizando un vector de pesos TF-IDF (Frecuencia de TÃ©rmino - Frecuencia Inversa de Documento), donde el tÃ©rmino puede ser una palabra o una entidad nombrada. Dado un perfil de consulta, es decir, la soluciÃ³n de regresiÃ³n logÃ­stica wâ como se describe en la SecciÃ³n 3.1, el sistema calcula la probabilidad posterior de relevancia para cada pasaje x como fRL(x) â¡ P(y = 1|x, wâ ) = 1 (1 + eâwâÂ·x) (1). Los pasajes se ordenan segÃºn sus puntajes de relevancia, y aquellos con puntajes por encima de un umbral (ajustado en un conjunto de entrenamiento) conforman la lista de relevancia que se pasa al paso de detecciÃ³n de novedad. 3.3 Componente de DetecciÃ³n de Novedad CAFÂ´E mantiene un historial de usuario H(t), que contiene todos los fragmentos de texto hi que el usuario resaltÃ³ (como retroalimentaciÃ³n) durante sus interacciones pasadas con el sistema, hasta el tiempo actual t. Denotando el historial como H(t) = n h1, h2, ..., ht o , (2), el puntaje de novedad de un nuevo pasaje candidato x se calcula como: fND(x) = 1 â max iâ1..t {cos(x, hi)} (3), donde tanto el pasaje candidato x como los fragmentos de texto resaltados hi se representan como vectores TF-IDF. El puntaje de novedad de cada pasaje se compara con un umbral predefinido (ajustado tambiÃ©n en un conjunto de entrenamiento), y cualquier pasaje con un puntaje por debajo de este umbral se elimina de la lista de relevancia. Componente de clasificaciÃ³n anti-redundante Aunque el componente de detecciÃ³n de novedad garantiza que solo la informaciÃ³n novedosa (no vista previamente) permanezca en la lista de relevancia, esta lista aÃºn podrÃ­a contener la misma informaciÃ³n novedosa en mÃºltiples posiciones en la lista clasificada. Supongamos, por ejemplo, que el usuario ya ha leÃ­do sobre una recompensa de $100,000 por informaciÃ³n sobre los convictos fugados. Una nueva noticia de que el premio ha sido aumentado a $200,000 es novedosa ya que el usuario aÃºn no la ha leÃ­do. Sin embargo, mÃºltiples fuentes de noticias podrÃ­an informar sobre esta noticia y podrÃ­amos terminar mostrando artÃ­culos (redundantes) de todas estas fuentes en una lista clasificada. Por lo tanto, una lista clasificada tambiÃ©n debe ser no redundante con respecto a su propio contenido. Utilizamos una versiÃ³n simplificada del mÃ©todo de MÃ¡xima Relevancia Marginal [5], originalmente desarrollado para combinar relevancia y novedad en la recuperaciÃ³n de texto y resÃºmenes. Nuestro procedimiento comienza con la lista actual de pasajes ordenados por relevancia (secciÃ³n 3.2), filtrados por el componente de DetecciÃ³n de Novedades (secciÃ³n 3.3), y genera una nueva lista no redundante de la siguiente manera: 1. Toma el pasaje superior en la lista actual como el primero en la nueva lista. 2. Agrega el siguiente pasaje x en la lista actual a la nueva lista solo si fAR(x) > t donde fAR(x) = 1 â max piâLnew {cos(x, pi)} y Lnew es el conjunto de pasajes ya seleccionados en la nueva lista. Repetir el paso 2 hasta que se hayan examinado todos los pasajes en la lista actual. DespuÃ©s de aplicar el algoritmo mencionado anteriormente, cada pasaje en la nueva lista es lo suficientemente diferente a los demÃ¡s, favoreciendo asÃ­ la diversidad en lugar de la redundancia en la nueva lista clasificada. El umbral anti-redundancia t se ajusta en un conjunto de entrenamiento. 4. METODOLOGÃA DE EVALUACIÃN El enfoque que propusimos anteriormente para la destilaciÃ³n de informaciÃ³n plantea importantes cuestiones sobre la metodologÃ­a de evaluaciÃ³n. En primer lugar, dado que nuestro marco de trabajo permite que la salida sean fragmentos en diferentes niveles de granularidad (por ejemplo, ventanas de k oraciones donde k puede variar) en lugar de una longitud fija, no es posible tener juicios de relevancia preanotados en todos esos niveles de granularidad. En segundo lugar, dado que deseamos medir la utilidad de la salida del sistema como una combinaciÃ³n de relevancia y novedad, las medidas tradicionales basadas Ãºnicamente en la relevancia deben ser reemplazadas por medidas que penalicen la repeticiÃ³n de la misma informaciÃ³n en la salida del sistema a lo largo del tiempo. En tercer lugar, dado que la salida del sistema son listas clasificadas, debemos recompensar a aquellos sistemas que presenten informaciÃ³n Ãºtil (tanto relevante como no vista previamente) utilizando listas clasificadas mÃ¡s cortas, y penalizar a aquellos que presenten la misma informaciÃ³n utilizando listas clasificadas mÃ¡s largas. Ninguna de las medidas existentes en la recuperaciÃ³n ad-hoc, filtrado adaptativo, detecciÃ³n de novedades u otras Ã¡reas relacionadas (resumen de texto y respuesta a preguntas) tiene propiedades deseables en los tres aspectos. Por lo tanto, debemos desarrollar una nueva metodologÃ­a de evaluaciÃ³n. 4.1 Claves de Respuesta Para permitir la evaluaciÃ³n de un sistema cuya salida consiste en pasajes de longitud arbitraria, tomamos prestado el concepto de claves de respuesta de la comunidad de Preguntas y Respuestas (QA), donde los sistemas pueden devolver fragmentos de texto arbitrarios como respuestas. Las claves de respuestas definen lo que debe estar presente en la respuesta de un sistema para recibir crÃ©dito, y estÃ¡n compuestas por una colecciÃ³n de \"nuggets\" de informaciÃ³n, es decir, unidades de datos sobre las cuales los evaluadores humanos pueden tomar decisiones binarias sobre si una respuesta del sistema las contiene o no. La definiciÃ³n de claves de respuestas y la toma de decisiones binarias asociadas son tareas conceptuales que requieren un mapeo semÃ¡ntico [19], ya que los pasajes devueltos por el sistema pueden contener la misma informaciÃ³n expresada de muchas formas diferentes. Por lo tanto, las evaluaciones de QA han dependido de evaluadores humanos para la correspondencia entre diversas expresiones, lo que hace que el proceso sea costoso, consume mucho tiempo y no sea escalable para grandes colecciones de consultas y documentos, y evaluaciones extensas de sistemas con diversos ajustes de parÃ¡metros. 4.1.1 AutomatizaciÃ³n de la evaluaciÃ³n basada en claves de respuestas Los mÃ©todos de evaluaciÃ³n automÃ¡tica permitirÃ­an una construcciÃ³n y ajuste mÃ¡s rÃ¡pidos del sistema, asÃ­ como proporcionar una forma objetiva y asequible de comparar diversos sistemas. Recientemente, se han propuesto mÃ©todos basados mÃ¡s o menos en la idea de las co-ocurrencias de n-gramos. Pourpre [10] asigna una puntuaciÃ³n de recuerdo fraccional a una respuesta del sistema basada en su superposiciÃ³n de unigramas con una descripciÃ³n de pepitas dada. Por ejemplo, una respuesta del sistema A B C tiene una recuperaciÃ³n de 3/4 con respecto a un fragmento con la descripciÃ³n A B C D. Sin embargo, este enfoque es injusto para los sistemas que presentan la misma informaciÃ³n pero utilizando palabras distintas a A, B, C y D. Otro problema abierto es cÃ³mo ponderar las palabras individuales al medir la cercanÃ­a de una coincidencia. Por ejemplo, considera la pregunta Â¿CuÃ¡ntos prisioneros escaparon? En el fragmento Siete prisioneros escaparon de una prisiÃ³n de Texas, no hay indicaciÃ³n de que siete sea la palabra clave y que debe coincidir para obtener algÃºn crÃ©dito de relevancia. El uso de los valores de IDF no ayuda, ya que la palabra \"seven\" generalmente no tendrÃ¡ un IDF mÃ¡s alto que palabras como \"texas\" y \"prison\". AdemÃ¡s, redefinir el fragmento como simplemente siete no resuelve el problema, ya que ahora podrÃ­a coincidir errÃ³neamente con cualquier menciÃ³n de siete fuera de contexto. Nuggeteer [13] trabaja sobre principios similares pero toma decisiones binarias sobre si hay un fragmento en una respuesta de sistema dada ajustando un umbral. Sin embargo, tambiÃ©n estÃ¡ plagado de relevancia espuria ya que no todas las palabras contenidas en la descripciÃ³n de la pepita (o respuestas correctas conocidas) son centrales para la pepita. 4.1.2 Reglas de Coincidencia de Pepitas Proponemos un mÃ©todo automÃ¡tico confiable para determinar si un fragmento de texto contiene una pepita dada, basado en <br>reglas de coincidencia de pepitas</br>, que se generan utilizando un procedimiento semiautomÃ¡tico explicado a continuaciÃ³n. Estas reglas son bÃ¡sicamente consultas booleanas que solo coincidirÃ¡n con fragmentos que contengan la pepita. Por ejemplo, una regla candidata para emparejar respuestas a Â¿CuÃ¡ntos prisioneros escaparon? es (Texas Y siete Y escapar Y (convictos O prisioneros)), posiblemente con otros sinÃ³nimos y variantes en la regla. Para un corpus de artÃ­culos de noticias, que suelen seguir una prosa formal tÃ­pica, es bastante fÃ¡cil escribir reglas simples para coincidir con respuestas esperadas utilizando un enfoque de arranque, como se describe a continuaciÃ³n. Proponemos un enfoque de dos etapas, inspirado en Autoslog [14], que combina la capacidad de los humanos para identificar expresiones semÃ¡nticamente equivalentes y la capacidad del sistema para recopilar evidencia estadÃ­stica de un corpus de documentos anotados por humanos. En la primera etapa, los sujetos humanos anotaron (utilizando una herramienta de resaltado) porciones de documentos relevantes que contenÃ­an respuestas a cada fragmento 2. En la segunda etapa, los sujetos utilizaron nuestra herramienta de generaciÃ³n de reglas para crear reglas que coincidieran con las anotaciones de cada fragmento. La herramienta permite a los usuarios ingresar una regla booleana como una disyunciÃ³n de conjunciones (por ejemplo, ((a Y b) O (a Y c Y d) O (e))). Dada una regla candidata, nuestra herramienta la utiliza como una consulta booleana sobre el conjunto completo de documentos relevantes y calcula su recall y precisiÃ³n con respecto a las anotaciones que se espera que coincida. Por lo tanto, los sujetos pueden comenzar con una regla simple y refinarla iterativamente hasta que estÃ©n satisfechos con su precisiÃ³n y recuperaciÃ³n. Observamos que era muy fÃ¡cil para los humanos mejorar la precisiÃ³n de una regla ajustando sus conjunciones existentes (agregando mÃ¡s Y) y mejorar la recuperaciÃ³n agregando mÃ¡s conjunciones a la disyunciÃ³n (agregando mÃ¡s O). Como ejemplo, intentemos crear una regla para el acertijo que dice que siete prisioneros escaparon de la prisiÃ³n de Texas. Empezamos con una regla simple - (siete). Cuando introducimos esto en la herramienta de generaciÃ³n de reglas, nos damos cuenta de que esta regla coincide con muchas ocurrencias espurias de siete (por ejemplo, ...siete estados...) y, por lo tanto, obtiene una puntuaciÃ³n baja de precisiÃ³n. Podemos calificar aÃºn mÃ¡s nuestra regla - Texas Y siete Y convictos. A continuaciÃ³n, al revisar las anotaciones omitidas, nos damos cuenta de que algunos artÃ­culos de noticias mencionaban ...siete prisioneros escaparon.... Luego reemplazamos convictos por la disyunciÃ³n (convictos O prisioneros). Continuamos ajustando la regla de esta manera hasta lograr un nivel de recordaciÃ³n y precisiÃ³n suficientemente alto, es decir, que los (pocos) errores y alarmas falsas se puedan ignorar de forma segura. AsÃ­ podemos crear <br>reglas de coincidencia de pepitas</br> que capturen de manera sucinta varias formas de expresar una pepita, evitando al mismo tiempo emparejar respuestas incorrectas (o fuera de contexto). La participaciÃ³n humana en el proceso de creaciÃ³n de reglas garantiza reglas genÃ©ricas de alta calidad que luego se pueden utilizar para evaluar de manera confiable las respuestas del sistema arbitrario. 4.2 EvaluaciÃ³n de la Utilidad de una Secuencia de Listas Clasificadas La utilidad de un sistema de recuperaciÃ³n se puede definir como la diferencia entre cuÃ¡nta informaciÃ³n Ãºtil obtuvo el usuario y cuÃ¡nto tiempo y energÃ­a perdiÃ³. Calculamos esta utilidad a partir de las utilidades de los pasajes individuales de la siguiente manera. DespuÃ©s de leer cada pasaje devuelto por el sistema, el usuario obtiene cierta ganancia dependiendo de la presencia de informaciÃ³n relevante y novedosa, y incurre en una pÃ©rdida en tÃ©rminos del tiempo y la energÃ­a invertidos en revisar el pasaje. Sin embargo, la probabilidad de que el usuario realmente lea un pasaje depende de su posiciÃ³n en la lista clasificada. Por lo tanto, para una consulta q, la utilidad esperada 2 LDC [18] ya proporciona juicios de relevancia para 100 temas en el corpus TDT4. AdemÃ¡s, nos aseguramos de que estos juicios sean exhaustivos en todo el corpus utilizando la agrupaciÃ³n. El valor de un pasaje pi en el rango i se puede definir como U(pi, q) = P(i) â (Ganancia(pi, q) - PÃ©rdida(pi, q)) (4) donde P(i) es la probabilidad de que el usuario pase por un pasaje en el rango i. La utilidad esperada para una lista clasificada completa de longitud n se puede calcular simplemente sumando la utilidad esperada de cada pasaje: U(q) = Î£ i=1 P(i) â (Ganancia(pi, q) â PÃ©rdida(pi, q)) (5) NÃ³tese que si ignoramos el tÃ©rmino de pÃ©rdida y definimos P(i) como P(i) â 1/ logb(b + i â 1) (6) entonces obtenemos la mÃ©trica recientemente popularizada llamada Ganancia Acumulada Descontada (DCG) [9], donde Ganancia(pi, q) se define como la relevancia graduada del pasaje pi. Sin embargo, sin el tÃ©rmino de pÃ©rdida, DCG es una mÃ©trica puramente orientada al recuerdo y no es adecuada para un entorno de filtrado adaptativo, donde la utilidad del sistema depende en parte de su capacidad para limitar el nÃºmero de elementos mostrados al usuario. Aunque P(i) podrÃ­a definirse en base a estudios empÃ­ricos del comportamiento del usuario, por simplicidad, utilizamos P(i) tal como se define exactamente en la ecuaciÃ³n 6. La ganancia G(pi, q) del pasaje pi con respecto a la consulta q es una funciÃ³n de - 1) el nÃºmero de pepitas relevantes presentes en pi, y 2) la novedad de cada una de estas pepitas. Combinamos estos dos factores de la siguiente manera. Para cada pepita Nj, asignamos un peso inicial wj, y tambiÃ©n mantenemos un conteo nj del nÃºmero de veces que esta pepita ha sido vista por el usuario en el pasado. La ganancia derivada de cada ocurrencia posterior del mismo hallazgo se asume que se reduce por un factor de amortiguamiento Î³. Por lo tanto, G(pi, q) se define como G(pi, q) = X Nj âC(pi,q) wj â Î³nj (7) donde C(pi, q) es el conjunto de todas las pepitas que aparecen en el pasaje pi y tambiÃ©n pertenecen a la clave de respuesta de la consulta q. Los pesos iniciales wj se establecen todos en 1.0 en nuestros experimentos, pero tambiÃ©n pueden establecerse segÃºn un enfoque de pirÃ¡mide [11]. La elecciÃ³n del factor de amortiguaciÃ³n Î³ determina la tolerancia del usuario a la redundancia. Cuando Î³ = 0, un pepita solo recibirÃ¡ crÃ©dito por su primera ocurrencia, es decir, cuando nj sea cero. Para 0 < Î³ < 1, un nugget recibe menos crÃ©dito por cada ocurrencia sucesiva. Cuando Î³ = 1, no se produce amortiguamiento y las repeticiones de un valor reciben el mismo crÃ©dito. Se debe tener en cuenta que los recuentos de ocurrencia de las pepitas se conservan entre la evaluaciÃ³n de las listas clasificadas sucesivas devueltas por el sistema, ya que se espera que los usuarios recuerden lo que el sistema les mostrÃ³ en el pasado. Definimos la pÃ©rdida L(pi, q) como un costo constante c (usamos 0.1) incurrido al leer un pasaje devuelto por el sistema. Por lo tanto, nuestra mÃ©trica puede ser reescrita como U(q) = nX i=1 Ganancia(pi, q) logb(b + i â 1) â L(n) (8) donde L(n) es la pÃ©rdida asociada con una lista clasificada de longitud n: L(n) = c Â· nX i=1 1 logb(b + i â 1) (9) 3 NÃ³tese que 00 = 1 Debido a la similitud con la Ganancia Acumulada Descontada (DCG), llamamos a nuestra mÃ©trica Utilidad Acumulada Descontada (DCU). El puntaje de DCU obtenido por el sistema se convierte en un puntaje de DCU normalizado (NDCU) dividiÃ©ndolo por el puntaje de DCU de la lista ideal clasificada, que se crea ordenando los pasajes por sus puntajes de utilidad decrecientes U(pi, q) y deteniÃ©ndose cuando U(pi, q) â¤ 0, es decir, cuando la ganancia es menor o igual al costo de leer el pasaje. El conjunto de datos TDT4 fue el corpus de referencia utilizado en las evaluaciones TDT2002 y TDT2003. El corpus consiste en mÃ¡s de 90,000 artÃ­culos de noticias de mÃºltiples fuentes (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) publicados entre octubre de 2000 y enero de 2001, en los idiomas Ã¡rabe, inglÃ©s y mandarÃ­n. Las versiones de los artÃ­culos en otros idiomas reconocidas por voz y traducidas por mÃ¡quina tambiÃ©n fueron proporcionadas. LDC [18] ha anotado el corpus con 100 temas, que corresponden a varios eventos de noticias en este perÃ­odo de tiempo. De entre estos, seleccionamos un subconjunto de 12 eventos accionables y definimos tareas correspondientes para ellos. Para cada tarea, definimos manualmente un perfil que consiste en un conjunto inicial de (5 a 10) consultas, una descripciÃ³n de texto libre del historial del usuario, es decir, lo que el usuario ya sabe sobre el evento, y una lista de documentos conocidos relevantes y no relevantes (si estÃ¡n disponibles) como ejemplos de entrenamiento. Para cada consulta, generamos claves de respuestas y reglas de coincidencia de pepitas correspondientes utilizando el procedimiento descrito en la secciÃ³n 4.1.2, y produjimos un total de 120 consultas, con un promedio de 7 pepitas por consulta. EXPERIMENTOS Y RESULTADOS 6.1 Baselines Utilizamos Indri [17], un motor de bÃºsqueda basado en modelos de lenguaje popular, como referencia para comparar con CAFÃ. Indri admite la funcionalidad estÃ¡ndar de los motores de bÃºsqueda, incluido el feedback de pseudo-relevancia (PRF) [3, 6], y es representativo de un sistema de recuperaciÃ³n basado en consultas tÃ­pico. Indri no admite ningÃºn tipo de detecciÃ³n de novedades. Comparamos Indri con PRF activado y desactivado, frente a CAFÃ con retroalimentaciÃ³n del usuario, detecciÃ³n de novedades y clasificaciÃ³n antirredundante activados y desactivados. 6.2 ConfiguraciÃ³n Experimental Dividimos el corpus TDT4 que abarca 4 meses en 10 fragmentos, cada uno definido como un perÃ­odo de 12 dÃ­as consecutivos. En cualquier momento dado del proceso de destilaciÃ³n, cada sistema accedÃ­a a los datos pasados hasta el punto actual, y devolvÃ­a una lista clasificada de hasta 50 pasajes por consulta. Las 12 tareas definidas en el corpus se dividieron en un conjunto de entrenamiento y prueba con 6 tareas cada uno. Cada sistema pudo utilizar el conjunto de entrenamiento para ajustar sus parÃ¡metros con el fin de optimizar NDCU (ecuaciÃ³n 8), incluyendo el umbral de relevancia tanto para Indri como para CAFÂ´E, y los umbrales de novedad y antirredundancia para CAFÂ´E. La NDCU para cada ejecuciÃ³n del sistema se calcula automÃ¡ticamente. TambiÃ©n se simulÃ³ la retroalimentaciÃ³n de los usuarios: los juicios de relevancia para cada pasaje devuelto por el sistema (segÃºn las reglas de coincidencia de pepitas descritas en la secciÃ³n 4.1.2) fueron utilizados como retroalimentaciÃ³n de los usuarios en la adaptaciÃ³n de perfiles de consulta e historiales de usuario. En la Tabla 1, mostramos los puntajes NDCU de los dos sistemas en diferentes configuraciones. Estas puntuaciones se promedian en las seis tareas del conjunto de pruebas, y se calculan con dos factores de amortiguaciÃ³n (ver secciÃ³n 4.2): Î³ = 0 y 0.1, para simular ninguna tolerancia y poca tolerancia para la redundancia, respectivamente. Usar Î³ = 0 crea una mÃ©trica mucho mÃ¡s estricta ya que no otorga ningÃºn crÃ©dito a un pasaje que contenga informaciÃ³n relevante pero redundante. Por lo tanto, la mejora obtenida al habilitar la retroalimentaciÃ³n del usuario es menor con Î³ = 0 que la mejora obtenida de la retroalimentaciÃ³n con Î³ = 0.1. Esto revela una deficiencia de los sistemas de recuperaciÃ³n contemporÃ¡neos: cuando el usuario da retroalimentaciÃ³n positiva sobre un pasaje, los sistemas otorgan mayor peso a los tÃ©rminos presentes en ese pasaje y tienden a recuperar otros pasajes que contienen los mismos tÃ©rminos, y por lo tanto, generalmente la misma informaciÃ³n. Sin embargo, el usuario no se beneficia al ver pasajes redundantes y generalmente estÃ¡ interesado en otros pasajes que contengan informaciÃ³n relacionada. Es informativo evaluar los sistemas de recuperaciÃ³n utilizando nuestra medida de utilidad (con Î³ = 0) que tiene en cuenta la novedad y, por lo tanto, ofrece una imagen mÃ¡s realista de cuÃ¡n bien un sistema puede generalizar a partir de la retroalimentaciÃ³n del usuario, en lugar de utilizar medidas tradicionales de RI como la recuperaciÃ³n y la precisiÃ³n, que ofrecen una imagen incompleta de la mejora obtenida a partir de la retroalimentaciÃ³n del usuario. A veces, sin embargo, los usuarios podrÃ­an estar interesados en ver la misma informaciÃ³n de mÃºltiples fuentes, como un indicador de su importancia o fiabilidad. En tal caso, simplemente podemos elegir un valor mÃ¡s alto para Î³ que corresponda a una mayor tolerancia para la redundancia, y asÃ­ permitir que el sistema ajuste sus parÃ¡metros en consecuencia. Dado que los documentos fueron procesados por fragmentos, serÃ­a interesante ver cÃ³mo mejora el rendimiento de los sistemas con el tiempo. Las figuras 1 y 2 muestran las tendencias de rendimiento de ambos sistemas a lo largo de los fragmentos. Si bien se espera que el rendimiento con y sin retroalimentaciÃ³n en los primeros fragmentos sea similar, para los fragmentos posteriores, la curva de rendimiento con la retroalimentaciÃ³n habilitada supera a la del ajuste sin retroalimentaciÃ³n. Las tendencias de rendimiento no son consistentes en todos los fragmentos porque los documentos relevantes no estÃ¡n distribuidos uniformemente en todos los fragmentos, lo que hace que algunas consultas sean mÃ¡s fÃ¡ciles que otras en ciertos fragmentos. AdemÃ¡s, dado que Indri utiliza retroalimentaciÃ³n de pseudo-relevancia mientras que CAFÂ´E utiliza retroalimentaciÃ³n basada en juicios de relevancia reales, la mejora en el caso de Indri es menos dramÃ¡tica que la de CAFÂ´E. 7. CONCLUSIONES FINALES Este artÃ­culo presenta la primera investigaciÃ³n sobre la destilaciÃ³n de informaciÃ³n basada en la utilidad con un sistema que aprende las necesidades de informaciÃ³n duraderas a partir de la retroalimentaciÃ³n detallada del usuario sobre una secuencia de pasajes clasificados. Nuestro sistema, llamado CAFÂ´E, combina filtrado adaptativo, detecciÃ³n de novedades y clasificaciÃ³n de pasajes antirredundantes en un marco unificado para la optimizaciÃ³n de la utilidad. Desarrollamos un nuevo esquema para la evaluaciÃ³n y retroalimentaciÃ³n automatizadas basado en un procedimiento semiautomÃ¡tico para adquirir reglas que permitan emparejar automÃ¡ticamente fragmentos con las respuestas del sistema. TambiÃ©n propusimos una extensiÃ³n de la mÃ©trica NDCG para evaluar la utilidad de los pasajes clasificados como una combinaciÃ³n ponderada de relevancia y novedad. Nuestros experimentos en el reciÃ©n anotado corpus de referencia TDT4 muestran un alentador aumento de utilidad sobre Indri, y tambiÃ©n sobre nuestro propio sistema con aprendizaje incremental y detecciÃ³n de novedades desactivados. 8. AGRADECIMIENTOS Nos gustarÃ­a agradecer a Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng y al Programa de AnÃ¡lisis de Datos Cualitativos de la Universidad de Pittsburgh liderado por el Dr. Stuart Shulman por su ayuda en la recolecciÃ³n y procesamiento de las anotaciones extendidas de TDT4 utilizadas en nuestros experimentos. Este trabajo estÃ¡ parcialmente respaldado por la FundaciÃ³n Nacional de Ciencias (NSF) bajo la subvenciÃ³n IIS0434035, y la Agencia de Proyectos de InvestigaciÃ³n Avanzada de Defensa (DARPA) bajo los contratos NBCHD030010 y W0550432. Cualquier opiniÃ³n, hallazgo, conclusiÃ³n o recomendaciÃ³n expresada en este material son responsabilidad de los autores y no reflejan necesariamente los puntos de vista de los patrocinadores. AUTORES ADICIONALES Jian Zhang (jianzhan@stat.purdue.edu)â , Jaime Carbonell (jgc@cs.cmu.edu)â  , Peter Brusilovsky (peterb+@pitt.edu)â¡ , Daqing He(dah44@pitt.edu)â¡ 10. REFERENCIAS [1] J. Allan. RetroalimentaciÃ³n incremental de relevancia para la filtraciÃ³n de informaciÃ³n. Actas de la 19Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 270-278, 1996. [2] J. Allan, C. Wade y A. Bolivar. RecuperaciÃ³n y DetecciÃ³n de Novedad a Nivel de OraciÃ³n. Actas de la conferencia ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, 2003. [3] C. Buckley, G. Salton y J. Allan. RecuperaciÃ³n automÃ¡tica con informaciÃ³n de localidad utilizando SMART. PublicaciÃ³n especial del NIST, (500207):59-72, 1993. [4] J. Callan. Aprendiendo mientras se filtran documentos. Actas de la 21Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 224-231, 1998. [5] J. Carbonell y J. Goldstein. El uso de MMR, Reordenamiento basado en Diversidad para Reorganizar Documentos y Producir ResÃºmenes. Actas de la 21Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 335-336, 1998. [6] E. Efthimiadis. ExpansiÃ³n de consulta. RevisiÃ³n Anual de Ciencia de la InformaciÃ³n y TecnologÃ­a (ARIST), 31:p121-87, 1996. [7] J. Fiscus y G. Duddington. Resumen de DetecciÃ³n y Seguimiento de Temas. DetecciÃ³n y seguimiento de temas: OrganizaciÃ³n de la informaciÃ³n basada en eventos, pÃ¡ginas 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov y S. Roukos. Un modelo estadÃ­stico para la detecciÃ³n y seguimiento de entidades multilingÃ¼es. NAACL/HLT, 2004. [9] K. JÃ¤rvelin y J. KekÃ¤lÃ¤inen. EvaluaciÃ³n basada en la Ganancia Acumulada de TÃ©cnicas de RecuperaciÃ³n de InformaciÃ³n. ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin y D. Demner-Fushman. EvaluaciÃ³n automÃ¡tica de respuestas a preguntas de definiciÃ³n. Actas de la Conferencia de TecnologÃ­a del Lenguaje Humano y Conferencia sobre MÃ©todos EmpÃ­ricos en Procesamiento del Lenguaje Natural (HLT/EMNLP 2005), 2005. â Departamento de EstadÃ­stica, Universidad de Purdue, West Lafayette, EE. UU. â  Instituto de TecnologÃ­as del Lenguaje, Universidad Carnegie Mellon, Pittsburgh, EE. UU. â¡ Escuela de Ciencias de la InformaciÃ³n, Universidad de Pittsburgh, Pittsburgh, EE. UU. [11] J. Lin y D. Demner-Fushman. Â¿Se derrumbarÃ¡n las pirÃ¡mides construidas con pepitas? Actas de HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla y S. Roukos. Un algoritmo de resoluciÃ³n de correferencia sÃ­ncrona basado en el Ã¡rbol de Bell. Actas de ACL, 4:136-143, 2004. [13] G. Marton. Nuggeteer: EvaluaciÃ³n AutomÃ¡tica Basada en Pepitas Utilizando Descripciones y Juicios. HLT/NAACL, 2006. [14] E. Riloff. ConstrucciÃ³n automÃ¡tica de un diccionario para tareas de extracciÃ³n de informaciÃ³n. Actas de la UndÃ©cima Conferencia Nacional sobre Inteligencia Artificial, pÃ¡ginas 811-816, 1993. [15] S. Robertson y S. Walker. Microsoft Cambridge en TREC-9: Pista de filtrado. La Novena Conferencia de RecuperaciÃ³n de Texto (TREC-9), pÃ¡ginas 361-368. [16] R. Schapire, Y. Cantante, y A. Singhal. Boosting y Rocchio aplicados a la filtraciÃ³n de texto. Actas de la 21Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle y W. Croft. Indri: Un motor de bÃºsqueda basado en modelos de lenguaje para consultas complejas. Actas de la Conferencia Internacional sobre AnÃ¡lisis de Inteligencia, 2004. [18] El Consorcio de Datos LingÃ¼Ã­sticos. http://www.ldc.upenn.edu/. [19] E. Voorhees. Resumen de la pista de preguntas y respuestas TREC 2003. Actas de la DuodÃ©cima Conferencia de RecuperaciÃ³n de Texto (TREC 2003), 2003. [20] Y. Yang y B. Kisiel. RegresiÃ³n local basada en mÃ¡rgenes para filtrado adaptativo. Actas de la duodÃ©cima conferencia internacional sobre gestiÃ³n de la informaciÃ³n y el conocimiento, pÃ¡ginas 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang y B. Kisiel. Robustez de los MÃ©todos de Filtrado Adaptativo en una EvaluaciÃ³n de Referencia Cruzada. Actas de la 28Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 98-105, 2005. [22] C. Zhai, W. Cohen y J. Lafferty. MÃ¡s allÃ¡ de la relevancia independiente: MÃ©todos y mÃ©tricas de evaluaciÃ³n para la recuperaciÃ³n de subtemas. Actas de la 26Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 10-17, 2003. [23] J. Zhang y Y. Yang. Robustez de los MÃ©todos de ClasificaciÃ³n Lineal Regularizados en la CategorizaciÃ³n de Textos. Actas de la 26Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 190-197, 2003. [24] Y. Zhang. Utilizando priors bayesianos para combinar clasificadores para filtrado adaptativo. Actas de la 27Âª conferencia internacional anual sobre InvestigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 345-352, 2004. [25] Y. Zhang, J. Callan y T. Minka. DetecciÃ³n de novedad y redundancia en el filtrado adaptativo. Actas de la 25Âª Conferencia Internacional Anual de ACM SIGIR sobre InvestigaciÃ³n y Desarrollo en RecuperaciÃ³n de InformaciÃ³n, 2002. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "unified framework": {
            "translated_key": "marco unificado",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Utility-based Information Distillation Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu ABSTRACT This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework.",
                "It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant passage ranking with respect to long-lasting information needs (tasks with multiple queries).",
                "Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings.",
                "For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task.",
                "Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty.",
                "Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval.",
                "Adaptive filtering (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics.",
                "Based on the initial query and a few positive examples (if available), an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user.",
                "The incremental learning nature of AF systems makes them more powerful than standard search engines that support ad-hoc retrieval (e.g.",
                "Google and Yahoo) in terms of finding relevant information with respect to long-lasting topics of interest, and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs, without having to modify their queries manually.",
                "A variety of supervised learning algorithms (Rocchio-style classifiers, Exponential-Gaussian models, local regression and logistic regression approaches) have been studied for adaptive settings, examined with explicit and implicit relevance feedback, and evaluated with respect to utility optimization on large benchmark data collections in TREC (Text Retrieval Conferences) and TDT (Topic Detection and Tracking) forums [1, 4, 7, 15, 16, 20, 24, 23].",
                "Regularized logistic regression [21] has been found representative for the state-of-the-art approaches, and highly efficient for frequent model adaptations over large document collections such as the TREC-10 corpus (over 800,000 documents and 84 topics).",
                "Despite substantial achievements in recent adaptive filtering research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently.",
                "Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications: 1.",
                "User has a rather passive role in the conventional adaptive filtering setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision.",
                "A more active alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents (or passages) per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists.",
                "The latter form of user interaction has been highly effective in standard retrieval for ad-hoc queries.",
                "How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2.",
                "The unit for receiving a relevance judgment (yes or no) is restricted to the document level in conventional AF.",
                "However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant.",
                "Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system.",
                "For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3.",
                "System-selected documents are often highly redundant.",
                "A major news event, for example, would be reported by multiple sources repeatedly for a while, making most of the information content in those articles redundant with each other.",
                "A conventional AF system would select all these redundant news stories for user feedback, wasting the users time while offering little gain.",
                "Clearly, techniques for novelty detection can help in principle [25, 2, 22] for improving the utility of the AF systems.",
                "However, the effectiveness of such techniques at passage level to detect novelty with respect to users (fine-grained) feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user.",
                "To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and novelty detection for removal of redundancy from user interactions with the system.",
                "We call the new process utility-based information distillation.",
                "Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach.",
                "Therefore, we extended a benchmark corpus - the TDT4 collection of news stories and TV broadcasts - with task definitions, multiple queries per task, and answer keys per query.",
                "We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 .",
                "To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses.",
                "Moreover, we propose an extension of NDCG (Normalized Discounted Cumulated Gain) [9] for assessing the utility of ranked passages as a function of both relevance and novelty.",
                "The rest of this paper is organized as follows.",
                "Section 2 outlines the information distillation process with a concrete example.",
                "Section 3 describes the technical cores of our system called CAFÂ´E - CMU Adaptive Filtering Engine.",
                "Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme.",
                "Section 5 describes the extended TDT4 corpus.",
                "Section 6 presents our experiments and results.",
                "Section 7 concludes the study and gives future perspectives. 2.",
                "A SAMPLE TASK Consider a news event - the escape of seven convicts from a Texas prison in December 2000 and their capture a month later.",
                "Assuming a user were interested in this event since its early stage, the information need could be: Find information about the escape of convicts from Texas prison, and information related to their recapture.",
                "The associated lower-level questions could be: 1.",
                "How many prisoners escaped? 2.",
                "Where and when were they sighted? 3. Who are their known contacts inside and outside the prison? 4.",
                "How are they armed? 5.",
                "Do they have any vehicles? 6.",
                "What steps have been taken so far?",
                "We call such an information need a task, and the associated questions as the queries in this task.",
                "A distillation system is supposed to monitor the incoming documents, process them chunk by chunk in a temporal order, select potentially relevant and novel passages from each chunk with respect to each query, and present a ranked list of passages to the user.",
                "Passage ranking here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list.",
                "When presented with a list of passages, the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant.",
                "These spans of text are taken as positive examples in the adaptation of the query profile, and also added to the users history.",
                "Passages not marked by the user are taken as negative examples.",
                "As soon as the query profile is updated, the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low, based on user preference.",
                "For example, if the user highlights ...officials have posted a $100,000 reward for their capture... as relevant answer to the query What steps have been taken so far?, then the highlighted piece is used as an additional positive training example in the adaptation of the query profile.",
                "This piece of feedback is also added to the user history as a seen example, so that in future, the system will not place another passage mentioning $100,000 reward at the top of the ranked list.",
                "However, an article mentioning ...officials have doubled the reward money to $200,000... might be ranked high since it is both relevant to the (updated) query profile and novel with respect to the (updated) user history.",
                "The user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly.",
                "Clearly, novelty detection is very important for the utility of such a system because of the iterative search.",
                "Without novelty detection, the old relevant passages would be shown to the user repeatedly in each ranked list.",
                "Through the above example, we can see the main properties of our new framework for utility-based information distillation over temporally ordered documents.",
                "Our framework combines and extends the power of adaptive filtering (AF), ad-hoc retrieval (IR) and novelty detection (ND).",
                "Compared to standard IR, our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task.",
                "Compared to conventional AF, it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system.",
                "Compared to past work, this is the first evaluation of novelty detection integrated with adaptive filtering for sequenced queries that allows flexible user feedback over ranked passages.",
                "The combination of AF, IR and ND with the new extensions raises an important research question regarding evaluation methodology: how can we measure the utility of such an information distillation system?",
                "Existing metrics in standard IR, AF and ND are insufficient, and new solutions must be explored, as we will discuss in Section 4, after describing the technical cores of our system in the next section. 3.",
                "TECHNICAL CORES The core components of CAFÂ´E are - 1) AF for incremental learning of query profiles, 2) IR for estimating relevance of passages with respect to query profiles, 3) ND for assessing novelty of passages with respect to users history, and 4) anti-redundancy component to remove redundancy from ranked lists. 3.1 Adaptive Filtering Component We use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for AF [21].",
                "Logistic regression (LR) is a supervised learning algorithm for statistical classification.",
                "Based on a training set of labeled instances, it learns a class model which can then by used to predict the labels of unseen instances.",
                "Its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required, as is the case in adaptive filtering, where the system must learn from each new feedback provided by the user. (See [21] and [23] for computational complexity and implementation issues).",
                "In adaptive filtering, each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query.",
                "For training the model, we use the query itself as the initial positive training example of the class, and the user-highlighted pieces of text (marked as Relevant or Not-relevant) during feedback as additional training examples.",
                "To address the cold start issue in the early stage before any user feedback is obtained, the system uses a small sample from a retrospective corpus as the initial negative examples in the training set.",
                "The details of using logistic regression for adaptive filtering (assigning different weights to positive and negative training instances, and regularizing the objective function to prevent over-fitting on training data) are presented in [21].",
                "The class model wâ learned by Logistic Regression, or the query profile, is a vector whose dimensions are individual terms and whose elements are the regression coefficients, indicating how influential each term is in the query profile.",
                "The query profile is updated whenever a new piece of user feedback is received.",
                "A temporally decaying weight can be applied to each training example, as an option, to emphasize the most recent user feedback. 3.2 Passage Retrieval Component We use standard IR techniques in this part of our system.",
                "Incoming documents are processed in chunks, where each chunk can be defined as a fixed span of time or as a fixed number of documents, as preferred by the user.",
                "For each incoming document, corpus statistics like the IDF (Inverted Document Frequency) of each term are updated.",
                "We use a state-of-the-art named entity identifier and tracker [8, 12] to identify person and location names, and merge them with co-referent named entities seen in the past.",
                "Then the documents are segmented into passages, which can be a whole document, a paragraph, a sentence, or any other continuous span of text, as preferred.",
                "Each passage is represented using a vector of TF-IDF (Term FrequencyInverse Document Frequency) weights, where term can be a word or a named entity.",
                "Given a query profile, i.e. the logistic regression solution wâ as described in Section 3.1, the system computes the posterior probability of relevance for each passage x as fRL(x) â¡ P(y = 1|x, wâ ) = 1 (1 + eâwâÂ·x) (1) Passages are ordered by their relevance scores, and the ones with scores above a threshold (tuned on a training set) comprise the relevance list that is passed on to the novelty detection step. 3.3 Novelty Detection Component CAFÂ´E maintains a user history H(t), which contains all the spans of text hi that the user highlighted (as feedback) during his or her past interactions with the system, up to the current time t. Denoting the history as H(t) = n h1, h2, ..., ht o , (2) the novelty score of a new candidate passage x is computed as: fND(x) = 1 â max iâ1..t {cos(x, hi)} (3) where both candidate passage x and highlighted spans of text hi are represented as TF-IDF vectors.",
                "The novelty score of each passage is compared to a prespecified threshold (also tuned on a training set), and any passage with a score below this threshold is removed from the relevance list. 3.4 Anti-redundant Ranking Component Although the novelty detection component ensures that only novel (previously unseen) information remains in the relevance list, this list might still contain the same novel information at multiple positions in the ranked list.",
                "Suppose, for example, that the user has already read about a $100,000 reward for information about the escaped convicts.",
                "A new piece of news that the award has been increased to $200,000 is novel since the user hasnt read about it yet.",
                "However, multiple news sources would report this news and we might end up showing (redundant) articles from all these sources in a ranked list.",
                "Hence, a ranked list should also be made non-redundant with respect to its own contents.",
                "We use a simplified version of the Maximal Marginal Relevance method [5], originally developed for combining relevance and novelty in text retrieval and summarization.",
                "Our procedure starts with the current list of passages sorted by relevance (section 3.2), filtered by Novelty Detection component (section 3.3), and generates a new non-redundant list as follows: 1.",
                "Take the top passage in the current list as the top one in the new list. 2.",
                "Add the next passage x in the current list to the new list only if fAR(x) > t where fAR(x) = 1 â max piâLnew {cos(x, pi)} and Lnew is the set of passages already selected in the new list. 3.",
                "Repeat step 2 until all the passages in the current list have been examined.",
                "After applying the above-mentioned algorithm, each passage in the new list is sufficiently dissimilar to others, thus favoring diversity rather than redundancy in the new ranked list.",
                "The anti-redundancy threshold t is tuned on a training set. 4.",
                "EVALUATION METHODOLOGY The approach we proposed above for information distillation raises important issues regarding evaluation methodology.",
                "Firstly, since our framework allows the output to be passages at different levels of granularity (e.g. k-sentence windows where k may vary) instead of a fixed length, it is not possible to have pre-annotated relevance judgments at all such granularity levels.",
                "Secondly, since we wish to measure the utility of the system output as a combination of both relevance and novelty, traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time.",
                "Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists.",
                "None of the existing measures in ad-hoc retrieval, adaptive filtering, novelty detection or other related areas (text summarization and question answering) have desirable properties in all the three aspects.",
                "Therefore, we must develop a new evaluation methodology. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers.",
                "Answer keys define what should be present in a system response to receive credit, and are comprised of a collection of information nuggets, i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them.",
                "Defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping [19], since system-returned passages can contain the same information expressed in many different ways.",
                "Hence, QA evaluations have relied on human assessors for the mapping between various expressions, making the process costly, time consuming, and not scalable to large query and document collections, and extensive system evaluations with various parameter settings. 4.1.1 Automating Evaluation based on Answer Keys Automatic evaluation methods would allow for faster system building and tuning, as well as provide an objective and affordable way of comparing various systems.",
                "Recently, such methods have been proposed, more or less, based on the idea of n-gram co-occurrences.",
                "Pourpre [10] assigns a fractional recall score to a system response based on its unigram overlap with a given nuggets description.",
                "For example, a system response A B C has recall 3/4 with respect to a nugget with description A B C D. However, such an approach is unfair to systems that present the same information but using words other than A, B, C, and D. Another open issue is how to weight individual words in measuring the closeness of a match.",
                "For example, consider the question How many prisoners escaped?.",
                "In the nugget Seven prisoners escaped from a Texas prison, there is no indication that seven is the keyword, and that it must be matched to get any relevance credit.",
                "Using IDF values does not help, since seven will generally not have a higher IDF than words like texas and prison.",
                "Also, redefining the nugget as just seven does not solve the problem since now it might spuriously match any mention of seven out of context.",
                "Nuggeteer [13] works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold.",
                "However, it is also plagued by spurious relevance since not all words contained in the nugget description (or known correct responses) are central to the nugget. 4.1.2 Nugget-Matching Rules We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on nugget-matching rules, which are generated using a semiautomatic procedure explained below.",
                "These rules are essentially Boolean queries that will only match against snippets that contain the nugget.",
                "For instance, a candidate rule for matching answers to How many prisoners escaped? is (Texas AND seven AND escape AND (convicts OR prisoners)), possibly with other synonyms and variants in the rule.",
                "For a corpus of news articles, which usually follow a typical formal prose, it is fairly easy to write such simple rules to match expected answers using a bootstrap approach, as described below.",
                "We propose a two-stage approach, inspired by Autoslog [14], that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents.",
                "In the first stage, human subjects annotated (using a highlighting tool) portions of ontopic documents that contained answers to each nugget 2 .",
                "In the second stage, subjects used our rule generation tool to create rules that would match the annotations for each nugget.",
                "The tool allows users to enter a Boolean rule as a disjunction of conjunctions (e.g. ((a AND b) OR (a AND c AND d) OR (e))).",
                "Given a candidate rule, our tool uses it as a Boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match.",
                "Hence, the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision.",
                "We observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions (adding more ANDs), and improving the recall by adding more conjunctions to the disjunction (adding more ORs).",
                "As an example, lets try to create a rule for the nugget which says that seven prisoners escaped from the Texas prison.",
                "We start with a simple rule - (seven).",
                "When we input this into the rule generation tool, we realize that this rule matches many spurious occurrences of seven (e.g. ...seven states...) and thus gets a low precision score.",
                "We can further qualify our rule - Texas AND seven AND convicts.",
                "Next, by looking at the missed annotations, we realize that some news articles mentioned ...seven prisoners escaped.... We then replace convicts with the disjunction (convicts OR prisoners).",
                "We continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the (small number of) misses and false alarms can be safely ignored.",
                "Thus we can create nugget-matching rules that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses.",
                "Human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably. 4.2 Evaluating the Utility of a Sequence of Ranked Lists The utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information, and how much the user lost in terms of time and energy.",
                "We calculate this utility from the utilities of individual passages as follows.",
                "After reading each passage returned by the system, the user derives some gain depending on the presence of relevant and novel information, and incurs a loss in terms of the time and energy spent in going through the passage.",
                "However, the likelihood that the user would actually read a passage depends on its position in the ranked list.",
                "Hence, for a query q, the expected utility 2 LDC [18] already provides relevance judgments for 100 topics on the TDT4 corpus.",
                "We further ensured that these judgments are exhaustive on the entire corpus using pooling. of a passage pi at rank i can be defined as U(pi, q) = P(i) â (Gain(pi, q) â Loss(pi, q)) (4) where P(i) is the probability that the user would go through a passage at rank i.",
                "The expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage: U(q) = nX i=1 P(i) â (Gain(pi, q) â Loss(pi, q)) (5) Note that if we ignore the loss term and define P(i) as P(i) â 1/ logb(b + i â 1) (6) then we get the recently popularized metric called Discounted Cumulated Gain (DCG) [9], where Gain(pi, q) is defined as the graded relevance of passage pi.",
                "However, without the loss term, DCG is a purely recall-oriented metric and not suitable for an adaptive filtering setting, where the systems utility depends in part on its ability to limit the number of items shown to the user.",
                "Although P(i) could be defined based on empirical studies of user behavior, for simplicity, we use P(i) exactly as defined in equation 6.",
                "The gain G(pi, q) of passage pi with respect to the query q is a function of - 1) the number of relevant nuggets present in pi, and 2) the novelty of each of these nuggets.",
                "We combine these two factors as follows.",
                "For each nugget Nj, we assign an initial weight wj, and also keep a count nj of the number of times this nugget has been seen by the user in the past.",
                "The gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor Î³.",
                "Thus, G(pi, q) is defined as G(pi, q) = X Nj âC(pi,q) wj â Î³nj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the answer key of query q.",
                "The initial weights wj are all set of be 1.0 in our experiments, but can also be set based on a pyramid approach [11].",
                "The choice of dampening factor Î³ determines the users tolerance for redundancy.",
                "When Î³ = 0, a nugget will only receive credit for its first occurrence i.e. when nj is zero3 .",
                "For 0 < Î³ < 1, a nugget receives smaller credit for each successive occurrence.",
                "When Î³ = 1, no dampening occurs and repeated occurrences of a nugget receive the same credit.",
                "Note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system, since the users are expected to remember what the system showed them in the past.",
                "We define the loss L(pi, q) as a constant cost c (we use 0.1) incurred when reading a system-returned passage.",
                "Thus, our metric can be re-written as U(q) = nX i=1 Gain(pi, q) logb(b + i â 1) â L(n) (8) where L(n) is the loss associated with a ranked list of length n: L(n) = c Â· nX i=1 1 logb(b + i â 1) (9) 3 Note that 00 = 1 Due to the similarity with Discounted Cumulated Gain (DCG), we call our metric Discounted Cumulated Utility (DCU).",
                "The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) â¤ 0 i.e. when the gain is less than or equal to the cost of reading the passage. 5.",
                "DATA TDT4 was the benchmark corpus used in TDT2002 and TDT2003 evaluations.",
                "The corpus consists of over 90, 000 news articles from multiple sources (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) published between October 2000 and January 2001, in the languages of Arabic, English, and Mandarin.",
                "Speech-recognized and machine-translated versions of the non-English articles were provided as well.",
                "LDC [18] has annotated the corpus with 100 topics, that correspond to various news events in this time period.",
                "Out of these, we selected a subset of 12 actionable events, and defined corresponding tasks for them4 .",
                "For each task, we manually defined a profile consisting of an initial set of (5 to 10) queries, a free-text description of the user history, i.e., what the user already knows about the event, and a list of known on-topic and off-topic documents (if available) as training examples.",
                "For each query, we generated answer keys and corresponding nugget matching rules using the procedure described in section 4.1.2, and produced a total of 120 queries, with an average of 7 nuggets per query. 6.",
                "EXPERIMENTS AND RESULTS 6.1 Baselines We used Indri [17], a popular language-model based retrieval engine, as a baseline for comparison with CAFÂ´E.",
                "Indri supports standard search engine functionality, including pseudo-relevance feedback (PRF) [3, 6], and is representative of a typical query-based retrieval system.",
                "Indri does not support any kind of novelty detection.",
                "We compare Indri with PRF turned on and off, against CAFÂ´E with user feedback, novelty detection and antiredundant ranking turned on and off. 6.2 Experimental Setup We divided the TDT4 corpus spanning 4 months into 10 chunks, each defined as a period of 12 consecutive days.",
                "At any given point of time in the distillation process, each system accessed the past data up to the current point, and returned a ranked list of up 50 passages per query.",
                "The 12 tasks defined on the corpus were divided into a training and test set with 6 tasks each.",
                "Each system was allowed to use the training set to tune its parameters for optimizing NDCU (equation 8), including the relevance threshold for both Indri and CAFÂ´E, and the novelty and antiredundancy thresholds for CAFÂ´E.",
                "The NDCU for each system run is calculated automatically.",
                "User feedback was also simulated - relevance judgments for each system-returned passage (as determined by the nugget matching rules described in section 4.1.2) were 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figure 1: Performance of Indri across chunks Figure 2: Performance of CAFÂ´E across chunks used as user feedback in the adaptation of query profiles and user histories. 6.3 Results In Table 1, we show the NDCU scores of the two systems under various settings.",
                "These scores are averaged over the six tasks in the test set, and are calculated with two dampening factors (see section 4.2): Î³ = 0 and 0.1, to simulate no tolerance and small tolerance for redundancy, respectively.",
                "Using Î³ = 0 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information.",
                "Hence, the improvement obtained from enabling user feedback is smaller with Î³ = 0 than the improvement obtained from feedback with Î³ = 0.1.",
                "This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information.",
                "However, the user does not benefit from seeing such redundant passages, and is usually interested in other passages containing related information.",
                "It is informative to evaluate retrieval systems using our utility measure (with Î³ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.",
                "Sometimes, however, users might indeed be interested in seeing the same information from multiple sources, as an Table 1: NDCU Scores of Indri and CAFÂ´E for two dampening factors (Î³), and various settings (F: Feedback, N: Novelty Detection, A: Anti-Redundant Ranking) Indri CAFÂ´E Î³ Base +PRF Base +F +F+N +F+A +F+N+A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.35 0.36 0.36 indicator of its importance or reliability.",
                "In such a case, we can simply choose a higher value for Î³ which corresponds to a higher tolerance for redundancy, and hence let the system tune its parameters accordingly.",
                "Since documents were processed chunk by chunk, it would be interesting to see how the performance of systems improves over time.",
                "Figures 1 and 2 show the performance trends for both the systems across chunks.",
                "While the performance with and without feedback on the first few chunks is expected to be close, for subsequent chunks, the performance curve with feedback enabled rises above the one with the no-feedback setting.",
                "The performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks, making some queries easier than others in certain chunks.",
                "Moreover, since Indri uses pseudo-relevance feedback while CAFÂ´E uses feedback based on actual relevance judgments, the improvement in case of Indri is less dramatic than that of CAFÂ´E. 7.",
                "CONCLUDING REMARKS This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages.",
                "Our system, called CAFÂ´E, combines adaptive filtering, novelty detection and antiredundant passage ranking in a <br>unified framework</br> for utility optimization.",
                "We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty.",
                "Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off. 8.",
                "ACKNOWLEDGMENTS We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments.",
                "This work is supported in parts by the National Science Foundation (NSF) under grant IIS0434035, and the Defense Advanced Research Project Agency (DARPA) under contracts NBCHD030010 and W0550432.",
                "Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. 9.",
                "ADDITIONAL AUTHORS Jian Zhang (jianzhan@stat.purdue.edu)â , Jaime Carbonell (jgc@cs.cmu.edu)â  , Peter Brusilovsky (peterb+@pitt.edu)â¡ , Daqing He(dah44@pitt.edu)â¡ 10.",
                "REFERENCES [1] J. Allan.",
                "Incremental Relevance Feedback for Information Filtering.",
                "Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 270-278, 1996. [2] J. Allan, C. Wade, and A. Bolivar.",
                "Retrieval and Novelty Detection at the Sentence Level.",
                "Proceedings of the ACM SIGIR conference on research and development in information retrieval, 2003. [3] C. Buckley, G. Salton, and J. Allan.",
                "Automatic Retrieval with Locality Information using SMART.",
                "NIST special publication, (500207):59-72, 1993. [4] J. Callan.",
                "Learning While Filtering Documents.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 224-231, 1998. [5] J. Carbonell and J. Goldstein.",
                "The use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335-336, 1998. [6] E. Efthimiadis.",
                "Query Expansion.",
                "Annual Review of Information Science and Technology (ARIST), 31:p121-87, 1996. [7] J. Fiscus and G. Duddington.",
                "Topic Detection and Tracking Overview.",
                "Topic Detection and Tracking: Event-based Information Organization, pages 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.",
                "A Statistical Model for Multilingual Entity Detection and Tracking.",
                "NAACL/HLT, 2004. [9] K. JÂ¨arvelin and J. KekÂ¨alÂ¨ainen.",
                "Cumulated Gain-based Evaluation of IR Techniques.",
                "ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin and D. Demner-Fushman.",
                "Automatically Evaluating Answers to Definition Questions.",
                "Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), 2005. â Statistics Dept., Purdue University, West Lafayette, USA â  Language Technologies Inst., Carnegie Mellon University, Pittsburgh, USA â¡ School of Information Sciences, Univ. of Pittsburgh, Pittsburgh, USA [11] J. Lin and D. Demner-Fushman.",
                "Will Pyramids Built of nUggets Topple Over.",
                "Proceedings of HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.",
                "A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree.",
                "Proc. of ACL, 4:136-143, 2004. [13] G. Marton.",
                "Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments.",
                "HLT/NAACL, 2006. [14] E. Riloff.",
                "Automatically Constructing a Dictionary for Information Extraction Tasks.",
                "Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, 1993. [15] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9: Filtering track.",
                "The Ninth Text REtrieval Conference (TREC-9), pages 361-368. [16] R. Schapire, Y.",
                "Singer, and A. Singhal.",
                "Boosting and Rocchio Applied to Text Filtering.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle, and W. Croft.",
                "Indri: A Language Model-based Search Engine for Complex Queries.",
                "Proceedings of the International Conference on Intelligence Analysis, 2004. [18] The Linguistic Data Consortium. http://www.ldc.upenn.edu/. [19] E. Voorhees.",
                "Overview of the TREC 2003 Question Answering Track.",
                "Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), 2003. [20] Y. Yang and B. Kisiel.",
                "Margin-based Local Regression for Adaptive Filtering.",
                "Proceedings of the twelfth international conference on Information and knowledge management, pages 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of Adaptive Filtering Methods in a Cross-benchmark Evaluation.",
                "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 98-105, 2005. [22] C. Zhai, W. Cohen, and J. Lafferty.",
                "Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-17, 2003. [23] J. Zhang and Y. Yang.",
                "Robustness of Regularized Linear Classification Methods in Text Categorization.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 190-197, 2003. [24] Y. Zhang.",
                "Using Bayesian Priors to Combine Classifiers for Adaptive Filtering.",
                "Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 345-352, 2004. [25] Y. Zhang, J. Callan, and T. Minka.",
                "Novelty and Redundancy Detection in Adaptive Filtering.",
                "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002."
            ],
            "original_annotated_samples": [
                "Our system, called CAFÂ´E, combines adaptive filtering, novelty detection and antiredundant passage ranking in a <br>unified framework</br> for utility optimization."
            ],
            "translated_annotated_samples": [
                "Nuestro sistema, llamado CAFÂ´E, combina filtrado adaptativo, detecciÃ³n de novedades y clasificaciÃ³n de pasajes antirredundantes en un <br>marco unificado</br> para la optimizaciÃ³n de la utilidad."
            ],
            "translated_text": "DestilaciÃ³n de informaciÃ³n basada en utilidad sobre documentos secuenciados temporalmente en el Instituto de TecnologÃ­as del Lenguaje de Yiming Yang. Universidad Carnegie Mellon, Pittsburgh, EE. UU. yiming@cs.cmu.edu Abhimanyu Lad Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. alad@cs.cmu.edu Instituto de TecnologÃ­as del Lenguaje Ni Lao. Universidad Carnegie Mellon, Pittsburgh, EE. UU. nlao@cs.cmu.edu Abhay Harpale Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. aharpale@cs.cmu.edu Bryan Kisiel Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. bkisiel@cs.cmu.edu MÃ³nica Rogati Instituto de TecnologÃ­as del Lenguaje. La Universidad Carnegie Mellon en Pittsburgh, EE. UU. mrogati@cs.cmu.edu RESUMEN Este artÃ­culo examina un nuevo enfoque para la destilaciÃ³n de informaciÃ³n en documentos ordenados temporalmente, y propone un esquema de evaluaciÃ³n novedoso para dicho marco. Combina las fortalezas y se extiende mÃ¡s allÃ¡ del filtrado adaptativo convencional, la detecciÃ³n de novedades y la clasificaciÃ³n de pasajes no redundantes con respecto a las necesidades de informaciÃ³n duraderas (tareas con mÃºltiples consultas). Nuestro enfoque respalda la retroalimentaciÃ³n detallada del usuario a travÃ©s del resaltado de fragmentos arbitrarios de texto, y aprovecha dicha informaciÃ³n para la optimizaciÃ³n de la utilidad en entornos adaptativos. Para nuestros experimentos, definimos tareas hipotÃ©ticas basadas en eventos de noticias en el corpus TDT4, con mÃºltiples consultas por tarea. Se generaron claves de respuestas (nuggets) para cada consulta y se utilizÃ³ un procedimiento semiautomÃ¡tico para adquirir reglas que permiten emparejar automÃ¡ticamente los nuggets con las respuestas del sistema. TambiÃ©n proponemos una extensiÃ³n de la mÃ©trica NDCG para evaluar la utilidad de los pasajes clasificados como una combinaciÃ³n de relevancia y novedad. Nuestros resultados muestran mejoras alentadoras en la utilidad utilizando el nuevo enfoque, en comparaciÃ³n con los sistemas base sin aprendizaje incremental o componentes de detecciÃ³n de novedades. CategorÃ­as y Descriptores de Asignaturas H.3.3 [BÃºsqueda y RecuperaciÃ³n de InformaciÃ³n]: Filtrado de informaciÃ³n, RetroalimentaciÃ³n de relevancia, Modelos de recuperaciÃ³n, Proceso de selecciÃ³n; I.5.2 TÃ©rminos Generales DiseÃ±o, MediciÃ³n, Rendimiento, ExperimentaciÃ³n. 1. INTRODUCCIÃN Seguir informaciÃ³n nueva y relevante de flujos de datos temporales para usuarios con necesidades duraderas ha sido un tema de investigaciÃ³n desafiante en la recuperaciÃ³n de informaciÃ³n. El filtrado adaptativo (AF) es una tarea de predicciÃ³n en lÃ­nea de la relevancia de cada nuevo documento con respecto a temas predefinidos. BasÃ¡ndose en la consulta inicial y algunos ejemplos positivos (si estÃ¡n disponibles), un sistema de AF mantiene un perfil para cada tema de interÃ©s, y lo actualiza constantemente segÃºn la retroalimentaciÃ³n del usuario. La naturaleza de aprendizaje incremental de los sistemas de AF los hace mÃ¡s poderosos que los motores de bÃºsqueda estÃ¡ndar que admiten recuperaciÃ³n ad-hoc (por ejemplo. Google y Yahoo) en tÃ©rminos de encontrar informaciÃ³n relevante con respecto a temas de interÃ©s duraderos, y mÃ¡s atractivo para los usuarios que estÃ¡n dispuestos a proporcionar retroalimentaciÃ³n para adaptar el sistema a sus necesidades de informaciÃ³n especÃ­ficas, sin tener que modificar sus consultas manualmente. Se han estudiado una variedad de algoritmos de aprendizaje supervisado (clasificadores de estilo Rocchio, modelos exponenciales-gaussianos, regresiÃ³n local y enfoques de regresiÃ³n logÃ­stica) para entornos adaptativos, examinados con retroalimentaciÃ³n de relevancia explÃ­cita e implÃ­cita, y evaluados con respecto a la optimizaciÃ³n de la utilidad en grandes colecciones de datos de referencia en los foros de TREC (Conferencias de RecuperaciÃ³n de Texto) y TDT (DetecciÃ³n y Seguimiento de Temas) [1, 4, 7, 15, 16, 20, 24, 23]. La regresiÃ³n logÃ­stica regularizada [21] se ha encontrado representativa para los enfoques de vanguardia, y altamente eficiente para adaptaciones frecuentes del modelo en grandes colecciones de documentos como el corpus TREC-10 (mÃ¡s de 800,000 documentos y 84 temas). A pesar de los logros sustanciales en la investigaciÃ³n reciente de filtrado adaptativo, aÃºn quedan problemas significativos sin resolver sobre cÃ³mo aprovechar de manera efectiva y eficiente la retroalimentaciÃ³n del usuario. EspecÃ­ficamente, los siguientes problemas pueden limitar seriamente la verdadera utilidad de los sistemas de AF en aplicaciones del mundo real: 1. El usuario tiene un papel bastante pasivo en la configuraciÃ³n convencional de filtrado adaptativo: solo reacciona ante el sistema cuando este toma una decisiÃ³n afirmativa sobre un documento, confirmando o rechazando esa decisiÃ³n. Una alternativa mÃ¡s activa serÃ­a permitir al usuario emitir mÃºltiples consultas sobre un tema, revisar una lista clasificada de documentos candidatos (o pasajes) por consulta, y proporcionar retroalimentaciÃ³n sobre la lista clasificada, refinando asÃ­ su necesidad de informaciÃ³n y solicitando listas clasificadas actualizadas. La Ãºltima forma de interacciÃ³n del usuario ha sido altamente efectiva en la recuperaciÃ³n estÃ¡ndar para consultas ad-hoc. CÃ³mo implementar una estrategia para necesidades de informaciÃ³n duraderas en entornos de FA es una pregunta abierta para la investigaciÃ³n. 2. La unidad para recibir un juicio de relevancia (sÃ­ o no) estÃ¡ restringida al nivel del documento en AF convencional. Sin embargo, un usuario real puede estar dispuesto a proporcionar comentarios mÃ¡s informativos y detallados resaltando algunas partes de texto en un documento recuperado como relevantes, en lugar de etiquetar todo el documento como relevante. Aprovechar de manera efectiva este tipo de retroalimentaciÃ³n detallada podrÃ­a mejorar sustancialmente la calidad de un sistema de AF. Para esto, necesitamos habilitar el aprendizaje supervisado a partir de fragmentos de texto etiquetados de longitud arbitraria en lugar de solo permitir documentos etiquetados. 3. Los documentos seleccionados por el sistema suelen ser altamente redundantes. Un evento importante de noticias, por ejemplo, serÃ­a reportado por mÃºltiples fuentes repetidamente durante un tiempo, haciendo que la mayor parte del contenido informativo en esos artÃ­culos sea redundante entre sÃ­. Un sistema de AF convencional seleccionarÃ­a todas estas historias de noticias redundantes para la retroalimentaciÃ³n del usuario, desperdiciando el tiempo de los usuarios mientras ofrece poco beneficio. Claramente, las tÃ©cnicas de detecciÃ³n de novedades pueden ayudar en principio [25, 2, 22] a mejorar la utilidad de los sistemas AF. Sin embargo, la efectividad de tales tÃ©cnicas a nivel de pasaje para detectar novedad con respecto a la retroalimentaciÃ³n de los usuarios a nivel detallado y para detectar redundancia en listas clasificadas aÃºn debe ser evaluada utilizando una medida de utilidad que imite las necesidades de un usuario real. Para abordar las limitaciones actuales de los sistemas de AF mencionadas anteriormente, proponemos y examinamos un nuevo enfoque en este artÃ­culo, que combina las fortalezas del AF convencional (aprendizaje incremental de modelos de temas), recuperaciÃ³n de pasajes multipase para consultas de larga duraciÃ³n condicionadas al tema, y detecciÃ³n de novedades para eliminar la redundancia de las interacciones del usuario con el sistema. Llamamos al nuevo proceso destilaciÃ³n de informaciÃ³n basada en utilidad. Ten en cuenta que los corpus de referencia convencionales para evaluaciones de AF, que tienen juicios de relevancia a nivel de documento y no definen tareas con mÃºltiples consultas, son insuficientes para evaluar el nuevo enfoque. Por lo tanto, ampliamos un corpus de referencia, la colecciÃ³n TDT4 de noticias y transmisiones de televisiÃ³n, con definiciones de tareas, mÃºltiples consultas por tarea y claves de respuestas por consulta. Hemos realizado nuestros experimentos en este corpus extendido TDT4 y hemos puesto a disposiciÃ³n del pÃºblico los datos generados adicionalmente para futuras evaluaciones comparativas. Para evaluar automÃ¡ticamente los fragmentos de texto arbitrarios devueltos por el sistema utilizando nuestras claves de respuestas, desarrollamos un esquema de evaluaciÃ³n con un procedimiento semiautomÃ¡tico para 1 URL: http://nyc.lti.cs.cmu.edu/downloads adquiriendo reglas que pueden comparar fragmentos con las respuestas del sistema. AdemÃ¡s, proponemos una extensiÃ³n de NDCG (Normalized Discounted Cumulated Gain) [9] para evaluar la utilidad de los pasajes clasificados como una funciÃ³n tanto de la relevancia como de la novedad. El resto de este documento estÃ¡ organizado de la siguiente manera. La secciÃ³n 2 describe el proceso de destilaciÃ³n de informaciÃ³n con un ejemplo concreto. La secciÃ³n 3 describe los nÃºcleos tÃ©cnicos de nuestro sistema llamado CAFÂ´E - CMU Adaptive Filtering Engine. La secciÃ³n 4 discute problemas con respecto a la metodologÃ­a de evaluaciÃ³n y propone un nuevo esquema. La secciÃ³n 5 describe el corpus extendido TDT4. La secciÃ³n 6 presenta nuestros experimentos y resultados. La secciÃ³n 7 concluye el estudio y ofrece perspectivas futuras. 2. Una TAREA DE EJEMPLO Considera un evento noticioso: la fuga de siete convictos de una prisiÃ³n de Texas en diciembre de 2000 y su captura un mes despuÃ©s. Suponiendo que un usuario estuviera interesado en este evento desde sus primeras etapas, la necesidad de informaciÃ³n podrÃ­a ser: Encontrar informaciÃ³n sobre la fuga de convictos de una prisiÃ³n de Texas, y la informaciÃ³n relacionada con su recaptura. Las preguntas de nivel inferior asociadas podrÃ­an ser: 1. Â¿CuÃ¡ntos prisioneros escaparon? 2. Â¿DÃ³nde y cuÃ¡ndo fueron avistados? 3. Â¿QuiÃ©nes son sus contactos conocidos dentro y fuera de la prisiÃ³n? 4. Â¿CÃ³mo estÃ¡n armados? 5. Â¿Tienen algÃºn vehÃ­culo? 6. Â¿QuÃ© pasos se han tomado hasta ahora? Llamamos a esta necesidad de informaciÃ³n una tarea, y a las preguntas asociadas como las consultas en esta tarea. Un sistema de destilaciÃ³n debe monitorear los documentos entrantes, procesarlos por fragmentos en un orden temporal, seleccionar pasajes potencialmente relevantes y novedosos de cada fragmento con respecto a cada consulta, y presentar una lista clasificada de pasajes al usuario. La clasificaciÃ³n de pasajes aquÃ­ se basa en cuÃ¡n relevante es un pasaje con respecto a la consulta actual, cuÃ¡n novedoso es con respecto al historial actual del usuario (de sus interacciones con el sistema) y cuÃ¡n redundante es en comparaciÃ³n con otros pasajes con un rango mÃ¡s alto en la lista. Cuando se le presenta una lista de pasajes, el usuario puede proporcionar retroalimentaciÃ³n resaltando tramos de texto arbitrarios que considere relevantes. Estos fragmentos de texto se toman como ejemplos positivos en la adaptaciÃ³n del perfil de consulta, y tambiÃ©n se aÃ±aden al historial de los usuarios. Los pasajes no marcados por el usuario se consideran ejemplos negativos. Tan pronto como se actualiza el perfil de consulta, el sistema vuelve a realizar una bÃºsqueda y devuelve otra lista clasificada de pasajes donde los pasajes previamente vistos son eliminados o clasificados como bajos, segÃºn la preferencia del usuario. Por ejemplo, si el usuario resalta ...los funcionarios han ofrecido una recompensa de $100,000 por su captura... como respuesta relevante a la consulta Â¿QuÃ© pasos se han tomado hasta ahora?, entonces la pieza resaltada se utiliza como un ejemplo de entrenamiento positivo adicional en la adaptaciÃ³n del perfil de la consulta. Este comentario tambiÃ©n se aÃ±ade al historial del usuario como un ejemplo visto, para que en el futuro, el sistema no coloque otro pasaje mencionando una recompensa de $100,000 en la parte superior de la lista clasificada. Sin embargo, un artÃ­culo que mencione que los funcionarios han duplicado el dinero de recompensa a $200,000 podrÃ­a clasificarse alto ya que es relevante tanto para el perfil de consulta (actualizado) como novedoso con respecto a la historia del usuario (actualizada). El usuario puede modificar las consultas originales o agregar una nueva consulta durante el proceso; los perfiles de consulta se cambiarÃ¡n en consecuencia. Claramente, la detecciÃ³n de novedades es muy importante para la utilidad de dicho sistema debido a la bÃºsqueda iterativa. Sin detecciÃ³n de novedad, los antiguos pasajes relevantes se mostrarÃ­an al usuario repetidamente en cada lista clasificada. A travÃ©s del ejemplo anterior, podemos ver las principales propiedades de nuestro nuevo marco de trabajo para la destilaciÃ³n de informaciÃ³n basada en utilidad sobre documentos ordenados temporalmente. Nuestro marco de trabajo combina y amplÃ­a el poder del filtrado adaptativo (AF), la recuperaciÃ³n ad-hoc (IR) y la detecciÃ³n de novedades (ND). En comparaciÃ³n con la IR estÃ¡ndar, nuestro enfoque tiene la capacidad de aprender de forma incremental las necesidades de informaciÃ³n a largo plazo y modelar una secuencia de consultas dentro de una tarea. En comparaciÃ³n con el AF convencional, permite un papel mÃ¡s activo del usuario en refinar sus necesidades de informaciÃ³n y solicitar nuevos resultados al permitir comentarios sobre relevancia y novedad mediante el resaltado de fragmentos arbitrarios de texto en los pasajes devueltos por el sistema. Comparado con trabajos anteriores, esta es la primera evaluaciÃ³n de detecciÃ³n de novedad integrada con filtrado adaptativo para consultas secuenciadas que permite retroalimentaciÃ³n flexible del usuario sobre pasajes clasificados. La combinaciÃ³n de AF, IR y ND con las nuevas extensiones plantea una importante pregunta de investigaciÃ³n sobre la metodologÃ­a de evaluaciÃ³n: Â¿cÃ³mo podemos medir la utilidad de un sistema de destilaciÃ³n de informaciÃ³n como este? Las mÃ©tricas existentes en IR estÃ¡ndar, AF y ND son insuficientes, y se deben explorar nuevas soluciones, como discutiremos en la SecciÃ³n 4, despuÃ©s de describir los nÃºcleos tÃ©cnicos de nuestro sistema en la prÃ³xima secciÃ³n. 3. NÃCLEOS TÃCNICOS Los componentes principales de CAFÃ son: 1) AF para el aprendizaje incremental de perfiles de consulta, 2) IR para estimar la relevancia de pasajes con respecto a los perfiles de consulta, 3) ND para evaluar la novedad de pasajes con respecto al historial de usuarios, y 4) un componente anti-redundancia para eliminar la redundancia de las listas clasificadas. 3.1 Componente de Filtrado Adaptativo Utilizamos un algoritmo de vanguardia en el campo, el mÃ©todo de regresiÃ³n logÃ­stica regularizada que obtuvo los mejores resultados en varios corpus de evaluaciÃ³n de referencia para AF [21]. La regresiÃ³n logÃ­stica (LR) es un algoritmo de aprendizaje supervisado para clasificaciÃ³n estadÃ­stica. BasÃ¡ndose en un conjunto de entrenamiento de instancias etiquetadas, aprende un modelo de clase que luego puede ser utilizado para predecir las etiquetas de instancias no vistas. Su rendimiento, asÃ­ como su eficiencia en tÃ©rminos de tiempo de entrenamiento, lo convierten en un buen candidato cuando se requieren actualizaciones frecuentes del modelo de clase, como es el caso en el filtrado adaptativo, donde el sistema debe aprender de cada nueva retroalimentaciÃ³n proporcionada por el usuario. (Consulte [21] y [23] para complejidad computacional y problemas de implementaciÃ³n). En el filtrado adaptativo, cada consulta se considera como una clase y la probabilidad de que un pasaje pertenezca a esta clase corresponde al grado de relevancia del pasaje con respecto a la consulta. Para entrenar el modelo, utilizamos la propia consulta como ejemplo inicial de entrenamiento positivo de la clase, y las piezas de texto resaltadas por el usuario (marcadas como Relevante o No relevante) durante la retroalimentaciÃ³n como ejemplos de entrenamiento adicionales. Para abordar el problema de inicio en frÃ­o en la etapa inicial antes de obtener cualquier retroalimentaciÃ³n del usuario, el sistema utiliza una pequeÃ±a muestra de un corpus retrospectivo como ejemplos negativos iniciales en el conjunto de entrenamiento. Los detalles de cÃ³mo utilizar la regresiÃ³n logÃ­stica para el filtrado adaptativo (asignando diferentes pesos a las instancias de entrenamiento positivas y negativas, y regularizando la funciÃ³n objetivo para prevenir el sobreajuste en los datos de entrenamiento) se presentan en [21]. El modelo de clase wâ aprendido por RegresiÃ³n LogÃ­stica, o el perfil de consulta, es un vector cuyas dimensiones son tÃ©rminos individuales y cuyos elementos son los coeficientes de regresiÃ³n, indicando cuÃ¡n influyente es cada tÃ©rmino en el perfil de consulta. El perfil de la consulta se actualiza cada vez que se recibe una nueva pieza de retroalimentaciÃ³n del usuario. Un peso decaÃ­do temporalmente se puede aplicar a cada ejemplo de entrenamiento, como una opciÃ³n, para enfatizar la retroalimentaciÃ³n mÃ¡s reciente del usuario. 3.2 Componente de RecuperaciÃ³n de Pasajes Utilizamos tÃ©cnicas estÃ¡ndar de IR en esta parte de nuestro sistema. Los documentos entrantes se procesan por bloques, donde cada bloque puede definirse como un intervalo de tiempo fijo o como un nÃºmero fijo de documentos, segÃºn lo prefiera el usuario. Para cada documento entrante, se actualizan las estadÃ­sticas del corpus como el IDF (Frecuencia Inversa de Documento) de cada tÃ©rmino. Utilizamos un identificador y rastreador de entidades con tecnologÃ­a de punta [8, 12] para identificar nombres de personas y lugares, y fusionarlos con entidades con referencia cruzada vistas en el pasado. Luego, los documentos se dividen en pasajes, que pueden ser un documento completo, un pÃ¡rrafo, una oraciÃ³n, o cualquier otro fragmento continuo de texto, segÃºn se prefiera. Cada pasaje se representa utilizando un vector de pesos TF-IDF (Frecuencia de TÃ©rmino - Frecuencia Inversa de Documento), donde el tÃ©rmino puede ser una palabra o una entidad nombrada. Dado un perfil de consulta, es decir, la soluciÃ³n de regresiÃ³n logÃ­stica wâ como se describe en la SecciÃ³n 3.1, el sistema calcula la probabilidad posterior de relevancia para cada pasaje x como fRL(x) â¡ P(y = 1|x, wâ ) = 1 (1 + eâwâÂ·x) (1). Los pasajes se ordenan segÃºn sus puntajes de relevancia, y aquellos con puntajes por encima de un umbral (ajustado en un conjunto de entrenamiento) conforman la lista de relevancia que se pasa al paso de detecciÃ³n de novedad. 3.3 Componente de DetecciÃ³n de Novedad CAFÂ´E mantiene un historial de usuario H(t), que contiene todos los fragmentos de texto hi que el usuario resaltÃ³ (como retroalimentaciÃ³n) durante sus interacciones pasadas con el sistema, hasta el tiempo actual t. Denotando el historial como H(t) = n h1, h2, ..., ht o , (2), el puntaje de novedad de un nuevo pasaje candidato x se calcula como: fND(x) = 1 â max iâ1..t {cos(x, hi)} (3), donde tanto el pasaje candidato x como los fragmentos de texto resaltados hi se representan como vectores TF-IDF. El puntaje de novedad de cada pasaje se compara con un umbral predefinido (ajustado tambiÃ©n en un conjunto de entrenamiento), y cualquier pasaje con un puntaje por debajo de este umbral se elimina de la lista de relevancia. Componente de clasificaciÃ³n anti-redundante Aunque el componente de detecciÃ³n de novedad garantiza que solo la informaciÃ³n novedosa (no vista previamente) permanezca en la lista de relevancia, esta lista aÃºn podrÃ­a contener la misma informaciÃ³n novedosa en mÃºltiples posiciones en la lista clasificada. Supongamos, por ejemplo, que el usuario ya ha leÃ­do sobre una recompensa de $100,000 por informaciÃ³n sobre los convictos fugados. Una nueva noticia de que el premio ha sido aumentado a $200,000 es novedosa ya que el usuario aÃºn no la ha leÃ­do. Sin embargo, mÃºltiples fuentes de noticias podrÃ­an informar sobre esta noticia y podrÃ­amos terminar mostrando artÃ­culos (redundantes) de todas estas fuentes en una lista clasificada. Por lo tanto, una lista clasificada tambiÃ©n debe ser no redundante con respecto a su propio contenido. Utilizamos una versiÃ³n simplificada del mÃ©todo de MÃ¡xima Relevancia Marginal [5], originalmente desarrollado para combinar relevancia y novedad en la recuperaciÃ³n de texto y resÃºmenes. Nuestro procedimiento comienza con la lista actual de pasajes ordenados por relevancia (secciÃ³n 3.2), filtrados por el componente de DetecciÃ³n de Novedades (secciÃ³n 3.3), y genera una nueva lista no redundante de la siguiente manera: 1. Toma el pasaje superior en la lista actual como el primero en la nueva lista. 2. Agrega el siguiente pasaje x en la lista actual a la nueva lista solo si fAR(x) > t donde fAR(x) = 1 â max piâLnew {cos(x, pi)} y Lnew es el conjunto de pasajes ya seleccionados en la nueva lista. Repetir el paso 2 hasta que se hayan examinado todos los pasajes en la lista actual. DespuÃ©s de aplicar el algoritmo mencionado anteriormente, cada pasaje en la nueva lista es lo suficientemente diferente a los demÃ¡s, favoreciendo asÃ­ la diversidad en lugar de la redundancia en la nueva lista clasificada. El umbral anti-redundancia t se ajusta en un conjunto de entrenamiento. 4. METODOLOGÃA DE EVALUACIÃN El enfoque que propusimos anteriormente para la destilaciÃ³n de informaciÃ³n plantea importantes cuestiones sobre la metodologÃ­a de evaluaciÃ³n. En primer lugar, dado que nuestro marco de trabajo permite que la salida sean fragmentos en diferentes niveles de granularidad (por ejemplo, ventanas de k oraciones donde k puede variar) en lugar de una longitud fija, no es posible tener juicios de relevancia preanotados en todos esos niveles de granularidad. En segundo lugar, dado que deseamos medir la utilidad de la salida del sistema como una combinaciÃ³n de relevancia y novedad, las medidas tradicionales basadas Ãºnicamente en la relevancia deben ser reemplazadas por medidas que penalicen la repeticiÃ³n de la misma informaciÃ³n en la salida del sistema a lo largo del tiempo. En tercer lugar, dado que la salida del sistema son listas clasificadas, debemos recompensar a aquellos sistemas que presenten informaciÃ³n Ãºtil (tanto relevante como no vista previamente) utilizando listas clasificadas mÃ¡s cortas, y penalizar a aquellos que presenten la misma informaciÃ³n utilizando listas clasificadas mÃ¡s largas. Ninguna de las medidas existentes en la recuperaciÃ³n ad-hoc, filtrado adaptativo, detecciÃ³n de novedades u otras Ã¡reas relacionadas (resumen de texto y respuesta a preguntas) tiene propiedades deseables en los tres aspectos. Por lo tanto, debemos desarrollar una nueva metodologÃ­a de evaluaciÃ³n. 4.1 Claves de Respuesta Para permitir la evaluaciÃ³n de un sistema cuya salida consiste en pasajes de longitud arbitraria, tomamos prestado el concepto de claves de respuesta de la comunidad de Preguntas y Respuestas (QA), donde los sistemas pueden devolver fragmentos de texto arbitrarios como respuestas. Las claves de respuestas definen lo que debe estar presente en la respuesta de un sistema para recibir crÃ©dito, y estÃ¡n compuestas por una colecciÃ³n de \"nuggets\" de informaciÃ³n, es decir, unidades de datos sobre las cuales los evaluadores humanos pueden tomar decisiones binarias sobre si una respuesta del sistema las contiene o no. La definiciÃ³n de claves de respuestas y la toma de decisiones binarias asociadas son tareas conceptuales que requieren un mapeo semÃ¡ntico [19], ya que los pasajes devueltos por el sistema pueden contener la misma informaciÃ³n expresada de muchas formas diferentes. Por lo tanto, las evaluaciones de QA han dependido de evaluadores humanos para la correspondencia entre diversas expresiones, lo que hace que el proceso sea costoso, consume mucho tiempo y no sea escalable para grandes colecciones de consultas y documentos, y evaluaciones extensas de sistemas con diversos ajustes de parÃ¡metros. 4.1.1 AutomatizaciÃ³n de la evaluaciÃ³n basada en claves de respuestas Los mÃ©todos de evaluaciÃ³n automÃ¡tica permitirÃ­an una construcciÃ³n y ajuste mÃ¡s rÃ¡pidos del sistema, asÃ­ como proporcionar una forma objetiva y asequible de comparar diversos sistemas. Recientemente, se han propuesto mÃ©todos basados mÃ¡s o menos en la idea de las co-ocurrencias de n-gramos. Pourpre [10] asigna una puntuaciÃ³n de recuerdo fraccional a una respuesta del sistema basada en su superposiciÃ³n de unigramas con una descripciÃ³n de pepitas dada. Por ejemplo, una respuesta del sistema A B C tiene una recuperaciÃ³n de 3/4 con respecto a un fragmento con la descripciÃ³n A B C D. Sin embargo, este enfoque es injusto para los sistemas que presentan la misma informaciÃ³n pero utilizando palabras distintas a A, B, C y D. Otro problema abierto es cÃ³mo ponderar las palabras individuales al medir la cercanÃ­a de una coincidencia. Por ejemplo, considera la pregunta Â¿CuÃ¡ntos prisioneros escaparon? En el fragmento Siete prisioneros escaparon de una prisiÃ³n de Texas, no hay indicaciÃ³n de que siete sea la palabra clave y que debe coincidir para obtener algÃºn crÃ©dito de relevancia. El uso de los valores de IDF no ayuda, ya que la palabra \"seven\" generalmente no tendrÃ¡ un IDF mÃ¡s alto que palabras como \"texas\" y \"prison\". AdemÃ¡s, redefinir el fragmento como simplemente siete no resuelve el problema, ya que ahora podrÃ­a coincidir errÃ³neamente con cualquier menciÃ³n de siete fuera de contexto. Nuggeteer [13] trabaja sobre principios similares pero toma decisiones binarias sobre si hay un fragmento en una respuesta de sistema dada ajustando un umbral. Sin embargo, tambiÃ©n estÃ¡ plagado de relevancia espuria ya que no todas las palabras contenidas en la descripciÃ³n de la pepita (o respuestas correctas conocidas) son centrales para la pepita. 4.1.2 Reglas de Coincidencia de Pepitas Proponemos un mÃ©todo automÃ¡tico confiable para determinar si un fragmento de texto contiene una pepita dada, basado en reglas de coincidencia de pepitas, que se generan utilizando un procedimiento semiautomÃ¡tico explicado a continuaciÃ³n. Estas reglas son bÃ¡sicamente consultas booleanas que solo coincidirÃ¡n con fragmentos que contengan la pepita. Por ejemplo, una regla candidata para emparejar respuestas a Â¿CuÃ¡ntos prisioneros escaparon? es (Texas Y siete Y escapar Y (convictos O prisioneros)), posiblemente con otros sinÃ³nimos y variantes en la regla. Para un corpus de artÃ­culos de noticias, que suelen seguir una prosa formal tÃ­pica, es bastante fÃ¡cil escribir reglas simples para coincidir con respuestas esperadas utilizando un enfoque de arranque, como se describe a continuaciÃ³n. Proponemos un enfoque de dos etapas, inspirado en Autoslog [14], que combina la capacidad de los humanos para identificar expresiones semÃ¡nticamente equivalentes y la capacidad del sistema para recopilar evidencia estadÃ­stica de un corpus de documentos anotados por humanos. En la primera etapa, los sujetos humanos anotaron (utilizando una herramienta de resaltado) porciones de documentos relevantes que contenÃ­an respuestas a cada fragmento 2. En la segunda etapa, los sujetos utilizaron nuestra herramienta de generaciÃ³n de reglas para crear reglas que coincidieran con las anotaciones de cada fragmento. La herramienta permite a los usuarios ingresar una regla booleana como una disyunciÃ³n de conjunciones (por ejemplo, ((a Y b) O (a Y c Y d) O (e))). Dada una regla candidata, nuestra herramienta la utiliza como una consulta booleana sobre el conjunto completo de documentos relevantes y calcula su recall y precisiÃ³n con respecto a las anotaciones que se espera que coincida. Por lo tanto, los sujetos pueden comenzar con una regla simple y refinarla iterativamente hasta que estÃ©n satisfechos con su precisiÃ³n y recuperaciÃ³n. Observamos que era muy fÃ¡cil para los humanos mejorar la precisiÃ³n de una regla ajustando sus conjunciones existentes (agregando mÃ¡s Y) y mejorar la recuperaciÃ³n agregando mÃ¡s conjunciones a la disyunciÃ³n (agregando mÃ¡s O). Como ejemplo, intentemos crear una regla para el acertijo que dice que siete prisioneros escaparon de la prisiÃ³n de Texas. Empezamos con una regla simple - (siete). Cuando introducimos esto en la herramienta de generaciÃ³n de reglas, nos damos cuenta de que esta regla coincide con muchas ocurrencias espurias de siete (por ejemplo, ...siete estados...) y, por lo tanto, obtiene una puntuaciÃ³n baja de precisiÃ³n. Podemos calificar aÃºn mÃ¡s nuestra regla - Texas Y siete Y convictos. A continuaciÃ³n, al revisar las anotaciones omitidas, nos damos cuenta de que algunos artÃ­culos de noticias mencionaban ...siete prisioneros escaparon.... Luego reemplazamos convictos por la disyunciÃ³n (convictos O prisioneros). Continuamos ajustando la regla de esta manera hasta lograr un nivel de recordaciÃ³n y precisiÃ³n suficientemente alto, es decir, que los (pocos) errores y alarmas falsas se puedan ignorar de forma segura. AsÃ­ podemos crear reglas de coincidencia de pepitas que capturen de manera sucinta varias formas de expresar una pepita, evitando al mismo tiempo emparejar respuestas incorrectas (o fuera de contexto). La participaciÃ³n humana en el proceso de creaciÃ³n de reglas garantiza reglas genÃ©ricas de alta calidad que luego se pueden utilizar para evaluar de manera confiable las respuestas del sistema arbitrario. 4.2 EvaluaciÃ³n de la Utilidad de una Secuencia de Listas Clasificadas La utilidad de un sistema de recuperaciÃ³n se puede definir como la diferencia entre cuÃ¡nta informaciÃ³n Ãºtil obtuvo el usuario y cuÃ¡nto tiempo y energÃ­a perdiÃ³. Calculamos esta utilidad a partir de las utilidades de los pasajes individuales de la siguiente manera. DespuÃ©s de leer cada pasaje devuelto por el sistema, el usuario obtiene cierta ganancia dependiendo de la presencia de informaciÃ³n relevante y novedosa, y incurre en una pÃ©rdida en tÃ©rminos del tiempo y la energÃ­a invertidos en revisar el pasaje. Sin embargo, la probabilidad de que el usuario realmente lea un pasaje depende de su posiciÃ³n en la lista clasificada. Por lo tanto, para una consulta q, la utilidad esperada 2 LDC [18] ya proporciona juicios de relevancia para 100 temas en el corpus TDT4. AdemÃ¡s, nos aseguramos de que estos juicios sean exhaustivos en todo el corpus utilizando la agrupaciÃ³n. El valor de un pasaje pi en el rango i se puede definir como U(pi, q) = P(i) â (Ganancia(pi, q) - PÃ©rdida(pi, q)) (4) donde P(i) es la probabilidad de que el usuario pase por un pasaje en el rango i. La utilidad esperada para una lista clasificada completa de longitud n se puede calcular simplemente sumando la utilidad esperada de cada pasaje: U(q) = Î£ i=1 P(i) â (Ganancia(pi, q) â PÃ©rdida(pi, q)) (5) NÃ³tese que si ignoramos el tÃ©rmino de pÃ©rdida y definimos P(i) como P(i) â 1/ logb(b + i â 1) (6) entonces obtenemos la mÃ©trica recientemente popularizada llamada Ganancia Acumulada Descontada (DCG) [9], donde Ganancia(pi, q) se define como la relevancia graduada del pasaje pi. Sin embargo, sin el tÃ©rmino de pÃ©rdida, DCG es una mÃ©trica puramente orientada al recuerdo y no es adecuada para un entorno de filtrado adaptativo, donde la utilidad del sistema depende en parte de su capacidad para limitar el nÃºmero de elementos mostrados al usuario. Aunque P(i) podrÃ­a definirse en base a estudios empÃ­ricos del comportamiento del usuario, por simplicidad, utilizamos P(i) tal como se define exactamente en la ecuaciÃ³n 6. La ganancia G(pi, q) del pasaje pi con respecto a la consulta q es una funciÃ³n de - 1) el nÃºmero de pepitas relevantes presentes en pi, y 2) la novedad de cada una de estas pepitas. Combinamos estos dos factores de la siguiente manera. Para cada pepita Nj, asignamos un peso inicial wj, y tambiÃ©n mantenemos un conteo nj del nÃºmero de veces que esta pepita ha sido vista por el usuario en el pasado. La ganancia derivada de cada ocurrencia posterior del mismo hallazgo se asume que se reduce por un factor de amortiguamiento Î³. Por lo tanto, G(pi, q) se define como G(pi, q) = X Nj âC(pi,q) wj â Î³nj (7) donde C(pi, q) es el conjunto de todas las pepitas que aparecen en el pasaje pi y tambiÃ©n pertenecen a la clave de respuesta de la consulta q. Los pesos iniciales wj se establecen todos en 1.0 en nuestros experimentos, pero tambiÃ©n pueden establecerse segÃºn un enfoque de pirÃ¡mide [11]. La elecciÃ³n del factor de amortiguaciÃ³n Î³ determina la tolerancia del usuario a la redundancia. Cuando Î³ = 0, un pepita solo recibirÃ¡ crÃ©dito por su primera ocurrencia, es decir, cuando nj sea cero. Para 0 < Î³ < 1, un nugget recibe menos crÃ©dito por cada ocurrencia sucesiva. Cuando Î³ = 1, no se produce amortiguamiento y las repeticiones de un valor reciben el mismo crÃ©dito. Se debe tener en cuenta que los recuentos de ocurrencia de las pepitas se conservan entre la evaluaciÃ³n de las listas clasificadas sucesivas devueltas por el sistema, ya que se espera que los usuarios recuerden lo que el sistema les mostrÃ³ en el pasado. Definimos la pÃ©rdida L(pi, q) como un costo constante c (usamos 0.1) incurrido al leer un pasaje devuelto por el sistema. Por lo tanto, nuestra mÃ©trica puede ser reescrita como U(q) = nX i=1 Ganancia(pi, q) logb(b + i â 1) â L(n) (8) donde L(n) es la pÃ©rdida asociada con una lista clasificada de longitud n: L(n) = c Â· nX i=1 1 logb(b + i â 1) (9) 3 NÃ³tese que 00 = 1 Debido a la similitud con la Ganancia Acumulada Descontada (DCG), llamamos a nuestra mÃ©trica Utilidad Acumulada Descontada (DCU). El puntaje de DCU obtenido por el sistema se convierte en un puntaje de DCU normalizado (NDCU) dividiÃ©ndolo por el puntaje de DCU de la lista ideal clasificada, que se crea ordenando los pasajes por sus puntajes de utilidad decrecientes U(pi, q) y deteniÃ©ndose cuando U(pi, q) â¤ 0, es decir, cuando la ganancia es menor o igual al costo de leer el pasaje. El conjunto de datos TDT4 fue el corpus de referencia utilizado en las evaluaciones TDT2002 y TDT2003. El corpus consiste en mÃ¡s de 90,000 artÃ­culos de noticias de mÃºltiples fuentes (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) publicados entre octubre de 2000 y enero de 2001, en los idiomas Ã¡rabe, inglÃ©s y mandarÃ­n. Las versiones de los artÃ­culos en otros idiomas reconocidas por voz y traducidas por mÃ¡quina tambiÃ©n fueron proporcionadas. LDC [18] ha anotado el corpus con 100 temas, que corresponden a varios eventos de noticias en este perÃ­odo de tiempo. De entre estos, seleccionamos un subconjunto de 12 eventos accionables y definimos tareas correspondientes para ellos. Para cada tarea, definimos manualmente un perfil que consiste en un conjunto inicial de (5 a 10) consultas, una descripciÃ³n de texto libre del historial del usuario, es decir, lo que el usuario ya sabe sobre el evento, y una lista de documentos conocidos relevantes y no relevantes (si estÃ¡n disponibles) como ejemplos de entrenamiento. Para cada consulta, generamos claves de respuestas y reglas de coincidencia de pepitas correspondientes utilizando el procedimiento descrito en la secciÃ³n 4.1.2, y produjimos un total de 120 consultas, con un promedio de 7 pepitas por consulta. EXPERIMENTOS Y RESULTADOS 6.1 Baselines Utilizamos Indri [17], un motor de bÃºsqueda basado en modelos de lenguaje popular, como referencia para comparar con CAFÃ. Indri admite la funcionalidad estÃ¡ndar de los motores de bÃºsqueda, incluido el feedback de pseudo-relevancia (PRF) [3, 6], y es representativo de un sistema de recuperaciÃ³n basado en consultas tÃ­pico. Indri no admite ningÃºn tipo de detecciÃ³n de novedades. Comparamos Indri con PRF activado y desactivado, frente a CAFÃ con retroalimentaciÃ³n del usuario, detecciÃ³n de novedades y clasificaciÃ³n antirredundante activados y desactivados. 6.2 ConfiguraciÃ³n Experimental Dividimos el corpus TDT4 que abarca 4 meses en 10 fragmentos, cada uno definido como un perÃ­odo de 12 dÃ­as consecutivos. En cualquier momento dado del proceso de destilaciÃ³n, cada sistema accedÃ­a a los datos pasados hasta el punto actual, y devolvÃ­a una lista clasificada de hasta 50 pasajes por consulta. Las 12 tareas definidas en el corpus se dividieron en un conjunto de entrenamiento y prueba con 6 tareas cada uno. Cada sistema pudo utilizar el conjunto de entrenamiento para ajustar sus parÃ¡metros con el fin de optimizar NDCU (ecuaciÃ³n 8), incluyendo el umbral de relevancia tanto para Indri como para CAFÂ´E, y los umbrales de novedad y antirredundancia para CAFÂ´E. La NDCU para cada ejecuciÃ³n del sistema se calcula automÃ¡ticamente. TambiÃ©n se simulÃ³ la retroalimentaciÃ³n de los usuarios: los juicios de relevancia para cada pasaje devuelto por el sistema (segÃºn las reglas de coincidencia de pepitas descritas en la secciÃ³n 4.1.2) fueron utilizados como retroalimentaciÃ³n de los usuarios en la adaptaciÃ³n de perfiles de consulta e historiales de usuario. En la Tabla 1, mostramos los puntajes NDCU de los dos sistemas en diferentes configuraciones. Estas puntuaciones se promedian en las seis tareas del conjunto de pruebas, y se calculan con dos factores de amortiguaciÃ³n (ver secciÃ³n 4.2): Î³ = 0 y 0.1, para simular ninguna tolerancia y poca tolerancia para la redundancia, respectivamente. Usar Î³ = 0 crea una mÃ©trica mucho mÃ¡s estricta ya que no otorga ningÃºn crÃ©dito a un pasaje que contenga informaciÃ³n relevante pero redundante. Por lo tanto, la mejora obtenida al habilitar la retroalimentaciÃ³n del usuario es menor con Î³ = 0 que la mejora obtenida de la retroalimentaciÃ³n con Î³ = 0.1. Esto revela una deficiencia de los sistemas de recuperaciÃ³n contemporÃ¡neos: cuando el usuario da retroalimentaciÃ³n positiva sobre un pasaje, los sistemas otorgan mayor peso a los tÃ©rminos presentes en ese pasaje y tienden a recuperar otros pasajes que contienen los mismos tÃ©rminos, y por lo tanto, generalmente la misma informaciÃ³n. Sin embargo, el usuario no se beneficia al ver pasajes redundantes y generalmente estÃ¡ interesado en otros pasajes que contengan informaciÃ³n relacionada. Es informativo evaluar los sistemas de recuperaciÃ³n utilizando nuestra medida de utilidad (con Î³ = 0) que tiene en cuenta la novedad y, por lo tanto, ofrece una imagen mÃ¡s realista de cuÃ¡n bien un sistema puede generalizar a partir de la retroalimentaciÃ³n del usuario, en lugar de utilizar medidas tradicionales de RI como la recuperaciÃ³n y la precisiÃ³n, que ofrecen una imagen incompleta de la mejora obtenida a partir de la retroalimentaciÃ³n del usuario. A veces, sin embargo, los usuarios podrÃ­an estar interesados en ver la misma informaciÃ³n de mÃºltiples fuentes, como un indicador de su importancia o fiabilidad. En tal caso, simplemente podemos elegir un valor mÃ¡s alto para Î³ que corresponda a una mayor tolerancia para la redundancia, y asÃ­ permitir que el sistema ajuste sus parÃ¡metros en consecuencia. Dado que los documentos fueron procesados por fragmentos, serÃ­a interesante ver cÃ³mo mejora el rendimiento de los sistemas con el tiempo. Las figuras 1 y 2 muestran las tendencias de rendimiento de ambos sistemas a lo largo de los fragmentos. Si bien se espera que el rendimiento con y sin retroalimentaciÃ³n en los primeros fragmentos sea similar, para los fragmentos posteriores, la curva de rendimiento con la retroalimentaciÃ³n habilitada supera a la del ajuste sin retroalimentaciÃ³n. Las tendencias de rendimiento no son consistentes en todos los fragmentos porque los documentos relevantes no estÃ¡n distribuidos uniformemente en todos los fragmentos, lo que hace que algunas consultas sean mÃ¡s fÃ¡ciles que otras en ciertos fragmentos. AdemÃ¡s, dado que Indri utiliza retroalimentaciÃ³n de pseudo-relevancia mientras que CAFÂ´E utiliza retroalimentaciÃ³n basada en juicios de relevancia reales, la mejora en el caso de Indri es menos dramÃ¡tica que la de CAFÂ´E. 7. CONCLUSIONES FINALES Este artÃ­culo presenta la primera investigaciÃ³n sobre la destilaciÃ³n de informaciÃ³n basada en la utilidad con un sistema que aprende las necesidades de informaciÃ³n duraderas a partir de la retroalimentaciÃ³n detallada del usuario sobre una secuencia de pasajes clasificados. Nuestro sistema, llamado CAFÂ´E, combina filtrado adaptativo, detecciÃ³n de novedades y clasificaciÃ³n de pasajes antirredundantes en un <br>marco unificado</br> para la optimizaciÃ³n de la utilidad. Desarrollamos un nuevo esquema para la evaluaciÃ³n y retroalimentaciÃ³n automatizadas basado en un procedimiento semiautomÃ¡tico para adquirir reglas que permitan emparejar automÃ¡ticamente fragmentos con las respuestas del sistema. TambiÃ©n propusimos una extensiÃ³n de la mÃ©trica NDCG para evaluar la utilidad de los pasajes clasificados como una combinaciÃ³n ponderada de relevancia y novedad. Nuestros experimentos en el reciÃ©n anotado corpus de referencia TDT4 muestran un alentador aumento de utilidad sobre Indri, y tambiÃ©n sobre nuestro propio sistema con aprendizaje incremental y detecciÃ³n de novedades desactivados. 8. AGRADECIMIENTOS Nos gustarÃ­a agradecer a Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng y al Programa de AnÃ¡lisis de Datos Cualitativos de la Universidad de Pittsburgh liderado por el Dr. Stuart Shulman por su ayuda en la recolecciÃ³n y procesamiento de las anotaciones extendidas de TDT4 utilizadas en nuestros experimentos. Este trabajo estÃ¡ parcialmente respaldado por la FundaciÃ³n Nacional de Ciencias (NSF) bajo la subvenciÃ³n IIS0434035, y la Agencia de Proyectos de InvestigaciÃ³n Avanzada de Defensa (DARPA) bajo los contratos NBCHD030010 y W0550432. Cualquier opiniÃ³n, hallazgo, conclusiÃ³n o recomendaciÃ³n expresada en este material son responsabilidad de los autores y no reflejan necesariamente los puntos de vista de los patrocinadores. AUTORES ADICIONALES Jian Zhang (jianzhan@stat.purdue.edu)â , Jaime Carbonell (jgc@cs.cmu.edu)â  , Peter Brusilovsky (peterb+@pitt.edu)â¡ , Daqing He(dah44@pitt.edu)â¡ 10. REFERENCIAS [1] J. Allan. RetroalimentaciÃ³n incremental de relevancia para la filtraciÃ³n de informaciÃ³n. Actas de la 19Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 270-278, 1996. [2] J. Allan, C. Wade y A. Bolivar. RecuperaciÃ³n y DetecciÃ³n de Novedad a Nivel de OraciÃ³n. Actas de la conferencia ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, 2003. [3] C. Buckley, G. Salton y J. Allan. RecuperaciÃ³n automÃ¡tica con informaciÃ³n de localidad utilizando SMART. PublicaciÃ³n especial del NIST, (500207):59-72, 1993. [4] J. Callan. Aprendiendo mientras se filtran documentos. Actas de la 21Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 224-231, 1998. [5] J. Carbonell y J. Goldstein. El uso de MMR, Reordenamiento basado en Diversidad para Reorganizar Documentos y Producir ResÃºmenes. Actas de la 21Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 335-336, 1998. [6] E. Efthimiadis. ExpansiÃ³n de consulta. RevisiÃ³n Anual de Ciencia de la InformaciÃ³n y TecnologÃ­a (ARIST), 31:p121-87, 1996. [7] J. Fiscus y G. Duddington. Resumen de DetecciÃ³n y Seguimiento de Temas. DetecciÃ³n y seguimiento de temas: OrganizaciÃ³n de la informaciÃ³n basada en eventos, pÃ¡ginas 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov y S. Roukos. Un modelo estadÃ­stico para la detecciÃ³n y seguimiento de entidades multilingÃ¼es. NAACL/HLT, 2004. [9] K. JÃ¤rvelin y J. KekÃ¤lÃ¤inen. EvaluaciÃ³n basada en la Ganancia Acumulada de TÃ©cnicas de RecuperaciÃ³n de InformaciÃ³n. ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin y D. Demner-Fushman. EvaluaciÃ³n automÃ¡tica de respuestas a preguntas de definiciÃ³n. Actas de la Conferencia de TecnologÃ­a del Lenguaje Humano y Conferencia sobre MÃ©todos EmpÃ­ricos en Procesamiento del Lenguaje Natural (HLT/EMNLP 2005), 2005. â Departamento de EstadÃ­stica, Universidad de Purdue, West Lafayette, EE. UU. â  Instituto de TecnologÃ­as del Lenguaje, Universidad Carnegie Mellon, Pittsburgh, EE. UU. â¡ Escuela de Ciencias de la InformaciÃ³n, Universidad de Pittsburgh, Pittsburgh, EE. UU. [11] J. Lin y D. Demner-Fushman. Â¿Se derrumbarÃ¡n las pirÃ¡mides construidas con pepitas? Actas de HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla y S. Roukos. Un algoritmo de resoluciÃ³n de correferencia sÃ­ncrona basado en el Ã¡rbol de Bell. Actas de ACL, 4:136-143, 2004. [13] G. Marton. Nuggeteer: EvaluaciÃ³n AutomÃ¡tica Basada en Pepitas Utilizando Descripciones y Juicios. HLT/NAACL, 2006. [14] E. Riloff. ConstrucciÃ³n automÃ¡tica de un diccionario para tareas de extracciÃ³n de informaciÃ³n. Actas de la UndÃ©cima Conferencia Nacional sobre Inteligencia Artificial, pÃ¡ginas 811-816, 1993. [15] S. Robertson y S. Walker. Microsoft Cambridge en TREC-9: Pista de filtrado. La Novena Conferencia de RecuperaciÃ³n de Texto (TREC-9), pÃ¡ginas 361-368. [16] R. Schapire, Y. Cantante, y A. Singhal. Boosting y Rocchio aplicados a la filtraciÃ³n de texto. Actas de la 21Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle y W. Croft. Indri: Un motor de bÃºsqueda basado en modelos de lenguaje para consultas complejas. Actas de la Conferencia Internacional sobre AnÃ¡lisis de Inteligencia, 2004. [18] El Consorcio de Datos LingÃ¼Ã­sticos. http://www.ldc.upenn.edu/. [19] E. Voorhees. Resumen de la pista de preguntas y respuestas TREC 2003. Actas de la DuodÃ©cima Conferencia de RecuperaciÃ³n de Texto (TREC 2003), 2003. [20] Y. Yang y B. Kisiel. RegresiÃ³n local basada en mÃ¡rgenes para filtrado adaptativo. Actas de la duodÃ©cima conferencia internacional sobre gestiÃ³n de la informaciÃ³n y el conocimiento, pÃ¡ginas 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang y B. Kisiel. Robustez de los MÃ©todos de Filtrado Adaptativo en una EvaluaciÃ³n de Referencia Cruzada. Actas de la 28Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 98-105, 2005. [22] C. Zhai, W. Cohen y J. Lafferty. MÃ¡s allÃ¡ de la relevancia independiente: MÃ©todos y mÃ©tricas de evaluaciÃ³n para la recuperaciÃ³n de subtemas. Actas de la 26Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 10-17, 2003. [23] J. Zhang y Y. Yang. Robustez de los MÃ©todos de ClasificaciÃ³n Lineal Regularizados en la CategorizaciÃ³n de Textos. Actas de la 26Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 190-197, 2003. [24] Y. Zhang. Utilizando priors bayesianos para combinar clasificadores para filtrado adaptativo. Actas de la 27Âª conferencia internacional anual sobre InvestigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 345-352, 2004. [25] Y. Zhang, J. Callan y T. Minka. DetecciÃ³n de novedad y redundancia en el filtrado adaptativo. Actas de la 25Âª Conferencia Internacional Anual de ACM SIGIR sobre InvestigaciÃ³n y Desarrollo en RecuperaciÃ³n de InformaciÃ³n, 2002. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "ndcg metric": {
            "translated_key": "mÃ©trica NDCG",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Utility-based Information Distillation Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu ABSTRACT This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework.",
                "It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant passage ranking with respect to long-lasting information needs (tasks with multiple queries).",
                "Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings.",
                "For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task.",
                "Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also propose an extension of the <br>ndcg metric</br> for assessing the utility of ranked passages as a combination of relevance and novelty.",
                "Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval.",
                "Adaptive filtering (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics.",
                "Based on the initial query and a few positive examples (if available), an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user.",
                "The incremental learning nature of AF systems makes them more powerful than standard search engines that support ad-hoc retrieval (e.g.",
                "Google and Yahoo) in terms of finding relevant information with respect to long-lasting topics of interest, and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs, without having to modify their queries manually.",
                "A variety of supervised learning algorithms (Rocchio-style classifiers, Exponential-Gaussian models, local regression and logistic regression approaches) have been studied for adaptive settings, examined with explicit and implicit relevance feedback, and evaluated with respect to utility optimization on large benchmark data collections in TREC (Text Retrieval Conferences) and TDT (Topic Detection and Tracking) forums [1, 4, 7, 15, 16, 20, 24, 23].",
                "Regularized logistic regression [21] has been found representative for the state-of-the-art approaches, and highly efficient for frequent model adaptations over large document collections such as the TREC-10 corpus (over 800,000 documents and 84 topics).",
                "Despite substantial achievements in recent adaptive filtering research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently.",
                "Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications: 1.",
                "User has a rather passive role in the conventional adaptive filtering setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision.",
                "A more active alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents (or passages) per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists.",
                "The latter form of user interaction has been highly effective in standard retrieval for ad-hoc queries.",
                "How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2.",
                "The unit for receiving a relevance judgment (yes or no) is restricted to the document level in conventional AF.",
                "However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant.",
                "Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system.",
                "For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3.",
                "System-selected documents are often highly redundant.",
                "A major news event, for example, would be reported by multiple sources repeatedly for a while, making most of the information content in those articles redundant with each other.",
                "A conventional AF system would select all these redundant news stories for user feedback, wasting the users time while offering little gain.",
                "Clearly, techniques for novelty detection can help in principle [25, 2, 22] for improving the utility of the AF systems.",
                "However, the effectiveness of such techniques at passage level to detect novelty with respect to users (fine-grained) feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user.",
                "To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and novelty detection for removal of redundancy from user interactions with the system.",
                "We call the new process utility-based information distillation.",
                "Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach.",
                "Therefore, we extended a benchmark corpus - the TDT4 collection of news stories and TV broadcasts - with task definitions, multiple queries per task, and answer keys per query.",
                "We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 .",
                "To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses.",
                "Moreover, we propose an extension of NDCG (Normalized Discounted Cumulated Gain) [9] for assessing the utility of ranked passages as a function of both relevance and novelty.",
                "The rest of this paper is organized as follows.",
                "Section 2 outlines the information distillation process with a concrete example.",
                "Section 3 describes the technical cores of our system called CAFÂ´E - CMU Adaptive Filtering Engine.",
                "Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme.",
                "Section 5 describes the extended TDT4 corpus.",
                "Section 6 presents our experiments and results.",
                "Section 7 concludes the study and gives future perspectives. 2.",
                "A SAMPLE TASK Consider a news event - the escape of seven convicts from a Texas prison in December 2000 and their capture a month later.",
                "Assuming a user were interested in this event since its early stage, the information need could be: Find information about the escape of convicts from Texas prison, and information related to their recapture.",
                "The associated lower-level questions could be: 1.",
                "How many prisoners escaped? 2.",
                "Where and when were they sighted? 3. Who are their known contacts inside and outside the prison? 4.",
                "How are they armed? 5.",
                "Do they have any vehicles? 6.",
                "What steps have been taken so far?",
                "We call such an information need a task, and the associated questions as the queries in this task.",
                "A distillation system is supposed to monitor the incoming documents, process them chunk by chunk in a temporal order, select potentially relevant and novel passages from each chunk with respect to each query, and present a ranked list of passages to the user.",
                "Passage ranking here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list.",
                "When presented with a list of passages, the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant.",
                "These spans of text are taken as positive examples in the adaptation of the query profile, and also added to the users history.",
                "Passages not marked by the user are taken as negative examples.",
                "As soon as the query profile is updated, the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low, based on user preference.",
                "For example, if the user highlights ...officials have posted a $100,000 reward for their capture... as relevant answer to the query What steps have been taken so far?, then the highlighted piece is used as an additional positive training example in the adaptation of the query profile.",
                "This piece of feedback is also added to the user history as a seen example, so that in future, the system will not place another passage mentioning $100,000 reward at the top of the ranked list.",
                "However, an article mentioning ...officials have doubled the reward money to $200,000... might be ranked high since it is both relevant to the (updated) query profile and novel with respect to the (updated) user history.",
                "The user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly.",
                "Clearly, novelty detection is very important for the utility of such a system because of the iterative search.",
                "Without novelty detection, the old relevant passages would be shown to the user repeatedly in each ranked list.",
                "Through the above example, we can see the main properties of our new framework for utility-based information distillation over temporally ordered documents.",
                "Our framework combines and extends the power of adaptive filtering (AF), ad-hoc retrieval (IR) and novelty detection (ND).",
                "Compared to standard IR, our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task.",
                "Compared to conventional AF, it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system.",
                "Compared to past work, this is the first evaluation of novelty detection integrated with adaptive filtering for sequenced queries that allows flexible user feedback over ranked passages.",
                "The combination of AF, IR and ND with the new extensions raises an important research question regarding evaluation methodology: how can we measure the utility of such an information distillation system?",
                "Existing metrics in standard IR, AF and ND are insufficient, and new solutions must be explored, as we will discuss in Section 4, after describing the technical cores of our system in the next section. 3.",
                "TECHNICAL CORES The core components of CAFÂ´E are - 1) AF for incremental learning of query profiles, 2) IR for estimating relevance of passages with respect to query profiles, 3) ND for assessing novelty of passages with respect to users history, and 4) anti-redundancy component to remove redundancy from ranked lists. 3.1 Adaptive Filtering Component We use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for AF [21].",
                "Logistic regression (LR) is a supervised learning algorithm for statistical classification.",
                "Based on a training set of labeled instances, it learns a class model which can then by used to predict the labels of unseen instances.",
                "Its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required, as is the case in adaptive filtering, where the system must learn from each new feedback provided by the user. (See [21] and [23] for computational complexity and implementation issues).",
                "In adaptive filtering, each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query.",
                "For training the model, we use the query itself as the initial positive training example of the class, and the user-highlighted pieces of text (marked as Relevant or Not-relevant) during feedback as additional training examples.",
                "To address the cold start issue in the early stage before any user feedback is obtained, the system uses a small sample from a retrospective corpus as the initial negative examples in the training set.",
                "The details of using logistic regression for adaptive filtering (assigning different weights to positive and negative training instances, and regularizing the objective function to prevent over-fitting on training data) are presented in [21].",
                "The class model wâ learned by Logistic Regression, or the query profile, is a vector whose dimensions are individual terms and whose elements are the regression coefficients, indicating how influential each term is in the query profile.",
                "The query profile is updated whenever a new piece of user feedback is received.",
                "A temporally decaying weight can be applied to each training example, as an option, to emphasize the most recent user feedback. 3.2 Passage Retrieval Component We use standard IR techniques in this part of our system.",
                "Incoming documents are processed in chunks, where each chunk can be defined as a fixed span of time or as a fixed number of documents, as preferred by the user.",
                "For each incoming document, corpus statistics like the IDF (Inverted Document Frequency) of each term are updated.",
                "We use a state-of-the-art named entity identifier and tracker [8, 12] to identify person and location names, and merge them with co-referent named entities seen in the past.",
                "Then the documents are segmented into passages, which can be a whole document, a paragraph, a sentence, or any other continuous span of text, as preferred.",
                "Each passage is represented using a vector of TF-IDF (Term FrequencyInverse Document Frequency) weights, where term can be a word or a named entity.",
                "Given a query profile, i.e. the logistic regression solution wâ as described in Section 3.1, the system computes the posterior probability of relevance for each passage x as fRL(x) â¡ P(y = 1|x, wâ ) = 1 (1 + eâwâÂ·x) (1) Passages are ordered by their relevance scores, and the ones with scores above a threshold (tuned on a training set) comprise the relevance list that is passed on to the novelty detection step. 3.3 Novelty Detection Component CAFÂ´E maintains a user history H(t), which contains all the spans of text hi that the user highlighted (as feedback) during his or her past interactions with the system, up to the current time t. Denoting the history as H(t) = n h1, h2, ..., ht o , (2) the novelty score of a new candidate passage x is computed as: fND(x) = 1 â max iâ1..t {cos(x, hi)} (3) where both candidate passage x and highlighted spans of text hi are represented as TF-IDF vectors.",
                "The novelty score of each passage is compared to a prespecified threshold (also tuned on a training set), and any passage with a score below this threshold is removed from the relevance list. 3.4 Anti-redundant Ranking Component Although the novelty detection component ensures that only novel (previously unseen) information remains in the relevance list, this list might still contain the same novel information at multiple positions in the ranked list.",
                "Suppose, for example, that the user has already read about a $100,000 reward for information about the escaped convicts.",
                "A new piece of news that the award has been increased to $200,000 is novel since the user hasnt read about it yet.",
                "However, multiple news sources would report this news and we might end up showing (redundant) articles from all these sources in a ranked list.",
                "Hence, a ranked list should also be made non-redundant with respect to its own contents.",
                "We use a simplified version of the Maximal Marginal Relevance method [5], originally developed for combining relevance and novelty in text retrieval and summarization.",
                "Our procedure starts with the current list of passages sorted by relevance (section 3.2), filtered by Novelty Detection component (section 3.3), and generates a new non-redundant list as follows: 1.",
                "Take the top passage in the current list as the top one in the new list. 2.",
                "Add the next passage x in the current list to the new list only if fAR(x) > t where fAR(x) = 1 â max piâLnew {cos(x, pi)} and Lnew is the set of passages already selected in the new list. 3.",
                "Repeat step 2 until all the passages in the current list have been examined.",
                "After applying the above-mentioned algorithm, each passage in the new list is sufficiently dissimilar to others, thus favoring diversity rather than redundancy in the new ranked list.",
                "The anti-redundancy threshold t is tuned on a training set. 4.",
                "EVALUATION METHODOLOGY The approach we proposed above for information distillation raises important issues regarding evaluation methodology.",
                "Firstly, since our framework allows the output to be passages at different levels of granularity (e.g. k-sentence windows where k may vary) instead of a fixed length, it is not possible to have pre-annotated relevance judgments at all such granularity levels.",
                "Secondly, since we wish to measure the utility of the system output as a combination of both relevance and novelty, traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time.",
                "Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists.",
                "None of the existing measures in ad-hoc retrieval, adaptive filtering, novelty detection or other related areas (text summarization and question answering) have desirable properties in all the three aspects.",
                "Therefore, we must develop a new evaluation methodology. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers.",
                "Answer keys define what should be present in a system response to receive credit, and are comprised of a collection of information nuggets, i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them.",
                "Defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping [19], since system-returned passages can contain the same information expressed in many different ways.",
                "Hence, QA evaluations have relied on human assessors for the mapping between various expressions, making the process costly, time consuming, and not scalable to large query and document collections, and extensive system evaluations with various parameter settings. 4.1.1 Automating Evaluation based on Answer Keys Automatic evaluation methods would allow for faster system building and tuning, as well as provide an objective and affordable way of comparing various systems.",
                "Recently, such methods have been proposed, more or less, based on the idea of n-gram co-occurrences.",
                "Pourpre [10] assigns a fractional recall score to a system response based on its unigram overlap with a given nuggets description.",
                "For example, a system response A B C has recall 3/4 with respect to a nugget with description A B C D. However, such an approach is unfair to systems that present the same information but using words other than A, B, C, and D. Another open issue is how to weight individual words in measuring the closeness of a match.",
                "For example, consider the question How many prisoners escaped?.",
                "In the nugget Seven prisoners escaped from a Texas prison, there is no indication that seven is the keyword, and that it must be matched to get any relevance credit.",
                "Using IDF values does not help, since seven will generally not have a higher IDF than words like texas and prison.",
                "Also, redefining the nugget as just seven does not solve the problem since now it might spuriously match any mention of seven out of context.",
                "Nuggeteer [13] works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold.",
                "However, it is also plagued by spurious relevance since not all words contained in the nugget description (or known correct responses) are central to the nugget. 4.1.2 Nugget-Matching Rules We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on nugget-matching rules, which are generated using a semiautomatic procedure explained below.",
                "These rules are essentially Boolean queries that will only match against snippets that contain the nugget.",
                "For instance, a candidate rule for matching answers to How many prisoners escaped? is (Texas AND seven AND escape AND (convicts OR prisoners)), possibly with other synonyms and variants in the rule.",
                "For a corpus of news articles, which usually follow a typical formal prose, it is fairly easy to write such simple rules to match expected answers using a bootstrap approach, as described below.",
                "We propose a two-stage approach, inspired by Autoslog [14], that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents.",
                "In the first stage, human subjects annotated (using a highlighting tool) portions of ontopic documents that contained answers to each nugget 2 .",
                "In the second stage, subjects used our rule generation tool to create rules that would match the annotations for each nugget.",
                "The tool allows users to enter a Boolean rule as a disjunction of conjunctions (e.g. ((a AND b) OR (a AND c AND d) OR (e))).",
                "Given a candidate rule, our tool uses it as a Boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match.",
                "Hence, the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision.",
                "We observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions (adding more ANDs), and improving the recall by adding more conjunctions to the disjunction (adding more ORs).",
                "As an example, lets try to create a rule for the nugget which says that seven prisoners escaped from the Texas prison.",
                "We start with a simple rule - (seven).",
                "When we input this into the rule generation tool, we realize that this rule matches many spurious occurrences of seven (e.g. ...seven states...) and thus gets a low precision score.",
                "We can further qualify our rule - Texas AND seven AND convicts.",
                "Next, by looking at the missed annotations, we realize that some news articles mentioned ...seven prisoners escaped.... We then replace convicts with the disjunction (convicts OR prisoners).",
                "We continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the (small number of) misses and false alarms can be safely ignored.",
                "Thus we can create nugget-matching rules that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses.",
                "Human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably. 4.2 Evaluating the Utility of a Sequence of Ranked Lists The utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information, and how much the user lost in terms of time and energy.",
                "We calculate this utility from the utilities of individual passages as follows.",
                "After reading each passage returned by the system, the user derives some gain depending on the presence of relevant and novel information, and incurs a loss in terms of the time and energy spent in going through the passage.",
                "However, the likelihood that the user would actually read a passage depends on its position in the ranked list.",
                "Hence, for a query q, the expected utility 2 LDC [18] already provides relevance judgments for 100 topics on the TDT4 corpus.",
                "We further ensured that these judgments are exhaustive on the entire corpus using pooling. of a passage pi at rank i can be defined as U(pi, q) = P(i) â (Gain(pi, q) â Loss(pi, q)) (4) where P(i) is the probability that the user would go through a passage at rank i.",
                "The expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage: U(q) = nX i=1 P(i) â (Gain(pi, q) â Loss(pi, q)) (5) Note that if we ignore the loss term and define P(i) as P(i) â 1/ logb(b + i â 1) (6) then we get the recently popularized metric called Discounted Cumulated Gain (DCG) [9], where Gain(pi, q) is defined as the graded relevance of passage pi.",
                "However, without the loss term, DCG is a purely recall-oriented metric and not suitable for an adaptive filtering setting, where the systems utility depends in part on its ability to limit the number of items shown to the user.",
                "Although P(i) could be defined based on empirical studies of user behavior, for simplicity, we use P(i) exactly as defined in equation 6.",
                "The gain G(pi, q) of passage pi with respect to the query q is a function of - 1) the number of relevant nuggets present in pi, and 2) the novelty of each of these nuggets.",
                "We combine these two factors as follows.",
                "For each nugget Nj, we assign an initial weight wj, and also keep a count nj of the number of times this nugget has been seen by the user in the past.",
                "The gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor Î³.",
                "Thus, G(pi, q) is defined as G(pi, q) = X Nj âC(pi,q) wj â Î³nj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the answer key of query q.",
                "The initial weights wj are all set of be 1.0 in our experiments, but can also be set based on a pyramid approach [11].",
                "The choice of dampening factor Î³ determines the users tolerance for redundancy.",
                "When Î³ = 0, a nugget will only receive credit for its first occurrence i.e. when nj is zero3 .",
                "For 0 < Î³ < 1, a nugget receives smaller credit for each successive occurrence.",
                "When Î³ = 1, no dampening occurs and repeated occurrences of a nugget receive the same credit.",
                "Note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system, since the users are expected to remember what the system showed them in the past.",
                "We define the loss L(pi, q) as a constant cost c (we use 0.1) incurred when reading a system-returned passage.",
                "Thus, our metric can be re-written as U(q) = nX i=1 Gain(pi, q) logb(b + i â 1) â L(n) (8) where L(n) is the loss associated with a ranked list of length n: L(n) = c Â· nX i=1 1 logb(b + i â 1) (9) 3 Note that 00 = 1 Due to the similarity with Discounted Cumulated Gain (DCG), we call our metric Discounted Cumulated Utility (DCU).",
                "The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) â¤ 0 i.e. when the gain is less than or equal to the cost of reading the passage. 5.",
                "DATA TDT4 was the benchmark corpus used in TDT2002 and TDT2003 evaluations.",
                "The corpus consists of over 90, 000 news articles from multiple sources (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) published between October 2000 and January 2001, in the languages of Arabic, English, and Mandarin.",
                "Speech-recognized and machine-translated versions of the non-English articles were provided as well.",
                "LDC [18] has annotated the corpus with 100 topics, that correspond to various news events in this time period.",
                "Out of these, we selected a subset of 12 actionable events, and defined corresponding tasks for them4 .",
                "For each task, we manually defined a profile consisting of an initial set of (5 to 10) queries, a free-text description of the user history, i.e., what the user already knows about the event, and a list of known on-topic and off-topic documents (if available) as training examples.",
                "For each query, we generated answer keys and corresponding nugget matching rules using the procedure described in section 4.1.2, and produced a total of 120 queries, with an average of 7 nuggets per query. 6.",
                "EXPERIMENTS AND RESULTS 6.1 Baselines We used Indri [17], a popular language-model based retrieval engine, as a baseline for comparison with CAFÂ´E.",
                "Indri supports standard search engine functionality, including pseudo-relevance feedback (PRF) [3, 6], and is representative of a typical query-based retrieval system.",
                "Indri does not support any kind of novelty detection.",
                "We compare Indri with PRF turned on and off, against CAFÂ´E with user feedback, novelty detection and antiredundant ranking turned on and off. 6.2 Experimental Setup We divided the TDT4 corpus spanning 4 months into 10 chunks, each defined as a period of 12 consecutive days.",
                "At any given point of time in the distillation process, each system accessed the past data up to the current point, and returned a ranked list of up 50 passages per query.",
                "The 12 tasks defined on the corpus were divided into a training and test set with 6 tasks each.",
                "Each system was allowed to use the training set to tune its parameters for optimizing NDCU (equation 8), including the relevance threshold for both Indri and CAFÂ´E, and the novelty and antiredundancy thresholds for CAFÂ´E.",
                "The NDCU for each system run is calculated automatically.",
                "User feedback was also simulated - relevance judgments for each system-returned passage (as determined by the nugget matching rules described in section 4.1.2) were 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figure 1: Performance of Indri across chunks Figure 2: Performance of CAFÂ´E across chunks used as user feedback in the adaptation of query profiles and user histories. 6.3 Results In Table 1, we show the NDCU scores of the two systems under various settings.",
                "These scores are averaged over the six tasks in the test set, and are calculated with two dampening factors (see section 4.2): Î³ = 0 and 0.1, to simulate no tolerance and small tolerance for redundancy, respectively.",
                "Using Î³ = 0 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information.",
                "Hence, the improvement obtained from enabling user feedback is smaller with Î³ = 0 than the improvement obtained from feedback with Î³ = 0.1.",
                "This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information.",
                "However, the user does not benefit from seeing such redundant passages, and is usually interested in other passages containing related information.",
                "It is informative to evaluate retrieval systems using our utility measure (with Î³ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.",
                "Sometimes, however, users might indeed be interested in seeing the same information from multiple sources, as an Table 1: NDCU Scores of Indri and CAFÂ´E for two dampening factors (Î³), and various settings (F: Feedback, N: Novelty Detection, A: Anti-Redundant Ranking) Indri CAFÂ´E Î³ Base +PRF Base +F +F+N +F+A +F+N+A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.35 0.36 0.36 indicator of its importance or reliability.",
                "In such a case, we can simply choose a higher value for Î³ which corresponds to a higher tolerance for redundancy, and hence let the system tune its parameters accordingly.",
                "Since documents were processed chunk by chunk, it would be interesting to see how the performance of systems improves over time.",
                "Figures 1 and 2 show the performance trends for both the systems across chunks.",
                "While the performance with and without feedback on the first few chunks is expected to be close, for subsequent chunks, the performance curve with feedback enabled rises above the one with the no-feedback setting.",
                "The performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks, making some queries easier than others in certain chunks.",
                "Moreover, since Indri uses pseudo-relevance feedback while CAFÂ´E uses feedback based on actual relevance judgments, the improvement in case of Indri is less dramatic than that of CAFÂ´E. 7.",
                "CONCLUDING REMARKS This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages.",
                "Our system, called CAFÂ´E, combines adaptive filtering, novelty detection and antiredundant passage ranking in a unified framework for utility optimization.",
                "We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also proposed an extension of the <br>ndcg metric</br> for assessing the utility of ranked passages as a weighted combination of relevance and novelty.",
                "Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off. 8.",
                "ACKNOWLEDGMENTS We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments.",
                "This work is supported in parts by the National Science Foundation (NSF) under grant IIS0434035, and the Defense Advanced Research Project Agency (DARPA) under contracts NBCHD030010 and W0550432.",
                "Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. 9.",
                "ADDITIONAL AUTHORS Jian Zhang (jianzhan@stat.purdue.edu)â , Jaime Carbonell (jgc@cs.cmu.edu)â  , Peter Brusilovsky (peterb+@pitt.edu)â¡ , Daqing He(dah44@pitt.edu)â¡ 10.",
                "REFERENCES [1] J. Allan.",
                "Incremental Relevance Feedback for Information Filtering.",
                "Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 270-278, 1996. [2] J. Allan, C. Wade, and A. Bolivar.",
                "Retrieval and Novelty Detection at the Sentence Level.",
                "Proceedings of the ACM SIGIR conference on research and development in information retrieval, 2003. [3] C. Buckley, G. Salton, and J. Allan.",
                "Automatic Retrieval with Locality Information using SMART.",
                "NIST special publication, (500207):59-72, 1993. [4] J. Callan.",
                "Learning While Filtering Documents.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 224-231, 1998. [5] J. Carbonell and J. Goldstein.",
                "The use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335-336, 1998. [6] E. Efthimiadis.",
                "Query Expansion.",
                "Annual Review of Information Science and Technology (ARIST), 31:p121-87, 1996. [7] J. Fiscus and G. Duddington.",
                "Topic Detection and Tracking Overview.",
                "Topic Detection and Tracking: Event-based Information Organization, pages 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.",
                "A Statistical Model for Multilingual Entity Detection and Tracking.",
                "NAACL/HLT, 2004. [9] K. JÂ¨arvelin and J. KekÂ¨alÂ¨ainen.",
                "Cumulated Gain-based Evaluation of IR Techniques.",
                "ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin and D. Demner-Fushman.",
                "Automatically Evaluating Answers to Definition Questions.",
                "Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), 2005. â Statistics Dept., Purdue University, West Lafayette, USA â  Language Technologies Inst., Carnegie Mellon University, Pittsburgh, USA â¡ School of Information Sciences, Univ. of Pittsburgh, Pittsburgh, USA [11] J. Lin and D. Demner-Fushman.",
                "Will Pyramids Built of nUggets Topple Over.",
                "Proceedings of HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.",
                "A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree.",
                "Proc. of ACL, 4:136-143, 2004. [13] G. Marton.",
                "Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments.",
                "HLT/NAACL, 2006. [14] E. Riloff.",
                "Automatically Constructing a Dictionary for Information Extraction Tasks.",
                "Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, 1993. [15] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9: Filtering track.",
                "The Ninth Text REtrieval Conference (TREC-9), pages 361-368. [16] R. Schapire, Y.",
                "Singer, and A. Singhal.",
                "Boosting and Rocchio Applied to Text Filtering.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle, and W. Croft.",
                "Indri: A Language Model-based Search Engine for Complex Queries.",
                "Proceedings of the International Conference on Intelligence Analysis, 2004. [18] The Linguistic Data Consortium. http://www.ldc.upenn.edu/. [19] E. Voorhees.",
                "Overview of the TREC 2003 Question Answering Track.",
                "Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), 2003. [20] Y. Yang and B. Kisiel.",
                "Margin-based Local Regression for Adaptive Filtering.",
                "Proceedings of the twelfth international conference on Information and knowledge management, pages 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of Adaptive Filtering Methods in a Cross-benchmark Evaluation.",
                "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 98-105, 2005. [22] C. Zhai, W. Cohen, and J. Lafferty.",
                "Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-17, 2003. [23] J. Zhang and Y. Yang.",
                "Robustness of Regularized Linear Classification Methods in Text Categorization.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 190-197, 2003. [24] Y. Zhang.",
                "Using Bayesian Priors to Combine Classifiers for Adaptive Filtering.",
                "Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 345-352, 2004. [25] Y. Zhang, J. Callan, and T. Minka.",
                "Novelty and Redundancy Detection in Adaptive Filtering.",
                "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002."
            ],
            "original_annotated_samples": [
                "We also propose an extension of the <br>ndcg metric</br> for assessing the utility of ranked passages as a combination of relevance and novelty.",
                "We also proposed an extension of the <br>ndcg metric</br> for assessing the utility of ranked passages as a weighted combination of relevance and novelty."
            ],
            "translated_annotated_samples": [
                "TambiÃ©n proponemos una extensiÃ³n de la <br>mÃ©trica NDCG</br> para evaluar la utilidad de los pasajes clasificados como una combinaciÃ³n de relevancia y novedad.",
                "TambiÃ©n propusimos una extensiÃ³n de la <br>mÃ©trica NDCG</br> para evaluar la utilidad de los pasajes clasificados como una combinaciÃ³n ponderada de relevancia y novedad."
            ],
            "translated_text": "DestilaciÃ³n de informaciÃ³n basada en utilidad sobre documentos secuenciados temporalmente en el Instituto de TecnologÃ­as del Lenguaje de Yiming Yang. Universidad Carnegie Mellon, Pittsburgh, EE. UU. yiming@cs.cmu.edu Abhimanyu Lad Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. alad@cs.cmu.edu Instituto de TecnologÃ­as del Lenguaje Ni Lao. Universidad Carnegie Mellon, Pittsburgh, EE. UU. nlao@cs.cmu.edu Abhay Harpale Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. aharpale@cs.cmu.edu Bryan Kisiel Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. bkisiel@cs.cmu.edu MÃ³nica Rogati Instituto de TecnologÃ­as del Lenguaje. La Universidad Carnegie Mellon en Pittsburgh, EE. UU. mrogati@cs.cmu.edu RESUMEN Este artÃ­culo examina un nuevo enfoque para la destilaciÃ³n de informaciÃ³n en documentos ordenados temporalmente, y propone un esquema de evaluaciÃ³n novedoso para dicho marco. Combina las fortalezas y se extiende mÃ¡s allÃ¡ del filtrado adaptativo convencional, la detecciÃ³n de novedades y la clasificaciÃ³n de pasajes no redundantes con respecto a las necesidades de informaciÃ³n duraderas (tareas con mÃºltiples consultas). Nuestro enfoque respalda la retroalimentaciÃ³n detallada del usuario a travÃ©s del resaltado de fragmentos arbitrarios de texto, y aprovecha dicha informaciÃ³n para la optimizaciÃ³n de la utilidad en entornos adaptativos. Para nuestros experimentos, definimos tareas hipotÃ©ticas basadas en eventos de noticias en el corpus TDT4, con mÃºltiples consultas por tarea. Se generaron claves de respuestas (nuggets) para cada consulta y se utilizÃ³ un procedimiento semiautomÃ¡tico para adquirir reglas que permiten emparejar automÃ¡ticamente los nuggets con las respuestas del sistema. TambiÃ©n proponemos una extensiÃ³n de la <br>mÃ©trica NDCG</br> para evaluar la utilidad de los pasajes clasificados como una combinaciÃ³n de relevancia y novedad. Nuestros resultados muestran mejoras alentadoras en la utilidad utilizando el nuevo enfoque, en comparaciÃ³n con los sistemas base sin aprendizaje incremental o componentes de detecciÃ³n de novedades. CategorÃ­as y Descriptores de Asignaturas H.3.3 [BÃºsqueda y RecuperaciÃ³n de InformaciÃ³n]: Filtrado de informaciÃ³n, RetroalimentaciÃ³n de relevancia, Modelos de recuperaciÃ³n, Proceso de selecciÃ³n; I.5.2 TÃ©rminos Generales DiseÃ±o, MediciÃ³n, Rendimiento, ExperimentaciÃ³n. 1. INTRODUCCIÃN Seguir informaciÃ³n nueva y relevante de flujos de datos temporales para usuarios con necesidades duraderas ha sido un tema de investigaciÃ³n desafiante en la recuperaciÃ³n de informaciÃ³n. El filtrado adaptativo (AF) es una tarea de predicciÃ³n en lÃ­nea de la relevancia de cada nuevo documento con respecto a temas predefinidos. BasÃ¡ndose en la consulta inicial y algunos ejemplos positivos (si estÃ¡n disponibles), un sistema de AF mantiene un perfil para cada tema de interÃ©s, y lo actualiza constantemente segÃºn la retroalimentaciÃ³n del usuario. La naturaleza de aprendizaje incremental de los sistemas de AF los hace mÃ¡s poderosos que los motores de bÃºsqueda estÃ¡ndar que admiten recuperaciÃ³n ad-hoc (por ejemplo. Google y Yahoo) en tÃ©rminos de encontrar informaciÃ³n relevante con respecto a temas de interÃ©s duraderos, y mÃ¡s atractivo para los usuarios que estÃ¡n dispuestos a proporcionar retroalimentaciÃ³n para adaptar el sistema a sus necesidades de informaciÃ³n especÃ­ficas, sin tener que modificar sus consultas manualmente. Se han estudiado una variedad de algoritmos de aprendizaje supervisado (clasificadores de estilo Rocchio, modelos exponenciales-gaussianos, regresiÃ³n local y enfoques de regresiÃ³n logÃ­stica) para entornos adaptativos, examinados con retroalimentaciÃ³n de relevancia explÃ­cita e implÃ­cita, y evaluados con respecto a la optimizaciÃ³n de la utilidad en grandes colecciones de datos de referencia en los foros de TREC (Conferencias de RecuperaciÃ³n de Texto) y TDT (DetecciÃ³n y Seguimiento de Temas) [1, 4, 7, 15, 16, 20, 24, 23]. La regresiÃ³n logÃ­stica regularizada [21] se ha encontrado representativa para los enfoques de vanguardia, y altamente eficiente para adaptaciones frecuentes del modelo en grandes colecciones de documentos como el corpus TREC-10 (mÃ¡s de 800,000 documentos y 84 temas). A pesar de los logros sustanciales en la investigaciÃ³n reciente de filtrado adaptativo, aÃºn quedan problemas significativos sin resolver sobre cÃ³mo aprovechar de manera efectiva y eficiente la retroalimentaciÃ³n del usuario. EspecÃ­ficamente, los siguientes problemas pueden limitar seriamente la verdadera utilidad de los sistemas de AF en aplicaciones del mundo real: 1. El usuario tiene un papel bastante pasivo en la configuraciÃ³n convencional de filtrado adaptativo: solo reacciona ante el sistema cuando este toma una decisiÃ³n afirmativa sobre un documento, confirmando o rechazando esa decisiÃ³n. Una alternativa mÃ¡s activa serÃ­a permitir al usuario emitir mÃºltiples consultas sobre un tema, revisar una lista clasificada de documentos candidatos (o pasajes) por consulta, y proporcionar retroalimentaciÃ³n sobre la lista clasificada, refinando asÃ­ su necesidad de informaciÃ³n y solicitando listas clasificadas actualizadas. La Ãºltima forma de interacciÃ³n del usuario ha sido altamente efectiva en la recuperaciÃ³n estÃ¡ndar para consultas ad-hoc. CÃ³mo implementar una estrategia para necesidades de informaciÃ³n duraderas en entornos de FA es una pregunta abierta para la investigaciÃ³n. 2. La unidad para recibir un juicio de relevancia (sÃ­ o no) estÃ¡ restringida al nivel del documento en AF convencional. Sin embargo, un usuario real puede estar dispuesto a proporcionar comentarios mÃ¡s informativos y detallados resaltando algunas partes de texto en un documento recuperado como relevantes, en lugar de etiquetar todo el documento como relevante. Aprovechar de manera efectiva este tipo de retroalimentaciÃ³n detallada podrÃ­a mejorar sustancialmente la calidad de un sistema de AF. Para esto, necesitamos habilitar el aprendizaje supervisado a partir de fragmentos de texto etiquetados de longitud arbitraria en lugar de solo permitir documentos etiquetados. 3. Los documentos seleccionados por el sistema suelen ser altamente redundantes. Un evento importante de noticias, por ejemplo, serÃ­a reportado por mÃºltiples fuentes repetidamente durante un tiempo, haciendo que la mayor parte del contenido informativo en esos artÃ­culos sea redundante entre sÃ­. Un sistema de AF convencional seleccionarÃ­a todas estas historias de noticias redundantes para la retroalimentaciÃ³n del usuario, desperdiciando el tiempo de los usuarios mientras ofrece poco beneficio. Claramente, las tÃ©cnicas de detecciÃ³n de novedades pueden ayudar en principio [25, 2, 22] a mejorar la utilidad de los sistemas AF. Sin embargo, la efectividad de tales tÃ©cnicas a nivel de pasaje para detectar novedad con respecto a la retroalimentaciÃ³n de los usuarios a nivel detallado y para detectar redundancia en listas clasificadas aÃºn debe ser evaluada utilizando una medida de utilidad que imite las necesidades de un usuario real. Para abordar las limitaciones actuales de los sistemas de AF mencionadas anteriormente, proponemos y examinamos un nuevo enfoque en este artÃ­culo, que combina las fortalezas del AF convencional (aprendizaje incremental de modelos de temas), recuperaciÃ³n de pasajes multipase para consultas de larga duraciÃ³n condicionadas al tema, y detecciÃ³n de novedades para eliminar la redundancia de las interacciones del usuario con el sistema. Llamamos al nuevo proceso destilaciÃ³n de informaciÃ³n basada en utilidad. Ten en cuenta que los corpus de referencia convencionales para evaluaciones de AF, que tienen juicios de relevancia a nivel de documento y no definen tareas con mÃºltiples consultas, son insuficientes para evaluar el nuevo enfoque. Por lo tanto, ampliamos un corpus de referencia, la colecciÃ³n TDT4 de noticias y transmisiones de televisiÃ³n, con definiciones de tareas, mÃºltiples consultas por tarea y claves de respuestas por consulta. Hemos realizado nuestros experimentos en este corpus extendido TDT4 y hemos puesto a disposiciÃ³n del pÃºblico los datos generados adicionalmente para futuras evaluaciones comparativas. Para evaluar automÃ¡ticamente los fragmentos de texto arbitrarios devueltos por el sistema utilizando nuestras claves de respuestas, desarrollamos un esquema de evaluaciÃ³n con un procedimiento semiautomÃ¡tico para 1 URL: http://nyc.lti.cs.cmu.edu/downloads adquiriendo reglas que pueden comparar fragmentos con las respuestas del sistema. AdemÃ¡s, proponemos una extensiÃ³n de NDCG (Normalized Discounted Cumulated Gain) [9] para evaluar la utilidad de los pasajes clasificados como una funciÃ³n tanto de la relevancia como de la novedad. El resto de este documento estÃ¡ organizado de la siguiente manera. La secciÃ³n 2 describe el proceso de destilaciÃ³n de informaciÃ³n con un ejemplo concreto. La secciÃ³n 3 describe los nÃºcleos tÃ©cnicos de nuestro sistema llamado CAFÂ´E - CMU Adaptive Filtering Engine. La secciÃ³n 4 discute problemas con respecto a la metodologÃ­a de evaluaciÃ³n y propone un nuevo esquema. La secciÃ³n 5 describe el corpus extendido TDT4. La secciÃ³n 6 presenta nuestros experimentos y resultados. La secciÃ³n 7 concluye el estudio y ofrece perspectivas futuras. 2. Una TAREA DE EJEMPLO Considera un evento noticioso: la fuga de siete convictos de una prisiÃ³n de Texas en diciembre de 2000 y su captura un mes despuÃ©s. Suponiendo que un usuario estuviera interesado en este evento desde sus primeras etapas, la necesidad de informaciÃ³n podrÃ­a ser: Encontrar informaciÃ³n sobre la fuga de convictos de una prisiÃ³n de Texas, y la informaciÃ³n relacionada con su recaptura. Las preguntas de nivel inferior asociadas podrÃ­an ser: 1. Â¿CuÃ¡ntos prisioneros escaparon? 2. Â¿DÃ³nde y cuÃ¡ndo fueron avistados? 3. Â¿QuiÃ©nes son sus contactos conocidos dentro y fuera de la prisiÃ³n? 4. Â¿CÃ³mo estÃ¡n armados? 5. Â¿Tienen algÃºn vehÃ­culo? 6. Â¿QuÃ© pasos se han tomado hasta ahora? Llamamos a esta necesidad de informaciÃ³n una tarea, y a las preguntas asociadas como las consultas en esta tarea. Un sistema de destilaciÃ³n debe monitorear los documentos entrantes, procesarlos por fragmentos en un orden temporal, seleccionar pasajes potencialmente relevantes y novedosos de cada fragmento con respecto a cada consulta, y presentar una lista clasificada de pasajes al usuario. La clasificaciÃ³n de pasajes aquÃ­ se basa en cuÃ¡n relevante es un pasaje con respecto a la consulta actual, cuÃ¡n novedoso es con respecto al historial actual del usuario (de sus interacciones con el sistema) y cuÃ¡n redundante es en comparaciÃ³n con otros pasajes con un rango mÃ¡s alto en la lista. Cuando se le presenta una lista de pasajes, el usuario puede proporcionar retroalimentaciÃ³n resaltando tramos de texto arbitrarios que considere relevantes. Estos fragmentos de texto se toman como ejemplos positivos en la adaptaciÃ³n del perfil de consulta, y tambiÃ©n se aÃ±aden al historial de los usuarios. Los pasajes no marcados por el usuario se consideran ejemplos negativos. Tan pronto como se actualiza el perfil de consulta, el sistema vuelve a realizar una bÃºsqueda y devuelve otra lista clasificada de pasajes donde los pasajes previamente vistos son eliminados o clasificados como bajos, segÃºn la preferencia del usuario. Por ejemplo, si el usuario resalta ...los funcionarios han ofrecido una recompensa de $100,000 por su captura... como respuesta relevante a la consulta Â¿QuÃ© pasos se han tomado hasta ahora?, entonces la pieza resaltada se utiliza como un ejemplo de entrenamiento positivo adicional en la adaptaciÃ³n del perfil de la consulta. Este comentario tambiÃ©n se aÃ±ade al historial del usuario como un ejemplo visto, para que en el futuro, el sistema no coloque otro pasaje mencionando una recompensa de $100,000 en la parte superior de la lista clasificada. Sin embargo, un artÃ­culo que mencione que los funcionarios han duplicado el dinero de recompensa a $200,000 podrÃ­a clasificarse alto ya que es relevante tanto para el perfil de consulta (actualizado) como novedoso con respecto a la historia del usuario (actualizada). El usuario puede modificar las consultas originales o agregar una nueva consulta durante el proceso; los perfiles de consulta se cambiarÃ¡n en consecuencia. Claramente, la detecciÃ³n de novedades es muy importante para la utilidad de dicho sistema debido a la bÃºsqueda iterativa. Sin detecciÃ³n de novedad, los antiguos pasajes relevantes se mostrarÃ­an al usuario repetidamente en cada lista clasificada. A travÃ©s del ejemplo anterior, podemos ver las principales propiedades de nuestro nuevo marco de trabajo para la destilaciÃ³n de informaciÃ³n basada en utilidad sobre documentos ordenados temporalmente. Nuestro marco de trabajo combina y amplÃ­a el poder del filtrado adaptativo (AF), la recuperaciÃ³n ad-hoc (IR) y la detecciÃ³n de novedades (ND). En comparaciÃ³n con la IR estÃ¡ndar, nuestro enfoque tiene la capacidad de aprender de forma incremental las necesidades de informaciÃ³n a largo plazo y modelar una secuencia de consultas dentro de una tarea. En comparaciÃ³n con el AF convencional, permite un papel mÃ¡s activo del usuario en refinar sus necesidades de informaciÃ³n y solicitar nuevos resultados al permitir comentarios sobre relevancia y novedad mediante el resaltado de fragmentos arbitrarios de texto en los pasajes devueltos por el sistema. Comparado con trabajos anteriores, esta es la primera evaluaciÃ³n de detecciÃ³n de novedad integrada con filtrado adaptativo para consultas secuenciadas que permite retroalimentaciÃ³n flexible del usuario sobre pasajes clasificados. La combinaciÃ³n de AF, IR y ND con las nuevas extensiones plantea una importante pregunta de investigaciÃ³n sobre la metodologÃ­a de evaluaciÃ³n: Â¿cÃ³mo podemos medir la utilidad de un sistema de destilaciÃ³n de informaciÃ³n como este? Las mÃ©tricas existentes en IR estÃ¡ndar, AF y ND son insuficientes, y se deben explorar nuevas soluciones, como discutiremos en la SecciÃ³n 4, despuÃ©s de describir los nÃºcleos tÃ©cnicos de nuestro sistema en la prÃ³xima secciÃ³n. 3. NÃCLEOS TÃCNICOS Los componentes principales de CAFÃ son: 1) AF para el aprendizaje incremental de perfiles de consulta, 2) IR para estimar la relevancia de pasajes con respecto a los perfiles de consulta, 3) ND para evaluar la novedad de pasajes con respecto al historial de usuarios, y 4) un componente anti-redundancia para eliminar la redundancia de las listas clasificadas. 3.1 Componente de Filtrado Adaptativo Utilizamos un algoritmo de vanguardia en el campo, el mÃ©todo de regresiÃ³n logÃ­stica regularizada que obtuvo los mejores resultados en varios corpus de evaluaciÃ³n de referencia para AF [21]. La regresiÃ³n logÃ­stica (LR) es un algoritmo de aprendizaje supervisado para clasificaciÃ³n estadÃ­stica. BasÃ¡ndose en un conjunto de entrenamiento de instancias etiquetadas, aprende un modelo de clase que luego puede ser utilizado para predecir las etiquetas de instancias no vistas. Su rendimiento, asÃ­ como su eficiencia en tÃ©rminos de tiempo de entrenamiento, lo convierten en un buen candidato cuando se requieren actualizaciones frecuentes del modelo de clase, como es el caso en el filtrado adaptativo, donde el sistema debe aprender de cada nueva retroalimentaciÃ³n proporcionada por el usuario. (Consulte [21] y [23] para complejidad computacional y problemas de implementaciÃ³n). En el filtrado adaptativo, cada consulta se considera como una clase y la probabilidad de que un pasaje pertenezca a esta clase corresponde al grado de relevancia del pasaje con respecto a la consulta. Para entrenar el modelo, utilizamos la propia consulta como ejemplo inicial de entrenamiento positivo de la clase, y las piezas de texto resaltadas por el usuario (marcadas como Relevante o No relevante) durante la retroalimentaciÃ³n como ejemplos de entrenamiento adicionales. Para abordar el problema de inicio en frÃ­o en la etapa inicial antes de obtener cualquier retroalimentaciÃ³n del usuario, el sistema utiliza una pequeÃ±a muestra de un corpus retrospectivo como ejemplos negativos iniciales en el conjunto de entrenamiento. Los detalles de cÃ³mo utilizar la regresiÃ³n logÃ­stica para el filtrado adaptativo (asignando diferentes pesos a las instancias de entrenamiento positivas y negativas, y regularizando la funciÃ³n objetivo para prevenir el sobreajuste en los datos de entrenamiento) se presentan en [21]. El modelo de clase wâ aprendido por RegresiÃ³n LogÃ­stica, o el perfil de consulta, es un vector cuyas dimensiones son tÃ©rminos individuales y cuyos elementos son los coeficientes de regresiÃ³n, indicando cuÃ¡n influyente es cada tÃ©rmino en el perfil de consulta. El perfil de la consulta se actualiza cada vez que se recibe una nueva pieza de retroalimentaciÃ³n del usuario. Un peso decaÃ­do temporalmente se puede aplicar a cada ejemplo de entrenamiento, como una opciÃ³n, para enfatizar la retroalimentaciÃ³n mÃ¡s reciente del usuario. 3.2 Componente de RecuperaciÃ³n de Pasajes Utilizamos tÃ©cnicas estÃ¡ndar de IR en esta parte de nuestro sistema. Los documentos entrantes se procesan por bloques, donde cada bloque puede definirse como un intervalo de tiempo fijo o como un nÃºmero fijo de documentos, segÃºn lo prefiera el usuario. Para cada documento entrante, se actualizan las estadÃ­sticas del corpus como el IDF (Frecuencia Inversa de Documento) de cada tÃ©rmino. Utilizamos un identificador y rastreador de entidades con tecnologÃ­a de punta [8, 12] para identificar nombres de personas y lugares, y fusionarlos con entidades con referencia cruzada vistas en el pasado. Luego, los documentos se dividen en pasajes, que pueden ser un documento completo, un pÃ¡rrafo, una oraciÃ³n, o cualquier otro fragmento continuo de texto, segÃºn se prefiera. Cada pasaje se representa utilizando un vector de pesos TF-IDF (Frecuencia de TÃ©rmino - Frecuencia Inversa de Documento), donde el tÃ©rmino puede ser una palabra o una entidad nombrada. Dado un perfil de consulta, es decir, la soluciÃ³n de regresiÃ³n logÃ­stica wâ como se describe en la SecciÃ³n 3.1, el sistema calcula la probabilidad posterior de relevancia para cada pasaje x como fRL(x) â¡ P(y = 1|x, wâ ) = 1 (1 + eâwâÂ·x) (1). Los pasajes se ordenan segÃºn sus puntajes de relevancia, y aquellos con puntajes por encima de un umbral (ajustado en un conjunto de entrenamiento) conforman la lista de relevancia que se pasa al paso de detecciÃ³n de novedad. 3.3 Componente de DetecciÃ³n de Novedad CAFÂ´E mantiene un historial de usuario H(t), que contiene todos los fragmentos de texto hi que el usuario resaltÃ³ (como retroalimentaciÃ³n) durante sus interacciones pasadas con el sistema, hasta el tiempo actual t. Denotando el historial como H(t) = n h1, h2, ..., ht o , (2), el puntaje de novedad de un nuevo pasaje candidato x se calcula como: fND(x) = 1 â max iâ1..t {cos(x, hi)} (3), donde tanto el pasaje candidato x como los fragmentos de texto resaltados hi se representan como vectores TF-IDF. El puntaje de novedad de cada pasaje se compara con un umbral predefinido (ajustado tambiÃ©n en un conjunto de entrenamiento), y cualquier pasaje con un puntaje por debajo de este umbral se elimina de la lista de relevancia. Componente de clasificaciÃ³n anti-redundante Aunque el componente de detecciÃ³n de novedad garantiza que solo la informaciÃ³n novedosa (no vista previamente) permanezca en la lista de relevancia, esta lista aÃºn podrÃ­a contener la misma informaciÃ³n novedosa en mÃºltiples posiciones en la lista clasificada. Supongamos, por ejemplo, que el usuario ya ha leÃ­do sobre una recompensa de $100,000 por informaciÃ³n sobre los convictos fugados. Una nueva noticia de que el premio ha sido aumentado a $200,000 es novedosa ya que el usuario aÃºn no la ha leÃ­do. Sin embargo, mÃºltiples fuentes de noticias podrÃ­an informar sobre esta noticia y podrÃ­amos terminar mostrando artÃ­culos (redundantes) de todas estas fuentes en una lista clasificada. Por lo tanto, una lista clasificada tambiÃ©n debe ser no redundante con respecto a su propio contenido. Utilizamos una versiÃ³n simplificada del mÃ©todo de MÃ¡xima Relevancia Marginal [5], originalmente desarrollado para combinar relevancia y novedad en la recuperaciÃ³n de texto y resÃºmenes. Nuestro procedimiento comienza con la lista actual de pasajes ordenados por relevancia (secciÃ³n 3.2), filtrados por el componente de DetecciÃ³n de Novedades (secciÃ³n 3.3), y genera una nueva lista no redundante de la siguiente manera: 1. Toma el pasaje superior en la lista actual como el primero en la nueva lista. 2. Agrega el siguiente pasaje x en la lista actual a la nueva lista solo si fAR(x) > t donde fAR(x) = 1 â max piâLnew {cos(x, pi)} y Lnew es el conjunto de pasajes ya seleccionados en la nueva lista. Repetir el paso 2 hasta que se hayan examinado todos los pasajes en la lista actual. DespuÃ©s de aplicar el algoritmo mencionado anteriormente, cada pasaje en la nueva lista es lo suficientemente diferente a los demÃ¡s, favoreciendo asÃ­ la diversidad en lugar de la redundancia en la nueva lista clasificada. El umbral anti-redundancia t se ajusta en un conjunto de entrenamiento. 4. METODOLOGÃA DE EVALUACIÃN El enfoque que propusimos anteriormente para la destilaciÃ³n de informaciÃ³n plantea importantes cuestiones sobre la metodologÃ­a de evaluaciÃ³n. En primer lugar, dado que nuestro marco de trabajo permite que la salida sean fragmentos en diferentes niveles de granularidad (por ejemplo, ventanas de k oraciones donde k puede variar) en lugar de una longitud fija, no es posible tener juicios de relevancia preanotados en todos esos niveles de granularidad. En segundo lugar, dado que deseamos medir la utilidad de la salida del sistema como una combinaciÃ³n de relevancia y novedad, las medidas tradicionales basadas Ãºnicamente en la relevancia deben ser reemplazadas por medidas que penalicen la repeticiÃ³n de la misma informaciÃ³n en la salida del sistema a lo largo del tiempo. En tercer lugar, dado que la salida del sistema son listas clasificadas, debemos recompensar a aquellos sistemas que presenten informaciÃ³n Ãºtil (tanto relevante como no vista previamente) utilizando listas clasificadas mÃ¡s cortas, y penalizar a aquellos que presenten la misma informaciÃ³n utilizando listas clasificadas mÃ¡s largas. Ninguna de las medidas existentes en la recuperaciÃ³n ad-hoc, filtrado adaptativo, detecciÃ³n de novedades u otras Ã¡reas relacionadas (resumen de texto y respuesta a preguntas) tiene propiedades deseables en los tres aspectos. Por lo tanto, debemos desarrollar una nueva metodologÃ­a de evaluaciÃ³n. 4.1 Claves de Respuesta Para permitir la evaluaciÃ³n de un sistema cuya salida consiste en pasajes de longitud arbitraria, tomamos prestado el concepto de claves de respuesta de la comunidad de Preguntas y Respuestas (QA), donde los sistemas pueden devolver fragmentos de texto arbitrarios como respuestas. Las claves de respuestas definen lo que debe estar presente en la respuesta de un sistema para recibir crÃ©dito, y estÃ¡n compuestas por una colecciÃ³n de \"nuggets\" de informaciÃ³n, es decir, unidades de datos sobre las cuales los evaluadores humanos pueden tomar decisiones binarias sobre si una respuesta del sistema las contiene o no. La definiciÃ³n de claves de respuestas y la toma de decisiones binarias asociadas son tareas conceptuales que requieren un mapeo semÃ¡ntico [19], ya que los pasajes devueltos por el sistema pueden contener la misma informaciÃ³n expresada de muchas formas diferentes. Por lo tanto, las evaluaciones de QA han dependido de evaluadores humanos para la correspondencia entre diversas expresiones, lo que hace que el proceso sea costoso, consume mucho tiempo y no sea escalable para grandes colecciones de consultas y documentos, y evaluaciones extensas de sistemas con diversos ajustes de parÃ¡metros. 4.1.1 AutomatizaciÃ³n de la evaluaciÃ³n basada en claves de respuestas Los mÃ©todos de evaluaciÃ³n automÃ¡tica permitirÃ­an una construcciÃ³n y ajuste mÃ¡s rÃ¡pidos del sistema, asÃ­ como proporcionar una forma objetiva y asequible de comparar diversos sistemas. Recientemente, se han propuesto mÃ©todos basados mÃ¡s o menos en la idea de las co-ocurrencias de n-gramos. Pourpre [10] asigna una puntuaciÃ³n de recuerdo fraccional a una respuesta del sistema basada en su superposiciÃ³n de unigramas con una descripciÃ³n de pepitas dada. Por ejemplo, una respuesta del sistema A B C tiene una recuperaciÃ³n de 3/4 con respecto a un fragmento con la descripciÃ³n A B C D. Sin embargo, este enfoque es injusto para los sistemas que presentan la misma informaciÃ³n pero utilizando palabras distintas a A, B, C y D. Otro problema abierto es cÃ³mo ponderar las palabras individuales al medir la cercanÃ­a de una coincidencia. Por ejemplo, considera la pregunta Â¿CuÃ¡ntos prisioneros escaparon? En el fragmento Siete prisioneros escaparon de una prisiÃ³n de Texas, no hay indicaciÃ³n de que siete sea la palabra clave y que debe coincidir para obtener algÃºn crÃ©dito de relevancia. El uso de los valores de IDF no ayuda, ya que la palabra \"seven\" generalmente no tendrÃ¡ un IDF mÃ¡s alto que palabras como \"texas\" y \"prison\". AdemÃ¡s, redefinir el fragmento como simplemente siete no resuelve el problema, ya que ahora podrÃ­a coincidir errÃ³neamente con cualquier menciÃ³n de siete fuera de contexto. Nuggeteer [13] trabaja sobre principios similares pero toma decisiones binarias sobre si hay un fragmento en una respuesta de sistema dada ajustando un umbral. Sin embargo, tambiÃ©n estÃ¡ plagado de relevancia espuria ya que no todas las palabras contenidas en la descripciÃ³n de la pepita (o respuestas correctas conocidas) son centrales para la pepita. 4.1.2 Reglas de Coincidencia de Pepitas Proponemos un mÃ©todo automÃ¡tico confiable para determinar si un fragmento de texto contiene una pepita dada, basado en reglas de coincidencia de pepitas, que se generan utilizando un procedimiento semiautomÃ¡tico explicado a continuaciÃ³n. Estas reglas son bÃ¡sicamente consultas booleanas que solo coincidirÃ¡n con fragmentos que contengan la pepita. Por ejemplo, una regla candidata para emparejar respuestas a Â¿CuÃ¡ntos prisioneros escaparon? es (Texas Y siete Y escapar Y (convictos O prisioneros)), posiblemente con otros sinÃ³nimos y variantes en la regla. Para un corpus de artÃ­culos de noticias, que suelen seguir una prosa formal tÃ­pica, es bastante fÃ¡cil escribir reglas simples para coincidir con respuestas esperadas utilizando un enfoque de arranque, como se describe a continuaciÃ³n. Proponemos un enfoque de dos etapas, inspirado en Autoslog [14], que combina la capacidad de los humanos para identificar expresiones semÃ¡nticamente equivalentes y la capacidad del sistema para recopilar evidencia estadÃ­stica de un corpus de documentos anotados por humanos. En la primera etapa, los sujetos humanos anotaron (utilizando una herramienta de resaltado) porciones de documentos relevantes que contenÃ­an respuestas a cada fragmento 2. En la segunda etapa, los sujetos utilizaron nuestra herramienta de generaciÃ³n de reglas para crear reglas que coincidieran con las anotaciones de cada fragmento. La herramienta permite a los usuarios ingresar una regla booleana como una disyunciÃ³n de conjunciones (por ejemplo, ((a Y b) O (a Y c Y d) O (e))). Dada una regla candidata, nuestra herramienta la utiliza como una consulta booleana sobre el conjunto completo de documentos relevantes y calcula su recall y precisiÃ³n con respecto a las anotaciones que se espera que coincida. Por lo tanto, los sujetos pueden comenzar con una regla simple y refinarla iterativamente hasta que estÃ©n satisfechos con su precisiÃ³n y recuperaciÃ³n. Observamos que era muy fÃ¡cil para los humanos mejorar la precisiÃ³n de una regla ajustando sus conjunciones existentes (agregando mÃ¡s Y) y mejorar la recuperaciÃ³n agregando mÃ¡s conjunciones a la disyunciÃ³n (agregando mÃ¡s O). Como ejemplo, intentemos crear una regla para el acertijo que dice que siete prisioneros escaparon de la prisiÃ³n de Texas. Empezamos con una regla simple - (siete). Cuando introducimos esto en la herramienta de generaciÃ³n de reglas, nos damos cuenta de que esta regla coincide con muchas ocurrencias espurias de siete (por ejemplo, ...siete estados...) y, por lo tanto, obtiene una puntuaciÃ³n baja de precisiÃ³n. Podemos calificar aÃºn mÃ¡s nuestra regla - Texas Y siete Y convictos. A continuaciÃ³n, al revisar las anotaciones omitidas, nos damos cuenta de que algunos artÃ­culos de noticias mencionaban ...siete prisioneros escaparon.... Luego reemplazamos convictos por la disyunciÃ³n (convictos O prisioneros). Continuamos ajustando la regla de esta manera hasta lograr un nivel de recordaciÃ³n y precisiÃ³n suficientemente alto, es decir, que los (pocos) errores y alarmas falsas se puedan ignorar de forma segura. AsÃ­ podemos crear reglas de coincidencia de pepitas que capturen de manera sucinta varias formas de expresar una pepita, evitando al mismo tiempo emparejar respuestas incorrectas (o fuera de contexto). La participaciÃ³n humana en el proceso de creaciÃ³n de reglas garantiza reglas genÃ©ricas de alta calidad que luego se pueden utilizar para evaluar de manera confiable las respuestas del sistema arbitrario. 4.2 EvaluaciÃ³n de la Utilidad de una Secuencia de Listas Clasificadas La utilidad de un sistema de recuperaciÃ³n se puede definir como la diferencia entre cuÃ¡nta informaciÃ³n Ãºtil obtuvo el usuario y cuÃ¡nto tiempo y energÃ­a perdiÃ³. Calculamos esta utilidad a partir de las utilidades de los pasajes individuales de la siguiente manera. DespuÃ©s de leer cada pasaje devuelto por el sistema, el usuario obtiene cierta ganancia dependiendo de la presencia de informaciÃ³n relevante y novedosa, y incurre en una pÃ©rdida en tÃ©rminos del tiempo y la energÃ­a invertidos en revisar el pasaje. Sin embargo, la probabilidad de que el usuario realmente lea un pasaje depende de su posiciÃ³n en la lista clasificada. Por lo tanto, para una consulta q, la utilidad esperada 2 LDC [18] ya proporciona juicios de relevancia para 100 temas en el corpus TDT4. AdemÃ¡s, nos aseguramos de que estos juicios sean exhaustivos en todo el corpus utilizando la agrupaciÃ³n. El valor de un pasaje pi en el rango i se puede definir como U(pi, q) = P(i) â (Ganancia(pi, q) - PÃ©rdida(pi, q)) (4) donde P(i) es la probabilidad de que el usuario pase por un pasaje en el rango i. La utilidad esperada para una lista clasificada completa de longitud n se puede calcular simplemente sumando la utilidad esperada de cada pasaje: U(q) = Î£ i=1 P(i) â (Ganancia(pi, q) â PÃ©rdida(pi, q)) (5) NÃ³tese que si ignoramos el tÃ©rmino de pÃ©rdida y definimos P(i) como P(i) â 1/ logb(b + i â 1) (6) entonces obtenemos la mÃ©trica recientemente popularizada llamada Ganancia Acumulada Descontada (DCG) [9], donde Ganancia(pi, q) se define como la relevancia graduada del pasaje pi. Sin embargo, sin el tÃ©rmino de pÃ©rdida, DCG es una mÃ©trica puramente orientada al recuerdo y no es adecuada para un entorno de filtrado adaptativo, donde la utilidad del sistema depende en parte de su capacidad para limitar el nÃºmero de elementos mostrados al usuario. Aunque P(i) podrÃ­a definirse en base a estudios empÃ­ricos del comportamiento del usuario, por simplicidad, utilizamos P(i) tal como se define exactamente en la ecuaciÃ³n 6. La ganancia G(pi, q) del pasaje pi con respecto a la consulta q es una funciÃ³n de - 1) el nÃºmero de pepitas relevantes presentes en pi, y 2) la novedad de cada una de estas pepitas. Combinamos estos dos factores de la siguiente manera. Para cada pepita Nj, asignamos un peso inicial wj, y tambiÃ©n mantenemos un conteo nj del nÃºmero de veces que esta pepita ha sido vista por el usuario en el pasado. La ganancia derivada de cada ocurrencia posterior del mismo hallazgo se asume que se reduce por un factor de amortiguamiento Î³. Por lo tanto, G(pi, q) se define como G(pi, q) = X Nj âC(pi,q) wj â Î³nj (7) donde C(pi, q) es el conjunto de todas las pepitas que aparecen en el pasaje pi y tambiÃ©n pertenecen a la clave de respuesta de la consulta q. Los pesos iniciales wj se establecen todos en 1.0 en nuestros experimentos, pero tambiÃ©n pueden establecerse segÃºn un enfoque de pirÃ¡mide [11]. La elecciÃ³n del factor de amortiguaciÃ³n Î³ determina la tolerancia del usuario a la redundancia. Cuando Î³ = 0, un pepita solo recibirÃ¡ crÃ©dito por su primera ocurrencia, es decir, cuando nj sea cero. Para 0 < Î³ < 1, un nugget recibe menos crÃ©dito por cada ocurrencia sucesiva. Cuando Î³ = 1, no se produce amortiguamiento y las repeticiones de un valor reciben el mismo crÃ©dito. Se debe tener en cuenta que los recuentos de ocurrencia de las pepitas se conservan entre la evaluaciÃ³n de las listas clasificadas sucesivas devueltas por el sistema, ya que se espera que los usuarios recuerden lo que el sistema les mostrÃ³ en el pasado. Definimos la pÃ©rdida L(pi, q) como un costo constante c (usamos 0.1) incurrido al leer un pasaje devuelto por el sistema. Por lo tanto, nuestra mÃ©trica puede ser reescrita como U(q) = nX i=1 Ganancia(pi, q) logb(b + i â 1) â L(n) (8) donde L(n) es la pÃ©rdida asociada con una lista clasificada de longitud n: L(n) = c Â· nX i=1 1 logb(b + i â 1) (9) 3 NÃ³tese que 00 = 1 Debido a la similitud con la Ganancia Acumulada Descontada (DCG), llamamos a nuestra mÃ©trica Utilidad Acumulada Descontada (DCU). El puntaje de DCU obtenido por el sistema se convierte en un puntaje de DCU normalizado (NDCU) dividiÃ©ndolo por el puntaje de DCU de la lista ideal clasificada, que se crea ordenando los pasajes por sus puntajes de utilidad decrecientes U(pi, q) y deteniÃ©ndose cuando U(pi, q) â¤ 0, es decir, cuando la ganancia es menor o igual al costo de leer el pasaje. El conjunto de datos TDT4 fue el corpus de referencia utilizado en las evaluaciones TDT2002 y TDT2003. El corpus consiste en mÃ¡s de 90,000 artÃ­culos de noticias de mÃºltiples fuentes (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) publicados entre octubre de 2000 y enero de 2001, en los idiomas Ã¡rabe, inglÃ©s y mandarÃ­n. Las versiones de los artÃ­culos en otros idiomas reconocidas por voz y traducidas por mÃ¡quina tambiÃ©n fueron proporcionadas. LDC [18] ha anotado el corpus con 100 temas, que corresponden a varios eventos de noticias en este perÃ­odo de tiempo. De entre estos, seleccionamos un subconjunto de 12 eventos accionables y definimos tareas correspondientes para ellos. Para cada tarea, definimos manualmente un perfil que consiste en un conjunto inicial de (5 a 10) consultas, una descripciÃ³n de texto libre del historial del usuario, es decir, lo que el usuario ya sabe sobre el evento, y una lista de documentos conocidos relevantes y no relevantes (si estÃ¡n disponibles) como ejemplos de entrenamiento. Para cada consulta, generamos claves de respuestas y reglas de coincidencia de pepitas correspondientes utilizando el procedimiento descrito en la secciÃ³n 4.1.2, y produjimos un total de 120 consultas, con un promedio de 7 pepitas por consulta. EXPERIMENTOS Y RESULTADOS 6.1 Baselines Utilizamos Indri [17], un motor de bÃºsqueda basado en modelos de lenguaje popular, como referencia para comparar con CAFÃ. Indri admite la funcionalidad estÃ¡ndar de los motores de bÃºsqueda, incluido el feedback de pseudo-relevancia (PRF) [3, 6], y es representativo de un sistema de recuperaciÃ³n basado en consultas tÃ­pico. Indri no admite ningÃºn tipo de detecciÃ³n de novedades. Comparamos Indri con PRF activado y desactivado, frente a CAFÃ con retroalimentaciÃ³n del usuario, detecciÃ³n de novedades y clasificaciÃ³n antirredundante activados y desactivados. 6.2 ConfiguraciÃ³n Experimental Dividimos el corpus TDT4 que abarca 4 meses en 10 fragmentos, cada uno definido como un perÃ­odo de 12 dÃ­as consecutivos. En cualquier momento dado del proceso de destilaciÃ³n, cada sistema accedÃ­a a los datos pasados hasta el punto actual, y devolvÃ­a una lista clasificada de hasta 50 pasajes por consulta. Las 12 tareas definidas en el corpus se dividieron en un conjunto de entrenamiento y prueba con 6 tareas cada uno. Cada sistema pudo utilizar el conjunto de entrenamiento para ajustar sus parÃ¡metros con el fin de optimizar NDCU (ecuaciÃ³n 8), incluyendo el umbral de relevancia tanto para Indri como para CAFÂ´E, y los umbrales de novedad y antirredundancia para CAFÂ´E. La NDCU para cada ejecuciÃ³n del sistema se calcula automÃ¡ticamente. TambiÃ©n se simulÃ³ la retroalimentaciÃ³n de los usuarios: los juicios de relevancia para cada pasaje devuelto por el sistema (segÃºn las reglas de coincidencia de pepitas descritas en la secciÃ³n 4.1.2) fueron utilizados como retroalimentaciÃ³n de los usuarios en la adaptaciÃ³n de perfiles de consulta e historiales de usuario. En la Tabla 1, mostramos los puntajes NDCU de los dos sistemas en diferentes configuraciones. Estas puntuaciones se promedian en las seis tareas del conjunto de pruebas, y se calculan con dos factores de amortiguaciÃ³n (ver secciÃ³n 4.2): Î³ = 0 y 0.1, para simular ninguna tolerancia y poca tolerancia para la redundancia, respectivamente. Usar Î³ = 0 crea una mÃ©trica mucho mÃ¡s estricta ya que no otorga ningÃºn crÃ©dito a un pasaje que contenga informaciÃ³n relevante pero redundante. Por lo tanto, la mejora obtenida al habilitar la retroalimentaciÃ³n del usuario es menor con Î³ = 0 que la mejora obtenida de la retroalimentaciÃ³n con Î³ = 0.1. Esto revela una deficiencia de los sistemas de recuperaciÃ³n contemporÃ¡neos: cuando el usuario da retroalimentaciÃ³n positiva sobre un pasaje, los sistemas otorgan mayor peso a los tÃ©rminos presentes en ese pasaje y tienden a recuperar otros pasajes que contienen los mismos tÃ©rminos, y por lo tanto, generalmente la misma informaciÃ³n. Sin embargo, el usuario no se beneficia al ver pasajes redundantes y generalmente estÃ¡ interesado en otros pasajes que contengan informaciÃ³n relacionada. Es informativo evaluar los sistemas de recuperaciÃ³n utilizando nuestra medida de utilidad (con Î³ = 0) que tiene en cuenta la novedad y, por lo tanto, ofrece una imagen mÃ¡s realista de cuÃ¡n bien un sistema puede generalizar a partir de la retroalimentaciÃ³n del usuario, en lugar de utilizar medidas tradicionales de RI como la recuperaciÃ³n y la precisiÃ³n, que ofrecen una imagen incompleta de la mejora obtenida a partir de la retroalimentaciÃ³n del usuario. A veces, sin embargo, los usuarios podrÃ­an estar interesados en ver la misma informaciÃ³n de mÃºltiples fuentes, como un indicador de su importancia o fiabilidad. En tal caso, simplemente podemos elegir un valor mÃ¡s alto para Î³ que corresponda a una mayor tolerancia para la redundancia, y asÃ­ permitir que el sistema ajuste sus parÃ¡metros en consecuencia. Dado que los documentos fueron procesados por fragmentos, serÃ­a interesante ver cÃ³mo mejora el rendimiento de los sistemas con el tiempo. Las figuras 1 y 2 muestran las tendencias de rendimiento de ambos sistemas a lo largo de los fragmentos. Si bien se espera que el rendimiento con y sin retroalimentaciÃ³n en los primeros fragmentos sea similar, para los fragmentos posteriores, la curva de rendimiento con la retroalimentaciÃ³n habilitada supera a la del ajuste sin retroalimentaciÃ³n. Las tendencias de rendimiento no son consistentes en todos los fragmentos porque los documentos relevantes no estÃ¡n distribuidos uniformemente en todos los fragmentos, lo que hace que algunas consultas sean mÃ¡s fÃ¡ciles que otras en ciertos fragmentos. AdemÃ¡s, dado que Indri utiliza retroalimentaciÃ³n de pseudo-relevancia mientras que CAFÂ´E utiliza retroalimentaciÃ³n basada en juicios de relevancia reales, la mejora en el caso de Indri es menos dramÃ¡tica que la de CAFÂ´E. 7. CONCLUSIONES FINALES Este artÃ­culo presenta la primera investigaciÃ³n sobre la destilaciÃ³n de informaciÃ³n basada en la utilidad con un sistema que aprende las necesidades de informaciÃ³n duraderas a partir de la retroalimentaciÃ³n detallada del usuario sobre una secuencia de pasajes clasificados. Nuestro sistema, llamado CAFÂ´E, combina filtrado adaptativo, detecciÃ³n de novedades y clasificaciÃ³n de pasajes antirredundantes en un marco unificado para la optimizaciÃ³n de la utilidad. Desarrollamos un nuevo esquema para la evaluaciÃ³n y retroalimentaciÃ³n automatizadas basado en un procedimiento semiautomÃ¡tico para adquirir reglas que permitan emparejar automÃ¡ticamente fragmentos con las respuestas del sistema. TambiÃ©n propusimos una extensiÃ³n de la <br>mÃ©trica NDCG</br> para evaluar la utilidad de los pasajes clasificados como una combinaciÃ³n ponderada de relevancia y novedad. Nuestros experimentos en el reciÃ©n anotado corpus de referencia TDT4 muestran un alentador aumento de utilidad sobre Indri, y tambiÃ©n sobre nuestro propio sistema con aprendizaje incremental y detecciÃ³n de novedades desactivados. 8. AGRADECIMIENTOS Nos gustarÃ­a agradecer a Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng y al Programa de AnÃ¡lisis de Datos Cualitativos de la Universidad de Pittsburgh liderado por el Dr. Stuart Shulman por su ayuda en la recolecciÃ³n y procesamiento de las anotaciones extendidas de TDT4 utilizadas en nuestros experimentos. Este trabajo estÃ¡ parcialmente respaldado por la FundaciÃ³n Nacional de Ciencias (NSF) bajo la subvenciÃ³n IIS0434035, y la Agencia de Proyectos de InvestigaciÃ³n Avanzada de Defensa (DARPA) bajo los contratos NBCHD030010 y W0550432. Cualquier opiniÃ³n, hallazgo, conclusiÃ³n o recomendaciÃ³n expresada en este material son responsabilidad de los autores y no reflejan necesariamente los puntos de vista de los patrocinadores. AUTORES ADICIONALES Jian Zhang (jianzhan@stat.purdue.edu)â , Jaime Carbonell (jgc@cs.cmu.edu)â  , Peter Brusilovsky (peterb+@pitt.edu)â¡ , Daqing He(dah44@pitt.edu)â¡ 10. REFERENCIAS [1] J. Allan. RetroalimentaciÃ³n incremental de relevancia para la filtraciÃ³n de informaciÃ³n. Actas de la 19Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 270-278, 1996. [2] J. Allan, C. Wade y A. Bolivar. RecuperaciÃ³n y DetecciÃ³n de Novedad a Nivel de OraciÃ³n. Actas de la conferencia ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, 2003. [3] C. Buckley, G. Salton y J. Allan. RecuperaciÃ³n automÃ¡tica con informaciÃ³n de localidad utilizando SMART. PublicaciÃ³n especial del NIST, (500207):59-72, 1993. [4] J. Callan. Aprendiendo mientras se filtran documentos. Actas de la 21Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 224-231, 1998. [5] J. Carbonell y J. Goldstein. El uso de MMR, Reordenamiento basado en Diversidad para Reorganizar Documentos y Producir ResÃºmenes. Actas de la 21Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 335-336, 1998. [6] E. Efthimiadis. ExpansiÃ³n de consulta. RevisiÃ³n Anual de Ciencia de la InformaciÃ³n y TecnologÃ­a (ARIST), 31:p121-87, 1996. [7] J. Fiscus y G. Duddington. Resumen de DetecciÃ³n y Seguimiento de Temas. DetecciÃ³n y seguimiento de temas: OrganizaciÃ³n de la informaciÃ³n basada en eventos, pÃ¡ginas 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov y S. Roukos. Un modelo estadÃ­stico para la detecciÃ³n y seguimiento de entidades multilingÃ¼es. NAACL/HLT, 2004. [9] K. JÃ¤rvelin y J. KekÃ¤lÃ¤inen. EvaluaciÃ³n basada en la Ganancia Acumulada de TÃ©cnicas de RecuperaciÃ³n de InformaciÃ³n. ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin y D. Demner-Fushman. EvaluaciÃ³n automÃ¡tica de respuestas a preguntas de definiciÃ³n. Actas de la Conferencia de TecnologÃ­a del Lenguaje Humano y Conferencia sobre MÃ©todos EmpÃ­ricos en Procesamiento del Lenguaje Natural (HLT/EMNLP 2005), 2005. â Departamento de EstadÃ­stica, Universidad de Purdue, West Lafayette, EE. UU. â  Instituto de TecnologÃ­as del Lenguaje, Universidad Carnegie Mellon, Pittsburgh, EE. UU. â¡ Escuela de Ciencias de la InformaciÃ³n, Universidad de Pittsburgh, Pittsburgh, EE. UU. [11] J. Lin y D. Demner-Fushman. Â¿Se derrumbarÃ¡n las pirÃ¡mides construidas con pepitas? Actas de HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla y S. Roukos. Un algoritmo de resoluciÃ³n de correferencia sÃ­ncrona basado en el Ã¡rbol de Bell. Actas de ACL, 4:136-143, 2004. [13] G. Marton. Nuggeteer: EvaluaciÃ³n AutomÃ¡tica Basada en Pepitas Utilizando Descripciones y Juicios. HLT/NAACL, 2006. [14] E. Riloff. ConstrucciÃ³n automÃ¡tica de un diccionario para tareas de extracciÃ³n de informaciÃ³n. Actas de la UndÃ©cima Conferencia Nacional sobre Inteligencia Artificial, pÃ¡ginas 811-816, 1993. [15] S. Robertson y S. Walker. Microsoft Cambridge en TREC-9: Pista de filtrado. La Novena Conferencia de RecuperaciÃ³n de Texto (TREC-9), pÃ¡ginas 361-368. [16] R. Schapire, Y. Cantante, y A. Singhal. Boosting y Rocchio aplicados a la filtraciÃ³n de texto. Actas de la 21Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle y W. Croft. Indri: Un motor de bÃºsqueda basado en modelos de lenguaje para consultas complejas. Actas de la Conferencia Internacional sobre AnÃ¡lisis de Inteligencia, 2004. [18] El Consorcio de Datos LingÃ¼Ã­sticos. http://www.ldc.upenn.edu/. [19] E. Voorhees. Resumen de la pista de preguntas y respuestas TREC 2003. Actas de la DuodÃ©cima Conferencia de RecuperaciÃ³n de Texto (TREC 2003), 2003. [20] Y. Yang y B. Kisiel. RegresiÃ³n local basada en mÃ¡rgenes para filtrado adaptativo. Actas de la duodÃ©cima conferencia internacional sobre gestiÃ³n de la informaciÃ³n y el conocimiento, pÃ¡ginas 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang y B. Kisiel. Robustez de los MÃ©todos de Filtrado Adaptativo en una EvaluaciÃ³n de Referencia Cruzada. Actas de la 28Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 98-105, 2005. [22] C. Zhai, W. Cohen y J. Lafferty. MÃ¡s allÃ¡ de la relevancia independiente: MÃ©todos y mÃ©tricas de evaluaciÃ³n para la recuperaciÃ³n de subtemas. Actas de la 26Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 10-17, 2003. [23] J. Zhang y Y. Yang. Robustez de los MÃ©todos de ClasificaciÃ³n Lineal Regularizados en la CategorizaciÃ³n de Textos. Actas de la 26Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 190-197, 2003. [24] Y. Zhang. Utilizando priors bayesianos para combinar clasificadores para filtrado adaptativo. Actas de la 27Âª conferencia internacional anual sobre InvestigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 345-352, 2004. [25] Y. Zhang, J. Callan y T. Minka. DetecciÃ³n de novedad y redundancia en el filtrado adaptativo. Actas de la 25Âª Conferencia Internacional Anual de ACM SIGIR sobre InvestigaciÃ³n y Desarrollo en RecuperaciÃ³n de InformaciÃ³n, 2002. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "utility-base distillation": {
            "translated_key": "destilaciÃ³n basada en la utilidad",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Utility-based Information Distillation Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu ABSTRACT This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework.",
                "It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant passage ranking with respect to long-lasting information needs (tasks with multiple queries).",
                "Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings.",
                "For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task.",
                "Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty.",
                "Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval.",
                "Adaptive filtering (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics.",
                "Based on the initial query and a few positive examples (if available), an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user.",
                "The incremental learning nature of AF systems makes them more powerful than standard search engines that support ad-hoc retrieval (e.g.",
                "Google and Yahoo) in terms of finding relevant information with respect to long-lasting topics of interest, and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs, without having to modify their queries manually.",
                "A variety of supervised learning algorithms (Rocchio-style classifiers, Exponential-Gaussian models, local regression and logistic regression approaches) have been studied for adaptive settings, examined with explicit and implicit relevance feedback, and evaluated with respect to utility optimization on large benchmark data collections in TREC (Text Retrieval Conferences) and TDT (Topic Detection and Tracking) forums [1, 4, 7, 15, 16, 20, 24, 23].",
                "Regularized logistic regression [21] has been found representative for the state-of-the-art approaches, and highly efficient for frequent model adaptations over large document collections such as the TREC-10 corpus (over 800,000 documents and 84 topics).",
                "Despite substantial achievements in recent adaptive filtering research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently.",
                "Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications: 1.",
                "User has a rather passive role in the conventional adaptive filtering setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision.",
                "A more active alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents (or passages) per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists.",
                "The latter form of user interaction has been highly effective in standard retrieval for ad-hoc queries.",
                "How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2.",
                "The unit for receiving a relevance judgment (yes or no) is restricted to the document level in conventional AF.",
                "However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant.",
                "Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system.",
                "For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3.",
                "System-selected documents are often highly redundant.",
                "A major news event, for example, would be reported by multiple sources repeatedly for a while, making most of the information content in those articles redundant with each other.",
                "A conventional AF system would select all these redundant news stories for user feedback, wasting the users time while offering little gain.",
                "Clearly, techniques for novelty detection can help in principle [25, 2, 22] for improving the utility of the AF systems.",
                "However, the effectiveness of such techniques at passage level to detect novelty with respect to users (fine-grained) feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user.",
                "To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and novelty detection for removal of redundancy from user interactions with the system.",
                "We call the new process utility-based information distillation.",
                "Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach.",
                "Therefore, we extended a benchmark corpus - the TDT4 collection of news stories and TV broadcasts - with task definitions, multiple queries per task, and answer keys per query.",
                "We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 .",
                "To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses.",
                "Moreover, we propose an extension of NDCG (Normalized Discounted Cumulated Gain) [9] for assessing the utility of ranked passages as a function of both relevance and novelty.",
                "The rest of this paper is organized as follows.",
                "Section 2 outlines the information distillation process with a concrete example.",
                "Section 3 describes the technical cores of our system called CAFÂ´E - CMU Adaptive Filtering Engine.",
                "Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme.",
                "Section 5 describes the extended TDT4 corpus.",
                "Section 6 presents our experiments and results.",
                "Section 7 concludes the study and gives future perspectives. 2.",
                "A SAMPLE TASK Consider a news event - the escape of seven convicts from a Texas prison in December 2000 and their capture a month later.",
                "Assuming a user were interested in this event since its early stage, the information need could be: Find information about the escape of convicts from Texas prison, and information related to their recapture.",
                "The associated lower-level questions could be: 1.",
                "How many prisoners escaped? 2.",
                "Where and when were they sighted? 3. Who are their known contacts inside and outside the prison? 4.",
                "How are they armed? 5.",
                "Do they have any vehicles? 6.",
                "What steps have been taken so far?",
                "We call such an information need a task, and the associated questions as the queries in this task.",
                "A distillation system is supposed to monitor the incoming documents, process them chunk by chunk in a temporal order, select potentially relevant and novel passages from each chunk with respect to each query, and present a ranked list of passages to the user.",
                "Passage ranking here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list.",
                "When presented with a list of passages, the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant.",
                "These spans of text are taken as positive examples in the adaptation of the query profile, and also added to the users history.",
                "Passages not marked by the user are taken as negative examples.",
                "As soon as the query profile is updated, the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low, based on user preference.",
                "For example, if the user highlights ...officials have posted a $100,000 reward for their capture... as relevant answer to the query What steps have been taken so far?, then the highlighted piece is used as an additional positive training example in the adaptation of the query profile.",
                "This piece of feedback is also added to the user history as a seen example, so that in future, the system will not place another passage mentioning $100,000 reward at the top of the ranked list.",
                "However, an article mentioning ...officials have doubled the reward money to $200,000... might be ranked high since it is both relevant to the (updated) query profile and novel with respect to the (updated) user history.",
                "The user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly.",
                "Clearly, novelty detection is very important for the utility of such a system because of the iterative search.",
                "Without novelty detection, the old relevant passages would be shown to the user repeatedly in each ranked list.",
                "Through the above example, we can see the main properties of our new framework for utility-based information distillation over temporally ordered documents.",
                "Our framework combines and extends the power of adaptive filtering (AF), ad-hoc retrieval (IR) and novelty detection (ND).",
                "Compared to standard IR, our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task.",
                "Compared to conventional AF, it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system.",
                "Compared to past work, this is the first evaluation of novelty detection integrated with adaptive filtering for sequenced queries that allows flexible user feedback over ranked passages.",
                "The combination of AF, IR and ND with the new extensions raises an important research question regarding evaluation methodology: how can we measure the utility of such an information distillation system?",
                "Existing metrics in standard IR, AF and ND are insufficient, and new solutions must be explored, as we will discuss in Section 4, after describing the technical cores of our system in the next section. 3.",
                "TECHNICAL CORES The core components of CAFÂ´E are - 1) AF for incremental learning of query profiles, 2) IR for estimating relevance of passages with respect to query profiles, 3) ND for assessing novelty of passages with respect to users history, and 4) anti-redundancy component to remove redundancy from ranked lists. 3.1 Adaptive Filtering Component We use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for AF [21].",
                "Logistic regression (LR) is a supervised learning algorithm for statistical classification.",
                "Based on a training set of labeled instances, it learns a class model which can then by used to predict the labels of unseen instances.",
                "Its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required, as is the case in adaptive filtering, where the system must learn from each new feedback provided by the user. (See [21] and [23] for computational complexity and implementation issues).",
                "In adaptive filtering, each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query.",
                "For training the model, we use the query itself as the initial positive training example of the class, and the user-highlighted pieces of text (marked as Relevant or Not-relevant) during feedback as additional training examples.",
                "To address the cold start issue in the early stage before any user feedback is obtained, the system uses a small sample from a retrospective corpus as the initial negative examples in the training set.",
                "The details of using logistic regression for adaptive filtering (assigning different weights to positive and negative training instances, and regularizing the objective function to prevent over-fitting on training data) are presented in [21].",
                "The class model wâ learned by Logistic Regression, or the query profile, is a vector whose dimensions are individual terms and whose elements are the regression coefficients, indicating how influential each term is in the query profile.",
                "The query profile is updated whenever a new piece of user feedback is received.",
                "A temporally decaying weight can be applied to each training example, as an option, to emphasize the most recent user feedback. 3.2 Passage Retrieval Component We use standard IR techniques in this part of our system.",
                "Incoming documents are processed in chunks, where each chunk can be defined as a fixed span of time or as a fixed number of documents, as preferred by the user.",
                "For each incoming document, corpus statistics like the IDF (Inverted Document Frequency) of each term are updated.",
                "We use a state-of-the-art named entity identifier and tracker [8, 12] to identify person and location names, and merge them with co-referent named entities seen in the past.",
                "Then the documents are segmented into passages, which can be a whole document, a paragraph, a sentence, or any other continuous span of text, as preferred.",
                "Each passage is represented using a vector of TF-IDF (Term FrequencyInverse Document Frequency) weights, where term can be a word or a named entity.",
                "Given a query profile, i.e. the logistic regression solution wâ as described in Section 3.1, the system computes the posterior probability of relevance for each passage x as fRL(x) â¡ P(y = 1|x, wâ ) = 1 (1 + eâwâÂ·x) (1) Passages are ordered by their relevance scores, and the ones with scores above a threshold (tuned on a training set) comprise the relevance list that is passed on to the novelty detection step. 3.3 Novelty Detection Component CAFÂ´E maintains a user history H(t), which contains all the spans of text hi that the user highlighted (as feedback) during his or her past interactions with the system, up to the current time t. Denoting the history as H(t) = n h1, h2, ..., ht o , (2) the novelty score of a new candidate passage x is computed as: fND(x) = 1 â max iâ1..t {cos(x, hi)} (3) where both candidate passage x and highlighted spans of text hi are represented as TF-IDF vectors.",
                "The novelty score of each passage is compared to a prespecified threshold (also tuned on a training set), and any passage with a score below this threshold is removed from the relevance list. 3.4 Anti-redundant Ranking Component Although the novelty detection component ensures that only novel (previously unseen) information remains in the relevance list, this list might still contain the same novel information at multiple positions in the ranked list.",
                "Suppose, for example, that the user has already read about a $100,000 reward for information about the escaped convicts.",
                "A new piece of news that the award has been increased to $200,000 is novel since the user hasnt read about it yet.",
                "However, multiple news sources would report this news and we might end up showing (redundant) articles from all these sources in a ranked list.",
                "Hence, a ranked list should also be made non-redundant with respect to its own contents.",
                "We use a simplified version of the Maximal Marginal Relevance method [5], originally developed for combining relevance and novelty in text retrieval and summarization.",
                "Our procedure starts with the current list of passages sorted by relevance (section 3.2), filtered by Novelty Detection component (section 3.3), and generates a new non-redundant list as follows: 1.",
                "Take the top passage in the current list as the top one in the new list. 2.",
                "Add the next passage x in the current list to the new list only if fAR(x) > t where fAR(x) = 1 â max piâLnew {cos(x, pi)} and Lnew is the set of passages already selected in the new list. 3.",
                "Repeat step 2 until all the passages in the current list have been examined.",
                "After applying the above-mentioned algorithm, each passage in the new list is sufficiently dissimilar to others, thus favoring diversity rather than redundancy in the new ranked list.",
                "The anti-redundancy threshold t is tuned on a training set. 4.",
                "EVALUATION METHODOLOGY The approach we proposed above for information distillation raises important issues regarding evaluation methodology.",
                "Firstly, since our framework allows the output to be passages at different levels of granularity (e.g. k-sentence windows where k may vary) instead of a fixed length, it is not possible to have pre-annotated relevance judgments at all such granularity levels.",
                "Secondly, since we wish to measure the utility of the system output as a combination of both relevance and novelty, traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time.",
                "Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists.",
                "None of the existing measures in ad-hoc retrieval, adaptive filtering, novelty detection or other related areas (text summarization and question answering) have desirable properties in all the three aspects.",
                "Therefore, we must develop a new evaluation methodology. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers.",
                "Answer keys define what should be present in a system response to receive credit, and are comprised of a collection of information nuggets, i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them.",
                "Defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping [19], since system-returned passages can contain the same information expressed in many different ways.",
                "Hence, QA evaluations have relied on human assessors for the mapping between various expressions, making the process costly, time consuming, and not scalable to large query and document collections, and extensive system evaluations with various parameter settings. 4.1.1 Automating Evaluation based on Answer Keys Automatic evaluation methods would allow for faster system building and tuning, as well as provide an objective and affordable way of comparing various systems.",
                "Recently, such methods have been proposed, more or less, based on the idea of n-gram co-occurrences.",
                "Pourpre [10] assigns a fractional recall score to a system response based on its unigram overlap with a given nuggets description.",
                "For example, a system response A B C has recall 3/4 with respect to a nugget with description A B C D. However, such an approach is unfair to systems that present the same information but using words other than A, B, C, and D. Another open issue is how to weight individual words in measuring the closeness of a match.",
                "For example, consider the question How many prisoners escaped?.",
                "In the nugget Seven prisoners escaped from a Texas prison, there is no indication that seven is the keyword, and that it must be matched to get any relevance credit.",
                "Using IDF values does not help, since seven will generally not have a higher IDF than words like texas and prison.",
                "Also, redefining the nugget as just seven does not solve the problem since now it might spuriously match any mention of seven out of context.",
                "Nuggeteer [13] works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold.",
                "However, it is also plagued by spurious relevance since not all words contained in the nugget description (or known correct responses) are central to the nugget. 4.1.2 Nugget-Matching Rules We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on nugget-matching rules, which are generated using a semiautomatic procedure explained below.",
                "These rules are essentially Boolean queries that will only match against snippets that contain the nugget.",
                "For instance, a candidate rule for matching answers to How many prisoners escaped? is (Texas AND seven AND escape AND (convicts OR prisoners)), possibly with other synonyms and variants in the rule.",
                "For a corpus of news articles, which usually follow a typical formal prose, it is fairly easy to write such simple rules to match expected answers using a bootstrap approach, as described below.",
                "We propose a two-stage approach, inspired by Autoslog [14], that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents.",
                "In the first stage, human subjects annotated (using a highlighting tool) portions of ontopic documents that contained answers to each nugget 2 .",
                "In the second stage, subjects used our rule generation tool to create rules that would match the annotations for each nugget.",
                "The tool allows users to enter a Boolean rule as a disjunction of conjunctions (e.g. ((a AND b) OR (a AND c AND d) OR (e))).",
                "Given a candidate rule, our tool uses it as a Boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match.",
                "Hence, the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision.",
                "We observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions (adding more ANDs), and improving the recall by adding more conjunctions to the disjunction (adding more ORs).",
                "As an example, lets try to create a rule for the nugget which says that seven prisoners escaped from the Texas prison.",
                "We start with a simple rule - (seven).",
                "When we input this into the rule generation tool, we realize that this rule matches many spurious occurrences of seven (e.g. ...seven states...) and thus gets a low precision score.",
                "We can further qualify our rule - Texas AND seven AND convicts.",
                "Next, by looking at the missed annotations, we realize that some news articles mentioned ...seven prisoners escaped.... We then replace convicts with the disjunction (convicts OR prisoners).",
                "We continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the (small number of) misses and false alarms can be safely ignored.",
                "Thus we can create nugget-matching rules that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses.",
                "Human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably. 4.2 Evaluating the Utility of a Sequence of Ranked Lists The utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information, and how much the user lost in terms of time and energy.",
                "We calculate this utility from the utilities of individual passages as follows.",
                "After reading each passage returned by the system, the user derives some gain depending on the presence of relevant and novel information, and incurs a loss in terms of the time and energy spent in going through the passage.",
                "However, the likelihood that the user would actually read a passage depends on its position in the ranked list.",
                "Hence, for a query q, the expected utility 2 LDC [18] already provides relevance judgments for 100 topics on the TDT4 corpus.",
                "We further ensured that these judgments are exhaustive on the entire corpus using pooling. of a passage pi at rank i can be defined as U(pi, q) = P(i) â (Gain(pi, q) â Loss(pi, q)) (4) where P(i) is the probability that the user would go through a passage at rank i.",
                "The expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage: U(q) = nX i=1 P(i) â (Gain(pi, q) â Loss(pi, q)) (5) Note that if we ignore the loss term and define P(i) as P(i) â 1/ logb(b + i â 1) (6) then we get the recently popularized metric called Discounted Cumulated Gain (DCG) [9], where Gain(pi, q) is defined as the graded relevance of passage pi.",
                "However, without the loss term, DCG is a purely recall-oriented metric and not suitable for an adaptive filtering setting, where the systems utility depends in part on its ability to limit the number of items shown to the user.",
                "Although P(i) could be defined based on empirical studies of user behavior, for simplicity, we use P(i) exactly as defined in equation 6.",
                "The gain G(pi, q) of passage pi with respect to the query q is a function of - 1) the number of relevant nuggets present in pi, and 2) the novelty of each of these nuggets.",
                "We combine these two factors as follows.",
                "For each nugget Nj, we assign an initial weight wj, and also keep a count nj of the number of times this nugget has been seen by the user in the past.",
                "The gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor Î³.",
                "Thus, G(pi, q) is defined as G(pi, q) = X Nj âC(pi,q) wj â Î³nj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the answer key of query q.",
                "The initial weights wj are all set of be 1.0 in our experiments, but can also be set based on a pyramid approach [11].",
                "The choice of dampening factor Î³ determines the users tolerance for redundancy.",
                "When Î³ = 0, a nugget will only receive credit for its first occurrence i.e. when nj is zero3 .",
                "For 0 < Î³ < 1, a nugget receives smaller credit for each successive occurrence.",
                "When Î³ = 1, no dampening occurs and repeated occurrences of a nugget receive the same credit.",
                "Note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system, since the users are expected to remember what the system showed them in the past.",
                "We define the loss L(pi, q) as a constant cost c (we use 0.1) incurred when reading a system-returned passage.",
                "Thus, our metric can be re-written as U(q) = nX i=1 Gain(pi, q) logb(b + i â 1) â L(n) (8) where L(n) is the loss associated with a ranked list of length n: L(n) = c Â· nX i=1 1 logb(b + i â 1) (9) 3 Note that 00 = 1 Due to the similarity with Discounted Cumulated Gain (DCG), we call our metric Discounted Cumulated Utility (DCU).",
                "The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) â¤ 0 i.e. when the gain is less than or equal to the cost of reading the passage. 5.",
                "DATA TDT4 was the benchmark corpus used in TDT2002 and TDT2003 evaluations.",
                "The corpus consists of over 90, 000 news articles from multiple sources (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) published between October 2000 and January 2001, in the languages of Arabic, English, and Mandarin.",
                "Speech-recognized and machine-translated versions of the non-English articles were provided as well.",
                "LDC [18] has annotated the corpus with 100 topics, that correspond to various news events in this time period.",
                "Out of these, we selected a subset of 12 actionable events, and defined corresponding tasks for them4 .",
                "For each task, we manually defined a profile consisting of an initial set of (5 to 10) queries, a free-text description of the user history, i.e., what the user already knows about the event, and a list of known on-topic and off-topic documents (if available) as training examples.",
                "For each query, we generated answer keys and corresponding nugget matching rules using the procedure described in section 4.1.2, and produced a total of 120 queries, with an average of 7 nuggets per query. 6.",
                "EXPERIMENTS AND RESULTS 6.1 Baselines We used Indri [17], a popular language-model based retrieval engine, as a baseline for comparison with CAFÂ´E.",
                "Indri supports standard search engine functionality, including pseudo-relevance feedback (PRF) [3, 6], and is representative of a typical query-based retrieval system.",
                "Indri does not support any kind of novelty detection.",
                "We compare Indri with PRF turned on and off, against CAFÂ´E with user feedback, novelty detection and antiredundant ranking turned on and off. 6.2 Experimental Setup We divided the TDT4 corpus spanning 4 months into 10 chunks, each defined as a period of 12 consecutive days.",
                "At any given point of time in the distillation process, each system accessed the past data up to the current point, and returned a ranked list of up 50 passages per query.",
                "The 12 tasks defined on the corpus were divided into a training and test set with 6 tasks each.",
                "Each system was allowed to use the training set to tune its parameters for optimizing NDCU (equation 8), including the relevance threshold for both Indri and CAFÂ´E, and the novelty and antiredundancy thresholds for CAFÂ´E.",
                "The NDCU for each system run is calculated automatically.",
                "User feedback was also simulated - relevance judgments for each system-returned passage (as determined by the nugget matching rules described in section 4.1.2) were 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figure 1: Performance of Indri across chunks Figure 2: Performance of CAFÂ´E across chunks used as user feedback in the adaptation of query profiles and user histories. 6.3 Results In Table 1, we show the NDCU scores of the two systems under various settings.",
                "These scores are averaged over the six tasks in the test set, and are calculated with two dampening factors (see section 4.2): Î³ = 0 and 0.1, to simulate no tolerance and small tolerance for redundancy, respectively.",
                "Using Î³ = 0 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information.",
                "Hence, the improvement obtained from enabling user feedback is smaller with Î³ = 0 than the improvement obtained from feedback with Î³ = 0.1.",
                "This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information.",
                "However, the user does not benefit from seeing such redundant passages, and is usually interested in other passages containing related information.",
                "It is informative to evaluate retrieval systems using our utility measure (with Î³ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.",
                "Sometimes, however, users might indeed be interested in seeing the same information from multiple sources, as an Table 1: NDCU Scores of Indri and CAFÂ´E for two dampening factors (Î³), and various settings (F: Feedback, N: Novelty Detection, A: Anti-Redundant Ranking) Indri CAFÂ´E Î³ Base +PRF Base +F +F+N +F+A +F+N+A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.35 0.36 0.36 indicator of its importance or reliability.",
                "In such a case, we can simply choose a higher value for Î³ which corresponds to a higher tolerance for redundancy, and hence let the system tune its parameters accordingly.",
                "Since documents were processed chunk by chunk, it would be interesting to see how the performance of systems improves over time.",
                "Figures 1 and 2 show the performance trends for both the systems across chunks.",
                "While the performance with and without feedback on the first few chunks is expected to be close, for subsequent chunks, the performance curve with feedback enabled rises above the one with the no-feedback setting.",
                "The performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks, making some queries easier than others in certain chunks.",
                "Moreover, since Indri uses pseudo-relevance feedback while CAFÂ´E uses feedback based on actual relevance judgments, the improvement in case of Indri is less dramatic than that of CAFÂ´E. 7.",
                "CONCLUDING REMARKS This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages.",
                "Our system, called CAFÂ´E, combines adaptive filtering, novelty detection and antiredundant passage ranking in a unified framework for utility optimization.",
                "We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty.",
                "Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off. 8.",
                "ACKNOWLEDGMENTS We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments.",
                "This work is supported in parts by the National Science Foundation (NSF) under grant IIS0434035, and the Defense Advanced Research Project Agency (DARPA) under contracts NBCHD030010 and W0550432.",
                "Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. 9.",
                "ADDITIONAL AUTHORS Jian Zhang (jianzhan@stat.purdue.edu)â , Jaime Carbonell (jgc@cs.cmu.edu)â  , Peter Brusilovsky (peterb+@pitt.edu)â¡ , Daqing He(dah44@pitt.edu)â¡ 10.",
                "REFERENCES [1] J. Allan.",
                "Incremental Relevance Feedback for Information Filtering.",
                "Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 270-278, 1996. [2] J. Allan, C. Wade, and A. Bolivar.",
                "Retrieval and Novelty Detection at the Sentence Level.",
                "Proceedings of the ACM SIGIR conference on research and development in information retrieval, 2003. [3] C. Buckley, G. Salton, and J. Allan.",
                "Automatic Retrieval with Locality Information using SMART.",
                "NIST special publication, (500207):59-72, 1993. [4] J. Callan.",
                "Learning While Filtering Documents.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 224-231, 1998. [5] J. Carbonell and J. Goldstein.",
                "The use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335-336, 1998. [6] E. Efthimiadis.",
                "Query Expansion.",
                "Annual Review of Information Science and Technology (ARIST), 31:p121-87, 1996. [7] J. Fiscus and G. Duddington.",
                "Topic Detection and Tracking Overview.",
                "Topic Detection and Tracking: Event-based Information Organization, pages 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.",
                "A Statistical Model for Multilingual Entity Detection and Tracking.",
                "NAACL/HLT, 2004. [9] K. JÂ¨arvelin and J. KekÂ¨alÂ¨ainen.",
                "Cumulated Gain-based Evaluation of IR Techniques.",
                "ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin and D. Demner-Fushman.",
                "Automatically Evaluating Answers to Definition Questions.",
                "Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), 2005. â Statistics Dept., Purdue University, West Lafayette, USA â  Language Technologies Inst., Carnegie Mellon University, Pittsburgh, USA â¡ School of Information Sciences, Univ. of Pittsburgh, Pittsburgh, USA [11] J. Lin and D. Demner-Fushman.",
                "Will Pyramids Built of nUggets Topple Over.",
                "Proceedings of HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.",
                "A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree.",
                "Proc. of ACL, 4:136-143, 2004. [13] G. Marton.",
                "Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments.",
                "HLT/NAACL, 2006. [14] E. Riloff.",
                "Automatically Constructing a Dictionary for Information Extraction Tasks.",
                "Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, 1993. [15] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9: Filtering track.",
                "The Ninth Text REtrieval Conference (TREC-9), pages 361-368. [16] R. Schapire, Y.",
                "Singer, and A. Singhal.",
                "Boosting and Rocchio Applied to Text Filtering.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle, and W. Croft.",
                "Indri: A Language Model-based Search Engine for Complex Queries.",
                "Proceedings of the International Conference on Intelligence Analysis, 2004. [18] The Linguistic Data Consortium. http://www.ldc.upenn.edu/. [19] E. Voorhees.",
                "Overview of the TREC 2003 Question Answering Track.",
                "Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), 2003. [20] Y. Yang and B. Kisiel.",
                "Margin-based Local Regression for Adaptive Filtering.",
                "Proceedings of the twelfth international conference on Information and knowledge management, pages 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of Adaptive Filtering Methods in a Cross-benchmark Evaluation.",
                "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 98-105, 2005. [22] C. Zhai, W. Cohen, and J. Lafferty.",
                "Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-17, 2003. [23] J. Zhang and Y. Yang.",
                "Robustness of Regularized Linear Classification Methods in Text Categorization.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 190-197, 2003. [24] Y. Zhang.",
                "Using Bayesian Priors to Combine Classifiers for Adaptive Filtering.",
                "Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 345-352, 2004. [25] Y. Zhang, J. Callan, and T. Minka.",
                "Novelty and Redundancy Detection in Adaptive Filtering.",
                "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "unify framework": {
            "translated_key": "marco unificado",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Utility-based Information Distillation Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu ABSTRACT This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework.",
                "It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant passage ranking with respect to long-lasting information needs (tasks with multiple queries).",
                "Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings.",
                "For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task.",
                "Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty.",
                "Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval.",
                "Adaptive filtering (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics.",
                "Based on the initial query and a few positive examples (if available), an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user.",
                "The incremental learning nature of AF systems makes them more powerful than standard search engines that support ad-hoc retrieval (e.g.",
                "Google and Yahoo) in terms of finding relevant information with respect to long-lasting topics of interest, and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs, without having to modify their queries manually.",
                "A variety of supervised learning algorithms (Rocchio-style classifiers, Exponential-Gaussian models, local regression and logistic regression approaches) have been studied for adaptive settings, examined with explicit and implicit relevance feedback, and evaluated with respect to utility optimization on large benchmark data collections in TREC (Text Retrieval Conferences) and TDT (Topic Detection and Tracking) forums [1, 4, 7, 15, 16, 20, 24, 23].",
                "Regularized logistic regression [21] has been found representative for the state-of-the-art approaches, and highly efficient for frequent model adaptations over large document collections such as the TREC-10 corpus (over 800,000 documents and 84 topics).",
                "Despite substantial achievements in recent adaptive filtering research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently.",
                "Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications: 1.",
                "User has a rather passive role in the conventional adaptive filtering setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision.",
                "A more active alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents (or passages) per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists.",
                "The latter form of user interaction has been highly effective in standard retrieval for ad-hoc queries.",
                "How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2.",
                "The unit for receiving a relevance judgment (yes or no) is restricted to the document level in conventional AF.",
                "However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant.",
                "Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system.",
                "For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3.",
                "System-selected documents are often highly redundant.",
                "A major news event, for example, would be reported by multiple sources repeatedly for a while, making most of the information content in those articles redundant with each other.",
                "A conventional AF system would select all these redundant news stories for user feedback, wasting the users time while offering little gain.",
                "Clearly, techniques for novelty detection can help in principle [25, 2, 22] for improving the utility of the AF systems.",
                "However, the effectiveness of such techniques at passage level to detect novelty with respect to users (fine-grained) feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user.",
                "To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and novelty detection for removal of redundancy from user interactions with the system.",
                "We call the new process utility-based information distillation.",
                "Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach.",
                "Therefore, we extended a benchmark corpus - the TDT4 collection of news stories and TV broadcasts - with task definitions, multiple queries per task, and answer keys per query.",
                "We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 .",
                "To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses.",
                "Moreover, we propose an extension of NDCG (Normalized Discounted Cumulated Gain) [9] for assessing the utility of ranked passages as a function of both relevance and novelty.",
                "The rest of this paper is organized as follows.",
                "Section 2 outlines the information distillation process with a concrete example.",
                "Section 3 describes the technical cores of our system called CAFÂ´E - CMU Adaptive Filtering Engine.",
                "Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme.",
                "Section 5 describes the extended TDT4 corpus.",
                "Section 6 presents our experiments and results.",
                "Section 7 concludes the study and gives future perspectives. 2.",
                "A SAMPLE TASK Consider a news event - the escape of seven convicts from a Texas prison in December 2000 and their capture a month later.",
                "Assuming a user were interested in this event since its early stage, the information need could be: Find information about the escape of convicts from Texas prison, and information related to their recapture.",
                "The associated lower-level questions could be: 1.",
                "How many prisoners escaped? 2.",
                "Where and when were they sighted? 3. Who are their known contacts inside and outside the prison? 4.",
                "How are they armed? 5.",
                "Do they have any vehicles? 6.",
                "What steps have been taken so far?",
                "We call such an information need a task, and the associated questions as the queries in this task.",
                "A distillation system is supposed to monitor the incoming documents, process them chunk by chunk in a temporal order, select potentially relevant and novel passages from each chunk with respect to each query, and present a ranked list of passages to the user.",
                "Passage ranking here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list.",
                "When presented with a list of passages, the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant.",
                "These spans of text are taken as positive examples in the adaptation of the query profile, and also added to the users history.",
                "Passages not marked by the user are taken as negative examples.",
                "As soon as the query profile is updated, the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low, based on user preference.",
                "For example, if the user highlights ...officials have posted a $100,000 reward for their capture... as relevant answer to the query What steps have been taken so far?, then the highlighted piece is used as an additional positive training example in the adaptation of the query profile.",
                "This piece of feedback is also added to the user history as a seen example, so that in future, the system will not place another passage mentioning $100,000 reward at the top of the ranked list.",
                "However, an article mentioning ...officials have doubled the reward money to $200,000... might be ranked high since it is both relevant to the (updated) query profile and novel with respect to the (updated) user history.",
                "The user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly.",
                "Clearly, novelty detection is very important for the utility of such a system because of the iterative search.",
                "Without novelty detection, the old relevant passages would be shown to the user repeatedly in each ranked list.",
                "Through the above example, we can see the main properties of our new framework for utility-based information distillation over temporally ordered documents.",
                "Our framework combines and extends the power of adaptive filtering (AF), ad-hoc retrieval (IR) and novelty detection (ND).",
                "Compared to standard IR, our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task.",
                "Compared to conventional AF, it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system.",
                "Compared to past work, this is the first evaluation of novelty detection integrated with adaptive filtering for sequenced queries that allows flexible user feedback over ranked passages.",
                "The combination of AF, IR and ND with the new extensions raises an important research question regarding evaluation methodology: how can we measure the utility of such an information distillation system?",
                "Existing metrics in standard IR, AF and ND are insufficient, and new solutions must be explored, as we will discuss in Section 4, after describing the technical cores of our system in the next section. 3.",
                "TECHNICAL CORES The core components of CAFÂ´E are - 1) AF for incremental learning of query profiles, 2) IR for estimating relevance of passages with respect to query profiles, 3) ND for assessing novelty of passages with respect to users history, and 4) anti-redundancy component to remove redundancy from ranked lists. 3.1 Adaptive Filtering Component We use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for AF [21].",
                "Logistic regression (LR) is a supervised learning algorithm for statistical classification.",
                "Based on a training set of labeled instances, it learns a class model which can then by used to predict the labels of unseen instances.",
                "Its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required, as is the case in adaptive filtering, where the system must learn from each new feedback provided by the user. (See [21] and [23] for computational complexity and implementation issues).",
                "In adaptive filtering, each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query.",
                "For training the model, we use the query itself as the initial positive training example of the class, and the user-highlighted pieces of text (marked as Relevant or Not-relevant) during feedback as additional training examples.",
                "To address the cold start issue in the early stage before any user feedback is obtained, the system uses a small sample from a retrospective corpus as the initial negative examples in the training set.",
                "The details of using logistic regression for adaptive filtering (assigning different weights to positive and negative training instances, and regularizing the objective function to prevent over-fitting on training data) are presented in [21].",
                "The class model wâ learned by Logistic Regression, or the query profile, is a vector whose dimensions are individual terms and whose elements are the regression coefficients, indicating how influential each term is in the query profile.",
                "The query profile is updated whenever a new piece of user feedback is received.",
                "A temporally decaying weight can be applied to each training example, as an option, to emphasize the most recent user feedback. 3.2 Passage Retrieval Component We use standard IR techniques in this part of our system.",
                "Incoming documents are processed in chunks, where each chunk can be defined as a fixed span of time or as a fixed number of documents, as preferred by the user.",
                "For each incoming document, corpus statistics like the IDF (Inverted Document Frequency) of each term are updated.",
                "We use a state-of-the-art named entity identifier and tracker [8, 12] to identify person and location names, and merge them with co-referent named entities seen in the past.",
                "Then the documents are segmented into passages, which can be a whole document, a paragraph, a sentence, or any other continuous span of text, as preferred.",
                "Each passage is represented using a vector of TF-IDF (Term FrequencyInverse Document Frequency) weights, where term can be a word or a named entity.",
                "Given a query profile, i.e. the logistic regression solution wâ as described in Section 3.1, the system computes the posterior probability of relevance for each passage x as fRL(x) â¡ P(y = 1|x, wâ ) = 1 (1 + eâwâÂ·x) (1) Passages are ordered by their relevance scores, and the ones with scores above a threshold (tuned on a training set) comprise the relevance list that is passed on to the novelty detection step. 3.3 Novelty Detection Component CAFÂ´E maintains a user history H(t), which contains all the spans of text hi that the user highlighted (as feedback) during his or her past interactions with the system, up to the current time t. Denoting the history as H(t) = n h1, h2, ..., ht o , (2) the novelty score of a new candidate passage x is computed as: fND(x) = 1 â max iâ1..t {cos(x, hi)} (3) where both candidate passage x and highlighted spans of text hi are represented as TF-IDF vectors.",
                "The novelty score of each passage is compared to a prespecified threshold (also tuned on a training set), and any passage with a score below this threshold is removed from the relevance list. 3.4 Anti-redundant Ranking Component Although the novelty detection component ensures that only novel (previously unseen) information remains in the relevance list, this list might still contain the same novel information at multiple positions in the ranked list.",
                "Suppose, for example, that the user has already read about a $100,000 reward for information about the escaped convicts.",
                "A new piece of news that the award has been increased to $200,000 is novel since the user hasnt read about it yet.",
                "However, multiple news sources would report this news and we might end up showing (redundant) articles from all these sources in a ranked list.",
                "Hence, a ranked list should also be made non-redundant with respect to its own contents.",
                "We use a simplified version of the Maximal Marginal Relevance method [5], originally developed for combining relevance and novelty in text retrieval and summarization.",
                "Our procedure starts with the current list of passages sorted by relevance (section 3.2), filtered by Novelty Detection component (section 3.3), and generates a new non-redundant list as follows: 1.",
                "Take the top passage in the current list as the top one in the new list. 2.",
                "Add the next passage x in the current list to the new list only if fAR(x) > t where fAR(x) = 1 â max piâLnew {cos(x, pi)} and Lnew is the set of passages already selected in the new list. 3.",
                "Repeat step 2 until all the passages in the current list have been examined.",
                "After applying the above-mentioned algorithm, each passage in the new list is sufficiently dissimilar to others, thus favoring diversity rather than redundancy in the new ranked list.",
                "The anti-redundancy threshold t is tuned on a training set. 4.",
                "EVALUATION METHODOLOGY The approach we proposed above for information distillation raises important issues regarding evaluation methodology.",
                "Firstly, since our framework allows the output to be passages at different levels of granularity (e.g. k-sentence windows where k may vary) instead of a fixed length, it is not possible to have pre-annotated relevance judgments at all such granularity levels.",
                "Secondly, since we wish to measure the utility of the system output as a combination of both relevance and novelty, traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time.",
                "Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists.",
                "None of the existing measures in ad-hoc retrieval, adaptive filtering, novelty detection or other related areas (text summarization and question answering) have desirable properties in all the three aspects.",
                "Therefore, we must develop a new evaluation methodology. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers.",
                "Answer keys define what should be present in a system response to receive credit, and are comprised of a collection of information nuggets, i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them.",
                "Defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping [19], since system-returned passages can contain the same information expressed in many different ways.",
                "Hence, QA evaluations have relied on human assessors for the mapping between various expressions, making the process costly, time consuming, and not scalable to large query and document collections, and extensive system evaluations with various parameter settings. 4.1.1 Automating Evaluation based on Answer Keys Automatic evaluation methods would allow for faster system building and tuning, as well as provide an objective and affordable way of comparing various systems.",
                "Recently, such methods have been proposed, more or less, based on the idea of n-gram co-occurrences.",
                "Pourpre [10] assigns a fractional recall score to a system response based on its unigram overlap with a given nuggets description.",
                "For example, a system response A B C has recall 3/4 with respect to a nugget with description A B C D. However, such an approach is unfair to systems that present the same information but using words other than A, B, C, and D. Another open issue is how to weight individual words in measuring the closeness of a match.",
                "For example, consider the question How many prisoners escaped?.",
                "In the nugget Seven prisoners escaped from a Texas prison, there is no indication that seven is the keyword, and that it must be matched to get any relevance credit.",
                "Using IDF values does not help, since seven will generally not have a higher IDF than words like texas and prison.",
                "Also, redefining the nugget as just seven does not solve the problem since now it might spuriously match any mention of seven out of context.",
                "Nuggeteer [13] works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold.",
                "However, it is also plagued by spurious relevance since not all words contained in the nugget description (or known correct responses) are central to the nugget. 4.1.2 Nugget-Matching Rules We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on nugget-matching rules, which are generated using a semiautomatic procedure explained below.",
                "These rules are essentially Boolean queries that will only match against snippets that contain the nugget.",
                "For instance, a candidate rule for matching answers to How many prisoners escaped? is (Texas AND seven AND escape AND (convicts OR prisoners)), possibly with other synonyms and variants in the rule.",
                "For a corpus of news articles, which usually follow a typical formal prose, it is fairly easy to write such simple rules to match expected answers using a bootstrap approach, as described below.",
                "We propose a two-stage approach, inspired by Autoslog [14], that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents.",
                "In the first stage, human subjects annotated (using a highlighting tool) portions of ontopic documents that contained answers to each nugget 2 .",
                "In the second stage, subjects used our rule generation tool to create rules that would match the annotations for each nugget.",
                "The tool allows users to enter a Boolean rule as a disjunction of conjunctions (e.g. ((a AND b) OR (a AND c AND d) OR (e))).",
                "Given a candidate rule, our tool uses it as a Boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match.",
                "Hence, the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision.",
                "We observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions (adding more ANDs), and improving the recall by adding more conjunctions to the disjunction (adding more ORs).",
                "As an example, lets try to create a rule for the nugget which says that seven prisoners escaped from the Texas prison.",
                "We start with a simple rule - (seven).",
                "When we input this into the rule generation tool, we realize that this rule matches many spurious occurrences of seven (e.g. ...seven states...) and thus gets a low precision score.",
                "We can further qualify our rule - Texas AND seven AND convicts.",
                "Next, by looking at the missed annotations, we realize that some news articles mentioned ...seven prisoners escaped.... We then replace convicts with the disjunction (convicts OR prisoners).",
                "We continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the (small number of) misses and false alarms can be safely ignored.",
                "Thus we can create nugget-matching rules that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses.",
                "Human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably. 4.2 Evaluating the Utility of a Sequence of Ranked Lists The utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information, and how much the user lost in terms of time and energy.",
                "We calculate this utility from the utilities of individual passages as follows.",
                "After reading each passage returned by the system, the user derives some gain depending on the presence of relevant and novel information, and incurs a loss in terms of the time and energy spent in going through the passage.",
                "However, the likelihood that the user would actually read a passage depends on its position in the ranked list.",
                "Hence, for a query q, the expected utility 2 LDC [18] already provides relevance judgments for 100 topics on the TDT4 corpus.",
                "We further ensured that these judgments are exhaustive on the entire corpus using pooling. of a passage pi at rank i can be defined as U(pi, q) = P(i) â (Gain(pi, q) â Loss(pi, q)) (4) where P(i) is the probability that the user would go through a passage at rank i.",
                "The expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage: U(q) = nX i=1 P(i) â (Gain(pi, q) â Loss(pi, q)) (5) Note that if we ignore the loss term and define P(i) as P(i) â 1/ logb(b + i â 1) (6) then we get the recently popularized metric called Discounted Cumulated Gain (DCG) [9], where Gain(pi, q) is defined as the graded relevance of passage pi.",
                "However, without the loss term, DCG is a purely recall-oriented metric and not suitable for an adaptive filtering setting, where the systems utility depends in part on its ability to limit the number of items shown to the user.",
                "Although P(i) could be defined based on empirical studies of user behavior, for simplicity, we use P(i) exactly as defined in equation 6.",
                "The gain G(pi, q) of passage pi with respect to the query q is a function of - 1) the number of relevant nuggets present in pi, and 2) the novelty of each of these nuggets.",
                "We combine these two factors as follows.",
                "For each nugget Nj, we assign an initial weight wj, and also keep a count nj of the number of times this nugget has been seen by the user in the past.",
                "The gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor Î³.",
                "Thus, G(pi, q) is defined as G(pi, q) = X Nj âC(pi,q) wj â Î³nj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the answer key of query q.",
                "The initial weights wj are all set of be 1.0 in our experiments, but can also be set based on a pyramid approach [11].",
                "The choice of dampening factor Î³ determines the users tolerance for redundancy.",
                "When Î³ = 0, a nugget will only receive credit for its first occurrence i.e. when nj is zero3 .",
                "For 0 < Î³ < 1, a nugget receives smaller credit for each successive occurrence.",
                "When Î³ = 1, no dampening occurs and repeated occurrences of a nugget receive the same credit.",
                "Note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system, since the users are expected to remember what the system showed them in the past.",
                "We define the loss L(pi, q) as a constant cost c (we use 0.1) incurred when reading a system-returned passage.",
                "Thus, our metric can be re-written as U(q) = nX i=1 Gain(pi, q) logb(b + i â 1) â L(n) (8) where L(n) is the loss associated with a ranked list of length n: L(n) = c Â· nX i=1 1 logb(b + i â 1) (9) 3 Note that 00 = 1 Due to the similarity with Discounted Cumulated Gain (DCG), we call our metric Discounted Cumulated Utility (DCU).",
                "The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) â¤ 0 i.e. when the gain is less than or equal to the cost of reading the passage. 5.",
                "DATA TDT4 was the benchmark corpus used in TDT2002 and TDT2003 evaluations.",
                "The corpus consists of over 90, 000 news articles from multiple sources (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) published between October 2000 and January 2001, in the languages of Arabic, English, and Mandarin.",
                "Speech-recognized and machine-translated versions of the non-English articles were provided as well.",
                "LDC [18] has annotated the corpus with 100 topics, that correspond to various news events in this time period.",
                "Out of these, we selected a subset of 12 actionable events, and defined corresponding tasks for them4 .",
                "For each task, we manually defined a profile consisting of an initial set of (5 to 10) queries, a free-text description of the user history, i.e., what the user already knows about the event, and a list of known on-topic and off-topic documents (if available) as training examples.",
                "For each query, we generated answer keys and corresponding nugget matching rules using the procedure described in section 4.1.2, and produced a total of 120 queries, with an average of 7 nuggets per query. 6.",
                "EXPERIMENTS AND RESULTS 6.1 Baselines We used Indri [17], a popular language-model based retrieval engine, as a baseline for comparison with CAFÂ´E.",
                "Indri supports standard search engine functionality, including pseudo-relevance feedback (PRF) [3, 6], and is representative of a typical query-based retrieval system.",
                "Indri does not support any kind of novelty detection.",
                "We compare Indri with PRF turned on and off, against CAFÂ´E with user feedback, novelty detection and antiredundant ranking turned on and off. 6.2 Experimental Setup We divided the TDT4 corpus spanning 4 months into 10 chunks, each defined as a period of 12 consecutive days.",
                "At any given point of time in the distillation process, each system accessed the past data up to the current point, and returned a ranked list of up 50 passages per query.",
                "The 12 tasks defined on the corpus were divided into a training and test set with 6 tasks each.",
                "Each system was allowed to use the training set to tune its parameters for optimizing NDCU (equation 8), including the relevance threshold for both Indri and CAFÂ´E, and the novelty and antiredundancy thresholds for CAFÂ´E.",
                "The NDCU for each system run is calculated automatically.",
                "User feedback was also simulated - relevance judgments for each system-returned passage (as determined by the nugget matching rules described in section 4.1.2) were 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figure 1: Performance of Indri across chunks Figure 2: Performance of CAFÂ´E across chunks used as user feedback in the adaptation of query profiles and user histories. 6.3 Results In Table 1, we show the NDCU scores of the two systems under various settings.",
                "These scores are averaged over the six tasks in the test set, and are calculated with two dampening factors (see section 4.2): Î³ = 0 and 0.1, to simulate no tolerance and small tolerance for redundancy, respectively.",
                "Using Î³ = 0 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information.",
                "Hence, the improvement obtained from enabling user feedback is smaller with Î³ = 0 than the improvement obtained from feedback with Î³ = 0.1.",
                "This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information.",
                "However, the user does not benefit from seeing such redundant passages, and is usually interested in other passages containing related information.",
                "It is informative to evaluate retrieval systems using our utility measure (with Î³ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.",
                "Sometimes, however, users might indeed be interested in seeing the same information from multiple sources, as an Table 1: NDCU Scores of Indri and CAFÂ´E for two dampening factors (Î³), and various settings (F: Feedback, N: Novelty Detection, A: Anti-Redundant Ranking) Indri CAFÂ´E Î³ Base +PRF Base +F +F+N +F+A +F+N+A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.35 0.36 0.36 indicator of its importance or reliability.",
                "In such a case, we can simply choose a higher value for Î³ which corresponds to a higher tolerance for redundancy, and hence let the system tune its parameters accordingly.",
                "Since documents were processed chunk by chunk, it would be interesting to see how the performance of systems improves over time.",
                "Figures 1 and 2 show the performance trends for both the systems across chunks.",
                "While the performance with and without feedback on the first few chunks is expected to be close, for subsequent chunks, the performance curve with feedback enabled rises above the one with the no-feedback setting.",
                "The performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks, making some queries easier than others in certain chunks.",
                "Moreover, since Indri uses pseudo-relevance feedback while CAFÂ´E uses feedback based on actual relevance judgments, the improvement in case of Indri is less dramatic than that of CAFÂ´E. 7.",
                "CONCLUDING REMARKS This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages.",
                "Our system, called CAFÂ´E, combines adaptive filtering, novelty detection and antiredundant passage ranking in a unified framework for utility optimization.",
                "We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty.",
                "Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off. 8.",
                "ACKNOWLEDGMENTS We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments.",
                "This work is supported in parts by the National Science Foundation (NSF) under grant IIS0434035, and the Defense Advanced Research Project Agency (DARPA) under contracts NBCHD030010 and W0550432.",
                "Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. 9.",
                "ADDITIONAL AUTHORS Jian Zhang (jianzhan@stat.purdue.edu)â , Jaime Carbonell (jgc@cs.cmu.edu)â  , Peter Brusilovsky (peterb+@pitt.edu)â¡ , Daqing He(dah44@pitt.edu)â¡ 10.",
                "REFERENCES [1] J. Allan.",
                "Incremental Relevance Feedback for Information Filtering.",
                "Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 270-278, 1996. [2] J. Allan, C. Wade, and A. Bolivar.",
                "Retrieval and Novelty Detection at the Sentence Level.",
                "Proceedings of the ACM SIGIR conference on research and development in information retrieval, 2003. [3] C. Buckley, G. Salton, and J. Allan.",
                "Automatic Retrieval with Locality Information using SMART.",
                "NIST special publication, (500207):59-72, 1993. [4] J. Callan.",
                "Learning While Filtering Documents.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 224-231, 1998. [5] J. Carbonell and J. Goldstein.",
                "The use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335-336, 1998. [6] E. Efthimiadis.",
                "Query Expansion.",
                "Annual Review of Information Science and Technology (ARIST), 31:p121-87, 1996. [7] J. Fiscus and G. Duddington.",
                "Topic Detection and Tracking Overview.",
                "Topic Detection and Tracking: Event-based Information Organization, pages 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.",
                "A Statistical Model for Multilingual Entity Detection and Tracking.",
                "NAACL/HLT, 2004. [9] K. JÂ¨arvelin and J. KekÂ¨alÂ¨ainen.",
                "Cumulated Gain-based Evaluation of IR Techniques.",
                "ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin and D. Demner-Fushman.",
                "Automatically Evaluating Answers to Definition Questions.",
                "Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), 2005. â Statistics Dept., Purdue University, West Lafayette, USA â  Language Technologies Inst., Carnegie Mellon University, Pittsburgh, USA â¡ School of Information Sciences, Univ. of Pittsburgh, Pittsburgh, USA [11] J. Lin and D. Demner-Fushman.",
                "Will Pyramids Built of nUggets Topple Over.",
                "Proceedings of HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.",
                "A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree.",
                "Proc. of ACL, 4:136-143, 2004. [13] G. Marton.",
                "Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments.",
                "HLT/NAACL, 2006. [14] E. Riloff.",
                "Automatically Constructing a Dictionary for Information Extraction Tasks.",
                "Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, 1993. [15] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9: Filtering track.",
                "The Ninth Text REtrieval Conference (TREC-9), pages 361-368. [16] R. Schapire, Y.",
                "Singer, and A. Singhal.",
                "Boosting and Rocchio Applied to Text Filtering.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle, and W. Croft.",
                "Indri: A Language Model-based Search Engine for Complex Queries.",
                "Proceedings of the International Conference on Intelligence Analysis, 2004. [18] The Linguistic Data Consortium. http://www.ldc.upenn.edu/. [19] E. Voorhees.",
                "Overview of the TREC 2003 Question Answering Track.",
                "Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), 2003. [20] Y. Yang and B. Kisiel.",
                "Margin-based Local Regression for Adaptive Filtering.",
                "Proceedings of the twelfth international conference on Information and knowledge management, pages 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of Adaptive Filtering Methods in a Cross-benchmark Evaluation.",
                "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 98-105, 2005. [22] C. Zhai, W. Cohen, and J. Lafferty.",
                "Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-17, 2003. [23] J. Zhang and Y. Yang.",
                "Robustness of Regularized Linear Classification Methods in Text Categorization.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 190-197, 2003. [24] Y. Zhang.",
                "Using Bayesian Priors to Combine Classifiers for Adaptive Filtering.",
                "Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 345-352, 2004. [25] Y. Zhang, J. Callan, and T. Minka.",
                "Novelty and Redundancy Detection in Adaptive Filtering.",
                "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "adaptive filter": {
            "translated_key": "filtrado adaptativo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Utility-based Information Distillation Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu ABSTRACT This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework.",
                "It combines the strengths of and extends beyond conventional <br>adaptive filter</br>ing, novelty detection and non-redundant passage ranking with respect to long-lasting information needs (tasks with multiple queries).",
                "Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings.",
                "For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task.",
                "Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty.",
                "Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval.",
                "Adaptive filtering (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics.",
                "Based on the initial query and a few positive examples (if available), an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user.",
                "The incremental learning nature of AF systems makes them more powerful than standard search engines that support ad-hoc retrieval (e.g.",
                "Google and Yahoo) in terms of finding relevant information with respect to long-lasting topics of interest, and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs, without having to modify their queries manually.",
                "A variety of supervised learning algorithms (Rocchio-style classifiers, Exponential-Gaussian models, local regression and logistic regression approaches) have been studied for adaptive settings, examined with explicit and implicit relevance feedback, and evaluated with respect to utility optimization on large benchmark data collections in TREC (Text Retrieval Conferences) and TDT (Topic Detection and Tracking) forums [1, 4, 7, 15, 16, 20, 24, 23].",
                "Regularized logistic regression [21] has been found representative for the state-of-the-art approaches, and highly efficient for frequent model adaptations over large document collections such as the TREC-10 corpus (over 800,000 documents and 84 topics).",
                "Despite substantial achievements in recent <br>adaptive filter</br>ing research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently.",
                "Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications: 1.",
                "User has a rather passive role in the conventional <br>adaptive filter</br>ing setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision.",
                "A more active alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents (or passages) per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists.",
                "The latter form of user interaction has been highly effective in standard retrieval for ad-hoc queries.",
                "How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2.",
                "The unit for receiving a relevance judgment (yes or no) is restricted to the document level in conventional AF.",
                "However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant.",
                "Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system.",
                "For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3.",
                "System-selected documents are often highly redundant.",
                "A major news event, for example, would be reported by multiple sources repeatedly for a while, making most of the information content in those articles redundant with each other.",
                "A conventional AF system would select all these redundant news stories for user feedback, wasting the users time while offering little gain.",
                "Clearly, techniques for novelty detection can help in principle [25, 2, 22] for improving the utility of the AF systems.",
                "However, the effectiveness of such techniques at passage level to detect novelty with respect to users (fine-grained) feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user.",
                "To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and novelty detection for removal of redundancy from user interactions with the system.",
                "We call the new process utility-based information distillation.",
                "Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach.",
                "Therefore, we extended a benchmark corpus - the TDT4 collection of news stories and TV broadcasts - with task definitions, multiple queries per task, and answer keys per query.",
                "We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 .",
                "To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses.",
                "Moreover, we propose an extension of NDCG (Normalized Discounted Cumulated Gain) [9] for assessing the utility of ranked passages as a function of both relevance and novelty.",
                "The rest of this paper is organized as follows.",
                "Section 2 outlines the information distillation process with a concrete example.",
                "Section 3 describes the technical cores of our system called CAFÂ´E - CMU Adaptive Filtering Engine.",
                "Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme.",
                "Section 5 describes the extended TDT4 corpus.",
                "Section 6 presents our experiments and results.",
                "Section 7 concludes the study and gives future perspectives. 2.",
                "A SAMPLE TASK Consider a news event - the escape of seven convicts from a Texas prison in December 2000 and their capture a month later.",
                "Assuming a user were interested in this event since its early stage, the information need could be: Find information about the escape of convicts from Texas prison, and information related to their recapture.",
                "The associated lower-level questions could be: 1.",
                "How many prisoners escaped? 2.",
                "Where and when were they sighted? 3. Who are their known contacts inside and outside the prison? 4.",
                "How are they armed? 5.",
                "Do they have any vehicles? 6.",
                "What steps have been taken so far?",
                "We call such an information need a task, and the associated questions as the queries in this task.",
                "A distillation system is supposed to monitor the incoming documents, process them chunk by chunk in a temporal order, select potentially relevant and novel passages from each chunk with respect to each query, and present a ranked list of passages to the user.",
                "Passage ranking here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list.",
                "When presented with a list of passages, the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant.",
                "These spans of text are taken as positive examples in the adaptation of the query profile, and also added to the users history.",
                "Passages not marked by the user are taken as negative examples.",
                "As soon as the query profile is updated, the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low, based on user preference.",
                "For example, if the user highlights ...officials have posted a $100,000 reward for their capture... as relevant answer to the query What steps have been taken so far?, then the highlighted piece is used as an additional positive training example in the adaptation of the query profile.",
                "This piece of feedback is also added to the user history as a seen example, so that in future, the system will not place another passage mentioning $100,000 reward at the top of the ranked list.",
                "However, an article mentioning ...officials have doubled the reward money to $200,000... might be ranked high since it is both relevant to the (updated) query profile and novel with respect to the (updated) user history.",
                "The user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly.",
                "Clearly, novelty detection is very important for the utility of such a system because of the iterative search.",
                "Without novelty detection, the old relevant passages would be shown to the user repeatedly in each ranked list.",
                "Through the above example, we can see the main properties of our new framework for utility-based information distillation over temporally ordered documents.",
                "Our framework combines and extends the power of <br>adaptive filter</br>ing (AF), ad-hoc retrieval (IR) and novelty detection (ND).",
                "Compared to standard IR, our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task.",
                "Compared to conventional AF, it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system.",
                "Compared to past work, this is the first evaluation of novelty detection integrated with <br>adaptive filter</br>ing for sequenced queries that allows flexible user feedback over ranked passages.",
                "The combination of AF, IR and ND with the new extensions raises an important research question regarding evaluation methodology: how can we measure the utility of such an information distillation system?",
                "Existing metrics in standard IR, AF and ND are insufficient, and new solutions must be explored, as we will discuss in Section 4, after describing the technical cores of our system in the next section. 3.",
                "TECHNICAL CORES The core components of CAFÂ´E are - 1) AF for incremental learning of query profiles, 2) IR for estimating relevance of passages with respect to query profiles, 3) ND for assessing novelty of passages with respect to users history, and 4) anti-redundancy component to remove redundancy from ranked lists. 3.1 Adaptive Filtering Component We use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for AF [21].",
                "Logistic regression (LR) is a supervised learning algorithm for statistical classification.",
                "Based on a training set of labeled instances, it learns a class model which can then by used to predict the labels of unseen instances.",
                "Its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required, as is the case in <br>adaptive filter</br>ing, where the system must learn from each new feedback provided by the user. (See [21] and [23] for computational complexity and implementation issues).",
                "In <br>adaptive filter</br>ing, each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query.",
                "For training the model, we use the query itself as the initial positive training example of the class, and the user-highlighted pieces of text (marked as Relevant or Not-relevant) during feedback as additional training examples.",
                "To address the cold start issue in the early stage before any user feedback is obtained, the system uses a small sample from a retrospective corpus as the initial negative examples in the training set.",
                "The details of using logistic regression for <br>adaptive filter</br>ing (assigning different weights to positive and negative training instances, and regularizing the objective function to prevent over-fitting on training data) are presented in [21].",
                "The class model wâ learned by Logistic Regression, or the query profile, is a vector whose dimensions are individual terms and whose elements are the regression coefficients, indicating how influential each term is in the query profile.",
                "The query profile is updated whenever a new piece of user feedback is received.",
                "A temporally decaying weight can be applied to each training example, as an option, to emphasize the most recent user feedback. 3.2 Passage Retrieval Component We use standard IR techniques in this part of our system.",
                "Incoming documents are processed in chunks, where each chunk can be defined as a fixed span of time or as a fixed number of documents, as preferred by the user.",
                "For each incoming document, corpus statistics like the IDF (Inverted Document Frequency) of each term are updated.",
                "We use a state-of-the-art named entity identifier and tracker [8, 12] to identify person and location names, and merge them with co-referent named entities seen in the past.",
                "Then the documents are segmented into passages, which can be a whole document, a paragraph, a sentence, or any other continuous span of text, as preferred.",
                "Each passage is represented using a vector of TF-IDF (Term FrequencyInverse Document Frequency) weights, where term can be a word or a named entity.",
                "Given a query profile, i.e. the logistic regression solution wâ as described in Section 3.1, the system computes the posterior probability of relevance for each passage x as fRL(x) â¡ P(y = 1|x, wâ ) = 1 (1 + eâwâÂ·x) (1) Passages are ordered by their relevance scores, and the ones with scores above a threshold (tuned on a training set) comprise the relevance list that is passed on to the novelty detection step. 3.3 Novelty Detection Component CAFÂ´E maintains a user history H(t), which contains all the spans of text hi that the user highlighted (as feedback) during his or her past interactions with the system, up to the current time t. Denoting the history as H(t) = n h1, h2, ..., ht o , (2) the novelty score of a new candidate passage x is computed as: fND(x) = 1 â max iâ1..t {cos(x, hi)} (3) where both candidate passage x and highlighted spans of text hi are represented as TF-IDF vectors.",
                "The novelty score of each passage is compared to a prespecified threshold (also tuned on a training set), and any passage with a score below this threshold is removed from the relevance list. 3.4 Anti-redundant Ranking Component Although the novelty detection component ensures that only novel (previously unseen) information remains in the relevance list, this list might still contain the same novel information at multiple positions in the ranked list.",
                "Suppose, for example, that the user has already read about a $100,000 reward for information about the escaped convicts.",
                "A new piece of news that the award has been increased to $200,000 is novel since the user hasnt read about it yet.",
                "However, multiple news sources would report this news and we might end up showing (redundant) articles from all these sources in a ranked list.",
                "Hence, a ranked list should also be made non-redundant with respect to its own contents.",
                "We use a simplified version of the Maximal Marginal Relevance method [5], originally developed for combining relevance and novelty in text retrieval and summarization.",
                "Our procedure starts with the current list of passages sorted by relevance (section 3.2), filtered by Novelty Detection component (section 3.3), and generates a new non-redundant list as follows: 1.",
                "Take the top passage in the current list as the top one in the new list. 2.",
                "Add the next passage x in the current list to the new list only if fAR(x) > t where fAR(x) = 1 â max piâLnew {cos(x, pi)} and Lnew is the set of passages already selected in the new list. 3.",
                "Repeat step 2 until all the passages in the current list have been examined.",
                "After applying the above-mentioned algorithm, each passage in the new list is sufficiently dissimilar to others, thus favoring diversity rather than redundancy in the new ranked list.",
                "The anti-redundancy threshold t is tuned on a training set. 4.",
                "EVALUATION METHODOLOGY The approach we proposed above for information distillation raises important issues regarding evaluation methodology.",
                "Firstly, since our framework allows the output to be passages at different levels of granularity (e.g. k-sentence windows where k may vary) instead of a fixed length, it is not possible to have pre-annotated relevance judgments at all such granularity levels.",
                "Secondly, since we wish to measure the utility of the system output as a combination of both relevance and novelty, traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time.",
                "Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists.",
                "None of the existing measures in ad-hoc retrieval, <br>adaptive filter</br>ing, novelty detection or other related areas (text summarization and question answering) have desirable properties in all the three aspects.",
                "Therefore, we must develop a new evaluation methodology. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers.",
                "Answer keys define what should be present in a system response to receive credit, and are comprised of a collection of information nuggets, i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them.",
                "Defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping [19], since system-returned passages can contain the same information expressed in many different ways.",
                "Hence, QA evaluations have relied on human assessors for the mapping between various expressions, making the process costly, time consuming, and not scalable to large query and document collections, and extensive system evaluations with various parameter settings. 4.1.1 Automating Evaluation based on Answer Keys Automatic evaluation methods would allow for faster system building and tuning, as well as provide an objective and affordable way of comparing various systems.",
                "Recently, such methods have been proposed, more or less, based on the idea of n-gram co-occurrences.",
                "Pourpre [10] assigns a fractional recall score to a system response based on its unigram overlap with a given nuggets description.",
                "For example, a system response A B C has recall 3/4 with respect to a nugget with description A B C D. However, such an approach is unfair to systems that present the same information but using words other than A, B, C, and D. Another open issue is how to weight individual words in measuring the closeness of a match.",
                "For example, consider the question How many prisoners escaped?.",
                "In the nugget Seven prisoners escaped from a Texas prison, there is no indication that seven is the keyword, and that it must be matched to get any relevance credit.",
                "Using IDF values does not help, since seven will generally not have a higher IDF than words like texas and prison.",
                "Also, redefining the nugget as just seven does not solve the problem since now it might spuriously match any mention of seven out of context.",
                "Nuggeteer [13] works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold.",
                "However, it is also plagued by spurious relevance since not all words contained in the nugget description (or known correct responses) are central to the nugget. 4.1.2 Nugget-Matching Rules We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on nugget-matching rules, which are generated using a semiautomatic procedure explained below.",
                "These rules are essentially Boolean queries that will only match against snippets that contain the nugget.",
                "For instance, a candidate rule for matching answers to How many prisoners escaped? is (Texas AND seven AND escape AND (convicts OR prisoners)), possibly with other synonyms and variants in the rule.",
                "For a corpus of news articles, which usually follow a typical formal prose, it is fairly easy to write such simple rules to match expected answers using a bootstrap approach, as described below.",
                "We propose a two-stage approach, inspired by Autoslog [14], that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents.",
                "In the first stage, human subjects annotated (using a highlighting tool) portions of ontopic documents that contained answers to each nugget 2 .",
                "In the second stage, subjects used our rule generation tool to create rules that would match the annotations for each nugget.",
                "The tool allows users to enter a Boolean rule as a disjunction of conjunctions (e.g. ((a AND b) OR (a AND c AND d) OR (e))).",
                "Given a candidate rule, our tool uses it as a Boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match.",
                "Hence, the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision.",
                "We observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions (adding more ANDs), and improving the recall by adding more conjunctions to the disjunction (adding more ORs).",
                "As an example, lets try to create a rule for the nugget which says that seven prisoners escaped from the Texas prison.",
                "We start with a simple rule - (seven).",
                "When we input this into the rule generation tool, we realize that this rule matches many spurious occurrences of seven (e.g. ...seven states...) and thus gets a low precision score.",
                "We can further qualify our rule - Texas AND seven AND convicts.",
                "Next, by looking at the missed annotations, we realize that some news articles mentioned ...seven prisoners escaped.... We then replace convicts with the disjunction (convicts OR prisoners).",
                "We continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the (small number of) misses and false alarms can be safely ignored.",
                "Thus we can create nugget-matching rules that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses.",
                "Human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably. 4.2 Evaluating the Utility of a Sequence of Ranked Lists The utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information, and how much the user lost in terms of time and energy.",
                "We calculate this utility from the utilities of individual passages as follows.",
                "After reading each passage returned by the system, the user derives some gain depending on the presence of relevant and novel information, and incurs a loss in terms of the time and energy spent in going through the passage.",
                "However, the likelihood that the user would actually read a passage depends on its position in the ranked list.",
                "Hence, for a query q, the expected utility 2 LDC [18] already provides relevance judgments for 100 topics on the TDT4 corpus.",
                "We further ensured that these judgments are exhaustive on the entire corpus using pooling. of a passage pi at rank i can be defined as U(pi, q) = P(i) â (Gain(pi, q) â Loss(pi, q)) (4) where P(i) is the probability that the user would go through a passage at rank i.",
                "The expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage: U(q) = nX i=1 P(i) â (Gain(pi, q) â Loss(pi, q)) (5) Note that if we ignore the loss term and define P(i) as P(i) â 1/ logb(b + i â 1) (6) then we get the recently popularized metric called Discounted Cumulated Gain (DCG) [9], where Gain(pi, q) is defined as the graded relevance of passage pi.",
                "However, without the loss term, DCG is a purely recall-oriented metric and not suitable for an <br>adaptive filter</br>ing setting, where the systems utility depends in part on its ability to limit the number of items shown to the user.",
                "Although P(i) could be defined based on empirical studies of user behavior, for simplicity, we use P(i) exactly as defined in equation 6.",
                "The gain G(pi, q) of passage pi with respect to the query q is a function of - 1) the number of relevant nuggets present in pi, and 2) the novelty of each of these nuggets.",
                "We combine these two factors as follows.",
                "For each nugget Nj, we assign an initial weight wj, and also keep a count nj of the number of times this nugget has been seen by the user in the past.",
                "The gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor Î³.",
                "Thus, G(pi, q) is defined as G(pi, q) = X Nj âC(pi,q) wj â Î³nj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the answer key of query q.",
                "The initial weights wj are all set of be 1.0 in our experiments, but can also be set based on a pyramid approach [11].",
                "The choice of dampening factor Î³ determines the users tolerance for redundancy.",
                "When Î³ = 0, a nugget will only receive credit for its first occurrence i.e. when nj is zero3 .",
                "For 0 < Î³ < 1, a nugget receives smaller credit for each successive occurrence.",
                "When Î³ = 1, no dampening occurs and repeated occurrences of a nugget receive the same credit.",
                "Note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system, since the users are expected to remember what the system showed them in the past.",
                "We define the loss L(pi, q) as a constant cost c (we use 0.1) incurred when reading a system-returned passage.",
                "Thus, our metric can be re-written as U(q) = nX i=1 Gain(pi, q) logb(b + i â 1) â L(n) (8) where L(n) is the loss associated with a ranked list of length n: L(n) = c Â· nX i=1 1 logb(b + i â 1) (9) 3 Note that 00 = 1 Due to the similarity with Discounted Cumulated Gain (DCG), we call our metric Discounted Cumulated Utility (DCU).",
                "The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) â¤ 0 i.e. when the gain is less than or equal to the cost of reading the passage. 5.",
                "DATA TDT4 was the benchmark corpus used in TDT2002 and TDT2003 evaluations.",
                "The corpus consists of over 90, 000 news articles from multiple sources (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) published between October 2000 and January 2001, in the languages of Arabic, English, and Mandarin.",
                "Speech-recognized and machine-translated versions of the non-English articles were provided as well.",
                "LDC [18] has annotated the corpus with 100 topics, that correspond to various news events in this time period.",
                "Out of these, we selected a subset of 12 actionable events, and defined corresponding tasks for them4 .",
                "For each task, we manually defined a profile consisting of an initial set of (5 to 10) queries, a free-text description of the user history, i.e., what the user already knows about the event, and a list of known on-topic and off-topic documents (if available) as training examples.",
                "For each query, we generated answer keys and corresponding nugget matching rules using the procedure described in section 4.1.2, and produced a total of 120 queries, with an average of 7 nuggets per query. 6.",
                "EXPERIMENTS AND RESULTS 6.1 Baselines We used Indri [17], a popular language-model based retrieval engine, as a baseline for comparison with CAFÂ´E.",
                "Indri supports standard search engine functionality, including pseudo-relevance feedback (PRF) [3, 6], and is representative of a typical query-based retrieval system.",
                "Indri does not support any kind of novelty detection.",
                "We compare Indri with PRF turned on and off, against CAFÂ´E with user feedback, novelty detection and antiredundant ranking turned on and off. 6.2 Experimental Setup We divided the TDT4 corpus spanning 4 months into 10 chunks, each defined as a period of 12 consecutive days.",
                "At any given point of time in the distillation process, each system accessed the past data up to the current point, and returned a ranked list of up 50 passages per query.",
                "The 12 tasks defined on the corpus were divided into a training and test set with 6 tasks each.",
                "Each system was allowed to use the training set to tune its parameters for optimizing NDCU (equation 8), including the relevance threshold for both Indri and CAFÂ´E, and the novelty and antiredundancy thresholds for CAFÂ´E.",
                "The NDCU for each system run is calculated automatically.",
                "User feedback was also simulated - relevance judgments for each system-returned passage (as determined by the nugget matching rules described in section 4.1.2) were 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figure 1: Performance of Indri across chunks Figure 2: Performance of CAFÂ´E across chunks used as user feedback in the adaptation of query profiles and user histories. 6.3 Results In Table 1, we show the NDCU scores of the two systems under various settings.",
                "These scores are averaged over the six tasks in the test set, and are calculated with two dampening factors (see section 4.2): Î³ = 0 and 0.1, to simulate no tolerance and small tolerance for redundancy, respectively.",
                "Using Î³ = 0 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information.",
                "Hence, the improvement obtained from enabling user feedback is smaller with Î³ = 0 than the improvement obtained from feedback with Î³ = 0.1.",
                "This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information.",
                "However, the user does not benefit from seeing such redundant passages, and is usually interested in other passages containing related information.",
                "It is informative to evaluate retrieval systems using our utility measure (with Î³ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.",
                "Sometimes, however, users might indeed be interested in seeing the same information from multiple sources, as an Table 1: NDCU Scores of Indri and CAFÂ´E for two dampening factors (Î³), and various settings (F: Feedback, N: Novelty Detection, A: Anti-Redundant Ranking) Indri CAFÂ´E Î³ Base +PRF Base +F +F+N +F+A +F+N+A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.35 0.36 0.36 indicator of its importance or reliability.",
                "In such a case, we can simply choose a higher value for Î³ which corresponds to a higher tolerance for redundancy, and hence let the system tune its parameters accordingly.",
                "Since documents were processed chunk by chunk, it would be interesting to see how the performance of systems improves over time.",
                "Figures 1 and 2 show the performance trends for both the systems across chunks.",
                "While the performance with and without feedback on the first few chunks is expected to be close, for subsequent chunks, the performance curve with feedback enabled rises above the one with the no-feedback setting.",
                "The performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks, making some queries easier than others in certain chunks.",
                "Moreover, since Indri uses pseudo-relevance feedback while CAFÂ´E uses feedback based on actual relevance judgments, the improvement in case of Indri is less dramatic than that of CAFÂ´E. 7.",
                "CONCLUDING REMARKS This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages.",
                "Our system, called CAFÂ´E, combines <br>adaptive filter</br>ing, novelty detection and antiredundant passage ranking in a unified framework for utility optimization.",
                "We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty.",
                "Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off. 8.",
                "ACKNOWLEDGMENTS We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments.",
                "This work is supported in parts by the National Science Foundation (NSF) under grant IIS0434035, and the Defense Advanced Research Project Agency (DARPA) under contracts NBCHD030010 and W0550432.",
                "Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. 9.",
                "ADDITIONAL AUTHORS Jian Zhang (jianzhan@stat.purdue.edu)â , Jaime Carbonell (jgc@cs.cmu.edu)â  , Peter Brusilovsky (peterb+@pitt.edu)â¡ , Daqing He(dah44@pitt.edu)â¡ 10.",
                "REFERENCES [1] J. Allan.",
                "Incremental Relevance Feedback for Information Filtering.",
                "Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 270-278, 1996. [2] J. Allan, C. Wade, and A. Bolivar.",
                "Retrieval and Novelty Detection at the Sentence Level.",
                "Proceedings of the ACM SIGIR conference on research and development in information retrieval, 2003. [3] C. Buckley, G. Salton, and J. Allan.",
                "Automatic Retrieval with Locality Information using SMART.",
                "NIST special publication, (500207):59-72, 1993. [4] J. Callan.",
                "Learning While Filtering Documents.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 224-231, 1998. [5] J. Carbonell and J. Goldstein.",
                "The use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335-336, 1998. [6] E. Efthimiadis.",
                "Query Expansion.",
                "Annual Review of Information Science and Technology (ARIST), 31:p121-87, 1996. [7] J. Fiscus and G. Duddington.",
                "Topic Detection and Tracking Overview.",
                "Topic Detection and Tracking: Event-based Information Organization, pages 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.",
                "A Statistical Model for Multilingual Entity Detection and Tracking.",
                "NAACL/HLT, 2004. [9] K. JÂ¨arvelin and J. KekÂ¨alÂ¨ainen.",
                "Cumulated Gain-based Evaluation of IR Techniques.",
                "ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin and D. Demner-Fushman.",
                "Automatically Evaluating Answers to Definition Questions.",
                "Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), 2005. â Statistics Dept., Purdue University, West Lafayette, USA â  Language Technologies Inst., Carnegie Mellon University, Pittsburgh, USA â¡ School of Information Sciences, Univ. of Pittsburgh, Pittsburgh, USA [11] J. Lin and D. Demner-Fushman.",
                "Will Pyramids Built of nUggets Topple Over.",
                "Proceedings of HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.",
                "A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree.",
                "Proc. of ACL, 4:136-143, 2004. [13] G. Marton.",
                "Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments.",
                "HLT/NAACL, 2006. [14] E. Riloff.",
                "Automatically Constructing a Dictionary for Information Extraction Tasks.",
                "Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, 1993. [15] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9: Filtering track.",
                "The Ninth Text REtrieval Conference (TREC-9), pages 361-368. [16] R. Schapire, Y.",
                "Singer, and A. Singhal.",
                "Boosting and Rocchio Applied to Text Filtering.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle, and W. Croft.",
                "Indri: A Language Model-based Search Engine for Complex Queries.",
                "Proceedings of the International Conference on Intelligence Analysis, 2004. [18] The Linguistic Data Consortium. http://www.ldc.upenn.edu/. [19] E. Voorhees.",
                "Overview of the TREC 2003 Question Answering Track.",
                "Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), 2003. [20] Y. Yang and B. Kisiel.",
                "Margin-based Local Regression for Adaptive Filtering.",
                "Proceedings of the twelfth international conference on Information and knowledge management, pages 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of Adaptive Filtering Methods in a Cross-benchmark Evaluation.",
                "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 98-105, 2005. [22] C. Zhai, W. Cohen, and J. Lafferty.",
                "Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-17, 2003. [23] J. Zhang and Y. Yang.",
                "Robustness of Regularized Linear Classification Methods in Text Categorization.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 190-197, 2003. [24] Y. Zhang.",
                "Using Bayesian Priors to Combine Classifiers for Adaptive Filtering.",
                "Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 345-352, 2004. [25] Y. Zhang, J. Callan, and T. Minka.",
                "Novelty and Redundancy Detection in Adaptive Filtering.",
                "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002."
            ],
            "original_annotated_samples": [
                "It combines the strengths of and extends beyond conventional <br>adaptive filter</br>ing, novelty detection and non-redundant passage ranking with respect to long-lasting information needs (tasks with multiple queries).",
                "Despite substantial achievements in recent <br>adaptive filter</br>ing research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently.",
                "User has a rather passive role in the conventional <br>adaptive filter</br>ing setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision.",
                "Our framework combines and extends the power of <br>adaptive filter</br>ing (AF), ad-hoc retrieval (IR) and novelty detection (ND).",
                "Compared to past work, this is the first evaluation of novelty detection integrated with <br>adaptive filter</br>ing for sequenced queries that allows flexible user feedback over ranked passages."
            ],
            "translated_annotated_samples": [
                "Combina las fortalezas y se extiende mÃ¡s allÃ¡ del <br>filtrado adaptativo</br> convencional, la detecciÃ³n de novedades y la clasificaciÃ³n de pasajes no redundantes con respecto a las necesidades de informaciÃ³n duraderas (tareas con mÃºltiples consultas).",
                "A pesar de los logros sustanciales en la investigaciÃ³n reciente de <br>filtrado adaptativo</br>, aÃºn quedan problemas significativos sin resolver sobre cÃ³mo aprovechar de manera efectiva y eficiente la retroalimentaciÃ³n del usuario.",
                "El usuario tiene un papel bastante pasivo en la configuraciÃ³n convencional de <br>filtrado adaptativo</br>: solo reacciona ante el sistema cuando este toma una decisiÃ³n afirmativa sobre un documento, confirmando o rechazando esa decisiÃ³n.",
                "Nuestro marco de trabajo combina y amplÃ­a el poder del <br>filtrado adaptativo</br> (AF), la recuperaciÃ³n ad-hoc (IR) y la detecciÃ³n de novedades (ND).",
                "Comparado con trabajos anteriores, esta es la primera evaluaciÃ³n de detecciÃ³n de novedad integrada con <br>filtrado adaptativo</br> para consultas secuenciadas que permite retroalimentaciÃ³n flexible del usuario sobre pasajes clasificados."
            ],
            "translated_text": "DestilaciÃ³n de informaciÃ³n basada en utilidad sobre documentos secuenciados temporalmente en el Instituto de TecnologÃ­as del Lenguaje de Yiming Yang. Universidad Carnegie Mellon, Pittsburgh, EE. UU. yiming@cs.cmu.edu Abhimanyu Lad Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. alad@cs.cmu.edu Instituto de TecnologÃ­as del Lenguaje Ni Lao. Universidad Carnegie Mellon, Pittsburgh, EE. UU. nlao@cs.cmu.edu Abhay Harpale Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. aharpale@cs.cmu.edu Bryan Kisiel Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. bkisiel@cs.cmu.edu MÃ³nica Rogati Instituto de TecnologÃ­as del Lenguaje. La Universidad Carnegie Mellon en Pittsburgh, EE. UU. mrogati@cs.cmu.edu RESUMEN Este artÃ­culo examina un nuevo enfoque para la destilaciÃ³n de informaciÃ³n en documentos ordenados temporalmente, y propone un esquema de evaluaciÃ³n novedoso para dicho marco. Combina las fortalezas y se extiende mÃ¡s allÃ¡ del <br>filtrado adaptativo</br> convencional, la detecciÃ³n de novedades y la clasificaciÃ³n de pasajes no redundantes con respecto a las necesidades de informaciÃ³n duraderas (tareas con mÃºltiples consultas). Nuestro enfoque respalda la retroalimentaciÃ³n detallada del usuario a travÃ©s del resaltado de fragmentos arbitrarios de texto, y aprovecha dicha informaciÃ³n para la optimizaciÃ³n de la utilidad en entornos adaptativos. Para nuestros experimentos, definimos tareas hipotÃ©ticas basadas en eventos de noticias en el corpus TDT4, con mÃºltiples consultas por tarea. Se generaron claves de respuestas (nuggets) para cada consulta y se utilizÃ³ un procedimiento semiautomÃ¡tico para adquirir reglas que permiten emparejar automÃ¡ticamente los nuggets con las respuestas del sistema. TambiÃ©n proponemos una extensiÃ³n de la mÃ©trica NDCG para evaluar la utilidad de los pasajes clasificados como una combinaciÃ³n de relevancia y novedad. Nuestros resultados muestran mejoras alentadoras en la utilidad utilizando el nuevo enfoque, en comparaciÃ³n con los sistemas base sin aprendizaje incremental o componentes de detecciÃ³n de novedades. CategorÃ­as y Descriptores de Asignaturas H.3.3 [BÃºsqueda y RecuperaciÃ³n de InformaciÃ³n]: Filtrado de informaciÃ³n, RetroalimentaciÃ³n de relevancia, Modelos de recuperaciÃ³n, Proceso de selecciÃ³n; I.5.2 TÃ©rminos Generales DiseÃ±o, MediciÃ³n, Rendimiento, ExperimentaciÃ³n. 1. INTRODUCCIÃN Seguir informaciÃ³n nueva y relevante de flujos de datos temporales para usuarios con necesidades duraderas ha sido un tema de investigaciÃ³n desafiante en la recuperaciÃ³n de informaciÃ³n. El filtrado adaptativo (AF) es una tarea de predicciÃ³n en lÃ­nea de la relevancia de cada nuevo documento con respecto a temas predefinidos. BasÃ¡ndose en la consulta inicial y algunos ejemplos positivos (si estÃ¡n disponibles), un sistema de AF mantiene un perfil para cada tema de interÃ©s, y lo actualiza constantemente segÃºn la retroalimentaciÃ³n del usuario. La naturaleza de aprendizaje incremental de los sistemas de AF los hace mÃ¡s poderosos que los motores de bÃºsqueda estÃ¡ndar que admiten recuperaciÃ³n ad-hoc (por ejemplo. Google y Yahoo) en tÃ©rminos de encontrar informaciÃ³n relevante con respecto a temas de interÃ©s duraderos, y mÃ¡s atractivo para los usuarios que estÃ¡n dispuestos a proporcionar retroalimentaciÃ³n para adaptar el sistema a sus necesidades de informaciÃ³n especÃ­ficas, sin tener que modificar sus consultas manualmente. Se han estudiado una variedad de algoritmos de aprendizaje supervisado (clasificadores de estilo Rocchio, modelos exponenciales-gaussianos, regresiÃ³n local y enfoques de regresiÃ³n logÃ­stica) para entornos adaptativos, examinados con retroalimentaciÃ³n de relevancia explÃ­cita e implÃ­cita, y evaluados con respecto a la optimizaciÃ³n de la utilidad en grandes colecciones de datos de referencia en los foros de TREC (Conferencias de RecuperaciÃ³n de Texto) y TDT (DetecciÃ³n y Seguimiento de Temas) [1, 4, 7, 15, 16, 20, 24, 23]. La regresiÃ³n logÃ­stica regularizada [21] se ha encontrado representativa para los enfoques de vanguardia, y altamente eficiente para adaptaciones frecuentes del modelo en grandes colecciones de documentos como el corpus TREC-10 (mÃ¡s de 800,000 documentos y 84 temas). A pesar de los logros sustanciales en la investigaciÃ³n reciente de <br>filtrado adaptativo</br>, aÃºn quedan problemas significativos sin resolver sobre cÃ³mo aprovechar de manera efectiva y eficiente la retroalimentaciÃ³n del usuario. EspecÃ­ficamente, los siguientes problemas pueden limitar seriamente la verdadera utilidad de los sistemas de AF en aplicaciones del mundo real: 1. El usuario tiene un papel bastante pasivo en la configuraciÃ³n convencional de <br>filtrado adaptativo</br>: solo reacciona ante el sistema cuando este toma una decisiÃ³n afirmativa sobre un documento, confirmando o rechazando esa decisiÃ³n. Una alternativa mÃ¡s activa serÃ­a permitir al usuario emitir mÃºltiples consultas sobre un tema, revisar una lista clasificada de documentos candidatos (o pasajes) por consulta, y proporcionar retroalimentaciÃ³n sobre la lista clasificada, refinando asÃ­ su necesidad de informaciÃ³n y solicitando listas clasificadas actualizadas. La Ãºltima forma de interacciÃ³n del usuario ha sido altamente efectiva en la recuperaciÃ³n estÃ¡ndar para consultas ad-hoc. CÃ³mo implementar una estrategia para necesidades de informaciÃ³n duraderas en entornos de FA es una pregunta abierta para la investigaciÃ³n. 2. La unidad para recibir un juicio de relevancia (sÃ­ o no) estÃ¡ restringida al nivel del documento en AF convencional. Sin embargo, un usuario real puede estar dispuesto a proporcionar comentarios mÃ¡s informativos y detallados resaltando algunas partes de texto en un documento recuperado como relevantes, en lugar de etiquetar todo el documento como relevante. Aprovechar de manera efectiva este tipo de retroalimentaciÃ³n detallada podrÃ­a mejorar sustancialmente la calidad de un sistema de AF. Para esto, necesitamos habilitar el aprendizaje supervisado a partir de fragmentos de texto etiquetados de longitud arbitraria en lugar de solo permitir documentos etiquetados. 3. Los documentos seleccionados por el sistema suelen ser altamente redundantes. Un evento importante de noticias, por ejemplo, serÃ­a reportado por mÃºltiples fuentes repetidamente durante un tiempo, haciendo que la mayor parte del contenido informativo en esos artÃ­culos sea redundante entre sÃ­. Un sistema de AF convencional seleccionarÃ­a todas estas historias de noticias redundantes para la retroalimentaciÃ³n del usuario, desperdiciando el tiempo de los usuarios mientras ofrece poco beneficio. Claramente, las tÃ©cnicas de detecciÃ³n de novedades pueden ayudar en principio [25, 2, 22] a mejorar la utilidad de los sistemas AF. Sin embargo, la efectividad de tales tÃ©cnicas a nivel de pasaje para detectar novedad con respecto a la retroalimentaciÃ³n de los usuarios a nivel detallado y para detectar redundancia en listas clasificadas aÃºn debe ser evaluada utilizando una medida de utilidad que imite las necesidades de un usuario real. Para abordar las limitaciones actuales de los sistemas de AF mencionadas anteriormente, proponemos y examinamos un nuevo enfoque en este artÃ­culo, que combina las fortalezas del AF convencional (aprendizaje incremental de modelos de temas), recuperaciÃ³n de pasajes multipase para consultas de larga duraciÃ³n condicionadas al tema, y detecciÃ³n de novedades para eliminar la redundancia de las interacciones del usuario con el sistema. Llamamos al nuevo proceso destilaciÃ³n de informaciÃ³n basada en utilidad. Ten en cuenta que los corpus de referencia convencionales para evaluaciones de AF, que tienen juicios de relevancia a nivel de documento y no definen tareas con mÃºltiples consultas, son insuficientes para evaluar el nuevo enfoque. Por lo tanto, ampliamos un corpus de referencia, la colecciÃ³n TDT4 de noticias y transmisiones de televisiÃ³n, con definiciones de tareas, mÃºltiples consultas por tarea y claves de respuestas por consulta. Hemos realizado nuestros experimentos en este corpus extendido TDT4 y hemos puesto a disposiciÃ³n del pÃºblico los datos generados adicionalmente para futuras evaluaciones comparativas. Para evaluar automÃ¡ticamente los fragmentos de texto arbitrarios devueltos por el sistema utilizando nuestras claves de respuestas, desarrollamos un esquema de evaluaciÃ³n con un procedimiento semiautomÃ¡tico para 1 URL: http://nyc.lti.cs.cmu.edu/downloads adquiriendo reglas que pueden comparar fragmentos con las respuestas del sistema. AdemÃ¡s, proponemos una extensiÃ³n de NDCG (Normalized Discounted Cumulated Gain) [9] para evaluar la utilidad de los pasajes clasificados como una funciÃ³n tanto de la relevancia como de la novedad. El resto de este documento estÃ¡ organizado de la siguiente manera. La secciÃ³n 2 describe el proceso de destilaciÃ³n de informaciÃ³n con un ejemplo concreto. La secciÃ³n 3 describe los nÃºcleos tÃ©cnicos de nuestro sistema llamado CAFÂ´E - CMU Adaptive Filtering Engine. La secciÃ³n 4 discute problemas con respecto a la metodologÃ­a de evaluaciÃ³n y propone un nuevo esquema. La secciÃ³n 5 describe el corpus extendido TDT4. La secciÃ³n 6 presenta nuestros experimentos y resultados. La secciÃ³n 7 concluye el estudio y ofrece perspectivas futuras. 2. Una TAREA DE EJEMPLO Considera un evento noticioso: la fuga de siete convictos de una prisiÃ³n de Texas en diciembre de 2000 y su captura un mes despuÃ©s. Suponiendo que un usuario estuviera interesado en este evento desde sus primeras etapas, la necesidad de informaciÃ³n podrÃ­a ser: Encontrar informaciÃ³n sobre la fuga de convictos de una prisiÃ³n de Texas, y la informaciÃ³n relacionada con su recaptura. Las preguntas de nivel inferior asociadas podrÃ­an ser: 1. Â¿CuÃ¡ntos prisioneros escaparon? 2. Â¿DÃ³nde y cuÃ¡ndo fueron avistados? 3. Â¿QuiÃ©nes son sus contactos conocidos dentro y fuera de la prisiÃ³n? 4. Â¿CÃ³mo estÃ¡n armados? 5. Â¿Tienen algÃºn vehÃ­culo? 6. Â¿QuÃ© pasos se han tomado hasta ahora? Llamamos a esta necesidad de informaciÃ³n una tarea, y a las preguntas asociadas como las consultas en esta tarea. Un sistema de destilaciÃ³n debe monitorear los documentos entrantes, procesarlos por fragmentos en un orden temporal, seleccionar pasajes potencialmente relevantes y novedosos de cada fragmento con respecto a cada consulta, y presentar una lista clasificada de pasajes al usuario. La clasificaciÃ³n de pasajes aquÃ­ se basa en cuÃ¡n relevante es un pasaje con respecto a la consulta actual, cuÃ¡n novedoso es con respecto al historial actual del usuario (de sus interacciones con el sistema) y cuÃ¡n redundante es en comparaciÃ³n con otros pasajes con un rango mÃ¡s alto en la lista. Cuando se le presenta una lista de pasajes, el usuario puede proporcionar retroalimentaciÃ³n resaltando tramos de texto arbitrarios que considere relevantes. Estos fragmentos de texto se toman como ejemplos positivos en la adaptaciÃ³n del perfil de consulta, y tambiÃ©n se aÃ±aden al historial de los usuarios. Los pasajes no marcados por el usuario se consideran ejemplos negativos. Tan pronto como se actualiza el perfil de consulta, el sistema vuelve a realizar una bÃºsqueda y devuelve otra lista clasificada de pasajes donde los pasajes previamente vistos son eliminados o clasificados como bajos, segÃºn la preferencia del usuario. Por ejemplo, si el usuario resalta ...los funcionarios han ofrecido una recompensa de $100,000 por su captura... como respuesta relevante a la consulta Â¿QuÃ© pasos se han tomado hasta ahora?, entonces la pieza resaltada se utiliza como un ejemplo de entrenamiento positivo adicional en la adaptaciÃ³n del perfil de la consulta. Este comentario tambiÃ©n se aÃ±ade al historial del usuario como un ejemplo visto, para que en el futuro, el sistema no coloque otro pasaje mencionando una recompensa de $100,000 en la parte superior de la lista clasificada. Sin embargo, un artÃ­culo que mencione que los funcionarios han duplicado el dinero de recompensa a $200,000 podrÃ­a clasificarse alto ya que es relevante tanto para el perfil de consulta (actualizado) como novedoso con respecto a la historia del usuario (actualizada). El usuario puede modificar las consultas originales o agregar una nueva consulta durante el proceso; los perfiles de consulta se cambiarÃ¡n en consecuencia. Claramente, la detecciÃ³n de novedades es muy importante para la utilidad de dicho sistema debido a la bÃºsqueda iterativa. Sin detecciÃ³n de novedad, los antiguos pasajes relevantes se mostrarÃ­an al usuario repetidamente en cada lista clasificada. A travÃ©s del ejemplo anterior, podemos ver las principales propiedades de nuestro nuevo marco de trabajo para la destilaciÃ³n de informaciÃ³n basada en utilidad sobre documentos ordenados temporalmente. Nuestro marco de trabajo combina y amplÃ­a el poder del <br>filtrado adaptativo</br> (AF), la recuperaciÃ³n ad-hoc (IR) y la detecciÃ³n de novedades (ND). En comparaciÃ³n con la IR estÃ¡ndar, nuestro enfoque tiene la capacidad de aprender de forma incremental las necesidades de informaciÃ³n a largo plazo y modelar una secuencia de consultas dentro de una tarea. En comparaciÃ³n con el AF convencional, permite un papel mÃ¡s activo del usuario en refinar sus necesidades de informaciÃ³n y solicitar nuevos resultados al permitir comentarios sobre relevancia y novedad mediante el resaltado de fragmentos arbitrarios de texto en los pasajes devueltos por el sistema. Comparado con trabajos anteriores, esta es la primera evaluaciÃ³n de detecciÃ³n de novedad integrada con <br>filtrado adaptativo</br> para consultas secuenciadas que permite retroalimentaciÃ³n flexible del usuario sobre pasajes clasificados. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "passage rank": {
            "translated_key": "clasificaciÃ³n de pasajes",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Utility-based Information Distillation Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu ABSTRACT This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework.",
                "It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant <br>passage rank</br>ing with respect to long-lasting information needs (tasks with multiple queries).",
                "Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings.",
                "For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task.",
                "Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty.",
                "Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval.",
                "Adaptive filtering (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics.",
                "Based on the initial query and a few positive examples (if available), an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user.",
                "The incremental learning nature of AF systems makes them more powerful than standard search engines that support ad-hoc retrieval (e.g.",
                "Google and Yahoo) in terms of finding relevant information with respect to long-lasting topics of interest, and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs, without having to modify their queries manually.",
                "A variety of supervised learning algorithms (Rocchio-style classifiers, Exponential-Gaussian models, local regression and logistic regression approaches) have been studied for adaptive settings, examined with explicit and implicit relevance feedback, and evaluated with respect to utility optimization on large benchmark data collections in TREC (Text Retrieval Conferences) and TDT (Topic Detection and Tracking) forums [1, 4, 7, 15, 16, 20, 24, 23].",
                "Regularized logistic regression [21] has been found representative for the state-of-the-art approaches, and highly efficient for frequent model adaptations over large document collections such as the TREC-10 corpus (over 800,000 documents and 84 topics).",
                "Despite substantial achievements in recent adaptive filtering research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently.",
                "Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications: 1.",
                "User has a rather passive role in the conventional adaptive filtering setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision.",
                "A more active alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents (or passages) per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists.",
                "The latter form of user interaction has been highly effective in standard retrieval for ad-hoc queries.",
                "How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2.",
                "The unit for receiving a relevance judgment (yes or no) is restricted to the document level in conventional AF.",
                "However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant.",
                "Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system.",
                "For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3.",
                "System-selected documents are often highly redundant.",
                "A major news event, for example, would be reported by multiple sources repeatedly for a while, making most of the information content in those articles redundant with each other.",
                "A conventional AF system would select all these redundant news stories for user feedback, wasting the users time while offering little gain.",
                "Clearly, techniques for novelty detection can help in principle [25, 2, 22] for improving the utility of the AF systems.",
                "However, the effectiveness of such techniques at passage level to detect novelty with respect to users (fine-grained) feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user.",
                "To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and novelty detection for removal of redundancy from user interactions with the system.",
                "We call the new process utility-based information distillation.",
                "Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach.",
                "Therefore, we extended a benchmark corpus - the TDT4 collection of news stories and TV broadcasts - with task definitions, multiple queries per task, and answer keys per query.",
                "We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 .",
                "To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses.",
                "Moreover, we propose an extension of NDCG (Normalized Discounted Cumulated Gain) [9] for assessing the utility of ranked passages as a function of both relevance and novelty.",
                "The rest of this paper is organized as follows.",
                "Section 2 outlines the information distillation process with a concrete example.",
                "Section 3 describes the technical cores of our system called CAFÂ´E - CMU Adaptive Filtering Engine.",
                "Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme.",
                "Section 5 describes the extended TDT4 corpus.",
                "Section 6 presents our experiments and results.",
                "Section 7 concludes the study and gives future perspectives. 2.",
                "A SAMPLE TASK Consider a news event - the escape of seven convicts from a Texas prison in December 2000 and their capture a month later.",
                "Assuming a user were interested in this event since its early stage, the information need could be: Find information about the escape of convicts from Texas prison, and information related to their recapture.",
                "The associated lower-level questions could be: 1.",
                "How many prisoners escaped? 2.",
                "Where and when were they sighted? 3. Who are their known contacts inside and outside the prison? 4.",
                "How are they armed? 5.",
                "Do they have any vehicles? 6.",
                "What steps have been taken so far?",
                "We call such an information need a task, and the associated questions as the queries in this task.",
                "A distillation system is supposed to monitor the incoming documents, process them chunk by chunk in a temporal order, select potentially relevant and novel passages from each chunk with respect to each query, and present a ranked list of passages to the user.",
                "Passage ranking here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list.",
                "When presented with a list of passages, the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant.",
                "These spans of text are taken as positive examples in the adaptation of the query profile, and also added to the users history.",
                "Passages not marked by the user are taken as negative examples.",
                "As soon as the query profile is updated, the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low, based on user preference.",
                "For example, if the user highlights ...officials have posted a $100,000 reward for their capture... as relevant answer to the query What steps have been taken so far?, then the highlighted piece is used as an additional positive training example in the adaptation of the query profile.",
                "This piece of feedback is also added to the user history as a seen example, so that in future, the system will not place another passage mentioning $100,000 reward at the top of the ranked list.",
                "However, an article mentioning ...officials have doubled the reward money to $200,000... might be ranked high since it is both relevant to the (updated) query profile and novel with respect to the (updated) user history.",
                "The user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly.",
                "Clearly, novelty detection is very important for the utility of such a system because of the iterative search.",
                "Without novelty detection, the old relevant passages would be shown to the user repeatedly in each ranked list.",
                "Through the above example, we can see the main properties of our new framework for utility-based information distillation over temporally ordered documents.",
                "Our framework combines and extends the power of adaptive filtering (AF), ad-hoc retrieval (IR) and novelty detection (ND).",
                "Compared to standard IR, our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task.",
                "Compared to conventional AF, it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system.",
                "Compared to past work, this is the first evaluation of novelty detection integrated with adaptive filtering for sequenced queries that allows flexible user feedback over ranked passages.",
                "The combination of AF, IR and ND with the new extensions raises an important research question regarding evaluation methodology: how can we measure the utility of such an information distillation system?",
                "Existing metrics in standard IR, AF and ND are insufficient, and new solutions must be explored, as we will discuss in Section 4, after describing the technical cores of our system in the next section. 3.",
                "TECHNICAL CORES The core components of CAFÂ´E are - 1) AF for incremental learning of query profiles, 2) IR for estimating relevance of passages with respect to query profiles, 3) ND for assessing novelty of passages with respect to users history, and 4) anti-redundancy component to remove redundancy from ranked lists. 3.1 Adaptive Filtering Component We use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for AF [21].",
                "Logistic regression (LR) is a supervised learning algorithm for statistical classification.",
                "Based on a training set of labeled instances, it learns a class model which can then by used to predict the labels of unseen instances.",
                "Its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required, as is the case in adaptive filtering, where the system must learn from each new feedback provided by the user. (See [21] and [23] for computational complexity and implementation issues).",
                "In adaptive filtering, each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query.",
                "For training the model, we use the query itself as the initial positive training example of the class, and the user-highlighted pieces of text (marked as Relevant or Not-relevant) during feedback as additional training examples.",
                "To address the cold start issue in the early stage before any user feedback is obtained, the system uses a small sample from a retrospective corpus as the initial negative examples in the training set.",
                "The details of using logistic regression for adaptive filtering (assigning different weights to positive and negative training instances, and regularizing the objective function to prevent over-fitting on training data) are presented in [21].",
                "The class model wâ learned by Logistic Regression, or the query profile, is a vector whose dimensions are individual terms and whose elements are the regression coefficients, indicating how influential each term is in the query profile.",
                "The query profile is updated whenever a new piece of user feedback is received.",
                "A temporally decaying weight can be applied to each training example, as an option, to emphasize the most recent user feedback. 3.2 Passage Retrieval Component We use standard IR techniques in this part of our system.",
                "Incoming documents are processed in chunks, where each chunk can be defined as a fixed span of time or as a fixed number of documents, as preferred by the user.",
                "For each incoming document, corpus statistics like the IDF (Inverted Document Frequency) of each term are updated.",
                "We use a state-of-the-art named entity identifier and tracker [8, 12] to identify person and location names, and merge them with co-referent named entities seen in the past.",
                "Then the documents are segmented into passages, which can be a whole document, a paragraph, a sentence, or any other continuous span of text, as preferred.",
                "Each passage is represented using a vector of TF-IDF (Term FrequencyInverse Document Frequency) weights, where term can be a word or a named entity.",
                "Given a query profile, i.e. the logistic regression solution wâ as described in Section 3.1, the system computes the posterior probability of relevance for each passage x as fRL(x) â¡ P(y = 1|x, wâ ) = 1 (1 + eâwâÂ·x) (1) Passages are ordered by their relevance scores, and the ones with scores above a threshold (tuned on a training set) comprise the relevance list that is passed on to the novelty detection step. 3.3 Novelty Detection Component CAFÂ´E maintains a user history H(t), which contains all the spans of text hi that the user highlighted (as feedback) during his or her past interactions with the system, up to the current time t. Denoting the history as H(t) = n h1, h2, ..., ht o , (2) the novelty score of a new candidate passage x is computed as: fND(x) = 1 â max iâ1..t {cos(x, hi)} (3) where both candidate passage x and highlighted spans of text hi are represented as TF-IDF vectors.",
                "The novelty score of each passage is compared to a prespecified threshold (also tuned on a training set), and any passage with a score below this threshold is removed from the relevance list. 3.4 Anti-redundant Ranking Component Although the novelty detection component ensures that only novel (previously unseen) information remains in the relevance list, this list might still contain the same novel information at multiple positions in the ranked list.",
                "Suppose, for example, that the user has already read about a $100,000 reward for information about the escaped convicts.",
                "A new piece of news that the award has been increased to $200,000 is novel since the user hasnt read about it yet.",
                "However, multiple news sources would report this news and we might end up showing (redundant) articles from all these sources in a ranked list.",
                "Hence, a ranked list should also be made non-redundant with respect to its own contents.",
                "We use a simplified version of the Maximal Marginal Relevance method [5], originally developed for combining relevance and novelty in text retrieval and summarization.",
                "Our procedure starts with the current list of passages sorted by relevance (section 3.2), filtered by Novelty Detection component (section 3.3), and generates a new non-redundant list as follows: 1.",
                "Take the top passage in the current list as the top one in the new list. 2.",
                "Add the next passage x in the current list to the new list only if fAR(x) > t where fAR(x) = 1 â max piâLnew {cos(x, pi)} and Lnew is the set of passages already selected in the new list. 3.",
                "Repeat step 2 until all the passages in the current list have been examined.",
                "After applying the above-mentioned algorithm, each passage in the new list is sufficiently dissimilar to others, thus favoring diversity rather than redundancy in the new ranked list.",
                "The anti-redundancy threshold t is tuned on a training set. 4.",
                "EVALUATION METHODOLOGY The approach we proposed above for information distillation raises important issues regarding evaluation methodology.",
                "Firstly, since our framework allows the output to be passages at different levels of granularity (e.g. k-sentence windows where k may vary) instead of a fixed length, it is not possible to have pre-annotated relevance judgments at all such granularity levels.",
                "Secondly, since we wish to measure the utility of the system output as a combination of both relevance and novelty, traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time.",
                "Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists.",
                "None of the existing measures in ad-hoc retrieval, adaptive filtering, novelty detection or other related areas (text summarization and question answering) have desirable properties in all the three aspects.",
                "Therefore, we must develop a new evaluation methodology. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers.",
                "Answer keys define what should be present in a system response to receive credit, and are comprised of a collection of information nuggets, i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them.",
                "Defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping [19], since system-returned passages can contain the same information expressed in many different ways.",
                "Hence, QA evaluations have relied on human assessors for the mapping between various expressions, making the process costly, time consuming, and not scalable to large query and document collections, and extensive system evaluations with various parameter settings. 4.1.1 Automating Evaluation based on Answer Keys Automatic evaluation methods would allow for faster system building and tuning, as well as provide an objective and affordable way of comparing various systems.",
                "Recently, such methods have been proposed, more or less, based on the idea of n-gram co-occurrences.",
                "Pourpre [10] assigns a fractional recall score to a system response based on its unigram overlap with a given nuggets description.",
                "For example, a system response A B C has recall 3/4 with respect to a nugget with description A B C D. However, such an approach is unfair to systems that present the same information but using words other than A, B, C, and D. Another open issue is how to weight individual words in measuring the closeness of a match.",
                "For example, consider the question How many prisoners escaped?.",
                "In the nugget Seven prisoners escaped from a Texas prison, there is no indication that seven is the keyword, and that it must be matched to get any relevance credit.",
                "Using IDF values does not help, since seven will generally not have a higher IDF than words like texas and prison.",
                "Also, redefining the nugget as just seven does not solve the problem since now it might spuriously match any mention of seven out of context.",
                "Nuggeteer [13] works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold.",
                "However, it is also plagued by spurious relevance since not all words contained in the nugget description (or known correct responses) are central to the nugget. 4.1.2 Nugget-Matching Rules We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on nugget-matching rules, which are generated using a semiautomatic procedure explained below.",
                "These rules are essentially Boolean queries that will only match against snippets that contain the nugget.",
                "For instance, a candidate rule for matching answers to How many prisoners escaped? is (Texas AND seven AND escape AND (convicts OR prisoners)), possibly with other synonyms and variants in the rule.",
                "For a corpus of news articles, which usually follow a typical formal prose, it is fairly easy to write such simple rules to match expected answers using a bootstrap approach, as described below.",
                "We propose a two-stage approach, inspired by Autoslog [14], that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents.",
                "In the first stage, human subjects annotated (using a highlighting tool) portions of ontopic documents that contained answers to each nugget 2 .",
                "In the second stage, subjects used our rule generation tool to create rules that would match the annotations for each nugget.",
                "The tool allows users to enter a Boolean rule as a disjunction of conjunctions (e.g. ((a AND b) OR (a AND c AND d) OR (e))).",
                "Given a candidate rule, our tool uses it as a Boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match.",
                "Hence, the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision.",
                "We observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions (adding more ANDs), and improving the recall by adding more conjunctions to the disjunction (adding more ORs).",
                "As an example, lets try to create a rule for the nugget which says that seven prisoners escaped from the Texas prison.",
                "We start with a simple rule - (seven).",
                "When we input this into the rule generation tool, we realize that this rule matches many spurious occurrences of seven (e.g. ...seven states...) and thus gets a low precision score.",
                "We can further qualify our rule - Texas AND seven AND convicts.",
                "Next, by looking at the missed annotations, we realize that some news articles mentioned ...seven prisoners escaped.... We then replace convicts with the disjunction (convicts OR prisoners).",
                "We continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the (small number of) misses and false alarms can be safely ignored.",
                "Thus we can create nugget-matching rules that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses.",
                "Human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably. 4.2 Evaluating the Utility of a Sequence of Ranked Lists The utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information, and how much the user lost in terms of time and energy.",
                "We calculate this utility from the utilities of individual passages as follows.",
                "After reading each passage returned by the system, the user derives some gain depending on the presence of relevant and novel information, and incurs a loss in terms of the time and energy spent in going through the passage.",
                "However, the likelihood that the user would actually read a passage depends on its position in the ranked list.",
                "Hence, for a query q, the expected utility 2 LDC [18] already provides relevance judgments for 100 topics on the TDT4 corpus.",
                "We further ensured that these judgments are exhaustive on the entire corpus using pooling. of a passage pi at rank i can be defined as U(pi, q) = P(i) â (Gain(pi, q) â Loss(pi, q)) (4) where P(i) is the probability that the user would go through a passage at rank i.",
                "The expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage: U(q) = nX i=1 P(i) â (Gain(pi, q) â Loss(pi, q)) (5) Note that if we ignore the loss term and define P(i) as P(i) â 1/ logb(b + i â 1) (6) then we get the recently popularized metric called Discounted Cumulated Gain (DCG) [9], where Gain(pi, q) is defined as the graded relevance of passage pi.",
                "However, without the loss term, DCG is a purely recall-oriented metric and not suitable for an adaptive filtering setting, where the systems utility depends in part on its ability to limit the number of items shown to the user.",
                "Although P(i) could be defined based on empirical studies of user behavior, for simplicity, we use P(i) exactly as defined in equation 6.",
                "The gain G(pi, q) of passage pi with respect to the query q is a function of - 1) the number of relevant nuggets present in pi, and 2) the novelty of each of these nuggets.",
                "We combine these two factors as follows.",
                "For each nugget Nj, we assign an initial weight wj, and also keep a count nj of the number of times this nugget has been seen by the user in the past.",
                "The gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor Î³.",
                "Thus, G(pi, q) is defined as G(pi, q) = X Nj âC(pi,q) wj â Î³nj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the answer key of query q.",
                "The initial weights wj are all set of be 1.0 in our experiments, but can also be set based on a pyramid approach [11].",
                "The choice of dampening factor Î³ determines the users tolerance for redundancy.",
                "When Î³ = 0, a nugget will only receive credit for its first occurrence i.e. when nj is zero3 .",
                "For 0 < Î³ < 1, a nugget receives smaller credit for each successive occurrence.",
                "When Î³ = 1, no dampening occurs and repeated occurrences of a nugget receive the same credit.",
                "Note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system, since the users are expected to remember what the system showed them in the past.",
                "We define the loss L(pi, q) as a constant cost c (we use 0.1) incurred when reading a system-returned passage.",
                "Thus, our metric can be re-written as U(q) = nX i=1 Gain(pi, q) logb(b + i â 1) â L(n) (8) where L(n) is the loss associated with a ranked list of length n: L(n) = c Â· nX i=1 1 logb(b + i â 1) (9) 3 Note that 00 = 1 Due to the similarity with Discounted Cumulated Gain (DCG), we call our metric Discounted Cumulated Utility (DCU).",
                "The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) â¤ 0 i.e. when the gain is less than or equal to the cost of reading the passage. 5.",
                "DATA TDT4 was the benchmark corpus used in TDT2002 and TDT2003 evaluations.",
                "The corpus consists of over 90, 000 news articles from multiple sources (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) published between October 2000 and January 2001, in the languages of Arabic, English, and Mandarin.",
                "Speech-recognized and machine-translated versions of the non-English articles were provided as well.",
                "LDC [18] has annotated the corpus with 100 topics, that correspond to various news events in this time period.",
                "Out of these, we selected a subset of 12 actionable events, and defined corresponding tasks for them4 .",
                "For each task, we manually defined a profile consisting of an initial set of (5 to 10) queries, a free-text description of the user history, i.e., what the user already knows about the event, and a list of known on-topic and off-topic documents (if available) as training examples.",
                "For each query, we generated answer keys and corresponding nugget matching rules using the procedure described in section 4.1.2, and produced a total of 120 queries, with an average of 7 nuggets per query. 6.",
                "EXPERIMENTS AND RESULTS 6.1 Baselines We used Indri [17], a popular language-model based retrieval engine, as a baseline for comparison with CAFÂ´E.",
                "Indri supports standard search engine functionality, including pseudo-relevance feedback (PRF) [3, 6], and is representative of a typical query-based retrieval system.",
                "Indri does not support any kind of novelty detection.",
                "We compare Indri with PRF turned on and off, against CAFÂ´E with user feedback, novelty detection and antiredundant ranking turned on and off. 6.2 Experimental Setup We divided the TDT4 corpus spanning 4 months into 10 chunks, each defined as a period of 12 consecutive days.",
                "At any given point of time in the distillation process, each system accessed the past data up to the current point, and returned a ranked list of up 50 passages per query.",
                "The 12 tasks defined on the corpus were divided into a training and test set with 6 tasks each.",
                "Each system was allowed to use the training set to tune its parameters for optimizing NDCU (equation 8), including the relevance threshold for both Indri and CAFÂ´E, and the novelty and antiredundancy thresholds for CAFÂ´E.",
                "The NDCU for each system run is calculated automatically.",
                "User feedback was also simulated - relevance judgments for each system-returned passage (as determined by the nugget matching rules described in section 4.1.2) were 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figure 1: Performance of Indri across chunks Figure 2: Performance of CAFÂ´E across chunks used as user feedback in the adaptation of query profiles and user histories. 6.3 Results In Table 1, we show the NDCU scores of the two systems under various settings.",
                "These scores are averaged over the six tasks in the test set, and are calculated with two dampening factors (see section 4.2): Î³ = 0 and 0.1, to simulate no tolerance and small tolerance for redundancy, respectively.",
                "Using Î³ = 0 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information.",
                "Hence, the improvement obtained from enabling user feedback is smaller with Î³ = 0 than the improvement obtained from feedback with Î³ = 0.1.",
                "This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information.",
                "However, the user does not benefit from seeing such redundant passages, and is usually interested in other passages containing related information.",
                "It is informative to evaluate retrieval systems using our utility measure (with Î³ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.",
                "Sometimes, however, users might indeed be interested in seeing the same information from multiple sources, as an Table 1: NDCU Scores of Indri and CAFÂ´E for two dampening factors (Î³), and various settings (F: Feedback, N: Novelty Detection, A: Anti-Redundant Ranking) Indri CAFÂ´E Î³ Base +PRF Base +F +F+N +F+A +F+N+A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.35 0.36 0.36 indicator of its importance or reliability.",
                "In such a case, we can simply choose a higher value for Î³ which corresponds to a higher tolerance for redundancy, and hence let the system tune its parameters accordingly.",
                "Since documents were processed chunk by chunk, it would be interesting to see how the performance of systems improves over time.",
                "Figures 1 and 2 show the performance trends for both the systems across chunks.",
                "While the performance with and without feedback on the first few chunks is expected to be close, for subsequent chunks, the performance curve with feedback enabled rises above the one with the no-feedback setting.",
                "The performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks, making some queries easier than others in certain chunks.",
                "Moreover, since Indri uses pseudo-relevance feedback while CAFÂ´E uses feedback based on actual relevance judgments, the improvement in case of Indri is less dramatic than that of CAFÂ´E. 7.",
                "CONCLUDING REMARKS This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages.",
                "Our system, called CAFÂ´E, combines adaptive filtering, novelty detection and antiredundant <br>passage rank</br>ing in a unified framework for utility optimization.",
                "We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty.",
                "Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off. 8.",
                "ACKNOWLEDGMENTS We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments.",
                "This work is supported in parts by the National Science Foundation (NSF) under grant IIS0434035, and the Defense Advanced Research Project Agency (DARPA) under contracts NBCHD030010 and W0550432.",
                "Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. 9.",
                "ADDITIONAL AUTHORS Jian Zhang (jianzhan@stat.purdue.edu)â , Jaime Carbonell (jgc@cs.cmu.edu)â  , Peter Brusilovsky (peterb+@pitt.edu)â¡ , Daqing He(dah44@pitt.edu)â¡ 10.",
                "REFERENCES [1] J. Allan.",
                "Incremental Relevance Feedback for Information Filtering.",
                "Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 270-278, 1996. [2] J. Allan, C. Wade, and A. Bolivar.",
                "Retrieval and Novelty Detection at the Sentence Level.",
                "Proceedings of the ACM SIGIR conference on research and development in information retrieval, 2003. [3] C. Buckley, G. Salton, and J. Allan.",
                "Automatic Retrieval with Locality Information using SMART.",
                "NIST special publication, (500207):59-72, 1993. [4] J. Callan.",
                "Learning While Filtering Documents.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 224-231, 1998. [5] J. Carbonell and J. Goldstein.",
                "The use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335-336, 1998. [6] E. Efthimiadis.",
                "Query Expansion.",
                "Annual Review of Information Science and Technology (ARIST), 31:p121-87, 1996. [7] J. Fiscus and G. Duddington.",
                "Topic Detection and Tracking Overview.",
                "Topic Detection and Tracking: Event-based Information Organization, pages 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.",
                "A Statistical Model for Multilingual Entity Detection and Tracking.",
                "NAACL/HLT, 2004. [9] K. JÂ¨arvelin and J. KekÂ¨alÂ¨ainen.",
                "Cumulated Gain-based Evaluation of IR Techniques.",
                "ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin and D. Demner-Fushman.",
                "Automatically Evaluating Answers to Definition Questions.",
                "Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), 2005. â Statistics Dept., Purdue University, West Lafayette, USA â  Language Technologies Inst., Carnegie Mellon University, Pittsburgh, USA â¡ School of Information Sciences, Univ. of Pittsburgh, Pittsburgh, USA [11] J. Lin and D. Demner-Fushman.",
                "Will Pyramids Built of nUggets Topple Over.",
                "Proceedings of HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.",
                "A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree.",
                "Proc. of ACL, 4:136-143, 2004. [13] G. Marton.",
                "Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments.",
                "HLT/NAACL, 2006. [14] E. Riloff.",
                "Automatically Constructing a Dictionary for Information Extraction Tasks.",
                "Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, 1993. [15] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9: Filtering track.",
                "The Ninth Text REtrieval Conference (TREC-9), pages 361-368. [16] R. Schapire, Y.",
                "Singer, and A. Singhal.",
                "Boosting and Rocchio Applied to Text Filtering.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle, and W. Croft.",
                "Indri: A Language Model-based Search Engine for Complex Queries.",
                "Proceedings of the International Conference on Intelligence Analysis, 2004. [18] The Linguistic Data Consortium. http://www.ldc.upenn.edu/. [19] E. Voorhees.",
                "Overview of the TREC 2003 Question Answering Track.",
                "Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), 2003. [20] Y. Yang and B. Kisiel.",
                "Margin-based Local Regression for Adaptive Filtering.",
                "Proceedings of the twelfth international conference on Information and knowledge management, pages 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of Adaptive Filtering Methods in a Cross-benchmark Evaluation.",
                "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 98-105, 2005. [22] C. Zhai, W. Cohen, and J. Lafferty.",
                "Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-17, 2003. [23] J. Zhang and Y. Yang.",
                "Robustness of Regularized Linear Classification Methods in Text Categorization.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 190-197, 2003. [24] Y. Zhang.",
                "Using Bayesian Priors to Combine Classifiers for Adaptive Filtering.",
                "Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 345-352, 2004. [25] Y. Zhang, J. Callan, and T. Minka.",
                "Novelty and Redundancy Detection in Adaptive Filtering.",
                "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002."
            ],
            "original_annotated_samples": [
                "It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant <br>passage rank</br>ing with respect to long-lasting information needs (tasks with multiple queries).",
                "Our system, called CAFÂ´E, combines adaptive filtering, novelty detection and antiredundant <br>passage rank</br>ing in a unified framework for utility optimization."
            ],
            "translated_annotated_samples": [
                "Combina las fortalezas y se extiende mÃ¡s allÃ¡ del filtrado adaptativo convencional, la detecciÃ³n de novedades y la <br>clasificaciÃ³n de pasajes</br> no redundantes con respecto a las necesidades de informaciÃ³n duraderas (tareas con mÃºltiples consultas).",
                "Nuestro sistema, llamado CAFÂ´E, combina filtrado adaptativo, detecciÃ³n de novedades y <br>clasificaciÃ³n de pasajes</br> antirredundantes en un marco unificado para la optimizaciÃ³n de la utilidad."
            ],
            "translated_text": "DestilaciÃ³n de informaciÃ³n basada en utilidad sobre documentos secuenciados temporalmente en el Instituto de TecnologÃ­as del Lenguaje de Yiming Yang. Universidad Carnegie Mellon, Pittsburgh, EE. UU. yiming@cs.cmu.edu Abhimanyu Lad Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. alad@cs.cmu.edu Instituto de TecnologÃ­as del Lenguaje Ni Lao. Universidad Carnegie Mellon, Pittsburgh, EE. UU. nlao@cs.cmu.edu Abhay Harpale Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. aharpale@cs.cmu.edu Bryan Kisiel Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. bkisiel@cs.cmu.edu MÃ³nica Rogati Instituto de TecnologÃ­as del Lenguaje. La Universidad Carnegie Mellon en Pittsburgh, EE. UU. mrogati@cs.cmu.edu RESUMEN Este artÃ­culo examina un nuevo enfoque para la destilaciÃ³n de informaciÃ³n en documentos ordenados temporalmente, y propone un esquema de evaluaciÃ³n novedoso para dicho marco. Combina las fortalezas y se extiende mÃ¡s allÃ¡ del filtrado adaptativo convencional, la detecciÃ³n de novedades y la <br>clasificaciÃ³n de pasajes</br> no redundantes con respecto a las necesidades de informaciÃ³n duraderas (tareas con mÃºltiples consultas). Nuestro enfoque respalda la retroalimentaciÃ³n detallada del usuario a travÃ©s del resaltado de fragmentos arbitrarios de texto, y aprovecha dicha informaciÃ³n para la optimizaciÃ³n de la utilidad en entornos adaptativos. Para nuestros experimentos, definimos tareas hipotÃ©ticas basadas en eventos de noticias en el corpus TDT4, con mÃºltiples consultas por tarea. Se generaron claves de respuestas (nuggets) para cada consulta y se utilizÃ³ un procedimiento semiautomÃ¡tico para adquirir reglas que permiten emparejar automÃ¡ticamente los nuggets con las respuestas del sistema. TambiÃ©n proponemos una extensiÃ³n de la mÃ©trica NDCG para evaluar la utilidad de los pasajes clasificados como una combinaciÃ³n de relevancia y novedad. Nuestros resultados muestran mejoras alentadoras en la utilidad utilizando el nuevo enfoque, en comparaciÃ³n con los sistemas base sin aprendizaje incremental o componentes de detecciÃ³n de novedades. CategorÃ­as y Descriptores de Asignaturas H.3.3 [BÃºsqueda y RecuperaciÃ³n de InformaciÃ³n]: Filtrado de informaciÃ³n, RetroalimentaciÃ³n de relevancia, Modelos de recuperaciÃ³n, Proceso de selecciÃ³n; I.5.2 TÃ©rminos Generales DiseÃ±o, MediciÃ³n, Rendimiento, ExperimentaciÃ³n. 1. INTRODUCCIÃN Seguir informaciÃ³n nueva y relevante de flujos de datos temporales para usuarios con necesidades duraderas ha sido un tema de investigaciÃ³n desafiante en la recuperaciÃ³n de informaciÃ³n. El filtrado adaptativo (AF) es una tarea de predicciÃ³n en lÃ­nea de la relevancia de cada nuevo documento con respecto a temas predefinidos. BasÃ¡ndose en la consulta inicial y algunos ejemplos positivos (si estÃ¡n disponibles), un sistema de AF mantiene un perfil para cada tema de interÃ©s, y lo actualiza constantemente segÃºn la retroalimentaciÃ³n del usuario. La naturaleza de aprendizaje incremental de los sistemas de AF los hace mÃ¡s poderosos que los motores de bÃºsqueda estÃ¡ndar que admiten recuperaciÃ³n ad-hoc (por ejemplo. Google y Yahoo) en tÃ©rminos de encontrar informaciÃ³n relevante con respecto a temas de interÃ©s duraderos, y mÃ¡s atractivo para los usuarios que estÃ¡n dispuestos a proporcionar retroalimentaciÃ³n para adaptar el sistema a sus necesidades de informaciÃ³n especÃ­ficas, sin tener que modificar sus consultas manualmente. Se han estudiado una variedad de algoritmos de aprendizaje supervisado (clasificadores de estilo Rocchio, modelos exponenciales-gaussianos, regresiÃ³n local y enfoques de regresiÃ³n logÃ­stica) para entornos adaptativos, examinados con retroalimentaciÃ³n de relevancia explÃ­cita e implÃ­cita, y evaluados con respecto a la optimizaciÃ³n de la utilidad en grandes colecciones de datos de referencia en los foros de TREC (Conferencias de RecuperaciÃ³n de Texto) y TDT (DetecciÃ³n y Seguimiento de Temas) [1, 4, 7, 15, 16, 20, 24, 23]. La regresiÃ³n logÃ­stica regularizada [21] se ha encontrado representativa para los enfoques de vanguardia, y altamente eficiente para adaptaciones frecuentes del modelo en grandes colecciones de documentos como el corpus TREC-10 (mÃ¡s de 800,000 documentos y 84 temas). A pesar de los logros sustanciales en la investigaciÃ³n reciente de filtrado adaptativo, aÃºn quedan problemas significativos sin resolver sobre cÃ³mo aprovechar de manera efectiva y eficiente la retroalimentaciÃ³n del usuario. EspecÃ­ficamente, los siguientes problemas pueden limitar seriamente la verdadera utilidad de los sistemas de AF en aplicaciones del mundo real: 1. El usuario tiene un papel bastante pasivo en la configuraciÃ³n convencional de filtrado adaptativo: solo reacciona ante el sistema cuando este toma una decisiÃ³n afirmativa sobre un documento, confirmando o rechazando esa decisiÃ³n. Una alternativa mÃ¡s activa serÃ­a permitir al usuario emitir mÃºltiples consultas sobre un tema, revisar una lista clasificada de documentos candidatos (o pasajes) por consulta, y proporcionar retroalimentaciÃ³n sobre la lista clasificada, refinando asÃ­ su necesidad de informaciÃ³n y solicitando listas clasificadas actualizadas. La Ãºltima forma de interacciÃ³n del usuario ha sido altamente efectiva en la recuperaciÃ³n estÃ¡ndar para consultas ad-hoc. CÃ³mo implementar una estrategia para necesidades de informaciÃ³n duraderas en entornos de FA es una pregunta abierta para la investigaciÃ³n. 2. La unidad para recibir un juicio de relevancia (sÃ­ o no) estÃ¡ restringida al nivel del documento en AF convencional. Sin embargo, un usuario real puede estar dispuesto a proporcionar comentarios mÃ¡s informativos y detallados resaltando algunas partes de texto en un documento recuperado como relevantes, en lugar de etiquetar todo el documento como relevante. Aprovechar de manera efectiva este tipo de retroalimentaciÃ³n detallada podrÃ­a mejorar sustancialmente la calidad de un sistema de AF. Para esto, necesitamos habilitar el aprendizaje supervisado a partir de fragmentos de texto etiquetados de longitud arbitraria en lugar de solo permitir documentos etiquetados. 3. Los documentos seleccionados por el sistema suelen ser altamente redundantes. Un evento importante de noticias, por ejemplo, serÃ­a reportado por mÃºltiples fuentes repetidamente durante un tiempo, haciendo que la mayor parte del contenido informativo en esos artÃ­culos sea redundante entre sÃ­. Un sistema de AF convencional seleccionarÃ­a todas estas historias de noticias redundantes para la retroalimentaciÃ³n del usuario, desperdiciando el tiempo de los usuarios mientras ofrece poco beneficio. Claramente, las tÃ©cnicas de detecciÃ³n de novedades pueden ayudar en principio [25, 2, 22] a mejorar la utilidad de los sistemas AF. Sin embargo, la efectividad de tales tÃ©cnicas a nivel de pasaje para detectar novedad con respecto a la retroalimentaciÃ³n de los usuarios a nivel detallado y para detectar redundancia en listas clasificadas aÃºn debe ser evaluada utilizando una medida de utilidad que imite las necesidades de un usuario real. Para abordar las limitaciones actuales de los sistemas de AF mencionadas anteriormente, proponemos y examinamos un nuevo enfoque en este artÃ­culo, que combina las fortalezas del AF convencional (aprendizaje incremental de modelos de temas), recuperaciÃ³n de pasajes multipase para consultas de larga duraciÃ³n condicionadas al tema, y detecciÃ³n de novedades para eliminar la redundancia de las interacciones del usuario con el sistema. Llamamos al nuevo proceso destilaciÃ³n de informaciÃ³n basada en utilidad. Ten en cuenta que los corpus de referencia convencionales para evaluaciones de AF, que tienen juicios de relevancia a nivel de documento y no definen tareas con mÃºltiples consultas, son insuficientes para evaluar el nuevo enfoque. Por lo tanto, ampliamos un corpus de referencia, la colecciÃ³n TDT4 de noticias y transmisiones de televisiÃ³n, con definiciones de tareas, mÃºltiples consultas por tarea y claves de respuestas por consulta. Hemos realizado nuestros experimentos en este corpus extendido TDT4 y hemos puesto a disposiciÃ³n del pÃºblico los datos generados adicionalmente para futuras evaluaciones comparativas. Para evaluar automÃ¡ticamente los fragmentos de texto arbitrarios devueltos por el sistema utilizando nuestras claves de respuestas, desarrollamos un esquema de evaluaciÃ³n con un procedimiento semiautomÃ¡tico para 1 URL: http://nyc.lti.cs.cmu.edu/downloads adquiriendo reglas que pueden comparar fragmentos con las respuestas del sistema. AdemÃ¡s, proponemos una extensiÃ³n de NDCG (Normalized Discounted Cumulated Gain) [9] para evaluar la utilidad de los pasajes clasificados como una funciÃ³n tanto de la relevancia como de la novedad. El resto de este documento estÃ¡ organizado de la siguiente manera. La secciÃ³n 2 describe el proceso de destilaciÃ³n de informaciÃ³n con un ejemplo concreto. La secciÃ³n 3 describe los nÃºcleos tÃ©cnicos de nuestro sistema llamado CAFÂ´E - CMU Adaptive Filtering Engine. La secciÃ³n 4 discute problemas con respecto a la metodologÃ­a de evaluaciÃ³n y propone un nuevo esquema. La secciÃ³n 5 describe el corpus extendido TDT4. La secciÃ³n 6 presenta nuestros experimentos y resultados. La secciÃ³n 7 concluye el estudio y ofrece perspectivas futuras. 2. Una TAREA DE EJEMPLO Considera un evento noticioso: la fuga de siete convictos de una prisiÃ³n de Texas en diciembre de 2000 y su captura un mes despuÃ©s. Suponiendo que un usuario estuviera interesado en este evento desde sus primeras etapas, la necesidad de informaciÃ³n podrÃ­a ser: Encontrar informaciÃ³n sobre la fuga de convictos de una prisiÃ³n de Texas, y la informaciÃ³n relacionada con su recaptura. Las preguntas de nivel inferior asociadas podrÃ­an ser: 1. Â¿CuÃ¡ntos prisioneros escaparon? 2. Â¿DÃ³nde y cuÃ¡ndo fueron avistados? 3. Â¿QuiÃ©nes son sus contactos conocidos dentro y fuera de la prisiÃ³n? 4. Â¿CÃ³mo estÃ¡n armados? 5. Â¿Tienen algÃºn vehÃ­culo? 6. Â¿QuÃ© pasos se han tomado hasta ahora? Llamamos a esta necesidad de informaciÃ³n una tarea, y a las preguntas asociadas como las consultas en esta tarea. Un sistema de destilaciÃ³n debe monitorear los documentos entrantes, procesarlos por fragmentos en un orden temporal, seleccionar pasajes potencialmente relevantes y novedosos de cada fragmento con respecto a cada consulta, y presentar una lista clasificada de pasajes al usuario. La clasificaciÃ³n de pasajes aquÃ­ se basa en cuÃ¡n relevante es un pasaje con respecto a la consulta actual, cuÃ¡n novedoso es con respecto al historial actual del usuario (de sus interacciones con el sistema) y cuÃ¡n redundante es en comparaciÃ³n con otros pasajes con un rango mÃ¡s alto en la lista. Cuando se le presenta una lista de pasajes, el usuario puede proporcionar retroalimentaciÃ³n resaltando tramos de texto arbitrarios que considere relevantes. Estos fragmentos de texto se toman como ejemplos positivos en la adaptaciÃ³n del perfil de consulta, y tambiÃ©n se aÃ±aden al historial de los usuarios. Los pasajes no marcados por el usuario se consideran ejemplos negativos. Tan pronto como se actualiza el perfil de consulta, el sistema vuelve a realizar una bÃºsqueda y devuelve otra lista clasificada de pasajes donde los pasajes previamente vistos son eliminados o clasificados como bajos, segÃºn la preferencia del usuario. Por ejemplo, si el usuario resalta ...los funcionarios han ofrecido una recompensa de $100,000 por su captura... como respuesta relevante a la consulta Â¿QuÃ© pasos se han tomado hasta ahora?, entonces la pieza resaltada se utiliza como un ejemplo de entrenamiento positivo adicional en la adaptaciÃ³n del perfil de la consulta. Este comentario tambiÃ©n se aÃ±ade al historial del usuario como un ejemplo visto, para que en el futuro, el sistema no coloque otro pasaje mencionando una recompensa de $100,000 en la parte superior de la lista clasificada. Sin embargo, un artÃ­culo que mencione que los funcionarios han duplicado el dinero de recompensa a $200,000 podrÃ­a clasificarse alto ya que es relevante tanto para el perfil de consulta (actualizado) como novedoso con respecto a la historia del usuario (actualizada). El usuario puede modificar las consultas originales o agregar una nueva consulta durante el proceso; los perfiles de consulta se cambiarÃ¡n en consecuencia. Claramente, la detecciÃ³n de novedades es muy importante para la utilidad de dicho sistema debido a la bÃºsqueda iterativa. Sin detecciÃ³n de novedad, los antiguos pasajes relevantes se mostrarÃ­an al usuario repetidamente en cada lista clasificada. A travÃ©s del ejemplo anterior, podemos ver las principales propiedades de nuestro nuevo marco de trabajo para la destilaciÃ³n de informaciÃ³n basada en utilidad sobre documentos ordenados temporalmente. Nuestro marco de trabajo combina y amplÃ­a el poder del filtrado adaptativo (AF), la recuperaciÃ³n ad-hoc (IR) y la detecciÃ³n de novedades (ND). En comparaciÃ³n con la IR estÃ¡ndar, nuestro enfoque tiene la capacidad de aprender de forma incremental las necesidades de informaciÃ³n a largo plazo y modelar una secuencia de consultas dentro de una tarea. En comparaciÃ³n con el AF convencional, permite un papel mÃ¡s activo del usuario en refinar sus necesidades de informaciÃ³n y solicitar nuevos resultados al permitir comentarios sobre relevancia y novedad mediante el resaltado de fragmentos arbitrarios de texto en los pasajes devueltos por el sistema. Comparado con trabajos anteriores, esta es la primera evaluaciÃ³n de detecciÃ³n de novedad integrada con filtrado adaptativo para consultas secuenciadas que permite retroalimentaciÃ³n flexible del usuario sobre pasajes clasificados. La combinaciÃ³n de AF, IR y ND con las nuevas extensiones plantea una importante pregunta de investigaciÃ³n sobre la metodologÃ­a de evaluaciÃ³n: Â¿cÃ³mo podemos medir la utilidad de un sistema de destilaciÃ³n de informaciÃ³n como este? Las mÃ©tricas existentes en IR estÃ¡ndar, AF y ND son insuficientes, y se deben explorar nuevas soluciones, como discutiremos en la SecciÃ³n 4, despuÃ©s de describir los nÃºcleos tÃ©cnicos de nuestro sistema en la prÃ³xima secciÃ³n. 3. NÃCLEOS TÃCNICOS Los componentes principales de CAFÃ son: 1) AF para el aprendizaje incremental de perfiles de consulta, 2) IR para estimar la relevancia de pasajes con respecto a los perfiles de consulta, 3) ND para evaluar la novedad de pasajes con respecto al historial de usuarios, y 4) un componente anti-redundancia para eliminar la redundancia de las listas clasificadas. 3.1 Componente de Filtrado Adaptativo Utilizamos un algoritmo de vanguardia en el campo, el mÃ©todo de regresiÃ³n logÃ­stica regularizada que obtuvo los mejores resultados en varios corpus de evaluaciÃ³n de referencia para AF [21]. La regresiÃ³n logÃ­stica (LR) es un algoritmo de aprendizaje supervisado para clasificaciÃ³n estadÃ­stica. BasÃ¡ndose en un conjunto de entrenamiento de instancias etiquetadas, aprende un modelo de clase que luego puede ser utilizado para predecir las etiquetas de instancias no vistas. Su rendimiento, asÃ­ como su eficiencia en tÃ©rminos de tiempo de entrenamiento, lo convierten en un buen candidato cuando se requieren actualizaciones frecuentes del modelo de clase, como es el caso en el filtrado adaptativo, donde el sistema debe aprender de cada nueva retroalimentaciÃ³n proporcionada por el usuario. (Consulte [21] y [23] para complejidad computacional y problemas de implementaciÃ³n). En el filtrado adaptativo, cada consulta se considera como una clase y la probabilidad de que un pasaje pertenezca a esta clase corresponde al grado de relevancia del pasaje con respecto a la consulta. Para entrenar el modelo, utilizamos la propia consulta como ejemplo inicial de entrenamiento positivo de la clase, y las piezas de texto resaltadas por el usuario (marcadas como Relevante o No relevante) durante la retroalimentaciÃ³n como ejemplos de entrenamiento adicionales. Para abordar el problema de inicio en frÃ­o en la etapa inicial antes de obtener cualquier retroalimentaciÃ³n del usuario, el sistema utiliza una pequeÃ±a muestra de un corpus retrospectivo como ejemplos negativos iniciales en el conjunto de entrenamiento. Los detalles de cÃ³mo utilizar la regresiÃ³n logÃ­stica para el filtrado adaptativo (asignando diferentes pesos a las instancias de entrenamiento positivas y negativas, y regularizando la funciÃ³n objetivo para prevenir el sobreajuste en los datos de entrenamiento) se presentan en [21]. El modelo de clase wâ aprendido por RegresiÃ³n LogÃ­stica, o el perfil de consulta, es un vector cuyas dimensiones son tÃ©rminos individuales y cuyos elementos son los coeficientes de regresiÃ³n, indicando cuÃ¡n influyente es cada tÃ©rmino en el perfil de consulta. El perfil de la consulta se actualiza cada vez que se recibe una nueva pieza de retroalimentaciÃ³n del usuario. Un peso decaÃ­do temporalmente se puede aplicar a cada ejemplo de entrenamiento, como una opciÃ³n, para enfatizar la retroalimentaciÃ³n mÃ¡s reciente del usuario. 3.2 Componente de RecuperaciÃ³n de Pasajes Utilizamos tÃ©cnicas estÃ¡ndar de IR en esta parte de nuestro sistema. Los documentos entrantes se procesan por bloques, donde cada bloque puede definirse como un intervalo de tiempo fijo o como un nÃºmero fijo de documentos, segÃºn lo prefiera el usuario. Para cada documento entrante, se actualizan las estadÃ­sticas del corpus como el IDF (Frecuencia Inversa de Documento) de cada tÃ©rmino. Utilizamos un identificador y rastreador de entidades con tecnologÃ­a de punta [8, 12] para identificar nombres de personas y lugares, y fusionarlos con entidades con referencia cruzada vistas en el pasado. Luego, los documentos se dividen en pasajes, que pueden ser un documento completo, un pÃ¡rrafo, una oraciÃ³n, o cualquier otro fragmento continuo de texto, segÃºn se prefiera. Cada pasaje se representa utilizando un vector de pesos TF-IDF (Frecuencia de TÃ©rmino - Frecuencia Inversa de Documento), donde el tÃ©rmino puede ser una palabra o una entidad nombrada. Dado un perfil de consulta, es decir, la soluciÃ³n de regresiÃ³n logÃ­stica wâ como se describe en la SecciÃ³n 3.1, el sistema calcula la probabilidad posterior de relevancia para cada pasaje x como fRL(x) â¡ P(y = 1|x, wâ ) = 1 (1 + eâwâÂ·x) (1). Los pasajes se ordenan segÃºn sus puntajes de relevancia, y aquellos con puntajes por encima de un umbral (ajustado en un conjunto de entrenamiento) conforman la lista de relevancia que se pasa al paso de detecciÃ³n de novedad. 3.3 Componente de DetecciÃ³n de Novedad CAFÂ´E mantiene un historial de usuario H(t), que contiene todos los fragmentos de texto hi que el usuario resaltÃ³ (como retroalimentaciÃ³n) durante sus interacciones pasadas con el sistema, hasta el tiempo actual t. Denotando el historial como H(t) = n h1, h2, ..., ht o , (2), el puntaje de novedad de un nuevo pasaje candidato x se calcula como: fND(x) = 1 â max iâ1..t {cos(x, hi)} (3), donde tanto el pasaje candidato x como los fragmentos de texto resaltados hi se representan como vectores TF-IDF. El puntaje de novedad de cada pasaje se compara con un umbral predefinido (ajustado tambiÃ©n en un conjunto de entrenamiento), y cualquier pasaje con un puntaje por debajo de este umbral se elimina de la lista de relevancia. Componente de clasificaciÃ³n anti-redundante Aunque el componente de detecciÃ³n de novedad garantiza que solo la informaciÃ³n novedosa (no vista previamente) permanezca en la lista de relevancia, esta lista aÃºn podrÃ­a contener la misma informaciÃ³n novedosa en mÃºltiples posiciones en la lista clasificada. Supongamos, por ejemplo, que el usuario ya ha leÃ­do sobre una recompensa de $100,000 por informaciÃ³n sobre los convictos fugados. Una nueva noticia de que el premio ha sido aumentado a $200,000 es novedosa ya que el usuario aÃºn no la ha leÃ­do. Sin embargo, mÃºltiples fuentes de noticias podrÃ­an informar sobre esta noticia y podrÃ­amos terminar mostrando artÃ­culos (redundantes) de todas estas fuentes en una lista clasificada. Por lo tanto, una lista clasificada tambiÃ©n debe ser no redundante con respecto a su propio contenido. Utilizamos una versiÃ³n simplificada del mÃ©todo de MÃ¡xima Relevancia Marginal [5], originalmente desarrollado para combinar relevancia y novedad en la recuperaciÃ³n de texto y resÃºmenes. Nuestro procedimiento comienza con la lista actual de pasajes ordenados por relevancia (secciÃ³n 3.2), filtrados por el componente de DetecciÃ³n de Novedades (secciÃ³n 3.3), y genera una nueva lista no redundante de la siguiente manera: 1. Toma el pasaje superior en la lista actual como el primero en la nueva lista. 2. Agrega el siguiente pasaje x en la lista actual a la nueva lista solo si fAR(x) > t donde fAR(x) = 1 â max piâLnew {cos(x, pi)} y Lnew es el conjunto de pasajes ya seleccionados en la nueva lista. Repetir el paso 2 hasta que se hayan examinado todos los pasajes en la lista actual. DespuÃ©s de aplicar el algoritmo mencionado anteriormente, cada pasaje en la nueva lista es lo suficientemente diferente a los demÃ¡s, favoreciendo asÃ­ la diversidad en lugar de la redundancia en la nueva lista clasificada. El umbral anti-redundancia t se ajusta en un conjunto de entrenamiento. 4. METODOLOGÃA DE EVALUACIÃN El enfoque que propusimos anteriormente para la destilaciÃ³n de informaciÃ³n plantea importantes cuestiones sobre la metodologÃ­a de evaluaciÃ³n. En primer lugar, dado que nuestro marco de trabajo permite que la salida sean fragmentos en diferentes niveles de granularidad (por ejemplo, ventanas de k oraciones donde k puede variar) en lugar de una longitud fija, no es posible tener juicios de relevancia preanotados en todos esos niveles de granularidad. En segundo lugar, dado que deseamos medir la utilidad de la salida del sistema como una combinaciÃ³n de relevancia y novedad, las medidas tradicionales basadas Ãºnicamente en la relevancia deben ser reemplazadas por medidas que penalicen la repeticiÃ³n de la misma informaciÃ³n en la salida del sistema a lo largo del tiempo. En tercer lugar, dado que la salida del sistema son listas clasificadas, debemos recompensar a aquellos sistemas que presenten informaciÃ³n Ãºtil (tanto relevante como no vista previamente) utilizando listas clasificadas mÃ¡s cortas, y penalizar a aquellos que presenten la misma informaciÃ³n utilizando listas clasificadas mÃ¡s largas. Ninguna de las medidas existentes en la recuperaciÃ³n ad-hoc, filtrado adaptativo, detecciÃ³n de novedades u otras Ã¡reas relacionadas (resumen de texto y respuesta a preguntas) tiene propiedades deseables en los tres aspectos. Por lo tanto, debemos desarrollar una nueva metodologÃ­a de evaluaciÃ³n. 4.1 Claves de Respuesta Para permitir la evaluaciÃ³n de un sistema cuya salida consiste en pasajes de longitud arbitraria, tomamos prestado el concepto de claves de respuesta de la comunidad de Preguntas y Respuestas (QA), donde los sistemas pueden devolver fragmentos de texto arbitrarios como respuestas. Las claves de respuestas definen lo que debe estar presente en la respuesta de un sistema para recibir crÃ©dito, y estÃ¡n compuestas por una colecciÃ³n de \"nuggets\" de informaciÃ³n, es decir, unidades de datos sobre las cuales los evaluadores humanos pueden tomar decisiones binarias sobre si una respuesta del sistema las contiene o no. La definiciÃ³n de claves de respuestas y la toma de decisiones binarias asociadas son tareas conceptuales que requieren un mapeo semÃ¡ntico [19], ya que los pasajes devueltos por el sistema pueden contener la misma informaciÃ³n expresada de muchas formas diferentes. Por lo tanto, las evaluaciones de QA han dependido de evaluadores humanos para la correspondencia entre diversas expresiones, lo que hace que el proceso sea costoso, consume mucho tiempo y no sea escalable para grandes colecciones de consultas y documentos, y evaluaciones extensas de sistemas con diversos ajustes de parÃ¡metros. 4.1.1 AutomatizaciÃ³n de la evaluaciÃ³n basada en claves de respuestas Los mÃ©todos de evaluaciÃ³n automÃ¡tica permitirÃ­an una construcciÃ³n y ajuste mÃ¡s rÃ¡pidos del sistema, asÃ­ como proporcionar una forma objetiva y asequible de comparar diversos sistemas. Recientemente, se han propuesto mÃ©todos basados mÃ¡s o menos en la idea de las co-ocurrencias de n-gramos. Pourpre [10] asigna una puntuaciÃ³n de recuerdo fraccional a una respuesta del sistema basada en su superposiciÃ³n de unigramas con una descripciÃ³n de pepitas dada. Por ejemplo, una respuesta del sistema A B C tiene una recuperaciÃ³n de 3/4 con respecto a un fragmento con la descripciÃ³n A B C D. Sin embargo, este enfoque es injusto para los sistemas que presentan la misma informaciÃ³n pero utilizando palabras distintas a A, B, C y D. Otro problema abierto es cÃ³mo ponderar las palabras individuales al medir la cercanÃ­a de una coincidencia. Por ejemplo, considera la pregunta Â¿CuÃ¡ntos prisioneros escaparon? En el fragmento Siete prisioneros escaparon de una prisiÃ³n de Texas, no hay indicaciÃ³n de que siete sea la palabra clave y que debe coincidir para obtener algÃºn crÃ©dito de relevancia. El uso de los valores de IDF no ayuda, ya que la palabra \"seven\" generalmente no tendrÃ¡ un IDF mÃ¡s alto que palabras como \"texas\" y \"prison\". AdemÃ¡s, redefinir el fragmento como simplemente siete no resuelve el problema, ya que ahora podrÃ­a coincidir errÃ³neamente con cualquier menciÃ³n de siete fuera de contexto. Nuggeteer [13] trabaja sobre principios similares pero toma decisiones binarias sobre si hay un fragmento en una respuesta de sistema dada ajustando un umbral. Sin embargo, tambiÃ©n estÃ¡ plagado de relevancia espuria ya que no todas las palabras contenidas en la descripciÃ³n de la pepita (o respuestas correctas conocidas) son centrales para la pepita. 4.1.2 Reglas de Coincidencia de Pepitas Proponemos un mÃ©todo automÃ¡tico confiable para determinar si un fragmento de texto contiene una pepita dada, basado en reglas de coincidencia de pepitas, que se generan utilizando un procedimiento semiautomÃ¡tico explicado a continuaciÃ³n. Estas reglas son bÃ¡sicamente consultas booleanas que solo coincidirÃ¡n con fragmentos que contengan la pepita. Por ejemplo, una regla candidata para emparejar respuestas a Â¿CuÃ¡ntos prisioneros escaparon? es (Texas Y siete Y escapar Y (convictos O prisioneros)), posiblemente con otros sinÃ³nimos y variantes en la regla. Para un corpus de artÃ­culos de noticias, que suelen seguir una prosa formal tÃ­pica, es bastante fÃ¡cil escribir reglas simples para coincidir con respuestas esperadas utilizando un enfoque de arranque, como se describe a continuaciÃ³n. Proponemos un enfoque de dos etapas, inspirado en Autoslog [14], que combina la capacidad de los humanos para identificar expresiones semÃ¡nticamente equivalentes y la capacidad del sistema para recopilar evidencia estadÃ­stica de un corpus de documentos anotados por humanos. En la primera etapa, los sujetos humanos anotaron (utilizando una herramienta de resaltado) porciones de documentos relevantes que contenÃ­an respuestas a cada fragmento 2. En la segunda etapa, los sujetos utilizaron nuestra herramienta de generaciÃ³n de reglas para crear reglas que coincidieran con las anotaciones de cada fragmento. La herramienta permite a los usuarios ingresar una regla booleana como una disyunciÃ³n de conjunciones (por ejemplo, ((a Y b) O (a Y c Y d) O (e))). Dada una regla candidata, nuestra herramienta la utiliza como una consulta booleana sobre el conjunto completo de documentos relevantes y calcula su recall y precisiÃ³n con respecto a las anotaciones que se espera que coincida. Por lo tanto, los sujetos pueden comenzar con una regla simple y refinarla iterativamente hasta que estÃ©n satisfechos con su precisiÃ³n y recuperaciÃ³n. Observamos que era muy fÃ¡cil para los humanos mejorar la precisiÃ³n de una regla ajustando sus conjunciones existentes (agregando mÃ¡s Y) y mejorar la recuperaciÃ³n agregando mÃ¡s conjunciones a la disyunciÃ³n (agregando mÃ¡s O). Como ejemplo, intentemos crear una regla para el acertijo que dice que siete prisioneros escaparon de la prisiÃ³n de Texas. Empezamos con una regla simple - (siete). Cuando introducimos esto en la herramienta de generaciÃ³n de reglas, nos damos cuenta de que esta regla coincide con muchas ocurrencias espurias de siete (por ejemplo, ...siete estados...) y, por lo tanto, obtiene una puntuaciÃ³n baja de precisiÃ³n. Podemos calificar aÃºn mÃ¡s nuestra regla - Texas Y siete Y convictos. A continuaciÃ³n, al revisar las anotaciones omitidas, nos damos cuenta de que algunos artÃ­culos de noticias mencionaban ...siete prisioneros escaparon.... Luego reemplazamos convictos por la disyunciÃ³n (convictos O prisioneros). Continuamos ajustando la regla de esta manera hasta lograr un nivel de recordaciÃ³n y precisiÃ³n suficientemente alto, es decir, que los (pocos) errores y alarmas falsas se puedan ignorar de forma segura. AsÃ­ podemos crear reglas de coincidencia de pepitas que capturen de manera sucinta varias formas de expresar una pepita, evitando al mismo tiempo emparejar respuestas incorrectas (o fuera de contexto). La participaciÃ³n humana en el proceso de creaciÃ³n de reglas garantiza reglas genÃ©ricas de alta calidad que luego se pueden utilizar para evaluar de manera confiable las respuestas del sistema arbitrario. 4.2 EvaluaciÃ³n de la Utilidad de una Secuencia de Listas Clasificadas La utilidad de un sistema de recuperaciÃ³n se puede definir como la diferencia entre cuÃ¡nta informaciÃ³n Ãºtil obtuvo el usuario y cuÃ¡nto tiempo y energÃ­a perdiÃ³. Calculamos esta utilidad a partir de las utilidades de los pasajes individuales de la siguiente manera. DespuÃ©s de leer cada pasaje devuelto por el sistema, el usuario obtiene cierta ganancia dependiendo de la presencia de informaciÃ³n relevante y novedosa, y incurre en una pÃ©rdida en tÃ©rminos del tiempo y la energÃ­a invertidos en revisar el pasaje. Sin embargo, la probabilidad de que el usuario realmente lea un pasaje depende de su posiciÃ³n en la lista clasificada. Por lo tanto, para una consulta q, la utilidad esperada 2 LDC [18] ya proporciona juicios de relevancia para 100 temas en el corpus TDT4. AdemÃ¡s, nos aseguramos de que estos juicios sean exhaustivos en todo el corpus utilizando la agrupaciÃ³n. El valor de un pasaje pi en el rango i se puede definir como U(pi, q) = P(i) â (Ganancia(pi, q) - PÃ©rdida(pi, q)) (4) donde P(i) es la probabilidad de que el usuario pase por un pasaje en el rango i. La utilidad esperada para una lista clasificada completa de longitud n se puede calcular simplemente sumando la utilidad esperada de cada pasaje: U(q) = Î£ i=1 P(i) â (Ganancia(pi, q) â PÃ©rdida(pi, q)) (5) NÃ³tese que si ignoramos el tÃ©rmino de pÃ©rdida y definimos P(i) como P(i) â 1/ logb(b + i â 1) (6) entonces obtenemos la mÃ©trica recientemente popularizada llamada Ganancia Acumulada Descontada (DCG) [9], donde Ganancia(pi, q) se define como la relevancia graduada del pasaje pi. Sin embargo, sin el tÃ©rmino de pÃ©rdida, DCG es una mÃ©trica puramente orientada al recuerdo y no es adecuada para un entorno de filtrado adaptativo, donde la utilidad del sistema depende en parte de su capacidad para limitar el nÃºmero de elementos mostrados al usuario. Aunque P(i) podrÃ­a definirse en base a estudios empÃ­ricos del comportamiento del usuario, por simplicidad, utilizamos P(i) tal como se define exactamente en la ecuaciÃ³n 6. La ganancia G(pi, q) del pasaje pi con respecto a la consulta q es una funciÃ³n de - 1) el nÃºmero de pepitas relevantes presentes en pi, y 2) la novedad de cada una de estas pepitas. Combinamos estos dos factores de la siguiente manera. Para cada pepita Nj, asignamos un peso inicial wj, y tambiÃ©n mantenemos un conteo nj del nÃºmero de veces que esta pepita ha sido vista por el usuario en el pasado. La ganancia derivada de cada ocurrencia posterior del mismo hallazgo se asume que se reduce por un factor de amortiguamiento Î³. Por lo tanto, G(pi, q) se define como G(pi, q) = X Nj âC(pi,q) wj â Î³nj (7) donde C(pi, q) es el conjunto de todas las pepitas que aparecen en el pasaje pi y tambiÃ©n pertenecen a la clave de respuesta de la consulta q. Los pesos iniciales wj se establecen todos en 1.0 en nuestros experimentos, pero tambiÃ©n pueden establecerse segÃºn un enfoque de pirÃ¡mide [11]. La elecciÃ³n del factor de amortiguaciÃ³n Î³ determina la tolerancia del usuario a la redundancia. Cuando Î³ = 0, un pepita solo recibirÃ¡ crÃ©dito por su primera ocurrencia, es decir, cuando nj sea cero. Para 0 < Î³ < 1, un nugget recibe menos crÃ©dito por cada ocurrencia sucesiva. Cuando Î³ = 1, no se produce amortiguamiento y las repeticiones de un valor reciben el mismo crÃ©dito. Se debe tener en cuenta que los recuentos de ocurrencia de las pepitas se conservan entre la evaluaciÃ³n de las listas clasificadas sucesivas devueltas por el sistema, ya que se espera que los usuarios recuerden lo que el sistema les mostrÃ³ en el pasado. Definimos la pÃ©rdida L(pi, q) como un costo constante c (usamos 0.1) incurrido al leer un pasaje devuelto por el sistema. Por lo tanto, nuestra mÃ©trica puede ser reescrita como U(q) = nX i=1 Ganancia(pi, q) logb(b + i â 1) â L(n) (8) donde L(n) es la pÃ©rdida asociada con una lista clasificada de longitud n: L(n) = c Â· nX i=1 1 logb(b + i â 1) (9) 3 NÃ³tese que 00 = 1 Debido a la similitud con la Ganancia Acumulada Descontada (DCG), llamamos a nuestra mÃ©trica Utilidad Acumulada Descontada (DCU). El puntaje de DCU obtenido por el sistema se convierte en un puntaje de DCU normalizado (NDCU) dividiÃ©ndolo por el puntaje de DCU de la lista ideal clasificada, que se crea ordenando los pasajes por sus puntajes de utilidad decrecientes U(pi, q) y deteniÃ©ndose cuando U(pi, q) â¤ 0, es decir, cuando la ganancia es menor o igual al costo de leer el pasaje. El conjunto de datos TDT4 fue el corpus de referencia utilizado en las evaluaciones TDT2002 y TDT2003. El corpus consiste en mÃ¡s de 90,000 artÃ­culos de noticias de mÃºltiples fuentes (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) publicados entre octubre de 2000 y enero de 2001, en los idiomas Ã¡rabe, inglÃ©s y mandarÃ­n. Las versiones de los artÃ­culos en otros idiomas reconocidas por voz y traducidas por mÃ¡quina tambiÃ©n fueron proporcionadas. LDC [18] ha anotado el corpus con 100 temas, que corresponden a varios eventos de noticias en este perÃ­odo de tiempo. De entre estos, seleccionamos un subconjunto de 12 eventos accionables y definimos tareas correspondientes para ellos. Para cada tarea, definimos manualmente un perfil que consiste en un conjunto inicial de (5 a 10) consultas, una descripciÃ³n de texto libre del historial del usuario, es decir, lo que el usuario ya sabe sobre el evento, y una lista de documentos conocidos relevantes y no relevantes (si estÃ¡n disponibles) como ejemplos de entrenamiento. Para cada consulta, generamos claves de respuestas y reglas de coincidencia de pepitas correspondientes utilizando el procedimiento descrito en la secciÃ³n 4.1.2, y produjimos un total de 120 consultas, con un promedio de 7 pepitas por consulta. EXPERIMENTOS Y RESULTADOS 6.1 Baselines Utilizamos Indri [17], un motor de bÃºsqueda basado en modelos de lenguaje popular, como referencia para comparar con CAFÃ. Indri admite la funcionalidad estÃ¡ndar de los motores de bÃºsqueda, incluido el feedback de pseudo-relevancia (PRF) [3, 6], y es representativo de un sistema de recuperaciÃ³n basado en consultas tÃ­pico. Indri no admite ningÃºn tipo de detecciÃ³n de novedades. Comparamos Indri con PRF activado y desactivado, frente a CAFÃ con retroalimentaciÃ³n del usuario, detecciÃ³n de novedades y clasificaciÃ³n antirredundante activados y desactivados. 6.2 ConfiguraciÃ³n Experimental Dividimos el corpus TDT4 que abarca 4 meses en 10 fragmentos, cada uno definido como un perÃ­odo de 12 dÃ­as consecutivos. En cualquier momento dado del proceso de destilaciÃ³n, cada sistema accedÃ­a a los datos pasados hasta el punto actual, y devolvÃ­a una lista clasificada de hasta 50 pasajes por consulta. Las 12 tareas definidas en el corpus se dividieron en un conjunto de entrenamiento y prueba con 6 tareas cada uno. Cada sistema pudo utilizar el conjunto de entrenamiento para ajustar sus parÃ¡metros con el fin de optimizar NDCU (ecuaciÃ³n 8), incluyendo el umbral de relevancia tanto para Indri como para CAFÂ´E, y los umbrales de novedad y antirredundancia para CAFÂ´E. La NDCU para cada ejecuciÃ³n del sistema se calcula automÃ¡ticamente. TambiÃ©n se simulÃ³ la retroalimentaciÃ³n de los usuarios: los juicios de relevancia para cada pasaje devuelto por el sistema (segÃºn las reglas de coincidencia de pepitas descritas en la secciÃ³n 4.1.2) fueron utilizados como retroalimentaciÃ³n de los usuarios en la adaptaciÃ³n de perfiles de consulta e historiales de usuario. En la Tabla 1, mostramos los puntajes NDCU de los dos sistemas en diferentes configuraciones. Estas puntuaciones se promedian en las seis tareas del conjunto de pruebas, y se calculan con dos factores de amortiguaciÃ³n (ver secciÃ³n 4.2): Î³ = 0 y 0.1, para simular ninguna tolerancia y poca tolerancia para la redundancia, respectivamente. Usar Î³ = 0 crea una mÃ©trica mucho mÃ¡s estricta ya que no otorga ningÃºn crÃ©dito a un pasaje que contenga informaciÃ³n relevante pero redundante. Por lo tanto, la mejora obtenida al habilitar la retroalimentaciÃ³n del usuario es menor con Î³ = 0 que la mejora obtenida de la retroalimentaciÃ³n con Î³ = 0.1. Esto revela una deficiencia de los sistemas de recuperaciÃ³n contemporÃ¡neos: cuando el usuario da retroalimentaciÃ³n positiva sobre un pasaje, los sistemas otorgan mayor peso a los tÃ©rminos presentes en ese pasaje y tienden a recuperar otros pasajes que contienen los mismos tÃ©rminos, y por lo tanto, generalmente la misma informaciÃ³n. Sin embargo, el usuario no se beneficia al ver pasajes redundantes y generalmente estÃ¡ interesado en otros pasajes que contengan informaciÃ³n relacionada. Es informativo evaluar los sistemas de recuperaciÃ³n utilizando nuestra medida de utilidad (con Î³ = 0) que tiene en cuenta la novedad y, por lo tanto, ofrece una imagen mÃ¡s realista de cuÃ¡n bien un sistema puede generalizar a partir de la retroalimentaciÃ³n del usuario, en lugar de utilizar medidas tradicionales de RI como la recuperaciÃ³n y la precisiÃ³n, que ofrecen una imagen incompleta de la mejora obtenida a partir de la retroalimentaciÃ³n del usuario. A veces, sin embargo, los usuarios podrÃ­an estar interesados en ver la misma informaciÃ³n de mÃºltiples fuentes, como un indicador de su importancia o fiabilidad. En tal caso, simplemente podemos elegir un valor mÃ¡s alto para Î³ que corresponda a una mayor tolerancia para la redundancia, y asÃ­ permitir que el sistema ajuste sus parÃ¡metros en consecuencia. Dado que los documentos fueron procesados por fragmentos, serÃ­a interesante ver cÃ³mo mejora el rendimiento de los sistemas con el tiempo. Las figuras 1 y 2 muestran las tendencias de rendimiento de ambos sistemas a lo largo de los fragmentos. Si bien se espera que el rendimiento con y sin retroalimentaciÃ³n en los primeros fragmentos sea similar, para los fragmentos posteriores, la curva de rendimiento con la retroalimentaciÃ³n habilitada supera a la del ajuste sin retroalimentaciÃ³n. Las tendencias de rendimiento no son consistentes en todos los fragmentos porque los documentos relevantes no estÃ¡n distribuidos uniformemente en todos los fragmentos, lo que hace que algunas consultas sean mÃ¡s fÃ¡ciles que otras en ciertos fragmentos. AdemÃ¡s, dado que Indri utiliza retroalimentaciÃ³n de pseudo-relevancia mientras que CAFÂ´E utiliza retroalimentaciÃ³n basada en juicios de relevancia reales, la mejora en el caso de Indri es menos dramÃ¡tica que la de CAFÂ´E. 7. CONCLUSIONES FINALES Este artÃ­culo presenta la primera investigaciÃ³n sobre la destilaciÃ³n de informaciÃ³n basada en la utilidad con un sistema que aprende las necesidades de informaciÃ³n duraderas a partir de la retroalimentaciÃ³n detallada del usuario sobre una secuencia de pasajes clasificados. Nuestro sistema, llamado CAFÂ´E, combina filtrado adaptativo, detecciÃ³n de novedades y <br>clasificaciÃ³n de pasajes</br> antirredundantes en un marco unificado para la optimizaciÃ³n de la utilidad. Desarrollamos un nuevo esquema para la evaluaciÃ³n y retroalimentaciÃ³n automatizadas basado en un procedimiento semiautomÃ¡tico para adquirir reglas que permitan emparejar automÃ¡ticamente fragmentos con las respuestas del sistema. TambiÃ©n propusimos una extensiÃ³n de la mÃ©trica NDCG para evaluar la utilidad de los pasajes clasificados como una combinaciÃ³n ponderada de relevancia y novedad. Nuestros experimentos en el reciÃ©n anotado corpus de referencia TDT4 muestran un alentador aumento de utilidad sobre Indri, y tambiÃ©n sobre nuestro propio sistema con aprendizaje incremental y detecciÃ³n de novedades desactivados. 8. AGRADECIMIENTOS Nos gustarÃ­a agradecer a Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng y al Programa de AnÃ¡lisis de Datos Cualitativos de la Universidad de Pittsburgh liderado por el Dr. Stuart Shulman por su ayuda en la recolecciÃ³n y procesamiento de las anotaciones extendidas de TDT4 utilizadas en nuestros experimentos. Este trabajo estÃ¡ parcialmente respaldado por la FundaciÃ³n Nacional de Ciencias (NSF) bajo la subvenciÃ³n IIS0434035, y la Agencia de Proyectos de InvestigaciÃ³n Avanzada de Defensa (DARPA) bajo los contratos NBCHD030010 y W0550432. Cualquier opiniÃ³n, hallazgo, conclusiÃ³n o recomendaciÃ³n expresada en este material son responsabilidad de los autores y no reflejan necesariamente los puntos de vista de los patrocinadores. AUTORES ADICIONALES Jian Zhang (jianzhan@stat.purdue.edu)â , Jaime Carbonell (jgc@cs.cmu.edu)â  , Peter Brusilovsky (peterb+@pitt.edu)â¡ , Daqing He(dah44@pitt.edu)â¡ 10. REFERENCIAS [1] J. Allan. RetroalimentaciÃ³n incremental de relevancia para la filtraciÃ³n de informaciÃ³n. Actas de la 19Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 270-278, 1996. [2] J. Allan, C. Wade y A. Bolivar. RecuperaciÃ³n y DetecciÃ³n de Novedad a Nivel de OraciÃ³n. Actas de la conferencia ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, 2003. [3] C. Buckley, G. Salton y J. Allan. RecuperaciÃ³n automÃ¡tica con informaciÃ³n de localidad utilizando SMART. PublicaciÃ³n especial del NIST, (500207):59-72, 1993. [4] J. Callan. Aprendiendo mientras se filtran documentos. Actas de la 21Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 224-231, 1998. [5] J. Carbonell y J. Goldstein. El uso de MMR, Reordenamiento basado en Diversidad para Reorganizar Documentos y Producir ResÃºmenes. Actas de la 21Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 335-336, 1998. [6] E. Efthimiadis. ExpansiÃ³n de consulta. RevisiÃ³n Anual de Ciencia de la InformaciÃ³n y TecnologÃ­a (ARIST), 31:p121-87, 1996. [7] J. Fiscus y G. Duddington. Resumen de DetecciÃ³n y Seguimiento de Temas. DetecciÃ³n y seguimiento de temas: OrganizaciÃ³n de la informaciÃ³n basada en eventos, pÃ¡ginas 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov y S. Roukos. Un modelo estadÃ­stico para la detecciÃ³n y seguimiento de entidades multilingÃ¼es. NAACL/HLT, 2004. [9] K. JÃ¤rvelin y J. KekÃ¤lÃ¤inen. EvaluaciÃ³n basada en la Ganancia Acumulada de TÃ©cnicas de RecuperaciÃ³n de InformaciÃ³n. ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin y D. Demner-Fushman. EvaluaciÃ³n automÃ¡tica de respuestas a preguntas de definiciÃ³n. Actas de la Conferencia de TecnologÃ­a del Lenguaje Humano y Conferencia sobre MÃ©todos EmpÃ­ricos en Procesamiento del Lenguaje Natural (HLT/EMNLP 2005), 2005. â Departamento de EstadÃ­stica, Universidad de Purdue, West Lafayette, EE. UU. â  Instituto de TecnologÃ­as del Lenguaje, Universidad Carnegie Mellon, Pittsburgh, EE. UU. â¡ Escuela de Ciencias de la InformaciÃ³n, Universidad de Pittsburgh, Pittsburgh, EE. UU. [11] J. Lin y D. Demner-Fushman. Â¿Se derrumbarÃ¡n las pirÃ¡mides construidas con pepitas? Actas de HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla y S. Roukos. Un algoritmo de resoluciÃ³n de correferencia sÃ­ncrona basado en el Ã¡rbol de Bell. Actas de ACL, 4:136-143, 2004. [13] G. Marton. Nuggeteer: EvaluaciÃ³n AutomÃ¡tica Basada en Pepitas Utilizando Descripciones y Juicios. HLT/NAACL, 2006. [14] E. Riloff. ConstrucciÃ³n automÃ¡tica de un diccionario para tareas de extracciÃ³n de informaciÃ³n. Actas de la UndÃ©cima Conferencia Nacional sobre Inteligencia Artificial, pÃ¡ginas 811-816, 1993. [15] S. Robertson y S. Walker. Microsoft Cambridge en TREC-9: Pista de filtrado. La Novena Conferencia de RecuperaciÃ³n de Texto (TREC-9), pÃ¡ginas 361-368. [16] R. Schapire, Y. Cantante, y A. Singhal. Boosting y Rocchio aplicados a la filtraciÃ³n de texto. Actas de la 21Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle y W. Croft. Indri: Un motor de bÃºsqueda basado en modelos de lenguaje para consultas complejas. Actas de la Conferencia Internacional sobre AnÃ¡lisis de Inteligencia, 2004. [18] El Consorcio de Datos LingÃ¼Ã­sticos. http://www.ldc.upenn.edu/. [19] E. Voorhees. Resumen de la pista de preguntas y respuestas TREC 2003. Actas de la DuodÃ©cima Conferencia de RecuperaciÃ³n de Texto (TREC 2003), 2003. [20] Y. Yang y B. Kisiel. RegresiÃ³n local basada en mÃ¡rgenes para filtrado adaptativo. Actas de la duodÃ©cima conferencia internacional sobre gestiÃ³n de la informaciÃ³n y el conocimiento, pÃ¡ginas 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang y B. Kisiel. Robustez de los MÃ©todos de Filtrado Adaptativo en una EvaluaciÃ³n de Referencia Cruzada. Actas de la 28Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 98-105, 2005. [22] C. Zhai, W. Cohen y J. Lafferty. MÃ¡s allÃ¡ de la relevancia independiente: MÃ©todos y mÃ©tricas de evaluaciÃ³n para la recuperaciÃ³n de subtemas. Actas de la 26Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 10-17, 2003. [23] J. Zhang y Y. Yang. Robustez de los MÃ©todos de ClasificaciÃ³n Lineal Regularizados en la CategorizaciÃ³n de Textos. Actas de la 26Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 190-197, 2003. [24] Y. Zhang. Utilizando priors bayesianos para combinar clasificadores para filtrado adaptativo. Actas de la 27Âª conferencia internacional anual sobre InvestigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 345-352, 2004. [25] Y. Zhang, J. Callan y T. Minka. DetecciÃ³n de novedad y redundancia en el filtrado adaptativo. Actas de la 25Âª Conferencia Internacional Anual de ACM SIGIR sobre InvestigaciÃ³n y Desarrollo en RecuperaciÃ³n de InformaciÃ³n, 2002. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "ï¬exible user feedback": {
            "translated_key": "retroalimentaciÃ³n flexible del usuario",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Utility-based Information Distillation Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu ABSTRACT This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework.",
                "It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant passage ranking with respect to long-lasting information needs (tasks with multiple queries).",
                "Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings.",
                "For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task.",
                "Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty.",
                "Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval.",
                "Adaptive filtering (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics.",
                "Based on the initial query and a few positive examples (if available), an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user.",
                "The incremental learning nature of AF systems makes them more powerful than standard search engines that support ad-hoc retrieval (e.g.",
                "Google and Yahoo) in terms of finding relevant information with respect to long-lasting topics of interest, and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs, without having to modify their queries manually.",
                "A variety of supervised learning algorithms (Rocchio-style classifiers, Exponential-Gaussian models, local regression and logistic regression approaches) have been studied for adaptive settings, examined with explicit and implicit relevance feedback, and evaluated with respect to utility optimization on large benchmark data collections in TREC (Text Retrieval Conferences) and TDT (Topic Detection and Tracking) forums [1, 4, 7, 15, 16, 20, 24, 23].",
                "Regularized logistic regression [21] has been found representative for the state-of-the-art approaches, and highly efficient for frequent model adaptations over large document collections such as the TREC-10 corpus (over 800,000 documents and 84 topics).",
                "Despite substantial achievements in recent adaptive filtering research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently.",
                "Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications: 1.",
                "User has a rather passive role in the conventional adaptive filtering setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision.",
                "A more active alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents (or passages) per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists.",
                "The latter form of user interaction has been highly effective in standard retrieval for ad-hoc queries.",
                "How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2.",
                "The unit for receiving a relevance judgment (yes or no) is restricted to the document level in conventional AF.",
                "However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant.",
                "Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system.",
                "For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3.",
                "System-selected documents are often highly redundant.",
                "A major news event, for example, would be reported by multiple sources repeatedly for a while, making most of the information content in those articles redundant with each other.",
                "A conventional AF system would select all these redundant news stories for user feedback, wasting the users time while offering little gain.",
                "Clearly, techniques for novelty detection can help in principle [25, 2, 22] for improving the utility of the AF systems.",
                "However, the effectiveness of such techniques at passage level to detect novelty with respect to users (fine-grained) feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user.",
                "To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and novelty detection for removal of redundancy from user interactions with the system.",
                "We call the new process utility-based information distillation.",
                "Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach.",
                "Therefore, we extended a benchmark corpus - the TDT4 collection of news stories and TV broadcasts - with task definitions, multiple queries per task, and answer keys per query.",
                "We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 .",
                "To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses.",
                "Moreover, we propose an extension of NDCG (Normalized Discounted Cumulated Gain) [9] for assessing the utility of ranked passages as a function of both relevance and novelty.",
                "The rest of this paper is organized as follows.",
                "Section 2 outlines the information distillation process with a concrete example.",
                "Section 3 describes the technical cores of our system called CAFÂ´E - CMU Adaptive Filtering Engine.",
                "Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme.",
                "Section 5 describes the extended TDT4 corpus.",
                "Section 6 presents our experiments and results.",
                "Section 7 concludes the study and gives future perspectives. 2.",
                "A SAMPLE TASK Consider a news event - the escape of seven convicts from a Texas prison in December 2000 and their capture a month later.",
                "Assuming a user were interested in this event since its early stage, the information need could be: Find information about the escape of convicts from Texas prison, and information related to their recapture.",
                "The associated lower-level questions could be: 1.",
                "How many prisoners escaped? 2.",
                "Where and when were they sighted? 3. Who are their known contacts inside and outside the prison? 4.",
                "How are they armed? 5.",
                "Do they have any vehicles? 6.",
                "What steps have been taken so far?",
                "We call such an information need a task, and the associated questions as the queries in this task.",
                "A distillation system is supposed to monitor the incoming documents, process them chunk by chunk in a temporal order, select potentially relevant and novel passages from each chunk with respect to each query, and present a ranked list of passages to the user.",
                "Passage ranking here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list.",
                "When presented with a list of passages, the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant.",
                "These spans of text are taken as positive examples in the adaptation of the query profile, and also added to the users history.",
                "Passages not marked by the user are taken as negative examples.",
                "As soon as the query profile is updated, the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low, based on user preference.",
                "For example, if the user highlights ...officials have posted a $100,000 reward for their capture... as relevant answer to the query What steps have been taken so far?, then the highlighted piece is used as an additional positive training example in the adaptation of the query profile.",
                "This piece of feedback is also added to the user history as a seen example, so that in future, the system will not place another passage mentioning $100,000 reward at the top of the ranked list.",
                "However, an article mentioning ...officials have doubled the reward money to $200,000... might be ranked high since it is both relevant to the (updated) query profile and novel with respect to the (updated) user history.",
                "The user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly.",
                "Clearly, novelty detection is very important for the utility of such a system because of the iterative search.",
                "Without novelty detection, the old relevant passages would be shown to the user repeatedly in each ranked list.",
                "Through the above example, we can see the main properties of our new framework for utility-based information distillation over temporally ordered documents.",
                "Our framework combines and extends the power of adaptive filtering (AF), ad-hoc retrieval (IR) and novelty detection (ND).",
                "Compared to standard IR, our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task.",
                "Compared to conventional AF, it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system.",
                "Compared to past work, this is the first evaluation of novelty detection integrated with adaptive filtering for sequenced queries that allows flexible user feedback over ranked passages.",
                "The combination of AF, IR and ND with the new extensions raises an important research question regarding evaluation methodology: how can we measure the utility of such an information distillation system?",
                "Existing metrics in standard IR, AF and ND are insufficient, and new solutions must be explored, as we will discuss in Section 4, after describing the technical cores of our system in the next section. 3.",
                "TECHNICAL CORES The core components of CAFÂ´E are - 1) AF for incremental learning of query profiles, 2) IR for estimating relevance of passages with respect to query profiles, 3) ND for assessing novelty of passages with respect to users history, and 4) anti-redundancy component to remove redundancy from ranked lists. 3.1 Adaptive Filtering Component We use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for AF [21].",
                "Logistic regression (LR) is a supervised learning algorithm for statistical classification.",
                "Based on a training set of labeled instances, it learns a class model which can then by used to predict the labels of unseen instances.",
                "Its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required, as is the case in adaptive filtering, where the system must learn from each new feedback provided by the user. (See [21] and [23] for computational complexity and implementation issues).",
                "In adaptive filtering, each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query.",
                "For training the model, we use the query itself as the initial positive training example of the class, and the user-highlighted pieces of text (marked as Relevant or Not-relevant) during feedback as additional training examples.",
                "To address the cold start issue in the early stage before any user feedback is obtained, the system uses a small sample from a retrospective corpus as the initial negative examples in the training set.",
                "The details of using logistic regression for adaptive filtering (assigning different weights to positive and negative training instances, and regularizing the objective function to prevent over-fitting on training data) are presented in [21].",
                "The class model wâ learned by Logistic Regression, or the query profile, is a vector whose dimensions are individual terms and whose elements are the regression coefficients, indicating how influential each term is in the query profile.",
                "The query profile is updated whenever a new piece of user feedback is received.",
                "A temporally decaying weight can be applied to each training example, as an option, to emphasize the most recent user feedback. 3.2 Passage Retrieval Component We use standard IR techniques in this part of our system.",
                "Incoming documents are processed in chunks, where each chunk can be defined as a fixed span of time or as a fixed number of documents, as preferred by the user.",
                "For each incoming document, corpus statistics like the IDF (Inverted Document Frequency) of each term are updated.",
                "We use a state-of-the-art named entity identifier and tracker [8, 12] to identify person and location names, and merge them with co-referent named entities seen in the past.",
                "Then the documents are segmented into passages, which can be a whole document, a paragraph, a sentence, or any other continuous span of text, as preferred.",
                "Each passage is represented using a vector of TF-IDF (Term FrequencyInverse Document Frequency) weights, where term can be a word or a named entity.",
                "Given a query profile, i.e. the logistic regression solution wâ as described in Section 3.1, the system computes the posterior probability of relevance for each passage x as fRL(x) â¡ P(y = 1|x, wâ ) = 1 (1 + eâwâÂ·x) (1) Passages are ordered by their relevance scores, and the ones with scores above a threshold (tuned on a training set) comprise the relevance list that is passed on to the novelty detection step. 3.3 Novelty Detection Component CAFÂ´E maintains a user history H(t), which contains all the spans of text hi that the user highlighted (as feedback) during his or her past interactions with the system, up to the current time t. Denoting the history as H(t) = n h1, h2, ..., ht o , (2) the novelty score of a new candidate passage x is computed as: fND(x) = 1 â max iâ1..t {cos(x, hi)} (3) where both candidate passage x and highlighted spans of text hi are represented as TF-IDF vectors.",
                "The novelty score of each passage is compared to a prespecified threshold (also tuned on a training set), and any passage with a score below this threshold is removed from the relevance list. 3.4 Anti-redundant Ranking Component Although the novelty detection component ensures that only novel (previously unseen) information remains in the relevance list, this list might still contain the same novel information at multiple positions in the ranked list.",
                "Suppose, for example, that the user has already read about a $100,000 reward for information about the escaped convicts.",
                "A new piece of news that the award has been increased to $200,000 is novel since the user hasnt read about it yet.",
                "However, multiple news sources would report this news and we might end up showing (redundant) articles from all these sources in a ranked list.",
                "Hence, a ranked list should also be made non-redundant with respect to its own contents.",
                "We use a simplified version of the Maximal Marginal Relevance method [5], originally developed for combining relevance and novelty in text retrieval and summarization.",
                "Our procedure starts with the current list of passages sorted by relevance (section 3.2), filtered by Novelty Detection component (section 3.3), and generates a new non-redundant list as follows: 1.",
                "Take the top passage in the current list as the top one in the new list. 2.",
                "Add the next passage x in the current list to the new list only if fAR(x) > t where fAR(x) = 1 â max piâLnew {cos(x, pi)} and Lnew is the set of passages already selected in the new list. 3.",
                "Repeat step 2 until all the passages in the current list have been examined.",
                "After applying the above-mentioned algorithm, each passage in the new list is sufficiently dissimilar to others, thus favoring diversity rather than redundancy in the new ranked list.",
                "The anti-redundancy threshold t is tuned on a training set. 4.",
                "EVALUATION METHODOLOGY The approach we proposed above for information distillation raises important issues regarding evaluation methodology.",
                "Firstly, since our framework allows the output to be passages at different levels of granularity (e.g. k-sentence windows where k may vary) instead of a fixed length, it is not possible to have pre-annotated relevance judgments at all such granularity levels.",
                "Secondly, since we wish to measure the utility of the system output as a combination of both relevance and novelty, traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time.",
                "Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists.",
                "None of the existing measures in ad-hoc retrieval, adaptive filtering, novelty detection or other related areas (text summarization and question answering) have desirable properties in all the three aspects.",
                "Therefore, we must develop a new evaluation methodology. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers.",
                "Answer keys define what should be present in a system response to receive credit, and are comprised of a collection of information nuggets, i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them.",
                "Defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping [19], since system-returned passages can contain the same information expressed in many different ways.",
                "Hence, QA evaluations have relied on human assessors for the mapping between various expressions, making the process costly, time consuming, and not scalable to large query and document collections, and extensive system evaluations with various parameter settings. 4.1.1 Automating Evaluation based on Answer Keys Automatic evaluation methods would allow for faster system building and tuning, as well as provide an objective and affordable way of comparing various systems.",
                "Recently, such methods have been proposed, more or less, based on the idea of n-gram co-occurrences.",
                "Pourpre [10] assigns a fractional recall score to a system response based on its unigram overlap with a given nuggets description.",
                "For example, a system response A B C has recall 3/4 with respect to a nugget with description A B C D. However, such an approach is unfair to systems that present the same information but using words other than A, B, C, and D. Another open issue is how to weight individual words in measuring the closeness of a match.",
                "For example, consider the question How many prisoners escaped?.",
                "In the nugget Seven prisoners escaped from a Texas prison, there is no indication that seven is the keyword, and that it must be matched to get any relevance credit.",
                "Using IDF values does not help, since seven will generally not have a higher IDF than words like texas and prison.",
                "Also, redefining the nugget as just seven does not solve the problem since now it might spuriously match any mention of seven out of context.",
                "Nuggeteer [13] works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold.",
                "However, it is also plagued by spurious relevance since not all words contained in the nugget description (or known correct responses) are central to the nugget. 4.1.2 Nugget-Matching Rules We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on nugget-matching rules, which are generated using a semiautomatic procedure explained below.",
                "These rules are essentially Boolean queries that will only match against snippets that contain the nugget.",
                "For instance, a candidate rule for matching answers to How many prisoners escaped? is (Texas AND seven AND escape AND (convicts OR prisoners)), possibly with other synonyms and variants in the rule.",
                "For a corpus of news articles, which usually follow a typical formal prose, it is fairly easy to write such simple rules to match expected answers using a bootstrap approach, as described below.",
                "We propose a two-stage approach, inspired by Autoslog [14], that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents.",
                "In the first stage, human subjects annotated (using a highlighting tool) portions of ontopic documents that contained answers to each nugget 2 .",
                "In the second stage, subjects used our rule generation tool to create rules that would match the annotations for each nugget.",
                "The tool allows users to enter a Boolean rule as a disjunction of conjunctions (e.g. ((a AND b) OR (a AND c AND d) OR (e))).",
                "Given a candidate rule, our tool uses it as a Boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match.",
                "Hence, the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision.",
                "We observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions (adding more ANDs), and improving the recall by adding more conjunctions to the disjunction (adding more ORs).",
                "As an example, lets try to create a rule for the nugget which says that seven prisoners escaped from the Texas prison.",
                "We start with a simple rule - (seven).",
                "When we input this into the rule generation tool, we realize that this rule matches many spurious occurrences of seven (e.g. ...seven states...) and thus gets a low precision score.",
                "We can further qualify our rule - Texas AND seven AND convicts.",
                "Next, by looking at the missed annotations, we realize that some news articles mentioned ...seven prisoners escaped.... We then replace convicts with the disjunction (convicts OR prisoners).",
                "We continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the (small number of) misses and false alarms can be safely ignored.",
                "Thus we can create nugget-matching rules that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses.",
                "Human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably. 4.2 Evaluating the Utility of a Sequence of Ranked Lists The utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information, and how much the user lost in terms of time and energy.",
                "We calculate this utility from the utilities of individual passages as follows.",
                "After reading each passage returned by the system, the user derives some gain depending on the presence of relevant and novel information, and incurs a loss in terms of the time and energy spent in going through the passage.",
                "However, the likelihood that the user would actually read a passage depends on its position in the ranked list.",
                "Hence, for a query q, the expected utility 2 LDC [18] already provides relevance judgments for 100 topics on the TDT4 corpus.",
                "We further ensured that these judgments are exhaustive on the entire corpus using pooling. of a passage pi at rank i can be defined as U(pi, q) = P(i) â (Gain(pi, q) â Loss(pi, q)) (4) where P(i) is the probability that the user would go through a passage at rank i.",
                "The expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage: U(q) = nX i=1 P(i) â (Gain(pi, q) â Loss(pi, q)) (5) Note that if we ignore the loss term and define P(i) as P(i) â 1/ logb(b + i â 1) (6) then we get the recently popularized metric called Discounted Cumulated Gain (DCG) [9], where Gain(pi, q) is defined as the graded relevance of passage pi.",
                "However, without the loss term, DCG is a purely recall-oriented metric and not suitable for an adaptive filtering setting, where the systems utility depends in part on its ability to limit the number of items shown to the user.",
                "Although P(i) could be defined based on empirical studies of user behavior, for simplicity, we use P(i) exactly as defined in equation 6.",
                "The gain G(pi, q) of passage pi with respect to the query q is a function of - 1) the number of relevant nuggets present in pi, and 2) the novelty of each of these nuggets.",
                "We combine these two factors as follows.",
                "For each nugget Nj, we assign an initial weight wj, and also keep a count nj of the number of times this nugget has been seen by the user in the past.",
                "The gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor Î³.",
                "Thus, G(pi, q) is defined as G(pi, q) = X Nj âC(pi,q) wj â Î³nj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the answer key of query q.",
                "The initial weights wj are all set of be 1.0 in our experiments, but can also be set based on a pyramid approach [11].",
                "The choice of dampening factor Î³ determines the users tolerance for redundancy.",
                "When Î³ = 0, a nugget will only receive credit for its first occurrence i.e. when nj is zero3 .",
                "For 0 < Î³ < 1, a nugget receives smaller credit for each successive occurrence.",
                "When Î³ = 1, no dampening occurs and repeated occurrences of a nugget receive the same credit.",
                "Note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system, since the users are expected to remember what the system showed them in the past.",
                "We define the loss L(pi, q) as a constant cost c (we use 0.1) incurred when reading a system-returned passage.",
                "Thus, our metric can be re-written as U(q) = nX i=1 Gain(pi, q) logb(b + i â 1) â L(n) (8) where L(n) is the loss associated with a ranked list of length n: L(n) = c Â· nX i=1 1 logb(b + i â 1) (9) 3 Note that 00 = 1 Due to the similarity with Discounted Cumulated Gain (DCG), we call our metric Discounted Cumulated Utility (DCU).",
                "The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) â¤ 0 i.e. when the gain is less than or equal to the cost of reading the passage. 5.",
                "DATA TDT4 was the benchmark corpus used in TDT2002 and TDT2003 evaluations.",
                "The corpus consists of over 90, 000 news articles from multiple sources (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) published between October 2000 and January 2001, in the languages of Arabic, English, and Mandarin.",
                "Speech-recognized and machine-translated versions of the non-English articles were provided as well.",
                "LDC [18] has annotated the corpus with 100 topics, that correspond to various news events in this time period.",
                "Out of these, we selected a subset of 12 actionable events, and defined corresponding tasks for them4 .",
                "For each task, we manually defined a profile consisting of an initial set of (5 to 10) queries, a free-text description of the user history, i.e., what the user already knows about the event, and a list of known on-topic and off-topic documents (if available) as training examples.",
                "For each query, we generated answer keys and corresponding nugget matching rules using the procedure described in section 4.1.2, and produced a total of 120 queries, with an average of 7 nuggets per query. 6.",
                "EXPERIMENTS AND RESULTS 6.1 Baselines We used Indri [17], a popular language-model based retrieval engine, as a baseline for comparison with CAFÂ´E.",
                "Indri supports standard search engine functionality, including pseudo-relevance feedback (PRF) [3, 6], and is representative of a typical query-based retrieval system.",
                "Indri does not support any kind of novelty detection.",
                "We compare Indri with PRF turned on and off, against CAFÂ´E with user feedback, novelty detection and antiredundant ranking turned on and off. 6.2 Experimental Setup We divided the TDT4 corpus spanning 4 months into 10 chunks, each defined as a period of 12 consecutive days.",
                "At any given point of time in the distillation process, each system accessed the past data up to the current point, and returned a ranked list of up 50 passages per query.",
                "The 12 tasks defined on the corpus were divided into a training and test set with 6 tasks each.",
                "Each system was allowed to use the training set to tune its parameters for optimizing NDCU (equation 8), including the relevance threshold for both Indri and CAFÂ´E, and the novelty and antiredundancy thresholds for CAFÂ´E.",
                "The NDCU for each system run is calculated automatically.",
                "User feedback was also simulated - relevance judgments for each system-returned passage (as determined by the nugget matching rules described in section 4.1.2) were 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figure 1: Performance of Indri across chunks Figure 2: Performance of CAFÂ´E across chunks used as user feedback in the adaptation of query profiles and user histories. 6.3 Results In Table 1, we show the NDCU scores of the two systems under various settings.",
                "These scores are averaged over the six tasks in the test set, and are calculated with two dampening factors (see section 4.2): Î³ = 0 and 0.1, to simulate no tolerance and small tolerance for redundancy, respectively.",
                "Using Î³ = 0 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information.",
                "Hence, the improvement obtained from enabling user feedback is smaller with Î³ = 0 than the improvement obtained from feedback with Î³ = 0.1.",
                "This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information.",
                "However, the user does not benefit from seeing such redundant passages, and is usually interested in other passages containing related information.",
                "It is informative to evaluate retrieval systems using our utility measure (with Î³ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.",
                "Sometimes, however, users might indeed be interested in seeing the same information from multiple sources, as an Table 1: NDCU Scores of Indri and CAFÂ´E for two dampening factors (Î³), and various settings (F: Feedback, N: Novelty Detection, A: Anti-Redundant Ranking) Indri CAFÂ´E Î³ Base +PRF Base +F +F+N +F+A +F+N+A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.35 0.36 0.36 indicator of its importance or reliability.",
                "In such a case, we can simply choose a higher value for Î³ which corresponds to a higher tolerance for redundancy, and hence let the system tune its parameters accordingly.",
                "Since documents were processed chunk by chunk, it would be interesting to see how the performance of systems improves over time.",
                "Figures 1 and 2 show the performance trends for both the systems across chunks.",
                "While the performance with and without feedback on the first few chunks is expected to be close, for subsequent chunks, the performance curve with feedback enabled rises above the one with the no-feedback setting.",
                "The performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks, making some queries easier than others in certain chunks.",
                "Moreover, since Indri uses pseudo-relevance feedback while CAFÂ´E uses feedback based on actual relevance judgments, the improvement in case of Indri is less dramatic than that of CAFÂ´E. 7.",
                "CONCLUDING REMARKS This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages.",
                "Our system, called CAFÂ´E, combines adaptive filtering, novelty detection and antiredundant passage ranking in a unified framework for utility optimization.",
                "We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty.",
                "Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off. 8.",
                "ACKNOWLEDGMENTS We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments.",
                "This work is supported in parts by the National Science Foundation (NSF) under grant IIS0434035, and the Defense Advanced Research Project Agency (DARPA) under contracts NBCHD030010 and W0550432.",
                "Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. 9.",
                "ADDITIONAL AUTHORS Jian Zhang (jianzhan@stat.purdue.edu)â , Jaime Carbonell (jgc@cs.cmu.edu)â  , Peter Brusilovsky (peterb+@pitt.edu)â¡ , Daqing He(dah44@pitt.edu)â¡ 10.",
                "REFERENCES [1] J. Allan.",
                "Incremental Relevance Feedback for Information Filtering.",
                "Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 270-278, 1996. [2] J. Allan, C. Wade, and A. Bolivar.",
                "Retrieval and Novelty Detection at the Sentence Level.",
                "Proceedings of the ACM SIGIR conference on research and development in information retrieval, 2003. [3] C. Buckley, G. Salton, and J. Allan.",
                "Automatic Retrieval with Locality Information using SMART.",
                "NIST special publication, (500207):59-72, 1993. [4] J. Callan.",
                "Learning While Filtering Documents.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 224-231, 1998. [5] J. Carbonell and J. Goldstein.",
                "The use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335-336, 1998. [6] E. Efthimiadis.",
                "Query Expansion.",
                "Annual Review of Information Science and Technology (ARIST), 31:p121-87, 1996. [7] J. Fiscus and G. Duddington.",
                "Topic Detection and Tracking Overview.",
                "Topic Detection and Tracking: Event-based Information Organization, pages 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.",
                "A Statistical Model for Multilingual Entity Detection and Tracking.",
                "NAACL/HLT, 2004. [9] K. JÂ¨arvelin and J. KekÂ¨alÂ¨ainen.",
                "Cumulated Gain-based Evaluation of IR Techniques.",
                "ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin and D. Demner-Fushman.",
                "Automatically Evaluating Answers to Definition Questions.",
                "Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), 2005. â Statistics Dept., Purdue University, West Lafayette, USA â  Language Technologies Inst., Carnegie Mellon University, Pittsburgh, USA â¡ School of Information Sciences, Univ. of Pittsburgh, Pittsburgh, USA [11] J. Lin and D. Demner-Fushman.",
                "Will Pyramids Built of nUggets Topple Over.",
                "Proceedings of HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.",
                "A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree.",
                "Proc. of ACL, 4:136-143, 2004. [13] G. Marton.",
                "Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments.",
                "HLT/NAACL, 2006. [14] E. Riloff.",
                "Automatically Constructing a Dictionary for Information Extraction Tasks.",
                "Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, 1993. [15] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9: Filtering track.",
                "The Ninth Text REtrieval Conference (TREC-9), pages 361-368. [16] R. Schapire, Y.",
                "Singer, and A. Singhal.",
                "Boosting and Rocchio Applied to Text Filtering.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle, and W. Croft.",
                "Indri: A Language Model-based Search Engine for Complex Queries.",
                "Proceedings of the International Conference on Intelligence Analysis, 2004. [18] The Linguistic Data Consortium. http://www.ldc.upenn.edu/. [19] E. Voorhees.",
                "Overview of the TREC 2003 Question Answering Track.",
                "Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), 2003. [20] Y. Yang and B. Kisiel.",
                "Margin-based Local Regression for Adaptive Filtering.",
                "Proceedings of the twelfth international conference on Information and knowledge management, pages 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of Adaptive Filtering Methods in a Cross-benchmark Evaluation.",
                "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 98-105, 2005. [22] C. Zhai, W. Cohen, and J. Lafferty.",
                "Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-17, 2003. [23] J. Zhang and Y. Yang.",
                "Robustness of Regularized Linear Classification Methods in Text Categorization.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 190-197, 2003. [24] Y. Zhang.",
                "Using Bayesian Priors to Combine Classifiers for Adaptive Filtering.",
                "Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 345-352, 2004. [25] Y. Zhang, J. Callan, and T. Minka.",
                "Novelty and Redundancy Detection in Adaptive Filtering.",
                "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "evaluation methodology": {
            "translated_key": "metodologÃ­a de evaluaciÃ³n",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Utility-based Information Distillation Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu ABSTRACT This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework.",
                "It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant passage ranking with respect to long-lasting information needs (tasks with multiple queries).",
                "Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings.",
                "For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task.",
                "Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty.",
                "Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval.",
                "Adaptive filtering (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics.",
                "Based on the initial query and a few positive examples (if available), an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user.",
                "The incremental learning nature of AF systems makes them more powerful than standard search engines that support ad-hoc retrieval (e.g.",
                "Google and Yahoo) in terms of finding relevant information with respect to long-lasting topics of interest, and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs, without having to modify their queries manually.",
                "A variety of supervised learning algorithms (Rocchio-style classifiers, Exponential-Gaussian models, local regression and logistic regression approaches) have been studied for adaptive settings, examined with explicit and implicit relevance feedback, and evaluated with respect to utility optimization on large benchmark data collections in TREC (Text Retrieval Conferences) and TDT (Topic Detection and Tracking) forums [1, 4, 7, 15, 16, 20, 24, 23].",
                "Regularized logistic regression [21] has been found representative for the state-of-the-art approaches, and highly efficient for frequent model adaptations over large document collections such as the TREC-10 corpus (over 800,000 documents and 84 topics).",
                "Despite substantial achievements in recent adaptive filtering research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently.",
                "Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications: 1.",
                "User has a rather passive role in the conventional adaptive filtering setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision.",
                "A more active alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents (or passages) per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists.",
                "The latter form of user interaction has been highly effective in standard retrieval for ad-hoc queries.",
                "How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2.",
                "The unit for receiving a relevance judgment (yes or no) is restricted to the document level in conventional AF.",
                "However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant.",
                "Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system.",
                "For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3.",
                "System-selected documents are often highly redundant.",
                "A major news event, for example, would be reported by multiple sources repeatedly for a while, making most of the information content in those articles redundant with each other.",
                "A conventional AF system would select all these redundant news stories for user feedback, wasting the users time while offering little gain.",
                "Clearly, techniques for novelty detection can help in principle [25, 2, 22] for improving the utility of the AF systems.",
                "However, the effectiveness of such techniques at passage level to detect novelty with respect to users (fine-grained) feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user.",
                "To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and novelty detection for removal of redundancy from user interactions with the system.",
                "We call the new process utility-based information distillation.",
                "Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach.",
                "Therefore, we extended a benchmark corpus - the TDT4 collection of news stories and TV broadcasts - with task definitions, multiple queries per task, and answer keys per query.",
                "We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 .",
                "To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses.",
                "Moreover, we propose an extension of NDCG (Normalized Discounted Cumulated Gain) [9] for assessing the utility of ranked passages as a function of both relevance and novelty.",
                "The rest of this paper is organized as follows.",
                "Section 2 outlines the information distillation process with a concrete example.",
                "Section 3 describes the technical cores of our system called CAFÂ´E - CMU Adaptive Filtering Engine.",
                "Section 4 discusses issues with respect to <br>evaluation methodology</br> and proposes a new scheme.",
                "Section 5 describes the extended TDT4 corpus.",
                "Section 6 presents our experiments and results.",
                "Section 7 concludes the study and gives future perspectives. 2.",
                "A SAMPLE TASK Consider a news event - the escape of seven convicts from a Texas prison in December 2000 and their capture a month later.",
                "Assuming a user were interested in this event since its early stage, the information need could be: Find information about the escape of convicts from Texas prison, and information related to their recapture.",
                "The associated lower-level questions could be: 1.",
                "How many prisoners escaped? 2.",
                "Where and when were they sighted? 3. Who are their known contacts inside and outside the prison? 4.",
                "How are they armed? 5.",
                "Do they have any vehicles? 6.",
                "What steps have been taken so far?",
                "We call such an information need a task, and the associated questions as the queries in this task.",
                "A distillation system is supposed to monitor the incoming documents, process them chunk by chunk in a temporal order, select potentially relevant and novel passages from each chunk with respect to each query, and present a ranked list of passages to the user.",
                "Passage ranking here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list.",
                "When presented with a list of passages, the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant.",
                "These spans of text are taken as positive examples in the adaptation of the query profile, and also added to the users history.",
                "Passages not marked by the user are taken as negative examples.",
                "As soon as the query profile is updated, the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low, based on user preference.",
                "For example, if the user highlights ...officials have posted a $100,000 reward for their capture... as relevant answer to the query What steps have been taken so far?, then the highlighted piece is used as an additional positive training example in the adaptation of the query profile.",
                "This piece of feedback is also added to the user history as a seen example, so that in future, the system will not place another passage mentioning $100,000 reward at the top of the ranked list.",
                "However, an article mentioning ...officials have doubled the reward money to $200,000... might be ranked high since it is both relevant to the (updated) query profile and novel with respect to the (updated) user history.",
                "The user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly.",
                "Clearly, novelty detection is very important for the utility of such a system because of the iterative search.",
                "Without novelty detection, the old relevant passages would be shown to the user repeatedly in each ranked list.",
                "Through the above example, we can see the main properties of our new framework for utility-based information distillation over temporally ordered documents.",
                "Our framework combines and extends the power of adaptive filtering (AF), ad-hoc retrieval (IR) and novelty detection (ND).",
                "Compared to standard IR, our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task.",
                "Compared to conventional AF, it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system.",
                "Compared to past work, this is the first evaluation of novelty detection integrated with adaptive filtering for sequenced queries that allows flexible user feedback over ranked passages.",
                "The combination of AF, IR and ND with the new extensions raises an important research question regarding <br>evaluation methodology</br>: how can we measure the utility of such an information distillation system?",
                "Existing metrics in standard IR, AF and ND are insufficient, and new solutions must be explored, as we will discuss in Section 4, after describing the technical cores of our system in the next section. 3.",
                "TECHNICAL CORES The core components of CAFÂ´E are - 1) AF for incremental learning of query profiles, 2) IR for estimating relevance of passages with respect to query profiles, 3) ND for assessing novelty of passages with respect to users history, and 4) anti-redundancy component to remove redundancy from ranked lists. 3.1 Adaptive Filtering Component We use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for AF [21].",
                "Logistic regression (LR) is a supervised learning algorithm for statistical classification.",
                "Based on a training set of labeled instances, it learns a class model which can then by used to predict the labels of unseen instances.",
                "Its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required, as is the case in adaptive filtering, where the system must learn from each new feedback provided by the user. (See [21] and [23] for computational complexity and implementation issues).",
                "In adaptive filtering, each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query.",
                "For training the model, we use the query itself as the initial positive training example of the class, and the user-highlighted pieces of text (marked as Relevant or Not-relevant) during feedback as additional training examples.",
                "To address the cold start issue in the early stage before any user feedback is obtained, the system uses a small sample from a retrospective corpus as the initial negative examples in the training set.",
                "The details of using logistic regression for adaptive filtering (assigning different weights to positive and negative training instances, and regularizing the objective function to prevent over-fitting on training data) are presented in [21].",
                "The class model wâ learned by Logistic Regression, or the query profile, is a vector whose dimensions are individual terms and whose elements are the regression coefficients, indicating how influential each term is in the query profile.",
                "The query profile is updated whenever a new piece of user feedback is received.",
                "A temporally decaying weight can be applied to each training example, as an option, to emphasize the most recent user feedback. 3.2 Passage Retrieval Component We use standard IR techniques in this part of our system.",
                "Incoming documents are processed in chunks, where each chunk can be defined as a fixed span of time or as a fixed number of documents, as preferred by the user.",
                "For each incoming document, corpus statistics like the IDF (Inverted Document Frequency) of each term are updated.",
                "We use a state-of-the-art named entity identifier and tracker [8, 12] to identify person and location names, and merge them with co-referent named entities seen in the past.",
                "Then the documents are segmented into passages, which can be a whole document, a paragraph, a sentence, or any other continuous span of text, as preferred.",
                "Each passage is represented using a vector of TF-IDF (Term FrequencyInverse Document Frequency) weights, where term can be a word or a named entity.",
                "Given a query profile, i.e. the logistic regression solution wâ as described in Section 3.1, the system computes the posterior probability of relevance for each passage x as fRL(x) â¡ P(y = 1|x, wâ ) = 1 (1 + eâwâÂ·x) (1) Passages are ordered by their relevance scores, and the ones with scores above a threshold (tuned on a training set) comprise the relevance list that is passed on to the novelty detection step. 3.3 Novelty Detection Component CAFÂ´E maintains a user history H(t), which contains all the spans of text hi that the user highlighted (as feedback) during his or her past interactions with the system, up to the current time t. Denoting the history as H(t) = n h1, h2, ..., ht o , (2) the novelty score of a new candidate passage x is computed as: fND(x) = 1 â max iâ1..t {cos(x, hi)} (3) where both candidate passage x and highlighted spans of text hi are represented as TF-IDF vectors.",
                "The novelty score of each passage is compared to a prespecified threshold (also tuned on a training set), and any passage with a score below this threshold is removed from the relevance list. 3.4 Anti-redundant Ranking Component Although the novelty detection component ensures that only novel (previously unseen) information remains in the relevance list, this list might still contain the same novel information at multiple positions in the ranked list.",
                "Suppose, for example, that the user has already read about a $100,000 reward for information about the escaped convicts.",
                "A new piece of news that the award has been increased to $200,000 is novel since the user hasnt read about it yet.",
                "However, multiple news sources would report this news and we might end up showing (redundant) articles from all these sources in a ranked list.",
                "Hence, a ranked list should also be made non-redundant with respect to its own contents.",
                "We use a simplified version of the Maximal Marginal Relevance method [5], originally developed for combining relevance and novelty in text retrieval and summarization.",
                "Our procedure starts with the current list of passages sorted by relevance (section 3.2), filtered by Novelty Detection component (section 3.3), and generates a new non-redundant list as follows: 1.",
                "Take the top passage in the current list as the top one in the new list. 2.",
                "Add the next passage x in the current list to the new list only if fAR(x) > t where fAR(x) = 1 â max piâLnew {cos(x, pi)} and Lnew is the set of passages already selected in the new list. 3.",
                "Repeat step 2 until all the passages in the current list have been examined.",
                "After applying the above-mentioned algorithm, each passage in the new list is sufficiently dissimilar to others, thus favoring diversity rather than redundancy in the new ranked list.",
                "The anti-redundancy threshold t is tuned on a training set. 4.",
                "<br>evaluation methodology</br> The approach we proposed above for information distillation raises important issues regarding <br>evaluation methodology</br>.",
                "Firstly, since our framework allows the output to be passages at different levels of granularity (e.g. k-sentence windows where k may vary) instead of a fixed length, it is not possible to have pre-annotated relevance judgments at all such granularity levels.",
                "Secondly, since we wish to measure the utility of the system output as a combination of both relevance and novelty, traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time.",
                "Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists.",
                "None of the existing measures in ad-hoc retrieval, adaptive filtering, novelty detection or other related areas (text summarization and question answering) have desirable properties in all the three aspects.",
                "Therefore, we must develop a new <br>evaluation methodology</br>. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers.",
                "Answer keys define what should be present in a system response to receive credit, and are comprised of a collection of information nuggets, i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them.",
                "Defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping [19], since system-returned passages can contain the same information expressed in many different ways.",
                "Hence, QA evaluations have relied on human assessors for the mapping between various expressions, making the process costly, time consuming, and not scalable to large query and document collections, and extensive system evaluations with various parameter settings. 4.1.1 Automating Evaluation based on Answer Keys Automatic evaluation methods would allow for faster system building and tuning, as well as provide an objective and affordable way of comparing various systems.",
                "Recently, such methods have been proposed, more or less, based on the idea of n-gram co-occurrences.",
                "Pourpre [10] assigns a fractional recall score to a system response based on its unigram overlap with a given nuggets description.",
                "For example, a system response A B C has recall 3/4 with respect to a nugget with description A B C D. However, such an approach is unfair to systems that present the same information but using words other than A, B, C, and D. Another open issue is how to weight individual words in measuring the closeness of a match.",
                "For example, consider the question How many prisoners escaped?.",
                "In the nugget Seven prisoners escaped from a Texas prison, there is no indication that seven is the keyword, and that it must be matched to get any relevance credit.",
                "Using IDF values does not help, since seven will generally not have a higher IDF than words like texas and prison.",
                "Also, redefining the nugget as just seven does not solve the problem since now it might spuriously match any mention of seven out of context.",
                "Nuggeteer [13] works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold.",
                "However, it is also plagued by spurious relevance since not all words contained in the nugget description (or known correct responses) are central to the nugget. 4.1.2 Nugget-Matching Rules We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on nugget-matching rules, which are generated using a semiautomatic procedure explained below.",
                "These rules are essentially Boolean queries that will only match against snippets that contain the nugget.",
                "For instance, a candidate rule for matching answers to How many prisoners escaped? is (Texas AND seven AND escape AND (convicts OR prisoners)), possibly with other synonyms and variants in the rule.",
                "For a corpus of news articles, which usually follow a typical formal prose, it is fairly easy to write such simple rules to match expected answers using a bootstrap approach, as described below.",
                "We propose a two-stage approach, inspired by Autoslog [14], that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents.",
                "In the first stage, human subjects annotated (using a highlighting tool) portions of ontopic documents that contained answers to each nugget 2 .",
                "In the second stage, subjects used our rule generation tool to create rules that would match the annotations for each nugget.",
                "The tool allows users to enter a Boolean rule as a disjunction of conjunctions (e.g. ((a AND b) OR (a AND c AND d) OR (e))).",
                "Given a candidate rule, our tool uses it as a Boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match.",
                "Hence, the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision.",
                "We observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions (adding more ANDs), and improving the recall by adding more conjunctions to the disjunction (adding more ORs).",
                "As an example, lets try to create a rule for the nugget which says that seven prisoners escaped from the Texas prison.",
                "We start with a simple rule - (seven).",
                "When we input this into the rule generation tool, we realize that this rule matches many spurious occurrences of seven (e.g. ...seven states...) and thus gets a low precision score.",
                "We can further qualify our rule - Texas AND seven AND convicts.",
                "Next, by looking at the missed annotations, we realize that some news articles mentioned ...seven prisoners escaped.... We then replace convicts with the disjunction (convicts OR prisoners).",
                "We continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the (small number of) misses and false alarms can be safely ignored.",
                "Thus we can create nugget-matching rules that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses.",
                "Human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably. 4.2 Evaluating the Utility of a Sequence of Ranked Lists The utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information, and how much the user lost in terms of time and energy.",
                "We calculate this utility from the utilities of individual passages as follows.",
                "After reading each passage returned by the system, the user derives some gain depending on the presence of relevant and novel information, and incurs a loss in terms of the time and energy spent in going through the passage.",
                "However, the likelihood that the user would actually read a passage depends on its position in the ranked list.",
                "Hence, for a query q, the expected utility 2 LDC [18] already provides relevance judgments for 100 topics on the TDT4 corpus.",
                "We further ensured that these judgments are exhaustive on the entire corpus using pooling. of a passage pi at rank i can be defined as U(pi, q) = P(i) â (Gain(pi, q) â Loss(pi, q)) (4) where P(i) is the probability that the user would go through a passage at rank i.",
                "The expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage: U(q) = nX i=1 P(i) â (Gain(pi, q) â Loss(pi, q)) (5) Note that if we ignore the loss term and define P(i) as P(i) â 1/ logb(b + i â 1) (6) then we get the recently popularized metric called Discounted Cumulated Gain (DCG) [9], where Gain(pi, q) is defined as the graded relevance of passage pi.",
                "However, without the loss term, DCG is a purely recall-oriented metric and not suitable for an adaptive filtering setting, where the systems utility depends in part on its ability to limit the number of items shown to the user.",
                "Although P(i) could be defined based on empirical studies of user behavior, for simplicity, we use P(i) exactly as defined in equation 6.",
                "The gain G(pi, q) of passage pi with respect to the query q is a function of - 1) the number of relevant nuggets present in pi, and 2) the novelty of each of these nuggets.",
                "We combine these two factors as follows.",
                "For each nugget Nj, we assign an initial weight wj, and also keep a count nj of the number of times this nugget has been seen by the user in the past.",
                "The gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor Î³.",
                "Thus, G(pi, q) is defined as G(pi, q) = X Nj âC(pi,q) wj â Î³nj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the answer key of query q.",
                "The initial weights wj are all set of be 1.0 in our experiments, but can also be set based on a pyramid approach [11].",
                "The choice of dampening factor Î³ determines the users tolerance for redundancy.",
                "When Î³ = 0, a nugget will only receive credit for its first occurrence i.e. when nj is zero3 .",
                "For 0 < Î³ < 1, a nugget receives smaller credit for each successive occurrence.",
                "When Î³ = 1, no dampening occurs and repeated occurrences of a nugget receive the same credit.",
                "Note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system, since the users are expected to remember what the system showed them in the past.",
                "We define the loss L(pi, q) as a constant cost c (we use 0.1) incurred when reading a system-returned passage.",
                "Thus, our metric can be re-written as U(q) = nX i=1 Gain(pi, q) logb(b + i â 1) â L(n) (8) where L(n) is the loss associated with a ranked list of length n: L(n) = c Â· nX i=1 1 logb(b + i â 1) (9) 3 Note that 00 = 1 Due to the similarity with Discounted Cumulated Gain (DCG), we call our metric Discounted Cumulated Utility (DCU).",
                "The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) â¤ 0 i.e. when the gain is less than or equal to the cost of reading the passage. 5.",
                "DATA TDT4 was the benchmark corpus used in TDT2002 and TDT2003 evaluations.",
                "The corpus consists of over 90, 000 news articles from multiple sources (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) published between October 2000 and January 2001, in the languages of Arabic, English, and Mandarin.",
                "Speech-recognized and machine-translated versions of the non-English articles were provided as well.",
                "LDC [18] has annotated the corpus with 100 topics, that correspond to various news events in this time period.",
                "Out of these, we selected a subset of 12 actionable events, and defined corresponding tasks for them4 .",
                "For each task, we manually defined a profile consisting of an initial set of (5 to 10) queries, a free-text description of the user history, i.e., what the user already knows about the event, and a list of known on-topic and off-topic documents (if available) as training examples.",
                "For each query, we generated answer keys and corresponding nugget matching rules using the procedure described in section 4.1.2, and produced a total of 120 queries, with an average of 7 nuggets per query. 6.",
                "EXPERIMENTS AND RESULTS 6.1 Baselines We used Indri [17], a popular language-model based retrieval engine, as a baseline for comparison with CAFÂ´E.",
                "Indri supports standard search engine functionality, including pseudo-relevance feedback (PRF) [3, 6], and is representative of a typical query-based retrieval system.",
                "Indri does not support any kind of novelty detection.",
                "We compare Indri with PRF turned on and off, against CAFÂ´E with user feedback, novelty detection and antiredundant ranking turned on and off. 6.2 Experimental Setup We divided the TDT4 corpus spanning 4 months into 10 chunks, each defined as a period of 12 consecutive days.",
                "At any given point of time in the distillation process, each system accessed the past data up to the current point, and returned a ranked list of up 50 passages per query.",
                "The 12 tasks defined on the corpus were divided into a training and test set with 6 tasks each.",
                "Each system was allowed to use the training set to tune its parameters for optimizing NDCU (equation 8), including the relevance threshold for both Indri and CAFÂ´E, and the novelty and antiredundancy thresholds for CAFÂ´E.",
                "The NDCU for each system run is calculated automatically.",
                "User feedback was also simulated - relevance judgments for each system-returned passage (as determined by the nugget matching rules described in section 4.1.2) were 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figure 1: Performance of Indri across chunks Figure 2: Performance of CAFÂ´E across chunks used as user feedback in the adaptation of query profiles and user histories. 6.3 Results In Table 1, we show the NDCU scores of the two systems under various settings.",
                "These scores are averaged over the six tasks in the test set, and are calculated with two dampening factors (see section 4.2): Î³ = 0 and 0.1, to simulate no tolerance and small tolerance for redundancy, respectively.",
                "Using Î³ = 0 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information.",
                "Hence, the improvement obtained from enabling user feedback is smaller with Î³ = 0 than the improvement obtained from feedback with Î³ = 0.1.",
                "This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information.",
                "However, the user does not benefit from seeing such redundant passages, and is usually interested in other passages containing related information.",
                "It is informative to evaluate retrieval systems using our utility measure (with Î³ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.",
                "Sometimes, however, users might indeed be interested in seeing the same information from multiple sources, as an Table 1: NDCU Scores of Indri and CAFÂ´E for two dampening factors (Î³), and various settings (F: Feedback, N: Novelty Detection, A: Anti-Redundant Ranking) Indri CAFÂ´E Î³ Base +PRF Base +F +F+N +F+A +F+N+A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.35 0.36 0.36 indicator of its importance or reliability.",
                "In such a case, we can simply choose a higher value for Î³ which corresponds to a higher tolerance for redundancy, and hence let the system tune its parameters accordingly.",
                "Since documents were processed chunk by chunk, it would be interesting to see how the performance of systems improves over time.",
                "Figures 1 and 2 show the performance trends for both the systems across chunks.",
                "While the performance with and without feedback on the first few chunks is expected to be close, for subsequent chunks, the performance curve with feedback enabled rises above the one with the no-feedback setting.",
                "The performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks, making some queries easier than others in certain chunks.",
                "Moreover, since Indri uses pseudo-relevance feedback while CAFÂ´E uses feedback based on actual relevance judgments, the improvement in case of Indri is less dramatic than that of CAFÂ´E. 7.",
                "CONCLUDING REMARKS This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages.",
                "Our system, called CAFÂ´E, combines adaptive filtering, novelty detection and antiredundant passage ranking in a unified framework for utility optimization.",
                "We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty.",
                "Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off. 8.",
                "ACKNOWLEDGMENTS We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments.",
                "This work is supported in parts by the National Science Foundation (NSF) under grant IIS0434035, and the Defense Advanced Research Project Agency (DARPA) under contracts NBCHD030010 and W0550432.",
                "Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. 9.",
                "ADDITIONAL AUTHORS Jian Zhang (jianzhan@stat.purdue.edu)â , Jaime Carbonell (jgc@cs.cmu.edu)â  , Peter Brusilovsky (peterb+@pitt.edu)â¡ , Daqing He(dah44@pitt.edu)â¡ 10.",
                "REFERENCES [1] J. Allan.",
                "Incremental Relevance Feedback for Information Filtering.",
                "Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 270-278, 1996. [2] J. Allan, C. Wade, and A. Bolivar.",
                "Retrieval and Novelty Detection at the Sentence Level.",
                "Proceedings of the ACM SIGIR conference on research and development in information retrieval, 2003. [3] C. Buckley, G. Salton, and J. Allan.",
                "Automatic Retrieval with Locality Information using SMART.",
                "NIST special publication, (500207):59-72, 1993. [4] J. Callan.",
                "Learning While Filtering Documents.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 224-231, 1998. [5] J. Carbonell and J. Goldstein.",
                "The use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335-336, 1998. [6] E. Efthimiadis.",
                "Query Expansion.",
                "Annual Review of Information Science and Technology (ARIST), 31:p121-87, 1996. [7] J. Fiscus and G. Duddington.",
                "Topic Detection and Tracking Overview.",
                "Topic Detection and Tracking: Event-based Information Organization, pages 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.",
                "A Statistical Model for Multilingual Entity Detection and Tracking.",
                "NAACL/HLT, 2004. [9] K. JÂ¨arvelin and J. KekÂ¨alÂ¨ainen.",
                "Cumulated Gain-based Evaluation of IR Techniques.",
                "ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin and D. Demner-Fushman.",
                "Automatically Evaluating Answers to Definition Questions.",
                "Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), 2005. â Statistics Dept., Purdue University, West Lafayette, USA â  Language Technologies Inst., Carnegie Mellon University, Pittsburgh, USA â¡ School of Information Sciences, Univ. of Pittsburgh, Pittsburgh, USA [11] J. Lin and D. Demner-Fushman.",
                "Will Pyramids Built of nUggets Topple Over.",
                "Proceedings of HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.",
                "A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree.",
                "Proc. of ACL, 4:136-143, 2004. [13] G. Marton.",
                "Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments.",
                "HLT/NAACL, 2006. [14] E. Riloff.",
                "Automatically Constructing a Dictionary for Information Extraction Tasks.",
                "Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, 1993. [15] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9: Filtering track.",
                "The Ninth Text REtrieval Conference (TREC-9), pages 361-368. [16] R. Schapire, Y.",
                "Singer, and A. Singhal.",
                "Boosting and Rocchio Applied to Text Filtering.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle, and W. Croft.",
                "Indri: A Language Model-based Search Engine for Complex Queries.",
                "Proceedings of the International Conference on Intelligence Analysis, 2004. [18] The Linguistic Data Consortium. http://www.ldc.upenn.edu/. [19] E. Voorhees.",
                "Overview of the TREC 2003 Question Answering Track.",
                "Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), 2003. [20] Y. Yang and B. Kisiel.",
                "Margin-based Local Regression for Adaptive Filtering.",
                "Proceedings of the twelfth international conference on Information and knowledge management, pages 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of Adaptive Filtering Methods in a Cross-benchmark Evaluation.",
                "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 98-105, 2005. [22] C. Zhai, W. Cohen, and J. Lafferty.",
                "Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-17, 2003. [23] J. Zhang and Y. Yang.",
                "Robustness of Regularized Linear Classification Methods in Text Categorization.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 190-197, 2003. [24] Y. Zhang.",
                "Using Bayesian Priors to Combine Classifiers for Adaptive Filtering.",
                "Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 345-352, 2004. [25] Y. Zhang, J. Callan, and T. Minka.",
                "Novelty and Redundancy Detection in Adaptive Filtering.",
                "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002."
            ],
            "original_annotated_samples": [
                "Section 4 discusses issues with respect to <br>evaluation methodology</br> and proposes a new scheme.",
                "The combination of AF, IR and ND with the new extensions raises an important research question regarding <br>evaluation methodology</br>: how can we measure the utility of such an information distillation system?",
                "<br>evaluation methodology</br> The approach we proposed above for information distillation raises important issues regarding <br>evaluation methodology</br>.",
                "Therefore, we must develop a new <br>evaluation methodology</br>. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers."
            ],
            "translated_annotated_samples": [
                "La secciÃ³n 4 discute problemas con respecto a la <br>metodologÃ­a de evaluaciÃ³n</br> y propone un nuevo esquema.",
                "La combinaciÃ³n de AF, IR y ND con las nuevas extensiones plantea una importante pregunta de investigaciÃ³n sobre la <br>metodologÃ­a de evaluaciÃ³n</br>: Â¿cÃ³mo podemos medir la utilidad de un sistema de destilaciÃ³n de informaciÃ³n como este?",
                "METODOLOGÃA DE EVALUACIÃN El enfoque que propusimos anteriormente para la destilaciÃ³n de informaciÃ³n plantea importantes cuestiones sobre la <br>metodologÃ­a de evaluaciÃ³n</br>.",
                "Por lo tanto, debemos desarrollar una nueva <br>metodologÃ­a de evaluaciÃ³n</br>. 4.1 Claves de Respuesta Para permitir la evaluaciÃ³n de un sistema cuya salida consiste en pasajes de longitud arbitraria, tomamos prestado el concepto de claves de respuesta de la comunidad de Preguntas y Respuestas (QA), donde los sistemas pueden devolver fragmentos de texto arbitrarios como respuestas."
            ],
            "translated_text": "DestilaciÃ³n de informaciÃ³n basada en utilidad sobre documentos secuenciados temporalmente en el Instituto de TecnologÃ­as del Lenguaje de Yiming Yang. Universidad Carnegie Mellon, Pittsburgh, EE. UU. yiming@cs.cmu.edu Abhimanyu Lad Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. alad@cs.cmu.edu Instituto de TecnologÃ­as del Lenguaje Ni Lao. Universidad Carnegie Mellon, Pittsburgh, EE. UU. nlao@cs.cmu.edu Abhay Harpale Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. aharpale@cs.cmu.edu Bryan Kisiel Instituto de TecnologÃ­as del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. bkisiel@cs.cmu.edu MÃ³nica Rogati Instituto de TecnologÃ­as del Lenguaje. La Universidad Carnegie Mellon en Pittsburgh, EE. UU. mrogati@cs.cmu.edu RESUMEN Este artÃ­culo examina un nuevo enfoque para la destilaciÃ³n de informaciÃ³n en documentos ordenados temporalmente, y propone un esquema de evaluaciÃ³n novedoso para dicho marco. Combina las fortalezas y se extiende mÃ¡s allÃ¡ del filtrado adaptativo convencional, la detecciÃ³n de novedades y la clasificaciÃ³n de pasajes no redundantes con respecto a las necesidades de informaciÃ³n duraderas (tareas con mÃºltiples consultas). Nuestro enfoque respalda la retroalimentaciÃ³n detallada del usuario a travÃ©s del resaltado de fragmentos arbitrarios de texto, y aprovecha dicha informaciÃ³n para la optimizaciÃ³n de la utilidad en entornos adaptativos. Para nuestros experimentos, definimos tareas hipotÃ©ticas basadas en eventos de noticias en el corpus TDT4, con mÃºltiples consultas por tarea. Se generaron claves de respuestas (nuggets) para cada consulta y se utilizÃ³ un procedimiento semiautomÃ¡tico para adquirir reglas que permiten emparejar automÃ¡ticamente los nuggets con las respuestas del sistema. TambiÃ©n proponemos una extensiÃ³n de la mÃ©trica NDCG para evaluar la utilidad de los pasajes clasificados como una combinaciÃ³n de relevancia y novedad. Nuestros resultados muestran mejoras alentadoras en la utilidad utilizando el nuevo enfoque, en comparaciÃ³n con los sistemas base sin aprendizaje incremental o componentes de detecciÃ³n de novedades. CategorÃ­as y Descriptores de Asignaturas H.3.3 [BÃºsqueda y RecuperaciÃ³n de InformaciÃ³n]: Filtrado de informaciÃ³n, RetroalimentaciÃ³n de relevancia, Modelos de recuperaciÃ³n, Proceso de selecciÃ³n; I.5.2 TÃ©rminos Generales DiseÃ±o, MediciÃ³n, Rendimiento, ExperimentaciÃ³n. 1. INTRODUCCIÃN Seguir informaciÃ³n nueva y relevante de flujos de datos temporales para usuarios con necesidades duraderas ha sido un tema de investigaciÃ³n desafiante en la recuperaciÃ³n de informaciÃ³n. El filtrado adaptativo (AF) es una tarea de predicciÃ³n en lÃ­nea de la relevancia de cada nuevo documento con respecto a temas predefinidos. BasÃ¡ndose en la consulta inicial y algunos ejemplos positivos (si estÃ¡n disponibles), un sistema de AF mantiene un perfil para cada tema de interÃ©s, y lo actualiza constantemente segÃºn la retroalimentaciÃ³n del usuario. La naturaleza de aprendizaje incremental de los sistemas de AF los hace mÃ¡s poderosos que los motores de bÃºsqueda estÃ¡ndar que admiten recuperaciÃ³n ad-hoc (por ejemplo. Google y Yahoo) en tÃ©rminos de encontrar informaciÃ³n relevante con respecto a temas de interÃ©s duraderos, y mÃ¡s atractivo para los usuarios que estÃ¡n dispuestos a proporcionar retroalimentaciÃ³n para adaptar el sistema a sus necesidades de informaciÃ³n especÃ­ficas, sin tener que modificar sus consultas manualmente. Se han estudiado una variedad de algoritmos de aprendizaje supervisado (clasificadores de estilo Rocchio, modelos exponenciales-gaussianos, regresiÃ³n local y enfoques de regresiÃ³n logÃ­stica) para entornos adaptativos, examinados con retroalimentaciÃ³n de relevancia explÃ­cita e implÃ­cita, y evaluados con respecto a la optimizaciÃ³n de la utilidad en grandes colecciones de datos de referencia en los foros de TREC (Conferencias de RecuperaciÃ³n de Texto) y TDT (DetecciÃ³n y Seguimiento de Temas) [1, 4, 7, 15, 16, 20, 24, 23]. La regresiÃ³n logÃ­stica regularizada [21] se ha encontrado representativa para los enfoques de vanguardia, y altamente eficiente para adaptaciones frecuentes del modelo en grandes colecciones de documentos como el corpus TREC-10 (mÃ¡s de 800,000 documentos y 84 temas). A pesar de los logros sustanciales en la investigaciÃ³n reciente de filtrado adaptativo, aÃºn quedan problemas significativos sin resolver sobre cÃ³mo aprovechar de manera efectiva y eficiente la retroalimentaciÃ³n del usuario. EspecÃ­ficamente, los siguientes problemas pueden limitar seriamente la verdadera utilidad de los sistemas de AF en aplicaciones del mundo real: 1. El usuario tiene un papel bastante pasivo en la configuraciÃ³n convencional de filtrado adaptativo: solo reacciona ante el sistema cuando este toma una decisiÃ³n afirmativa sobre un documento, confirmando o rechazando esa decisiÃ³n. Una alternativa mÃ¡s activa serÃ­a permitir al usuario emitir mÃºltiples consultas sobre un tema, revisar una lista clasificada de documentos candidatos (o pasajes) por consulta, y proporcionar retroalimentaciÃ³n sobre la lista clasificada, refinando asÃ­ su necesidad de informaciÃ³n y solicitando listas clasificadas actualizadas. La Ãºltima forma de interacciÃ³n del usuario ha sido altamente efectiva en la recuperaciÃ³n estÃ¡ndar para consultas ad-hoc. CÃ³mo implementar una estrategia para necesidades de informaciÃ³n duraderas en entornos de FA es una pregunta abierta para la investigaciÃ³n. 2. La unidad para recibir un juicio de relevancia (sÃ­ o no) estÃ¡ restringida al nivel del documento en AF convencional. Sin embargo, un usuario real puede estar dispuesto a proporcionar comentarios mÃ¡s informativos y detallados resaltando algunas partes de texto en un documento recuperado como relevantes, en lugar de etiquetar todo el documento como relevante. Aprovechar de manera efectiva este tipo de retroalimentaciÃ³n detallada podrÃ­a mejorar sustancialmente la calidad de un sistema de AF. Para esto, necesitamos habilitar el aprendizaje supervisado a partir de fragmentos de texto etiquetados de longitud arbitraria en lugar de solo permitir documentos etiquetados. 3. Los documentos seleccionados por el sistema suelen ser altamente redundantes. Un evento importante de noticias, por ejemplo, serÃ­a reportado por mÃºltiples fuentes repetidamente durante un tiempo, haciendo que la mayor parte del contenido informativo en esos artÃ­culos sea redundante entre sÃ­. Un sistema de AF convencional seleccionarÃ­a todas estas historias de noticias redundantes para la retroalimentaciÃ³n del usuario, desperdiciando el tiempo de los usuarios mientras ofrece poco beneficio. Claramente, las tÃ©cnicas de detecciÃ³n de novedades pueden ayudar en principio [25, 2, 22] a mejorar la utilidad de los sistemas AF. Sin embargo, la efectividad de tales tÃ©cnicas a nivel de pasaje para detectar novedad con respecto a la retroalimentaciÃ³n de los usuarios a nivel detallado y para detectar redundancia en listas clasificadas aÃºn debe ser evaluada utilizando una medida de utilidad que imite las necesidades de un usuario real. Para abordar las limitaciones actuales de los sistemas de AF mencionadas anteriormente, proponemos y examinamos un nuevo enfoque en este artÃ­culo, que combina las fortalezas del AF convencional (aprendizaje incremental de modelos de temas), recuperaciÃ³n de pasajes multipase para consultas de larga duraciÃ³n condicionadas al tema, y detecciÃ³n de novedades para eliminar la redundancia de las interacciones del usuario con el sistema. Llamamos al nuevo proceso destilaciÃ³n de informaciÃ³n basada en utilidad. Ten en cuenta que los corpus de referencia convencionales para evaluaciones de AF, que tienen juicios de relevancia a nivel de documento y no definen tareas con mÃºltiples consultas, son insuficientes para evaluar el nuevo enfoque. Por lo tanto, ampliamos un corpus de referencia, la colecciÃ³n TDT4 de noticias y transmisiones de televisiÃ³n, con definiciones de tareas, mÃºltiples consultas por tarea y claves de respuestas por consulta. Hemos realizado nuestros experimentos en este corpus extendido TDT4 y hemos puesto a disposiciÃ³n del pÃºblico los datos generados adicionalmente para futuras evaluaciones comparativas. Para evaluar automÃ¡ticamente los fragmentos de texto arbitrarios devueltos por el sistema utilizando nuestras claves de respuestas, desarrollamos un esquema de evaluaciÃ³n con un procedimiento semiautomÃ¡tico para 1 URL: http://nyc.lti.cs.cmu.edu/downloads adquiriendo reglas que pueden comparar fragmentos con las respuestas del sistema. AdemÃ¡s, proponemos una extensiÃ³n de NDCG (Normalized Discounted Cumulated Gain) [9] para evaluar la utilidad de los pasajes clasificados como una funciÃ³n tanto de la relevancia como de la novedad. El resto de este documento estÃ¡ organizado de la siguiente manera. La secciÃ³n 2 describe el proceso de destilaciÃ³n de informaciÃ³n con un ejemplo concreto. La secciÃ³n 3 describe los nÃºcleos tÃ©cnicos de nuestro sistema llamado CAFÂ´E - CMU Adaptive Filtering Engine. La secciÃ³n 4 discute problemas con respecto a la <br>metodologÃ­a de evaluaciÃ³n</br> y propone un nuevo esquema. La secciÃ³n 5 describe el corpus extendido TDT4. La secciÃ³n 6 presenta nuestros experimentos y resultados. La secciÃ³n 7 concluye el estudio y ofrece perspectivas futuras. 2. Una TAREA DE EJEMPLO Considera un evento noticioso: la fuga de siete convictos de una prisiÃ³n de Texas en diciembre de 2000 y su captura un mes despuÃ©s. Suponiendo que un usuario estuviera interesado en este evento desde sus primeras etapas, la necesidad de informaciÃ³n podrÃ­a ser: Encontrar informaciÃ³n sobre la fuga de convictos de una prisiÃ³n de Texas, y la informaciÃ³n relacionada con su recaptura. Las preguntas de nivel inferior asociadas podrÃ­an ser: 1. Â¿CuÃ¡ntos prisioneros escaparon? 2. Â¿DÃ³nde y cuÃ¡ndo fueron avistados? 3. Â¿QuiÃ©nes son sus contactos conocidos dentro y fuera de la prisiÃ³n? 4. Â¿CÃ³mo estÃ¡n armados? 5. Â¿Tienen algÃºn vehÃ­culo? 6. Â¿QuÃ© pasos se han tomado hasta ahora? Llamamos a esta necesidad de informaciÃ³n una tarea, y a las preguntas asociadas como las consultas en esta tarea. Un sistema de destilaciÃ³n debe monitorear los documentos entrantes, procesarlos por fragmentos en un orden temporal, seleccionar pasajes potencialmente relevantes y novedosos de cada fragmento con respecto a cada consulta, y presentar una lista clasificada de pasajes al usuario. La clasificaciÃ³n de pasajes aquÃ­ se basa en cuÃ¡n relevante es un pasaje con respecto a la consulta actual, cuÃ¡n novedoso es con respecto al historial actual del usuario (de sus interacciones con el sistema) y cuÃ¡n redundante es en comparaciÃ³n con otros pasajes con un rango mÃ¡s alto en la lista. Cuando se le presenta una lista de pasajes, el usuario puede proporcionar retroalimentaciÃ³n resaltando tramos de texto arbitrarios que considere relevantes. Estos fragmentos de texto se toman como ejemplos positivos en la adaptaciÃ³n del perfil de consulta, y tambiÃ©n se aÃ±aden al historial de los usuarios. Los pasajes no marcados por el usuario se consideran ejemplos negativos. Tan pronto como se actualiza el perfil de consulta, el sistema vuelve a realizar una bÃºsqueda y devuelve otra lista clasificada de pasajes donde los pasajes previamente vistos son eliminados o clasificados como bajos, segÃºn la preferencia del usuario. Por ejemplo, si el usuario resalta ...los funcionarios han ofrecido una recompensa de $100,000 por su captura... como respuesta relevante a la consulta Â¿QuÃ© pasos se han tomado hasta ahora?, entonces la pieza resaltada se utiliza como un ejemplo de entrenamiento positivo adicional en la adaptaciÃ³n del perfil de la consulta. Este comentario tambiÃ©n se aÃ±ade al historial del usuario como un ejemplo visto, para que en el futuro, el sistema no coloque otro pasaje mencionando una recompensa de $100,000 en la parte superior de la lista clasificada. Sin embargo, un artÃ­culo que mencione que los funcionarios han duplicado el dinero de recompensa a $200,000 podrÃ­a clasificarse alto ya que es relevante tanto para el perfil de consulta (actualizado) como novedoso con respecto a la historia del usuario (actualizada). El usuario puede modificar las consultas originales o agregar una nueva consulta durante el proceso; los perfiles de consulta se cambiarÃ¡n en consecuencia. Claramente, la detecciÃ³n de novedades es muy importante para la utilidad de dicho sistema debido a la bÃºsqueda iterativa. Sin detecciÃ³n de novedad, los antiguos pasajes relevantes se mostrarÃ­an al usuario repetidamente en cada lista clasificada. A travÃ©s del ejemplo anterior, podemos ver las principales propiedades de nuestro nuevo marco de trabajo para la destilaciÃ³n de informaciÃ³n basada en utilidad sobre documentos ordenados temporalmente. Nuestro marco de trabajo combina y amplÃ­a el poder del filtrado adaptativo (AF), la recuperaciÃ³n ad-hoc (IR) y la detecciÃ³n de novedades (ND). En comparaciÃ³n con la IR estÃ¡ndar, nuestro enfoque tiene la capacidad de aprender de forma incremental las necesidades de informaciÃ³n a largo plazo y modelar una secuencia de consultas dentro de una tarea. En comparaciÃ³n con el AF convencional, permite un papel mÃ¡s activo del usuario en refinar sus necesidades de informaciÃ³n y solicitar nuevos resultados al permitir comentarios sobre relevancia y novedad mediante el resaltado de fragmentos arbitrarios de texto en los pasajes devueltos por el sistema. Comparado con trabajos anteriores, esta es la primera evaluaciÃ³n de detecciÃ³n de novedad integrada con filtrado adaptativo para consultas secuenciadas que permite retroalimentaciÃ³n flexible del usuario sobre pasajes clasificados. La combinaciÃ³n de AF, IR y ND con las nuevas extensiones plantea una importante pregunta de investigaciÃ³n sobre la <br>metodologÃ­a de evaluaciÃ³n</br>: Â¿cÃ³mo podemos medir la utilidad de un sistema de destilaciÃ³n de informaciÃ³n como este? Las mÃ©tricas existentes en IR estÃ¡ndar, AF y ND son insuficientes, y se deben explorar nuevas soluciones, como discutiremos en la SecciÃ³n 4, despuÃ©s de describir los nÃºcleos tÃ©cnicos de nuestro sistema en la prÃ³xima secciÃ³n. 3. NÃCLEOS TÃCNICOS Los componentes principales de CAFÃ son: 1) AF para el aprendizaje incremental de perfiles de consulta, 2) IR para estimar la relevancia de pasajes con respecto a los perfiles de consulta, 3) ND para evaluar la novedad de pasajes con respecto al historial de usuarios, y 4) un componente anti-redundancia para eliminar la redundancia de las listas clasificadas. 3.1 Componente de Filtrado Adaptativo Utilizamos un algoritmo de vanguardia en el campo, el mÃ©todo de regresiÃ³n logÃ­stica regularizada que obtuvo los mejores resultados en varios corpus de evaluaciÃ³n de referencia para AF [21]. La regresiÃ³n logÃ­stica (LR) es un algoritmo de aprendizaje supervisado para clasificaciÃ³n estadÃ­stica. BasÃ¡ndose en un conjunto de entrenamiento de instancias etiquetadas, aprende un modelo de clase que luego puede ser utilizado para predecir las etiquetas de instancias no vistas. Su rendimiento, asÃ­ como su eficiencia en tÃ©rminos de tiempo de entrenamiento, lo convierten en un buen candidato cuando se requieren actualizaciones frecuentes del modelo de clase, como es el caso en el filtrado adaptativo, donde el sistema debe aprender de cada nueva retroalimentaciÃ³n proporcionada por el usuario. (Consulte [21] y [23] para complejidad computacional y problemas de implementaciÃ³n). En el filtrado adaptativo, cada consulta se considera como una clase y la probabilidad de que un pasaje pertenezca a esta clase corresponde al grado de relevancia del pasaje con respecto a la consulta. Para entrenar el modelo, utilizamos la propia consulta como ejemplo inicial de entrenamiento positivo de la clase, y las piezas de texto resaltadas por el usuario (marcadas como Relevante o No relevante) durante la retroalimentaciÃ³n como ejemplos de entrenamiento adicionales. Para abordar el problema de inicio en frÃ­o en la etapa inicial antes de obtener cualquier retroalimentaciÃ³n del usuario, el sistema utiliza una pequeÃ±a muestra de un corpus retrospectivo como ejemplos negativos iniciales en el conjunto de entrenamiento. Los detalles de cÃ³mo utilizar la regresiÃ³n logÃ­stica para el filtrado adaptativo (asignando diferentes pesos a las instancias de entrenamiento positivas y negativas, y regularizando la funciÃ³n objetivo para prevenir el sobreajuste en los datos de entrenamiento) se presentan en [21]. El modelo de clase wâ aprendido por RegresiÃ³n LogÃ­stica, o el perfil de consulta, es un vector cuyas dimensiones son tÃ©rminos individuales y cuyos elementos son los coeficientes de regresiÃ³n, indicando cuÃ¡n influyente es cada tÃ©rmino en el perfil de consulta. El perfil de la consulta se actualiza cada vez que se recibe una nueva pieza de retroalimentaciÃ³n del usuario. Un peso decaÃ­do temporalmente se puede aplicar a cada ejemplo de entrenamiento, como una opciÃ³n, para enfatizar la retroalimentaciÃ³n mÃ¡s reciente del usuario. 3.2 Componente de RecuperaciÃ³n de Pasajes Utilizamos tÃ©cnicas estÃ¡ndar de IR en esta parte de nuestro sistema. Los documentos entrantes se procesan por bloques, donde cada bloque puede definirse como un intervalo de tiempo fijo o como un nÃºmero fijo de documentos, segÃºn lo prefiera el usuario. Para cada documento entrante, se actualizan las estadÃ­sticas del corpus como el IDF (Frecuencia Inversa de Documento) de cada tÃ©rmino. Utilizamos un identificador y rastreador de entidades con tecnologÃ­a de punta [8, 12] para identificar nombres de personas y lugares, y fusionarlos con entidades con referencia cruzada vistas en el pasado. Luego, los documentos se dividen en pasajes, que pueden ser un documento completo, un pÃ¡rrafo, una oraciÃ³n, o cualquier otro fragmento continuo de texto, segÃºn se prefiera. Cada pasaje se representa utilizando un vector de pesos TF-IDF (Frecuencia de TÃ©rmino - Frecuencia Inversa de Documento), donde el tÃ©rmino puede ser una palabra o una entidad nombrada. Dado un perfil de consulta, es decir, la soluciÃ³n de regresiÃ³n logÃ­stica wâ como se describe en la SecciÃ³n 3.1, el sistema calcula la probabilidad posterior de relevancia para cada pasaje x como fRL(x) â¡ P(y = 1|x, wâ ) = 1 (1 + eâwâÂ·x) (1). Los pasajes se ordenan segÃºn sus puntajes de relevancia, y aquellos con puntajes por encima de un umbral (ajustado en un conjunto de entrenamiento) conforman la lista de relevancia que se pasa al paso de detecciÃ³n de novedad. 3.3 Componente de DetecciÃ³n de Novedad CAFÂ´E mantiene un historial de usuario H(t), que contiene todos los fragmentos de texto hi que el usuario resaltÃ³ (como retroalimentaciÃ³n) durante sus interacciones pasadas con el sistema, hasta el tiempo actual t. Denotando el historial como H(t) = n h1, h2, ..., ht o , (2), el puntaje de novedad de un nuevo pasaje candidato x se calcula como: fND(x) = 1 â max iâ1..t {cos(x, hi)} (3), donde tanto el pasaje candidato x como los fragmentos de texto resaltados hi se representan como vectores TF-IDF. El puntaje de novedad de cada pasaje se compara con un umbral predefinido (ajustado tambiÃ©n en un conjunto de entrenamiento), y cualquier pasaje con un puntaje por debajo de este umbral se elimina de la lista de relevancia. Componente de clasificaciÃ³n anti-redundante Aunque el componente de detecciÃ³n de novedad garantiza que solo la informaciÃ³n novedosa (no vista previamente) permanezca en la lista de relevancia, esta lista aÃºn podrÃ­a contener la misma informaciÃ³n novedosa en mÃºltiples posiciones en la lista clasificada. Supongamos, por ejemplo, que el usuario ya ha leÃ­do sobre una recompensa de $100,000 por informaciÃ³n sobre los convictos fugados. Una nueva noticia de que el premio ha sido aumentado a $200,000 es novedosa ya que el usuario aÃºn no la ha leÃ­do. Sin embargo, mÃºltiples fuentes de noticias podrÃ­an informar sobre esta noticia y podrÃ­amos terminar mostrando artÃ­culos (redundantes) de todas estas fuentes en una lista clasificada. Por lo tanto, una lista clasificada tambiÃ©n debe ser no redundante con respecto a su propio contenido. Utilizamos una versiÃ³n simplificada del mÃ©todo de MÃ¡xima Relevancia Marginal [5], originalmente desarrollado para combinar relevancia y novedad en la recuperaciÃ³n de texto y resÃºmenes. Nuestro procedimiento comienza con la lista actual de pasajes ordenados por relevancia (secciÃ³n 3.2), filtrados por el componente de DetecciÃ³n de Novedades (secciÃ³n 3.3), y genera una nueva lista no redundante de la siguiente manera: 1. Toma el pasaje superior en la lista actual como el primero en la nueva lista. 2. Agrega el siguiente pasaje x en la lista actual a la nueva lista solo si fAR(x) > t donde fAR(x) = 1 â max piâLnew {cos(x, pi)} y Lnew es el conjunto de pasajes ya seleccionados en la nueva lista. Repetir el paso 2 hasta que se hayan examinado todos los pasajes en la lista actual. DespuÃ©s de aplicar el algoritmo mencionado anteriormente, cada pasaje en la nueva lista es lo suficientemente diferente a los demÃ¡s, favoreciendo asÃ­ la diversidad en lugar de la redundancia en la nueva lista clasificada. El umbral anti-redundancia t se ajusta en un conjunto de entrenamiento. 4. METODOLOGÃA DE EVALUACIÃN El enfoque que propusimos anteriormente para la destilaciÃ³n de informaciÃ³n plantea importantes cuestiones sobre la <br>metodologÃ­a de evaluaciÃ³n</br>. En primer lugar, dado que nuestro marco de trabajo permite que la salida sean fragmentos en diferentes niveles de granularidad (por ejemplo, ventanas de k oraciones donde k puede variar) en lugar de una longitud fija, no es posible tener juicios de relevancia preanotados en todos esos niveles de granularidad. En segundo lugar, dado que deseamos medir la utilidad de la salida del sistema como una combinaciÃ³n de relevancia y novedad, las medidas tradicionales basadas Ãºnicamente en la relevancia deben ser reemplazadas por medidas que penalicen la repeticiÃ³n de la misma informaciÃ³n en la salida del sistema a lo largo del tiempo. En tercer lugar, dado que la salida del sistema son listas clasificadas, debemos recompensar a aquellos sistemas que presenten informaciÃ³n Ãºtil (tanto relevante como no vista previamente) utilizando listas clasificadas mÃ¡s cortas, y penalizar a aquellos que presenten la misma informaciÃ³n utilizando listas clasificadas mÃ¡s largas. Ninguna de las medidas existentes en la recuperaciÃ³n ad-hoc, filtrado adaptativo, detecciÃ³n de novedades u otras Ã¡reas relacionadas (resumen de texto y respuesta a preguntas) tiene propiedades deseables en los tres aspectos. Por lo tanto, debemos desarrollar una nueva <br>metodologÃ­a de evaluaciÃ³n</br>. 4.1 Claves de Respuesta Para permitir la evaluaciÃ³n de un sistema cuya salida consiste en pasajes de longitud arbitraria, tomamos prestado el concepto de claves de respuesta de la comunidad de Preguntas y Respuestas (QA), donde los sistemas pueden devolver fragmentos de texto arbitrarios como respuestas. Las claves de respuestas definen lo que debe estar presente en la respuesta de un sistema para recibir crÃ©dito, y estÃ¡n compuestas por una colecciÃ³n de \"nuggets\" de informaciÃ³n, es decir, unidades de datos sobre las cuales los evaluadores humanos pueden tomar decisiones binarias sobre si una respuesta del sistema las contiene o no. La definiciÃ³n de claves de respuestas y la toma de decisiones binarias asociadas son tareas conceptuales que requieren un mapeo semÃ¡ntico [19], ya que los pasajes devueltos por el sistema pueden contener la misma informaciÃ³n expresada de muchas formas diferentes. Por lo tanto, las evaluaciones de QA han dependido de evaluadores humanos para la correspondencia entre diversas expresiones, lo que hace que el proceso sea costoso, consume mucho tiempo y no sea escalable para grandes colecciones de consultas y documentos, y evaluaciones extensas de sistemas con diversos ajustes de parÃ¡metros. 4.1.1 AutomatizaciÃ³n de la evaluaciÃ³n basada en claves de respuestas Los mÃ©todos de evaluaciÃ³n automÃ¡tica permitirÃ­an una construcciÃ³n y ajuste mÃ¡s rÃ¡pidos del sistema, asÃ­ como proporcionar una forma objetiva y asequible de comparar diversos sistemas. Recientemente, se han propuesto mÃ©todos basados mÃ¡s o menos en la idea de las co-ocurrencias de n-gramos. Pourpre [10] asigna una puntuaciÃ³n de recuerdo fraccional a una respuesta del sistema basada en su superposiciÃ³n de unigramas con una descripciÃ³n de pepitas dada. Por ejemplo, una respuesta del sistema A B C tiene una recuperaciÃ³n de 3/4 con respecto a un fragmento con la descripciÃ³n A B C D. Sin embargo, este enfoque es injusto para los sistemas que presentan la misma informaciÃ³n pero utilizando palabras distintas a A, B, C y D. Otro problema abierto es cÃ³mo ponderar las palabras individuales al medir la cercanÃ­a de una coincidencia. Por ejemplo, considera la pregunta Â¿CuÃ¡ntos prisioneros escaparon? En el fragmento Siete prisioneros escaparon de una prisiÃ³n de Texas, no hay indicaciÃ³n de que siete sea la palabra clave y que debe coincidir para obtener algÃºn crÃ©dito de relevancia. El uso de los valores de IDF no ayuda, ya que la palabra \"seven\" generalmente no tendrÃ¡ un IDF mÃ¡s alto que palabras como \"texas\" y \"prison\". AdemÃ¡s, redefinir el fragmento como simplemente siete no resuelve el problema, ya que ahora podrÃ­a coincidir errÃ³neamente con cualquier menciÃ³n de siete fuera de contexto. Nuggeteer [13] trabaja sobre principios similares pero toma decisiones binarias sobre si hay un fragmento en una respuesta de sistema dada ajustando un umbral. Sin embargo, tambiÃ©n estÃ¡ plagado de relevancia espuria ya que no todas las palabras contenidas en la descripciÃ³n de la pepita (o respuestas correctas conocidas) son centrales para la pepita. 4.1.2 Reglas de Coincidencia de Pepitas Proponemos un mÃ©todo automÃ¡tico confiable para determinar si un fragmento de texto contiene una pepita dada, basado en reglas de coincidencia de pepitas, que se generan utilizando un procedimiento semiautomÃ¡tico explicado a continuaciÃ³n. Estas reglas son bÃ¡sicamente consultas booleanas que solo coincidirÃ¡n con fragmentos que contengan la pepita. Por ejemplo, una regla candidata para emparejar respuestas a Â¿CuÃ¡ntos prisioneros escaparon? es (Texas Y siete Y escapar Y (convictos O prisioneros)), posiblemente con otros sinÃ³nimos y variantes en la regla. Para un corpus de artÃ­culos de noticias, que suelen seguir una prosa formal tÃ­pica, es bastante fÃ¡cil escribir reglas simples para coincidir con respuestas esperadas utilizando un enfoque de arranque, como se describe a continuaciÃ³n. Proponemos un enfoque de dos etapas, inspirado en Autoslog [14], que combina la capacidad de los humanos para identificar expresiones semÃ¡nticamente equivalentes y la capacidad del sistema para recopilar evidencia estadÃ­stica de un corpus de documentos anotados por humanos. En la primera etapa, los sujetos humanos anotaron (utilizando una herramienta de resaltado) porciones de documentos relevantes que contenÃ­an respuestas a cada fragmento 2. En la segunda etapa, los sujetos utilizaron nuestra herramienta de generaciÃ³n de reglas para crear reglas que coincidieran con las anotaciones de cada fragmento. La herramienta permite a los usuarios ingresar una regla booleana como una disyunciÃ³n de conjunciones (por ejemplo, ((a Y b) O (a Y c Y d) O (e))). Dada una regla candidata, nuestra herramienta la utiliza como una consulta booleana sobre el conjunto completo de documentos relevantes y calcula su recall y precisiÃ³n con respecto a las anotaciones que se espera que coincida. Por lo tanto, los sujetos pueden comenzar con una regla simple y refinarla iterativamente hasta que estÃ©n satisfechos con su precisiÃ³n y recuperaciÃ³n. Observamos que era muy fÃ¡cil para los humanos mejorar la precisiÃ³n de una regla ajustando sus conjunciones existentes (agregando mÃ¡s Y) y mejorar la recuperaciÃ³n agregando mÃ¡s conjunciones a la disyunciÃ³n (agregando mÃ¡s O). Como ejemplo, intentemos crear una regla para el acertijo que dice que siete prisioneros escaparon de la prisiÃ³n de Texas. Empezamos con una regla simple - (siete). Cuando introducimos esto en la herramienta de generaciÃ³n de reglas, nos damos cuenta de que esta regla coincide con muchas ocurrencias espurias de siete (por ejemplo, ...siete estados...) y, por lo tanto, obtiene una puntuaciÃ³n baja de precisiÃ³n. Podemos calificar aÃºn mÃ¡s nuestra regla - Texas Y siete Y convictos. A continuaciÃ³n, al revisar las anotaciones omitidas, nos damos cuenta de que algunos artÃ­culos de noticias mencionaban ...siete prisioneros escaparon.... Luego reemplazamos convictos por la disyunciÃ³n (convictos O prisioneros). Continuamos ajustando la regla de esta manera hasta lograr un nivel de recordaciÃ³n y precisiÃ³n suficientemente alto, es decir, que los (pocos) errores y alarmas falsas se puedan ignorar de forma segura. AsÃ­ podemos crear reglas de coincidencia de pepitas que capturen de manera sucinta varias formas de expresar una pepita, evitando al mismo tiempo emparejar respuestas incorrectas (o fuera de contexto). La participaciÃ³n humana en el proceso de creaciÃ³n de reglas garantiza reglas genÃ©ricas de alta calidad que luego se pueden utilizar para evaluar de manera confiable las respuestas del sistema arbitrario. 4.2 EvaluaciÃ³n de la Utilidad de una Secuencia de Listas Clasificadas La utilidad de un sistema de recuperaciÃ³n se puede definir como la diferencia entre cuÃ¡nta informaciÃ³n Ãºtil obtuvo el usuario y cuÃ¡nto tiempo y energÃ­a perdiÃ³. Calculamos esta utilidad a partir de las utilidades de los pasajes individuales de la siguiente manera. DespuÃ©s de leer cada pasaje devuelto por el sistema, el usuario obtiene cierta ganancia dependiendo de la presencia de informaciÃ³n relevante y novedosa, y incurre en una pÃ©rdida en tÃ©rminos del tiempo y la energÃ­a invertidos en revisar el pasaje. Sin embargo, la probabilidad de que el usuario realmente lea un pasaje depende de su posiciÃ³n en la lista clasificada. Por lo tanto, para una consulta q, la utilidad esperada 2 LDC [18] ya proporciona juicios de relevancia para 100 temas en el corpus TDT4. AdemÃ¡s, nos aseguramos de que estos juicios sean exhaustivos en todo el corpus utilizando la agrupaciÃ³n. El valor de un pasaje pi en el rango i se puede definir como U(pi, q) = P(i) â (Ganancia(pi, q) - PÃ©rdida(pi, q)) (4) donde P(i) es la probabilidad de que el usuario pase por un pasaje en el rango i. La utilidad esperada para una lista clasificada completa de longitud n se puede calcular simplemente sumando la utilidad esperada de cada pasaje: U(q) = Î£ i=1 P(i) â (Ganancia(pi, q) â PÃ©rdida(pi, q)) (5) NÃ³tese que si ignoramos el tÃ©rmino de pÃ©rdida y definimos P(i) como P(i) â 1/ logb(b + i â 1) (6) entonces obtenemos la mÃ©trica recientemente popularizada llamada Ganancia Acumulada Descontada (DCG) [9], donde Ganancia(pi, q) se define como la relevancia graduada del pasaje pi. Sin embargo, sin el tÃ©rmino de pÃ©rdida, DCG es una mÃ©trica puramente orientada al recuerdo y no es adecuada para un entorno de filtrado adaptativo, donde la utilidad del sistema depende en parte de su capacidad para limitar el nÃºmero de elementos mostrados al usuario. Aunque P(i) podrÃ­a definirse en base a estudios empÃ­ricos del comportamiento del usuario, por simplicidad, utilizamos P(i) tal como se define exactamente en la ecuaciÃ³n 6. La ganancia G(pi, q) del pasaje pi con respecto a la consulta q es una funciÃ³n de - 1) el nÃºmero de pepitas relevantes presentes en pi, y 2) la novedad de cada una de estas pepitas. Combinamos estos dos factores de la siguiente manera. Para cada pepita Nj, asignamos un peso inicial wj, y tambiÃ©n mantenemos un conteo nj del nÃºmero de veces que esta pepita ha sido vista por el usuario en el pasado. La ganancia derivada de cada ocurrencia posterior del mismo hallazgo se asume que se reduce por un factor de amortiguamiento Î³. Por lo tanto, G(pi, q) se define como G(pi, q) = X Nj âC(pi,q) wj â Î³nj (7) donde C(pi, q) es el conjunto de todas las pepitas que aparecen en el pasaje pi y tambiÃ©n pertenecen a la clave de respuesta de la consulta q. Los pesos iniciales wj se establecen todos en 1.0 en nuestros experimentos, pero tambiÃ©n pueden establecerse segÃºn un enfoque de pirÃ¡mide [11]. La elecciÃ³n del factor de amortiguaciÃ³n Î³ determina la tolerancia del usuario a la redundancia. Cuando Î³ = 0, un pepita solo recibirÃ¡ crÃ©dito por su primera ocurrencia, es decir, cuando nj sea cero. Para 0 < Î³ < 1, un nugget recibe menos crÃ©dito por cada ocurrencia sucesiva. Cuando Î³ = 1, no se produce amortiguamiento y las repeticiones de un valor reciben el mismo crÃ©dito. Se debe tener en cuenta que los recuentos de ocurrencia de las pepitas se conservan entre la evaluaciÃ³n de las listas clasificadas sucesivas devueltas por el sistema, ya que se espera que los usuarios recuerden lo que el sistema les mostrÃ³ en el pasado. Definimos la pÃ©rdida L(pi, q) como un costo constante c (usamos 0.1) incurrido al leer un pasaje devuelto por el sistema. Por lo tanto, nuestra mÃ©trica puede ser reescrita como U(q) = nX i=1 Ganancia(pi, q) logb(b + i â 1) â L(n) (8) donde L(n) es la pÃ©rdida asociada con una lista clasificada de longitud n: L(n) = c Â· nX i=1 1 logb(b + i â 1) (9) 3 NÃ³tese que 00 = 1 Debido a la similitud con la Ganancia Acumulada Descontada (DCG), llamamos a nuestra mÃ©trica Utilidad Acumulada Descontada (DCU). El puntaje de DCU obtenido por el sistema se convierte en un puntaje de DCU normalizado (NDCU) dividiÃ©ndolo por el puntaje de DCU de la lista ideal clasificada, que se crea ordenando los pasajes por sus puntajes de utilidad decrecientes U(pi, q) y deteniÃ©ndose cuando U(pi, q) â¤ 0, es decir, cuando la ganancia es menor o igual al costo de leer el pasaje. El conjunto de datos TDT4 fue el corpus de referencia utilizado en las evaluaciones TDT2002 y TDT2003. El corpus consiste en mÃ¡s de 90,000 artÃ­culos de noticias de mÃºltiples fuentes (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) publicados entre octubre de 2000 y enero de 2001, en los idiomas Ã¡rabe, inglÃ©s y mandarÃ­n. Las versiones de los artÃ­culos en otros idiomas reconocidas por voz y traducidas por mÃ¡quina tambiÃ©n fueron proporcionadas. LDC [18] ha anotado el corpus con 100 temas, que corresponden a varios eventos de noticias en este perÃ­odo de tiempo. De entre estos, seleccionamos un subconjunto de 12 eventos accionables y definimos tareas correspondientes para ellos. Para cada tarea, definimos manualmente un perfil que consiste en un conjunto inicial de (5 a 10) consultas, una descripciÃ³n de texto libre del historial del usuario, es decir, lo que el usuario ya sabe sobre el evento, y una lista de documentos conocidos relevantes y no relevantes (si estÃ¡n disponibles) como ejemplos de entrenamiento. Para cada consulta, generamos claves de respuestas y reglas de coincidencia de pepitas correspondientes utilizando el procedimiento descrito en la secciÃ³n 4.1.2, y produjimos un total de 120 consultas, con un promedio de 7 pepitas por consulta. EXPERIMENTOS Y RESULTADOS 6.1 Baselines Utilizamos Indri [17], un motor de bÃºsqueda basado en modelos de lenguaje popular, como referencia para comparar con CAFÃ. Indri admite la funcionalidad estÃ¡ndar de los motores de bÃºsqueda, incluido el feedback de pseudo-relevancia (PRF) [3, 6], y es representativo de un sistema de recuperaciÃ³n basado en consultas tÃ­pico. Indri no admite ningÃºn tipo de detecciÃ³n de novedades. Comparamos Indri con PRF activado y desactivado, frente a CAFÃ con retroalimentaciÃ³n del usuario, detecciÃ³n de novedades y clasificaciÃ³n antirredundante activados y desactivados. 6.2 ConfiguraciÃ³n Experimental Dividimos el corpus TDT4 que abarca 4 meses en 10 fragmentos, cada uno definido como un perÃ­odo de 12 dÃ­as consecutivos. En cualquier momento dado del proceso de destilaciÃ³n, cada sistema accedÃ­a a los datos pasados hasta el punto actual, y devolvÃ­a una lista clasificada de hasta 50 pasajes por consulta. Las 12 tareas definidas en el corpus se dividieron en un conjunto de entrenamiento y prueba con 6 tareas cada uno. Cada sistema pudo utilizar el conjunto de entrenamiento para ajustar sus parÃ¡metros con el fin de optimizar NDCU (ecuaciÃ³n 8), incluyendo el umbral de relevancia tanto para Indri como para CAFÂ´E, y los umbrales de novedad y antirredundancia para CAFÂ´E. La NDCU para cada ejecuciÃ³n del sistema se calcula automÃ¡ticamente. TambiÃ©n se simulÃ³ la retroalimentaciÃ³n de los usuarios: los juicios de relevancia para cada pasaje devuelto por el sistema (segÃºn las reglas de coincidencia de pepitas descritas en la secciÃ³n 4.1.2) fueron utilizados como retroalimentaciÃ³n de los usuarios en la adaptaciÃ³n de perfiles de consulta e historiales de usuario. En la Tabla 1, mostramos los puntajes NDCU de los dos sistemas en diferentes configuraciones. Estas puntuaciones se promedian en las seis tareas del conjunto de pruebas, y se calculan con dos factores de amortiguaciÃ³n (ver secciÃ³n 4.2): Î³ = 0 y 0.1, para simular ninguna tolerancia y poca tolerancia para la redundancia, respectivamente. Usar Î³ = 0 crea una mÃ©trica mucho mÃ¡s estricta ya que no otorga ningÃºn crÃ©dito a un pasaje que contenga informaciÃ³n relevante pero redundante. Por lo tanto, la mejora obtenida al habilitar la retroalimentaciÃ³n del usuario es menor con Î³ = 0 que la mejora obtenida de la retroalimentaciÃ³n con Î³ = 0.1. Esto revela una deficiencia de los sistemas de recuperaciÃ³n contemporÃ¡neos: cuando el usuario da retroalimentaciÃ³n positiva sobre un pasaje, los sistemas otorgan mayor peso a los tÃ©rminos presentes en ese pasaje y tienden a recuperar otros pasajes que contienen los mismos tÃ©rminos, y por lo tanto, generalmente la misma informaciÃ³n. Sin embargo, el usuario no se beneficia al ver pasajes redundantes y generalmente estÃ¡ interesado en otros pasajes que contengan informaciÃ³n relacionada. Es informativo evaluar los sistemas de recuperaciÃ³n utilizando nuestra medida de utilidad (con Î³ = 0) que tiene en cuenta la novedad y, por lo tanto, ofrece una imagen mÃ¡s realista de cuÃ¡n bien un sistema puede generalizar a partir de la retroalimentaciÃ³n del usuario, en lugar de utilizar medidas tradicionales de RI como la recuperaciÃ³n y la precisiÃ³n, que ofrecen una imagen incompleta de la mejora obtenida a partir de la retroalimentaciÃ³n del usuario. A veces, sin embargo, los usuarios podrÃ­an estar interesados en ver la misma informaciÃ³n de mÃºltiples fuentes, como un indicador de su importancia o fiabilidad. En tal caso, simplemente podemos elegir un valor mÃ¡s alto para Î³ que corresponda a una mayor tolerancia para la redundancia, y asÃ­ permitir que el sistema ajuste sus parÃ¡metros en consecuencia. Dado que los documentos fueron procesados por fragmentos, serÃ­a interesante ver cÃ³mo mejora el rendimiento de los sistemas con el tiempo. Las figuras 1 y 2 muestran las tendencias de rendimiento de ambos sistemas a lo largo de los fragmentos. Si bien se espera que el rendimiento con y sin retroalimentaciÃ³n en los primeros fragmentos sea similar, para los fragmentos posteriores, la curva de rendimiento con la retroalimentaciÃ³n habilitada supera a la del ajuste sin retroalimentaciÃ³n. Las tendencias de rendimiento no son consistentes en todos los fragmentos porque los documentos relevantes no estÃ¡n distribuidos uniformemente en todos los fragmentos, lo que hace que algunas consultas sean mÃ¡s fÃ¡ciles que otras en ciertos fragmentos. AdemÃ¡s, dado que Indri utiliza retroalimentaciÃ³n de pseudo-relevancia mientras que CAFÂ´E utiliza retroalimentaciÃ³n basada en juicios de relevancia reales, la mejora en el caso de Indri es menos dramÃ¡tica que la de CAFÂ´E. 7. CONCLUSIONES FINALES Este artÃ­culo presenta la primera investigaciÃ³n sobre la destilaciÃ³n de informaciÃ³n basada en la utilidad con un sistema que aprende las necesidades de informaciÃ³n duraderas a partir de la retroalimentaciÃ³n detallada del usuario sobre una secuencia de pasajes clasificados. Nuestro sistema, llamado CAFÂ´E, combina filtrado adaptativo, detecciÃ³n de novedades y clasificaciÃ³n de pasajes antirredundantes en un marco unificado para la optimizaciÃ³n de la utilidad. Desarrollamos un nuevo esquema para la evaluaciÃ³n y retroalimentaciÃ³n automatizadas basado en un procedimiento semiautomÃ¡tico para adquirir reglas que permitan emparejar automÃ¡ticamente fragmentos con las respuestas del sistema. TambiÃ©n propusimos una extensiÃ³n de la mÃ©trica NDCG para evaluar la utilidad de los pasajes clasificados como una combinaciÃ³n ponderada de relevancia y novedad. Nuestros experimentos en el reciÃ©n anotado corpus de referencia TDT4 muestran un alentador aumento de utilidad sobre Indri, y tambiÃ©n sobre nuestro propio sistema con aprendizaje incremental y detecciÃ³n de novedades desactivados. 8. AGRADECIMIENTOS Nos gustarÃ­a agradecer a Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng y al Programa de AnÃ¡lisis de Datos Cualitativos de la Universidad de Pittsburgh liderado por el Dr. Stuart Shulman por su ayuda en la recolecciÃ³n y procesamiento de las anotaciones extendidas de TDT4 utilizadas en nuestros experimentos. Este trabajo estÃ¡ parcialmente respaldado por la FundaciÃ³n Nacional de Ciencias (NSF) bajo la subvenciÃ³n IIS0434035, y la Agencia de Proyectos de InvestigaciÃ³n Avanzada de Defensa (DARPA) bajo los contratos NBCHD030010 y W0550432. Cualquier opiniÃ³n, hallazgo, conclusiÃ³n o recomendaciÃ³n expresada en este material son responsabilidad de los autores y no reflejan necesariamente los puntos de vista de los patrocinadores. AUTORES ADICIONALES Jian Zhang (jianzhan@stat.purdue.edu)â , Jaime Carbonell (jgc@cs.cmu.edu)â  , Peter Brusilovsky (peterb+@pitt.edu)â¡ , Daqing He(dah44@pitt.edu)â¡ 10. REFERENCIAS [1] J. Allan. RetroalimentaciÃ³n incremental de relevancia para la filtraciÃ³n de informaciÃ³n. Actas de la 19Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 270-278, 1996. [2] J. Allan, C. Wade y A. Bolivar. RecuperaciÃ³n y DetecciÃ³n de Novedad a Nivel de OraciÃ³n. Actas de la conferencia ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, 2003. [3] C. Buckley, G. Salton y J. Allan. RecuperaciÃ³n automÃ¡tica con informaciÃ³n de localidad utilizando SMART. PublicaciÃ³n especial del NIST, (500207):59-72, 1993. [4] J. Callan. Aprendiendo mientras se filtran documentos. Actas de la 21Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 224-231, 1998. [5] J. Carbonell y J. Goldstein. El uso de MMR, Reordenamiento basado en Diversidad para Reorganizar Documentos y Producir ResÃºmenes. Actas de la 21Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 335-336, 1998. [6] E. Efthimiadis. ExpansiÃ³n de consulta. RevisiÃ³n Anual de Ciencia de la InformaciÃ³n y TecnologÃ­a (ARIST), 31:p121-87, 1996. [7] J. Fiscus y G. Duddington. Resumen de DetecciÃ³n y Seguimiento de Temas. DetecciÃ³n y seguimiento de temas: OrganizaciÃ³n de la informaciÃ³n basada en eventos, pÃ¡ginas 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov y S. Roukos. Un modelo estadÃ­stico para la detecciÃ³n y seguimiento de entidades multilingÃ¼es. NAACL/HLT, 2004. [9] K. JÃ¤rvelin y J. KekÃ¤lÃ¤inen. EvaluaciÃ³n basada en la Ganancia Acumulada de TÃ©cnicas de RecuperaciÃ³n de InformaciÃ³n. ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin y D. Demner-Fushman. EvaluaciÃ³n automÃ¡tica de respuestas a preguntas de definiciÃ³n. Actas de la Conferencia de TecnologÃ­a del Lenguaje Humano y Conferencia sobre MÃ©todos EmpÃ­ricos en Procesamiento del Lenguaje Natural (HLT/EMNLP 2005), 2005. â Departamento de EstadÃ­stica, Universidad de Purdue, West Lafayette, EE. UU. â  Instituto de TecnologÃ­as del Lenguaje, Universidad Carnegie Mellon, Pittsburgh, EE. UU. â¡ Escuela de Ciencias de la InformaciÃ³n, Universidad de Pittsburgh, Pittsburgh, EE. UU. [11] J. Lin y D. Demner-Fushman. Â¿Se derrumbarÃ¡n las pirÃ¡mides construidas con pepitas? Actas de HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla y S. Roukos. Un algoritmo de resoluciÃ³n de correferencia sÃ­ncrona basado en el Ã¡rbol de Bell. Actas de ACL, 4:136-143, 2004. [13] G. Marton. Nuggeteer: EvaluaciÃ³n AutomÃ¡tica Basada en Pepitas Utilizando Descripciones y Juicios. HLT/NAACL, 2006. [14] E. Riloff. ConstrucciÃ³n automÃ¡tica de un diccionario para tareas de extracciÃ³n de informaciÃ³n. Actas de la UndÃ©cima Conferencia Nacional sobre Inteligencia Artificial, pÃ¡ginas 811-816, 1993. [15] S. Robertson y S. Walker. Microsoft Cambridge en TREC-9: Pista de filtrado. La Novena Conferencia de RecuperaciÃ³n de Texto (TREC-9), pÃ¡ginas 361-368. [16] R. Schapire, Y. Cantante, y A. Singhal. Boosting y Rocchio aplicados a la filtraciÃ³n de texto. Actas de la 21Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle y W. Croft. Indri: Un motor de bÃºsqueda basado en modelos de lenguaje para consultas complejas. Actas de la Conferencia Internacional sobre AnÃ¡lisis de Inteligencia, 2004. [18] El Consorcio de Datos LingÃ¼Ã­sticos. http://www.ldc.upenn.edu/. [19] E. Voorhees. Resumen de la pista de preguntas y respuestas TREC 2003. Actas de la DuodÃ©cima Conferencia de RecuperaciÃ³n de Texto (TREC 2003), 2003. [20] Y. Yang y B. Kisiel. RegresiÃ³n local basada en mÃ¡rgenes para filtrado adaptativo. Actas de la duodÃ©cima conferencia internacional sobre gestiÃ³n de la informaciÃ³n y el conocimiento, pÃ¡ginas 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang y B. Kisiel. Robustez de los MÃ©todos de Filtrado Adaptativo en una EvaluaciÃ³n de Referencia Cruzada. Actas de la 28Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 98-105, 2005. [22] C. Zhai, W. Cohen y J. Lafferty. MÃ¡s allÃ¡ de la relevancia independiente: MÃ©todos y mÃ©tricas de evaluaciÃ³n para la recuperaciÃ³n de subtemas. Actas de la 26Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 10-17, 2003. [23] J. Zhang y Y. Yang. Robustez de los MÃ©todos de ClasificaciÃ³n Lineal Regularizados en la CategorizaciÃ³n de Textos. Actas de la 26Âª conferencia internacional anual de ACM SIGIR sobre investigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 190-197, 2003. [24] Y. Zhang. Utilizando priors bayesianos para combinar clasificadores para filtrado adaptativo. Actas de la 27Âª conferencia internacional anual sobre InvestigaciÃ³n y desarrollo en recuperaciÃ³n de informaciÃ³n, pÃ¡ginas 345-352, 2004. [25] Y. Zhang, J. Callan y T. Minka. DetecciÃ³n de novedad y redundancia en el filtrado adaptativo. Actas de la 25Âª Conferencia Internacional Anual de ACM SIGIR sobre InvestigaciÃ³n y Desarrollo en RecuperaciÃ³n de InformaciÃ³n, 2002. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        }
    }
}