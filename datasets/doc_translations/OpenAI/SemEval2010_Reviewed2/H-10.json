{
    "id": "H-10",
    "original_text": "Regularized Clustering for Documents ∗ Fei Wang, Changshui Zhang State Key Lab of Intelligent Tech. and Systems Department of Automation, Tsinghua University Beijing, China, 100084 feiwang03@gmail.com Tao Li School of Computer Science Florida International University Miami, FL 33199, U.S.A. taoli@cs.fiu.edu ABSTRACT In recent years, document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering. In this paper, we propose a novel method for clustering documents using regularization. Unlike traditional globally regularized clustering methods, our method first construct a local regularized linear label predictor for each document vector, and then combine all those local regularizers with a global smoothness regularizer. So we call our algorithm Clustering with Local and Global Regularization (CLGR). We will show that the cluster memberships of the documents can be achieved by eigenvalue decomposition of a sparse symmetric matrix, which can be efficiently solved by iterative methods. Finally our experimental evaluations on several datasets are presented to show the superiorities of CLGR over traditional document clustering methods. Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Clustering; I.2.6 [Artificial Intelligence]: Learning-Concept Learning General Terms Algorithms 1. INTRODUCTION Document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering. A good document clustering approach can assist the computers to automatically organize the document corpus into a meaningful cluster hierarchy for efficient browsing and navigation, which is very valuable for complementing the deficiencies of traditional information retrieval technologies. As pointed out by [8], the information retrieval needs can be expressed by a spectrum ranged from narrow keyword-matching based search to broad information browsing such as what are the major international events in recent months. Traditional document retrieval engines tend to fit well with the search end of the spectrum, i.e. they usually provide specified search for documents matching the users query, however, it is hard for them to meet the needs from the rest of the spectrum in which a rather broad or vague information is needed. In such cases, efficient browsing through a good cluster hierarchy will be definitely helpful. Generally, document clustering methods can be mainly categorized into two classes: hierarchical methods and partitioning methods. The hierarchical methods group the data points into a hierarchical tree structure using bottom-up or top-down approaches. For example, hierarchical agglomerative clustering (HAC) [13] is a typical bottom-up hierarchical clustering method. It takes each data point as a single cluster to start off with and then builds bigger and bigger clusters by grouping similar data points together until the entire dataset is encapsulated into one final cluster. On the other hand, partitioning methods decompose the dataset into a number of disjoint clusters which are usually optimal in terms of some predefined criterion functions. For instance, K-means [13] is a typical partitioning method which aims to minimize the sum of the squared distance between the data points and their corresponding cluster centers. In this paper, we will focus on the partitioning methods. As we know that there are two main problems existing in partitioning methods (like Kmeans and Gaussian Mixture Model (GMM) [16]): (1) the predefined criterion is usually non-convex which causes many local optimal solutions; (2) the iterative procedure (e.g. the Expectation Maximization (EM) algorithm) for optimizing the criterions usually makes the final solutions heavily depend on the initializations. In the last decades, many methods have been proposed to overcome the above problems of the partitioning methods [19][28]. Recently, another type of partitioning methods based on clustering on data graphs have aroused considerable interests in the machine learning and data mining community. The basic idea behind these methods is to first model the whole dataset as a weighted graph, in which the graph nodes represent the data points, and the weights on the edges correspond to the similarities between pairwise points. Then the cluster assignments of the dataset can be achieved by optimizing some criterions defined on the graph. For example Spectral Clustering is one kind of the most representative graph-based clustering approaches, it generally aims to optimize some cut value (e.g. Normalized Cut [22], Ratio Cut [7], Min-Max Cut [11]) defined on an undirected graph. After some relaxations, these criterions can usually be optimized via eigen-decompositions, which is guaranteed to be global optimal. In this way, spectral clustering efficiently avoids the problems of the traditional partitioning methods as we introduced in last paragraph. In this paper, we propose a novel document clustering algorithm that inherits the superiority of spectral clustering, i.e. the final cluster results can also be obtained by exploit the eigen-structure of a symmetric matrix. However, unlike spectral clustering, which just enforces a smoothness constraint on the data labels over the whole data manifold [2], our method first construct a regularized linear label predictor for each data point from its neighborhood as in [25], and then combine the results of all these local label predictors with a global label smoothness regularizer. So we call our method Clustering with Local and Global Regularization (CLGR). The idea of incorporating both local and global information into label prediction is inspired by the recent works on semi-supervised learning [31], and our experimental evaluations on several real document datasets show that CLGR performs better than many state-of-the-art clustering methods. The rest of this paper is organized as follows: in section 2 we will introduce our CLGR algorithm in detail. The experimental results on several datasets are presented in section 3, followed by the conclusions and discussions in section 4. 2. THE PROPOSED ALGORITHM In this section, we will introduce our Clustering with Local and Global Regularization (CLGR) algorithm in detail. First lets see the how the documents are represented throughout this paper. 2.1 Document Representation In our work, all the documents are represented by the weighted term-frequency vectors. Let W = {w1, w2, · · · , wm} be the complete vocabulary set of the document corpus (which is preprocessed by the stopwords removal and words stemming operations). The term-frequency vector xi of document di is defined as xi = [xi1, xi2, · · · , xim]T , xik = tik log n idfk , where tik is the term frequency of wk ∈ W, n is the size of the document corpus, idfk is the number of documents that contain word wk. In this way, xi is also called the TFIDF representation of document di. Furthermore, we also normalize each xi (1 i n) to have a unit length, so that each document is represented by a normalized TF-IDF vector. 2.2 Local Regularization As its name suggests, CLGR is composed of two parts: local regularization and global regularization. In this subsection we will introduce the local regularization part in detail. 2.2.1 Motivation As we know that clustering is one type of learning techniques, it aims to organize the dataset in a reasonable way. Generally speaking, learning can be posed as a problem of function estimation, from which we can get a good classification function that will assign labels to the training dataset and even the unseen testing dataset with some cost minimized [24]. For example, in the two-class classification scenario1 (in which we exactly know the label of each document), a linear classifier with least square fit aims to learn a column vector w such that the squared cost J = 1 n (wT xi − yi)2 (1) is minimized, where yi ∈ {+1, −1} is the label of xi. By taking ∂J /∂w = 0, we get the solution w∗ = n i=1 xixT i −1 n i=1 xiyi , (2) which can further be written in its matrix form as w∗ = XXT −1 Xy, (3) where X = [x1, x2, · · · , xn] is an m × n document matrix, y = [y1, y2, · · · , yn]T is the label vector. Then for a test document t, we can determine its label by l = sign(w∗T u), (4) where sign(·) is the sign function. A natural problem in Eq.(3) is that the matrix XXT may be singular and thus not invertable (e.g. when m n). To avoid such a problem, we can add a regularization term and minimize the following criterion J = 1 n n i=1 (wT xi − yi)2 + λ w 2 , (5) where λ is a regularization parameter. Then the optimal solution that minimize J is given by w∗ = XXT + λnI −1 Xy, (6) where I is an m × m identity matrix. It has been reported that the regularized linear classifier can achieve very good results on text classification problems [29]. However, despite its empirical success, the regularized linear classifier is on earth a global classifier, i.e. w∗ is estimated using the whole training set. According to [24], this may not be a smart idea, since a unique w∗ may not be good enough for predicting the labels of the whole input space. In order to get better predictions, [6] proposed to train classifiers locally and use them to classify the testing points. For example, a testing point will be classified by the local classifier trained using the training points located in the vicinity 1 In the following discussions we all assume that the documents coming from only two classes. The generalizations of our method to multi-class cases will be discussed in section 2.5. of it. Although this method seems slow and stupid, it is reported that it can get better performances than using a unique global classifier on certain tasks [6]. 2.2.2 Constructing the Local Regularized Predictors Inspired by their success, we proposed to apply the local learning algorithms for clustering. The basic idea is that, for each document vector xi (1 i n), we train a local label predictor based on its k-nearest neighborhood Ni, and then use it to predict the label of xi. Finally we will combine all those local predictors by minimizing the sum of their prediction errors. In this subsection we will introduce how to construct those local predictors. Due to the simplicity and effectiveness of the regularized linear classifier that we have introduced in section 2.2.1, we choose it to be our local label predictor, such that for each document xi, the following criterion is minimized Ji = 1 ni xj ∈Ni wT i xj − qj 2 + λi wi 2 , (7) where ni = |Ni| is the cardinality of Ni, and qj is the cluster membership of xj. Then using Eq.(6), we can get the optimal solution is w∗ i = XiXT i + λiniI −1 Xiqi, (8) where Xi = [xi1, xi2, · · · , xini ], and we use xik to denote the k-th nearest neighbor of xi. qi = [qi1, qi2, · · · , qini ]T with qik representing the cluster assignment of xik. The problem here is that XiXT i is an m × m matrix with m ni, i.e. we should compute the inverse of an m × m matrix for every document vector, which is computationally prohibited. Fortunately, we have the following theorem: Theorem 1. w∗ i in Eq.(8) can be rewritten as w∗ i = Xi XT i Xi + λiniIi −1 qi, (9) where Ii is an ni × ni identity matrix. Proof. Since w∗ i = XiXT i + λiniI −1 Xiqi, then XiXT i + λiniI w∗ i = Xiqi =⇒ XiXT i w∗ i + λiniw∗ i = Xiqi =⇒ w∗ i = (λini)−1 Xi qi − XT i w∗ i . Let β = (λini)−1 qi − XT i w∗ i , then w∗ i = Xiβ =⇒ λiniβ = qi − XT i w∗ i = qi − XT i Xiβ =⇒ qi = XT i Xi + λiniIi β =⇒ β = XT i Xi + λiniIi −1 qi. Therefore w∗ i = Xiβ = Xi XT i Xi + λiniIi −1 qi 2 Using theorem 1, we only need to compute the inverse of an ni × ni matrix for every document to train a local label predictor. Moreover, for a new testing point u that falls into Ni, we can classify it by the sign of qu = w∗T i u = uT wi = uT Xi XT i Xi + λiniIi −1 qi. This is an attractive expression since we can determine the cluster assignment of u by using the inner-products between the points in {u ∪ Ni}, which suggests that such a local regularizer can easily be kernelized [21] as long as we define a proper kernel function. 2.2.3 Combining the Local Regularized Predictors After all the local predictors having been constructed, we will combine them together by minimizing Jl = n i=1 w∗T i xi − qi 2 , (10) which stands for the sum of the prediction errors for all the local predictors. Combining Eq.(10) with Eq.(6), we can get Jl = n i=1 w∗T i xi − qi 2 = n i=1 xT i Xi XT i Xi + λiniIi −1 qi − qi 2 = Pq − q 2 , (11) where q = [q1, q2, · · · , qn]T , and the P is an n × n matrix constructing in the following way. Let αi = xT i Xi XT i Xi + λiniIi −1 , then Pij = αi j, if xj ∈ Ni 0, otherwise , (12) where Pij is the (i, j)-th entry of P, and αi j represents the j-th entry of αi . Till now we can write the criterion of clustering by combining locally regularized linear label predictors Jl in an explicit mathematical form, and we can minimize it directly using some standard optimization techniques. However, the results may not be good enough since we only exploit the local informations of the dataset. In the next subsection, we will introduce a global regularization criterion and combine it with Jl, which aims to find a good clustering result in a local-global way. 2.3 Global Regularization In data clustering, we usually require that the cluster assignments of the data points should be sufficiently smooth with respect to the underlying data manifold, which implies (1) the nearby points tend to have the same cluster assignments; (2) the points on the same structure (e.g. submanifold or cluster) tend to have the same cluster assignments [31]. Without the loss of generality, we assume that the data points reside (roughly) on a low-dimensional manifold M2 , and q is the cluster assignment function defined on M, i.e. 2 We believe that the text data are also sampled from some low dimensional manifold, since it is impossible for them to for ∀x ∈ M, q(x) returns the cluster membership of x. The smoothness of q over M can be calculated by the following Dirichlet integral [2] D[q] = 1 2 M q(x) 2 dM, (13) where the gradient q is a vector in the tangent space T Mx, and the integral is taken with respect to the standard measure on M. If we restrict the scale of q by q, q M = 1 (where ·, · M is the inner product induced on M), then it turns out that finding the smoothest function minimizing D[q] reduces to finding the eigenfunctions of the Laplace Beltrami operator L, which is defined as Lq −div q, (14) where div is the divergence of a vector field. Generally, the graph can be viewed as the discretized form of manifold. We can model the dataset as an weighted undirected graph as in spectral clustering [22], where the graph nodes are just the data points, and the weights on the edges represent the similarities between pairwise points. Then it can be shown that minimizing Eq.(13) corresponds to minimizing Jg = qT Lq = n i=1 (qi − qj)2 wij, (15) where q = [q1, q2, · · · , qn]T with qi = q(xi), L is the graph Laplacian with its (i, j)-th entry Lij =    di − wii, if i = j −wij, if xi and xj are adjacent 0, otherwise, (16) where di = j wij is the degree of xi, wij is the similarity between xi and xj. If xi and xj are adjacent3 , wij is usually computed in the following way wij = e − xi−xj 2 2σ2 , (17) where σ is a dataset dependent parameter. It is proved that under certain conditions, such a form of wij to determine the weights on graph edges leads to the convergence of graph Laplacian to the Laplace Beltrami operator [3][18]. In summary, using Eq.(15) with exponential weights can effectively measure the smoothness of the data assignments with respect to the intrinsic data manifold. Thus we adopt it as a global regularizer to punish the smoothness of the predicted data assignments. 2.4 Clustering with Local and Global Regularization Combining the contents we have introduced in section 2.2 and section 2.3 we can derive the clustering criterion is minq J = Jl + λJg = Pq − q 2 + λqT Lq s.t. qi ∈ {−1, +1}, (18) where P is defined as in Eq.(12), and λ is a regularization parameter to trade off Jl and Jg. However, the discrete fill in the whole high-dimensional sample space. And it has been shown that the manifold based methods can achieve good results on text classification tasks [31]. 3 In this paper, we define xi and xj to be adjacent if xi ∈ N(xj) or xj ∈ N(xi). constraint of pi makes the problem an NP hard integer programming problem. A natural way for making the problem solvable is to remove the constraint and relax qi to be continuous, then the objective that we aims to minimize becomes J = Pq − q 2 + λqT Lq = qT (P − I)T (P − I)q + λqT Lq = qT (P − I)T (P − I) + λL q, (19) and we further add a constraint qT q = 1 to restrict the scale of q. Then our objective becomes minq J = qT (P − I)T (P − I) + λL q s.t. qT q = 1 (20) Using the Lagrangian method, we can derive that the optimal solution q corresponds to the smallest eigenvector of the matrix M = (P − I)T (P − I) + λL, and the cluster assignment of xi can be determined by the sign of qi, i.e. xi will be classified as class one if qi > 0, otherwise it will be classified as class 2. 2.5 Multi-Class CLGR In the above we have introduced the basic framework of Clustering with Local and Global Regularization (CLGR) for the two-class clustering problem, and we will extending it to multi-class clustering in this subsection. First we assume that all the documents belong to C classes indexed by L = {1, 2, · · · , C}. qc is the classification function for class c (1 c C), such that qc (xi) returns the confidence that xi belongs to class c. Our goal is to obtain the value of qc (xi) (1 c C, 1 i n), and the cluster assignment of xi can be determined by {qc (xi)}C c=1 using some proper discretization methods that we will introduce later. Therefore, in this multi-class case, for each document xi (1 i n), we will construct C locally linear regularized label predictors whose normal vectors are wc∗ i = Xi XT i Xi + λiniIi −1 qc i (1 c C), (21) where Xi = [xi1, xi2, · · · , xini ] with xik being the k-th neighbor of xi, and qc i = [qc i1, qc i2, · · · , qc ini ]T with qc ik = qc (xik). Then (wc∗ i )T xi returns the predicted confidence of xi belonging to class c. Hence the local prediction error for class c can be defined as J c l = n i=1 (wc∗ i ) T xi − qc i 2 , (22) And the total local prediction error becomes Jl = C c=1 J c l = C c=1 n i=1 (wc∗ i ) T xi − qc i 2 . (23) As in Eq.(11), we can define an n×n matrix P (see Eq.(12)) and rewrite Jl as Jl = C c=1 J c l = C c=1 Pqc − qc 2 . (24) Similarly we can define the global smoothness regularizer in multi-class case as Jg = C c=1 n i=1 (qc i − qc j )2 wij = C c=1 (qc )T Lqc . (25) Then the criterion to be minimized for CLGR in multi-class case becomes J = Jl + λJg = C c=1 Pqc − qc 2 + λ(qc )T Lqc = C c=1 (qc )T (P − I)T (P − I) + λL qc = trace QT (P − I)T (P − I) + λL Q , (26) where Q = [q1 , q2 , · · · , qc ] is an n × c matrix, and trace(·) returns the trace of a matrix. The same as in Eq.(20), we also add the constraint that QT Q = I to restrict the scale of Q. Then our optimization problem becomes minQ J = trace QT (P − I)T (P − I) + λL Q s.t. QT Q = I, (27) From the Ky Fan theorem [28], we know the optimal solution of the above problem is Q∗ = [q∗ 1, q∗ 2, · · · , q∗ C ]R, (28) where q∗ k (1 k C) is the eigenvector corresponds to the k-th smallest eigenvalue of matrix (P − I)T (P − I) + λL, and R is an arbitrary C × C matrix. Since the values of the entries in Q∗ is continuous, we need to further discretize Q∗ to get the cluster assignments of all the data points. There are mainly two approaches to achieve this goal: 1. As in [20], we can treat the i-th row of Q as the embedding of xi in a C-dimensional space, and apply some traditional clustering methods like kmeans to clustering these embeddings into C clusters. 2. Since the optimal Q∗ is not unique (because of the existence of an arbitrary matrix R), we can pursue an optimal R that will rotate Q∗ to an indication matrix4 . The detailed algorithm can be referred to [26]. The detailed algorithm procedure for CLGR is summarized in table 1. 3. EXPERIMENTS In this section, experiments are conducted to empirically compare the clustering results of CLGR with other 8 representitive document clustering algorithms on 5 datasets. First we will introduce the basic informations of those datasets. 3.1 Datasets We use a variety of datasets, most of which are frequently used in the information retrieval research. Table 2 summarizes the characteristics of the datasets. 4 Here an indication matrix T is a n×c matrix with its (i, j)th entry Tij ∈ {0, 1} such that for each row of Q∗ there is only one 1. Then the xi can be assigned to the j-th cluster such that j = argjQ∗ ij = 1. Table 1: Clustering with Local and Global Regularization (CLGR) Input: 1. Dataset X = {xi}n i=1; 2. Number of clusters C; 3. Size of the neighborhood K; 4. Local regularization parameters {λi}n i=1; 5. Global regularization parameter λ; Output: The cluster membership of each data point. Procedure: 1. Construct the K nearest neighborhoods for each data point; 2. Construct the matrix P using Eq.(12); 3. Construct the Laplacian matrix L using Eq.(16); 4. Construct the matrix M = (P − I)T (P − I) + λL; 5. Do eigenvalue decomposition on M, and construct the matrix Q∗ according to Eq.(28); 6. Output the cluster assignments of each data point by properly discretize Q∗ . Table 2: Descriptions of the document datasets Datasets Number of documents Number of classes CSTR 476 4 WebKB4 4199 4 Reuters 2900 10 WebACE 2340 20 Newsgroup4 3970 4 CSTR. This is the dataset of the abstracts of technical reports published in the Department of Computer Science at a university. The dataset contained 476 abstracts, which were divided into four research areas: Natural Language Processing(NLP), Robotics/Vision, Systems, and Theory. WebKB. The WebKB dataset contains webpages gathered from university computer science departments. There are about 8280 documents and they are divided into 7 categories: student, faculty, staff, course, project, department and other. The raw text is about 27MB. Among these 7 categories, student, faculty, course and project are four most populous entity-representing categories. The associated subset is typically called WebKB4. Reuters. The Reuters-21578 Text Categorization Test collection contains documents collected from the Reuters newswire in 1987. It is a standard text categorization benchmark and contains 135 categories. In our experiments, we use a subset of the data collection which includes the 10 most frequent categories among the 135 topics and we call it Reuters-top 10. WebACE. The WebACE dataset was from WebACE project and has been used for document clustering [17][5]. The WebACE dataset contains 2340 documents consisting news articles from Reuters new service via the Web in October 1997. These documents are divided into 20 classes. News4. The News4 dataset used in our experiments are selected from the famous 20-newsgroups dataset5 . The topic rec containing autos, motorcycles, baseball and hockey was selected from the version 20news-18828. The News4 dataset contains 3970 document vectors. 5 http://people.csail.mit.edu/jrennie/20Newsgroups/ To pre-process the datasets, we remove the stop words using a standard stop list, all HTML tags are skipped and all header fields except subject and organization of the posted articles are ignored. In all our experiments, we first select the top 1000 words by mutual information with class labels. 3.2 Evaluation Metrics In the experiments, we set the number of clusters equal to the true number of classes C for all the clustering algorithms. To evaluate their performance, we compare the clusters generated by these algorithms with the true classes by computing the following two performance measures. Clustering Accuracy (Acc). The first performance measure is the Clustering Accuracy, which discovers the one-toone relationship between clusters and classes and measures the extent to which each cluster contained data points from the corresponding class. It sums up the whole matching degree between all pair class-clusters. Clustering accuracy can be computed as: Acc = 1 N max   Ck,Lm T(Ck, Lm)   , (29) where Ck denotes the k-th cluster in the final results, and Lm is the true m-th class. T(Ck, Lm) is the number of entities which belong to class m are assigned to cluster k. Accuracy computes the maximum sum of T(Ck, Lm) for all pairs of clusters and classes, and these pairs have no overlaps. The greater clustering accuracy means the better clustering performance. Normalized Mutual Information (NMI). Another evaluation metric we adopt here is the Normalized Mutual Information NMI [23], which is widely used for determining the quality of clusters. For two random variable X and Y, the NMI is defined as: NMI(X, Y) = I(X, Y) H(X)H(Y) , (30) where I(X, Y) is the mutual information between X and Y, while H(X) and H(Y) are the entropies of X and Y respectively. One can see that NMI(X, X) = 1, which is the maximal possible value of NMI. Given a clustering result, the NMI in Eq.(30) is estimated as NMI = C k=1 C m=1 nk,mlog n·nk,m nk ˆnm C k=1 nklog nk n C m=1 ˆnmlog ˆnm n , (31) where nk denotes the number of data contained in the cluster Ck (1 k C), ˆnm is the number of data belonging to the m-th class (1 m C), and nk,m denotes the number of data that are in the intersection between the cluster Ck and the m-th class. The value calculated in Eq.(31) is used as a performance measure for the given clustering result. The larger this value, the better the clustering performance. 3.3 Comparisons We have conducted comprehensive performance evaluations by testing our method and comparing it with 8 other representative data clustering methods using the same data corpora. The algorithms that we evaluated are listed below. 1. Traditional k-means (KM). 2. Spherical k-means (SKM). The implementation is based on [9]. 3. Gaussian Mixture Model (GMM). The implementation is based on [16]. 4. Spectral Clustering with Normalized Cuts (Ncut). The implementation is based on [26], and the variance of the Gaussian similarity is determined by Local Scaling [30]. Note that the criterion that Ncut aims to minimize is just the global regularizer in our CLGR algorithm except that Ncut used the normalized Laplacian. 5. Clustering using Pure Local Regularization (CPLR). In this method we just minimize Jl (defined in Eq.(24)), and the clustering results can be obtained by doing eigenvalue decomposition on matrix (I − P)T (I − P) with some proper discretization methods. 6. Adaptive Subspace Iteration (ASI). The implementation is based on [14]. 7. Nonnegative Matrix Factorization (NMF). The implementation is based on [27]. 8. Tri-Factorization Nonnegative Matrix Factorization (TNMF) [12]. The implementation is based on [15]. For computational efficiency, in the implementation of CPLR and our CLGR algorithm, we have set all the local regularization parameters {λi}n i=1 to be identical, which is set by grid search from {0.1, 1, 10}. The size of the k-nearest neighborhoods is set by grid search from {20, 40, 80}. For the CLGR method, its global regularization parameter is set by grid search from {0.1, 1, 10}. When constructing the global regularizer, we have adopted the local scaling method [30] to construct the Laplacian matrix. The final discretization method adopted in these two methods is the same as in [26], since our experiments show that using such method can achieve better results than using kmeans based methods as in [20]. 3.4 Experimental Results The clustering accuracies comparison results are shown in table 3, and the normalized mutual information comparison results are summarized in table 4. From the two tables we mainly observe that: 1. Our CLGR method outperforms all other document clustering methods in most of the datasets; 2. For document clustering, the Spherical k-means method usually outperforms the traditional k-means clustering method, and the GMM method can achieve competitive results compared to the Spherical k-means method; 3. The results achieved from the k-means and GMM type algorithms are usually worse than the results achieved from Spectral Clustering. Since Spectral Clustering can be viewed as a weighted version of kernel k-means, it can obtain good results the data clusters are arbitrarily shaped. This corroborates that the documents vectors are not regularly distributed (spherical or elliptical). 4. The experimental comparisons empirically verify the equivalence between NMF and Spectral Clustering, which Table 3: Clustering accuracies of the various methods CSTR WebKB4 Reuters WebACE News4 KM 0.4256 0.3888 0.4448 0.4001 0.3527 SKM 0.4690 0.4318 0.5025 0.4458 0.3912 GMM 0.4487 0.4271 0.4897 0.4521 0.3844 NMF 0.5713 0.4418 0.4947 0.4761 0.4213 Ncut 0.5435 0.4521 0.4896 0.4513 0.4189 ASI 0.5621 0.4752 0.5235 0.4823 0.4335 TNMF 0.6040 0.4832 0.5541 0.5102 0.4613 CPLR 0.5974 0.5020 0.4832 0.5213 0.4890 CLGR 0.6235 0.5228 0.5341 0.5376 0.5102 Table 4: Normalized mutual information results of the various methods CSTR WebKB4 Reuters WebACE News4 KM 0.3675 0.3023 0.4012 0.3864 0.3318 SKM 0.4027 0.4155 0.4587 0.4003 0.4085 GMM 0.4034 0.4093 0.4356 0.4209 0.3994 NMF 0.5235 0.4517 0.4402 0.4359 0.4130 Ncut 0.4833 0.4497 0.4392 0.4289 0.4231 ASI 0.5008 0.4833 0.4769 0.4817 0.4503 TNMF 0.5724 0.5011 0.5132 0.5328 0.4749 CPLR 0.5695 0.5231 0.4402 0.5543 0.4690 CLGR 0.6012 0.5434 0.4935 0.5390 0.4908 has been proved theoretically in [10]. It can be observed from the tables that NMF and Spectral Clustering usually lead to similar clustering results. 5. The co-clustering based methods (TNMF and ASI) can usually achieve better results than traditional purely document vector based methods. Since these methods perform an implicit feature selection at each iteration, provide an adaptive metric for measuring the neighborhood, and thus tend to yield better clustering results. 6. The results achieved from CPLR are usually better than the results achieved from Spectral Clustering, which supports Vapniks theory [24] that sometimes local learning algorithms can obtain better results than global learning algorithms. Besides the above comparison experiments, we also test the parameter sensibility of our method. There are mainly two sets of parameters in our CLGR algorithm, the local and global regularization parameters ({λi}n i=1 and λ, as we have said in section 3.3, we have set all λis to be identical to λ∗ in our experiments), and the size of the neighborhoods. Therefore we have also done two sets of experiments: 1. Fixing the size of the neighborhoods, and testing the clustering performance with varying λ∗ and λ. In this set of experiments, we find that our CLGR algorithm can achieve good results when the two regularization parameters are neither too large nor too small. Typically our method can achieve good results when λ∗ and λ are around 0.1. Figure 1 shows us such a testing example on the WebACE dataset. 2. Fixing the local and global regularization parameters, and testing the clustering performance with different −5 −4.5 −4 −3.5 −3 −5 −4.5 −4 −3.5 −3 0.35 0.4 0.45 0.5 0.55 local regularization para (log 2 value) global regularization para (log 2 value) clusteringaccuracy Figure 1: Parameter sensibility testing results on the WebACE dataset with the neighborhood size fixed to 20, and the x-axis and y-axis represents the log2 value of λ∗ and λ. sizes of neighborhoods. In this set of experiments, we find that the neighborhood with a too large or too small size will all deteriorate the final clustering results. This can be easily understood since when the neighborhood size is very small, then the data points used for training the local classifiers may not be sufficient; when the neighborhood size is very large, the trained classifiers will tend to be global and cannot capture the typical local characteristics. Figure 2 shows us a testing example on the WebACE dataset. Therefore, we can see that our CLGR algorithm (1) can achieve satisfactory results and (2) is not very sensitive to the choice of parameters, which makes it practical in real world applications. 4. CONCLUSIONS AND FUTURE WORKS In this paper, we derived a new clustering algorithm called clustering with local and global regularization. Our method preserves the merit of local learning algorithms and spectral clustering. Our experiments show that the proposed algorithm outperforms most of the state of the art algorithms on many benchmark datasets. In the future, we will focus on the parameter selection and acceleration issues of the CLGR algorithm. 5. REFERENCES [1] L. Baker and A. McCallum. Distributional Clustering of Words for Text Classification. In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 1998. [2] M. Belkin and P. Niyogi. Laplacian Eigenmaps for Dimensionality Reduction and Data Representation. Neural Computation, 15 (6):1373-1396. June 2003. [3] M. Belkin and P. Niyogi. Towards a Theoretical Foundation for Laplacian-Based Manifold Methods. In Proceedings of the 18th Conference on Learning Theory (COLT). 2005. 10 20 30 40 50 60 70 80 90 100 0.35 0.4 0.45 0.5 0.55 size of the neighborhood clusteringaccuracy Figure 2: Parameter sensibility testing results on the WebACE dataset with the regularization parameters being fixed to 0.1, and the neighborhood size varing from 10 to 100. [4] M. Belkin, P. Niyogi and V. Sindhwani. Manifold Regularization: a Geometric Framework for Learning from Examples. Journal of Machine Learning Research 7, 1-48, 2006. [5] D. Boley. Principal Direction Divisive Partitioning. Data mining and knowledge discovery, 2:325-344, 1998. [6] L. Bottou and V. Vapnik. Local learning algorithms. Neural Computation, 4:888-900, 1992. [7] P. K. Chan, D. F. Schlag and J. Y. Zien. Spectral K-way Ratio-Cut Partitioning and Clustering. IEEE Trans. Computer-Aided Design, 13:1088-1096, Sep. 1994. [8] D. R. Cutting, D. R. Karger, J. O. Pederson and J. W. Tukey. Scatter/Gather: A Cluster-Based Approach to Browsing Large Document Collections. In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 1992. [9] I. S. Dhillon and D. S. Modha. Concept Decompositions for Large Sparse Text Data using Clustering. Machine Learning, vol. 42(1), pages 143-175, January 2001. [10] C. Ding, X. He, and H. Simon. On the equivalence of nonnegative matrix factorization and spectral clustering. In Proceedings of the SIAM Data Mining Conference, 2005. [11] C. Ding, X. He, H. Zha, M. Gu, and H. D. Simon. A min-max cut algorithm for graph partitioning and data clustering. In Proc. of the 1st International Conference on Data Mining (ICDM), pages 107-114, 2001. [12] C. Ding, T. Li, W. Peng, and H. Park. Orthogonal Nonnegative Matrix Tri-Factorizations for Clustering. In Proceedings of the Twelfth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2006. [13] R. O. Duda, P. E. Hart, and D. G. Stork. Pattern Classification. John Wiley & Sons, Inc., 2001. [14] T. Li, S. Ma, and M. Ogihara. Document Clustering via Adaptive Subspace Iteration. In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2004. [15] T. Li and C. Ding. The Relationships Among Various Nonnegative Matrix Factorization Methods for Clustering. In Proceedings of the 6th International Conference on Data Mining (ICDM). 2006. [16] X. Liu and Y. Gong. Document Clustering with Cluster Refinement and Model Selection Capabilities. In Proc. of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002. [17] E. Han, D. Boley, M. Gini, R. Gross, K. Hastings, G. Karypis, V. Kumar, B. Mobasher, and J. Moore. WebACE: A Web Agent for Document Categorization and Exploration. In Proceedings of the 2nd International Conference on Autonomous Agents (Agents98). ACM Press, 1998. [18] M. Hein, J. Y. Audibert, and U. von Luxburg. From Graphs to Manifolds - Weak and Strong Pointwise Consistency of Graph Laplacians. In Proceedings of the 18th Conference on Learning Theory (COLT), 470-485. 2005. [19] J. He, M. Lan, C.-L. Tan, S.-Y. Sung, and H.-B. Low. Initialization of Cluster Refinement Algorithms: A Review and Comparative Study. In Proc. of Inter. Joint Conference on Neural Networks, 2004. [20] A. Y. Ng, M. I. Jordan, Y. Weiss. On Spectral Clustering: Analysis and an algorithm. In Advances in Neural Information Processing Systems 14. 2002. [21] B. Sch¨olkopf and A. Smola. Learning with Kernels. The MIT Press. Cambridge, Massachusetts. 2002. [22] J. Shi and J. Malik. Normalized Cuts and Image Segmentation. IEEE Trans. on Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000. [23] A. Strehl and J. Ghosh. Cluster Ensembles - A Knowledge Reuse Framework for Combining Multiple Partitions. Journal of Machine Learning Research, 3:583-617, 2002. [24] V. N. Vapnik. The Nature of Statistical Learning Theory. Berlin: Springer-Verlag, 1995. [25] Wu, M. and Sch¨olkopf, B. A Local Learning Approach for Clustering. In Advances in Neural Information Processing Systems 18. 2006. [26] S. X. Yu, J. Shi. Multiclass Spectral Clustering. In Proceedings of the International Conference on Computer Vision, 2003. [27] W. Xu, X. Liu and Y. Gong. Document Clustering Based On Non-Negative Matrix Factorization. In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2003. [28] H. Zha, X. He, C. Ding, M. Gu and H. Simon. Spectral Relaxation for K-means Clustering. In NIPS 14. 2001. [29] T. Zhang and F. J. Oles. Text Categorization Based on Regularized Linear Classification Methods. Journal of Information Retrieval, 4:5-31, 2001. [30] L. Zelnik-Manor and P. Perona. Self-Tuning Spectral Clustering. In NIPS 17. 2005. [31] D. Zhou, O. Bousquet, T. N. Lal, J. Weston and B. Sch¨olkopf. Learning with Local and Global Consistency. NIPS 17, 2005.",
    "original_translation": "Agrupamiento regularizado para documentos ∗ Fei Wang, Changshui Zhang Laboratorio Clave del Estado de Tecnología Inteligente. En los últimos años, la agrupación de documentos ha estado recibiendo cada vez más atención como una técnica importante y fundamental para la organización no supervisada de documentos, la extracción automática de temas y la recuperación o filtrado rápido de información. En este artículo, proponemos un método novedoso para agrupar documentos utilizando regularización. A diferencia de los métodos de agrupamiento regularizados globalmente tradicionales, nuestro método primero construye un predictor de etiquetas lineal regularizado local para cada vector de documento, y luego combina todos esos regularizadores locales con un regularizador de suavidad global. Así que llamamos a nuestro algoritmo Agrupamiento con Regularización Local y Global (CLGR). Mostraremos que las pertenencias de los documentos a los grupos se pueden lograr mediante la descomposición de los valores propios de una matriz simétrica dispersa, la cual puede resolverse eficientemente mediante métodos iterativos. Finalmente, se presentan nuestras evaluaciones experimentales en varios conjuntos de datos para mostrar las ventajas de CLGR sobre los métodos tradicionales de agrupamiento de documentos. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información-Agrupamiento; I.2.6 [Inteligencia Artificial]: Aprendizaje-Aprendizaje de Conceptos Términos Generales Algoritmos 1. La agrupación de documentos ha estado recibiendo cada vez más atención como una técnica importante y fundamental para la organización no supervisada de documentos, la extracción automática de temas y la recuperación o filtrado rápido de información. Un buen enfoque de agrupación de documentos puede ayudar a las computadoras a organizar automáticamente el corpus de documentos en una jerarquía de clusters significativa para una navegación eficiente, lo cual es muy valioso para complementar las deficiencias de las tecnologías tradicionales de recuperación de información. Como señaló [8], las necesidades de recuperación de información pueden expresarse mediante un espectro que va desde la búsqueda basada en palabras clave estrechas hasta la exploración de información amplia, como cuáles son los principales eventos internacionales de los últimos meses. Los motores de búsqueda de documentos tradicionales tienden a adaptarse bien al extremo de la búsqueda, es decir, suelen proporcionar búsquedas específicas de documentos que coinciden con la consulta de los usuarios, sin embargo, les resulta difícil satisfacer las necesidades del resto del espectro en el que se necesita información más amplia o vaga. En tales casos, la navegación eficiente a través de una buena jerarquía de clústeres será definitivamente útil. Generalmente, los métodos de agrupamiento de documentos se pueden clasificar principalmente en dos clases: métodos jerárquicos y métodos de partición. Los métodos jerárquicos agrupan los puntos de datos en una estructura de árbol jerárquico utilizando enfoques de abajo hacia arriba o de arriba hacia abajo. Por ejemplo, el clustering jerárquico aglomerativo (HAC) [13] es un método típico de clustering jerárquico ascendente. Toma cada punto de datos como un único clúster para comenzar y luego construye clústeres más grandes agrupando puntos de datos similares juntos hasta que todo el conjunto de datos esté encapsulado en un único clúster final. Por otro lado, los métodos de particionamiento descomponen el conjunto de datos en un número de grupos disjuntos que suelen ser óptimos en términos de algunas funciones de criterio predefinidas. Por ejemplo, K-means es un método de particionamiento típico que tiene como objetivo minimizar la suma de las distancias al cuadrado entre los puntos de datos y sus centros de clúster correspondientes. En este documento, nos centraremos en los métodos de particionamiento. Como sabemos, existen dos problemas principales en los métodos de particionamiento (como Kmeans y el Modelo de Mezcla Gaussiana (GMM) [16]): (1) el criterio predefinido suele ser no convexo, lo que provoca muchas soluciones óptimas locales; (2) el procedimiento iterativo (por ejemplo, el algoritmo de Expectation Maximization (EM)) para optimizar los criterios suele hacer que las soluciones finales dependan en gran medida de las inicializaciones. En las últimas décadas, se han propuesto muchos métodos para superar los problemas mencionados de los métodos de particionamiento [19][28]. Recientemente, otro tipo de métodos de particionamiento basados en agrupamiento en grafos de datos han despertado un considerable interés en la comunidad de aprendizaje automático y minería de datos. La idea básica detrás de estos métodos es modelar primero el conjunto de datos completo como un grafo ponderado, en el que los nodos del grafo representan los puntos de datos y los pesos en los bordes corresponden a las similitudes entre los puntos en pares. Entonces, las asignaciones de clústeres del conjunto de datos se pueden lograr optimizando algunos criterios definidos en el gráfico. Por ejemplo, el Clustering Espectral es uno de los enfoques de clustering basados en grafos más representativos, generalmente tiene como objetivo optimizar algún valor de corte (por ejemplo, Corte normalizado [22], corte de razón [7], corte mínimo-máximo [11]) definidos en un grafo no dirigido. Después de algunas relajaciones, estos criterios generalmente pueden ser optimizados a través de descomposiciones propias, lo cual está garantizado que sea óptimo a nivel global. De esta manera, el agrupamiento espectral evita eficientemente los problemas de los métodos de particionamiento tradicionales que presentamos en el párrafo anterior. En este artículo, proponemos un algoritmo novedoso de agrupamiento de documentos que hereda la superioridad del agrupamiento espectral, es decir, los resultados finales de los grupos también pueden obtenerse explotando la estructura de los autovalores de una matriz simétrica. Sin embargo, a diferencia del agrupamiento espectral, que simplemente impone una restricción de suavidad en las etiquetas de los datos sobre todo el conjunto de datos [2], nuestro método primero construye un predictor de etiquetas lineal regularizado para cada punto de datos a partir de su vecindario como en [25], y luego combina los resultados de todos estos predictores de etiquetas locales con un regularizador de suavidad de etiquetas global. Así que llamamos a nuestro método Agrupamiento con Regularización Local y Global (CLGR). La idea de incorporar tanto información local como global en la predicción de etiquetas está inspirada en los trabajos recientes sobre aprendizaje semi-supervisado [31], y nuestras evaluaciones experimentales en varios conjuntos de datos reales de documentos muestran que CLGR funciona mejor que muchos métodos de agrupamiento de vanguardia. El resto de este documento está organizado de la siguiente manera: en la sección 2 presentaremos nuestro algoritmo CLGR en detalle. Los resultados experimentales en varios conjuntos de datos se presentan en la sección 3, seguidos de las conclusiones y discusiones en la sección 4. El ALGORITMO PROPUESTO En esta sección, presentaremos en detalle nuestro algoritmo de Agrupamiento con Regularización Local y Global (CLGR). Primero veamos cómo se representan los documentos a lo largo de este documento. 2.1 Representación de documentos En nuestro trabajo, todos los documentos se representan mediante vectores de frecuencia de términos ponderados. Sea W = {w1, w2, · · · , wm} el conjunto completo de vocabulario del corpus de documentos (que ha sido preprocesado mediante la eliminación de palabras vacías y operaciones de derivación de palabras). El vector de frecuencia de término xi del documento di se define como xi = [xi1, xi2, · · · , xim]T , xik = tik log n idfk , donde tik es la frecuencia del término wk ∈ W, n es el tamaño del corpus de documentos, idfk es el número de documentos que contienen la palabra wk. De esta manera, xi también se llama la representación TFIDF del documento di. Además, también normalizamos cada xi (1 i n) para que tenga una longitud unitaria, de modo que cada documento esté representado por un vector TF-IDF normalizado. 2.2 Regularización Local Como su nombre sugiere, CLGR está compuesto por dos partes: regularización local y regularización global. En esta subsección introduciremos detalladamente la parte de regularización local. 2.2.1 Motivación Como sabemos que el agrupamiento es un tipo de técnica de aprendizaje, tiene como objetivo organizar el conjunto de datos de una manera razonable. En general, el aprendizaje puede plantearse como un problema de estimación de funciones, a partir del cual podemos obtener una buena función de clasificación que asignará etiquetas al conjunto de datos de entrenamiento e incluso al conjunto de datos de prueba no vistos con algún costo minimizado [24]. Por ejemplo, en el escenario de clasificación de dos clases (en el que conocemos exactamente la etiqueta de cada documento), un clasificador lineal con ajuste de mínimos cuadrados tiene como objetivo aprender un vector columna w de manera que el costo al cuadrado J = 1 n (wT xi − yi)2 (1) se minimice, donde yi ∈ {+1, −1} es la etiqueta de xi. Al tomar ∂J /∂w = 0, obtenemos la solución w∗ = Σ i=1 n xixT i −1 Σ i=1 n xiyi, (2) que puede escribirse en su forma matricial como w∗ = XXT −1 Xy, (3) donde X = [x1, x2, · · · , xn] es una matriz de documentos m × n, y = [y1, y2, · · · , yn]T es el vector de etiquetas. Entonces, para un documento de prueba t, podemos determinar su etiqueta mediante l = signo(w∗T u), donde signo(·) es la función signo. Un problema natural en la ecuación (3) es que la matriz XXT puede ser singular y, por lo tanto, no invertible (por ejemplo, cuando m n). Para evitar tal problema, podemos agregar un término de regularización y minimizar el siguiente criterio J = 1 n n i=1 (wT xi − yi)2 + λ w 2, (5), donde λ es un parámetro de regularización. Entonces, la solución óptima que minimiza J está dada por w∗ = XXT + λnI −1 Xy, (6) donde I es una matriz identidad de tamaño m × m. Se ha informado que el clasificador lineal regularizado puede lograr resultados muy buenos en problemas de clasificación de texto [29]. Sin embargo, a pesar de su éxito empírico, el clasificador lineal regularizado es en realidad un clasificador global, es decir, w∗ se estima utilizando todo el conjunto de entrenamiento. Según [24], esta puede que no sea una idea inteligente, ya que un único w∗ puede que no sea lo suficientemente bueno para predecir las etiquetas de todo el espacio de entrada. Para obtener predicciones más precisas, [6] propuso entrenar clasificadores localmente y utilizarlos para clasificar los puntos de prueba. Por ejemplo, un punto de prueba será clasificado por el clasificador local entrenado utilizando los puntos de entrenamiento ubicados en la vecindad 1. En las siguientes discusiones todos asumimos que los documentos provienen solo de dos clases. Las generalizaciones de nuestro método a casos de múltiples clases se discutirán en la sección 2.5 de él. Aunque este método parece lento y estúpido, se informa que puede obtener mejores rendimientos que usar un clasificador global único en ciertas tareas [6]. 2.2.2 Construyendo los Predictores Localmente Regularizados Inspirados en su éxito, propusimos aplicar los algoritmos de aprendizaje local para el agrupamiento. La idea básica es que, para cada vector de documento xi (1 i n), entrenamos un predictor de etiquetas local basado en su vecindario k-más cercano Ni, y luego lo usamos para predecir la etiqueta de xi. Finalmente combinaremos todos esos predictores locales minimizando la suma de sus errores de predicción. En esta subsección introduciremos cómo construir esos predictores locales. Debido a la simplicidad y efectividad del clasificador lineal regularizado que hemos introducido en la sección 2.2.1, lo elegimos como nuestro predictor de etiquetas local, de modo que para cada documento xi, se minimiza el siguiente criterio Ji = 1 ni xj ∈Ni wT i xj − qj 2 + λi wi 2, (7), donde ni = |Ni| es la cardinalidad de Ni, y qj es la pertenencia al clúster de xj. Luego, utilizando la Ecuación (6), podemos obtener que la solución óptima es w∗ i = XiXT i + λiniI −1 Xiqi, (8), donde Xi = [xi1, xi2, · · · , xini], y usamos xik para denotar al k-ésimo vecino más cercano de xi. qi = [qi1, qi2, · · · , qini]T con qik representando la asignación de clúster de xik. El problema aquí es que XiXT i es una matriz m × m con m ni, es decir, deberíamos calcular la inversa de una matriz m × m para cada vector de documento, lo cual está prohibido computacionalmente. Afortunadamente, tenemos el siguiente teorema: Teorema 1. w∗ i en la Ec. (8) puede ser reescrito como w∗ i = Xi XT i Xi + λiniIi −1 qi, (9), donde Ii es una matriz identidad de tamaño ni × ni. Prueba. Dado que w∗ i = XiXT i + λiniI −1 Xiqi, entonces XiXT i + λiniI w∗ i = Xiqi =⇒ XiXT i w∗ i + λiniw∗ i = Xiqi =⇒ w∗ i = (λini)−1 Xi qi − XT i w∗ i. Sea β = (λini)−1 qi − XT i w∗ i, entonces w∗ i = Xiβ =⇒ λiniβ = qi − XT i w∗ i = qi − XT i Xiβ =⇒ qi = XT i Xi + λiniIi β =⇒ β = XT i Xi + λiniIi −1 qi. Por lo tanto, w∗ i = Xiβ = Xi XT i Xi + λiniIi −1 qi 2. Utilizando el teorema 1, solo necesitamos calcular la inversa de una matriz ni × ni para cada documento para entrenar un predictor de etiquetas local. Además, para un nuevo punto de prueba u que cae en Ni, podemos clasificarlo por el signo de qu = w∗T i u = uT wi = uT Xi XT i Xi + λiniIi −1 qi. Esta es una expresión atractiva ya que podemos determinar la asignación del clúster de u utilizando los productos internos entre los puntos en {u ∪ Ni}, lo que sugiere que un regularizador local de este tipo puede ser fácilmente kernelizado [21] siempre y cuando definamos una función de kernel adecuada. 2.2.3 Combinando los Predictores Regularizados Localmente Una vez que todos los predictores locales hayan sido construidos, los combinaremos minimizando Jl = n i=1 w∗T i xi − qi 2, (10), lo que representa la suma de los errores de predicción de todos los predictores locales. Combinando la Ecuación (10) con la Ecuación (6), podemos obtener Jl = n i=1 w∗T i xi − qi 2 = n i=1 xT i Xi XT i Xi + λiniIi −1 qi − qi 2 = Pq − q 2, (11), donde q = [q1, q2, · · · , qn]T, y P es una matriz n × n construida de la siguiente manera. Sea αi = xT i Xi XT i Xi + λiniIi −1 , entonces Pij = αi j, si xj ∈ Ni 0, de lo contrario , (12) donde Pij es la entrada (i, j)-ésima de P, y αi j representa la entrada j-ésima de αi. Hasta ahora podemos escribir el criterio de agrupamiento combinando predictores de etiquetas lineales localmente regularizados Jl en una forma matemática explícita, y podemos minimizarlo directamente utilizando algunas técnicas estándar de optimización. Sin embargo, los resultados pueden no ser lo suficientemente buenos ya que solo explotamos la información local del conjunto de datos. En la siguiente subsección, introduciremos un criterio de regularización global y lo combinaremos con Jl, que tiene como objetivo encontrar un buen resultado de agrupamiento de manera local-global. 2.3 Regularización Global En el agrupamiento de datos, generalmente requerimos que las asignaciones de clúster de los puntos de datos sean lo suficientemente suaves con respecto a la variedad de datos subyacente, lo que implica (1) que los puntos cercanos tienden a tener las mismas asignaciones de clúster; (2) que los puntos en la misma estructura (por ejemplo, subvariedad o clúster) tienden a tener las mismas asignaciones de clúster [31]. Sin pérdida de generalidad, asumimos que los puntos de datos residen (aproximadamente) en una variedad de baja dimensión M2, y q es la función de asignación de clúster definida en M, es decir, 2 Creemos que los datos de texto también se muestrean de alguna variedad de baja dimensión, ya que es imposible que para todos los x ∈ M, q(x) devuelva la membresía del clúster de x. La suavidad de q sobre M se puede calcular mediante la siguiente integral de Dirichlet [2] D[q] = 1 2 M q(x) 2 dM, (13) donde el gradiente q es un vector en el espacio tangente T Mx, y la integral se toma con respecto a la medida estándar en M. Si restringimos la escala de q por q, q M = 1 (donde ·, · M es el producto interno inducido en M), entonces resulta que encontrar la función más suave que minimiza D[q] se reduce a encontrar las autofunciones del operador Laplace Beltrami L, que se define como Lq −div q, (14) donde div es la divergencia de un campo vectorial. Generalmente, el gráfico se puede ver como la forma discretizada de una variedad. Podemos modelar el conjunto de datos como un grafo ponderado no dirigido como en el agrupamiento espectral [22], donde los nodos del grafo son simplemente los puntos de datos, y los pesos en las aristas representan las similitudes entre pares de puntos. Entonces se puede demostrar que minimizar la Ec. (13) corresponde a minimizar Jg = qT Lq = n i=1 (qi − qj)2 wij, (15), donde q = [q1, q2, · · · , qn]T con qi = q(xi), L es el Laplaciano del grafo con su entrada (i, j)-ésima Lij =    di − wii, si i = j −wij, si xi y xj son adyacentes 0, en otro caso, (16), donde di = j wij es el grado de xi, wij es la similitud entre xi y xj. Si xi y xj son adyacentes, wij generalmente se calcula de la siguiente manera: wij = e − xi−xj 2 2σ2, donde σ es un parámetro dependiente del conjunto de datos. Se ha demostrado que bajo ciertas condiciones, tal forma de wij para determinar los pesos en los bordes del grafo conduce a la convergencia del Laplaciano del grafo al operador Laplace Beltrami [3][18]. En resumen, el uso de la Ec. (15) con pesos exponenciales puede medir de manera efectiva la suavidad de las asignaciones de datos con respecto a la variedad intrínseca de datos. Por lo tanto, lo adoptamos como un regularizador global para penalizar la suavidad de las asignaciones de datos predichas. 2.4 Agrupamiento con Regularización Local y Global Combinando los contenidos que hemos introducido en la sección 2.2 y la sección 2.3, podemos derivar que el criterio de agrupamiento es minq J = Jl + λJg = Pq − q 2 + λqT Lq sujeto a qi ∈ {−1, +1}, (18) donde P está definido como en la Ecuación (12), y λ es un parámetro de regularización para equilibrar Jl y Jg. Sin embargo, los valores discretos llenan todo el espacio muestral de alta dimensionalidad. Y se ha demostrado que los métodos basados en variedades pueden lograr buenos resultados en tareas de clasificación de texto [31]. En este artículo, definimos xi y xj como adyacentes si xi ∈ N(xj) o xj ∈ N(xi). La restricción de pi convierte el problema en un problema de programación entera NP difícil. Una forma natural de hacer que el problema sea resoluble es eliminar la restricción y relajar qi para que sea continuo, luego el objetivo que buscamos minimizar se convierte en J = Pq − q 2 + λqT Lq = qT (P − I)T (P − I)q + λqT Lq = qT (P − I)T (P − I) + λL q, (19) y luego agregamos una restricción adicional qT q = 1 para restringir la escala de q. Entonces nuestro objetivo se convierte en minq J = qT (P − I)T (P − I) + λL q s.t. qT q = 1 (20) Utilizando el método de Lagrange, podemos derivar que la solución óptima q corresponde al menor vector propio de la matriz M = (P − I)T (P − I) + λL, y la asignación de clúster de xi puede determinarse por el signo de qi, es decir, xi se clasificará como clase uno si qi > 0, de lo contrario se clasificará como clase 2. 2.5 CLGR Multiclase En lo anterior hemos introducido el marco básico de Agrupamiento con Regularización Local y Global (CLGR) para el problema de agrupamiento de dos clases, y lo extenderemos al agrupamiento multiclase en esta subsección. Primero asumimos que todos los documentos pertenecen a C clases indexadas por L = {1, 2, · · · , C}. qc es la función de clasificación para la clase c (1 ≤ c ≤ C), tal que qc(xi) devuelve la confianza de que xi pertenece a la clase c. Nuestro objetivo es obtener el valor de qc(xi) (1 ≤ c ≤ C, 1 ≤ i ≤ n), y la asignación de cluster de xi puede ser determinada por {qc(xi)}C c=1 utilizando algunos métodos adecuados de discretización que presentaremos más adelante. Por lo tanto, en este caso de múltiples clases, para cada documento xi (1 i n), construiremos C predictores de etiquetas regularizados localmente lineales cuyos vectores normales son wc∗ i = Xi XT i Xi + λiniIi −1 qc i (1 c C), (21), donde Xi = [xi1, xi2, · · · , xini ] con xik siendo el k-ésimo vecino de xi, y qc i = [qc i1, qc i2, · · · , qc ini ]T con qc ik = qc (xik). Entonces (wc∗ i )T xi devuelve la confianza predicha de xi perteneciente a la clase c. Por lo tanto, el error de predicción local para la clase c se puede definir como J c l = n i=1 (wc∗ i ) T xi − qc i 2 , (22) Y el error de predicción local total se convierte en Jl = C c=1 J c l = C c=1 n i=1 (wc∗ i ) T xi − qc i 2 . (23) Como en la Ec. (11), podemos definir una matriz n×n P (ver Ec. (12)) y reescribir Jl como Jl = C c=1 J c l = C c=1 Pqc − qc 2 . (24) De manera similar, podemos definir el regularizador de suavidad global en el caso de múltiples clases como Jg = C c=1 n i=1 (qc i − qc j )2 wij = C c=1 (qc )T Lqc . (25) Luego, el criterio a minimizar para CLGR en el caso de múltiples clases se convierte en J = Jl + λJg = C c=1 Pqc − qc 2 + λ(qc )T Lqc = C c=1 (qc )T (P − I)T (P − I) + λL qc = trace QT (P − I)T (P − I) + λL Q , (26) donde Q = [q1 , q2 , · · · , qc ] es una matriz n × c, y trace(·) devuelve la traza de una matriz. Al igual que en la ecuación (20), también agregamos la restricción de que QT Q = I para limitar la escala de Q. Entonces nuestro problema de optimización se convierte en minQ J = traza QT (P − I)T (P − I) + λL Q sujeto a. Según el teorema de Ky Fan [28], sabemos que la solución óptima del problema anterior es Q∗ = [q∗ 1, q∗ 2, · · · , q∗ C ]R, donde q∗ k (1 ≤ k ≤ C) es el autovector que corresponde al k-ésimo valor propio más pequeño de la matriz (P − I)T (P − I) + λL, y R es una matriz arbitraria de tamaño C × C. Dado que los valores de las entradas en Q∗ son continuos, necesitamos discretizar aún más Q∗ para obtener las asignaciones de clúster de todos los puntos de datos. Principalmente hay dos enfoques para lograr este objetivo: 1. Como en [20], podemos tratar la i-ésima fila de Q como la incrustación de xi en un espacio de C dimensiones, y aplicar algunos métodos de agrupamiento tradicionales como kmeans para agrupar estas incrustaciones en C grupos. 2. Dado que el Q∗ óptimo no es único (debido a la existencia de una matriz R arbitraria), podemos buscar un R óptimo que rote Q∗ a una matriz de indicación4. El algoritmo detallado se puede consultar en [26]. El procedimiento detallado del algoritmo para CLGR se resume en la tabla 1. 3. EXPERIMENTOS En esta sección, se realizan experimentos para comparar empíricamente los resultados de agrupamiento de CLGR con otros 8 algoritmos representativos de agrupamiento de documentos en 5 conjuntos de datos. Primero presentaremos la información básica de esos conjuntos de datos. 3.1 Conjuntos de datos Utilizamos una variedad de conjuntos de datos, la mayoría de los cuales son frecuentemente utilizados en la investigación de recuperación de información. La Tabla 2 resume las características de los conjuntos de datos. Aquí, una matriz de indicación T es una matriz n×c con su entrada (i, j) Tij ∈ {0, 1} de modo que para cada fila de Q∗ solo hay un 1. Entonces, el xi puede ser asignado al j-ésimo grupo tal que j = argjQ∗ ij = 1. Tabla 1: Agrupamiento con Regularización Local y Global (CLGR) Entrada: 1. Conjunto de datos X = {xi}n i=1; 2. Número de grupos C: 3. Tamaño del vecindario K: 4. Parámetros locales de regularización {λi}n i=1; 5. Parámetro de regularización global λ; Salida: La membresía del clúster de cada punto de datos. Procedimiento: 1. Construir los K vecindarios más cercanos para cada punto de datos; 2. Construya la matriz P utilizando la Ecuación (12); 3. Construya la matriz Laplaciana L utilizando la ecuación (16); 4. Construye la matriz M = (P − I)T (P − I) + λL; 5. Realice la descomposición de valores propios en M y construya la matriz Q∗ de acuerdo con la Ecuación (28); 6. Generar las asignaciones de clúster de cada punto de datos al discretizar adecuadamente Q∗. Tabla 2: Descripciones de los conjuntos de datos de documentos Conjuntos de datos Número de documentos Número de clases CSTR 476 4 WebKB4 4199 4 Reuters 2900 10 WebACE 2340 20 Newsgroup4 3970 4 CSTR. Este es el conjunto de datos de los resúmenes de informes técnicos publicados en el Departamento de Ciencias de la Computación de una universidad. El conjunto de datos contenía 476 resúmenes, los cuales fueron divididos en cuatro áreas de investigación: Procesamiento de Lenguaje Natural (NLP), Robótica/Visión, Sistemas y Teoría. WebKB. El conjunto de datos WebKB contiene páginas web recopiladas de los departamentos de informática de universidades. Hay alrededor de 8280 documentos y están divididos en 7 categorías: estudiante, facultad, personal, curso, proyecto, departamento y otros. El texto sin procesar es de aproximadamente 27MB. Entre estas 7 categorías, estudiante, facultad, curso y proyecto son las cuatro categorías que representan entidades más pobladas. El subconjunto asociado suele ser llamado WebKB4. Reuters. La colección de pruebas de categorización de texto Reuters-21578 contiene documentos recopilados de la agencia de noticias Reuters en 1987. Es un punto de referencia estándar para la categorización de texto y contiene 135 categorías. En nuestros experimentos, utilizamos un subconjunto de la colección de datos que incluye las 10 categorías más frecuentes entre los 135 temas y lo llamamos Reuters-top 10. WebACE. El conjunto de datos WebACE proviene del proyecto WebACE y ha sido utilizado para la agrupación de documentos [17][5]. El conjunto de datos WebACE contiene 2340 documentos que consisten en artículos de noticias del servicio de noticias de Reuters a través de la web en octubre de 1997. Estos documentos están divididos en 20 clases. Not enough context provided. El conjunto de datos News4 utilizado en nuestros experimentos se selecciona del famoso conjunto de datos 20-newsgroups. El tema rec que contiene autos, motocicletas, béisbol y hockey fue seleccionado de la versión 20news-18828. El conjunto de datos News4 contiene 3970 vectores de documentos. Para preprocesar los conjuntos de datos, eliminamos las palabras vacías utilizando una lista estándar de palabras vacías, se omiten todas las etiquetas HTML y se ignoran todos los campos de encabezado excepto el asunto y la organización de los artículos publicados. En todos nuestros experimentos, primero seleccionamos las 1000 palabras principales por información mutua con las etiquetas de clase. 3.2 Métricas de Evaluación En los experimentos, establecemos el número de clústeres igual al número real de clases C para todos los algoritmos de agrupamiento. Para evaluar su rendimiento, comparamos los grupos generados por estos algoritmos con las clases reales mediante el cálculo de las siguientes dos medidas de rendimiento. Precisión de agrupamiento (Acc). La primera medida de rendimiento es la Precisión de Agrupamiento, que descubre la relación uno a uno entre los grupos y las clases, y mide en qué medida cada grupo contenía puntos de datos de la clase correspondiente. Resume todo el grado de coincidencia entre todos los pares de clases y clusters. La precisión del agrupamiento se puede calcular como: Acc = 1 N max   Ck,Lm T(Ck, Lm)   , (29) donde Ck denota el k-ésimo clúster en los resultados finales, y Lm es la verdadera clase m-ésima. T(Ck, Lm) es el número de entidades que pertenecen a la clase m y están asignadas al clúster k. La precisión calcula la suma máxima de T(Ck, Lm) para todos los pares de clústeres y clases, y estos pares no tienen superposiciones. La mayor precisión de agrupamiento significa un mejor rendimiento de agrupamiento. Información Mutua Normalizada (NMI). Otro métrica de evaluación que adoptamos aquí es la Información Mutua Normalizada (NMI) [23], la cual es ampliamente utilizada para determinar la calidad de los grupos. Para dos variables aleatorias X e Y, el NMI se define como: NMI(X, Y) = I(X, Y) H(X)H(Y), donde I(X, Y) es la información mutua entre X e Y, mientras que H(X) y H(Y) son las entropías de X e Y respectivamente. Se puede ver que NMI(X, X) = 1, que es el valor máximo posible de NMI. Dado un resultado de agrupamiento, el NMI en la ecuación (30) se estima como NMI = C k=1 C m=1 nk,mlog n·nk,m nk ˆnm C k=1 nklog nk n C m=1 ˆnmlog ˆnm n, (31), donde nk denota el número de datos contenidos en el grupo Ck (1 k C), ˆnm es el número de datos pertenecientes a la clase m-ésima (1 m C), y nk,m denota el número de datos que están en la intersección entre el grupo Ck y la clase m-ésima. El valor calculado en la Ecuación (31) se utiliza como medida de rendimiento para el resultado de agrupamiento dado. A mayor valor, mejor rendimiento de agrupamiento. 3.3 Comparaciones Hemos realizado evaluaciones de rendimiento exhaustivas probando nuestro método y comparándolo con otros 8 métodos representativos de agrupamiento de datos utilizando las mismas corpora de datos. Los algoritmos que evaluamos se enumeran a continuación. 1. K-means tradicional (KM). 2. K-medias esférico (SKM). La implementación se basa en [9]. 3. Modelo de Mezcla Gaussiana (GMM). La implementación se basa en [16]. 4. Agrupamiento espectral con cortes normalizados (Ncut). La implementación se basa en [26], y la varianza de la similitud gaussiana se determina mediante Escalado Local [30]. Ten en cuenta que el criterio que Ncut busca minimizar es simplemente el regularizador global en nuestro algoritmo CLGR, excepto que Ncut utiliza el Laplaciano normalizado. 5. Agrupamiento utilizando Regularización Local Pura (CPLR). En este método simplemente minimizamos Jl (definido en la Ecuación (24)), y los resultados de agrupamiento pueden obtenerse realizando la descomposición de valores propios en la matriz (I − P)T (I − P) con algunos métodos de discretización adecuados. 6. Iteración de subespacio adaptativa (ASI). La implementación se basa en [14]. 7. Factorización de matrices no negativas (NMF). La implementación se basa en [27]. 8. Factorización Tri-Factorización de Matrices No Negativas (TNMF) [12]. La implementación se basa en [15]. Para la eficiencia computacional, en la implementación de CPLR y nuestro algoritmo CLGR, hemos establecido todos los parámetros de regularización local {λi}n i=1 para ser idénticos, los cuales se establecen mediante búsqueda en cuadrícula de {0.1, 1, 10}. El tamaño de los vecindarios k más cercanos se establece mediante una búsqueda en cuadrícula de {20, 40, 80}. Para el método CLGR, su parámetro de regularización global se establece mediante una búsqueda en cuadrícula de {0.1, 1, 10}. Al construir el regularizador global, hemos adoptado el método de escalamiento local [30] para construir la matriz Laplaciana. El método de discretización final adoptado en estos dos métodos es el mismo que en [26], ya que nuestros experimentos muestran que el uso de dicho método puede lograr mejores resultados que el uso de métodos basados en kmeans como en [20]. 3.4 Resultados Experimentales Los resultados de comparación de precisión de agrupamiento se muestran en la tabla 3, y los resultados de comparación de información mutua normalizada se resumen en la tabla 4. De las dos tablas principalmente observamos que: 1. Nuestro método CLGR supera a todos los demás métodos de agrupamiento de documentos en la mayoría de los conjuntos de datos. Para la agrupación de documentos, el método Spherical k-means suele superar al método tradicional de agrupación k-means, y el método GMM puede lograr resultados competitivos en comparación con el método Spherical k-means; 3. Los resultados obtenidos de los algoritmos tipo k-means y GMM suelen ser peores que los resultados obtenidos de Clustering Espectral. Dado que el Agrupamiento Espectral puede ser visto como una versión ponderada del kernel k-means, puede obtener buenos resultados cuando los clusters de datos tienen formas arbitrarias. Esto corrobora que los vectores de los documentos no están distribuidos regularmente (esféricos o elípticos). 4. Las comparaciones experimentales verifican empíricamente la equivalencia entre NMF y Clustering Espectral, como se muestra en la Tabla 3: Precisión de agrupamiento de los diversos métodos CSTR WebKB4 Reuters WebACE News4 KM 0.4256 0.3888 0.4448 0.4001 0.3527 SKM 0.4690 0.4318 0.5025 0.4458 0.3912 GMM 0.4487 0.4271 0.4897 0.4521 0.3844 NMF 0.5713 0.4418 0.4947 0.4761 0.4213 Ncut 0.5435 0.4521 0.4896 0.4513 0.4189 ASI 0.5621 0.4752 0.5235 0.4823 0.4335 TNMF 0.6040 0.4832 0.5541 0.5102 0.4613 CPLR 0.5974 0.5020 0.4832 0.5213 0.4890 CLGR 0.6235 0.5228 0.5341 0.5376 0.5102 Tabla 4: Resultados de información mutua normalizada de los diversos métodos CSTR WebKB4 Reuters WebACE News4 KM 0.3675 0.3023 0.4012 0.3864 0.3318 SKM 0.4027 0.4155 0.4587 0.4003 0.4085 GMM 0.4034 0.4093 0.4356 0.4209 0.3994 NMF 0.5235 0.4517 0.4402 0.4359 0.4130 Ncut 0.4833 0.4497 0.4392 0.4289 0.4231 ASI 0.5008 0.4833 0.4769 0.4817 0.4503 TNMF 0.5724 0.5011 0.5132 0.5328 0.4749 CPLR 0.5695 0.5231 0.4402 0.5543 0.4690 CLGR 0.6012 0.5434 0.4935 0.5390 0.4908 ha sido demostrado teóricamente en [10]. Se puede observar en las tablas que NMF y el Agrupamiento Espectral suelen llevar a resultados de agrupamiento similares. 5. Los métodos basados en co-agrupamiento (TNMF y ASI) suelen lograr mejores resultados que los métodos tradicionales basados únicamente en vectores de documentos. Dado que estos métodos realizan una selección implícita de características en cada iteración, proporcionan una métrica adaptativa para medir el vecindario y, por lo tanto, tienden a producir mejores resultados de agrupamiento. 6. Los resultados obtenidos a través de CPLR suelen ser mejores que los resultados obtenidos a través de Agrupamiento Espectral, lo cual respalda la teoría de Vapnik [24] de que a veces los algoritmos de aprendizaje local pueden obtener mejores resultados que los algoritmos de aprendizaje global. Además de los experimentos de comparación mencionados anteriormente, también probamos la sensibilidad de los parámetros de nuestro método. Principalmente hay dos conjuntos de parámetros en nuestro algoritmo CLGR, los parámetros de regularización locales y globales ({λi}n i=1 y λ, como hemos mencionado en la sección 3.3, hemos establecido que todos los λis son idénticos a λ∗ en nuestros experimentos), y el tamaño de los vecindarios. Por lo tanto, también hemos realizado dos series de experimentos: 1. Fijando el tamaño de los vecindarios y probando el rendimiento del agrupamiento con diferentes valores de λ∗ y λ. En este conjunto de experimentos, encontramos que nuestro algoritmo CLGR puede lograr buenos resultados cuando los dos parámetros de regularización no son ni demasiado grandes ni demasiado pequeños. Normalmente nuestro método puede lograr buenos resultados cuando λ∗ y λ están alrededor de 0.1. La Figura 1 nos muestra un ejemplo de prueba en el conjunto de datos WebACE. Fijando los parámetros de regularización local y global, y probando el rendimiento del agrupamiento con diferentes valores de -5, -4.5, -4, -3.5, -3, -5, -4.5, -4, -3.5, -3, 0.35, 0.4, 0.45, 0.5, 0.55 para el parámetro de regularización local (valor logarítmico base 2) y para el parámetro de regularización global (valor logarítmico base 2), se obtuvo una precisión de agrupamiento. Figura 1: Resultados de la prueba de sensibilidad de parámetros en el conjunto de datos WebACE con el tamaño de vecindario fijo en 20, donde el eje x e y representan el valor logarítmico base 2 de λ∗ y λ. En este conjunto de experimentos, encontramos que el vecindario con un tamaño demasiado grande o demasiado pequeño deteriorará todos los resultados finales de agrupamiento. Esto puede entenderse fácilmente ya que cuando el tamaño del vecindario es muy pequeño, los puntos de datos utilizados para entrenar los clasificadores locales pueden no ser suficientes; cuando el tamaño del vecindario es muy grande, los clasificadores entrenados tenderán a ser globales y no podrán capturar las características locales típicas. La Figura 2 nos muestra un ejemplo de prueba en el conjunto de datos WebACE. Por lo tanto, podemos ver que nuestro algoritmo CLGR (1) puede lograr resultados satisfactorios y (2) no es muy sensible a la elección de parámetros, lo que lo hace práctico en aplicaciones del mundo real. CONCLUSIONES Y TRABAJOS FUTUROS En este artículo, desarrollamos un nuevo algoritmo de agrupamiento llamado agrupamiento con regularización local y global. Nuestro método conserva el mérito de los algoritmos de aprendizaje local y el agrupamiento espectral. Nuestros experimentos muestran que el algoritmo propuesto supera a la mayoría de los algoritmos de vanguardia en muchos conjuntos de datos de referencia. En el futuro, nos enfocaremos en la selección de parámetros y los problemas de aceleración del algoritmo CLGR. REFERENCIAS [1] L. Baker y A. McCallum. Agrupamiento distribucional de palabras para clasificación de texto. En Actas de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1998. [2] M. Belkin y P. Niyogi. Mapeo de Eigen de Laplaciano para Reducción de Dimensionalidad y Representación de Datos. Computación Neural, 15 (6):1373-1396. Junio de 2003. [3] M. Belkin y P. Niyogi. Hacia una Fundación Teórica para Métodos de Manifold Basados en Laplacianos. En Actas de la 18ª Conferencia sobre Teoría del Aprendizaje (COLT). 2005. 10 20 30 40 50 60 70 80 90 100 0.35 0.4 0.45 0.5 0.55 tamaño de la precisión de agrupamiento del vecindario Figura 2: Resultados de pruebas de sensibilidad de parámetros en el conjunto de datos WebACE con los parámetros de regularización fijados en 0.1, y el tamaño del vecindario variando de 10 a 100. [4] M. Belkin, P. Niyogi y V. Sindhwani. Regularización de variedades: un marco geométrico para el aprendizaje a partir de ejemplos. Revista de Investigación en Aprendizaje Automático 7, 1-48, 2006. [5] D. Boley. División Direccional Principal. Minería de datos y descubrimiento de conocimiento, 2:325-344, 1998. [6] L. Bottou y V. Vapnik. Algoritmos de aprendizaje local. Computación Neural, 4:888-900, 1992. [7] P. K. Chan, D. F. Schlag y J. Y. Zien. Particionamiento y agrupamiento K-way de corte de razón espectral. IEEE Trans. Diseño Asistido por Computadora, 13:1088-1096, Sep. 1994. [8] D. R. Cutting, D. R. Karger, J. O. Pederson y J. W. Tukey. Dispersión/Recolección: Un enfoque basado en clústeres para explorar grandes colecciones de documentos. En Actas de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1992. [9] I. S. Dhillon y D. S. Modha. Descomposiciones de conceptos para datos de texto grandes y dispersos utilizando agrupamiento. Aprendizaje automático, vol. 42(1), páginas 143-175, enero de 2001. [10] C. Ding, X. Él, y H. Simon. Sobre la equivalencia entre la factorización de matrices no negativas y el agrupamiento espectral. En Actas de la Conferencia de Minería de Datos de SIAM, 2005. [11] C. Ding, X. Él, H. Zha, M. Gu y H. D. Simon. Un algoritmo de corte min-max para particionar grafos y agrupar datos. En Proc. de la 1ra Conferencia Internacional sobre Minería de Datos (ICDM), páginas 107-114, 2001. [12] C. Ding, T. Li, W. Peng y H. Park. Tri-factorizaciones de matrices no negativas ortogonales para agrupamiento. En Actas de la Duodécima Conferencia Internacional de la ACM SIGKDD sobre Descubrimiento de Conocimiento y Minería de Datos, 2006. [13] R. O. Duda, P. E. Hart y D. G. Stork. Clasificación de patrones. John Wiley & Sons, Inc., 2001. [14] T. Li, S. Ma y M. Ogihara. Agrupamiento de documentos a través de la Iteración de Subespacios Adaptativos. En Actas de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2004. [15] T. Li y C. Ding. Las relaciones entre varios métodos de factorización de matrices no negativas para agrupamiento. En Actas de la 6ta Conferencia Internacional sobre Minería de Datos (ICDM). 2006. [16] X. Liu y Y. Gong. Agrupación de documentos con capacidades de refinamiento de clústeres y selección de modelos. En Proc. de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2002. [17] E. Han, D. Boley, M. Gini, R. Gross, K. Hastings, G. Karypis, V. Kumar, B. Mobasher y J. Moore. WebACE: Un agente web para la categorización y exploración de documentos. En Actas de la 2ª Conferencia Internacional sobre Agentes Autónomos (Agents98). ACM Press, 1998. [18] M. Hein, J. Y. Audibert y U. von Luxburg. De Gráficas a Variedades - Consistencia Puntual Débil y Fuerte de los Laplacianos de Gráficas. En Actas de la 18ª Conferencia sobre Teoría del Aprendizaje (COLT), 470-485. 2005. [19] J. Él, M. Lan, C.-L. Tan, S.-Y. Sung, y H.-B. Bajo. Inicialización de algoritmos de refinamiento de clústeres: una revisión y estudio comparativo. En Proc. de Inter. Conferencia Conjunta sobre Redes Neuronales, 2004. [20] A. Y. Ng, M. I. Jordan, Y. Weiss. En el agrupamiento espectral: análisis y un algoritmo. En Avances en Sistemas de Procesamiento de Información Neural 14. 2002. [21] B. Sch¨olkopf y A. Smola. Aprendizaje con Kernels. El MIT Press. Cambridge, Massachusetts. 2002. [22] J. Shi y J. Malik. Cortes normalizados y segmentación de imágenes. IEEE Trans. on Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000. [23] A. Strehl and J. Ghosh.\nIEEE Trans. on Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000. [23] A. Strehl y J. Ghosh. Conjuntos de agrupamientos: un marco de reutilización de conocimiento para combinar múltiples particiones. Revista de Investigación de Aprendizaje Automático, 3:583-617, 2002. [24] V. N. Vapnik. La naturaleza de la teoría del aprendizaje estadístico. Berlín: Springer-Verlag, 1995. [25] Wu, M. y Schölkopf, B. Un enfoque de aprendizaje local para el agrupamiento. En Avances en Sistemas de Procesamiento de Información Neural 18. 2006. [26] S. X. Yu, J. Shi. Agrupamiento espectral multiclase. En Actas de la Conferencia Internacional sobre Visión por Computadora, 2003. [27] W. Xu, X. Liu y Y. Gong. Agrupación de documentos basada en la factorización de matrices no negativas. En Actas de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2003. [28] H. Zha, X. Él, C. Ding, M. Gu y H. Simon. Relajación espectral para el agrupamiento K-means. En NIPS 14. 2001. [29] T. Zhang y F. J. Oles. Categorización de texto basada en métodos de clasificación lineal regularizados. Revista de Recuperación de Información, 4:5-31, 2001. [30] L. Zelnik-Manor y P. Perona. Agrupamiento espectral autoajustable. En NIPS 17. 2005. [31] D. Zhou, O. Bousquet, T. N. Lal, J. Weston y B. Schölkopf. Aprendizaje con Consistencia Local y Global. NIPS 17, 2005. \n\nNIPS 17, 2005.",
    "original_sentences": [
        "Regularized Clustering for Documents ∗ Fei Wang, Changshui Zhang State Key Lab of Intelligent Tech.",
        "and Systems Department of Automation, Tsinghua University Beijing, China, 100084 feiwang03@gmail.com Tao Li School of Computer Science Florida International University Miami, FL 33199, U.S.A. taoli@cs.fiu.edu ABSTRACT In recent years, document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering.",
        "In this paper, we propose a novel method for clustering documents using regularization.",
        "Unlike traditional globally regularized clustering methods, our method first construct a local regularized linear label predictor for each document vector, and then combine all those local regularizers with a global smoothness regularizer.",
        "So we call our algorithm Clustering with Local and Global Regularization (CLGR).",
        "We will show that the cluster memberships of the documents can be achieved by eigenvalue decomposition of a sparse symmetric matrix, which can be efficiently solved by iterative methods.",
        "Finally our experimental evaluations on several datasets are presented to show the superiorities of CLGR over traditional document clustering methods.",
        "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Clustering; I.2.6 [Artificial Intelligence]: Learning-Concept Learning General Terms Algorithms 1.",
        "INTRODUCTION Document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering.",
        "A good document clustering approach can assist the computers to automatically organize the document corpus into a meaningful cluster hierarchy for efficient browsing and navigation, which is very valuable for complementing the deficiencies of traditional information retrieval technologies.",
        "As pointed out by [8], the information retrieval needs can be expressed by a spectrum ranged from narrow keyword-matching based search to broad information browsing such as what are the major international events in recent months.",
        "Traditional document retrieval engines tend to fit well with the search end of the spectrum, i.e. they usually provide specified search for documents matching the users query, however, it is hard for them to meet the needs from the rest of the spectrum in which a rather broad or vague information is needed.",
        "In such cases, efficient browsing through a good cluster hierarchy will be definitely helpful.",
        "Generally, document clustering methods can be mainly categorized into two classes: hierarchical methods and partitioning methods.",
        "The hierarchical methods group the data points into a hierarchical tree structure using bottom-up or top-down approaches.",
        "For example, hierarchical agglomerative clustering (HAC) [13] is a typical bottom-up hierarchical clustering method.",
        "It takes each data point as a single cluster to start off with and then builds bigger and bigger clusters by grouping similar data points together until the entire dataset is encapsulated into one final cluster.",
        "On the other hand, partitioning methods decompose the dataset into a number of disjoint clusters which are usually optimal in terms of some predefined criterion functions.",
        "For instance, K-means [13] is a typical partitioning method which aims to minimize the sum of the squared distance between the data points and their corresponding cluster centers.",
        "In this paper, we will focus on the partitioning methods.",
        "As we know that there are two main problems existing in partitioning methods (like Kmeans and Gaussian Mixture Model (GMM) [16]): (1) the predefined criterion is usually non-convex which causes many local optimal solutions; (2) the iterative procedure (e.g. the Expectation Maximization (EM) algorithm) for optimizing the criterions usually makes the final solutions heavily depend on the initializations.",
        "In the last decades, many methods have been proposed to overcome the above problems of the partitioning methods [19][28].",
        "Recently, another type of partitioning methods based on clustering on data graphs have aroused considerable interests in the machine learning and data mining community.",
        "The basic idea behind these methods is to first model the whole dataset as a weighted graph, in which the graph nodes represent the data points, and the weights on the edges correspond to the similarities between pairwise points.",
        "Then the cluster assignments of the dataset can be achieved by optimizing some criterions defined on the graph.",
        "For example Spectral Clustering is one kind of the most representative graph-based clustering approaches, it generally aims to optimize some cut value (e.g.",
        "Normalized Cut [22], Ratio Cut [7], Min-Max Cut [11]) defined on an undirected graph.",
        "After some relaxations, these criterions can usually be optimized via eigen-decompositions, which is guaranteed to be global optimal.",
        "In this way, spectral clustering efficiently avoids the problems of the traditional partitioning methods as we introduced in last paragraph.",
        "In this paper, we propose a novel document clustering algorithm that inherits the superiority of spectral clustering, i.e. the final cluster results can also be obtained by exploit the eigen-structure of a symmetric matrix.",
        "However, unlike spectral clustering, which just enforces a smoothness constraint on the data labels over the whole data manifold [2], our method first construct a regularized linear label predictor for each data point from its neighborhood as in [25], and then combine the results of all these local label predictors with a global label smoothness regularizer.",
        "So we call our method Clustering with Local and Global Regularization (CLGR).",
        "The idea of incorporating both local and global information into label prediction is inspired by the recent works on semi-supervised learning [31], and our experimental evaluations on several real document datasets show that CLGR performs better than many state-of-the-art clustering methods.",
        "The rest of this paper is organized as follows: in section 2 we will introduce our CLGR algorithm in detail.",
        "The experimental results on several datasets are presented in section 3, followed by the conclusions and discussions in section 4. 2.",
        "THE PROPOSED ALGORITHM In this section, we will introduce our Clustering with Local and Global Regularization (CLGR) algorithm in detail.",
        "First lets see the how the documents are represented throughout this paper. 2.1 Document Representation In our work, all the documents are represented by the weighted term-frequency vectors.",
        "Let W = {w1, w2, · · · , wm} be the complete vocabulary set of the document corpus (which is preprocessed by the stopwords removal and words stemming operations).",
        "The term-frequency vector xi of document di is defined as xi = [xi1, xi2, · · · , xim]T , xik = tik log n idfk , where tik is the term frequency of wk ∈ W, n is the size of the document corpus, idfk is the number of documents that contain word wk.",
        "In this way, xi is also called the TFIDF representation of document di.",
        "Furthermore, we also normalize each xi (1 i n) to have a unit length, so that each document is represented by a normalized TF-IDF vector. 2.2 Local Regularization As its name suggests, CLGR is composed of two parts: local regularization and global regularization.",
        "In this subsection we will introduce the local regularization part in detail. 2.2.1 Motivation As we know that clustering is one type of learning techniques, it aims to organize the dataset in a reasonable way.",
        "Generally speaking, learning can be posed as a problem of function estimation, from which we can get a good classification function that will assign labels to the training dataset and even the unseen testing dataset with some cost minimized [24].",
        "For example, in the two-class classification scenario1 (in which we exactly know the label of each document), a linear classifier with least square fit aims to learn a column vector w such that the squared cost J = 1 n (wT xi − yi)2 (1) is minimized, where yi ∈ {+1, −1} is the label of xi.",
        "By taking ∂J /∂w = 0, we get the solution w∗ = n i=1 xixT i −1 n i=1 xiyi , (2) which can further be written in its matrix form as w∗ = XXT −1 Xy, (3) where X = [x1, x2, · · · , xn] is an m × n document matrix, y = [y1, y2, · · · , yn]T is the label vector.",
        "Then for a test document t, we can determine its label by l = sign(w∗T u), (4) where sign(·) is the sign function.",
        "A natural problem in Eq. (3) is that the matrix XXT may be singular and thus not invertable (e.g. when m n).",
        "To avoid such a problem, we can add a regularization term and minimize the following criterion J = 1 n n i=1 (wT xi − yi)2 + λ w 2 , (5) where λ is a regularization parameter.",
        "Then the optimal solution that minimize J is given by w∗ = XXT + λnI −1 Xy, (6) where I is an m × m identity matrix.",
        "It has been reported that the regularized linear classifier can achieve very good results on text classification problems [29].",
        "However, despite its empirical success, the regularized linear classifier is on earth a global classifier, i.e. w∗ is estimated using the whole training set.",
        "According to [24], this may not be a smart idea, since a unique w∗ may not be good enough for predicting the labels of the whole input space.",
        "In order to get better predictions, [6] proposed to train classifiers locally and use them to classify the testing points.",
        "For example, a testing point will be classified by the local classifier trained using the training points located in the vicinity 1 In the following discussions we all assume that the documents coming from only two classes.",
        "The generalizations of our method to multi-class cases will be discussed in section 2.5. of it.",
        "Although this method seems slow and stupid, it is reported that it can get better performances than using a unique global classifier on certain tasks [6]. 2.2.2 Constructing the Local Regularized Predictors Inspired by their success, we proposed to apply the local learning algorithms for clustering.",
        "The basic idea is that, for each document vector xi (1 i n), we train a local label predictor based on its k-nearest neighborhood Ni, and then use it to predict the label of xi.",
        "Finally we will combine all those local predictors by minimizing the sum of their prediction errors.",
        "In this subsection we will introduce how to construct those local predictors.",
        "Due to the simplicity and effectiveness of the regularized linear classifier that we have introduced in section 2.2.1, we choose it to be our local label predictor, such that for each document xi, the following criterion is minimized Ji = 1 ni xj ∈Ni wT i xj − qj 2 + λi wi 2 , (7) where ni = |Ni| is the cardinality of Ni, and qj is the cluster membership of xj.",
        "Then using Eq. (6), we can get the optimal solution is w∗ i = XiXT i + λiniI −1 Xiqi, (8) where Xi = [xi1, xi2, · · · , xini ], and we use xik to denote the k-th nearest neighbor of xi. qi = [qi1, qi2, · · · , qini ]T with qik representing the cluster assignment of xik.",
        "The problem here is that XiXT i is an m × m matrix with m ni, i.e. we should compute the inverse of an m × m matrix for every document vector, which is computationally prohibited.",
        "Fortunately, we have the following theorem: Theorem 1. w∗ i in Eq. (8) can be rewritten as w∗ i = Xi XT i Xi + λiniIi −1 qi, (9) where Ii is an ni × ni identity matrix.",
        "Proof.",
        "Since w∗ i = XiXT i + λiniI −1 Xiqi, then XiXT i + λiniI w∗ i = Xiqi =⇒ XiXT i w∗ i + λiniw∗ i = Xiqi =⇒ w∗ i = (λini)−1 Xi qi − XT i w∗ i .",
        "Let β = (λini)−1 qi − XT i w∗ i , then w∗ i = Xiβ =⇒ λiniβ = qi − XT i w∗ i = qi − XT i Xiβ =⇒ qi = XT i Xi + λiniIi β =⇒ β = XT i Xi + λiniIi −1 qi.",
        "Therefore w∗ i = Xiβ = Xi XT i Xi + λiniIi −1 qi 2 Using theorem 1, we only need to compute the inverse of an ni × ni matrix for every document to train a local label predictor.",
        "Moreover, for a new testing point u that falls into Ni, we can classify it by the sign of qu = w∗T i u = uT wi = uT Xi XT i Xi + λiniIi −1 qi.",
        "This is an attractive expression since we can determine the cluster assignment of u by using the inner-products between the points in {u ∪ Ni}, which suggests that such a local regularizer can easily be kernelized [21] as long as we define a proper kernel function. 2.2.3 Combining the Local Regularized Predictors After all the local predictors having been constructed, we will combine them together by minimizing Jl = n i=1 w∗T i xi − qi 2 , (10) which stands for the sum of the prediction errors for all the local predictors.",
        "Combining Eq. (10) with Eq. (6), we can get Jl = n i=1 w∗T i xi − qi 2 = n i=1 xT i Xi XT i Xi + λiniIi −1 qi − qi 2 = Pq − q 2 , (11) where q = [q1, q2, · · · , qn]T , and the P is an n × n matrix constructing in the following way.",
        "Let αi = xT i Xi XT i Xi + λiniIi −1 , then Pij = αi j, if xj ∈ Ni 0, otherwise , (12) where Pij is the (i, j)-th entry of P, and αi j represents the j-th entry of αi .",
        "Till now we can write the criterion of clustering by combining locally regularized linear label predictors Jl in an explicit mathematical form, and we can minimize it directly using some standard optimization techniques.",
        "However, the results may not be good enough since we only exploit the local informations of the dataset.",
        "In the next subsection, we will introduce a global regularization criterion and combine it with Jl, which aims to find a good clustering result in a local-global way. 2.3 Global Regularization In data clustering, we usually require that the cluster assignments of the data points should be sufficiently smooth with respect to the underlying data manifold, which implies (1) the nearby points tend to have the same cluster assignments; (2) the points on the same structure (e.g. submanifold or cluster) tend to have the same cluster assignments [31].",
        "Without the loss of generality, we assume that the data points reside (roughly) on a low-dimensional manifold M2 , and q is the cluster assignment function defined on M, i.e. 2 We believe that the text data are also sampled from some low dimensional manifold, since it is impossible for them to for ∀x ∈ M, q(x) returns the cluster membership of x.",
        "The smoothness of q over M can be calculated by the following Dirichlet integral [2] D[q] = 1 2 M q(x) 2 dM, (13) where the gradient q is a vector in the tangent space T Mx, and the integral is taken with respect to the standard measure on M. If we restrict the scale of q by q, q M = 1 (where ·, · M is the inner product induced on M), then it turns out that finding the smoothest function minimizing D[q] reduces to finding the eigenfunctions of the Laplace Beltrami operator L, which is defined as Lq −div q, (14) where div is the divergence of a vector field.",
        "Generally, the graph can be viewed as the discretized form of manifold.",
        "We can model the dataset as an weighted undirected graph as in spectral clustering [22], where the graph nodes are just the data points, and the weights on the edges represent the similarities between pairwise points.",
        "Then it can be shown that minimizing Eq. (13) corresponds to minimizing Jg = qT Lq = n i=1 (qi − qj)2 wij, (15) where q = [q1, q2, · · · , qn]T with qi = q(xi), L is the graph Laplacian with its (i, j)-th entry Lij =    di − wii, if i = j −wij, if xi and xj are adjacent 0, otherwise, (16) where di = j wij is the degree of xi, wij is the similarity between xi and xj.",
        "If xi and xj are adjacent3 , wij is usually computed in the following way wij = e − xi−xj 2 2σ2 , (17) where σ is a dataset dependent parameter.",
        "It is proved that under certain conditions, such a form of wij to determine the weights on graph edges leads to the convergence of graph Laplacian to the Laplace Beltrami operator [3][18].",
        "In summary, using Eq. (15) with exponential weights can effectively measure the smoothness of the data assignments with respect to the intrinsic data manifold.",
        "Thus we adopt it as a global regularizer to punish the smoothness of the predicted data assignments. 2.4 Clustering with Local and Global Regularization Combining the contents we have introduced in section 2.2 and section 2.3 we can derive the clustering criterion is minq J = Jl + λJg = Pq − q 2 + λqT Lq s.t. qi ∈ {−1, +1}, (18) where P is defined as in Eq. (12), and λ is a regularization parameter to trade off Jl and Jg.",
        "However, the discrete fill in the whole high-dimensional sample space.",
        "And it has been shown that the manifold based methods can achieve good results on text classification tasks [31]. 3 In this paper, we define xi and xj to be adjacent if xi ∈ N(xj) or xj ∈ N(xi). constraint of pi makes the problem an NP hard integer programming problem.",
        "A natural way for making the problem solvable is to remove the constraint and relax qi to be continuous, then the objective that we aims to minimize becomes J = Pq − q 2 + λqT Lq = qT (P − I)T (P − I)q + λqT Lq = qT (P − I)T (P − I) + λL q, (19) and we further add a constraint qT q = 1 to restrict the scale of q.",
        "Then our objective becomes minq J = qT (P − I)T (P − I) + λL q s.t. qT q = 1 (20) Using the Lagrangian method, we can derive that the optimal solution q corresponds to the smallest eigenvector of the matrix M = (P − I)T (P − I) + λL, and the cluster assignment of xi can be determined by the sign of qi, i.e. xi will be classified as class one if qi > 0, otherwise it will be classified as class 2. 2.5 Multi-Class CLGR In the above we have introduced the basic framework of Clustering with Local and Global Regularization (CLGR) for the two-class clustering problem, and we will extending it to multi-class clustering in this subsection.",
        "First we assume that all the documents belong to C classes indexed by L = {1, 2, · · · , C}. qc is the classification function for class c (1 c C), such that qc (xi) returns the confidence that xi belongs to class c. Our goal is to obtain the value of qc (xi) (1 c C, 1 i n), and the cluster assignment of xi can be determined by {qc (xi)}C c=1 using some proper discretization methods that we will introduce later.",
        "Therefore, in this multi-class case, for each document xi (1 i n), we will construct C locally linear regularized label predictors whose normal vectors are wc∗ i = Xi XT i Xi + λiniIi −1 qc i (1 c C), (21) where Xi = [xi1, xi2, · · · , xini ] with xik being the k-th neighbor of xi, and qc i = [qc i1, qc i2, · · · , qc ini ]T with qc ik = qc (xik).",
        "Then (wc∗ i )T xi returns the predicted confidence of xi belonging to class c. Hence the local prediction error for class c can be defined as J c l = n i=1 (wc∗ i ) T xi − qc i 2 , (22) And the total local prediction error becomes Jl = C c=1 J c l = C c=1 n i=1 (wc∗ i ) T xi − qc i 2 . (23) As in Eq. (11), we can define an n×n matrix P (see Eq. (12)) and rewrite Jl as Jl = C c=1 J c l = C c=1 Pqc − qc 2 . (24) Similarly we can define the global smoothness regularizer in multi-class case as Jg = C c=1 n i=1 (qc i − qc j )2 wij = C c=1 (qc )T Lqc . (25) Then the criterion to be minimized for CLGR in multi-class case becomes J = Jl + λJg = C c=1 Pqc − qc 2 + λ(qc )T Lqc = C c=1 (qc )T (P − I)T (P − I) + λL qc = trace QT (P − I)T (P − I) + λL Q , (26) where Q = [q1 , q2 , · · · , qc ] is an n × c matrix, and trace(·) returns the trace of a matrix.",
        "The same as in Eq. (20), we also add the constraint that QT Q = I to restrict the scale of Q.",
        "Then our optimization problem becomes minQ J = trace QT (P − I)T (P − I) + λL Q s.t.",
        "QT Q = I, (27) From the Ky Fan theorem [28], we know the optimal solution of the above problem is Q∗ = [q∗ 1, q∗ 2, · · · , q∗ C ]R, (28) where q∗ k (1 k C) is the eigenvector corresponds to the k-th smallest eigenvalue of matrix (P − I)T (P − I) + λL, and R is an arbitrary C × C matrix.",
        "Since the values of the entries in Q∗ is continuous, we need to further discretize Q∗ to get the cluster assignments of all the data points.",
        "There are mainly two approaches to achieve this goal: 1.",
        "As in [20], we can treat the i-th row of Q as the embedding of xi in a C-dimensional space, and apply some traditional clustering methods like kmeans to clustering these embeddings into C clusters. 2.",
        "Since the optimal Q∗ is not unique (because of the existence of an arbitrary matrix R), we can pursue an optimal R that will rotate Q∗ to an indication matrix4 .",
        "The detailed algorithm can be referred to [26].",
        "The detailed algorithm procedure for CLGR is summarized in table 1. 3.",
        "EXPERIMENTS In this section, experiments are conducted to empirically compare the clustering results of CLGR with other 8 representitive document clustering algorithms on 5 datasets.",
        "First we will introduce the basic informations of those datasets. 3.1 Datasets We use a variety of datasets, most of which are frequently used in the information retrieval research.",
        "Table 2 summarizes the characteristics of the datasets. 4 Here an indication matrix T is a n×c matrix with its (i, j)th entry Tij ∈ {0, 1} such that for each row of Q∗ there is only one 1.",
        "Then the xi can be assigned to the j-th cluster such that j = argjQ∗ ij = 1.",
        "Table 1: Clustering with Local and Global Regularization (CLGR) Input: 1.",
        "Dataset X = {xi}n i=1; 2.",
        "Number of clusters C; 3.",
        "Size of the neighborhood K; 4.",
        "Local regularization parameters {λi}n i=1; 5.",
        "Global regularization parameter λ; Output: The cluster membership of each data point.",
        "Procedure: 1.",
        "Construct the K nearest neighborhoods for each data point; 2.",
        "Construct the matrix P using Eq. (12); 3.",
        "Construct the Laplacian matrix L using Eq. (16); 4.",
        "Construct the matrix M = (P − I)T (P − I) + λL; 5.",
        "Do eigenvalue decomposition on M, and construct the matrix Q∗ according to Eq. (28); 6.",
        "Output the cluster assignments of each data point by properly discretize Q∗ .",
        "Table 2: Descriptions of the document datasets Datasets Number of documents Number of classes CSTR 476 4 WebKB4 4199 4 Reuters 2900 10 WebACE 2340 20 Newsgroup4 3970 4 CSTR.",
        "This is the dataset of the abstracts of technical reports published in the Department of Computer Science at a university.",
        "The dataset contained 476 abstracts, which were divided into four research areas: Natural Language Processing(NLP), Robotics/Vision, Systems, and Theory.",
        "WebKB.",
        "The WebKB dataset contains webpages gathered from university computer science departments.",
        "There are about 8280 documents and they are divided into 7 categories: student, faculty, staff, course, project, department and other.",
        "The raw text is about 27MB.",
        "Among these 7 categories, student, faculty, course and project are four most populous entity-representing categories.",
        "The associated subset is typically called WebKB4.",
        "Reuters.",
        "The Reuters-21578 Text Categorization Test collection contains documents collected from the Reuters newswire in 1987.",
        "It is a standard text categorization benchmark and contains 135 categories.",
        "In our experiments, we use a subset of the data collection which includes the 10 most frequent categories among the 135 topics and we call it Reuters-top 10.",
        "WebACE.",
        "The WebACE dataset was from WebACE project and has been used for document clustering [17][5].",
        "The WebACE dataset contains 2340 documents consisting news articles from Reuters new service via the Web in October 1997.",
        "These documents are divided into 20 classes.",
        "News4.",
        "The News4 dataset used in our experiments are selected from the famous 20-newsgroups dataset5 .",
        "The topic rec containing autos, motorcycles, baseball and hockey was selected from the version 20news-18828.",
        "The News4 dataset contains 3970 document vectors. 5 http://people.csail.mit.edu/jrennie/20Newsgroups/ To pre-process the datasets, we remove the stop words using a standard stop list, all HTML tags are skipped and all header fields except subject and organization of the posted articles are ignored.",
        "In all our experiments, we first select the top 1000 words by mutual information with class labels. 3.2 Evaluation Metrics In the experiments, we set the number of clusters equal to the true number of classes C for all the clustering algorithms.",
        "To evaluate their performance, we compare the clusters generated by these algorithms with the true classes by computing the following two performance measures.",
        "Clustering Accuracy (Acc).",
        "The first performance measure is the Clustering Accuracy, which discovers the one-toone relationship between clusters and classes and measures the extent to which each cluster contained data points from the corresponding class.",
        "It sums up the whole matching degree between all pair class-clusters.",
        "Clustering accuracy can be computed as: Acc = 1 N max   Ck,Lm T(Ck, Lm)   , (29) where Ck denotes the k-th cluster in the final results, and Lm is the true m-th class.",
        "T(Ck, Lm) is the number of entities which belong to class m are assigned to cluster k. Accuracy computes the maximum sum of T(Ck, Lm) for all pairs of clusters and classes, and these pairs have no overlaps.",
        "The greater clustering accuracy means the better clustering performance.",
        "Normalized Mutual Information (NMI).",
        "Another evaluation metric we adopt here is the Normalized Mutual Information NMI [23], which is widely used for determining the quality of clusters.",
        "For two random variable X and Y, the NMI is defined as: NMI(X, Y) = I(X, Y) H(X)H(Y) , (30) where I(X, Y) is the mutual information between X and Y, while H(X) and H(Y) are the entropies of X and Y respectively.",
        "One can see that NMI(X, X) = 1, which is the maximal possible value of NMI.",
        "Given a clustering result, the NMI in Eq. (30) is estimated as NMI = C k=1 C m=1 nk,mlog n·nk,m nk ˆnm C k=1 nklog nk n C m=1 ˆnmlog ˆnm n , (31) where nk denotes the number of data contained in the cluster Ck (1 k C), ˆnm is the number of data belonging to the m-th class (1 m C), and nk,m denotes the number of data that are in the intersection between the cluster Ck and the m-th class.",
        "The value calculated in Eq. (31) is used as a performance measure for the given clustering result.",
        "The larger this value, the better the clustering performance. 3.3 Comparisons We have conducted comprehensive performance evaluations by testing our method and comparing it with 8 other representative data clustering methods using the same data corpora.",
        "The algorithms that we evaluated are listed below. 1.",
        "Traditional k-means (KM). 2.",
        "Spherical k-means (SKM).",
        "The implementation is based on [9]. 3.",
        "Gaussian Mixture Model (GMM).",
        "The implementation is based on [16]. 4.",
        "Spectral Clustering with Normalized Cuts (Ncut).",
        "The implementation is based on [26], and the variance of the Gaussian similarity is determined by Local Scaling [30].",
        "Note that the criterion that Ncut aims to minimize is just the global regularizer in our CLGR algorithm except that Ncut used the normalized Laplacian. 5.",
        "Clustering using Pure Local Regularization (CPLR).",
        "In this method we just minimize Jl (defined in Eq. (24)), and the clustering results can be obtained by doing eigenvalue decomposition on matrix (I − P)T (I − P) with some proper discretization methods. 6.",
        "Adaptive Subspace Iteration (ASI).",
        "The implementation is based on [14]. 7.",
        "Nonnegative Matrix Factorization (NMF).",
        "The implementation is based on [27]. 8.",
        "Tri-Factorization Nonnegative Matrix Factorization (TNMF) [12].",
        "The implementation is based on [15].",
        "For computational efficiency, in the implementation of CPLR and our CLGR algorithm, we have set all the local regularization parameters {λi}n i=1 to be identical, which is set by grid search from {0.1, 1, 10}.",
        "The size of the k-nearest neighborhoods is set by grid search from {20, 40, 80}.",
        "For the CLGR method, its global regularization parameter is set by grid search from {0.1, 1, 10}.",
        "When constructing the global regularizer, we have adopted the local scaling method [30] to construct the Laplacian matrix.",
        "The final discretization method adopted in these two methods is the same as in [26], since our experiments show that using such method can achieve better results than using kmeans based methods as in [20]. 3.4 Experimental Results The clustering accuracies comparison results are shown in table 3, and the normalized mutual information comparison results are summarized in table 4.",
        "From the two tables we mainly observe that: 1.",
        "Our CLGR method outperforms all other document clustering methods in most of the datasets; 2.",
        "For document clustering, the Spherical k-means method usually outperforms the traditional k-means clustering method, and the GMM method can achieve competitive results compared to the Spherical k-means method; 3.",
        "The results achieved from the k-means and GMM type algorithms are usually worse than the results achieved from Spectral Clustering.",
        "Since Spectral Clustering can be viewed as a weighted version of kernel k-means, it can obtain good results the data clusters are arbitrarily shaped.",
        "This corroborates that the documents vectors are not regularly distributed (spherical or elliptical). 4.",
        "The experimental comparisons empirically verify the equivalence between NMF and Spectral Clustering, which Table 3: Clustering accuracies of the various methods CSTR WebKB4 Reuters WebACE News4 KM 0.4256 0.3888 0.4448 0.4001 0.3527 SKM 0.4690 0.4318 0.5025 0.4458 0.3912 GMM 0.4487 0.4271 0.4897 0.4521 0.3844 NMF 0.5713 0.4418 0.4947 0.4761 0.4213 Ncut 0.5435 0.4521 0.4896 0.4513 0.4189 ASI 0.5621 0.4752 0.5235 0.4823 0.4335 TNMF 0.6040 0.4832 0.5541 0.5102 0.4613 CPLR 0.5974 0.5020 0.4832 0.5213 0.4890 CLGR 0.6235 0.5228 0.5341 0.5376 0.5102 Table 4: Normalized mutual information results of the various methods CSTR WebKB4 Reuters WebACE News4 KM 0.3675 0.3023 0.4012 0.3864 0.3318 SKM 0.4027 0.4155 0.4587 0.4003 0.4085 GMM 0.4034 0.4093 0.4356 0.4209 0.3994 NMF 0.5235 0.4517 0.4402 0.4359 0.4130 Ncut 0.4833 0.4497 0.4392 0.4289 0.4231 ASI 0.5008 0.4833 0.4769 0.4817 0.4503 TNMF 0.5724 0.5011 0.5132 0.5328 0.4749 CPLR 0.5695 0.5231 0.4402 0.5543 0.4690 CLGR 0.6012 0.5434 0.4935 0.5390 0.4908 has been proved theoretically in [10].",
        "It can be observed from the tables that NMF and Spectral Clustering usually lead to similar clustering results. 5.",
        "The co-clustering based methods (TNMF and ASI) can usually achieve better results than traditional purely document vector based methods.",
        "Since these methods perform an implicit feature selection at each iteration, provide an adaptive metric for measuring the neighborhood, and thus tend to yield better clustering results. 6.",
        "The results achieved from CPLR are usually better than the results achieved from Spectral Clustering, which supports Vapniks theory [24] that sometimes local learning algorithms can obtain better results than global learning algorithms.",
        "Besides the above comparison experiments, we also test the parameter sensibility of our method.",
        "There are mainly two sets of parameters in our CLGR algorithm, the local and global regularization parameters ({λi}n i=1 and λ, as we have said in section 3.3, we have set all λis to be identical to λ∗ in our experiments), and the size of the neighborhoods.",
        "Therefore we have also done two sets of experiments: 1.",
        "Fixing the size of the neighborhoods, and testing the clustering performance with varying λ∗ and λ.",
        "In this set of experiments, we find that our CLGR algorithm can achieve good results when the two regularization parameters are neither too large nor too small.",
        "Typically our method can achieve good results when λ∗ and λ are around 0.1.",
        "Figure 1 shows us such a testing example on the WebACE dataset. 2.",
        "Fixing the local and global regularization parameters, and testing the clustering performance with different −5 −4.5 −4 −3.5 −3 −5 −4.5 −4 −3.5 −3 0.35 0.4 0.45 0.5 0.55 local regularization para (log 2 value) global regularization para (log 2 value) clusteringaccuracy Figure 1: Parameter sensibility testing results on the WebACE dataset with the neighborhood size fixed to 20, and the x-axis and y-axis represents the log2 value of λ∗ and λ. sizes of neighborhoods.",
        "In this set of experiments, we find that the neighborhood with a too large or too small size will all deteriorate the final clustering results.",
        "This can be easily understood since when the neighborhood size is very small, then the data points used for training the local classifiers may not be sufficient; when the neighborhood size is very large, the trained classifiers will tend to be global and cannot capture the typical local characteristics.",
        "Figure 2 shows us a testing example on the WebACE dataset.",
        "Therefore, we can see that our CLGR algorithm (1) can achieve satisfactory results and (2) is not very sensitive to the choice of parameters, which makes it practical in real world applications. 4.",
        "CONCLUSIONS AND FUTURE WORKS In this paper, we derived a new clustering algorithm called clustering with local and global regularization.",
        "Our method preserves the merit of local learning algorithms and spectral clustering.",
        "Our experiments show that the proposed algorithm outperforms most of the state of the art algorithms on many benchmark datasets.",
        "In the future, we will focus on the parameter selection and acceleration issues of the CLGR algorithm. 5.",
        "REFERENCES [1] L. Baker and A. McCallum.",
        "Distributional Clustering of Words for Text Classification.",
        "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 1998. [2] M. Belkin and P. Niyogi.",
        "Laplacian Eigenmaps for Dimensionality Reduction and Data Representation.",
        "Neural Computation, 15 (6):1373-1396.",
        "June 2003. [3] M. Belkin and P. Niyogi.",
        "Towards a Theoretical Foundation for Laplacian-Based Manifold Methods.",
        "In Proceedings of the 18th Conference on Learning Theory (COLT). 2005. 10 20 30 40 50 60 70 80 90 100 0.35 0.4 0.45 0.5 0.55 size of the neighborhood clusteringaccuracy Figure 2: Parameter sensibility testing results on the WebACE dataset with the regularization parameters being fixed to 0.1, and the neighborhood size varing from 10 to 100. [4] M. Belkin, P. Niyogi and V. Sindhwani.",
        "Manifold Regularization: a Geometric Framework for Learning from Examples.",
        "Journal of Machine Learning Research 7, 1-48, 2006. [5] D. Boley.",
        "Principal Direction Divisive Partitioning.",
        "Data mining and knowledge discovery, 2:325-344, 1998. [6] L. Bottou and V. Vapnik.",
        "Local learning algorithms.",
        "Neural Computation, 4:888-900, 1992. [7] P. K. Chan, D. F. Schlag and J. Y. Zien.",
        "Spectral K-way Ratio-Cut Partitioning and Clustering.",
        "IEEE Trans.",
        "Computer-Aided Design, 13:1088-1096, Sep. 1994. [8] D. R. Cutting, D. R. Karger, J. O. Pederson and J. W. Tukey.",
        "Scatter/Gather: A Cluster-Based Approach to Browsing Large Document Collections.",
        "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 1992. [9] I. S. Dhillon and D. S. Modha.",
        "Concept Decompositions for Large Sparse Text Data using Clustering.",
        "Machine Learning, vol. 42(1), pages 143-175, January 2001. [10] C. Ding, X.",
        "He, and H. Simon.",
        "On the equivalence of nonnegative matrix factorization and spectral clustering.",
        "In Proceedings of the SIAM Data Mining Conference, 2005. [11] C. Ding, X.",
        "He, H. Zha, M. Gu, and H. D. Simon.",
        "A min-max cut algorithm for graph partitioning and data clustering.",
        "In Proc. of the 1st International Conference on Data Mining (ICDM), pages 107-114, 2001. [12] C. Ding, T. Li, W. Peng, and H. Park.",
        "Orthogonal Nonnegative Matrix Tri-Factorizations for Clustering.",
        "In Proceedings of the Twelfth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2006. [13] R. O. Duda, P. E. Hart, and D. G. Stork.",
        "Pattern Classification.",
        "John Wiley & Sons, Inc., 2001. [14] T. Li, S. Ma, and M. Ogihara.",
        "Document Clustering via Adaptive Subspace Iteration.",
        "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2004. [15] T. Li and C. Ding.",
        "The Relationships Among Various Nonnegative Matrix Factorization Methods for Clustering.",
        "In Proceedings of the 6th International Conference on Data Mining (ICDM). 2006. [16] X. Liu and Y. Gong.",
        "Document Clustering with Cluster Refinement and Model Selection Capabilities.",
        "In Proc. of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002. [17] E. Han, D. Boley, M. Gini, R. Gross, K. Hastings, G. Karypis, V. Kumar, B. Mobasher, and J. Moore.",
        "WebACE: A Web Agent for Document Categorization and Exploration.",
        "In Proceedings of the 2nd International Conference on Autonomous Agents (Agents98).",
        "ACM Press, 1998. [18] M. Hein, J. Y. Audibert, and U. von Luxburg.",
        "From Graphs to Manifolds - Weak and Strong Pointwise Consistency of Graph Laplacians.",
        "In Proceedings of the 18th Conference on Learning Theory (COLT), 470-485. 2005. [19] J.",
        "He, M. Lan, C.-L. Tan, S.-Y.",
        "Sung, and H.-B.",
        "Low.",
        "Initialization of Cluster Refinement Algorithms: A Review and Comparative Study.",
        "In Proc. of Inter.",
        "Joint Conference on Neural Networks, 2004. [20] A. Y. Ng, M. I. Jordan, Y. Weiss.",
        "On Spectral Clustering: Analysis and an algorithm.",
        "In Advances in Neural Information Processing Systems 14. 2002. [21] B. Sch¨olkopf and A. Smola.",
        "Learning with Kernels.",
        "The MIT Press.",
        "Cambridge, Massachusetts. 2002. [22] J. Shi and J. Malik.",
        "Normalized Cuts and Image Segmentation.",
        "IEEE Trans. on Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000. [23] A. Strehl and J. Ghosh.",
        "Cluster Ensembles - A Knowledge Reuse Framework for Combining Multiple Partitions.",
        "Journal of Machine Learning Research, 3:583-617, 2002. [24] V. N. Vapnik.",
        "The Nature of Statistical Learning Theory.",
        "Berlin: Springer-Verlag, 1995. [25] Wu, M. and Sch¨olkopf, B.",
        "A Local Learning Approach for Clustering.",
        "In Advances in Neural Information Processing Systems 18. 2006. [26] S. X. Yu, J. Shi.",
        "Multiclass Spectral Clustering.",
        "In Proceedings of the International Conference on Computer Vision, 2003. [27] W. Xu, X. Liu and Y. Gong.",
        "Document Clustering Based On Non-Negative Matrix Factorization.",
        "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2003. [28] H. Zha, X.",
        "He, C. Ding, M. Gu and H. Simon.",
        "Spectral Relaxation for K-means Clustering.",
        "In NIPS 14. 2001. [29] T. Zhang and F. J. Oles.",
        "Text Categorization Based on Regularized Linear Classification Methods.",
        "Journal of Information Retrieval, 4:5-31, 2001. [30] L. Zelnik-Manor and P. Perona.",
        "Self-Tuning Spectral Clustering.",
        "In NIPS 17. 2005. [31] D. Zhou, O. Bousquet, T. N. Lal, J. Weston and B. Sch¨olkopf.",
        "Learning with Local and Global Consistency.",
        "NIPS 17, 2005."
    ],
    "translated_text_sentences": [
        "Agrupamiento regularizado para documentos ∗ Fei Wang, Changshui Zhang Laboratorio Clave del Estado de Tecnología Inteligente.",
        "En los últimos años, la agrupación de documentos ha estado recibiendo cada vez más atención como una técnica importante y fundamental para la organización no supervisada de documentos, la extracción automática de temas y la recuperación o filtrado rápido de información.",
        "En este artículo, proponemos un método novedoso para agrupar documentos utilizando regularización.",
        "A diferencia de los métodos de agrupamiento regularizados globalmente tradicionales, nuestro método primero construye un predictor de etiquetas lineal regularizado local para cada vector de documento, y luego combina todos esos regularizadores locales con un regularizador de suavidad global.",
        "Así que llamamos a nuestro algoritmo Agrupamiento con Regularización Local y Global (CLGR).",
        "Mostraremos que las pertenencias de los documentos a los grupos se pueden lograr mediante la descomposición de los valores propios de una matriz simétrica dispersa, la cual puede resolverse eficientemente mediante métodos iterativos.",
        "Finalmente, se presentan nuestras evaluaciones experimentales en varios conjuntos de datos para mostrar las ventajas de CLGR sobre los métodos tradicionales de agrupamiento de documentos.",
        "Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información-Agrupamiento; I.2.6 [Inteligencia Artificial]: Aprendizaje-Aprendizaje de Conceptos Términos Generales Algoritmos 1.",
        "La agrupación de documentos ha estado recibiendo cada vez más atención como una técnica importante y fundamental para la organización no supervisada de documentos, la extracción automática de temas y la recuperación o filtrado rápido de información.",
        "Un buen enfoque de agrupación de documentos puede ayudar a las computadoras a organizar automáticamente el corpus de documentos en una jerarquía de clusters significativa para una navegación eficiente, lo cual es muy valioso para complementar las deficiencias de las tecnologías tradicionales de recuperación de información.",
        "Como señaló [8], las necesidades de recuperación de información pueden expresarse mediante un espectro que va desde la búsqueda basada en palabras clave estrechas hasta la exploración de información amplia, como cuáles son los principales eventos internacionales de los últimos meses.",
        "Los motores de búsqueda de documentos tradicionales tienden a adaptarse bien al extremo de la búsqueda, es decir, suelen proporcionar búsquedas específicas de documentos que coinciden con la consulta de los usuarios, sin embargo, les resulta difícil satisfacer las necesidades del resto del espectro en el que se necesita información más amplia o vaga.",
        "En tales casos, la navegación eficiente a través de una buena jerarquía de clústeres será definitivamente útil.",
        "Generalmente, los métodos de agrupamiento de documentos se pueden clasificar principalmente en dos clases: métodos jerárquicos y métodos de partición.",
        "Los métodos jerárquicos agrupan los puntos de datos en una estructura de árbol jerárquico utilizando enfoques de abajo hacia arriba o de arriba hacia abajo.",
        "Por ejemplo, el clustering jerárquico aglomerativo (HAC) [13] es un método típico de clustering jerárquico ascendente.",
        "Toma cada punto de datos como un único clúster para comenzar y luego construye clústeres más grandes agrupando puntos de datos similares juntos hasta que todo el conjunto de datos esté encapsulado en un único clúster final.",
        "Por otro lado, los métodos de particionamiento descomponen el conjunto de datos en un número de grupos disjuntos que suelen ser óptimos en términos de algunas funciones de criterio predefinidas.",
        "Por ejemplo, K-means es un método de particionamiento típico que tiene como objetivo minimizar la suma de las distancias al cuadrado entre los puntos de datos y sus centros de clúster correspondientes.",
        "En este documento, nos centraremos en los métodos de particionamiento.",
        "Como sabemos, existen dos problemas principales en los métodos de particionamiento (como Kmeans y el Modelo de Mezcla Gaussiana (GMM) [16]): (1) el criterio predefinido suele ser no convexo, lo que provoca muchas soluciones óptimas locales; (2) el procedimiento iterativo (por ejemplo, el algoritmo de Expectation Maximization (EM)) para optimizar los criterios suele hacer que las soluciones finales dependan en gran medida de las inicializaciones.",
        "En las últimas décadas, se han propuesto muchos métodos para superar los problemas mencionados de los métodos de particionamiento [19][28].",
        "Recientemente, otro tipo de métodos de particionamiento basados en agrupamiento en grafos de datos han despertado un considerable interés en la comunidad de aprendizaje automático y minería de datos.",
        "La idea básica detrás de estos métodos es modelar primero el conjunto de datos completo como un grafo ponderado, en el que los nodos del grafo representan los puntos de datos y los pesos en los bordes corresponden a las similitudes entre los puntos en pares.",
        "Entonces, las asignaciones de clústeres del conjunto de datos se pueden lograr optimizando algunos criterios definidos en el gráfico.",
        "Por ejemplo, el Clustering Espectral es uno de los enfoques de clustering basados en grafos más representativos, generalmente tiene como objetivo optimizar algún valor de corte (por ejemplo,",
        "Corte normalizado [22], corte de razón [7], corte mínimo-máximo [11]) definidos en un grafo no dirigido.",
        "Después de algunas relajaciones, estos criterios generalmente pueden ser optimizados a través de descomposiciones propias, lo cual está garantizado que sea óptimo a nivel global.",
        "De esta manera, el agrupamiento espectral evita eficientemente los problemas de los métodos de particionamiento tradicionales que presentamos en el párrafo anterior.",
        "En este artículo, proponemos un algoritmo novedoso de agrupamiento de documentos que hereda la superioridad del agrupamiento espectral, es decir, los resultados finales de los grupos también pueden obtenerse explotando la estructura de los autovalores de una matriz simétrica.",
        "Sin embargo, a diferencia del agrupamiento espectral, que simplemente impone una restricción de suavidad en las etiquetas de los datos sobre todo el conjunto de datos [2], nuestro método primero construye un predictor de etiquetas lineal regularizado para cada punto de datos a partir de su vecindario como en [25], y luego combina los resultados de todos estos predictores de etiquetas locales con un regularizador de suavidad de etiquetas global.",
        "Así que llamamos a nuestro método Agrupamiento con Regularización Local y Global (CLGR).",
        "La idea de incorporar tanto información local como global en la predicción de etiquetas está inspirada en los trabajos recientes sobre aprendizaje semi-supervisado [31], y nuestras evaluaciones experimentales en varios conjuntos de datos reales de documentos muestran que CLGR funciona mejor que muchos métodos de agrupamiento de vanguardia.",
        "El resto de este documento está organizado de la siguiente manera: en la sección 2 presentaremos nuestro algoritmo CLGR en detalle.",
        "Los resultados experimentales en varios conjuntos de datos se presentan en la sección 3, seguidos de las conclusiones y discusiones en la sección 4.",
        "El ALGORITMO PROPUESTO En esta sección, presentaremos en detalle nuestro algoritmo de Agrupamiento con Regularización Local y Global (CLGR).",
        "Primero veamos cómo se representan los documentos a lo largo de este documento. 2.1 Representación de documentos En nuestro trabajo, todos los documentos se representan mediante vectores de frecuencia de términos ponderados.",
        "Sea W = {w1, w2, · · · , wm} el conjunto completo de vocabulario del corpus de documentos (que ha sido preprocesado mediante la eliminación de palabras vacías y operaciones de derivación de palabras).",
        "El vector de frecuencia de término xi del documento di se define como xi = [xi1, xi2, · · · , xim]T , xik = tik log n idfk , donde tik es la frecuencia del término wk ∈ W, n es el tamaño del corpus de documentos, idfk es el número de documentos que contienen la palabra wk.",
        "De esta manera, xi también se llama la representación TFIDF del documento di.",
        "Además, también normalizamos cada xi (1 i n) para que tenga una longitud unitaria, de modo que cada documento esté representado por un vector TF-IDF normalizado. 2.2 Regularización Local Como su nombre sugiere, CLGR está compuesto por dos partes: regularización local y regularización global.",
        "En esta subsección introduciremos detalladamente la parte de regularización local. 2.2.1 Motivación Como sabemos que el agrupamiento es un tipo de técnica de aprendizaje, tiene como objetivo organizar el conjunto de datos de una manera razonable.",
        "En general, el aprendizaje puede plantearse como un problema de estimación de funciones, a partir del cual podemos obtener una buena función de clasificación que asignará etiquetas al conjunto de datos de entrenamiento e incluso al conjunto de datos de prueba no vistos con algún costo minimizado [24].",
        "Por ejemplo, en el escenario de clasificación de dos clases (en el que conocemos exactamente la etiqueta de cada documento), un clasificador lineal con ajuste de mínimos cuadrados tiene como objetivo aprender un vector columna w de manera que el costo al cuadrado J = 1 n (wT xi − yi)2 (1) se minimice, donde yi ∈ {+1, −1} es la etiqueta de xi.",
        "Al tomar ∂J /∂w = 0, obtenemos la solución w∗ = Σ i=1 n xixT i −1 Σ i=1 n xiyi, (2) que puede escribirse en su forma matricial como w∗ = XXT −1 Xy, (3) donde X = [x1, x2, · · · , xn] es una matriz de documentos m × n, y = [y1, y2, · · · , yn]T es el vector de etiquetas.",
        "Entonces, para un documento de prueba t, podemos determinar su etiqueta mediante l = signo(w∗T u), donde signo(·) es la función signo.",
        "Un problema natural en la ecuación (3) es que la matriz XXT puede ser singular y, por lo tanto, no invertible (por ejemplo, cuando m n).",
        "Para evitar tal problema, podemos agregar un término de regularización y minimizar el siguiente criterio J = 1 n n i=1 (wT xi − yi)2 + λ w 2, (5), donde λ es un parámetro de regularización.",
        "Entonces, la solución óptima que minimiza J está dada por w∗ = XXT + λnI −1 Xy, (6) donde I es una matriz identidad de tamaño m × m.",
        "Se ha informado que el clasificador lineal regularizado puede lograr resultados muy buenos en problemas de clasificación de texto [29].",
        "Sin embargo, a pesar de su éxito empírico, el clasificador lineal regularizado es en realidad un clasificador global, es decir, w∗ se estima utilizando todo el conjunto de entrenamiento.",
        "Según [24], esta puede que no sea una idea inteligente, ya que un único w∗ puede que no sea lo suficientemente bueno para predecir las etiquetas de todo el espacio de entrada.",
        "Para obtener predicciones más precisas, [6] propuso entrenar clasificadores localmente y utilizarlos para clasificar los puntos de prueba.",
        "Por ejemplo, un punto de prueba será clasificado por el clasificador local entrenado utilizando los puntos de entrenamiento ubicados en la vecindad 1. En las siguientes discusiones todos asumimos que los documentos provienen solo de dos clases.",
        "Las generalizaciones de nuestro método a casos de múltiples clases se discutirán en la sección 2.5 de él.",
        "Aunque este método parece lento y estúpido, se informa que puede obtener mejores rendimientos que usar un clasificador global único en ciertas tareas [6]. 2.2.2 Construyendo los Predictores Localmente Regularizados Inspirados en su éxito, propusimos aplicar los algoritmos de aprendizaje local para el agrupamiento.",
        "La idea básica es que, para cada vector de documento xi (1 i n), entrenamos un predictor de etiquetas local basado en su vecindario k-más cercano Ni, y luego lo usamos para predecir la etiqueta de xi.",
        "Finalmente combinaremos todos esos predictores locales minimizando la suma de sus errores de predicción.",
        "En esta subsección introduciremos cómo construir esos predictores locales.",
        "Debido a la simplicidad y efectividad del clasificador lineal regularizado que hemos introducido en la sección 2.2.1, lo elegimos como nuestro predictor de etiquetas local, de modo que para cada documento xi, se minimiza el siguiente criterio Ji = 1 ni xj ∈Ni wT i xj − qj 2 + λi wi 2, (7), donde ni = |Ni| es la cardinalidad de Ni, y qj es la pertenencia al clúster de xj.",
        "Luego, utilizando la Ecuación (6), podemos obtener que la solución óptima es w∗ i = XiXT i + λiniI −1 Xiqi, (8), donde Xi = [xi1, xi2, · · · , xini], y usamos xik para denotar al k-ésimo vecino más cercano de xi. qi = [qi1, qi2, · · · , qini]T con qik representando la asignación de clúster de xik.",
        "El problema aquí es que XiXT i es una matriz m × m con m ni, es decir, deberíamos calcular la inversa de una matriz m × m para cada vector de documento, lo cual está prohibido computacionalmente.",
        "Afortunadamente, tenemos el siguiente teorema: Teorema 1. w∗ i en la Ec. (8) puede ser reescrito como w∗ i = Xi XT i Xi + λiniIi −1 qi, (9), donde Ii es una matriz identidad de tamaño ni × ni.",
        "Prueba.",
        "Dado que w∗ i = XiXT i + λiniI −1 Xiqi, entonces XiXT i + λiniI w∗ i = Xiqi =⇒ XiXT i w∗ i + λiniw∗ i = Xiqi =⇒ w∗ i = (λini)−1 Xi qi − XT i w∗ i.",
        "Sea β = (λini)−1 qi − XT i w∗ i, entonces w∗ i = Xiβ =⇒ λiniβ = qi − XT i w∗ i = qi − XT i Xiβ =⇒ qi = XT i Xi + λiniIi β =⇒ β = XT i Xi + λiniIi −1 qi.",
        "Por lo tanto, w∗ i = Xiβ = Xi XT i Xi + λiniIi −1 qi 2. Utilizando el teorema 1, solo necesitamos calcular la inversa de una matriz ni × ni para cada documento para entrenar un predictor de etiquetas local.",
        "Además, para un nuevo punto de prueba u que cae en Ni, podemos clasificarlo por el signo de qu = w∗T i u = uT wi = uT Xi XT i Xi + λiniIi −1 qi.",
        "Esta es una expresión atractiva ya que podemos determinar la asignación del clúster de u utilizando los productos internos entre los puntos en {u ∪ Ni}, lo que sugiere que un regularizador local de este tipo puede ser fácilmente kernelizado [21] siempre y cuando definamos una función de kernel adecuada. 2.2.3 Combinando los Predictores Regularizados Localmente Una vez que todos los predictores locales hayan sido construidos, los combinaremos minimizando Jl = n i=1 w∗T i xi − qi 2, (10), lo que representa la suma de los errores de predicción de todos los predictores locales.",
        "Combinando la Ecuación (10) con la Ecuación (6), podemos obtener Jl = n i=1 w∗T i xi − qi 2 = n i=1 xT i Xi XT i Xi + λiniIi −1 qi − qi 2 = Pq − q 2, (11), donde q = [q1, q2, · · · , qn]T, y P es una matriz n × n construida de la siguiente manera.",
        "Sea αi = xT i Xi XT i Xi + λiniIi −1 , entonces Pij = αi j, si xj ∈ Ni 0, de lo contrario , (12) donde Pij es la entrada (i, j)-ésima de P, y αi j representa la entrada j-ésima de αi.",
        "Hasta ahora podemos escribir el criterio de agrupamiento combinando predictores de etiquetas lineales localmente regularizados Jl en una forma matemática explícita, y podemos minimizarlo directamente utilizando algunas técnicas estándar de optimización.",
        "Sin embargo, los resultados pueden no ser lo suficientemente buenos ya que solo explotamos la información local del conjunto de datos.",
        "En la siguiente subsección, introduciremos un criterio de regularización global y lo combinaremos con Jl, que tiene como objetivo encontrar un buen resultado de agrupamiento de manera local-global. 2.3 Regularización Global En el agrupamiento de datos, generalmente requerimos que las asignaciones de clúster de los puntos de datos sean lo suficientemente suaves con respecto a la variedad de datos subyacente, lo que implica (1) que los puntos cercanos tienden a tener las mismas asignaciones de clúster; (2) que los puntos en la misma estructura (por ejemplo, subvariedad o clúster) tienden a tener las mismas asignaciones de clúster [31].",
        "Sin pérdida de generalidad, asumimos que los puntos de datos residen (aproximadamente) en una variedad de baja dimensión M2, y q es la función de asignación de clúster definida en M, es decir, 2 Creemos que los datos de texto también se muestrean de alguna variedad de baja dimensión, ya que es imposible que para todos los x ∈ M, q(x) devuelva la membresía del clúster de x.",
        "La suavidad de q sobre M se puede calcular mediante la siguiente integral de Dirichlet [2] D[q] = 1 2 M q(x) 2 dM, (13) donde el gradiente q es un vector en el espacio tangente T Mx, y la integral se toma con respecto a la medida estándar en M. Si restringimos la escala de q por q, q M = 1 (donde ·, · M es el producto interno inducido en M), entonces resulta que encontrar la función más suave que minimiza D[q] se reduce a encontrar las autofunciones del operador Laplace Beltrami L, que se define como Lq −div q, (14) donde div es la divergencia de un campo vectorial.",
        "Generalmente, el gráfico se puede ver como la forma discretizada de una variedad.",
        "Podemos modelar el conjunto de datos como un grafo ponderado no dirigido como en el agrupamiento espectral [22], donde los nodos del grafo son simplemente los puntos de datos, y los pesos en las aristas representan las similitudes entre pares de puntos.",
        "Entonces se puede demostrar que minimizar la Ec. (13) corresponde a minimizar Jg = qT Lq = n i=1 (qi − qj)2 wij, (15), donde q = [q1, q2, · · · , qn]T con qi = q(xi), L es el Laplaciano del grafo con su entrada (i, j)-ésima Lij =    di − wii, si i = j −wij, si xi y xj son adyacentes 0, en otro caso, (16), donde di = j wij es el grado de xi, wij es la similitud entre xi y xj.",
        "Si xi y xj son adyacentes, wij generalmente se calcula de la siguiente manera: wij = e − xi−xj 2 2σ2, donde σ es un parámetro dependiente del conjunto de datos.",
        "Se ha demostrado que bajo ciertas condiciones, tal forma de wij para determinar los pesos en los bordes del grafo conduce a la convergencia del Laplaciano del grafo al operador Laplace Beltrami [3][18].",
        "En resumen, el uso de la Ec. (15) con pesos exponenciales puede medir de manera efectiva la suavidad de las asignaciones de datos con respecto a la variedad intrínseca de datos.",
        "Por lo tanto, lo adoptamos como un regularizador global para penalizar la suavidad de las asignaciones de datos predichas. 2.4 Agrupamiento con Regularización Local y Global Combinando los contenidos que hemos introducido en la sección 2.2 y la sección 2.3, podemos derivar que el criterio de agrupamiento es minq J = Jl + λJg = Pq − q 2 + λqT Lq sujeto a qi ∈ {−1, +1}, (18) donde P está definido como en la Ecuación (12), y λ es un parámetro de regularización para equilibrar Jl y Jg.",
        "Sin embargo, los valores discretos llenan todo el espacio muestral de alta dimensionalidad.",
        "Y se ha demostrado que los métodos basados en variedades pueden lograr buenos resultados en tareas de clasificación de texto [31]. En este artículo, definimos xi y xj como adyacentes si xi ∈ N(xj) o xj ∈ N(xi). La restricción de pi convierte el problema en un problema de programación entera NP difícil.",
        "Una forma natural de hacer que el problema sea resoluble es eliminar la restricción y relajar qi para que sea continuo, luego el objetivo que buscamos minimizar se convierte en J = Pq − q 2 + λqT Lq = qT (P − I)T (P − I)q + λqT Lq = qT (P − I)T (P − I) + λL q, (19) y luego agregamos una restricción adicional qT q = 1 para restringir la escala de q.",
        "Entonces nuestro objetivo se convierte en minq J = qT (P − I)T (P − I) + λL q s.t. qT q = 1 (20) Utilizando el método de Lagrange, podemos derivar que la solución óptima q corresponde al menor vector propio de la matriz M = (P − I)T (P − I) + λL, y la asignación de clúster de xi puede determinarse por el signo de qi, es decir, xi se clasificará como clase uno si qi > 0, de lo contrario se clasificará como clase 2. 2.5 CLGR Multiclase En lo anterior hemos introducido el marco básico de Agrupamiento con Regularización Local y Global (CLGR) para el problema de agrupamiento de dos clases, y lo extenderemos al agrupamiento multiclase en esta subsección.",
        "Primero asumimos que todos los documentos pertenecen a C clases indexadas por L = {1, 2, · · · , C}. qc es la función de clasificación para la clase c (1 ≤ c ≤ C), tal que qc(xi) devuelve la confianza de que xi pertenece a la clase c. Nuestro objetivo es obtener el valor de qc(xi) (1 ≤ c ≤ C, 1 ≤ i ≤ n), y la asignación de cluster de xi puede ser determinada por {qc(xi)}C c=1 utilizando algunos métodos adecuados de discretización que presentaremos más adelante.",
        "Por lo tanto, en este caso de múltiples clases, para cada documento xi (1 i n), construiremos C predictores de etiquetas regularizados localmente lineales cuyos vectores normales son wc∗ i = Xi XT i Xi + λiniIi −1 qc i (1 c C), (21), donde Xi = [xi1, xi2, · · · , xini ] con xik siendo el k-ésimo vecino de xi, y qc i = [qc i1, qc i2, · · · , qc ini ]T con qc ik = qc (xik).",
        "Entonces (wc∗ i )T xi devuelve la confianza predicha de xi perteneciente a la clase c. Por lo tanto, el error de predicción local para la clase c se puede definir como J c l = n i=1 (wc∗ i ) T xi − qc i 2 , (22) Y el error de predicción local total se convierte en Jl = C c=1 J c l = C c=1 n i=1 (wc∗ i ) T xi − qc i 2 . (23) Como en la Ec. (11), podemos definir una matriz n×n P (ver Ec. (12)) y reescribir Jl como Jl = C c=1 J c l = C c=1 Pqc − qc 2 . (24) De manera similar, podemos definir el regularizador de suavidad global en el caso de múltiples clases como Jg = C c=1 n i=1 (qc i − qc j )2 wij = C c=1 (qc )T Lqc . (25) Luego, el criterio a minimizar para CLGR en el caso de múltiples clases se convierte en J = Jl + λJg = C c=1 Pqc − qc 2 + λ(qc )T Lqc = C c=1 (qc )T (P − I)T (P − I) + λL qc = trace QT (P − I)T (P − I) + λL Q , (26) donde Q = [q1 , q2 , · · · , qc ] es una matriz n × c, y trace(·) devuelve la traza de una matriz.",
        "Al igual que en la ecuación (20), también agregamos la restricción de que QT Q = I para limitar la escala de Q.",
        "Entonces nuestro problema de optimización se convierte en minQ J = traza QT (P − I)T (P − I) + λL Q sujeto a.",
        "Según el teorema de Ky Fan [28], sabemos que la solución óptima del problema anterior es Q∗ = [q∗ 1, q∗ 2, · · · , q∗ C ]R, donde q∗ k (1 ≤ k ≤ C) es el autovector que corresponde al k-ésimo valor propio más pequeño de la matriz (P − I)T (P − I) + λL, y R es una matriz arbitraria de tamaño C × C.",
        "Dado que los valores de las entradas en Q∗ son continuos, necesitamos discretizar aún más Q∗ para obtener las asignaciones de clúster de todos los puntos de datos.",
        "Principalmente hay dos enfoques para lograr este objetivo: 1.",
        "Como en [20], podemos tratar la i-ésima fila de Q como la incrustación de xi en un espacio de C dimensiones, y aplicar algunos métodos de agrupamiento tradicionales como kmeans para agrupar estas incrustaciones en C grupos. 2.",
        "Dado que el Q∗ óptimo no es único (debido a la existencia de una matriz R arbitraria), podemos buscar un R óptimo que rote Q∗ a una matriz de indicación4.",
        "El algoritmo detallado se puede consultar en [26].",
        "El procedimiento detallado del algoritmo para CLGR se resume en la tabla 1. 3.",
        "EXPERIMENTOS En esta sección, se realizan experimentos para comparar empíricamente los resultados de agrupamiento de CLGR con otros 8 algoritmos representativos de agrupamiento de documentos en 5 conjuntos de datos.",
        "Primero presentaremos la información básica de esos conjuntos de datos. 3.1 Conjuntos de datos Utilizamos una variedad de conjuntos de datos, la mayoría de los cuales son frecuentemente utilizados en la investigación de recuperación de información.",
        "La Tabla 2 resume las características de los conjuntos de datos. Aquí, una matriz de indicación T es una matriz n×c con su entrada (i, j) Tij ∈ {0, 1} de modo que para cada fila de Q∗ solo hay un 1.",
        "Entonces, el xi puede ser asignado al j-ésimo grupo tal que j = argjQ∗ ij = 1.",
        "Tabla 1: Agrupamiento con Regularización Local y Global (CLGR) Entrada: 1.",
        "Conjunto de datos X = {xi}n i=1; 2.",
        "Número de grupos C: 3.",
        "Tamaño del vecindario K: 4.",
        "Parámetros locales de regularización {λi}n i=1; 5.",
        "Parámetro de regularización global λ; Salida: La membresía del clúster de cada punto de datos.",
        "Procedimiento: 1.",
        "Construir los K vecindarios más cercanos para cada punto de datos; 2.",
        "Construya la matriz P utilizando la Ecuación (12); 3.",
        "Construya la matriz Laplaciana L utilizando la ecuación (16); 4.",
        "Construye la matriz M = (P − I)T (P − I) + λL; 5.",
        "Realice la descomposición de valores propios en M y construya la matriz Q∗ de acuerdo con la Ecuación (28); 6.",
        "Generar las asignaciones de clúster de cada punto de datos al discretizar adecuadamente Q∗.",
        "Tabla 2: Descripciones de los conjuntos de datos de documentos Conjuntos de datos Número de documentos Número de clases CSTR 476 4 WebKB4 4199 4 Reuters 2900 10 WebACE 2340 20 Newsgroup4 3970 4 CSTR.",
        "Este es el conjunto de datos de los resúmenes de informes técnicos publicados en el Departamento de Ciencias de la Computación de una universidad.",
        "El conjunto de datos contenía 476 resúmenes, los cuales fueron divididos en cuatro áreas de investigación: Procesamiento de Lenguaje Natural (NLP), Robótica/Visión, Sistemas y Teoría.",
        "WebKB.",
        "El conjunto de datos WebKB contiene páginas web recopiladas de los departamentos de informática de universidades.",
        "Hay alrededor de 8280 documentos y están divididos en 7 categorías: estudiante, facultad, personal, curso, proyecto, departamento y otros.",
        "El texto sin procesar es de aproximadamente 27MB.",
        "Entre estas 7 categorías, estudiante, facultad, curso y proyecto son las cuatro categorías que representan entidades más pobladas.",
        "El subconjunto asociado suele ser llamado WebKB4.",
        "Reuters.",
        "La colección de pruebas de categorización de texto Reuters-21578 contiene documentos recopilados de la agencia de noticias Reuters en 1987.",
        "Es un punto de referencia estándar para la categorización de texto y contiene 135 categorías.",
        "En nuestros experimentos, utilizamos un subconjunto de la colección de datos que incluye las 10 categorías más frecuentes entre los 135 temas y lo llamamos Reuters-top 10.",
        "WebACE.",
        "El conjunto de datos WebACE proviene del proyecto WebACE y ha sido utilizado para la agrupación de documentos [17][5].",
        "El conjunto de datos WebACE contiene 2340 documentos que consisten en artículos de noticias del servicio de noticias de Reuters a través de la web en octubre de 1997.",
        "Estos documentos están divididos en 20 clases.",
        "Not enough context provided.",
        "El conjunto de datos News4 utilizado en nuestros experimentos se selecciona del famoso conjunto de datos 20-newsgroups.",
        "El tema rec que contiene autos, motocicletas, béisbol y hockey fue seleccionado de la versión 20news-18828.",
        "El conjunto de datos News4 contiene 3970 vectores de documentos. Para preprocesar los conjuntos de datos, eliminamos las palabras vacías utilizando una lista estándar de palabras vacías, se omiten todas las etiquetas HTML y se ignoran todos los campos de encabezado excepto el asunto y la organización de los artículos publicados.",
        "En todos nuestros experimentos, primero seleccionamos las 1000 palabras principales por información mutua con las etiquetas de clase. 3.2 Métricas de Evaluación En los experimentos, establecemos el número de clústeres igual al número real de clases C para todos los algoritmos de agrupamiento.",
        "Para evaluar su rendimiento, comparamos los grupos generados por estos algoritmos con las clases reales mediante el cálculo de las siguientes dos medidas de rendimiento.",
        "Precisión de agrupamiento (Acc).",
        "La primera medida de rendimiento es la Precisión de Agrupamiento, que descubre la relación uno a uno entre los grupos y las clases, y mide en qué medida cada grupo contenía puntos de datos de la clase correspondiente.",
        "Resume todo el grado de coincidencia entre todos los pares de clases y clusters.",
        "La precisión del agrupamiento se puede calcular como: Acc = 1 N max   Ck,Lm T(Ck, Lm)   , (29) donde Ck denota el k-ésimo clúster en los resultados finales, y Lm es la verdadera clase m-ésima.",
        "T(Ck, Lm) es el número de entidades que pertenecen a la clase m y están asignadas al clúster k. La precisión calcula la suma máxima de T(Ck, Lm) para todos los pares de clústeres y clases, y estos pares no tienen superposiciones.",
        "La mayor precisión de agrupamiento significa un mejor rendimiento de agrupamiento.",
        "Información Mutua Normalizada (NMI).",
        "Otro métrica de evaluación que adoptamos aquí es la Información Mutua Normalizada (NMI) [23], la cual es ampliamente utilizada para determinar la calidad de los grupos.",
        "Para dos variables aleatorias X e Y, el NMI se define como: NMI(X, Y) = I(X, Y) H(X)H(Y), donde I(X, Y) es la información mutua entre X e Y, mientras que H(X) y H(Y) son las entropías de X e Y respectivamente.",
        "Se puede ver que NMI(X, X) = 1, que es el valor máximo posible de NMI.",
        "Dado un resultado de agrupamiento, el NMI en la ecuación (30) se estima como NMI = C k=1 C m=1 nk,mlog n·nk,m nk ˆnm C k=1 nklog nk n C m=1 ˆnmlog ˆnm n, (31), donde nk denota el número de datos contenidos en el grupo Ck (1 k C), ˆnm es el número de datos pertenecientes a la clase m-ésima (1 m C), y nk,m denota el número de datos que están en la intersección entre el grupo Ck y la clase m-ésima.",
        "El valor calculado en la Ecuación (31) se utiliza como medida de rendimiento para el resultado de agrupamiento dado.",
        "A mayor valor, mejor rendimiento de agrupamiento. 3.3 Comparaciones Hemos realizado evaluaciones de rendimiento exhaustivas probando nuestro método y comparándolo con otros 8 métodos representativos de agrupamiento de datos utilizando las mismas corpora de datos.",
        "Los algoritmos que evaluamos se enumeran a continuación. 1.",
        "K-means tradicional (KM). 2.",
        "K-medias esférico (SKM).",
        "La implementación se basa en [9]. 3.",
        "Modelo de Mezcla Gaussiana (GMM).",
        "La implementación se basa en [16]. 4.",
        "Agrupamiento espectral con cortes normalizados (Ncut).",
        "La implementación se basa en [26], y la varianza de la similitud gaussiana se determina mediante Escalado Local [30].",
        "Ten en cuenta que el criterio que Ncut busca minimizar es simplemente el regularizador global en nuestro algoritmo CLGR, excepto que Ncut utiliza el Laplaciano normalizado. 5.",
        "Agrupamiento utilizando Regularización Local Pura (CPLR).",
        "En este método simplemente minimizamos Jl (definido en la Ecuación (24)), y los resultados de agrupamiento pueden obtenerse realizando la descomposición de valores propios en la matriz (I − P)T (I − P) con algunos métodos de discretización adecuados. 6.",
        "Iteración de subespacio adaptativa (ASI).",
        "La implementación se basa en [14]. 7.",
        "Factorización de matrices no negativas (NMF).",
        "La implementación se basa en [27]. 8.",
        "Factorización Tri-Factorización de Matrices No Negativas (TNMF) [12].",
        "La implementación se basa en [15].",
        "Para la eficiencia computacional, en la implementación de CPLR y nuestro algoritmo CLGR, hemos establecido todos los parámetros de regularización local {λi}n i=1 para ser idénticos, los cuales se establecen mediante búsqueda en cuadrícula de {0.1, 1, 10}.",
        "El tamaño de los vecindarios k más cercanos se establece mediante una búsqueda en cuadrícula de {20, 40, 80}.",
        "Para el método CLGR, su parámetro de regularización global se establece mediante una búsqueda en cuadrícula de {0.1, 1, 10}.",
        "Al construir el regularizador global, hemos adoptado el método de escalamiento local [30] para construir la matriz Laplaciana.",
        "El método de discretización final adoptado en estos dos métodos es el mismo que en [26], ya que nuestros experimentos muestran que el uso de dicho método puede lograr mejores resultados que el uso de métodos basados en kmeans como en [20]. 3.4 Resultados Experimentales Los resultados de comparación de precisión de agrupamiento se muestran en la tabla 3, y los resultados de comparación de información mutua normalizada se resumen en la tabla 4.",
        "De las dos tablas principalmente observamos que: 1.",
        "Nuestro método CLGR supera a todos los demás métodos de agrupamiento de documentos en la mayoría de los conjuntos de datos.",
        "Para la agrupación de documentos, el método Spherical k-means suele superar al método tradicional de agrupación k-means, y el método GMM puede lograr resultados competitivos en comparación con el método Spherical k-means; 3.",
        "Los resultados obtenidos de los algoritmos tipo k-means y GMM suelen ser peores que los resultados obtenidos de Clustering Espectral.",
        "Dado que el Agrupamiento Espectral puede ser visto como una versión ponderada del kernel k-means, puede obtener buenos resultados cuando los clusters de datos tienen formas arbitrarias.",
        "Esto corrobora que los vectores de los documentos no están distribuidos regularmente (esféricos o elípticos). 4.",
        "Las comparaciones experimentales verifican empíricamente la equivalencia entre NMF y Clustering Espectral, como se muestra en la Tabla 3: Precisión de agrupamiento de los diversos métodos CSTR WebKB4 Reuters WebACE News4 KM 0.4256 0.3888 0.4448 0.4001 0.3527 SKM 0.4690 0.4318 0.5025 0.4458 0.3912 GMM 0.4487 0.4271 0.4897 0.4521 0.3844 NMF 0.5713 0.4418 0.4947 0.4761 0.4213 Ncut 0.5435 0.4521 0.4896 0.4513 0.4189 ASI 0.5621 0.4752 0.5235 0.4823 0.4335 TNMF 0.6040 0.4832 0.5541 0.5102 0.4613 CPLR 0.5974 0.5020 0.4832 0.5213 0.4890 CLGR 0.6235 0.5228 0.5341 0.5376 0.5102 Tabla 4: Resultados de información mutua normalizada de los diversos métodos CSTR WebKB4 Reuters WebACE News4 KM 0.3675 0.3023 0.4012 0.3864 0.3318 SKM 0.4027 0.4155 0.4587 0.4003 0.4085 GMM 0.4034 0.4093 0.4356 0.4209 0.3994 NMF 0.5235 0.4517 0.4402 0.4359 0.4130 Ncut 0.4833 0.4497 0.4392 0.4289 0.4231 ASI 0.5008 0.4833 0.4769 0.4817 0.4503 TNMF 0.5724 0.5011 0.5132 0.5328 0.4749 CPLR 0.5695 0.5231 0.4402 0.5543 0.4690 CLGR 0.6012 0.5434 0.4935 0.5390 0.4908 ha sido demostrado teóricamente en [10].",
        "Se puede observar en las tablas que NMF y el Agrupamiento Espectral suelen llevar a resultados de agrupamiento similares. 5.",
        "Los métodos basados en co-agrupamiento (TNMF y ASI) suelen lograr mejores resultados que los métodos tradicionales basados únicamente en vectores de documentos.",
        "Dado que estos métodos realizan una selección implícita de características en cada iteración, proporcionan una métrica adaptativa para medir el vecindario y, por lo tanto, tienden a producir mejores resultados de agrupamiento. 6.",
        "Los resultados obtenidos a través de CPLR suelen ser mejores que los resultados obtenidos a través de Agrupamiento Espectral, lo cual respalda la teoría de Vapnik [24] de que a veces los algoritmos de aprendizaje local pueden obtener mejores resultados que los algoritmos de aprendizaje global.",
        "Además de los experimentos de comparación mencionados anteriormente, también probamos la sensibilidad de los parámetros de nuestro método.",
        "Principalmente hay dos conjuntos de parámetros en nuestro algoritmo CLGR, los parámetros de regularización locales y globales ({λi}n i=1 y λ, como hemos mencionado en la sección 3.3, hemos establecido que todos los λis son idénticos a λ∗ en nuestros experimentos), y el tamaño de los vecindarios.",
        "Por lo tanto, también hemos realizado dos series de experimentos: 1.",
        "Fijando el tamaño de los vecindarios y probando el rendimiento del agrupamiento con diferentes valores de λ∗ y λ.",
        "En este conjunto de experimentos, encontramos que nuestro algoritmo CLGR puede lograr buenos resultados cuando los dos parámetros de regularización no son ni demasiado grandes ni demasiado pequeños.",
        "Normalmente nuestro método puede lograr buenos resultados cuando λ∗ y λ están alrededor de 0.1.",
        "La Figura 1 nos muestra un ejemplo de prueba en el conjunto de datos WebACE.",
        "Fijando los parámetros de regularización local y global, y probando el rendimiento del agrupamiento con diferentes valores de -5, -4.5, -4, -3.5, -3, -5, -4.5, -4, -3.5, -3, 0.35, 0.4, 0.45, 0.5, 0.55 para el parámetro de regularización local (valor logarítmico base 2) y para el parámetro de regularización global (valor logarítmico base 2), se obtuvo una precisión de agrupamiento. Figura 1: Resultados de la prueba de sensibilidad de parámetros en el conjunto de datos WebACE con el tamaño de vecindario fijo en 20, donde el eje x e y representan el valor logarítmico base 2 de λ∗ y λ.",
        "En este conjunto de experimentos, encontramos que el vecindario con un tamaño demasiado grande o demasiado pequeño deteriorará todos los resultados finales de agrupamiento.",
        "Esto puede entenderse fácilmente ya que cuando el tamaño del vecindario es muy pequeño, los puntos de datos utilizados para entrenar los clasificadores locales pueden no ser suficientes; cuando el tamaño del vecindario es muy grande, los clasificadores entrenados tenderán a ser globales y no podrán capturar las características locales típicas.",
        "La Figura 2 nos muestra un ejemplo de prueba en el conjunto de datos WebACE.",
        "Por lo tanto, podemos ver que nuestro algoritmo CLGR (1) puede lograr resultados satisfactorios y (2) no es muy sensible a la elección de parámetros, lo que lo hace práctico en aplicaciones del mundo real.",
        "CONCLUSIONES Y TRABAJOS FUTUROS En este artículo, desarrollamos un nuevo algoritmo de agrupamiento llamado agrupamiento con regularización local y global.",
        "Nuestro método conserva el mérito de los algoritmos de aprendizaje local y el agrupamiento espectral.",
        "Nuestros experimentos muestran que el algoritmo propuesto supera a la mayoría de los algoritmos de vanguardia en muchos conjuntos de datos de referencia.",
        "En el futuro, nos enfocaremos en la selección de parámetros y los problemas de aceleración del algoritmo CLGR.",
        "REFERENCIAS [1] L. Baker y A. McCallum.",
        "Agrupamiento distribucional de palabras para clasificación de texto.",
        "En Actas de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1998. [2] M. Belkin y P. Niyogi.",
        "Mapeo de Eigen de Laplaciano para Reducción de Dimensionalidad y Representación de Datos.",
        "Computación Neural, 15 (6):1373-1396.",
        "Junio de 2003. [3] M. Belkin y P. Niyogi.",
        "Hacia una Fundación Teórica para Métodos de Manifold Basados en Laplacianos.",
        "En Actas de la 18ª Conferencia sobre Teoría del Aprendizaje (COLT). 2005. 10 20 30 40 50 60 70 80 90 100 0.35 0.4 0.45 0.5 0.55 tamaño de la precisión de agrupamiento del vecindario Figura 2: Resultados de pruebas de sensibilidad de parámetros en el conjunto de datos WebACE con los parámetros de regularización fijados en 0.1, y el tamaño del vecindario variando de 10 a 100. [4] M. Belkin, P. Niyogi y V. Sindhwani.",
        "Regularización de variedades: un marco geométrico para el aprendizaje a partir de ejemplos.",
        "Revista de Investigación en Aprendizaje Automático 7, 1-48, 2006. [5] D. Boley.",
        "División Direccional Principal.",
        "Minería de datos y descubrimiento de conocimiento, 2:325-344, 1998. [6] L. Bottou y V. Vapnik.",
        "Algoritmos de aprendizaje local.",
        "Computación Neural, 4:888-900, 1992. [7] P. K. Chan, D. F. Schlag y J. Y. Zien.",
        "Particionamiento y agrupamiento K-way de corte de razón espectral.",
        "IEEE Trans.",
        "Diseño Asistido por Computadora, 13:1088-1096, Sep. 1994. [8] D. R. Cutting, D. R. Karger, J. O. Pederson y J. W. Tukey.",
        "Dispersión/Recolección: Un enfoque basado en clústeres para explorar grandes colecciones de documentos.",
        "En Actas de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1992. [9] I. S. Dhillon y D. S. Modha.",
        "Descomposiciones de conceptos para datos de texto grandes y dispersos utilizando agrupamiento.",
        "Aprendizaje automático, vol. 42(1), páginas 143-175, enero de 2001. [10] C. Ding, X.",
        "Él, y H. Simon.",
        "Sobre la equivalencia entre la factorización de matrices no negativas y el agrupamiento espectral.",
        "En Actas de la Conferencia de Minería de Datos de SIAM, 2005. [11] C. Ding, X.",
        "Él, H. Zha, M. Gu y H. D. Simon.",
        "Un algoritmo de corte min-max para particionar grafos y agrupar datos.",
        "En Proc. de la 1ra Conferencia Internacional sobre Minería de Datos (ICDM), páginas 107-114, 2001. [12] C. Ding, T. Li, W. Peng y H. Park.",
        "Tri-factorizaciones de matrices no negativas ortogonales para agrupamiento.",
        "En Actas de la Duodécima Conferencia Internacional de la ACM SIGKDD sobre Descubrimiento de Conocimiento y Minería de Datos, 2006. [13] R. O. Duda, P. E. Hart y D. G. Stork.",
        "Clasificación de patrones.",
        "John Wiley & Sons, Inc., 2001. [14] T. Li, S. Ma y M. Ogihara.",
        "Agrupamiento de documentos a través de la Iteración de Subespacios Adaptativos.",
        "En Actas de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2004. [15] T. Li y C. Ding.",
        "Las relaciones entre varios métodos de factorización de matrices no negativas para agrupamiento.",
        "En Actas de la 6ta Conferencia Internacional sobre Minería de Datos (ICDM). 2006. [16] X. Liu y Y. Gong.",
        "Agrupación de documentos con capacidades de refinamiento de clústeres y selección de modelos.",
        "En Proc. de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2002. [17] E. Han, D. Boley, M. Gini, R. Gross, K. Hastings, G. Karypis, V. Kumar, B. Mobasher y J. Moore.",
        "WebACE: Un agente web para la categorización y exploración de documentos.",
        "En Actas de la 2ª Conferencia Internacional sobre Agentes Autónomos (Agents98).",
        "ACM Press, 1998. [18] M. Hein, J. Y. Audibert y U. von Luxburg.",
        "De Gráficas a Variedades - Consistencia Puntual Débil y Fuerte de los Laplacianos de Gráficas.",
        "En Actas de la 18ª Conferencia sobre Teoría del Aprendizaje (COLT), 470-485. 2005. [19] J.",
        "Él, M. Lan, C.-L. Tan, S.-Y.",
        "Sung, y H.-B.",
        "Bajo.",
        "Inicialización de algoritmos de refinamiento de clústeres: una revisión y estudio comparativo.",
        "En Proc. de Inter.",
        "Conferencia Conjunta sobre Redes Neuronales, 2004. [20] A. Y. Ng, M. I. Jordan, Y. Weiss.",
        "En el agrupamiento espectral: análisis y un algoritmo.",
        "En Avances en Sistemas de Procesamiento de Información Neural 14. 2002. [21] B. Sch¨olkopf y A. Smola.",
        "Aprendizaje con Kernels.",
        "El MIT Press.",
        "Cambridge, Massachusetts. 2002. [22] J. Shi y J. Malik.",
        "Cortes normalizados y segmentación de imágenes.",
        "IEEE Trans. on Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000. [23] A. Strehl and J. Ghosh.\nIEEE Trans. on Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000. [23] A. Strehl y J. Ghosh.",
        "Conjuntos de agrupamientos: un marco de reutilización de conocimiento para combinar múltiples particiones.",
        "Revista de Investigación de Aprendizaje Automático, 3:583-617, 2002. [24] V. N. Vapnik.",
        "La naturaleza de la teoría del aprendizaje estadístico.",
        "Berlín: Springer-Verlag, 1995. [25] Wu, M. y Schölkopf, B.",
        "Un enfoque de aprendizaje local para el agrupamiento.",
        "En Avances en Sistemas de Procesamiento de Información Neural 18. 2006. [26] S. X. Yu, J. Shi.",
        "Agrupamiento espectral multiclase.",
        "En Actas de la Conferencia Internacional sobre Visión por Computadora, 2003. [27] W. Xu, X. Liu y Y. Gong.",
        "Agrupación de documentos basada en la factorización de matrices no negativas.",
        "En Actas de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2003. [28] H. Zha, X.",
        "Él, C. Ding, M. Gu y H. Simon.",
        "Relajación espectral para el agrupamiento K-means.",
        "En NIPS 14. 2001. [29] T. Zhang y F. J. Oles.",
        "Categorización de texto basada en métodos de clasificación lineal regularizados.",
        "Revista de Recuperación de Información, 4:5-31, 2001. [30] L. Zelnik-Manor y P. Perona.",
        "Agrupamiento espectral autoajustable.",
        "En NIPS 17. 2005. [31] D. Zhou, O. Bousquet, T. N. Lal, J. Weston y B. Schölkopf.",
        "Aprendizaje con Consistencia Local y Global.",
        "NIPS 17, 2005. \n\nNIPS 17, 2005."
    ],
    "error_count": 0,
    "keys": {
        "document clustering": {
            "translated_key": "agrupación de documentos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Regularized Clustering for Documents ∗ Fei Wang, Changshui Zhang State Key Lab of Intelligent Tech.",
                "and Systems Department of Automation, Tsinghua University Beijing, China, 100084 feiwang03@gmail.com Tao Li School of Computer Science Florida International University Miami, FL 33199, U.S.A. taoli@cs.fiu.edu ABSTRACT In recent years, <br>document clustering</br> has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering.",
                "In this paper, we propose a novel method for clustering documents using regularization.",
                "Unlike traditional globally regularized clustering methods, our method first construct a local regularized linear label predictor for each document vector, and then combine all those local regularizers with a global smoothness regularizer.",
                "So we call our algorithm Clustering with Local and Global Regularization (CLGR).",
                "We will show that the cluster memberships of the documents can be achieved by eigenvalue decomposition of a sparse symmetric matrix, which can be efficiently solved by iterative methods.",
                "Finally our experimental evaluations on several datasets are presented to show the superiorities of CLGR over traditional <br>document clustering</br> methods.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Clustering; I.2.6 [Artificial Intelligence]: Learning-Concept Learning General Terms Algorithms 1.",
                "INTRODUCTION <br>document clustering</br> has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering.",
                "A good <br>document clustering</br> approach can assist the computers to automatically organize the document corpus into a meaningful cluster hierarchy for efficient browsing and navigation, which is very valuable for complementing the deficiencies of traditional information retrieval technologies.",
                "As pointed out by [8], the information retrieval needs can be expressed by a spectrum ranged from narrow keyword-matching based search to broad information browsing such as what are the major international events in recent months.",
                "Traditional document retrieval engines tend to fit well with the search end of the spectrum, i.e. they usually provide specified search for documents matching the users query, however, it is hard for them to meet the needs from the rest of the spectrum in which a rather broad or vague information is needed.",
                "In such cases, efficient browsing through a good cluster hierarchy will be definitely helpful.",
                "Generally, <br>document clustering</br> methods can be mainly categorized into two classes: hierarchical methods and partitioning methods.",
                "The hierarchical methods group the data points into a hierarchical tree structure using bottom-up or top-down approaches.",
                "For example, hierarchical agglomerative clustering (HAC) [13] is a typical bottom-up hierarchical clustering method.",
                "It takes each data point as a single cluster to start off with and then builds bigger and bigger clusters by grouping similar data points together until the entire dataset is encapsulated into one final cluster.",
                "On the other hand, partitioning methods decompose the dataset into a number of disjoint clusters which are usually optimal in terms of some predefined criterion functions.",
                "For instance, K-means [13] is a typical partitioning method which aims to minimize the sum of the squared distance between the data points and their corresponding cluster centers.",
                "In this paper, we will focus on the partitioning methods.",
                "As we know that there are two main problems existing in partitioning methods (like Kmeans and Gaussian Mixture Model (GMM) [16]): (1) the predefined criterion is usually non-convex which causes many local optimal solutions; (2) the iterative procedure (e.g. the Expectation Maximization (EM) algorithm) for optimizing the criterions usually makes the final solutions heavily depend on the initializations.",
                "In the last decades, many methods have been proposed to overcome the above problems of the partitioning methods [19][28].",
                "Recently, another type of partitioning methods based on clustering on data graphs have aroused considerable interests in the machine learning and data mining community.",
                "The basic idea behind these methods is to first model the whole dataset as a weighted graph, in which the graph nodes represent the data points, and the weights on the edges correspond to the similarities between pairwise points.",
                "Then the cluster assignments of the dataset can be achieved by optimizing some criterions defined on the graph.",
                "For example Spectral Clustering is one kind of the most representative graph-based clustering approaches, it generally aims to optimize some cut value (e.g.",
                "Normalized Cut [22], Ratio Cut [7], Min-Max Cut [11]) defined on an undirected graph.",
                "After some relaxations, these criterions can usually be optimized via eigen-decompositions, which is guaranteed to be global optimal.",
                "In this way, spectral clustering efficiently avoids the problems of the traditional partitioning methods as we introduced in last paragraph.",
                "In this paper, we propose a novel <br>document clustering</br> algorithm that inherits the superiority of spectral clustering, i.e. the final cluster results can also be obtained by exploit the eigen-structure of a symmetric matrix.",
                "However, unlike spectral clustering, which just enforces a smoothness constraint on the data labels over the whole data manifold [2], our method first construct a regularized linear label predictor for each data point from its neighborhood as in [25], and then combine the results of all these local label predictors with a global label smoothness regularizer.",
                "So we call our method Clustering with Local and Global Regularization (CLGR).",
                "The idea of incorporating both local and global information into label prediction is inspired by the recent works on semi-supervised learning [31], and our experimental evaluations on several real document datasets show that CLGR performs better than many state-of-the-art clustering methods.",
                "The rest of this paper is organized as follows: in section 2 we will introduce our CLGR algorithm in detail.",
                "The experimental results on several datasets are presented in section 3, followed by the conclusions and discussions in section 4. 2.",
                "THE PROPOSED ALGORITHM In this section, we will introduce our Clustering with Local and Global Regularization (CLGR) algorithm in detail.",
                "First lets see the how the documents are represented throughout this paper. 2.1 Document Representation In our work, all the documents are represented by the weighted term-frequency vectors.",
                "Let W = {w1, w2, · · · , wm} be the complete vocabulary set of the document corpus (which is preprocessed by the stopwords removal and words stemming operations).",
                "The term-frequency vector xi of document di is defined as xi = [xi1, xi2, · · · , xim]T , xik = tik log n idfk , where tik is the term frequency of wk ∈ W, n is the size of the document corpus, idfk is the number of documents that contain word wk.",
                "In this way, xi is also called the TFIDF representation of document di.",
                "Furthermore, we also normalize each xi (1 i n) to have a unit length, so that each document is represented by a normalized TF-IDF vector. 2.2 Local Regularization As its name suggests, CLGR is composed of two parts: local regularization and global regularization.",
                "In this subsection we will introduce the local regularization part in detail. 2.2.1 Motivation As we know that clustering is one type of learning techniques, it aims to organize the dataset in a reasonable way.",
                "Generally speaking, learning can be posed as a problem of function estimation, from which we can get a good classification function that will assign labels to the training dataset and even the unseen testing dataset with some cost minimized [24].",
                "For example, in the two-class classification scenario1 (in which we exactly know the label of each document), a linear classifier with least square fit aims to learn a column vector w such that the squared cost J = 1 n (wT xi − yi)2 (1) is minimized, where yi ∈ {+1, −1} is the label of xi.",
                "By taking ∂J /∂w = 0, we get the solution w∗ = n i=1 xixT i −1 n i=1 xiyi , (2) which can further be written in its matrix form as w∗ = XXT −1 Xy, (3) where X = [x1, x2, · · · , xn] is an m × n document matrix, y = [y1, y2, · · · , yn]T is the label vector.",
                "Then for a test document t, we can determine its label by l = sign(w∗T u), (4) where sign(·) is the sign function.",
                "A natural problem in Eq. (3) is that the matrix XXT may be singular and thus not invertable (e.g. when m n).",
                "To avoid such a problem, we can add a regularization term and minimize the following criterion J = 1 n n i=1 (wT xi − yi)2 + λ w 2 , (5) where λ is a regularization parameter.",
                "Then the optimal solution that minimize J is given by w∗ = XXT + λnI −1 Xy, (6) where I is an m × m identity matrix.",
                "It has been reported that the regularized linear classifier can achieve very good results on text classification problems [29].",
                "However, despite its empirical success, the regularized linear classifier is on earth a global classifier, i.e. w∗ is estimated using the whole training set.",
                "According to [24], this may not be a smart idea, since a unique w∗ may not be good enough for predicting the labels of the whole input space.",
                "In order to get better predictions, [6] proposed to train classifiers locally and use them to classify the testing points.",
                "For example, a testing point will be classified by the local classifier trained using the training points located in the vicinity 1 In the following discussions we all assume that the documents coming from only two classes.",
                "The generalizations of our method to multi-class cases will be discussed in section 2.5. of it.",
                "Although this method seems slow and stupid, it is reported that it can get better performances than using a unique global classifier on certain tasks [6]. 2.2.2 Constructing the Local Regularized Predictors Inspired by their success, we proposed to apply the local learning algorithms for clustering.",
                "The basic idea is that, for each document vector xi (1 i n), we train a local label predictor based on its k-nearest neighborhood Ni, and then use it to predict the label of xi.",
                "Finally we will combine all those local predictors by minimizing the sum of their prediction errors.",
                "In this subsection we will introduce how to construct those local predictors.",
                "Due to the simplicity and effectiveness of the regularized linear classifier that we have introduced in section 2.2.1, we choose it to be our local label predictor, such that for each document xi, the following criterion is minimized Ji = 1 ni xj ∈Ni wT i xj − qj 2 + λi wi 2 , (7) where ni = |Ni| is the cardinality of Ni, and qj is the cluster membership of xj.",
                "Then using Eq. (6), we can get the optimal solution is w∗ i = XiXT i + λiniI −1 Xiqi, (8) where Xi = [xi1, xi2, · · · , xini ], and we use xik to denote the k-th nearest neighbor of xi. qi = [qi1, qi2, · · · , qini ]T with qik representing the cluster assignment of xik.",
                "The problem here is that XiXT i is an m × m matrix with m ni, i.e. we should compute the inverse of an m × m matrix for every document vector, which is computationally prohibited.",
                "Fortunately, we have the following theorem: Theorem 1. w∗ i in Eq. (8) can be rewritten as w∗ i = Xi XT i Xi + λiniIi −1 qi, (9) where Ii is an ni × ni identity matrix.",
                "Proof.",
                "Since w∗ i = XiXT i + λiniI −1 Xiqi, then XiXT i + λiniI w∗ i = Xiqi =⇒ XiXT i w∗ i + λiniw∗ i = Xiqi =⇒ w∗ i = (λini)−1 Xi qi − XT i w∗ i .",
                "Let β = (λini)−1 qi − XT i w∗ i , then w∗ i = Xiβ =⇒ λiniβ = qi − XT i w∗ i = qi − XT i Xiβ =⇒ qi = XT i Xi + λiniIi β =⇒ β = XT i Xi + λiniIi −1 qi.",
                "Therefore w∗ i = Xiβ = Xi XT i Xi + λiniIi −1 qi 2 Using theorem 1, we only need to compute the inverse of an ni × ni matrix for every document to train a local label predictor.",
                "Moreover, for a new testing point u that falls into Ni, we can classify it by the sign of qu = w∗T i u = uT wi = uT Xi XT i Xi + λiniIi −1 qi.",
                "This is an attractive expression since we can determine the cluster assignment of u by using the inner-products between the points in {u ∪ Ni}, which suggests that such a local regularizer can easily be kernelized [21] as long as we define a proper kernel function. 2.2.3 Combining the Local Regularized Predictors After all the local predictors having been constructed, we will combine them together by minimizing Jl = n i=1 w∗T i xi − qi 2 , (10) which stands for the sum of the prediction errors for all the local predictors.",
                "Combining Eq. (10) with Eq. (6), we can get Jl = n i=1 w∗T i xi − qi 2 = n i=1 xT i Xi XT i Xi + λiniIi −1 qi − qi 2 = Pq − q 2 , (11) where q = [q1, q2, · · · , qn]T , and the P is an n × n matrix constructing in the following way.",
                "Let αi = xT i Xi XT i Xi + λiniIi −1 , then Pij = αi j, if xj ∈ Ni 0, otherwise , (12) where Pij is the (i, j)-th entry of P, and αi j represents the j-th entry of αi .",
                "Till now we can write the criterion of clustering by combining locally regularized linear label predictors Jl in an explicit mathematical form, and we can minimize it directly using some standard optimization techniques.",
                "However, the results may not be good enough since we only exploit the local informations of the dataset.",
                "In the next subsection, we will introduce a global regularization criterion and combine it with Jl, which aims to find a good clustering result in a local-global way. 2.3 Global Regularization In data clustering, we usually require that the cluster assignments of the data points should be sufficiently smooth with respect to the underlying data manifold, which implies (1) the nearby points tend to have the same cluster assignments; (2) the points on the same structure (e.g. submanifold or cluster) tend to have the same cluster assignments [31].",
                "Without the loss of generality, we assume that the data points reside (roughly) on a low-dimensional manifold M2 , and q is the cluster assignment function defined on M, i.e. 2 We believe that the text data are also sampled from some low dimensional manifold, since it is impossible for them to for ∀x ∈ M, q(x) returns the cluster membership of x.",
                "The smoothness of q over M can be calculated by the following Dirichlet integral [2] D[q] = 1 2 M q(x) 2 dM, (13) where the gradient q is a vector in the tangent space T Mx, and the integral is taken with respect to the standard measure on M. If we restrict the scale of q by q, q M = 1 (where ·, · M is the inner product induced on M), then it turns out that finding the smoothest function minimizing D[q] reduces to finding the eigenfunctions of the Laplace Beltrami operator L, which is defined as Lq −div q, (14) where div is the divergence of a vector field.",
                "Generally, the graph can be viewed as the discretized form of manifold.",
                "We can model the dataset as an weighted undirected graph as in spectral clustering [22], where the graph nodes are just the data points, and the weights on the edges represent the similarities between pairwise points.",
                "Then it can be shown that minimizing Eq. (13) corresponds to minimizing Jg = qT Lq = n i=1 (qi − qj)2 wij, (15) where q = [q1, q2, · · · , qn]T with qi = q(xi), L is the graph Laplacian with its (i, j)-th entry Lij =    di − wii, if i = j −wij, if xi and xj are adjacent 0, otherwise, (16) where di = j wij is the degree of xi, wij is the similarity between xi and xj.",
                "If xi and xj are adjacent3 , wij is usually computed in the following way wij = e − xi−xj 2 2σ2 , (17) where σ is a dataset dependent parameter.",
                "It is proved that under certain conditions, such a form of wij to determine the weights on graph edges leads to the convergence of graph Laplacian to the Laplace Beltrami operator [3][18].",
                "In summary, using Eq. (15) with exponential weights can effectively measure the smoothness of the data assignments with respect to the intrinsic data manifold.",
                "Thus we adopt it as a global regularizer to punish the smoothness of the predicted data assignments. 2.4 Clustering with Local and Global Regularization Combining the contents we have introduced in section 2.2 and section 2.3 we can derive the clustering criterion is minq J = Jl + λJg = Pq − q 2 + λqT Lq s.t. qi ∈ {−1, +1}, (18) where P is defined as in Eq. (12), and λ is a regularization parameter to trade off Jl and Jg.",
                "However, the discrete fill in the whole high-dimensional sample space.",
                "And it has been shown that the manifold based methods can achieve good results on text classification tasks [31]. 3 In this paper, we define xi and xj to be adjacent if xi ∈ N(xj) or xj ∈ N(xi). constraint of pi makes the problem an NP hard integer programming problem.",
                "A natural way for making the problem solvable is to remove the constraint and relax qi to be continuous, then the objective that we aims to minimize becomes J = Pq − q 2 + λqT Lq = qT (P − I)T (P − I)q + λqT Lq = qT (P − I)T (P − I) + λL q, (19) and we further add a constraint qT q = 1 to restrict the scale of q.",
                "Then our objective becomes minq J = qT (P − I)T (P − I) + λL q s.t. qT q = 1 (20) Using the Lagrangian method, we can derive that the optimal solution q corresponds to the smallest eigenvector of the matrix M = (P − I)T (P − I) + λL, and the cluster assignment of xi can be determined by the sign of qi, i.e. xi will be classified as class one if qi > 0, otherwise it will be classified as class 2. 2.5 Multi-Class CLGR In the above we have introduced the basic framework of Clustering with Local and Global Regularization (CLGR) for the two-class clustering problem, and we will extending it to multi-class clustering in this subsection.",
                "First we assume that all the documents belong to C classes indexed by L = {1, 2, · · · , C}. qc is the classification function for class c (1 c C), such that qc (xi) returns the confidence that xi belongs to class c. Our goal is to obtain the value of qc (xi) (1 c C, 1 i n), and the cluster assignment of xi can be determined by {qc (xi)}C c=1 using some proper discretization methods that we will introduce later.",
                "Therefore, in this multi-class case, for each document xi (1 i n), we will construct C locally linear regularized label predictors whose normal vectors are wc∗ i = Xi XT i Xi + λiniIi −1 qc i (1 c C), (21) where Xi = [xi1, xi2, · · · , xini ] with xik being the k-th neighbor of xi, and qc i = [qc i1, qc i2, · · · , qc ini ]T with qc ik = qc (xik).",
                "Then (wc∗ i )T xi returns the predicted confidence of xi belonging to class c. Hence the local prediction error for class c can be defined as J c l = n i=1 (wc∗ i ) T xi − qc i 2 , (22) And the total local prediction error becomes Jl = C c=1 J c l = C c=1 n i=1 (wc∗ i ) T xi − qc i 2 . (23) As in Eq. (11), we can define an n×n matrix P (see Eq. (12)) and rewrite Jl as Jl = C c=1 J c l = C c=1 Pqc − qc 2 . (24) Similarly we can define the global smoothness regularizer in multi-class case as Jg = C c=1 n i=1 (qc i − qc j )2 wij = C c=1 (qc )T Lqc . (25) Then the criterion to be minimized for CLGR in multi-class case becomes J = Jl + λJg = C c=1 Pqc − qc 2 + λ(qc )T Lqc = C c=1 (qc )T (P − I)T (P − I) + λL qc = trace QT (P − I)T (P − I) + λL Q , (26) where Q = [q1 , q2 , · · · , qc ] is an n × c matrix, and trace(·) returns the trace of a matrix.",
                "The same as in Eq. (20), we also add the constraint that QT Q = I to restrict the scale of Q.",
                "Then our optimization problem becomes minQ J = trace QT (P − I)T (P − I) + λL Q s.t.",
                "QT Q = I, (27) From the Ky Fan theorem [28], we know the optimal solution of the above problem is Q∗ = [q∗ 1, q∗ 2, · · · , q∗ C ]R, (28) where q∗ k (1 k C) is the eigenvector corresponds to the k-th smallest eigenvalue of matrix (P − I)T (P − I) + λL, and R is an arbitrary C × C matrix.",
                "Since the values of the entries in Q∗ is continuous, we need to further discretize Q∗ to get the cluster assignments of all the data points.",
                "There are mainly two approaches to achieve this goal: 1.",
                "As in [20], we can treat the i-th row of Q as the embedding of xi in a C-dimensional space, and apply some traditional clustering methods like kmeans to clustering these embeddings into C clusters. 2.",
                "Since the optimal Q∗ is not unique (because of the existence of an arbitrary matrix R), we can pursue an optimal R that will rotate Q∗ to an indication matrix4 .",
                "The detailed algorithm can be referred to [26].",
                "The detailed algorithm procedure for CLGR is summarized in table 1. 3.",
                "EXPERIMENTS In this section, experiments are conducted to empirically compare the clustering results of CLGR with other 8 representitive <br>document clustering</br> algorithms on 5 datasets.",
                "First we will introduce the basic informations of those datasets. 3.1 Datasets We use a variety of datasets, most of which are frequently used in the information retrieval research.",
                "Table 2 summarizes the characteristics of the datasets. 4 Here an indication matrix T is a n×c matrix with its (i, j)th entry Tij ∈ {0, 1} such that for each row of Q∗ there is only one 1.",
                "Then the xi can be assigned to the j-th cluster such that j = argjQ∗ ij = 1.",
                "Table 1: Clustering with Local and Global Regularization (CLGR) Input: 1.",
                "Dataset X = {xi}n i=1; 2.",
                "Number of clusters C; 3.",
                "Size of the neighborhood K; 4.",
                "Local regularization parameters {λi}n i=1; 5.",
                "Global regularization parameter λ; Output: The cluster membership of each data point.",
                "Procedure: 1.",
                "Construct the K nearest neighborhoods for each data point; 2.",
                "Construct the matrix P using Eq. (12); 3.",
                "Construct the Laplacian matrix L using Eq. (16); 4.",
                "Construct the matrix M = (P − I)T (P − I) + λL; 5.",
                "Do eigenvalue decomposition on M, and construct the matrix Q∗ according to Eq. (28); 6.",
                "Output the cluster assignments of each data point by properly discretize Q∗ .",
                "Table 2: Descriptions of the document datasets Datasets Number of documents Number of classes CSTR 476 4 WebKB4 4199 4 Reuters 2900 10 WebACE 2340 20 Newsgroup4 3970 4 CSTR.",
                "This is the dataset of the abstracts of technical reports published in the Department of Computer Science at a university.",
                "The dataset contained 476 abstracts, which were divided into four research areas: Natural Language Processing(NLP), Robotics/Vision, Systems, and Theory.",
                "WebKB.",
                "The WebKB dataset contains webpages gathered from university computer science departments.",
                "There are about 8280 documents and they are divided into 7 categories: student, faculty, staff, course, project, department and other.",
                "The raw text is about 27MB.",
                "Among these 7 categories, student, faculty, course and project are four most populous entity-representing categories.",
                "The associated subset is typically called WebKB4.",
                "Reuters.",
                "The Reuters-21578 Text Categorization Test collection contains documents collected from the Reuters newswire in 1987.",
                "It is a standard text categorization benchmark and contains 135 categories.",
                "In our experiments, we use a subset of the data collection which includes the 10 most frequent categories among the 135 topics and we call it Reuters-top 10.",
                "WebACE.",
                "The WebACE dataset was from WebACE project and has been used for <br>document clustering</br> [17][5].",
                "The WebACE dataset contains 2340 documents consisting news articles from Reuters new service via the Web in October 1997.",
                "These documents are divided into 20 classes.",
                "News4.",
                "The News4 dataset used in our experiments are selected from the famous 20-newsgroups dataset5 .",
                "The topic rec containing autos, motorcycles, baseball and hockey was selected from the version 20news-18828.",
                "The News4 dataset contains 3970 document vectors. 5 http://people.csail.mit.edu/jrennie/20Newsgroups/ To pre-process the datasets, we remove the stop words using a standard stop list, all HTML tags are skipped and all header fields except subject and organization of the posted articles are ignored.",
                "In all our experiments, we first select the top 1000 words by mutual information with class labels. 3.2 Evaluation Metrics In the experiments, we set the number of clusters equal to the true number of classes C for all the clustering algorithms.",
                "To evaluate their performance, we compare the clusters generated by these algorithms with the true classes by computing the following two performance measures.",
                "Clustering Accuracy (Acc).",
                "The first performance measure is the Clustering Accuracy, which discovers the one-toone relationship between clusters and classes and measures the extent to which each cluster contained data points from the corresponding class.",
                "It sums up the whole matching degree between all pair class-clusters.",
                "Clustering accuracy can be computed as: Acc = 1 N max   Ck,Lm T(Ck, Lm)   , (29) where Ck denotes the k-th cluster in the final results, and Lm is the true m-th class.",
                "T(Ck, Lm) is the number of entities which belong to class m are assigned to cluster k. Accuracy computes the maximum sum of T(Ck, Lm) for all pairs of clusters and classes, and these pairs have no overlaps.",
                "The greater clustering accuracy means the better clustering performance.",
                "Normalized Mutual Information (NMI).",
                "Another evaluation metric we adopt here is the Normalized Mutual Information NMI [23], which is widely used for determining the quality of clusters.",
                "For two random variable X and Y, the NMI is defined as: NMI(X, Y) = I(X, Y) H(X)H(Y) , (30) where I(X, Y) is the mutual information between X and Y, while H(X) and H(Y) are the entropies of X and Y respectively.",
                "One can see that NMI(X, X) = 1, which is the maximal possible value of NMI.",
                "Given a clustering result, the NMI in Eq. (30) is estimated as NMI = C k=1 C m=1 nk,mlog n·nk,m nk ˆnm C k=1 nklog nk n C m=1 ˆnmlog ˆnm n , (31) where nk denotes the number of data contained in the cluster Ck (1 k C), ˆnm is the number of data belonging to the m-th class (1 m C), and nk,m denotes the number of data that are in the intersection between the cluster Ck and the m-th class.",
                "The value calculated in Eq. (31) is used as a performance measure for the given clustering result.",
                "The larger this value, the better the clustering performance. 3.3 Comparisons We have conducted comprehensive performance evaluations by testing our method and comparing it with 8 other representative data clustering methods using the same data corpora.",
                "The algorithms that we evaluated are listed below. 1.",
                "Traditional k-means (KM). 2.",
                "Spherical k-means (SKM).",
                "The implementation is based on [9]. 3.",
                "Gaussian Mixture Model (GMM).",
                "The implementation is based on [16]. 4.",
                "Spectral Clustering with Normalized Cuts (Ncut).",
                "The implementation is based on [26], and the variance of the Gaussian similarity is determined by Local Scaling [30].",
                "Note that the criterion that Ncut aims to minimize is just the global regularizer in our CLGR algorithm except that Ncut used the normalized Laplacian. 5.",
                "Clustering using Pure Local Regularization (CPLR).",
                "In this method we just minimize Jl (defined in Eq. (24)), and the clustering results can be obtained by doing eigenvalue decomposition on matrix (I − P)T (I − P) with some proper discretization methods. 6.",
                "Adaptive Subspace Iteration (ASI).",
                "The implementation is based on [14]. 7.",
                "Nonnegative Matrix Factorization (NMF).",
                "The implementation is based on [27]. 8.",
                "Tri-Factorization Nonnegative Matrix Factorization (TNMF) [12].",
                "The implementation is based on [15].",
                "For computational efficiency, in the implementation of CPLR and our CLGR algorithm, we have set all the local regularization parameters {λi}n i=1 to be identical, which is set by grid search from {0.1, 1, 10}.",
                "The size of the k-nearest neighborhoods is set by grid search from {20, 40, 80}.",
                "For the CLGR method, its global regularization parameter is set by grid search from {0.1, 1, 10}.",
                "When constructing the global regularizer, we have adopted the local scaling method [30] to construct the Laplacian matrix.",
                "The final discretization method adopted in these two methods is the same as in [26], since our experiments show that using such method can achieve better results than using kmeans based methods as in [20]. 3.4 Experimental Results The clustering accuracies comparison results are shown in table 3, and the normalized mutual information comparison results are summarized in table 4.",
                "From the two tables we mainly observe that: 1.",
                "Our CLGR method outperforms all other <br>document clustering</br> methods in most of the datasets; 2.",
                "For <br>document clustering</br>, the Spherical k-means method usually outperforms the traditional k-means clustering method, and the GMM method can achieve competitive results compared to the Spherical k-means method; 3.",
                "The results achieved from the k-means and GMM type algorithms are usually worse than the results achieved from Spectral Clustering.",
                "Since Spectral Clustering can be viewed as a weighted version of kernel k-means, it can obtain good results the data clusters are arbitrarily shaped.",
                "This corroborates that the documents vectors are not regularly distributed (spherical or elliptical). 4.",
                "The experimental comparisons empirically verify the equivalence between NMF and Spectral Clustering, which Table 3: Clustering accuracies of the various methods CSTR WebKB4 Reuters WebACE News4 KM 0.4256 0.3888 0.4448 0.4001 0.3527 SKM 0.4690 0.4318 0.5025 0.4458 0.3912 GMM 0.4487 0.4271 0.4897 0.4521 0.3844 NMF 0.5713 0.4418 0.4947 0.4761 0.4213 Ncut 0.5435 0.4521 0.4896 0.4513 0.4189 ASI 0.5621 0.4752 0.5235 0.4823 0.4335 TNMF 0.6040 0.4832 0.5541 0.5102 0.4613 CPLR 0.5974 0.5020 0.4832 0.5213 0.4890 CLGR 0.6235 0.5228 0.5341 0.5376 0.5102 Table 4: Normalized mutual information results of the various methods CSTR WebKB4 Reuters WebACE News4 KM 0.3675 0.3023 0.4012 0.3864 0.3318 SKM 0.4027 0.4155 0.4587 0.4003 0.4085 GMM 0.4034 0.4093 0.4356 0.4209 0.3994 NMF 0.5235 0.4517 0.4402 0.4359 0.4130 Ncut 0.4833 0.4497 0.4392 0.4289 0.4231 ASI 0.5008 0.4833 0.4769 0.4817 0.4503 TNMF 0.5724 0.5011 0.5132 0.5328 0.4749 CPLR 0.5695 0.5231 0.4402 0.5543 0.4690 CLGR 0.6012 0.5434 0.4935 0.5390 0.4908 has been proved theoretically in [10].",
                "It can be observed from the tables that NMF and Spectral Clustering usually lead to similar clustering results. 5.",
                "The co-clustering based methods (TNMF and ASI) can usually achieve better results than traditional purely document vector based methods.",
                "Since these methods perform an implicit feature selection at each iteration, provide an adaptive metric for measuring the neighborhood, and thus tend to yield better clustering results. 6.",
                "The results achieved from CPLR are usually better than the results achieved from Spectral Clustering, which supports Vapniks theory [24] that sometimes local learning algorithms can obtain better results than global learning algorithms.",
                "Besides the above comparison experiments, we also test the parameter sensibility of our method.",
                "There are mainly two sets of parameters in our CLGR algorithm, the local and global regularization parameters ({λi}n i=1 and λ, as we have said in section 3.3, we have set all λis to be identical to λ∗ in our experiments), and the size of the neighborhoods.",
                "Therefore we have also done two sets of experiments: 1.",
                "Fixing the size of the neighborhoods, and testing the clustering performance with varying λ∗ and λ.",
                "In this set of experiments, we find that our CLGR algorithm can achieve good results when the two regularization parameters are neither too large nor too small.",
                "Typically our method can achieve good results when λ∗ and λ are around 0.1.",
                "Figure 1 shows us such a testing example on the WebACE dataset. 2.",
                "Fixing the local and global regularization parameters, and testing the clustering performance with different −5 −4.5 −4 −3.5 −3 −5 −4.5 −4 −3.5 −3 0.35 0.4 0.45 0.5 0.55 local regularization para (log 2 value) global regularization para (log 2 value) clusteringaccuracy Figure 1: Parameter sensibility testing results on the WebACE dataset with the neighborhood size fixed to 20, and the x-axis and y-axis represents the log2 value of λ∗ and λ. sizes of neighborhoods.",
                "In this set of experiments, we find that the neighborhood with a too large or too small size will all deteriorate the final clustering results.",
                "This can be easily understood since when the neighborhood size is very small, then the data points used for training the local classifiers may not be sufficient; when the neighborhood size is very large, the trained classifiers will tend to be global and cannot capture the typical local characteristics.",
                "Figure 2 shows us a testing example on the WebACE dataset.",
                "Therefore, we can see that our CLGR algorithm (1) can achieve satisfactory results and (2) is not very sensitive to the choice of parameters, which makes it practical in real world applications. 4.",
                "CONCLUSIONS AND FUTURE WORKS In this paper, we derived a new clustering algorithm called clustering with local and global regularization.",
                "Our method preserves the merit of local learning algorithms and spectral clustering.",
                "Our experiments show that the proposed algorithm outperforms most of the state of the art algorithms on many benchmark datasets.",
                "In the future, we will focus on the parameter selection and acceleration issues of the CLGR algorithm. 5.",
                "REFERENCES [1] L. Baker and A. McCallum.",
                "Distributional Clustering of Words for Text Classification.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 1998. [2] M. Belkin and P. Niyogi.",
                "Laplacian Eigenmaps for Dimensionality Reduction and Data Representation.",
                "Neural Computation, 15 (6):1373-1396.",
                "June 2003. [3] M. Belkin and P. Niyogi.",
                "Towards a Theoretical Foundation for Laplacian-Based Manifold Methods.",
                "In Proceedings of the 18th Conference on Learning Theory (COLT). 2005. 10 20 30 40 50 60 70 80 90 100 0.35 0.4 0.45 0.5 0.55 size of the neighborhood clusteringaccuracy Figure 2: Parameter sensibility testing results on the WebACE dataset with the regularization parameters being fixed to 0.1, and the neighborhood size varing from 10 to 100. [4] M. Belkin, P. Niyogi and V. Sindhwani.",
                "Manifold Regularization: a Geometric Framework for Learning from Examples.",
                "Journal of Machine Learning Research 7, 1-48, 2006. [5] D. Boley.",
                "Principal Direction Divisive Partitioning.",
                "Data mining and knowledge discovery, 2:325-344, 1998. [6] L. Bottou and V. Vapnik.",
                "Local learning algorithms.",
                "Neural Computation, 4:888-900, 1992. [7] P. K. Chan, D. F. Schlag and J. Y. Zien.",
                "Spectral K-way Ratio-Cut Partitioning and Clustering.",
                "IEEE Trans.",
                "Computer-Aided Design, 13:1088-1096, Sep. 1994. [8] D. R. Cutting, D. R. Karger, J. O. Pederson and J. W. Tukey.",
                "Scatter/Gather: A Cluster-Based Approach to Browsing Large Document Collections.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 1992. [9] I. S. Dhillon and D. S. Modha.",
                "Concept Decompositions for Large Sparse Text Data using Clustering.",
                "Machine Learning, vol. 42(1), pages 143-175, January 2001. [10] C. Ding, X.",
                "He, and H. Simon.",
                "On the equivalence of nonnegative matrix factorization and spectral clustering.",
                "In Proceedings of the SIAM Data Mining Conference, 2005. [11] C. Ding, X.",
                "He, H. Zha, M. Gu, and H. D. Simon.",
                "A min-max cut algorithm for graph partitioning and data clustering.",
                "In Proc. of the 1st International Conference on Data Mining (ICDM), pages 107-114, 2001. [12] C. Ding, T. Li, W. Peng, and H. Park.",
                "Orthogonal Nonnegative Matrix Tri-Factorizations for Clustering.",
                "In Proceedings of the Twelfth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2006. [13] R. O. Duda, P. E. Hart, and D. G. Stork.",
                "Pattern Classification.",
                "John Wiley & Sons, Inc., 2001. [14] T. Li, S. Ma, and M. Ogihara.",
                "<br>document clustering</br> via Adaptive Subspace Iteration.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2004. [15] T. Li and C. Ding.",
                "The Relationships Among Various Nonnegative Matrix Factorization Methods for Clustering.",
                "In Proceedings of the 6th International Conference on Data Mining (ICDM). 2006. [16] X. Liu and Y. Gong.",
                "<br>document clustering</br> with Cluster Refinement and Model Selection Capabilities.",
                "In Proc. of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002. [17] E. Han, D. Boley, M. Gini, R. Gross, K. Hastings, G. Karypis, V. Kumar, B. Mobasher, and J. Moore.",
                "WebACE: A Web Agent for Document Categorization and Exploration.",
                "In Proceedings of the 2nd International Conference on Autonomous Agents (Agents98).",
                "ACM Press, 1998. [18] M. Hein, J. Y. Audibert, and U. von Luxburg.",
                "From Graphs to Manifolds - Weak and Strong Pointwise Consistency of Graph Laplacians.",
                "In Proceedings of the 18th Conference on Learning Theory (COLT), 470-485. 2005. [19] J.",
                "He, M. Lan, C.-L. Tan, S.-Y.",
                "Sung, and H.-B.",
                "Low.",
                "Initialization of Cluster Refinement Algorithms: A Review and Comparative Study.",
                "In Proc. of Inter.",
                "Joint Conference on Neural Networks, 2004. [20] A. Y. Ng, M. I. Jordan, Y. Weiss.",
                "On Spectral Clustering: Analysis and an algorithm.",
                "In Advances in Neural Information Processing Systems 14. 2002. [21] B. Sch¨olkopf and A. Smola.",
                "Learning with Kernels.",
                "The MIT Press.",
                "Cambridge, Massachusetts. 2002. [22] J. Shi and J. Malik.",
                "Normalized Cuts and Image Segmentation.",
                "IEEE Trans. on Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000. [23] A. Strehl and J. Ghosh.",
                "Cluster Ensembles - A Knowledge Reuse Framework for Combining Multiple Partitions.",
                "Journal of Machine Learning Research, 3:583-617, 2002. [24] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Berlin: Springer-Verlag, 1995. [25] Wu, M. and Sch¨olkopf, B.",
                "A Local Learning Approach for Clustering.",
                "In Advances in Neural Information Processing Systems 18. 2006. [26] S. X. Yu, J. Shi.",
                "Multiclass Spectral Clustering.",
                "In Proceedings of the International Conference on Computer Vision, 2003. [27] W. Xu, X. Liu and Y. Gong.",
                "<br>document clustering</br> Based On Non-Negative Matrix Factorization.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2003. [28] H. Zha, X.",
                "He, C. Ding, M. Gu and H. Simon.",
                "Spectral Relaxation for K-means Clustering.",
                "In NIPS 14. 2001. [29] T. Zhang and F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Journal of Information Retrieval, 4:5-31, 2001. [30] L. Zelnik-Manor and P. Perona.",
                "Self-Tuning Spectral Clustering.",
                "In NIPS 17. 2005. [31] D. Zhou, O. Bousquet, T. N. Lal, J. Weston and B. Sch¨olkopf.",
                "Learning with Local and Global Consistency.",
                "NIPS 17, 2005."
            ],
            "original_annotated_samples": [
                "and Systems Department of Automation, Tsinghua University Beijing, China, 100084 feiwang03@gmail.com Tao Li School of Computer Science Florida International University Miami, FL 33199, U.S.A. taoli@cs.fiu.edu ABSTRACT In recent years, <br>document clustering</br> has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering.",
                "Finally our experimental evaluations on several datasets are presented to show the superiorities of CLGR over traditional <br>document clustering</br> methods.",
                "INTRODUCTION <br>document clustering</br> has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering.",
                "A good <br>document clustering</br> approach can assist the computers to automatically organize the document corpus into a meaningful cluster hierarchy for efficient browsing and navigation, which is very valuable for complementing the deficiencies of traditional information retrieval technologies.",
                "Generally, <br>document clustering</br> methods can be mainly categorized into two classes: hierarchical methods and partitioning methods."
            ],
            "translated_annotated_samples": [
                "En los últimos años, la <br>agrupación de documentos</br> ha estado recibiendo cada vez más atención como una técnica importante y fundamental para la organización no supervisada de documentos, la extracción automática de temas y la recuperación o filtrado rápido de información.",
                "Finalmente, se presentan nuestras evaluaciones experimentales en varios conjuntos de datos para mostrar las ventajas de CLGR sobre los métodos tradicionales de <br>agrupamiento de documentos</br>.",
                "La <br>agrupación de documentos</br> ha estado recibiendo cada vez más atención como una técnica importante y fundamental para la organización no supervisada de documentos, la extracción automática de temas y la recuperación o filtrado rápido de información.",
                "Un buen enfoque de <br>agrupación de documentos</br> puede ayudar a las computadoras a organizar automáticamente el corpus de documentos en una jerarquía de clusters significativa para una navegación eficiente, lo cual es muy valioso para complementar las deficiencias de las tecnologías tradicionales de recuperación de información.",
                "Generalmente, los métodos de <br>agrupamiento de documentos</br> se pueden clasificar principalmente en dos clases: métodos jerárquicos y métodos de partición."
            ],
            "translated_text": "Agrupamiento regularizado para documentos ∗ Fei Wang, Changshui Zhang Laboratorio Clave del Estado de Tecnología Inteligente. En los últimos años, la <br>agrupación de documentos</br> ha estado recibiendo cada vez más atención como una técnica importante y fundamental para la organización no supervisada de documentos, la extracción automática de temas y la recuperación o filtrado rápido de información. En este artículo, proponemos un método novedoso para agrupar documentos utilizando regularización. A diferencia de los métodos de agrupamiento regularizados globalmente tradicionales, nuestro método primero construye un predictor de etiquetas lineal regularizado local para cada vector de documento, y luego combina todos esos regularizadores locales con un regularizador de suavidad global. Así que llamamos a nuestro algoritmo Agrupamiento con Regularización Local y Global (CLGR). Mostraremos que las pertenencias de los documentos a los grupos se pueden lograr mediante la descomposición de los valores propios de una matriz simétrica dispersa, la cual puede resolverse eficientemente mediante métodos iterativos. Finalmente, se presentan nuestras evaluaciones experimentales en varios conjuntos de datos para mostrar las ventajas de CLGR sobre los métodos tradicionales de <br>agrupamiento de documentos</br>. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información-Agrupamiento; I.2.6 [Inteligencia Artificial]: Aprendizaje-Aprendizaje de Conceptos Términos Generales Algoritmos 1. La <br>agrupación de documentos</br> ha estado recibiendo cada vez más atención como una técnica importante y fundamental para la organización no supervisada de documentos, la extracción automática de temas y la recuperación o filtrado rápido de información. Un buen enfoque de <br>agrupación de documentos</br> puede ayudar a las computadoras a organizar automáticamente el corpus de documentos en una jerarquía de clusters significativa para una navegación eficiente, lo cual es muy valioso para complementar las deficiencias de las tecnologías tradicionales de recuperación de información. Como señaló [8], las necesidades de recuperación de información pueden expresarse mediante un espectro que va desde la búsqueda basada en palabras clave estrechas hasta la exploración de información amplia, como cuáles son los principales eventos internacionales de los últimos meses. Los motores de búsqueda de documentos tradicionales tienden a adaptarse bien al extremo de la búsqueda, es decir, suelen proporcionar búsquedas específicas de documentos que coinciden con la consulta de los usuarios, sin embargo, les resulta difícil satisfacer las necesidades del resto del espectro en el que se necesita información más amplia o vaga. En tales casos, la navegación eficiente a través de una buena jerarquía de clústeres será definitivamente útil. Generalmente, los métodos de <br>agrupamiento de documentos</br> se pueden clasificar principalmente en dos clases: métodos jerárquicos y métodos de partición. ",
            "candidates": [],
            "error": [
                [
                    "agrupación de documentos",
                    "agrupamiento de documentos",
                    "agrupación de documentos",
                    "agrupación de documentos",
                    "agrupamiento de documentos"
                ]
            ]
        },
        "regularization": {
            "translated_key": "regularización",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Regularized Clustering for Documents ∗ Fei Wang, Changshui Zhang State Key Lab of Intelligent Tech.",
                "and Systems Department of Automation, Tsinghua University Beijing, China, 100084 feiwang03@gmail.com Tao Li School of Computer Science Florida International University Miami, FL 33199, U.S.A. taoli@cs.fiu.edu ABSTRACT In recent years, document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering.",
                "In this paper, we propose a novel method for clustering documents using <br>regularization</br>.",
                "Unlike traditional globally regularized clustering methods, our method first construct a local regularized linear label predictor for each document vector, and then combine all those local regularizers with a global smoothness regularizer.",
                "So we call our algorithm Clustering with Local and Global <br>regularization</br> (CLGR).",
                "We will show that the cluster memberships of the documents can be achieved by eigenvalue decomposition of a sparse symmetric matrix, which can be efficiently solved by iterative methods.",
                "Finally our experimental evaluations on several datasets are presented to show the superiorities of CLGR over traditional document clustering methods.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Clustering; I.2.6 [Artificial Intelligence]: Learning-Concept Learning General Terms Algorithms 1.",
                "INTRODUCTION Document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering.",
                "A good document clustering approach can assist the computers to automatically organize the document corpus into a meaningful cluster hierarchy for efficient browsing and navigation, which is very valuable for complementing the deficiencies of traditional information retrieval technologies.",
                "As pointed out by [8], the information retrieval needs can be expressed by a spectrum ranged from narrow keyword-matching based search to broad information browsing such as what are the major international events in recent months.",
                "Traditional document retrieval engines tend to fit well with the search end of the spectrum, i.e. they usually provide specified search for documents matching the users query, however, it is hard for them to meet the needs from the rest of the spectrum in which a rather broad or vague information is needed.",
                "In such cases, efficient browsing through a good cluster hierarchy will be definitely helpful.",
                "Generally, document clustering methods can be mainly categorized into two classes: hierarchical methods and partitioning methods.",
                "The hierarchical methods group the data points into a hierarchical tree structure using bottom-up or top-down approaches.",
                "For example, hierarchical agglomerative clustering (HAC) [13] is a typical bottom-up hierarchical clustering method.",
                "It takes each data point as a single cluster to start off with and then builds bigger and bigger clusters by grouping similar data points together until the entire dataset is encapsulated into one final cluster.",
                "On the other hand, partitioning methods decompose the dataset into a number of disjoint clusters which are usually optimal in terms of some predefined criterion functions.",
                "For instance, K-means [13] is a typical partitioning method which aims to minimize the sum of the squared distance between the data points and their corresponding cluster centers.",
                "In this paper, we will focus on the partitioning methods.",
                "As we know that there are two main problems existing in partitioning methods (like Kmeans and Gaussian Mixture Model (GMM) [16]): (1) the predefined criterion is usually non-convex which causes many local optimal solutions; (2) the iterative procedure (e.g. the Expectation Maximization (EM) algorithm) for optimizing the criterions usually makes the final solutions heavily depend on the initializations.",
                "In the last decades, many methods have been proposed to overcome the above problems of the partitioning methods [19][28].",
                "Recently, another type of partitioning methods based on clustering on data graphs have aroused considerable interests in the machine learning and data mining community.",
                "The basic idea behind these methods is to first model the whole dataset as a weighted graph, in which the graph nodes represent the data points, and the weights on the edges correspond to the similarities between pairwise points.",
                "Then the cluster assignments of the dataset can be achieved by optimizing some criterions defined on the graph.",
                "For example Spectral Clustering is one kind of the most representative graph-based clustering approaches, it generally aims to optimize some cut value (e.g.",
                "Normalized Cut [22], Ratio Cut [7], Min-Max Cut [11]) defined on an undirected graph.",
                "After some relaxations, these criterions can usually be optimized via eigen-decompositions, which is guaranteed to be global optimal.",
                "In this way, spectral clustering efficiently avoids the problems of the traditional partitioning methods as we introduced in last paragraph.",
                "In this paper, we propose a novel document clustering algorithm that inherits the superiority of spectral clustering, i.e. the final cluster results can also be obtained by exploit the eigen-structure of a symmetric matrix.",
                "However, unlike spectral clustering, which just enforces a smoothness constraint on the data labels over the whole data manifold [2], our method first construct a regularized linear label predictor for each data point from its neighborhood as in [25], and then combine the results of all these local label predictors with a global label smoothness regularizer.",
                "So we call our method Clustering with Local and Global <br>regularization</br> (CLGR).",
                "The idea of incorporating both local and global information into label prediction is inspired by the recent works on semi-supervised learning [31], and our experimental evaluations on several real document datasets show that CLGR performs better than many state-of-the-art clustering methods.",
                "The rest of this paper is organized as follows: in section 2 we will introduce our CLGR algorithm in detail.",
                "The experimental results on several datasets are presented in section 3, followed by the conclusions and discussions in section 4. 2.",
                "THE PROPOSED ALGORITHM In this section, we will introduce our Clustering with Local and Global <br>regularization</br> (CLGR) algorithm in detail.",
                "First lets see the how the documents are represented throughout this paper. 2.1 Document Representation In our work, all the documents are represented by the weighted term-frequency vectors.",
                "Let W = {w1, w2, · · · , wm} be the complete vocabulary set of the document corpus (which is preprocessed by the stopwords removal and words stemming operations).",
                "The term-frequency vector xi of document di is defined as xi = [xi1, xi2, · · · , xim]T , xik = tik log n idfk , where tik is the term frequency of wk ∈ W, n is the size of the document corpus, idfk is the number of documents that contain word wk.",
                "In this way, xi is also called the TFIDF representation of document di.",
                "Furthermore, we also normalize each xi (1 i n) to have a unit length, so that each document is represented by a normalized TF-IDF vector. 2.2 Local <br>regularization</br> As its name suggests, CLGR is composed of two parts: local <br>regularization</br> and global regularization.",
                "In this subsection we will introduce the local <br>regularization</br> part in detail. 2.2.1 Motivation As we know that clustering is one type of learning techniques, it aims to organize the dataset in a reasonable way.",
                "Generally speaking, learning can be posed as a problem of function estimation, from which we can get a good classification function that will assign labels to the training dataset and even the unseen testing dataset with some cost minimized [24].",
                "For example, in the two-class classification scenario1 (in which we exactly know the label of each document), a linear classifier with least square fit aims to learn a column vector w such that the squared cost J = 1 n (wT xi − yi)2 (1) is minimized, where yi ∈ {+1, −1} is the label of xi.",
                "By taking ∂J /∂w = 0, we get the solution w∗ = n i=1 xixT i −1 n i=1 xiyi , (2) which can further be written in its matrix form as w∗ = XXT −1 Xy, (3) where X = [x1, x2, · · · , xn] is an m × n document matrix, y = [y1, y2, · · · , yn]T is the label vector.",
                "Then for a test document t, we can determine its label by l = sign(w∗T u), (4) where sign(·) is the sign function.",
                "A natural problem in Eq. (3) is that the matrix XXT may be singular and thus not invertable (e.g. when m n).",
                "To avoid such a problem, we can add a <br>regularization</br> term and minimize the following criterion J = 1 n n i=1 (wT xi − yi)2 + λ w 2 , (5) where λ is a <br>regularization</br> parameter.",
                "Then the optimal solution that minimize J is given by w∗ = XXT + λnI −1 Xy, (6) where I is an m × m identity matrix.",
                "It has been reported that the regularized linear classifier can achieve very good results on text classification problems [29].",
                "However, despite its empirical success, the regularized linear classifier is on earth a global classifier, i.e. w∗ is estimated using the whole training set.",
                "According to [24], this may not be a smart idea, since a unique w∗ may not be good enough for predicting the labels of the whole input space.",
                "In order to get better predictions, [6] proposed to train classifiers locally and use them to classify the testing points.",
                "For example, a testing point will be classified by the local classifier trained using the training points located in the vicinity 1 In the following discussions we all assume that the documents coming from only two classes.",
                "The generalizations of our method to multi-class cases will be discussed in section 2.5. of it.",
                "Although this method seems slow and stupid, it is reported that it can get better performances than using a unique global classifier on certain tasks [6]. 2.2.2 Constructing the Local Regularized Predictors Inspired by their success, we proposed to apply the local learning algorithms for clustering.",
                "The basic idea is that, for each document vector xi (1 i n), we train a local label predictor based on its k-nearest neighborhood Ni, and then use it to predict the label of xi.",
                "Finally we will combine all those local predictors by minimizing the sum of their prediction errors.",
                "In this subsection we will introduce how to construct those local predictors.",
                "Due to the simplicity and effectiveness of the regularized linear classifier that we have introduced in section 2.2.1, we choose it to be our local label predictor, such that for each document xi, the following criterion is minimized Ji = 1 ni xj ∈Ni wT i xj − qj 2 + λi wi 2 , (7) where ni = |Ni| is the cardinality of Ni, and qj is the cluster membership of xj.",
                "Then using Eq. (6), we can get the optimal solution is w∗ i = XiXT i + λiniI −1 Xiqi, (8) where Xi = [xi1, xi2, · · · , xini ], and we use xik to denote the k-th nearest neighbor of xi. qi = [qi1, qi2, · · · , qini ]T with qik representing the cluster assignment of xik.",
                "The problem here is that XiXT i is an m × m matrix with m ni, i.e. we should compute the inverse of an m × m matrix for every document vector, which is computationally prohibited.",
                "Fortunately, we have the following theorem: Theorem 1. w∗ i in Eq. (8) can be rewritten as w∗ i = Xi XT i Xi + λiniIi −1 qi, (9) where Ii is an ni × ni identity matrix.",
                "Proof.",
                "Since w∗ i = XiXT i + λiniI −1 Xiqi, then XiXT i + λiniI w∗ i = Xiqi =⇒ XiXT i w∗ i + λiniw∗ i = Xiqi =⇒ w∗ i = (λini)−1 Xi qi − XT i w∗ i .",
                "Let β = (λini)−1 qi − XT i w∗ i , then w∗ i = Xiβ =⇒ λiniβ = qi − XT i w∗ i = qi − XT i Xiβ =⇒ qi = XT i Xi + λiniIi β =⇒ β = XT i Xi + λiniIi −1 qi.",
                "Therefore w∗ i = Xiβ = Xi XT i Xi + λiniIi −1 qi 2 Using theorem 1, we only need to compute the inverse of an ni × ni matrix for every document to train a local label predictor.",
                "Moreover, for a new testing point u that falls into Ni, we can classify it by the sign of qu = w∗T i u = uT wi = uT Xi XT i Xi + λiniIi −1 qi.",
                "This is an attractive expression since we can determine the cluster assignment of u by using the inner-products between the points in {u ∪ Ni}, which suggests that such a local regularizer can easily be kernelized [21] as long as we define a proper kernel function. 2.2.3 Combining the Local Regularized Predictors After all the local predictors having been constructed, we will combine them together by minimizing Jl = n i=1 w∗T i xi − qi 2 , (10) which stands for the sum of the prediction errors for all the local predictors.",
                "Combining Eq. (10) with Eq. (6), we can get Jl = n i=1 w∗T i xi − qi 2 = n i=1 xT i Xi XT i Xi + λiniIi −1 qi − qi 2 = Pq − q 2 , (11) where q = [q1, q2, · · · , qn]T , and the P is an n × n matrix constructing in the following way.",
                "Let αi = xT i Xi XT i Xi + λiniIi −1 , then Pij = αi j, if xj ∈ Ni 0, otherwise , (12) where Pij is the (i, j)-th entry of P, and αi j represents the j-th entry of αi .",
                "Till now we can write the criterion of clustering by combining locally regularized linear label predictors Jl in an explicit mathematical form, and we can minimize it directly using some standard optimization techniques.",
                "However, the results may not be good enough since we only exploit the local informations of the dataset.",
                "In the next subsection, we will introduce a global <br>regularization</br> criterion and combine it with Jl, which aims to find a good clustering result in a local-global way. 2.3 Global <br>regularization</br> In data clustering, we usually require that the cluster assignments of the data points should be sufficiently smooth with respect to the underlying data manifold, which implies (1) the nearby points tend to have the same cluster assignments; (2) the points on the same structure (e.g. submanifold or cluster) tend to have the same cluster assignments [31].",
                "Without the loss of generality, we assume that the data points reside (roughly) on a low-dimensional manifold M2 , and q is the cluster assignment function defined on M, i.e. 2 We believe that the text data are also sampled from some low dimensional manifold, since it is impossible for them to for ∀x ∈ M, q(x) returns the cluster membership of x.",
                "The smoothness of q over M can be calculated by the following Dirichlet integral [2] D[q] = 1 2 M q(x) 2 dM, (13) where the gradient q is a vector in the tangent space T Mx, and the integral is taken with respect to the standard measure on M. If we restrict the scale of q by q, q M = 1 (where ·, · M is the inner product induced on M), then it turns out that finding the smoothest function minimizing D[q] reduces to finding the eigenfunctions of the Laplace Beltrami operator L, which is defined as Lq −div q, (14) where div is the divergence of a vector field.",
                "Generally, the graph can be viewed as the discretized form of manifold.",
                "We can model the dataset as an weighted undirected graph as in spectral clustering [22], where the graph nodes are just the data points, and the weights on the edges represent the similarities between pairwise points.",
                "Then it can be shown that minimizing Eq. (13) corresponds to minimizing Jg = qT Lq = n i=1 (qi − qj)2 wij, (15) where q = [q1, q2, · · · , qn]T with qi = q(xi), L is the graph Laplacian with its (i, j)-th entry Lij =    di − wii, if i = j −wij, if xi and xj are adjacent 0, otherwise, (16) where di = j wij is the degree of xi, wij is the similarity between xi and xj.",
                "If xi and xj are adjacent3 , wij is usually computed in the following way wij = e − xi−xj 2 2σ2 , (17) where σ is a dataset dependent parameter.",
                "It is proved that under certain conditions, such a form of wij to determine the weights on graph edges leads to the convergence of graph Laplacian to the Laplace Beltrami operator [3][18].",
                "In summary, using Eq. (15) with exponential weights can effectively measure the smoothness of the data assignments with respect to the intrinsic data manifold.",
                "Thus we adopt it as a global regularizer to punish the smoothness of the predicted data assignments. 2.4 Clustering with Local and Global <br>regularization</br> Combining the contents we have introduced in section 2.2 and section 2.3 we can derive the clustering criterion is minq J = Jl + λJg = Pq − q 2 + λqT Lq s.t. qi ∈ {−1, +1}, (18) where P is defined as in Eq. (12), and λ is a <br>regularization</br> parameter to trade off Jl and Jg.",
                "However, the discrete fill in the whole high-dimensional sample space.",
                "And it has been shown that the manifold based methods can achieve good results on text classification tasks [31]. 3 In this paper, we define xi and xj to be adjacent if xi ∈ N(xj) or xj ∈ N(xi). constraint of pi makes the problem an NP hard integer programming problem.",
                "A natural way for making the problem solvable is to remove the constraint and relax qi to be continuous, then the objective that we aims to minimize becomes J = Pq − q 2 + λqT Lq = qT (P − I)T (P − I)q + λqT Lq = qT (P − I)T (P − I) + λL q, (19) and we further add a constraint qT q = 1 to restrict the scale of q.",
                "Then our objective becomes minq J = qT (P − I)T (P − I) + λL q s.t. qT q = 1 (20) Using the Lagrangian method, we can derive that the optimal solution q corresponds to the smallest eigenvector of the matrix M = (P − I)T (P − I) + λL, and the cluster assignment of xi can be determined by the sign of qi, i.e. xi will be classified as class one if qi > 0, otherwise it will be classified as class 2. 2.5 Multi-Class CLGR In the above we have introduced the basic framework of Clustering with Local and Global <br>regularization</br> (CLGR) for the two-class clustering problem, and we will extending it to multi-class clustering in this subsection.",
                "First we assume that all the documents belong to C classes indexed by L = {1, 2, · · · , C}. qc is the classification function for class c (1 c C), such that qc (xi) returns the confidence that xi belongs to class c. Our goal is to obtain the value of qc (xi) (1 c C, 1 i n), and the cluster assignment of xi can be determined by {qc (xi)}C c=1 using some proper discretization methods that we will introduce later.",
                "Therefore, in this multi-class case, for each document xi (1 i n), we will construct C locally linear regularized label predictors whose normal vectors are wc∗ i = Xi XT i Xi + λiniIi −1 qc i (1 c C), (21) where Xi = [xi1, xi2, · · · , xini ] with xik being the k-th neighbor of xi, and qc i = [qc i1, qc i2, · · · , qc ini ]T with qc ik = qc (xik).",
                "Then (wc∗ i )T xi returns the predicted confidence of xi belonging to class c. Hence the local prediction error for class c can be defined as J c l = n i=1 (wc∗ i ) T xi − qc i 2 , (22) And the total local prediction error becomes Jl = C c=1 J c l = C c=1 n i=1 (wc∗ i ) T xi − qc i 2 . (23) As in Eq. (11), we can define an n×n matrix P (see Eq. (12)) and rewrite Jl as Jl = C c=1 J c l = C c=1 Pqc − qc 2 . (24) Similarly we can define the global smoothness regularizer in multi-class case as Jg = C c=1 n i=1 (qc i − qc j )2 wij = C c=1 (qc )T Lqc . (25) Then the criterion to be minimized for CLGR in multi-class case becomes J = Jl + λJg = C c=1 Pqc − qc 2 + λ(qc )T Lqc = C c=1 (qc )T (P − I)T (P − I) + λL qc = trace QT (P − I)T (P − I) + λL Q , (26) where Q = [q1 , q2 , · · · , qc ] is an n × c matrix, and trace(·) returns the trace of a matrix.",
                "The same as in Eq. (20), we also add the constraint that QT Q = I to restrict the scale of Q.",
                "Then our optimization problem becomes minQ J = trace QT (P − I)T (P − I) + λL Q s.t.",
                "QT Q = I, (27) From the Ky Fan theorem [28], we know the optimal solution of the above problem is Q∗ = [q∗ 1, q∗ 2, · · · , q∗ C ]R, (28) where q∗ k (1 k C) is the eigenvector corresponds to the k-th smallest eigenvalue of matrix (P − I)T (P − I) + λL, and R is an arbitrary C × C matrix.",
                "Since the values of the entries in Q∗ is continuous, we need to further discretize Q∗ to get the cluster assignments of all the data points.",
                "There are mainly two approaches to achieve this goal: 1.",
                "As in [20], we can treat the i-th row of Q as the embedding of xi in a C-dimensional space, and apply some traditional clustering methods like kmeans to clustering these embeddings into C clusters. 2.",
                "Since the optimal Q∗ is not unique (because of the existence of an arbitrary matrix R), we can pursue an optimal R that will rotate Q∗ to an indication matrix4 .",
                "The detailed algorithm can be referred to [26].",
                "The detailed algorithm procedure for CLGR is summarized in table 1. 3.",
                "EXPERIMENTS In this section, experiments are conducted to empirically compare the clustering results of CLGR with other 8 representitive document clustering algorithms on 5 datasets.",
                "First we will introduce the basic informations of those datasets. 3.1 Datasets We use a variety of datasets, most of which are frequently used in the information retrieval research.",
                "Table 2 summarizes the characteristics of the datasets. 4 Here an indication matrix T is a n×c matrix with its (i, j)th entry Tij ∈ {0, 1} such that for each row of Q∗ there is only one 1.",
                "Then the xi can be assigned to the j-th cluster such that j = argjQ∗ ij = 1.",
                "Table 1: Clustering with Local and Global <br>regularization</br> (CLGR) Input: 1.",
                "Dataset X = {xi}n i=1; 2.",
                "Number of clusters C; 3.",
                "Size of the neighborhood K; 4.",
                "Local <br>regularization</br> parameters {λi}n i=1; 5.",
                "Global <br>regularization</br> parameter λ; Output: The cluster membership of each data point.",
                "Procedure: 1.",
                "Construct the K nearest neighborhoods for each data point; 2.",
                "Construct the matrix P using Eq. (12); 3.",
                "Construct the Laplacian matrix L using Eq. (16); 4.",
                "Construct the matrix M = (P − I)T (P − I) + λL; 5.",
                "Do eigenvalue decomposition on M, and construct the matrix Q∗ according to Eq. (28); 6.",
                "Output the cluster assignments of each data point by properly discretize Q∗ .",
                "Table 2: Descriptions of the document datasets Datasets Number of documents Number of classes CSTR 476 4 WebKB4 4199 4 Reuters 2900 10 WebACE 2340 20 Newsgroup4 3970 4 CSTR.",
                "This is the dataset of the abstracts of technical reports published in the Department of Computer Science at a university.",
                "The dataset contained 476 abstracts, which were divided into four research areas: Natural Language Processing(NLP), Robotics/Vision, Systems, and Theory.",
                "WebKB.",
                "The WebKB dataset contains webpages gathered from university computer science departments.",
                "There are about 8280 documents and they are divided into 7 categories: student, faculty, staff, course, project, department and other.",
                "The raw text is about 27MB.",
                "Among these 7 categories, student, faculty, course and project are four most populous entity-representing categories.",
                "The associated subset is typically called WebKB4.",
                "Reuters.",
                "The Reuters-21578 Text Categorization Test collection contains documents collected from the Reuters newswire in 1987.",
                "It is a standard text categorization benchmark and contains 135 categories.",
                "In our experiments, we use a subset of the data collection which includes the 10 most frequent categories among the 135 topics and we call it Reuters-top 10.",
                "WebACE.",
                "The WebACE dataset was from WebACE project and has been used for document clustering [17][5].",
                "The WebACE dataset contains 2340 documents consisting news articles from Reuters new service via the Web in October 1997.",
                "These documents are divided into 20 classes.",
                "News4.",
                "The News4 dataset used in our experiments are selected from the famous 20-newsgroups dataset5 .",
                "The topic rec containing autos, motorcycles, baseball and hockey was selected from the version 20news-18828.",
                "The News4 dataset contains 3970 document vectors. 5 http://people.csail.mit.edu/jrennie/20Newsgroups/ To pre-process the datasets, we remove the stop words using a standard stop list, all HTML tags are skipped and all header fields except subject and organization of the posted articles are ignored.",
                "In all our experiments, we first select the top 1000 words by mutual information with class labels. 3.2 Evaluation Metrics In the experiments, we set the number of clusters equal to the true number of classes C for all the clustering algorithms.",
                "To evaluate their performance, we compare the clusters generated by these algorithms with the true classes by computing the following two performance measures.",
                "Clustering Accuracy (Acc).",
                "The first performance measure is the Clustering Accuracy, which discovers the one-toone relationship between clusters and classes and measures the extent to which each cluster contained data points from the corresponding class.",
                "It sums up the whole matching degree between all pair class-clusters.",
                "Clustering accuracy can be computed as: Acc = 1 N max   Ck,Lm T(Ck, Lm)   , (29) where Ck denotes the k-th cluster in the final results, and Lm is the true m-th class.",
                "T(Ck, Lm) is the number of entities which belong to class m are assigned to cluster k. Accuracy computes the maximum sum of T(Ck, Lm) for all pairs of clusters and classes, and these pairs have no overlaps.",
                "The greater clustering accuracy means the better clustering performance.",
                "Normalized Mutual Information (NMI).",
                "Another evaluation metric we adopt here is the Normalized Mutual Information NMI [23], which is widely used for determining the quality of clusters.",
                "For two random variable X and Y, the NMI is defined as: NMI(X, Y) = I(X, Y) H(X)H(Y) , (30) where I(X, Y) is the mutual information between X and Y, while H(X) and H(Y) are the entropies of X and Y respectively.",
                "One can see that NMI(X, X) = 1, which is the maximal possible value of NMI.",
                "Given a clustering result, the NMI in Eq. (30) is estimated as NMI = C k=1 C m=1 nk,mlog n·nk,m nk ˆnm C k=1 nklog nk n C m=1 ˆnmlog ˆnm n , (31) where nk denotes the number of data contained in the cluster Ck (1 k C), ˆnm is the number of data belonging to the m-th class (1 m C), and nk,m denotes the number of data that are in the intersection between the cluster Ck and the m-th class.",
                "The value calculated in Eq. (31) is used as a performance measure for the given clustering result.",
                "The larger this value, the better the clustering performance. 3.3 Comparisons We have conducted comprehensive performance evaluations by testing our method and comparing it with 8 other representative data clustering methods using the same data corpora.",
                "The algorithms that we evaluated are listed below. 1.",
                "Traditional k-means (KM). 2.",
                "Spherical k-means (SKM).",
                "The implementation is based on [9]. 3.",
                "Gaussian Mixture Model (GMM).",
                "The implementation is based on [16]. 4.",
                "Spectral Clustering with Normalized Cuts (Ncut).",
                "The implementation is based on [26], and the variance of the Gaussian similarity is determined by Local Scaling [30].",
                "Note that the criterion that Ncut aims to minimize is just the global regularizer in our CLGR algorithm except that Ncut used the normalized Laplacian. 5.",
                "Clustering using Pure Local <br>regularization</br> (CPLR).",
                "In this method we just minimize Jl (defined in Eq. (24)), and the clustering results can be obtained by doing eigenvalue decomposition on matrix (I − P)T (I − P) with some proper discretization methods. 6.",
                "Adaptive Subspace Iteration (ASI).",
                "The implementation is based on [14]. 7.",
                "Nonnegative Matrix Factorization (NMF).",
                "The implementation is based on [27]. 8.",
                "Tri-Factorization Nonnegative Matrix Factorization (TNMF) [12].",
                "The implementation is based on [15].",
                "For computational efficiency, in the implementation of CPLR and our CLGR algorithm, we have set all the local <br>regularization</br> parameters {λi}n i=1 to be identical, which is set by grid search from {0.1, 1, 10}.",
                "The size of the k-nearest neighborhoods is set by grid search from {20, 40, 80}.",
                "For the CLGR method, its global <br>regularization</br> parameter is set by grid search from {0.1, 1, 10}.",
                "When constructing the global regularizer, we have adopted the local scaling method [30] to construct the Laplacian matrix.",
                "The final discretization method adopted in these two methods is the same as in [26], since our experiments show that using such method can achieve better results than using kmeans based methods as in [20]. 3.4 Experimental Results The clustering accuracies comparison results are shown in table 3, and the normalized mutual information comparison results are summarized in table 4.",
                "From the two tables we mainly observe that: 1.",
                "Our CLGR method outperforms all other document clustering methods in most of the datasets; 2.",
                "For document clustering, the Spherical k-means method usually outperforms the traditional k-means clustering method, and the GMM method can achieve competitive results compared to the Spherical k-means method; 3.",
                "The results achieved from the k-means and GMM type algorithms are usually worse than the results achieved from Spectral Clustering.",
                "Since Spectral Clustering can be viewed as a weighted version of kernel k-means, it can obtain good results the data clusters are arbitrarily shaped.",
                "This corroborates that the documents vectors are not regularly distributed (spherical or elliptical). 4.",
                "The experimental comparisons empirically verify the equivalence between NMF and Spectral Clustering, which Table 3: Clustering accuracies of the various methods CSTR WebKB4 Reuters WebACE News4 KM 0.4256 0.3888 0.4448 0.4001 0.3527 SKM 0.4690 0.4318 0.5025 0.4458 0.3912 GMM 0.4487 0.4271 0.4897 0.4521 0.3844 NMF 0.5713 0.4418 0.4947 0.4761 0.4213 Ncut 0.5435 0.4521 0.4896 0.4513 0.4189 ASI 0.5621 0.4752 0.5235 0.4823 0.4335 TNMF 0.6040 0.4832 0.5541 0.5102 0.4613 CPLR 0.5974 0.5020 0.4832 0.5213 0.4890 CLGR 0.6235 0.5228 0.5341 0.5376 0.5102 Table 4: Normalized mutual information results of the various methods CSTR WebKB4 Reuters WebACE News4 KM 0.3675 0.3023 0.4012 0.3864 0.3318 SKM 0.4027 0.4155 0.4587 0.4003 0.4085 GMM 0.4034 0.4093 0.4356 0.4209 0.3994 NMF 0.5235 0.4517 0.4402 0.4359 0.4130 Ncut 0.4833 0.4497 0.4392 0.4289 0.4231 ASI 0.5008 0.4833 0.4769 0.4817 0.4503 TNMF 0.5724 0.5011 0.5132 0.5328 0.4749 CPLR 0.5695 0.5231 0.4402 0.5543 0.4690 CLGR 0.6012 0.5434 0.4935 0.5390 0.4908 has been proved theoretically in [10].",
                "It can be observed from the tables that NMF and Spectral Clustering usually lead to similar clustering results. 5.",
                "The co-clustering based methods (TNMF and ASI) can usually achieve better results than traditional purely document vector based methods.",
                "Since these methods perform an implicit feature selection at each iteration, provide an adaptive metric for measuring the neighborhood, and thus tend to yield better clustering results. 6.",
                "The results achieved from CPLR are usually better than the results achieved from Spectral Clustering, which supports Vapniks theory [24] that sometimes local learning algorithms can obtain better results than global learning algorithms.",
                "Besides the above comparison experiments, we also test the parameter sensibility of our method.",
                "There are mainly two sets of parameters in our CLGR algorithm, the local and global <br>regularization</br> parameters ({λi}n i=1 and λ, as we have said in section 3.3, we have set all λis to be identical to λ∗ in our experiments), and the size of the neighborhoods.",
                "Therefore we have also done two sets of experiments: 1.",
                "Fixing the size of the neighborhoods, and testing the clustering performance with varying λ∗ and λ.",
                "In this set of experiments, we find that our CLGR algorithm can achieve good results when the two <br>regularization</br> parameters are neither too large nor too small.",
                "Typically our method can achieve good results when λ∗ and λ are around 0.1.",
                "Figure 1 shows us such a testing example on the WebACE dataset. 2.",
                "Fixing the local and global <br>regularization</br> parameters, and testing the clustering performance with different −5 −4.5 −4 −3.5 −3 −5 −4.5 −4 −3.5 −3 0.35 0.4 0.45 0.5 0.55 local <br>regularization</br> para (log 2 value) global regularization para (log 2 value) clusteringaccuracy Figure 1: Parameter sensibility testing results on the WebACE dataset with the neighborhood size fixed to 20, and the x-axis and y-axis represents the log2 value of λ∗ and λ. sizes of neighborhoods.",
                "In this set of experiments, we find that the neighborhood with a too large or too small size will all deteriorate the final clustering results.",
                "This can be easily understood since when the neighborhood size is very small, then the data points used for training the local classifiers may not be sufficient; when the neighborhood size is very large, the trained classifiers will tend to be global and cannot capture the typical local characteristics.",
                "Figure 2 shows us a testing example on the WebACE dataset.",
                "Therefore, we can see that our CLGR algorithm (1) can achieve satisfactory results and (2) is not very sensitive to the choice of parameters, which makes it practical in real world applications. 4.",
                "CONCLUSIONS AND FUTURE WORKS In this paper, we derived a new clustering algorithm called clustering with local and global <br>regularization</br>.",
                "Our method preserves the merit of local learning algorithms and spectral clustering.",
                "Our experiments show that the proposed algorithm outperforms most of the state of the art algorithms on many benchmark datasets.",
                "In the future, we will focus on the parameter selection and acceleration issues of the CLGR algorithm. 5.",
                "REFERENCES [1] L. Baker and A. McCallum.",
                "Distributional Clustering of Words for Text Classification.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 1998. [2] M. Belkin and P. Niyogi.",
                "Laplacian Eigenmaps for Dimensionality Reduction and Data Representation.",
                "Neural Computation, 15 (6):1373-1396.",
                "June 2003. [3] M. Belkin and P. Niyogi.",
                "Towards a Theoretical Foundation for Laplacian-Based Manifold Methods.",
                "In Proceedings of the 18th Conference on Learning Theory (COLT). 2005. 10 20 30 40 50 60 70 80 90 100 0.35 0.4 0.45 0.5 0.55 size of the neighborhood clusteringaccuracy Figure 2: Parameter sensibility testing results on the WebACE dataset with the <br>regularization</br> parameters being fixed to 0.1, and the neighborhood size varing from 10 to 100. [4] M. Belkin, P. Niyogi and V. Sindhwani.",
                "Manifold <br>regularization</br>: a Geometric Framework for Learning from Examples.",
                "Journal of Machine Learning Research 7, 1-48, 2006. [5] D. Boley.",
                "Principal Direction Divisive Partitioning.",
                "Data mining and knowledge discovery, 2:325-344, 1998. [6] L. Bottou and V. Vapnik.",
                "Local learning algorithms.",
                "Neural Computation, 4:888-900, 1992. [7] P. K. Chan, D. F. Schlag and J. Y. Zien.",
                "Spectral K-way Ratio-Cut Partitioning and Clustering.",
                "IEEE Trans.",
                "Computer-Aided Design, 13:1088-1096, Sep. 1994. [8] D. R. Cutting, D. R. Karger, J. O. Pederson and J. W. Tukey.",
                "Scatter/Gather: A Cluster-Based Approach to Browsing Large Document Collections.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 1992. [9] I. S. Dhillon and D. S. Modha.",
                "Concept Decompositions for Large Sparse Text Data using Clustering.",
                "Machine Learning, vol. 42(1), pages 143-175, January 2001. [10] C. Ding, X.",
                "He, and H. Simon.",
                "On the equivalence of nonnegative matrix factorization and spectral clustering.",
                "In Proceedings of the SIAM Data Mining Conference, 2005. [11] C. Ding, X.",
                "He, H. Zha, M. Gu, and H. D. Simon.",
                "A min-max cut algorithm for graph partitioning and data clustering.",
                "In Proc. of the 1st International Conference on Data Mining (ICDM), pages 107-114, 2001. [12] C. Ding, T. Li, W. Peng, and H. Park.",
                "Orthogonal Nonnegative Matrix Tri-Factorizations for Clustering.",
                "In Proceedings of the Twelfth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2006. [13] R. O. Duda, P. E. Hart, and D. G. Stork.",
                "Pattern Classification.",
                "John Wiley & Sons, Inc., 2001. [14] T. Li, S. Ma, and M. Ogihara.",
                "Document Clustering via Adaptive Subspace Iteration.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2004. [15] T. Li and C. Ding.",
                "The Relationships Among Various Nonnegative Matrix Factorization Methods for Clustering.",
                "In Proceedings of the 6th International Conference on Data Mining (ICDM). 2006. [16] X. Liu and Y. Gong.",
                "Document Clustering with Cluster Refinement and Model Selection Capabilities.",
                "In Proc. of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002. [17] E. Han, D. Boley, M. Gini, R. Gross, K. Hastings, G. Karypis, V. Kumar, B. Mobasher, and J. Moore.",
                "WebACE: A Web Agent for Document Categorization and Exploration.",
                "In Proceedings of the 2nd International Conference on Autonomous Agents (Agents98).",
                "ACM Press, 1998. [18] M. Hein, J. Y. Audibert, and U. von Luxburg.",
                "From Graphs to Manifolds - Weak and Strong Pointwise Consistency of Graph Laplacians.",
                "In Proceedings of the 18th Conference on Learning Theory (COLT), 470-485. 2005. [19] J.",
                "He, M. Lan, C.-L. Tan, S.-Y.",
                "Sung, and H.-B.",
                "Low.",
                "Initialization of Cluster Refinement Algorithms: A Review and Comparative Study.",
                "In Proc. of Inter.",
                "Joint Conference on Neural Networks, 2004. [20] A. Y. Ng, M. I. Jordan, Y. Weiss.",
                "On Spectral Clustering: Analysis and an algorithm.",
                "In Advances in Neural Information Processing Systems 14. 2002. [21] B. Sch¨olkopf and A. Smola.",
                "Learning with Kernels.",
                "The MIT Press.",
                "Cambridge, Massachusetts. 2002. [22] J. Shi and J. Malik.",
                "Normalized Cuts and Image Segmentation.",
                "IEEE Trans. on Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000. [23] A. Strehl and J. Ghosh.",
                "Cluster Ensembles - A Knowledge Reuse Framework for Combining Multiple Partitions.",
                "Journal of Machine Learning Research, 3:583-617, 2002. [24] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Berlin: Springer-Verlag, 1995. [25] Wu, M. and Sch¨olkopf, B.",
                "A Local Learning Approach for Clustering.",
                "In Advances in Neural Information Processing Systems 18. 2006. [26] S. X. Yu, J. Shi.",
                "Multiclass Spectral Clustering.",
                "In Proceedings of the International Conference on Computer Vision, 2003. [27] W. Xu, X. Liu and Y. Gong.",
                "Document Clustering Based On Non-Negative Matrix Factorization.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2003. [28] H. Zha, X.",
                "He, C. Ding, M. Gu and H. Simon.",
                "Spectral Relaxation for K-means Clustering.",
                "In NIPS 14. 2001. [29] T. Zhang and F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Journal of Information Retrieval, 4:5-31, 2001. [30] L. Zelnik-Manor and P. Perona.",
                "Self-Tuning Spectral Clustering.",
                "In NIPS 17. 2005. [31] D. Zhou, O. Bousquet, T. N. Lal, J. Weston and B. Sch¨olkopf.",
                "Learning with Local and Global Consistency.",
                "NIPS 17, 2005."
            ],
            "original_annotated_samples": [
                "In this paper, we propose a novel method for clustering documents using <br>regularization</br>.",
                "So we call our algorithm Clustering with Local and Global <br>regularization</br> (CLGR).",
                "So we call our method Clustering with Local and Global <br>regularization</br> (CLGR).",
                "THE PROPOSED ALGORITHM In this section, we will introduce our Clustering with Local and Global <br>regularization</br> (CLGR) algorithm in detail.",
                "Furthermore, we also normalize each xi (1 i n) to have a unit length, so that each document is represented by a normalized TF-IDF vector. 2.2 Local <br>regularization</br> As its name suggests, CLGR is composed of two parts: local <br>regularization</br> and global regularization."
            ],
            "translated_annotated_samples": [
                "En este artículo, proponemos un método novedoso para agrupar documentos utilizando <br>regularización</br>.",
                "Así que llamamos a nuestro algoritmo Agrupamiento con Regularización Local y Global (CLGR).",
                "Así que llamamos a nuestro método Agrupamiento con Regularización Local y Global (CLGR).",
                "El ALGORITMO PROPUESTO En esta sección, presentaremos en detalle nuestro algoritmo de Agrupamiento con Regularización Local y Global (CLGR).",
                "Además, también normalizamos cada xi (1 i n) para que tenga una longitud unitaria, de modo que cada documento esté representado por un vector TF-IDF normalizado. 2.2 Regularización Local Como su nombre sugiere, CLGR está compuesto por dos partes: <br>regularización</br> local y <br>regularización</br> global."
            ],
            "translated_text": "Agrupamiento regularizado para documentos ∗ Fei Wang, Changshui Zhang Laboratorio Clave del Estado de Tecnología Inteligente. En los últimos años, la agrupación de documentos ha estado recibiendo cada vez más atención como una técnica importante y fundamental para la organización no supervisada de documentos, la extracción automática de temas y la recuperación o filtrado rápido de información. En este artículo, proponemos un método novedoso para agrupar documentos utilizando <br>regularización</br>. A diferencia de los métodos de agrupamiento regularizados globalmente tradicionales, nuestro método primero construye un predictor de etiquetas lineal regularizado local para cada vector de documento, y luego combina todos esos regularizadores locales con un regularizador de suavidad global. Así que llamamos a nuestro algoritmo Agrupamiento con Regularización Local y Global (CLGR). Mostraremos que las pertenencias de los documentos a los grupos se pueden lograr mediante la descomposición de los valores propios de una matriz simétrica dispersa, la cual puede resolverse eficientemente mediante métodos iterativos. Finalmente, se presentan nuestras evaluaciones experimentales en varios conjuntos de datos para mostrar las ventajas de CLGR sobre los métodos tradicionales de agrupamiento de documentos. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información-Agrupamiento; I.2.6 [Inteligencia Artificial]: Aprendizaje-Aprendizaje de Conceptos Términos Generales Algoritmos 1. La agrupación de documentos ha estado recibiendo cada vez más atención como una técnica importante y fundamental para la organización no supervisada de documentos, la extracción automática de temas y la recuperación o filtrado rápido de información. Un buen enfoque de agrupación de documentos puede ayudar a las computadoras a organizar automáticamente el corpus de documentos en una jerarquía de clusters significativa para una navegación eficiente, lo cual es muy valioso para complementar las deficiencias de las tecnologías tradicionales de recuperación de información. Como señaló [8], las necesidades de recuperación de información pueden expresarse mediante un espectro que va desde la búsqueda basada en palabras clave estrechas hasta la exploración de información amplia, como cuáles son los principales eventos internacionales de los últimos meses. Los motores de búsqueda de documentos tradicionales tienden a adaptarse bien al extremo de la búsqueda, es decir, suelen proporcionar búsquedas específicas de documentos que coinciden con la consulta de los usuarios, sin embargo, les resulta difícil satisfacer las necesidades del resto del espectro en el que se necesita información más amplia o vaga. En tales casos, la navegación eficiente a través de una buena jerarquía de clústeres será definitivamente útil. Generalmente, los métodos de agrupamiento de documentos se pueden clasificar principalmente en dos clases: métodos jerárquicos y métodos de partición. Los métodos jerárquicos agrupan los puntos de datos en una estructura de árbol jerárquico utilizando enfoques de abajo hacia arriba o de arriba hacia abajo. Por ejemplo, el clustering jerárquico aglomerativo (HAC) [13] es un método típico de clustering jerárquico ascendente. Toma cada punto de datos como un único clúster para comenzar y luego construye clústeres más grandes agrupando puntos de datos similares juntos hasta que todo el conjunto de datos esté encapsulado en un único clúster final. Por otro lado, los métodos de particionamiento descomponen el conjunto de datos en un número de grupos disjuntos que suelen ser óptimos en términos de algunas funciones de criterio predefinidas. Por ejemplo, K-means es un método de particionamiento típico que tiene como objetivo minimizar la suma de las distancias al cuadrado entre los puntos de datos y sus centros de clúster correspondientes. En este documento, nos centraremos en los métodos de particionamiento. Como sabemos, existen dos problemas principales en los métodos de particionamiento (como Kmeans y el Modelo de Mezcla Gaussiana (GMM) [16]): (1) el criterio predefinido suele ser no convexo, lo que provoca muchas soluciones óptimas locales; (2) el procedimiento iterativo (por ejemplo, el algoritmo de Expectation Maximization (EM)) para optimizar los criterios suele hacer que las soluciones finales dependan en gran medida de las inicializaciones. En las últimas décadas, se han propuesto muchos métodos para superar los problemas mencionados de los métodos de particionamiento [19][28]. Recientemente, otro tipo de métodos de particionamiento basados en agrupamiento en grafos de datos han despertado un considerable interés en la comunidad de aprendizaje automático y minería de datos. La idea básica detrás de estos métodos es modelar primero el conjunto de datos completo como un grafo ponderado, en el que los nodos del grafo representan los puntos de datos y los pesos en los bordes corresponden a las similitudes entre los puntos en pares. Entonces, las asignaciones de clústeres del conjunto de datos se pueden lograr optimizando algunos criterios definidos en el gráfico. Por ejemplo, el Clustering Espectral es uno de los enfoques de clustering basados en grafos más representativos, generalmente tiene como objetivo optimizar algún valor de corte (por ejemplo, Corte normalizado [22], corte de razón [7], corte mínimo-máximo [11]) definidos en un grafo no dirigido. Después de algunas relajaciones, estos criterios generalmente pueden ser optimizados a través de descomposiciones propias, lo cual está garantizado que sea óptimo a nivel global. De esta manera, el agrupamiento espectral evita eficientemente los problemas de los métodos de particionamiento tradicionales que presentamos en el párrafo anterior. En este artículo, proponemos un algoritmo novedoso de agrupamiento de documentos que hereda la superioridad del agrupamiento espectral, es decir, los resultados finales de los grupos también pueden obtenerse explotando la estructura de los autovalores de una matriz simétrica. Sin embargo, a diferencia del agrupamiento espectral, que simplemente impone una restricción de suavidad en las etiquetas de los datos sobre todo el conjunto de datos [2], nuestro método primero construye un predictor de etiquetas lineal regularizado para cada punto de datos a partir de su vecindario como en [25], y luego combina los resultados de todos estos predictores de etiquetas locales con un regularizador de suavidad de etiquetas global. Así que llamamos a nuestro método Agrupamiento con Regularización Local y Global (CLGR). La idea de incorporar tanto información local como global en la predicción de etiquetas está inspirada en los trabajos recientes sobre aprendizaje semi-supervisado [31], y nuestras evaluaciones experimentales en varios conjuntos de datos reales de documentos muestran que CLGR funciona mejor que muchos métodos de agrupamiento de vanguardia. El resto de este documento está organizado de la siguiente manera: en la sección 2 presentaremos nuestro algoritmo CLGR en detalle. Los resultados experimentales en varios conjuntos de datos se presentan en la sección 3, seguidos de las conclusiones y discusiones en la sección 4. El ALGORITMO PROPUESTO En esta sección, presentaremos en detalle nuestro algoritmo de Agrupamiento con Regularización Local y Global (CLGR). Primero veamos cómo se representan los documentos a lo largo de este documento. 2.1 Representación de documentos En nuestro trabajo, todos los documentos se representan mediante vectores de frecuencia de términos ponderados. Sea W = {w1, w2, · · · , wm} el conjunto completo de vocabulario del corpus de documentos (que ha sido preprocesado mediante la eliminación de palabras vacías y operaciones de derivación de palabras). El vector de frecuencia de término xi del documento di se define como xi = [xi1, xi2, · · · , xim]T , xik = tik log n idfk , donde tik es la frecuencia del término wk ∈ W, n es el tamaño del corpus de documentos, idfk es el número de documentos que contienen la palabra wk. De esta manera, xi también se llama la representación TFIDF del documento di. Además, también normalizamos cada xi (1 i n) para que tenga una longitud unitaria, de modo que cada documento esté representado por un vector TF-IDF normalizado. 2.2 Regularización Local Como su nombre sugiere, CLGR está compuesto por dos partes: <br>regularización</br> local y <br>regularización</br> global. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "global regularization": {
            "translated_key": "regularización global",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Regularized Clustering for Documents ∗ Fei Wang, Changshui Zhang State Key Lab of Intelligent Tech.",
                "and Systems Department of Automation, Tsinghua University Beijing, China, 100084 feiwang03@gmail.com Tao Li School of Computer Science Florida International University Miami, FL 33199, U.S.A. taoli@cs.fiu.edu ABSTRACT In recent years, document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering.",
                "In this paper, we propose a novel method for clustering documents using regularization.",
                "Unlike traditional globally regularized clustering methods, our method first construct a local regularized linear label predictor for each document vector, and then combine all those local regularizers with a global smoothness regularizer.",
                "So we call our algorithm Clustering with Local and <br>global regularization</br> (CLGR).",
                "We will show that the cluster memberships of the documents can be achieved by eigenvalue decomposition of a sparse symmetric matrix, which can be efficiently solved by iterative methods.",
                "Finally our experimental evaluations on several datasets are presented to show the superiorities of CLGR over traditional document clustering methods.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Clustering; I.2.6 [Artificial Intelligence]: Learning-Concept Learning General Terms Algorithms 1.",
                "INTRODUCTION Document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering.",
                "A good document clustering approach can assist the computers to automatically organize the document corpus into a meaningful cluster hierarchy for efficient browsing and navigation, which is very valuable for complementing the deficiencies of traditional information retrieval technologies.",
                "As pointed out by [8], the information retrieval needs can be expressed by a spectrum ranged from narrow keyword-matching based search to broad information browsing such as what are the major international events in recent months.",
                "Traditional document retrieval engines tend to fit well with the search end of the spectrum, i.e. they usually provide specified search for documents matching the users query, however, it is hard for them to meet the needs from the rest of the spectrum in which a rather broad or vague information is needed.",
                "In such cases, efficient browsing through a good cluster hierarchy will be definitely helpful.",
                "Generally, document clustering methods can be mainly categorized into two classes: hierarchical methods and partitioning methods.",
                "The hierarchical methods group the data points into a hierarchical tree structure using bottom-up or top-down approaches.",
                "For example, hierarchical agglomerative clustering (HAC) [13] is a typical bottom-up hierarchical clustering method.",
                "It takes each data point as a single cluster to start off with and then builds bigger and bigger clusters by grouping similar data points together until the entire dataset is encapsulated into one final cluster.",
                "On the other hand, partitioning methods decompose the dataset into a number of disjoint clusters which are usually optimal in terms of some predefined criterion functions.",
                "For instance, K-means [13] is a typical partitioning method which aims to minimize the sum of the squared distance between the data points and their corresponding cluster centers.",
                "In this paper, we will focus on the partitioning methods.",
                "As we know that there are two main problems existing in partitioning methods (like Kmeans and Gaussian Mixture Model (GMM) [16]): (1) the predefined criterion is usually non-convex which causes many local optimal solutions; (2) the iterative procedure (e.g. the Expectation Maximization (EM) algorithm) for optimizing the criterions usually makes the final solutions heavily depend on the initializations.",
                "In the last decades, many methods have been proposed to overcome the above problems of the partitioning methods [19][28].",
                "Recently, another type of partitioning methods based on clustering on data graphs have aroused considerable interests in the machine learning and data mining community.",
                "The basic idea behind these methods is to first model the whole dataset as a weighted graph, in which the graph nodes represent the data points, and the weights on the edges correspond to the similarities between pairwise points.",
                "Then the cluster assignments of the dataset can be achieved by optimizing some criterions defined on the graph.",
                "For example Spectral Clustering is one kind of the most representative graph-based clustering approaches, it generally aims to optimize some cut value (e.g.",
                "Normalized Cut [22], Ratio Cut [7], Min-Max Cut [11]) defined on an undirected graph.",
                "After some relaxations, these criterions can usually be optimized via eigen-decompositions, which is guaranteed to be global optimal.",
                "In this way, spectral clustering efficiently avoids the problems of the traditional partitioning methods as we introduced in last paragraph.",
                "In this paper, we propose a novel document clustering algorithm that inherits the superiority of spectral clustering, i.e. the final cluster results can also be obtained by exploit the eigen-structure of a symmetric matrix.",
                "However, unlike spectral clustering, which just enforces a smoothness constraint on the data labels over the whole data manifold [2], our method first construct a regularized linear label predictor for each data point from its neighborhood as in [25], and then combine the results of all these local label predictors with a global label smoothness regularizer.",
                "So we call our method Clustering with Local and <br>global regularization</br> (CLGR).",
                "The idea of incorporating both local and global information into label prediction is inspired by the recent works on semi-supervised learning [31], and our experimental evaluations on several real document datasets show that CLGR performs better than many state-of-the-art clustering methods.",
                "The rest of this paper is organized as follows: in section 2 we will introduce our CLGR algorithm in detail.",
                "The experimental results on several datasets are presented in section 3, followed by the conclusions and discussions in section 4. 2.",
                "THE PROPOSED ALGORITHM In this section, we will introduce our Clustering with Local and <br>global regularization</br> (CLGR) algorithm in detail.",
                "First lets see the how the documents are represented throughout this paper. 2.1 Document Representation In our work, all the documents are represented by the weighted term-frequency vectors.",
                "Let W = {w1, w2, · · · , wm} be the complete vocabulary set of the document corpus (which is preprocessed by the stopwords removal and words stemming operations).",
                "The term-frequency vector xi of document di is defined as xi = [xi1, xi2, · · · , xim]T , xik = tik log n idfk , where tik is the term frequency of wk ∈ W, n is the size of the document corpus, idfk is the number of documents that contain word wk.",
                "In this way, xi is also called the TFIDF representation of document di.",
                "Furthermore, we also normalize each xi (1 i n) to have a unit length, so that each document is represented by a normalized TF-IDF vector. 2.2 Local Regularization As its name suggests, CLGR is composed of two parts: local regularization and <br>global regularization</br>.",
                "In this subsection we will introduce the local regularization part in detail. 2.2.1 Motivation As we know that clustering is one type of learning techniques, it aims to organize the dataset in a reasonable way.",
                "Generally speaking, learning can be posed as a problem of function estimation, from which we can get a good classification function that will assign labels to the training dataset and even the unseen testing dataset with some cost minimized [24].",
                "For example, in the two-class classification scenario1 (in which we exactly know the label of each document), a linear classifier with least square fit aims to learn a column vector w such that the squared cost J = 1 n (wT xi − yi)2 (1) is minimized, where yi ∈ {+1, −1} is the label of xi.",
                "By taking ∂J /∂w = 0, we get the solution w∗ = n i=1 xixT i −1 n i=1 xiyi , (2) which can further be written in its matrix form as w∗ = XXT −1 Xy, (3) where X = [x1, x2, · · · , xn] is an m × n document matrix, y = [y1, y2, · · · , yn]T is the label vector.",
                "Then for a test document t, we can determine its label by l = sign(w∗T u), (4) where sign(·) is the sign function.",
                "A natural problem in Eq. (3) is that the matrix XXT may be singular and thus not invertable (e.g. when m n).",
                "To avoid such a problem, we can add a regularization term and minimize the following criterion J = 1 n n i=1 (wT xi − yi)2 + λ w 2 , (5) where λ is a regularization parameter.",
                "Then the optimal solution that minimize J is given by w∗ = XXT + λnI −1 Xy, (6) where I is an m × m identity matrix.",
                "It has been reported that the regularized linear classifier can achieve very good results on text classification problems [29].",
                "However, despite its empirical success, the regularized linear classifier is on earth a global classifier, i.e. w∗ is estimated using the whole training set.",
                "According to [24], this may not be a smart idea, since a unique w∗ may not be good enough for predicting the labels of the whole input space.",
                "In order to get better predictions, [6] proposed to train classifiers locally and use them to classify the testing points.",
                "For example, a testing point will be classified by the local classifier trained using the training points located in the vicinity 1 In the following discussions we all assume that the documents coming from only two classes.",
                "The generalizations of our method to multi-class cases will be discussed in section 2.5. of it.",
                "Although this method seems slow and stupid, it is reported that it can get better performances than using a unique global classifier on certain tasks [6]. 2.2.2 Constructing the Local Regularized Predictors Inspired by their success, we proposed to apply the local learning algorithms for clustering.",
                "The basic idea is that, for each document vector xi (1 i n), we train a local label predictor based on its k-nearest neighborhood Ni, and then use it to predict the label of xi.",
                "Finally we will combine all those local predictors by minimizing the sum of their prediction errors.",
                "In this subsection we will introduce how to construct those local predictors.",
                "Due to the simplicity and effectiveness of the regularized linear classifier that we have introduced in section 2.2.1, we choose it to be our local label predictor, such that for each document xi, the following criterion is minimized Ji = 1 ni xj ∈Ni wT i xj − qj 2 + λi wi 2 , (7) where ni = |Ni| is the cardinality of Ni, and qj is the cluster membership of xj.",
                "Then using Eq. (6), we can get the optimal solution is w∗ i = XiXT i + λiniI −1 Xiqi, (8) where Xi = [xi1, xi2, · · · , xini ], and we use xik to denote the k-th nearest neighbor of xi. qi = [qi1, qi2, · · · , qini ]T with qik representing the cluster assignment of xik.",
                "The problem here is that XiXT i is an m × m matrix with m ni, i.e. we should compute the inverse of an m × m matrix for every document vector, which is computationally prohibited.",
                "Fortunately, we have the following theorem: Theorem 1. w∗ i in Eq. (8) can be rewritten as w∗ i = Xi XT i Xi + λiniIi −1 qi, (9) where Ii is an ni × ni identity matrix.",
                "Proof.",
                "Since w∗ i = XiXT i + λiniI −1 Xiqi, then XiXT i + λiniI w∗ i = Xiqi =⇒ XiXT i w∗ i + λiniw∗ i = Xiqi =⇒ w∗ i = (λini)−1 Xi qi − XT i w∗ i .",
                "Let β = (λini)−1 qi − XT i w∗ i , then w∗ i = Xiβ =⇒ λiniβ = qi − XT i w∗ i = qi − XT i Xiβ =⇒ qi = XT i Xi + λiniIi β =⇒ β = XT i Xi + λiniIi −1 qi.",
                "Therefore w∗ i = Xiβ = Xi XT i Xi + λiniIi −1 qi 2 Using theorem 1, we only need to compute the inverse of an ni × ni matrix for every document to train a local label predictor.",
                "Moreover, for a new testing point u that falls into Ni, we can classify it by the sign of qu = w∗T i u = uT wi = uT Xi XT i Xi + λiniIi −1 qi.",
                "This is an attractive expression since we can determine the cluster assignment of u by using the inner-products between the points in {u ∪ Ni}, which suggests that such a local regularizer can easily be kernelized [21] as long as we define a proper kernel function. 2.2.3 Combining the Local Regularized Predictors After all the local predictors having been constructed, we will combine them together by minimizing Jl = n i=1 w∗T i xi − qi 2 , (10) which stands for the sum of the prediction errors for all the local predictors.",
                "Combining Eq. (10) with Eq. (6), we can get Jl = n i=1 w∗T i xi − qi 2 = n i=1 xT i Xi XT i Xi + λiniIi −1 qi − qi 2 = Pq − q 2 , (11) where q = [q1, q2, · · · , qn]T , and the P is an n × n matrix constructing in the following way.",
                "Let αi = xT i Xi XT i Xi + λiniIi −1 , then Pij = αi j, if xj ∈ Ni 0, otherwise , (12) where Pij is the (i, j)-th entry of P, and αi j represents the j-th entry of αi .",
                "Till now we can write the criterion of clustering by combining locally regularized linear label predictors Jl in an explicit mathematical form, and we can minimize it directly using some standard optimization techniques.",
                "However, the results may not be good enough since we only exploit the local informations of the dataset.",
                "In the next subsection, we will introduce a <br>global regularization</br> criterion and combine it with Jl, which aims to find a good clustering result in a local-global way. 2.3 <br>global regularization</br> In data clustering, we usually require that the cluster assignments of the data points should be sufficiently smooth with respect to the underlying data manifold, which implies (1) the nearby points tend to have the same cluster assignments; (2) the points on the same structure (e.g. submanifold or cluster) tend to have the same cluster assignments [31].",
                "Without the loss of generality, we assume that the data points reside (roughly) on a low-dimensional manifold M2 , and q is the cluster assignment function defined on M, i.e. 2 We believe that the text data are also sampled from some low dimensional manifold, since it is impossible for them to for ∀x ∈ M, q(x) returns the cluster membership of x.",
                "The smoothness of q over M can be calculated by the following Dirichlet integral [2] D[q] = 1 2 M q(x) 2 dM, (13) where the gradient q is a vector in the tangent space T Mx, and the integral is taken with respect to the standard measure on M. If we restrict the scale of q by q, q M = 1 (where ·, · M is the inner product induced on M), then it turns out that finding the smoothest function minimizing D[q] reduces to finding the eigenfunctions of the Laplace Beltrami operator L, which is defined as Lq −div q, (14) where div is the divergence of a vector field.",
                "Generally, the graph can be viewed as the discretized form of manifold.",
                "We can model the dataset as an weighted undirected graph as in spectral clustering [22], where the graph nodes are just the data points, and the weights on the edges represent the similarities between pairwise points.",
                "Then it can be shown that minimizing Eq. (13) corresponds to minimizing Jg = qT Lq = n i=1 (qi − qj)2 wij, (15) where q = [q1, q2, · · · , qn]T with qi = q(xi), L is the graph Laplacian with its (i, j)-th entry Lij =    di − wii, if i = j −wij, if xi and xj are adjacent 0, otherwise, (16) where di = j wij is the degree of xi, wij is the similarity between xi and xj.",
                "If xi and xj are adjacent3 , wij is usually computed in the following way wij = e − xi−xj 2 2σ2 , (17) where σ is a dataset dependent parameter.",
                "It is proved that under certain conditions, such a form of wij to determine the weights on graph edges leads to the convergence of graph Laplacian to the Laplace Beltrami operator [3][18].",
                "In summary, using Eq. (15) with exponential weights can effectively measure the smoothness of the data assignments with respect to the intrinsic data manifold.",
                "Thus we adopt it as a global regularizer to punish the smoothness of the predicted data assignments. 2.4 Clustering with Local and <br>global regularization</br> Combining the contents we have introduced in section 2.2 and section 2.3 we can derive the clustering criterion is minq J = Jl + λJg = Pq − q 2 + λqT Lq s.t. qi ∈ {−1, +1}, (18) where P is defined as in Eq. (12), and λ is a regularization parameter to trade off Jl and Jg.",
                "However, the discrete fill in the whole high-dimensional sample space.",
                "And it has been shown that the manifold based methods can achieve good results on text classification tasks [31]. 3 In this paper, we define xi and xj to be adjacent if xi ∈ N(xj) or xj ∈ N(xi). constraint of pi makes the problem an NP hard integer programming problem.",
                "A natural way for making the problem solvable is to remove the constraint and relax qi to be continuous, then the objective that we aims to minimize becomes J = Pq − q 2 + λqT Lq = qT (P − I)T (P − I)q + λqT Lq = qT (P − I)T (P − I) + λL q, (19) and we further add a constraint qT q = 1 to restrict the scale of q.",
                "Then our objective becomes minq J = qT (P − I)T (P − I) + λL q s.t. qT q = 1 (20) Using the Lagrangian method, we can derive that the optimal solution q corresponds to the smallest eigenvector of the matrix M = (P − I)T (P − I) + λL, and the cluster assignment of xi can be determined by the sign of qi, i.e. xi will be classified as class one if qi > 0, otherwise it will be classified as class 2. 2.5 Multi-Class CLGR In the above we have introduced the basic framework of Clustering with Local and <br>global regularization</br> (CLGR) for the two-class clustering problem, and we will extending it to multi-class clustering in this subsection.",
                "First we assume that all the documents belong to C classes indexed by L = {1, 2, · · · , C}. qc is the classification function for class c (1 c C), such that qc (xi) returns the confidence that xi belongs to class c. Our goal is to obtain the value of qc (xi) (1 c C, 1 i n), and the cluster assignment of xi can be determined by {qc (xi)}C c=1 using some proper discretization methods that we will introduce later.",
                "Therefore, in this multi-class case, for each document xi (1 i n), we will construct C locally linear regularized label predictors whose normal vectors are wc∗ i = Xi XT i Xi + λiniIi −1 qc i (1 c C), (21) where Xi = [xi1, xi2, · · · , xini ] with xik being the k-th neighbor of xi, and qc i = [qc i1, qc i2, · · · , qc ini ]T with qc ik = qc (xik).",
                "Then (wc∗ i )T xi returns the predicted confidence of xi belonging to class c. Hence the local prediction error for class c can be defined as J c l = n i=1 (wc∗ i ) T xi − qc i 2 , (22) And the total local prediction error becomes Jl = C c=1 J c l = C c=1 n i=1 (wc∗ i ) T xi − qc i 2 . (23) As in Eq. (11), we can define an n×n matrix P (see Eq. (12)) and rewrite Jl as Jl = C c=1 J c l = C c=1 Pqc − qc 2 . (24) Similarly we can define the global smoothness regularizer in multi-class case as Jg = C c=1 n i=1 (qc i − qc j )2 wij = C c=1 (qc )T Lqc . (25) Then the criterion to be minimized for CLGR in multi-class case becomes J = Jl + λJg = C c=1 Pqc − qc 2 + λ(qc )T Lqc = C c=1 (qc )T (P − I)T (P − I) + λL qc = trace QT (P − I)T (P − I) + λL Q , (26) where Q = [q1 , q2 , · · · , qc ] is an n × c matrix, and trace(·) returns the trace of a matrix.",
                "The same as in Eq. (20), we also add the constraint that QT Q = I to restrict the scale of Q.",
                "Then our optimization problem becomes minQ J = trace QT (P − I)T (P − I) + λL Q s.t.",
                "QT Q = I, (27) From the Ky Fan theorem [28], we know the optimal solution of the above problem is Q∗ = [q∗ 1, q∗ 2, · · · , q∗ C ]R, (28) where q∗ k (1 k C) is the eigenvector corresponds to the k-th smallest eigenvalue of matrix (P − I)T (P − I) + λL, and R is an arbitrary C × C matrix.",
                "Since the values of the entries in Q∗ is continuous, we need to further discretize Q∗ to get the cluster assignments of all the data points.",
                "There are mainly two approaches to achieve this goal: 1.",
                "As in [20], we can treat the i-th row of Q as the embedding of xi in a C-dimensional space, and apply some traditional clustering methods like kmeans to clustering these embeddings into C clusters. 2.",
                "Since the optimal Q∗ is not unique (because of the existence of an arbitrary matrix R), we can pursue an optimal R that will rotate Q∗ to an indication matrix4 .",
                "The detailed algorithm can be referred to [26].",
                "The detailed algorithm procedure for CLGR is summarized in table 1. 3.",
                "EXPERIMENTS In this section, experiments are conducted to empirically compare the clustering results of CLGR with other 8 representitive document clustering algorithms on 5 datasets.",
                "First we will introduce the basic informations of those datasets. 3.1 Datasets We use a variety of datasets, most of which are frequently used in the information retrieval research.",
                "Table 2 summarizes the characteristics of the datasets. 4 Here an indication matrix T is a n×c matrix with its (i, j)th entry Tij ∈ {0, 1} such that for each row of Q∗ there is only one 1.",
                "Then the xi can be assigned to the j-th cluster such that j = argjQ∗ ij = 1.",
                "Table 1: Clustering with Local and <br>global regularization</br> (CLGR) Input: 1.",
                "Dataset X = {xi}n i=1; 2.",
                "Number of clusters C; 3.",
                "Size of the neighborhood K; 4.",
                "Local regularization parameters {λi}n i=1; 5.",
                "<br>global regularization</br> parameter λ; Output: The cluster membership of each data point.",
                "Procedure: 1.",
                "Construct the K nearest neighborhoods for each data point; 2.",
                "Construct the matrix P using Eq. (12); 3.",
                "Construct the Laplacian matrix L using Eq. (16); 4.",
                "Construct the matrix M = (P − I)T (P − I) + λL; 5.",
                "Do eigenvalue decomposition on M, and construct the matrix Q∗ according to Eq. (28); 6.",
                "Output the cluster assignments of each data point by properly discretize Q∗ .",
                "Table 2: Descriptions of the document datasets Datasets Number of documents Number of classes CSTR 476 4 WebKB4 4199 4 Reuters 2900 10 WebACE 2340 20 Newsgroup4 3970 4 CSTR.",
                "This is the dataset of the abstracts of technical reports published in the Department of Computer Science at a university.",
                "The dataset contained 476 abstracts, which were divided into four research areas: Natural Language Processing(NLP), Robotics/Vision, Systems, and Theory.",
                "WebKB.",
                "The WebKB dataset contains webpages gathered from university computer science departments.",
                "There are about 8280 documents and they are divided into 7 categories: student, faculty, staff, course, project, department and other.",
                "The raw text is about 27MB.",
                "Among these 7 categories, student, faculty, course and project are four most populous entity-representing categories.",
                "The associated subset is typically called WebKB4.",
                "Reuters.",
                "The Reuters-21578 Text Categorization Test collection contains documents collected from the Reuters newswire in 1987.",
                "It is a standard text categorization benchmark and contains 135 categories.",
                "In our experiments, we use a subset of the data collection which includes the 10 most frequent categories among the 135 topics and we call it Reuters-top 10.",
                "WebACE.",
                "The WebACE dataset was from WebACE project and has been used for document clustering [17][5].",
                "The WebACE dataset contains 2340 documents consisting news articles from Reuters new service via the Web in October 1997.",
                "These documents are divided into 20 classes.",
                "News4.",
                "The News4 dataset used in our experiments are selected from the famous 20-newsgroups dataset5 .",
                "The topic rec containing autos, motorcycles, baseball and hockey was selected from the version 20news-18828.",
                "The News4 dataset contains 3970 document vectors. 5 http://people.csail.mit.edu/jrennie/20Newsgroups/ To pre-process the datasets, we remove the stop words using a standard stop list, all HTML tags are skipped and all header fields except subject and organization of the posted articles are ignored.",
                "In all our experiments, we first select the top 1000 words by mutual information with class labels. 3.2 Evaluation Metrics In the experiments, we set the number of clusters equal to the true number of classes C for all the clustering algorithms.",
                "To evaluate their performance, we compare the clusters generated by these algorithms with the true classes by computing the following two performance measures.",
                "Clustering Accuracy (Acc).",
                "The first performance measure is the Clustering Accuracy, which discovers the one-toone relationship between clusters and classes and measures the extent to which each cluster contained data points from the corresponding class.",
                "It sums up the whole matching degree between all pair class-clusters.",
                "Clustering accuracy can be computed as: Acc = 1 N max   Ck,Lm T(Ck, Lm)   , (29) where Ck denotes the k-th cluster in the final results, and Lm is the true m-th class.",
                "T(Ck, Lm) is the number of entities which belong to class m are assigned to cluster k. Accuracy computes the maximum sum of T(Ck, Lm) for all pairs of clusters and classes, and these pairs have no overlaps.",
                "The greater clustering accuracy means the better clustering performance.",
                "Normalized Mutual Information (NMI).",
                "Another evaluation metric we adopt here is the Normalized Mutual Information NMI [23], which is widely used for determining the quality of clusters.",
                "For two random variable X and Y, the NMI is defined as: NMI(X, Y) = I(X, Y) H(X)H(Y) , (30) where I(X, Y) is the mutual information between X and Y, while H(X) and H(Y) are the entropies of X and Y respectively.",
                "One can see that NMI(X, X) = 1, which is the maximal possible value of NMI.",
                "Given a clustering result, the NMI in Eq. (30) is estimated as NMI = C k=1 C m=1 nk,mlog n·nk,m nk ˆnm C k=1 nklog nk n C m=1 ˆnmlog ˆnm n , (31) where nk denotes the number of data contained in the cluster Ck (1 k C), ˆnm is the number of data belonging to the m-th class (1 m C), and nk,m denotes the number of data that are in the intersection between the cluster Ck and the m-th class.",
                "The value calculated in Eq. (31) is used as a performance measure for the given clustering result.",
                "The larger this value, the better the clustering performance. 3.3 Comparisons We have conducted comprehensive performance evaluations by testing our method and comparing it with 8 other representative data clustering methods using the same data corpora.",
                "The algorithms that we evaluated are listed below. 1.",
                "Traditional k-means (KM). 2.",
                "Spherical k-means (SKM).",
                "The implementation is based on [9]. 3.",
                "Gaussian Mixture Model (GMM).",
                "The implementation is based on [16]. 4.",
                "Spectral Clustering with Normalized Cuts (Ncut).",
                "The implementation is based on [26], and the variance of the Gaussian similarity is determined by Local Scaling [30].",
                "Note that the criterion that Ncut aims to minimize is just the global regularizer in our CLGR algorithm except that Ncut used the normalized Laplacian. 5.",
                "Clustering using Pure Local Regularization (CPLR).",
                "In this method we just minimize Jl (defined in Eq. (24)), and the clustering results can be obtained by doing eigenvalue decomposition on matrix (I − P)T (I − P) with some proper discretization methods. 6.",
                "Adaptive Subspace Iteration (ASI).",
                "The implementation is based on [14]. 7.",
                "Nonnegative Matrix Factorization (NMF).",
                "The implementation is based on [27]. 8.",
                "Tri-Factorization Nonnegative Matrix Factorization (TNMF) [12].",
                "The implementation is based on [15].",
                "For computational efficiency, in the implementation of CPLR and our CLGR algorithm, we have set all the local regularization parameters {λi}n i=1 to be identical, which is set by grid search from {0.1, 1, 10}.",
                "The size of the k-nearest neighborhoods is set by grid search from {20, 40, 80}.",
                "For the CLGR method, its <br>global regularization</br> parameter is set by grid search from {0.1, 1, 10}.",
                "When constructing the global regularizer, we have adopted the local scaling method [30] to construct the Laplacian matrix.",
                "The final discretization method adopted in these two methods is the same as in [26], since our experiments show that using such method can achieve better results than using kmeans based methods as in [20]. 3.4 Experimental Results The clustering accuracies comparison results are shown in table 3, and the normalized mutual information comparison results are summarized in table 4.",
                "From the two tables we mainly observe that: 1.",
                "Our CLGR method outperforms all other document clustering methods in most of the datasets; 2.",
                "For document clustering, the Spherical k-means method usually outperforms the traditional k-means clustering method, and the GMM method can achieve competitive results compared to the Spherical k-means method; 3.",
                "The results achieved from the k-means and GMM type algorithms are usually worse than the results achieved from Spectral Clustering.",
                "Since Spectral Clustering can be viewed as a weighted version of kernel k-means, it can obtain good results the data clusters are arbitrarily shaped.",
                "This corroborates that the documents vectors are not regularly distributed (spherical or elliptical). 4.",
                "The experimental comparisons empirically verify the equivalence between NMF and Spectral Clustering, which Table 3: Clustering accuracies of the various methods CSTR WebKB4 Reuters WebACE News4 KM 0.4256 0.3888 0.4448 0.4001 0.3527 SKM 0.4690 0.4318 0.5025 0.4458 0.3912 GMM 0.4487 0.4271 0.4897 0.4521 0.3844 NMF 0.5713 0.4418 0.4947 0.4761 0.4213 Ncut 0.5435 0.4521 0.4896 0.4513 0.4189 ASI 0.5621 0.4752 0.5235 0.4823 0.4335 TNMF 0.6040 0.4832 0.5541 0.5102 0.4613 CPLR 0.5974 0.5020 0.4832 0.5213 0.4890 CLGR 0.6235 0.5228 0.5341 0.5376 0.5102 Table 4: Normalized mutual information results of the various methods CSTR WebKB4 Reuters WebACE News4 KM 0.3675 0.3023 0.4012 0.3864 0.3318 SKM 0.4027 0.4155 0.4587 0.4003 0.4085 GMM 0.4034 0.4093 0.4356 0.4209 0.3994 NMF 0.5235 0.4517 0.4402 0.4359 0.4130 Ncut 0.4833 0.4497 0.4392 0.4289 0.4231 ASI 0.5008 0.4833 0.4769 0.4817 0.4503 TNMF 0.5724 0.5011 0.5132 0.5328 0.4749 CPLR 0.5695 0.5231 0.4402 0.5543 0.4690 CLGR 0.6012 0.5434 0.4935 0.5390 0.4908 has been proved theoretically in [10].",
                "It can be observed from the tables that NMF and Spectral Clustering usually lead to similar clustering results. 5.",
                "The co-clustering based methods (TNMF and ASI) can usually achieve better results than traditional purely document vector based methods.",
                "Since these methods perform an implicit feature selection at each iteration, provide an adaptive metric for measuring the neighborhood, and thus tend to yield better clustering results. 6.",
                "The results achieved from CPLR are usually better than the results achieved from Spectral Clustering, which supports Vapniks theory [24] that sometimes local learning algorithms can obtain better results than global learning algorithms.",
                "Besides the above comparison experiments, we also test the parameter sensibility of our method.",
                "There are mainly two sets of parameters in our CLGR algorithm, the local and <br>global regularization</br> parameters ({λi}n i=1 and λ, as we have said in section 3.3, we have set all λis to be identical to λ∗ in our experiments), and the size of the neighborhoods.",
                "Therefore we have also done two sets of experiments: 1.",
                "Fixing the size of the neighborhoods, and testing the clustering performance with varying λ∗ and λ.",
                "In this set of experiments, we find that our CLGR algorithm can achieve good results when the two regularization parameters are neither too large nor too small.",
                "Typically our method can achieve good results when λ∗ and λ are around 0.1.",
                "Figure 1 shows us such a testing example on the WebACE dataset. 2.",
                "Fixing the local and <br>global regularization</br> parameters, and testing the clustering performance with different −5 −4.5 −4 −3.5 −3 −5 −4.5 −4 −3.5 −3 0.35 0.4 0.45 0.5 0.55 local regularization para (log 2 value) <br>global regularization</br> para (log 2 value) clusteringaccuracy Figure 1: Parameter sensibility testing results on the WebACE dataset with the neighborhood size fixed to 20, and the x-axis and y-axis represents the log2 value of λ∗ and λ. sizes of neighborhoods.",
                "In this set of experiments, we find that the neighborhood with a too large or too small size will all deteriorate the final clustering results.",
                "This can be easily understood since when the neighborhood size is very small, then the data points used for training the local classifiers may not be sufficient; when the neighborhood size is very large, the trained classifiers will tend to be global and cannot capture the typical local characteristics.",
                "Figure 2 shows us a testing example on the WebACE dataset.",
                "Therefore, we can see that our CLGR algorithm (1) can achieve satisfactory results and (2) is not very sensitive to the choice of parameters, which makes it practical in real world applications. 4.",
                "CONCLUSIONS AND FUTURE WORKS In this paper, we derived a new clustering algorithm called clustering with local and <br>global regularization</br>.",
                "Our method preserves the merit of local learning algorithms and spectral clustering.",
                "Our experiments show that the proposed algorithm outperforms most of the state of the art algorithms on many benchmark datasets.",
                "In the future, we will focus on the parameter selection and acceleration issues of the CLGR algorithm. 5.",
                "REFERENCES [1] L. Baker and A. McCallum.",
                "Distributional Clustering of Words for Text Classification.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 1998. [2] M. Belkin and P. Niyogi.",
                "Laplacian Eigenmaps for Dimensionality Reduction and Data Representation.",
                "Neural Computation, 15 (6):1373-1396.",
                "June 2003. [3] M. Belkin and P. Niyogi.",
                "Towards a Theoretical Foundation for Laplacian-Based Manifold Methods.",
                "In Proceedings of the 18th Conference on Learning Theory (COLT). 2005. 10 20 30 40 50 60 70 80 90 100 0.35 0.4 0.45 0.5 0.55 size of the neighborhood clusteringaccuracy Figure 2: Parameter sensibility testing results on the WebACE dataset with the regularization parameters being fixed to 0.1, and the neighborhood size varing from 10 to 100. [4] M. Belkin, P. Niyogi and V. Sindhwani.",
                "Manifold Regularization: a Geometric Framework for Learning from Examples.",
                "Journal of Machine Learning Research 7, 1-48, 2006. [5] D. Boley.",
                "Principal Direction Divisive Partitioning.",
                "Data mining and knowledge discovery, 2:325-344, 1998. [6] L. Bottou and V. Vapnik.",
                "Local learning algorithms.",
                "Neural Computation, 4:888-900, 1992. [7] P. K. Chan, D. F. Schlag and J. Y. Zien.",
                "Spectral K-way Ratio-Cut Partitioning and Clustering.",
                "IEEE Trans.",
                "Computer-Aided Design, 13:1088-1096, Sep. 1994. [8] D. R. Cutting, D. R. Karger, J. O. Pederson and J. W. Tukey.",
                "Scatter/Gather: A Cluster-Based Approach to Browsing Large Document Collections.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 1992. [9] I. S. Dhillon and D. S. Modha.",
                "Concept Decompositions for Large Sparse Text Data using Clustering.",
                "Machine Learning, vol. 42(1), pages 143-175, January 2001. [10] C. Ding, X.",
                "He, and H. Simon.",
                "On the equivalence of nonnegative matrix factorization and spectral clustering.",
                "In Proceedings of the SIAM Data Mining Conference, 2005. [11] C. Ding, X.",
                "He, H. Zha, M. Gu, and H. D. Simon.",
                "A min-max cut algorithm for graph partitioning and data clustering.",
                "In Proc. of the 1st International Conference on Data Mining (ICDM), pages 107-114, 2001. [12] C. Ding, T. Li, W. Peng, and H. Park.",
                "Orthogonal Nonnegative Matrix Tri-Factorizations for Clustering.",
                "In Proceedings of the Twelfth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2006. [13] R. O. Duda, P. E. Hart, and D. G. Stork.",
                "Pattern Classification.",
                "John Wiley & Sons, Inc., 2001. [14] T. Li, S. Ma, and M. Ogihara.",
                "Document Clustering via Adaptive Subspace Iteration.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2004. [15] T. Li and C. Ding.",
                "The Relationships Among Various Nonnegative Matrix Factorization Methods for Clustering.",
                "In Proceedings of the 6th International Conference on Data Mining (ICDM). 2006. [16] X. Liu and Y. Gong.",
                "Document Clustering with Cluster Refinement and Model Selection Capabilities.",
                "In Proc. of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002. [17] E. Han, D. Boley, M. Gini, R. Gross, K. Hastings, G. Karypis, V. Kumar, B. Mobasher, and J. Moore.",
                "WebACE: A Web Agent for Document Categorization and Exploration.",
                "In Proceedings of the 2nd International Conference on Autonomous Agents (Agents98).",
                "ACM Press, 1998. [18] M. Hein, J. Y. Audibert, and U. von Luxburg.",
                "From Graphs to Manifolds - Weak and Strong Pointwise Consistency of Graph Laplacians.",
                "In Proceedings of the 18th Conference on Learning Theory (COLT), 470-485. 2005. [19] J.",
                "He, M. Lan, C.-L. Tan, S.-Y.",
                "Sung, and H.-B.",
                "Low.",
                "Initialization of Cluster Refinement Algorithms: A Review and Comparative Study.",
                "In Proc. of Inter.",
                "Joint Conference on Neural Networks, 2004. [20] A. Y. Ng, M. I. Jordan, Y. Weiss.",
                "On Spectral Clustering: Analysis and an algorithm.",
                "In Advances in Neural Information Processing Systems 14. 2002. [21] B. Sch¨olkopf and A. Smola.",
                "Learning with Kernels.",
                "The MIT Press.",
                "Cambridge, Massachusetts. 2002. [22] J. Shi and J. Malik.",
                "Normalized Cuts and Image Segmentation.",
                "IEEE Trans. on Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000. [23] A. Strehl and J. Ghosh.",
                "Cluster Ensembles - A Knowledge Reuse Framework for Combining Multiple Partitions.",
                "Journal of Machine Learning Research, 3:583-617, 2002. [24] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Berlin: Springer-Verlag, 1995. [25] Wu, M. and Sch¨olkopf, B.",
                "A Local Learning Approach for Clustering.",
                "In Advances in Neural Information Processing Systems 18. 2006. [26] S. X. Yu, J. Shi.",
                "Multiclass Spectral Clustering.",
                "In Proceedings of the International Conference on Computer Vision, 2003. [27] W. Xu, X. Liu and Y. Gong.",
                "Document Clustering Based On Non-Negative Matrix Factorization.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2003. [28] H. Zha, X.",
                "He, C. Ding, M. Gu and H. Simon.",
                "Spectral Relaxation for K-means Clustering.",
                "In NIPS 14. 2001. [29] T. Zhang and F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Journal of Information Retrieval, 4:5-31, 2001. [30] L. Zelnik-Manor and P. Perona.",
                "Self-Tuning Spectral Clustering.",
                "In NIPS 17. 2005. [31] D. Zhou, O. Bousquet, T. N. Lal, J. Weston and B. Sch¨olkopf.",
                "Learning with Local and Global Consistency.",
                "NIPS 17, 2005."
            ],
            "original_annotated_samples": [
                "So we call our algorithm Clustering with Local and <br>global regularization</br> (CLGR).",
                "So we call our method Clustering with Local and <br>global regularization</br> (CLGR).",
                "THE PROPOSED ALGORITHM In this section, we will introduce our Clustering with Local and <br>global regularization</br> (CLGR) algorithm in detail.",
                "Furthermore, we also normalize each xi (1 i n) to have a unit length, so that each document is represented by a normalized TF-IDF vector. 2.2 Local Regularization As its name suggests, CLGR is composed of two parts: local regularization and <br>global regularization</br>.",
                "In the next subsection, we will introduce a <br>global regularization</br> criterion and combine it with Jl, which aims to find a good clustering result in a local-global way. 2.3 <br>global regularization</br> In data clustering, we usually require that the cluster assignments of the data points should be sufficiently smooth with respect to the underlying data manifold, which implies (1) the nearby points tend to have the same cluster assignments; (2) the points on the same structure (e.g. submanifold or cluster) tend to have the same cluster assignments [31]."
            ],
            "translated_annotated_samples": [
                "Así que llamamos a nuestro algoritmo Agrupamiento con <br>Regularización Local y Global</br> (CLGR).",
                "Así que llamamos a nuestro método Agrupamiento con Regularización Local y Global (CLGR).",
                "El ALGORITMO PROPUESTO En esta sección, presentaremos en detalle nuestro algoritmo de Agrupamiento con Regularización Local y Global (CLGR).",
                "Además, también normalizamos cada xi (1 i n) para que tenga una longitud unitaria, de modo que cada documento esté representado por un vector TF-IDF normalizado. 2.2 Regularización Local Como su nombre sugiere, CLGR está compuesto por dos partes: regularización local y <br>regularización global</br>.",
                "En la siguiente subsección, introduciremos un criterio de <br>regularización global</br> y lo combinaremos con Jl, que tiene como objetivo encontrar un buen resultado de agrupamiento de manera local-global. 2.3 Regularización Global En el agrupamiento de datos, generalmente requerimos que las asignaciones de clúster de los puntos de datos sean lo suficientemente suaves con respecto a la variedad de datos subyacente, lo que implica (1) que los puntos cercanos tienden a tener las mismas asignaciones de clúster; (2) que los puntos en la misma estructura (por ejemplo, subvariedad o clúster) tienden a tener las mismas asignaciones de clúster [31]."
            ],
            "translated_text": "Agrupamiento regularizado para documentos ∗ Fei Wang, Changshui Zhang Laboratorio Clave del Estado de Tecnología Inteligente. En los últimos años, la agrupación de documentos ha estado recibiendo cada vez más atención como una técnica importante y fundamental para la organización no supervisada de documentos, la extracción automática de temas y la recuperación o filtrado rápido de información. En este artículo, proponemos un método novedoso para agrupar documentos utilizando regularización. A diferencia de los métodos de agrupamiento regularizados globalmente tradicionales, nuestro método primero construye un predictor de etiquetas lineal regularizado local para cada vector de documento, y luego combina todos esos regularizadores locales con un regularizador de suavidad global. Así que llamamos a nuestro algoritmo Agrupamiento con <br>Regularización Local y Global</br> (CLGR). Mostraremos que las pertenencias de los documentos a los grupos se pueden lograr mediante la descomposición de los valores propios de una matriz simétrica dispersa, la cual puede resolverse eficientemente mediante métodos iterativos. Finalmente, se presentan nuestras evaluaciones experimentales en varios conjuntos de datos para mostrar las ventajas de CLGR sobre los métodos tradicionales de agrupamiento de documentos. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información-Agrupamiento; I.2.6 [Inteligencia Artificial]: Aprendizaje-Aprendizaje de Conceptos Términos Generales Algoritmos 1. La agrupación de documentos ha estado recibiendo cada vez más atención como una técnica importante y fundamental para la organización no supervisada de documentos, la extracción automática de temas y la recuperación o filtrado rápido de información. Un buen enfoque de agrupación de documentos puede ayudar a las computadoras a organizar automáticamente el corpus de documentos en una jerarquía de clusters significativa para una navegación eficiente, lo cual es muy valioso para complementar las deficiencias de las tecnologías tradicionales de recuperación de información. Como señaló [8], las necesidades de recuperación de información pueden expresarse mediante un espectro que va desde la búsqueda basada en palabras clave estrechas hasta la exploración de información amplia, como cuáles son los principales eventos internacionales de los últimos meses. Los motores de búsqueda de documentos tradicionales tienden a adaptarse bien al extremo de la búsqueda, es decir, suelen proporcionar búsquedas específicas de documentos que coinciden con la consulta de los usuarios, sin embargo, les resulta difícil satisfacer las necesidades del resto del espectro en el que se necesita información más amplia o vaga. En tales casos, la navegación eficiente a través de una buena jerarquía de clústeres será definitivamente útil. Generalmente, los métodos de agrupamiento de documentos se pueden clasificar principalmente en dos clases: métodos jerárquicos y métodos de partición. Los métodos jerárquicos agrupan los puntos de datos en una estructura de árbol jerárquico utilizando enfoques de abajo hacia arriba o de arriba hacia abajo. Por ejemplo, el clustering jerárquico aglomerativo (HAC) [13] es un método típico de clustering jerárquico ascendente. Toma cada punto de datos como un único clúster para comenzar y luego construye clústeres más grandes agrupando puntos de datos similares juntos hasta que todo el conjunto de datos esté encapsulado en un único clúster final. Por otro lado, los métodos de particionamiento descomponen el conjunto de datos en un número de grupos disjuntos que suelen ser óptimos en términos de algunas funciones de criterio predefinidas. Por ejemplo, K-means es un método de particionamiento típico que tiene como objetivo minimizar la suma de las distancias al cuadrado entre los puntos de datos y sus centros de clúster correspondientes. En este documento, nos centraremos en los métodos de particionamiento. Como sabemos, existen dos problemas principales en los métodos de particionamiento (como Kmeans y el Modelo de Mezcla Gaussiana (GMM) [16]): (1) el criterio predefinido suele ser no convexo, lo que provoca muchas soluciones óptimas locales; (2) el procedimiento iterativo (por ejemplo, el algoritmo de Expectation Maximization (EM)) para optimizar los criterios suele hacer que las soluciones finales dependan en gran medida de las inicializaciones. En las últimas décadas, se han propuesto muchos métodos para superar los problemas mencionados de los métodos de particionamiento [19][28]. Recientemente, otro tipo de métodos de particionamiento basados en agrupamiento en grafos de datos han despertado un considerable interés en la comunidad de aprendizaje automático y minería de datos. La idea básica detrás de estos métodos es modelar primero el conjunto de datos completo como un grafo ponderado, en el que los nodos del grafo representan los puntos de datos y los pesos en los bordes corresponden a las similitudes entre los puntos en pares. Entonces, las asignaciones de clústeres del conjunto de datos se pueden lograr optimizando algunos criterios definidos en el gráfico. Por ejemplo, el Clustering Espectral es uno de los enfoques de clustering basados en grafos más representativos, generalmente tiene como objetivo optimizar algún valor de corte (por ejemplo, Corte normalizado [22], corte de razón [7], corte mínimo-máximo [11]) definidos en un grafo no dirigido. Después de algunas relajaciones, estos criterios generalmente pueden ser optimizados a través de descomposiciones propias, lo cual está garantizado que sea óptimo a nivel global. De esta manera, el agrupamiento espectral evita eficientemente los problemas de los métodos de particionamiento tradicionales que presentamos en el párrafo anterior. En este artículo, proponemos un algoritmo novedoso de agrupamiento de documentos que hereda la superioridad del agrupamiento espectral, es decir, los resultados finales de los grupos también pueden obtenerse explotando la estructura de los autovalores de una matriz simétrica. Sin embargo, a diferencia del agrupamiento espectral, que simplemente impone una restricción de suavidad en las etiquetas de los datos sobre todo el conjunto de datos [2], nuestro método primero construye un predictor de etiquetas lineal regularizado para cada punto de datos a partir de su vecindario como en [25], y luego combina los resultados de todos estos predictores de etiquetas locales con un regularizador de suavidad de etiquetas global. Así que llamamos a nuestro método Agrupamiento con Regularización Local y Global (CLGR). La idea de incorporar tanto información local como global en la predicción de etiquetas está inspirada en los trabajos recientes sobre aprendizaje semi-supervisado [31], y nuestras evaluaciones experimentales en varios conjuntos de datos reales de documentos muestran que CLGR funciona mejor que muchos métodos de agrupamiento de vanguardia. El resto de este documento está organizado de la siguiente manera: en la sección 2 presentaremos nuestro algoritmo CLGR en detalle. Los resultados experimentales en varios conjuntos de datos se presentan en la sección 3, seguidos de las conclusiones y discusiones en la sección 4. El ALGORITMO PROPUESTO En esta sección, presentaremos en detalle nuestro algoritmo de Agrupamiento con Regularización Local y Global (CLGR). Primero veamos cómo se representan los documentos a lo largo de este documento. 2.1 Representación de documentos En nuestro trabajo, todos los documentos se representan mediante vectores de frecuencia de términos ponderados. Sea W = {w1, w2, · · · , wm} el conjunto completo de vocabulario del corpus de documentos (que ha sido preprocesado mediante la eliminación de palabras vacías y operaciones de derivación de palabras). El vector de frecuencia de término xi del documento di se define como xi = [xi1, xi2, · · · , xim]T , xik = tik log n idfk , donde tik es la frecuencia del término wk ∈ W, n es el tamaño del corpus de documentos, idfk es el número de documentos que contienen la palabra wk. De esta manera, xi también se llama la representación TFIDF del documento di. Además, también normalizamos cada xi (1 i n) para que tenga una longitud unitaria, de modo que cada documento esté representado por un vector TF-IDF normalizado. 2.2 Regularización Local Como su nombre sugiere, CLGR está compuesto por dos partes: regularización local y <br>regularización global</br>. En esta subsección introduciremos detalladamente la parte de regularización local. 2.2.1 Motivación Como sabemos que el agrupamiento es un tipo de técnica de aprendizaje, tiene como objetivo organizar el conjunto de datos de una manera razonable. En general, el aprendizaje puede plantearse como un problema de estimación de funciones, a partir del cual podemos obtener una buena función de clasificación que asignará etiquetas al conjunto de datos de entrenamiento e incluso al conjunto de datos de prueba no vistos con algún costo minimizado [24]. Por ejemplo, en el escenario de clasificación de dos clases (en el que conocemos exactamente la etiqueta de cada documento), un clasificador lineal con ajuste de mínimos cuadrados tiene como objetivo aprender un vector columna w de manera que el costo al cuadrado J = 1 n (wT xi − yi)2 (1) se minimice, donde yi ∈ {+1, −1} es la etiqueta de xi. Al tomar ∂J /∂w = 0, obtenemos la solución w∗ = Σ i=1 n xixT i −1 Σ i=1 n xiyi, (2) que puede escribirse en su forma matricial como w∗ = XXT −1 Xy, (3) donde X = [x1, x2, · · · , xn] es una matriz de documentos m × n, y = [y1, y2, · · · , yn]T es el vector de etiquetas. Entonces, para un documento de prueba t, podemos determinar su etiqueta mediante l = signo(w∗T u), donde signo(·) es la función signo. Un problema natural en la ecuación (3) es que la matriz XXT puede ser singular y, por lo tanto, no invertible (por ejemplo, cuando m n). Para evitar tal problema, podemos agregar un término de regularización y minimizar el siguiente criterio J = 1 n n i=1 (wT xi − yi)2 + λ w 2, (5), donde λ es un parámetro de regularización. Entonces, la solución óptima que minimiza J está dada por w∗ = XXT + λnI −1 Xy, (6) donde I es una matriz identidad de tamaño m × m. Se ha informado que el clasificador lineal regularizado puede lograr resultados muy buenos en problemas de clasificación de texto [29]. Sin embargo, a pesar de su éxito empírico, el clasificador lineal regularizado es en realidad un clasificador global, es decir, w∗ se estima utilizando todo el conjunto de entrenamiento. Según [24], esta puede que no sea una idea inteligente, ya que un único w∗ puede que no sea lo suficientemente bueno para predecir las etiquetas de todo el espacio de entrada. Para obtener predicciones más precisas, [6] propuso entrenar clasificadores localmente y utilizarlos para clasificar los puntos de prueba. Por ejemplo, un punto de prueba será clasificado por el clasificador local entrenado utilizando los puntos de entrenamiento ubicados en la vecindad 1. En las siguientes discusiones todos asumimos que los documentos provienen solo de dos clases. Las generalizaciones de nuestro método a casos de múltiples clases se discutirán en la sección 2.5 de él. Aunque este método parece lento y estúpido, se informa que puede obtener mejores rendimientos que usar un clasificador global único en ciertas tareas [6]. 2.2.2 Construyendo los Predictores Localmente Regularizados Inspirados en su éxito, propusimos aplicar los algoritmos de aprendizaje local para el agrupamiento. La idea básica es que, para cada vector de documento xi (1 i n), entrenamos un predictor de etiquetas local basado en su vecindario k-más cercano Ni, y luego lo usamos para predecir la etiqueta de xi. Finalmente combinaremos todos esos predictores locales minimizando la suma de sus errores de predicción. En esta subsección introduciremos cómo construir esos predictores locales. Debido a la simplicidad y efectividad del clasificador lineal regularizado que hemos introducido en la sección 2.2.1, lo elegimos como nuestro predictor de etiquetas local, de modo que para cada documento xi, se minimiza el siguiente criterio Ji = 1 ni xj ∈Ni wT i xj − qj 2 + λi wi 2, (7), donde ni = |Ni| es la cardinalidad de Ni, y qj es la pertenencia al clúster de xj. Luego, utilizando la Ecuación (6), podemos obtener que la solución óptima es w∗ i = XiXT i + λiniI −1 Xiqi, (8), donde Xi = [xi1, xi2, · · · , xini], y usamos xik para denotar al k-ésimo vecino más cercano de xi. qi = [qi1, qi2, · · · , qini]T con qik representando la asignación de clúster de xik. El problema aquí es que XiXT i es una matriz m × m con m ni, es decir, deberíamos calcular la inversa de una matriz m × m para cada vector de documento, lo cual está prohibido computacionalmente. Afortunadamente, tenemos el siguiente teorema: Teorema 1. w∗ i en la Ec. (8) puede ser reescrito como w∗ i = Xi XT i Xi + λiniIi −1 qi, (9), donde Ii es una matriz identidad de tamaño ni × ni. Prueba. Dado que w∗ i = XiXT i + λiniI −1 Xiqi, entonces XiXT i + λiniI w∗ i = Xiqi =⇒ XiXT i w∗ i + λiniw∗ i = Xiqi =⇒ w∗ i = (λini)−1 Xi qi − XT i w∗ i. Sea β = (λini)−1 qi − XT i w∗ i, entonces w∗ i = Xiβ =⇒ λiniβ = qi − XT i w∗ i = qi − XT i Xiβ =⇒ qi = XT i Xi + λiniIi β =⇒ β = XT i Xi + λiniIi −1 qi. Por lo tanto, w∗ i = Xiβ = Xi XT i Xi + λiniIi −1 qi 2. Utilizando el teorema 1, solo necesitamos calcular la inversa de una matriz ni × ni para cada documento para entrenar un predictor de etiquetas local. Además, para un nuevo punto de prueba u que cae en Ni, podemos clasificarlo por el signo de qu = w∗T i u = uT wi = uT Xi XT i Xi + λiniIi −1 qi. Esta es una expresión atractiva ya que podemos determinar la asignación del clúster de u utilizando los productos internos entre los puntos en {u ∪ Ni}, lo que sugiere que un regularizador local de este tipo puede ser fácilmente kernelizado [21] siempre y cuando definamos una función de kernel adecuada. 2.2.3 Combinando los Predictores Regularizados Localmente Una vez que todos los predictores locales hayan sido construidos, los combinaremos minimizando Jl = n i=1 w∗T i xi − qi 2, (10), lo que representa la suma de los errores de predicción de todos los predictores locales. Combinando la Ecuación (10) con la Ecuación (6), podemos obtener Jl = n i=1 w∗T i xi − qi 2 = n i=1 xT i Xi XT i Xi + λiniIi −1 qi − qi 2 = Pq − q 2, (11), donde q = [q1, q2, · · · , qn]T, y P es una matriz n × n construida de la siguiente manera. Sea αi = xT i Xi XT i Xi + λiniIi −1 , entonces Pij = αi j, si xj ∈ Ni 0, de lo contrario , (12) donde Pij es la entrada (i, j)-ésima de P, y αi j representa la entrada j-ésima de αi. Hasta ahora podemos escribir el criterio de agrupamiento combinando predictores de etiquetas lineales localmente regularizados Jl en una forma matemática explícita, y podemos minimizarlo directamente utilizando algunas técnicas estándar de optimización. Sin embargo, los resultados pueden no ser lo suficientemente buenos ya que solo explotamos la información local del conjunto de datos. En la siguiente subsección, introduciremos un criterio de <br>regularización global</br> y lo combinaremos con Jl, que tiene como objetivo encontrar un buen resultado de agrupamiento de manera local-global. 2.3 Regularización Global En el agrupamiento de datos, generalmente requerimos que las asignaciones de clúster de los puntos de datos sean lo suficientemente suaves con respecto a la variedad de datos subyacente, lo que implica (1) que los puntos cercanos tienden a tener las mismas asignaciones de clúster; (2) que los puntos en la misma estructura (por ejemplo, subvariedad o clúster) tienden a tener las mismas asignaciones de clúster [31]. ",
            "candidates": [],
            "error": [
                [
                    "Regularización Local y Global",
                    "regularización global",
                    "regularización global"
                ]
            ]
        },
        "cluster hierarchy": {
            "translated_key": "jerarquía de clusters",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Regularized Clustering for Documents ∗ Fei Wang, Changshui Zhang State Key Lab of Intelligent Tech.",
                "and Systems Department of Automation, Tsinghua University Beijing, China, 100084 feiwang03@gmail.com Tao Li School of Computer Science Florida International University Miami, FL 33199, U.S.A. taoli@cs.fiu.edu ABSTRACT In recent years, document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering.",
                "In this paper, we propose a novel method for clustering documents using regularization.",
                "Unlike traditional globally regularized clustering methods, our method first construct a local regularized linear label predictor for each document vector, and then combine all those local regularizers with a global smoothness regularizer.",
                "So we call our algorithm Clustering with Local and Global Regularization (CLGR).",
                "We will show that the cluster memberships of the documents can be achieved by eigenvalue decomposition of a sparse symmetric matrix, which can be efficiently solved by iterative methods.",
                "Finally our experimental evaluations on several datasets are presented to show the superiorities of CLGR over traditional document clustering methods.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Clustering; I.2.6 [Artificial Intelligence]: Learning-Concept Learning General Terms Algorithms 1.",
                "INTRODUCTION Document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering.",
                "A good document clustering approach can assist the computers to automatically organize the document corpus into a meaningful <br>cluster hierarchy</br> for efficient browsing and navigation, which is very valuable for complementing the deficiencies of traditional information retrieval technologies.",
                "As pointed out by [8], the information retrieval needs can be expressed by a spectrum ranged from narrow keyword-matching based search to broad information browsing such as what are the major international events in recent months.",
                "Traditional document retrieval engines tend to fit well with the search end of the spectrum, i.e. they usually provide specified search for documents matching the users query, however, it is hard for them to meet the needs from the rest of the spectrum in which a rather broad or vague information is needed.",
                "In such cases, efficient browsing through a good <br>cluster hierarchy</br> will be definitely helpful.",
                "Generally, document clustering methods can be mainly categorized into two classes: hierarchical methods and partitioning methods.",
                "The hierarchical methods group the data points into a hierarchical tree structure using bottom-up or top-down approaches.",
                "For example, hierarchical agglomerative clustering (HAC) [13] is a typical bottom-up hierarchical clustering method.",
                "It takes each data point as a single cluster to start off with and then builds bigger and bigger clusters by grouping similar data points together until the entire dataset is encapsulated into one final cluster.",
                "On the other hand, partitioning methods decompose the dataset into a number of disjoint clusters which are usually optimal in terms of some predefined criterion functions.",
                "For instance, K-means [13] is a typical partitioning method which aims to minimize the sum of the squared distance between the data points and their corresponding cluster centers.",
                "In this paper, we will focus on the partitioning methods.",
                "As we know that there are two main problems existing in partitioning methods (like Kmeans and Gaussian Mixture Model (GMM) [16]): (1) the predefined criterion is usually non-convex which causes many local optimal solutions; (2) the iterative procedure (e.g. the Expectation Maximization (EM) algorithm) for optimizing the criterions usually makes the final solutions heavily depend on the initializations.",
                "In the last decades, many methods have been proposed to overcome the above problems of the partitioning methods [19][28].",
                "Recently, another type of partitioning methods based on clustering on data graphs have aroused considerable interests in the machine learning and data mining community.",
                "The basic idea behind these methods is to first model the whole dataset as a weighted graph, in which the graph nodes represent the data points, and the weights on the edges correspond to the similarities between pairwise points.",
                "Then the cluster assignments of the dataset can be achieved by optimizing some criterions defined on the graph.",
                "For example Spectral Clustering is one kind of the most representative graph-based clustering approaches, it generally aims to optimize some cut value (e.g.",
                "Normalized Cut [22], Ratio Cut [7], Min-Max Cut [11]) defined on an undirected graph.",
                "After some relaxations, these criterions can usually be optimized via eigen-decompositions, which is guaranteed to be global optimal.",
                "In this way, spectral clustering efficiently avoids the problems of the traditional partitioning methods as we introduced in last paragraph.",
                "In this paper, we propose a novel document clustering algorithm that inherits the superiority of spectral clustering, i.e. the final cluster results can also be obtained by exploit the eigen-structure of a symmetric matrix.",
                "However, unlike spectral clustering, which just enforces a smoothness constraint on the data labels over the whole data manifold [2], our method first construct a regularized linear label predictor for each data point from its neighborhood as in [25], and then combine the results of all these local label predictors with a global label smoothness regularizer.",
                "So we call our method Clustering with Local and Global Regularization (CLGR).",
                "The idea of incorporating both local and global information into label prediction is inspired by the recent works on semi-supervised learning [31], and our experimental evaluations on several real document datasets show that CLGR performs better than many state-of-the-art clustering methods.",
                "The rest of this paper is organized as follows: in section 2 we will introduce our CLGR algorithm in detail.",
                "The experimental results on several datasets are presented in section 3, followed by the conclusions and discussions in section 4. 2.",
                "THE PROPOSED ALGORITHM In this section, we will introduce our Clustering with Local and Global Regularization (CLGR) algorithm in detail.",
                "First lets see the how the documents are represented throughout this paper. 2.1 Document Representation In our work, all the documents are represented by the weighted term-frequency vectors.",
                "Let W = {w1, w2, · · · , wm} be the complete vocabulary set of the document corpus (which is preprocessed by the stopwords removal and words stemming operations).",
                "The term-frequency vector xi of document di is defined as xi = [xi1, xi2, · · · , xim]T , xik = tik log n idfk , where tik is the term frequency of wk ∈ W, n is the size of the document corpus, idfk is the number of documents that contain word wk.",
                "In this way, xi is also called the TFIDF representation of document di.",
                "Furthermore, we also normalize each xi (1 i n) to have a unit length, so that each document is represented by a normalized TF-IDF vector. 2.2 Local Regularization As its name suggests, CLGR is composed of two parts: local regularization and global regularization.",
                "In this subsection we will introduce the local regularization part in detail. 2.2.1 Motivation As we know that clustering is one type of learning techniques, it aims to organize the dataset in a reasonable way.",
                "Generally speaking, learning can be posed as a problem of function estimation, from which we can get a good classification function that will assign labels to the training dataset and even the unseen testing dataset with some cost minimized [24].",
                "For example, in the two-class classification scenario1 (in which we exactly know the label of each document), a linear classifier with least square fit aims to learn a column vector w such that the squared cost J = 1 n (wT xi − yi)2 (1) is minimized, where yi ∈ {+1, −1} is the label of xi.",
                "By taking ∂J /∂w = 0, we get the solution w∗ = n i=1 xixT i −1 n i=1 xiyi , (2) which can further be written in its matrix form as w∗ = XXT −1 Xy, (3) where X = [x1, x2, · · · , xn] is an m × n document matrix, y = [y1, y2, · · · , yn]T is the label vector.",
                "Then for a test document t, we can determine its label by l = sign(w∗T u), (4) where sign(·) is the sign function.",
                "A natural problem in Eq. (3) is that the matrix XXT may be singular and thus not invertable (e.g. when m n).",
                "To avoid such a problem, we can add a regularization term and minimize the following criterion J = 1 n n i=1 (wT xi − yi)2 + λ w 2 , (5) where λ is a regularization parameter.",
                "Then the optimal solution that minimize J is given by w∗ = XXT + λnI −1 Xy, (6) where I is an m × m identity matrix.",
                "It has been reported that the regularized linear classifier can achieve very good results on text classification problems [29].",
                "However, despite its empirical success, the regularized linear classifier is on earth a global classifier, i.e. w∗ is estimated using the whole training set.",
                "According to [24], this may not be a smart idea, since a unique w∗ may not be good enough for predicting the labels of the whole input space.",
                "In order to get better predictions, [6] proposed to train classifiers locally and use them to classify the testing points.",
                "For example, a testing point will be classified by the local classifier trained using the training points located in the vicinity 1 In the following discussions we all assume that the documents coming from only two classes.",
                "The generalizations of our method to multi-class cases will be discussed in section 2.5. of it.",
                "Although this method seems slow and stupid, it is reported that it can get better performances than using a unique global classifier on certain tasks [6]. 2.2.2 Constructing the Local Regularized Predictors Inspired by their success, we proposed to apply the local learning algorithms for clustering.",
                "The basic idea is that, for each document vector xi (1 i n), we train a local label predictor based on its k-nearest neighborhood Ni, and then use it to predict the label of xi.",
                "Finally we will combine all those local predictors by minimizing the sum of their prediction errors.",
                "In this subsection we will introduce how to construct those local predictors.",
                "Due to the simplicity and effectiveness of the regularized linear classifier that we have introduced in section 2.2.1, we choose it to be our local label predictor, such that for each document xi, the following criterion is minimized Ji = 1 ni xj ∈Ni wT i xj − qj 2 + λi wi 2 , (7) where ni = |Ni| is the cardinality of Ni, and qj is the cluster membership of xj.",
                "Then using Eq. (6), we can get the optimal solution is w∗ i = XiXT i + λiniI −1 Xiqi, (8) where Xi = [xi1, xi2, · · · , xini ], and we use xik to denote the k-th nearest neighbor of xi. qi = [qi1, qi2, · · · , qini ]T with qik representing the cluster assignment of xik.",
                "The problem here is that XiXT i is an m × m matrix with m ni, i.e. we should compute the inverse of an m × m matrix for every document vector, which is computationally prohibited.",
                "Fortunately, we have the following theorem: Theorem 1. w∗ i in Eq. (8) can be rewritten as w∗ i = Xi XT i Xi + λiniIi −1 qi, (9) where Ii is an ni × ni identity matrix.",
                "Proof.",
                "Since w∗ i = XiXT i + λiniI −1 Xiqi, then XiXT i + λiniI w∗ i = Xiqi =⇒ XiXT i w∗ i + λiniw∗ i = Xiqi =⇒ w∗ i = (λini)−1 Xi qi − XT i w∗ i .",
                "Let β = (λini)−1 qi − XT i w∗ i , then w∗ i = Xiβ =⇒ λiniβ = qi − XT i w∗ i = qi − XT i Xiβ =⇒ qi = XT i Xi + λiniIi β =⇒ β = XT i Xi + λiniIi −1 qi.",
                "Therefore w∗ i = Xiβ = Xi XT i Xi + λiniIi −1 qi 2 Using theorem 1, we only need to compute the inverse of an ni × ni matrix for every document to train a local label predictor.",
                "Moreover, for a new testing point u that falls into Ni, we can classify it by the sign of qu = w∗T i u = uT wi = uT Xi XT i Xi + λiniIi −1 qi.",
                "This is an attractive expression since we can determine the cluster assignment of u by using the inner-products between the points in {u ∪ Ni}, which suggests that such a local regularizer can easily be kernelized [21] as long as we define a proper kernel function. 2.2.3 Combining the Local Regularized Predictors After all the local predictors having been constructed, we will combine them together by minimizing Jl = n i=1 w∗T i xi − qi 2 , (10) which stands for the sum of the prediction errors for all the local predictors.",
                "Combining Eq. (10) with Eq. (6), we can get Jl = n i=1 w∗T i xi − qi 2 = n i=1 xT i Xi XT i Xi + λiniIi −1 qi − qi 2 = Pq − q 2 , (11) where q = [q1, q2, · · · , qn]T , and the P is an n × n matrix constructing in the following way.",
                "Let αi = xT i Xi XT i Xi + λiniIi −1 , then Pij = αi j, if xj ∈ Ni 0, otherwise , (12) where Pij is the (i, j)-th entry of P, and αi j represents the j-th entry of αi .",
                "Till now we can write the criterion of clustering by combining locally regularized linear label predictors Jl in an explicit mathematical form, and we can minimize it directly using some standard optimization techniques.",
                "However, the results may not be good enough since we only exploit the local informations of the dataset.",
                "In the next subsection, we will introduce a global regularization criterion and combine it with Jl, which aims to find a good clustering result in a local-global way. 2.3 Global Regularization In data clustering, we usually require that the cluster assignments of the data points should be sufficiently smooth with respect to the underlying data manifold, which implies (1) the nearby points tend to have the same cluster assignments; (2) the points on the same structure (e.g. submanifold or cluster) tend to have the same cluster assignments [31].",
                "Without the loss of generality, we assume that the data points reside (roughly) on a low-dimensional manifold M2 , and q is the cluster assignment function defined on M, i.e. 2 We believe that the text data are also sampled from some low dimensional manifold, since it is impossible for them to for ∀x ∈ M, q(x) returns the cluster membership of x.",
                "The smoothness of q over M can be calculated by the following Dirichlet integral [2] D[q] = 1 2 M q(x) 2 dM, (13) where the gradient q is a vector in the tangent space T Mx, and the integral is taken with respect to the standard measure on M. If we restrict the scale of q by q, q M = 1 (where ·, · M is the inner product induced on M), then it turns out that finding the smoothest function minimizing D[q] reduces to finding the eigenfunctions of the Laplace Beltrami operator L, which is defined as Lq −div q, (14) where div is the divergence of a vector field.",
                "Generally, the graph can be viewed as the discretized form of manifold.",
                "We can model the dataset as an weighted undirected graph as in spectral clustering [22], where the graph nodes are just the data points, and the weights on the edges represent the similarities between pairwise points.",
                "Then it can be shown that minimizing Eq. (13) corresponds to minimizing Jg = qT Lq = n i=1 (qi − qj)2 wij, (15) where q = [q1, q2, · · · , qn]T with qi = q(xi), L is the graph Laplacian with its (i, j)-th entry Lij =    di − wii, if i = j −wij, if xi and xj are adjacent 0, otherwise, (16) where di = j wij is the degree of xi, wij is the similarity between xi and xj.",
                "If xi and xj are adjacent3 , wij is usually computed in the following way wij = e − xi−xj 2 2σ2 , (17) where σ is a dataset dependent parameter.",
                "It is proved that under certain conditions, such a form of wij to determine the weights on graph edges leads to the convergence of graph Laplacian to the Laplace Beltrami operator [3][18].",
                "In summary, using Eq. (15) with exponential weights can effectively measure the smoothness of the data assignments with respect to the intrinsic data manifold.",
                "Thus we adopt it as a global regularizer to punish the smoothness of the predicted data assignments. 2.4 Clustering with Local and Global Regularization Combining the contents we have introduced in section 2.2 and section 2.3 we can derive the clustering criterion is minq J = Jl + λJg = Pq − q 2 + λqT Lq s.t. qi ∈ {−1, +1}, (18) where P is defined as in Eq. (12), and λ is a regularization parameter to trade off Jl and Jg.",
                "However, the discrete fill in the whole high-dimensional sample space.",
                "And it has been shown that the manifold based methods can achieve good results on text classification tasks [31]. 3 In this paper, we define xi and xj to be adjacent if xi ∈ N(xj) or xj ∈ N(xi). constraint of pi makes the problem an NP hard integer programming problem.",
                "A natural way for making the problem solvable is to remove the constraint and relax qi to be continuous, then the objective that we aims to minimize becomes J = Pq − q 2 + λqT Lq = qT (P − I)T (P − I)q + λqT Lq = qT (P − I)T (P − I) + λL q, (19) and we further add a constraint qT q = 1 to restrict the scale of q.",
                "Then our objective becomes minq J = qT (P − I)T (P − I) + λL q s.t. qT q = 1 (20) Using the Lagrangian method, we can derive that the optimal solution q corresponds to the smallest eigenvector of the matrix M = (P − I)T (P − I) + λL, and the cluster assignment of xi can be determined by the sign of qi, i.e. xi will be classified as class one if qi > 0, otherwise it will be classified as class 2. 2.5 Multi-Class CLGR In the above we have introduced the basic framework of Clustering with Local and Global Regularization (CLGR) for the two-class clustering problem, and we will extending it to multi-class clustering in this subsection.",
                "First we assume that all the documents belong to C classes indexed by L = {1, 2, · · · , C}. qc is the classification function for class c (1 c C), such that qc (xi) returns the confidence that xi belongs to class c. Our goal is to obtain the value of qc (xi) (1 c C, 1 i n), and the cluster assignment of xi can be determined by {qc (xi)}C c=1 using some proper discretization methods that we will introduce later.",
                "Therefore, in this multi-class case, for each document xi (1 i n), we will construct C locally linear regularized label predictors whose normal vectors are wc∗ i = Xi XT i Xi + λiniIi −1 qc i (1 c C), (21) where Xi = [xi1, xi2, · · · , xini ] with xik being the k-th neighbor of xi, and qc i = [qc i1, qc i2, · · · , qc ini ]T with qc ik = qc (xik).",
                "Then (wc∗ i )T xi returns the predicted confidence of xi belonging to class c. Hence the local prediction error for class c can be defined as J c l = n i=1 (wc∗ i ) T xi − qc i 2 , (22) And the total local prediction error becomes Jl = C c=1 J c l = C c=1 n i=1 (wc∗ i ) T xi − qc i 2 . (23) As in Eq. (11), we can define an n×n matrix P (see Eq. (12)) and rewrite Jl as Jl = C c=1 J c l = C c=1 Pqc − qc 2 . (24) Similarly we can define the global smoothness regularizer in multi-class case as Jg = C c=1 n i=1 (qc i − qc j )2 wij = C c=1 (qc )T Lqc . (25) Then the criterion to be minimized for CLGR in multi-class case becomes J = Jl + λJg = C c=1 Pqc − qc 2 + λ(qc )T Lqc = C c=1 (qc )T (P − I)T (P − I) + λL qc = trace QT (P − I)T (P − I) + λL Q , (26) where Q = [q1 , q2 , · · · , qc ] is an n × c matrix, and trace(·) returns the trace of a matrix.",
                "The same as in Eq. (20), we also add the constraint that QT Q = I to restrict the scale of Q.",
                "Then our optimization problem becomes minQ J = trace QT (P − I)T (P − I) + λL Q s.t.",
                "QT Q = I, (27) From the Ky Fan theorem [28], we know the optimal solution of the above problem is Q∗ = [q∗ 1, q∗ 2, · · · , q∗ C ]R, (28) where q∗ k (1 k C) is the eigenvector corresponds to the k-th smallest eigenvalue of matrix (P − I)T (P − I) + λL, and R is an arbitrary C × C matrix.",
                "Since the values of the entries in Q∗ is continuous, we need to further discretize Q∗ to get the cluster assignments of all the data points.",
                "There are mainly two approaches to achieve this goal: 1.",
                "As in [20], we can treat the i-th row of Q as the embedding of xi in a C-dimensional space, and apply some traditional clustering methods like kmeans to clustering these embeddings into C clusters. 2.",
                "Since the optimal Q∗ is not unique (because of the existence of an arbitrary matrix R), we can pursue an optimal R that will rotate Q∗ to an indication matrix4 .",
                "The detailed algorithm can be referred to [26].",
                "The detailed algorithm procedure for CLGR is summarized in table 1. 3.",
                "EXPERIMENTS In this section, experiments are conducted to empirically compare the clustering results of CLGR with other 8 representitive document clustering algorithms on 5 datasets.",
                "First we will introduce the basic informations of those datasets. 3.1 Datasets We use a variety of datasets, most of which are frequently used in the information retrieval research.",
                "Table 2 summarizes the characteristics of the datasets. 4 Here an indication matrix T is a n×c matrix with its (i, j)th entry Tij ∈ {0, 1} such that for each row of Q∗ there is only one 1.",
                "Then the xi can be assigned to the j-th cluster such that j = argjQ∗ ij = 1.",
                "Table 1: Clustering with Local and Global Regularization (CLGR) Input: 1.",
                "Dataset X = {xi}n i=1; 2.",
                "Number of clusters C; 3.",
                "Size of the neighborhood K; 4.",
                "Local regularization parameters {λi}n i=1; 5.",
                "Global regularization parameter λ; Output: The cluster membership of each data point.",
                "Procedure: 1.",
                "Construct the K nearest neighborhoods for each data point; 2.",
                "Construct the matrix P using Eq. (12); 3.",
                "Construct the Laplacian matrix L using Eq. (16); 4.",
                "Construct the matrix M = (P − I)T (P − I) + λL; 5.",
                "Do eigenvalue decomposition on M, and construct the matrix Q∗ according to Eq. (28); 6.",
                "Output the cluster assignments of each data point by properly discretize Q∗ .",
                "Table 2: Descriptions of the document datasets Datasets Number of documents Number of classes CSTR 476 4 WebKB4 4199 4 Reuters 2900 10 WebACE 2340 20 Newsgroup4 3970 4 CSTR.",
                "This is the dataset of the abstracts of technical reports published in the Department of Computer Science at a university.",
                "The dataset contained 476 abstracts, which were divided into four research areas: Natural Language Processing(NLP), Robotics/Vision, Systems, and Theory.",
                "WebKB.",
                "The WebKB dataset contains webpages gathered from university computer science departments.",
                "There are about 8280 documents and they are divided into 7 categories: student, faculty, staff, course, project, department and other.",
                "The raw text is about 27MB.",
                "Among these 7 categories, student, faculty, course and project are four most populous entity-representing categories.",
                "The associated subset is typically called WebKB4.",
                "Reuters.",
                "The Reuters-21578 Text Categorization Test collection contains documents collected from the Reuters newswire in 1987.",
                "It is a standard text categorization benchmark and contains 135 categories.",
                "In our experiments, we use a subset of the data collection which includes the 10 most frequent categories among the 135 topics and we call it Reuters-top 10.",
                "WebACE.",
                "The WebACE dataset was from WebACE project and has been used for document clustering [17][5].",
                "The WebACE dataset contains 2340 documents consisting news articles from Reuters new service via the Web in October 1997.",
                "These documents are divided into 20 classes.",
                "News4.",
                "The News4 dataset used in our experiments are selected from the famous 20-newsgroups dataset5 .",
                "The topic rec containing autos, motorcycles, baseball and hockey was selected from the version 20news-18828.",
                "The News4 dataset contains 3970 document vectors. 5 http://people.csail.mit.edu/jrennie/20Newsgroups/ To pre-process the datasets, we remove the stop words using a standard stop list, all HTML tags are skipped and all header fields except subject and organization of the posted articles are ignored.",
                "In all our experiments, we first select the top 1000 words by mutual information with class labels. 3.2 Evaluation Metrics In the experiments, we set the number of clusters equal to the true number of classes C for all the clustering algorithms.",
                "To evaluate their performance, we compare the clusters generated by these algorithms with the true classes by computing the following two performance measures.",
                "Clustering Accuracy (Acc).",
                "The first performance measure is the Clustering Accuracy, which discovers the one-toone relationship between clusters and classes and measures the extent to which each cluster contained data points from the corresponding class.",
                "It sums up the whole matching degree between all pair class-clusters.",
                "Clustering accuracy can be computed as: Acc = 1 N max   Ck,Lm T(Ck, Lm)   , (29) where Ck denotes the k-th cluster in the final results, and Lm is the true m-th class.",
                "T(Ck, Lm) is the number of entities which belong to class m are assigned to cluster k. Accuracy computes the maximum sum of T(Ck, Lm) for all pairs of clusters and classes, and these pairs have no overlaps.",
                "The greater clustering accuracy means the better clustering performance.",
                "Normalized Mutual Information (NMI).",
                "Another evaluation metric we adopt here is the Normalized Mutual Information NMI [23], which is widely used for determining the quality of clusters.",
                "For two random variable X and Y, the NMI is defined as: NMI(X, Y) = I(X, Y) H(X)H(Y) , (30) where I(X, Y) is the mutual information between X and Y, while H(X) and H(Y) are the entropies of X and Y respectively.",
                "One can see that NMI(X, X) = 1, which is the maximal possible value of NMI.",
                "Given a clustering result, the NMI in Eq. (30) is estimated as NMI = C k=1 C m=1 nk,mlog n·nk,m nk ˆnm C k=1 nklog nk n C m=1 ˆnmlog ˆnm n , (31) where nk denotes the number of data contained in the cluster Ck (1 k C), ˆnm is the number of data belonging to the m-th class (1 m C), and nk,m denotes the number of data that are in the intersection between the cluster Ck and the m-th class.",
                "The value calculated in Eq. (31) is used as a performance measure for the given clustering result.",
                "The larger this value, the better the clustering performance. 3.3 Comparisons We have conducted comprehensive performance evaluations by testing our method and comparing it with 8 other representative data clustering methods using the same data corpora.",
                "The algorithms that we evaluated are listed below. 1.",
                "Traditional k-means (KM). 2.",
                "Spherical k-means (SKM).",
                "The implementation is based on [9]. 3.",
                "Gaussian Mixture Model (GMM).",
                "The implementation is based on [16]. 4.",
                "Spectral Clustering with Normalized Cuts (Ncut).",
                "The implementation is based on [26], and the variance of the Gaussian similarity is determined by Local Scaling [30].",
                "Note that the criterion that Ncut aims to minimize is just the global regularizer in our CLGR algorithm except that Ncut used the normalized Laplacian. 5.",
                "Clustering using Pure Local Regularization (CPLR).",
                "In this method we just minimize Jl (defined in Eq. (24)), and the clustering results can be obtained by doing eigenvalue decomposition on matrix (I − P)T (I − P) with some proper discretization methods. 6.",
                "Adaptive Subspace Iteration (ASI).",
                "The implementation is based on [14]. 7.",
                "Nonnegative Matrix Factorization (NMF).",
                "The implementation is based on [27]. 8.",
                "Tri-Factorization Nonnegative Matrix Factorization (TNMF) [12].",
                "The implementation is based on [15].",
                "For computational efficiency, in the implementation of CPLR and our CLGR algorithm, we have set all the local regularization parameters {λi}n i=1 to be identical, which is set by grid search from {0.1, 1, 10}.",
                "The size of the k-nearest neighborhoods is set by grid search from {20, 40, 80}.",
                "For the CLGR method, its global regularization parameter is set by grid search from {0.1, 1, 10}.",
                "When constructing the global regularizer, we have adopted the local scaling method [30] to construct the Laplacian matrix.",
                "The final discretization method adopted in these two methods is the same as in [26], since our experiments show that using such method can achieve better results than using kmeans based methods as in [20]. 3.4 Experimental Results The clustering accuracies comparison results are shown in table 3, and the normalized mutual information comparison results are summarized in table 4.",
                "From the two tables we mainly observe that: 1.",
                "Our CLGR method outperforms all other document clustering methods in most of the datasets; 2.",
                "For document clustering, the Spherical k-means method usually outperforms the traditional k-means clustering method, and the GMM method can achieve competitive results compared to the Spherical k-means method; 3.",
                "The results achieved from the k-means and GMM type algorithms are usually worse than the results achieved from Spectral Clustering.",
                "Since Spectral Clustering can be viewed as a weighted version of kernel k-means, it can obtain good results the data clusters are arbitrarily shaped.",
                "This corroborates that the documents vectors are not regularly distributed (spherical or elliptical). 4.",
                "The experimental comparisons empirically verify the equivalence between NMF and Spectral Clustering, which Table 3: Clustering accuracies of the various methods CSTR WebKB4 Reuters WebACE News4 KM 0.4256 0.3888 0.4448 0.4001 0.3527 SKM 0.4690 0.4318 0.5025 0.4458 0.3912 GMM 0.4487 0.4271 0.4897 0.4521 0.3844 NMF 0.5713 0.4418 0.4947 0.4761 0.4213 Ncut 0.5435 0.4521 0.4896 0.4513 0.4189 ASI 0.5621 0.4752 0.5235 0.4823 0.4335 TNMF 0.6040 0.4832 0.5541 0.5102 0.4613 CPLR 0.5974 0.5020 0.4832 0.5213 0.4890 CLGR 0.6235 0.5228 0.5341 0.5376 0.5102 Table 4: Normalized mutual information results of the various methods CSTR WebKB4 Reuters WebACE News4 KM 0.3675 0.3023 0.4012 0.3864 0.3318 SKM 0.4027 0.4155 0.4587 0.4003 0.4085 GMM 0.4034 0.4093 0.4356 0.4209 0.3994 NMF 0.5235 0.4517 0.4402 0.4359 0.4130 Ncut 0.4833 0.4497 0.4392 0.4289 0.4231 ASI 0.5008 0.4833 0.4769 0.4817 0.4503 TNMF 0.5724 0.5011 0.5132 0.5328 0.4749 CPLR 0.5695 0.5231 0.4402 0.5543 0.4690 CLGR 0.6012 0.5434 0.4935 0.5390 0.4908 has been proved theoretically in [10].",
                "It can be observed from the tables that NMF and Spectral Clustering usually lead to similar clustering results. 5.",
                "The co-clustering based methods (TNMF and ASI) can usually achieve better results than traditional purely document vector based methods.",
                "Since these methods perform an implicit feature selection at each iteration, provide an adaptive metric for measuring the neighborhood, and thus tend to yield better clustering results. 6.",
                "The results achieved from CPLR are usually better than the results achieved from Spectral Clustering, which supports Vapniks theory [24] that sometimes local learning algorithms can obtain better results than global learning algorithms.",
                "Besides the above comparison experiments, we also test the parameter sensibility of our method.",
                "There are mainly two sets of parameters in our CLGR algorithm, the local and global regularization parameters ({λi}n i=1 and λ, as we have said in section 3.3, we have set all λis to be identical to λ∗ in our experiments), and the size of the neighborhoods.",
                "Therefore we have also done two sets of experiments: 1.",
                "Fixing the size of the neighborhoods, and testing the clustering performance with varying λ∗ and λ.",
                "In this set of experiments, we find that our CLGR algorithm can achieve good results when the two regularization parameters are neither too large nor too small.",
                "Typically our method can achieve good results when λ∗ and λ are around 0.1.",
                "Figure 1 shows us such a testing example on the WebACE dataset. 2.",
                "Fixing the local and global regularization parameters, and testing the clustering performance with different −5 −4.5 −4 −3.5 −3 −5 −4.5 −4 −3.5 −3 0.35 0.4 0.45 0.5 0.55 local regularization para (log 2 value) global regularization para (log 2 value) clusteringaccuracy Figure 1: Parameter sensibility testing results on the WebACE dataset with the neighborhood size fixed to 20, and the x-axis and y-axis represents the log2 value of λ∗ and λ. sizes of neighborhoods.",
                "In this set of experiments, we find that the neighborhood with a too large or too small size will all deteriorate the final clustering results.",
                "This can be easily understood since when the neighborhood size is very small, then the data points used for training the local classifiers may not be sufficient; when the neighborhood size is very large, the trained classifiers will tend to be global and cannot capture the typical local characteristics.",
                "Figure 2 shows us a testing example on the WebACE dataset.",
                "Therefore, we can see that our CLGR algorithm (1) can achieve satisfactory results and (2) is not very sensitive to the choice of parameters, which makes it practical in real world applications. 4.",
                "CONCLUSIONS AND FUTURE WORKS In this paper, we derived a new clustering algorithm called clustering with local and global regularization.",
                "Our method preserves the merit of local learning algorithms and spectral clustering.",
                "Our experiments show that the proposed algorithm outperforms most of the state of the art algorithms on many benchmark datasets.",
                "In the future, we will focus on the parameter selection and acceleration issues of the CLGR algorithm. 5.",
                "REFERENCES [1] L. Baker and A. McCallum.",
                "Distributional Clustering of Words for Text Classification.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 1998. [2] M. Belkin and P. Niyogi.",
                "Laplacian Eigenmaps for Dimensionality Reduction and Data Representation.",
                "Neural Computation, 15 (6):1373-1396.",
                "June 2003. [3] M. Belkin and P. Niyogi.",
                "Towards a Theoretical Foundation for Laplacian-Based Manifold Methods.",
                "In Proceedings of the 18th Conference on Learning Theory (COLT). 2005. 10 20 30 40 50 60 70 80 90 100 0.35 0.4 0.45 0.5 0.55 size of the neighborhood clusteringaccuracy Figure 2: Parameter sensibility testing results on the WebACE dataset with the regularization parameters being fixed to 0.1, and the neighborhood size varing from 10 to 100. [4] M. Belkin, P. Niyogi and V. Sindhwani.",
                "Manifold Regularization: a Geometric Framework for Learning from Examples.",
                "Journal of Machine Learning Research 7, 1-48, 2006. [5] D. Boley.",
                "Principal Direction Divisive Partitioning.",
                "Data mining and knowledge discovery, 2:325-344, 1998. [6] L. Bottou and V. Vapnik.",
                "Local learning algorithms.",
                "Neural Computation, 4:888-900, 1992. [7] P. K. Chan, D. F. Schlag and J. Y. Zien.",
                "Spectral K-way Ratio-Cut Partitioning and Clustering.",
                "IEEE Trans.",
                "Computer-Aided Design, 13:1088-1096, Sep. 1994. [8] D. R. Cutting, D. R. Karger, J. O. Pederson and J. W. Tukey.",
                "Scatter/Gather: A Cluster-Based Approach to Browsing Large Document Collections.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 1992. [9] I. S. Dhillon and D. S. Modha.",
                "Concept Decompositions for Large Sparse Text Data using Clustering.",
                "Machine Learning, vol. 42(1), pages 143-175, January 2001. [10] C. Ding, X.",
                "He, and H. Simon.",
                "On the equivalence of nonnegative matrix factorization and spectral clustering.",
                "In Proceedings of the SIAM Data Mining Conference, 2005. [11] C. Ding, X.",
                "He, H. Zha, M. Gu, and H. D. Simon.",
                "A min-max cut algorithm for graph partitioning and data clustering.",
                "In Proc. of the 1st International Conference on Data Mining (ICDM), pages 107-114, 2001. [12] C. Ding, T. Li, W. Peng, and H. Park.",
                "Orthogonal Nonnegative Matrix Tri-Factorizations for Clustering.",
                "In Proceedings of the Twelfth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2006. [13] R. O. Duda, P. E. Hart, and D. G. Stork.",
                "Pattern Classification.",
                "John Wiley & Sons, Inc., 2001. [14] T. Li, S. Ma, and M. Ogihara.",
                "Document Clustering via Adaptive Subspace Iteration.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2004. [15] T. Li and C. Ding.",
                "The Relationships Among Various Nonnegative Matrix Factorization Methods for Clustering.",
                "In Proceedings of the 6th International Conference on Data Mining (ICDM). 2006. [16] X. Liu and Y. Gong.",
                "Document Clustering with Cluster Refinement and Model Selection Capabilities.",
                "In Proc. of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002. [17] E. Han, D. Boley, M. Gini, R. Gross, K. Hastings, G. Karypis, V. Kumar, B. Mobasher, and J. Moore.",
                "WebACE: A Web Agent for Document Categorization and Exploration.",
                "In Proceedings of the 2nd International Conference on Autonomous Agents (Agents98).",
                "ACM Press, 1998. [18] M. Hein, J. Y. Audibert, and U. von Luxburg.",
                "From Graphs to Manifolds - Weak and Strong Pointwise Consistency of Graph Laplacians.",
                "In Proceedings of the 18th Conference on Learning Theory (COLT), 470-485. 2005. [19] J.",
                "He, M. Lan, C.-L. Tan, S.-Y.",
                "Sung, and H.-B.",
                "Low.",
                "Initialization of Cluster Refinement Algorithms: A Review and Comparative Study.",
                "In Proc. of Inter.",
                "Joint Conference on Neural Networks, 2004. [20] A. Y. Ng, M. I. Jordan, Y. Weiss.",
                "On Spectral Clustering: Analysis and an algorithm.",
                "In Advances in Neural Information Processing Systems 14. 2002. [21] B. Sch¨olkopf and A. Smola.",
                "Learning with Kernels.",
                "The MIT Press.",
                "Cambridge, Massachusetts. 2002. [22] J. Shi and J. Malik.",
                "Normalized Cuts and Image Segmentation.",
                "IEEE Trans. on Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000. [23] A. Strehl and J. Ghosh.",
                "Cluster Ensembles - A Knowledge Reuse Framework for Combining Multiple Partitions.",
                "Journal of Machine Learning Research, 3:583-617, 2002. [24] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Berlin: Springer-Verlag, 1995. [25] Wu, M. and Sch¨olkopf, B.",
                "A Local Learning Approach for Clustering.",
                "In Advances in Neural Information Processing Systems 18. 2006. [26] S. X. Yu, J. Shi.",
                "Multiclass Spectral Clustering.",
                "In Proceedings of the International Conference on Computer Vision, 2003. [27] W. Xu, X. Liu and Y. Gong.",
                "Document Clustering Based On Non-Negative Matrix Factorization.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2003. [28] H. Zha, X.",
                "He, C. Ding, M. Gu and H. Simon.",
                "Spectral Relaxation for K-means Clustering.",
                "In NIPS 14. 2001. [29] T. Zhang and F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Journal of Information Retrieval, 4:5-31, 2001. [30] L. Zelnik-Manor and P. Perona.",
                "Self-Tuning Spectral Clustering.",
                "In NIPS 17. 2005. [31] D. Zhou, O. Bousquet, T. N. Lal, J. Weston and B. Sch¨olkopf.",
                "Learning with Local and Global Consistency.",
                "NIPS 17, 2005."
            ],
            "original_annotated_samples": [
                "A good document clustering approach can assist the computers to automatically organize the document corpus into a meaningful <br>cluster hierarchy</br> for efficient browsing and navigation, which is very valuable for complementing the deficiencies of traditional information retrieval technologies.",
                "In such cases, efficient browsing through a good <br>cluster hierarchy</br> will be definitely helpful."
            ],
            "translated_annotated_samples": [
                "Un buen enfoque de agrupación de documentos puede ayudar a las computadoras a organizar automáticamente el corpus de documentos en una <br>jerarquía de clusters</br> significativa para una navegación eficiente, lo cual es muy valioso para complementar las deficiencias de las tecnologías tradicionales de recuperación de información.",
                "En tales casos, la navegación eficiente a través de una buena <br>jerarquía de clústeres</br> será definitivamente útil."
            ],
            "translated_text": "Agrupamiento regularizado para documentos ∗ Fei Wang, Changshui Zhang Laboratorio Clave del Estado de Tecnología Inteligente. En los últimos años, la agrupación de documentos ha estado recibiendo cada vez más atención como una técnica importante y fundamental para la organización no supervisada de documentos, la extracción automática de temas y la recuperación o filtrado rápido de información. En este artículo, proponemos un método novedoso para agrupar documentos utilizando regularización. A diferencia de los métodos de agrupamiento regularizados globalmente tradicionales, nuestro método primero construye un predictor de etiquetas lineal regularizado local para cada vector de documento, y luego combina todos esos regularizadores locales con un regularizador de suavidad global. Así que llamamos a nuestro algoritmo Agrupamiento con Regularización Local y Global (CLGR). Mostraremos que las pertenencias de los documentos a los grupos se pueden lograr mediante la descomposición de los valores propios de una matriz simétrica dispersa, la cual puede resolverse eficientemente mediante métodos iterativos. Finalmente, se presentan nuestras evaluaciones experimentales en varios conjuntos de datos para mostrar las ventajas de CLGR sobre los métodos tradicionales de agrupamiento de documentos. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información-Agrupamiento; I.2.6 [Inteligencia Artificial]: Aprendizaje-Aprendizaje de Conceptos Términos Generales Algoritmos 1. La agrupación de documentos ha estado recibiendo cada vez más atención como una técnica importante y fundamental para la organización no supervisada de documentos, la extracción automática de temas y la recuperación o filtrado rápido de información. Un buen enfoque de agrupación de documentos puede ayudar a las computadoras a organizar automáticamente el corpus de documentos en una <br>jerarquía de clusters</br> significativa para una navegación eficiente, lo cual es muy valioso para complementar las deficiencias de las tecnologías tradicionales de recuperación de información. Como señaló [8], las necesidades de recuperación de información pueden expresarse mediante un espectro que va desde la búsqueda basada en palabras clave estrechas hasta la exploración de información amplia, como cuáles son los principales eventos internacionales de los últimos meses. Los motores de búsqueda de documentos tradicionales tienden a adaptarse bien al extremo de la búsqueda, es decir, suelen proporcionar búsquedas específicas de documentos que coinciden con la consulta de los usuarios, sin embargo, les resulta difícil satisfacer las necesidades del resto del espectro en el que se necesita información más amplia o vaga. En tales casos, la navegación eficiente a través de una buena <br>jerarquía de clústeres</br> será definitivamente útil. Generalmente, los métodos de agrupamiento de documentos se pueden clasificar principalmente en dos clases: métodos jerárquicos y métodos de partición. Los métodos jerárquicos agrupan los puntos de datos en una estructura de árbol jerárquico utilizando enfoques de abajo hacia arriba o de arriba hacia abajo. Por ejemplo, el clustering jerárquico aglomerativo (HAC) [13] es un método típico de clustering jerárquico ascendente. Toma cada punto de datos como un único clúster para comenzar y luego construye clústeres más grandes agrupando puntos de datos similares juntos hasta que todo el conjunto de datos esté encapsulado en un único clúster final. Por otro lado, los métodos de particionamiento descomponen el conjunto de datos en un número de grupos disjuntos que suelen ser óptimos en términos de algunas funciones de criterio predefinidas. Por ejemplo, K-means es un método de particionamiento típico que tiene como objetivo minimizar la suma de las distancias al cuadrado entre los puntos de datos y sus centros de clúster correspondientes. En este documento, nos centraremos en los métodos de particionamiento. Como sabemos, existen dos problemas principales en los métodos de particionamiento (como Kmeans y el Modelo de Mezcla Gaussiana (GMM) [16]): (1) el criterio predefinido suele ser no convexo, lo que provoca muchas soluciones óptimas locales; (2) el procedimiento iterativo (por ejemplo, el algoritmo de Expectation Maximization (EM)) para optimizar los criterios suele hacer que las soluciones finales dependan en gran medida de las inicializaciones. En las últimas décadas, se han propuesto muchos métodos para superar los problemas mencionados de los métodos de particionamiento [19][28]. Recientemente, otro tipo de métodos de particionamiento basados en agrupamiento en grafos de datos han despertado un considerable interés en la comunidad de aprendizaje automático y minería de datos. La idea básica detrás de estos métodos es modelar primero el conjunto de datos completo como un grafo ponderado, en el que los nodos del grafo representan los puntos de datos y los pesos en los bordes corresponden a las similitudes entre los puntos en pares. Entonces, las asignaciones de clústeres del conjunto de datos se pueden lograr optimizando algunos criterios definidos en el gráfico. Por ejemplo, el Clustering Espectral es uno de los enfoques de clustering basados en grafos más representativos, generalmente tiene como objetivo optimizar algún valor de corte (por ejemplo, Corte normalizado [22], corte de razón [7], corte mínimo-máximo [11]) definidos en un grafo no dirigido. Después de algunas relajaciones, estos criterios generalmente pueden ser optimizados a través de descomposiciones propias, lo cual está garantizado que sea óptimo a nivel global. De esta manera, el agrupamiento espectral evita eficientemente los problemas de los métodos de particionamiento tradicionales que presentamos en el párrafo anterior. En este artículo, proponemos un algoritmo novedoso de agrupamiento de documentos que hereda la superioridad del agrupamiento espectral, es decir, los resultados finales de los grupos también pueden obtenerse explotando la estructura de los autovalores de una matriz simétrica. Sin embargo, a diferencia del agrupamiento espectral, que simplemente impone una restricción de suavidad en las etiquetas de los datos sobre todo el conjunto de datos [2], nuestro método primero construye un predictor de etiquetas lineal regularizado para cada punto de datos a partir de su vecindario como en [25], y luego combina los resultados de todos estos predictores de etiquetas locales con un regularizador de suavidad de etiquetas global. Así que llamamos a nuestro método Agrupamiento con Regularización Local y Global (CLGR). La idea de incorporar tanto información local como global en la predicción de etiquetas está inspirada en los trabajos recientes sobre aprendizaje semi-supervisado [31], y nuestras evaluaciones experimentales en varios conjuntos de datos reales de documentos muestran que CLGR funciona mejor que muchos métodos de agrupamiento de vanguardia. El resto de este documento está organizado de la siguiente manera: en la sección 2 presentaremos nuestro algoritmo CLGR en detalle. Los resultados experimentales en varios conjuntos de datos se presentan en la sección 3, seguidos de las conclusiones y discusiones en la sección 4. El ALGORITMO PROPUESTO En esta sección, presentaremos en detalle nuestro algoritmo de Agrupamiento con Regularización Local y Global (CLGR). Primero veamos cómo se representan los documentos a lo largo de este documento. 2.1 Representación de documentos En nuestro trabajo, todos los documentos se representan mediante vectores de frecuencia de términos ponderados. Sea W = {w1, w2, · · · , wm} el conjunto completo de vocabulario del corpus de documentos (que ha sido preprocesado mediante la eliminación de palabras vacías y operaciones de derivación de palabras). El vector de frecuencia de término xi del documento di se define como xi = [xi1, xi2, · · · , xim]T , xik = tik log n idfk , donde tik es la frecuencia del término wk ∈ W, n es el tamaño del corpus de documentos, idfk es el número de documentos que contienen la palabra wk. De esta manera, xi también se llama la representación TFIDF del documento di. Además, también normalizamos cada xi (1 i n) para que tenga una longitud unitaria, de modo que cada documento esté representado por un vector TF-IDF normalizado. 2.2 Regularización Local Como su nombre sugiere, CLGR está compuesto por dos partes: regularización local y regularización global. En esta subsección introduciremos detalladamente la parte de regularización local. 2.2.1 Motivación Como sabemos que el agrupamiento es un tipo de técnica de aprendizaje, tiene como objetivo organizar el conjunto de datos de una manera razonable. En general, el aprendizaje puede plantearse como un problema de estimación de funciones, a partir del cual podemos obtener una buena función de clasificación que asignará etiquetas al conjunto de datos de entrenamiento e incluso al conjunto de datos de prueba no vistos con algún costo minimizado [24]. Por ejemplo, en el escenario de clasificación de dos clases (en el que conocemos exactamente la etiqueta de cada documento), un clasificador lineal con ajuste de mínimos cuadrados tiene como objetivo aprender un vector columna w de manera que el costo al cuadrado J = 1 n (wT xi − yi)2 (1) se minimice, donde yi ∈ {+1, −1} es la etiqueta de xi. Al tomar ∂J /∂w = 0, obtenemos la solución w∗ = Σ i=1 n xixT i −1 Σ i=1 n xiyi, (2) que puede escribirse en su forma matricial como w∗ = XXT −1 Xy, (3) donde X = [x1, x2, · · · , xn] es una matriz de documentos m × n, y = [y1, y2, · · · , yn]T es el vector de etiquetas. Entonces, para un documento de prueba t, podemos determinar su etiqueta mediante l = signo(w∗T u), donde signo(·) es la función signo. Un problema natural en la ecuación (3) es que la matriz XXT puede ser singular y, por lo tanto, no invertible (por ejemplo, cuando m n). Para evitar tal problema, podemos agregar un término de regularización y minimizar el siguiente criterio J = 1 n n i=1 (wT xi − yi)2 + λ w 2, (5), donde λ es un parámetro de regularización. Entonces, la solución óptima que minimiza J está dada por w∗ = XXT + λnI −1 Xy, (6) donde I es una matriz identidad de tamaño m × m. Se ha informado que el clasificador lineal regularizado puede lograr resultados muy buenos en problemas de clasificación de texto [29]. Sin embargo, a pesar de su éxito empírico, el clasificador lineal regularizado es en realidad un clasificador global, es decir, w∗ se estima utilizando todo el conjunto de entrenamiento. Según [24], esta puede que no sea una idea inteligente, ya que un único w∗ puede que no sea lo suficientemente bueno para predecir las etiquetas de todo el espacio de entrada. Para obtener predicciones más precisas, [6] propuso entrenar clasificadores localmente y utilizarlos para clasificar los puntos de prueba. Por ejemplo, un punto de prueba será clasificado por el clasificador local entrenado utilizando los puntos de entrenamiento ubicados en la vecindad 1. En las siguientes discusiones todos asumimos que los documentos provienen solo de dos clases. Las generalizaciones de nuestro método a casos de múltiples clases se discutirán en la sección 2.5 de él. Aunque este método parece lento y estúpido, se informa que puede obtener mejores rendimientos que usar un clasificador global único en ciertas tareas [6]. 2.2.2 Construyendo los Predictores Localmente Regularizados Inspirados en su éxito, propusimos aplicar los algoritmos de aprendizaje local para el agrupamiento. La idea básica es que, para cada vector de documento xi (1 i n), entrenamos un predictor de etiquetas local basado en su vecindario k-más cercano Ni, y luego lo usamos para predecir la etiqueta de xi. Finalmente combinaremos todos esos predictores locales minimizando la suma de sus errores de predicción. En esta subsección introduciremos cómo construir esos predictores locales. Debido a la simplicidad y efectividad del clasificador lineal regularizado que hemos introducido en la sección 2.2.1, lo elegimos como nuestro predictor de etiquetas local, de modo que para cada documento xi, se minimiza el siguiente criterio Ji = 1 ni xj ∈Ni wT i xj − qj 2 + λi wi 2, (7), donde ni = |Ni| es la cardinalidad de Ni, y qj es la pertenencia al clúster de xj. Luego, utilizando la Ecuación (6), podemos obtener que la solución óptima es w∗ i = XiXT i + λiniI −1 Xiqi, (8), donde Xi = [xi1, xi2, · · · , xini], y usamos xik para denotar al k-ésimo vecino más cercano de xi. qi = [qi1, qi2, · · · , qini]T con qik representando la asignación de clúster de xik. El problema aquí es que XiXT i es una matriz m × m con m ni, es decir, deberíamos calcular la inversa de una matriz m × m para cada vector de documento, lo cual está prohibido computacionalmente. Afortunadamente, tenemos el siguiente teorema: Teorema 1. w∗ i en la Ec. (8) puede ser reescrito como w∗ i = Xi XT i Xi + λiniIi −1 qi, (9), donde Ii es una matriz identidad de tamaño ni × ni. Prueba. Dado que w∗ i = XiXT i + λiniI −1 Xiqi, entonces XiXT i + λiniI w∗ i = Xiqi =⇒ XiXT i w∗ i + λiniw∗ i = Xiqi =⇒ w∗ i = (λini)−1 Xi qi − XT i w∗ i. Sea β = (λini)−1 qi − XT i w∗ i, entonces w∗ i = Xiβ =⇒ λiniβ = qi − XT i w∗ i = qi − XT i Xiβ =⇒ qi = XT i Xi + λiniIi β =⇒ β = XT i Xi + λiniIi −1 qi. Por lo tanto, w∗ i = Xiβ = Xi XT i Xi + λiniIi −1 qi 2. Utilizando el teorema 1, solo necesitamos calcular la inversa de una matriz ni × ni para cada documento para entrenar un predictor de etiquetas local. Además, para un nuevo punto de prueba u que cae en Ni, podemos clasificarlo por el signo de qu = w∗T i u = uT wi = uT Xi XT i Xi + λiniIi −1 qi. Esta es una expresión atractiva ya que podemos determinar la asignación del clúster de u utilizando los productos internos entre los puntos en {u ∪ Ni}, lo que sugiere que un regularizador local de este tipo puede ser fácilmente kernelizado [21] siempre y cuando definamos una función de kernel adecuada. 2.2.3 Combinando los Predictores Regularizados Localmente Una vez que todos los predictores locales hayan sido construidos, los combinaremos minimizando Jl = n i=1 w∗T i xi − qi 2, (10), lo que representa la suma de los errores de predicción de todos los predictores locales. Combinando la Ecuación (10) con la Ecuación (6), podemos obtener Jl = n i=1 w∗T i xi − qi 2 = n i=1 xT i Xi XT i Xi + λiniIi −1 qi − qi 2 = Pq − q 2, (11), donde q = [q1, q2, · · · , qn]T, y P es una matriz n × n construida de la siguiente manera. Sea αi = xT i Xi XT i Xi + λiniIi −1 , entonces Pij = αi j, si xj ∈ Ni 0, de lo contrario , (12) donde Pij es la entrada (i, j)-ésima de P, y αi j representa la entrada j-ésima de αi. Hasta ahora podemos escribir el criterio de agrupamiento combinando predictores de etiquetas lineales localmente regularizados Jl en una forma matemática explícita, y podemos minimizarlo directamente utilizando algunas técnicas estándar de optimización. Sin embargo, los resultados pueden no ser lo suficientemente buenos ya que solo explotamos la información local del conjunto de datos. En la siguiente subsección, introduciremos un criterio de regularización global y lo combinaremos con Jl, que tiene como objetivo encontrar un buen resultado de agrupamiento de manera local-global. 2.3 Regularización Global En el agrupamiento de datos, generalmente requerimos que las asignaciones de clúster de los puntos de datos sean lo suficientemente suaves con respecto a la variedad de datos subyacente, lo que implica (1) que los puntos cercanos tienden a tener las mismas asignaciones de clúster; (2) que los puntos en la misma estructura (por ejemplo, subvariedad o clúster) tienden a tener las mismas asignaciones de clúster [31]. Sin pérdida de generalidad, asumimos que los puntos de datos residen (aproximadamente) en una variedad de baja dimensión M2, y q es la función de asignación de clúster definida en M, es decir, 2 Creemos que los datos de texto también se muestrean de alguna variedad de baja dimensión, ya que es imposible que para todos los x ∈ M, q(x) devuelva la membresía del clúster de x. La suavidad de q sobre M se puede calcular mediante la siguiente integral de Dirichlet [2] D[q] = 1 2 M q(x) 2 dM, (13) donde el gradiente q es un vector en el espacio tangente T Mx, y la integral se toma con respecto a la medida estándar en M. Si restringimos la escala de q por q, q M = 1 (donde ·, · M es el producto interno inducido en M), entonces resulta que encontrar la función más suave que minimiza D[q] se reduce a encontrar las autofunciones del operador Laplace Beltrami L, que se define como Lq −div q, (14) donde div es la divergencia de un campo vectorial. Generalmente, el gráfico se puede ver como la forma discretizada de una variedad. Podemos modelar el conjunto de datos como un grafo ponderado no dirigido como en el agrupamiento espectral [22], donde los nodos del grafo son simplemente los puntos de datos, y los pesos en las aristas representan las similitudes entre pares de puntos. Entonces se puede demostrar que minimizar la Ec. (13) corresponde a minimizar Jg = qT Lq = n i=1 (qi − qj)2 wij, (15), donde q = [q1, q2, · · · , qn]T con qi = q(xi), L es el Laplaciano del grafo con su entrada (i, j)-ésima Lij =    di − wii, si i = j −wij, si xi y xj son adyacentes 0, en otro caso, (16), donde di = j wij es el grado de xi, wij es la similitud entre xi y xj. Si xi y xj son adyacentes, wij generalmente se calcula de la siguiente manera: wij = e − xi−xj 2 2σ2, donde σ es un parámetro dependiente del conjunto de datos. Se ha demostrado que bajo ciertas condiciones, tal forma de wij para determinar los pesos en los bordes del grafo conduce a la convergencia del Laplaciano del grafo al operador Laplace Beltrami [3][18]. En resumen, el uso de la Ec. (15) con pesos exponenciales puede medir de manera efectiva la suavidad de las asignaciones de datos con respecto a la variedad intrínseca de datos. Por lo tanto, lo adoptamos como un regularizador global para penalizar la suavidad de las asignaciones de datos predichas. 2.4 Agrupamiento con Regularización Local y Global Combinando los contenidos que hemos introducido en la sección 2.2 y la sección 2.3, podemos derivar que el criterio de agrupamiento es minq J = Jl + λJg = Pq − q 2 + λqT Lq sujeto a qi ∈ {−1, +1}, (18) donde P está definido como en la Ecuación (12), y λ es un parámetro de regularización para equilibrar Jl y Jg. Sin embargo, los valores discretos llenan todo el espacio muestral de alta dimensionalidad. Y se ha demostrado que los métodos basados en variedades pueden lograr buenos resultados en tareas de clasificación de texto [31]. En este artículo, definimos xi y xj como adyacentes si xi ∈ N(xj) o xj ∈ N(xi). La restricción de pi convierte el problema en un problema de programación entera NP difícil. Una forma natural de hacer que el problema sea resoluble es eliminar la restricción y relajar qi para que sea continuo, luego el objetivo que buscamos minimizar se convierte en J = Pq − q 2 + λqT Lq = qT (P − I)T (P − I)q + λqT Lq = qT (P − I)T (P − I) + λL q, (19) y luego agregamos una restricción adicional qT q = 1 para restringir la escala de q. Entonces nuestro objetivo se convierte en minq J = qT (P − I)T (P − I) + λL q s.t. qT q = 1 (20) Utilizando el método de Lagrange, podemos derivar que la solución óptima q corresponde al menor vector propio de la matriz M = (P − I)T (P − I) + λL, y la asignación de clúster de xi puede determinarse por el signo de qi, es decir, xi se clasificará como clase uno si qi > 0, de lo contrario se clasificará como clase 2. 2.5 CLGR Multiclase En lo anterior hemos introducido el marco básico de Agrupamiento con Regularización Local y Global (CLGR) para el problema de agrupamiento de dos clases, y lo extenderemos al agrupamiento multiclase en esta subsección. Primero asumimos que todos los documentos pertenecen a C clases indexadas por L = {1, 2, · · · , C}. qc es la función de clasificación para la clase c (1 ≤ c ≤ C), tal que qc(xi) devuelve la confianza de que xi pertenece a la clase c. Nuestro objetivo es obtener el valor de qc(xi) (1 ≤ c ≤ C, 1 ≤ i ≤ n), y la asignación de cluster de xi puede ser determinada por {qc(xi)}C c=1 utilizando algunos métodos adecuados de discretización que presentaremos más adelante. Por lo tanto, en este caso de múltiples clases, para cada documento xi (1 i n), construiremos C predictores de etiquetas regularizados localmente lineales cuyos vectores normales son wc∗ i = Xi XT i Xi + λiniIi −1 qc i (1 c C), (21), donde Xi = [xi1, xi2, · · · , xini ] con xik siendo el k-ésimo vecino de xi, y qc i = [qc i1, qc i2, · · · , qc ini ]T con qc ik = qc (xik). Entonces (wc∗ i )T xi devuelve la confianza predicha de xi perteneciente a la clase c. Por lo tanto, el error de predicción local para la clase c se puede definir como J c l = n i=1 (wc∗ i ) T xi − qc i 2 , (22) Y el error de predicción local total se convierte en Jl = C c=1 J c l = C c=1 n i=1 (wc∗ i ) T xi − qc i 2 . (23) Como en la Ec. (11), podemos definir una matriz n×n P (ver Ec. (12)) y reescribir Jl como Jl = C c=1 J c l = C c=1 Pqc − qc 2 . (24) De manera similar, podemos definir el regularizador de suavidad global en el caso de múltiples clases como Jg = C c=1 n i=1 (qc i − qc j )2 wij = C c=1 (qc )T Lqc . (25) Luego, el criterio a minimizar para CLGR en el caso de múltiples clases se convierte en J = Jl + λJg = C c=1 Pqc − qc 2 + λ(qc )T Lqc = C c=1 (qc )T (P − I)T (P − I) + λL qc = trace QT (P − I)T (P − I) + λL Q , (26) donde Q = [q1 , q2 , · · · , qc ] es una matriz n × c, y trace(·) devuelve la traza de una matriz. Al igual que en la ecuación (20), también agregamos la restricción de que QT Q = I para limitar la escala de Q. Entonces nuestro problema de optimización se convierte en minQ J = traza QT (P − I)T (P − I) + λL Q sujeto a. Según el teorema de Ky Fan [28], sabemos que la solución óptima del problema anterior es Q∗ = [q∗ 1, q∗ 2, · · · , q∗ C ]R, donde q∗ k (1 ≤ k ≤ C) es el autovector que corresponde al k-ésimo valor propio más pequeño de la matriz (P − I)T (P − I) + λL, y R es una matriz arbitraria de tamaño C × C. Dado que los valores de las entradas en Q∗ son continuos, necesitamos discretizar aún más Q∗ para obtener las asignaciones de clúster de todos los puntos de datos. Principalmente hay dos enfoques para lograr este objetivo: 1. Como en [20], podemos tratar la i-ésima fila de Q como la incrustación de xi en un espacio de C dimensiones, y aplicar algunos métodos de agrupamiento tradicionales como kmeans para agrupar estas incrustaciones en C grupos. 2. Dado que el Q∗ óptimo no es único (debido a la existencia de una matriz R arbitraria), podemos buscar un R óptimo que rote Q∗ a una matriz de indicación4. El algoritmo detallado se puede consultar en [26]. El procedimiento detallado del algoritmo para CLGR se resume en la tabla 1. 3. EXPERIMENTOS En esta sección, se realizan experimentos para comparar empíricamente los resultados de agrupamiento de CLGR con otros 8 algoritmos representativos de agrupamiento de documentos en 5 conjuntos de datos. Primero presentaremos la información básica de esos conjuntos de datos. 3.1 Conjuntos de datos Utilizamos una variedad de conjuntos de datos, la mayoría de los cuales son frecuentemente utilizados en la investigación de recuperación de información. La Tabla 2 resume las características de los conjuntos de datos. Aquí, una matriz de indicación T es una matriz n×c con su entrada (i, j) Tij ∈ {0, 1} de modo que para cada fila de Q∗ solo hay un 1. Entonces, el xi puede ser asignado al j-ésimo grupo tal que j = argjQ∗ ij = 1. Tabla 1: Agrupamiento con Regularización Local y Global (CLGR) Entrada: 1. Conjunto de datos X = {xi}n i=1; 2. Número de grupos C: 3. Tamaño del vecindario K: 4. Parámetros locales de regularización {λi}n i=1; 5. Parámetro de regularización global λ; Salida: La membresía del clúster de cada punto de datos. Procedimiento: 1. Construir los K vecindarios más cercanos para cada punto de datos; 2. Construya la matriz P utilizando la Ecuación (12); 3. Construya la matriz Laplaciana L utilizando la ecuación (16); 4. Construye la matriz M = (P − I)T (P − I) + λL; 5. Realice la descomposición de valores propios en M y construya la matriz Q∗ de acuerdo con la Ecuación (28); 6. Generar las asignaciones de clúster de cada punto de datos al discretizar adecuadamente Q∗. Tabla 2: Descripciones de los conjuntos de datos de documentos Conjuntos de datos Número de documentos Número de clases CSTR 476 4 WebKB4 4199 4 Reuters 2900 10 WebACE 2340 20 Newsgroup4 3970 4 CSTR. Este es el conjunto de datos de los resúmenes de informes técnicos publicados en el Departamento de Ciencias de la Computación de una universidad. El conjunto de datos contenía 476 resúmenes, los cuales fueron divididos en cuatro áreas de investigación: Procesamiento de Lenguaje Natural (NLP), Robótica/Visión, Sistemas y Teoría. WebKB. El conjunto de datos WebKB contiene páginas web recopiladas de los departamentos de informática de universidades. Hay alrededor de 8280 documentos y están divididos en 7 categorías: estudiante, facultad, personal, curso, proyecto, departamento y otros. El texto sin procesar es de aproximadamente 27MB. Entre estas 7 categorías, estudiante, facultad, curso y proyecto son las cuatro categorías que representan entidades más pobladas. El subconjunto asociado suele ser llamado WebKB4. Reuters. La colección de pruebas de categorización de texto Reuters-21578 contiene documentos recopilados de la agencia de noticias Reuters en 1987. Es un punto de referencia estándar para la categorización de texto y contiene 135 categorías. En nuestros experimentos, utilizamos un subconjunto de la colección de datos que incluye las 10 categorías más frecuentes entre los 135 temas y lo llamamos Reuters-top 10. WebACE. El conjunto de datos WebACE proviene del proyecto WebACE y ha sido utilizado para la agrupación de documentos [17][5]. El conjunto de datos WebACE contiene 2340 documentos que consisten en artículos de noticias del servicio de noticias de Reuters a través de la web en octubre de 1997. Estos documentos están divididos en 20 clases. Not enough context provided. El conjunto de datos News4 utilizado en nuestros experimentos se selecciona del famoso conjunto de datos 20-newsgroups. El tema rec que contiene autos, motocicletas, béisbol y hockey fue seleccionado de la versión 20news-18828. El conjunto de datos News4 contiene 3970 vectores de documentos. Para preprocesar los conjuntos de datos, eliminamos las palabras vacías utilizando una lista estándar de palabras vacías, se omiten todas las etiquetas HTML y se ignoran todos los campos de encabezado excepto el asunto y la organización de los artículos publicados. En todos nuestros experimentos, primero seleccionamos las 1000 palabras principales por información mutua con las etiquetas de clase. 3.2 Métricas de Evaluación En los experimentos, establecemos el número de clústeres igual al número real de clases C para todos los algoritmos de agrupamiento. Para evaluar su rendimiento, comparamos los grupos generados por estos algoritmos con las clases reales mediante el cálculo de las siguientes dos medidas de rendimiento. Precisión de agrupamiento (Acc). La primera medida de rendimiento es la Precisión de Agrupamiento, que descubre la relación uno a uno entre los grupos y las clases, y mide en qué medida cada grupo contenía puntos de datos de la clase correspondiente. Resume todo el grado de coincidencia entre todos los pares de clases y clusters. La precisión del agrupamiento se puede calcular como: Acc = 1 N max   Ck,Lm T(Ck, Lm)   , (29) donde Ck denota el k-ésimo clúster en los resultados finales, y Lm es la verdadera clase m-ésima. T(Ck, Lm) es el número de entidades que pertenecen a la clase m y están asignadas al clúster k. La precisión calcula la suma máxima de T(Ck, Lm) para todos los pares de clústeres y clases, y estos pares no tienen superposiciones. La mayor precisión de agrupamiento significa un mejor rendimiento de agrupamiento. Información Mutua Normalizada (NMI). Otro métrica de evaluación que adoptamos aquí es la Información Mutua Normalizada (NMI) [23], la cual es ampliamente utilizada para determinar la calidad de los grupos. Para dos variables aleatorias X e Y, el NMI se define como: NMI(X, Y) = I(X, Y) H(X)H(Y), donde I(X, Y) es la información mutua entre X e Y, mientras que H(X) y H(Y) son las entropías de X e Y respectivamente. Se puede ver que NMI(X, X) = 1, que es el valor máximo posible de NMI. Dado un resultado de agrupamiento, el NMI en la ecuación (30) se estima como NMI = C k=1 C m=1 nk,mlog n·nk,m nk ˆnm C k=1 nklog nk n C m=1 ˆnmlog ˆnm n, (31), donde nk denota el número de datos contenidos en el grupo Ck (1 k C), ˆnm es el número de datos pertenecientes a la clase m-ésima (1 m C), y nk,m denota el número de datos que están en la intersección entre el grupo Ck y la clase m-ésima. El valor calculado en la Ecuación (31) se utiliza como medida de rendimiento para el resultado de agrupamiento dado. A mayor valor, mejor rendimiento de agrupamiento. 3.3 Comparaciones Hemos realizado evaluaciones de rendimiento exhaustivas probando nuestro método y comparándolo con otros 8 métodos representativos de agrupamiento de datos utilizando las mismas corpora de datos. Los algoritmos que evaluamos se enumeran a continuación. 1. K-means tradicional (KM). 2. K-medias esférico (SKM). La implementación se basa en [9]. 3. Modelo de Mezcla Gaussiana (GMM). La implementación se basa en [16]. 4. Agrupamiento espectral con cortes normalizados (Ncut). La implementación se basa en [26], y la varianza de la similitud gaussiana se determina mediante Escalado Local [30]. Ten en cuenta que el criterio que Ncut busca minimizar es simplemente el regularizador global en nuestro algoritmo CLGR, excepto que Ncut utiliza el Laplaciano normalizado. 5. Agrupamiento utilizando Regularización Local Pura (CPLR). En este método simplemente minimizamos Jl (definido en la Ecuación (24)), y los resultados de agrupamiento pueden obtenerse realizando la descomposición de valores propios en la matriz (I − P)T (I − P) con algunos métodos de discretización adecuados. 6. Iteración de subespacio adaptativa (ASI). La implementación se basa en [14]. 7. Factorización de matrices no negativas (NMF). La implementación se basa en [27]. 8. Factorización Tri-Factorización de Matrices No Negativas (TNMF) [12]. La implementación se basa en [15]. Para la eficiencia computacional, en la implementación de CPLR y nuestro algoritmo CLGR, hemos establecido todos los parámetros de regularización local {λi}n i=1 para ser idénticos, los cuales se establecen mediante búsqueda en cuadrícula de {0.1, 1, 10}. El tamaño de los vecindarios k más cercanos se establece mediante una búsqueda en cuadrícula de {20, 40, 80}. Para el método CLGR, su parámetro de regularización global se establece mediante una búsqueda en cuadrícula de {0.1, 1, 10}. Al construir el regularizador global, hemos adoptado el método de escalamiento local [30] para construir la matriz Laplaciana. El método de discretización final adoptado en estos dos métodos es el mismo que en [26], ya que nuestros experimentos muestran que el uso de dicho método puede lograr mejores resultados que el uso de métodos basados en kmeans como en [20]. 3.4 Resultados Experimentales Los resultados de comparación de precisión de agrupamiento se muestran en la tabla 3, y los resultados de comparación de información mutua normalizada se resumen en la tabla 4. De las dos tablas principalmente observamos que: 1. Nuestro método CLGR supera a todos los demás métodos de agrupamiento de documentos en la mayoría de los conjuntos de datos. Para la agrupación de documentos, el método Spherical k-means suele superar al método tradicional de agrupación k-means, y el método GMM puede lograr resultados competitivos en comparación con el método Spherical k-means; 3. Los resultados obtenidos de los algoritmos tipo k-means y GMM suelen ser peores que los resultados obtenidos de Clustering Espectral. Dado que el Agrupamiento Espectral puede ser visto como una versión ponderada del kernel k-means, puede obtener buenos resultados cuando los clusters de datos tienen formas arbitrarias. Esto corrobora que los vectores de los documentos no están distribuidos regularmente (esféricos o elípticos). 4. Las comparaciones experimentales verifican empíricamente la equivalencia entre NMF y Clustering Espectral, como se muestra en la Tabla 3: Precisión de agrupamiento de los diversos métodos CSTR WebKB4 Reuters WebACE News4 KM 0.4256 0.3888 0.4448 0.4001 0.3527 SKM 0.4690 0.4318 0.5025 0.4458 0.3912 GMM 0.4487 0.4271 0.4897 0.4521 0.3844 NMF 0.5713 0.4418 0.4947 0.4761 0.4213 Ncut 0.5435 0.4521 0.4896 0.4513 0.4189 ASI 0.5621 0.4752 0.5235 0.4823 0.4335 TNMF 0.6040 0.4832 0.5541 0.5102 0.4613 CPLR 0.5974 0.5020 0.4832 0.5213 0.4890 CLGR 0.6235 0.5228 0.5341 0.5376 0.5102 Tabla 4: Resultados de información mutua normalizada de los diversos métodos CSTR WebKB4 Reuters WebACE News4 KM 0.3675 0.3023 0.4012 0.3864 0.3318 SKM 0.4027 0.4155 0.4587 0.4003 0.4085 GMM 0.4034 0.4093 0.4356 0.4209 0.3994 NMF 0.5235 0.4517 0.4402 0.4359 0.4130 Ncut 0.4833 0.4497 0.4392 0.4289 0.4231 ASI 0.5008 0.4833 0.4769 0.4817 0.4503 TNMF 0.5724 0.5011 0.5132 0.5328 0.4749 CPLR 0.5695 0.5231 0.4402 0.5543 0.4690 CLGR 0.6012 0.5434 0.4935 0.5390 0.4908 ha sido demostrado teóricamente en [10]. Se puede observar en las tablas que NMF y el Agrupamiento Espectral suelen llevar a resultados de agrupamiento similares. 5. Los métodos basados en co-agrupamiento (TNMF y ASI) suelen lograr mejores resultados que los métodos tradicionales basados únicamente en vectores de documentos. Dado que estos métodos realizan una selección implícita de características en cada iteración, proporcionan una métrica adaptativa para medir el vecindario y, por lo tanto, tienden a producir mejores resultados de agrupamiento. 6. Los resultados obtenidos a través de CPLR suelen ser mejores que los resultados obtenidos a través de Agrupamiento Espectral, lo cual respalda la teoría de Vapnik [24] de que a veces los algoritmos de aprendizaje local pueden obtener mejores resultados que los algoritmos de aprendizaje global. Además de los experimentos de comparación mencionados anteriormente, también probamos la sensibilidad de los parámetros de nuestro método. Principalmente hay dos conjuntos de parámetros en nuestro algoritmo CLGR, los parámetros de regularización locales y globales ({λi}n i=1 y λ, como hemos mencionado en la sección 3.3, hemos establecido que todos los λis son idénticos a λ∗ en nuestros experimentos), y el tamaño de los vecindarios. Por lo tanto, también hemos realizado dos series de experimentos: 1. Fijando el tamaño de los vecindarios y probando el rendimiento del agrupamiento con diferentes valores de λ∗ y λ. En este conjunto de experimentos, encontramos que nuestro algoritmo CLGR puede lograr buenos resultados cuando los dos parámetros de regularización no son ni demasiado grandes ni demasiado pequeños. Normalmente nuestro método puede lograr buenos resultados cuando λ∗ y λ están alrededor de 0.1. La Figura 1 nos muestra un ejemplo de prueba en el conjunto de datos WebACE. Fijando los parámetros de regularización local y global, y probando el rendimiento del agrupamiento con diferentes valores de -5, -4.5, -4, -3.5, -3, -5, -4.5, -4, -3.5, -3, 0.35, 0.4, 0.45, 0.5, 0.55 para el parámetro de regularización local (valor logarítmico base 2) y para el parámetro de regularización global (valor logarítmico base 2), se obtuvo una precisión de agrupamiento. Figura 1: Resultados de la prueba de sensibilidad de parámetros en el conjunto de datos WebACE con el tamaño de vecindario fijo en 20, donde el eje x e y representan el valor logarítmico base 2 de λ∗ y λ. En este conjunto de experimentos, encontramos que el vecindario con un tamaño demasiado grande o demasiado pequeño deteriorará todos los resultados finales de agrupamiento. Esto puede entenderse fácilmente ya que cuando el tamaño del vecindario es muy pequeño, los puntos de datos utilizados para entrenar los clasificadores locales pueden no ser suficientes; cuando el tamaño del vecindario es muy grande, los clasificadores entrenados tenderán a ser globales y no podrán capturar las características locales típicas. La Figura 2 nos muestra un ejemplo de prueba en el conjunto de datos WebACE. Por lo tanto, podemos ver que nuestro algoritmo CLGR (1) puede lograr resultados satisfactorios y (2) no es muy sensible a la elección de parámetros, lo que lo hace práctico en aplicaciones del mundo real. CONCLUSIONES Y TRABAJOS FUTUROS En este artículo, desarrollamos un nuevo algoritmo de agrupamiento llamado agrupamiento con regularización local y global. Nuestro método conserva el mérito de los algoritmos de aprendizaje local y el agrupamiento espectral. Nuestros experimentos muestran que el algoritmo propuesto supera a la mayoría de los algoritmos de vanguardia en muchos conjuntos de datos de referencia. En el futuro, nos enfocaremos en la selección de parámetros y los problemas de aceleración del algoritmo CLGR. REFERENCIAS [1] L. Baker y A. McCallum. Agrupamiento distribucional de palabras para clasificación de texto. En Actas de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1998. [2] M. Belkin y P. Niyogi. Mapeo de Eigen de Laplaciano para Reducción de Dimensionalidad y Representación de Datos. Computación Neural, 15 (6):1373-1396. Junio de 2003. [3] M. Belkin y P. Niyogi. Hacia una Fundación Teórica para Métodos de Manifold Basados en Laplacianos. En Actas de la 18ª Conferencia sobre Teoría del Aprendizaje (COLT). 2005. 10 20 30 40 50 60 70 80 90 100 0.35 0.4 0.45 0.5 0.55 tamaño de la precisión de agrupamiento del vecindario Figura 2: Resultados de pruebas de sensibilidad de parámetros en el conjunto de datos WebACE con los parámetros de regularización fijados en 0.1, y el tamaño del vecindario variando de 10 a 100. [4] M. Belkin, P. Niyogi y V. Sindhwani. Regularización de variedades: un marco geométrico para el aprendizaje a partir de ejemplos. Revista de Investigación en Aprendizaje Automático 7, 1-48, 2006. [5] D. Boley. División Direccional Principal. Minería de datos y descubrimiento de conocimiento, 2:325-344, 1998. [6] L. Bottou y V. Vapnik. Algoritmos de aprendizaje local. Computación Neural, 4:888-900, 1992. [7] P. K. Chan, D. F. Schlag y J. Y. Zien. Particionamiento y agrupamiento K-way de corte de razón espectral. IEEE Trans. Diseño Asistido por Computadora, 13:1088-1096, Sep. 1994. [8] D. R. Cutting, D. R. Karger, J. O. Pederson y J. W. Tukey. Dispersión/Recolección: Un enfoque basado en clústeres para explorar grandes colecciones de documentos. En Actas de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1992. [9] I. S. Dhillon y D. S. Modha. Descomposiciones de conceptos para datos de texto grandes y dispersos utilizando agrupamiento. Aprendizaje automático, vol. 42(1), páginas 143-175, enero de 2001. [10] C. Ding, X. Él, y H. Simon. Sobre la equivalencia entre la factorización de matrices no negativas y el agrupamiento espectral. En Actas de la Conferencia de Minería de Datos de SIAM, 2005. [11] C. Ding, X. Él, H. Zha, M. Gu y H. D. Simon. Un algoritmo de corte min-max para particionar grafos y agrupar datos. En Proc. de la 1ra Conferencia Internacional sobre Minería de Datos (ICDM), páginas 107-114, 2001. [12] C. Ding, T. Li, W. Peng y H. Park. Tri-factorizaciones de matrices no negativas ortogonales para agrupamiento. En Actas de la Duodécima Conferencia Internacional de la ACM SIGKDD sobre Descubrimiento de Conocimiento y Minería de Datos, 2006. [13] R. O. Duda, P. E. Hart y D. G. Stork. Clasificación de patrones. John Wiley & Sons, Inc., 2001. [14] T. Li, S. Ma y M. Ogihara. Agrupamiento de documentos a través de la Iteración de Subespacios Adaptativos. En Actas de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2004. [15] T. Li y C. Ding. Las relaciones entre varios métodos de factorización de matrices no negativas para agrupamiento. En Actas de la 6ta Conferencia Internacional sobre Minería de Datos (ICDM). 2006. [16] X. Liu y Y. Gong. Agrupación de documentos con capacidades de refinamiento de clústeres y selección de modelos. En Proc. de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2002. [17] E. Han, D. Boley, M. Gini, R. Gross, K. Hastings, G. Karypis, V. Kumar, B. Mobasher y J. Moore. WebACE: Un agente web para la categorización y exploración de documentos. En Actas de la 2ª Conferencia Internacional sobre Agentes Autónomos (Agents98). ACM Press, 1998. [18] M. Hein, J. Y. Audibert y U. von Luxburg. De Gráficas a Variedades - Consistencia Puntual Débil y Fuerte de los Laplacianos de Gráficas. En Actas de la 18ª Conferencia sobre Teoría del Aprendizaje (COLT), 470-485. 2005. [19] J. Él, M. Lan, C.-L. Tan, S.-Y. Sung, y H.-B. Bajo. Inicialización de algoritmos de refinamiento de clústeres: una revisión y estudio comparativo. En Proc. de Inter. Conferencia Conjunta sobre Redes Neuronales, 2004. [20] A. Y. Ng, M. I. Jordan, Y. Weiss. En el agrupamiento espectral: análisis y un algoritmo. En Avances en Sistemas de Procesamiento de Información Neural 14. 2002. [21] B. Sch¨olkopf y A. Smola. Aprendizaje con Kernels. El MIT Press. Cambridge, Massachusetts. 2002. [22] J. Shi y J. Malik. Cortes normalizados y segmentación de imágenes. IEEE Trans. on Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000. [23] A. Strehl and J. Ghosh.\nIEEE Trans. on Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000. [23] A. Strehl y J. Ghosh. Conjuntos de agrupamientos: un marco de reutilización de conocimiento para combinar múltiples particiones. Revista de Investigación de Aprendizaje Automático, 3:583-617, 2002. [24] V. N. Vapnik. La naturaleza de la teoría del aprendizaje estadístico. Berlín: Springer-Verlag, 1995. [25] Wu, M. y Schölkopf, B. Un enfoque de aprendizaje local para el agrupamiento. En Avances en Sistemas de Procesamiento de Información Neural 18. 2006. [26] S. X. Yu, J. Shi. Agrupamiento espectral multiclase. En Actas de la Conferencia Internacional sobre Visión por Computadora, 2003. [27] W. Xu, X. Liu y Y. Gong. Agrupación de documentos basada en la factorización de matrices no negativas. En Actas de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2003. [28] H. Zha, X. Él, C. Ding, M. Gu y H. Simon. Relajación espectral para el agrupamiento K-means. En NIPS 14. 2001. [29] T. Zhang y F. J. Oles. Categorización de texto basada en métodos de clasificación lineal regularizados. Revista de Recuperación de Información, 4:5-31, 2001. [30] L. Zelnik-Manor y P. Perona. Agrupamiento espectral autoajustable. En NIPS 17. 2005. [31] D. Zhou, O. Bousquet, T. N. Lal, J. Weston y B. Schölkopf. Aprendizaje con Consistencia Local y Global. NIPS 17, 2005. \n\nNIPS 17, 2005. ",
            "candidates": [],
            "error": [
                [
                    "jerarquía de clusters",
                    "jerarquía de clústeres"
                ]
            ]
        },
        "spectrum": {
            "translated_key": "espectro",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Regularized Clustering for Documents ∗ Fei Wang, Changshui Zhang State Key Lab of Intelligent Tech.",
                "and Systems Department of Automation, Tsinghua University Beijing, China, 100084 feiwang03@gmail.com Tao Li School of Computer Science Florida International University Miami, FL 33199, U.S.A. taoli@cs.fiu.edu ABSTRACT In recent years, document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering.",
                "In this paper, we propose a novel method for clustering documents using regularization.",
                "Unlike traditional globally regularized clustering methods, our method first construct a local regularized linear label predictor for each document vector, and then combine all those local regularizers with a global smoothness regularizer.",
                "So we call our algorithm Clustering with Local and Global Regularization (CLGR).",
                "We will show that the cluster memberships of the documents can be achieved by eigenvalue decomposition of a sparse symmetric matrix, which can be efficiently solved by iterative methods.",
                "Finally our experimental evaluations on several datasets are presented to show the superiorities of CLGR over traditional document clustering methods.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Clustering; I.2.6 [Artificial Intelligence]: Learning-Concept Learning General Terms Algorithms 1.",
                "INTRODUCTION Document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering.",
                "A good document clustering approach can assist the computers to automatically organize the document corpus into a meaningful cluster hierarchy for efficient browsing and navigation, which is very valuable for complementing the deficiencies of traditional information retrieval technologies.",
                "As pointed out by [8], the information retrieval needs can be expressed by a <br>spectrum</br> ranged from narrow keyword-matching based search to broad information browsing such as what are the major international events in recent months.",
                "Traditional document retrieval engines tend to fit well with the search end of the <br>spectrum</br>, i.e. they usually provide specified search for documents matching the users query, however, it is hard for them to meet the needs from the rest of the <br>spectrum</br> in which a rather broad or vague information is needed.",
                "In such cases, efficient browsing through a good cluster hierarchy will be definitely helpful.",
                "Generally, document clustering methods can be mainly categorized into two classes: hierarchical methods and partitioning methods.",
                "The hierarchical methods group the data points into a hierarchical tree structure using bottom-up or top-down approaches.",
                "For example, hierarchical agglomerative clustering (HAC) [13] is a typical bottom-up hierarchical clustering method.",
                "It takes each data point as a single cluster to start off with and then builds bigger and bigger clusters by grouping similar data points together until the entire dataset is encapsulated into one final cluster.",
                "On the other hand, partitioning methods decompose the dataset into a number of disjoint clusters which are usually optimal in terms of some predefined criterion functions.",
                "For instance, K-means [13] is a typical partitioning method which aims to minimize the sum of the squared distance between the data points and their corresponding cluster centers.",
                "In this paper, we will focus on the partitioning methods.",
                "As we know that there are two main problems existing in partitioning methods (like Kmeans and Gaussian Mixture Model (GMM) [16]): (1) the predefined criterion is usually non-convex which causes many local optimal solutions; (2) the iterative procedure (e.g. the Expectation Maximization (EM) algorithm) for optimizing the criterions usually makes the final solutions heavily depend on the initializations.",
                "In the last decades, many methods have been proposed to overcome the above problems of the partitioning methods [19][28].",
                "Recently, another type of partitioning methods based on clustering on data graphs have aroused considerable interests in the machine learning and data mining community.",
                "The basic idea behind these methods is to first model the whole dataset as a weighted graph, in which the graph nodes represent the data points, and the weights on the edges correspond to the similarities between pairwise points.",
                "Then the cluster assignments of the dataset can be achieved by optimizing some criterions defined on the graph.",
                "For example Spectral Clustering is one kind of the most representative graph-based clustering approaches, it generally aims to optimize some cut value (e.g.",
                "Normalized Cut [22], Ratio Cut [7], Min-Max Cut [11]) defined on an undirected graph.",
                "After some relaxations, these criterions can usually be optimized via eigen-decompositions, which is guaranteed to be global optimal.",
                "In this way, spectral clustering efficiently avoids the problems of the traditional partitioning methods as we introduced in last paragraph.",
                "In this paper, we propose a novel document clustering algorithm that inherits the superiority of spectral clustering, i.e. the final cluster results can also be obtained by exploit the eigen-structure of a symmetric matrix.",
                "However, unlike spectral clustering, which just enforces a smoothness constraint on the data labels over the whole data manifold [2], our method first construct a regularized linear label predictor for each data point from its neighborhood as in [25], and then combine the results of all these local label predictors with a global label smoothness regularizer.",
                "So we call our method Clustering with Local and Global Regularization (CLGR).",
                "The idea of incorporating both local and global information into label prediction is inspired by the recent works on semi-supervised learning [31], and our experimental evaluations on several real document datasets show that CLGR performs better than many state-of-the-art clustering methods.",
                "The rest of this paper is organized as follows: in section 2 we will introduce our CLGR algorithm in detail.",
                "The experimental results on several datasets are presented in section 3, followed by the conclusions and discussions in section 4. 2.",
                "THE PROPOSED ALGORITHM In this section, we will introduce our Clustering with Local and Global Regularization (CLGR) algorithm in detail.",
                "First lets see the how the documents are represented throughout this paper. 2.1 Document Representation In our work, all the documents are represented by the weighted term-frequency vectors.",
                "Let W = {w1, w2, · · · , wm} be the complete vocabulary set of the document corpus (which is preprocessed by the stopwords removal and words stemming operations).",
                "The term-frequency vector xi of document di is defined as xi = [xi1, xi2, · · · , xim]T , xik = tik log n idfk , where tik is the term frequency of wk ∈ W, n is the size of the document corpus, idfk is the number of documents that contain word wk.",
                "In this way, xi is also called the TFIDF representation of document di.",
                "Furthermore, we also normalize each xi (1 i n) to have a unit length, so that each document is represented by a normalized TF-IDF vector. 2.2 Local Regularization As its name suggests, CLGR is composed of two parts: local regularization and global regularization.",
                "In this subsection we will introduce the local regularization part in detail. 2.2.1 Motivation As we know that clustering is one type of learning techniques, it aims to organize the dataset in a reasonable way.",
                "Generally speaking, learning can be posed as a problem of function estimation, from which we can get a good classification function that will assign labels to the training dataset and even the unseen testing dataset with some cost minimized [24].",
                "For example, in the two-class classification scenario1 (in which we exactly know the label of each document), a linear classifier with least square fit aims to learn a column vector w such that the squared cost J = 1 n (wT xi − yi)2 (1) is minimized, where yi ∈ {+1, −1} is the label of xi.",
                "By taking ∂J /∂w = 0, we get the solution w∗ = n i=1 xixT i −1 n i=1 xiyi , (2) which can further be written in its matrix form as w∗ = XXT −1 Xy, (3) where X = [x1, x2, · · · , xn] is an m × n document matrix, y = [y1, y2, · · · , yn]T is the label vector.",
                "Then for a test document t, we can determine its label by l = sign(w∗T u), (4) where sign(·) is the sign function.",
                "A natural problem in Eq. (3) is that the matrix XXT may be singular and thus not invertable (e.g. when m n).",
                "To avoid such a problem, we can add a regularization term and minimize the following criterion J = 1 n n i=1 (wT xi − yi)2 + λ w 2 , (5) where λ is a regularization parameter.",
                "Then the optimal solution that minimize J is given by w∗ = XXT + λnI −1 Xy, (6) where I is an m × m identity matrix.",
                "It has been reported that the regularized linear classifier can achieve very good results on text classification problems [29].",
                "However, despite its empirical success, the regularized linear classifier is on earth a global classifier, i.e. w∗ is estimated using the whole training set.",
                "According to [24], this may not be a smart idea, since a unique w∗ may not be good enough for predicting the labels of the whole input space.",
                "In order to get better predictions, [6] proposed to train classifiers locally and use them to classify the testing points.",
                "For example, a testing point will be classified by the local classifier trained using the training points located in the vicinity 1 In the following discussions we all assume that the documents coming from only two classes.",
                "The generalizations of our method to multi-class cases will be discussed in section 2.5. of it.",
                "Although this method seems slow and stupid, it is reported that it can get better performances than using a unique global classifier on certain tasks [6]. 2.2.2 Constructing the Local Regularized Predictors Inspired by their success, we proposed to apply the local learning algorithms for clustering.",
                "The basic idea is that, for each document vector xi (1 i n), we train a local label predictor based on its k-nearest neighborhood Ni, and then use it to predict the label of xi.",
                "Finally we will combine all those local predictors by minimizing the sum of their prediction errors.",
                "In this subsection we will introduce how to construct those local predictors.",
                "Due to the simplicity and effectiveness of the regularized linear classifier that we have introduced in section 2.2.1, we choose it to be our local label predictor, such that for each document xi, the following criterion is minimized Ji = 1 ni xj ∈Ni wT i xj − qj 2 + λi wi 2 , (7) where ni = |Ni| is the cardinality of Ni, and qj is the cluster membership of xj.",
                "Then using Eq. (6), we can get the optimal solution is w∗ i = XiXT i + λiniI −1 Xiqi, (8) where Xi = [xi1, xi2, · · · , xini ], and we use xik to denote the k-th nearest neighbor of xi. qi = [qi1, qi2, · · · , qini ]T with qik representing the cluster assignment of xik.",
                "The problem here is that XiXT i is an m × m matrix with m ni, i.e. we should compute the inverse of an m × m matrix for every document vector, which is computationally prohibited.",
                "Fortunately, we have the following theorem: Theorem 1. w∗ i in Eq. (8) can be rewritten as w∗ i = Xi XT i Xi + λiniIi −1 qi, (9) where Ii is an ni × ni identity matrix.",
                "Proof.",
                "Since w∗ i = XiXT i + λiniI −1 Xiqi, then XiXT i + λiniI w∗ i = Xiqi =⇒ XiXT i w∗ i + λiniw∗ i = Xiqi =⇒ w∗ i = (λini)−1 Xi qi − XT i w∗ i .",
                "Let β = (λini)−1 qi − XT i w∗ i , then w∗ i = Xiβ =⇒ λiniβ = qi − XT i w∗ i = qi − XT i Xiβ =⇒ qi = XT i Xi + λiniIi β =⇒ β = XT i Xi + λiniIi −1 qi.",
                "Therefore w∗ i = Xiβ = Xi XT i Xi + λiniIi −1 qi 2 Using theorem 1, we only need to compute the inverse of an ni × ni matrix for every document to train a local label predictor.",
                "Moreover, for a new testing point u that falls into Ni, we can classify it by the sign of qu = w∗T i u = uT wi = uT Xi XT i Xi + λiniIi −1 qi.",
                "This is an attractive expression since we can determine the cluster assignment of u by using the inner-products between the points in {u ∪ Ni}, which suggests that such a local regularizer can easily be kernelized [21] as long as we define a proper kernel function. 2.2.3 Combining the Local Regularized Predictors After all the local predictors having been constructed, we will combine them together by minimizing Jl = n i=1 w∗T i xi − qi 2 , (10) which stands for the sum of the prediction errors for all the local predictors.",
                "Combining Eq. (10) with Eq. (6), we can get Jl = n i=1 w∗T i xi − qi 2 = n i=1 xT i Xi XT i Xi + λiniIi −1 qi − qi 2 = Pq − q 2 , (11) where q = [q1, q2, · · · , qn]T , and the P is an n × n matrix constructing in the following way.",
                "Let αi = xT i Xi XT i Xi + λiniIi −1 , then Pij = αi j, if xj ∈ Ni 0, otherwise , (12) where Pij is the (i, j)-th entry of P, and αi j represents the j-th entry of αi .",
                "Till now we can write the criterion of clustering by combining locally regularized linear label predictors Jl in an explicit mathematical form, and we can minimize it directly using some standard optimization techniques.",
                "However, the results may not be good enough since we only exploit the local informations of the dataset.",
                "In the next subsection, we will introduce a global regularization criterion and combine it with Jl, which aims to find a good clustering result in a local-global way. 2.3 Global Regularization In data clustering, we usually require that the cluster assignments of the data points should be sufficiently smooth with respect to the underlying data manifold, which implies (1) the nearby points tend to have the same cluster assignments; (2) the points on the same structure (e.g. submanifold or cluster) tend to have the same cluster assignments [31].",
                "Without the loss of generality, we assume that the data points reside (roughly) on a low-dimensional manifold M2 , and q is the cluster assignment function defined on M, i.e. 2 We believe that the text data are also sampled from some low dimensional manifold, since it is impossible for them to for ∀x ∈ M, q(x) returns the cluster membership of x.",
                "The smoothness of q over M can be calculated by the following Dirichlet integral [2] D[q] = 1 2 M q(x) 2 dM, (13) where the gradient q is a vector in the tangent space T Mx, and the integral is taken with respect to the standard measure on M. If we restrict the scale of q by q, q M = 1 (where ·, · M is the inner product induced on M), then it turns out that finding the smoothest function minimizing D[q] reduces to finding the eigenfunctions of the Laplace Beltrami operator L, which is defined as Lq −div q, (14) where div is the divergence of a vector field.",
                "Generally, the graph can be viewed as the discretized form of manifold.",
                "We can model the dataset as an weighted undirected graph as in spectral clustering [22], where the graph nodes are just the data points, and the weights on the edges represent the similarities between pairwise points.",
                "Then it can be shown that minimizing Eq. (13) corresponds to minimizing Jg = qT Lq = n i=1 (qi − qj)2 wij, (15) where q = [q1, q2, · · · , qn]T with qi = q(xi), L is the graph Laplacian with its (i, j)-th entry Lij =    di − wii, if i = j −wij, if xi and xj are adjacent 0, otherwise, (16) where di = j wij is the degree of xi, wij is the similarity between xi and xj.",
                "If xi and xj are adjacent3 , wij is usually computed in the following way wij = e − xi−xj 2 2σ2 , (17) where σ is a dataset dependent parameter.",
                "It is proved that under certain conditions, such a form of wij to determine the weights on graph edges leads to the convergence of graph Laplacian to the Laplace Beltrami operator [3][18].",
                "In summary, using Eq. (15) with exponential weights can effectively measure the smoothness of the data assignments with respect to the intrinsic data manifold.",
                "Thus we adopt it as a global regularizer to punish the smoothness of the predicted data assignments. 2.4 Clustering with Local and Global Regularization Combining the contents we have introduced in section 2.2 and section 2.3 we can derive the clustering criterion is minq J = Jl + λJg = Pq − q 2 + λqT Lq s.t. qi ∈ {−1, +1}, (18) where P is defined as in Eq. (12), and λ is a regularization parameter to trade off Jl and Jg.",
                "However, the discrete fill in the whole high-dimensional sample space.",
                "And it has been shown that the manifold based methods can achieve good results on text classification tasks [31]. 3 In this paper, we define xi and xj to be adjacent if xi ∈ N(xj) or xj ∈ N(xi). constraint of pi makes the problem an NP hard integer programming problem.",
                "A natural way for making the problem solvable is to remove the constraint and relax qi to be continuous, then the objective that we aims to minimize becomes J = Pq − q 2 + λqT Lq = qT (P − I)T (P − I)q + λqT Lq = qT (P − I)T (P − I) + λL q, (19) and we further add a constraint qT q = 1 to restrict the scale of q.",
                "Then our objective becomes minq J = qT (P − I)T (P − I) + λL q s.t. qT q = 1 (20) Using the Lagrangian method, we can derive that the optimal solution q corresponds to the smallest eigenvector of the matrix M = (P − I)T (P − I) + λL, and the cluster assignment of xi can be determined by the sign of qi, i.e. xi will be classified as class one if qi > 0, otherwise it will be classified as class 2. 2.5 Multi-Class CLGR In the above we have introduced the basic framework of Clustering with Local and Global Regularization (CLGR) for the two-class clustering problem, and we will extending it to multi-class clustering in this subsection.",
                "First we assume that all the documents belong to C classes indexed by L = {1, 2, · · · , C}. qc is the classification function for class c (1 c C), such that qc (xi) returns the confidence that xi belongs to class c. Our goal is to obtain the value of qc (xi) (1 c C, 1 i n), and the cluster assignment of xi can be determined by {qc (xi)}C c=1 using some proper discretization methods that we will introduce later.",
                "Therefore, in this multi-class case, for each document xi (1 i n), we will construct C locally linear regularized label predictors whose normal vectors are wc∗ i = Xi XT i Xi + λiniIi −1 qc i (1 c C), (21) where Xi = [xi1, xi2, · · · , xini ] with xik being the k-th neighbor of xi, and qc i = [qc i1, qc i2, · · · , qc ini ]T with qc ik = qc (xik).",
                "Then (wc∗ i )T xi returns the predicted confidence of xi belonging to class c. Hence the local prediction error for class c can be defined as J c l = n i=1 (wc∗ i ) T xi − qc i 2 , (22) And the total local prediction error becomes Jl = C c=1 J c l = C c=1 n i=1 (wc∗ i ) T xi − qc i 2 . (23) As in Eq. (11), we can define an n×n matrix P (see Eq. (12)) and rewrite Jl as Jl = C c=1 J c l = C c=1 Pqc − qc 2 . (24) Similarly we can define the global smoothness regularizer in multi-class case as Jg = C c=1 n i=1 (qc i − qc j )2 wij = C c=1 (qc )T Lqc . (25) Then the criterion to be minimized for CLGR in multi-class case becomes J = Jl + λJg = C c=1 Pqc − qc 2 + λ(qc )T Lqc = C c=1 (qc )T (P − I)T (P − I) + λL qc = trace QT (P − I)T (P − I) + λL Q , (26) where Q = [q1 , q2 , · · · , qc ] is an n × c matrix, and trace(·) returns the trace of a matrix.",
                "The same as in Eq. (20), we also add the constraint that QT Q = I to restrict the scale of Q.",
                "Then our optimization problem becomes minQ J = trace QT (P − I)T (P − I) + λL Q s.t.",
                "QT Q = I, (27) From the Ky Fan theorem [28], we know the optimal solution of the above problem is Q∗ = [q∗ 1, q∗ 2, · · · , q∗ C ]R, (28) where q∗ k (1 k C) is the eigenvector corresponds to the k-th smallest eigenvalue of matrix (P − I)T (P − I) + λL, and R is an arbitrary C × C matrix.",
                "Since the values of the entries in Q∗ is continuous, we need to further discretize Q∗ to get the cluster assignments of all the data points.",
                "There are mainly two approaches to achieve this goal: 1.",
                "As in [20], we can treat the i-th row of Q as the embedding of xi in a C-dimensional space, and apply some traditional clustering methods like kmeans to clustering these embeddings into C clusters. 2.",
                "Since the optimal Q∗ is not unique (because of the existence of an arbitrary matrix R), we can pursue an optimal R that will rotate Q∗ to an indication matrix4 .",
                "The detailed algorithm can be referred to [26].",
                "The detailed algorithm procedure for CLGR is summarized in table 1. 3.",
                "EXPERIMENTS In this section, experiments are conducted to empirically compare the clustering results of CLGR with other 8 representitive document clustering algorithms on 5 datasets.",
                "First we will introduce the basic informations of those datasets. 3.1 Datasets We use a variety of datasets, most of which are frequently used in the information retrieval research.",
                "Table 2 summarizes the characteristics of the datasets. 4 Here an indication matrix T is a n×c matrix with its (i, j)th entry Tij ∈ {0, 1} such that for each row of Q∗ there is only one 1.",
                "Then the xi can be assigned to the j-th cluster such that j = argjQ∗ ij = 1.",
                "Table 1: Clustering with Local and Global Regularization (CLGR) Input: 1.",
                "Dataset X = {xi}n i=1; 2.",
                "Number of clusters C; 3.",
                "Size of the neighborhood K; 4.",
                "Local regularization parameters {λi}n i=1; 5.",
                "Global regularization parameter λ; Output: The cluster membership of each data point.",
                "Procedure: 1.",
                "Construct the K nearest neighborhoods for each data point; 2.",
                "Construct the matrix P using Eq. (12); 3.",
                "Construct the Laplacian matrix L using Eq. (16); 4.",
                "Construct the matrix M = (P − I)T (P − I) + λL; 5.",
                "Do eigenvalue decomposition on M, and construct the matrix Q∗ according to Eq. (28); 6.",
                "Output the cluster assignments of each data point by properly discretize Q∗ .",
                "Table 2: Descriptions of the document datasets Datasets Number of documents Number of classes CSTR 476 4 WebKB4 4199 4 Reuters 2900 10 WebACE 2340 20 Newsgroup4 3970 4 CSTR.",
                "This is the dataset of the abstracts of technical reports published in the Department of Computer Science at a university.",
                "The dataset contained 476 abstracts, which were divided into four research areas: Natural Language Processing(NLP), Robotics/Vision, Systems, and Theory.",
                "WebKB.",
                "The WebKB dataset contains webpages gathered from university computer science departments.",
                "There are about 8280 documents and they are divided into 7 categories: student, faculty, staff, course, project, department and other.",
                "The raw text is about 27MB.",
                "Among these 7 categories, student, faculty, course and project are four most populous entity-representing categories.",
                "The associated subset is typically called WebKB4.",
                "Reuters.",
                "The Reuters-21578 Text Categorization Test collection contains documents collected from the Reuters newswire in 1987.",
                "It is a standard text categorization benchmark and contains 135 categories.",
                "In our experiments, we use a subset of the data collection which includes the 10 most frequent categories among the 135 topics and we call it Reuters-top 10.",
                "WebACE.",
                "The WebACE dataset was from WebACE project and has been used for document clustering [17][5].",
                "The WebACE dataset contains 2340 documents consisting news articles from Reuters new service via the Web in October 1997.",
                "These documents are divided into 20 classes.",
                "News4.",
                "The News4 dataset used in our experiments are selected from the famous 20-newsgroups dataset5 .",
                "The topic rec containing autos, motorcycles, baseball and hockey was selected from the version 20news-18828.",
                "The News4 dataset contains 3970 document vectors. 5 http://people.csail.mit.edu/jrennie/20Newsgroups/ To pre-process the datasets, we remove the stop words using a standard stop list, all HTML tags are skipped and all header fields except subject and organization of the posted articles are ignored.",
                "In all our experiments, we first select the top 1000 words by mutual information with class labels. 3.2 Evaluation Metrics In the experiments, we set the number of clusters equal to the true number of classes C for all the clustering algorithms.",
                "To evaluate their performance, we compare the clusters generated by these algorithms with the true classes by computing the following two performance measures.",
                "Clustering Accuracy (Acc).",
                "The first performance measure is the Clustering Accuracy, which discovers the one-toone relationship between clusters and classes and measures the extent to which each cluster contained data points from the corresponding class.",
                "It sums up the whole matching degree between all pair class-clusters.",
                "Clustering accuracy can be computed as: Acc = 1 N max   Ck,Lm T(Ck, Lm)   , (29) where Ck denotes the k-th cluster in the final results, and Lm is the true m-th class.",
                "T(Ck, Lm) is the number of entities which belong to class m are assigned to cluster k. Accuracy computes the maximum sum of T(Ck, Lm) for all pairs of clusters and classes, and these pairs have no overlaps.",
                "The greater clustering accuracy means the better clustering performance.",
                "Normalized Mutual Information (NMI).",
                "Another evaluation metric we adopt here is the Normalized Mutual Information NMI [23], which is widely used for determining the quality of clusters.",
                "For two random variable X and Y, the NMI is defined as: NMI(X, Y) = I(X, Y) H(X)H(Y) , (30) where I(X, Y) is the mutual information between X and Y, while H(X) and H(Y) are the entropies of X and Y respectively.",
                "One can see that NMI(X, X) = 1, which is the maximal possible value of NMI.",
                "Given a clustering result, the NMI in Eq. (30) is estimated as NMI = C k=1 C m=1 nk,mlog n·nk,m nk ˆnm C k=1 nklog nk n C m=1 ˆnmlog ˆnm n , (31) where nk denotes the number of data contained in the cluster Ck (1 k C), ˆnm is the number of data belonging to the m-th class (1 m C), and nk,m denotes the number of data that are in the intersection between the cluster Ck and the m-th class.",
                "The value calculated in Eq. (31) is used as a performance measure for the given clustering result.",
                "The larger this value, the better the clustering performance. 3.3 Comparisons We have conducted comprehensive performance evaluations by testing our method and comparing it with 8 other representative data clustering methods using the same data corpora.",
                "The algorithms that we evaluated are listed below. 1.",
                "Traditional k-means (KM). 2.",
                "Spherical k-means (SKM).",
                "The implementation is based on [9]. 3.",
                "Gaussian Mixture Model (GMM).",
                "The implementation is based on [16]. 4.",
                "Spectral Clustering with Normalized Cuts (Ncut).",
                "The implementation is based on [26], and the variance of the Gaussian similarity is determined by Local Scaling [30].",
                "Note that the criterion that Ncut aims to minimize is just the global regularizer in our CLGR algorithm except that Ncut used the normalized Laplacian. 5.",
                "Clustering using Pure Local Regularization (CPLR).",
                "In this method we just minimize Jl (defined in Eq. (24)), and the clustering results can be obtained by doing eigenvalue decomposition on matrix (I − P)T (I − P) with some proper discretization methods. 6.",
                "Adaptive Subspace Iteration (ASI).",
                "The implementation is based on [14]. 7.",
                "Nonnegative Matrix Factorization (NMF).",
                "The implementation is based on [27]. 8.",
                "Tri-Factorization Nonnegative Matrix Factorization (TNMF) [12].",
                "The implementation is based on [15].",
                "For computational efficiency, in the implementation of CPLR and our CLGR algorithm, we have set all the local regularization parameters {λi}n i=1 to be identical, which is set by grid search from {0.1, 1, 10}.",
                "The size of the k-nearest neighborhoods is set by grid search from {20, 40, 80}.",
                "For the CLGR method, its global regularization parameter is set by grid search from {0.1, 1, 10}.",
                "When constructing the global regularizer, we have adopted the local scaling method [30] to construct the Laplacian matrix.",
                "The final discretization method adopted in these two methods is the same as in [26], since our experiments show that using such method can achieve better results than using kmeans based methods as in [20]. 3.4 Experimental Results The clustering accuracies comparison results are shown in table 3, and the normalized mutual information comparison results are summarized in table 4.",
                "From the two tables we mainly observe that: 1.",
                "Our CLGR method outperforms all other document clustering methods in most of the datasets; 2.",
                "For document clustering, the Spherical k-means method usually outperforms the traditional k-means clustering method, and the GMM method can achieve competitive results compared to the Spherical k-means method; 3.",
                "The results achieved from the k-means and GMM type algorithms are usually worse than the results achieved from Spectral Clustering.",
                "Since Spectral Clustering can be viewed as a weighted version of kernel k-means, it can obtain good results the data clusters are arbitrarily shaped.",
                "This corroborates that the documents vectors are not regularly distributed (spherical or elliptical). 4.",
                "The experimental comparisons empirically verify the equivalence between NMF and Spectral Clustering, which Table 3: Clustering accuracies of the various methods CSTR WebKB4 Reuters WebACE News4 KM 0.4256 0.3888 0.4448 0.4001 0.3527 SKM 0.4690 0.4318 0.5025 0.4458 0.3912 GMM 0.4487 0.4271 0.4897 0.4521 0.3844 NMF 0.5713 0.4418 0.4947 0.4761 0.4213 Ncut 0.5435 0.4521 0.4896 0.4513 0.4189 ASI 0.5621 0.4752 0.5235 0.4823 0.4335 TNMF 0.6040 0.4832 0.5541 0.5102 0.4613 CPLR 0.5974 0.5020 0.4832 0.5213 0.4890 CLGR 0.6235 0.5228 0.5341 0.5376 0.5102 Table 4: Normalized mutual information results of the various methods CSTR WebKB4 Reuters WebACE News4 KM 0.3675 0.3023 0.4012 0.3864 0.3318 SKM 0.4027 0.4155 0.4587 0.4003 0.4085 GMM 0.4034 0.4093 0.4356 0.4209 0.3994 NMF 0.5235 0.4517 0.4402 0.4359 0.4130 Ncut 0.4833 0.4497 0.4392 0.4289 0.4231 ASI 0.5008 0.4833 0.4769 0.4817 0.4503 TNMF 0.5724 0.5011 0.5132 0.5328 0.4749 CPLR 0.5695 0.5231 0.4402 0.5543 0.4690 CLGR 0.6012 0.5434 0.4935 0.5390 0.4908 has been proved theoretically in [10].",
                "It can be observed from the tables that NMF and Spectral Clustering usually lead to similar clustering results. 5.",
                "The co-clustering based methods (TNMF and ASI) can usually achieve better results than traditional purely document vector based methods.",
                "Since these methods perform an implicit feature selection at each iteration, provide an adaptive metric for measuring the neighborhood, and thus tend to yield better clustering results. 6.",
                "The results achieved from CPLR are usually better than the results achieved from Spectral Clustering, which supports Vapniks theory [24] that sometimes local learning algorithms can obtain better results than global learning algorithms.",
                "Besides the above comparison experiments, we also test the parameter sensibility of our method.",
                "There are mainly two sets of parameters in our CLGR algorithm, the local and global regularization parameters ({λi}n i=1 and λ, as we have said in section 3.3, we have set all λis to be identical to λ∗ in our experiments), and the size of the neighborhoods.",
                "Therefore we have also done two sets of experiments: 1.",
                "Fixing the size of the neighborhoods, and testing the clustering performance with varying λ∗ and λ.",
                "In this set of experiments, we find that our CLGR algorithm can achieve good results when the two regularization parameters are neither too large nor too small.",
                "Typically our method can achieve good results when λ∗ and λ are around 0.1.",
                "Figure 1 shows us such a testing example on the WebACE dataset. 2.",
                "Fixing the local and global regularization parameters, and testing the clustering performance with different −5 −4.5 −4 −3.5 −3 −5 −4.5 −4 −3.5 −3 0.35 0.4 0.45 0.5 0.55 local regularization para (log 2 value) global regularization para (log 2 value) clusteringaccuracy Figure 1: Parameter sensibility testing results on the WebACE dataset with the neighborhood size fixed to 20, and the x-axis and y-axis represents the log2 value of λ∗ and λ. sizes of neighborhoods.",
                "In this set of experiments, we find that the neighborhood with a too large or too small size will all deteriorate the final clustering results.",
                "This can be easily understood since when the neighborhood size is very small, then the data points used for training the local classifiers may not be sufficient; when the neighborhood size is very large, the trained classifiers will tend to be global and cannot capture the typical local characteristics.",
                "Figure 2 shows us a testing example on the WebACE dataset.",
                "Therefore, we can see that our CLGR algorithm (1) can achieve satisfactory results and (2) is not very sensitive to the choice of parameters, which makes it practical in real world applications. 4.",
                "CONCLUSIONS AND FUTURE WORKS In this paper, we derived a new clustering algorithm called clustering with local and global regularization.",
                "Our method preserves the merit of local learning algorithms and spectral clustering.",
                "Our experiments show that the proposed algorithm outperforms most of the state of the art algorithms on many benchmark datasets.",
                "In the future, we will focus on the parameter selection and acceleration issues of the CLGR algorithm. 5.",
                "REFERENCES [1] L. Baker and A. McCallum.",
                "Distributional Clustering of Words for Text Classification.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 1998. [2] M. Belkin and P. Niyogi.",
                "Laplacian Eigenmaps for Dimensionality Reduction and Data Representation.",
                "Neural Computation, 15 (6):1373-1396.",
                "June 2003. [3] M. Belkin and P. Niyogi.",
                "Towards a Theoretical Foundation for Laplacian-Based Manifold Methods.",
                "In Proceedings of the 18th Conference on Learning Theory (COLT). 2005. 10 20 30 40 50 60 70 80 90 100 0.35 0.4 0.45 0.5 0.55 size of the neighborhood clusteringaccuracy Figure 2: Parameter sensibility testing results on the WebACE dataset with the regularization parameters being fixed to 0.1, and the neighborhood size varing from 10 to 100. [4] M. Belkin, P. Niyogi and V. Sindhwani.",
                "Manifold Regularization: a Geometric Framework for Learning from Examples.",
                "Journal of Machine Learning Research 7, 1-48, 2006. [5] D. Boley.",
                "Principal Direction Divisive Partitioning.",
                "Data mining and knowledge discovery, 2:325-344, 1998. [6] L. Bottou and V. Vapnik.",
                "Local learning algorithms.",
                "Neural Computation, 4:888-900, 1992. [7] P. K. Chan, D. F. Schlag and J. Y. Zien.",
                "Spectral K-way Ratio-Cut Partitioning and Clustering.",
                "IEEE Trans.",
                "Computer-Aided Design, 13:1088-1096, Sep. 1994. [8] D. R. Cutting, D. R. Karger, J. O. Pederson and J. W. Tukey.",
                "Scatter/Gather: A Cluster-Based Approach to Browsing Large Document Collections.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 1992. [9] I. S. Dhillon and D. S. Modha.",
                "Concept Decompositions for Large Sparse Text Data using Clustering.",
                "Machine Learning, vol. 42(1), pages 143-175, January 2001. [10] C. Ding, X.",
                "He, and H. Simon.",
                "On the equivalence of nonnegative matrix factorization and spectral clustering.",
                "In Proceedings of the SIAM Data Mining Conference, 2005. [11] C. Ding, X.",
                "He, H. Zha, M. Gu, and H. D. Simon.",
                "A min-max cut algorithm for graph partitioning and data clustering.",
                "In Proc. of the 1st International Conference on Data Mining (ICDM), pages 107-114, 2001. [12] C. Ding, T. Li, W. Peng, and H. Park.",
                "Orthogonal Nonnegative Matrix Tri-Factorizations for Clustering.",
                "In Proceedings of the Twelfth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2006. [13] R. O. Duda, P. E. Hart, and D. G. Stork.",
                "Pattern Classification.",
                "John Wiley & Sons, Inc., 2001. [14] T. Li, S. Ma, and M. Ogihara.",
                "Document Clustering via Adaptive Subspace Iteration.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2004. [15] T. Li and C. Ding.",
                "The Relationships Among Various Nonnegative Matrix Factorization Methods for Clustering.",
                "In Proceedings of the 6th International Conference on Data Mining (ICDM). 2006. [16] X. Liu and Y. Gong.",
                "Document Clustering with Cluster Refinement and Model Selection Capabilities.",
                "In Proc. of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002. [17] E. Han, D. Boley, M. Gini, R. Gross, K. Hastings, G. Karypis, V. Kumar, B. Mobasher, and J. Moore.",
                "WebACE: A Web Agent for Document Categorization and Exploration.",
                "In Proceedings of the 2nd International Conference on Autonomous Agents (Agents98).",
                "ACM Press, 1998. [18] M. Hein, J. Y. Audibert, and U. von Luxburg.",
                "From Graphs to Manifolds - Weak and Strong Pointwise Consistency of Graph Laplacians.",
                "In Proceedings of the 18th Conference on Learning Theory (COLT), 470-485. 2005. [19] J.",
                "He, M. Lan, C.-L. Tan, S.-Y.",
                "Sung, and H.-B.",
                "Low.",
                "Initialization of Cluster Refinement Algorithms: A Review and Comparative Study.",
                "In Proc. of Inter.",
                "Joint Conference on Neural Networks, 2004. [20] A. Y. Ng, M. I. Jordan, Y. Weiss.",
                "On Spectral Clustering: Analysis and an algorithm.",
                "In Advances in Neural Information Processing Systems 14. 2002. [21] B. Sch¨olkopf and A. Smola.",
                "Learning with Kernels.",
                "The MIT Press.",
                "Cambridge, Massachusetts. 2002. [22] J. Shi and J. Malik.",
                "Normalized Cuts and Image Segmentation.",
                "IEEE Trans. on Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000. [23] A. Strehl and J. Ghosh.",
                "Cluster Ensembles - A Knowledge Reuse Framework for Combining Multiple Partitions.",
                "Journal of Machine Learning Research, 3:583-617, 2002. [24] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Berlin: Springer-Verlag, 1995. [25] Wu, M. and Sch¨olkopf, B.",
                "A Local Learning Approach for Clustering.",
                "In Advances in Neural Information Processing Systems 18. 2006. [26] S. X. Yu, J. Shi.",
                "Multiclass Spectral Clustering.",
                "In Proceedings of the International Conference on Computer Vision, 2003. [27] W. Xu, X. Liu and Y. Gong.",
                "Document Clustering Based On Non-Negative Matrix Factorization.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2003. [28] H. Zha, X.",
                "He, C. Ding, M. Gu and H. Simon.",
                "Spectral Relaxation for K-means Clustering.",
                "In NIPS 14. 2001. [29] T. Zhang and F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Journal of Information Retrieval, 4:5-31, 2001. [30] L. Zelnik-Manor and P. Perona.",
                "Self-Tuning Spectral Clustering.",
                "In NIPS 17. 2005. [31] D. Zhou, O. Bousquet, T. N. Lal, J. Weston and B. Sch¨olkopf.",
                "Learning with Local and Global Consistency.",
                "NIPS 17, 2005."
            ],
            "original_annotated_samples": [
                "As pointed out by [8], the information retrieval needs can be expressed by a <br>spectrum</br> ranged from narrow keyword-matching based search to broad information browsing such as what are the major international events in recent months.",
                "Traditional document retrieval engines tend to fit well with the search end of the <br>spectrum</br>, i.e. they usually provide specified search for documents matching the users query, however, it is hard for them to meet the needs from the rest of the <br>spectrum</br> in which a rather broad or vague information is needed."
            ],
            "translated_annotated_samples": [
                "Como señaló [8], las necesidades de recuperación de información pueden expresarse mediante un <br>espectro</br> que va desde la búsqueda basada en palabras clave estrechas hasta la exploración de información amplia, como cuáles son los principales eventos internacionales de los últimos meses.",
                "Los motores de búsqueda de documentos tradicionales tienden a adaptarse bien al extremo de la búsqueda, es decir, suelen proporcionar búsquedas específicas de documentos que coinciden con la consulta de los usuarios, sin embargo, les resulta difícil satisfacer las necesidades del resto del <br>espectro</br> en el que se necesita información más amplia o vaga."
            ],
            "translated_text": "Agrupamiento regularizado para documentos ∗ Fei Wang, Changshui Zhang Laboratorio Clave del Estado de Tecnología Inteligente. En los últimos años, la agrupación de documentos ha estado recibiendo cada vez más atención como una técnica importante y fundamental para la organización no supervisada de documentos, la extracción automática de temas y la recuperación o filtrado rápido de información. En este artículo, proponemos un método novedoso para agrupar documentos utilizando regularización. A diferencia de los métodos de agrupamiento regularizados globalmente tradicionales, nuestro método primero construye un predictor de etiquetas lineal regularizado local para cada vector de documento, y luego combina todos esos regularizadores locales con un regularizador de suavidad global. Así que llamamos a nuestro algoritmo Agrupamiento con Regularización Local y Global (CLGR). Mostraremos que las pertenencias de los documentos a los grupos se pueden lograr mediante la descomposición de los valores propios de una matriz simétrica dispersa, la cual puede resolverse eficientemente mediante métodos iterativos. Finalmente, se presentan nuestras evaluaciones experimentales en varios conjuntos de datos para mostrar las ventajas de CLGR sobre los métodos tradicionales de agrupamiento de documentos. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información-Agrupamiento; I.2.6 [Inteligencia Artificial]: Aprendizaje-Aprendizaje de Conceptos Términos Generales Algoritmos 1. La agrupación de documentos ha estado recibiendo cada vez más atención como una técnica importante y fundamental para la organización no supervisada de documentos, la extracción automática de temas y la recuperación o filtrado rápido de información. Un buen enfoque de agrupación de documentos puede ayudar a las computadoras a organizar automáticamente el corpus de documentos en una jerarquía de clusters significativa para una navegación eficiente, lo cual es muy valioso para complementar las deficiencias de las tecnologías tradicionales de recuperación de información. Como señaló [8], las necesidades de recuperación de información pueden expresarse mediante un <br>espectro</br> que va desde la búsqueda basada en palabras clave estrechas hasta la exploración de información amplia, como cuáles son los principales eventos internacionales de los últimos meses. Los motores de búsqueda de documentos tradicionales tienden a adaptarse bien al extremo de la búsqueda, es decir, suelen proporcionar búsquedas específicas de documentos que coinciden con la consulta de los usuarios, sin embargo, les resulta difícil satisfacer las necesidades del resto del <br>espectro</br> en el que se necesita información más amplia o vaga. En tales casos, la navegación eficiente a través de una buena jerarquía de clústeres será definitivamente útil. Generalmente, los métodos de agrupamiento de documentos se pueden clasificar principalmente en dos clases: métodos jerárquicos y métodos de partición. Los métodos jerárquicos agrupan los puntos de datos en una estructura de árbol jerárquico utilizando enfoques de abajo hacia arriba o de arriba hacia abajo. Por ejemplo, el clustering jerárquico aglomerativo (HAC) [13] es un método típico de clustering jerárquico ascendente. Toma cada punto de datos como un único clúster para comenzar y luego construye clústeres más grandes agrupando puntos de datos similares juntos hasta que todo el conjunto de datos esté encapsulado en un único clúster final. Por otro lado, los métodos de particionamiento descomponen el conjunto de datos en un número de grupos disjuntos que suelen ser óptimos en términos de algunas funciones de criterio predefinidas. Por ejemplo, K-means es un método de particionamiento típico que tiene como objetivo minimizar la suma de las distancias al cuadrado entre los puntos de datos y sus centros de clúster correspondientes. En este documento, nos centraremos en los métodos de particionamiento. Como sabemos, existen dos problemas principales en los métodos de particionamiento (como Kmeans y el Modelo de Mezcla Gaussiana (GMM) [16]): (1) el criterio predefinido suele ser no convexo, lo que provoca muchas soluciones óptimas locales; (2) el procedimiento iterativo (por ejemplo, el algoritmo de Expectation Maximization (EM)) para optimizar los criterios suele hacer que las soluciones finales dependan en gran medida de las inicializaciones. En las últimas décadas, se han propuesto muchos métodos para superar los problemas mencionados de los métodos de particionamiento [19][28]. Recientemente, otro tipo de métodos de particionamiento basados en agrupamiento en grafos de datos han despertado un considerable interés en la comunidad de aprendizaje automático y minería de datos. La idea básica detrás de estos métodos es modelar primero el conjunto de datos completo como un grafo ponderado, en el que los nodos del grafo representan los puntos de datos y los pesos en los bordes corresponden a las similitudes entre los puntos en pares. Entonces, las asignaciones de clústeres del conjunto de datos se pueden lograr optimizando algunos criterios definidos en el gráfico. Por ejemplo, el Clustering Espectral es uno de los enfoques de clustering basados en grafos más representativos, generalmente tiene como objetivo optimizar algún valor de corte (por ejemplo, Corte normalizado [22], corte de razón [7], corte mínimo-máximo [11]) definidos en un grafo no dirigido. Después de algunas relajaciones, estos criterios generalmente pueden ser optimizados a través de descomposiciones propias, lo cual está garantizado que sea óptimo a nivel global. De esta manera, el agrupamiento espectral evita eficientemente los problemas de los métodos de particionamiento tradicionales que presentamos en el párrafo anterior. En este artículo, proponemos un algoritmo novedoso de agrupamiento de documentos que hereda la superioridad del agrupamiento espectral, es decir, los resultados finales de los grupos también pueden obtenerse explotando la estructura de los autovalores de una matriz simétrica. Sin embargo, a diferencia del agrupamiento espectral, que simplemente impone una restricción de suavidad en las etiquetas de los datos sobre todo el conjunto de datos [2], nuestro método primero construye un predictor de etiquetas lineal regularizado para cada punto de datos a partir de su vecindario como en [25], y luego combina los resultados de todos estos predictores de etiquetas locales con un regularizador de suavidad de etiquetas global. Así que llamamos a nuestro método Agrupamiento con Regularización Local y Global (CLGR). La idea de incorporar tanto información local como global en la predicción de etiquetas está inspirada en los trabajos recientes sobre aprendizaje semi-supervisado [31], y nuestras evaluaciones experimentales en varios conjuntos de datos reales de documentos muestran que CLGR funciona mejor que muchos métodos de agrupamiento de vanguardia. El resto de este documento está organizado de la siguiente manera: en la sección 2 presentaremos nuestro algoritmo CLGR en detalle. Los resultados experimentales en varios conjuntos de datos se presentan en la sección 3, seguidos de las conclusiones y discusiones en la sección 4. El ALGORITMO PROPUESTO En esta sección, presentaremos en detalle nuestro algoritmo de Agrupamiento con Regularización Local y Global (CLGR). Primero veamos cómo se representan los documentos a lo largo de este documento. 2.1 Representación de documentos En nuestro trabajo, todos los documentos se representan mediante vectores de frecuencia de términos ponderados. Sea W = {w1, w2, · · · , wm} el conjunto completo de vocabulario del corpus de documentos (que ha sido preprocesado mediante la eliminación de palabras vacías y operaciones de derivación de palabras). El vector de frecuencia de término xi del documento di se define como xi = [xi1, xi2, · · · , xim]T , xik = tik log n idfk , donde tik es la frecuencia del término wk ∈ W, n es el tamaño del corpus de documentos, idfk es el número de documentos que contienen la palabra wk. De esta manera, xi también se llama la representación TFIDF del documento di. Además, también normalizamos cada xi (1 i n) para que tenga una longitud unitaria, de modo que cada documento esté representado por un vector TF-IDF normalizado. 2.2 Regularización Local Como su nombre sugiere, CLGR está compuesto por dos partes: regularización local y regularización global. En esta subsección introduciremos detalladamente la parte de regularización local. 2.2.1 Motivación Como sabemos que el agrupamiento es un tipo de técnica de aprendizaje, tiene como objetivo organizar el conjunto de datos de una manera razonable. En general, el aprendizaje puede plantearse como un problema de estimación de funciones, a partir del cual podemos obtener una buena función de clasificación que asignará etiquetas al conjunto de datos de entrenamiento e incluso al conjunto de datos de prueba no vistos con algún costo minimizado [24]. Por ejemplo, en el escenario de clasificación de dos clases (en el que conocemos exactamente la etiqueta de cada documento), un clasificador lineal con ajuste de mínimos cuadrados tiene como objetivo aprender un vector columna w de manera que el costo al cuadrado J = 1 n (wT xi − yi)2 (1) se minimice, donde yi ∈ {+1, −1} es la etiqueta de xi. Al tomar ∂J /∂w = 0, obtenemos la solución w∗ = Σ i=1 n xixT i −1 Σ i=1 n xiyi, (2) que puede escribirse en su forma matricial como w∗ = XXT −1 Xy, (3) donde X = [x1, x2, · · · , xn] es una matriz de documentos m × n, y = [y1, y2, · · · , yn]T es el vector de etiquetas. Entonces, para un documento de prueba t, podemos determinar su etiqueta mediante l = signo(w∗T u), donde signo(·) es la función signo. Un problema natural en la ecuación (3) es que la matriz XXT puede ser singular y, por lo tanto, no invertible (por ejemplo, cuando m n). Para evitar tal problema, podemos agregar un término de regularización y minimizar el siguiente criterio J = 1 n n i=1 (wT xi − yi)2 + λ w 2, (5), donde λ es un parámetro de regularización. Entonces, la solución óptima que minimiza J está dada por w∗ = XXT + λnI −1 Xy, (6) donde I es una matriz identidad de tamaño m × m. Se ha informado que el clasificador lineal regularizado puede lograr resultados muy buenos en problemas de clasificación de texto [29]. Sin embargo, a pesar de su éxito empírico, el clasificador lineal regularizado es en realidad un clasificador global, es decir, w∗ se estima utilizando todo el conjunto de entrenamiento. Según [24], esta puede que no sea una idea inteligente, ya que un único w∗ puede que no sea lo suficientemente bueno para predecir las etiquetas de todo el espacio de entrada. Para obtener predicciones más precisas, [6] propuso entrenar clasificadores localmente y utilizarlos para clasificar los puntos de prueba. Por ejemplo, un punto de prueba será clasificado por el clasificador local entrenado utilizando los puntos de entrenamiento ubicados en la vecindad 1. En las siguientes discusiones todos asumimos que los documentos provienen solo de dos clases. Las generalizaciones de nuestro método a casos de múltiples clases se discutirán en la sección 2.5 de él. Aunque este método parece lento y estúpido, se informa que puede obtener mejores rendimientos que usar un clasificador global único en ciertas tareas [6]. 2.2.2 Construyendo los Predictores Localmente Regularizados Inspirados en su éxito, propusimos aplicar los algoritmos de aprendizaje local para el agrupamiento. La idea básica es que, para cada vector de documento xi (1 i n), entrenamos un predictor de etiquetas local basado en su vecindario k-más cercano Ni, y luego lo usamos para predecir la etiqueta de xi. Finalmente combinaremos todos esos predictores locales minimizando la suma de sus errores de predicción. En esta subsección introduciremos cómo construir esos predictores locales. Debido a la simplicidad y efectividad del clasificador lineal regularizado que hemos introducido en la sección 2.2.1, lo elegimos como nuestro predictor de etiquetas local, de modo que para cada documento xi, se minimiza el siguiente criterio Ji = 1 ni xj ∈Ni wT i xj − qj 2 + λi wi 2, (7), donde ni = |Ni| es la cardinalidad de Ni, y qj es la pertenencia al clúster de xj. Luego, utilizando la Ecuación (6), podemos obtener que la solución óptima es w∗ i = XiXT i + λiniI −1 Xiqi, (8), donde Xi = [xi1, xi2, · · · , xini], y usamos xik para denotar al k-ésimo vecino más cercano de xi. qi = [qi1, qi2, · · · , qini]T con qik representando la asignación de clúster de xik. El problema aquí es que XiXT i es una matriz m × m con m ni, es decir, deberíamos calcular la inversa de una matriz m × m para cada vector de documento, lo cual está prohibido computacionalmente. Afortunadamente, tenemos el siguiente teorema: Teorema 1. w∗ i en la Ec. (8) puede ser reescrito como w∗ i = Xi XT i Xi + λiniIi −1 qi, (9), donde Ii es una matriz identidad de tamaño ni × ni. Prueba. Dado que w∗ i = XiXT i + λiniI −1 Xiqi, entonces XiXT i + λiniI w∗ i = Xiqi =⇒ XiXT i w∗ i + λiniw∗ i = Xiqi =⇒ w∗ i = (λini)−1 Xi qi − XT i w∗ i. Sea β = (λini)−1 qi − XT i w∗ i, entonces w∗ i = Xiβ =⇒ λiniβ = qi − XT i w∗ i = qi − XT i Xiβ =⇒ qi = XT i Xi + λiniIi β =⇒ β = XT i Xi + λiniIi −1 qi. Por lo tanto, w∗ i = Xiβ = Xi XT i Xi + λiniIi −1 qi 2. Utilizando el teorema 1, solo necesitamos calcular la inversa de una matriz ni × ni para cada documento para entrenar un predictor de etiquetas local. Además, para un nuevo punto de prueba u que cae en Ni, podemos clasificarlo por el signo de qu = w∗T i u = uT wi = uT Xi XT i Xi + λiniIi −1 qi. Esta es una expresión atractiva ya que podemos determinar la asignación del clúster de u utilizando los productos internos entre los puntos en {u ∪ Ni}, lo que sugiere que un regularizador local de este tipo puede ser fácilmente kernelizado [21] siempre y cuando definamos una función de kernel adecuada. 2.2.3 Combinando los Predictores Regularizados Localmente Una vez que todos los predictores locales hayan sido construidos, los combinaremos minimizando Jl = n i=1 w∗T i xi − qi 2, (10), lo que representa la suma de los errores de predicción de todos los predictores locales. Combinando la Ecuación (10) con la Ecuación (6), podemos obtener Jl = n i=1 w∗T i xi − qi 2 = n i=1 xT i Xi XT i Xi + λiniIi −1 qi − qi 2 = Pq − q 2, (11), donde q = [q1, q2, · · · , qn]T, y P es una matriz n × n construida de la siguiente manera. Sea αi = xT i Xi XT i Xi + λiniIi −1 , entonces Pij = αi j, si xj ∈ Ni 0, de lo contrario , (12) donde Pij es la entrada (i, j)-ésima de P, y αi j representa la entrada j-ésima de αi. Hasta ahora podemos escribir el criterio de agrupamiento combinando predictores de etiquetas lineales localmente regularizados Jl en una forma matemática explícita, y podemos minimizarlo directamente utilizando algunas técnicas estándar de optimización. Sin embargo, los resultados pueden no ser lo suficientemente buenos ya que solo explotamos la información local del conjunto de datos. En la siguiente subsección, introduciremos un criterio de regularización global y lo combinaremos con Jl, que tiene como objetivo encontrar un buen resultado de agrupamiento de manera local-global. 2.3 Regularización Global En el agrupamiento de datos, generalmente requerimos que las asignaciones de clúster de los puntos de datos sean lo suficientemente suaves con respecto a la variedad de datos subyacente, lo que implica (1) que los puntos cercanos tienden a tener las mismas asignaciones de clúster; (2) que los puntos en la misma estructura (por ejemplo, subvariedad o clúster) tienden a tener las mismas asignaciones de clúster [31]. Sin pérdida de generalidad, asumimos que los puntos de datos residen (aproximadamente) en una variedad de baja dimensión M2, y q es la función de asignación de clúster definida en M, es decir, 2 Creemos que los datos de texto también se muestrean de alguna variedad de baja dimensión, ya que es imposible que para todos los x ∈ M, q(x) devuelva la membresía del clúster de x. La suavidad de q sobre M se puede calcular mediante la siguiente integral de Dirichlet [2] D[q] = 1 2 M q(x) 2 dM, (13) donde el gradiente q es un vector en el espacio tangente T Mx, y la integral se toma con respecto a la medida estándar en M. Si restringimos la escala de q por q, q M = 1 (donde ·, · M es el producto interno inducido en M), entonces resulta que encontrar la función más suave que minimiza D[q] se reduce a encontrar las autofunciones del operador Laplace Beltrami L, que se define como Lq −div q, (14) donde div es la divergencia de un campo vectorial. Generalmente, el gráfico se puede ver como la forma discretizada de una variedad. Podemos modelar el conjunto de datos como un grafo ponderado no dirigido como en el agrupamiento espectral [22], donde los nodos del grafo son simplemente los puntos de datos, y los pesos en las aristas representan las similitudes entre pares de puntos. Entonces se puede demostrar que minimizar la Ec. (13) corresponde a minimizar Jg = qT Lq = n i=1 (qi − qj)2 wij, (15), donde q = [q1, q2, · · · , qn]T con qi = q(xi), L es el Laplaciano del grafo con su entrada (i, j)-ésima Lij =    di − wii, si i = j −wij, si xi y xj son adyacentes 0, en otro caso, (16), donde di = j wij es el grado de xi, wij es la similitud entre xi y xj. Si xi y xj son adyacentes, wij generalmente se calcula de la siguiente manera: wij = e − xi−xj 2 2σ2, donde σ es un parámetro dependiente del conjunto de datos. Se ha demostrado que bajo ciertas condiciones, tal forma de wij para determinar los pesos en los bordes del grafo conduce a la convergencia del Laplaciano del grafo al operador Laplace Beltrami [3][18]. En resumen, el uso de la Ec. (15) con pesos exponenciales puede medir de manera efectiva la suavidad de las asignaciones de datos con respecto a la variedad intrínseca de datos. Por lo tanto, lo adoptamos como un regularizador global para penalizar la suavidad de las asignaciones de datos predichas. 2.4 Agrupamiento con Regularización Local y Global Combinando los contenidos que hemos introducido en la sección 2.2 y la sección 2.3, podemos derivar que el criterio de agrupamiento es minq J = Jl + λJg = Pq − q 2 + λqT Lq sujeto a qi ∈ {−1, +1}, (18) donde P está definido como en la Ecuación (12), y λ es un parámetro de regularización para equilibrar Jl y Jg. Sin embargo, los valores discretos llenan todo el espacio muestral de alta dimensionalidad. Y se ha demostrado que los métodos basados en variedades pueden lograr buenos resultados en tareas de clasificación de texto [31]. En este artículo, definimos xi y xj como adyacentes si xi ∈ N(xj) o xj ∈ N(xi). La restricción de pi convierte el problema en un problema de programación entera NP difícil. Una forma natural de hacer que el problema sea resoluble es eliminar la restricción y relajar qi para que sea continuo, luego el objetivo que buscamos minimizar se convierte en J = Pq − q 2 + λqT Lq = qT (P − I)T (P − I)q + λqT Lq = qT (P − I)T (P − I) + λL q, (19) y luego agregamos una restricción adicional qT q = 1 para restringir la escala de q. Entonces nuestro objetivo se convierte en minq J = qT (P − I)T (P − I) + λL q s.t. qT q = 1 (20) Utilizando el método de Lagrange, podemos derivar que la solución óptima q corresponde al menor vector propio de la matriz M = (P − I)T (P − I) + λL, y la asignación de clúster de xi puede determinarse por el signo de qi, es decir, xi se clasificará como clase uno si qi > 0, de lo contrario se clasificará como clase 2. 2.5 CLGR Multiclase En lo anterior hemos introducido el marco básico de Agrupamiento con Regularización Local y Global (CLGR) para el problema de agrupamiento de dos clases, y lo extenderemos al agrupamiento multiclase en esta subsección. Primero asumimos que todos los documentos pertenecen a C clases indexadas por L = {1, 2, · · · , C}. qc es la función de clasificación para la clase c (1 ≤ c ≤ C), tal que qc(xi) devuelve la confianza de que xi pertenece a la clase c. Nuestro objetivo es obtener el valor de qc(xi) (1 ≤ c ≤ C, 1 ≤ i ≤ n), y la asignación de cluster de xi puede ser determinada por {qc(xi)}C c=1 utilizando algunos métodos adecuados de discretización que presentaremos más adelante. Por lo tanto, en este caso de múltiples clases, para cada documento xi (1 i n), construiremos C predictores de etiquetas regularizados localmente lineales cuyos vectores normales son wc∗ i = Xi XT i Xi + λiniIi −1 qc i (1 c C), (21), donde Xi = [xi1, xi2, · · · , xini ] con xik siendo el k-ésimo vecino de xi, y qc i = [qc i1, qc i2, · · · , qc ini ]T con qc ik = qc (xik). Entonces (wc∗ i )T xi devuelve la confianza predicha de xi perteneciente a la clase c. Por lo tanto, el error de predicción local para la clase c se puede definir como J c l = n i=1 (wc∗ i ) T xi − qc i 2 , (22) Y el error de predicción local total se convierte en Jl = C c=1 J c l = C c=1 n i=1 (wc∗ i ) T xi − qc i 2 . (23) Como en la Ec. (11), podemos definir una matriz n×n P (ver Ec. (12)) y reescribir Jl como Jl = C c=1 J c l = C c=1 Pqc − qc 2 . (24) De manera similar, podemos definir el regularizador de suavidad global en el caso de múltiples clases como Jg = C c=1 n i=1 (qc i − qc j )2 wij = C c=1 (qc )T Lqc . (25) Luego, el criterio a minimizar para CLGR en el caso de múltiples clases se convierte en J = Jl + λJg = C c=1 Pqc − qc 2 + λ(qc )T Lqc = C c=1 (qc )T (P − I)T (P − I) + λL qc = trace QT (P − I)T (P − I) + λL Q , (26) donde Q = [q1 , q2 , · · · , qc ] es una matriz n × c, y trace(·) devuelve la traza de una matriz. Al igual que en la ecuación (20), también agregamos la restricción de que QT Q = I para limitar la escala de Q. Entonces nuestro problema de optimización se convierte en minQ J = traza QT (P − I)T (P − I) + λL Q sujeto a. Según el teorema de Ky Fan [28], sabemos que la solución óptima del problema anterior es Q∗ = [q∗ 1, q∗ 2, · · · , q∗ C ]R, donde q∗ k (1 ≤ k ≤ C) es el autovector que corresponde al k-ésimo valor propio más pequeño de la matriz (P − I)T (P − I) + λL, y R es una matriz arbitraria de tamaño C × C. Dado que los valores de las entradas en Q∗ son continuos, necesitamos discretizar aún más Q∗ para obtener las asignaciones de clúster de todos los puntos de datos. Principalmente hay dos enfoques para lograr este objetivo: 1. Como en [20], podemos tratar la i-ésima fila de Q como la incrustación de xi en un espacio de C dimensiones, y aplicar algunos métodos de agrupamiento tradicionales como kmeans para agrupar estas incrustaciones en C grupos. 2. Dado que el Q∗ óptimo no es único (debido a la existencia de una matriz R arbitraria), podemos buscar un R óptimo que rote Q∗ a una matriz de indicación4. El algoritmo detallado se puede consultar en [26]. El procedimiento detallado del algoritmo para CLGR se resume en la tabla 1. 3. EXPERIMENTOS En esta sección, se realizan experimentos para comparar empíricamente los resultados de agrupamiento de CLGR con otros 8 algoritmos representativos de agrupamiento de documentos en 5 conjuntos de datos. Primero presentaremos la información básica de esos conjuntos de datos. 3.1 Conjuntos de datos Utilizamos una variedad de conjuntos de datos, la mayoría de los cuales son frecuentemente utilizados en la investigación de recuperación de información. La Tabla 2 resume las características de los conjuntos de datos. Aquí, una matriz de indicación T es una matriz n×c con su entrada (i, j) Tij ∈ {0, 1} de modo que para cada fila de Q∗ solo hay un 1. Entonces, el xi puede ser asignado al j-ésimo grupo tal que j = argjQ∗ ij = 1. Tabla 1: Agrupamiento con Regularización Local y Global (CLGR) Entrada: 1. Conjunto de datos X = {xi}n i=1; 2. Número de grupos C: 3. Tamaño del vecindario K: 4. Parámetros locales de regularización {λi}n i=1; 5. Parámetro de regularización global λ; Salida: La membresía del clúster de cada punto de datos. Procedimiento: 1. Construir los K vecindarios más cercanos para cada punto de datos; 2. Construya la matriz P utilizando la Ecuación (12); 3. Construya la matriz Laplaciana L utilizando la ecuación (16); 4. Construye la matriz M = (P − I)T (P − I) + λL; 5. Realice la descomposición de valores propios en M y construya la matriz Q∗ de acuerdo con la Ecuación (28); 6. Generar las asignaciones de clúster de cada punto de datos al discretizar adecuadamente Q∗. Tabla 2: Descripciones de los conjuntos de datos de documentos Conjuntos de datos Número de documentos Número de clases CSTR 476 4 WebKB4 4199 4 Reuters 2900 10 WebACE 2340 20 Newsgroup4 3970 4 CSTR. Este es el conjunto de datos de los resúmenes de informes técnicos publicados en el Departamento de Ciencias de la Computación de una universidad. El conjunto de datos contenía 476 resúmenes, los cuales fueron divididos en cuatro áreas de investigación: Procesamiento de Lenguaje Natural (NLP), Robótica/Visión, Sistemas y Teoría. WebKB. El conjunto de datos WebKB contiene páginas web recopiladas de los departamentos de informática de universidades. Hay alrededor de 8280 documentos y están divididos en 7 categorías: estudiante, facultad, personal, curso, proyecto, departamento y otros. El texto sin procesar es de aproximadamente 27MB. Entre estas 7 categorías, estudiante, facultad, curso y proyecto son las cuatro categorías que representan entidades más pobladas. El subconjunto asociado suele ser llamado WebKB4. Reuters. La colección de pruebas de categorización de texto Reuters-21578 contiene documentos recopilados de la agencia de noticias Reuters en 1987. Es un punto de referencia estándar para la categorización de texto y contiene 135 categorías. En nuestros experimentos, utilizamos un subconjunto de la colección de datos que incluye las 10 categorías más frecuentes entre los 135 temas y lo llamamos Reuters-top 10. WebACE. El conjunto de datos WebACE proviene del proyecto WebACE y ha sido utilizado para la agrupación de documentos [17][5]. El conjunto de datos WebACE contiene 2340 documentos que consisten en artículos de noticias del servicio de noticias de Reuters a través de la web en octubre de 1997. Estos documentos están divididos en 20 clases. Not enough context provided. El conjunto de datos News4 utilizado en nuestros experimentos se selecciona del famoso conjunto de datos 20-newsgroups. El tema rec que contiene autos, motocicletas, béisbol y hockey fue seleccionado de la versión 20news-18828. El conjunto de datos News4 contiene 3970 vectores de documentos. Para preprocesar los conjuntos de datos, eliminamos las palabras vacías utilizando una lista estándar de palabras vacías, se omiten todas las etiquetas HTML y se ignoran todos los campos de encabezado excepto el asunto y la organización de los artículos publicados. En todos nuestros experimentos, primero seleccionamos las 1000 palabras principales por información mutua con las etiquetas de clase. 3.2 Métricas de Evaluación En los experimentos, establecemos el número de clústeres igual al número real de clases C para todos los algoritmos de agrupamiento. Para evaluar su rendimiento, comparamos los grupos generados por estos algoritmos con las clases reales mediante el cálculo de las siguientes dos medidas de rendimiento. Precisión de agrupamiento (Acc). La primera medida de rendimiento es la Precisión de Agrupamiento, que descubre la relación uno a uno entre los grupos y las clases, y mide en qué medida cada grupo contenía puntos de datos de la clase correspondiente. Resume todo el grado de coincidencia entre todos los pares de clases y clusters. La precisión del agrupamiento se puede calcular como: Acc = 1 N max   Ck,Lm T(Ck, Lm)   , (29) donde Ck denota el k-ésimo clúster en los resultados finales, y Lm es la verdadera clase m-ésima. T(Ck, Lm) es el número de entidades que pertenecen a la clase m y están asignadas al clúster k. La precisión calcula la suma máxima de T(Ck, Lm) para todos los pares de clústeres y clases, y estos pares no tienen superposiciones. La mayor precisión de agrupamiento significa un mejor rendimiento de agrupamiento. Información Mutua Normalizada (NMI). Otro métrica de evaluación que adoptamos aquí es la Información Mutua Normalizada (NMI) [23], la cual es ampliamente utilizada para determinar la calidad de los grupos. Para dos variables aleatorias X e Y, el NMI se define como: NMI(X, Y) = I(X, Y) H(X)H(Y), donde I(X, Y) es la información mutua entre X e Y, mientras que H(X) y H(Y) son las entropías de X e Y respectivamente. Se puede ver que NMI(X, X) = 1, que es el valor máximo posible de NMI. Dado un resultado de agrupamiento, el NMI en la ecuación (30) se estima como NMI = C k=1 C m=1 nk,mlog n·nk,m nk ˆnm C k=1 nklog nk n C m=1 ˆnmlog ˆnm n, (31), donde nk denota el número de datos contenidos en el grupo Ck (1 k C), ˆnm es el número de datos pertenecientes a la clase m-ésima (1 m C), y nk,m denota el número de datos que están en la intersección entre el grupo Ck y la clase m-ésima. El valor calculado en la Ecuación (31) se utiliza como medida de rendimiento para el resultado de agrupamiento dado. A mayor valor, mejor rendimiento de agrupamiento. 3.3 Comparaciones Hemos realizado evaluaciones de rendimiento exhaustivas probando nuestro método y comparándolo con otros 8 métodos representativos de agrupamiento de datos utilizando las mismas corpora de datos. Los algoritmos que evaluamos se enumeran a continuación. 1. K-means tradicional (KM). 2. K-medias esférico (SKM). La implementación se basa en [9]. 3. Modelo de Mezcla Gaussiana (GMM). La implementación se basa en [16]. 4. Agrupamiento espectral con cortes normalizados (Ncut). La implementación se basa en [26], y la varianza de la similitud gaussiana se determina mediante Escalado Local [30]. Ten en cuenta que el criterio que Ncut busca minimizar es simplemente el regularizador global en nuestro algoritmo CLGR, excepto que Ncut utiliza el Laplaciano normalizado. 5. Agrupamiento utilizando Regularización Local Pura (CPLR). En este método simplemente minimizamos Jl (definido en la Ecuación (24)), y los resultados de agrupamiento pueden obtenerse realizando la descomposición de valores propios en la matriz (I − P)T (I − P) con algunos métodos de discretización adecuados. 6. Iteración de subespacio adaptativa (ASI). La implementación se basa en [14]. 7. Factorización de matrices no negativas (NMF). La implementación se basa en [27]. 8. Factorización Tri-Factorización de Matrices No Negativas (TNMF) [12]. La implementación se basa en [15]. Para la eficiencia computacional, en la implementación de CPLR y nuestro algoritmo CLGR, hemos establecido todos los parámetros de regularización local {λi}n i=1 para ser idénticos, los cuales se establecen mediante búsqueda en cuadrícula de {0.1, 1, 10}. El tamaño de los vecindarios k más cercanos se establece mediante una búsqueda en cuadrícula de {20, 40, 80}. Para el método CLGR, su parámetro de regularización global se establece mediante una búsqueda en cuadrícula de {0.1, 1, 10}. Al construir el regularizador global, hemos adoptado el método de escalamiento local [30] para construir la matriz Laplaciana. El método de discretización final adoptado en estos dos métodos es el mismo que en [26], ya que nuestros experimentos muestran que el uso de dicho método puede lograr mejores resultados que el uso de métodos basados en kmeans como en [20]. 3.4 Resultados Experimentales Los resultados de comparación de precisión de agrupamiento se muestran en la tabla 3, y los resultados de comparación de información mutua normalizada se resumen en la tabla 4. De las dos tablas principalmente observamos que: 1. Nuestro método CLGR supera a todos los demás métodos de agrupamiento de documentos en la mayoría de los conjuntos de datos. Para la agrupación de documentos, el método Spherical k-means suele superar al método tradicional de agrupación k-means, y el método GMM puede lograr resultados competitivos en comparación con el método Spherical k-means; 3. Los resultados obtenidos de los algoritmos tipo k-means y GMM suelen ser peores que los resultados obtenidos de Clustering Espectral. Dado que el Agrupamiento Espectral puede ser visto como una versión ponderada del kernel k-means, puede obtener buenos resultados cuando los clusters de datos tienen formas arbitrarias. Esto corrobora que los vectores de los documentos no están distribuidos regularmente (esféricos o elípticos). 4. Las comparaciones experimentales verifican empíricamente la equivalencia entre NMF y Clustering Espectral, como se muestra en la Tabla 3: Precisión de agrupamiento de los diversos métodos CSTR WebKB4 Reuters WebACE News4 KM 0.4256 0.3888 0.4448 0.4001 0.3527 SKM 0.4690 0.4318 0.5025 0.4458 0.3912 GMM 0.4487 0.4271 0.4897 0.4521 0.3844 NMF 0.5713 0.4418 0.4947 0.4761 0.4213 Ncut 0.5435 0.4521 0.4896 0.4513 0.4189 ASI 0.5621 0.4752 0.5235 0.4823 0.4335 TNMF 0.6040 0.4832 0.5541 0.5102 0.4613 CPLR 0.5974 0.5020 0.4832 0.5213 0.4890 CLGR 0.6235 0.5228 0.5341 0.5376 0.5102 Tabla 4: Resultados de información mutua normalizada de los diversos métodos CSTR WebKB4 Reuters WebACE News4 KM 0.3675 0.3023 0.4012 0.3864 0.3318 SKM 0.4027 0.4155 0.4587 0.4003 0.4085 GMM 0.4034 0.4093 0.4356 0.4209 0.3994 NMF 0.5235 0.4517 0.4402 0.4359 0.4130 Ncut 0.4833 0.4497 0.4392 0.4289 0.4231 ASI 0.5008 0.4833 0.4769 0.4817 0.4503 TNMF 0.5724 0.5011 0.5132 0.5328 0.4749 CPLR 0.5695 0.5231 0.4402 0.5543 0.4690 CLGR 0.6012 0.5434 0.4935 0.5390 0.4908 ha sido demostrado teóricamente en [10]. Se puede observar en las tablas que NMF y el Agrupamiento Espectral suelen llevar a resultados de agrupamiento similares. 5. Los métodos basados en co-agrupamiento (TNMF y ASI) suelen lograr mejores resultados que los métodos tradicionales basados únicamente en vectores de documentos. Dado que estos métodos realizan una selección implícita de características en cada iteración, proporcionan una métrica adaptativa para medir el vecindario y, por lo tanto, tienden a producir mejores resultados de agrupamiento. 6. Los resultados obtenidos a través de CPLR suelen ser mejores que los resultados obtenidos a través de Agrupamiento Espectral, lo cual respalda la teoría de Vapnik [24] de que a veces los algoritmos de aprendizaje local pueden obtener mejores resultados que los algoritmos de aprendizaje global. Además de los experimentos de comparación mencionados anteriormente, también probamos la sensibilidad de los parámetros de nuestro método. Principalmente hay dos conjuntos de parámetros en nuestro algoritmo CLGR, los parámetros de regularización locales y globales ({λi}n i=1 y λ, como hemos mencionado en la sección 3.3, hemos establecido que todos los λis son idénticos a λ∗ en nuestros experimentos), y el tamaño de los vecindarios. Por lo tanto, también hemos realizado dos series de experimentos: 1. Fijando el tamaño de los vecindarios y probando el rendimiento del agrupamiento con diferentes valores de λ∗ y λ. En este conjunto de experimentos, encontramos que nuestro algoritmo CLGR puede lograr buenos resultados cuando los dos parámetros de regularización no son ni demasiado grandes ni demasiado pequeños. Normalmente nuestro método puede lograr buenos resultados cuando λ∗ y λ están alrededor de 0.1. La Figura 1 nos muestra un ejemplo de prueba en el conjunto de datos WebACE. Fijando los parámetros de regularización local y global, y probando el rendimiento del agrupamiento con diferentes valores de -5, -4.5, -4, -3.5, -3, -5, -4.5, -4, -3.5, -3, 0.35, 0.4, 0.45, 0.5, 0.55 para el parámetro de regularización local (valor logarítmico base 2) y para el parámetro de regularización global (valor logarítmico base 2), se obtuvo una precisión de agrupamiento. Figura 1: Resultados de la prueba de sensibilidad de parámetros en el conjunto de datos WebACE con el tamaño de vecindario fijo en 20, donde el eje x e y representan el valor logarítmico base 2 de λ∗ y λ. En este conjunto de experimentos, encontramos que el vecindario con un tamaño demasiado grande o demasiado pequeño deteriorará todos los resultados finales de agrupamiento. Esto puede entenderse fácilmente ya que cuando el tamaño del vecindario es muy pequeño, los puntos de datos utilizados para entrenar los clasificadores locales pueden no ser suficientes; cuando el tamaño del vecindario es muy grande, los clasificadores entrenados tenderán a ser globales y no podrán capturar las características locales típicas. La Figura 2 nos muestra un ejemplo de prueba en el conjunto de datos WebACE. Por lo tanto, podemos ver que nuestro algoritmo CLGR (1) puede lograr resultados satisfactorios y (2) no es muy sensible a la elección de parámetros, lo que lo hace práctico en aplicaciones del mundo real. CONCLUSIONES Y TRABAJOS FUTUROS En este artículo, desarrollamos un nuevo algoritmo de agrupamiento llamado agrupamiento con regularización local y global. Nuestro método conserva el mérito de los algoritmos de aprendizaje local y el agrupamiento espectral. Nuestros experimentos muestran que el algoritmo propuesto supera a la mayoría de los algoritmos de vanguardia en muchos conjuntos de datos de referencia. En el futuro, nos enfocaremos en la selección de parámetros y los problemas de aceleración del algoritmo CLGR. REFERENCIAS [1] L. Baker y A. McCallum. Agrupamiento distribucional de palabras para clasificación de texto. En Actas de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1998. [2] M. Belkin y P. Niyogi. Mapeo de Eigen de Laplaciano para Reducción de Dimensionalidad y Representación de Datos. Computación Neural, 15 (6):1373-1396. Junio de 2003. [3] M. Belkin y P. Niyogi. Hacia una Fundación Teórica para Métodos de Manifold Basados en Laplacianos. En Actas de la 18ª Conferencia sobre Teoría del Aprendizaje (COLT). 2005. 10 20 30 40 50 60 70 80 90 100 0.35 0.4 0.45 0.5 0.55 tamaño de la precisión de agrupamiento del vecindario Figura 2: Resultados de pruebas de sensibilidad de parámetros en el conjunto de datos WebACE con los parámetros de regularización fijados en 0.1, y el tamaño del vecindario variando de 10 a 100. [4] M. Belkin, P. Niyogi y V. Sindhwani. Regularización de variedades: un marco geométrico para el aprendizaje a partir de ejemplos. Revista de Investigación en Aprendizaje Automático 7, 1-48, 2006. [5] D. Boley. División Direccional Principal. Minería de datos y descubrimiento de conocimiento, 2:325-344, 1998. [6] L. Bottou y V. Vapnik. Algoritmos de aprendizaje local. Computación Neural, 4:888-900, 1992. [7] P. K. Chan, D. F. Schlag y J. Y. Zien. Particionamiento y agrupamiento K-way de corte de razón espectral. IEEE Trans. Diseño Asistido por Computadora, 13:1088-1096, Sep. 1994. [8] D. R. Cutting, D. R. Karger, J. O. Pederson y J. W. Tukey. Dispersión/Recolección: Un enfoque basado en clústeres para explorar grandes colecciones de documentos. En Actas de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1992. [9] I. S. Dhillon y D. S. Modha. Descomposiciones de conceptos para datos de texto grandes y dispersos utilizando agrupamiento. Aprendizaje automático, vol. 42(1), páginas 143-175, enero de 2001. [10] C. Ding, X. Él, y H. Simon. Sobre la equivalencia entre la factorización de matrices no negativas y el agrupamiento espectral. En Actas de la Conferencia de Minería de Datos de SIAM, 2005. [11] C. Ding, X. Él, H. Zha, M. Gu y H. D. Simon. Un algoritmo de corte min-max para particionar grafos y agrupar datos. En Proc. de la 1ra Conferencia Internacional sobre Minería de Datos (ICDM), páginas 107-114, 2001. [12] C. Ding, T. Li, W. Peng y H. Park. Tri-factorizaciones de matrices no negativas ortogonales para agrupamiento. En Actas de la Duodécima Conferencia Internacional de la ACM SIGKDD sobre Descubrimiento de Conocimiento y Minería de Datos, 2006. [13] R. O. Duda, P. E. Hart y D. G. Stork. Clasificación de patrones. John Wiley & Sons, Inc., 2001. [14] T. Li, S. Ma y M. Ogihara. Agrupamiento de documentos a través de la Iteración de Subespacios Adaptativos. En Actas de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2004. [15] T. Li y C. Ding. Las relaciones entre varios métodos de factorización de matrices no negativas para agrupamiento. En Actas de la 6ta Conferencia Internacional sobre Minería de Datos (ICDM). 2006. [16] X. Liu y Y. Gong. Agrupación de documentos con capacidades de refinamiento de clústeres y selección de modelos. En Proc. de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2002. [17] E. Han, D. Boley, M. Gini, R. Gross, K. Hastings, G. Karypis, V. Kumar, B. Mobasher y J. Moore. WebACE: Un agente web para la categorización y exploración de documentos. En Actas de la 2ª Conferencia Internacional sobre Agentes Autónomos (Agents98). ACM Press, 1998. [18] M. Hein, J. Y. Audibert y U. von Luxburg. De Gráficas a Variedades - Consistencia Puntual Débil y Fuerte de los Laplacianos de Gráficas. En Actas de la 18ª Conferencia sobre Teoría del Aprendizaje (COLT), 470-485. 2005. [19] J. Él, M. Lan, C.-L. Tan, S.-Y. Sung, y H.-B. Bajo. Inicialización de algoritmos de refinamiento de clústeres: una revisión y estudio comparativo. En Proc. de Inter. Conferencia Conjunta sobre Redes Neuronales, 2004. [20] A. Y. Ng, M. I. Jordan, Y. Weiss. En el agrupamiento espectral: análisis y un algoritmo. En Avances en Sistemas de Procesamiento de Información Neural 14. 2002. [21] B. Sch¨olkopf y A. Smola. Aprendizaje con Kernels. El MIT Press. Cambridge, Massachusetts. 2002. [22] J. Shi y J. Malik. Cortes normalizados y segmentación de imágenes. IEEE Trans. on Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000. [23] A. Strehl and J. Ghosh.\nIEEE Trans. on Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000. [23] A. Strehl y J. Ghosh. Conjuntos de agrupamientos: un marco de reutilización de conocimiento para combinar múltiples particiones. Revista de Investigación de Aprendizaje Automático, 3:583-617, 2002. [24] V. N. Vapnik. La naturaleza de la teoría del aprendizaje estadístico. Berlín: Springer-Verlag, 1995. [25] Wu, M. y Schölkopf, B. Un enfoque de aprendizaje local para el agrupamiento. En Avances en Sistemas de Procesamiento de Información Neural 18. 2006. [26] S. X. Yu, J. Shi. Agrupamiento espectral multiclase. En Actas de la Conferencia Internacional sobre Visión por Computadora, 2003. [27] W. Xu, X. Liu y Y. Gong. Agrupación de documentos basada en la factorización de matrices no negativas. En Actas de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2003. [28] H. Zha, X. Él, C. Ding, M. Gu y H. Simon. Relajación espectral para el agrupamiento K-means. En NIPS 14. 2001. [29] T. Zhang y F. J. Oles. Categorización de texto basada en métodos de clasificación lineal regularizados. Revista de Recuperación de Información, 4:5-31, 2001. [30] L. Zelnik-Manor y P. Perona. Agrupamiento espectral autoajustable. En NIPS 17. 2005. [31] D. Zhou, O. Bousquet, T. N. Lal, J. Weston y B. Schölkopf. Aprendizaje con Consistencia Local y Global. NIPS 17, 2005. \n\nNIPS 17, 2005. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "specified search": {
            "translated_key": "búsquedas específicas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Regularized Clustering for Documents ∗ Fei Wang, Changshui Zhang State Key Lab of Intelligent Tech.",
                "and Systems Department of Automation, Tsinghua University Beijing, China, 100084 feiwang03@gmail.com Tao Li School of Computer Science Florida International University Miami, FL 33199, U.S.A. taoli@cs.fiu.edu ABSTRACT In recent years, document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering.",
                "In this paper, we propose a novel method for clustering documents using regularization.",
                "Unlike traditional globally regularized clustering methods, our method first construct a local regularized linear label predictor for each document vector, and then combine all those local regularizers with a global smoothness regularizer.",
                "So we call our algorithm Clustering with Local and Global Regularization (CLGR).",
                "We will show that the cluster memberships of the documents can be achieved by eigenvalue decomposition of a sparse symmetric matrix, which can be efficiently solved by iterative methods.",
                "Finally our experimental evaluations on several datasets are presented to show the superiorities of CLGR over traditional document clustering methods.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Clustering; I.2.6 [Artificial Intelligence]: Learning-Concept Learning General Terms Algorithms 1.",
                "INTRODUCTION Document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering.",
                "A good document clustering approach can assist the computers to automatically organize the document corpus into a meaningful cluster hierarchy for efficient browsing and navigation, which is very valuable for complementing the deficiencies of traditional information retrieval technologies.",
                "As pointed out by [8], the information retrieval needs can be expressed by a spectrum ranged from narrow keyword-matching based search to broad information browsing such as what are the major international events in recent months.",
                "Traditional document retrieval engines tend to fit well with the search end of the spectrum, i.e. they usually provide <br>specified search</br> for documents matching the users query, however, it is hard for them to meet the needs from the rest of the spectrum in which a rather broad or vague information is needed.",
                "In such cases, efficient browsing through a good cluster hierarchy will be definitely helpful.",
                "Generally, document clustering methods can be mainly categorized into two classes: hierarchical methods and partitioning methods.",
                "The hierarchical methods group the data points into a hierarchical tree structure using bottom-up or top-down approaches.",
                "For example, hierarchical agglomerative clustering (HAC) [13] is a typical bottom-up hierarchical clustering method.",
                "It takes each data point as a single cluster to start off with and then builds bigger and bigger clusters by grouping similar data points together until the entire dataset is encapsulated into one final cluster.",
                "On the other hand, partitioning methods decompose the dataset into a number of disjoint clusters which are usually optimal in terms of some predefined criterion functions.",
                "For instance, K-means [13] is a typical partitioning method which aims to minimize the sum of the squared distance between the data points and their corresponding cluster centers.",
                "In this paper, we will focus on the partitioning methods.",
                "As we know that there are two main problems existing in partitioning methods (like Kmeans and Gaussian Mixture Model (GMM) [16]): (1) the predefined criterion is usually non-convex which causes many local optimal solutions; (2) the iterative procedure (e.g. the Expectation Maximization (EM) algorithm) for optimizing the criterions usually makes the final solutions heavily depend on the initializations.",
                "In the last decades, many methods have been proposed to overcome the above problems of the partitioning methods [19][28].",
                "Recently, another type of partitioning methods based on clustering on data graphs have aroused considerable interests in the machine learning and data mining community.",
                "The basic idea behind these methods is to first model the whole dataset as a weighted graph, in which the graph nodes represent the data points, and the weights on the edges correspond to the similarities between pairwise points.",
                "Then the cluster assignments of the dataset can be achieved by optimizing some criterions defined on the graph.",
                "For example Spectral Clustering is one kind of the most representative graph-based clustering approaches, it generally aims to optimize some cut value (e.g.",
                "Normalized Cut [22], Ratio Cut [7], Min-Max Cut [11]) defined on an undirected graph.",
                "After some relaxations, these criterions can usually be optimized via eigen-decompositions, which is guaranteed to be global optimal.",
                "In this way, spectral clustering efficiently avoids the problems of the traditional partitioning methods as we introduced in last paragraph.",
                "In this paper, we propose a novel document clustering algorithm that inherits the superiority of spectral clustering, i.e. the final cluster results can also be obtained by exploit the eigen-structure of a symmetric matrix.",
                "However, unlike spectral clustering, which just enforces a smoothness constraint on the data labels over the whole data manifold [2], our method first construct a regularized linear label predictor for each data point from its neighborhood as in [25], and then combine the results of all these local label predictors with a global label smoothness regularizer.",
                "So we call our method Clustering with Local and Global Regularization (CLGR).",
                "The idea of incorporating both local and global information into label prediction is inspired by the recent works on semi-supervised learning [31], and our experimental evaluations on several real document datasets show that CLGR performs better than many state-of-the-art clustering methods.",
                "The rest of this paper is organized as follows: in section 2 we will introduce our CLGR algorithm in detail.",
                "The experimental results on several datasets are presented in section 3, followed by the conclusions and discussions in section 4. 2.",
                "THE PROPOSED ALGORITHM In this section, we will introduce our Clustering with Local and Global Regularization (CLGR) algorithm in detail.",
                "First lets see the how the documents are represented throughout this paper. 2.1 Document Representation In our work, all the documents are represented by the weighted term-frequency vectors.",
                "Let W = {w1, w2, · · · , wm} be the complete vocabulary set of the document corpus (which is preprocessed by the stopwords removal and words stemming operations).",
                "The term-frequency vector xi of document di is defined as xi = [xi1, xi2, · · · , xim]T , xik = tik log n idfk , where tik is the term frequency of wk ∈ W, n is the size of the document corpus, idfk is the number of documents that contain word wk.",
                "In this way, xi is also called the TFIDF representation of document di.",
                "Furthermore, we also normalize each xi (1 i n) to have a unit length, so that each document is represented by a normalized TF-IDF vector. 2.2 Local Regularization As its name suggests, CLGR is composed of two parts: local regularization and global regularization.",
                "In this subsection we will introduce the local regularization part in detail. 2.2.1 Motivation As we know that clustering is one type of learning techniques, it aims to organize the dataset in a reasonable way.",
                "Generally speaking, learning can be posed as a problem of function estimation, from which we can get a good classification function that will assign labels to the training dataset and even the unseen testing dataset with some cost minimized [24].",
                "For example, in the two-class classification scenario1 (in which we exactly know the label of each document), a linear classifier with least square fit aims to learn a column vector w such that the squared cost J = 1 n (wT xi − yi)2 (1) is minimized, where yi ∈ {+1, −1} is the label of xi.",
                "By taking ∂J /∂w = 0, we get the solution w∗ = n i=1 xixT i −1 n i=1 xiyi , (2) which can further be written in its matrix form as w∗ = XXT −1 Xy, (3) where X = [x1, x2, · · · , xn] is an m × n document matrix, y = [y1, y2, · · · , yn]T is the label vector.",
                "Then for a test document t, we can determine its label by l = sign(w∗T u), (4) where sign(·) is the sign function.",
                "A natural problem in Eq. (3) is that the matrix XXT may be singular and thus not invertable (e.g. when m n).",
                "To avoid such a problem, we can add a regularization term and minimize the following criterion J = 1 n n i=1 (wT xi − yi)2 + λ w 2 , (5) where λ is a regularization parameter.",
                "Then the optimal solution that minimize J is given by w∗ = XXT + λnI −1 Xy, (6) where I is an m × m identity matrix.",
                "It has been reported that the regularized linear classifier can achieve very good results on text classification problems [29].",
                "However, despite its empirical success, the regularized linear classifier is on earth a global classifier, i.e. w∗ is estimated using the whole training set.",
                "According to [24], this may not be a smart idea, since a unique w∗ may not be good enough for predicting the labels of the whole input space.",
                "In order to get better predictions, [6] proposed to train classifiers locally and use them to classify the testing points.",
                "For example, a testing point will be classified by the local classifier trained using the training points located in the vicinity 1 In the following discussions we all assume that the documents coming from only two classes.",
                "The generalizations of our method to multi-class cases will be discussed in section 2.5. of it.",
                "Although this method seems slow and stupid, it is reported that it can get better performances than using a unique global classifier on certain tasks [6]. 2.2.2 Constructing the Local Regularized Predictors Inspired by their success, we proposed to apply the local learning algorithms for clustering.",
                "The basic idea is that, for each document vector xi (1 i n), we train a local label predictor based on its k-nearest neighborhood Ni, and then use it to predict the label of xi.",
                "Finally we will combine all those local predictors by minimizing the sum of their prediction errors.",
                "In this subsection we will introduce how to construct those local predictors.",
                "Due to the simplicity and effectiveness of the regularized linear classifier that we have introduced in section 2.2.1, we choose it to be our local label predictor, such that for each document xi, the following criterion is minimized Ji = 1 ni xj ∈Ni wT i xj − qj 2 + λi wi 2 , (7) where ni = |Ni| is the cardinality of Ni, and qj is the cluster membership of xj.",
                "Then using Eq. (6), we can get the optimal solution is w∗ i = XiXT i + λiniI −1 Xiqi, (8) where Xi = [xi1, xi2, · · · , xini ], and we use xik to denote the k-th nearest neighbor of xi. qi = [qi1, qi2, · · · , qini ]T with qik representing the cluster assignment of xik.",
                "The problem here is that XiXT i is an m × m matrix with m ni, i.e. we should compute the inverse of an m × m matrix for every document vector, which is computationally prohibited.",
                "Fortunately, we have the following theorem: Theorem 1. w∗ i in Eq. (8) can be rewritten as w∗ i = Xi XT i Xi + λiniIi −1 qi, (9) where Ii is an ni × ni identity matrix.",
                "Proof.",
                "Since w∗ i = XiXT i + λiniI −1 Xiqi, then XiXT i + λiniI w∗ i = Xiqi =⇒ XiXT i w∗ i + λiniw∗ i = Xiqi =⇒ w∗ i = (λini)−1 Xi qi − XT i w∗ i .",
                "Let β = (λini)−1 qi − XT i w∗ i , then w∗ i = Xiβ =⇒ λiniβ = qi − XT i w∗ i = qi − XT i Xiβ =⇒ qi = XT i Xi + λiniIi β =⇒ β = XT i Xi + λiniIi −1 qi.",
                "Therefore w∗ i = Xiβ = Xi XT i Xi + λiniIi −1 qi 2 Using theorem 1, we only need to compute the inverse of an ni × ni matrix for every document to train a local label predictor.",
                "Moreover, for a new testing point u that falls into Ni, we can classify it by the sign of qu = w∗T i u = uT wi = uT Xi XT i Xi + λiniIi −1 qi.",
                "This is an attractive expression since we can determine the cluster assignment of u by using the inner-products between the points in {u ∪ Ni}, which suggests that such a local regularizer can easily be kernelized [21] as long as we define a proper kernel function. 2.2.3 Combining the Local Regularized Predictors After all the local predictors having been constructed, we will combine them together by minimizing Jl = n i=1 w∗T i xi − qi 2 , (10) which stands for the sum of the prediction errors for all the local predictors.",
                "Combining Eq. (10) with Eq. (6), we can get Jl = n i=1 w∗T i xi − qi 2 = n i=1 xT i Xi XT i Xi + λiniIi −1 qi − qi 2 = Pq − q 2 , (11) where q = [q1, q2, · · · , qn]T , and the P is an n × n matrix constructing in the following way.",
                "Let αi = xT i Xi XT i Xi + λiniIi −1 , then Pij = αi j, if xj ∈ Ni 0, otherwise , (12) where Pij is the (i, j)-th entry of P, and αi j represents the j-th entry of αi .",
                "Till now we can write the criterion of clustering by combining locally regularized linear label predictors Jl in an explicit mathematical form, and we can minimize it directly using some standard optimization techniques.",
                "However, the results may not be good enough since we only exploit the local informations of the dataset.",
                "In the next subsection, we will introduce a global regularization criterion and combine it with Jl, which aims to find a good clustering result in a local-global way. 2.3 Global Regularization In data clustering, we usually require that the cluster assignments of the data points should be sufficiently smooth with respect to the underlying data manifold, which implies (1) the nearby points tend to have the same cluster assignments; (2) the points on the same structure (e.g. submanifold or cluster) tend to have the same cluster assignments [31].",
                "Without the loss of generality, we assume that the data points reside (roughly) on a low-dimensional manifold M2 , and q is the cluster assignment function defined on M, i.e. 2 We believe that the text data are also sampled from some low dimensional manifold, since it is impossible for them to for ∀x ∈ M, q(x) returns the cluster membership of x.",
                "The smoothness of q over M can be calculated by the following Dirichlet integral [2] D[q] = 1 2 M q(x) 2 dM, (13) where the gradient q is a vector in the tangent space T Mx, and the integral is taken with respect to the standard measure on M. If we restrict the scale of q by q, q M = 1 (where ·, · M is the inner product induced on M), then it turns out that finding the smoothest function minimizing D[q] reduces to finding the eigenfunctions of the Laplace Beltrami operator L, which is defined as Lq −div q, (14) where div is the divergence of a vector field.",
                "Generally, the graph can be viewed as the discretized form of manifold.",
                "We can model the dataset as an weighted undirected graph as in spectral clustering [22], where the graph nodes are just the data points, and the weights on the edges represent the similarities between pairwise points.",
                "Then it can be shown that minimizing Eq. (13) corresponds to minimizing Jg = qT Lq = n i=1 (qi − qj)2 wij, (15) where q = [q1, q2, · · · , qn]T with qi = q(xi), L is the graph Laplacian with its (i, j)-th entry Lij =    di − wii, if i = j −wij, if xi and xj are adjacent 0, otherwise, (16) where di = j wij is the degree of xi, wij is the similarity between xi and xj.",
                "If xi and xj are adjacent3 , wij is usually computed in the following way wij = e − xi−xj 2 2σ2 , (17) where σ is a dataset dependent parameter.",
                "It is proved that under certain conditions, such a form of wij to determine the weights on graph edges leads to the convergence of graph Laplacian to the Laplace Beltrami operator [3][18].",
                "In summary, using Eq. (15) with exponential weights can effectively measure the smoothness of the data assignments with respect to the intrinsic data manifold.",
                "Thus we adopt it as a global regularizer to punish the smoothness of the predicted data assignments. 2.4 Clustering with Local and Global Regularization Combining the contents we have introduced in section 2.2 and section 2.3 we can derive the clustering criterion is minq J = Jl + λJg = Pq − q 2 + λqT Lq s.t. qi ∈ {−1, +1}, (18) where P is defined as in Eq. (12), and λ is a regularization parameter to trade off Jl and Jg.",
                "However, the discrete fill in the whole high-dimensional sample space.",
                "And it has been shown that the manifold based methods can achieve good results on text classification tasks [31]. 3 In this paper, we define xi and xj to be adjacent if xi ∈ N(xj) or xj ∈ N(xi). constraint of pi makes the problem an NP hard integer programming problem.",
                "A natural way for making the problem solvable is to remove the constraint and relax qi to be continuous, then the objective that we aims to minimize becomes J = Pq − q 2 + λqT Lq = qT (P − I)T (P − I)q + λqT Lq = qT (P − I)T (P − I) + λL q, (19) and we further add a constraint qT q = 1 to restrict the scale of q.",
                "Then our objective becomes minq J = qT (P − I)T (P − I) + λL q s.t. qT q = 1 (20) Using the Lagrangian method, we can derive that the optimal solution q corresponds to the smallest eigenvector of the matrix M = (P − I)T (P − I) + λL, and the cluster assignment of xi can be determined by the sign of qi, i.e. xi will be classified as class one if qi > 0, otherwise it will be classified as class 2. 2.5 Multi-Class CLGR In the above we have introduced the basic framework of Clustering with Local and Global Regularization (CLGR) for the two-class clustering problem, and we will extending it to multi-class clustering in this subsection.",
                "First we assume that all the documents belong to C classes indexed by L = {1, 2, · · · , C}. qc is the classification function for class c (1 c C), such that qc (xi) returns the confidence that xi belongs to class c. Our goal is to obtain the value of qc (xi) (1 c C, 1 i n), and the cluster assignment of xi can be determined by {qc (xi)}C c=1 using some proper discretization methods that we will introduce later.",
                "Therefore, in this multi-class case, for each document xi (1 i n), we will construct C locally linear regularized label predictors whose normal vectors are wc∗ i = Xi XT i Xi + λiniIi −1 qc i (1 c C), (21) where Xi = [xi1, xi2, · · · , xini ] with xik being the k-th neighbor of xi, and qc i = [qc i1, qc i2, · · · , qc ini ]T with qc ik = qc (xik).",
                "Then (wc∗ i )T xi returns the predicted confidence of xi belonging to class c. Hence the local prediction error for class c can be defined as J c l = n i=1 (wc∗ i ) T xi − qc i 2 , (22) And the total local prediction error becomes Jl = C c=1 J c l = C c=1 n i=1 (wc∗ i ) T xi − qc i 2 . (23) As in Eq. (11), we can define an n×n matrix P (see Eq. (12)) and rewrite Jl as Jl = C c=1 J c l = C c=1 Pqc − qc 2 . (24) Similarly we can define the global smoothness regularizer in multi-class case as Jg = C c=1 n i=1 (qc i − qc j )2 wij = C c=1 (qc )T Lqc . (25) Then the criterion to be minimized for CLGR in multi-class case becomes J = Jl + λJg = C c=1 Pqc − qc 2 + λ(qc )T Lqc = C c=1 (qc )T (P − I)T (P − I) + λL qc = trace QT (P − I)T (P − I) + λL Q , (26) where Q = [q1 , q2 , · · · , qc ] is an n × c matrix, and trace(·) returns the trace of a matrix.",
                "The same as in Eq. (20), we also add the constraint that QT Q = I to restrict the scale of Q.",
                "Then our optimization problem becomes minQ J = trace QT (P − I)T (P − I) + λL Q s.t.",
                "QT Q = I, (27) From the Ky Fan theorem [28], we know the optimal solution of the above problem is Q∗ = [q∗ 1, q∗ 2, · · · , q∗ C ]R, (28) where q∗ k (1 k C) is the eigenvector corresponds to the k-th smallest eigenvalue of matrix (P − I)T (P − I) + λL, and R is an arbitrary C × C matrix.",
                "Since the values of the entries in Q∗ is continuous, we need to further discretize Q∗ to get the cluster assignments of all the data points.",
                "There are mainly two approaches to achieve this goal: 1.",
                "As in [20], we can treat the i-th row of Q as the embedding of xi in a C-dimensional space, and apply some traditional clustering methods like kmeans to clustering these embeddings into C clusters. 2.",
                "Since the optimal Q∗ is not unique (because of the existence of an arbitrary matrix R), we can pursue an optimal R that will rotate Q∗ to an indication matrix4 .",
                "The detailed algorithm can be referred to [26].",
                "The detailed algorithm procedure for CLGR is summarized in table 1. 3.",
                "EXPERIMENTS In this section, experiments are conducted to empirically compare the clustering results of CLGR with other 8 representitive document clustering algorithms on 5 datasets.",
                "First we will introduce the basic informations of those datasets. 3.1 Datasets We use a variety of datasets, most of which are frequently used in the information retrieval research.",
                "Table 2 summarizes the characteristics of the datasets. 4 Here an indication matrix T is a n×c matrix with its (i, j)th entry Tij ∈ {0, 1} such that for each row of Q∗ there is only one 1.",
                "Then the xi can be assigned to the j-th cluster such that j = argjQ∗ ij = 1.",
                "Table 1: Clustering with Local and Global Regularization (CLGR) Input: 1.",
                "Dataset X = {xi}n i=1; 2.",
                "Number of clusters C; 3.",
                "Size of the neighborhood K; 4.",
                "Local regularization parameters {λi}n i=1; 5.",
                "Global regularization parameter λ; Output: The cluster membership of each data point.",
                "Procedure: 1.",
                "Construct the K nearest neighborhoods for each data point; 2.",
                "Construct the matrix P using Eq. (12); 3.",
                "Construct the Laplacian matrix L using Eq. (16); 4.",
                "Construct the matrix M = (P − I)T (P − I) + λL; 5.",
                "Do eigenvalue decomposition on M, and construct the matrix Q∗ according to Eq. (28); 6.",
                "Output the cluster assignments of each data point by properly discretize Q∗ .",
                "Table 2: Descriptions of the document datasets Datasets Number of documents Number of classes CSTR 476 4 WebKB4 4199 4 Reuters 2900 10 WebACE 2340 20 Newsgroup4 3970 4 CSTR.",
                "This is the dataset of the abstracts of technical reports published in the Department of Computer Science at a university.",
                "The dataset contained 476 abstracts, which were divided into four research areas: Natural Language Processing(NLP), Robotics/Vision, Systems, and Theory.",
                "WebKB.",
                "The WebKB dataset contains webpages gathered from university computer science departments.",
                "There are about 8280 documents and they are divided into 7 categories: student, faculty, staff, course, project, department and other.",
                "The raw text is about 27MB.",
                "Among these 7 categories, student, faculty, course and project are four most populous entity-representing categories.",
                "The associated subset is typically called WebKB4.",
                "Reuters.",
                "The Reuters-21578 Text Categorization Test collection contains documents collected from the Reuters newswire in 1987.",
                "It is a standard text categorization benchmark and contains 135 categories.",
                "In our experiments, we use a subset of the data collection which includes the 10 most frequent categories among the 135 topics and we call it Reuters-top 10.",
                "WebACE.",
                "The WebACE dataset was from WebACE project and has been used for document clustering [17][5].",
                "The WebACE dataset contains 2340 documents consisting news articles from Reuters new service via the Web in October 1997.",
                "These documents are divided into 20 classes.",
                "News4.",
                "The News4 dataset used in our experiments are selected from the famous 20-newsgroups dataset5 .",
                "The topic rec containing autos, motorcycles, baseball and hockey was selected from the version 20news-18828.",
                "The News4 dataset contains 3970 document vectors. 5 http://people.csail.mit.edu/jrennie/20Newsgroups/ To pre-process the datasets, we remove the stop words using a standard stop list, all HTML tags are skipped and all header fields except subject and organization of the posted articles are ignored.",
                "In all our experiments, we first select the top 1000 words by mutual information with class labels. 3.2 Evaluation Metrics In the experiments, we set the number of clusters equal to the true number of classes C for all the clustering algorithms.",
                "To evaluate their performance, we compare the clusters generated by these algorithms with the true classes by computing the following two performance measures.",
                "Clustering Accuracy (Acc).",
                "The first performance measure is the Clustering Accuracy, which discovers the one-toone relationship between clusters and classes and measures the extent to which each cluster contained data points from the corresponding class.",
                "It sums up the whole matching degree between all pair class-clusters.",
                "Clustering accuracy can be computed as: Acc = 1 N max   Ck,Lm T(Ck, Lm)   , (29) where Ck denotes the k-th cluster in the final results, and Lm is the true m-th class.",
                "T(Ck, Lm) is the number of entities which belong to class m are assigned to cluster k. Accuracy computes the maximum sum of T(Ck, Lm) for all pairs of clusters and classes, and these pairs have no overlaps.",
                "The greater clustering accuracy means the better clustering performance.",
                "Normalized Mutual Information (NMI).",
                "Another evaluation metric we adopt here is the Normalized Mutual Information NMI [23], which is widely used for determining the quality of clusters.",
                "For two random variable X and Y, the NMI is defined as: NMI(X, Y) = I(X, Y) H(X)H(Y) , (30) where I(X, Y) is the mutual information between X and Y, while H(X) and H(Y) are the entropies of X and Y respectively.",
                "One can see that NMI(X, X) = 1, which is the maximal possible value of NMI.",
                "Given a clustering result, the NMI in Eq. (30) is estimated as NMI = C k=1 C m=1 nk,mlog n·nk,m nk ˆnm C k=1 nklog nk n C m=1 ˆnmlog ˆnm n , (31) where nk denotes the number of data contained in the cluster Ck (1 k C), ˆnm is the number of data belonging to the m-th class (1 m C), and nk,m denotes the number of data that are in the intersection between the cluster Ck and the m-th class.",
                "The value calculated in Eq. (31) is used as a performance measure for the given clustering result.",
                "The larger this value, the better the clustering performance. 3.3 Comparisons We have conducted comprehensive performance evaluations by testing our method and comparing it with 8 other representative data clustering methods using the same data corpora.",
                "The algorithms that we evaluated are listed below. 1.",
                "Traditional k-means (KM). 2.",
                "Spherical k-means (SKM).",
                "The implementation is based on [9]. 3.",
                "Gaussian Mixture Model (GMM).",
                "The implementation is based on [16]. 4.",
                "Spectral Clustering with Normalized Cuts (Ncut).",
                "The implementation is based on [26], and the variance of the Gaussian similarity is determined by Local Scaling [30].",
                "Note that the criterion that Ncut aims to minimize is just the global regularizer in our CLGR algorithm except that Ncut used the normalized Laplacian. 5.",
                "Clustering using Pure Local Regularization (CPLR).",
                "In this method we just minimize Jl (defined in Eq. (24)), and the clustering results can be obtained by doing eigenvalue decomposition on matrix (I − P)T (I − P) with some proper discretization methods. 6.",
                "Adaptive Subspace Iteration (ASI).",
                "The implementation is based on [14]. 7.",
                "Nonnegative Matrix Factorization (NMF).",
                "The implementation is based on [27]. 8.",
                "Tri-Factorization Nonnegative Matrix Factorization (TNMF) [12].",
                "The implementation is based on [15].",
                "For computational efficiency, in the implementation of CPLR and our CLGR algorithm, we have set all the local regularization parameters {λi}n i=1 to be identical, which is set by grid search from {0.1, 1, 10}.",
                "The size of the k-nearest neighborhoods is set by grid search from {20, 40, 80}.",
                "For the CLGR method, its global regularization parameter is set by grid search from {0.1, 1, 10}.",
                "When constructing the global regularizer, we have adopted the local scaling method [30] to construct the Laplacian matrix.",
                "The final discretization method adopted in these two methods is the same as in [26], since our experiments show that using such method can achieve better results than using kmeans based methods as in [20]. 3.4 Experimental Results The clustering accuracies comparison results are shown in table 3, and the normalized mutual information comparison results are summarized in table 4.",
                "From the two tables we mainly observe that: 1.",
                "Our CLGR method outperforms all other document clustering methods in most of the datasets; 2.",
                "For document clustering, the Spherical k-means method usually outperforms the traditional k-means clustering method, and the GMM method can achieve competitive results compared to the Spherical k-means method; 3.",
                "The results achieved from the k-means and GMM type algorithms are usually worse than the results achieved from Spectral Clustering.",
                "Since Spectral Clustering can be viewed as a weighted version of kernel k-means, it can obtain good results the data clusters are arbitrarily shaped.",
                "This corroborates that the documents vectors are not regularly distributed (spherical or elliptical). 4.",
                "The experimental comparisons empirically verify the equivalence between NMF and Spectral Clustering, which Table 3: Clustering accuracies of the various methods CSTR WebKB4 Reuters WebACE News4 KM 0.4256 0.3888 0.4448 0.4001 0.3527 SKM 0.4690 0.4318 0.5025 0.4458 0.3912 GMM 0.4487 0.4271 0.4897 0.4521 0.3844 NMF 0.5713 0.4418 0.4947 0.4761 0.4213 Ncut 0.5435 0.4521 0.4896 0.4513 0.4189 ASI 0.5621 0.4752 0.5235 0.4823 0.4335 TNMF 0.6040 0.4832 0.5541 0.5102 0.4613 CPLR 0.5974 0.5020 0.4832 0.5213 0.4890 CLGR 0.6235 0.5228 0.5341 0.5376 0.5102 Table 4: Normalized mutual information results of the various methods CSTR WebKB4 Reuters WebACE News4 KM 0.3675 0.3023 0.4012 0.3864 0.3318 SKM 0.4027 0.4155 0.4587 0.4003 0.4085 GMM 0.4034 0.4093 0.4356 0.4209 0.3994 NMF 0.5235 0.4517 0.4402 0.4359 0.4130 Ncut 0.4833 0.4497 0.4392 0.4289 0.4231 ASI 0.5008 0.4833 0.4769 0.4817 0.4503 TNMF 0.5724 0.5011 0.5132 0.5328 0.4749 CPLR 0.5695 0.5231 0.4402 0.5543 0.4690 CLGR 0.6012 0.5434 0.4935 0.5390 0.4908 has been proved theoretically in [10].",
                "It can be observed from the tables that NMF and Spectral Clustering usually lead to similar clustering results. 5.",
                "The co-clustering based methods (TNMF and ASI) can usually achieve better results than traditional purely document vector based methods.",
                "Since these methods perform an implicit feature selection at each iteration, provide an adaptive metric for measuring the neighborhood, and thus tend to yield better clustering results. 6.",
                "The results achieved from CPLR are usually better than the results achieved from Spectral Clustering, which supports Vapniks theory [24] that sometimes local learning algorithms can obtain better results than global learning algorithms.",
                "Besides the above comparison experiments, we also test the parameter sensibility of our method.",
                "There are mainly two sets of parameters in our CLGR algorithm, the local and global regularization parameters ({λi}n i=1 and λ, as we have said in section 3.3, we have set all λis to be identical to λ∗ in our experiments), and the size of the neighborhoods.",
                "Therefore we have also done two sets of experiments: 1.",
                "Fixing the size of the neighborhoods, and testing the clustering performance with varying λ∗ and λ.",
                "In this set of experiments, we find that our CLGR algorithm can achieve good results when the two regularization parameters are neither too large nor too small.",
                "Typically our method can achieve good results when λ∗ and λ are around 0.1.",
                "Figure 1 shows us such a testing example on the WebACE dataset. 2.",
                "Fixing the local and global regularization parameters, and testing the clustering performance with different −5 −4.5 −4 −3.5 −3 −5 −4.5 −4 −3.5 −3 0.35 0.4 0.45 0.5 0.55 local regularization para (log 2 value) global regularization para (log 2 value) clusteringaccuracy Figure 1: Parameter sensibility testing results on the WebACE dataset with the neighborhood size fixed to 20, and the x-axis and y-axis represents the log2 value of λ∗ and λ. sizes of neighborhoods.",
                "In this set of experiments, we find that the neighborhood with a too large or too small size will all deteriorate the final clustering results.",
                "This can be easily understood since when the neighborhood size is very small, then the data points used for training the local classifiers may not be sufficient; when the neighborhood size is very large, the trained classifiers will tend to be global and cannot capture the typical local characteristics.",
                "Figure 2 shows us a testing example on the WebACE dataset.",
                "Therefore, we can see that our CLGR algorithm (1) can achieve satisfactory results and (2) is not very sensitive to the choice of parameters, which makes it practical in real world applications. 4.",
                "CONCLUSIONS AND FUTURE WORKS In this paper, we derived a new clustering algorithm called clustering with local and global regularization.",
                "Our method preserves the merit of local learning algorithms and spectral clustering.",
                "Our experiments show that the proposed algorithm outperforms most of the state of the art algorithms on many benchmark datasets.",
                "In the future, we will focus on the parameter selection and acceleration issues of the CLGR algorithm. 5.",
                "REFERENCES [1] L. Baker and A. McCallum.",
                "Distributional Clustering of Words for Text Classification.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 1998. [2] M. Belkin and P. Niyogi.",
                "Laplacian Eigenmaps for Dimensionality Reduction and Data Representation.",
                "Neural Computation, 15 (6):1373-1396.",
                "June 2003. [3] M. Belkin and P. Niyogi.",
                "Towards a Theoretical Foundation for Laplacian-Based Manifold Methods.",
                "In Proceedings of the 18th Conference on Learning Theory (COLT). 2005. 10 20 30 40 50 60 70 80 90 100 0.35 0.4 0.45 0.5 0.55 size of the neighborhood clusteringaccuracy Figure 2: Parameter sensibility testing results on the WebACE dataset with the regularization parameters being fixed to 0.1, and the neighborhood size varing from 10 to 100. [4] M. Belkin, P. Niyogi and V. Sindhwani.",
                "Manifold Regularization: a Geometric Framework for Learning from Examples.",
                "Journal of Machine Learning Research 7, 1-48, 2006. [5] D. Boley.",
                "Principal Direction Divisive Partitioning.",
                "Data mining and knowledge discovery, 2:325-344, 1998. [6] L. Bottou and V. Vapnik.",
                "Local learning algorithms.",
                "Neural Computation, 4:888-900, 1992. [7] P. K. Chan, D. F. Schlag and J. Y. Zien.",
                "Spectral K-way Ratio-Cut Partitioning and Clustering.",
                "IEEE Trans.",
                "Computer-Aided Design, 13:1088-1096, Sep. 1994. [8] D. R. Cutting, D. R. Karger, J. O. Pederson and J. W. Tukey.",
                "Scatter/Gather: A Cluster-Based Approach to Browsing Large Document Collections.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 1992. [9] I. S. Dhillon and D. S. Modha.",
                "Concept Decompositions for Large Sparse Text Data using Clustering.",
                "Machine Learning, vol. 42(1), pages 143-175, January 2001. [10] C. Ding, X.",
                "He, and H. Simon.",
                "On the equivalence of nonnegative matrix factorization and spectral clustering.",
                "In Proceedings of the SIAM Data Mining Conference, 2005. [11] C. Ding, X.",
                "He, H. Zha, M. Gu, and H. D. Simon.",
                "A min-max cut algorithm for graph partitioning and data clustering.",
                "In Proc. of the 1st International Conference on Data Mining (ICDM), pages 107-114, 2001. [12] C. Ding, T. Li, W. Peng, and H. Park.",
                "Orthogonal Nonnegative Matrix Tri-Factorizations for Clustering.",
                "In Proceedings of the Twelfth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2006. [13] R. O. Duda, P. E. Hart, and D. G. Stork.",
                "Pattern Classification.",
                "John Wiley & Sons, Inc., 2001. [14] T. Li, S. Ma, and M. Ogihara.",
                "Document Clustering via Adaptive Subspace Iteration.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2004. [15] T. Li and C. Ding.",
                "The Relationships Among Various Nonnegative Matrix Factorization Methods for Clustering.",
                "In Proceedings of the 6th International Conference on Data Mining (ICDM). 2006. [16] X. Liu and Y. Gong.",
                "Document Clustering with Cluster Refinement and Model Selection Capabilities.",
                "In Proc. of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002. [17] E. Han, D. Boley, M. Gini, R. Gross, K. Hastings, G. Karypis, V. Kumar, B. Mobasher, and J. Moore.",
                "WebACE: A Web Agent for Document Categorization and Exploration.",
                "In Proceedings of the 2nd International Conference on Autonomous Agents (Agents98).",
                "ACM Press, 1998. [18] M. Hein, J. Y. Audibert, and U. von Luxburg.",
                "From Graphs to Manifolds - Weak and Strong Pointwise Consistency of Graph Laplacians.",
                "In Proceedings of the 18th Conference on Learning Theory (COLT), 470-485. 2005. [19] J.",
                "He, M. Lan, C.-L. Tan, S.-Y.",
                "Sung, and H.-B.",
                "Low.",
                "Initialization of Cluster Refinement Algorithms: A Review and Comparative Study.",
                "In Proc. of Inter.",
                "Joint Conference on Neural Networks, 2004. [20] A. Y. Ng, M. I. Jordan, Y. Weiss.",
                "On Spectral Clustering: Analysis and an algorithm.",
                "In Advances in Neural Information Processing Systems 14. 2002. [21] B. Sch¨olkopf and A. Smola.",
                "Learning with Kernels.",
                "The MIT Press.",
                "Cambridge, Massachusetts. 2002. [22] J. Shi and J. Malik.",
                "Normalized Cuts and Image Segmentation.",
                "IEEE Trans. on Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000. [23] A. Strehl and J. Ghosh.",
                "Cluster Ensembles - A Knowledge Reuse Framework for Combining Multiple Partitions.",
                "Journal of Machine Learning Research, 3:583-617, 2002. [24] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Berlin: Springer-Verlag, 1995. [25] Wu, M. and Sch¨olkopf, B.",
                "A Local Learning Approach for Clustering.",
                "In Advances in Neural Information Processing Systems 18. 2006. [26] S. X. Yu, J. Shi.",
                "Multiclass Spectral Clustering.",
                "In Proceedings of the International Conference on Computer Vision, 2003. [27] W. Xu, X. Liu and Y. Gong.",
                "Document Clustering Based On Non-Negative Matrix Factorization.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2003. [28] H. Zha, X.",
                "He, C. Ding, M. Gu and H. Simon.",
                "Spectral Relaxation for K-means Clustering.",
                "In NIPS 14. 2001. [29] T. Zhang and F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Journal of Information Retrieval, 4:5-31, 2001. [30] L. Zelnik-Manor and P. Perona.",
                "Self-Tuning Spectral Clustering.",
                "In NIPS 17. 2005. [31] D. Zhou, O. Bousquet, T. N. Lal, J. Weston and B. Sch¨olkopf.",
                "Learning with Local and Global Consistency.",
                "NIPS 17, 2005."
            ],
            "original_annotated_samples": [
                "Traditional document retrieval engines tend to fit well with the search end of the spectrum, i.e. they usually provide <br>specified search</br> for documents matching the users query, however, it is hard for them to meet the needs from the rest of the spectrum in which a rather broad or vague information is needed."
            ],
            "translated_annotated_samples": [
                "Los motores de búsqueda de documentos tradicionales tienden a adaptarse bien al extremo de la búsqueda, es decir, suelen proporcionar <br>búsquedas específicas</br> de documentos que coinciden con la consulta de los usuarios, sin embargo, les resulta difícil satisfacer las necesidades del resto del espectro en el que se necesita información más amplia o vaga."
            ],
            "translated_text": "Agrupamiento regularizado para documentos ∗ Fei Wang, Changshui Zhang Laboratorio Clave del Estado de Tecnología Inteligente. En los últimos años, la agrupación de documentos ha estado recibiendo cada vez más atención como una técnica importante y fundamental para la organización no supervisada de documentos, la extracción automática de temas y la recuperación o filtrado rápido de información. En este artículo, proponemos un método novedoso para agrupar documentos utilizando regularización. A diferencia de los métodos de agrupamiento regularizados globalmente tradicionales, nuestro método primero construye un predictor de etiquetas lineal regularizado local para cada vector de documento, y luego combina todos esos regularizadores locales con un regularizador de suavidad global. Así que llamamos a nuestro algoritmo Agrupamiento con Regularización Local y Global (CLGR). Mostraremos que las pertenencias de los documentos a los grupos se pueden lograr mediante la descomposición de los valores propios de una matriz simétrica dispersa, la cual puede resolverse eficientemente mediante métodos iterativos. Finalmente, se presentan nuestras evaluaciones experimentales en varios conjuntos de datos para mostrar las ventajas de CLGR sobre los métodos tradicionales de agrupamiento de documentos. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información-Agrupamiento; I.2.6 [Inteligencia Artificial]: Aprendizaje-Aprendizaje de Conceptos Términos Generales Algoritmos 1. La agrupación de documentos ha estado recibiendo cada vez más atención como una técnica importante y fundamental para la organización no supervisada de documentos, la extracción automática de temas y la recuperación o filtrado rápido de información. Un buen enfoque de agrupación de documentos puede ayudar a las computadoras a organizar automáticamente el corpus de documentos en una jerarquía de clusters significativa para una navegación eficiente, lo cual es muy valioso para complementar las deficiencias de las tecnologías tradicionales de recuperación de información. Como señaló [8], las necesidades de recuperación de información pueden expresarse mediante un espectro que va desde la búsqueda basada en palabras clave estrechas hasta la exploración de información amplia, como cuáles son los principales eventos internacionales de los últimos meses. Los motores de búsqueda de documentos tradicionales tienden a adaptarse bien al extremo de la búsqueda, es decir, suelen proporcionar <br>búsquedas específicas</br> de documentos que coinciden con la consulta de los usuarios, sin embargo, les resulta difícil satisfacer las necesidades del resto del espectro en el que se necesita información más amplia o vaga. En tales casos, la navegación eficiente a través de una buena jerarquía de clústeres será definitivamente útil. Generalmente, los métodos de agrupamiento de documentos se pueden clasificar principalmente en dos clases: métodos jerárquicos y métodos de partición. Los métodos jerárquicos agrupan los puntos de datos en una estructura de árbol jerárquico utilizando enfoques de abajo hacia arriba o de arriba hacia abajo. Por ejemplo, el clustering jerárquico aglomerativo (HAC) [13] es un método típico de clustering jerárquico ascendente. Toma cada punto de datos como un único clúster para comenzar y luego construye clústeres más grandes agrupando puntos de datos similares juntos hasta que todo el conjunto de datos esté encapsulado en un único clúster final. Por otro lado, los métodos de particionamiento descomponen el conjunto de datos en un número de grupos disjuntos que suelen ser óptimos en términos de algunas funciones de criterio predefinidas. Por ejemplo, K-means es un método de particionamiento típico que tiene como objetivo minimizar la suma de las distancias al cuadrado entre los puntos de datos y sus centros de clúster correspondientes. En este documento, nos centraremos en los métodos de particionamiento. Como sabemos, existen dos problemas principales en los métodos de particionamiento (como Kmeans y el Modelo de Mezcla Gaussiana (GMM) [16]): (1) el criterio predefinido suele ser no convexo, lo que provoca muchas soluciones óptimas locales; (2) el procedimiento iterativo (por ejemplo, el algoritmo de Expectation Maximization (EM)) para optimizar los criterios suele hacer que las soluciones finales dependan en gran medida de las inicializaciones. En las últimas décadas, se han propuesto muchos métodos para superar los problemas mencionados de los métodos de particionamiento [19][28]. Recientemente, otro tipo de métodos de particionamiento basados en agrupamiento en grafos de datos han despertado un considerable interés en la comunidad de aprendizaje automático y minería de datos. La idea básica detrás de estos métodos es modelar primero el conjunto de datos completo como un grafo ponderado, en el que los nodos del grafo representan los puntos de datos y los pesos en los bordes corresponden a las similitudes entre los puntos en pares. Entonces, las asignaciones de clústeres del conjunto de datos se pueden lograr optimizando algunos criterios definidos en el gráfico. Por ejemplo, el Clustering Espectral es uno de los enfoques de clustering basados en grafos más representativos, generalmente tiene como objetivo optimizar algún valor de corte (por ejemplo, Corte normalizado [22], corte de razón [7], corte mínimo-máximo [11]) definidos en un grafo no dirigido. Después de algunas relajaciones, estos criterios generalmente pueden ser optimizados a través de descomposiciones propias, lo cual está garantizado que sea óptimo a nivel global. De esta manera, el agrupamiento espectral evita eficientemente los problemas de los métodos de particionamiento tradicionales que presentamos en el párrafo anterior. En este artículo, proponemos un algoritmo novedoso de agrupamiento de documentos que hereda la superioridad del agrupamiento espectral, es decir, los resultados finales de los grupos también pueden obtenerse explotando la estructura de los autovalores de una matriz simétrica. Sin embargo, a diferencia del agrupamiento espectral, que simplemente impone una restricción de suavidad en las etiquetas de los datos sobre todo el conjunto de datos [2], nuestro método primero construye un predictor de etiquetas lineal regularizado para cada punto de datos a partir de su vecindario como en [25], y luego combina los resultados de todos estos predictores de etiquetas locales con un regularizador de suavidad de etiquetas global. Así que llamamos a nuestro método Agrupamiento con Regularización Local y Global (CLGR). La idea de incorporar tanto información local como global en la predicción de etiquetas está inspirada en los trabajos recientes sobre aprendizaje semi-supervisado [31], y nuestras evaluaciones experimentales en varios conjuntos de datos reales de documentos muestran que CLGR funciona mejor que muchos métodos de agrupamiento de vanguardia. El resto de este documento está organizado de la siguiente manera: en la sección 2 presentaremos nuestro algoritmo CLGR en detalle. Los resultados experimentales en varios conjuntos de datos se presentan en la sección 3, seguidos de las conclusiones y discusiones en la sección 4. El ALGORITMO PROPUESTO En esta sección, presentaremos en detalle nuestro algoritmo de Agrupamiento con Regularización Local y Global (CLGR). Primero veamos cómo se representan los documentos a lo largo de este documento. 2.1 Representación de documentos En nuestro trabajo, todos los documentos se representan mediante vectores de frecuencia de términos ponderados. Sea W = {w1, w2, · · · , wm} el conjunto completo de vocabulario del corpus de documentos (que ha sido preprocesado mediante la eliminación de palabras vacías y operaciones de derivación de palabras). El vector de frecuencia de término xi del documento di se define como xi = [xi1, xi2, · · · , xim]T , xik = tik log n idfk , donde tik es la frecuencia del término wk ∈ W, n es el tamaño del corpus de documentos, idfk es el número de documentos que contienen la palabra wk. De esta manera, xi también se llama la representación TFIDF del documento di. Además, también normalizamos cada xi (1 i n) para que tenga una longitud unitaria, de modo que cada documento esté representado por un vector TF-IDF normalizado. 2.2 Regularización Local Como su nombre sugiere, CLGR está compuesto por dos partes: regularización local y regularización global. En esta subsección introduciremos detalladamente la parte de regularización local. 2.2.1 Motivación Como sabemos que el agrupamiento es un tipo de técnica de aprendizaje, tiene como objetivo organizar el conjunto de datos de una manera razonable. En general, el aprendizaje puede plantearse como un problema de estimación de funciones, a partir del cual podemos obtener una buena función de clasificación que asignará etiquetas al conjunto de datos de entrenamiento e incluso al conjunto de datos de prueba no vistos con algún costo minimizado [24]. Por ejemplo, en el escenario de clasificación de dos clases (en el que conocemos exactamente la etiqueta de cada documento), un clasificador lineal con ajuste de mínimos cuadrados tiene como objetivo aprender un vector columna w de manera que el costo al cuadrado J = 1 n (wT xi − yi)2 (1) se minimice, donde yi ∈ {+1, −1} es la etiqueta de xi. Al tomar ∂J /∂w = 0, obtenemos la solución w∗ = Σ i=1 n xixT i −1 Σ i=1 n xiyi, (2) que puede escribirse en su forma matricial como w∗ = XXT −1 Xy, (3) donde X = [x1, x2, · · · , xn] es una matriz de documentos m × n, y = [y1, y2, · · · , yn]T es el vector de etiquetas. Entonces, para un documento de prueba t, podemos determinar su etiqueta mediante l = signo(w∗T u), donde signo(·) es la función signo. Un problema natural en la ecuación (3) es que la matriz XXT puede ser singular y, por lo tanto, no invertible (por ejemplo, cuando m n). Para evitar tal problema, podemos agregar un término de regularización y minimizar el siguiente criterio J = 1 n n i=1 (wT xi − yi)2 + λ w 2, (5), donde λ es un parámetro de regularización. Entonces, la solución óptima que minimiza J está dada por w∗ = XXT + λnI −1 Xy, (6) donde I es una matriz identidad de tamaño m × m. Se ha informado que el clasificador lineal regularizado puede lograr resultados muy buenos en problemas de clasificación de texto [29]. Sin embargo, a pesar de su éxito empírico, el clasificador lineal regularizado es en realidad un clasificador global, es decir, w∗ se estima utilizando todo el conjunto de entrenamiento. Según [24], esta puede que no sea una idea inteligente, ya que un único w∗ puede que no sea lo suficientemente bueno para predecir las etiquetas de todo el espacio de entrada. Para obtener predicciones más precisas, [6] propuso entrenar clasificadores localmente y utilizarlos para clasificar los puntos de prueba. Por ejemplo, un punto de prueba será clasificado por el clasificador local entrenado utilizando los puntos de entrenamiento ubicados en la vecindad 1. En las siguientes discusiones todos asumimos que los documentos provienen solo de dos clases. Las generalizaciones de nuestro método a casos de múltiples clases se discutirán en la sección 2.5 de él. Aunque este método parece lento y estúpido, se informa que puede obtener mejores rendimientos que usar un clasificador global único en ciertas tareas [6]. 2.2.2 Construyendo los Predictores Localmente Regularizados Inspirados en su éxito, propusimos aplicar los algoritmos de aprendizaje local para el agrupamiento. La idea básica es que, para cada vector de documento xi (1 i n), entrenamos un predictor de etiquetas local basado en su vecindario k-más cercano Ni, y luego lo usamos para predecir la etiqueta de xi. Finalmente combinaremos todos esos predictores locales minimizando la suma de sus errores de predicción. En esta subsección introduciremos cómo construir esos predictores locales. Debido a la simplicidad y efectividad del clasificador lineal regularizado que hemos introducido en la sección 2.2.1, lo elegimos como nuestro predictor de etiquetas local, de modo que para cada documento xi, se minimiza el siguiente criterio Ji = 1 ni xj ∈Ni wT i xj − qj 2 + λi wi 2, (7), donde ni = |Ni| es la cardinalidad de Ni, y qj es la pertenencia al clúster de xj. Luego, utilizando la Ecuación (6), podemos obtener que la solución óptima es w∗ i = XiXT i + λiniI −1 Xiqi, (8), donde Xi = [xi1, xi2, · · · , xini], y usamos xik para denotar al k-ésimo vecino más cercano de xi. qi = [qi1, qi2, · · · , qini]T con qik representando la asignación de clúster de xik. El problema aquí es que XiXT i es una matriz m × m con m ni, es decir, deberíamos calcular la inversa de una matriz m × m para cada vector de documento, lo cual está prohibido computacionalmente. Afortunadamente, tenemos el siguiente teorema: Teorema 1. w∗ i en la Ec. (8) puede ser reescrito como w∗ i = Xi XT i Xi + λiniIi −1 qi, (9), donde Ii es una matriz identidad de tamaño ni × ni. Prueba. Dado que w∗ i = XiXT i + λiniI −1 Xiqi, entonces XiXT i + λiniI w∗ i = Xiqi =⇒ XiXT i w∗ i + λiniw∗ i = Xiqi =⇒ w∗ i = (λini)−1 Xi qi − XT i w∗ i. Sea β = (λini)−1 qi − XT i w∗ i, entonces w∗ i = Xiβ =⇒ λiniβ = qi − XT i w∗ i = qi − XT i Xiβ =⇒ qi = XT i Xi + λiniIi β =⇒ β = XT i Xi + λiniIi −1 qi. Por lo tanto, w∗ i = Xiβ = Xi XT i Xi + λiniIi −1 qi 2. Utilizando el teorema 1, solo necesitamos calcular la inversa de una matriz ni × ni para cada documento para entrenar un predictor de etiquetas local. Además, para un nuevo punto de prueba u que cae en Ni, podemos clasificarlo por el signo de qu = w∗T i u = uT wi = uT Xi XT i Xi + λiniIi −1 qi. Esta es una expresión atractiva ya que podemos determinar la asignación del clúster de u utilizando los productos internos entre los puntos en {u ∪ Ni}, lo que sugiere que un regularizador local de este tipo puede ser fácilmente kernelizado [21] siempre y cuando definamos una función de kernel adecuada. 2.2.3 Combinando los Predictores Regularizados Localmente Una vez que todos los predictores locales hayan sido construidos, los combinaremos minimizando Jl = n i=1 w∗T i xi − qi 2, (10), lo que representa la suma de los errores de predicción de todos los predictores locales. Combinando la Ecuación (10) con la Ecuación (6), podemos obtener Jl = n i=1 w∗T i xi − qi 2 = n i=1 xT i Xi XT i Xi + λiniIi −1 qi − qi 2 = Pq − q 2, (11), donde q = [q1, q2, · · · , qn]T, y P es una matriz n × n construida de la siguiente manera. Sea αi = xT i Xi XT i Xi + λiniIi −1 , entonces Pij = αi j, si xj ∈ Ni 0, de lo contrario , (12) donde Pij es la entrada (i, j)-ésima de P, y αi j representa la entrada j-ésima de αi. Hasta ahora podemos escribir el criterio de agrupamiento combinando predictores de etiquetas lineales localmente regularizados Jl en una forma matemática explícita, y podemos minimizarlo directamente utilizando algunas técnicas estándar de optimización. Sin embargo, los resultados pueden no ser lo suficientemente buenos ya que solo explotamos la información local del conjunto de datos. En la siguiente subsección, introduciremos un criterio de regularización global y lo combinaremos con Jl, que tiene como objetivo encontrar un buen resultado de agrupamiento de manera local-global. 2.3 Regularización Global En el agrupamiento de datos, generalmente requerimos que las asignaciones de clúster de los puntos de datos sean lo suficientemente suaves con respecto a la variedad de datos subyacente, lo que implica (1) que los puntos cercanos tienden a tener las mismas asignaciones de clúster; (2) que los puntos en la misma estructura (por ejemplo, subvariedad o clúster) tienden a tener las mismas asignaciones de clúster [31]. Sin pérdida de generalidad, asumimos que los puntos de datos residen (aproximadamente) en una variedad de baja dimensión M2, y q es la función de asignación de clúster definida en M, es decir, 2 Creemos que los datos de texto también se muestrean de alguna variedad de baja dimensión, ya que es imposible que para todos los x ∈ M, q(x) devuelva la membresía del clúster de x. La suavidad de q sobre M se puede calcular mediante la siguiente integral de Dirichlet [2] D[q] = 1 2 M q(x) 2 dM, (13) donde el gradiente q es un vector en el espacio tangente T Mx, y la integral se toma con respecto a la medida estándar en M. Si restringimos la escala de q por q, q M = 1 (donde ·, · M es el producto interno inducido en M), entonces resulta que encontrar la función más suave que minimiza D[q] se reduce a encontrar las autofunciones del operador Laplace Beltrami L, que se define como Lq −div q, (14) donde div es la divergencia de un campo vectorial. Generalmente, el gráfico se puede ver como la forma discretizada de una variedad. Podemos modelar el conjunto de datos como un grafo ponderado no dirigido como en el agrupamiento espectral [22], donde los nodos del grafo son simplemente los puntos de datos, y los pesos en las aristas representan las similitudes entre pares de puntos. Entonces se puede demostrar que minimizar la Ec. (13) corresponde a minimizar Jg = qT Lq = n i=1 (qi − qj)2 wij, (15), donde q = [q1, q2, · · · , qn]T con qi = q(xi), L es el Laplaciano del grafo con su entrada (i, j)-ésima Lij =    di − wii, si i = j −wij, si xi y xj son adyacentes 0, en otro caso, (16), donde di = j wij es el grado de xi, wij es la similitud entre xi y xj. Si xi y xj son adyacentes, wij generalmente se calcula de la siguiente manera: wij = e − xi−xj 2 2σ2, donde σ es un parámetro dependiente del conjunto de datos. Se ha demostrado que bajo ciertas condiciones, tal forma de wij para determinar los pesos en los bordes del grafo conduce a la convergencia del Laplaciano del grafo al operador Laplace Beltrami [3][18]. En resumen, el uso de la Ec. (15) con pesos exponenciales puede medir de manera efectiva la suavidad de las asignaciones de datos con respecto a la variedad intrínseca de datos. Por lo tanto, lo adoptamos como un regularizador global para penalizar la suavidad de las asignaciones de datos predichas. 2.4 Agrupamiento con Regularización Local y Global Combinando los contenidos que hemos introducido en la sección 2.2 y la sección 2.3, podemos derivar que el criterio de agrupamiento es minq J = Jl + λJg = Pq − q 2 + λqT Lq sujeto a qi ∈ {−1, +1}, (18) donde P está definido como en la Ecuación (12), y λ es un parámetro de regularización para equilibrar Jl y Jg. Sin embargo, los valores discretos llenan todo el espacio muestral de alta dimensionalidad. Y se ha demostrado que los métodos basados en variedades pueden lograr buenos resultados en tareas de clasificación de texto [31]. En este artículo, definimos xi y xj como adyacentes si xi ∈ N(xj) o xj ∈ N(xi). La restricción de pi convierte el problema en un problema de programación entera NP difícil. Una forma natural de hacer que el problema sea resoluble es eliminar la restricción y relajar qi para que sea continuo, luego el objetivo que buscamos minimizar se convierte en J = Pq − q 2 + λqT Lq = qT (P − I)T (P − I)q + λqT Lq = qT (P − I)T (P − I) + λL q, (19) y luego agregamos una restricción adicional qT q = 1 para restringir la escala de q. Entonces nuestro objetivo se convierte en minq J = qT (P − I)T (P − I) + λL q s.t. qT q = 1 (20) Utilizando el método de Lagrange, podemos derivar que la solución óptima q corresponde al menor vector propio de la matriz M = (P − I)T (P − I) + λL, y la asignación de clúster de xi puede determinarse por el signo de qi, es decir, xi se clasificará como clase uno si qi > 0, de lo contrario se clasificará como clase 2. 2.5 CLGR Multiclase En lo anterior hemos introducido el marco básico de Agrupamiento con Regularización Local y Global (CLGR) para el problema de agrupamiento de dos clases, y lo extenderemos al agrupamiento multiclase en esta subsección. Primero asumimos que todos los documentos pertenecen a C clases indexadas por L = {1, 2, · · · , C}. qc es la función de clasificación para la clase c (1 ≤ c ≤ C), tal que qc(xi) devuelve la confianza de que xi pertenece a la clase c. Nuestro objetivo es obtener el valor de qc(xi) (1 ≤ c ≤ C, 1 ≤ i ≤ n), y la asignación de cluster de xi puede ser determinada por {qc(xi)}C c=1 utilizando algunos métodos adecuados de discretización que presentaremos más adelante. Por lo tanto, en este caso de múltiples clases, para cada documento xi (1 i n), construiremos C predictores de etiquetas regularizados localmente lineales cuyos vectores normales son wc∗ i = Xi XT i Xi + λiniIi −1 qc i (1 c C), (21), donde Xi = [xi1, xi2, · · · , xini ] con xik siendo el k-ésimo vecino de xi, y qc i = [qc i1, qc i2, · · · , qc ini ]T con qc ik = qc (xik). Entonces (wc∗ i )T xi devuelve la confianza predicha de xi perteneciente a la clase c. Por lo tanto, el error de predicción local para la clase c se puede definir como J c l = n i=1 (wc∗ i ) T xi − qc i 2 , (22) Y el error de predicción local total se convierte en Jl = C c=1 J c l = C c=1 n i=1 (wc∗ i ) T xi − qc i 2 . (23) Como en la Ec. (11), podemos definir una matriz n×n P (ver Ec. (12)) y reescribir Jl como Jl = C c=1 J c l = C c=1 Pqc − qc 2 . (24) De manera similar, podemos definir el regularizador de suavidad global en el caso de múltiples clases como Jg = C c=1 n i=1 (qc i − qc j )2 wij = C c=1 (qc )T Lqc . (25) Luego, el criterio a minimizar para CLGR en el caso de múltiples clases se convierte en J = Jl + λJg = C c=1 Pqc − qc 2 + λ(qc )T Lqc = C c=1 (qc )T (P − I)T (P − I) + λL qc = trace QT (P − I)T (P − I) + λL Q , (26) donde Q = [q1 , q2 , · · · , qc ] es una matriz n × c, y trace(·) devuelve la traza de una matriz. Al igual que en la ecuación (20), también agregamos la restricción de que QT Q = I para limitar la escala de Q. Entonces nuestro problema de optimización se convierte en minQ J = traza QT (P − I)T (P − I) + λL Q sujeto a. Según el teorema de Ky Fan [28], sabemos que la solución óptima del problema anterior es Q∗ = [q∗ 1, q∗ 2, · · · , q∗ C ]R, donde q∗ k (1 ≤ k ≤ C) es el autovector que corresponde al k-ésimo valor propio más pequeño de la matriz (P − I)T (P − I) + λL, y R es una matriz arbitraria de tamaño C × C. Dado que los valores de las entradas en Q∗ son continuos, necesitamos discretizar aún más Q∗ para obtener las asignaciones de clúster de todos los puntos de datos. Principalmente hay dos enfoques para lograr este objetivo: 1. Como en [20], podemos tratar la i-ésima fila de Q como la incrustación de xi en un espacio de C dimensiones, y aplicar algunos métodos de agrupamiento tradicionales como kmeans para agrupar estas incrustaciones en C grupos. 2. Dado que el Q∗ óptimo no es único (debido a la existencia de una matriz R arbitraria), podemos buscar un R óptimo que rote Q∗ a una matriz de indicación4. El algoritmo detallado se puede consultar en [26]. El procedimiento detallado del algoritmo para CLGR se resume en la tabla 1. 3. EXPERIMENTOS En esta sección, se realizan experimentos para comparar empíricamente los resultados de agrupamiento de CLGR con otros 8 algoritmos representativos de agrupamiento de documentos en 5 conjuntos de datos. Primero presentaremos la información básica de esos conjuntos de datos. 3.1 Conjuntos de datos Utilizamos una variedad de conjuntos de datos, la mayoría de los cuales son frecuentemente utilizados en la investigación de recuperación de información. La Tabla 2 resume las características de los conjuntos de datos. Aquí, una matriz de indicación T es una matriz n×c con su entrada (i, j) Tij ∈ {0, 1} de modo que para cada fila de Q∗ solo hay un 1. Entonces, el xi puede ser asignado al j-ésimo grupo tal que j = argjQ∗ ij = 1. Tabla 1: Agrupamiento con Regularización Local y Global (CLGR) Entrada: 1. Conjunto de datos X = {xi}n i=1; 2. Número de grupos C: 3. Tamaño del vecindario K: 4. Parámetros locales de regularización {λi}n i=1; 5. Parámetro de regularización global λ; Salida: La membresía del clúster de cada punto de datos. Procedimiento: 1. Construir los K vecindarios más cercanos para cada punto de datos; 2. Construya la matriz P utilizando la Ecuación (12); 3. Construya la matriz Laplaciana L utilizando la ecuación (16); 4. Construye la matriz M = (P − I)T (P − I) + λL; 5. Realice la descomposición de valores propios en M y construya la matriz Q∗ de acuerdo con la Ecuación (28); 6. Generar las asignaciones de clúster de cada punto de datos al discretizar adecuadamente Q∗. Tabla 2: Descripciones de los conjuntos de datos de documentos Conjuntos de datos Número de documentos Número de clases CSTR 476 4 WebKB4 4199 4 Reuters 2900 10 WebACE 2340 20 Newsgroup4 3970 4 CSTR. Este es el conjunto de datos de los resúmenes de informes técnicos publicados en el Departamento de Ciencias de la Computación de una universidad. El conjunto de datos contenía 476 resúmenes, los cuales fueron divididos en cuatro áreas de investigación: Procesamiento de Lenguaje Natural (NLP), Robótica/Visión, Sistemas y Teoría. WebKB. El conjunto de datos WebKB contiene páginas web recopiladas de los departamentos de informática de universidades. Hay alrededor de 8280 documentos y están divididos en 7 categorías: estudiante, facultad, personal, curso, proyecto, departamento y otros. El texto sin procesar es de aproximadamente 27MB. Entre estas 7 categorías, estudiante, facultad, curso y proyecto son las cuatro categorías que representan entidades más pobladas. El subconjunto asociado suele ser llamado WebKB4. Reuters. La colección de pruebas de categorización de texto Reuters-21578 contiene documentos recopilados de la agencia de noticias Reuters en 1987. Es un punto de referencia estándar para la categorización de texto y contiene 135 categorías. En nuestros experimentos, utilizamos un subconjunto de la colección de datos que incluye las 10 categorías más frecuentes entre los 135 temas y lo llamamos Reuters-top 10. WebACE. El conjunto de datos WebACE proviene del proyecto WebACE y ha sido utilizado para la agrupación de documentos [17][5]. El conjunto de datos WebACE contiene 2340 documentos que consisten en artículos de noticias del servicio de noticias de Reuters a través de la web en octubre de 1997. Estos documentos están divididos en 20 clases. Not enough context provided. El conjunto de datos News4 utilizado en nuestros experimentos se selecciona del famoso conjunto de datos 20-newsgroups. El tema rec que contiene autos, motocicletas, béisbol y hockey fue seleccionado de la versión 20news-18828. El conjunto de datos News4 contiene 3970 vectores de documentos. Para preprocesar los conjuntos de datos, eliminamos las palabras vacías utilizando una lista estándar de palabras vacías, se omiten todas las etiquetas HTML y se ignoran todos los campos de encabezado excepto el asunto y la organización de los artículos publicados. En todos nuestros experimentos, primero seleccionamos las 1000 palabras principales por información mutua con las etiquetas de clase. 3.2 Métricas de Evaluación En los experimentos, establecemos el número de clústeres igual al número real de clases C para todos los algoritmos de agrupamiento. Para evaluar su rendimiento, comparamos los grupos generados por estos algoritmos con las clases reales mediante el cálculo de las siguientes dos medidas de rendimiento. Precisión de agrupamiento (Acc). La primera medida de rendimiento es la Precisión de Agrupamiento, que descubre la relación uno a uno entre los grupos y las clases, y mide en qué medida cada grupo contenía puntos de datos de la clase correspondiente. Resume todo el grado de coincidencia entre todos los pares de clases y clusters. La precisión del agrupamiento se puede calcular como: Acc = 1 N max   Ck,Lm T(Ck, Lm)   , (29) donde Ck denota el k-ésimo clúster en los resultados finales, y Lm es la verdadera clase m-ésima. T(Ck, Lm) es el número de entidades que pertenecen a la clase m y están asignadas al clúster k. La precisión calcula la suma máxima de T(Ck, Lm) para todos los pares de clústeres y clases, y estos pares no tienen superposiciones. La mayor precisión de agrupamiento significa un mejor rendimiento de agrupamiento. Información Mutua Normalizada (NMI). Otro métrica de evaluación que adoptamos aquí es la Información Mutua Normalizada (NMI) [23], la cual es ampliamente utilizada para determinar la calidad de los grupos. Para dos variables aleatorias X e Y, el NMI se define como: NMI(X, Y) = I(X, Y) H(X)H(Y), donde I(X, Y) es la información mutua entre X e Y, mientras que H(X) y H(Y) son las entropías de X e Y respectivamente. Se puede ver que NMI(X, X) = 1, que es el valor máximo posible de NMI. Dado un resultado de agrupamiento, el NMI en la ecuación (30) se estima como NMI = C k=1 C m=1 nk,mlog n·nk,m nk ˆnm C k=1 nklog nk n C m=1 ˆnmlog ˆnm n, (31), donde nk denota el número de datos contenidos en el grupo Ck (1 k C), ˆnm es el número de datos pertenecientes a la clase m-ésima (1 m C), y nk,m denota el número de datos que están en la intersección entre el grupo Ck y la clase m-ésima. El valor calculado en la Ecuación (31) se utiliza como medida de rendimiento para el resultado de agrupamiento dado. A mayor valor, mejor rendimiento de agrupamiento. 3.3 Comparaciones Hemos realizado evaluaciones de rendimiento exhaustivas probando nuestro método y comparándolo con otros 8 métodos representativos de agrupamiento de datos utilizando las mismas corpora de datos. Los algoritmos que evaluamos se enumeran a continuación. 1. K-means tradicional (KM). 2. K-medias esférico (SKM). La implementación se basa en [9]. 3. Modelo de Mezcla Gaussiana (GMM). La implementación se basa en [16]. 4. Agrupamiento espectral con cortes normalizados (Ncut). La implementación se basa en [26], y la varianza de la similitud gaussiana se determina mediante Escalado Local [30]. Ten en cuenta que el criterio que Ncut busca minimizar es simplemente el regularizador global en nuestro algoritmo CLGR, excepto que Ncut utiliza el Laplaciano normalizado. 5. Agrupamiento utilizando Regularización Local Pura (CPLR). En este método simplemente minimizamos Jl (definido en la Ecuación (24)), y los resultados de agrupamiento pueden obtenerse realizando la descomposición de valores propios en la matriz (I − P)T (I − P) con algunos métodos de discretización adecuados. 6. Iteración de subespacio adaptativa (ASI). La implementación se basa en [14]. 7. Factorización de matrices no negativas (NMF). La implementación se basa en [27]. 8. Factorización Tri-Factorización de Matrices No Negativas (TNMF) [12]. La implementación se basa en [15]. Para la eficiencia computacional, en la implementación de CPLR y nuestro algoritmo CLGR, hemos establecido todos los parámetros de regularización local {λi}n i=1 para ser idénticos, los cuales se establecen mediante búsqueda en cuadrícula de {0.1, 1, 10}. El tamaño de los vecindarios k más cercanos se establece mediante una búsqueda en cuadrícula de {20, 40, 80}. Para el método CLGR, su parámetro de regularización global se establece mediante una búsqueda en cuadrícula de {0.1, 1, 10}. Al construir el regularizador global, hemos adoptado el método de escalamiento local [30] para construir la matriz Laplaciana. El método de discretización final adoptado en estos dos métodos es el mismo que en [26], ya que nuestros experimentos muestran que el uso de dicho método puede lograr mejores resultados que el uso de métodos basados en kmeans como en [20]. 3.4 Resultados Experimentales Los resultados de comparación de precisión de agrupamiento se muestran en la tabla 3, y los resultados de comparación de información mutua normalizada se resumen en la tabla 4. De las dos tablas principalmente observamos que: 1. Nuestro método CLGR supera a todos los demás métodos de agrupamiento de documentos en la mayoría de los conjuntos de datos. Para la agrupación de documentos, el método Spherical k-means suele superar al método tradicional de agrupación k-means, y el método GMM puede lograr resultados competitivos en comparación con el método Spherical k-means; 3. Los resultados obtenidos de los algoritmos tipo k-means y GMM suelen ser peores que los resultados obtenidos de Clustering Espectral. Dado que el Agrupamiento Espectral puede ser visto como una versión ponderada del kernel k-means, puede obtener buenos resultados cuando los clusters de datos tienen formas arbitrarias. Esto corrobora que los vectores de los documentos no están distribuidos regularmente (esféricos o elípticos). 4. Las comparaciones experimentales verifican empíricamente la equivalencia entre NMF y Clustering Espectral, como se muestra en la Tabla 3: Precisión de agrupamiento de los diversos métodos CSTR WebKB4 Reuters WebACE News4 KM 0.4256 0.3888 0.4448 0.4001 0.3527 SKM 0.4690 0.4318 0.5025 0.4458 0.3912 GMM 0.4487 0.4271 0.4897 0.4521 0.3844 NMF 0.5713 0.4418 0.4947 0.4761 0.4213 Ncut 0.5435 0.4521 0.4896 0.4513 0.4189 ASI 0.5621 0.4752 0.5235 0.4823 0.4335 TNMF 0.6040 0.4832 0.5541 0.5102 0.4613 CPLR 0.5974 0.5020 0.4832 0.5213 0.4890 CLGR 0.6235 0.5228 0.5341 0.5376 0.5102 Tabla 4: Resultados de información mutua normalizada de los diversos métodos CSTR WebKB4 Reuters WebACE News4 KM 0.3675 0.3023 0.4012 0.3864 0.3318 SKM 0.4027 0.4155 0.4587 0.4003 0.4085 GMM 0.4034 0.4093 0.4356 0.4209 0.3994 NMF 0.5235 0.4517 0.4402 0.4359 0.4130 Ncut 0.4833 0.4497 0.4392 0.4289 0.4231 ASI 0.5008 0.4833 0.4769 0.4817 0.4503 TNMF 0.5724 0.5011 0.5132 0.5328 0.4749 CPLR 0.5695 0.5231 0.4402 0.5543 0.4690 CLGR 0.6012 0.5434 0.4935 0.5390 0.4908 ha sido demostrado teóricamente en [10]. Se puede observar en las tablas que NMF y el Agrupamiento Espectral suelen llevar a resultados de agrupamiento similares. 5. Los métodos basados en co-agrupamiento (TNMF y ASI) suelen lograr mejores resultados que los métodos tradicionales basados únicamente en vectores de documentos. Dado que estos métodos realizan una selección implícita de características en cada iteración, proporcionan una métrica adaptativa para medir el vecindario y, por lo tanto, tienden a producir mejores resultados de agrupamiento. 6. Los resultados obtenidos a través de CPLR suelen ser mejores que los resultados obtenidos a través de Agrupamiento Espectral, lo cual respalda la teoría de Vapnik [24] de que a veces los algoritmos de aprendizaje local pueden obtener mejores resultados que los algoritmos de aprendizaje global. Además de los experimentos de comparación mencionados anteriormente, también probamos la sensibilidad de los parámetros de nuestro método. Principalmente hay dos conjuntos de parámetros en nuestro algoritmo CLGR, los parámetros de regularización locales y globales ({λi}n i=1 y λ, como hemos mencionado en la sección 3.3, hemos establecido que todos los λis son idénticos a λ∗ en nuestros experimentos), y el tamaño de los vecindarios. Por lo tanto, también hemos realizado dos series de experimentos: 1. Fijando el tamaño de los vecindarios y probando el rendimiento del agrupamiento con diferentes valores de λ∗ y λ. En este conjunto de experimentos, encontramos que nuestro algoritmo CLGR puede lograr buenos resultados cuando los dos parámetros de regularización no son ni demasiado grandes ni demasiado pequeños. Normalmente nuestro método puede lograr buenos resultados cuando λ∗ y λ están alrededor de 0.1. La Figura 1 nos muestra un ejemplo de prueba en el conjunto de datos WebACE. Fijando los parámetros de regularización local y global, y probando el rendimiento del agrupamiento con diferentes valores de -5, -4.5, -4, -3.5, -3, -5, -4.5, -4, -3.5, -3, 0.35, 0.4, 0.45, 0.5, 0.55 para el parámetro de regularización local (valor logarítmico base 2) y para el parámetro de regularización global (valor logarítmico base 2), se obtuvo una precisión de agrupamiento. Figura 1: Resultados de la prueba de sensibilidad de parámetros en el conjunto de datos WebACE con el tamaño de vecindario fijo en 20, donde el eje x e y representan el valor logarítmico base 2 de λ∗ y λ. En este conjunto de experimentos, encontramos que el vecindario con un tamaño demasiado grande o demasiado pequeño deteriorará todos los resultados finales de agrupamiento. Esto puede entenderse fácilmente ya que cuando el tamaño del vecindario es muy pequeño, los puntos de datos utilizados para entrenar los clasificadores locales pueden no ser suficientes; cuando el tamaño del vecindario es muy grande, los clasificadores entrenados tenderán a ser globales y no podrán capturar las características locales típicas. La Figura 2 nos muestra un ejemplo de prueba en el conjunto de datos WebACE. Por lo tanto, podemos ver que nuestro algoritmo CLGR (1) puede lograr resultados satisfactorios y (2) no es muy sensible a la elección de parámetros, lo que lo hace práctico en aplicaciones del mundo real. CONCLUSIONES Y TRABAJOS FUTUROS En este artículo, desarrollamos un nuevo algoritmo de agrupamiento llamado agrupamiento con regularización local y global. Nuestro método conserva el mérito de los algoritmos de aprendizaje local y el agrupamiento espectral. Nuestros experimentos muestran que el algoritmo propuesto supera a la mayoría de los algoritmos de vanguardia en muchos conjuntos de datos de referencia. En el futuro, nos enfocaremos en la selección de parámetros y los problemas de aceleración del algoritmo CLGR. REFERENCIAS [1] L. Baker y A. McCallum. Agrupamiento distribucional de palabras para clasificación de texto. En Actas de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1998. [2] M. Belkin y P. Niyogi. Mapeo de Eigen de Laplaciano para Reducción de Dimensionalidad y Representación de Datos. Computación Neural, 15 (6):1373-1396. Junio de 2003. [3] M. Belkin y P. Niyogi. Hacia una Fundación Teórica para Métodos de Manifold Basados en Laplacianos. En Actas de la 18ª Conferencia sobre Teoría del Aprendizaje (COLT). 2005. 10 20 30 40 50 60 70 80 90 100 0.35 0.4 0.45 0.5 0.55 tamaño de la precisión de agrupamiento del vecindario Figura 2: Resultados de pruebas de sensibilidad de parámetros en el conjunto de datos WebACE con los parámetros de regularización fijados en 0.1, y el tamaño del vecindario variando de 10 a 100. [4] M. Belkin, P. Niyogi y V. Sindhwani. Regularización de variedades: un marco geométrico para el aprendizaje a partir de ejemplos. Revista de Investigación en Aprendizaje Automático 7, 1-48, 2006. [5] D. Boley. División Direccional Principal. Minería de datos y descubrimiento de conocimiento, 2:325-344, 1998. [6] L. Bottou y V. Vapnik. Algoritmos de aprendizaje local. Computación Neural, 4:888-900, 1992. [7] P. K. Chan, D. F. Schlag y J. Y. Zien. Particionamiento y agrupamiento K-way de corte de razón espectral. IEEE Trans. Diseño Asistido por Computadora, 13:1088-1096, Sep. 1994. [8] D. R. Cutting, D. R. Karger, J. O. Pederson y J. W. Tukey. Dispersión/Recolección: Un enfoque basado en clústeres para explorar grandes colecciones de documentos. En Actas de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1992. [9] I. S. Dhillon y D. S. Modha. Descomposiciones de conceptos para datos de texto grandes y dispersos utilizando agrupamiento. Aprendizaje automático, vol. 42(1), páginas 143-175, enero de 2001. [10] C. Ding, X. Él, y H. Simon. Sobre la equivalencia entre la factorización de matrices no negativas y el agrupamiento espectral. En Actas de la Conferencia de Minería de Datos de SIAM, 2005. [11] C. Ding, X. Él, H. Zha, M. Gu y H. D. Simon. Un algoritmo de corte min-max para particionar grafos y agrupar datos. En Proc. de la 1ra Conferencia Internacional sobre Minería de Datos (ICDM), páginas 107-114, 2001. [12] C. Ding, T. Li, W. Peng y H. Park. Tri-factorizaciones de matrices no negativas ortogonales para agrupamiento. En Actas de la Duodécima Conferencia Internacional de la ACM SIGKDD sobre Descubrimiento de Conocimiento y Minería de Datos, 2006. [13] R. O. Duda, P. E. Hart y D. G. Stork. Clasificación de patrones. John Wiley & Sons, Inc., 2001. [14] T. Li, S. Ma y M. Ogihara. Agrupamiento de documentos a través de la Iteración de Subespacios Adaptativos. En Actas de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2004. [15] T. Li y C. Ding. Las relaciones entre varios métodos de factorización de matrices no negativas para agrupamiento. En Actas de la 6ta Conferencia Internacional sobre Minería de Datos (ICDM). 2006. [16] X. Liu y Y. Gong. Agrupación de documentos con capacidades de refinamiento de clústeres y selección de modelos. En Proc. de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2002. [17] E. Han, D. Boley, M. Gini, R. Gross, K. Hastings, G. Karypis, V. Kumar, B. Mobasher y J. Moore. WebACE: Un agente web para la categorización y exploración de documentos. En Actas de la 2ª Conferencia Internacional sobre Agentes Autónomos (Agents98). ACM Press, 1998. [18] M. Hein, J. Y. Audibert y U. von Luxburg. De Gráficas a Variedades - Consistencia Puntual Débil y Fuerte de los Laplacianos de Gráficas. En Actas de la 18ª Conferencia sobre Teoría del Aprendizaje (COLT), 470-485. 2005. [19] J. Él, M. Lan, C.-L. Tan, S.-Y. Sung, y H.-B. Bajo. Inicialización de algoritmos de refinamiento de clústeres: una revisión y estudio comparativo. En Proc. de Inter. Conferencia Conjunta sobre Redes Neuronales, 2004. [20] A. Y. Ng, M. I. Jordan, Y. Weiss. En el agrupamiento espectral: análisis y un algoritmo. En Avances en Sistemas de Procesamiento de Información Neural 14. 2002. [21] B. Sch¨olkopf y A. Smola. Aprendizaje con Kernels. El MIT Press. Cambridge, Massachusetts. 2002. [22] J. Shi y J. Malik. Cortes normalizados y segmentación de imágenes. IEEE Trans. on Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000. [23] A. Strehl and J. Ghosh.\nIEEE Trans. on Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000. [23] A. Strehl y J. Ghosh. Conjuntos de agrupamientos: un marco de reutilización de conocimiento para combinar múltiples particiones. Revista de Investigación de Aprendizaje Automático, 3:583-617, 2002. [24] V. N. Vapnik. La naturaleza de la teoría del aprendizaje estadístico. Berlín: Springer-Verlag, 1995. [25] Wu, M. y Schölkopf, B. Un enfoque de aprendizaje local para el agrupamiento. En Avances en Sistemas de Procesamiento de Información Neural 18. 2006. [26] S. X. Yu, J. Shi. Agrupamiento espectral multiclase. En Actas de la Conferencia Internacional sobre Visión por Computadora, 2003. [27] W. Xu, X. Liu y Y. Gong. Agrupación de documentos basada en la factorización de matrices no negativas. En Actas de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2003. [28] H. Zha, X. Él, C. Ding, M. Gu y H. Simon. Relajación espectral para el agrupamiento K-means. En NIPS 14. 2001. [29] T. Zhang y F. J. Oles. Categorización de texto basada en métodos de clasificación lineal regularizados. Revista de Recuperación de Información, 4:5-31, 2001. [30] L. Zelnik-Manor y P. Perona. Agrupamiento espectral autoajustable. En NIPS 17. 2005. [31] D. Zhou, O. Bousquet, T. N. Lal, J. Weston y B. Schölkopf. Aprendizaje con Consistencia Local y Global. NIPS 17, 2005. \n\nNIPS 17, 2005. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "hierarchical method": {
            "translated_key": "métodos jerárquicos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Regularized Clustering for Documents ∗ Fei Wang, Changshui Zhang State Key Lab of Intelligent Tech.",
                "and Systems Department of Automation, Tsinghua University Beijing, China, 100084 feiwang03@gmail.com Tao Li School of Computer Science Florida International University Miami, FL 33199, U.S.A. taoli@cs.fiu.edu ABSTRACT In recent years, document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering.",
                "In this paper, we propose a novel method for clustering documents using regularization.",
                "Unlike traditional globally regularized clustering methods, our method first construct a local regularized linear label predictor for each document vector, and then combine all those local regularizers with a global smoothness regularizer.",
                "So we call our algorithm Clustering with Local and Global Regularization (CLGR).",
                "We will show that the cluster memberships of the documents can be achieved by eigenvalue decomposition of a sparse symmetric matrix, which can be efficiently solved by iterative methods.",
                "Finally our experimental evaluations on several datasets are presented to show the superiorities of CLGR over traditional document clustering methods.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Clustering; I.2.6 [Artificial Intelligence]: Learning-Concept Learning General Terms Algorithms 1.",
                "INTRODUCTION Document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering.",
                "A good document clustering approach can assist the computers to automatically organize the document corpus into a meaningful cluster hierarchy for efficient browsing and navigation, which is very valuable for complementing the deficiencies of traditional information retrieval technologies.",
                "As pointed out by [8], the information retrieval needs can be expressed by a spectrum ranged from narrow keyword-matching based search to broad information browsing such as what are the major international events in recent months.",
                "Traditional document retrieval engines tend to fit well with the search end of the spectrum, i.e. they usually provide specified search for documents matching the users query, however, it is hard for them to meet the needs from the rest of the spectrum in which a rather broad or vague information is needed.",
                "In such cases, efficient browsing through a good cluster hierarchy will be definitely helpful.",
                "Generally, document clustering methods can be mainly categorized into two classes: <br>hierarchical method</br>s and partitioning methods.",
                "The <br>hierarchical method</br>s group the data points into a hierarchical tree structure using bottom-up or top-down approaches.",
                "For example, hierarchical agglomerative clustering (HAC) [13] is a typical bottom-up hierarchical clustering method.",
                "It takes each data point as a single cluster to start off with and then builds bigger and bigger clusters by grouping similar data points together until the entire dataset is encapsulated into one final cluster.",
                "On the other hand, partitioning methods decompose the dataset into a number of disjoint clusters which are usually optimal in terms of some predefined criterion functions.",
                "For instance, K-means [13] is a typical partitioning method which aims to minimize the sum of the squared distance between the data points and their corresponding cluster centers.",
                "In this paper, we will focus on the partitioning methods.",
                "As we know that there are two main problems existing in partitioning methods (like Kmeans and Gaussian Mixture Model (GMM) [16]): (1) the predefined criterion is usually non-convex which causes many local optimal solutions; (2) the iterative procedure (e.g. the Expectation Maximization (EM) algorithm) for optimizing the criterions usually makes the final solutions heavily depend on the initializations.",
                "In the last decades, many methods have been proposed to overcome the above problems of the partitioning methods [19][28].",
                "Recently, another type of partitioning methods based on clustering on data graphs have aroused considerable interests in the machine learning and data mining community.",
                "The basic idea behind these methods is to first model the whole dataset as a weighted graph, in which the graph nodes represent the data points, and the weights on the edges correspond to the similarities between pairwise points.",
                "Then the cluster assignments of the dataset can be achieved by optimizing some criterions defined on the graph.",
                "For example Spectral Clustering is one kind of the most representative graph-based clustering approaches, it generally aims to optimize some cut value (e.g.",
                "Normalized Cut [22], Ratio Cut [7], Min-Max Cut [11]) defined on an undirected graph.",
                "After some relaxations, these criterions can usually be optimized via eigen-decompositions, which is guaranteed to be global optimal.",
                "In this way, spectral clustering efficiently avoids the problems of the traditional partitioning methods as we introduced in last paragraph.",
                "In this paper, we propose a novel document clustering algorithm that inherits the superiority of spectral clustering, i.e. the final cluster results can also be obtained by exploit the eigen-structure of a symmetric matrix.",
                "However, unlike spectral clustering, which just enforces a smoothness constraint on the data labels over the whole data manifold [2], our method first construct a regularized linear label predictor for each data point from its neighborhood as in [25], and then combine the results of all these local label predictors with a global label smoothness regularizer.",
                "So we call our method Clustering with Local and Global Regularization (CLGR).",
                "The idea of incorporating both local and global information into label prediction is inspired by the recent works on semi-supervised learning [31], and our experimental evaluations on several real document datasets show that CLGR performs better than many state-of-the-art clustering methods.",
                "The rest of this paper is organized as follows: in section 2 we will introduce our CLGR algorithm in detail.",
                "The experimental results on several datasets are presented in section 3, followed by the conclusions and discussions in section 4. 2.",
                "THE PROPOSED ALGORITHM In this section, we will introduce our Clustering with Local and Global Regularization (CLGR) algorithm in detail.",
                "First lets see the how the documents are represented throughout this paper. 2.1 Document Representation In our work, all the documents are represented by the weighted term-frequency vectors.",
                "Let W = {w1, w2, · · · , wm} be the complete vocabulary set of the document corpus (which is preprocessed by the stopwords removal and words stemming operations).",
                "The term-frequency vector xi of document di is defined as xi = [xi1, xi2, · · · , xim]T , xik = tik log n idfk , where tik is the term frequency of wk ∈ W, n is the size of the document corpus, idfk is the number of documents that contain word wk.",
                "In this way, xi is also called the TFIDF representation of document di.",
                "Furthermore, we also normalize each xi (1 i n) to have a unit length, so that each document is represented by a normalized TF-IDF vector. 2.2 Local Regularization As its name suggests, CLGR is composed of two parts: local regularization and global regularization.",
                "In this subsection we will introduce the local regularization part in detail. 2.2.1 Motivation As we know that clustering is one type of learning techniques, it aims to organize the dataset in a reasonable way.",
                "Generally speaking, learning can be posed as a problem of function estimation, from which we can get a good classification function that will assign labels to the training dataset and even the unseen testing dataset with some cost minimized [24].",
                "For example, in the two-class classification scenario1 (in which we exactly know the label of each document), a linear classifier with least square fit aims to learn a column vector w such that the squared cost J = 1 n (wT xi − yi)2 (1) is minimized, where yi ∈ {+1, −1} is the label of xi.",
                "By taking ∂J /∂w = 0, we get the solution w∗ = n i=1 xixT i −1 n i=1 xiyi , (2) which can further be written in its matrix form as w∗ = XXT −1 Xy, (3) where X = [x1, x2, · · · , xn] is an m × n document matrix, y = [y1, y2, · · · , yn]T is the label vector.",
                "Then for a test document t, we can determine its label by l = sign(w∗T u), (4) where sign(·) is the sign function.",
                "A natural problem in Eq. (3) is that the matrix XXT may be singular and thus not invertable (e.g. when m n).",
                "To avoid such a problem, we can add a regularization term and minimize the following criterion J = 1 n n i=1 (wT xi − yi)2 + λ w 2 , (5) where λ is a regularization parameter.",
                "Then the optimal solution that minimize J is given by w∗ = XXT + λnI −1 Xy, (6) where I is an m × m identity matrix.",
                "It has been reported that the regularized linear classifier can achieve very good results on text classification problems [29].",
                "However, despite its empirical success, the regularized linear classifier is on earth a global classifier, i.e. w∗ is estimated using the whole training set.",
                "According to [24], this may not be a smart idea, since a unique w∗ may not be good enough for predicting the labels of the whole input space.",
                "In order to get better predictions, [6] proposed to train classifiers locally and use them to classify the testing points.",
                "For example, a testing point will be classified by the local classifier trained using the training points located in the vicinity 1 In the following discussions we all assume that the documents coming from only two classes.",
                "The generalizations of our method to multi-class cases will be discussed in section 2.5. of it.",
                "Although this method seems slow and stupid, it is reported that it can get better performances than using a unique global classifier on certain tasks [6]. 2.2.2 Constructing the Local Regularized Predictors Inspired by their success, we proposed to apply the local learning algorithms for clustering.",
                "The basic idea is that, for each document vector xi (1 i n), we train a local label predictor based on its k-nearest neighborhood Ni, and then use it to predict the label of xi.",
                "Finally we will combine all those local predictors by minimizing the sum of their prediction errors.",
                "In this subsection we will introduce how to construct those local predictors.",
                "Due to the simplicity and effectiveness of the regularized linear classifier that we have introduced in section 2.2.1, we choose it to be our local label predictor, such that for each document xi, the following criterion is minimized Ji = 1 ni xj ∈Ni wT i xj − qj 2 + λi wi 2 , (7) where ni = |Ni| is the cardinality of Ni, and qj is the cluster membership of xj.",
                "Then using Eq. (6), we can get the optimal solution is w∗ i = XiXT i + λiniI −1 Xiqi, (8) where Xi = [xi1, xi2, · · · , xini ], and we use xik to denote the k-th nearest neighbor of xi. qi = [qi1, qi2, · · · , qini ]T with qik representing the cluster assignment of xik.",
                "The problem here is that XiXT i is an m × m matrix with m ni, i.e. we should compute the inverse of an m × m matrix for every document vector, which is computationally prohibited.",
                "Fortunately, we have the following theorem: Theorem 1. w∗ i in Eq. (8) can be rewritten as w∗ i = Xi XT i Xi + λiniIi −1 qi, (9) where Ii is an ni × ni identity matrix.",
                "Proof.",
                "Since w∗ i = XiXT i + λiniI −1 Xiqi, then XiXT i + λiniI w∗ i = Xiqi =⇒ XiXT i w∗ i + λiniw∗ i = Xiqi =⇒ w∗ i = (λini)−1 Xi qi − XT i w∗ i .",
                "Let β = (λini)−1 qi − XT i w∗ i , then w∗ i = Xiβ =⇒ λiniβ = qi − XT i w∗ i = qi − XT i Xiβ =⇒ qi = XT i Xi + λiniIi β =⇒ β = XT i Xi + λiniIi −1 qi.",
                "Therefore w∗ i = Xiβ = Xi XT i Xi + λiniIi −1 qi 2 Using theorem 1, we only need to compute the inverse of an ni × ni matrix for every document to train a local label predictor.",
                "Moreover, for a new testing point u that falls into Ni, we can classify it by the sign of qu = w∗T i u = uT wi = uT Xi XT i Xi + λiniIi −1 qi.",
                "This is an attractive expression since we can determine the cluster assignment of u by using the inner-products between the points in {u ∪ Ni}, which suggests that such a local regularizer can easily be kernelized [21] as long as we define a proper kernel function. 2.2.3 Combining the Local Regularized Predictors After all the local predictors having been constructed, we will combine them together by minimizing Jl = n i=1 w∗T i xi − qi 2 , (10) which stands for the sum of the prediction errors for all the local predictors.",
                "Combining Eq. (10) with Eq. (6), we can get Jl = n i=1 w∗T i xi − qi 2 = n i=1 xT i Xi XT i Xi + λiniIi −1 qi − qi 2 = Pq − q 2 , (11) where q = [q1, q2, · · · , qn]T , and the P is an n × n matrix constructing in the following way.",
                "Let αi = xT i Xi XT i Xi + λiniIi −1 , then Pij = αi j, if xj ∈ Ni 0, otherwise , (12) where Pij is the (i, j)-th entry of P, and αi j represents the j-th entry of αi .",
                "Till now we can write the criterion of clustering by combining locally regularized linear label predictors Jl in an explicit mathematical form, and we can minimize it directly using some standard optimization techniques.",
                "However, the results may not be good enough since we only exploit the local informations of the dataset.",
                "In the next subsection, we will introduce a global regularization criterion and combine it with Jl, which aims to find a good clustering result in a local-global way. 2.3 Global Regularization In data clustering, we usually require that the cluster assignments of the data points should be sufficiently smooth with respect to the underlying data manifold, which implies (1) the nearby points tend to have the same cluster assignments; (2) the points on the same structure (e.g. submanifold or cluster) tend to have the same cluster assignments [31].",
                "Without the loss of generality, we assume that the data points reside (roughly) on a low-dimensional manifold M2 , and q is the cluster assignment function defined on M, i.e. 2 We believe that the text data are also sampled from some low dimensional manifold, since it is impossible for them to for ∀x ∈ M, q(x) returns the cluster membership of x.",
                "The smoothness of q over M can be calculated by the following Dirichlet integral [2] D[q] = 1 2 M q(x) 2 dM, (13) where the gradient q is a vector in the tangent space T Mx, and the integral is taken with respect to the standard measure on M. If we restrict the scale of q by q, q M = 1 (where ·, · M is the inner product induced on M), then it turns out that finding the smoothest function minimizing D[q] reduces to finding the eigenfunctions of the Laplace Beltrami operator L, which is defined as Lq −div q, (14) where div is the divergence of a vector field.",
                "Generally, the graph can be viewed as the discretized form of manifold.",
                "We can model the dataset as an weighted undirected graph as in spectral clustering [22], where the graph nodes are just the data points, and the weights on the edges represent the similarities between pairwise points.",
                "Then it can be shown that minimizing Eq. (13) corresponds to minimizing Jg = qT Lq = n i=1 (qi − qj)2 wij, (15) where q = [q1, q2, · · · , qn]T with qi = q(xi), L is the graph Laplacian with its (i, j)-th entry Lij =    di − wii, if i = j −wij, if xi and xj are adjacent 0, otherwise, (16) where di = j wij is the degree of xi, wij is the similarity between xi and xj.",
                "If xi and xj are adjacent3 , wij is usually computed in the following way wij = e − xi−xj 2 2σ2 , (17) where σ is a dataset dependent parameter.",
                "It is proved that under certain conditions, such a form of wij to determine the weights on graph edges leads to the convergence of graph Laplacian to the Laplace Beltrami operator [3][18].",
                "In summary, using Eq. (15) with exponential weights can effectively measure the smoothness of the data assignments with respect to the intrinsic data manifold.",
                "Thus we adopt it as a global regularizer to punish the smoothness of the predicted data assignments. 2.4 Clustering with Local and Global Regularization Combining the contents we have introduced in section 2.2 and section 2.3 we can derive the clustering criterion is minq J = Jl + λJg = Pq − q 2 + λqT Lq s.t. qi ∈ {−1, +1}, (18) where P is defined as in Eq. (12), and λ is a regularization parameter to trade off Jl and Jg.",
                "However, the discrete fill in the whole high-dimensional sample space.",
                "And it has been shown that the manifold based methods can achieve good results on text classification tasks [31]. 3 In this paper, we define xi and xj to be adjacent if xi ∈ N(xj) or xj ∈ N(xi). constraint of pi makes the problem an NP hard integer programming problem.",
                "A natural way for making the problem solvable is to remove the constraint and relax qi to be continuous, then the objective that we aims to minimize becomes J = Pq − q 2 + λqT Lq = qT (P − I)T (P − I)q + λqT Lq = qT (P − I)T (P − I) + λL q, (19) and we further add a constraint qT q = 1 to restrict the scale of q.",
                "Then our objective becomes minq J = qT (P − I)T (P − I) + λL q s.t. qT q = 1 (20) Using the Lagrangian method, we can derive that the optimal solution q corresponds to the smallest eigenvector of the matrix M = (P − I)T (P − I) + λL, and the cluster assignment of xi can be determined by the sign of qi, i.e. xi will be classified as class one if qi > 0, otherwise it will be classified as class 2. 2.5 Multi-Class CLGR In the above we have introduced the basic framework of Clustering with Local and Global Regularization (CLGR) for the two-class clustering problem, and we will extending it to multi-class clustering in this subsection.",
                "First we assume that all the documents belong to C classes indexed by L = {1, 2, · · · , C}. qc is the classification function for class c (1 c C), such that qc (xi) returns the confidence that xi belongs to class c. Our goal is to obtain the value of qc (xi) (1 c C, 1 i n), and the cluster assignment of xi can be determined by {qc (xi)}C c=1 using some proper discretization methods that we will introduce later.",
                "Therefore, in this multi-class case, for each document xi (1 i n), we will construct C locally linear regularized label predictors whose normal vectors are wc∗ i = Xi XT i Xi + λiniIi −1 qc i (1 c C), (21) where Xi = [xi1, xi2, · · · , xini ] with xik being the k-th neighbor of xi, and qc i = [qc i1, qc i2, · · · , qc ini ]T with qc ik = qc (xik).",
                "Then (wc∗ i )T xi returns the predicted confidence of xi belonging to class c. Hence the local prediction error for class c can be defined as J c l = n i=1 (wc∗ i ) T xi − qc i 2 , (22) And the total local prediction error becomes Jl = C c=1 J c l = C c=1 n i=1 (wc∗ i ) T xi − qc i 2 . (23) As in Eq. (11), we can define an n×n matrix P (see Eq. (12)) and rewrite Jl as Jl = C c=1 J c l = C c=1 Pqc − qc 2 . (24) Similarly we can define the global smoothness regularizer in multi-class case as Jg = C c=1 n i=1 (qc i − qc j )2 wij = C c=1 (qc )T Lqc . (25) Then the criterion to be minimized for CLGR in multi-class case becomes J = Jl + λJg = C c=1 Pqc − qc 2 + λ(qc )T Lqc = C c=1 (qc )T (P − I)T (P − I) + λL qc = trace QT (P − I)T (P − I) + λL Q , (26) where Q = [q1 , q2 , · · · , qc ] is an n × c matrix, and trace(·) returns the trace of a matrix.",
                "The same as in Eq. (20), we also add the constraint that QT Q = I to restrict the scale of Q.",
                "Then our optimization problem becomes minQ J = trace QT (P − I)T (P − I) + λL Q s.t.",
                "QT Q = I, (27) From the Ky Fan theorem [28], we know the optimal solution of the above problem is Q∗ = [q∗ 1, q∗ 2, · · · , q∗ C ]R, (28) where q∗ k (1 k C) is the eigenvector corresponds to the k-th smallest eigenvalue of matrix (P − I)T (P − I) + λL, and R is an arbitrary C × C matrix.",
                "Since the values of the entries in Q∗ is continuous, we need to further discretize Q∗ to get the cluster assignments of all the data points.",
                "There are mainly two approaches to achieve this goal: 1.",
                "As in [20], we can treat the i-th row of Q as the embedding of xi in a C-dimensional space, and apply some traditional clustering methods like kmeans to clustering these embeddings into C clusters. 2.",
                "Since the optimal Q∗ is not unique (because of the existence of an arbitrary matrix R), we can pursue an optimal R that will rotate Q∗ to an indication matrix4 .",
                "The detailed algorithm can be referred to [26].",
                "The detailed algorithm procedure for CLGR is summarized in table 1. 3.",
                "EXPERIMENTS In this section, experiments are conducted to empirically compare the clustering results of CLGR with other 8 representitive document clustering algorithms on 5 datasets.",
                "First we will introduce the basic informations of those datasets. 3.1 Datasets We use a variety of datasets, most of which are frequently used in the information retrieval research.",
                "Table 2 summarizes the characteristics of the datasets. 4 Here an indication matrix T is a n×c matrix with its (i, j)th entry Tij ∈ {0, 1} such that for each row of Q∗ there is only one 1.",
                "Then the xi can be assigned to the j-th cluster such that j = argjQ∗ ij = 1.",
                "Table 1: Clustering with Local and Global Regularization (CLGR) Input: 1.",
                "Dataset X = {xi}n i=1; 2.",
                "Number of clusters C; 3.",
                "Size of the neighborhood K; 4.",
                "Local regularization parameters {λi}n i=1; 5.",
                "Global regularization parameter λ; Output: The cluster membership of each data point.",
                "Procedure: 1.",
                "Construct the K nearest neighborhoods for each data point; 2.",
                "Construct the matrix P using Eq. (12); 3.",
                "Construct the Laplacian matrix L using Eq. (16); 4.",
                "Construct the matrix M = (P − I)T (P − I) + λL; 5.",
                "Do eigenvalue decomposition on M, and construct the matrix Q∗ according to Eq. (28); 6.",
                "Output the cluster assignments of each data point by properly discretize Q∗ .",
                "Table 2: Descriptions of the document datasets Datasets Number of documents Number of classes CSTR 476 4 WebKB4 4199 4 Reuters 2900 10 WebACE 2340 20 Newsgroup4 3970 4 CSTR.",
                "This is the dataset of the abstracts of technical reports published in the Department of Computer Science at a university.",
                "The dataset contained 476 abstracts, which were divided into four research areas: Natural Language Processing(NLP), Robotics/Vision, Systems, and Theory.",
                "WebKB.",
                "The WebKB dataset contains webpages gathered from university computer science departments.",
                "There are about 8280 documents and they are divided into 7 categories: student, faculty, staff, course, project, department and other.",
                "The raw text is about 27MB.",
                "Among these 7 categories, student, faculty, course and project are four most populous entity-representing categories.",
                "The associated subset is typically called WebKB4.",
                "Reuters.",
                "The Reuters-21578 Text Categorization Test collection contains documents collected from the Reuters newswire in 1987.",
                "It is a standard text categorization benchmark and contains 135 categories.",
                "In our experiments, we use a subset of the data collection which includes the 10 most frequent categories among the 135 topics and we call it Reuters-top 10.",
                "WebACE.",
                "The WebACE dataset was from WebACE project and has been used for document clustering [17][5].",
                "The WebACE dataset contains 2340 documents consisting news articles from Reuters new service via the Web in October 1997.",
                "These documents are divided into 20 classes.",
                "News4.",
                "The News4 dataset used in our experiments are selected from the famous 20-newsgroups dataset5 .",
                "The topic rec containing autos, motorcycles, baseball and hockey was selected from the version 20news-18828.",
                "The News4 dataset contains 3970 document vectors. 5 http://people.csail.mit.edu/jrennie/20Newsgroups/ To pre-process the datasets, we remove the stop words using a standard stop list, all HTML tags are skipped and all header fields except subject and organization of the posted articles are ignored.",
                "In all our experiments, we first select the top 1000 words by mutual information with class labels. 3.2 Evaluation Metrics In the experiments, we set the number of clusters equal to the true number of classes C for all the clustering algorithms.",
                "To evaluate their performance, we compare the clusters generated by these algorithms with the true classes by computing the following two performance measures.",
                "Clustering Accuracy (Acc).",
                "The first performance measure is the Clustering Accuracy, which discovers the one-toone relationship between clusters and classes and measures the extent to which each cluster contained data points from the corresponding class.",
                "It sums up the whole matching degree between all pair class-clusters.",
                "Clustering accuracy can be computed as: Acc = 1 N max   Ck,Lm T(Ck, Lm)   , (29) where Ck denotes the k-th cluster in the final results, and Lm is the true m-th class.",
                "T(Ck, Lm) is the number of entities which belong to class m are assigned to cluster k. Accuracy computes the maximum sum of T(Ck, Lm) for all pairs of clusters and classes, and these pairs have no overlaps.",
                "The greater clustering accuracy means the better clustering performance.",
                "Normalized Mutual Information (NMI).",
                "Another evaluation metric we adopt here is the Normalized Mutual Information NMI [23], which is widely used for determining the quality of clusters.",
                "For two random variable X and Y, the NMI is defined as: NMI(X, Y) = I(X, Y) H(X)H(Y) , (30) where I(X, Y) is the mutual information between X and Y, while H(X) and H(Y) are the entropies of X and Y respectively.",
                "One can see that NMI(X, X) = 1, which is the maximal possible value of NMI.",
                "Given a clustering result, the NMI in Eq. (30) is estimated as NMI = C k=1 C m=1 nk,mlog n·nk,m nk ˆnm C k=1 nklog nk n C m=1 ˆnmlog ˆnm n , (31) where nk denotes the number of data contained in the cluster Ck (1 k C), ˆnm is the number of data belonging to the m-th class (1 m C), and nk,m denotes the number of data that are in the intersection between the cluster Ck and the m-th class.",
                "The value calculated in Eq. (31) is used as a performance measure for the given clustering result.",
                "The larger this value, the better the clustering performance. 3.3 Comparisons We have conducted comprehensive performance evaluations by testing our method and comparing it with 8 other representative data clustering methods using the same data corpora.",
                "The algorithms that we evaluated are listed below. 1.",
                "Traditional k-means (KM). 2.",
                "Spherical k-means (SKM).",
                "The implementation is based on [9]. 3.",
                "Gaussian Mixture Model (GMM).",
                "The implementation is based on [16]. 4.",
                "Spectral Clustering with Normalized Cuts (Ncut).",
                "The implementation is based on [26], and the variance of the Gaussian similarity is determined by Local Scaling [30].",
                "Note that the criterion that Ncut aims to minimize is just the global regularizer in our CLGR algorithm except that Ncut used the normalized Laplacian. 5.",
                "Clustering using Pure Local Regularization (CPLR).",
                "In this method we just minimize Jl (defined in Eq. (24)), and the clustering results can be obtained by doing eigenvalue decomposition on matrix (I − P)T (I − P) with some proper discretization methods. 6.",
                "Adaptive Subspace Iteration (ASI).",
                "The implementation is based on [14]. 7.",
                "Nonnegative Matrix Factorization (NMF).",
                "The implementation is based on [27]. 8.",
                "Tri-Factorization Nonnegative Matrix Factorization (TNMF) [12].",
                "The implementation is based on [15].",
                "For computational efficiency, in the implementation of CPLR and our CLGR algorithm, we have set all the local regularization parameters {λi}n i=1 to be identical, which is set by grid search from {0.1, 1, 10}.",
                "The size of the k-nearest neighborhoods is set by grid search from {20, 40, 80}.",
                "For the CLGR method, its global regularization parameter is set by grid search from {0.1, 1, 10}.",
                "When constructing the global regularizer, we have adopted the local scaling method [30] to construct the Laplacian matrix.",
                "The final discretization method adopted in these two methods is the same as in [26], since our experiments show that using such method can achieve better results than using kmeans based methods as in [20]. 3.4 Experimental Results The clustering accuracies comparison results are shown in table 3, and the normalized mutual information comparison results are summarized in table 4.",
                "From the two tables we mainly observe that: 1.",
                "Our CLGR method outperforms all other document clustering methods in most of the datasets; 2.",
                "For document clustering, the Spherical k-means method usually outperforms the traditional k-means clustering method, and the GMM method can achieve competitive results compared to the Spherical k-means method; 3.",
                "The results achieved from the k-means and GMM type algorithms are usually worse than the results achieved from Spectral Clustering.",
                "Since Spectral Clustering can be viewed as a weighted version of kernel k-means, it can obtain good results the data clusters are arbitrarily shaped.",
                "This corroborates that the documents vectors are not regularly distributed (spherical or elliptical). 4.",
                "The experimental comparisons empirically verify the equivalence between NMF and Spectral Clustering, which Table 3: Clustering accuracies of the various methods CSTR WebKB4 Reuters WebACE News4 KM 0.4256 0.3888 0.4448 0.4001 0.3527 SKM 0.4690 0.4318 0.5025 0.4458 0.3912 GMM 0.4487 0.4271 0.4897 0.4521 0.3844 NMF 0.5713 0.4418 0.4947 0.4761 0.4213 Ncut 0.5435 0.4521 0.4896 0.4513 0.4189 ASI 0.5621 0.4752 0.5235 0.4823 0.4335 TNMF 0.6040 0.4832 0.5541 0.5102 0.4613 CPLR 0.5974 0.5020 0.4832 0.5213 0.4890 CLGR 0.6235 0.5228 0.5341 0.5376 0.5102 Table 4: Normalized mutual information results of the various methods CSTR WebKB4 Reuters WebACE News4 KM 0.3675 0.3023 0.4012 0.3864 0.3318 SKM 0.4027 0.4155 0.4587 0.4003 0.4085 GMM 0.4034 0.4093 0.4356 0.4209 0.3994 NMF 0.5235 0.4517 0.4402 0.4359 0.4130 Ncut 0.4833 0.4497 0.4392 0.4289 0.4231 ASI 0.5008 0.4833 0.4769 0.4817 0.4503 TNMF 0.5724 0.5011 0.5132 0.5328 0.4749 CPLR 0.5695 0.5231 0.4402 0.5543 0.4690 CLGR 0.6012 0.5434 0.4935 0.5390 0.4908 has been proved theoretically in [10].",
                "It can be observed from the tables that NMF and Spectral Clustering usually lead to similar clustering results. 5.",
                "The co-clustering based methods (TNMF and ASI) can usually achieve better results than traditional purely document vector based methods.",
                "Since these methods perform an implicit feature selection at each iteration, provide an adaptive metric for measuring the neighborhood, and thus tend to yield better clustering results. 6.",
                "The results achieved from CPLR are usually better than the results achieved from Spectral Clustering, which supports Vapniks theory [24] that sometimes local learning algorithms can obtain better results than global learning algorithms.",
                "Besides the above comparison experiments, we also test the parameter sensibility of our method.",
                "There are mainly two sets of parameters in our CLGR algorithm, the local and global regularization parameters ({λi}n i=1 and λ, as we have said in section 3.3, we have set all λis to be identical to λ∗ in our experiments), and the size of the neighborhoods.",
                "Therefore we have also done two sets of experiments: 1.",
                "Fixing the size of the neighborhoods, and testing the clustering performance with varying λ∗ and λ.",
                "In this set of experiments, we find that our CLGR algorithm can achieve good results when the two regularization parameters are neither too large nor too small.",
                "Typically our method can achieve good results when λ∗ and λ are around 0.1.",
                "Figure 1 shows us such a testing example on the WebACE dataset. 2.",
                "Fixing the local and global regularization parameters, and testing the clustering performance with different −5 −4.5 −4 −3.5 −3 −5 −4.5 −4 −3.5 −3 0.35 0.4 0.45 0.5 0.55 local regularization para (log 2 value) global regularization para (log 2 value) clusteringaccuracy Figure 1: Parameter sensibility testing results on the WebACE dataset with the neighborhood size fixed to 20, and the x-axis and y-axis represents the log2 value of λ∗ and λ. sizes of neighborhoods.",
                "In this set of experiments, we find that the neighborhood with a too large or too small size will all deteriorate the final clustering results.",
                "This can be easily understood since when the neighborhood size is very small, then the data points used for training the local classifiers may not be sufficient; when the neighborhood size is very large, the trained classifiers will tend to be global and cannot capture the typical local characteristics.",
                "Figure 2 shows us a testing example on the WebACE dataset.",
                "Therefore, we can see that our CLGR algorithm (1) can achieve satisfactory results and (2) is not very sensitive to the choice of parameters, which makes it practical in real world applications. 4.",
                "CONCLUSIONS AND FUTURE WORKS In this paper, we derived a new clustering algorithm called clustering with local and global regularization.",
                "Our method preserves the merit of local learning algorithms and spectral clustering.",
                "Our experiments show that the proposed algorithm outperforms most of the state of the art algorithms on many benchmark datasets.",
                "In the future, we will focus on the parameter selection and acceleration issues of the CLGR algorithm. 5.",
                "REFERENCES [1] L. Baker and A. McCallum.",
                "Distributional Clustering of Words for Text Classification.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 1998. [2] M. Belkin and P. Niyogi.",
                "Laplacian Eigenmaps for Dimensionality Reduction and Data Representation.",
                "Neural Computation, 15 (6):1373-1396.",
                "June 2003. [3] M. Belkin and P. Niyogi.",
                "Towards a Theoretical Foundation for Laplacian-Based Manifold Methods.",
                "In Proceedings of the 18th Conference on Learning Theory (COLT). 2005. 10 20 30 40 50 60 70 80 90 100 0.35 0.4 0.45 0.5 0.55 size of the neighborhood clusteringaccuracy Figure 2: Parameter sensibility testing results on the WebACE dataset with the regularization parameters being fixed to 0.1, and the neighborhood size varing from 10 to 100. [4] M. Belkin, P. Niyogi and V. Sindhwani.",
                "Manifold Regularization: a Geometric Framework for Learning from Examples.",
                "Journal of Machine Learning Research 7, 1-48, 2006. [5] D. Boley.",
                "Principal Direction Divisive Partitioning.",
                "Data mining and knowledge discovery, 2:325-344, 1998. [6] L. Bottou and V. Vapnik.",
                "Local learning algorithms.",
                "Neural Computation, 4:888-900, 1992. [7] P. K. Chan, D. F. Schlag and J. Y. Zien.",
                "Spectral K-way Ratio-Cut Partitioning and Clustering.",
                "IEEE Trans.",
                "Computer-Aided Design, 13:1088-1096, Sep. 1994. [8] D. R. Cutting, D. R. Karger, J. O. Pederson and J. W. Tukey.",
                "Scatter/Gather: A Cluster-Based Approach to Browsing Large Document Collections.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 1992. [9] I. S. Dhillon and D. S. Modha.",
                "Concept Decompositions for Large Sparse Text Data using Clustering.",
                "Machine Learning, vol. 42(1), pages 143-175, January 2001. [10] C. Ding, X.",
                "He, and H. Simon.",
                "On the equivalence of nonnegative matrix factorization and spectral clustering.",
                "In Proceedings of the SIAM Data Mining Conference, 2005. [11] C. Ding, X.",
                "He, H. Zha, M. Gu, and H. D. Simon.",
                "A min-max cut algorithm for graph partitioning and data clustering.",
                "In Proc. of the 1st International Conference on Data Mining (ICDM), pages 107-114, 2001. [12] C. Ding, T. Li, W. Peng, and H. Park.",
                "Orthogonal Nonnegative Matrix Tri-Factorizations for Clustering.",
                "In Proceedings of the Twelfth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2006. [13] R. O. Duda, P. E. Hart, and D. G. Stork.",
                "Pattern Classification.",
                "John Wiley & Sons, Inc., 2001. [14] T. Li, S. Ma, and M. Ogihara.",
                "Document Clustering via Adaptive Subspace Iteration.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2004. [15] T. Li and C. Ding.",
                "The Relationships Among Various Nonnegative Matrix Factorization Methods for Clustering.",
                "In Proceedings of the 6th International Conference on Data Mining (ICDM). 2006. [16] X. Liu and Y. Gong.",
                "Document Clustering with Cluster Refinement and Model Selection Capabilities.",
                "In Proc. of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002. [17] E. Han, D. Boley, M. Gini, R. Gross, K. Hastings, G. Karypis, V. Kumar, B. Mobasher, and J. Moore.",
                "WebACE: A Web Agent for Document Categorization and Exploration.",
                "In Proceedings of the 2nd International Conference on Autonomous Agents (Agents98).",
                "ACM Press, 1998. [18] M. Hein, J. Y. Audibert, and U. von Luxburg.",
                "From Graphs to Manifolds - Weak and Strong Pointwise Consistency of Graph Laplacians.",
                "In Proceedings of the 18th Conference on Learning Theory (COLT), 470-485. 2005. [19] J.",
                "He, M. Lan, C.-L. Tan, S.-Y.",
                "Sung, and H.-B.",
                "Low.",
                "Initialization of Cluster Refinement Algorithms: A Review and Comparative Study.",
                "In Proc. of Inter.",
                "Joint Conference on Neural Networks, 2004. [20] A. Y. Ng, M. I. Jordan, Y. Weiss.",
                "On Spectral Clustering: Analysis and an algorithm.",
                "In Advances in Neural Information Processing Systems 14. 2002. [21] B. Sch¨olkopf and A. Smola.",
                "Learning with Kernels.",
                "The MIT Press.",
                "Cambridge, Massachusetts. 2002. [22] J. Shi and J. Malik.",
                "Normalized Cuts and Image Segmentation.",
                "IEEE Trans. on Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000. [23] A. Strehl and J. Ghosh.",
                "Cluster Ensembles - A Knowledge Reuse Framework for Combining Multiple Partitions.",
                "Journal of Machine Learning Research, 3:583-617, 2002. [24] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Berlin: Springer-Verlag, 1995. [25] Wu, M. and Sch¨olkopf, B.",
                "A Local Learning Approach for Clustering.",
                "In Advances in Neural Information Processing Systems 18. 2006. [26] S. X. Yu, J. Shi.",
                "Multiclass Spectral Clustering.",
                "In Proceedings of the International Conference on Computer Vision, 2003. [27] W. Xu, X. Liu and Y. Gong.",
                "Document Clustering Based On Non-Negative Matrix Factorization.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2003. [28] H. Zha, X.",
                "He, C. Ding, M. Gu and H. Simon.",
                "Spectral Relaxation for K-means Clustering.",
                "In NIPS 14. 2001. [29] T. Zhang and F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Journal of Information Retrieval, 4:5-31, 2001. [30] L. Zelnik-Manor and P. Perona.",
                "Self-Tuning Spectral Clustering.",
                "In NIPS 17. 2005. [31] D. Zhou, O. Bousquet, T. N. Lal, J. Weston and B. Sch¨olkopf.",
                "Learning with Local and Global Consistency.",
                "NIPS 17, 2005."
            ],
            "original_annotated_samples": [
                "Generally, document clustering methods can be mainly categorized into two classes: <br>hierarchical method</br>s and partitioning methods.",
                "The <br>hierarchical method</br>s group the data points into a hierarchical tree structure using bottom-up or top-down approaches."
            ],
            "translated_annotated_samples": [
                "Generalmente, los métodos de agrupamiento de documentos se pueden clasificar principalmente en dos clases: <br>métodos jerárquicos</br> y métodos de partición.",
                "Los <br>métodos jerárquicos</br> agrupan los puntos de datos en una estructura de árbol jerárquico utilizando enfoques de abajo hacia arriba o de arriba hacia abajo."
            ],
            "translated_text": "Agrupamiento regularizado para documentos ∗ Fei Wang, Changshui Zhang Laboratorio Clave del Estado de Tecnología Inteligente. En los últimos años, la agrupación de documentos ha estado recibiendo cada vez más atención como una técnica importante y fundamental para la organización no supervisada de documentos, la extracción automática de temas y la recuperación o filtrado rápido de información. En este artículo, proponemos un método novedoso para agrupar documentos utilizando regularización. A diferencia de los métodos de agrupamiento regularizados globalmente tradicionales, nuestro método primero construye un predictor de etiquetas lineal regularizado local para cada vector de documento, y luego combina todos esos regularizadores locales con un regularizador de suavidad global. Así que llamamos a nuestro algoritmo Agrupamiento con Regularización Local y Global (CLGR). Mostraremos que las pertenencias de los documentos a los grupos se pueden lograr mediante la descomposición de los valores propios de una matriz simétrica dispersa, la cual puede resolverse eficientemente mediante métodos iterativos. Finalmente, se presentan nuestras evaluaciones experimentales en varios conjuntos de datos para mostrar las ventajas de CLGR sobre los métodos tradicionales de agrupamiento de documentos. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información-Agrupamiento; I.2.6 [Inteligencia Artificial]: Aprendizaje-Aprendizaje de Conceptos Términos Generales Algoritmos 1. La agrupación de documentos ha estado recibiendo cada vez más atención como una técnica importante y fundamental para la organización no supervisada de documentos, la extracción automática de temas y la recuperación o filtrado rápido de información. Un buen enfoque de agrupación de documentos puede ayudar a las computadoras a organizar automáticamente el corpus de documentos en una jerarquía de clusters significativa para una navegación eficiente, lo cual es muy valioso para complementar las deficiencias de las tecnologías tradicionales de recuperación de información. Como señaló [8], las necesidades de recuperación de información pueden expresarse mediante un espectro que va desde la búsqueda basada en palabras clave estrechas hasta la exploración de información amplia, como cuáles son los principales eventos internacionales de los últimos meses. Los motores de búsqueda de documentos tradicionales tienden a adaptarse bien al extremo de la búsqueda, es decir, suelen proporcionar búsquedas específicas de documentos que coinciden con la consulta de los usuarios, sin embargo, les resulta difícil satisfacer las necesidades del resto del espectro en el que se necesita información más amplia o vaga. En tales casos, la navegación eficiente a través de una buena jerarquía de clústeres será definitivamente útil. Generalmente, los métodos de agrupamiento de documentos se pueden clasificar principalmente en dos clases: <br>métodos jerárquicos</br> y métodos de partición. Los <br>métodos jerárquicos</br> agrupan los puntos de datos en una estructura de árbol jerárquico utilizando enfoques de abajo hacia arriba o de arriba hacia abajo. Por ejemplo, el clustering jerárquico aglomerativo (HAC) [13] es un método típico de clustering jerárquico ascendente. Toma cada punto de datos como un único clúster para comenzar y luego construye clústeres más grandes agrupando puntos de datos similares juntos hasta que todo el conjunto de datos esté encapsulado en un único clúster final. Por otro lado, los métodos de particionamiento descomponen el conjunto de datos en un número de grupos disjuntos que suelen ser óptimos en términos de algunas funciones de criterio predefinidas. Por ejemplo, K-means es un método de particionamiento típico que tiene como objetivo minimizar la suma de las distancias al cuadrado entre los puntos de datos y sus centros de clúster correspondientes. En este documento, nos centraremos en los métodos de particionamiento. Como sabemos, existen dos problemas principales en los métodos de particionamiento (como Kmeans y el Modelo de Mezcla Gaussiana (GMM) [16]): (1) el criterio predefinido suele ser no convexo, lo que provoca muchas soluciones óptimas locales; (2) el procedimiento iterativo (por ejemplo, el algoritmo de Expectation Maximization (EM)) para optimizar los criterios suele hacer que las soluciones finales dependan en gran medida de las inicializaciones. En las últimas décadas, se han propuesto muchos métodos para superar los problemas mencionados de los métodos de particionamiento [19][28]. Recientemente, otro tipo de métodos de particionamiento basados en agrupamiento en grafos de datos han despertado un considerable interés en la comunidad de aprendizaje automático y minería de datos. La idea básica detrás de estos métodos es modelar primero el conjunto de datos completo como un grafo ponderado, en el que los nodos del grafo representan los puntos de datos y los pesos en los bordes corresponden a las similitudes entre los puntos en pares. Entonces, las asignaciones de clústeres del conjunto de datos se pueden lograr optimizando algunos criterios definidos en el gráfico. Por ejemplo, el Clustering Espectral es uno de los enfoques de clustering basados en grafos más representativos, generalmente tiene como objetivo optimizar algún valor de corte (por ejemplo, Corte normalizado [22], corte de razón [7], corte mínimo-máximo [11]) definidos en un grafo no dirigido. Después de algunas relajaciones, estos criterios generalmente pueden ser optimizados a través de descomposiciones propias, lo cual está garantizado que sea óptimo a nivel global. De esta manera, el agrupamiento espectral evita eficientemente los problemas de los métodos de particionamiento tradicionales que presentamos en el párrafo anterior. En este artículo, proponemos un algoritmo novedoso de agrupamiento de documentos que hereda la superioridad del agrupamiento espectral, es decir, los resultados finales de los grupos también pueden obtenerse explotando la estructura de los autovalores de una matriz simétrica. Sin embargo, a diferencia del agrupamiento espectral, que simplemente impone una restricción de suavidad en las etiquetas de los datos sobre todo el conjunto de datos [2], nuestro método primero construye un predictor de etiquetas lineal regularizado para cada punto de datos a partir de su vecindario como en [25], y luego combina los resultados de todos estos predictores de etiquetas locales con un regularizador de suavidad de etiquetas global. Así que llamamos a nuestro método Agrupamiento con Regularización Local y Global (CLGR). La idea de incorporar tanto información local como global en la predicción de etiquetas está inspirada en los trabajos recientes sobre aprendizaje semi-supervisado [31], y nuestras evaluaciones experimentales en varios conjuntos de datos reales de documentos muestran que CLGR funciona mejor que muchos métodos de agrupamiento de vanguardia. El resto de este documento está organizado de la siguiente manera: en la sección 2 presentaremos nuestro algoritmo CLGR en detalle. Los resultados experimentales en varios conjuntos de datos se presentan en la sección 3, seguidos de las conclusiones y discusiones en la sección 4. El ALGORITMO PROPUESTO En esta sección, presentaremos en detalle nuestro algoritmo de Agrupamiento con Regularización Local y Global (CLGR). Primero veamos cómo se representan los documentos a lo largo de este documento. 2.1 Representación de documentos En nuestro trabajo, todos los documentos se representan mediante vectores de frecuencia de términos ponderados. Sea W = {w1, w2, · · · , wm} el conjunto completo de vocabulario del corpus de documentos (que ha sido preprocesado mediante la eliminación de palabras vacías y operaciones de derivación de palabras). El vector de frecuencia de término xi del documento di se define como xi = [xi1, xi2, · · · , xim]T , xik = tik log n idfk , donde tik es la frecuencia del término wk ∈ W, n es el tamaño del corpus de documentos, idfk es el número de documentos que contienen la palabra wk. De esta manera, xi también se llama la representación TFIDF del documento di. Además, también normalizamos cada xi (1 i n) para que tenga una longitud unitaria, de modo que cada documento esté representado por un vector TF-IDF normalizado. 2.2 Regularización Local Como su nombre sugiere, CLGR está compuesto por dos partes: regularización local y regularización global. En esta subsección introduciremos detalladamente la parte de regularización local. 2.2.1 Motivación Como sabemos que el agrupamiento es un tipo de técnica de aprendizaje, tiene como objetivo organizar el conjunto de datos de una manera razonable. En general, el aprendizaje puede plantearse como un problema de estimación de funciones, a partir del cual podemos obtener una buena función de clasificación que asignará etiquetas al conjunto de datos de entrenamiento e incluso al conjunto de datos de prueba no vistos con algún costo minimizado [24]. Por ejemplo, en el escenario de clasificación de dos clases (en el que conocemos exactamente la etiqueta de cada documento), un clasificador lineal con ajuste de mínimos cuadrados tiene como objetivo aprender un vector columna w de manera que el costo al cuadrado J = 1 n (wT xi − yi)2 (1) se minimice, donde yi ∈ {+1, −1} es la etiqueta de xi. Al tomar ∂J /∂w = 0, obtenemos la solución w∗ = Σ i=1 n xixT i −1 Σ i=1 n xiyi, (2) que puede escribirse en su forma matricial como w∗ = XXT −1 Xy, (3) donde X = [x1, x2, · · · , xn] es una matriz de documentos m × n, y = [y1, y2, · · · , yn]T es el vector de etiquetas. Entonces, para un documento de prueba t, podemos determinar su etiqueta mediante l = signo(w∗T u), donde signo(·) es la función signo. Un problema natural en la ecuación (3) es que la matriz XXT puede ser singular y, por lo tanto, no invertible (por ejemplo, cuando m n). Para evitar tal problema, podemos agregar un término de regularización y minimizar el siguiente criterio J = 1 n n i=1 (wT xi − yi)2 + λ w 2, (5), donde λ es un parámetro de regularización. Entonces, la solución óptima que minimiza J está dada por w∗ = XXT + λnI −1 Xy, (6) donde I es una matriz identidad de tamaño m × m. Se ha informado que el clasificador lineal regularizado puede lograr resultados muy buenos en problemas de clasificación de texto [29]. Sin embargo, a pesar de su éxito empírico, el clasificador lineal regularizado es en realidad un clasificador global, es decir, w∗ se estima utilizando todo el conjunto de entrenamiento. Según [24], esta puede que no sea una idea inteligente, ya que un único w∗ puede que no sea lo suficientemente bueno para predecir las etiquetas de todo el espacio de entrada. Para obtener predicciones más precisas, [6] propuso entrenar clasificadores localmente y utilizarlos para clasificar los puntos de prueba. Por ejemplo, un punto de prueba será clasificado por el clasificador local entrenado utilizando los puntos de entrenamiento ubicados en la vecindad 1. En las siguientes discusiones todos asumimos que los documentos provienen solo de dos clases. Las generalizaciones de nuestro método a casos de múltiples clases se discutirán en la sección 2.5 de él. Aunque este método parece lento y estúpido, se informa que puede obtener mejores rendimientos que usar un clasificador global único en ciertas tareas [6]. 2.2.2 Construyendo los Predictores Localmente Regularizados Inspirados en su éxito, propusimos aplicar los algoritmos de aprendizaje local para el agrupamiento. La idea básica es que, para cada vector de documento xi (1 i n), entrenamos un predictor de etiquetas local basado en su vecindario k-más cercano Ni, y luego lo usamos para predecir la etiqueta de xi. Finalmente combinaremos todos esos predictores locales minimizando la suma de sus errores de predicción. En esta subsección introduciremos cómo construir esos predictores locales. Debido a la simplicidad y efectividad del clasificador lineal regularizado que hemos introducido en la sección 2.2.1, lo elegimos como nuestro predictor de etiquetas local, de modo que para cada documento xi, se minimiza el siguiente criterio Ji = 1 ni xj ∈Ni wT i xj − qj 2 + λi wi 2, (7), donde ni = |Ni| es la cardinalidad de Ni, y qj es la pertenencia al clúster de xj. Luego, utilizando la Ecuación (6), podemos obtener que la solución óptima es w∗ i = XiXT i + λiniI −1 Xiqi, (8), donde Xi = [xi1, xi2, · · · , xini], y usamos xik para denotar al k-ésimo vecino más cercano de xi. qi = [qi1, qi2, · · · , qini]T con qik representando la asignación de clúster de xik. El problema aquí es que XiXT i es una matriz m × m con m ni, es decir, deberíamos calcular la inversa de una matriz m × m para cada vector de documento, lo cual está prohibido computacionalmente. Afortunadamente, tenemos el siguiente teorema: Teorema 1. w∗ i en la Ec. (8) puede ser reescrito como w∗ i = Xi XT i Xi + λiniIi −1 qi, (9), donde Ii es una matriz identidad de tamaño ni × ni. Prueba. Dado que w∗ i = XiXT i + λiniI −1 Xiqi, entonces XiXT i + λiniI w∗ i = Xiqi =⇒ XiXT i w∗ i + λiniw∗ i = Xiqi =⇒ w∗ i = (λini)−1 Xi qi − XT i w∗ i. Sea β = (λini)−1 qi − XT i w∗ i, entonces w∗ i = Xiβ =⇒ λiniβ = qi − XT i w∗ i = qi − XT i Xiβ =⇒ qi = XT i Xi + λiniIi β =⇒ β = XT i Xi + λiniIi −1 qi. Por lo tanto, w∗ i = Xiβ = Xi XT i Xi + λiniIi −1 qi 2. Utilizando el teorema 1, solo necesitamos calcular la inversa de una matriz ni × ni para cada documento para entrenar un predictor de etiquetas local. Además, para un nuevo punto de prueba u que cae en Ni, podemos clasificarlo por el signo de qu = w∗T i u = uT wi = uT Xi XT i Xi + λiniIi −1 qi. Esta es una expresión atractiva ya que podemos determinar la asignación del clúster de u utilizando los productos internos entre los puntos en {u ∪ Ni}, lo que sugiere que un regularizador local de este tipo puede ser fácilmente kernelizado [21] siempre y cuando definamos una función de kernel adecuada. 2.2.3 Combinando los Predictores Regularizados Localmente Una vez que todos los predictores locales hayan sido construidos, los combinaremos minimizando Jl = n i=1 w∗T i xi − qi 2, (10), lo que representa la suma de los errores de predicción de todos los predictores locales. Combinando la Ecuación (10) con la Ecuación (6), podemos obtener Jl = n i=1 w∗T i xi − qi 2 = n i=1 xT i Xi XT i Xi + λiniIi −1 qi − qi 2 = Pq − q 2, (11), donde q = [q1, q2, · · · , qn]T, y P es una matriz n × n construida de la siguiente manera. Sea αi = xT i Xi XT i Xi + λiniIi −1 , entonces Pij = αi j, si xj ∈ Ni 0, de lo contrario , (12) donde Pij es la entrada (i, j)-ésima de P, y αi j representa la entrada j-ésima de αi. Hasta ahora podemos escribir el criterio de agrupamiento combinando predictores de etiquetas lineales localmente regularizados Jl en una forma matemática explícita, y podemos minimizarlo directamente utilizando algunas técnicas estándar de optimización. Sin embargo, los resultados pueden no ser lo suficientemente buenos ya que solo explotamos la información local del conjunto de datos. En la siguiente subsección, introduciremos un criterio de regularización global y lo combinaremos con Jl, que tiene como objetivo encontrar un buen resultado de agrupamiento de manera local-global. 2.3 Regularización Global En el agrupamiento de datos, generalmente requerimos que las asignaciones de clúster de los puntos de datos sean lo suficientemente suaves con respecto a la variedad de datos subyacente, lo que implica (1) que los puntos cercanos tienden a tener las mismas asignaciones de clúster; (2) que los puntos en la misma estructura (por ejemplo, subvariedad o clúster) tienden a tener las mismas asignaciones de clúster [31]. Sin pérdida de generalidad, asumimos que los puntos de datos residen (aproximadamente) en una variedad de baja dimensión M2, y q es la función de asignación de clúster definida en M, es decir, 2 Creemos que los datos de texto también se muestrean de alguna variedad de baja dimensión, ya que es imposible que para todos los x ∈ M, q(x) devuelva la membresía del clúster de x. La suavidad de q sobre M se puede calcular mediante la siguiente integral de Dirichlet [2] D[q] = 1 2 M q(x) 2 dM, (13) donde el gradiente q es un vector en el espacio tangente T Mx, y la integral se toma con respecto a la medida estándar en M. Si restringimos la escala de q por q, q M = 1 (donde ·, · M es el producto interno inducido en M), entonces resulta que encontrar la función más suave que minimiza D[q] se reduce a encontrar las autofunciones del operador Laplace Beltrami L, que se define como Lq −div q, (14) donde div es la divergencia de un campo vectorial. Generalmente, el gráfico se puede ver como la forma discretizada de una variedad. Podemos modelar el conjunto de datos como un grafo ponderado no dirigido como en el agrupamiento espectral [22], donde los nodos del grafo son simplemente los puntos de datos, y los pesos en las aristas representan las similitudes entre pares de puntos. Entonces se puede demostrar que minimizar la Ec. (13) corresponde a minimizar Jg = qT Lq = n i=1 (qi − qj)2 wij, (15), donde q = [q1, q2, · · · , qn]T con qi = q(xi), L es el Laplaciano del grafo con su entrada (i, j)-ésima Lij =    di − wii, si i = j −wij, si xi y xj son adyacentes 0, en otro caso, (16), donde di = j wij es el grado de xi, wij es la similitud entre xi y xj. Si xi y xj son adyacentes, wij generalmente se calcula de la siguiente manera: wij = e − xi−xj 2 2σ2, donde σ es un parámetro dependiente del conjunto de datos. Se ha demostrado que bajo ciertas condiciones, tal forma de wij para determinar los pesos en los bordes del grafo conduce a la convergencia del Laplaciano del grafo al operador Laplace Beltrami [3][18]. En resumen, el uso de la Ec. (15) con pesos exponenciales puede medir de manera efectiva la suavidad de las asignaciones de datos con respecto a la variedad intrínseca de datos. Por lo tanto, lo adoptamos como un regularizador global para penalizar la suavidad de las asignaciones de datos predichas. 2.4 Agrupamiento con Regularización Local y Global Combinando los contenidos que hemos introducido en la sección 2.2 y la sección 2.3, podemos derivar que el criterio de agrupamiento es minq J = Jl + λJg = Pq − q 2 + λqT Lq sujeto a qi ∈ {−1, +1}, (18) donde P está definido como en la Ecuación (12), y λ es un parámetro de regularización para equilibrar Jl y Jg. Sin embargo, los valores discretos llenan todo el espacio muestral de alta dimensionalidad. Y se ha demostrado que los métodos basados en variedades pueden lograr buenos resultados en tareas de clasificación de texto [31]. En este artículo, definimos xi y xj como adyacentes si xi ∈ N(xj) o xj ∈ N(xi). La restricción de pi convierte el problema en un problema de programación entera NP difícil. Una forma natural de hacer que el problema sea resoluble es eliminar la restricción y relajar qi para que sea continuo, luego el objetivo que buscamos minimizar se convierte en J = Pq − q 2 + λqT Lq = qT (P − I)T (P − I)q + λqT Lq = qT (P − I)T (P − I) + λL q, (19) y luego agregamos una restricción adicional qT q = 1 para restringir la escala de q. Entonces nuestro objetivo se convierte en minq J = qT (P − I)T (P − I) + λL q s.t. qT q = 1 (20) Utilizando el método de Lagrange, podemos derivar que la solución óptima q corresponde al menor vector propio de la matriz M = (P − I)T (P − I) + λL, y la asignación de clúster de xi puede determinarse por el signo de qi, es decir, xi se clasificará como clase uno si qi > 0, de lo contrario se clasificará como clase 2. 2.5 CLGR Multiclase En lo anterior hemos introducido el marco básico de Agrupamiento con Regularización Local y Global (CLGR) para el problema de agrupamiento de dos clases, y lo extenderemos al agrupamiento multiclase en esta subsección. Primero asumimos que todos los documentos pertenecen a C clases indexadas por L = {1, 2, · · · , C}. qc es la función de clasificación para la clase c (1 ≤ c ≤ C), tal que qc(xi) devuelve la confianza de que xi pertenece a la clase c. Nuestro objetivo es obtener el valor de qc(xi) (1 ≤ c ≤ C, 1 ≤ i ≤ n), y la asignación de cluster de xi puede ser determinada por {qc(xi)}C c=1 utilizando algunos métodos adecuados de discretización que presentaremos más adelante. Por lo tanto, en este caso de múltiples clases, para cada documento xi (1 i n), construiremos C predictores de etiquetas regularizados localmente lineales cuyos vectores normales son wc∗ i = Xi XT i Xi + λiniIi −1 qc i (1 c C), (21), donde Xi = [xi1, xi2, · · · , xini ] con xik siendo el k-ésimo vecino de xi, y qc i = [qc i1, qc i2, · · · , qc ini ]T con qc ik = qc (xik). Entonces (wc∗ i )T xi devuelve la confianza predicha de xi perteneciente a la clase c. Por lo tanto, el error de predicción local para la clase c se puede definir como J c l = n i=1 (wc∗ i ) T xi − qc i 2 , (22) Y el error de predicción local total se convierte en Jl = C c=1 J c l = C c=1 n i=1 (wc∗ i ) T xi − qc i 2 . (23) Como en la Ec. (11), podemos definir una matriz n×n P (ver Ec. (12)) y reescribir Jl como Jl = C c=1 J c l = C c=1 Pqc − qc 2 . (24) De manera similar, podemos definir el regularizador de suavidad global en el caso de múltiples clases como Jg = C c=1 n i=1 (qc i − qc j )2 wij = C c=1 (qc )T Lqc . (25) Luego, el criterio a minimizar para CLGR en el caso de múltiples clases se convierte en J = Jl + λJg = C c=1 Pqc − qc 2 + λ(qc )T Lqc = C c=1 (qc )T (P − I)T (P − I) + λL qc = trace QT (P − I)T (P − I) + λL Q , (26) donde Q = [q1 , q2 , · · · , qc ] es una matriz n × c, y trace(·) devuelve la traza de una matriz. Al igual que en la ecuación (20), también agregamos la restricción de que QT Q = I para limitar la escala de Q. Entonces nuestro problema de optimización se convierte en minQ J = traza QT (P − I)T (P − I) + λL Q sujeto a. Según el teorema de Ky Fan [28], sabemos que la solución óptima del problema anterior es Q∗ = [q∗ 1, q∗ 2, · · · , q∗ C ]R, donde q∗ k (1 ≤ k ≤ C) es el autovector que corresponde al k-ésimo valor propio más pequeño de la matriz (P − I)T (P − I) + λL, y R es una matriz arbitraria de tamaño C × C. Dado que los valores de las entradas en Q∗ son continuos, necesitamos discretizar aún más Q∗ para obtener las asignaciones de clúster de todos los puntos de datos. Principalmente hay dos enfoques para lograr este objetivo: 1. Como en [20], podemos tratar la i-ésima fila de Q como la incrustación de xi en un espacio de C dimensiones, y aplicar algunos métodos de agrupamiento tradicionales como kmeans para agrupar estas incrustaciones en C grupos. 2. Dado que el Q∗ óptimo no es único (debido a la existencia de una matriz R arbitraria), podemos buscar un R óptimo que rote Q∗ a una matriz de indicación4. El algoritmo detallado se puede consultar en [26]. El procedimiento detallado del algoritmo para CLGR se resume en la tabla 1. 3. EXPERIMENTOS En esta sección, se realizan experimentos para comparar empíricamente los resultados de agrupamiento de CLGR con otros 8 algoritmos representativos de agrupamiento de documentos en 5 conjuntos de datos. Primero presentaremos la información básica de esos conjuntos de datos. 3.1 Conjuntos de datos Utilizamos una variedad de conjuntos de datos, la mayoría de los cuales son frecuentemente utilizados en la investigación de recuperación de información. La Tabla 2 resume las características de los conjuntos de datos. Aquí, una matriz de indicación T es una matriz n×c con su entrada (i, j) Tij ∈ {0, 1} de modo que para cada fila de Q∗ solo hay un 1. Entonces, el xi puede ser asignado al j-ésimo grupo tal que j = argjQ∗ ij = 1. Tabla 1: Agrupamiento con Regularización Local y Global (CLGR) Entrada: 1. Conjunto de datos X = {xi}n i=1; 2. Número de grupos C: 3. Tamaño del vecindario K: 4. Parámetros locales de regularización {λi}n i=1; 5. Parámetro de regularización global λ; Salida: La membresía del clúster de cada punto de datos. Procedimiento: 1. Construir los K vecindarios más cercanos para cada punto de datos; 2. Construya la matriz P utilizando la Ecuación (12); 3. Construya la matriz Laplaciana L utilizando la ecuación (16); 4. Construye la matriz M = (P − I)T (P − I) + λL; 5. Realice la descomposición de valores propios en M y construya la matriz Q∗ de acuerdo con la Ecuación (28); 6. Generar las asignaciones de clúster de cada punto de datos al discretizar adecuadamente Q∗. Tabla 2: Descripciones de los conjuntos de datos de documentos Conjuntos de datos Número de documentos Número de clases CSTR 476 4 WebKB4 4199 4 Reuters 2900 10 WebACE 2340 20 Newsgroup4 3970 4 CSTR. Este es el conjunto de datos de los resúmenes de informes técnicos publicados en el Departamento de Ciencias de la Computación de una universidad. El conjunto de datos contenía 476 resúmenes, los cuales fueron divididos en cuatro áreas de investigación: Procesamiento de Lenguaje Natural (NLP), Robótica/Visión, Sistemas y Teoría. WebKB. El conjunto de datos WebKB contiene páginas web recopiladas de los departamentos de informática de universidades. Hay alrededor de 8280 documentos y están divididos en 7 categorías: estudiante, facultad, personal, curso, proyecto, departamento y otros. El texto sin procesar es de aproximadamente 27MB. Entre estas 7 categorías, estudiante, facultad, curso y proyecto son las cuatro categorías que representan entidades más pobladas. El subconjunto asociado suele ser llamado WebKB4. Reuters. La colección de pruebas de categorización de texto Reuters-21578 contiene documentos recopilados de la agencia de noticias Reuters en 1987. Es un punto de referencia estándar para la categorización de texto y contiene 135 categorías. En nuestros experimentos, utilizamos un subconjunto de la colección de datos que incluye las 10 categorías más frecuentes entre los 135 temas y lo llamamos Reuters-top 10. WebACE. El conjunto de datos WebACE proviene del proyecto WebACE y ha sido utilizado para la agrupación de documentos [17][5]. El conjunto de datos WebACE contiene 2340 documentos que consisten en artículos de noticias del servicio de noticias de Reuters a través de la web en octubre de 1997. Estos documentos están divididos en 20 clases. Not enough context provided. El conjunto de datos News4 utilizado en nuestros experimentos se selecciona del famoso conjunto de datos 20-newsgroups. El tema rec que contiene autos, motocicletas, béisbol y hockey fue seleccionado de la versión 20news-18828. El conjunto de datos News4 contiene 3970 vectores de documentos. Para preprocesar los conjuntos de datos, eliminamos las palabras vacías utilizando una lista estándar de palabras vacías, se omiten todas las etiquetas HTML y se ignoran todos los campos de encabezado excepto el asunto y la organización de los artículos publicados. En todos nuestros experimentos, primero seleccionamos las 1000 palabras principales por información mutua con las etiquetas de clase. 3.2 Métricas de Evaluación En los experimentos, establecemos el número de clústeres igual al número real de clases C para todos los algoritmos de agrupamiento. Para evaluar su rendimiento, comparamos los grupos generados por estos algoritmos con las clases reales mediante el cálculo de las siguientes dos medidas de rendimiento. Precisión de agrupamiento (Acc). La primera medida de rendimiento es la Precisión de Agrupamiento, que descubre la relación uno a uno entre los grupos y las clases, y mide en qué medida cada grupo contenía puntos de datos de la clase correspondiente. Resume todo el grado de coincidencia entre todos los pares de clases y clusters. La precisión del agrupamiento se puede calcular como: Acc = 1 N max   Ck,Lm T(Ck, Lm)   , (29) donde Ck denota el k-ésimo clúster en los resultados finales, y Lm es la verdadera clase m-ésima. T(Ck, Lm) es el número de entidades que pertenecen a la clase m y están asignadas al clúster k. La precisión calcula la suma máxima de T(Ck, Lm) para todos los pares de clústeres y clases, y estos pares no tienen superposiciones. La mayor precisión de agrupamiento significa un mejor rendimiento de agrupamiento. Información Mutua Normalizada (NMI). Otro métrica de evaluación que adoptamos aquí es la Información Mutua Normalizada (NMI) [23], la cual es ampliamente utilizada para determinar la calidad de los grupos. Para dos variables aleatorias X e Y, el NMI se define como: NMI(X, Y) = I(X, Y) H(X)H(Y), donde I(X, Y) es la información mutua entre X e Y, mientras que H(X) y H(Y) son las entropías de X e Y respectivamente. Se puede ver que NMI(X, X) = 1, que es el valor máximo posible de NMI. Dado un resultado de agrupamiento, el NMI en la ecuación (30) se estima como NMI = C k=1 C m=1 nk,mlog n·nk,m nk ˆnm C k=1 nklog nk n C m=1 ˆnmlog ˆnm n, (31), donde nk denota el número de datos contenidos en el grupo Ck (1 k C), ˆnm es el número de datos pertenecientes a la clase m-ésima (1 m C), y nk,m denota el número de datos que están en la intersección entre el grupo Ck y la clase m-ésima. El valor calculado en la Ecuación (31) se utiliza como medida de rendimiento para el resultado de agrupamiento dado. A mayor valor, mejor rendimiento de agrupamiento. 3.3 Comparaciones Hemos realizado evaluaciones de rendimiento exhaustivas probando nuestro método y comparándolo con otros 8 métodos representativos de agrupamiento de datos utilizando las mismas corpora de datos. Los algoritmos que evaluamos se enumeran a continuación. 1. K-means tradicional (KM). 2. K-medias esférico (SKM). La implementación se basa en [9]. 3. Modelo de Mezcla Gaussiana (GMM). La implementación se basa en [16]. 4. Agrupamiento espectral con cortes normalizados (Ncut). La implementación se basa en [26], y la varianza de la similitud gaussiana se determina mediante Escalado Local [30]. Ten en cuenta que el criterio que Ncut busca minimizar es simplemente el regularizador global en nuestro algoritmo CLGR, excepto que Ncut utiliza el Laplaciano normalizado. 5. Agrupamiento utilizando Regularización Local Pura (CPLR). En este método simplemente minimizamos Jl (definido en la Ecuación (24)), y los resultados de agrupamiento pueden obtenerse realizando la descomposición de valores propios en la matriz (I − P)T (I − P) con algunos métodos de discretización adecuados. 6. Iteración de subespacio adaptativa (ASI). La implementación se basa en [14]. 7. Factorización de matrices no negativas (NMF). La implementación se basa en [27]. 8. Factorización Tri-Factorización de Matrices No Negativas (TNMF) [12]. La implementación se basa en [15]. Para la eficiencia computacional, en la implementación de CPLR y nuestro algoritmo CLGR, hemos establecido todos los parámetros de regularización local {λi}n i=1 para ser idénticos, los cuales se establecen mediante búsqueda en cuadrícula de {0.1, 1, 10}. El tamaño de los vecindarios k más cercanos se establece mediante una búsqueda en cuadrícula de {20, 40, 80}. Para el método CLGR, su parámetro de regularización global se establece mediante una búsqueda en cuadrícula de {0.1, 1, 10}. Al construir el regularizador global, hemos adoptado el método de escalamiento local [30] para construir la matriz Laplaciana. El método de discretización final adoptado en estos dos métodos es el mismo que en [26], ya que nuestros experimentos muestran que el uso de dicho método puede lograr mejores resultados que el uso de métodos basados en kmeans como en [20]. 3.4 Resultados Experimentales Los resultados de comparación de precisión de agrupamiento se muestran en la tabla 3, y los resultados de comparación de información mutua normalizada se resumen en la tabla 4. De las dos tablas principalmente observamos que: 1. Nuestro método CLGR supera a todos los demás métodos de agrupamiento de documentos en la mayoría de los conjuntos de datos. Para la agrupación de documentos, el método Spherical k-means suele superar al método tradicional de agrupación k-means, y el método GMM puede lograr resultados competitivos en comparación con el método Spherical k-means; 3. Los resultados obtenidos de los algoritmos tipo k-means y GMM suelen ser peores que los resultados obtenidos de Clustering Espectral. Dado que el Agrupamiento Espectral puede ser visto como una versión ponderada del kernel k-means, puede obtener buenos resultados cuando los clusters de datos tienen formas arbitrarias. Esto corrobora que los vectores de los documentos no están distribuidos regularmente (esféricos o elípticos). 4. Las comparaciones experimentales verifican empíricamente la equivalencia entre NMF y Clustering Espectral, como se muestra en la Tabla 3: Precisión de agrupamiento de los diversos métodos CSTR WebKB4 Reuters WebACE News4 KM 0.4256 0.3888 0.4448 0.4001 0.3527 SKM 0.4690 0.4318 0.5025 0.4458 0.3912 GMM 0.4487 0.4271 0.4897 0.4521 0.3844 NMF 0.5713 0.4418 0.4947 0.4761 0.4213 Ncut 0.5435 0.4521 0.4896 0.4513 0.4189 ASI 0.5621 0.4752 0.5235 0.4823 0.4335 TNMF 0.6040 0.4832 0.5541 0.5102 0.4613 CPLR 0.5974 0.5020 0.4832 0.5213 0.4890 CLGR 0.6235 0.5228 0.5341 0.5376 0.5102 Tabla 4: Resultados de información mutua normalizada de los diversos métodos CSTR WebKB4 Reuters WebACE News4 KM 0.3675 0.3023 0.4012 0.3864 0.3318 SKM 0.4027 0.4155 0.4587 0.4003 0.4085 GMM 0.4034 0.4093 0.4356 0.4209 0.3994 NMF 0.5235 0.4517 0.4402 0.4359 0.4130 Ncut 0.4833 0.4497 0.4392 0.4289 0.4231 ASI 0.5008 0.4833 0.4769 0.4817 0.4503 TNMF 0.5724 0.5011 0.5132 0.5328 0.4749 CPLR 0.5695 0.5231 0.4402 0.5543 0.4690 CLGR 0.6012 0.5434 0.4935 0.5390 0.4908 ha sido demostrado teóricamente en [10]. Se puede observar en las tablas que NMF y el Agrupamiento Espectral suelen llevar a resultados de agrupamiento similares. 5. Los métodos basados en co-agrupamiento (TNMF y ASI) suelen lograr mejores resultados que los métodos tradicionales basados únicamente en vectores de documentos. Dado que estos métodos realizan una selección implícita de características en cada iteración, proporcionan una métrica adaptativa para medir el vecindario y, por lo tanto, tienden a producir mejores resultados de agrupamiento. 6. Los resultados obtenidos a través de CPLR suelen ser mejores que los resultados obtenidos a través de Agrupamiento Espectral, lo cual respalda la teoría de Vapnik [24] de que a veces los algoritmos de aprendizaje local pueden obtener mejores resultados que los algoritmos de aprendizaje global. Además de los experimentos de comparación mencionados anteriormente, también probamos la sensibilidad de los parámetros de nuestro método. Principalmente hay dos conjuntos de parámetros en nuestro algoritmo CLGR, los parámetros de regularización locales y globales ({λi}n i=1 y λ, como hemos mencionado en la sección 3.3, hemos establecido que todos los λis son idénticos a λ∗ en nuestros experimentos), y el tamaño de los vecindarios. Por lo tanto, también hemos realizado dos series de experimentos: 1. Fijando el tamaño de los vecindarios y probando el rendimiento del agrupamiento con diferentes valores de λ∗ y λ. En este conjunto de experimentos, encontramos que nuestro algoritmo CLGR puede lograr buenos resultados cuando los dos parámetros de regularización no son ni demasiado grandes ni demasiado pequeños. Normalmente nuestro método puede lograr buenos resultados cuando λ∗ y λ están alrededor de 0.1. La Figura 1 nos muestra un ejemplo de prueba en el conjunto de datos WebACE. Fijando los parámetros de regularización local y global, y probando el rendimiento del agrupamiento con diferentes valores de -5, -4.5, -4, -3.5, -3, -5, -4.5, -4, -3.5, -3, 0.35, 0.4, 0.45, 0.5, 0.55 para el parámetro de regularización local (valor logarítmico base 2) y para el parámetro de regularización global (valor logarítmico base 2), se obtuvo una precisión de agrupamiento. Figura 1: Resultados de la prueba de sensibilidad de parámetros en el conjunto de datos WebACE con el tamaño de vecindario fijo en 20, donde el eje x e y representan el valor logarítmico base 2 de λ∗ y λ. En este conjunto de experimentos, encontramos que el vecindario con un tamaño demasiado grande o demasiado pequeño deteriorará todos los resultados finales de agrupamiento. Esto puede entenderse fácilmente ya que cuando el tamaño del vecindario es muy pequeño, los puntos de datos utilizados para entrenar los clasificadores locales pueden no ser suficientes; cuando el tamaño del vecindario es muy grande, los clasificadores entrenados tenderán a ser globales y no podrán capturar las características locales típicas. La Figura 2 nos muestra un ejemplo de prueba en el conjunto de datos WebACE. Por lo tanto, podemos ver que nuestro algoritmo CLGR (1) puede lograr resultados satisfactorios y (2) no es muy sensible a la elección de parámetros, lo que lo hace práctico en aplicaciones del mundo real. CONCLUSIONES Y TRABAJOS FUTUROS En este artículo, desarrollamos un nuevo algoritmo de agrupamiento llamado agrupamiento con regularización local y global. Nuestro método conserva el mérito de los algoritmos de aprendizaje local y el agrupamiento espectral. Nuestros experimentos muestran que el algoritmo propuesto supera a la mayoría de los algoritmos de vanguardia en muchos conjuntos de datos de referencia. En el futuro, nos enfocaremos en la selección de parámetros y los problemas de aceleración del algoritmo CLGR. REFERENCIAS [1] L. Baker y A. McCallum. Agrupamiento distribucional de palabras para clasificación de texto. En Actas de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1998. [2] M. Belkin y P. Niyogi. Mapeo de Eigen de Laplaciano para Reducción de Dimensionalidad y Representación de Datos. Computación Neural, 15 (6):1373-1396. Junio de 2003. [3] M. Belkin y P. Niyogi. Hacia una Fundación Teórica para Métodos de Manifold Basados en Laplacianos. En Actas de la 18ª Conferencia sobre Teoría del Aprendizaje (COLT). 2005. 10 20 30 40 50 60 70 80 90 100 0.35 0.4 0.45 0.5 0.55 tamaño de la precisión de agrupamiento del vecindario Figura 2: Resultados de pruebas de sensibilidad de parámetros en el conjunto de datos WebACE con los parámetros de regularización fijados en 0.1, y el tamaño del vecindario variando de 10 a 100. [4] M. Belkin, P. Niyogi y V. Sindhwani. Regularización de variedades: un marco geométrico para el aprendizaje a partir de ejemplos. Revista de Investigación en Aprendizaje Automático 7, 1-48, 2006. [5] D. Boley. División Direccional Principal. Minería de datos y descubrimiento de conocimiento, 2:325-344, 1998. [6] L. Bottou y V. Vapnik. Algoritmos de aprendizaje local. Computación Neural, 4:888-900, 1992. [7] P. K. Chan, D. F. Schlag y J. Y. Zien. Particionamiento y agrupamiento K-way de corte de razón espectral. IEEE Trans. Diseño Asistido por Computadora, 13:1088-1096, Sep. 1994. [8] D. R. Cutting, D. R. Karger, J. O. Pederson y J. W. Tukey. Dispersión/Recolección: Un enfoque basado en clústeres para explorar grandes colecciones de documentos. En Actas de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1992. [9] I. S. Dhillon y D. S. Modha. Descomposiciones de conceptos para datos de texto grandes y dispersos utilizando agrupamiento. Aprendizaje automático, vol. 42(1), páginas 143-175, enero de 2001. [10] C. Ding, X. Él, y H. Simon. Sobre la equivalencia entre la factorización de matrices no negativas y el agrupamiento espectral. En Actas de la Conferencia de Minería de Datos de SIAM, 2005. [11] C. Ding, X. Él, H. Zha, M. Gu y H. D. Simon. Un algoritmo de corte min-max para particionar grafos y agrupar datos. En Proc. de la 1ra Conferencia Internacional sobre Minería de Datos (ICDM), páginas 107-114, 2001. [12] C. Ding, T. Li, W. Peng y H. Park. Tri-factorizaciones de matrices no negativas ortogonales para agrupamiento. En Actas de la Duodécima Conferencia Internacional de la ACM SIGKDD sobre Descubrimiento de Conocimiento y Minería de Datos, 2006. [13] R. O. Duda, P. E. Hart y D. G. Stork. Clasificación de patrones. John Wiley & Sons, Inc., 2001. [14] T. Li, S. Ma y M. Ogihara. Agrupamiento de documentos a través de la Iteración de Subespacios Adaptativos. En Actas de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2004. [15] T. Li y C. Ding. Las relaciones entre varios métodos de factorización de matrices no negativas para agrupamiento. En Actas de la 6ta Conferencia Internacional sobre Minería de Datos (ICDM). 2006. [16] X. Liu y Y. Gong. Agrupación de documentos con capacidades de refinamiento de clústeres y selección de modelos. En Proc. de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2002. [17] E. Han, D. Boley, M. Gini, R. Gross, K. Hastings, G. Karypis, V. Kumar, B. Mobasher y J. Moore. WebACE: Un agente web para la categorización y exploración de documentos. En Actas de la 2ª Conferencia Internacional sobre Agentes Autónomos (Agents98). ACM Press, 1998. [18] M. Hein, J. Y. Audibert y U. von Luxburg. De Gráficas a Variedades - Consistencia Puntual Débil y Fuerte de los Laplacianos de Gráficas. En Actas de la 18ª Conferencia sobre Teoría del Aprendizaje (COLT), 470-485. 2005. [19] J. Él, M. Lan, C.-L. Tan, S.-Y. Sung, y H.-B. Bajo. Inicialización de algoritmos de refinamiento de clústeres: una revisión y estudio comparativo. En Proc. de Inter. Conferencia Conjunta sobre Redes Neuronales, 2004. [20] A. Y. Ng, M. I. Jordan, Y. Weiss. En el agrupamiento espectral: análisis y un algoritmo. En Avances en Sistemas de Procesamiento de Información Neural 14. 2002. [21] B. Sch¨olkopf y A. Smola. Aprendizaje con Kernels. El MIT Press. Cambridge, Massachusetts. 2002. [22] J. Shi y J. Malik. Cortes normalizados y segmentación de imágenes. IEEE Trans. on Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000. [23] A. Strehl and J. Ghosh.\nIEEE Trans. on Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000. [23] A. Strehl y J. Ghosh. Conjuntos de agrupamientos: un marco de reutilización de conocimiento para combinar múltiples particiones. Revista de Investigación de Aprendizaje Automático, 3:583-617, 2002. [24] V. N. Vapnik. La naturaleza de la teoría del aprendizaje estadístico. Berlín: Springer-Verlag, 1995. [25] Wu, M. y Schölkopf, B. Un enfoque de aprendizaje local para el agrupamiento. En Avances en Sistemas de Procesamiento de Información Neural 18. 2006. [26] S. X. Yu, J. Shi. Agrupamiento espectral multiclase. En Actas de la Conferencia Internacional sobre Visión por Computadora, 2003. [27] W. Xu, X. Liu y Y. Gong. Agrupación de documentos basada en la factorización de matrices no negativas. En Actas de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2003. [28] H. Zha, X. Él, C. Ding, M. Gu y H. Simon. Relajación espectral para el agrupamiento K-means. En NIPS 14. 2001. [29] T. Zhang y F. J. Oles. Categorización de texto basada en métodos de clasificación lineal regularizados. Revista de Recuperación de Información, 4:5-31, 2001. [30] L. Zelnik-Manor y P. Perona. Agrupamiento espectral autoajustable. En NIPS 17. 2005. [31] D. Zhou, O. Bousquet, T. N. Lal, J. Weston y B. Schölkopf. Aprendizaje con Consistencia Local y Global. NIPS 17, 2005. \n\nNIPS 17, 2005. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "partitioning method": {
            "translated_key": "método de particionamiento",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Regularized Clustering for Documents ∗ Fei Wang, Changshui Zhang State Key Lab of Intelligent Tech.",
                "and Systems Department of Automation, Tsinghua University Beijing, China, 100084 feiwang03@gmail.com Tao Li School of Computer Science Florida International University Miami, FL 33199, U.S.A. taoli@cs.fiu.edu ABSTRACT In recent years, document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering.",
                "In this paper, we propose a novel method for clustering documents using regularization.",
                "Unlike traditional globally regularized clustering methods, our method first construct a local regularized linear label predictor for each document vector, and then combine all those local regularizers with a global smoothness regularizer.",
                "So we call our algorithm Clustering with Local and Global Regularization (CLGR).",
                "We will show that the cluster memberships of the documents can be achieved by eigenvalue decomposition of a sparse symmetric matrix, which can be efficiently solved by iterative methods.",
                "Finally our experimental evaluations on several datasets are presented to show the superiorities of CLGR over traditional document clustering methods.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Clustering; I.2.6 [Artificial Intelligence]: Learning-Concept Learning General Terms Algorithms 1.",
                "INTRODUCTION Document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering.",
                "A good document clustering approach can assist the computers to automatically organize the document corpus into a meaningful cluster hierarchy for efficient browsing and navigation, which is very valuable for complementing the deficiencies of traditional information retrieval technologies.",
                "As pointed out by [8], the information retrieval needs can be expressed by a spectrum ranged from narrow keyword-matching based search to broad information browsing such as what are the major international events in recent months.",
                "Traditional document retrieval engines tend to fit well with the search end of the spectrum, i.e. they usually provide specified search for documents matching the users query, however, it is hard for them to meet the needs from the rest of the spectrum in which a rather broad or vague information is needed.",
                "In such cases, efficient browsing through a good cluster hierarchy will be definitely helpful.",
                "Generally, document clustering methods can be mainly categorized into two classes: hierarchical methods and partitioning methods.",
                "The hierarchical methods group the data points into a hierarchical tree structure using bottom-up or top-down approaches.",
                "For example, hierarchical agglomerative clustering (HAC) [13] is a typical bottom-up hierarchical clustering method.",
                "It takes each data point as a single cluster to start off with and then builds bigger and bigger clusters by grouping similar data points together until the entire dataset is encapsulated into one final cluster.",
                "On the other hand, partitioning methods decompose the dataset into a number of disjoint clusters which are usually optimal in terms of some predefined criterion functions.",
                "For instance, K-means [13] is a typical <br>partitioning method</br> which aims to minimize the sum of the squared distance between the data points and their corresponding cluster centers.",
                "In this paper, we will focus on the partitioning methods.",
                "As we know that there are two main problems existing in partitioning methods (like Kmeans and Gaussian Mixture Model (GMM) [16]): (1) the predefined criterion is usually non-convex which causes many local optimal solutions; (2) the iterative procedure (e.g. the Expectation Maximization (EM) algorithm) for optimizing the criterions usually makes the final solutions heavily depend on the initializations.",
                "In the last decades, many methods have been proposed to overcome the above problems of the partitioning methods [19][28].",
                "Recently, another type of partitioning methods based on clustering on data graphs have aroused considerable interests in the machine learning and data mining community.",
                "The basic idea behind these methods is to first model the whole dataset as a weighted graph, in which the graph nodes represent the data points, and the weights on the edges correspond to the similarities between pairwise points.",
                "Then the cluster assignments of the dataset can be achieved by optimizing some criterions defined on the graph.",
                "For example Spectral Clustering is one kind of the most representative graph-based clustering approaches, it generally aims to optimize some cut value (e.g.",
                "Normalized Cut [22], Ratio Cut [7], Min-Max Cut [11]) defined on an undirected graph.",
                "After some relaxations, these criterions can usually be optimized via eigen-decompositions, which is guaranteed to be global optimal.",
                "In this way, spectral clustering efficiently avoids the problems of the traditional partitioning methods as we introduced in last paragraph.",
                "In this paper, we propose a novel document clustering algorithm that inherits the superiority of spectral clustering, i.e. the final cluster results can also be obtained by exploit the eigen-structure of a symmetric matrix.",
                "However, unlike spectral clustering, which just enforces a smoothness constraint on the data labels over the whole data manifold [2], our method first construct a regularized linear label predictor for each data point from its neighborhood as in [25], and then combine the results of all these local label predictors with a global label smoothness regularizer.",
                "So we call our method Clustering with Local and Global Regularization (CLGR).",
                "The idea of incorporating both local and global information into label prediction is inspired by the recent works on semi-supervised learning [31], and our experimental evaluations on several real document datasets show that CLGR performs better than many state-of-the-art clustering methods.",
                "The rest of this paper is organized as follows: in section 2 we will introduce our CLGR algorithm in detail.",
                "The experimental results on several datasets are presented in section 3, followed by the conclusions and discussions in section 4. 2.",
                "THE PROPOSED ALGORITHM In this section, we will introduce our Clustering with Local and Global Regularization (CLGR) algorithm in detail.",
                "First lets see the how the documents are represented throughout this paper. 2.1 Document Representation In our work, all the documents are represented by the weighted term-frequency vectors.",
                "Let W = {w1, w2, · · · , wm} be the complete vocabulary set of the document corpus (which is preprocessed by the stopwords removal and words stemming operations).",
                "The term-frequency vector xi of document di is defined as xi = [xi1, xi2, · · · , xim]T , xik = tik log n idfk , where tik is the term frequency of wk ∈ W, n is the size of the document corpus, idfk is the number of documents that contain word wk.",
                "In this way, xi is also called the TFIDF representation of document di.",
                "Furthermore, we also normalize each xi (1 i n) to have a unit length, so that each document is represented by a normalized TF-IDF vector. 2.2 Local Regularization As its name suggests, CLGR is composed of two parts: local regularization and global regularization.",
                "In this subsection we will introduce the local regularization part in detail. 2.2.1 Motivation As we know that clustering is one type of learning techniques, it aims to organize the dataset in a reasonable way.",
                "Generally speaking, learning can be posed as a problem of function estimation, from which we can get a good classification function that will assign labels to the training dataset and even the unseen testing dataset with some cost minimized [24].",
                "For example, in the two-class classification scenario1 (in which we exactly know the label of each document), a linear classifier with least square fit aims to learn a column vector w such that the squared cost J = 1 n (wT xi − yi)2 (1) is minimized, where yi ∈ {+1, −1} is the label of xi.",
                "By taking ∂J /∂w = 0, we get the solution w∗ = n i=1 xixT i −1 n i=1 xiyi , (2) which can further be written in its matrix form as w∗ = XXT −1 Xy, (3) where X = [x1, x2, · · · , xn] is an m × n document matrix, y = [y1, y2, · · · , yn]T is the label vector.",
                "Then for a test document t, we can determine its label by l = sign(w∗T u), (4) where sign(·) is the sign function.",
                "A natural problem in Eq. (3) is that the matrix XXT may be singular and thus not invertable (e.g. when m n).",
                "To avoid such a problem, we can add a regularization term and minimize the following criterion J = 1 n n i=1 (wT xi − yi)2 + λ w 2 , (5) where λ is a regularization parameter.",
                "Then the optimal solution that minimize J is given by w∗ = XXT + λnI −1 Xy, (6) where I is an m × m identity matrix.",
                "It has been reported that the regularized linear classifier can achieve very good results on text classification problems [29].",
                "However, despite its empirical success, the regularized linear classifier is on earth a global classifier, i.e. w∗ is estimated using the whole training set.",
                "According to [24], this may not be a smart idea, since a unique w∗ may not be good enough for predicting the labels of the whole input space.",
                "In order to get better predictions, [6] proposed to train classifiers locally and use them to classify the testing points.",
                "For example, a testing point will be classified by the local classifier trained using the training points located in the vicinity 1 In the following discussions we all assume that the documents coming from only two classes.",
                "The generalizations of our method to multi-class cases will be discussed in section 2.5. of it.",
                "Although this method seems slow and stupid, it is reported that it can get better performances than using a unique global classifier on certain tasks [6]. 2.2.2 Constructing the Local Regularized Predictors Inspired by their success, we proposed to apply the local learning algorithms for clustering.",
                "The basic idea is that, for each document vector xi (1 i n), we train a local label predictor based on its k-nearest neighborhood Ni, and then use it to predict the label of xi.",
                "Finally we will combine all those local predictors by minimizing the sum of their prediction errors.",
                "In this subsection we will introduce how to construct those local predictors.",
                "Due to the simplicity and effectiveness of the regularized linear classifier that we have introduced in section 2.2.1, we choose it to be our local label predictor, such that for each document xi, the following criterion is minimized Ji = 1 ni xj ∈Ni wT i xj − qj 2 + λi wi 2 , (7) where ni = |Ni| is the cardinality of Ni, and qj is the cluster membership of xj.",
                "Then using Eq. (6), we can get the optimal solution is w∗ i = XiXT i + λiniI −1 Xiqi, (8) where Xi = [xi1, xi2, · · · , xini ], and we use xik to denote the k-th nearest neighbor of xi. qi = [qi1, qi2, · · · , qini ]T with qik representing the cluster assignment of xik.",
                "The problem here is that XiXT i is an m × m matrix with m ni, i.e. we should compute the inverse of an m × m matrix for every document vector, which is computationally prohibited.",
                "Fortunately, we have the following theorem: Theorem 1. w∗ i in Eq. (8) can be rewritten as w∗ i = Xi XT i Xi + λiniIi −1 qi, (9) where Ii is an ni × ni identity matrix.",
                "Proof.",
                "Since w∗ i = XiXT i + λiniI −1 Xiqi, then XiXT i + λiniI w∗ i = Xiqi =⇒ XiXT i w∗ i + λiniw∗ i = Xiqi =⇒ w∗ i = (λini)−1 Xi qi − XT i w∗ i .",
                "Let β = (λini)−1 qi − XT i w∗ i , then w∗ i = Xiβ =⇒ λiniβ = qi − XT i w∗ i = qi − XT i Xiβ =⇒ qi = XT i Xi + λiniIi β =⇒ β = XT i Xi + λiniIi −1 qi.",
                "Therefore w∗ i = Xiβ = Xi XT i Xi + λiniIi −1 qi 2 Using theorem 1, we only need to compute the inverse of an ni × ni matrix for every document to train a local label predictor.",
                "Moreover, for a new testing point u that falls into Ni, we can classify it by the sign of qu = w∗T i u = uT wi = uT Xi XT i Xi + λiniIi −1 qi.",
                "This is an attractive expression since we can determine the cluster assignment of u by using the inner-products between the points in {u ∪ Ni}, which suggests that such a local regularizer can easily be kernelized [21] as long as we define a proper kernel function. 2.2.3 Combining the Local Regularized Predictors After all the local predictors having been constructed, we will combine them together by minimizing Jl = n i=1 w∗T i xi − qi 2 , (10) which stands for the sum of the prediction errors for all the local predictors.",
                "Combining Eq. (10) with Eq. (6), we can get Jl = n i=1 w∗T i xi − qi 2 = n i=1 xT i Xi XT i Xi + λiniIi −1 qi − qi 2 = Pq − q 2 , (11) where q = [q1, q2, · · · , qn]T , and the P is an n × n matrix constructing in the following way.",
                "Let αi = xT i Xi XT i Xi + λiniIi −1 , then Pij = αi j, if xj ∈ Ni 0, otherwise , (12) where Pij is the (i, j)-th entry of P, and αi j represents the j-th entry of αi .",
                "Till now we can write the criterion of clustering by combining locally regularized linear label predictors Jl in an explicit mathematical form, and we can minimize it directly using some standard optimization techniques.",
                "However, the results may not be good enough since we only exploit the local informations of the dataset.",
                "In the next subsection, we will introduce a global regularization criterion and combine it with Jl, which aims to find a good clustering result in a local-global way. 2.3 Global Regularization In data clustering, we usually require that the cluster assignments of the data points should be sufficiently smooth with respect to the underlying data manifold, which implies (1) the nearby points tend to have the same cluster assignments; (2) the points on the same structure (e.g. submanifold or cluster) tend to have the same cluster assignments [31].",
                "Without the loss of generality, we assume that the data points reside (roughly) on a low-dimensional manifold M2 , and q is the cluster assignment function defined on M, i.e. 2 We believe that the text data are also sampled from some low dimensional manifold, since it is impossible for them to for ∀x ∈ M, q(x) returns the cluster membership of x.",
                "The smoothness of q over M can be calculated by the following Dirichlet integral [2] D[q] = 1 2 M q(x) 2 dM, (13) where the gradient q is a vector in the tangent space T Mx, and the integral is taken with respect to the standard measure on M. If we restrict the scale of q by q, q M = 1 (where ·, · M is the inner product induced on M), then it turns out that finding the smoothest function minimizing D[q] reduces to finding the eigenfunctions of the Laplace Beltrami operator L, which is defined as Lq −div q, (14) where div is the divergence of a vector field.",
                "Generally, the graph can be viewed as the discretized form of manifold.",
                "We can model the dataset as an weighted undirected graph as in spectral clustering [22], where the graph nodes are just the data points, and the weights on the edges represent the similarities between pairwise points.",
                "Then it can be shown that minimizing Eq. (13) corresponds to minimizing Jg = qT Lq = n i=1 (qi − qj)2 wij, (15) where q = [q1, q2, · · · , qn]T with qi = q(xi), L is the graph Laplacian with its (i, j)-th entry Lij =    di − wii, if i = j −wij, if xi and xj are adjacent 0, otherwise, (16) where di = j wij is the degree of xi, wij is the similarity between xi and xj.",
                "If xi and xj are adjacent3 , wij is usually computed in the following way wij = e − xi−xj 2 2σ2 , (17) where σ is a dataset dependent parameter.",
                "It is proved that under certain conditions, such a form of wij to determine the weights on graph edges leads to the convergence of graph Laplacian to the Laplace Beltrami operator [3][18].",
                "In summary, using Eq. (15) with exponential weights can effectively measure the smoothness of the data assignments with respect to the intrinsic data manifold.",
                "Thus we adopt it as a global regularizer to punish the smoothness of the predicted data assignments. 2.4 Clustering with Local and Global Regularization Combining the contents we have introduced in section 2.2 and section 2.3 we can derive the clustering criterion is minq J = Jl + λJg = Pq − q 2 + λqT Lq s.t. qi ∈ {−1, +1}, (18) where P is defined as in Eq. (12), and λ is a regularization parameter to trade off Jl and Jg.",
                "However, the discrete fill in the whole high-dimensional sample space.",
                "And it has been shown that the manifold based methods can achieve good results on text classification tasks [31]. 3 In this paper, we define xi and xj to be adjacent if xi ∈ N(xj) or xj ∈ N(xi). constraint of pi makes the problem an NP hard integer programming problem.",
                "A natural way for making the problem solvable is to remove the constraint and relax qi to be continuous, then the objective that we aims to minimize becomes J = Pq − q 2 + λqT Lq = qT (P − I)T (P − I)q + λqT Lq = qT (P − I)T (P − I) + λL q, (19) and we further add a constraint qT q = 1 to restrict the scale of q.",
                "Then our objective becomes minq J = qT (P − I)T (P − I) + λL q s.t. qT q = 1 (20) Using the Lagrangian method, we can derive that the optimal solution q corresponds to the smallest eigenvector of the matrix M = (P − I)T (P − I) + λL, and the cluster assignment of xi can be determined by the sign of qi, i.e. xi will be classified as class one if qi > 0, otherwise it will be classified as class 2. 2.5 Multi-Class CLGR In the above we have introduced the basic framework of Clustering with Local and Global Regularization (CLGR) for the two-class clustering problem, and we will extending it to multi-class clustering in this subsection.",
                "First we assume that all the documents belong to C classes indexed by L = {1, 2, · · · , C}. qc is the classification function for class c (1 c C), such that qc (xi) returns the confidence that xi belongs to class c. Our goal is to obtain the value of qc (xi) (1 c C, 1 i n), and the cluster assignment of xi can be determined by {qc (xi)}C c=1 using some proper discretization methods that we will introduce later.",
                "Therefore, in this multi-class case, for each document xi (1 i n), we will construct C locally linear regularized label predictors whose normal vectors are wc∗ i = Xi XT i Xi + λiniIi −1 qc i (1 c C), (21) where Xi = [xi1, xi2, · · · , xini ] with xik being the k-th neighbor of xi, and qc i = [qc i1, qc i2, · · · , qc ini ]T with qc ik = qc (xik).",
                "Then (wc∗ i )T xi returns the predicted confidence of xi belonging to class c. Hence the local prediction error for class c can be defined as J c l = n i=1 (wc∗ i ) T xi − qc i 2 , (22) And the total local prediction error becomes Jl = C c=1 J c l = C c=1 n i=1 (wc∗ i ) T xi − qc i 2 . (23) As in Eq. (11), we can define an n×n matrix P (see Eq. (12)) and rewrite Jl as Jl = C c=1 J c l = C c=1 Pqc − qc 2 . (24) Similarly we can define the global smoothness regularizer in multi-class case as Jg = C c=1 n i=1 (qc i − qc j )2 wij = C c=1 (qc )T Lqc . (25) Then the criterion to be minimized for CLGR in multi-class case becomes J = Jl + λJg = C c=1 Pqc − qc 2 + λ(qc )T Lqc = C c=1 (qc )T (P − I)T (P − I) + λL qc = trace QT (P − I)T (P − I) + λL Q , (26) where Q = [q1 , q2 , · · · , qc ] is an n × c matrix, and trace(·) returns the trace of a matrix.",
                "The same as in Eq. (20), we also add the constraint that QT Q = I to restrict the scale of Q.",
                "Then our optimization problem becomes minQ J = trace QT (P − I)T (P − I) + λL Q s.t.",
                "QT Q = I, (27) From the Ky Fan theorem [28], we know the optimal solution of the above problem is Q∗ = [q∗ 1, q∗ 2, · · · , q∗ C ]R, (28) where q∗ k (1 k C) is the eigenvector corresponds to the k-th smallest eigenvalue of matrix (P − I)T (P − I) + λL, and R is an arbitrary C × C matrix.",
                "Since the values of the entries in Q∗ is continuous, we need to further discretize Q∗ to get the cluster assignments of all the data points.",
                "There are mainly two approaches to achieve this goal: 1.",
                "As in [20], we can treat the i-th row of Q as the embedding of xi in a C-dimensional space, and apply some traditional clustering methods like kmeans to clustering these embeddings into C clusters. 2.",
                "Since the optimal Q∗ is not unique (because of the existence of an arbitrary matrix R), we can pursue an optimal R that will rotate Q∗ to an indication matrix4 .",
                "The detailed algorithm can be referred to [26].",
                "The detailed algorithm procedure for CLGR is summarized in table 1. 3.",
                "EXPERIMENTS In this section, experiments are conducted to empirically compare the clustering results of CLGR with other 8 representitive document clustering algorithms on 5 datasets.",
                "First we will introduce the basic informations of those datasets. 3.1 Datasets We use a variety of datasets, most of which are frequently used in the information retrieval research.",
                "Table 2 summarizes the characteristics of the datasets. 4 Here an indication matrix T is a n×c matrix with its (i, j)th entry Tij ∈ {0, 1} such that for each row of Q∗ there is only one 1.",
                "Then the xi can be assigned to the j-th cluster such that j = argjQ∗ ij = 1.",
                "Table 1: Clustering with Local and Global Regularization (CLGR) Input: 1.",
                "Dataset X = {xi}n i=1; 2.",
                "Number of clusters C; 3.",
                "Size of the neighborhood K; 4.",
                "Local regularization parameters {λi}n i=1; 5.",
                "Global regularization parameter λ; Output: The cluster membership of each data point.",
                "Procedure: 1.",
                "Construct the K nearest neighborhoods for each data point; 2.",
                "Construct the matrix P using Eq. (12); 3.",
                "Construct the Laplacian matrix L using Eq. (16); 4.",
                "Construct the matrix M = (P − I)T (P − I) + λL; 5.",
                "Do eigenvalue decomposition on M, and construct the matrix Q∗ according to Eq. (28); 6.",
                "Output the cluster assignments of each data point by properly discretize Q∗ .",
                "Table 2: Descriptions of the document datasets Datasets Number of documents Number of classes CSTR 476 4 WebKB4 4199 4 Reuters 2900 10 WebACE 2340 20 Newsgroup4 3970 4 CSTR.",
                "This is the dataset of the abstracts of technical reports published in the Department of Computer Science at a university.",
                "The dataset contained 476 abstracts, which were divided into four research areas: Natural Language Processing(NLP), Robotics/Vision, Systems, and Theory.",
                "WebKB.",
                "The WebKB dataset contains webpages gathered from university computer science departments.",
                "There are about 8280 documents and they are divided into 7 categories: student, faculty, staff, course, project, department and other.",
                "The raw text is about 27MB.",
                "Among these 7 categories, student, faculty, course and project are four most populous entity-representing categories.",
                "The associated subset is typically called WebKB4.",
                "Reuters.",
                "The Reuters-21578 Text Categorization Test collection contains documents collected from the Reuters newswire in 1987.",
                "It is a standard text categorization benchmark and contains 135 categories.",
                "In our experiments, we use a subset of the data collection which includes the 10 most frequent categories among the 135 topics and we call it Reuters-top 10.",
                "WebACE.",
                "The WebACE dataset was from WebACE project and has been used for document clustering [17][5].",
                "The WebACE dataset contains 2340 documents consisting news articles from Reuters new service via the Web in October 1997.",
                "These documents are divided into 20 classes.",
                "News4.",
                "The News4 dataset used in our experiments are selected from the famous 20-newsgroups dataset5 .",
                "The topic rec containing autos, motorcycles, baseball and hockey was selected from the version 20news-18828.",
                "The News4 dataset contains 3970 document vectors. 5 http://people.csail.mit.edu/jrennie/20Newsgroups/ To pre-process the datasets, we remove the stop words using a standard stop list, all HTML tags are skipped and all header fields except subject and organization of the posted articles are ignored.",
                "In all our experiments, we first select the top 1000 words by mutual information with class labels. 3.2 Evaluation Metrics In the experiments, we set the number of clusters equal to the true number of classes C for all the clustering algorithms.",
                "To evaluate their performance, we compare the clusters generated by these algorithms with the true classes by computing the following two performance measures.",
                "Clustering Accuracy (Acc).",
                "The first performance measure is the Clustering Accuracy, which discovers the one-toone relationship between clusters and classes and measures the extent to which each cluster contained data points from the corresponding class.",
                "It sums up the whole matching degree between all pair class-clusters.",
                "Clustering accuracy can be computed as: Acc = 1 N max   Ck,Lm T(Ck, Lm)   , (29) where Ck denotes the k-th cluster in the final results, and Lm is the true m-th class.",
                "T(Ck, Lm) is the number of entities which belong to class m are assigned to cluster k. Accuracy computes the maximum sum of T(Ck, Lm) for all pairs of clusters and classes, and these pairs have no overlaps.",
                "The greater clustering accuracy means the better clustering performance.",
                "Normalized Mutual Information (NMI).",
                "Another evaluation metric we adopt here is the Normalized Mutual Information NMI [23], which is widely used for determining the quality of clusters.",
                "For two random variable X and Y, the NMI is defined as: NMI(X, Y) = I(X, Y) H(X)H(Y) , (30) where I(X, Y) is the mutual information between X and Y, while H(X) and H(Y) are the entropies of X and Y respectively.",
                "One can see that NMI(X, X) = 1, which is the maximal possible value of NMI.",
                "Given a clustering result, the NMI in Eq. (30) is estimated as NMI = C k=1 C m=1 nk,mlog n·nk,m nk ˆnm C k=1 nklog nk n C m=1 ˆnmlog ˆnm n , (31) where nk denotes the number of data contained in the cluster Ck (1 k C), ˆnm is the number of data belonging to the m-th class (1 m C), and nk,m denotes the number of data that are in the intersection between the cluster Ck and the m-th class.",
                "The value calculated in Eq. (31) is used as a performance measure for the given clustering result.",
                "The larger this value, the better the clustering performance. 3.3 Comparisons We have conducted comprehensive performance evaluations by testing our method and comparing it with 8 other representative data clustering methods using the same data corpora.",
                "The algorithms that we evaluated are listed below. 1.",
                "Traditional k-means (KM). 2.",
                "Spherical k-means (SKM).",
                "The implementation is based on [9]. 3.",
                "Gaussian Mixture Model (GMM).",
                "The implementation is based on [16]. 4.",
                "Spectral Clustering with Normalized Cuts (Ncut).",
                "The implementation is based on [26], and the variance of the Gaussian similarity is determined by Local Scaling [30].",
                "Note that the criterion that Ncut aims to minimize is just the global regularizer in our CLGR algorithm except that Ncut used the normalized Laplacian. 5.",
                "Clustering using Pure Local Regularization (CPLR).",
                "In this method we just minimize Jl (defined in Eq. (24)), and the clustering results can be obtained by doing eigenvalue decomposition on matrix (I − P)T (I − P) with some proper discretization methods. 6.",
                "Adaptive Subspace Iteration (ASI).",
                "The implementation is based on [14]. 7.",
                "Nonnegative Matrix Factorization (NMF).",
                "The implementation is based on [27]. 8.",
                "Tri-Factorization Nonnegative Matrix Factorization (TNMF) [12].",
                "The implementation is based on [15].",
                "For computational efficiency, in the implementation of CPLR and our CLGR algorithm, we have set all the local regularization parameters {λi}n i=1 to be identical, which is set by grid search from {0.1, 1, 10}.",
                "The size of the k-nearest neighborhoods is set by grid search from {20, 40, 80}.",
                "For the CLGR method, its global regularization parameter is set by grid search from {0.1, 1, 10}.",
                "When constructing the global regularizer, we have adopted the local scaling method [30] to construct the Laplacian matrix.",
                "The final discretization method adopted in these two methods is the same as in [26], since our experiments show that using such method can achieve better results than using kmeans based methods as in [20]. 3.4 Experimental Results The clustering accuracies comparison results are shown in table 3, and the normalized mutual information comparison results are summarized in table 4.",
                "From the two tables we mainly observe that: 1.",
                "Our CLGR method outperforms all other document clustering methods in most of the datasets; 2.",
                "For document clustering, the Spherical k-means method usually outperforms the traditional k-means clustering method, and the GMM method can achieve competitive results compared to the Spherical k-means method; 3.",
                "The results achieved from the k-means and GMM type algorithms are usually worse than the results achieved from Spectral Clustering.",
                "Since Spectral Clustering can be viewed as a weighted version of kernel k-means, it can obtain good results the data clusters are arbitrarily shaped.",
                "This corroborates that the documents vectors are not regularly distributed (spherical or elliptical). 4.",
                "The experimental comparisons empirically verify the equivalence between NMF and Spectral Clustering, which Table 3: Clustering accuracies of the various methods CSTR WebKB4 Reuters WebACE News4 KM 0.4256 0.3888 0.4448 0.4001 0.3527 SKM 0.4690 0.4318 0.5025 0.4458 0.3912 GMM 0.4487 0.4271 0.4897 0.4521 0.3844 NMF 0.5713 0.4418 0.4947 0.4761 0.4213 Ncut 0.5435 0.4521 0.4896 0.4513 0.4189 ASI 0.5621 0.4752 0.5235 0.4823 0.4335 TNMF 0.6040 0.4832 0.5541 0.5102 0.4613 CPLR 0.5974 0.5020 0.4832 0.5213 0.4890 CLGR 0.6235 0.5228 0.5341 0.5376 0.5102 Table 4: Normalized mutual information results of the various methods CSTR WebKB4 Reuters WebACE News4 KM 0.3675 0.3023 0.4012 0.3864 0.3318 SKM 0.4027 0.4155 0.4587 0.4003 0.4085 GMM 0.4034 0.4093 0.4356 0.4209 0.3994 NMF 0.5235 0.4517 0.4402 0.4359 0.4130 Ncut 0.4833 0.4497 0.4392 0.4289 0.4231 ASI 0.5008 0.4833 0.4769 0.4817 0.4503 TNMF 0.5724 0.5011 0.5132 0.5328 0.4749 CPLR 0.5695 0.5231 0.4402 0.5543 0.4690 CLGR 0.6012 0.5434 0.4935 0.5390 0.4908 has been proved theoretically in [10].",
                "It can be observed from the tables that NMF and Spectral Clustering usually lead to similar clustering results. 5.",
                "The co-clustering based methods (TNMF and ASI) can usually achieve better results than traditional purely document vector based methods.",
                "Since these methods perform an implicit feature selection at each iteration, provide an adaptive metric for measuring the neighborhood, and thus tend to yield better clustering results. 6.",
                "The results achieved from CPLR are usually better than the results achieved from Spectral Clustering, which supports Vapniks theory [24] that sometimes local learning algorithms can obtain better results than global learning algorithms.",
                "Besides the above comparison experiments, we also test the parameter sensibility of our method.",
                "There are mainly two sets of parameters in our CLGR algorithm, the local and global regularization parameters ({λi}n i=1 and λ, as we have said in section 3.3, we have set all λis to be identical to λ∗ in our experiments), and the size of the neighborhoods.",
                "Therefore we have also done two sets of experiments: 1.",
                "Fixing the size of the neighborhoods, and testing the clustering performance with varying λ∗ and λ.",
                "In this set of experiments, we find that our CLGR algorithm can achieve good results when the two regularization parameters are neither too large nor too small.",
                "Typically our method can achieve good results when λ∗ and λ are around 0.1.",
                "Figure 1 shows us such a testing example on the WebACE dataset. 2.",
                "Fixing the local and global regularization parameters, and testing the clustering performance with different −5 −4.5 −4 −3.5 −3 −5 −4.5 −4 −3.5 −3 0.35 0.4 0.45 0.5 0.55 local regularization para (log 2 value) global regularization para (log 2 value) clusteringaccuracy Figure 1: Parameter sensibility testing results on the WebACE dataset with the neighborhood size fixed to 20, and the x-axis and y-axis represents the log2 value of λ∗ and λ. sizes of neighborhoods.",
                "In this set of experiments, we find that the neighborhood with a too large or too small size will all deteriorate the final clustering results.",
                "This can be easily understood since when the neighborhood size is very small, then the data points used for training the local classifiers may not be sufficient; when the neighborhood size is very large, the trained classifiers will tend to be global and cannot capture the typical local characteristics.",
                "Figure 2 shows us a testing example on the WebACE dataset.",
                "Therefore, we can see that our CLGR algorithm (1) can achieve satisfactory results and (2) is not very sensitive to the choice of parameters, which makes it practical in real world applications. 4.",
                "CONCLUSIONS AND FUTURE WORKS In this paper, we derived a new clustering algorithm called clustering with local and global regularization.",
                "Our method preserves the merit of local learning algorithms and spectral clustering.",
                "Our experiments show that the proposed algorithm outperforms most of the state of the art algorithms on many benchmark datasets.",
                "In the future, we will focus on the parameter selection and acceleration issues of the CLGR algorithm. 5.",
                "REFERENCES [1] L. Baker and A. McCallum.",
                "Distributional Clustering of Words for Text Classification.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 1998. [2] M. Belkin and P. Niyogi.",
                "Laplacian Eigenmaps for Dimensionality Reduction and Data Representation.",
                "Neural Computation, 15 (6):1373-1396.",
                "June 2003. [3] M. Belkin and P. Niyogi.",
                "Towards a Theoretical Foundation for Laplacian-Based Manifold Methods.",
                "In Proceedings of the 18th Conference on Learning Theory (COLT). 2005. 10 20 30 40 50 60 70 80 90 100 0.35 0.4 0.45 0.5 0.55 size of the neighborhood clusteringaccuracy Figure 2: Parameter sensibility testing results on the WebACE dataset with the regularization parameters being fixed to 0.1, and the neighborhood size varing from 10 to 100. [4] M. Belkin, P. Niyogi and V. Sindhwani.",
                "Manifold Regularization: a Geometric Framework for Learning from Examples.",
                "Journal of Machine Learning Research 7, 1-48, 2006. [5] D. Boley.",
                "Principal Direction Divisive Partitioning.",
                "Data mining and knowledge discovery, 2:325-344, 1998. [6] L. Bottou and V. Vapnik.",
                "Local learning algorithms.",
                "Neural Computation, 4:888-900, 1992. [7] P. K. Chan, D. F. Schlag and J. Y. Zien.",
                "Spectral K-way Ratio-Cut Partitioning and Clustering.",
                "IEEE Trans.",
                "Computer-Aided Design, 13:1088-1096, Sep. 1994. [8] D. R. Cutting, D. R. Karger, J. O. Pederson and J. W. Tukey.",
                "Scatter/Gather: A Cluster-Based Approach to Browsing Large Document Collections.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 1992. [9] I. S. Dhillon and D. S. Modha.",
                "Concept Decompositions for Large Sparse Text Data using Clustering.",
                "Machine Learning, vol. 42(1), pages 143-175, January 2001. [10] C. Ding, X.",
                "He, and H. Simon.",
                "On the equivalence of nonnegative matrix factorization and spectral clustering.",
                "In Proceedings of the SIAM Data Mining Conference, 2005. [11] C. Ding, X.",
                "He, H. Zha, M. Gu, and H. D. Simon.",
                "A min-max cut algorithm for graph partitioning and data clustering.",
                "In Proc. of the 1st International Conference on Data Mining (ICDM), pages 107-114, 2001. [12] C. Ding, T. Li, W. Peng, and H. Park.",
                "Orthogonal Nonnegative Matrix Tri-Factorizations for Clustering.",
                "In Proceedings of the Twelfth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2006. [13] R. O. Duda, P. E. Hart, and D. G. Stork.",
                "Pattern Classification.",
                "John Wiley & Sons, Inc., 2001. [14] T. Li, S. Ma, and M. Ogihara.",
                "Document Clustering via Adaptive Subspace Iteration.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2004. [15] T. Li and C. Ding.",
                "The Relationships Among Various Nonnegative Matrix Factorization Methods for Clustering.",
                "In Proceedings of the 6th International Conference on Data Mining (ICDM). 2006. [16] X. Liu and Y. Gong.",
                "Document Clustering with Cluster Refinement and Model Selection Capabilities.",
                "In Proc. of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002. [17] E. Han, D. Boley, M. Gini, R. Gross, K. Hastings, G. Karypis, V. Kumar, B. Mobasher, and J. Moore.",
                "WebACE: A Web Agent for Document Categorization and Exploration.",
                "In Proceedings of the 2nd International Conference on Autonomous Agents (Agents98).",
                "ACM Press, 1998. [18] M. Hein, J. Y. Audibert, and U. von Luxburg.",
                "From Graphs to Manifolds - Weak and Strong Pointwise Consistency of Graph Laplacians.",
                "In Proceedings of the 18th Conference on Learning Theory (COLT), 470-485. 2005. [19] J.",
                "He, M. Lan, C.-L. Tan, S.-Y.",
                "Sung, and H.-B.",
                "Low.",
                "Initialization of Cluster Refinement Algorithms: A Review and Comparative Study.",
                "In Proc. of Inter.",
                "Joint Conference on Neural Networks, 2004. [20] A. Y. Ng, M. I. Jordan, Y. Weiss.",
                "On Spectral Clustering: Analysis and an algorithm.",
                "In Advances in Neural Information Processing Systems 14. 2002. [21] B. Sch¨olkopf and A. Smola.",
                "Learning with Kernels.",
                "The MIT Press.",
                "Cambridge, Massachusetts. 2002. [22] J. Shi and J. Malik.",
                "Normalized Cuts and Image Segmentation.",
                "IEEE Trans. on Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000. [23] A. Strehl and J. Ghosh.",
                "Cluster Ensembles - A Knowledge Reuse Framework for Combining Multiple Partitions.",
                "Journal of Machine Learning Research, 3:583-617, 2002. [24] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Berlin: Springer-Verlag, 1995. [25] Wu, M. and Sch¨olkopf, B.",
                "A Local Learning Approach for Clustering.",
                "In Advances in Neural Information Processing Systems 18. 2006. [26] S. X. Yu, J. Shi.",
                "Multiclass Spectral Clustering.",
                "In Proceedings of the International Conference on Computer Vision, 2003. [27] W. Xu, X. Liu and Y. Gong.",
                "Document Clustering Based On Non-Negative Matrix Factorization.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2003. [28] H. Zha, X.",
                "He, C. Ding, M. Gu and H. Simon.",
                "Spectral Relaxation for K-means Clustering.",
                "In NIPS 14. 2001. [29] T. Zhang and F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Journal of Information Retrieval, 4:5-31, 2001. [30] L. Zelnik-Manor and P. Perona.",
                "Self-Tuning Spectral Clustering.",
                "In NIPS 17. 2005. [31] D. Zhou, O. Bousquet, T. N. Lal, J. Weston and B. Sch¨olkopf.",
                "Learning with Local and Global Consistency.",
                "NIPS 17, 2005."
            ],
            "original_annotated_samples": [
                "For instance, K-means [13] is a typical <br>partitioning method</br> which aims to minimize the sum of the squared distance between the data points and their corresponding cluster centers."
            ],
            "translated_annotated_samples": [
                "Por ejemplo, K-means es un <br>método de particionamiento</br> típico que tiene como objetivo minimizar la suma de las distancias al cuadrado entre los puntos de datos y sus centros de clúster correspondientes."
            ],
            "translated_text": "Agrupamiento regularizado para documentos ∗ Fei Wang, Changshui Zhang Laboratorio Clave del Estado de Tecnología Inteligente. En los últimos años, la agrupación de documentos ha estado recibiendo cada vez más atención como una técnica importante y fundamental para la organización no supervisada de documentos, la extracción automática de temas y la recuperación o filtrado rápido de información. En este artículo, proponemos un método novedoso para agrupar documentos utilizando regularización. A diferencia de los métodos de agrupamiento regularizados globalmente tradicionales, nuestro método primero construye un predictor de etiquetas lineal regularizado local para cada vector de documento, y luego combina todos esos regularizadores locales con un regularizador de suavidad global. Así que llamamos a nuestro algoritmo Agrupamiento con Regularización Local y Global (CLGR). Mostraremos que las pertenencias de los documentos a los grupos se pueden lograr mediante la descomposición de los valores propios de una matriz simétrica dispersa, la cual puede resolverse eficientemente mediante métodos iterativos. Finalmente, se presentan nuestras evaluaciones experimentales en varios conjuntos de datos para mostrar las ventajas de CLGR sobre los métodos tradicionales de agrupamiento de documentos. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información-Agrupamiento; I.2.6 [Inteligencia Artificial]: Aprendizaje-Aprendizaje de Conceptos Términos Generales Algoritmos 1. La agrupación de documentos ha estado recibiendo cada vez más atención como una técnica importante y fundamental para la organización no supervisada de documentos, la extracción automática de temas y la recuperación o filtrado rápido de información. Un buen enfoque de agrupación de documentos puede ayudar a las computadoras a organizar automáticamente el corpus de documentos en una jerarquía de clusters significativa para una navegación eficiente, lo cual es muy valioso para complementar las deficiencias de las tecnologías tradicionales de recuperación de información. Como señaló [8], las necesidades de recuperación de información pueden expresarse mediante un espectro que va desde la búsqueda basada en palabras clave estrechas hasta la exploración de información amplia, como cuáles son los principales eventos internacionales de los últimos meses. Los motores de búsqueda de documentos tradicionales tienden a adaptarse bien al extremo de la búsqueda, es decir, suelen proporcionar búsquedas específicas de documentos que coinciden con la consulta de los usuarios, sin embargo, les resulta difícil satisfacer las necesidades del resto del espectro en el que se necesita información más amplia o vaga. En tales casos, la navegación eficiente a través de una buena jerarquía de clústeres será definitivamente útil. Generalmente, los métodos de agrupamiento de documentos se pueden clasificar principalmente en dos clases: métodos jerárquicos y métodos de partición. Los métodos jerárquicos agrupan los puntos de datos en una estructura de árbol jerárquico utilizando enfoques de abajo hacia arriba o de arriba hacia abajo. Por ejemplo, el clustering jerárquico aglomerativo (HAC) [13] es un método típico de clustering jerárquico ascendente. Toma cada punto de datos como un único clúster para comenzar y luego construye clústeres más grandes agrupando puntos de datos similares juntos hasta que todo el conjunto de datos esté encapsulado en un único clúster final. Por otro lado, los métodos de particionamiento descomponen el conjunto de datos en un número de grupos disjuntos que suelen ser óptimos en términos de algunas funciones de criterio predefinidas. Por ejemplo, K-means es un <br>método de particionamiento</br> típico que tiene como objetivo minimizar la suma de las distancias al cuadrado entre los puntos de datos y sus centros de clúster correspondientes. En este documento, nos centraremos en los métodos de particionamiento. Como sabemos, existen dos problemas principales en los métodos de particionamiento (como Kmeans y el Modelo de Mezcla Gaussiana (GMM) [16]): (1) el criterio predefinido suele ser no convexo, lo que provoca muchas soluciones óptimas locales; (2) el procedimiento iterativo (por ejemplo, el algoritmo de Expectation Maximization (EM)) para optimizar los criterios suele hacer que las soluciones finales dependan en gran medida de las inicializaciones. En las últimas décadas, se han propuesto muchos métodos para superar los problemas mencionados de los métodos de particionamiento [19][28]. Recientemente, otro tipo de métodos de particionamiento basados en agrupamiento en grafos de datos han despertado un considerable interés en la comunidad de aprendizaje automático y minería de datos. La idea básica detrás de estos métodos es modelar primero el conjunto de datos completo como un grafo ponderado, en el que los nodos del grafo representan los puntos de datos y los pesos en los bordes corresponden a las similitudes entre los puntos en pares. Entonces, las asignaciones de clústeres del conjunto de datos se pueden lograr optimizando algunos criterios definidos en el gráfico. Por ejemplo, el Clustering Espectral es uno de los enfoques de clustering basados en grafos más representativos, generalmente tiene como objetivo optimizar algún valor de corte (por ejemplo, Corte normalizado [22], corte de razón [7], corte mínimo-máximo [11]) definidos en un grafo no dirigido. Después de algunas relajaciones, estos criterios generalmente pueden ser optimizados a través de descomposiciones propias, lo cual está garantizado que sea óptimo a nivel global. De esta manera, el agrupamiento espectral evita eficientemente los problemas de los métodos de particionamiento tradicionales que presentamos en el párrafo anterior. En este artículo, proponemos un algoritmo novedoso de agrupamiento de documentos que hereda la superioridad del agrupamiento espectral, es decir, los resultados finales de los grupos también pueden obtenerse explotando la estructura de los autovalores de una matriz simétrica. Sin embargo, a diferencia del agrupamiento espectral, que simplemente impone una restricción de suavidad en las etiquetas de los datos sobre todo el conjunto de datos [2], nuestro método primero construye un predictor de etiquetas lineal regularizado para cada punto de datos a partir de su vecindario como en [25], y luego combina los resultados de todos estos predictores de etiquetas locales con un regularizador de suavidad de etiquetas global. Así que llamamos a nuestro método Agrupamiento con Regularización Local y Global (CLGR). La idea de incorporar tanto información local como global en la predicción de etiquetas está inspirada en los trabajos recientes sobre aprendizaje semi-supervisado [31], y nuestras evaluaciones experimentales en varios conjuntos de datos reales de documentos muestran que CLGR funciona mejor que muchos métodos de agrupamiento de vanguardia. El resto de este documento está organizado de la siguiente manera: en la sección 2 presentaremos nuestro algoritmo CLGR en detalle. Los resultados experimentales en varios conjuntos de datos se presentan en la sección 3, seguidos de las conclusiones y discusiones en la sección 4. El ALGORITMO PROPUESTO En esta sección, presentaremos en detalle nuestro algoritmo de Agrupamiento con Regularización Local y Global (CLGR). Primero veamos cómo se representan los documentos a lo largo de este documento. 2.1 Representación de documentos En nuestro trabajo, todos los documentos se representan mediante vectores de frecuencia de términos ponderados. Sea W = {w1, w2, · · · , wm} el conjunto completo de vocabulario del corpus de documentos (que ha sido preprocesado mediante la eliminación de palabras vacías y operaciones de derivación de palabras). El vector de frecuencia de término xi del documento di se define como xi = [xi1, xi2, · · · , xim]T , xik = tik log n idfk , donde tik es la frecuencia del término wk ∈ W, n es el tamaño del corpus de documentos, idfk es el número de documentos que contienen la palabra wk. De esta manera, xi también se llama la representación TFIDF del documento di. Además, también normalizamos cada xi (1 i n) para que tenga una longitud unitaria, de modo que cada documento esté representado por un vector TF-IDF normalizado. 2.2 Regularización Local Como su nombre sugiere, CLGR está compuesto por dos partes: regularización local y regularización global. En esta subsección introduciremos detalladamente la parte de regularización local. 2.2.1 Motivación Como sabemos que el agrupamiento es un tipo de técnica de aprendizaje, tiene como objetivo organizar el conjunto de datos de una manera razonable. En general, el aprendizaje puede plantearse como un problema de estimación de funciones, a partir del cual podemos obtener una buena función de clasificación que asignará etiquetas al conjunto de datos de entrenamiento e incluso al conjunto de datos de prueba no vistos con algún costo minimizado [24]. Por ejemplo, en el escenario de clasificación de dos clases (en el que conocemos exactamente la etiqueta de cada documento), un clasificador lineal con ajuste de mínimos cuadrados tiene como objetivo aprender un vector columna w de manera que el costo al cuadrado J = 1 n (wT xi − yi)2 (1) se minimice, donde yi ∈ {+1, −1} es la etiqueta de xi. Al tomar ∂J /∂w = 0, obtenemos la solución w∗ = Σ i=1 n xixT i −1 Σ i=1 n xiyi, (2) que puede escribirse en su forma matricial como w∗ = XXT −1 Xy, (3) donde X = [x1, x2, · · · , xn] es una matriz de documentos m × n, y = [y1, y2, · · · , yn]T es el vector de etiquetas. Entonces, para un documento de prueba t, podemos determinar su etiqueta mediante l = signo(w∗T u), donde signo(·) es la función signo. Un problema natural en la ecuación (3) es que la matriz XXT puede ser singular y, por lo tanto, no invertible (por ejemplo, cuando m n). Para evitar tal problema, podemos agregar un término de regularización y minimizar el siguiente criterio J = 1 n n i=1 (wT xi − yi)2 + λ w 2, (5), donde λ es un parámetro de regularización. Entonces, la solución óptima que minimiza J está dada por w∗ = XXT + λnI −1 Xy, (6) donde I es una matriz identidad de tamaño m × m. Se ha informado que el clasificador lineal regularizado puede lograr resultados muy buenos en problemas de clasificación de texto [29]. Sin embargo, a pesar de su éxito empírico, el clasificador lineal regularizado es en realidad un clasificador global, es decir, w∗ se estima utilizando todo el conjunto de entrenamiento. Según [24], esta puede que no sea una idea inteligente, ya que un único w∗ puede que no sea lo suficientemente bueno para predecir las etiquetas de todo el espacio de entrada. Para obtener predicciones más precisas, [6] propuso entrenar clasificadores localmente y utilizarlos para clasificar los puntos de prueba. Por ejemplo, un punto de prueba será clasificado por el clasificador local entrenado utilizando los puntos de entrenamiento ubicados en la vecindad 1. En las siguientes discusiones todos asumimos que los documentos provienen solo de dos clases. Las generalizaciones de nuestro método a casos de múltiples clases se discutirán en la sección 2.5 de él. Aunque este método parece lento y estúpido, se informa que puede obtener mejores rendimientos que usar un clasificador global único en ciertas tareas [6]. 2.2.2 Construyendo los Predictores Localmente Regularizados Inspirados en su éxito, propusimos aplicar los algoritmos de aprendizaje local para el agrupamiento. La idea básica es que, para cada vector de documento xi (1 i n), entrenamos un predictor de etiquetas local basado en su vecindario k-más cercano Ni, y luego lo usamos para predecir la etiqueta de xi. Finalmente combinaremos todos esos predictores locales minimizando la suma de sus errores de predicción. En esta subsección introduciremos cómo construir esos predictores locales. Debido a la simplicidad y efectividad del clasificador lineal regularizado que hemos introducido en la sección 2.2.1, lo elegimos como nuestro predictor de etiquetas local, de modo que para cada documento xi, se minimiza el siguiente criterio Ji = 1 ni xj ∈Ni wT i xj − qj 2 + λi wi 2, (7), donde ni = |Ni| es la cardinalidad de Ni, y qj es la pertenencia al clúster de xj. Luego, utilizando la Ecuación (6), podemos obtener que la solución óptima es w∗ i = XiXT i + λiniI −1 Xiqi, (8), donde Xi = [xi1, xi2, · · · , xini], y usamos xik para denotar al k-ésimo vecino más cercano de xi. qi = [qi1, qi2, · · · , qini]T con qik representando la asignación de clúster de xik. El problema aquí es que XiXT i es una matriz m × m con m ni, es decir, deberíamos calcular la inversa de una matriz m × m para cada vector de documento, lo cual está prohibido computacionalmente. Afortunadamente, tenemos el siguiente teorema: Teorema 1. w∗ i en la Ec. (8) puede ser reescrito como w∗ i = Xi XT i Xi + λiniIi −1 qi, (9), donde Ii es una matriz identidad de tamaño ni × ni. Prueba. Dado que w∗ i = XiXT i + λiniI −1 Xiqi, entonces XiXT i + λiniI w∗ i = Xiqi =⇒ XiXT i w∗ i + λiniw∗ i = Xiqi =⇒ w∗ i = (λini)−1 Xi qi − XT i w∗ i. Sea β = (λini)−1 qi − XT i w∗ i, entonces w∗ i = Xiβ =⇒ λiniβ = qi − XT i w∗ i = qi − XT i Xiβ =⇒ qi = XT i Xi + λiniIi β =⇒ β = XT i Xi + λiniIi −1 qi. Por lo tanto, w∗ i = Xiβ = Xi XT i Xi + λiniIi −1 qi 2. Utilizando el teorema 1, solo necesitamos calcular la inversa de una matriz ni × ni para cada documento para entrenar un predictor de etiquetas local. Además, para un nuevo punto de prueba u que cae en Ni, podemos clasificarlo por el signo de qu = w∗T i u = uT wi = uT Xi XT i Xi + λiniIi −1 qi. Esta es una expresión atractiva ya que podemos determinar la asignación del clúster de u utilizando los productos internos entre los puntos en {u ∪ Ni}, lo que sugiere que un regularizador local de este tipo puede ser fácilmente kernelizado [21] siempre y cuando definamos una función de kernel adecuada. 2.2.3 Combinando los Predictores Regularizados Localmente Una vez que todos los predictores locales hayan sido construidos, los combinaremos minimizando Jl = n i=1 w∗T i xi − qi 2, (10), lo que representa la suma de los errores de predicción de todos los predictores locales. Combinando la Ecuación (10) con la Ecuación (6), podemos obtener Jl = n i=1 w∗T i xi − qi 2 = n i=1 xT i Xi XT i Xi + λiniIi −1 qi − qi 2 = Pq − q 2, (11), donde q = [q1, q2, · · · , qn]T, y P es una matriz n × n construida de la siguiente manera. Sea αi = xT i Xi XT i Xi + λiniIi −1 , entonces Pij = αi j, si xj ∈ Ni 0, de lo contrario , (12) donde Pij es la entrada (i, j)-ésima de P, y αi j representa la entrada j-ésima de αi. Hasta ahora podemos escribir el criterio de agrupamiento combinando predictores de etiquetas lineales localmente regularizados Jl en una forma matemática explícita, y podemos minimizarlo directamente utilizando algunas técnicas estándar de optimización. Sin embargo, los resultados pueden no ser lo suficientemente buenos ya que solo explotamos la información local del conjunto de datos. En la siguiente subsección, introduciremos un criterio de regularización global y lo combinaremos con Jl, que tiene como objetivo encontrar un buen resultado de agrupamiento de manera local-global. 2.3 Regularización Global En el agrupamiento de datos, generalmente requerimos que las asignaciones de clúster de los puntos de datos sean lo suficientemente suaves con respecto a la variedad de datos subyacente, lo que implica (1) que los puntos cercanos tienden a tener las mismas asignaciones de clúster; (2) que los puntos en la misma estructura (por ejemplo, subvariedad o clúster) tienden a tener las mismas asignaciones de clúster [31]. Sin pérdida de generalidad, asumimos que los puntos de datos residen (aproximadamente) en una variedad de baja dimensión M2, y q es la función de asignación de clúster definida en M, es decir, 2 Creemos que los datos de texto también se muestrean de alguna variedad de baja dimensión, ya que es imposible que para todos los x ∈ M, q(x) devuelva la membresía del clúster de x. La suavidad de q sobre M se puede calcular mediante la siguiente integral de Dirichlet [2] D[q] = 1 2 M q(x) 2 dM, (13) donde el gradiente q es un vector en el espacio tangente T Mx, y la integral se toma con respecto a la medida estándar en M. Si restringimos la escala de q por q, q M = 1 (donde ·, · M es el producto interno inducido en M), entonces resulta que encontrar la función más suave que minimiza D[q] se reduce a encontrar las autofunciones del operador Laplace Beltrami L, que se define como Lq −div q, (14) donde div es la divergencia de un campo vectorial. Generalmente, el gráfico se puede ver como la forma discretizada de una variedad. Podemos modelar el conjunto de datos como un grafo ponderado no dirigido como en el agrupamiento espectral [22], donde los nodos del grafo son simplemente los puntos de datos, y los pesos en las aristas representan las similitudes entre pares de puntos. Entonces se puede demostrar que minimizar la Ec. (13) corresponde a minimizar Jg = qT Lq = n i=1 (qi − qj)2 wij, (15), donde q = [q1, q2, · · · , qn]T con qi = q(xi), L es el Laplaciano del grafo con su entrada (i, j)-ésima Lij =    di − wii, si i = j −wij, si xi y xj son adyacentes 0, en otro caso, (16), donde di = j wij es el grado de xi, wij es la similitud entre xi y xj. Si xi y xj son adyacentes, wij generalmente se calcula de la siguiente manera: wij = e − xi−xj 2 2σ2, donde σ es un parámetro dependiente del conjunto de datos. Se ha demostrado que bajo ciertas condiciones, tal forma de wij para determinar los pesos en los bordes del grafo conduce a la convergencia del Laplaciano del grafo al operador Laplace Beltrami [3][18]. En resumen, el uso de la Ec. (15) con pesos exponenciales puede medir de manera efectiva la suavidad de las asignaciones de datos con respecto a la variedad intrínseca de datos. Por lo tanto, lo adoptamos como un regularizador global para penalizar la suavidad de las asignaciones de datos predichas. 2.4 Agrupamiento con Regularización Local y Global Combinando los contenidos que hemos introducido en la sección 2.2 y la sección 2.3, podemos derivar que el criterio de agrupamiento es minq J = Jl + λJg = Pq − q 2 + λqT Lq sujeto a qi ∈ {−1, +1}, (18) donde P está definido como en la Ecuación (12), y λ es un parámetro de regularización para equilibrar Jl y Jg. Sin embargo, los valores discretos llenan todo el espacio muestral de alta dimensionalidad. Y se ha demostrado que los métodos basados en variedades pueden lograr buenos resultados en tareas de clasificación de texto [31]. En este artículo, definimos xi y xj como adyacentes si xi ∈ N(xj) o xj ∈ N(xi). La restricción de pi convierte el problema en un problema de programación entera NP difícil. Una forma natural de hacer que el problema sea resoluble es eliminar la restricción y relajar qi para que sea continuo, luego el objetivo que buscamos minimizar se convierte en J = Pq − q 2 + λqT Lq = qT (P − I)T (P − I)q + λqT Lq = qT (P − I)T (P − I) + λL q, (19) y luego agregamos una restricción adicional qT q = 1 para restringir la escala de q. Entonces nuestro objetivo se convierte en minq J = qT (P − I)T (P − I) + λL q s.t. qT q = 1 (20) Utilizando el método de Lagrange, podemos derivar que la solución óptima q corresponde al menor vector propio de la matriz M = (P − I)T (P − I) + λL, y la asignación de clúster de xi puede determinarse por el signo de qi, es decir, xi se clasificará como clase uno si qi > 0, de lo contrario se clasificará como clase 2. 2.5 CLGR Multiclase En lo anterior hemos introducido el marco básico de Agrupamiento con Regularización Local y Global (CLGR) para el problema de agrupamiento de dos clases, y lo extenderemos al agrupamiento multiclase en esta subsección. Primero asumimos que todos los documentos pertenecen a C clases indexadas por L = {1, 2, · · · , C}. qc es la función de clasificación para la clase c (1 ≤ c ≤ C), tal que qc(xi) devuelve la confianza de que xi pertenece a la clase c. Nuestro objetivo es obtener el valor de qc(xi) (1 ≤ c ≤ C, 1 ≤ i ≤ n), y la asignación de cluster de xi puede ser determinada por {qc(xi)}C c=1 utilizando algunos métodos adecuados de discretización que presentaremos más adelante. Por lo tanto, en este caso de múltiples clases, para cada documento xi (1 i n), construiremos C predictores de etiquetas regularizados localmente lineales cuyos vectores normales son wc∗ i = Xi XT i Xi + λiniIi −1 qc i (1 c C), (21), donde Xi = [xi1, xi2, · · · , xini ] con xik siendo el k-ésimo vecino de xi, y qc i = [qc i1, qc i2, · · · , qc ini ]T con qc ik = qc (xik). Entonces (wc∗ i )T xi devuelve la confianza predicha de xi perteneciente a la clase c. Por lo tanto, el error de predicción local para la clase c se puede definir como J c l = n i=1 (wc∗ i ) T xi − qc i 2 , (22) Y el error de predicción local total se convierte en Jl = C c=1 J c l = C c=1 n i=1 (wc∗ i ) T xi − qc i 2 . (23) Como en la Ec. (11), podemos definir una matriz n×n P (ver Ec. (12)) y reescribir Jl como Jl = C c=1 J c l = C c=1 Pqc − qc 2 . (24) De manera similar, podemos definir el regularizador de suavidad global en el caso de múltiples clases como Jg = C c=1 n i=1 (qc i − qc j )2 wij = C c=1 (qc )T Lqc . (25) Luego, el criterio a minimizar para CLGR en el caso de múltiples clases se convierte en J = Jl + λJg = C c=1 Pqc − qc 2 + λ(qc )T Lqc = C c=1 (qc )T (P − I)T (P − I) + λL qc = trace QT (P − I)T (P − I) + λL Q , (26) donde Q = [q1 , q2 , · · · , qc ] es una matriz n × c, y trace(·) devuelve la traza de una matriz. Al igual que en la ecuación (20), también agregamos la restricción de que QT Q = I para limitar la escala de Q. Entonces nuestro problema de optimización se convierte en minQ J = traza QT (P − I)T (P − I) + λL Q sujeto a. Según el teorema de Ky Fan [28], sabemos que la solución óptima del problema anterior es Q∗ = [q∗ 1, q∗ 2, · · · , q∗ C ]R, donde q∗ k (1 ≤ k ≤ C) es el autovector que corresponde al k-ésimo valor propio más pequeño de la matriz (P − I)T (P − I) + λL, y R es una matriz arbitraria de tamaño C × C. Dado que los valores de las entradas en Q∗ son continuos, necesitamos discretizar aún más Q∗ para obtener las asignaciones de clúster de todos los puntos de datos. Principalmente hay dos enfoques para lograr este objetivo: 1. Como en [20], podemos tratar la i-ésima fila de Q como la incrustación de xi en un espacio de C dimensiones, y aplicar algunos métodos de agrupamiento tradicionales como kmeans para agrupar estas incrustaciones en C grupos. 2. Dado que el Q∗ óptimo no es único (debido a la existencia de una matriz R arbitraria), podemos buscar un R óptimo que rote Q∗ a una matriz de indicación4. El algoritmo detallado se puede consultar en [26]. El procedimiento detallado del algoritmo para CLGR se resume en la tabla 1. 3. EXPERIMENTOS En esta sección, se realizan experimentos para comparar empíricamente los resultados de agrupamiento de CLGR con otros 8 algoritmos representativos de agrupamiento de documentos en 5 conjuntos de datos. Primero presentaremos la información básica de esos conjuntos de datos. 3.1 Conjuntos de datos Utilizamos una variedad de conjuntos de datos, la mayoría de los cuales son frecuentemente utilizados en la investigación de recuperación de información. La Tabla 2 resume las características de los conjuntos de datos. Aquí, una matriz de indicación T es una matriz n×c con su entrada (i, j) Tij ∈ {0, 1} de modo que para cada fila de Q∗ solo hay un 1. Entonces, el xi puede ser asignado al j-ésimo grupo tal que j = argjQ∗ ij = 1. Tabla 1: Agrupamiento con Regularización Local y Global (CLGR) Entrada: 1. Conjunto de datos X = {xi}n i=1; 2. Número de grupos C: 3. Tamaño del vecindario K: 4. Parámetros locales de regularización {λi}n i=1; 5. Parámetro de regularización global λ; Salida: La membresía del clúster de cada punto de datos. Procedimiento: 1. Construir los K vecindarios más cercanos para cada punto de datos; 2. Construya la matriz P utilizando la Ecuación (12); 3. Construya la matriz Laplaciana L utilizando la ecuación (16); 4. Construye la matriz M = (P − I)T (P − I) + λL; 5. Realice la descomposición de valores propios en M y construya la matriz Q∗ de acuerdo con la Ecuación (28); 6. Generar las asignaciones de clúster de cada punto de datos al discretizar adecuadamente Q∗. Tabla 2: Descripciones de los conjuntos de datos de documentos Conjuntos de datos Número de documentos Número de clases CSTR 476 4 WebKB4 4199 4 Reuters 2900 10 WebACE 2340 20 Newsgroup4 3970 4 CSTR. Este es el conjunto de datos de los resúmenes de informes técnicos publicados en el Departamento de Ciencias de la Computación de una universidad. El conjunto de datos contenía 476 resúmenes, los cuales fueron divididos en cuatro áreas de investigación: Procesamiento de Lenguaje Natural (NLP), Robótica/Visión, Sistemas y Teoría. WebKB. El conjunto de datos WebKB contiene páginas web recopiladas de los departamentos de informática de universidades. Hay alrededor de 8280 documentos y están divididos en 7 categorías: estudiante, facultad, personal, curso, proyecto, departamento y otros. El texto sin procesar es de aproximadamente 27MB. Entre estas 7 categorías, estudiante, facultad, curso y proyecto son las cuatro categorías que representan entidades más pobladas. El subconjunto asociado suele ser llamado WebKB4. Reuters. La colección de pruebas de categorización de texto Reuters-21578 contiene documentos recopilados de la agencia de noticias Reuters en 1987. Es un punto de referencia estándar para la categorización de texto y contiene 135 categorías. En nuestros experimentos, utilizamos un subconjunto de la colección de datos que incluye las 10 categorías más frecuentes entre los 135 temas y lo llamamos Reuters-top 10. WebACE. El conjunto de datos WebACE proviene del proyecto WebACE y ha sido utilizado para la agrupación de documentos [17][5]. El conjunto de datos WebACE contiene 2340 documentos que consisten en artículos de noticias del servicio de noticias de Reuters a través de la web en octubre de 1997. Estos documentos están divididos en 20 clases. Not enough context provided. El conjunto de datos News4 utilizado en nuestros experimentos se selecciona del famoso conjunto de datos 20-newsgroups. El tema rec que contiene autos, motocicletas, béisbol y hockey fue seleccionado de la versión 20news-18828. El conjunto de datos News4 contiene 3970 vectores de documentos. Para preprocesar los conjuntos de datos, eliminamos las palabras vacías utilizando una lista estándar de palabras vacías, se omiten todas las etiquetas HTML y se ignoran todos los campos de encabezado excepto el asunto y la organización de los artículos publicados. En todos nuestros experimentos, primero seleccionamos las 1000 palabras principales por información mutua con las etiquetas de clase. 3.2 Métricas de Evaluación En los experimentos, establecemos el número de clústeres igual al número real de clases C para todos los algoritmos de agrupamiento. Para evaluar su rendimiento, comparamos los grupos generados por estos algoritmos con las clases reales mediante el cálculo de las siguientes dos medidas de rendimiento. Precisión de agrupamiento (Acc). La primera medida de rendimiento es la Precisión de Agrupamiento, que descubre la relación uno a uno entre los grupos y las clases, y mide en qué medida cada grupo contenía puntos de datos de la clase correspondiente. Resume todo el grado de coincidencia entre todos los pares de clases y clusters. La precisión del agrupamiento se puede calcular como: Acc = 1 N max   Ck,Lm T(Ck, Lm)   , (29) donde Ck denota el k-ésimo clúster en los resultados finales, y Lm es la verdadera clase m-ésima. T(Ck, Lm) es el número de entidades que pertenecen a la clase m y están asignadas al clúster k. La precisión calcula la suma máxima de T(Ck, Lm) para todos los pares de clústeres y clases, y estos pares no tienen superposiciones. La mayor precisión de agrupamiento significa un mejor rendimiento de agrupamiento. Información Mutua Normalizada (NMI). Otro métrica de evaluación que adoptamos aquí es la Información Mutua Normalizada (NMI) [23], la cual es ampliamente utilizada para determinar la calidad de los grupos. Para dos variables aleatorias X e Y, el NMI se define como: NMI(X, Y) = I(X, Y) H(X)H(Y), donde I(X, Y) es la información mutua entre X e Y, mientras que H(X) y H(Y) son las entropías de X e Y respectivamente. Se puede ver que NMI(X, X) = 1, que es el valor máximo posible de NMI. Dado un resultado de agrupamiento, el NMI en la ecuación (30) se estima como NMI = C k=1 C m=1 nk,mlog n·nk,m nk ˆnm C k=1 nklog nk n C m=1 ˆnmlog ˆnm n, (31), donde nk denota el número de datos contenidos en el grupo Ck (1 k C), ˆnm es el número de datos pertenecientes a la clase m-ésima (1 m C), y nk,m denota el número de datos que están en la intersección entre el grupo Ck y la clase m-ésima. El valor calculado en la Ecuación (31) se utiliza como medida de rendimiento para el resultado de agrupamiento dado. A mayor valor, mejor rendimiento de agrupamiento. 3.3 Comparaciones Hemos realizado evaluaciones de rendimiento exhaustivas probando nuestro método y comparándolo con otros 8 métodos representativos de agrupamiento de datos utilizando las mismas corpora de datos. Los algoritmos que evaluamos se enumeran a continuación. 1. K-means tradicional (KM). 2. K-medias esférico (SKM). La implementación se basa en [9]. 3. Modelo de Mezcla Gaussiana (GMM). La implementación se basa en [16]. 4. Agrupamiento espectral con cortes normalizados (Ncut). La implementación se basa en [26], y la varianza de la similitud gaussiana se determina mediante Escalado Local [30]. Ten en cuenta que el criterio que Ncut busca minimizar es simplemente el regularizador global en nuestro algoritmo CLGR, excepto que Ncut utiliza el Laplaciano normalizado. 5. Agrupamiento utilizando Regularización Local Pura (CPLR). En este método simplemente minimizamos Jl (definido en la Ecuación (24)), y los resultados de agrupamiento pueden obtenerse realizando la descomposición de valores propios en la matriz (I − P)T (I − P) con algunos métodos de discretización adecuados. 6. Iteración de subespacio adaptativa (ASI). La implementación se basa en [14]. 7. Factorización de matrices no negativas (NMF). La implementación se basa en [27]. 8. Factorización Tri-Factorización de Matrices No Negativas (TNMF) [12]. La implementación se basa en [15]. Para la eficiencia computacional, en la implementación de CPLR y nuestro algoritmo CLGR, hemos establecido todos los parámetros de regularización local {λi}n i=1 para ser idénticos, los cuales se establecen mediante búsqueda en cuadrícula de {0.1, 1, 10}. El tamaño de los vecindarios k más cercanos se establece mediante una búsqueda en cuadrícula de {20, 40, 80}. Para el método CLGR, su parámetro de regularización global se establece mediante una búsqueda en cuadrícula de {0.1, 1, 10}. Al construir el regularizador global, hemos adoptado el método de escalamiento local [30] para construir la matriz Laplaciana. El método de discretización final adoptado en estos dos métodos es el mismo que en [26], ya que nuestros experimentos muestran que el uso de dicho método puede lograr mejores resultados que el uso de métodos basados en kmeans como en [20]. 3.4 Resultados Experimentales Los resultados de comparación de precisión de agrupamiento se muestran en la tabla 3, y los resultados de comparación de información mutua normalizada se resumen en la tabla 4. De las dos tablas principalmente observamos que: 1. Nuestro método CLGR supera a todos los demás métodos de agrupamiento de documentos en la mayoría de los conjuntos de datos. Para la agrupación de documentos, el método Spherical k-means suele superar al método tradicional de agrupación k-means, y el método GMM puede lograr resultados competitivos en comparación con el método Spherical k-means; 3. Los resultados obtenidos de los algoritmos tipo k-means y GMM suelen ser peores que los resultados obtenidos de Clustering Espectral. Dado que el Agrupamiento Espectral puede ser visto como una versión ponderada del kernel k-means, puede obtener buenos resultados cuando los clusters de datos tienen formas arbitrarias. Esto corrobora que los vectores de los documentos no están distribuidos regularmente (esféricos o elípticos). 4. Las comparaciones experimentales verifican empíricamente la equivalencia entre NMF y Clustering Espectral, como se muestra en la Tabla 3: Precisión de agrupamiento de los diversos métodos CSTR WebKB4 Reuters WebACE News4 KM 0.4256 0.3888 0.4448 0.4001 0.3527 SKM 0.4690 0.4318 0.5025 0.4458 0.3912 GMM 0.4487 0.4271 0.4897 0.4521 0.3844 NMF 0.5713 0.4418 0.4947 0.4761 0.4213 Ncut 0.5435 0.4521 0.4896 0.4513 0.4189 ASI 0.5621 0.4752 0.5235 0.4823 0.4335 TNMF 0.6040 0.4832 0.5541 0.5102 0.4613 CPLR 0.5974 0.5020 0.4832 0.5213 0.4890 CLGR 0.6235 0.5228 0.5341 0.5376 0.5102 Tabla 4: Resultados de información mutua normalizada de los diversos métodos CSTR WebKB4 Reuters WebACE News4 KM 0.3675 0.3023 0.4012 0.3864 0.3318 SKM 0.4027 0.4155 0.4587 0.4003 0.4085 GMM 0.4034 0.4093 0.4356 0.4209 0.3994 NMF 0.5235 0.4517 0.4402 0.4359 0.4130 Ncut 0.4833 0.4497 0.4392 0.4289 0.4231 ASI 0.5008 0.4833 0.4769 0.4817 0.4503 TNMF 0.5724 0.5011 0.5132 0.5328 0.4749 CPLR 0.5695 0.5231 0.4402 0.5543 0.4690 CLGR 0.6012 0.5434 0.4935 0.5390 0.4908 ha sido demostrado teóricamente en [10]. Se puede observar en las tablas que NMF y el Agrupamiento Espectral suelen llevar a resultados de agrupamiento similares. 5. Los métodos basados en co-agrupamiento (TNMF y ASI) suelen lograr mejores resultados que los métodos tradicionales basados únicamente en vectores de documentos. Dado que estos métodos realizan una selección implícita de características en cada iteración, proporcionan una métrica adaptativa para medir el vecindario y, por lo tanto, tienden a producir mejores resultados de agrupamiento. 6. Los resultados obtenidos a través de CPLR suelen ser mejores que los resultados obtenidos a través de Agrupamiento Espectral, lo cual respalda la teoría de Vapnik [24] de que a veces los algoritmos de aprendizaje local pueden obtener mejores resultados que los algoritmos de aprendizaje global. Además de los experimentos de comparación mencionados anteriormente, también probamos la sensibilidad de los parámetros de nuestro método. Principalmente hay dos conjuntos de parámetros en nuestro algoritmo CLGR, los parámetros de regularización locales y globales ({λi}n i=1 y λ, como hemos mencionado en la sección 3.3, hemos establecido que todos los λis son idénticos a λ∗ en nuestros experimentos), y el tamaño de los vecindarios. Por lo tanto, también hemos realizado dos series de experimentos: 1. Fijando el tamaño de los vecindarios y probando el rendimiento del agrupamiento con diferentes valores de λ∗ y λ. En este conjunto de experimentos, encontramos que nuestro algoritmo CLGR puede lograr buenos resultados cuando los dos parámetros de regularización no son ni demasiado grandes ni demasiado pequeños. Normalmente nuestro método puede lograr buenos resultados cuando λ∗ y λ están alrededor de 0.1. La Figura 1 nos muestra un ejemplo de prueba en el conjunto de datos WebACE. Fijando los parámetros de regularización local y global, y probando el rendimiento del agrupamiento con diferentes valores de -5, -4.5, -4, -3.5, -3, -5, -4.5, -4, -3.5, -3, 0.35, 0.4, 0.45, 0.5, 0.55 para el parámetro de regularización local (valor logarítmico base 2) y para el parámetro de regularización global (valor logarítmico base 2), se obtuvo una precisión de agrupamiento. Figura 1: Resultados de la prueba de sensibilidad de parámetros en el conjunto de datos WebACE con el tamaño de vecindario fijo en 20, donde el eje x e y representan el valor logarítmico base 2 de λ∗ y λ. En este conjunto de experimentos, encontramos que el vecindario con un tamaño demasiado grande o demasiado pequeño deteriorará todos los resultados finales de agrupamiento. Esto puede entenderse fácilmente ya que cuando el tamaño del vecindario es muy pequeño, los puntos de datos utilizados para entrenar los clasificadores locales pueden no ser suficientes; cuando el tamaño del vecindario es muy grande, los clasificadores entrenados tenderán a ser globales y no podrán capturar las características locales típicas. La Figura 2 nos muestra un ejemplo de prueba en el conjunto de datos WebACE. Por lo tanto, podemos ver que nuestro algoritmo CLGR (1) puede lograr resultados satisfactorios y (2) no es muy sensible a la elección de parámetros, lo que lo hace práctico en aplicaciones del mundo real. CONCLUSIONES Y TRABAJOS FUTUROS En este artículo, desarrollamos un nuevo algoritmo de agrupamiento llamado agrupamiento con regularización local y global. Nuestro método conserva el mérito de los algoritmos de aprendizaje local y el agrupamiento espectral. Nuestros experimentos muestran que el algoritmo propuesto supera a la mayoría de los algoritmos de vanguardia en muchos conjuntos de datos de referencia. En el futuro, nos enfocaremos en la selección de parámetros y los problemas de aceleración del algoritmo CLGR. REFERENCIAS [1] L. Baker y A. McCallum. Agrupamiento distribucional de palabras para clasificación de texto. En Actas de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1998. [2] M. Belkin y P. Niyogi. Mapeo de Eigen de Laplaciano para Reducción de Dimensionalidad y Representación de Datos. Computación Neural, 15 (6):1373-1396. Junio de 2003. [3] M. Belkin y P. Niyogi. Hacia una Fundación Teórica para Métodos de Manifold Basados en Laplacianos. En Actas de la 18ª Conferencia sobre Teoría del Aprendizaje (COLT). 2005. 10 20 30 40 50 60 70 80 90 100 0.35 0.4 0.45 0.5 0.55 tamaño de la precisión de agrupamiento del vecindario Figura 2: Resultados de pruebas de sensibilidad de parámetros en el conjunto de datos WebACE con los parámetros de regularización fijados en 0.1, y el tamaño del vecindario variando de 10 a 100. [4] M. Belkin, P. Niyogi y V. Sindhwani. Regularización de variedades: un marco geométrico para el aprendizaje a partir de ejemplos. Revista de Investigación en Aprendizaje Automático 7, 1-48, 2006. [5] D. Boley. División Direccional Principal. Minería de datos y descubrimiento de conocimiento, 2:325-344, 1998. [6] L. Bottou y V. Vapnik. Algoritmos de aprendizaje local. Computación Neural, 4:888-900, 1992. [7] P. K. Chan, D. F. Schlag y J. Y. Zien. Particionamiento y agrupamiento K-way de corte de razón espectral. IEEE Trans. Diseño Asistido por Computadora, 13:1088-1096, Sep. 1994. [8] D. R. Cutting, D. R. Karger, J. O. Pederson y J. W. Tukey. Dispersión/Recolección: Un enfoque basado en clústeres para explorar grandes colecciones de documentos. En Actas de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1992. [9] I. S. Dhillon y D. S. Modha. Descomposiciones de conceptos para datos de texto grandes y dispersos utilizando agrupamiento. Aprendizaje automático, vol. 42(1), páginas 143-175, enero de 2001. [10] C. Ding, X. Él, y H. Simon. Sobre la equivalencia entre la factorización de matrices no negativas y el agrupamiento espectral. En Actas de la Conferencia de Minería de Datos de SIAM, 2005. [11] C. Ding, X. Él, H. Zha, M. Gu y H. D. Simon. Un algoritmo de corte min-max para particionar grafos y agrupar datos. En Proc. de la 1ra Conferencia Internacional sobre Minería de Datos (ICDM), páginas 107-114, 2001. [12] C. Ding, T. Li, W. Peng y H. Park. Tri-factorizaciones de matrices no negativas ortogonales para agrupamiento. En Actas de la Duodécima Conferencia Internacional de la ACM SIGKDD sobre Descubrimiento de Conocimiento y Minería de Datos, 2006. [13] R. O. Duda, P. E. Hart y D. G. Stork. Clasificación de patrones. John Wiley & Sons, Inc., 2001. [14] T. Li, S. Ma y M. Ogihara. Agrupamiento de documentos a través de la Iteración de Subespacios Adaptativos. En Actas de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2004. [15] T. Li y C. Ding. Las relaciones entre varios métodos de factorización de matrices no negativas para agrupamiento. En Actas de la 6ta Conferencia Internacional sobre Minería de Datos (ICDM). 2006. [16] X. Liu y Y. Gong. Agrupación de documentos con capacidades de refinamiento de clústeres y selección de modelos. En Proc. de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2002. [17] E. Han, D. Boley, M. Gini, R. Gross, K. Hastings, G. Karypis, V. Kumar, B. Mobasher y J. Moore. WebACE: Un agente web para la categorización y exploración de documentos. En Actas de la 2ª Conferencia Internacional sobre Agentes Autónomos (Agents98). ACM Press, 1998. [18] M. Hein, J. Y. Audibert y U. von Luxburg. De Gráficas a Variedades - Consistencia Puntual Débil y Fuerte de los Laplacianos de Gráficas. En Actas de la 18ª Conferencia sobre Teoría del Aprendizaje (COLT), 470-485. 2005. [19] J. Él, M. Lan, C.-L. Tan, S.-Y. Sung, y H.-B. Bajo. Inicialización de algoritmos de refinamiento de clústeres: una revisión y estudio comparativo. En Proc. de Inter. Conferencia Conjunta sobre Redes Neuronales, 2004. [20] A. Y. Ng, M. I. Jordan, Y. Weiss. En el agrupamiento espectral: análisis y un algoritmo. En Avances en Sistemas de Procesamiento de Información Neural 14. 2002. [21] B. Sch¨olkopf y A. Smola. Aprendizaje con Kernels. El MIT Press. Cambridge, Massachusetts. 2002. [22] J. Shi y J. Malik. Cortes normalizados y segmentación de imágenes. IEEE Trans. on Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000. [23] A. Strehl and J. Ghosh.\nIEEE Trans. on Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000. [23] A. Strehl y J. Ghosh. Conjuntos de agrupamientos: un marco de reutilización de conocimiento para combinar múltiples particiones. Revista de Investigación de Aprendizaje Automático, 3:583-617, 2002. [24] V. N. Vapnik. La naturaleza de la teoría del aprendizaje estadístico. Berlín: Springer-Verlag, 1995. [25] Wu, M. y Schölkopf, B. Un enfoque de aprendizaje local para el agrupamiento. En Avances en Sistemas de Procesamiento de Información Neural 18. 2006. [26] S. X. Yu, J. Shi. Agrupamiento espectral multiclase. En Actas de la Conferencia Internacional sobre Visión por Computadora, 2003. [27] W. Xu, X. Liu y Y. Gong. Agrupación de documentos basada en la factorización de matrices no negativas. En Actas de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2003. [28] H. Zha, X. Él, C. Ding, M. Gu y H. Simon. Relajación espectral para el agrupamiento K-means. En NIPS 14. 2001. [29] T. Zhang y F. J. Oles. Categorización de texto basada en métodos de clasificación lineal regularizados. Revista de Recuperación de Información, 4:5-31, 2001. [30] L. Zelnik-Manor y P. Perona. Agrupamiento espectral autoajustable. En NIPS 17. 2005. [31] D. Zhou, O. Bousquet, T. N. Lal, J. Weston y B. Schölkopf. Aprendizaje con Consistencia Local y Global. NIPS 17, 2005. \n\nNIPS 17, 2005. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "label prediction": {
            "translated_key": "predicción de etiquetas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Regularized Clustering for Documents ∗ Fei Wang, Changshui Zhang State Key Lab of Intelligent Tech.",
                "and Systems Department of Automation, Tsinghua University Beijing, China, 100084 feiwang03@gmail.com Tao Li School of Computer Science Florida International University Miami, FL 33199, U.S.A. taoli@cs.fiu.edu ABSTRACT In recent years, document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering.",
                "In this paper, we propose a novel method for clustering documents using regularization.",
                "Unlike traditional globally regularized clustering methods, our method first construct a local regularized linear label predictor for each document vector, and then combine all those local regularizers with a global smoothness regularizer.",
                "So we call our algorithm Clustering with Local and Global Regularization (CLGR).",
                "We will show that the cluster memberships of the documents can be achieved by eigenvalue decomposition of a sparse symmetric matrix, which can be efficiently solved by iterative methods.",
                "Finally our experimental evaluations on several datasets are presented to show the superiorities of CLGR over traditional document clustering methods.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Clustering; I.2.6 [Artificial Intelligence]: Learning-Concept Learning General Terms Algorithms 1.",
                "INTRODUCTION Document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering.",
                "A good document clustering approach can assist the computers to automatically organize the document corpus into a meaningful cluster hierarchy for efficient browsing and navigation, which is very valuable for complementing the deficiencies of traditional information retrieval technologies.",
                "As pointed out by [8], the information retrieval needs can be expressed by a spectrum ranged from narrow keyword-matching based search to broad information browsing such as what are the major international events in recent months.",
                "Traditional document retrieval engines tend to fit well with the search end of the spectrum, i.e. they usually provide specified search for documents matching the users query, however, it is hard for them to meet the needs from the rest of the spectrum in which a rather broad or vague information is needed.",
                "In such cases, efficient browsing through a good cluster hierarchy will be definitely helpful.",
                "Generally, document clustering methods can be mainly categorized into two classes: hierarchical methods and partitioning methods.",
                "The hierarchical methods group the data points into a hierarchical tree structure using bottom-up or top-down approaches.",
                "For example, hierarchical agglomerative clustering (HAC) [13] is a typical bottom-up hierarchical clustering method.",
                "It takes each data point as a single cluster to start off with and then builds bigger and bigger clusters by grouping similar data points together until the entire dataset is encapsulated into one final cluster.",
                "On the other hand, partitioning methods decompose the dataset into a number of disjoint clusters which are usually optimal in terms of some predefined criterion functions.",
                "For instance, K-means [13] is a typical partitioning method which aims to minimize the sum of the squared distance between the data points and their corresponding cluster centers.",
                "In this paper, we will focus on the partitioning methods.",
                "As we know that there are two main problems existing in partitioning methods (like Kmeans and Gaussian Mixture Model (GMM) [16]): (1) the predefined criterion is usually non-convex which causes many local optimal solutions; (2) the iterative procedure (e.g. the Expectation Maximization (EM) algorithm) for optimizing the criterions usually makes the final solutions heavily depend on the initializations.",
                "In the last decades, many methods have been proposed to overcome the above problems of the partitioning methods [19][28].",
                "Recently, another type of partitioning methods based on clustering on data graphs have aroused considerable interests in the machine learning and data mining community.",
                "The basic idea behind these methods is to first model the whole dataset as a weighted graph, in which the graph nodes represent the data points, and the weights on the edges correspond to the similarities between pairwise points.",
                "Then the cluster assignments of the dataset can be achieved by optimizing some criterions defined on the graph.",
                "For example Spectral Clustering is one kind of the most representative graph-based clustering approaches, it generally aims to optimize some cut value (e.g.",
                "Normalized Cut [22], Ratio Cut [7], Min-Max Cut [11]) defined on an undirected graph.",
                "After some relaxations, these criterions can usually be optimized via eigen-decompositions, which is guaranteed to be global optimal.",
                "In this way, spectral clustering efficiently avoids the problems of the traditional partitioning methods as we introduced in last paragraph.",
                "In this paper, we propose a novel document clustering algorithm that inherits the superiority of spectral clustering, i.e. the final cluster results can also be obtained by exploit the eigen-structure of a symmetric matrix.",
                "However, unlike spectral clustering, which just enforces a smoothness constraint on the data labels over the whole data manifold [2], our method first construct a regularized linear label predictor for each data point from its neighborhood as in [25], and then combine the results of all these local label predictors with a global label smoothness regularizer.",
                "So we call our method Clustering with Local and Global Regularization (CLGR).",
                "The idea of incorporating both local and global information into <br>label prediction</br> is inspired by the recent works on semi-supervised learning [31], and our experimental evaluations on several real document datasets show that CLGR performs better than many state-of-the-art clustering methods.",
                "The rest of this paper is organized as follows: in section 2 we will introduce our CLGR algorithm in detail.",
                "The experimental results on several datasets are presented in section 3, followed by the conclusions and discussions in section 4. 2.",
                "THE PROPOSED ALGORITHM In this section, we will introduce our Clustering with Local and Global Regularization (CLGR) algorithm in detail.",
                "First lets see the how the documents are represented throughout this paper. 2.1 Document Representation In our work, all the documents are represented by the weighted term-frequency vectors.",
                "Let W = {w1, w2, · · · , wm} be the complete vocabulary set of the document corpus (which is preprocessed by the stopwords removal and words stemming operations).",
                "The term-frequency vector xi of document di is defined as xi = [xi1, xi2, · · · , xim]T , xik = tik log n idfk , where tik is the term frequency of wk ∈ W, n is the size of the document corpus, idfk is the number of documents that contain word wk.",
                "In this way, xi is also called the TFIDF representation of document di.",
                "Furthermore, we also normalize each xi (1 i n) to have a unit length, so that each document is represented by a normalized TF-IDF vector. 2.2 Local Regularization As its name suggests, CLGR is composed of two parts: local regularization and global regularization.",
                "In this subsection we will introduce the local regularization part in detail. 2.2.1 Motivation As we know that clustering is one type of learning techniques, it aims to organize the dataset in a reasonable way.",
                "Generally speaking, learning can be posed as a problem of function estimation, from which we can get a good classification function that will assign labels to the training dataset and even the unseen testing dataset with some cost minimized [24].",
                "For example, in the two-class classification scenario1 (in which we exactly know the label of each document), a linear classifier with least square fit aims to learn a column vector w such that the squared cost J = 1 n (wT xi − yi)2 (1) is minimized, where yi ∈ {+1, −1} is the label of xi.",
                "By taking ∂J /∂w = 0, we get the solution w∗ = n i=1 xixT i −1 n i=1 xiyi , (2) which can further be written in its matrix form as w∗ = XXT −1 Xy, (3) where X = [x1, x2, · · · , xn] is an m × n document matrix, y = [y1, y2, · · · , yn]T is the label vector.",
                "Then for a test document t, we can determine its label by l = sign(w∗T u), (4) where sign(·) is the sign function.",
                "A natural problem in Eq. (3) is that the matrix XXT may be singular and thus not invertable (e.g. when m n).",
                "To avoid such a problem, we can add a regularization term and minimize the following criterion J = 1 n n i=1 (wT xi − yi)2 + λ w 2 , (5) where λ is a regularization parameter.",
                "Then the optimal solution that minimize J is given by w∗ = XXT + λnI −1 Xy, (6) where I is an m × m identity matrix.",
                "It has been reported that the regularized linear classifier can achieve very good results on text classification problems [29].",
                "However, despite its empirical success, the regularized linear classifier is on earth a global classifier, i.e. w∗ is estimated using the whole training set.",
                "According to [24], this may not be a smart idea, since a unique w∗ may not be good enough for predicting the labels of the whole input space.",
                "In order to get better predictions, [6] proposed to train classifiers locally and use them to classify the testing points.",
                "For example, a testing point will be classified by the local classifier trained using the training points located in the vicinity 1 In the following discussions we all assume that the documents coming from only two classes.",
                "The generalizations of our method to multi-class cases will be discussed in section 2.5. of it.",
                "Although this method seems slow and stupid, it is reported that it can get better performances than using a unique global classifier on certain tasks [6]. 2.2.2 Constructing the Local Regularized Predictors Inspired by their success, we proposed to apply the local learning algorithms for clustering.",
                "The basic idea is that, for each document vector xi (1 i n), we train a local label predictor based on its k-nearest neighborhood Ni, and then use it to predict the label of xi.",
                "Finally we will combine all those local predictors by minimizing the sum of their prediction errors.",
                "In this subsection we will introduce how to construct those local predictors.",
                "Due to the simplicity and effectiveness of the regularized linear classifier that we have introduced in section 2.2.1, we choose it to be our local label predictor, such that for each document xi, the following criterion is minimized Ji = 1 ni xj ∈Ni wT i xj − qj 2 + λi wi 2 , (7) where ni = |Ni| is the cardinality of Ni, and qj is the cluster membership of xj.",
                "Then using Eq. (6), we can get the optimal solution is w∗ i = XiXT i + λiniI −1 Xiqi, (8) where Xi = [xi1, xi2, · · · , xini ], and we use xik to denote the k-th nearest neighbor of xi. qi = [qi1, qi2, · · · , qini ]T with qik representing the cluster assignment of xik.",
                "The problem here is that XiXT i is an m × m matrix with m ni, i.e. we should compute the inverse of an m × m matrix for every document vector, which is computationally prohibited.",
                "Fortunately, we have the following theorem: Theorem 1. w∗ i in Eq. (8) can be rewritten as w∗ i = Xi XT i Xi + λiniIi −1 qi, (9) where Ii is an ni × ni identity matrix.",
                "Proof.",
                "Since w∗ i = XiXT i + λiniI −1 Xiqi, then XiXT i + λiniI w∗ i = Xiqi =⇒ XiXT i w∗ i + λiniw∗ i = Xiqi =⇒ w∗ i = (λini)−1 Xi qi − XT i w∗ i .",
                "Let β = (λini)−1 qi − XT i w∗ i , then w∗ i = Xiβ =⇒ λiniβ = qi − XT i w∗ i = qi − XT i Xiβ =⇒ qi = XT i Xi + λiniIi β =⇒ β = XT i Xi + λiniIi −1 qi.",
                "Therefore w∗ i = Xiβ = Xi XT i Xi + λiniIi −1 qi 2 Using theorem 1, we only need to compute the inverse of an ni × ni matrix for every document to train a local label predictor.",
                "Moreover, for a new testing point u that falls into Ni, we can classify it by the sign of qu = w∗T i u = uT wi = uT Xi XT i Xi + λiniIi −1 qi.",
                "This is an attractive expression since we can determine the cluster assignment of u by using the inner-products between the points in {u ∪ Ni}, which suggests that such a local regularizer can easily be kernelized [21] as long as we define a proper kernel function. 2.2.3 Combining the Local Regularized Predictors After all the local predictors having been constructed, we will combine them together by minimizing Jl = n i=1 w∗T i xi − qi 2 , (10) which stands for the sum of the prediction errors for all the local predictors.",
                "Combining Eq. (10) with Eq. (6), we can get Jl = n i=1 w∗T i xi − qi 2 = n i=1 xT i Xi XT i Xi + λiniIi −1 qi − qi 2 = Pq − q 2 , (11) where q = [q1, q2, · · · , qn]T , and the P is an n × n matrix constructing in the following way.",
                "Let αi = xT i Xi XT i Xi + λiniIi −1 , then Pij = αi j, if xj ∈ Ni 0, otherwise , (12) where Pij is the (i, j)-th entry of P, and αi j represents the j-th entry of αi .",
                "Till now we can write the criterion of clustering by combining locally regularized linear label predictors Jl in an explicit mathematical form, and we can minimize it directly using some standard optimization techniques.",
                "However, the results may not be good enough since we only exploit the local informations of the dataset.",
                "In the next subsection, we will introduce a global regularization criterion and combine it with Jl, which aims to find a good clustering result in a local-global way. 2.3 Global Regularization In data clustering, we usually require that the cluster assignments of the data points should be sufficiently smooth with respect to the underlying data manifold, which implies (1) the nearby points tend to have the same cluster assignments; (2) the points on the same structure (e.g. submanifold or cluster) tend to have the same cluster assignments [31].",
                "Without the loss of generality, we assume that the data points reside (roughly) on a low-dimensional manifold M2 , and q is the cluster assignment function defined on M, i.e. 2 We believe that the text data are also sampled from some low dimensional manifold, since it is impossible for them to for ∀x ∈ M, q(x) returns the cluster membership of x.",
                "The smoothness of q over M can be calculated by the following Dirichlet integral [2] D[q] = 1 2 M q(x) 2 dM, (13) where the gradient q is a vector in the tangent space T Mx, and the integral is taken with respect to the standard measure on M. If we restrict the scale of q by q, q M = 1 (where ·, · M is the inner product induced on M), then it turns out that finding the smoothest function minimizing D[q] reduces to finding the eigenfunctions of the Laplace Beltrami operator L, which is defined as Lq −div q, (14) where div is the divergence of a vector field.",
                "Generally, the graph can be viewed as the discretized form of manifold.",
                "We can model the dataset as an weighted undirected graph as in spectral clustering [22], where the graph nodes are just the data points, and the weights on the edges represent the similarities between pairwise points.",
                "Then it can be shown that minimizing Eq. (13) corresponds to minimizing Jg = qT Lq = n i=1 (qi − qj)2 wij, (15) where q = [q1, q2, · · · , qn]T with qi = q(xi), L is the graph Laplacian with its (i, j)-th entry Lij =    di − wii, if i = j −wij, if xi and xj are adjacent 0, otherwise, (16) where di = j wij is the degree of xi, wij is the similarity between xi and xj.",
                "If xi and xj are adjacent3 , wij is usually computed in the following way wij = e − xi−xj 2 2σ2 , (17) where σ is a dataset dependent parameter.",
                "It is proved that under certain conditions, such a form of wij to determine the weights on graph edges leads to the convergence of graph Laplacian to the Laplace Beltrami operator [3][18].",
                "In summary, using Eq. (15) with exponential weights can effectively measure the smoothness of the data assignments with respect to the intrinsic data manifold.",
                "Thus we adopt it as a global regularizer to punish the smoothness of the predicted data assignments. 2.4 Clustering with Local and Global Regularization Combining the contents we have introduced in section 2.2 and section 2.3 we can derive the clustering criterion is minq J = Jl + λJg = Pq − q 2 + λqT Lq s.t. qi ∈ {−1, +1}, (18) where P is defined as in Eq. (12), and λ is a regularization parameter to trade off Jl and Jg.",
                "However, the discrete fill in the whole high-dimensional sample space.",
                "And it has been shown that the manifold based methods can achieve good results on text classification tasks [31]. 3 In this paper, we define xi and xj to be adjacent if xi ∈ N(xj) or xj ∈ N(xi). constraint of pi makes the problem an NP hard integer programming problem.",
                "A natural way for making the problem solvable is to remove the constraint and relax qi to be continuous, then the objective that we aims to minimize becomes J = Pq − q 2 + λqT Lq = qT (P − I)T (P − I)q + λqT Lq = qT (P − I)T (P − I) + λL q, (19) and we further add a constraint qT q = 1 to restrict the scale of q.",
                "Then our objective becomes minq J = qT (P − I)T (P − I) + λL q s.t. qT q = 1 (20) Using the Lagrangian method, we can derive that the optimal solution q corresponds to the smallest eigenvector of the matrix M = (P − I)T (P − I) + λL, and the cluster assignment of xi can be determined by the sign of qi, i.e. xi will be classified as class one if qi > 0, otherwise it will be classified as class 2. 2.5 Multi-Class CLGR In the above we have introduced the basic framework of Clustering with Local and Global Regularization (CLGR) for the two-class clustering problem, and we will extending it to multi-class clustering in this subsection.",
                "First we assume that all the documents belong to C classes indexed by L = {1, 2, · · · , C}. qc is the classification function for class c (1 c C), such that qc (xi) returns the confidence that xi belongs to class c. Our goal is to obtain the value of qc (xi) (1 c C, 1 i n), and the cluster assignment of xi can be determined by {qc (xi)}C c=1 using some proper discretization methods that we will introduce later.",
                "Therefore, in this multi-class case, for each document xi (1 i n), we will construct C locally linear regularized label predictors whose normal vectors are wc∗ i = Xi XT i Xi + λiniIi −1 qc i (1 c C), (21) where Xi = [xi1, xi2, · · · , xini ] with xik being the k-th neighbor of xi, and qc i = [qc i1, qc i2, · · · , qc ini ]T with qc ik = qc (xik).",
                "Then (wc∗ i )T xi returns the predicted confidence of xi belonging to class c. Hence the local prediction error for class c can be defined as J c l = n i=1 (wc∗ i ) T xi − qc i 2 , (22) And the total local prediction error becomes Jl = C c=1 J c l = C c=1 n i=1 (wc∗ i ) T xi − qc i 2 . (23) As in Eq. (11), we can define an n×n matrix P (see Eq. (12)) and rewrite Jl as Jl = C c=1 J c l = C c=1 Pqc − qc 2 . (24) Similarly we can define the global smoothness regularizer in multi-class case as Jg = C c=1 n i=1 (qc i − qc j )2 wij = C c=1 (qc )T Lqc . (25) Then the criterion to be minimized for CLGR in multi-class case becomes J = Jl + λJg = C c=1 Pqc − qc 2 + λ(qc )T Lqc = C c=1 (qc )T (P − I)T (P − I) + λL qc = trace QT (P − I)T (P − I) + λL Q , (26) where Q = [q1 , q2 , · · · , qc ] is an n × c matrix, and trace(·) returns the trace of a matrix.",
                "The same as in Eq. (20), we also add the constraint that QT Q = I to restrict the scale of Q.",
                "Then our optimization problem becomes minQ J = trace QT (P − I)T (P − I) + λL Q s.t.",
                "QT Q = I, (27) From the Ky Fan theorem [28], we know the optimal solution of the above problem is Q∗ = [q∗ 1, q∗ 2, · · · , q∗ C ]R, (28) where q∗ k (1 k C) is the eigenvector corresponds to the k-th smallest eigenvalue of matrix (P − I)T (P − I) + λL, and R is an arbitrary C × C matrix.",
                "Since the values of the entries in Q∗ is continuous, we need to further discretize Q∗ to get the cluster assignments of all the data points.",
                "There are mainly two approaches to achieve this goal: 1.",
                "As in [20], we can treat the i-th row of Q as the embedding of xi in a C-dimensional space, and apply some traditional clustering methods like kmeans to clustering these embeddings into C clusters. 2.",
                "Since the optimal Q∗ is not unique (because of the existence of an arbitrary matrix R), we can pursue an optimal R that will rotate Q∗ to an indication matrix4 .",
                "The detailed algorithm can be referred to [26].",
                "The detailed algorithm procedure for CLGR is summarized in table 1. 3.",
                "EXPERIMENTS In this section, experiments are conducted to empirically compare the clustering results of CLGR with other 8 representitive document clustering algorithms on 5 datasets.",
                "First we will introduce the basic informations of those datasets. 3.1 Datasets We use a variety of datasets, most of which are frequently used in the information retrieval research.",
                "Table 2 summarizes the characteristics of the datasets. 4 Here an indication matrix T is a n×c matrix with its (i, j)th entry Tij ∈ {0, 1} such that for each row of Q∗ there is only one 1.",
                "Then the xi can be assigned to the j-th cluster such that j = argjQ∗ ij = 1.",
                "Table 1: Clustering with Local and Global Regularization (CLGR) Input: 1.",
                "Dataset X = {xi}n i=1; 2.",
                "Number of clusters C; 3.",
                "Size of the neighborhood K; 4.",
                "Local regularization parameters {λi}n i=1; 5.",
                "Global regularization parameter λ; Output: The cluster membership of each data point.",
                "Procedure: 1.",
                "Construct the K nearest neighborhoods for each data point; 2.",
                "Construct the matrix P using Eq. (12); 3.",
                "Construct the Laplacian matrix L using Eq. (16); 4.",
                "Construct the matrix M = (P − I)T (P − I) + λL; 5.",
                "Do eigenvalue decomposition on M, and construct the matrix Q∗ according to Eq. (28); 6.",
                "Output the cluster assignments of each data point by properly discretize Q∗ .",
                "Table 2: Descriptions of the document datasets Datasets Number of documents Number of classes CSTR 476 4 WebKB4 4199 4 Reuters 2900 10 WebACE 2340 20 Newsgroup4 3970 4 CSTR.",
                "This is the dataset of the abstracts of technical reports published in the Department of Computer Science at a university.",
                "The dataset contained 476 abstracts, which were divided into four research areas: Natural Language Processing(NLP), Robotics/Vision, Systems, and Theory.",
                "WebKB.",
                "The WebKB dataset contains webpages gathered from university computer science departments.",
                "There are about 8280 documents and they are divided into 7 categories: student, faculty, staff, course, project, department and other.",
                "The raw text is about 27MB.",
                "Among these 7 categories, student, faculty, course and project are four most populous entity-representing categories.",
                "The associated subset is typically called WebKB4.",
                "Reuters.",
                "The Reuters-21578 Text Categorization Test collection contains documents collected from the Reuters newswire in 1987.",
                "It is a standard text categorization benchmark and contains 135 categories.",
                "In our experiments, we use a subset of the data collection which includes the 10 most frequent categories among the 135 topics and we call it Reuters-top 10.",
                "WebACE.",
                "The WebACE dataset was from WebACE project and has been used for document clustering [17][5].",
                "The WebACE dataset contains 2340 documents consisting news articles from Reuters new service via the Web in October 1997.",
                "These documents are divided into 20 classes.",
                "News4.",
                "The News4 dataset used in our experiments are selected from the famous 20-newsgroups dataset5 .",
                "The topic rec containing autos, motorcycles, baseball and hockey was selected from the version 20news-18828.",
                "The News4 dataset contains 3970 document vectors. 5 http://people.csail.mit.edu/jrennie/20Newsgroups/ To pre-process the datasets, we remove the stop words using a standard stop list, all HTML tags are skipped and all header fields except subject and organization of the posted articles are ignored.",
                "In all our experiments, we first select the top 1000 words by mutual information with class labels. 3.2 Evaluation Metrics In the experiments, we set the number of clusters equal to the true number of classes C for all the clustering algorithms.",
                "To evaluate their performance, we compare the clusters generated by these algorithms with the true classes by computing the following two performance measures.",
                "Clustering Accuracy (Acc).",
                "The first performance measure is the Clustering Accuracy, which discovers the one-toone relationship between clusters and classes and measures the extent to which each cluster contained data points from the corresponding class.",
                "It sums up the whole matching degree between all pair class-clusters.",
                "Clustering accuracy can be computed as: Acc = 1 N max   Ck,Lm T(Ck, Lm)   , (29) where Ck denotes the k-th cluster in the final results, and Lm is the true m-th class.",
                "T(Ck, Lm) is the number of entities which belong to class m are assigned to cluster k. Accuracy computes the maximum sum of T(Ck, Lm) for all pairs of clusters and classes, and these pairs have no overlaps.",
                "The greater clustering accuracy means the better clustering performance.",
                "Normalized Mutual Information (NMI).",
                "Another evaluation metric we adopt here is the Normalized Mutual Information NMI [23], which is widely used for determining the quality of clusters.",
                "For two random variable X and Y, the NMI is defined as: NMI(X, Y) = I(X, Y) H(X)H(Y) , (30) where I(X, Y) is the mutual information between X and Y, while H(X) and H(Y) are the entropies of X and Y respectively.",
                "One can see that NMI(X, X) = 1, which is the maximal possible value of NMI.",
                "Given a clustering result, the NMI in Eq. (30) is estimated as NMI = C k=1 C m=1 nk,mlog n·nk,m nk ˆnm C k=1 nklog nk n C m=1 ˆnmlog ˆnm n , (31) where nk denotes the number of data contained in the cluster Ck (1 k C), ˆnm is the number of data belonging to the m-th class (1 m C), and nk,m denotes the number of data that are in the intersection between the cluster Ck and the m-th class.",
                "The value calculated in Eq. (31) is used as a performance measure for the given clustering result.",
                "The larger this value, the better the clustering performance. 3.3 Comparisons We have conducted comprehensive performance evaluations by testing our method and comparing it with 8 other representative data clustering methods using the same data corpora.",
                "The algorithms that we evaluated are listed below. 1.",
                "Traditional k-means (KM). 2.",
                "Spherical k-means (SKM).",
                "The implementation is based on [9]. 3.",
                "Gaussian Mixture Model (GMM).",
                "The implementation is based on [16]. 4.",
                "Spectral Clustering with Normalized Cuts (Ncut).",
                "The implementation is based on [26], and the variance of the Gaussian similarity is determined by Local Scaling [30].",
                "Note that the criterion that Ncut aims to minimize is just the global regularizer in our CLGR algorithm except that Ncut used the normalized Laplacian. 5.",
                "Clustering using Pure Local Regularization (CPLR).",
                "In this method we just minimize Jl (defined in Eq. (24)), and the clustering results can be obtained by doing eigenvalue decomposition on matrix (I − P)T (I − P) with some proper discretization methods. 6.",
                "Adaptive Subspace Iteration (ASI).",
                "The implementation is based on [14]. 7.",
                "Nonnegative Matrix Factorization (NMF).",
                "The implementation is based on [27]. 8.",
                "Tri-Factorization Nonnegative Matrix Factorization (TNMF) [12].",
                "The implementation is based on [15].",
                "For computational efficiency, in the implementation of CPLR and our CLGR algorithm, we have set all the local regularization parameters {λi}n i=1 to be identical, which is set by grid search from {0.1, 1, 10}.",
                "The size of the k-nearest neighborhoods is set by grid search from {20, 40, 80}.",
                "For the CLGR method, its global regularization parameter is set by grid search from {0.1, 1, 10}.",
                "When constructing the global regularizer, we have adopted the local scaling method [30] to construct the Laplacian matrix.",
                "The final discretization method adopted in these two methods is the same as in [26], since our experiments show that using such method can achieve better results than using kmeans based methods as in [20]. 3.4 Experimental Results The clustering accuracies comparison results are shown in table 3, and the normalized mutual information comparison results are summarized in table 4.",
                "From the two tables we mainly observe that: 1.",
                "Our CLGR method outperforms all other document clustering methods in most of the datasets; 2.",
                "For document clustering, the Spherical k-means method usually outperforms the traditional k-means clustering method, and the GMM method can achieve competitive results compared to the Spherical k-means method; 3.",
                "The results achieved from the k-means and GMM type algorithms are usually worse than the results achieved from Spectral Clustering.",
                "Since Spectral Clustering can be viewed as a weighted version of kernel k-means, it can obtain good results the data clusters are arbitrarily shaped.",
                "This corroborates that the documents vectors are not regularly distributed (spherical or elliptical). 4.",
                "The experimental comparisons empirically verify the equivalence between NMF and Spectral Clustering, which Table 3: Clustering accuracies of the various methods CSTR WebKB4 Reuters WebACE News4 KM 0.4256 0.3888 0.4448 0.4001 0.3527 SKM 0.4690 0.4318 0.5025 0.4458 0.3912 GMM 0.4487 0.4271 0.4897 0.4521 0.3844 NMF 0.5713 0.4418 0.4947 0.4761 0.4213 Ncut 0.5435 0.4521 0.4896 0.4513 0.4189 ASI 0.5621 0.4752 0.5235 0.4823 0.4335 TNMF 0.6040 0.4832 0.5541 0.5102 0.4613 CPLR 0.5974 0.5020 0.4832 0.5213 0.4890 CLGR 0.6235 0.5228 0.5341 0.5376 0.5102 Table 4: Normalized mutual information results of the various methods CSTR WebKB4 Reuters WebACE News4 KM 0.3675 0.3023 0.4012 0.3864 0.3318 SKM 0.4027 0.4155 0.4587 0.4003 0.4085 GMM 0.4034 0.4093 0.4356 0.4209 0.3994 NMF 0.5235 0.4517 0.4402 0.4359 0.4130 Ncut 0.4833 0.4497 0.4392 0.4289 0.4231 ASI 0.5008 0.4833 0.4769 0.4817 0.4503 TNMF 0.5724 0.5011 0.5132 0.5328 0.4749 CPLR 0.5695 0.5231 0.4402 0.5543 0.4690 CLGR 0.6012 0.5434 0.4935 0.5390 0.4908 has been proved theoretically in [10].",
                "It can be observed from the tables that NMF and Spectral Clustering usually lead to similar clustering results. 5.",
                "The co-clustering based methods (TNMF and ASI) can usually achieve better results than traditional purely document vector based methods.",
                "Since these methods perform an implicit feature selection at each iteration, provide an adaptive metric for measuring the neighborhood, and thus tend to yield better clustering results. 6.",
                "The results achieved from CPLR are usually better than the results achieved from Spectral Clustering, which supports Vapniks theory [24] that sometimes local learning algorithms can obtain better results than global learning algorithms.",
                "Besides the above comparison experiments, we also test the parameter sensibility of our method.",
                "There are mainly two sets of parameters in our CLGR algorithm, the local and global regularization parameters ({λi}n i=1 and λ, as we have said in section 3.3, we have set all λis to be identical to λ∗ in our experiments), and the size of the neighborhoods.",
                "Therefore we have also done two sets of experiments: 1.",
                "Fixing the size of the neighborhoods, and testing the clustering performance with varying λ∗ and λ.",
                "In this set of experiments, we find that our CLGR algorithm can achieve good results when the two regularization parameters are neither too large nor too small.",
                "Typically our method can achieve good results when λ∗ and λ are around 0.1.",
                "Figure 1 shows us such a testing example on the WebACE dataset. 2.",
                "Fixing the local and global regularization parameters, and testing the clustering performance with different −5 −4.5 −4 −3.5 −3 −5 −4.5 −4 −3.5 −3 0.35 0.4 0.45 0.5 0.55 local regularization para (log 2 value) global regularization para (log 2 value) clusteringaccuracy Figure 1: Parameter sensibility testing results on the WebACE dataset with the neighborhood size fixed to 20, and the x-axis and y-axis represents the log2 value of λ∗ and λ. sizes of neighborhoods.",
                "In this set of experiments, we find that the neighborhood with a too large or too small size will all deteriorate the final clustering results.",
                "This can be easily understood since when the neighborhood size is very small, then the data points used for training the local classifiers may not be sufficient; when the neighborhood size is very large, the trained classifiers will tend to be global and cannot capture the typical local characteristics.",
                "Figure 2 shows us a testing example on the WebACE dataset.",
                "Therefore, we can see that our CLGR algorithm (1) can achieve satisfactory results and (2) is not very sensitive to the choice of parameters, which makes it practical in real world applications. 4.",
                "CONCLUSIONS AND FUTURE WORKS In this paper, we derived a new clustering algorithm called clustering with local and global regularization.",
                "Our method preserves the merit of local learning algorithms and spectral clustering.",
                "Our experiments show that the proposed algorithm outperforms most of the state of the art algorithms on many benchmark datasets.",
                "In the future, we will focus on the parameter selection and acceleration issues of the CLGR algorithm. 5.",
                "REFERENCES [1] L. Baker and A. McCallum.",
                "Distributional Clustering of Words for Text Classification.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 1998. [2] M. Belkin and P. Niyogi.",
                "Laplacian Eigenmaps for Dimensionality Reduction and Data Representation.",
                "Neural Computation, 15 (6):1373-1396.",
                "June 2003. [3] M. Belkin and P. Niyogi.",
                "Towards a Theoretical Foundation for Laplacian-Based Manifold Methods.",
                "In Proceedings of the 18th Conference on Learning Theory (COLT). 2005. 10 20 30 40 50 60 70 80 90 100 0.35 0.4 0.45 0.5 0.55 size of the neighborhood clusteringaccuracy Figure 2: Parameter sensibility testing results on the WebACE dataset with the regularization parameters being fixed to 0.1, and the neighborhood size varing from 10 to 100. [4] M. Belkin, P. Niyogi and V. Sindhwani.",
                "Manifold Regularization: a Geometric Framework for Learning from Examples.",
                "Journal of Machine Learning Research 7, 1-48, 2006. [5] D. Boley.",
                "Principal Direction Divisive Partitioning.",
                "Data mining and knowledge discovery, 2:325-344, 1998. [6] L. Bottou and V. Vapnik.",
                "Local learning algorithms.",
                "Neural Computation, 4:888-900, 1992. [7] P. K. Chan, D. F. Schlag and J. Y. Zien.",
                "Spectral K-way Ratio-Cut Partitioning and Clustering.",
                "IEEE Trans.",
                "Computer-Aided Design, 13:1088-1096, Sep. 1994. [8] D. R. Cutting, D. R. Karger, J. O. Pederson and J. W. Tukey.",
                "Scatter/Gather: A Cluster-Based Approach to Browsing Large Document Collections.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 1992. [9] I. S. Dhillon and D. S. Modha.",
                "Concept Decompositions for Large Sparse Text Data using Clustering.",
                "Machine Learning, vol. 42(1), pages 143-175, January 2001. [10] C. Ding, X.",
                "He, and H. Simon.",
                "On the equivalence of nonnegative matrix factorization and spectral clustering.",
                "In Proceedings of the SIAM Data Mining Conference, 2005. [11] C. Ding, X.",
                "He, H. Zha, M. Gu, and H. D. Simon.",
                "A min-max cut algorithm for graph partitioning and data clustering.",
                "In Proc. of the 1st International Conference on Data Mining (ICDM), pages 107-114, 2001. [12] C. Ding, T. Li, W. Peng, and H. Park.",
                "Orthogonal Nonnegative Matrix Tri-Factorizations for Clustering.",
                "In Proceedings of the Twelfth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2006. [13] R. O. Duda, P. E. Hart, and D. G. Stork.",
                "Pattern Classification.",
                "John Wiley & Sons, Inc., 2001. [14] T. Li, S. Ma, and M. Ogihara.",
                "Document Clustering via Adaptive Subspace Iteration.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2004. [15] T. Li and C. Ding.",
                "The Relationships Among Various Nonnegative Matrix Factorization Methods for Clustering.",
                "In Proceedings of the 6th International Conference on Data Mining (ICDM). 2006. [16] X. Liu and Y. Gong.",
                "Document Clustering with Cluster Refinement and Model Selection Capabilities.",
                "In Proc. of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002. [17] E. Han, D. Boley, M. Gini, R. Gross, K. Hastings, G. Karypis, V. Kumar, B. Mobasher, and J. Moore.",
                "WebACE: A Web Agent for Document Categorization and Exploration.",
                "In Proceedings of the 2nd International Conference on Autonomous Agents (Agents98).",
                "ACM Press, 1998. [18] M. Hein, J. Y. Audibert, and U. von Luxburg.",
                "From Graphs to Manifolds - Weak and Strong Pointwise Consistency of Graph Laplacians.",
                "In Proceedings of the 18th Conference on Learning Theory (COLT), 470-485. 2005. [19] J.",
                "He, M. Lan, C.-L. Tan, S.-Y.",
                "Sung, and H.-B.",
                "Low.",
                "Initialization of Cluster Refinement Algorithms: A Review and Comparative Study.",
                "In Proc. of Inter.",
                "Joint Conference on Neural Networks, 2004. [20] A. Y. Ng, M. I. Jordan, Y. Weiss.",
                "On Spectral Clustering: Analysis and an algorithm.",
                "In Advances in Neural Information Processing Systems 14. 2002. [21] B. Sch¨olkopf and A. Smola.",
                "Learning with Kernels.",
                "The MIT Press.",
                "Cambridge, Massachusetts. 2002. [22] J. Shi and J. Malik.",
                "Normalized Cuts and Image Segmentation.",
                "IEEE Trans. on Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000. [23] A. Strehl and J. Ghosh.",
                "Cluster Ensembles - A Knowledge Reuse Framework for Combining Multiple Partitions.",
                "Journal of Machine Learning Research, 3:583-617, 2002. [24] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Berlin: Springer-Verlag, 1995. [25] Wu, M. and Sch¨olkopf, B.",
                "A Local Learning Approach for Clustering.",
                "In Advances in Neural Information Processing Systems 18. 2006. [26] S. X. Yu, J. Shi.",
                "Multiclass Spectral Clustering.",
                "In Proceedings of the International Conference on Computer Vision, 2003. [27] W. Xu, X. Liu and Y. Gong.",
                "Document Clustering Based On Non-Negative Matrix Factorization.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2003. [28] H. Zha, X.",
                "He, C. Ding, M. Gu and H. Simon.",
                "Spectral Relaxation for K-means Clustering.",
                "In NIPS 14. 2001. [29] T. Zhang and F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Journal of Information Retrieval, 4:5-31, 2001. [30] L. Zelnik-Manor and P. Perona.",
                "Self-Tuning Spectral Clustering.",
                "In NIPS 17. 2005. [31] D. Zhou, O. Bousquet, T. N. Lal, J. Weston and B. Sch¨olkopf.",
                "Learning with Local and Global Consistency.",
                "NIPS 17, 2005."
            ],
            "original_annotated_samples": [
                "The idea of incorporating both local and global information into <br>label prediction</br> is inspired by the recent works on semi-supervised learning [31], and our experimental evaluations on several real document datasets show that CLGR performs better than many state-of-the-art clustering methods."
            ],
            "translated_annotated_samples": [
                "La idea de incorporar tanto información local como global en la <br>predicción de etiquetas</br> está inspirada en los trabajos recientes sobre aprendizaje semi-supervisado [31], y nuestras evaluaciones experimentales en varios conjuntos de datos reales de documentos muestran que CLGR funciona mejor que muchos métodos de agrupamiento de vanguardia."
            ],
            "translated_text": "Agrupamiento regularizado para documentos ∗ Fei Wang, Changshui Zhang Laboratorio Clave del Estado de Tecnología Inteligente. En los últimos años, la agrupación de documentos ha estado recibiendo cada vez más atención como una técnica importante y fundamental para la organización no supervisada de documentos, la extracción automática de temas y la recuperación o filtrado rápido de información. En este artículo, proponemos un método novedoso para agrupar documentos utilizando regularización. A diferencia de los métodos de agrupamiento regularizados globalmente tradicionales, nuestro método primero construye un predictor de etiquetas lineal regularizado local para cada vector de documento, y luego combina todos esos regularizadores locales con un regularizador de suavidad global. Así que llamamos a nuestro algoritmo Agrupamiento con Regularización Local y Global (CLGR). Mostraremos que las pertenencias de los documentos a los grupos se pueden lograr mediante la descomposición de los valores propios de una matriz simétrica dispersa, la cual puede resolverse eficientemente mediante métodos iterativos. Finalmente, se presentan nuestras evaluaciones experimentales en varios conjuntos de datos para mostrar las ventajas de CLGR sobre los métodos tradicionales de agrupamiento de documentos. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información-Agrupamiento; I.2.6 [Inteligencia Artificial]: Aprendizaje-Aprendizaje de Conceptos Términos Generales Algoritmos 1. La agrupación de documentos ha estado recibiendo cada vez más atención como una técnica importante y fundamental para la organización no supervisada de documentos, la extracción automática de temas y la recuperación o filtrado rápido de información. Un buen enfoque de agrupación de documentos puede ayudar a las computadoras a organizar automáticamente el corpus de documentos en una jerarquía de clusters significativa para una navegación eficiente, lo cual es muy valioso para complementar las deficiencias de las tecnologías tradicionales de recuperación de información. Como señaló [8], las necesidades de recuperación de información pueden expresarse mediante un espectro que va desde la búsqueda basada en palabras clave estrechas hasta la exploración de información amplia, como cuáles son los principales eventos internacionales de los últimos meses. Los motores de búsqueda de documentos tradicionales tienden a adaptarse bien al extremo de la búsqueda, es decir, suelen proporcionar búsquedas específicas de documentos que coinciden con la consulta de los usuarios, sin embargo, les resulta difícil satisfacer las necesidades del resto del espectro en el que se necesita información más amplia o vaga. En tales casos, la navegación eficiente a través de una buena jerarquía de clústeres será definitivamente útil. Generalmente, los métodos de agrupamiento de documentos se pueden clasificar principalmente en dos clases: métodos jerárquicos y métodos de partición. Los métodos jerárquicos agrupan los puntos de datos en una estructura de árbol jerárquico utilizando enfoques de abajo hacia arriba o de arriba hacia abajo. Por ejemplo, el clustering jerárquico aglomerativo (HAC) [13] es un método típico de clustering jerárquico ascendente. Toma cada punto de datos como un único clúster para comenzar y luego construye clústeres más grandes agrupando puntos de datos similares juntos hasta que todo el conjunto de datos esté encapsulado en un único clúster final. Por otro lado, los métodos de particionamiento descomponen el conjunto de datos en un número de grupos disjuntos que suelen ser óptimos en términos de algunas funciones de criterio predefinidas. Por ejemplo, K-means es un método de particionamiento típico que tiene como objetivo minimizar la suma de las distancias al cuadrado entre los puntos de datos y sus centros de clúster correspondientes. En este documento, nos centraremos en los métodos de particionamiento. Como sabemos, existen dos problemas principales en los métodos de particionamiento (como Kmeans y el Modelo de Mezcla Gaussiana (GMM) [16]): (1) el criterio predefinido suele ser no convexo, lo que provoca muchas soluciones óptimas locales; (2) el procedimiento iterativo (por ejemplo, el algoritmo de Expectation Maximization (EM)) para optimizar los criterios suele hacer que las soluciones finales dependan en gran medida de las inicializaciones. En las últimas décadas, se han propuesto muchos métodos para superar los problemas mencionados de los métodos de particionamiento [19][28]. Recientemente, otro tipo de métodos de particionamiento basados en agrupamiento en grafos de datos han despertado un considerable interés en la comunidad de aprendizaje automático y minería de datos. La idea básica detrás de estos métodos es modelar primero el conjunto de datos completo como un grafo ponderado, en el que los nodos del grafo representan los puntos de datos y los pesos en los bordes corresponden a las similitudes entre los puntos en pares. Entonces, las asignaciones de clústeres del conjunto de datos se pueden lograr optimizando algunos criterios definidos en el gráfico. Por ejemplo, el Clustering Espectral es uno de los enfoques de clustering basados en grafos más representativos, generalmente tiene como objetivo optimizar algún valor de corte (por ejemplo, Corte normalizado [22], corte de razón [7], corte mínimo-máximo [11]) definidos en un grafo no dirigido. Después de algunas relajaciones, estos criterios generalmente pueden ser optimizados a través de descomposiciones propias, lo cual está garantizado que sea óptimo a nivel global. De esta manera, el agrupamiento espectral evita eficientemente los problemas de los métodos de particionamiento tradicionales que presentamos en el párrafo anterior. En este artículo, proponemos un algoritmo novedoso de agrupamiento de documentos que hereda la superioridad del agrupamiento espectral, es decir, los resultados finales de los grupos también pueden obtenerse explotando la estructura de los autovalores de una matriz simétrica. Sin embargo, a diferencia del agrupamiento espectral, que simplemente impone una restricción de suavidad en las etiquetas de los datos sobre todo el conjunto de datos [2], nuestro método primero construye un predictor de etiquetas lineal regularizado para cada punto de datos a partir de su vecindario como en [25], y luego combina los resultados de todos estos predictores de etiquetas locales con un regularizador de suavidad de etiquetas global. Así que llamamos a nuestro método Agrupamiento con Regularización Local y Global (CLGR). La idea de incorporar tanto información local como global en la <br>predicción de etiquetas</br> está inspirada en los trabajos recientes sobre aprendizaje semi-supervisado [31], y nuestras evaluaciones experimentales en varios conjuntos de datos reales de documentos muestran que CLGR funciona mejor que muchos métodos de agrupamiento de vanguardia. El resto de este documento está organizado de la siguiente manera: en la sección 2 presentaremos nuestro algoritmo CLGR en detalle. Los resultados experimentales en varios conjuntos de datos se presentan en la sección 3, seguidos de las conclusiones y discusiones en la sección 4. El ALGORITMO PROPUESTO En esta sección, presentaremos en detalle nuestro algoritmo de Agrupamiento con Regularización Local y Global (CLGR). Primero veamos cómo se representan los documentos a lo largo de este documento. 2.1 Representación de documentos En nuestro trabajo, todos los documentos se representan mediante vectores de frecuencia de términos ponderados. Sea W = {w1, w2, · · · , wm} el conjunto completo de vocabulario del corpus de documentos (que ha sido preprocesado mediante la eliminación de palabras vacías y operaciones de derivación de palabras). El vector de frecuencia de término xi del documento di se define como xi = [xi1, xi2, · · · , xim]T , xik = tik log n idfk , donde tik es la frecuencia del término wk ∈ W, n es el tamaño del corpus de documentos, idfk es el número de documentos que contienen la palabra wk. De esta manera, xi también se llama la representación TFIDF del documento di. Además, también normalizamos cada xi (1 i n) para que tenga una longitud unitaria, de modo que cada documento esté representado por un vector TF-IDF normalizado. 2.2 Regularización Local Como su nombre sugiere, CLGR está compuesto por dos partes: regularización local y regularización global. En esta subsección introduciremos detalladamente la parte de regularización local. 2.2.1 Motivación Como sabemos que el agrupamiento es un tipo de técnica de aprendizaje, tiene como objetivo organizar el conjunto de datos de una manera razonable. En general, el aprendizaje puede plantearse como un problema de estimación de funciones, a partir del cual podemos obtener una buena función de clasificación que asignará etiquetas al conjunto de datos de entrenamiento e incluso al conjunto de datos de prueba no vistos con algún costo minimizado [24]. Por ejemplo, en el escenario de clasificación de dos clases (en el que conocemos exactamente la etiqueta de cada documento), un clasificador lineal con ajuste de mínimos cuadrados tiene como objetivo aprender un vector columna w de manera que el costo al cuadrado J = 1 n (wT xi − yi)2 (1) se minimice, donde yi ∈ {+1, −1} es la etiqueta de xi. Al tomar ∂J /∂w = 0, obtenemos la solución w∗ = Σ i=1 n xixT i −1 Σ i=1 n xiyi, (2) que puede escribirse en su forma matricial como w∗ = XXT −1 Xy, (3) donde X = [x1, x2, · · · , xn] es una matriz de documentos m × n, y = [y1, y2, · · · , yn]T es el vector de etiquetas. Entonces, para un documento de prueba t, podemos determinar su etiqueta mediante l = signo(w∗T u), donde signo(·) es la función signo. Un problema natural en la ecuación (3) es que la matriz XXT puede ser singular y, por lo tanto, no invertible (por ejemplo, cuando m n). Para evitar tal problema, podemos agregar un término de regularización y minimizar el siguiente criterio J = 1 n n i=1 (wT xi − yi)2 + λ w 2, (5), donde λ es un parámetro de regularización. Entonces, la solución óptima que minimiza J está dada por w∗ = XXT + λnI −1 Xy, (6) donde I es una matriz identidad de tamaño m × m. Se ha informado que el clasificador lineal regularizado puede lograr resultados muy buenos en problemas de clasificación de texto [29]. Sin embargo, a pesar de su éxito empírico, el clasificador lineal regularizado es en realidad un clasificador global, es decir, w∗ se estima utilizando todo el conjunto de entrenamiento. Según [24], esta puede que no sea una idea inteligente, ya que un único w∗ puede que no sea lo suficientemente bueno para predecir las etiquetas de todo el espacio de entrada. Para obtener predicciones más precisas, [6] propuso entrenar clasificadores localmente y utilizarlos para clasificar los puntos de prueba. Por ejemplo, un punto de prueba será clasificado por el clasificador local entrenado utilizando los puntos de entrenamiento ubicados en la vecindad 1. En las siguientes discusiones todos asumimos que los documentos provienen solo de dos clases. Las generalizaciones de nuestro método a casos de múltiples clases se discutirán en la sección 2.5 de él. Aunque este método parece lento y estúpido, se informa que puede obtener mejores rendimientos que usar un clasificador global único en ciertas tareas [6]. 2.2.2 Construyendo los Predictores Localmente Regularizados Inspirados en su éxito, propusimos aplicar los algoritmos de aprendizaje local para el agrupamiento. La idea básica es que, para cada vector de documento xi (1 i n), entrenamos un predictor de etiquetas local basado en su vecindario k-más cercano Ni, y luego lo usamos para predecir la etiqueta de xi. Finalmente combinaremos todos esos predictores locales minimizando la suma de sus errores de predicción. En esta subsección introduciremos cómo construir esos predictores locales. Debido a la simplicidad y efectividad del clasificador lineal regularizado que hemos introducido en la sección 2.2.1, lo elegimos como nuestro predictor de etiquetas local, de modo que para cada documento xi, se minimiza el siguiente criterio Ji = 1 ni xj ∈Ni wT i xj − qj 2 + λi wi 2, (7), donde ni = |Ni| es la cardinalidad de Ni, y qj es la pertenencia al clúster de xj. Luego, utilizando la Ecuación (6), podemos obtener que la solución óptima es w∗ i = XiXT i + λiniI −1 Xiqi, (8), donde Xi = [xi1, xi2, · · · , xini], y usamos xik para denotar al k-ésimo vecino más cercano de xi. qi = [qi1, qi2, · · · , qini]T con qik representando la asignación de clúster de xik. El problema aquí es que XiXT i es una matriz m × m con m ni, es decir, deberíamos calcular la inversa de una matriz m × m para cada vector de documento, lo cual está prohibido computacionalmente. Afortunadamente, tenemos el siguiente teorema: Teorema 1. w∗ i en la Ec. (8) puede ser reescrito como w∗ i = Xi XT i Xi + λiniIi −1 qi, (9), donde Ii es una matriz identidad de tamaño ni × ni. Prueba. Dado que w∗ i = XiXT i + λiniI −1 Xiqi, entonces XiXT i + λiniI w∗ i = Xiqi =⇒ XiXT i w∗ i + λiniw∗ i = Xiqi =⇒ w∗ i = (λini)−1 Xi qi − XT i w∗ i. Sea β = (λini)−1 qi − XT i w∗ i, entonces w∗ i = Xiβ =⇒ λiniβ = qi − XT i w∗ i = qi − XT i Xiβ =⇒ qi = XT i Xi + λiniIi β =⇒ β = XT i Xi + λiniIi −1 qi. Por lo tanto, w∗ i = Xiβ = Xi XT i Xi + λiniIi −1 qi 2. Utilizando el teorema 1, solo necesitamos calcular la inversa de una matriz ni × ni para cada documento para entrenar un predictor de etiquetas local. Además, para un nuevo punto de prueba u que cae en Ni, podemos clasificarlo por el signo de qu = w∗T i u = uT wi = uT Xi XT i Xi + λiniIi −1 qi. Esta es una expresión atractiva ya que podemos determinar la asignación del clúster de u utilizando los productos internos entre los puntos en {u ∪ Ni}, lo que sugiere que un regularizador local de este tipo puede ser fácilmente kernelizado [21] siempre y cuando definamos una función de kernel adecuada. 2.2.3 Combinando los Predictores Regularizados Localmente Una vez que todos los predictores locales hayan sido construidos, los combinaremos minimizando Jl = n i=1 w∗T i xi − qi 2, (10), lo que representa la suma de los errores de predicción de todos los predictores locales. Combinando la Ecuación (10) con la Ecuación (6), podemos obtener Jl = n i=1 w∗T i xi − qi 2 = n i=1 xT i Xi XT i Xi + λiniIi −1 qi − qi 2 = Pq − q 2, (11), donde q = [q1, q2, · · · , qn]T, y P es una matriz n × n construida de la siguiente manera. Sea αi = xT i Xi XT i Xi + λiniIi −1 , entonces Pij = αi j, si xj ∈ Ni 0, de lo contrario , (12) donde Pij es la entrada (i, j)-ésima de P, y αi j representa la entrada j-ésima de αi. Hasta ahora podemos escribir el criterio de agrupamiento combinando predictores de etiquetas lineales localmente regularizados Jl en una forma matemática explícita, y podemos minimizarlo directamente utilizando algunas técnicas estándar de optimización. Sin embargo, los resultados pueden no ser lo suficientemente buenos ya que solo explotamos la información local del conjunto de datos. En la siguiente subsección, introduciremos un criterio de regularización global y lo combinaremos con Jl, que tiene como objetivo encontrar un buen resultado de agrupamiento de manera local-global. 2.3 Regularización Global En el agrupamiento de datos, generalmente requerimos que las asignaciones de clúster de los puntos de datos sean lo suficientemente suaves con respecto a la variedad de datos subyacente, lo que implica (1) que los puntos cercanos tienden a tener las mismas asignaciones de clúster; (2) que los puntos en la misma estructura (por ejemplo, subvariedad o clúster) tienden a tener las mismas asignaciones de clúster [31]. Sin pérdida de generalidad, asumimos que los puntos de datos residen (aproximadamente) en una variedad de baja dimensión M2, y q es la función de asignación de clúster definida en M, es decir, 2 Creemos que los datos de texto también se muestrean de alguna variedad de baja dimensión, ya que es imposible que para todos los x ∈ M, q(x) devuelva la membresía del clúster de x. La suavidad de q sobre M se puede calcular mediante la siguiente integral de Dirichlet [2] D[q] = 1 2 M q(x) 2 dM, (13) donde el gradiente q es un vector en el espacio tangente T Mx, y la integral se toma con respecto a la medida estándar en M. Si restringimos la escala de q por q, q M = 1 (donde ·, · M es el producto interno inducido en M), entonces resulta que encontrar la función más suave que minimiza D[q] se reduce a encontrar las autofunciones del operador Laplace Beltrami L, que se define como Lq −div q, (14) donde div es la divergencia de un campo vectorial. Generalmente, el gráfico se puede ver como la forma discretizada de una variedad. Podemos modelar el conjunto de datos como un grafo ponderado no dirigido como en el agrupamiento espectral [22], donde los nodos del grafo son simplemente los puntos de datos, y los pesos en las aristas representan las similitudes entre pares de puntos. Entonces se puede demostrar que minimizar la Ec. (13) corresponde a minimizar Jg = qT Lq = n i=1 (qi − qj)2 wij, (15), donde q = [q1, q2, · · · , qn]T con qi = q(xi), L es el Laplaciano del grafo con su entrada (i, j)-ésima Lij =    di − wii, si i = j −wij, si xi y xj son adyacentes 0, en otro caso, (16), donde di = j wij es el grado de xi, wij es la similitud entre xi y xj. Si xi y xj son adyacentes, wij generalmente se calcula de la siguiente manera: wij = e − xi−xj 2 2σ2, donde σ es un parámetro dependiente del conjunto de datos. Se ha demostrado que bajo ciertas condiciones, tal forma de wij para determinar los pesos en los bordes del grafo conduce a la convergencia del Laplaciano del grafo al operador Laplace Beltrami [3][18]. En resumen, el uso de la Ec. (15) con pesos exponenciales puede medir de manera efectiva la suavidad de las asignaciones de datos con respecto a la variedad intrínseca de datos. Por lo tanto, lo adoptamos como un regularizador global para penalizar la suavidad de las asignaciones de datos predichas. 2.4 Agrupamiento con Regularización Local y Global Combinando los contenidos que hemos introducido en la sección 2.2 y la sección 2.3, podemos derivar que el criterio de agrupamiento es minq J = Jl + λJg = Pq − q 2 + λqT Lq sujeto a qi ∈ {−1, +1}, (18) donde P está definido como en la Ecuación (12), y λ es un parámetro de regularización para equilibrar Jl y Jg. Sin embargo, los valores discretos llenan todo el espacio muestral de alta dimensionalidad. Y se ha demostrado que los métodos basados en variedades pueden lograr buenos resultados en tareas de clasificación de texto [31]. En este artículo, definimos xi y xj como adyacentes si xi ∈ N(xj) o xj ∈ N(xi). La restricción de pi convierte el problema en un problema de programación entera NP difícil. Una forma natural de hacer que el problema sea resoluble es eliminar la restricción y relajar qi para que sea continuo, luego el objetivo que buscamos minimizar se convierte en J = Pq − q 2 + λqT Lq = qT (P − I)T (P − I)q + λqT Lq = qT (P − I)T (P − I) + λL q, (19) y luego agregamos una restricción adicional qT q = 1 para restringir la escala de q. Entonces nuestro objetivo se convierte en minq J = qT (P − I)T (P − I) + λL q s.t. qT q = 1 (20) Utilizando el método de Lagrange, podemos derivar que la solución óptima q corresponde al menor vector propio de la matriz M = (P − I)T (P − I) + λL, y la asignación de clúster de xi puede determinarse por el signo de qi, es decir, xi se clasificará como clase uno si qi > 0, de lo contrario se clasificará como clase 2. 2.5 CLGR Multiclase En lo anterior hemos introducido el marco básico de Agrupamiento con Regularización Local y Global (CLGR) para el problema de agrupamiento de dos clases, y lo extenderemos al agrupamiento multiclase en esta subsección. Primero asumimos que todos los documentos pertenecen a C clases indexadas por L = {1, 2, · · · , C}. qc es la función de clasificación para la clase c (1 ≤ c ≤ C), tal que qc(xi) devuelve la confianza de que xi pertenece a la clase c. Nuestro objetivo es obtener el valor de qc(xi) (1 ≤ c ≤ C, 1 ≤ i ≤ n), y la asignación de cluster de xi puede ser determinada por {qc(xi)}C c=1 utilizando algunos métodos adecuados de discretización que presentaremos más adelante. Por lo tanto, en este caso de múltiples clases, para cada documento xi (1 i n), construiremos C predictores de etiquetas regularizados localmente lineales cuyos vectores normales son wc∗ i = Xi XT i Xi + λiniIi −1 qc i (1 c C), (21), donde Xi = [xi1, xi2, · · · , xini ] con xik siendo el k-ésimo vecino de xi, y qc i = [qc i1, qc i2, · · · , qc ini ]T con qc ik = qc (xik). Entonces (wc∗ i )T xi devuelve la confianza predicha de xi perteneciente a la clase c. Por lo tanto, el error de predicción local para la clase c se puede definir como J c l = n i=1 (wc∗ i ) T xi − qc i 2 , (22) Y el error de predicción local total se convierte en Jl = C c=1 J c l = C c=1 n i=1 (wc∗ i ) T xi − qc i 2 . (23) Como en la Ec. (11), podemos definir una matriz n×n P (ver Ec. (12)) y reescribir Jl como Jl = C c=1 J c l = C c=1 Pqc − qc 2 . (24) De manera similar, podemos definir el regularizador de suavidad global en el caso de múltiples clases como Jg = C c=1 n i=1 (qc i − qc j )2 wij = C c=1 (qc )T Lqc . (25) Luego, el criterio a minimizar para CLGR en el caso de múltiples clases se convierte en J = Jl + λJg = C c=1 Pqc − qc 2 + λ(qc )T Lqc = C c=1 (qc )T (P − I)T (P − I) + λL qc = trace QT (P − I)T (P − I) + λL Q , (26) donde Q = [q1 , q2 , · · · , qc ] es una matriz n × c, y trace(·) devuelve la traza de una matriz. Al igual que en la ecuación (20), también agregamos la restricción de que QT Q = I para limitar la escala de Q. Entonces nuestro problema de optimización se convierte en minQ J = traza QT (P − I)T (P − I) + λL Q sujeto a. Según el teorema de Ky Fan [28], sabemos que la solución óptima del problema anterior es Q∗ = [q∗ 1, q∗ 2, · · · , q∗ C ]R, donde q∗ k (1 ≤ k ≤ C) es el autovector que corresponde al k-ésimo valor propio más pequeño de la matriz (P − I)T (P − I) + λL, y R es una matriz arbitraria de tamaño C × C. Dado que los valores de las entradas en Q∗ son continuos, necesitamos discretizar aún más Q∗ para obtener las asignaciones de clúster de todos los puntos de datos. Principalmente hay dos enfoques para lograr este objetivo: 1. Como en [20], podemos tratar la i-ésima fila de Q como la incrustación de xi en un espacio de C dimensiones, y aplicar algunos métodos de agrupamiento tradicionales como kmeans para agrupar estas incrustaciones en C grupos. 2. Dado que el Q∗ óptimo no es único (debido a la existencia de una matriz R arbitraria), podemos buscar un R óptimo que rote Q∗ a una matriz de indicación4. El algoritmo detallado se puede consultar en [26]. El procedimiento detallado del algoritmo para CLGR se resume en la tabla 1. 3. EXPERIMENTOS En esta sección, se realizan experimentos para comparar empíricamente los resultados de agrupamiento de CLGR con otros 8 algoritmos representativos de agrupamiento de documentos en 5 conjuntos de datos. Primero presentaremos la información básica de esos conjuntos de datos. 3.1 Conjuntos de datos Utilizamos una variedad de conjuntos de datos, la mayoría de los cuales son frecuentemente utilizados en la investigación de recuperación de información. La Tabla 2 resume las características de los conjuntos de datos. Aquí, una matriz de indicación T es una matriz n×c con su entrada (i, j) Tij ∈ {0, 1} de modo que para cada fila de Q∗ solo hay un 1. Entonces, el xi puede ser asignado al j-ésimo grupo tal que j = argjQ∗ ij = 1. Tabla 1: Agrupamiento con Regularización Local y Global (CLGR) Entrada: 1. Conjunto de datos X = {xi}n i=1; 2. Número de grupos C: 3. Tamaño del vecindario K: 4. Parámetros locales de regularización {λi}n i=1; 5. Parámetro de regularización global λ; Salida: La membresía del clúster de cada punto de datos. Procedimiento: 1. Construir los K vecindarios más cercanos para cada punto de datos; 2. Construya la matriz P utilizando la Ecuación (12); 3. Construya la matriz Laplaciana L utilizando la ecuación (16); 4. Construye la matriz M = (P − I)T (P − I) + λL; 5. Realice la descomposición de valores propios en M y construya la matriz Q∗ de acuerdo con la Ecuación (28); 6. Generar las asignaciones de clúster de cada punto de datos al discretizar adecuadamente Q∗. Tabla 2: Descripciones de los conjuntos de datos de documentos Conjuntos de datos Número de documentos Número de clases CSTR 476 4 WebKB4 4199 4 Reuters 2900 10 WebACE 2340 20 Newsgroup4 3970 4 CSTR. Este es el conjunto de datos de los resúmenes de informes técnicos publicados en el Departamento de Ciencias de la Computación de una universidad. El conjunto de datos contenía 476 resúmenes, los cuales fueron divididos en cuatro áreas de investigación: Procesamiento de Lenguaje Natural (NLP), Robótica/Visión, Sistemas y Teoría. WebKB. El conjunto de datos WebKB contiene páginas web recopiladas de los departamentos de informática de universidades. Hay alrededor de 8280 documentos y están divididos en 7 categorías: estudiante, facultad, personal, curso, proyecto, departamento y otros. El texto sin procesar es de aproximadamente 27MB. Entre estas 7 categorías, estudiante, facultad, curso y proyecto son las cuatro categorías que representan entidades más pobladas. El subconjunto asociado suele ser llamado WebKB4. Reuters. La colección de pruebas de categorización de texto Reuters-21578 contiene documentos recopilados de la agencia de noticias Reuters en 1987. Es un punto de referencia estándar para la categorización de texto y contiene 135 categorías. En nuestros experimentos, utilizamos un subconjunto de la colección de datos que incluye las 10 categorías más frecuentes entre los 135 temas y lo llamamos Reuters-top 10. WebACE. El conjunto de datos WebACE proviene del proyecto WebACE y ha sido utilizado para la agrupación de documentos [17][5]. El conjunto de datos WebACE contiene 2340 documentos que consisten en artículos de noticias del servicio de noticias de Reuters a través de la web en octubre de 1997. Estos documentos están divididos en 20 clases. Not enough context provided. El conjunto de datos News4 utilizado en nuestros experimentos se selecciona del famoso conjunto de datos 20-newsgroups. El tema rec que contiene autos, motocicletas, béisbol y hockey fue seleccionado de la versión 20news-18828. El conjunto de datos News4 contiene 3970 vectores de documentos. Para preprocesar los conjuntos de datos, eliminamos las palabras vacías utilizando una lista estándar de palabras vacías, se omiten todas las etiquetas HTML y se ignoran todos los campos de encabezado excepto el asunto y la organización de los artículos publicados. En todos nuestros experimentos, primero seleccionamos las 1000 palabras principales por información mutua con las etiquetas de clase. 3.2 Métricas de Evaluación En los experimentos, establecemos el número de clústeres igual al número real de clases C para todos los algoritmos de agrupamiento. Para evaluar su rendimiento, comparamos los grupos generados por estos algoritmos con las clases reales mediante el cálculo de las siguientes dos medidas de rendimiento. Precisión de agrupamiento (Acc). La primera medida de rendimiento es la Precisión de Agrupamiento, que descubre la relación uno a uno entre los grupos y las clases, y mide en qué medida cada grupo contenía puntos de datos de la clase correspondiente. Resume todo el grado de coincidencia entre todos los pares de clases y clusters. La precisión del agrupamiento se puede calcular como: Acc = 1 N max   Ck,Lm T(Ck, Lm)   , (29) donde Ck denota el k-ésimo clúster en los resultados finales, y Lm es la verdadera clase m-ésima. T(Ck, Lm) es el número de entidades que pertenecen a la clase m y están asignadas al clúster k. La precisión calcula la suma máxima de T(Ck, Lm) para todos los pares de clústeres y clases, y estos pares no tienen superposiciones. La mayor precisión de agrupamiento significa un mejor rendimiento de agrupamiento. Información Mutua Normalizada (NMI). Otro métrica de evaluación que adoptamos aquí es la Información Mutua Normalizada (NMI) [23], la cual es ampliamente utilizada para determinar la calidad de los grupos. Para dos variables aleatorias X e Y, el NMI se define como: NMI(X, Y) = I(X, Y) H(X)H(Y), donde I(X, Y) es la información mutua entre X e Y, mientras que H(X) y H(Y) son las entropías de X e Y respectivamente. Se puede ver que NMI(X, X) = 1, que es el valor máximo posible de NMI. Dado un resultado de agrupamiento, el NMI en la ecuación (30) se estima como NMI = C k=1 C m=1 nk,mlog n·nk,m nk ˆnm C k=1 nklog nk n C m=1 ˆnmlog ˆnm n, (31), donde nk denota el número de datos contenidos en el grupo Ck (1 k C), ˆnm es el número de datos pertenecientes a la clase m-ésima (1 m C), y nk,m denota el número de datos que están en la intersección entre el grupo Ck y la clase m-ésima. El valor calculado en la Ecuación (31) se utiliza como medida de rendimiento para el resultado de agrupamiento dado. A mayor valor, mejor rendimiento de agrupamiento. 3.3 Comparaciones Hemos realizado evaluaciones de rendimiento exhaustivas probando nuestro método y comparándolo con otros 8 métodos representativos de agrupamiento de datos utilizando las mismas corpora de datos. Los algoritmos que evaluamos se enumeran a continuación. 1. K-means tradicional (KM). 2. K-medias esférico (SKM). La implementación se basa en [9]. 3. Modelo de Mezcla Gaussiana (GMM). La implementación se basa en [16]. 4. Agrupamiento espectral con cortes normalizados (Ncut). La implementación se basa en [26], y la varianza de la similitud gaussiana se determina mediante Escalado Local [30]. Ten en cuenta que el criterio que Ncut busca minimizar es simplemente el regularizador global en nuestro algoritmo CLGR, excepto que Ncut utiliza el Laplaciano normalizado. 5. Agrupamiento utilizando Regularización Local Pura (CPLR). En este método simplemente minimizamos Jl (definido en la Ecuación (24)), y los resultados de agrupamiento pueden obtenerse realizando la descomposición de valores propios en la matriz (I − P)T (I − P) con algunos métodos de discretización adecuados. 6. Iteración de subespacio adaptativa (ASI). La implementación se basa en [14]. 7. Factorización de matrices no negativas (NMF). La implementación se basa en [27]. 8. Factorización Tri-Factorización de Matrices No Negativas (TNMF) [12]. La implementación se basa en [15]. Para la eficiencia computacional, en la implementación de CPLR y nuestro algoritmo CLGR, hemos establecido todos los parámetros de regularización local {λi}n i=1 para ser idénticos, los cuales se establecen mediante búsqueda en cuadrícula de {0.1, 1, 10}. El tamaño de los vecindarios k más cercanos se establece mediante una búsqueda en cuadrícula de {20, 40, 80}. Para el método CLGR, su parámetro de regularización global se establece mediante una búsqueda en cuadrícula de {0.1, 1, 10}. Al construir el regularizador global, hemos adoptado el método de escalamiento local [30] para construir la matriz Laplaciana. El método de discretización final adoptado en estos dos métodos es el mismo que en [26], ya que nuestros experimentos muestran que el uso de dicho método puede lograr mejores resultados que el uso de métodos basados en kmeans como en [20]. 3.4 Resultados Experimentales Los resultados de comparación de precisión de agrupamiento se muestran en la tabla 3, y los resultados de comparación de información mutua normalizada se resumen en la tabla 4. De las dos tablas principalmente observamos que: 1. Nuestro método CLGR supera a todos los demás métodos de agrupamiento de documentos en la mayoría de los conjuntos de datos. Para la agrupación de documentos, el método Spherical k-means suele superar al método tradicional de agrupación k-means, y el método GMM puede lograr resultados competitivos en comparación con el método Spherical k-means; 3. Los resultados obtenidos de los algoritmos tipo k-means y GMM suelen ser peores que los resultados obtenidos de Clustering Espectral. Dado que el Agrupamiento Espectral puede ser visto como una versión ponderada del kernel k-means, puede obtener buenos resultados cuando los clusters de datos tienen formas arbitrarias. Esto corrobora que los vectores de los documentos no están distribuidos regularmente (esféricos o elípticos). 4. Las comparaciones experimentales verifican empíricamente la equivalencia entre NMF y Clustering Espectral, como se muestra en la Tabla 3: Precisión de agrupamiento de los diversos métodos CSTR WebKB4 Reuters WebACE News4 KM 0.4256 0.3888 0.4448 0.4001 0.3527 SKM 0.4690 0.4318 0.5025 0.4458 0.3912 GMM 0.4487 0.4271 0.4897 0.4521 0.3844 NMF 0.5713 0.4418 0.4947 0.4761 0.4213 Ncut 0.5435 0.4521 0.4896 0.4513 0.4189 ASI 0.5621 0.4752 0.5235 0.4823 0.4335 TNMF 0.6040 0.4832 0.5541 0.5102 0.4613 CPLR 0.5974 0.5020 0.4832 0.5213 0.4890 CLGR 0.6235 0.5228 0.5341 0.5376 0.5102 Tabla 4: Resultados de información mutua normalizada de los diversos métodos CSTR WebKB4 Reuters WebACE News4 KM 0.3675 0.3023 0.4012 0.3864 0.3318 SKM 0.4027 0.4155 0.4587 0.4003 0.4085 GMM 0.4034 0.4093 0.4356 0.4209 0.3994 NMF 0.5235 0.4517 0.4402 0.4359 0.4130 Ncut 0.4833 0.4497 0.4392 0.4289 0.4231 ASI 0.5008 0.4833 0.4769 0.4817 0.4503 TNMF 0.5724 0.5011 0.5132 0.5328 0.4749 CPLR 0.5695 0.5231 0.4402 0.5543 0.4690 CLGR 0.6012 0.5434 0.4935 0.5390 0.4908 ha sido demostrado teóricamente en [10]. Se puede observar en las tablas que NMF y el Agrupamiento Espectral suelen llevar a resultados de agrupamiento similares. 5. Los métodos basados en co-agrupamiento (TNMF y ASI) suelen lograr mejores resultados que los métodos tradicionales basados únicamente en vectores de documentos. Dado que estos métodos realizan una selección implícita de características en cada iteración, proporcionan una métrica adaptativa para medir el vecindario y, por lo tanto, tienden a producir mejores resultados de agrupamiento. 6. Los resultados obtenidos a través de CPLR suelen ser mejores que los resultados obtenidos a través de Agrupamiento Espectral, lo cual respalda la teoría de Vapnik [24] de que a veces los algoritmos de aprendizaje local pueden obtener mejores resultados que los algoritmos de aprendizaje global. Además de los experimentos de comparación mencionados anteriormente, también probamos la sensibilidad de los parámetros de nuestro método. Principalmente hay dos conjuntos de parámetros en nuestro algoritmo CLGR, los parámetros de regularización locales y globales ({λi}n i=1 y λ, como hemos mencionado en la sección 3.3, hemos establecido que todos los λis son idénticos a λ∗ en nuestros experimentos), y el tamaño de los vecindarios. Por lo tanto, también hemos realizado dos series de experimentos: 1. Fijando el tamaño de los vecindarios y probando el rendimiento del agrupamiento con diferentes valores de λ∗ y λ. En este conjunto de experimentos, encontramos que nuestro algoritmo CLGR puede lograr buenos resultados cuando los dos parámetros de regularización no son ni demasiado grandes ni demasiado pequeños. Normalmente nuestro método puede lograr buenos resultados cuando λ∗ y λ están alrededor de 0.1. La Figura 1 nos muestra un ejemplo de prueba en el conjunto de datos WebACE. Fijando los parámetros de regularización local y global, y probando el rendimiento del agrupamiento con diferentes valores de -5, -4.5, -4, -3.5, -3, -5, -4.5, -4, -3.5, -3, 0.35, 0.4, 0.45, 0.5, 0.55 para el parámetro de regularización local (valor logarítmico base 2) y para el parámetro de regularización global (valor logarítmico base 2), se obtuvo una precisión de agrupamiento. Figura 1: Resultados de la prueba de sensibilidad de parámetros en el conjunto de datos WebACE con el tamaño de vecindario fijo en 20, donde el eje x e y representan el valor logarítmico base 2 de λ∗ y λ. En este conjunto de experimentos, encontramos que el vecindario con un tamaño demasiado grande o demasiado pequeño deteriorará todos los resultados finales de agrupamiento. Esto puede entenderse fácilmente ya que cuando el tamaño del vecindario es muy pequeño, los puntos de datos utilizados para entrenar los clasificadores locales pueden no ser suficientes; cuando el tamaño del vecindario es muy grande, los clasificadores entrenados tenderán a ser globales y no podrán capturar las características locales típicas. La Figura 2 nos muestra un ejemplo de prueba en el conjunto de datos WebACE. Por lo tanto, podemos ver que nuestro algoritmo CLGR (1) puede lograr resultados satisfactorios y (2) no es muy sensible a la elección de parámetros, lo que lo hace práctico en aplicaciones del mundo real. CONCLUSIONES Y TRABAJOS FUTUROS En este artículo, desarrollamos un nuevo algoritmo de agrupamiento llamado agrupamiento con regularización local y global. Nuestro método conserva el mérito de los algoritmos de aprendizaje local y el agrupamiento espectral. Nuestros experimentos muestran que el algoritmo propuesto supera a la mayoría de los algoritmos de vanguardia en muchos conjuntos de datos de referencia. En el futuro, nos enfocaremos en la selección de parámetros y los problemas de aceleración del algoritmo CLGR. REFERENCIAS [1] L. Baker y A. McCallum. Agrupamiento distribucional de palabras para clasificación de texto. En Actas de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1998. [2] M. Belkin y P. Niyogi. Mapeo de Eigen de Laplaciano para Reducción de Dimensionalidad y Representación de Datos. Computación Neural, 15 (6):1373-1396. Junio de 2003. [3] M. Belkin y P. Niyogi. Hacia una Fundación Teórica para Métodos de Manifold Basados en Laplacianos. En Actas de la 18ª Conferencia sobre Teoría del Aprendizaje (COLT). 2005. 10 20 30 40 50 60 70 80 90 100 0.35 0.4 0.45 0.5 0.55 tamaño de la precisión de agrupamiento del vecindario Figura 2: Resultados de pruebas de sensibilidad de parámetros en el conjunto de datos WebACE con los parámetros de regularización fijados en 0.1, y el tamaño del vecindario variando de 10 a 100. [4] M. Belkin, P. Niyogi y V. Sindhwani. Regularización de variedades: un marco geométrico para el aprendizaje a partir de ejemplos. Revista de Investigación en Aprendizaje Automático 7, 1-48, 2006. [5] D. Boley. División Direccional Principal. Minería de datos y descubrimiento de conocimiento, 2:325-344, 1998. [6] L. Bottou y V. Vapnik. Algoritmos de aprendizaje local. Computación Neural, 4:888-900, 1992. [7] P. K. Chan, D. F. Schlag y J. Y. Zien. Particionamiento y agrupamiento K-way de corte de razón espectral. IEEE Trans. Diseño Asistido por Computadora, 13:1088-1096, Sep. 1994. [8] D. R. Cutting, D. R. Karger, J. O. Pederson y J. W. Tukey. Dispersión/Recolección: Un enfoque basado en clústeres para explorar grandes colecciones de documentos. En Actas de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1992. [9] I. S. Dhillon y D. S. Modha. Descomposiciones de conceptos para datos de texto grandes y dispersos utilizando agrupamiento. Aprendizaje automático, vol. 42(1), páginas 143-175, enero de 2001. [10] C. Ding, X. Él, y H. Simon. Sobre la equivalencia entre la factorización de matrices no negativas y el agrupamiento espectral. En Actas de la Conferencia de Minería de Datos de SIAM, 2005. [11] C. Ding, X. Él, H. Zha, M. Gu y H. D. Simon. Un algoritmo de corte min-max para particionar grafos y agrupar datos. En Proc. de la 1ra Conferencia Internacional sobre Minería de Datos (ICDM), páginas 107-114, 2001. [12] C. Ding, T. Li, W. Peng y H. Park. Tri-factorizaciones de matrices no negativas ortogonales para agrupamiento. En Actas de la Duodécima Conferencia Internacional de la ACM SIGKDD sobre Descubrimiento de Conocimiento y Minería de Datos, 2006. [13] R. O. Duda, P. E. Hart y D. G. Stork. Clasificación de patrones. John Wiley & Sons, Inc., 2001. [14] T. Li, S. Ma y M. Ogihara. Agrupamiento de documentos a través de la Iteración de Subespacios Adaptativos. En Actas de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2004. [15] T. Li y C. Ding. Las relaciones entre varios métodos de factorización de matrices no negativas para agrupamiento. En Actas de la 6ta Conferencia Internacional sobre Minería de Datos (ICDM). 2006. [16] X. Liu y Y. Gong. Agrupación de documentos con capacidades de refinamiento de clústeres y selección de modelos. En Proc. de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2002. [17] E. Han, D. Boley, M. Gini, R. Gross, K. Hastings, G. Karypis, V. Kumar, B. Mobasher y J. Moore. WebACE: Un agente web para la categorización y exploración de documentos. En Actas de la 2ª Conferencia Internacional sobre Agentes Autónomos (Agents98). ACM Press, 1998. [18] M. Hein, J. Y. Audibert y U. von Luxburg. De Gráficas a Variedades - Consistencia Puntual Débil y Fuerte de los Laplacianos de Gráficas. En Actas de la 18ª Conferencia sobre Teoría del Aprendizaje (COLT), 470-485. 2005. [19] J. Él, M. Lan, C.-L. Tan, S.-Y. Sung, y H.-B. Bajo. Inicialización de algoritmos de refinamiento de clústeres: una revisión y estudio comparativo. En Proc. de Inter. Conferencia Conjunta sobre Redes Neuronales, 2004. [20] A. Y. Ng, M. I. Jordan, Y. Weiss. En el agrupamiento espectral: análisis y un algoritmo. En Avances en Sistemas de Procesamiento de Información Neural 14. 2002. [21] B. Sch¨olkopf y A. Smola. Aprendizaje con Kernels. El MIT Press. Cambridge, Massachusetts. 2002. [22] J. Shi y J. Malik. Cortes normalizados y segmentación de imágenes. IEEE Trans. on Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000. [23] A. Strehl and J. Ghosh.\nIEEE Trans. on Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000. [23] A. Strehl y J. Ghosh. Conjuntos de agrupamientos: un marco de reutilización de conocimiento para combinar múltiples particiones. Revista de Investigación de Aprendizaje Automático, 3:583-617, 2002. [24] V. N. Vapnik. La naturaleza de la teoría del aprendizaje estadístico. Berlín: Springer-Verlag, 1995. [25] Wu, M. y Schölkopf, B. Un enfoque de aprendizaje local para el agrupamiento. En Avances en Sistemas de Procesamiento de Información Neural 18. 2006. [26] S. X. Yu, J. Shi. Agrupamiento espectral multiclase. En Actas de la Conferencia Internacional sobre Visión por Computadora, 2003. [27] W. Xu, X. Liu y Y. Gong. Agrupación de documentos basada en la factorización de matrices no negativas. En Actas de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2003. [28] H. Zha, X. Él, C. Ding, M. Gu y H. Simon. Relajación espectral para el agrupamiento K-means. En NIPS 14. 2001. [29] T. Zhang y F. J. Oles. Categorización de texto basada en métodos de clasificación lineal regularizados. Revista de Recuperación de Información, 4:5-31, 2001. [30] L. Zelnik-Manor y P. Perona. Agrupamiento espectral autoajustable. En NIPS 17. 2005. [31] D. Zhou, O. Bousquet, T. N. Lal, J. Weston y B. Schölkopf. Aprendizaje con Consistencia Local y Global. NIPS 17, 2005. \n\nNIPS 17, 2005. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "function estimation": {
            "translated_key": "estimación de funciones",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Regularized Clustering for Documents ∗ Fei Wang, Changshui Zhang State Key Lab of Intelligent Tech.",
                "and Systems Department of Automation, Tsinghua University Beijing, China, 100084 feiwang03@gmail.com Tao Li School of Computer Science Florida International University Miami, FL 33199, U.S.A. taoli@cs.fiu.edu ABSTRACT In recent years, document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering.",
                "In this paper, we propose a novel method for clustering documents using regularization.",
                "Unlike traditional globally regularized clustering methods, our method first construct a local regularized linear label predictor for each document vector, and then combine all those local regularizers with a global smoothness regularizer.",
                "So we call our algorithm Clustering with Local and Global Regularization (CLGR).",
                "We will show that the cluster memberships of the documents can be achieved by eigenvalue decomposition of a sparse symmetric matrix, which can be efficiently solved by iterative methods.",
                "Finally our experimental evaluations on several datasets are presented to show the superiorities of CLGR over traditional document clustering methods.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Clustering; I.2.6 [Artificial Intelligence]: Learning-Concept Learning General Terms Algorithms 1.",
                "INTRODUCTION Document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering.",
                "A good document clustering approach can assist the computers to automatically organize the document corpus into a meaningful cluster hierarchy for efficient browsing and navigation, which is very valuable for complementing the deficiencies of traditional information retrieval technologies.",
                "As pointed out by [8], the information retrieval needs can be expressed by a spectrum ranged from narrow keyword-matching based search to broad information browsing such as what are the major international events in recent months.",
                "Traditional document retrieval engines tend to fit well with the search end of the spectrum, i.e. they usually provide specified search for documents matching the users query, however, it is hard for them to meet the needs from the rest of the spectrum in which a rather broad or vague information is needed.",
                "In such cases, efficient browsing through a good cluster hierarchy will be definitely helpful.",
                "Generally, document clustering methods can be mainly categorized into two classes: hierarchical methods and partitioning methods.",
                "The hierarchical methods group the data points into a hierarchical tree structure using bottom-up or top-down approaches.",
                "For example, hierarchical agglomerative clustering (HAC) [13] is a typical bottom-up hierarchical clustering method.",
                "It takes each data point as a single cluster to start off with and then builds bigger and bigger clusters by grouping similar data points together until the entire dataset is encapsulated into one final cluster.",
                "On the other hand, partitioning methods decompose the dataset into a number of disjoint clusters which are usually optimal in terms of some predefined criterion functions.",
                "For instance, K-means [13] is a typical partitioning method which aims to minimize the sum of the squared distance between the data points and their corresponding cluster centers.",
                "In this paper, we will focus on the partitioning methods.",
                "As we know that there are two main problems existing in partitioning methods (like Kmeans and Gaussian Mixture Model (GMM) [16]): (1) the predefined criterion is usually non-convex which causes many local optimal solutions; (2) the iterative procedure (e.g. the Expectation Maximization (EM) algorithm) for optimizing the criterions usually makes the final solutions heavily depend on the initializations.",
                "In the last decades, many methods have been proposed to overcome the above problems of the partitioning methods [19][28].",
                "Recently, another type of partitioning methods based on clustering on data graphs have aroused considerable interests in the machine learning and data mining community.",
                "The basic idea behind these methods is to first model the whole dataset as a weighted graph, in which the graph nodes represent the data points, and the weights on the edges correspond to the similarities between pairwise points.",
                "Then the cluster assignments of the dataset can be achieved by optimizing some criterions defined on the graph.",
                "For example Spectral Clustering is one kind of the most representative graph-based clustering approaches, it generally aims to optimize some cut value (e.g.",
                "Normalized Cut [22], Ratio Cut [7], Min-Max Cut [11]) defined on an undirected graph.",
                "After some relaxations, these criterions can usually be optimized via eigen-decompositions, which is guaranteed to be global optimal.",
                "In this way, spectral clustering efficiently avoids the problems of the traditional partitioning methods as we introduced in last paragraph.",
                "In this paper, we propose a novel document clustering algorithm that inherits the superiority of spectral clustering, i.e. the final cluster results can also be obtained by exploit the eigen-structure of a symmetric matrix.",
                "However, unlike spectral clustering, which just enforces a smoothness constraint on the data labels over the whole data manifold [2], our method first construct a regularized linear label predictor for each data point from its neighborhood as in [25], and then combine the results of all these local label predictors with a global label smoothness regularizer.",
                "So we call our method Clustering with Local and Global Regularization (CLGR).",
                "The idea of incorporating both local and global information into label prediction is inspired by the recent works on semi-supervised learning [31], and our experimental evaluations on several real document datasets show that CLGR performs better than many state-of-the-art clustering methods.",
                "The rest of this paper is organized as follows: in section 2 we will introduce our CLGR algorithm in detail.",
                "The experimental results on several datasets are presented in section 3, followed by the conclusions and discussions in section 4. 2.",
                "THE PROPOSED ALGORITHM In this section, we will introduce our Clustering with Local and Global Regularization (CLGR) algorithm in detail.",
                "First lets see the how the documents are represented throughout this paper. 2.1 Document Representation In our work, all the documents are represented by the weighted term-frequency vectors.",
                "Let W = {w1, w2, · · · , wm} be the complete vocabulary set of the document corpus (which is preprocessed by the stopwords removal and words stemming operations).",
                "The term-frequency vector xi of document di is defined as xi = [xi1, xi2, · · · , xim]T , xik = tik log n idfk , where tik is the term frequency of wk ∈ W, n is the size of the document corpus, idfk is the number of documents that contain word wk.",
                "In this way, xi is also called the TFIDF representation of document di.",
                "Furthermore, we also normalize each xi (1 i n) to have a unit length, so that each document is represented by a normalized TF-IDF vector. 2.2 Local Regularization As its name suggests, CLGR is composed of two parts: local regularization and global regularization.",
                "In this subsection we will introduce the local regularization part in detail. 2.2.1 Motivation As we know that clustering is one type of learning techniques, it aims to organize the dataset in a reasonable way.",
                "Generally speaking, learning can be posed as a problem of <br>function estimation</br>, from which we can get a good classification function that will assign labels to the training dataset and even the unseen testing dataset with some cost minimized [24].",
                "For example, in the two-class classification scenario1 (in which we exactly know the label of each document), a linear classifier with least square fit aims to learn a column vector w such that the squared cost J = 1 n (wT xi − yi)2 (1) is minimized, where yi ∈ {+1, −1} is the label of xi.",
                "By taking ∂J /∂w = 0, we get the solution w∗ = n i=1 xixT i −1 n i=1 xiyi , (2) which can further be written in its matrix form as w∗ = XXT −1 Xy, (3) where X = [x1, x2, · · · , xn] is an m × n document matrix, y = [y1, y2, · · · , yn]T is the label vector.",
                "Then for a test document t, we can determine its label by l = sign(w∗T u), (4) where sign(·) is the sign function.",
                "A natural problem in Eq. (3) is that the matrix XXT may be singular and thus not invertable (e.g. when m n).",
                "To avoid such a problem, we can add a regularization term and minimize the following criterion J = 1 n n i=1 (wT xi − yi)2 + λ w 2 , (5) where λ is a regularization parameter.",
                "Then the optimal solution that minimize J is given by w∗ = XXT + λnI −1 Xy, (6) where I is an m × m identity matrix.",
                "It has been reported that the regularized linear classifier can achieve very good results on text classification problems [29].",
                "However, despite its empirical success, the regularized linear classifier is on earth a global classifier, i.e. w∗ is estimated using the whole training set.",
                "According to [24], this may not be a smart idea, since a unique w∗ may not be good enough for predicting the labels of the whole input space.",
                "In order to get better predictions, [6] proposed to train classifiers locally and use them to classify the testing points.",
                "For example, a testing point will be classified by the local classifier trained using the training points located in the vicinity 1 In the following discussions we all assume that the documents coming from only two classes.",
                "The generalizations of our method to multi-class cases will be discussed in section 2.5. of it.",
                "Although this method seems slow and stupid, it is reported that it can get better performances than using a unique global classifier on certain tasks [6]. 2.2.2 Constructing the Local Regularized Predictors Inspired by their success, we proposed to apply the local learning algorithms for clustering.",
                "The basic idea is that, for each document vector xi (1 i n), we train a local label predictor based on its k-nearest neighborhood Ni, and then use it to predict the label of xi.",
                "Finally we will combine all those local predictors by minimizing the sum of their prediction errors.",
                "In this subsection we will introduce how to construct those local predictors.",
                "Due to the simplicity and effectiveness of the regularized linear classifier that we have introduced in section 2.2.1, we choose it to be our local label predictor, such that for each document xi, the following criterion is minimized Ji = 1 ni xj ∈Ni wT i xj − qj 2 + λi wi 2 , (7) where ni = |Ni| is the cardinality of Ni, and qj is the cluster membership of xj.",
                "Then using Eq. (6), we can get the optimal solution is w∗ i = XiXT i + λiniI −1 Xiqi, (8) where Xi = [xi1, xi2, · · · , xini ], and we use xik to denote the k-th nearest neighbor of xi. qi = [qi1, qi2, · · · , qini ]T with qik representing the cluster assignment of xik.",
                "The problem here is that XiXT i is an m × m matrix with m ni, i.e. we should compute the inverse of an m × m matrix for every document vector, which is computationally prohibited.",
                "Fortunately, we have the following theorem: Theorem 1. w∗ i in Eq. (8) can be rewritten as w∗ i = Xi XT i Xi + λiniIi −1 qi, (9) where Ii is an ni × ni identity matrix.",
                "Proof.",
                "Since w∗ i = XiXT i + λiniI −1 Xiqi, then XiXT i + λiniI w∗ i = Xiqi =⇒ XiXT i w∗ i + λiniw∗ i = Xiqi =⇒ w∗ i = (λini)−1 Xi qi − XT i w∗ i .",
                "Let β = (λini)−1 qi − XT i w∗ i , then w∗ i = Xiβ =⇒ λiniβ = qi − XT i w∗ i = qi − XT i Xiβ =⇒ qi = XT i Xi + λiniIi β =⇒ β = XT i Xi + λiniIi −1 qi.",
                "Therefore w∗ i = Xiβ = Xi XT i Xi + λiniIi −1 qi 2 Using theorem 1, we only need to compute the inverse of an ni × ni matrix for every document to train a local label predictor.",
                "Moreover, for a new testing point u that falls into Ni, we can classify it by the sign of qu = w∗T i u = uT wi = uT Xi XT i Xi + λiniIi −1 qi.",
                "This is an attractive expression since we can determine the cluster assignment of u by using the inner-products between the points in {u ∪ Ni}, which suggests that such a local regularizer can easily be kernelized [21] as long as we define a proper kernel function. 2.2.3 Combining the Local Regularized Predictors After all the local predictors having been constructed, we will combine them together by minimizing Jl = n i=1 w∗T i xi − qi 2 , (10) which stands for the sum of the prediction errors for all the local predictors.",
                "Combining Eq. (10) with Eq. (6), we can get Jl = n i=1 w∗T i xi − qi 2 = n i=1 xT i Xi XT i Xi + λiniIi −1 qi − qi 2 = Pq − q 2 , (11) where q = [q1, q2, · · · , qn]T , and the P is an n × n matrix constructing in the following way.",
                "Let αi = xT i Xi XT i Xi + λiniIi −1 , then Pij = αi j, if xj ∈ Ni 0, otherwise , (12) where Pij is the (i, j)-th entry of P, and αi j represents the j-th entry of αi .",
                "Till now we can write the criterion of clustering by combining locally regularized linear label predictors Jl in an explicit mathematical form, and we can minimize it directly using some standard optimization techniques.",
                "However, the results may not be good enough since we only exploit the local informations of the dataset.",
                "In the next subsection, we will introduce a global regularization criterion and combine it with Jl, which aims to find a good clustering result in a local-global way. 2.3 Global Regularization In data clustering, we usually require that the cluster assignments of the data points should be sufficiently smooth with respect to the underlying data manifold, which implies (1) the nearby points tend to have the same cluster assignments; (2) the points on the same structure (e.g. submanifold or cluster) tend to have the same cluster assignments [31].",
                "Without the loss of generality, we assume that the data points reside (roughly) on a low-dimensional manifold M2 , and q is the cluster assignment function defined on M, i.e. 2 We believe that the text data are also sampled from some low dimensional manifold, since it is impossible for them to for ∀x ∈ M, q(x) returns the cluster membership of x.",
                "The smoothness of q over M can be calculated by the following Dirichlet integral [2] D[q] = 1 2 M q(x) 2 dM, (13) where the gradient q is a vector in the tangent space T Mx, and the integral is taken with respect to the standard measure on M. If we restrict the scale of q by q, q M = 1 (where ·, · M is the inner product induced on M), then it turns out that finding the smoothest function minimizing D[q] reduces to finding the eigenfunctions of the Laplace Beltrami operator L, which is defined as Lq −div q, (14) where div is the divergence of a vector field.",
                "Generally, the graph can be viewed as the discretized form of manifold.",
                "We can model the dataset as an weighted undirected graph as in spectral clustering [22], where the graph nodes are just the data points, and the weights on the edges represent the similarities between pairwise points.",
                "Then it can be shown that minimizing Eq. (13) corresponds to minimizing Jg = qT Lq = n i=1 (qi − qj)2 wij, (15) where q = [q1, q2, · · · , qn]T with qi = q(xi), L is the graph Laplacian with its (i, j)-th entry Lij =    di − wii, if i = j −wij, if xi and xj are adjacent 0, otherwise, (16) where di = j wij is the degree of xi, wij is the similarity between xi and xj.",
                "If xi and xj are adjacent3 , wij is usually computed in the following way wij = e − xi−xj 2 2σ2 , (17) where σ is a dataset dependent parameter.",
                "It is proved that under certain conditions, such a form of wij to determine the weights on graph edges leads to the convergence of graph Laplacian to the Laplace Beltrami operator [3][18].",
                "In summary, using Eq. (15) with exponential weights can effectively measure the smoothness of the data assignments with respect to the intrinsic data manifold.",
                "Thus we adopt it as a global regularizer to punish the smoothness of the predicted data assignments. 2.4 Clustering with Local and Global Regularization Combining the contents we have introduced in section 2.2 and section 2.3 we can derive the clustering criterion is minq J = Jl + λJg = Pq − q 2 + λqT Lq s.t. qi ∈ {−1, +1}, (18) where P is defined as in Eq. (12), and λ is a regularization parameter to trade off Jl and Jg.",
                "However, the discrete fill in the whole high-dimensional sample space.",
                "And it has been shown that the manifold based methods can achieve good results on text classification tasks [31]. 3 In this paper, we define xi and xj to be adjacent if xi ∈ N(xj) or xj ∈ N(xi). constraint of pi makes the problem an NP hard integer programming problem.",
                "A natural way for making the problem solvable is to remove the constraint and relax qi to be continuous, then the objective that we aims to minimize becomes J = Pq − q 2 + λqT Lq = qT (P − I)T (P − I)q + λqT Lq = qT (P − I)T (P − I) + λL q, (19) and we further add a constraint qT q = 1 to restrict the scale of q.",
                "Then our objective becomes minq J = qT (P − I)T (P − I) + λL q s.t. qT q = 1 (20) Using the Lagrangian method, we can derive that the optimal solution q corresponds to the smallest eigenvector of the matrix M = (P − I)T (P − I) + λL, and the cluster assignment of xi can be determined by the sign of qi, i.e. xi will be classified as class one if qi > 0, otherwise it will be classified as class 2. 2.5 Multi-Class CLGR In the above we have introduced the basic framework of Clustering with Local and Global Regularization (CLGR) for the two-class clustering problem, and we will extending it to multi-class clustering in this subsection.",
                "First we assume that all the documents belong to C classes indexed by L = {1, 2, · · · , C}. qc is the classification function for class c (1 c C), such that qc (xi) returns the confidence that xi belongs to class c. Our goal is to obtain the value of qc (xi) (1 c C, 1 i n), and the cluster assignment of xi can be determined by {qc (xi)}C c=1 using some proper discretization methods that we will introduce later.",
                "Therefore, in this multi-class case, for each document xi (1 i n), we will construct C locally linear regularized label predictors whose normal vectors are wc∗ i = Xi XT i Xi + λiniIi −1 qc i (1 c C), (21) where Xi = [xi1, xi2, · · · , xini ] with xik being the k-th neighbor of xi, and qc i = [qc i1, qc i2, · · · , qc ini ]T with qc ik = qc (xik).",
                "Then (wc∗ i )T xi returns the predicted confidence of xi belonging to class c. Hence the local prediction error for class c can be defined as J c l = n i=1 (wc∗ i ) T xi − qc i 2 , (22) And the total local prediction error becomes Jl = C c=1 J c l = C c=1 n i=1 (wc∗ i ) T xi − qc i 2 . (23) As in Eq. (11), we can define an n×n matrix P (see Eq. (12)) and rewrite Jl as Jl = C c=1 J c l = C c=1 Pqc − qc 2 . (24) Similarly we can define the global smoothness regularizer in multi-class case as Jg = C c=1 n i=1 (qc i − qc j )2 wij = C c=1 (qc )T Lqc . (25) Then the criterion to be minimized for CLGR in multi-class case becomes J = Jl + λJg = C c=1 Pqc − qc 2 + λ(qc )T Lqc = C c=1 (qc )T (P − I)T (P − I) + λL qc = trace QT (P − I)T (P − I) + λL Q , (26) where Q = [q1 , q2 , · · · , qc ] is an n × c matrix, and trace(·) returns the trace of a matrix.",
                "The same as in Eq. (20), we also add the constraint that QT Q = I to restrict the scale of Q.",
                "Then our optimization problem becomes minQ J = trace QT (P − I)T (P − I) + λL Q s.t.",
                "QT Q = I, (27) From the Ky Fan theorem [28], we know the optimal solution of the above problem is Q∗ = [q∗ 1, q∗ 2, · · · , q∗ C ]R, (28) where q∗ k (1 k C) is the eigenvector corresponds to the k-th smallest eigenvalue of matrix (P − I)T (P − I) + λL, and R is an arbitrary C × C matrix.",
                "Since the values of the entries in Q∗ is continuous, we need to further discretize Q∗ to get the cluster assignments of all the data points.",
                "There are mainly two approaches to achieve this goal: 1.",
                "As in [20], we can treat the i-th row of Q as the embedding of xi in a C-dimensional space, and apply some traditional clustering methods like kmeans to clustering these embeddings into C clusters. 2.",
                "Since the optimal Q∗ is not unique (because of the existence of an arbitrary matrix R), we can pursue an optimal R that will rotate Q∗ to an indication matrix4 .",
                "The detailed algorithm can be referred to [26].",
                "The detailed algorithm procedure for CLGR is summarized in table 1. 3.",
                "EXPERIMENTS In this section, experiments are conducted to empirically compare the clustering results of CLGR with other 8 representitive document clustering algorithms on 5 datasets.",
                "First we will introduce the basic informations of those datasets. 3.1 Datasets We use a variety of datasets, most of which are frequently used in the information retrieval research.",
                "Table 2 summarizes the characteristics of the datasets. 4 Here an indication matrix T is a n×c matrix with its (i, j)th entry Tij ∈ {0, 1} such that for each row of Q∗ there is only one 1.",
                "Then the xi can be assigned to the j-th cluster such that j = argjQ∗ ij = 1.",
                "Table 1: Clustering with Local and Global Regularization (CLGR) Input: 1.",
                "Dataset X = {xi}n i=1; 2.",
                "Number of clusters C; 3.",
                "Size of the neighborhood K; 4.",
                "Local regularization parameters {λi}n i=1; 5.",
                "Global regularization parameter λ; Output: The cluster membership of each data point.",
                "Procedure: 1.",
                "Construct the K nearest neighborhoods for each data point; 2.",
                "Construct the matrix P using Eq. (12); 3.",
                "Construct the Laplacian matrix L using Eq. (16); 4.",
                "Construct the matrix M = (P − I)T (P − I) + λL; 5.",
                "Do eigenvalue decomposition on M, and construct the matrix Q∗ according to Eq. (28); 6.",
                "Output the cluster assignments of each data point by properly discretize Q∗ .",
                "Table 2: Descriptions of the document datasets Datasets Number of documents Number of classes CSTR 476 4 WebKB4 4199 4 Reuters 2900 10 WebACE 2340 20 Newsgroup4 3970 4 CSTR.",
                "This is the dataset of the abstracts of technical reports published in the Department of Computer Science at a university.",
                "The dataset contained 476 abstracts, which were divided into four research areas: Natural Language Processing(NLP), Robotics/Vision, Systems, and Theory.",
                "WebKB.",
                "The WebKB dataset contains webpages gathered from university computer science departments.",
                "There are about 8280 documents and they are divided into 7 categories: student, faculty, staff, course, project, department and other.",
                "The raw text is about 27MB.",
                "Among these 7 categories, student, faculty, course and project are four most populous entity-representing categories.",
                "The associated subset is typically called WebKB4.",
                "Reuters.",
                "The Reuters-21578 Text Categorization Test collection contains documents collected from the Reuters newswire in 1987.",
                "It is a standard text categorization benchmark and contains 135 categories.",
                "In our experiments, we use a subset of the data collection which includes the 10 most frequent categories among the 135 topics and we call it Reuters-top 10.",
                "WebACE.",
                "The WebACE dataset was from WebACE project and has been used for document clustering [17][5].",
                "The WebACE dataset contains 2340 documents consisting news articles from Reuters new service via the Web in October 1997.",
                "These documents are divided into 20 classes.",
                "News4.",
                "The News4 dataset used in our experiments are selected from the famous 20-newsgroups dataset5 .",
                "The topic rec containing autos, motorcycles, baseball and hockey was selected from the version 20news-18828.",
                "The News4 dataset contains 3970 document vectors. 5 http://people.csail.mit.edu/jrennie/20Newsgroups/ To pre-process the datasets, we remove the stop words using a standard stop list, all HTML tags are skipped and all header fields except subject and organization of the posted articles are ignored.",
                "In all our experiments, we first select the top 1000 words by mutual information with class labels. 3.2 Evaluation Metrics In the experiments, we set the number of clusters equal to the true number of classes C for all the clustering algorithms.",
                "To evaluate their performance, we compare the clusters generated by these algorithms with the true classes by computing the following two performance measures.",
                "Clustering Accuracy (Acc).",
                "The first performance measure is the Clustering Accuracy, which discovers the one-toone relationship between clusters and classes and measures the extent to which each cluster contained data points from the corresponding class.",
                "It sums up the whole matching degree between all pair class-clusters.",
                "Clustering accuracy can be computed as: Acc = 1 N max   Ck,Lm T(Ck, Lm)   , (29) where Ck denotes the k-th cluster in the final results, and Lm is the true m-th class.",
                "T(Ck, Lm) is the number of entities which belong to class m are assigned to cluster k. Accuracy computes the maximum sum of T(Ck, Lm) for all pairs of clusters and classes, and these pairs have no overlaps.",
                "The greater clustering accuracy means the better clustering performance.",
                "Normalized Mutual Information (NMI).",
                "Another evaluation metric we adopt here is the Normalized Mutual Information NMI [23], which is widely used for determining the quality of clusters.",
                "For two random variable X and Y, the NMI is defined as: NMI(X, Y) = I(X, Y) H(X)H(Y) , (30) where I(X, Y) is the mutual information between X and Y, while H(X) and H(Y) are the entropies of X and Y respectively.",
                "One can see that NMI(X, X) = 1, which is the maximal possible value of NMI.",
                "Given a clustering result, the NMI in Eq. (30) is estimated as NMI = C k=1 C m=1 nk,mlog n·nk,m nk ˆnm C k=1 nklog nk n C m=1 ˆnmlog ˆnm n , (31) where nk denotes the number of data contained in the cluster Ck (1 k C), ˆnm is the number of data belonging to the m-th class (1 m C), and nk,m denotes the number of data that are in the intersection between the cluster Ck and the m-th class.",
                "The value calculated in Eq. (31) is used as a performance measure for the given clustering result.",
                "The larger this value, the better the clustering performance. 3.3 Comparisons We have conducted comprehensive performance evaluations by testing our method and comparing it with 8 other representative data clustering methods using the same data corpora.",
                "The algorithms that we evaluated are listed below. 1.",
                "Traditional k-means (KM). 2.",
                "Spherical k-means (SKM).",
                "The implementation is based on [9]. 3.",
                "Gaussian Mixture Model (GMM).",
                "The implementation is based on [16]. 4.",
                "Spectral Clustering with Normalized Cuts (Ncut).",
                "The implementation is based on [26], and the variance of the Gaussian similarity is determined by Local Scaling [30].",
                "Note that the criterion that Ncut aims to minimize is just the global regularizer in our CLGR algorithm except that Ncut used the normalized Laplacian. 5.",
                "Clustering using Pure Local Regularization (CPLR).",
                "In this method we just minimize Jl (defined in Eq. (24)), and the clustering results can be obtained by doing eigenvalue decomposition on matrix (I − P)T (I − P) with some proper discretization methods. 6.",
                "Adaptive Subspace Iteration (ASI).",
                "The implementation is based on [14]. 7.",
                "Nonnegative Matrix Factorization (NMF).",
                "The implementation is based on [27]. 8.",
                "Tri-Factorization Nonnegative Matrix Factorization (TNMF) [12].",
                "The implementation is based on [15].",
                "For computational efficiency, in the implementation of CPLR and our CLGR algorithm, we have set all the local regularization parameters {λi}n i=1 to be identical, which is set by grid search from {0.1, 1, 10}.",
                "The size of the k-nearest neighborhoods is set by grid search from {20, 40, 80}.",
                "For the CLGR method, its global regularization parameter is set by grid search from {0.1, 1, 10}.",
                "When constructing the global regularizer, we have adopted the local scaling method [30] to construct the Laplacian matrix.",
                "The final discretization method adopted in these two methods is the same as in [26], since our experiments show that using such method can achieve better results than using kmeans based methods as in [20]. 3.4 Experimental Results The clustering accuracies comparison results are shown in table 3, and the normalized mutual information comparison results are summarized in table 4.",
                "From the two tables we mainly observe that: 1.",
                "Our CLGR method outperforms all other document clustering methods in most of the datasets; 2.",
                "For document clustering, the Spherical k-means method usually outperforms the traditional k-means clustering method, and the GMM method can achieve competitive results compared to the Spherical k-means method; 3.",
                "The results achieved from the k-means and GMM type algorithms are usually worse than the results achieved from Spectral Clustering.",
                "Since Spectral Clustering can be viewed as a weighted version of kernel k-means, it can obtain good results the data clusters are arbitrarily shaped.",
                "This corroborates that the documents vectors are not regularly distributed (spherical or elliptical). 4.",
                "The experimental comparisons empirically verify the equivalence between NMF and Spectral Clustering, which Table 3: Clustering accuracies of the various methods CSTR WebKB4 Reuters WebACE News4 KM 0.4256 0.3888 0.4448 0.4001 0.3527 SKM 0.4690 0.4318 0.5025 0.4458 0.3912 GMM 0.4487 0.4271 0.4897 0.4521 0.3844 NMF 0.5713 0.4418 0.4947 0.4761 0.4213 Ncut 0.5435 0.4521 0.4896 0.4513 0.4189 ASI 0.5621 0.4752 0.5235 0.4823 0.4335 TNMF 0.6040 0.4832 0.5541 0.5102 0.4613 CPLR 0.5974 0.5020 0.4832 0.5213 0.4890 CLGR 0.6235 0.5228 0.5341 0.5376 0.5102 Table 4: Normalized mutual information results of the various methods CSTR WebKB4 Reuters WebACE News4 KM 0.3675 0.3023 0.4012 0.3864 0.3318 SKM 0.4027 0.4155 0.4587 0.4003 0.4085 GMM 0.4034 0.4093 0.4356 0.4209 0.3994 NMF 0.5235 0.4517 0.4402 0.4359 0.4130 Ncut 0.4833 0.4497 0.4392 0.4289 0.4231 ASI 0.5008 0.4833 0.4769 0.4817 0.4503 TNMF 0.5724 0.5011 0.5132 0.5328 0.4749 CPLR 0.5695 0.5231 0.4402 0.5543 0.4690 CLGR 0.6012 0.5434 0.4935 0.5390 0.4908 has been proved theoretically in [10].",
                "It can be observed from the tables that NMF and Spectral Clustering usually lead to similar clustering results. 5.",
                "The co-clustering based methods (TNMF and ASI) can usually achieve better results than traditional purely document vector based methods.",
                "Since these methods perform an implicit feature selection at each iteration, provide an adaptive metric for measuring the neighborhood, and thus tend to yield better clustering results. 6.",
                "The results achieved from CPLR are usually better than the results achieved from Spectral Clustering, which supports Vapniks theory [24] that sometimes local learning algorithms can obtain better results than global learning algorithms.",
                "Besides the above comparison experiments, we also test the parameter sensibility of our method.",
                "There are mainly two sets of parameters in our CLGR algorithm, the local and global regularization parameters ({λi}n i=1 and λ, as we have said in section 3.3, we have set all λis to be identical to λ∗ in our experiments), and the size of the neighborhoods.",
                "Therefore we have also done two sets of experiments: 1.",
                "Fixing the size of the neighborhoods, and testing the clustering performance with varying λ∗ and λ.",
                "In this set of experiments, we find that our CLGR algorithm can achieve good results when the two regularization parameters are neither too large nor too small.",
                "Typically our method can achieve good results when λ∗ and λ are around 0.1.",
                "Figure 1 shows us such a testing example on the WebACE dataset. 2.",
                "Fixing the local and global regularization parameters, and testing the clustering performance with different −5 −4.5 −4 −3.5 −3 −5 −4.5 −4 −3.5 −3 0.35 0.4 0.45 0.5 0.55 local regularization para (log 2 value) global regularization para (log 2 value) clusteringaccuracy Figure 1: Parameter sensibility testing results on the WebACE dataset with the neighborhood size fixed to 20, and the x-axis and y-axis represents the log2 value of λ∗ and λ. sizes of neighborhoods.",
                "In this set of experiments, we find that the neighborhood with a too large or too small size will all deteriorate the final clustering results.",
                "This can be easily understood since when the neighborhood size is very small, then the data points used for training the local classifiers may not be sufficient; when the neighborhood size is very large, the trained classifiers will tend to be global and cannot capture the typical local characteristics.",
                "Figure 2 shows us a testing example on the WebACE dataset.",
                "Therefore, we can see that our CLGR algorithm (1) can achieve satisfactory results and (2) is not very sensitive to the choice of parameters, which makes it practical in real world applications. 4.",
                "CONCLUSIONS AND FUTURE WORKS In this paper, we derived a new clustering algorithm called clustering with local and global regularization.",
                "Our method preserves the merit of local learning algorithms and spectral clustering.",
                "Our experiments show that the proposed algorithm outperforms most of the state of the art algorithms on many benchmark datasets.",
                "In the future, we will focus on the parameter selection and acceleration issues of the CLGR algorithm. 5.",
                "REFERENCES [1] L. Baker and A. McCallum.",
                "Distributional Clustering of Words for Text Classification.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 1998. [2] M. Belkin and P. Niyogi.",
                "Laplacian Eigenmaps for Dimensionality Reduction and Data Representation.",
                "Neural Computation, 15 (6):1373-1396.",
                "June 2003. [3] M. Belkin and P. Niyogi.",
                "Towards a Theoretical Foundation for Laplacian-Based Manifold Methods.",
                "In Proceedings of the 18th Conference on Learning Theory (COLT). 2005. 10 20 30 40 50 60 70 80 90 100 0.35 0.4 0.45 0.5 0.55 size of the neighborhood clusteringaccuracy Figure 2: Parameter sensibility testing results on the WebACE dataset with the regularization parameters being fixed to 0.1, and the neighborhood size varing from 10 to 100. [4] M. Belkin, P. Niyogi and V. Sindhwani.",
                "Manifold Regularization: a Geometric Framework for Learning from Examples.",
                "Journal of Machine Learning Research 7, 1-48, 2006. [5] D. Boley.",
                "Principal Direction Divisive Partitioning.",
                "Data mining and knowledge discovery, 2:325-344, 1998. [6] L. Bottou and V. Vapnik.",
                "Local learning algorithms.",
                "Neural Computation, 4:888-900, 1992. [7] P. K. Chan, D. F. Schlag and J. Y. Zien.",
                "Spectral K-way Ratio-Cut Partitioning and Clustering.",
                "IEEE Trans.",
                "Computer-Aided Design, 13:1088-1096, Sep. 1994. [8] D. R. Cutting, D. R. Karger, J. O. Pederson and J. W. Tukey.",
                "Scatter/Gather: A Cluster-Based Approach to Browsing Large Document Collections.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 1992. [9] I. S. Dhillon and D. S. Modha.",
                "Concept Decompositions for Large Sparse Text Data using Clustering.",
                "Machine Learning, vol. 42(1), pages 143-175, January 2001. [10] C. Ding, X.",
                "He, and H. Simon.",
                "On the equivalence of nonnegative matrix factorization and spectral clustering.",
                "In Proceedings of the SIAM Data Mining Conference, 2005. [11] C. Ding, X.",
                "He, H. Zha, M. Gu, and H. D. Simon.",
                "A min-max cut algorithm for graph partitioning and data clustering.",
                "In Proc. of the 1st International Conference on Data Mining (ICDM), pages 107-114, 2001. [12] C. Ding, T. Li, W. Peng, and H. Park.",
                "Orthogonal Nonnegative Matrix Tri-Factorizations for Clustering.",
                "In Proceedings of the Twelfth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2006. [13] R. O. Duda, P. E. Hart, and D. G. Stork.",
                "Pattern Classification.",
                "John Wiley & Sons, Inc., 2001. [14] T. Li, S. Ma, and M. Ogihara.",
                "Document Clustering via Adaptive Subspace Iteration.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2004. [15] T. Li and C. Ding.",
                "The Relationships Among Various Nonnegative Matrix Factorization Methods for Clustering.",
                "In Proceedings of the 6th International Conference on Data Mining (ICDM). 2006. [16] X. Liu and Y. Gong.",
                "Document Clustering with Cluster Refinement and Model Selection Capabilities.",
                "In Proc. of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002. [17] E. Han, D. Boley, M. Gini, R. Gross, K. Hastings, G. Karypis, V. Kumar, B. Mobasher, and J. Moore.",
                "WebACE: A Web Agent for Document Categorization and Exploration.",
                "In Proceedings of the 2nd International Conference on Autonomous Agents (Agents98).",
                "ACM Press, 1998. [18] M. Hein, J. Y. Audibert, and U. von Luxburg.",
                "From Graphs to Manifolds - Weak and Strong Pointwise Consistency of Graph Laplacians.",
                "In Proceedings of the 18th Conference on Learning Theory (COLT), 470-485. 2005. [19] J.",
                "He, M. Lan, C.-L. Tan, S.-Y.",
                "Sung, and H.-B.",
                "Low.",
                "Initialization of Cluster Refinement Algorithms: A Review and Comparative Study.",
                "In Proc. of Inter.",
                "Joint Conference on Neural Networks, 2004. [20] A. Y. Ng, M. I. Jordan, Y. Weiss.",
                "On Spectral Clustering: Analysis and an algorithm.",
                "In Advances in Neural Information Processing Systems 14. 2002. [21] B. Sch¨olkopf and A. Smola.",
                "Learning with Kernels.",
                "The MIT Press.",
                "Cambridge, Massachusetts. 2002. [22] J. Shi and J. Malik.",
                "Normalized Cuts and Image Segmentation.",
                "IEEE Trans. on Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000. [23] A. Strehl and J. Ghosh.",
                "Cluster Ensembles - A Knowledge Reuse Framework for Combining Multiple Partitions.",
                "Journal of Machine Learning Research, 3:583-617, 2002. [24] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Berlin: Springer-Verlag, 1995. [25] Wu, M. and Sch¨olkopf, B.",
                "A Local Learning Approach for Clustering.",
                "In Advances in Neural Information Processing Systems 18. 2006. [26] S. X. Yu, J. Shi.",
                "Multiclass Spectral Clustering.",
                "In Proceedings of the International Conference on Computer Vision, 2003. [27] W. Xu, X. Liu and Y. Gong.",
                "Document Clustering Based On Non-Negative Matrix Factorization.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2003. [28] H. Zha, X.",
                "He, C. Ding, M. Gu and H. Simon.",
                "Spectral Relaxation for K-means Clustering.",
                "In NIPS 14. 2001. [29] T. Zhang and F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Journal of Information Retrieval, 4:5-31, 2001. [30] L. Zelnik-Manor and P. Perona.",
                "Self-Tuning Spectral Clustering.",
                "In NIPS 17. 2005. [31] D. Zhou, O. Bousquet, T. N. Lal, J. Weston and B. Sch¨olkopf.",
                "Learning with Local and Global Consistency.",
                "NIPS 17, 2005."
            ],
            "original_annotated_samples": [
                "Generally speaking, learning can be posed as a problem of <br>function estimation</br>, from which we can get a good classification function that will assign labels to the training dataset and even the unseen testing dataset with some cost minimized [24]."
            ],
            "translated_annotated_samples": [
                "En general, el aprendizaje puede plantearse como un problema de <br>estimación de funciones</br>, a partir del cual podemos obtener una buena función de clasificación que asignará etiquetas al conjunto de datos de entrenamiento e incluso al conjunto de datos de prueba no vistos con algún costo minimizado [24]."
            ],
            "translated_text": "Agrupamiento regularizado para documentos ∗ Fei Wang, Changshui Zhang Laboratorio Clave del Estado de Tecnología Inteligente. En los últimos años, la agrupación de documentos ha estado recibiendo cada vez más atención como una técnica importante y fundamental para la organización no supervisada de documentos, la extracción automática de temas y la recuperación o filtrado rápido de información. En este artículo, proponemos un método novedoso para agrupar documentos utilizando regularización. A diferencia de los métodos de agrupamiento regularizados globalmente tradicionales, nuestro método primero construye un predictor de etiquetas lineal regularizado local para cada vector de documento, y luego combina todos esos regularizadores locales con un regularizador de suavidad global. Así que llamamos a nuestro algoritmo Agrupamiento con Regularización Local y Global (CLGR). Mostraremos que las pertenencias de los documentos a los grupos se pueden lograr mediante la descomposición de los valores propios de una matriz simétrica dispersa, la cual puede resolverse eficientemente mediante métodos iterativos. Finalmente, se presentan nuestras evaluaciones experimentales en varios conjuntos de datos para mostrar las ventajas de CLGR sobre los métodos tradicionales de agrupamiento de documentos. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información-Agrupamiento; I.2.6 [Inteligencia Artificial]: Aprendizaje-Aprendizaje de Conceptos Términos Generales Algoritmos 1. La agrupación de documentos ha estado recibiendo cada vez más atención como una técnica importante y fundamental para la organización no supervisada de documentos, la extracción automática de temas y la recuperación o filtrado rápido de información. Un buen enfoque de agrupación de documentos puede ayudar a las computadoras a organizar automáticamente el corpus de documentos en una jerarquía de clusters significativa para una navegación eficiente, lo cual es muy valioso para complementar las deficiencias de las tecnologías tradicionales de recuperación de información. Como señaló [8], las necesidades de recuperación de información pueden expresarse mediante un espectro que va desde la búsqueda basada en palabras clave estrechas hasta la exploración de información amplia, como cuáles son los principales eventos internacionales de los últimos meses. Los motores de búsqueda de documentos tradicionales tienden a adaptarse bien al extremo de la búsqueda, es decir, suelen proporcionar búsquedas específicas de documentos que coinciden con la consulta de los usuarios, sin embargo, les resulta difícil satisfacer las necesidades del resto del espectro en el que se necesita información más amplia o vaga. En tales casos, la navegación eficiente a través de una buena jerarquía de clústeres será definitivamente útil. Generalmente, los métodos de agrupamiento de documentos se pueden clasificar principalmente en dos clases: métodos jerárquicos y métodos de partición. Los métodos jerárquicos agrupan los puntos de datos en una estructura de árbol jerárquico utilizando enfoques de abajo hacia arriba o de arriba hacia abajo. Por ejemplo, el clustering jerárquico aglomerativo (HAC) [13] es un método típico de clustering jerárquico ascendente. Toma cada punto de datos como un único clúster para comenzar y luego construye clústeres más grandes agrupando puntos de datos similares juntos hasta que todo el conjunto de datos esté encapsulado en un único clúster final. Por otro lado, los métodos de particionamiento descomponen el conjunto de datos en un número de grupos disjuntos que suelen ser óptimos en términos de algunas funciones de criterio predefinidas. Por ejemplo, K-means es un método de particionamiento típico que tiene como objetivo minimizar la suma de las distancias al cuadrado entre los puntos de datos y sus centros de clúster correspondientes. En este documento, nos centraremos en los métodos de particionamiento. Como sabemos, existen dos problemas principales en los métodos de particionamiento (como Kmeans y el Modelo de Mezcla Gaussiana (GMM) [16]): (1) el criterio predefinido suele ser no convexo, lo que provoca muchas soluciones óptimas locales; (2) el procedimiento iterativo (por ejemplo, el algoritmo de Expectation Maximization (EM)) para optimizar los criterios suele hacer que las soluciones finales dependan en gran medida de las inicializaciones. En las últimas décadas, se han propuesto muchos métodos para superar los problemas mencionados de los métodos de particionamiento [19][28]. Recientemente, otro tipo de métodos de particionamiento basados en agrupamiento en grafos de datos han despertado un considerable interés en la comunidad de aprendizaje automático y minería de datos. La idea básica detrás de estos métodos es modelar primero el conjunto de datos completo como un grafo ponderado, en el que los nodos del grafo representan los puntos de datos y los pesos en los bordes corresponden a las similitudes entre los puntos en pares. Entonces, las asignaciones de clústeres del conjunto de datos se pueden lograr optimizando algunos criterios definidos en el gráfico. Por ejemplo, el Clustering Espectral es uno de los enfoques de clustering basados en grafos más representativos, generalmente tiene como objetivo optimizar algún valor de corte (por ejemplo, Corte normalizado [22], corte de razón [7], corte mínimo-máximo [11]) definidos en un grafo no dirigido. Después de algunas relajaciones, estos criterios generalmente pueden ser optimizados a través de descomposiciones propias, lo cual está garantizado que sea óptimo a nivel global. De esta manera, el agrupamiento espectral evita eficientemente los problemas de los métodos de particionamiento tradicionales que presentamos en el párrafo anterior. En este artículo, proponemos un algoritmo novedoso de agrupamiento de documentos que hereda la superioridad del agrupamiento espectral, es decir, los resultados finales de los grupos también pueden obtenerse explotando la estructura de los autovalores de una matriz simétrica. Sin embargo, a diferencia del agrupamiento espectral, que simplemente impone una restricción de suavidad en las etiquetas de los datos sobre todo el conjunto de datos [2], nuestro método primero construye un predictor de etiquetas lineal regularizado para cada punto de datos a partir de su vecindario como en [25], y luego combina los resultados de todos estos predictores de etiquetas locales con un regularizador de suavidad de etiquetas global. Así que llamamos a nuestro método Agrupamiento con Regularización Local y Global (CLGR). La idea de incorporar tanto información local como global en la predicción de etiquetas está inspirada en los trabajos recientes sobre aprendizaje semi-supervisado [31], y nuestras evaluaciones experimentales en varios conjuntos de datos reales de documentos muestran que CLGR funciona mejor que muchos métodos de agrupamiento de vanguardia. El resto de este documento está organizado de la siguiente manera: en la sección 2 presentaremos nuestro algoritmo CLGR en detalle. Los resultados experimentales en varios conjuntos de datos se presentan en la sección 3, seguidos de las conclusiones y discusiones en la sección 4. El ALGORITMO PROPUESTO En esta sección, presentaremos en detalle nuestro algoritmo de Agrupamiento con Regularización Local y Global (CLGR). Primero veamos cómo se representan los documentos a lo largo de este documento. 2.1 Representación de documentos En nuestro trabajo, todos los documentos se representan mediante vectores de frecuencia de términos ponderados. Sea W = {w1, w2, · · · , wm} el conjunto completo de vocabulario del corpus de documentos (que ha sido preprocesado mediante la eliminación de palabras vacías y operaciones de derivación de palabras). El vector de frecuencia de término xi del documento di se define como xi = [xi1, xi2, · · · , xim]T , xik = tik log n idfk , donde tik es la frecuencia del término wk ∈ W, n es el tamaño del corpus de documentos, idfk es el número de documentos que contienen la palabra wk. De esta manera, xi también se llama la representación TFIDF del documento di. Además, también normalizamos cada xi (1 i n) para que tenga una longitud unitaria, de modo que cada documento esté representado por un vector TF-IDF normalizado. 2.2 Regularización Local Como su nombre sugiere, CLGR está compuesto por dos partes: regularización local y regularización global. En esta subsección introduciremos detalladamente la parte de regularización local. 2.2.1 Motivación Como sabemos que el agrupamiento es un tipo de técnica de aprendizaje, tiene como objetivo organizar el conjunto de datos de una manera razonable. En general, el aprendizaje puede plantearse como un problema de <br>estimación de funciones</br>, a partir del cual podemos obtener una buena función de clasificación que asignará etiquetas al conjunto de datos de entrenamiento e incluso al conjunto de datos de prueba no vistos con algún costo minimizado [24]. Por ejemplo, en el escenario de clasificación de dos clases (en el que conocemos exactamente la etiqueta de cada documento), un clasificador lineal con ajuste de mínimos cuadrados tiene como objetivo aprender un vector columna w de manera que el costo al cuadrado J = 1 n (wT xi − yi)2 (1) se minimice, donde yi ∈ {+1, −1} es la etiqueta de xi. Al tomar ∂J /∂w = 0, obtenemos la solución w∗ = Σ i=1 n xixT i −1 Σ i=1 n xiyi, (2) que puede escribirse en su forma matricial como w∗ = XXT −1 Xy, (3) donde X = [x1, x2, · · · , xn] es una matriz de documentos m × n, y = [y1, y2, · · · , yn]T es el vector de etiquetas. Entonces, para un documento de prueba t, podemos determinar su etiqueta mediante l = signo(w∗T u), donde signo(·) es la función signo. Un problema natural en la ecuación (3) es que la matriz XXT puede ser singular y, por lo tanto, no invertible (por ejemplo, cuando m n). Para evitar tal problema, podemos agregar un término de regularización y minimizar el siguiente criterio J = 1 n n i=1 (wT xi − yi)2 + λ w 2, (5), donde λ es un parámetro de regularización. Entonces, la solución óptima que minimiza J está dada por w∗ = XXT + λnI −1 Xy, (6) donde I es una matriz identidad de tamaño m × m. Se ha informado que el clasificador lineal regularizado puede lograr resultados muy buenos en problemas de clasificación de texto [29]. Sin embargo, a pesar de su éxito empírico, el clasificador lineal regularizado es en realidad un clasificador global, es decir, w∗ se estima utilizando todo el conjunto de entrenamiento. Según [24], esta puede que no sea una idea inteligente, ya que un único w∗ puede que no sea lo suficientemente bueno para predecir las etiquetas de todo el espacio de entrada. Para obtener predicciones más precisas, [6] propuso entrenar clasificadores localmente y utilizarlos para clasificar los puntos de prueba. Por ejemplo, un punto de prueba será clasificado por el clasificador local entrenado utilizando los puntos de entrenamiento ubicados en la vecindad 1. En las siguientes discusiones todos asumimos que los documentos provienen solo de dos clases. Las generalizaciones de nuestro método a casos de múltiples clases se discutirán en la sección 2.5 de él. Aunque este método parece lento y estúpido, se informa que puede obtener mejores rendimientos que usar un clasificador global único en ciertas tareas [6]. 2.2.2 Construyendo los Predictores Localmente Regularizados Inspirados en su éxito, propusimos aplicar los algoritmos de aprendizaje local para el agrupamiento. La idea básica es que, para cada vector de documento xi (1 i n), entrenamos un predictor de etiquetas local basado en su vecindario k-más cercano Ni, y luego lo usamos para predecir la etiqueta de xi. Finalmente combinaremos todos esos predictores locales minimizando la suma de sus errores de predicción. En esta subsección introduciremos cómo construir esos predictores locales. Debido a la simplicidad y efectividad del clasificador lineal regularizado que hemos introducido en la sección 2.2.1, lo elegimos como nuestro predictor de etiquetas local, de modo que para cada documento xi, se minimiza el siguiente criterio Ji = 1 ni xj ∈Ni wT i xj − qj 2 + λi wi 2, (7), donde ni = |Ni| es la cardinalidad de Ni, y qj es la pertenencia al clúster de xj. Luego, utilizando la Ecuación (6), podemos obtener que la solución óptima es w∗ i = XiXT i + λiniI −1 Xiqi, (8), donde Xi = [xi1, xi2, · · · , xini], y usamos xik para denotar al k-ésimo vecino más cercano de xi. qi = [qi1, qi2, · · · , qini]T con qik representando la asignación de clúster de xik. El problema aquí es que XiXT i es una matriz m × m con m ni, es decir, deberíamos calcular la inversa de una matriz m × m para cada vector de documento, lo cual está prohibido computacionalmente. Afortunadamente, tenemos el siguiente teorema: Teorema 1. w∗ i en la Ec. (8) puede ser reescrito como w∗ i = Xi XT i Xi + λiniIi −1 qi, (9), donde Ii es una matriz identidad de tamaño ni × ni. Prueba. Dado que w∗ i = XiXT i + λiniI −1 Xiqi, entonces XiXT i + λiniI w∗ i = Xiqi =⇒ XiXT i w∗ i + λiniw∗ i = Xiqi =⇒ w∗ i = (λini)−1 Xi qi − XT i w∗ i. Sea β = (λini)−1 qi − XT i w∗ i, entonces w∗ i = Xiβ =⇒ λiniβ = qi − XT i w∗ i = qi − XT i Xiβ =⇒ qi = XT i Xi + λiniIi β =⇒ β = XT i Xi + λiniIi −1 qi. Por lo tanto, w∗ i = Xiβ = Xi XT i Xi + λiniIi −1 qi 2. Utilizando el teorema 1, solo necesitamos calcular la inversa de una matriz ni × ni para cada documento para entrenar un predictor de etiquetas local. Además, para un nuevo punto de prueba u que cae en Ni, podemos clasificarlo por el signo de qu = w∗T i u = uT wi = uT Xi XT i Xi + λiniIi −1 qi. Esta es una expresión atractiva ya que podemos determinar la asignación del clúster de u utilizando los productos internos entre los puntos en {u ∪ Ni}, lo que sugiere que un regularizador local de este tipo puede ser fácilmente kernelizado [21] siempre y cuando definamos una función de kernel adecuada. 2.2.3 Combinando los Predictores Regularizados Localmente Una vez que todos los predictores locales hayan sido construidos, los combinaremos minimizando Jl = n i=1 w∗T i xi − qi 2, (10), lo que representa la suma de los errores de predicción de todos los predictores locales. Combinando la Ecuación (10) con la Ecuación (6), podemos obtener Jl = n i=1 w∗T i xi − qi 2 = n i=1 xT i Xi XT i Xi + λiniIi −1 qi − qi 2 = Pq − q 2, (11), donde q = [q1, q2, · · · , qn]T, y P es una matriz n × n construida de la siguiente manera. Sea αi = xT i Xi XT i Xi + λiniIi −1 , entonces Pij = αi j, si xj ∈ Ni 0, de lo contrario , (12) donde Pij es la entrada (i, j)-ésima de P, y αi j representa la entrada j-ésima de αi. Hasta ahora podemos escribir el criterio de agrupamiento combinando predictores de etiquetas lineales localmente regularizados Jl en una forma matemática explícita, y podemos minimizarlo directamente utilizando algunas técnicas estándar de optimización. Sin embargo, los resultados pueden no ser lo suficientemente buenos ya que solo explotamos la información local del conjunto de datos. En la siguiente subsección, introduciremos un criterio de regularización global y lo combinaremos con Jl, que tiene como objetivo encontrar un buen resultado de agrupamiento de manera local-global. 2.3 Regularización Global En el agrupamiento de datos, generalmente requerimos que las asignaciones de clúster de los puntos de datos sean lo suficientemente suaves con respecto a la variedad de datos subyacente, lo que implica (1) que los puntos cercanos tienden a tener las mismas asignaciones de clúster; (2) que los puntos en la misma estructura (por ejemplo, subvariedad o clúster) tienden a tener las mismas asignaciones de clúster [31]. Sin pérdida de generalidad, asumimos que los puntos de datos residen (aproximadamente) en una variedad de baja dimensión M2, y q es la función de asignación de clúster definida en M, es decir, 2 Creemos que los datos de texto también se muestrean de alguna variedad de baja dimensión, ya que es imposible que para todos los x ∈ M, q(x) devuelva la membresía del clúster de x. La suavidad de q sobre M se puede calcular mediante la siguiente integral de Dirichlet [2] D[q] = 1 2 M q(x) 2 dM, (13) donde el gradiente q es un vector en el espacio tangente T Mx, y la integral se toma con respecto a la medida estándar en M. Si restringimos la escala de q por q, q M = 1 (donde ·, · M es el producto interno inducido en M), entonces resulta que encontrar la función más suave que minimiza D[q] se reduce a encontrar las autofunciones del operador Laplace Beltrami L, que se define como Lq −div q, (14) donde div es la divergencia de un campo vectorial. Generalmente, el gráfico se puede ver como la forma discretizada de una variedad. Podemos modelar el conjunto de datos como un grafo ponderado no dirigido como en el agrupamiento espectral [22], donde los nodos del grafo son simplemente los puntos de datos, y los pesos en las aristas representan las similitudes entre pares de puntos. Entonces se puede demostrar que minimizar la Ec. (13) corresponde a minimizar Jg = qT Lq = n i=1 (qi − qj)2 wij, (15), donde q = [q1, q2, · · · , qn]T con qi = q(xi), L es el Laplaciano del grafo con su entrada (i, j)-ésima Lij =    di − wii, si i = j −wij, si xi y xj son adyacentes 0, en otro caso, (16), donde di = j wij es el grado de xi, wij es la similitud entre xi y xj. Si xi y xj son adyacentes, wij generalmente se calcula de la siguiente manera: wij = e − xi−xj 2 2σ2, donde σ es un parámetro dependiente del conjunto de datos. Se ha demostrado que bajo ciertas condiciones, tal forma de wij para determinar los pesos en los bordes del grafo conduce a la convergencia del Laplaciano del grafo al operador Laplace Beltrami [3][18]. En resumen, el uso de la Ec. (15) con pesos exponenciales puede medir de manera efectiva la suavidad de las asignaciones de datos con respecto a la variedad intrínseca de datos. Por lo tanto, lo adoptamos como un regularizador global para penalizar la suavidad de las asignaciones de datos predichas. 2.4 Agrupamiento con Regularización Local y Global Combinando los contenidos que hemos introducido en la sección 2.2 y la sección 2.3, podemos derivar que el criterio de agrupamiento es minq J = Jl + λJg = Pq − q 2 + λqT Lq sujeto a qi ∈ {−1, +1}, (18) donde P está definido como en la Ecuación (12), y λ es un parámetro de regularización para equilibrar Jl y Jg. Sin embargo, los valores discretos llenan todo el espacio muestral de alta dimensionalidad. Y se ha demostrado que los métodos basados en variedades pueden lograr buenos resultados en tareas de clasificación de texto [31]. En este artículo, definimos xi y xj como adyacentes si xi ∈ N(xj) o xj ∈ N(xi). La restricción de pi convierte el problema en un problema de programación entera NP difícil. Una forma natural de hacer que el problema sea resoluble es eliminar la restricción y relajar qi para que sea continuo, luego el objetivo que buscamos minimizar se convierte en J = Pq − q 2 + λqT Lq = qT (P − I)T (P − I)q + λqT Lq = qT (P − I)T (P − I) + λL q, (19) y luego agregamos una restricción adicional qT q = 1 para restringir la escala de q. Entonces nuestro objetivo se convierte en minq J = qT (P − I)T (P − I) + λL q s.t. qT q = 1 (20) Utilizando el método de Lagrange, podemos derivar que la solución óptima q corresponde al menor vector propio de la matriz M = (P − I)T (P − I) + λL, y la asignación de clúster de xi puede determinarse por el signo de qi, es decir, xi se clasificará como clase uno si qi > 0, de lo contrario se clasificará como clase 2. 2.5 CLGR Multiclase En lo anterior hemos introducido el marco básico de Agrupamiento con Regularización Local y Global (CLGR) para el problema de agrupamiento de dos clases, y lo extenderemos al agrupamiento multiclase en esta subsección. Primero asumimos que todos los documentos pertenecen a C clases indexadas por L = {1, 2, · · · , C}. qc es la función de clasificación para la clase c (1 ≤ c ≤ C), tal que qc(xi) devuelve la confianza de que xi pertenece a la clase c. Nuestro objetivo es obtener el valor de qc(xi) (1 ≤ c ≤ C, 1 ≤ i ≤ n), y la asignación de cluster de xi puede ser determinada por {qc(xi)}C c=1 utilizando algunos métodos adecuados de discretización que presentaremos más adelante. Por lo tanto, en este caso de múltiples clases, para cada documento xi (1 i n), construiremos C predictores de etiquetas regularizados localmente lineales cuyos vectores normales son wc∗ i = Xi XT i Xi + λiniIi −1 qc i (1 c C), (21), donde Xi = [xi1, xi2, · · · , xini ] con xik siendo el k-ésimo vecino de xi, y qc i = [qc i1, qc i2, · · · , qc ini ]T con qc ik = qc (xik). Entonces (wc∗ i )T xi devuelve la confianza predicha de xi perteneciente a la clase c. Por lo tanto, el error de predicción local para la clase c se puede definir como J c l = n i=1 (wc∗ i ) T xi − qc i 2 , (22) Y el error de predicción local total se convierte en Jl = C c=1 J c l = C c=1 n i=1 (wc∗ i ) T xi − qc i 2 . (23) Como en la Ec. (11), podemos definir una matriz n×n P (ver Ec. (12)) y reescribir Jl como Jl = C c=1 J c l = C c=1 Pqc − qc 2 . (24) De manera similar, podemos definir el regularizador de suavidad global en el caso de múltiples clases como Jg = C c=1 n i=1 (qc i − qc j )2 wij = C c=1 (qc )T Lqc . (25) Luego, el criterio a minimizar para CLGR en el caso de múltiples clases se convierte en J = Jl + λJg = C c=1 Pqc − qc 2 + λ(qc )T Lqc = C c=1 (qc )T (P − I)T (P − I) + λL qc = trace QT (P − I)T (P − I) + λL Q , (26) donde Q = [q1 , q2 , · · · , qc ] es una matriz n × c, y trace(·) devuelve la traza de una matriz. Al igual que en la ecuación (20), también agregamos la restricción de que QT Q = I para limitar la escala de Q. Entonces nuestro problema de optimización se convierte en minQ J = traza QT (P − I)T (P − I) + λL Q sujeto a. Según el teorema de Ky Fan [28], sabemos que la solución óptima del problema anterior es Q∗ = [q∗ 1, q∗ 2, · · · , q∗ C ]R, donde q∗ k (1 ≤ k ≤ C) es el autovector que corresponde al k-ésimo valor propio más pequeño de la matriz (P − I)T (P − I) + λL, y R es una matriz arbitraria de tamaño C × C. Dado que los valores de las entradas en Q∗ son continuos, necesitamos discretizar aún más Q∗ para obtener las asignaciones de clúster de todos los puntos de datos. Principalmente hay dos enfoques para lograr este objetivo: 1. Como en [20], podemos tratar la i-ésima fila de Q como la incrustación de xi en un espacio de C dimensiones, y aplicar algunos métodos de agrupamiento tradicionales como kmeans para agrupar estas incrustaciones en C grupos. 2. Dado que el Q∗ óptimo no es único (debido a la existencia de una matriz R arbitraria), podemos buscar un R óptimo que rote Q∗ a una matriz de indicación4. El algoritmo detallado se puede consultar en [26]. El procedimiento detallado del algoritmo para CLGR se resume en la tabla 1. 3. EXPERIMENTOS En esta sección, se realizan experimentos para comparar empíricamente los resultados de agrupamiento de CLGR con otros 8 algoritmos representativos de agrupamiento de documentos en 5 conjuntos de datos. Primero presentaremos la información básica de esos conjuntos de datos. 3.1 Conjuntos de datos Utilizamos una variedad de conjuntos de datos, la mayoría de los cuales son frecuentemente utilizados en la investigación de recuperación de información. La Tabla 2 resume las características de los conjuntos de datos. Aquí, una matriz de indicación T es una matriz n×c con su entrada (i, j) Tij ∈ {0, 1} de modo que para cada fila de Q∗ solo hay un 1. Entonces, el xi puede ser asignado al j-ésimo grupo tal que j = argjQ∗ ij = 1. Tabla 1: Agrupamiento con Regularización Local y Global (CLGR) Entrada: 1. Conjunto de datos X = {xi}n i=1; 2. Número de grupos C: 3. Tamaño del vecindario K: 4. Parámetros locales de regularización {λi}n i=1; 5. Parámetro de regularización global λ; Salida: La membresía del clúster de cada punto de datos. Procedimiento: 1. Construir los K vecindarios más cercanos para cada punto de datos; 2. Construya la matriz P utilizando la Ecuación (12); 3. Construya la matriz Laplaciana L utilizando la ecuación (16); 4. Construye la matriz M = (P − I)T (P − I) + λL; 5. Realice la descomposición de valores propios en M y construya la matriz Q∗ de acuerdo con la Ecuación (28); 6. Generar las asignaciones de clúster de cada punto de datos al discretizar adecuadamente Q∗. Tabla 2: Descripciones de los conjuntos de datos de documentos Conjuntos de datos Número de documentos Número de clases CSTR 476 4 WebKB4 4199 4 Reuters 2900 10 WebACE 2340 20 Newsgroup4 3970 4 CSTR. Este es el conjunto de datos de los resúmenes de informes técnicos publicados en el Departamento de Ciencias de la Computación de una universidad. El conjunto de datos contenía 476 resúmenes, los cuales fueron divididos en cuatro áreas de investigación: Procesamiento de Lenguaje Natural (NLP), Robótica/Visión, Sistemas y Teoría. WebKB. El conjunto de datos WebKB contiene páginas web recopiladas de los departamentos de informática de universidades. Hay alrededor de 8280 documentos y están divididos en 7 categorías: estudiante, facultad, personal, curso, proyecto, departamento y otros. El texto sin procesar es de aproximadamente 27MB. Entre estas 7 categorías, estudiante, facultad, curso y proyecto son las cuatro categorías que representan entidades más pobladas. El subconjunto asociado suele ser llamado WebKB4. Reuters. La colección de pruebas de categorización de texto Reuters-21578 contiene documentos recopilados de la agencia de noticias Reuters en 1987. Es un punto de referencia estándar para la categorización de texto y contiene 135 categorías. En nuestros experimentos, utilizamos un subconjunto de la colección de datos que incluye las 10 categorías más frecuentes entre los 135 temas y lo llamamos Reuters-top 10. WebACE. El conjunto de datos WebACE proviene del proyecto WebACE y ha sido utilizado para la agrupación de documentos [17][5]. El conjunto de datos WebACE contiene 2340 documentos que consisten en artículos de noticias del servicio de noticias de Reuters a través de la web en octubre de 1997. Estos documentos están divididos en 20 clases. Not enough context provided. El conjunto de datos News4 utilizado en nuestros experimentos se selecciona del famoso conjunto de datos 20-newsgroups. El tema rec que contiene autos, motocicletas, béisbol y hockey fue seleccionado de la versión 20news-18828. El conjunto de datos News4 contiene 3970 vectores de documentos. Para preprocesar los conjuntos de datos, eliminamos las palabras vacías utilizando una lista estándar de palabras vacías, se omiten todas las etiquetas HTML y se ignoran todos los campos de encabezado excepto el asunto y la organización de los artículos publicados. En todos nuestros experimentos, primero seleccionamos las 1000 palabras principales por información mutua con las etiquetas de clase. 3.2 Métricas de Evaluación En los experimentos, establecemos el número de clústeres igual al número real de clases C para todos los algoritmos de agrupamiento. Para evaluar su rendimiento, comparamos los grupos generados por estos algoritmos con las clases reales mediante el cálculo de las siguientes dos medidas de rendimiento. Precisión de agrupamiento (Acc). La primera medida de rendimiento es la Precisión de Agrupamiento, que descubre la relación uno a uno entre los grupos y las clases, y mide en qué medida cada grupo contenía puntos de datos de la clase correspondiente. Resume todo el grado de coincidencia entre todos los pares de clases y clusters. La precisión del agrupamiento se puede calcular como: Acc = 1 N max   Ck,Lm T(Ck, Lm)   , (29) donde Ck denota el k-ésimo clúster en los resultados finales, y Lm es la verdadera clase m-ésima. T(Ck, Lm) es el número de entidades que pertenecen a la clase m y están asignadas al clúster k. La precisión calcula la suma máxima de T(Ck, Lm) para todos los pares de clústeres y clases, y estos pares no tienen superposiciones. La mayor precisión de agrupamiento significa un mejor rendimiento de agrupamiento. Información Mutua Normalizada (NMI). Otro métrica de evaluación que adoptamos aquí es la Información Mutua Normalizada (NMI) [23], la cual es ampliamente utilizada para determinar la calidad de los grupos. Para dos variables aleatorias X e Y, el NMI se define como: NMI(X, Y) = I(X, Y) H(X)H(Y), donde I(X, Y) es la información mutua entre X e Y, mientras que H(X) y H(Y) son las entropías de X e Y respectivamente. Se puede ver que NMI(X, X) = 1, que es el valor máximo posible de NMI. Dado un resultado de agrupamiento, el NMI en la ecuación (30) se estima como NMI = C k=1 C m=1 nk,mlog n·nk,m nk ˆnm C k=1 nklog nk n C m=1 ˆnmlog ˆnm n, (31), donde nk denota el número de datos contenidos en el grupo Ck (1 k C), ˆnm es el número de datos pertenecientes a la clase m-ésima (1 m C), y nk,m denota el número de datos que están en la intersección entre el grupo Ck y la clase m-ésima. El valor calculado en la Ecuación (31) se utiliza como medida de rendimiento para el resultado de agrupamiento dado. A mayor valor, mejor rendimiento de agrupamiento. 3.3 Comparaciones Hemos realizado evaluaciones de rendimiento exhaustivas probando nuestro método y comparándolo con otros 8 métodos representativos de agrupamiento de datos utilizando las mismas corpora de datos. Los algoritmos que evaluamos se enumeran a continuación. 1. K-means tradicional (KM). 2. K-medias esférico (SKM). La implementación se basa en [9]. 3. Modelo de Mezcla Gaussiana (GMM). La implementación se basa en [16]. 4. Agrupamiento espectral con cortes normalizados (Ncut). La implementación se basa en [26], y la varianza de la similitud gaussiana se determina mediante Escalado Local [30]. Ten en cuenta que el criterio que Ncut busca minimizar es simplemente el regularizador global en nuestro algoritmo CLGR, excepto que Ncut utiliza el Laplaciano normalizado. 5. Agrupamiento utilizando Regularización Local Pura (CPLR). En este método simplemente minimizamos Jl (definido en la Ecuación (24)), y los resultados de agrupamiento pueden obtenerse realizando la descomposición de valores propios en la matriz (I − P)T (I − P) con algunos métodos de discretización adecuados. 6. Iteración de subespacio adaptativa (ASI). La implementación se basa en [14]. 7. Factorización de matrices no negativas (NMF). La implementación se basa en [27]. 8. Factorización Tri-Factorización de Matrices No Negativas (TNMF) [12]. La implementación se basa en [15]. Para la eficiencia computacional, en la implementación de CPLR y nuestro algoritmo CLGR, hemos establecido todos los parámetros de regularización local {λi}n i=1 para ser idénticos, los cuales se establecen mediante búsqueda en cuadrícula de {0.1, 1, 10}. El tamaño de los vecindarios k más cercanos se establece mediante una búsqueda en cuadrícula de {20, 40, 80}. Para el método CLGR, su parámetro de regularización global se establece mediante una búsqueda en cuadrícula de {0.1, 1, 10}. Al construir el regularizador global, hemos adoptado el método de escalamiento local [30] para construir la matriz Laplaciana. El método de discretización final adoptado en estos dos métodos es el mismo que en [26], ya que nuestros experimentos muestran que el uso de dicho método puede lograr mejores resultados que el uso de métodos basados en kmeans como en [20]. 3.4 Resultados Experimentales Los resultados de comparación de precisión de agrupamiento se muestran en la tabla 3, y los resultados de comparación de información mutua normalizada se resumen en la tabla 4. De las dos tablas principalmente observamos que: 1. Nuestro método CLGR supera a todos los demás métodos de agrupamiento de documentos en la mayoría de los conjuntos de datos. Para la agrupación de documentos, el método Spherical k-means suele superar al método tradicional de agrupación k-means, y el método GMM puede lograr resultados competitivos en comparación con el método Spherical k-means; 3. Los resultados obtenidos de los algoritmos tipo k-means y GMM suelen ser peores que los resultados obtenidos de Clustering Espectral. Dado que el Agrupamiento Espectral puede ser visto como una versión ponderada del kernel k-means, puede obtener buenos resultados cuando los clusters de datos tienen formas arbitrarias. Esto corrobora que los vectores de los documentos no están distribuidos regularmente (esféricos o elípticos). 4. Las comparaciones experimentales verifican empíricamente la equivalencia entre NMF y Clustering Espectral, como se muestra en la Tabla 3: Precisión de agrupamiento de los diversos métodos CSTR WebKB4 Reuters WebACE News4 KM 0.4256 0.3888 0.4448 0.4001 0.3527 SKM 0.4690 0.4318 0.5025 0.4458 0.3912 GMM 0.4487 0.4271 0.4897 0.4521 0.3844 NMF 0.5713 0.4418 0.4947 0.4761 0.4213 Ncut 0.5435 0.4521 0.4896 0.4513 0.4189 ASI 0.5621 0.4752 0.5235 0.4823 0.4335 TNMF 0.6040 0.4832 0.5541 0.5102 0.4613 CPLR 0.5974 0.5020 0.4832 0.5213 0.4890 CLGR 0.6235 0.5228 0.5341 0.5376 0.5102 Tabla 4: Resultados de información mutua normalizada de los diversos métodos CSTR WebKB4 Reuters WebACE News4 KM 0.3675 0.3023 0.4012 0.3864 0.3318 SKM 0.4027 0.4155 0.4587 0.4003 0.4085 GMM 0.4034 0.4093 0.4356 0.4209 0.3994 NMF 0.5235 0.4517 0.4402 0.4359 0.4130 Ncut 0.4833 0.4497 0.4392 0.4289 0.4231 ASI 0.5008 0.4833 0.4769 0.4817 0.4503 TNMF 0.5724 0.5011 0.5132 0.5328 0.4749 CPLR 0.5695 0.5231 0.4402 0.5543 0.4690 CLGR 0.6012 0.5434 0.4935 0.5390 0.4908 ha sido demostrado teóricamente en [10]. Se puede observar en las tablas que NMF y el Agrupamiento Espectral suelen llevar a resultados de agrupamiento similares. 5. Los métodos basados en co-agrupamiento (TNMF y ASI) suelen lograr mejores resultados que los métodos tradicionales basados únicamente en vectores de documentos. Dado que estos métodos realizan una selección implícita de características en cada iteración, proporcionan una métrica adaptativa para medir el vecindario y, por lo tanto, tienden a producir mejores resultados de agrupamiento. 6. Los resultados obtenidos a través de CPLR suelen ser mejores que los resultados obtenidos a través de Agrupamiento Espectral, lo cual respalda la teoría de Vapnik [24] de que a veces los algoritmos de aprendizaje local pueden obtener mejores resultados que los algoritmos de aprendizaje global. Además de los experimentos de comparación mencionados anteriormente, también probamos la sensibilidad de los parámetros de nuestro método. Principalmente hay dos conjuntos de parámetros en nuestro algoritmo CLGR, los parámetros de regularización locales y globales ({λi}n i=1 y λ, como hemos mencionado en la sección 3.3, hemos establecido que todos los λis son idénticos a λ∗ en nuestros experimentos), y el tamaño de los vecindarios. Por lo tanto, también hemos realizado dos series de experimentos: 1. Fijando el tamaño de los vecindarios y probando el rendimiento del agrupamiento con diferentes valores de λ∗ y λ. En este conjunto de experimentos, encontramos que nuestro algoritmo CLGR puede lograr buenos resultados cuando los dos parámetros de regularización no son ni demasiado grandes ni demasiado pequeños. Normalmente nuestro método puede lograr buenos resultados cuando λ∗ y λ están alrededor de 0.1. La Figura 1 nos muestra un ejemplo de prueba en el conjunto de datos WebACE. Fijando los parámetros de regularización local y global, y probando el rendimiento del agrupamiento con diferentes valores de -5, -4.5, -4, -3.5, -3, -5, -4.5, -4, -3.5, -3, 0.35, 0.4, 0.45, 0.5, 0.55 para el parámetro de regularización local (valor logarítmico base 2) y para el parámetro de regularización global (valor logarítmico base 2), se obtuvo una precisión de agrupamiento. Figura 1: Resultados de la prueba de sensibilidad de parámetros en el conjunto de datos WebACE con el tamaño de vecindario fijo en 20, donde el eje x e y representan el valor logarítmico base 2 de λ∗ y λ. En este conjunto de experimentos, encontramos que el vecindario con un tamaño demasiado grande o demasiado pequeño deteriorará todos los resultados finales de agrupamiento. Esto puede entenderse fácilmente ya que cuando el tamaño del vecindario es muy pequeño, los puntos de datos utilizados para entrenar los clasificadores locales pueden no ser suficientes; cuando el tamaño del vecindario es muy grande, los clasificadores entrenados tenderán a ser globales y no podrán capturar las características locales típicas. La Figura 2 nos muestra un ejemplo de prueba en el conjunto de datos WebACE. Por lo tanto, podemos ver que nuestro algoritmo CLGR (1) puede lograr resultados satisfactorios y (2) no es muy sensible a la elección de parámetros, lo que lo hace práctico en aplicaciones del mundo real. CONCLUSIONES Y TRABAJOS FUTUROS En este artículo, desarrollamos un nuevo algoritmo de agrupamiento llamado agrupamiento con regularización local y global. Nuestro método conserva el mérito de los algoritmos de aprendizaje local y el agrupamiento espectral. Nuestros experimentos muestran que el algoritmo propuesto supera a la mayoría de los algoritmos de vanguardia en muchos conjuntos de datos de referencia. En el futuro, nos enfocaremos en la selección de parámetros y los problemas de aceleración del algoritmo CLGR. REFERENCIAS [1] L. Baker y A. McCallum. Agrupamiento distribucional de palabras para clasificación de texto. En Actas de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1998. [2] M. Belkin y P. Niyogi. Mapeo de Eigen de Laplaciano para Reducción de Dimensionalidad y Representación de Datos. Computación Neural, 15 (6):1373-1396. Junio de 2003. [3] M. Belkin y P. Niyogi. Hacia una Fundación Teórica para Métodos de Manifold Basados en Laplacianos. En Actas de la 18ª Conferencia sobre Teoría del Aprendizaje (COLT). 2005. 10 20 30 40 50 60 70 80 90 100 0.35 0.4 0.45 0.5 0.55 tamaño de la precisión de agrupamiento del vecindario Figura 2: Resultados de pruebas de sensibilidad de parámetros en el conjunto de datos WebACE con los parámetros de regularización fijados en 0.1, y el tamaño del vecindario variando de 10 a 100. [4] M. Belkin, P. Niyogi y V. Sindhwani. Regularización de variedades: un marco geométrico para el aprendizaje a partir de ejemplos. Revista de Investigación en Aprendizaje Automático 7, 1-48, 2006. [5] D. Boley. División Direccional Principal. Minería de datos y descubrimiento de conocimiento, 2:325-344, 1998. [6] L. Bottou y V. Vapnik. Algoritmos de aprendizaje local. Computación Neural, 4:888-900, 1992. [7] P. K. Chan, D. F. Schlag y J. Y. Zien. Particionamiento y agrupamiento K-way de corte de razón espectral. IEEE Trans. Diseño Asistido por Computadora, 13:1088-1096, Sep. 1994. [8] D. R. Cutting, D. R. Karger, J. O. Pederson y J. W. Tukey. Dispersión/Recolección: Un enfoque basado en clústeres para explorar grandes colecciones de documentos. En Actas de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1992. [9] I. S. Dhillon y D. S. Modha. Descomposiciones de conceptos para datos de texto grandes y dispersos utilizando agrupamiento. Aprendizaje automático, vol. 42(1), páginas 143-175, enero de 2001. [10] C. Ding, X. Él, y H. Simon. Sobre la equivalencia entre la factorización de matrices no negativas y el agrupamiento espectral. En Actas de la Conferencia de Minería de Datos de SIAM, 2005. [11] C. Ding, X. Él, H. Zha, M. Gu y H. D. Simon. Un algoritmo de corte min-max para particionar grafos y agrupar datos. En Proc. de la 1ra Conferencia Internacional sobre Minería de Datos (ICDM), páginas 107-114, 2001. [12] C. Ding, T. Li, W. Peng y H. Park. Tri-factorizaciones de matrices no negativas ortogonales para agrupamiento. En Actas de la Duodécima Conferencia Internacional de la ACM SIGKDD sobre Descubrimiento de Conocimiento y Minería de Datos, 2006. [13] R. O. Duda, P. E. Hart y D. G. Stork. Clasificación de patrones. John Wiley & Sons, Inc., 2001. [14] T. Li, S. Ma y M. Ogihara. Agrupamiento de documentos a través de la Iteración de Subespacios Adaptativos. En Actas de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2004. [15] T. Li y C. Ding. Las relaciones entre varios métodos de factorización de matrices no negativas para agrupamiento. En Actas de la 6ta Conferencia Internacional sobre Minería de Datos (ICDM). 2006. [16] X. Liu y Y. Gong. Agrupación de documentos con capacidades de refinamiento de clústeres y selección de modelos. En Proc. de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2002. [17] E. Han, D. Boley, M. Gini, R. Gross, K. Hastings, G. Karypis, V. Kumar, B. Mobasher y J. Moore. WebACE: Un agente web para la categorización y exploración de documentos. En Actas de la 2ª Conferencia Internacional sobre Agentes Autónomos (Agents98). ACM Press, 1998. [18] M. Hein, J. Y. Audibert y U. von Luxburg. De Gráficas a Variedades - Consistencia Puntual Débil y Fuerte de los Laplacianos de Gráficas. En Actas de la 18ª Conferencia sobre Teoría del Aprendizaje (COLT), 470-485. 2005. [19] J. Él, M. Lan, C.-L. Tan, S.-Y. Sung, y H.-B. Bajo. Inicialización de algoritmos de refinamiento de clústeres: una revisión y estudio comparativo. En Proc. de Inter. Conferencia Conjunta sobre Redes Neuronales, 2004. [20] A. Y. Ng, M. I. Jordan, Y. Weiss. En el agrupamiento espectral: análisis y un algoritmo. En Avances en Sistemas de Procesamiento de Información Neural 14. 2002. [21] B. Sch¨olkopf y A. Smola. Aprendizaje con Kernels. El MIT Press. Cambridge, Massachusetts. 2002. [22] J. Shi y J. Malik. Cortes normalizados y segmentación de imágenes. IEEE Trans. on Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000. [23] A. Strehl and J. Ghosh.\nIEEE Trans. on Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000. [23] A. Strehl y J. Ghosh. Conjuntos de agrupamientos: un marco de reutilización de conocimiento para combinar múltiples particiones. Revista de Investigación de Aprendizaje Automático, 3:583-617, 2002. [24] V. N. Vapnik. La naturaleza de la teoría del aprendizaje estadístico. Berlín: Springer-Verlag, 1995. [25] Wu, M. y Schölkopf, B. Un enfoque de aprendizaje local para el agrupamiento. En Avances en Sistemas de Procesamiento de Información Neural 18. 2006. [26] S. X. Yu, J. Shi. Agrupamiento espectral multiclase. En Actas de la Conferencia Internacional sobre Visión por Computadora, 2003. [27] W. Xu, X. Liu y Y. Gong. Agrupación de documentos basada en la factorización de matrices no negativas. En Actas de la Conferencia Internacional ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2003. [28] H. Zha, X. Él, C. Ding, M. Gu y H. Simon. Relajación espectral para el agrupamiento K-means. En NIPS 14. 2001. [29] T. Zhang y F. J. Oles. Categorización de texto basada en métodos de clasificación lineal regularizados. Revista de Recuperación de Información, 4:5-31, 2001. [30] L. Zelnik-Manor y P. Perona. Agrupamiento espectral autoajustable. En NIPS 17. 2005. [31] D. Zhou, O. Bousquet, T. N. Lal, J. Weston y B. Schölkopf. Aprendizaje con Consistencia Local y Global. NIPS 17, 2005. \n\nNIPS 17, 2005. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "manifold": {
            "translated_key": "variedad",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Regularized Clustering for Documents ∗ Fei Wang, Changshui Zhang State Key Lab of Intelligent Tech.",
                "and Systems Department of Automation, Tsinghua University Beijing, China, 100084 feiwang03@gmail.com Tao Li School of Computer Science Florida International University Miami, FL 33199, U.S.A. taoli@cs.fiu.edu ABSTRACT In recent years, document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering.",
                "In this paper, we propose a novel method for clustering documents using regularization.",
                "Unlike traditional globally regularized clustering methods, our method first construct a local regularized linear label predictor for each document vector, and then combine all those local regularizers with a global smoothness regularizer.",
                "So we call our algorithm Clustering with Local and Global Regularization (CLGR).",
                "We will show that the cluster memberships of the documents can be achieved by eigenvalue decomposition of a sparse symmetric matrix, which can be efficiently solved by iterative methods.",
                "Finally our experimental evaluations on several datasets are presented to show the superiorities of CLGR over traditional document clustering methods.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Clustering; I.2.6 [Artificial Intelligence]: Learning-Concept Learning General Terms Algorithms 1.",
                "INTRODUCTION Document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering.",
                "A good document clustering approach can assist the computers to automatically organize the document corpus into a meaningful cluster hierarchy for efficient browsing and navigation, which is very valuable for complementing the deficiencies of traditional information retrieval technologies.",
                "As pointed out by [8], the information retrieval needs can be expressed by a spectrum ranged from narrow keyword-matching based search to broad information browsing such as what are the major international events in recent months.",
                "Traditional document retrieval engines tend to fit well with the search end of the spectrum, i.e. they usually provide specified search for documents matching the users query, however, it is hard for them to meet the needs from the rest of the spectrum in which a rather broad or vague information is needed.",
                "In such cases, efficient browsing through a good cluster hierarchy will be definitely helpful.",
                "Generally, document clustering methods can be mainly categorized into two classes: hierarchical methods and partitioning methods.",
                "The hierarchical methods group the data points into a hierarchical tree structure using bottom-up or top-down approaches.",
                "For example, hierarchical agglomerative clustering (HAC) [13] is a typical bottom-up hierarchical clustering method.",
                "It takes each data point as a single cluster to start off with and then builds bigger and bigger clusters by grouping similar data points together until the entire dataset is encapsulated into one final cluster.",
                "On the other hand, partitioning methods decompose the dataset into a number of disjoint clusters which are usually optimal in terms of some predefined criterion functions.",
                "For instance, K-means [13] is a typical partitioning method which aims to minimize the sum of the squared distance between the data points and their corresponding cluster centers.",
                "In this paper, we will focus on the partitioning methods.",
                "As we know that there are two main problems existing in partitioning methods (like Kmeans and Gaussian Mixture Model (GMM) [16]): (1) the predefined criterion is usually non-convex which causes many local optimal solutions; (2) the iterative procedure (e.g. the Expectation Maximization (EM) algorithm) for optimizing the criterions usually makes the final solutions heavily depend on the initializations.",
                "In the last decades, many methods have been proposed to overcome the above problems of the partitioning methods [19][28].",
                "Recently, another type of partitioning methods based on clustering on data graphs have aroused considerable interests in the machine learning and data mining community.",
                "The basic idea behind these methods is to first model the whole dataset as a weighted graph, in which the graph nodes represent the data points, and the weights on the edges correspond to the similarities between pairwise points.",
                "Then the cluster assignments of the dataset can be achieved by optimizing some criterions defined on the graph.",
                "For example Spectral Clustering is one kind of the most representative graph-based clustering approaches, it generally aims to optimize some cut value (e.g.",
                "Normalized Cut [22], Ratio Cut [7], Min-Max Cut [11]) defined on an undirected graph.",
                "After some relaxations, these criterions can usually be optimized via eigen-decompositions, which is guaranteed to be global optimal.",
                "In this way, spectral clustering efficiently avoids the problems of the traditional partitioning methods as we introduced in last paragraph.",
                "In this paper, we propose a novel document clustering algorithm that inherits the superiority of spectral clustering, i.e. the final cluster results can also be obtained by exploit the eigen-structure of a symmetric matrix.",
                "However, unlike spectral clustering, which just enforces a smoothness constraint on the data labels over the whole data <br>manifold</br> [2], our method first construct a regularized linear label predictor for each data point from its neighborhood as in [25], and then combine the results of all these local label predictors with a global label smoothness regularizer.",
                "So we call our method Clustering with Local and Global Regularization (CLGR).",
                "The idea of incorporating both local and global information into label prediction is inspired by the recent works on semi-supervised learning [31], and our experimental evaluations on several real document datasets show that CLGR performs better than many state-of-the-art clustering methods.",
                "The rest of this paper is organized as follows: in section 2 we will introduce our CLGR algorithm in detail.",
                "The experimental results on several datasets are presented in section 3, followed by the conclusions and discussions in section 4. 2.",
                "THE PROPOSED ALGORITHM In this section, we will introduce our Clustering with Local and Global Regularization (CLGR) algorithm in detail.",
                "First lets see the how the documents are represented throughout this paper. 2.1 Document Representation In our work, all the documents are represented by the weighted term-frequency vectors.",
                "Let W = {w1, w2, · · · , wm} be the complete vocabulary set of the document corpus (which is preprocessed by the stopwords removal and words stemming operations).",
                "The term-frequency vector xi of document di is defined as xi = [xi1, xi2, · · · , xim]T , xik = tik log n idfk , where tik is the term frequency of wk ∈ W, n is the size of the document corpus, idfk is the number of documents that contain word wk.",
                "In this way, xi is also called the TFIDF representation of document di.",
                "Furthermore, we also normalize each xi (1 i n) to have a unit length, so that each document is represented by a normalized TF-IDF vector. 2.2 Local Regularization As its name suggests, CLGR is composed of two parts: local regularization and global regularization.",
                "In this subsection we will introduce the local regularization part in detail. 2.2.1 Motivation As we know that clustering is one type of learning techniques, it aims to organize the dataset in a reasonable way.",
                "Generally speaking, learning can be posed as a problem of function estimation, from which we can get a good classification function that will assign labels to the training dataset and even the unseen testing dataset with some cost minimized [24].",
                "For example, in the two-class classification scenario1 (in which we exactly know the label of each document), a linear classifier with least square fit aims to learn a column vector w such that the squared cost J = 1 n (wT xi − yi)2 (1) is minimized, where yi ∈ {+1, −1} is the label of xi.",
                "By taking ∂J /∂w = 0, we get the solution w∗ = n i=1 xixT i −1 n i=1 xiyi , (2) which can further be written in its matrix form as w∗ = XXT −1 Xy, (3) where X = [x1, x2, · · · , xn] is an m × n document matrix, y = [y1, y2, · · · , yn]T is the label vector.",
                "Then for a test document t, we can determine its label by l = sign(w∗T u), (4) where sign(·) is the sign function.",
                "A natural problem in Eq. (3) is that the matrix XXT may be singular and thus not invertable (e.g. when m n).",
                "To avoid such a problem, we can add a regularization term and minimize the following criterion J = 1 n n i=1 (wT xi − yi)2 + λ w 2 , (5) where λ is a regularization parameter.",
                "Then the optimal solution that minimize J is given by w∗ = XXT + λnI −1 Xy, (6) where I is an m × m identity matrix.",
                "It has been reported that the regularized linear classifier can achieve very good results on text classification problems [29].",
                "However, despite its empirical success, the regularized linear classifier is on earth a global classifier, i.e. w∗ is estimated using the whole training set.",
                "According to [24], this may not be a smart idea, since a unique w∗ may not be good enough for predicting the labels of the whole input space.",
                "In order to get better predictions, [6] proposed to train classifiers locally and use them to classify the testing points.",
                "For example, a testing point will be classified by the local classifier trained using the training points located in the vicinity 1 In the following discussions we all assume that the documents coming from only two classes.",
                "The generalizations of our method to multi-class cases will be discussed in section 2.5. of it.",
                "Although this method seems slow and stupid, it is reported that it can get better performances than using a unique global classifier on certain tasks [6]. 2.2.2 Constructing the Local Regularized Predictors Inspired by their success, we proposed to apply the local learning algorithms for clustering.",
                "The basic idea is that, for each document vector xi (1 i n), we train a local label predictor based on its k-nearest neighborhood Ni, and then use it to predict the label of xi.",
                "Finally we will combine all those local predictors by minimizing the sum of their prediction errors.",
                "In this subsection we will introduce how to construct those local predictors.",
                "Due to the simplicity and effectiveness of the regularized linear classifier that we have introduced in section 2.2.1, we choose it to be our local label predictor, such that for each document xi, the following criterion is minimized Ji = 1 ni xj ∈Ni wT i xj − qj 2 + λi wi 2 , (7) where ni = |Ni| is the cardinality of Ni, and qj is the cluster membership of xj.",
                "Then using Eq. (6), we can get the optimal solution is w∗ i = XiXT i + λiniI −1 Xiqi, (8) where Xi = [xi1, xi2, · · · , xini ], and we use xik to denote the k-th nearest neighbor of xi. qi = [qi1, qi2, · · · , qini ]T with qik representing the cluster assignment of xik.",
                "The problem here is that XiXT i is an m × m matrix with m ni, i.e. we should compute the inverse of an m × m matrix for every document vector, which is computationally prohibited.",
                "Fortunately, we have the following theorem: Theorem 1. w∗ i in Eq. (8) can be rewritten as w∗ i = Xi XT i Xi + λiniIi −1 qi, (9) where Ii is an ni × ni identity matrix.",
                "Proof.",
                "Since w∗ i = XiXT i + λiniI −1 Xiqi, then XiXT i + λiniI w∗ i = Xiqi =⇒ XiXT i w∗ i + λiniw∗ i = Xiqi =⇒ w∗ i = (λini)−1 Xi qi − XT i w∗ i .",
                "Let β = (λini)−1 qi − XT i w∗ i , then w∗ i = Xiβ =⇒ λiniβ = qi − XT i w∗ i = qi − XT i Xiβ =⇒ qi = XT i Xi + λiniIi β =⇒ β = XT i Xi + λiniIi −1 qi.",
                "Therefore w∗ i = Xiβ = Xi XT i Xi + λiniIi −1 qi 2 Using theorem 1, we only need to compute the inverse of an ni × ni matrix for every document to train a local label predictor.",
                "Moreover, for a new testing point u that falls into Ni, we can classify it by the sign of qu = w∗T i u = uT wi = uT Xi XT i Xi + λiniIi −1 qi.",
                "This is an attractive expression since we can determine the cluster assignment of u by using the inner-products between the points in {u ∪ Ni}, which suggests that such a local regularizer can easily be kernelized [21] as long as we define a proper kernel function. 2.2.3 Combining the Local Regularized Predictors After all the local predictors having been constructed, we will combine them together by minimizing Jl = n i=1 w∗T i xi − qi 2 , (10) which stands for the sum of the prediction errors for all the local predictors.",
                "Combining Eq. (10) with Eq. (6), we can get Jl = n i=1 w∗T i xi − qi 2 = n i=1 xT i Xi XT i Xi + λiniIi −1 qi − qi 2 = Pq − q 2 , (11) where q = [q1, q2, · · · , qn]T , and the P is an n × n matrix constructing in the following way.",
                "Let αi = xT i Xi XT i Xi + λiniIi −1 , then Pij = αi j, if xj ∈ Ni 0, otherwise , (12) where Pij is the (i, j)-th entry of P, and αi j represents the j-th entry of αi .",
                "Till now we can write the criterion of clustering by combining locally regularized linear label predictors Jl in an explicit mathematical form, and we can minimize it directly using some standard optimization techniques.",
                "However, the results may not be good enough since we only exploit the local informations of the dataset.",
                "In the next subsection, we will introduce a global regularization criterion and combine it with Jl, which aims to find a good clustering result in a local-global way. 2.3 Global Regularization In data clustering, we usually require that the cluster assignments of the data points should be sufficiently smooth with respect to the underlying data <br>manifold</br>, which implies (1) the nearby points tend to have the same cluster assignments; (2) the points on the same structure (e.g. submanifold or cluster) tend to have the same cluster assignments [31].",
                "Without the loss of generality, we assume that the data points reside (roughly) on a low-dimensional <br>manifold</br> M2 , and q is the cluster assignment function defined on M, i.e. 2 We believe that the text data are also sampled from some low dimensional <br>manifold</br>, since it is impossible for them to for ∀x ∈ M, q(x) returns the cluster membership of x.",
                "The smoothness of q over M can be calculated by the following Dirichlet integral [2] D[q] = 1 2 M q(x) 2 dM, (13) where the gradient q is a vector in the tangent space T Mx, and the integral is taken with respect to the standard measure on M. If we restrict the scale of q by q, q M = 1 (where ·, · M is the inner product induced on M), then it turns out that finding the smoothest function minimizing D[q] reduces to finding the eigenfunctions of the Laplace Beltrami operator L, which is defined as Lq −div q, (14) where div is the divergence of a vector field.",
                "Generally, the graph can be viewed as the discretized form of <br>manifold</br>.",
                "We can model the dataset as an weighted undirected graph as in spectral clustering [22], where the graph nodes are just the data points, and the weights on the edges represent the similarities between pairwise points.",
                "Then it can be shown that minimizing Eq. (13) corresponds to minimizing Jg = qT Lq = n i=1 (qi − qj)2 wij, (15) where q = [q1, q2, · · · , qn]T with qi = q(xi), L is the graph Laplacian with its (i, j)-th entry Lij =    di − wii, if i = j −wij, if xi and xj are adjacent 0, otherwise, (16) where di = j wij is the degree of xi, wij is the similarity between xi and xj.",
                "If xi and xj are adjacent3 , wij is usually computed in the following way wij = e − xi−xj 2 2σ2 , (17) where σ is a dataset dependent parameter.",
                "It is proved that under certain conditions, such a form of wij to determine the weights on graph edges leads to the convergence of graph Laplacian to the Laplace Beltrami operator [3][18].",
                "In summary, using Eq. (15) with exponential weights can effectively measure the smoothness of the data assignments with respect to the intrinsic data <br>manifold</br>.",
                "Thus we adopt it as a global regularizer to punish the smoothness of the predicted data assignments. 2.4 Clustering with Local and Global Regularization Combining the contents we have introduced in section 2.2 and section 2.3 we can derive the clustering criterion is minq J = Jl + λJg = Pq − q 2 + λqT Lq s.t. qi ∈ {−1, +1}, (18) where P is defined as in Eq. (12), and λ is a regularization parameter to trade off Jl and Jg.",
                "However, the discrete fill in the whole high-dimensional sample space.",
                "And it has been shown that the <br>manifold</br> based methods can achieve good results on text classification tasks [31]. 3 In this paper, we define xi and xj to be adjacent if xi ∈ N(xj) or xj ∈ N(xi). constraint of pi makes the problem an NP hard integer programming problem.",
                "A natural way for making the problem solvable is to remove the constraint and relax qi to be continuous, then the objective that we aims to minimize becomes J = Pq − q 2 + λqT Lq = qT (P − I)T (P − I)q + λqT Lq = qT (P − I)T (P − I) + λL q, (19) and we further add a constraint qT q = 1 to restrict the scale of q.",
                "Then our objective becomes minq J = qT (P − I)T (P − I) + λL q s.t. qT q = 1 (20) Using the Lagrangian method, we can derive that the optimal solution q corresponds to the smallest eigenvector of the matrix M = (P − I)T (P − I) + λL, and the cluster assignment of xi can be determined by the sign of qi, i.e. xi will be classified as class one if qi > 0, otherwise it will be classified as class 2. 2.5 Multi-Class CLGR In the above we have introduced the basic framework of Clustering with Local and Global Regularization (CLGR) for the two-class clustering problem, and we will extending it to multi-class clustering in this subsection.",
                "First we assume that all the documents belong to C classes indexed by L = {1, 2, · · · , C}. qc is the classification function for class c (1 c C), such that qc (xi) returns the confidence that xi belongs to class c. Our goal is to obtain the value of qc (xi) (1 c C, 1 i n), and the cluster assignment of xi can be determined by {qc (xi)}C c=1 using some proper discretization methods that we will introduce later.",
                "Therefore, in this multi-class case, for each document xi (1 i n), we will construct C locally linear regularized label predictors whose normal vectors are wc∗ i = Xi XT i Xi + λiniIi −1 qc i (1 c C), (21) where Xi = [xi1, xi2, · · · , xini ] with xik being the k-th neighbor of xi, and qc i = [qc i1, qc i2, · · · , qc ini ]T with qc ik = qc (xik).",
                "Then (wc∗ i )T xi returns the predicted confidence of xi belonging to class c. Hence the local prediction error for class c can be defined as J c l = n i=1 (wc∗ i ) T xi − qc i 2 , (22) And the total local prediction error becomes Jl = C c=1 J c l = C c=1 n i=1 (wc∗ i ) T xi − qc i 2 . (23) As in Eq. (11), we can define an n×n matrix P (see Eq. (12)) and rewrite Jl as Jl = C c=1 J c l = C c=1 Pqc − qc 2 . (24) Similarly we can define the global smoothness regularizer in multi-class case as Jg = C c=1 n i=1 (qc i − qc j )2 wij = C c=1 (qc )T Lqc . (25) Then the criterion to be minimized for CLGR in multi-class case becomes J = Jl + λJg = C c=1 Pqc − qc 2 + λ(qc )T Lqc = C c=1 (qc )T (P − I)T (P − I) + λL qc = trace QT (P − I)T (P − I) + λL Q , (26) where Q = [q1 , q2 , · · · , qc ] is an n × c matrix, and trace(·) returns the trace of a matrix.",
                "The same as in Eq. (20), we also add the constraint that QT Q = I to restrict the scale of Q.",
                "Then our optimization problem becomes minQ J = trace QT (P − I)T (P − I) + λL Q s.t.",
                "QT Q = I, (27) From the Ky Fan theorem [28], we know the optimal solution of the above problem is Q∗ = [q∗ 1, q∗ 2, · · · , q∗ C ]R, (28) where q∗ k (1 k C) is the eigenvector corresponds to the k-th smallest eigenvalue of matrix (P − I)T (P − I) + λL, and R is an arbitrary C × C matrix.",
                "Since the values of the entries in Q∗ is continuous, we need to further discretize Q∗ to get the cluster assignments of all the data points.",
                "There are mainly two approaches to achieve this goal: 1.",
                "As in [20], we can treat the i-th row of Q as the embedding of xi in a C-dimensional space, and apply some traditional clustering methods like kmeans to clustering these embeddings into C clusters. 2.",
                "Since the optimal Q∗ is not unique (because of the existence of an arbitrary matrix R), we can pursue an optimal R that will rotate Q∗ to an indication matrix4 .",
                "The detailed algorithm can be referred to [26].",
                "The detailed algorithm procedure for CLGR is summarized in table 1. 3.",
                "EXPERIMENTS In this section, experiments are conducted to empirically compare the clustering results of CLGR with other 8 representitive document clustering algorithms on 5 datasets.",
                "First we will introduce the basic informations of those datasets. 3.1 Datasets We use a variety of datasets, most of which are frequently used in the information retrieval research.",
                "Table 2 summarizes the characteristics of the datasets. 4 Here an indication matrix T is a n×c matrix with its (i, j)th entry Tij ∈ {0, 1} such that for each row of Q∗ there is only one 1.",
                "Then the xi can be assigned to the j-th cluster such that j = argjQ∗ ij = 1.",
                "Table 1: Clustering with Local and Global Regularization (CLGR) Input: 1.",
                "Dataset X = {xi}n i=1; 2.",
                "Number of clusters C; 3.",
                "Size of the neighborhood K; 4.",
                "Local regularization parameters {λi}n i=1; 5.",
                "Global regularization parameter λ; Output: The cluster membership of each data point.",
                "Procedure: 1.",
                "Construct the K nearest neighborhoods for each data point; 2.",
                "Construct the matrix P using Eq. (12); 3.",
                "Construct the Laplacian matrix L using Eq. (16); 4.",
                "Construct the matrix M = (P − I)T (P − I) + λL; 5.",
                "Do eigenvalue decomposition on M, and construct the matrix Q∗ according to Eq. (28); 6.",
                "Output the cluster assignments of each data point by properly discretize Q∗ .",
                "Table 2: Descriptions of the document datasets Datasets Number of documents Number of classes CSTR 476 4 WebKB4 4199 4 Reuters 2900 10 WebACE 2340 20 Newsgroup4 3970 4 CSTR.",
                "This is the dataset of the abstracts of technical reports published in the Department of Computer Science at a university.",
                "The dataset contained 476 abstracts, which were divided into four research areas: Natural Language Processing(NLP), Robotics/Vision, Systems, and Theory.",
                "WebKB.",
                "The WebKB dataset contains webpages gathered from university computer science departments.",
                "There are about 8280 documents and they are divided into 7 categories: student, faculty, staff, course, project, department and other.",
                "The raw text is about 27MB.",
                "Among these 7 categories, student, faculty, course and project are four most populous entity-representing categories.",
                "The associated subset is typically called WebKB4.",
                "Reuters.",
                "The Reuters-21578 Text Categorization Test collection contains documents collected from the Reuters newswire in 1987.",
                "It is a standard text categorization benchmark and contains 135 categories.",
                "In our experiments, we use a subset of the data collection which includes the 10 most frequent categories among the 135 topics and we call it Reuters-top 10.",
                "WebACE.",
                "The WebACE dataset was from WebACE project and has been used for document clustering [17][5].",
                "The WebACE dataset contains 2340 documents consisting news articles from Reuters new service via the Web in October 1997.",
                "These documents are divided into 20 classes.",
                "News4.",
                "The News4 dataset used in our experiments are selected from the famous 20-newsgroups dataset5 .",
                "The topic rec containing autos, motorcycles, baseball and hockey was selected from the version 20news-18828.",
                "The News4 dataset contains 3970 document vectors. 5 http://people.csail.mit.edu/jrennie/20Newsgroups/ To pre-process the datasets, we remove the stop words using a standard stop list, all HTML tags are skipped and all header fields except subject and organization of the posted articles are ignored.",
                "In all our experiments, we first select the top 1000 words by mutual information with class labels. 3.2 Evaluation Metrics In the experiments, we set the number of clusters equal to the true number of classes C for all the clustering algorithms.",
                "To evaluate their performance, we compare the clusters generated by these algorithms with the true classes by computing the following two performance measures.",
                "Clustering Accuracy (Acc).",
                "The first performance measure is the Clustering Accuracy, which discovers the one-toone relationship between clusters and classes and measures the extent to which each cluster contained data points from the corresponding class.",
                "It sums up the whole matching degree between all pair class-clusters.",
                "Clustering accuracy can be computed as: Acc = 1 N max   Ck,Lm T(Ck, Lm)   , (29) where Ck denotes the k-th cluster in the final results, and Lm is the true m-th class.",
                "T(Ck, Lm) is the number of entities which belong to class m are assigned to cluster k. Accuracy computes the maximum sum of T(Ck, Lm) for all pairs of clusters and classes, and these pairs have no overlaps.",
                "The greater clustering accuracy means the better clustering performance.",
                "Normalized Mutual Information (NMI).",
                "Another evaluation metric we adopt here is the Normalized Mutual Information NMI [23], which is widely used for determining the quality of clusters.",
                "For two random variable X and Y, the NMI is defined as: NMI(X, Y) = I(X, Y) H(X)H(Y) , (30) where I(X, Y) is the mutual information between X and Y, while H(X) and H(Y) are the entropies of X and Y respectively.",
                "One can see that NMI(X, X) = 1, which is the maximal possible value of NMI.",
                "Given a clustering result, the NMI in Eq. (30) is estimated as NMI = C k=1 C m=1 nk,mlog n·nk,m nk ˆnm C k=1 nklog nk n C m=1 ˆnmlog ˆnm n , (31) where nk denotes the number of data contained in the cluster Ck (1 k C), ˆnm is the number of data belonging to the m-th class (1 m C), and nk,m denotes the number of data that are in the intersection between the cluster Ck and the m-th class.",
                "The value calculated in Eq. (31) is used as a performance measure for the given clustering result.",
                "The larger this value, the better the clustering performance. 3.3 Comparisons We have conducted comprehensive performance evaluations by testing our method and comparing it with 8 other representative data clustering methods using the same data corpora.",
                "The algorithms that we evaluated are listed below. 1.",
                "Traditional k-means (KM). 2.",
                "Spherical k-means (SKM).",
                "The implementation is based on [9]. 3.",
                "Gaussian Mixture Model (GMM).",
                "The implementation is based on [16]. 4.",
                "Spectral Clustering with Normalized Cuts (Ncut).",
                "The implementation is based on [26], and the variance of the Gaussian similarity is determined by Local Scaling [30].",
                "Note that the criterion that Ncut aims to minimize is just the global regularizer in our CLGR algorithm except that Ncut used the normalized Laplacian. 5.",
                "Clustering using Pure Local Regularization (CPLR).",
                "In this method we just minimize Jl (defined in Eq. (24)), and the clustering results can be obtained by doing eigenvalue decomposition on matrix (I − P)T (I − P) with some proper discretization methods. 6.",
                "Adaptive Subspace Iteration (ASI).",
                "The implementation is based on [14]. 7.",
                "Nonnegative Matrix Factorization (NMF).",
                "The implementation is based on [27]. 8.",
                "Tri-Factorization Nonnegative Matrix Factorization (TNMF) [12].",
                "The implementation is based on [15].",
                "For computational efficiency, in the implementation of CPLR and our CLGR algorithm, we have set all the local regularization parameters {λi}n i=1 to be identical, which is set by grid search from {0.1, 1, 10}.",
                "The size of the k-nearest neighborhoods is set by grid search from {20, 40, 80}.",
                "For the CLGR method, its global regularization parameter is set by grid search from {0.1, 1, 10}.",
                "When constructing the global regularizer, we have adopted the local scaling method [30] to construct the Laplacian matrix.",
                "The final discretization method adopted in these two methods is the same as in [26], since our experiments show that using such method can achieve better results than using kmeans based methods as in [20]. 3.4 Experimental Results The clustering accuracies comparison results are shown in table 3, and the normalized mutual information comparison results are summarized in table 4.",
                "From the two tables we mainly observe that: 1.",
                "Our CLGR method outperforms all other document clustering methods in most of the datasets; 2.",
                "For document clustering, the Spherical k-means method usually outperforms the traditional k-means clustering method, and the GMM method can achieve competitive results compared to the Spherical k-means method; 3.",
                "The results achieved from the k-means and GMM type algorithms are usually worse than the results achieved from Spectral Clustering.",
                "Since Spectral Clustering can be viewed as a weighted version of kernel k-means, it can obtain good results the data clusters are arbitrarily shaped.",
                "This corroborates that the documents vectors are not regularly distributed (spherical or elliptical). 4.",
                "The experimental comparisons empirically verify the equivalence between NMF and Spectral Clustering, which Table 3: Clustering accuracies of the various methods CSTR WebKB4 Reuters WebACE News4 KM 0.4256 0.3888 0.4448 0.4001 0.3527 SKM 0.4690 0.4318 0.5025 0.4458 0.3912 GMM 0.4487 0.4271 0.4897 0.4521 0.3844 NMF 0.5713 0.4418 0.4947 0.4761 0.4213 Ncut 0.5435 0.4521 0.4896 0.4513 0.4189 ASI 0.5621 0.4752 0.5235 0.4823 0.4335 TNMF 0.6040 0.4832 0.5541 0.5102 0.4613 CPLR 0.5974 0.5020 0.4832 0.5213 0.4890 CLGR 0.6235 0.5228 0.5341 0.5376 0.5102 Table 4: Normalized mutual information results of the various methods CSTR WebKB4 Reuters WebACE News4 KM 0.3675 0.3023 0.4012 0.3864 0.3318 SKM 0.4027 0.4155 0.4587 0.4003 0.4085 GMM 0.4034 0.4093 0.4356 0.4209 0.3994 NMF 0.5235 0.4517 0.4402 0.4359 0.4130 Ncut 0.4833 0.4497 0.4392 0.4289 0.4231 ASI 0.5008 0.4833 0.4769 0.4817 0.4503 TNMF 0.5724 0.5011 0.5132 0.5328 0.4749 CPLR 0.5695 0.5231 0.4402 0.5543 0.4690 CLGR 0.6012 0.5434 0.4935 0.5390 0.4908 has been proved theoretically in [10].",
                "It can be observed from the tables that NMF and Spectral Clustering usually lead to similar clustering results. 5.",
                "The co-clustering based methods (TNMF and ASI) can usually achieve better results than traditional purely document vector based methods.",
                "Since these methods perform an implicit feature selection at each iteration, provide an adaptive metric for measuring the neighborhood, and thus tend to yield better clustering results. 6.",
                "The results achieved from CPLR are usually better than the results achieved from Spectral Clustering, which supports Vapniks theory [24] that sometimes local learning algorithms can obtain better results than global learning algorithms.",
                "Besides the above comparison experiments, we also test the parameter sensibility of our method.",
                "There are mainly two sets of parameters in our CLGR algorithm, the local and global regularization parameters ({λi}n i=1 and λ, as we have said in section 3.3, we have set all λis to be identical to λ∗ in our experiments), and the size of the neighborhoods.",
                "Therefore we have also done two sets of experiments: 1.",
                "Fixing the size of the neighborhoods, and testing the clustering performance with varying λ∗ and λ.",
                "In this set of experiments, we find that our CLGR algorithm can achieve good results when the two regularization parameters are neither too large nor too small.",
                "Typically our method can achieve good results when λ∗ and λ are around 0.1.",
                "Figure 1 shows us such a testing example on the WebACE dataset. 2.",
                "Fixing the local and global regularization parameters, and testing the clustering performance with different −5 −4.5 −4 −3.5 −3 −5 −4.5 −4 −3.5 −3 0.35 0.4 0.45 0.5 0.55 local regularization para (log 2 value) global regularization para (log 2 value) clusteringaccuracy Figure 1: Parameter sensibility testing results on the WebACE dataset with the neighborhood size fixed to 20, and the x-axis and y-axis represents the log2 value of λ∗ and λ. sizes of neighborhoods.",
                "In this set of experiments, we find that the neighborhood with a too large or too small size will all deteriorate the final clustering results.",
                "This can be easily understood since when the neighborhood size is very small, then the data points used for training the local classifiers may not be sufficient; when the neighborhood size is very large, the trained classifiers will tend to be global and cannot capture the typical local characteristics.",
                "Figure 2 shows us a testing example on the WebACE dataset.",
                "Therefore, we can see that our CLGR algorithm (1) can achieve satisfactory results and (2) is not very sensitive to the choice of parameters, which makes it practical in real world applications. 4.",
                "CONCLUSIONS AND FUTURE WORKS In this paper, we derived a new clustering algorithm called clustering with local and global regularization.",
                "Our method preserves the merit of local learning algorithms and spectral clustering.",
                "Our experiments show that the proposed algorithm outperforms most of the state of the art algorithms on many benchmark datasets.",
                "In the future, we will focus on the parameter selection and acceleration issues of the CLGR algorithm. 5.",
                "REFERENCES [1] L. Baker and A. McCallum.",
                "Distributional Clustering of Words for Text Classification.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 1998. [2] M. Belkin and P. Niyogi.",
                "Laplacian Eigenmaps for Dimensionality Reduction and Data Representation.",
                "Neural Computation, 15 (6):1373-1396.",
                "June 2003. [3] M. Belkin and P. Niyogi.",
                "Towards a Theoretical Foundation for Laplacian-Based <br>manifold</br> Methods.",
                "In Proceedings of the 18th Conference on Learning Theory (COLT). 2005. 10 20 30 40 50 60 70 80 90 100 0.35 0.4 0.45 0.5 0.55 size of the neighborhood clusteringaccuracy Figure 2: Parameter sensibility testing results on the WebACE dataset with the regularization parameters being fixed to 0.1, and the neighborhood size varing from 10 to 100. [4] M. Belkin, P. Niyogi and V. Sindhwani.",
                "<br>manifold</br> Regularization: a Geometric Framework for Learning from Examples.",
                "Journal of Machine Learning Research 7, 1-48, 2006. [5] D. Boley.",
                "Principal Direction Divisive Partitioning.",
                "Data mining and knowledge discovery, 2:325-344, 1998. [6] L. Bottou and V. Vapnik.",
                "Local learning algorithms.",
                "Neural Computation, 4:888-900, 1992. [7] P. K. Chan, D. F. Schlag and J. Y. Zien.",
                "Spectral K-way Ratio-Cut Partitioning and Clustering.",
                "IEEE Trans.",
                "Computer-Aided Design, 13:1088-1096, Sep. 1994. [8] D. R. Cutting, D. R. Karger, J. O. Pederson and J. W. Tukey.",
                "Scatter/Gather: A Cluster-Based Approach to Browsing Large Document Collections.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 1992. [9] I. S. Dhillon and D. S. Modha.",
                "Concept Decompositions for Large Sparse Text Data using Clustering.",
                "Machine Learning, vol. 42(1), pages 143-175, January 2001. [10] C. Ding, X.",
                "He, and H. Simon.",
                "On the equivalence of nonnegative matrix factorization and spectral clustering.",
                "In Proceedings of the SIAM Data Mining Conference, 2005. [11] C. Ding, X.",
                "He, H. Zha, M. Gu, and H. D. Simon.",
                "A min-max cut algorithm for graph partitioning and data clustering.",
                "In Proc. of the 1st International Conference on Data Mining (ICDM), pages 107-114, 2001. [12] C. Ding, T. Li, W. Peng, and H. Park.",
                "Orthogonal Nonnegative Matrix Tri-Factorizations for Clustering.",
                "In Proceedings of the Twelfth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2006. [13] R. O. Duda, P. E. Hart, and D. G. Stork.",
                "Pattern Classification.",
                "John Wiley & Sons, Inc., 2001. [14] T. Li, S. Ma, and M. Ogihara.",
                "Document Clustering via Adaptive Subspace Iteration.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2004. [15] T. Li and C. Ding.",
                "The Relationships Among Various Nonnegative Matrix Factorization Methods for Clustering.",
                "In Proceedings of the 6th International Conference on Data Mining (ICDM). 2006. [16] X. Liu and Y. Gong.",
                "Document Clustering with Cluster Refinement and Model Selection Capabilities.",
                "In Proc. of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002. [17] E. Han, D. Boley, M. Gini, R. Gross, K. Hastings, G. Karypis, V. Kumar, B. Mobasher, and J. Moore.",
                "WebACE: A Web Agent for Document Categorization and Exploration.",
                "In Proceedings of the 2nd International Conference on Autonomous Agents (Agents98).",
                "ACM Press, 1998. [18] M. Hein, J. Y. Audibert, and U. von Luxburg.",
                "From Graphs to Manifolds - Weak and Strong Pointwise Consistency of Graph Laplacians.",
                "In Proceedings of the 18th Conference on Learning Theory (COLT), 470-485. 2005. [19] J.",
                "He, M. Lan, C.-L. Tan, S.-Y.",
                "Sung, and H.-B.",
                "Low.",
                "Initialization of Cluster Refinement Algorithms: A Review and Comparative Study.",
                "In Proc. of Inter.",
                "Joint Conference on Neural Networks, 2004. [20] A. Y. Ng, M. I. Jordan, Y. Weiss.",
                "On Spectral Clustering: Analysis and an algorithm.",
                "In Advances in Neural Information Processing Systems 14. 2002. [21] B. Sch¨olkopf and A. Smola.",
                "Learning with Kernels.",
                "The MIT Press.",
                "Cambridge, Massachusetts. 2002. [22] J. Shi and J. Malik.",
                "Normalized Cuts and Image Segmentation.",
                "IEEE Trans. on Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000. [23] A. Strehl and J. Ghosh.",
                "Cluster Ensembles - A Knowledge Reuse Framework for Combining Multiple Partitions.",
                "Journal of Machine Learning Research, 3:583-617, 2002. [24] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Berlin: Springer-Verlag, 1995. [25] Wu, M. and Sch¨olkopf, B.",
                "A Local Learning Approach for Clustering.",
                "In Advances in Neural Information Processing Systems 18. 2006. [26] S. X. Yu, J. Shi.",
                "Multiclass Spectral Clustering.",
                "In Proceedings of the International Conference on Computer Vision, 2003. [27] W. Xu, X. Liu and Y. Gong.",
                "Document Clustering Based On Non-Negative Matrix Factorization.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2003. [28] H. Zha, X.",
                "He, C. Ding, M. Gu and H. Simon.",
                "Spectral Relaxation for K-means Clustering.",
                "In NIPS 14. 2001. [29] T. Zhang and F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Journal of Information Retrieval, 4:5-31, 2001. [30] L. Zelnik-Manor and P. Perona.",
                "Self-Tuning Spectral Clustering.",
                "In NIPS 17. 2005. [31] D. Zhou, O. Bousquet, T. N. Lal, J. Weston and B. Sch¨olkopf.",
                "Learning with Local and Global Consistency.",
                "NIPS 17, 2005."
            ],
            "original_annotated_samples": [
                "However, unlike spectral clustering, which just enforces a smoothness constraint on the data labels over the whole data <br>manifold</br> [2], our method first construct a regularized linear label predictor for each data point from its neighborhood as in [25], and then combine the results of all these local label predictors with a global label smoothness regularizer.",
                "In the next subsection, we will introduce a global regularization criterion and combine it with Jl, which aims to find a good clustering result in a local-global way. 2.3 Global Regularization In data clustering, we usually require that the cluster assignments of the data points should be sufficiently smooth with respect to the underlying data <br>manifold</br>, which implies (1) the nearby points tend to have the same cluster assignments; (2) the points on the same structure (e.g. submanifold or cluster) tend to have the same cluster assignments [31].",
                "Without the loss of generality, we assume that the data points reside (roughly) on a low-dimensional <br>manifold</br> M2 , and q is the cluster assignment function defined on M, i.e. 2 We believe that the text data are also sampled from some low dimensional <br>manifold</br>, since it is impossible for them to for ∀x ∈ M, q(x) returns the cluster membership of x.",
                "Generally, the graph can be viewed as the discretized form of <br>manifold</br>.",
                "In summary, using Eq. (15) with exponential weights can effectively measure the smoothness of the data assignments with respect to the intrinsic data <br>manifold</br>."
            ],
            "translated_annotated_samples": [
                "Sin embargo, a diferencia del agrupamiento espectral, que simplemente impone una restricción de suavidad en las etiquetas de los datos sobre todo el <br>conjunto de datos</br> [2], nuestro método primero construye un predictor de etiquetas lineal regularizado para cada punto de datos a partir de su vecindario como en [25], y luego combina los resultados de todos estos predictores de etiquetas locales con un regularizador de suavidad de etiquetas global.",
                "En la siguiente subsección, introduciremos un criterio de regularización global y lo combinaremos con Jl, que tiene como objetivo encontrar un buen resultado de agrupamiento de manera local-global. 2.3 Regularización Global En el agrupamiento de datos, generalmente requerimos que las asignaciones de clúster de los puntos de datos sean lo suficientemente suaves con respecto a la <br>variedad</br> de datos subyacente, lo que implica (1) que los puntos cercanos tienden a tener las mismas asignaciones de clúster; (2) que los puntos en la misma estructura (por ejemplo, sub<br>variedad</br> o clúster) tienden a tener las mismas asignaciones de clúster [31].",
                "Sin pérdida de generalidad, asumimos que los puntos de datos residen (aproximadamente) en una <br>variedad</br> de baja dimensión M2, y q es la función de asignación de clúster definida en M, es decir, 2 Creemos que los datos de texto también se muestrean de alguna <br>variedad</br> de baja dimensión, ya que es imposible que para todos los x ∈ M, q(x) devuelva la membresía del clúster de x.",
                "Generalmente, el gráfico se puede ver como la forma discretizada de <br>una variedad</br>.",
                "En resumen, el uso de la Ec. (15) con pesos exponenciales puede medir de manera efectiva la suavidad de las asignaciones de datos con respecto a la <br>variedad intrínseca</br> de datos."
            ],
            "translated_text": "Agrupamiento regularizado para documentos ∗ Fei Wang, Changshui Zhang Laboratorio Clave del Estado de Tecnología Inteligente. En los últimos años, la agrupación de documentos ha estado recibiendo cada vez más atención como una técnica importante y fundamental para la organización no supervisada de documentos, la extracción automática de temas y la recuperación o filtrado rápido de información. En este artículo, proponemos un método novedoso para agrupar documentos utilizando regularización. A diferencia de los métodos de agrupamiento regularizados globalmente tradicionales, nuestro método primero construye un predictor de etiquetas lineal regularizado local para cada vector de documento, y luego combina todos esos regularizadores locales con un regularizador de suavidad global. Así que llamamos a nuestro algoritmo Agrupamiento con Regularización Local y Global (CLGR). Mostraremos que las pertenencias de los documentos a los grupos se pueden lograr mediante la descomposición de los valores propios de una matriz simétrica dispersa, la cual puede resolverse eficientemente mediante métodos iterativos. Finalmente, se presentan nuestras evaluaciones experimentales en varios conjuntos de datos para mostrar las ventajas de CLGR sobre los métodos tradicionales de agrupamiento de documentos. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información-Agrupamiento; I.2.6 [Inteligencia Artificial]: Aprendizaje-Aprendizaje de Conceptos Términos Generales Algoritmos 1. La agrupación de documentos ha estado recibiendo cada vez más atención como una técnica importante y fundamental para la organización no supervisada de documentos, la extracción automática de temas y la recuperación o filtrado rápido de información. Un buen enfoque de agrupación de documentos puede ayudar a las computadoras a organizar automáticamente el corpus de documentos en una jerarquía de clusters significativa para una navegación eficiente, lo cual es muy valioso para complementar las deficiencias de las tecnologías tradicionales de recuperación de información. Como señaló [8], las necesidades de recuperación de información pueden expresarse mediante un espectro que va desde la búsqueda basada en palabras clave estrechas hasta la exploración de información amplia, como cuáles son los principales eventos internacionales de los últimos meses. Los motores de búsqueda de documentos tradicionales tienden a adaptarse bien al extremo de la búsqueda, es decir, suelen proporcionar búsquedas específicas de documentos que coinciden con la consulta de los usuarios, sin embargo, les resulta difícil satisfacer las necesidades del resto del espectro en el que se necesita información más amplia o vaga. En tales casos, la navegación eficiente a través de una buena jerarquía de clústeres será definitivamente útil. Generalmente, los métodos de agrupamiento de documentos se pueden clasificar principalmente en dos clases: métodos jerárquicos y métodos de partición. Los métodos jerárquicos agrupan los puntos de datos en una estructura de árbol jerárquico utilizando enfoques de abajo hacia arriba o de arriba hacia abajo. Por ejemplo, el clustering jerárquico aglomerativo (HAC) [13] es un método típico de clustering jerárquico ascendente. Toma cada punto de datos como un único clúster para comenzar y luego construye clústeres más grandes agrupando puntos de datos similares juntos hasta que todo el conjunto de datos esté encapsulado en un único clúster final. Por otro lado, los métodos de particionamiento descomponen el conjunto de datos en un número de grupos disjuntos que suelen ser óptimos en términos de algunas funciones de criterio predefinidas. Por ejemplo, K-means es un método de particionamiento típico que tiene como objetivo minimizar la suma de las distancias al cuadrado entre los puntos de datos y sus centros de clúster correspondientes. En este documento, nos centraremos en los métodos de particionamiento. Como sabemos, existen dos problemas principales en los métodos de particionamiento (como Kmeans y el Modelo de Mezcla Gaussiana (GMM) [16]): (1) el criterio predefinido suele ser no convexo, lo que provoca muchas soluciones óptimas locales; (2) el procedimiento iterativo (por ejemplo, el algoritmo de Expectation Maximization (EM)) para optimizar los criterios suele hacer que las soluciones finales dependan en gran medida de las inicializaciones. En las últimas décadas, se han propuesto muchos métodos para superar los problemas mencionados de los métodos de particionamiento [19][28]. Recientemente, otro tipo de métodos de particionamiento basados en agrupamiento en grafos de datos han despertado un considerable interés en la comunidad de aprendizaje automático y minería de datos. La idea básica detrás de estos métodos es modelar primero el conjunto de datos completo como un grafo ponderado, en el que los nodos del grafo representan los puntos de datos y los pesos en los bordes corresponden a las similitudes entre los puntos en pares. Entonces, las asignaciones de clústeres del conjunto de datos se pueden lograr optimizando algunos criterios definidos en el gráfico. Por ejemplo, el Clustering Espectral es uno de los enfoques de clustering basados en grafos más representativos, generalmente tiene como objetivo optimizar algún valor de corte (por ejemplo, Corte normalizado [22], corte de razón [7], corte mínimo-máximo [11]) definidos en un grafo no dirigido. Después de algunas relajaciones, estos criterios generalmente pueden ser optimizados a través de descomposiciones propias, lo cual está garantizado que sea óptimo a nivel global. De esta manera, el agrupamiento espectral evita eficientemente los problemas de los métodos de particionamiento tradicionales que presentamos en el párrafo anterior. En este artículo, proponemos un algoritmo novedoso de agrupamiento de documentos que hereda la superioridad del agrupamiento espectral, es decir, los resultados finales de los grupos también pueden obtenerse explotando la estructura de los autovalores de una matriz simétrica. Sin embargo, a diferencia del agrupamiento espectral, que simplemente impone una restricción de suavidad en las etiquetas de los datos sobre todo el <br>conjunto de datos</br> [2], nuestro método primero construye un predictor de etiquetas lineal regularizado para cada punto de datos a partir de su vecindario como en [25], y luego combina los resultados de todos estos predictores de etiquetas locales con un regularizador de suavidad de etiquetas global. Así que llamamos a nuestro método Agrupamiento con Regularización Local y Global (CLGR). La idea de incorporar tanto información local como global en la predicción de etiquetas está inspirada en los trabajos recientes sobre aprendizaje semi-supervisado [31], y nuestras evaluaciones experimentales en varios conjuntos de datos reales de documentos muestran que CLGR funciona mejor que muchos métodos de agrupamiento de vanguardia. El resto de este documento está organizado de la siguiente manera: en la sección 2 presentaremos nuestro algoritmo CLGR en detalle. Los resultados experimentales en varios conjuntos de datos se presentan en la sección 3, seguidos de las conclusiones y discusiones en la sección 4. El ALGORITMO PROPUESTO En esta sección, presentaremos en detalle nuestro algoritmo de Agrupamiento con Regularización Local y Global (CLGR). Primero veamos cómo se representan los documentos a lo largo de este documento. 2.1 Representación de documentos En nuestro trabajo, todos los documentos se representan mediante vectores de frecuencia de términos ponderados. Sea W = {w1, w2, · · · , wm} el conjunto completo de vocabulario del corpus de documentos (que ha sido preprocesado mediante la eliminación de palabras vacías y operaciones de derivación de palabras). El vector de frecuencia de término xi del documento di se define como xi = [xi1, xi2, · · · , xim]T , xik = tik log n idfk , donde tik es la frecuencia del término wk ∈ W, n es el tamaño del corpus de documentos, idfk es el número de documentos que contienen la palabra wk. De esta manera, xi también se llama la representación TFIDF del documento di. Además, también normalizamos cada xi (1 i n) para que tenga una longitud unitaria, de modo que cada documento esté representado por un vector TF-IDF normalizado. 2.2 Regularización Local Como su nombre sugiere, CLGR está compuesto por dos partes: regularización local y regularización global. En esta subsección introduciremos detalladamente la parte de regularización local. 2.2.1 Motivación Como sabemos que el agrupamiento es un tipo de técnica de aprendizaje, tiene como objetivo organizar el conjunto de datos de una manera razonable. En general, el aprendizaje puede plantearse como un problema de estimación de funciones, a partir del cual podemos obtener una buena función de clasificación que asignará etiquetas al conjunto de datos de entrenamiento e incluso al conjunto de datos de prueba no vistos con algún costo minimizado [24]. Por ejemplo, en el escenario de clasificación de dos clases (en el que conocemos exactamente la etiqueta de cada documento), un clasificador lineal con ajuste de mínimos cuadrados tiene como objetivo aprender un vector columna w de manera que el costo al cuadrado J = 1 n (wT xi − yi)2 (1) se minimice, donde yi ∈ {+1, −1} es la etiqueta de xi. Al tomar ∂J /∂w = 0, obtenemos la solución w∗ = Σ i=1 n xixT i −1 Σ i=1 n xiyi, (2) que puede escribirse en su forma matricial como w∗ = XXT −1 Xy, (3) donde X = [x1, x2, · · · , xn] es una matriz de documentos m × n, y = [y1, y2, · · · , yn]T es el vector de etiquetas. Entonces, para un documento de prueba t, podemos determinar su etiqueta mediante l = signo(w∗T u), donde signo(·) es la función signo. Un problema natural en la ecuación (3) es que la matriz XXT puede ser singular y, por lo tanto, no invertible (por ejemplo, cuando m n). Para evitar tal problema, podemos agregar un término de regularización y minimizar el siguiente criterio J = 1 n n i=1 (wT xi − yi)2 + λ w 2, (5), donde λ es un parámetro de regularización. Entonces, la solución óptima que minimiza J está dada por w∗ = XXT + λnI −1 Xy, (6) donde I es una matriz identidad de tamaño m × m. Se ha informado que el clasificador lineal regularizado puede lograr resultados muy buenos en problemas de clasificación de texto [29]. Sin embargo, a pesar de su éxito empírico, el clasificador lineal regularizado es en realidad un clasificador global, es decir, w∗ se estima utilizando todo el conjunto de entrenamiento. Según [24], esta puede que no sea una idea inteligente, ya que un único w∗ puede que no sea lo suficientemente bueno para predecir las etiquetas de todo el espacio de entrada. Para obtener predicciones más precisas, [6] propuso entrenar clasificadores localmente y utilizarlos para clasificar los puntos de prueba. Por ejemplo, un punto de prueba será clasificado por el clasificador local entrenado utilizando los puntos de entrenamiento ubicados en la vecindad 1. En las siguientes discusiones todos asumimos que los documentos provienen solo de dos clases. Las generalizaciones de nuestro método a casos de múltiples clases se discutirán en la sección 2.5 de él. Aunque este método parece lento y estúpido, se informa que puede obtener mejores rendimientos que usar un clasificador global único en ciertas tareas [6]. 2.2.2 Construyendo los Predictores Localmente Regularizados Inspirados en su éxito, propusimos aplicar los algoritmos de aprendizaje local para el agrupamiento. La idea básica es que, para cada vector de documento xi (1 i n), entrenamos un predictor de etiquetas local basado en su vecindario k-más cercano Ni, y luego lo usamos para predecir la etiqueta de xi. Finalmente combinaremos todos esos predictores locales minimizando la suma de sus errores de predicción. En esta subsección introduciremos cómo construir esos predictores locales. Debido a la simplicidad y efectividad del clasificador lineal regularizado que hemos introducido en la sección 2.2.1, lo elegimos como nuestro predictor de etiquetas local, de modo que para cada documento xi, se minimiza el siguiente criterio Ji = 1 ni xj ∈Ni wT i xj − qj 2 + λi wi 2, (7), donde ni = |Ni| es la cardinalidad de Ni, y qj es la pertenencia al clúster de xj. Luego, utilizando la Ecuación (6), podemos obtener que la solución óptima es w∗ i = XiXT i + λiniI −1 Xiqi, (8), donde Xi = [xi1, xi2, · · · , xini], y usamos xik para denotar al k-ésimo vecino más cercano de xi. qi = [qi1, qi2, · · · , qini]T con qik representando la asignación de clúster de xik. El problema aquí es que XiXT i es una matriz m × m con m ni, es decir, deberíamos calcular la inversa de una matriz m × m para cada vector de documento, lo cual está prohibido computacionalmente. Afortunadamente, tenemos el siguiente teorema: Teorema 1. w∗ i en la Ec. (8) puede ser reescrito como w∗ i = Xi XT i Xi + λiniIi −1 qi, (9), donde Ii es una matriz identidad de tamaño ni × ni. Prueba. Dado que w∗ i = XiXT i + λiniI −1 Xiqi, entonces XiXT i + λiniI w∗ i = Xiqi =⇒ XiXT i w∗ i + λiniw∗ i = Xiqi =⇒ w∗ i = (λini)−1 Xi qi − XT i w∗ i. Sea β = (λini)−1 qi − XT i w∗ i, entonces w∗ i = Xiβ =⇒ λiniβ = qi − XT i w∗ i = qi − XT i Xiβ =⇒ qi = XT i Xi + λiniIi β =⇒ β = XT i Xi + λiniIi −1 qi. Por lo tanto, w∗ i = Xiβ = Xi XT i Xi + λiniIi −1 qi 2. Utilizando el teorema 1, solo necesitamos calcular la inversa de una matriz ni × ni para cada documento para entrenar un predictor de etiquetas local. Además, para un nuevo punto de prueba u que cae en Ni, podemos clasificarlo por el signo de qu = w∗T i u = uT wi = uT Xi XT i Xi + λiniIi −1 qi. Esta es una expresión atractiva ya que podemos determinar la asignación del clúster de u utilizando los productos internos entre los puntos en {u ∪ Ni}, lo que sugiere que un regularizador local de este tipo puede ser fácilmente kernelizado [21] siempre y cuando definamos una función de kernel adecuada. 2.2.3 Combinando los Predictores Regularizados Localmente Una vez que todos los predictores locales hayan sido construidos, los combinaremos minimizando Jl = n i=1 w∗T i xi − qi 2, (10), lo que representa la suma de los errores de predicción de todos los predictores locales. Combinando la Ecuación (10) con la Ecuación (6), podemos obtener Jl = n i=1 w∗T i xi − qi 2 = n i=1 xT i Xi XT i Xi + λiniIi −1 qi − qi 2 = Pq − q 2, (11), donde q = [q1, q2, · · · , qn]T, y P es una matriz n × n construida de la siguiente manera. Sea αi = xT i Xi XT i Xi + λiniIi −1 , entonces Pij = αi j, si xj ∈ Ni 0, de lo contrario , (12) donde Pij es la entrada (i, j)-ésima de P, y αi j representa la entrada j-ésima de αi. Hasta ahora podemos escribir el criterio de agrupamiento combinando predictores de etiquetas lineales localmente regularizados Jl en una forma matemática explícita, y podemos minimizarlo directamente utilizando algunas técnicas estándar de optimización. Sin embargo, los resultados pueden no ser lo suficientemente buenos ya que solo explotamos la información local del conjunto de datos. En la siguiente subsección, introduciremos un criterio de regularización global y lo combinaremos con Jl, que tiene como objetivo encontrar un buen resultado de agrupamiento de manera local-global. 2.3 Regularización Global En el agrupamiento de datos, generalmente requerimos que las asignaciones de clúster de los puntos de datos sean lo suficientemente suaves con respecto a la <br>variedad</br> de datos subyacente, lo que implica (1) que los puntos cercanos tienden a tener las mismas asignaciones de clúster; (2) que los puntos en la misma estructura (por ejemplo, sub<br>variedad</br> o clúster) tienden a tener las mismas asignaciones de clúster [31]. Sin pérdida de generalidad, asumimos que los puntos de datos residen (aproximadamente) en una <br>variedad</br> de baja dimensión M2, y q es la función de asignación de clúster definida en M, es decir, 2 Creemos que los datos de texto también se muestrean de alguna <br>variedad</br> de baja dimensión, ya que es imposible que para todos los x ∈ M, q(x) devuelva la membresía del clúster de x. La suavidad de q sobre M se puede calcular mediante la siguiente integral de Dirichlet [2] D[q] = 1 2 M q(x) 2 dM, (13) donde el gradiente q es un vector en el espacio tangente T Mx, y la integral se toma con respecto a la medida estándar en M. Si restringimos la escala de q por q, q M = 1 (donde ·, · M es el producto interno inducido en M), entonces resulta que encontrar la función más suave que minimiza D[q] se reduce a encontrar las autofunciones del operador Laplace Beltrami L, que se define como Lq −div q, (14) donde div es la divergencia de un campo vectorial. Generalmente, el gráfico se puede ver como la forma discretizada de <br>una variedad</br>. Podemos modelar el conjunto de datos como un grafo ponderado no dirigido como en el agrupamiento espectral [22], donde los nodos del grafo son simplemente los puntos de datos, y los pesos en las aristas representan las similitudes entre pares de puntos. Entonces se puede demostrar que minimizar la Ec. (13) corresponde a minimizar Jg = qT Lq = n i=1 (qi − qj)2 wij, (15), donde q = [q1, q2, · · · , qn]T con qi = q(xi), L es el Laplaciano del grafo con su entrada (i, j)-ésima Lij =    di − wii, si i = j −wij, si xi y xj son adyacentes 0, en otro caso, (16), donde di = j wij es el grado de xi, wij es la similitud entre xi y xj. Si xi y xj son adyacentes, wij generalmente se calcula de la siguiente manera: wij = e − xi−xj 2 2σ2, donde σ es un parámetro dependiente del conjunto de datos. Se ha demostrado que bajo ciertas condiciones, tal forma de wij para determinar los pesos en los bordes del grafo conduce a la convergencia del Laplaciano del grafo al operador Laplace Beltrami [3][18]. En resumen, el uso de la Ec. (15) con pesos exponenciales puede medir de manera efectiva la suavidad de las asignaciones de datos con respecto a la <br>variedad intrínseca</br> de datos. ",
            "candidates": [],
            "error": [
                [
                    "conjunto de datos",
                    "variedad",
                    "variedad",
                    "variedad",
                    "variedad",
                    "una variedad",
                    "variedad intrínseca"
                ]
            ]
        },
        "document cluster": {
            "translated_key": "agrupamiento de documentos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Regularized Clustering for Documents ∗ Fei Wang, Changshui Zhang State Key Lab of Intelligent Tech.",
                "and Systems Department of Automation, Tsinghua University Beijing, China, 100084 feiwang03@gmail.com Tao Li School of Computer Science Florida International University Miami, FL 33199, U.S.A. taoli@cs.fiu.edu ABSTRACT In recent years, <br>document cluster</br>ing has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering.",
                "In this paper, we propose a novel method for clustering documents using regularization.",
                "Unlike traditional globally regularized clustering methods, our method first construct a local regularized linear label predictor for each document vector, and then combine all those local regularizers with a global smoothness regularizer.",
                "So we call our algorithm Clustering with Local and Global Regularization (CLGR).",
                "We will show that the cluster memberships of the documents can be achieved by eigenvalue decomposition of a sparse symmetric matrix, which can be efficiently solved by iterative methods.",
                "Finally our experimental evaluations on several datasets are presented to show the superiorities of CLGR over traditional <br>document cluster</br>ing methods.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Clustering; I.2.6 [Artificial Intelligence]: Learning-Concept Learning General Terms Algorithms 1.",
                "INTRODUCTION Document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering.",
                "A good <br>document cluster</br>ing approach can assist the computers to automatically organize the document corpus into a meaningful cluster hierarchy for efficient browsing and navigation, which is very valuable for complementing the deficiencies of traditional information retrieval technologies.",
                "As pointed out by [8], the information retrieval needs can be expressed by a spectrum ranged from narrow keyword-matching based search to broad information browsing such as what are the major international events in recent months.",
                "Traditional document retrieval engines tend to fit well with the search end of the spectrum, i.e. they usually provide specified search for documents matching the users query, however, it is hard for them to meet the needs from the rest of the spectrum in which a rather broad or vague information is needed.",
                "In such cases, efficient browsing through a good cluster hierarchy will be definitely helpful.",
                "Generally, <br>document cluster</br>ing methods can be mainly categorized into two classes: hierarchical methods and partitioning methods.",
                "The hierarchical methods group the data points into a hierarchical tree structure using bottom-up or top-down approaches.",
                "For example, hierarchical agglomerative clustering (HAC) [13] is a typical bottom-up hierarchical clustering method.",
                "It takes each data point as a single cluster to start off with and then builds bigger and bigger clusters by grouping similar data points together until the entire dataset is encapsulated into one final cluster.",
                "On the other hand, partitioning methods decompose the dataset into a number of disjoint clusters which are usually optimal in terms of some predefined criterion functions.",
                "For instance, K-means [13] is a typical partitioning method which aims to minimize the sum of the squared distance between the data points and their corresponding cluster centers.",
                "In this paper, we will focus on the partitioning methods.",
                "As we know that there are two main problems existing in partitioning methods (like Kmeans and Gaussian Mixture Model (GMM) [16]): (1) the predefined criterion is usually non-convex which causes many local optimal solutions; (2) the iterative procedure (e.g. the Expectation Maximization (EM) algorithm) for optimizing the criterions usually makes the final solutions heavily depend on the initializations.",
                "In the last decades, many methods have been proposed to overcome the above problems of the partitioning methods [19][28].",
                "Recently, another type of partitioning methods based on clustering on data graphs have aroused considerable interests in the machine learning and data mining community.",
                "The basic idea behind these methods is to first model the whole dataset as a weighted graph, in which the graph nodes represent the data points, and the weights on the edges correspond to the similarities between pairwise points.",
                "Then the cluster assignments of the dataset can be achieved by optimizing some criterions defined on the graph.",
                "For example Spectral Clustering is one kind of the most representative graph-based clustering approaches, it generally aims to optimize some cut value (e.g.",
                "Normalized Cut [22], Ratio Cut [7], Min-Max Cut [11]) defined on an undirected graph.",
                "After some relaxations, these criterions can usually be optimized via eigen-decompositions, which is guaranteed to be global optimal.",
                "In this way, spectral clustering efficiently avoids the problems of the traditional partitioning methods as we introduced in last paragraph.",
                "In this paper, we propose a novel <br>document cluster</br>ing algorithm that inherits the superiority of spectral clustering, i.e. the final cluster results can also be obtained by exploit the eigen-structure of a symmetric matrix.",
                "However, unlike spectral clustering, which just enforces a smoothness constraint on the data labels over the whole data manifold [2], our method first construct a regularized linear label predictor for each data point from its neighborhood as in [25], and then combine the results of all these local label predictors with a global label smoothness regularizer.",
                "So we call our method Clustering with Local and Global Regularization (CLGR).",
                "The idea of incorporating both local and global information into label prediction is inspired by the recent works on semi-supervised learning [31], and our experimental evaluations on several real document datasets show that CLGR performs better than many state-of-the-art clustering methods.",
                "The rest of this paper is organized as follows: in section 2 we will introduce our CLGR algorithm in detail.",
                "The experimental results on several datasets are presented in section 3, followed by the conclusions and discussions in section 4. 2.",
                "THE PROPOSED ALGORITHM In this section, we will introduce our Clustering with Local and Global Regularization (CLGR) algorithm in detail.",
                "First lets see the how the documents are represented throughout this paper. 2.1 Document Representation In our work, all the documents are represented by the weighted term-frequency vectors.",
                "Let W = {w1, w2, · · · , wm} be the complete vocabulary set of the document corpus (which is preprocessed by the stopwords removal and words stemming operations).",
                "The term-frequency vector xi of document di is defined as xi = [xi1, xi2, · · · , xim]T , xik = tik log n idfk , where tik is the term frequency of wk ∈ W, n is the size of the document corpus, idfk is the number of documents that contain word wk.",
                "In this way, xi is also called the TFIDF representation of document di.",
                "Furthermore, we also normalize each xi (1 i n) to have a unit length, so that each document is represented by a normalized TF-IDF vector. 2.2 Local Regularization As its name suggests, CLGR is composed of two parts: local regularization and global regularization.",
                "In this subsection we will introduce the local regularization part in detail. 2.2.1 Motivation As we know that clustering is one type of learning techniques, it aims to organize the dataset in a reasonable way.",
                "Generally speaking, learning can be posed as a problem of function estimation, from which we can get a good classification function that will assign labels to the training dataset and even the unseen testing dataset with some cost minimized [24].",
                "For example, in the two-class classification scenario1 (in which we exactly know the label of each document), a linear classifier with least square fit aims to learn a column vector w such that the squared cost J = 1 n (wT xi − yi)2 (1) is minimized, where yi ∈ {+1, −1} is the label of xi.",
                "By taking ∂J /∂w = 0, we get the solution w∗ = n i=1 xixT i −1 n i=1 xiyi , (2) which can further be written in its matrix form as w∗ = XXT −1 Xy, (3) where X = [x1, x2, · · · , xn] is an m × n document matrix, y = [y1, y2, · · · , yn]T is the label vector.",
                "Then for a test document t, we can determine its label by l = sign(w∗T u), (4) where sign(·) is the sign function.",
                "A natural problem in Eq. (3) is that the matrix XXT may be singular and thus not invertable (e.g. when m n).",
                "To avoid such a problem, we can add a regularization term and minimize the following criterion J = 1 n n i=1 (wT xi − yi)2 + λ w 2 , (5) where λ is a regularization parameter.",
                "Then the optimal solution that minimize J is given by w∗ = XXT + λnI −1 Xy, (6) where I is an m × m identity matrix.",
                "It has been reported that the regularized linear classifier can achieve very good results on text classification problems [29].",
                "However, despite its empirical success, the regularized linear classifier is on earth a global classifier, i.e. w∗ is estimated using the whole training set.",
                "According to [24], this may not be a smart idea, since a unique w∗ may not be good enough for predicting the labels of the whole input space.",
                "In order to get better predictions, [6] proposed to train classifiers locally and use them to classify the testing points.",
                "For example, a testing point will be classified by the local classifier trained using the training points located in the vicinity 1 In the following discussions we all assume that the documents coming from only two classes.",
                "The generalizations of our method to multi-class cases will be discussed in section 2.5. of it.",
                "Although this method seems slow and stupid, it is reported that it can get better performances than using a unique global classifier on certain tasks [6]. 2.2.2 Constructing the Local Regularized Predictors Inspired by their success, we proposed to apply the local learning algorithms for clustering.",
                "The basic idea is that, for each document vector xi (1 i n), we train a local label predictor based on its k-nearest neighborhood Ni, and then use it to predict the label of xi.",
                "Finally we will combine all those local predictors by minimizing the sum of their prediction errors.",
                "In this subsection we will introduce how to construct those local predictors.",
                "Due to the simplicity and effectiveness of the regularized linear classifier that we have introduced in section 2.2.1, we choose it to be our local label predictor, such that for each document xi, the following criterion is minimized Ji = 1 ni xj ∈Ni wT i xj − qj 2 + λi wi 2 , (7) where ni = |Ni| is the cardinality of Ni, and qj is the cluster membership of xj.",
                "Then using Eq. (6), we can get the optimal solution is w∗ i = XiXT i + λiniI −1 Xiqi, (8) where Xi = [xi1, xi2, · · · , xini ], and we use xik to denote the k-th nearest neighbor of xi. qi = [qi1, qi2, · · · , qini ]T with qik representing the cluster assignment of xik.",
                "The problem here is that XiXT i is an m × m matrix with m ni, i.e. we should compute the inverse of an m × m matrix for every document vector, which is computationally prohibited.",
                "Fortunately, we have the following theorem: Theorem 1. w∗ i in Eq. (8) can be rewritten as w∗ i = Xi XT i Xi + λiniIi −1 qi, (9) where Ii is an ni × ni identity matrix.",
                "Proof.",
                "Since w∗ i = XiXT i + λiniI −1 Xiqi, then XiXT i + λiniI w∗ i = Xiqi =⇒ XiXT i w∗ i + λiniw∗ i = Xiqi =⇒ w∗ i = (λini)−1 Xi qi − XT i w∗ i .",
                "Let β = (λini)−1 qi − XT i w∗ i , then w∗ i = Xiβ =⇒ λiniβ = qi − XT i w∗ i = qi − XT i Xiβ =⇒ qi = XT i Xi + λiniIi β =⇒ β = XT i Xi + λiniIi −1 qi.",
                "Therefore w∗ i = Xiβ = Xi XT i Xi + λiniIi −1 qi 2 Using theorem 1, we only need to compute the inverse of an ni × ni matrix for every document to train a local label predictor.",
                "Moreover, for a new testing point u that falls into Ni, we can classify it by the sign of qu = w∗T i u = uT wi = uT Xi XT i Xi + λiniIi −1 qi.",
                "This is an attractive expression since we can determine the cluster assignment of u by using the inner-products between the points in {u ∪ Ni}, which suggests that such a local regularizer can easily be kernelized [21] as long as we define a proper kernel function. 2.2.3 Combining the Local Regularized Predictors After all the local predictors having been constructed, we will combine them together by minimizing Jl = n i=1 w∗T i xi − qi 2 , (10) which stands for the sum of the prediction errors for all the local predictors.",
                "Combining Eq. (10) with Eq. (6), we can get Jl = n i=1 w∗T i xi − qi 2 = n i=1 xT i Xi XT i Xi + λiniIi −1 qi − qi 2 = Pq − q 2 , (11) where q = [q1, q2, · · · , qn]T , and the P is an n × n matrix constructing in the following way.",
                "Let αi = xT i Xi XT i Xi + λiniIi −1 , then Pij = αi j, if xj ∈ Ni 0, otherwise , (12) where Pij is the (i, j)-th entry of P, and αi j represents the j-th entry of αi .",
                "Till now we can write the criterion of clustering by combining locally regularized linear label predictors Jl in an explicit mathematical form, and we can minimize it directly using some standard optimization techniques.",
                "However, the results may not be good enough since we only exploit the local informations of the dataset.",
                "In the next subsection, we will introduce a global regularization criterion and combine it with Jl, which aims to find a good clustering result in a local-global way. 2.3 Global Regularization In data clustering, we usually require that the cluster assignments of the data points should be sufficiently smooth with respect to the underlying data manifold, which implies (1) the nearby points tend to have the same cluster assignments; (2) the points on the same structure (e.g. submanifold or cluster) tend to have the same cluster assignments [31].",
                "Without the loss of generality, we assume that the data points reside (roughly) on a low-dimensional manifold M2 , and q is the cluster assignment function defined on M, i.e. 2 We believe that the text data are also sampled from some low dimensional manifold, since it is impossible for them to for ∀x ∈ M, q(x) returns the cluster membership of x.",
                "The smoothness of q over M can be calculated by the following Dirichlet integral [2] D[q] = 1 2 M q(x) 2 dM, (13) where the gradient q is a vector in the tangent space T Mx, and the integral is taken with respect to the standard measure on M. If we restrict the scale of q by q, q M = 1 (where ·, · M is the inner product induced on M), then it turns out that finding the smoothest function minimizing D[q] reduces to finding the eigenfunctions of the Laplace Beltrami operator L, which is defined as Lq −div q, (14) where div is the divergence of a vector field.",
                "Generally, the graph can be viewed as the discretized form of manifold.",
                "We can model the dataset as an weighted undirected graph as in spectral clustering [22], where the graph nodes are just the data points, and the weights on the edges represent the similarities between pairwise points.",
                "Then it can be shown that minimizing Eq. (13) corresponds to minimizing Jg = qT Lq = n i=1 (qi − qj)2 wij, (15) where q = [q1, q2, · · · , qn]T with qi = q(xi), L is the graph Laplacian with its (i, j)-th entry Lij =    di − wii, if i = j −wij, if xi and xj are adjacent 0, otherwise, (16) where di = j wij is the degree of xi, wij is the similarity between xi and xj.",
                "If xi and xj are adjacent3 , wij is usually computed in the following way wij = e − xi−xj 2 2σ2 , (17) where σ is a dataset dependent parameter.",
                "It is proved that under certain conditions, such a form of wij to determine the weights on graph edges leads to the convergence of graph Laplacian to the Laplace Beltrami operator [3][18].",
                "In summary, using Eq. (15) with exponential weights can effectively measure the smoothness of the data assignments with respect to the intrinsic data manifold.",
                "Thus we adopt it as a global regularizer to punish the smoothness of the predicted data assignments. 2.4 Clustering with Local and Global Regularization Combining the contents we have introduced in section 2.2 and section 2.3 we can derive the clustering criterion is minq J = Jl + λJg = Pq − q 2 + λqT Lq s.t. qi ∈ {−1, +1}, (18) where P is defined as in Eq. (12), and λ is a regularization parameter to trade off Jl and Jg.",
                "However, the discrete fill in the whole high-dimensional sample space.",
                "And it has been shown that the manifold based methods can achieve good results on text classification tasks [31]. 3 In this paper, we define xi and xj to be adjacent if xi ∈ N(xj) or xj ∈ N(xi). constraint of pi makes the problem an NP hard integer programming problem.",
                "A natural way for making the problem solvable is to remove the constraint and relax qi to be continuous, then the objective that we aims to minimize becomes J = Pq − q 2 + λqT Lq = qT (P − I)T (P − I)q + λqT Lq = qT (P − I)T (P − I) + λL q, (19) and we further add a constraint qT q = 1 to restrict the scale of q.",
                "Then our objective becomes minq J = qT (P − I)T (P − I) + λL q s.t. qT q = 1 (20) Using the Lagrangian method, we can derive that the optimal solution q corresponds to the smallest eigenvector of the matrix M = (P − I)T (P − I) + λL, and the cluster assignment of xi can be determined by the sign of qi, i.e. xi will be classified as class one if qi > 0, otherwise it will be classified as class 2. 2.5 Multi-Class CLGR In the above we have introduced the basic framework of Clustering with Local and Global Regularization (CLGR) for the two-class clustering problem, and we will extending it to multi-class clustering in this subsection.",
                "First we assume that all the documents belong to C classes indexed by L = {1, 2, · · · , C}. qc is the classification function for class c (1 c C), such that qc (xi) returns the confidence that xi belongs to class c. Our goal is to obtain the value of qc (xi) (1 c C, 1 i n), and the cluster assignment of xi can be determined by {qc (xi)}C c=1 using some proper discretization methods that we will introduce later.",
                "Therefore, in this multi-class case, for each document xi (1 i n), we will construct C locally linear regularized label predictors whose normal vectors are wc∗ i = Xi XT i Xi + λiniIi −1 qc i (1 c C), (21) where Xi = [xi1, xi2, · · · , xini ] with xik being the k-th neighbor of xi, and qc i = [qc i1, qc i2, · · · , qc ini ]T with qc ik = qc (xik).",
                "Then (wc∗ i )T xi returns the predicted confidence of xi belonging to class c. Hence the local prediction error for class c can be defined as J c l = n i=1 (wc∗ i ) T xi − qc i 2 , (22) And the total local prediction error becomes Jl = C c=1 J c l = C c=1 n i=1 (wc∗ i ) T xi − qc i 2 . (23) As in Eq. (11), we can define an n×n matrix P (see Eq. (12)) and rewrite Jl as Jl = C c=1 J c l = C c=1 Pqc − qc 2 . (24) Similarly we can define the global smoothness regularizer in multi-class case as Jg = C c=1 n i=1 (qc i − qc j )2 wij = C c=1 (qc )T Lqc . (25) Then the criterion to be minimized for CLGR in multi-class case becomes J = Jl + λJg = C c=1 Pqc − qc 2 + λ(qc )T Lqc = C c=1 (qc )T (P − I)T (P − I) + λL qc = trace QT (P − I)T (P − I) + λL Q , (26) where Q = [q1 , q2 , · · · , qc ] is an n × c matrix, and trace(·) returns the trace of a matrix.",
                "The same as in Eq. (20), we also add the constraint that QT Q = I to restrict the scale of Q.",
                "Then our optimization problem becomes minQ J = trace QT (P − I)T (P − I) + λL Q s.t.",
                "QT Q = I, (27) From the Ky Fan theorem [28], we know the optimal solution of the above problem is Q∗ = [q∗ 1, q∗ 2, · · · , q∗ C ]R, (28) where q∗ k (1 k C) is the eigenvector corresponds to the k-th smallest eigenvalue of matrix (P − I)T (P − I) + λL, and R is an arbitrary C × C matrix.",
                "Since the values of the entries in Q∗ is continuous, we need to further discretize Q∗ to get the cluster assignments of all the data points.",
                "There are mainly two approaches to achieve this goal: 1.",
                "As in [20], we can treat the i-th row of Q as the embedding of xi in a C-dimensional space, and apply some traditional clustering methods like kmeans to clustering these embeddings into C clusters. 2.",
                "Since the optimal Q∗ is not unique (because of the existence of an arbitrary matrix R), we can pursue an optimal R that will rotate Q∗ to an indication matrix4 .",
                "The detailed algorithm can be referred to [26].",
                "The detailed algorithm procedure for CLGR is summarized in table 1. 3.",
                "EXPERIMENTS In this section, experiments are conducted to empirically compare the clustering results of CLGR with other 8 representitive <br>document cluster</br>ing algorithms on 5 datasets.",
                "First we will introduce the basic informations of those datasets. 3.1 Datasets We use a variety of datasets, most of which are frequently used in the information retrieval research.",
                "Table 2 summarizes the characteristics of the datasets. 4 Here an indication matrix T is a n×c matrix with its (i, j)th entry Tij ∈ {0, 1} such that for each row of Q∗ there is only one 1.",
                "Then the xi can be assigned to the j-th cluster such that j = argjQ∗ ij = 1.",
                "Table 1: Clustering with Local and Global Regularization (CLGR) Input: 1.",
                "Dataset X = {xi}n i=1; 2.",
                "Number of clusters C; 3.",
                "Size of the neighborhood K; 4.",
                "Local regularization parameters {λi}n i=1; 5.",
                "Global regularization parameter λ; Output: The cluster membership of each data point.",
                "Procedure: 1.",
                "Construct the K nearest neighborhoods for each data point; 2.",
                "Construct the matrix P using Eq. (12); 3.",
                "Construct the Laplacian matrix L using Eq. (16); 4.",
                "Construct the matrix M = (P − I)T (P − I) + λL; 5.",
                "Do eigenvalue decomposition on M, and construct the matrix Q∗ according to Eq. (28); 6.",
                "Output the cluster assignments of each data point by properly discretize Q∗ .",
                "Table 2: Descriptions of the document datasets Datasets Number of documents Number of classes CSTR 476 4 WebKB4 4199 4 Reuters 2900 10 WebACE 2340 20 Newsgroup4 3970 4 CSTR.",
                "This is the dataset of the abstracts of technical reports published in the Department of Computer Science at a university.",
                "The dataset contained 476 abstracts, which were divided into four research areas: Natural Language Processing(NLP), Robotics/Vision, Systems, and Theory.",
                "WebKB.",
                "The WebKB dataset contains webpages gathered from university computer science departments.",
                "There are about 8280 documents and they are divided into 7 categories: student, faculty, staff, course, project, department and other.",
                "The raw text is about 27MB.",
                "Among these 7 categories, student, faculty, course and project are four most populous entity-representing categories.",
                "The associated subset is typically called WebKB4.",
                "Reuters.",
                "The Reuters-21578 Text Categorization Test collection contains documents collected from the Reuters newswire in 1987.",
                "It is a standard text categorization benchmark and contains 135 categories.",
                "In our experiments, we use a subset of the data collection which includes the 10 most frequent categories among the 135 topics and we call it Reuters-top 10.",
                "WebACE.",
                "The WebACE dataset was from WebACE project and has been used for <br>document cluster</br>ing [17][5].",
                "The WebACE dataset contains 2340 documents consisting news articles from Reuters new service via the Web in October 1997.",
                "These documents are divided into 20 classes.",
                "News4.",
                "The News4 dataset used in our experiments are selected from the famous 20-newsgroups dataset5 .",
                "The topic rec containing autos, motorcycles, baseball and hockey was selected from the version 20news-18828.",
                "The News4 dataset contains 3970 document vectors. 5 http://people.csail.mit.edu/jrennie/20Newsgroups/ To pre-process the datasets, we remove the stop words using a standard stop list, all HTML tags are skipped and all header fields except subject and organization of the posted articles are ignored.",
                "In all our experiments, we first select the top 1000 words by mutual information with class labels. 3.2 Evaluation Metrics In the experiments, we set the number of clusters equal to the true number of classes C for all the clustering algorithms.",
                "To evaluate their performance, we compare the clusters generated by these algorithms with the true classes by computing the following two performance measures.",
                "Clustering Accuracy (Acc).",
                "The first performance measure is the Clustering Accuracy, which discovers the one-toone relationship between clusters and classes and measures the extent to which each cluster contained data points from the corresponding class.",
                "It sums up the whole matching degree between all pair class-clusters.",
                "Clustering accuracy can be computed as: Acc = 1 N max   Ck,Lm T(Ck, Lm)   , (29) where Ck denotes the k-th cluster in the final results, and Lm is the true m-th class.",
                "T(Ck, Lm) is the number of entities which belong to class m are assigned to cluster k. Accuracy computes the maximum sum of T(Ck, Lm) for all pairs of clusters and classes, and these pairs have no overlaps.",
                "The greater clustering accuracy means the better clustering performance.",
                "Normalized Mutual Information (NMI).",
                "Another evaluation metric we adopt here is the Normalized Mutual Information NMI [23], which is widely used for determining the quality of clusters.",
                "For two random variable X and Y, the NMI is defined as: NMI(X, Y) = I(X, Y) H(X)H(Y) , (30) where I(X, Y) is the mutual information between X and Y, while H(X) and H(Y) are the entropies of X and Y respectively.",
                "One can see that NMI(X, X) = 1, which is the maximal possible value of NMI.",
                "Given a clustering result, the NMI in Eq. (30) is estimated as NMI = C k=1 C m=1 nk,mlog n·nk,m nk ˆnm C k=1 nklog nk n C m=1 ˆnmlog ˆnm n , (31) where nk denotes the number of data contained in the cluster Ck (1 k C), ˆnm is the number of data belonging to the m-th class (1 m C), and nk,m denotes the number of data that are in the intersection between the cluster Ck and the m-th class.",
                "The value calculated in Eq. (31) is used as a performance measure for the given clustering result.",
                "The larger this value, the better the clustering performance. 3.3 Comparisons We have conducted comprehensive performance evaluations by testing our method and comparing it with 8 other representative data clustering methods using the same data corpora.",
                "The algorithms that we evaluated are listed below. 1.",
                "Traditional k-means (KM). 2.",
                "Spherical k-means (SKM).",
                "The implementation is based on [9]. 3.",
                "Gaussian Mixture Model (GMM).",
                "The implementation is based on [16]. 4.",
                "Spectral Clustering with Normalized Cuts (Ncut).",
                "The implementation is based on [26], and the variance of the Gaussian similarity is determined by Local Scaling [30].",
                "Note that the criterion that Ncut aims to minimize is just the global regularizer in our CLGR algorithm except that Ncut used the normalized Laplacian. 5.",
                "Clustering using Pure Local Regularization (CPLR).",
                "In this method we just minimize Jl (defined in Eq. (24)), and the clustering results can be obtained by doing eigenvalue decomposition on matrix (I − P)T (I − P) with some proper discretization methods. 6.",
                "Adaptive Subspace Iteration (ASI).",
                "The implementation is based on [14]. 7.",
                "Nonnegative Matrix Factorization (NMF).",
                "The implementation is based on [27]. 8.",
                "Tri-Factorization Nonnegative Matrix Factorization (TNMF) [12].",
                "The implementation is based on [15].",
                "For computational efficiency, in the implementation of CPLR and our CLGR algorithm, we have set all the local regularization parameters {λi}n i=1 to be identical, which is set by grid search from {0.1, 1, 10}.",
                "The size of the k-nearest neighborhoods is set by grid search from {20, 40, 80}.",
                "For the CLGR method, its global regularization parameter is set by grid search from {0.1, 1, 10}.",
                "When constructing the global regularizer, we have adopted the local scaling method [30] to construct the Laplacian matrix.",
                "The final discretization method adopted in these two methods is the same as in [26], since our experiments show that using such method can achieve better results than using kmeans based methods as in [20]. 3.4 Experimental Results The clustering accuracies comparison results are shown in table 3, and the normalized mutual information comparison results are summarized in table 4.",
                "From the two tables we mainly observe that: 1.",
                "Our CLGR method outperforms all other <br>document cluster</br>ing methods in most of the datasets; 2.",
                "For <br>document cluster</br>ing, the Spherical k-means method usually outperforms the traditional k-means clustering method, and the GMM method can achieve competitive results compared to the Spherical k-means method; 3.",
                "The results achieved from the k-means and GMM type algorithms are usually worse than the results achieved from Spectral Clustering.",
                "Since Spectral Clustering can be viewed as a weighted version of kernel k-means, it can obtain good results the data clusters are arbitrarily shaped.",
                "This corroborates that the documents vectors are not regularly distributed (spherical or elliptical). 4.",
                "The experimental comparisons empirically verify the equivalence between NMF and Spectral Clustering, which Table 3: Clustering accuracies of the various methods CSTR WebKB4 Reuters WebACE News4 KM 0.4256 0.3888 0.4448 0.4001 0.3527 SKM 0.4690 0.4318 0.5025 0.4458 0.3912 GMM 0.4487 0.4271 0.4897 0.4521 0.3844 NMF 0.5713 0.4418 0.4947 0.4761 0.4213 Ncut 0.5435 0.4521 0.4896 0.4513 0.4189 ASI 0.5621 0.4752 0.5235 0.4823 0.4335 TNMF 0.6040 0.4832 0.5541 0.5102 0.4613 CPLR 0.5974 0.5020 0.4832 0.5213 0.4890 CLGR 0.6235 0.5228 0.5341 0.5376 0.5102 Table 4: Normalized mutual information results of the various methods CSTR WebKB4 Reuters WebACE News4 KM 0.3675 0.3023 0.4012 0.3864 0.3318 SKM 0.4027 0.4155 0.4587 0.4003 0.4085 GMM 0.4034 0.4093 0.4356 0.4209 0.3994 NMF 0.5235 0.4517 0.4402 0.4359 0.4130 Ncut 0.4833 0.4497 0.4392 0.4289 0.4231 ASI 0.5008 0.4833 0.4769 0.4817 0.4503 TNMF 0.5724 0.5011 0.5132 0.5328 0.4749 CPLR 0.5695 0.5231 0.4402 0.5543 0.4690 CLGR 0.6012 0.5434 0.4935 0.5390 0.4908 has been proved theoretically in [10].",
                "It can be observed from the tables that NMF and Spectral Clustering usually lead to similar clustering results. 5.",
                "The co-clustering based methods (TNMF and ASI) can usually achieve better results than traditional purely document vector based methods.",
                "Since these methods perform an implicit feature selection at each iteration, provide an adaptive metric for measuring the neighborhood, and thus tend to yield better clustering results. 6.",
                "The results achieved from CPLR are usually better than the results achieved from Spectral Clustering, which supports Vapniks theory [24] that sometimes local learning algorithms can obtain better results than global learning algorithms.",
                "Besides the above comparison experiments, we also test the parameter sensibility of our method.",
                "There are mainly two sets of parameters in our CLGR algorithm, the local and global regularization parameters ({λi}n i=1 and λ, as we have said in section 3.3, we have set all λis to be identical to λ∗ in our experiments), and the size of the neighborhoods.",
                "Therefore we have also done two sets of experiments: 1.",
                "Fixing the size of the neighborhoods, and testing the clustering performance with varying λ∗ and λ.",
                "In this set of experiments, we find that our CLGR algorithm can achieve good results when the two regularization parameters are neither too large nor too small.",
                "Typically our method can achieve good results when λ∗ and λ are around 0.1.",
                "Figure 1 shows us such a testing example on the WebACE dataset. 2.",
                "Fixing the local and global regularization parameters, and testing the clustering performance with different −5 −4.5 −4 −3.5 −3 −5 −4.5 −4 −3.5 −3 0.35 0.4 0.45 0.5 0.55 local regularization para (log 2 value) global regularization para (log 2 value) clusteringaccuracy Figure 1: Parameter sensibility testing results on the WebACE dataset with the neighborhood size fixed to 20, and the x-axis and y-axis represents the log2 value of λ∗ and λ. sizes of neighborhoods.",
                "In this set of experiments, we find that the neighborhood with a too large or too small size will all deteriorate the final clustering results.",
                "This can be easily understood since when the neighborhood size is very small, then the data points used for training the local classifiers may not be sufficient; when the neighborhood size is very large, the trained classifiers will tend to be global and cannot capture the typical local characteristics.",
                "Figure 2 shows us a testing example on the WebACE dataset.",
                "Therefore, we can see that our CLGR algorithm (1) can achieve satisfactory results and (2) is not very sensitive to the choice of parameters, which makes it practical in real world applications. 4.",
                "CONCLUSIONS AND FUTURE WORKS In this paper, we derived a new clustering algorithm called clustering with local and global regularization.",
                "Our method preserves the merit of local learning algorithms and spectral clustering.",
                "Our experiments show that the proposed algorithm outperforms most of the state of the art algorithms on many benchmark datasets.",
                "In the future, we will focus on the parameter selection and acceleration issues of the CLGR algorithm. 5.",
                "REFERENCES [1] L. Baker and A. McCallum.",
                "Distributional Clustering of Words for Text Classification.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 1998. [2] M. Belkin and P. Niyogi.",
                "Laplacian Eigenmaps for Dimensionality Reduction and Data Representation.",
                "Neural Computation, 15 (6):1373-1396.",
                "June 2003. [3] M. Belkin and P. Niyogi.",
                "Towards a Theoretical Foundation for Laplacian-Based Manifold Methods.",
                "In Proceedings of the 18th Conference on Learning Theory (COLT). 2005. 10 20 30 40 50 60 70 80 90 100 0.35 0.4 0.45 0.5 0.55 size of the neighborhood clusteringaccuracy Figure 2: Parameter sensibility testing results on the WebACE dataset with the regularization parameters being fixed to 0.1, and the neighborhood size varing from 10 to 100. [4] M. Belkin, P. Niyogi and V. Sindhwani.",
                "Manifold Regularization: a Geometric Framework for Learning from Examples.",
                "Journal of Machine Learning Research 7, 1-48, 2006. [5] D. Boley.",
                "Principal Direction Divisive Partitioning.",
                "Data mining and knowledge discovery, 2:325-344, 1998. [6] L. Bottou and V. Vapnik.",
                "Local learning algorithms.",
                "Neural Computation, 4:888-900, 1992. [7] P. K. Chan, D. F. Schlag and J. Y. Zien.",
                "Spectral K-way Ratio-Cut Partitioning and Clustering.",
                "IEEE Trans.",
                "Computer-Aided Design, 13:1088-1096, Sep. 1994. [8] D. R. Cutting, D. R. Karger, J. O. Pederson and J. W. Tukey.",
                "Scatter/Gather: A Cluster-Based Approach to Browsing Large Document Collections.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 1992. [9] I. S. Dhillon and D. S. Modha.",
                "Concept Decompositions for Large Sparse Text Data using Clustering.",
                "Machine Learning, vol. 42(1), pages 143-175, January 2001. [10] C. Ding, X.",
                "He, and H. Simon.",
                "On the equivalence of nonnegative matrix factorization and spectral clustering.",
                "In Proceedings of the SIAM Data Mining Conference, 2005. [11] C. Ding, X.",
                "He, H. Zha, M. Gu, and H. D. Simon.",
                "A min-max cut algorithm for graph partitioning and data clustering.",
                "In Proc. of the 1st International Conference on Data Mining (ICDM), pages 107-114, 2001. [12] C. Ding, T. Li, W. Peng, and H. Park.",
                "Orthogonal Nonnegative Matrix Tri-Factorizations for Clustering.",
                "In Proceedings of the Twelfth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2006. [13] R. O. Duda, P. E. Hart, and D. G. Stork.",
                "Pattern Classification.",
                "John Wiley & Sons, Inc., 2001. [14] T. Li, S. Ma, and M. Ogihara.",
                "Document Clustering via Adaptive Subspace Iteration.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2004. [15] T. Li and C. Ding.",
                "The Relationships Among Various Nonnegative Matrix Factorization Methods for Clustering.",
                "In Proceedings of the 6th International Conference on Data Mining (ICDM). 2006. [16] X. Liu and Y. Gong.",
                "Document Clustering with Cluster Refinement and Model Selection Capabilities.",
                "In Proc. of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002. [17] E. Han, D. Boley, M. Gini, R. Gross, K. Hastings, G. Karypis, V. Kumar, B. Mobasher, and J. Moore.",
                "WebACE: A Web Agent for Document Categorization and Exploration.",
                "In Proceedings of the 2nd International Conference on Autonomous Agents (Agents98).",
                "ACM Press, 1998. [18] M. Hein, J. Y. Audibert, and U. von Luxburg.",
                "From Graphs to Manifolds - Weak and Strong Pointwise Consistency of Graph Laplacians.",
                "In Proceedings of the 18th Conference on Learning Theory (COLT), 470-485. 2005. [19] J.",
                "He, M. Lan, C.-L. Tan, S.-Y.",
                "Sung, and H.-B.",
                "Low.",
                "Initialization of Cluster Refinement Algorithms: A Review and Comparative Study.",
                "In Proc. of Inter.",
                "Joint Conference on Neural Networks, 2004. [20] A. Y. Ng, M. I. Jordan, Y. Weiss.",
                "On Spectral Clustering: Analysis and an algorithm.",
                "In Advances in Neural Information Processing Systems 14. 2002. [21] B. Sch¨olkopf and A. Smola.",
                "Learning with Kernels.",
                "The MIT Press.",
                "Cambridge, Massachusetts. 2002. [22] J. Shi and J. Malik.",
                "Normalized Cuts and Image Segmentation.",
                "IEEE Trans. on Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000. [23] A. Strehl and J. Ghosh.",
                "Cluster Ensembles - A Knowledge Reuse Framework for Combining Multiple Partitions.",
                "Journal of Machine Learning Research, 3:583-617, 2002. [24] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Berlin: Springer-Verlag, 1995. [25] Wu, M. and Sch¨olkopf, B.",
                "A Local Learning Approach for Clustering.",
                "In Advances in Neural Information Processing Systems 18. 2006. [26] S. X. Yu, J. Shi.",
                "Multiclass Spectral Clustering.",
                "In Proceedings of the International Conference on Computer Vision, 2003. [27] W. Xu, X. Liu and Y. Gong.",
                "Document Clustering Based On Non-Negative Matrix Factorization.",
                "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2003. [28] H. Zha, X.",
                "He, C. Ding, M. Gu and H. Simon.",
                "Spectral Relaxation for K-means Clustering.",
                "In NIPS 14. 2001. [29] T. Zhang and F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Journal of Information Retrieval, 4:5-31, 2001. [30] L. Zelnik-Manor and P. Perona.",
                "Self-Tuning Spectral Clustering.",
                "In NIPS 17. 2005. [31] D. Zhou, O. Bousquet, T. N. Lal, J. Weston and B. Sch¨olkopf.",
                "Learning with Local and Global Consistency.",
                "NIPS 17, 2005."
            ],
            "original_annotated_samples": [
                "and Systems Department of Automation, Tsinghua University Beijing, China, 100084 feiwang03@gmail.com Tao Li School of Computer Science Florida International University Miami, FL 33199, U.S.A. taoli@cs.fiu.edu ABSTRACT In recent years, <br>document cluster</br>ing has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering.",
                "Finally our experimental evaluations on several datasets are presented to show the superiorities of CLGR over traditional <br>document cluster</br>ing methods.",
                "A good <br>document cluster</br>ing approach can assist the computers to automatically organize the document corpus into a meaningful cluster hierarchy for efficient browsing and navigation, which is very valuable for complementing the deficiencies of traditional information retrieval technologies.",
                "Generally, <br>document cluster</br>ing methods can be mainly categorized into two classes: hierarchical methods and partitioning methods.",
                "In this paper, we propose a novel <br>document cluster</br>ing algorithm that inherits the superiority of spectral clustering, i.e. the final cluster results can also be obtained by exploit the eigen-structure of a symmetric matrix."
            ],
            "translated_annotated_samples": [
                "En los últimos años, la <br>agrupación de documentos</br> ha estado recibiendo cada vez más atención como una técnica importante y fundamental para la organización no supervisada de documentos, la extracción automática de temas y la recuperación o filtrado rápido de información.",
                "Finalmente, se presentan nuestras evaluaciones experimentales en varios conjuntos de datos para mostrar las ventajas de CLGR sobre los métodos tradicionales de <br>agrupamiento de documentos</br>.",
                "Un buen enfoque de <br>agrupación de documentos</br> puede ayudar a las computadoras a organizar automáticamente el corpus de documentos en una jerarquía de clusters significativa para una navegación eficiente, lo cual es muy valioso para complementar las deficiencias de las tecnologías tradicionales de recuperación de información.",
                "Generalmente, los métodos de <br>agrupamiento de documentos</br> se pueden clasificar principalmente en dos clases: métodos jerárquicos y métodos de partición.",
                "En este artículo, proponemos un algoritmo novedoso de <br>agrupamiento de documentos</br> que hereda la superioridad del agrupamiento espectral, es decir, los resultados finales de los grupos también pueden obtenerse explotando la estructura de los autovalores de una matriz simétrica."
            ],
            "translated_text": "Agrupamiento regularizado para documentos ∗ Fei Wang, Changshui Zhang Laboratorio Clave del Estado de Tecnología Inteligente. En los últimos años, la <br>agrupación de documentos</br> ha estado recibiendo cada vez más atención como una técnica importante y fundamental para la organización no supervisada de documentos, la extracción automática de temas y la recuperación o filtrado rápido de información. En este artículo, proponemos un método novedoso para agrupar documentos utilizando regularización. A diferencia de los métodos de agrupamiento regularizados globalmente tradicionales, nuestro método primero construye un predictor de etiquetas lineal regularizado local para cada vector de documento, y luego combina todos esos regularizadores locales con un regularizador de suavidad global. Así que llamamos a nuestro algoritmo Agrupamiento con Regularización Local y Global (CLGR). Mostraremos que las pertenencias de los documentos a los grupos se pueden lograr mediante la descomposición de los valores propios de una matriz simétrica dispersa, la cual puede resolverse eficientemente mediante métodos iterativos. Finalmente, se presentan nuestras evaluaciones experimentales en varios conjuntos de datos para mostrar las ventajas de CLGR sobre los métodos tradicionales de <br>agrupamiento de documentos</br>. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información-Agrupamiento; I.2.6 [Inteligencia Artificial]: Aprendizaje-Aprendizaje de Conceptos Términos Generales Algoritmos 1. La agrupación de documentos ha estado recibiendo cada vez más atención como una técnica importante y fundamental para la organización no supervisada de documentos, la extracción automática de temas y la recuperación o filtrado rápido de información. Un buen enfoque de <br>agrupación de documentos</br> puede ayudar a las computadoras a organizar automáticamente el corpus de documentos en una jerarquía de clusters significativa para una navegación eficiente, lo cual es muy valioso para complementar las deficiencias de las tecnologías tradicionales de recuperación de información. Como señaló [8], las necesidades de recuperación de información pueden expresarse mediante un espectro que va desde la búsqueda basada en palabras clave estrechas hasta la exploración de información amplia, como cuáles son los principales eventos internacionales de los últimos meses. Los motores de búsqueda de documentos tradicionales tienden a adaptarse bien al extremo de la búsqueda, es decir, suelen proporcionar búsquedas específicas de documentos que coinciden con la consulta de los usuarios, sin embargo, les resulta difícil satisfacer las necesidades del resto del espectro en el que se necesita información más amplia o vaga. En tales casos, la navegación eficiente a través de una buena jerarquía de clústeres será definitivamente útil. Generalmente, los métodos de <br>agrupamiento de documentos</br> se pueden clasificar principalmente en dos clases: métodos jerárquicos y métodos de partición. Los métodos jerárquicos agrupan los puntos de datos en una estructura de árbol jerárquico utilizando enfoques de abajo hacia arriba o de arriba hacia abajo. Por ejemplo, el clustering jerárquico aglomerativo (HAC) [13] es un método típico de clustering jerárquico ascendente. Toma cada punto de datos como un único clúster para comenzar y luego construye clústeres más grandes agrupando puntos de datos similares juntos hasta que todo el conjunto de datos esté encapsulado en un único clúster final. Por otro lado, los métodos de particionamiento descomponen el conjunto de datos en un número de grupos disjuntos que suelen ser óptimos en términos de algunas funciones de criterio predefinidas. Por ejemplo, K-means es un método de particionamiento típico que tiene como objetivo minimizar la suma de las distancias al cuadrado entre los puntos de datos y sus centros de clúster correspondientes. En este documento, nos centraremos en los métodos de particionamiento. Como sabemos, existen dos problemas principales en los métodos de particionamiento (como Kmeans y el Modelo de Mezcla Gaussiana (GMM) [16]): (1) el criterio predefinido suele ser no convexo, lo que provoca muchas soluciones óptimas locales; (2) el procedimiento iterativo (por ejemplo, el algoritmo de Expectation Maximization (EM)) para optimizar los criterios suele hacer que las soluciones finales dependan en gran medida de las inicializaciones. En las últimas décadas, se han propuesto muchos métodos para superar los problemas mencionados de los métodos de particionamiento [19][28]. Recientemente, otro tipo de métodos de particionamiento basados en agrupamiento en grafos de datos han despertado un considerable interés en la comunidad de aprendizaje automático y minería de datos. La idea básica detrás de estos métodos es modelar primero el conjunto de datos completo como un grafo ponderado, en el que los nodos del grafo representan los puntos de datos y los pesos en los bordes corresponden a las similitudes entre los puntos en pares. Entonces, las asignaciones de clústeres del conjunto de datos se pueden lograr optimizando algunos criterios definidos en el gráfico. Por ejemplo, el Clustering Espectral es uno de los enfoques de clustering basados en grafos más representativos, generalmente tiene como objetivo optimizar algún valor de corte (por ejemplo, Corte normalizado [22], corte de razón [7], corte mínimo-máximo [11]) definidos en un grafo no dirigido. Después de algunas relajaciones, estos criterios generalmente pueden ser optimizados a través de descomposiciones propias, lo cual está garantizado que sea óptimo a nivel global. De esta manera, el agrupamiento espectral evita eficientemente los problemas de los métodos de particionamiento tradicionales que presentamos en el párrafo anterior. En este artículo, proponemos un algoritmo novedoso de <br>agrupamiento de documentos</br> que hereda la superioridad del agrupamiento espectral, es decir, los resultados finales de los grupos también pueden obtenerse explotando la estructura de los autovalores de una matriz simétrica. ",
            "candidates": [],
            "error": [
                [
                    "agrupación de documentos",
                    "agrupamiento de documentos",
                    "agrupación de documentos",
                    "agrupamiento de documentos",
                    "agrupamiento de documentos"
                ]
            ]
        }
    }
}