{
    "id": "H-92",
    "original_text": "Improving Web Search Ranking by Incorporating User Behavior Information Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com ABSTRACT We show that incorporating user behavior data can significantly improve ordering of top results in real web search setting. We examine alternatives for incorporating feedback into the ranking process and explore the contributions of user feedback compared to other common web search features. We report results of a large scale evaluation over 3,000 queries and 12 million user interactions with a popular web search engine. We show that incorporating implicit feedback can augment other features, improving the accuracy of a competitive web search ranking algorithms by as much as 31% relative to the original performance. Categories and Subject Descriptors H.3.3 Information Search and Retrieval - Relevance feedback, search process; H.3.5 Online Information Services - Web-based services. General Terms Algorithms, Measurement, Experimentation 1. INTRODUCTION Millions of users interact with search engines daily. They issue queries, follow some of the links in the results, click on ads, spend time on pages, reformulate their queries, and perform other actions. These interactions can serve as a valuable source of information for tuning and improving web search result ranking and can compliment more costly explicit judgments. Implicit relevance feedback for ranking and personalization has become an active area of research. Recent work by Joachims and others exploring implicit feedback in controlled environments have shown the value of incorporating implicit feedback into the ranking process. Our motivation for this work is to understand how implicit feedback can be used in a large-scale operational environment to improve retrieval. How does it compare to and compliment evidence from page content, anchor text, or link-based features such as inlinks or PageRank? While it is intuitive that user interactions with the web search engine should reveal at least some information that could be used for ranking, estimating user preferences in real web search settings is a challenging problem, since real user interactions tend to be more noisy than commonly assumed in the controlled settings of previous studies. Our paper explores whether implicit feedback can be helpful in realistic environments, where user feedback can be noisy (or adversarial) and a web search engine already uses hundreds of features and is heavily tuned. To this end, we explore different approaches for ranking web search results using real user behavior obtained as part of normal interactions with the web search engine. The specific contributions of this paper include: • Analysis of alternatives for incorporating user behavior into web search ranking (Section 3). • An application of a robust implicit feedback model derived from mining millions of user interactions with a major web search engine (Section 4). • A large scale evaluation over real user queries and search results, showing significant improvements derived from incorporating user feedback (Section 6). We summarize our findings and discuss extensions to the current work in Section 7, which concludes the paper. 2. BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval. Most common approaches primarily focus on similarity of query and a page, as well as the overall page quality [3,4,24]. However, with increasing popularity of search engines, implicit feedback (i.e., the actions users take when interacting with the search engine) can be used to improve the rankings. Implicit relevance measures have been studied by several research groups. An overview of implicit measures is compiled in Kelly and Teevan [14]. This research, while developing valuable insights into implicit relevance measures, was not applied to improve the ranking of web search results in realistic settings. Closely related to our work, Joachims [11] collected implicit measures in place of explicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions. Fox et al. [8] explored the relationship between implicit and explicit measures in Web search, and developed Bayesian models to correlate implicit measures and explicit relevance judgments for both individual queries and search sessions. This work considered a wide range of user behaviors (e.g., dwell time, scroll time, reformulation patterns) in addition to the popular clickthrough behavior. However, the modeling effort was aimed at predicting explicit relevance judgments from implicit user actions and not specifically at learning ranking functions. Other studies of user behavior in web search include Pharo and Järvelin [19], but were not directly applied to improve ranking. More recently, Joachims et al. [12] presented an empirical evaluation of interpreting clickthrough evidence. By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthroughs in a controlled, laboratory setting. Unfortunately, the extent to which previous research applies to real-world web search is unclear. At the same time, while recent work (e.g., [26]) on using clickthrough information for improving web search ranking is promising, it captures only one aspect of the user interactions with web search engines. We build on existing research to develop robust user behavior interpretation techniques for the real web search setting. Instead of treating each user as a reliable expert, we aggregate information from multiple, unreliable, user search session traces, as we describe in the next two sections. 3. INCORPORATING IMPLICIT FEEDBACK We consider two complementary approaches to ranking with implicit feedback: (1) treating implicit feedback as independent evidence for ranking results, and (2) integrating implicit feedback features directly into the ranking algorithm. We describe the two general ranking approaches next. The specific implicit feedback features are described in Section 4, and the algorithms for interpreting and incorporating implicit feedback are described in Section 5. 3.1 Implicit Feedback as Independent Evidence The general approach is to re-rank the results obtained by a web search engine according to observed clickthrough and other user interactions for the query in previous search sessions. Each result is assigned a score according to expected relevance/user satisfaction based on previous interactions, resulting in some preference ordering based on user interactions alone. While there has been significant work on merging multiple rankings, we adapt a simple and robust approach of ignoring the original rankers scores, and instead simply merge the rank orders. The main reason for ignoring the original scores is that since the feature spaces and learning algorithms are different, the scores are not directly comparable, and re-normalization tends to remove the benefit of incorporating classifier scores. We experimented with a variety of merging functions on the development set of queries (and using a set of interactions from a different time period from final evaluation sets). We found that a simple rank merging heuristic combination works well, and is robust to variations in score values from original rankers. For a given query q, the implicit score ISd is computed for each result d from available user interaction features, resulting in the implicit rank Id for each result. We compute a merged score SM(d) for d by combining the ranks obtained from implicit feedback, Id with the original rank of d, Od:     ¡     ¢ £ + + + + = otherwise O dforexistsfeedbackimplicitif OI w wOIdS d dd I IddM 1 1 1 1 1 1 ),,,( where the weight wI is a heuristically tuned scaling factor representing the relative importance of the implicit feedback. The query results are ordered in by decreasing values of SM to produce the final ranking. One special case of this model arises when setting wI to a very large value, effectively forcing clicked results to be ranked higher than un-clicked results - an intuitive and effective heuristic that we will use as a baseline. Applying more sophisticated classifier and ranker combination algorithms may result in additional improvements, and is a promising direction for future work. The approach above assumes that there are no interactions between the underlying features producing the original web search ranking and the implicit feedback features. We now relax this assumption by integrating implicit feedback features directly into the ranking process. 3.2 Ranking with Implicit Feedback Features Modern web search engines rank results based on a large number of features, including content-based features (i.e., how closely a query matches the text or title or anchor text of the document), and query-independent page quality features (e.g., PageRank of the document or the domain). In most cases, automatic (or semiautomatic) methods are developed for tuning the specific ranking function that combines these feature values. Hence, a natural approach is to incorporate implicit feedback features directly as features for the ranking algorithm. During training or tuning, the ranker can be tuned as before but with additional features. At runtime, the search engine would fetch the implicit feedback features associated with each query-result URL pair. This model requires a ranking algorithm to be robust to missing values: more than 50% of queries to web search engines are unique, with no previous implicit feedback available. We now describe such a ranker that we used to learn over the combined feature sets including implicit feedback. 3.3 Learning to Rank Web Search Results A key aspect of our approach is exploiting recent advances in machine learning, namely trainable ranking algorithms for web search and information retrieval (e.g., [5, 11] and classical results reviewed in [3]). In our setting, explicit human relevance judgments (labels) are available for a set of web search queries and results. Hence, an attractive choice to use is a supervised machine learning technique to learn a ranking function that best predicts relevance judgments. RankNet is one such algorithm. It is a neural net tuning algorithm that optimizes feature weights to best match explicitly provided pairwise user preferences. While the specific training algorithms used by RankNet are beyond the scope of this paper, it is described in detail in [5] and includes extensive evaluation and comparison with other ranking methods. An attractive feature of RankNet is both train- and run-time efficiency - runtime ranking can be quickly computed and can scale to the web, and training can be done over thousands of queries and associated judged results. We use a 2-layer implementation of RankNet in order to model non-linear relationships between features. Furthermore, RankNet can learn with many (differentiable) cost functions, and hence can automatically learn a ranking function from human-provided labels, an attractive alternative to heuristic feature combination techniques. Hence, we will also use RankNet as a generic ranker to explore the contribution of implicit feedback for different ranking alternatives. 4. IMPLICIT USER FEEDBACK MODEL Our goal is to accurately interpret noisy user feedback obtained as by tracing user interactions with the search engine. Interpreting implicit feedback in real web search setting is not an easy task. We characterize this problem in detail in [1], where we motivate and evaluate a wide variety of models of implicit user activities. The general approach is to represent user actions for each search result as a vector of features, and then train a ranker on these features to discover feature values indicative of relevant (and nonrelevant) search results. We first briefly summarize our features and model, and the learning approach (Section 4.2) in order to provide sufficient information to replicate our ranking methods and the subsequent experiments. 4.1 Representing User Actions as Features We model observed web search behaviors as a combination of a ``background component (i.e., query- and relevance-independent noise in user behavior, including positional biases with result interactions), and a ``relevance component (i.e., query-specific behavior indicative of relevance of a result to a query). We design our features to take advantage of aggregated user behavior. The feature set is comprised of directly observed features (computed directly from observations for each query), as well as queryspecific derived features, computed as the deviation from the overall query-independent distribution of values for the corresponding directly observed feature values. The features used to represent user interactions with web search results are summarized in Table 4.1. This information was obtained via opt-in client-side instrumentation from users of a major web search engine. We include the traditional implicit feedback features such as clickthrough counts for the results, as well as our novel derived features such as the deviation of the observed clickthrough number for a given query-URL pair from the expected number of clicks on a result in the given position. We also model the browsing behavior after a result was clicked - e.g., the average page dwell time for a given query-URL pair, as well as its deviation from the expected (average) dwell time. Furthermore, the feature set was designed to provide essential information about the user experience to make feedback interpretation robust. For example, web search users can often determine whether a result is relevant by looking at the result title, URL, and summary - in many cases, looking at the original document is not necessary. To model this aspect of user experience we include features such as overlap in words in title and words in query (TitleOverlap) and the fraction of words shared by the query and the result summary. Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickProbability Probability of a click for this query and URL ClickDeviation Deviation from expected click probability IsNextClicked 1 if clicked on next position, 0 otherwise IsPreviousClicked 1 if clicked on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, no parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from average dwell time on page CumulativeDeviation Deviation from average cumulative dwell time DomainDeviation Deviation from average dwell time on domain Query-text features TitleOverlap Words shared between query and title SummaryOverlap Words shared between query and snippet QueryURLOverlap Words shared between query and URL QueryDomainOverlap Words shared between query and URL domain QueryLength Number of tokens in query QueryNextOverlap Fraction of words shared with next query Table 4.1: Some features used to represent post-search navigation history for a given query and search result URL. Having described our feature set, we briefly review our general method for deriving a user behavior model. 4.2 Deriving a User Feedback Model To learn to interpret the observed user behavior, we correlate user actions (i.e., the features in Table 4.1 representing the actions) with the explicit user judgments for a set of training queries. We find all the instances in our session logs where these queries were submitted to the search engine, and aggregate the user behavior features for all search sessions involving these queries. Each observed query-URL pair is represented by the features in Table 4.1, with values averaged over all search sessions, and assigned one of six possible relevance labels, ranging from Perfect to Bad, as assigned by explicit relevance judgments. These labeled feature vectors are used as input to the RankNet training algorithm (Section 3.3) which produces a trained user behavior model. This approach is particularly attractive as it does not require heuristics beyond feature engineering. The resulting user behavior model is used to help rank web search resultseither directly or in combination with other features, as described below. 5. EXPERIMENTAL SETUP The ultimate goal of incorporating implicit feedback into ranking is to improve the relevance of the returned web search results. Hence, we compare the ranking methods over a large set of judged queries with explicit relevance labels provided by human judges. In order for the evaluation to be realistic we obtained a random sample of queries from web search logs of a major search engine, with associated results and traces for user actions. We describe this dataset in detail next. Our metrics are described in Section 5.2 that we use to evaluate the ranking alternatives, listed in Section 5.3 in the experiments of Section 6. 5.1 Datasets We compared our ranking methods over a random sample of 3,000 queries from the search engine query logs. The queries were drawn from the logs uniformly at random by token without replacement, resulting in a query sample representative of the overall query distribution. On average, 30 results were explicitly labeled by human judges using a six point scale ranging from Perfect down to Bad. Overall, there were over 83,000 results with explicit relevance judgments. In order to compute various statistics, documents with label Good or better will be considered relevant, and with lower labels to be non-relevant. Note that the experiments were performed over the results already highly ranked by a web search engine, which corresponds to a typical user experience which is limited to the small number of the highly ranked results for a typical web search query. The user interactions were collected over a period of 8 weeks using voluntary opt-in information. In total, over 1.2 million unique queries were instrumented, resulting in over 12 million individual interactions with the search engine. The data consisted of user interactions with the web search engine (e.g., clicking on a result link, going back to search results, etc.) performed after a query was submitted. These actions were aggregated across users and search sessions and converted to features in Table 4.1. To create the training, validation, and test query sets, we created three different random splits of 1,500 training, 500 validation, and 1000 test queries. The splits were done randomly by query, so that there was no overlap in training, validation, and test queries. 5.2 Evaluation Metrics We evaluate the ranking algorithms over a range of accepted information retrieval metrics, namely Precision at K (P(K)), Normalized Discounted Cumulative Gain (NDCG), and Mean Average Precision (MAP). Each metric focuses on a deferent aspect of system performance, as we describe below. • Precision at K: As the most intuitive metric, P(K) reports the fraction of documents ranked in the top K results that are labeled as relevant. In our setting, we require a relevant document to be labeled Good or higher. The position of relevant documents within the top K is irrelevant, and hence this metric measure overall user satisfaction with the top K results. • NDCG at K: NDCG is a retrieval measure devised specifically for web search evaluation [10]. For a given query q, the ranked results are examined from the top ranked down, and the NDCG computed as:   = +−= K j jr qq jMN 1 )( )1log(/)12( Where Mq is a normalization constant calculated so that a perfect ordering would obtain NDCG of 1; and each r(j) is an integer relevance label (0=Bad and 5=Perfect) of result returned at position j. Note that unlabeled and Bad documents do not contribute to the sum, but will reduce NDCG for the query pushing down the relevant labeled documents, reducing their contributions. NDCG is well suited to web search evaluation, as it rewards relevant documents in the top ranked results more heavily than those ranked lower. • MAP: Average precision for each query is defined as the mean of the precision at K values computed after each relevant document was retrieved. The final MAP value is defined as the mean of average precisions of all queries in the test set. This metric is the most commonly used single-value summary of a run over a set of queries. 5.3 Ranking Methods Compared Recall that our goal is to quantify the effectiveness of implicit behavior for real web search. One dimension is to compare the utility of implicit feedback with other information available to a web search engine. Specifically, we compare effectiveness of implicit user behaviors with content-based matching, static page quality features, and combinations of all features. • BM25F: As a strong web search baseline we used the BM25F scoring, which was used in one of the best performing systems in the TREC 2004 Web track [23,27]. BM25F and its variants have been extensively described and evaluated in IR literature, and hence serve as a strong, reproducible baseline. The BM25F variant we used for our experiments computes separate match scores for each field for a result document (e.g., body text, title, and anchor text), and incorporates query-independent linkbased information (e.g., PageRank, ClickDistance, and URL depth). The scoring function and field-specific tuning is described in detail in [23]. Note that BM25F does not directly consider explicit or implicit feedback for tuning. • RN: The ranking produced by a neural net ranker (RankNet, described in Section 3.3) that learns to rank web search results by incorporating BM25F and a large number of additional static and dynamic features describing each search result. This system automatically learns weights for all features (including the BM25F score for a document) based on explicit human labels for a large set of queries. A system incorporating an implementation of RankNet is currently in use by a major search engine and can be considered representative of the state of the art in web search. • BM25F-RerankCT: The ranking produced by incorporating clickthrough statistics to reorder web search results ranked by BM25F above. Clickthrough is a particularly important special case of implicit feedback, and has been shown to correlate with result relevance. This is a special case of the ranking method in Section 3.1, with the weight wI set to 1000 and the ranking Id is simply the number of clicks on the result corresponding to d. In effect, this ranking brings to the top all returned web search results with at least one click (and orders them in decreasing order by number of clicks). The relative ranking of the remainder of results is unchanged and they are inserted below all clicked results. This method serves as our baseline implicit feedback reranking method. BM25F-RerankAll The ranking produced by reordering the BM25F results using all user behavior features (Section 4). This method learns a model of user preferences by correlating feature values with explicit relevance labels using the RankNet neural net algorithm (Section 4.2). At runtime, for a given query the implicit score Ir is computed for each result r with available user interaction features, and the implicit ranking is produced. The merged ranking is computed as described in Section 3.1. Based on the experiments over the development set we fix the value of wI to 3 (the effect of the wI parameter for this ranker turned out to be negligible). • BM25F+All: Ranking derived by training the RankNet (Section 3.3) learner over the features set of the BM25F score as well as all implicit feedback features (Section 3.2). We used the 2-layer implementation of RankNet [5] trained on the queries and labels in the training and validation sets. • RN+All: Ranking derived by training the 2-layer RankNet ranking algorithm (Section 3.3) over the union of all content, dynamic, and implicit feedback features (i.e., all of the features described above as well as all of the new implicit feedback features we introduced). The ranking methods above span the range of the information used for ranking, from not using the implicit or explicit feedback at all (i.e., BM25F) to a modern web search engine using hundreds of features and tuned on explicit judgments (RN). As we will show next, incorporating user behavior into these ranking systems dramatically improves the relevance of the returned documents. 6. EXPERIMENTAL RESULTS Implicit feedback for web search ranking can be exploited in a number of ways. We compare alternative methods of exploiting implicit feedback, both by re-ranking the top results (i.e., the BM25F-RerankCT and BM25F-RerankAll methods that reorder BM25F results), as well as by integrating the implicit features directly into the ranking process (i.e., the RN+ALL and BM25F+All methods which learn to rank results over the implicit feedback and other features). We compare our methods over strong baselines (BM25F and RN) over the NDCG, Precision at K, and MAP measures defined in Section 5.2. The results were averaged over three random splits of the overall dataset. Each split contained 1500 training, 500 validation, and 1000 test queries, all query sets disjoint. We first present the results over all 1000 test queries (i.e., including queries for which there are no implicit measures so we use the original web rankings). We then drill down to examine the effects on reranking for the attempted queries in more detail, analyzing where implicit feedback proved most beneficial. We first experimented with different methods of re-ranking the output of the BM25F search results. Figures 6.1 and 6.2 report NDCG and Precision for BM25F, as well as for the strategies reranking results with user feedback (Section 3.1). Incorporating all user feedback (either in reranking framework or as features to the learner directly) results in significant improvements (using two-tailed t-test with p=0.01) over both the original BM25F ranking as well as over reranking with clickthrough alone. The improvement is consistent across the top 10 results and largest for the top result: NDCG at 1 for BM25F+All is 0.622 compared to 0.518 of the original results, and precision at 1 similarly increases from 0.5 to 0.63. Based on these results we will use the direct feature combination (i.e., BM25F+All) ranker for subsequent comparisons involving implicit feedback. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 1 2 3 4 5 6 7 8 9 10K NDCG BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.1: NDCG at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K 0.35 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.2: Precision at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K Interestingly, using clickthrough alone, while giving significant benefit over the original BM25F ranking, is not as effective as considering the full set of features in Table 4.1. While we analyze user behavior (and most effective component features) in a separate paper [1], it is worthwhile to give a concrete example of the kind of noise inherent in real user feedback in web search setting. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 Result position Relativeclickfrequency PTR=2 PTR=3 PTR=5 Figure 6.3: Relative clickthrough frequency for queries with varying Position of Top Relevant result (PTR). If users considered only the relevance of a result to their query, they would click on the topmost relevant results. Unfortunately, as Joachims and others have shown, presentation also influences which results users click on quite dramatically. Users often click on results above the relevant one presumably because the short summaries do not provide enough information to make accurate relevance assessments and they have learned that on average topranked items are relevant. Figure 6.3 shows relative clickthrough frequencies for queries with known relevant items at positions other than the first position; the position of the top relevant result (PTR) ranges from 2-10 in the figure. For example, for queries with first relevant result at position 5 (PTR=5), there are more clicks on the non-relevant results in higher ranked positions than on the first relevant result at position 5. As we will see, learning over a richer behavior feature set, results in substantial accuracy improvement over clickthrough alone. We now consider incorporating user behavior into a much richer feature set, RN (Section 5.3) used by a major web search engine. RN incorporates BM25F, link-based features, and hundreds of other features. Figure 6.4 reports NDCG at K and Figure 6.5 reports Precision at K. Interestingly, while the original RN rankings are significantly more accurate than BM25F alone, incorporating implicit feedback features (BM25F+All) results in ranking that significantly outperforms the original RN rankings. In other words, implicit feedback incorporates sufficient information to replace the hundreds of other features available to the RankNet learner trained on the RN feature set. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 1 2 3 4 5 6 7 8 9 10K NDCG RN RN+All BM25 BM25+All Figure 6.4: NDCG at K for BM25F, BM25F+All, RN, and RN+All for varying K Furthermore, enriching the RN features with implicit feedback set exhibits significant gain on all measures, allowing RN+All to outperform all other methods. This demonstrates the complementary nature of implicit feedback with other features available to a state of the art web search engine. 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision RN RN+All BM25 BM25+All Figure 6.5: Precision at K for BM25F, BM25F+All, RN, and RN+All for varying K We summarize the performance of the different ranking methods in Table 6.1. We report the Mean Average Precision (MAP) score for each system. While not intuitive to interpret, MAP allows quantitative comparison on a single metric. The gains marked with * are significant at p=0.01 level using two tailed t-test. MAP Gain P(1) Gain BM25F 0.184 - 0.503BM25F-Rerank-CT 0.215 0.031* 0.577 0.073* BM25F-RerankImplicit 0.218 0.003 0.605 0.028* BM25F+Implicit 0.222 0.004 0.620 0.015* RN 0.215 - 0.597RN+All 0.248 0.033* 0.629 0.032* Table 6.1: Mean Average Precision (MAP) for all strategies. So far we reported results averaged across all queries in the test set. Unfortunately, less than half had sufficient interactions to attempt reranking. Out of the 1000 queries in test, between 46% and 49%, depending on the train-test split, had sufficient interaction information to make predictions (i.e., there was at least 1 search session in which at least 1 result URL was clicked on by the user). This is not surprising: web search is heavy-tailed, and there are many unique queries. We now consider the performance on the queries for which user interactions were available. Figure 6.6 reports NDCG for the subset of the test queries with the implicit feedback features. The gains at top 1 are dramatic. The NDCG at 1 of BM25F+All increases from 0.6 to 0.75 (a 31% relative gain), achieving performance comparable to RN+All operating over a much richer feature set. 0.6 0.65 0.7 0.75 0.8 1 3 5 10K NDCG RN RN+All BM25 BM25+All Figure 6.6: NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions Similarly, gains on precision at top 1 are substantial (Figure 6.7), and are likely to be apparent to web search users. When implicit feedback is available, the BM25F+All system returns relevant document at top 1 almost 70% of the time, compared 53% of the time when implicit feedback is not considered by the original BM25F system. 0.45 0.5 0.55 0.6 0.65 0.7 1 3 5 10K Precision RN RN+All BM25 BM25+All Figure 6.7: Precision at K NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions We summarize the results on the MAP measure for attempted queries in Table 6.2. MAP improvements are both substantial and significant, with improvements over the BM25F ranker most pronounced. Method MAP Gain P(1) Gain RN 0.269 0.632 RN+All 0.321 0.051 (19%) 0.693 0.061(10%) BM25F 0.236 0.525 BM25F+All 0.292 0.056 (24%) 0.687 0.162 (31%) Table 6.2: Mean Average Precision (MAP) on attempted queries for best performing methods We now analyze the cases where implicit feedback was shown most helpful. Figure 6.8 reports the MAP improvements over the baseline BM25F run for each query with MAP under 0.6. Note that most of the improvement is for poorly performing queries (i.e., MAP < 0.1). Interestingly, incorporating user behavior information degrades accuracy for queries with high original MAP score. One possible explanation is that these easy queries tend to be navigational (i.e., having a single, highly-ranked most appropriate answer), and user interactions with lower-ranked results may indicate divergent information needs that are better served by the less popular results (with correspondingly poor overall relevance ratings). 0 50 100 150 200 250 300 350 0.1 0.2 0.3 0.4 0.5 0.6 -0.4 -0.35 -0.3 -0.25 -0.2 -0.15 -0.1 -0.05 0 0.05 0.1 0.15 0.2 Frequency Average Gain Figure 6.8: Gain of BM25F+All over original BM25F ranking To summarize our experimental results, incorporating implicit feedback in real web search setting resulted in significant improvements over the original rankings, using both BM25F and RN baselines. Our rich set of implicit features, such as time on page and deviations from the average behavior, provides advantages over using clickthrough alone as an indicator of interest. Furthermore, incorporating implicit feedback features directly into the learned ranking function is more effective than using implicit feedback for reranking. The improvements observed over large test sets of queries (1,000 total, between 466 and 495 with implicit feedback available) are both substantial and statistically significant. 7. CONCLUSIONS AND FUTURE WORK In this paper we explored the utility of incorporating noisy implicit feedback obtained in a real web search setting to improve web search ranking. We performed a large-scale evaluation over 3,000 queries and more than 12 million user interactions with a major search engine, establishing the utility of incorporating noisy implicit feedback to improve web search relevance. We compared two alternatives of incorporating implicit feedback into the search process, namely reranking with implicit feedback and incorporating implicit feedback features directly into the trained ranking function. Our experiments showed significant improvement over methods that do not consider implicit feedback. The gains are particularly dramatic for the top K=1 result in the final ranking, with precision improvements as high as 31%, and the gains are substantial for all values of K. Our experiments showed that implicit user feedback can further improve web search performance, when incorporated directly with popular content- and link-based features. Interestingly, implicit feedback is particularly valuable for queries with poor original ranking of results (e.g., MAP lower than 0.1). One promising direction for future work is to apply recent research on automatically predicting query difficulty, and only attempt to incorporate implicit feedback for the difficult queries. As another research direction we are exploring methods for extending our predictions to the previously unseen queries (e.g., query clustering), which should further improve the web search experience of users. ACKNOWLEDGMENTS We thank Chris Burges and Matt Richardson for an implementation of RankNet for our experiments. We also thank Robert Ragno for his valuable suggestions and many discussions. 8. REFERENCES [1] E. Agichtein, E. Brill, S. Dumais, and R.Ragno, Learning User Interaction Models for Predicting Web Search Result Preferences. In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan, HARD Track Overview in TREC 2003, High Accuracy Retrieval from Documents, 2003 [3] R. Baeza-Yates and B. Ribeiro-Neto, Modern Information Retrieval, Addison-Wesley, 1999. [4] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine, in Proceedings of WWW, 1997 [5] C.J.C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning, 2005 [6] D.M. Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [7] M. Claypool, D. Brown, P. Lee and M. Waseda. Inferring user interest. IEEE Internet Computing. 2001 [8] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White. Evaluating implicit measures to improve the search experience. In ACM Transactions on Information Systems, 2005 [9] J. Goecks and J. Shavlick. Learning users interests by unobtrusively observing their normal behavior. In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [10] K Jarvelin and J. Kekalainen. IR evaluation methods for retrieving highly relevant documents. In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2000 [11] T. Joachims, Optimizing Search Engines Using Clickthrough Data. In Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [12] T. Joachims, L. Granka, B. Pang, H. Hembrooke, and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [13] T. Joachims, Making Large-Scale SVM Learning Practical. Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [14] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography. In SIGIR Forum, 2003 [15] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon, and J. Riedl. GroupLens: Applying collaborative filtering to usenet news. In Communications of ACM, 1997. [16] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval. Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [17] D. Oard and J. Kim. Implicit feedback for recommender systems. In Proceedings of the AAAI Workshop on Recommender Systems. 1998 [18] D. Oard and J. Kim. Modeling information content using observable behavior. In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [19] N. Pharo, N. and K. Järvelin. The SST method: a tool for analyzing web information search processes. In Information Processing & Management, 2004 [20] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web. In Working with Technology in Mind: Brunswikian. Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [21] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback. In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (SIGKDD), 2005. [22] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in Proceedings of the ICML Workshop on Learning in Web Search, 2005 [23] S. E. Robertson, H. Zaragoza, and M. Taylor, Simple BM25 extension to multiple weighted fields, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [24] G. Salton & M. McGill. Introduction to modern information retrieval. McGraw-Hill, 1983 [25] E.M. Voorhees, D. Harman, Overview of TREC, 2001 [26] G.R. Xue, H.J. Zeng, Z. Chen, Y. Yu, W.Y. Ma, W.S. Xi, and W.G. Fan, Optimizing web search using web clickthrough data, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [27] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson. Microsoft Cambridge at TREC 13: Web and Hard Tracks. In Proceedings of TREC 2004",
    "original_translation": "Mejorando la clasificación de búsqueda web al incorporar información sobre el comportamiento del usuario. Eugene Agichtein, Microsoft Research eugeneag@microsoft.com Eric Brill, Microsoft Research brill@microsoft.com Susan Dumais, Microsoft Research sdumais@microsoft.com RESUMEN Mostramos que la incorporación de datos sobre el comportamiento del usuario puede mejorar significativamente el orden de los principales resultados en una configuración real de búsqueda web. Examinamos alternativas para incorporar retroalimentación en el proceso de clasificación y exploramos las contribuciones de la retroalimentación de usuarios en comparación con otras características comunes de búsqueda en la web. Informamos los resultados de una evaluación a gran escala con más de 3,000 consultas y 12 millones de interacciones de usuarios con un motor de búsqueda web popular. Mostramos que la incorporación de retroalimentación implícita puede aumentar otras características, mejorando la precisión de los algoritmos de clasificación de búsqueda web competitivos hasta en un 31% en comparación con el rendimiento original. Categorías y Descriptores de Asignaturas H.3.3 Búsqueda y Recuperación de Información - Retroalimentación de relevancia, proceso de búsqueda; H.3.5 Servicios de Información en Línea - Servicios basados en la web. Términos generales Algoritmos, Medición, Experimentación 1. INTRODUCCIÓN Millones de usuarios interactúan con los motores de búsqueda a diario. Ellos emiten consultas, siguen algunos de los enlaces en los resultados, hacen clic en anuncios, pasan tiempo en páginas, reformulan sus consultas y realizan otras acciones. Estas interacciones pueden servir como una valiosa fuente de información para ajustar y mejorar la clasificación de los resultados de búsqueda en la web y pueden complementar juicios explícitos más costosos. La retroalimentación implícita de relevancia para la clasificación y personalización se ha convertido en un área activa de investigación. El trabajo reciente de Joachims y otros explorando el feedback implícito en entornos controlados ha demostrado el valor de incorporar el feedback implícito en el proceso de clasificación. Nuestra motivación para este trabajo es comprender cómo se puede utilizar la retroalimentación implícita en un entorno operativo a gran escala para mejorar la recuperación. ¿Cómo se compara y complementa con la evidencia del contenido de la página, el texto del ancla o las características basadas en enlaces como inlinks o PageRank? Si bien es intuitivo que las interacciones de los usuarios con el motor de búsqueda web deberían revelar al menos alguna información que podría ser utilizada para la clasificación, estimar las preferencias de los usuarios en entornos reales de búsqueda web es un problema desafiante, ya que las interacciones reales tienden a ser más ruidosas de lo que comúnmente se asume en los entornos controlados de estudios previos. Nuestro artículo explora si la retroalimentación implícita puede ser útil en entornos realistas, donde la retroalimentación del usuario puede ser ruidosa (o adversa) y un motor de búsqueda web ya utiliza cientos de características y está altamente ajustado. Con este fin, exploramos diferentes enfoques para clasificar los resultados de búsqueda web utilizando el comportamiento real de los usuarios obtenido como parte de las interacciones normales con el motor de búsqueda web. Las contribuciones específicas de este artículo incluyen: • Análisis de alternativas para incorporar el comportamiento del usuario en la clasificación de búsqueda web (Sección 3). • Aplicación de un modelo robusto de retroalimentación implícita derivado de la minería de millones de interacciones de usuarios con un importante motor de búsqueda web (Sección 4). • Una evaluación a gran escala sobre consultas reales de usuarios y resultados de búsqueda, mostrando mejoras significativas derivadas de la incorporación de la retroalimentación del usuario (Sección 6). Resumimos nuestros hallazgos y discutimos extensiones al trabajo actual en la Sección 7, que concluye el artículo. ANTECEDENTES Y TRABAJO RELACIONADO Clasificar los resultados de búsqueda es un problema fundamental en la recuperación de información. La mayoría de los enfoques comunes se centran principalmente en la similitud entre la consulta y una página, así como en la calidad general de la página [3,4,24]. Sin embargo, con la creciente popularidad de los motores de búsqueda, la retroalimentación implícita (es decir, las acciones que los usuarios realizan al interactuar con el motor de búsqueda) se puede utilizar para mejorar las clasificaciones. Las medidas de relevancia implícitas han sido estudiadas por varios grupos de investigación. Un resumen de las medidas implícitas se recopila en Kelly y Teevan [14]. Esta investigación, si bien desarrolló valiosas ideas sobre medidas implícitas de relevancia, no se aplicó para mejorar la clasificación de los resultados de búsqueda en la web en entornos realistas. Estrechamente relacionado con nuestro trabajo, Joachims [11] recopiló medidas implícitas en lugar de medidas explícitas, introduciendo una técnica basada completamente en datos de clics para aprender funciones de clasificación. Fox et al. [8] exploraron la relación entre medidas implícitas y explícitas en la búsqueda web, y desarrollaron modelos bayesianos para correlacionar medidas implícitas y juicios explícitos de relevancia tanto para consultas individuales como para sesiones de búsqueda. Este trabajo consideró una amplia gama de comportamientos de usuario (por ejemplo, tiempo de permanencia, tiempo de desplazamiento, patrones de reformulación) además del popular comportamiento de clics. Sin embargo, el esfuerzo de modelado se centró en predecir juicios de relevancia explícitos a partir de acciones implícitas de los usuarios y no específicamente en aprender funciones de clasificación. Otros estudios sobre el comportamiento de los usuarios en la búsqueda web incluyen a Pharo y Järvelin [19], pero no se aplicaron directamente para mejorar la clasificación. Más recientemente, Joachims et al. [12] presentaron una evaluación empírica de la interpretación de la evidencia de clics. Al realizar estudios de seguimiento ocular y correlacionar las predicciones de sus estrategias con las calificaciones explícitas, los autores demostraron que es posible interpretar con precisión los clics en un entorno controlado de laboratorio. Desafortunadamente, no está claro en qué medida la investigación previa se aplica a la búsqueda web del mundo real. Al mismo tiempo, si bien el trabajo reciente (por ejemplo, [26]) sobre el uso de la información de clics para mejorar la clasificación de búsqueda en la web es prometedor, solo abarca un aspecto de las interacciones de los usuarios con los motores de búsqueda en la web. Nos basamos en investigaciones existentes para desarrollar técnicas robustas de interpretación del comportamiento del usuario para el entorno real de búsqueda en la web. En lugar de tratar a cada usuario como un experto confiable, agregamos información de múltiples trazas de sesiones de búsqueda de usuarios no confiables, como describimos en las siguientes dos secciones. 3. INTEGRANDO LA RETROALIMENTACIÓN IMPLÍCITA Consideramos dos enfoques complementarios para la clasificación con retroalimentación implícita: (1) tratar la retroalimentación implícita como evidencia independiente para clasificar resultados, y (2) integrar características de retroalimentación implícita directamente en el algoritmo de clasificación. Describimos a continuación los dos enfoques generales de clasificación. Las características específicas del feedback implícito se describen en la Sección 4, y los algoritmos para interpretar e incorporar el feedback implícito se describen en la Sección 5. 3.1 Feedback Implícito como Evidencia Independiente. El enfoque general es reordenar los resultados obtenidos por un motor de búsqueda web según los clics observados y otras interacciones de usuario para la consulta en sesiones de búsqueda anteriores. Cada resultado se le asigna una puntuación según la relevancia esperada/ satisfacción del usuario basada en interacciones previas, lo que resulta en un cierto orden de preferencia basado únicamente en las interacciones del usuario. Si bien ha habido un trabajo significativo en la fusión de múltiples clasificaciones, adaptamos un enfoque simple y robusto de ignorar las puntuaciones de los clasificadores originales, y en su lugar simplemente fusionamos los órdenes de clasificación. La razón principal para ignorar las puntuaciones originales es que, dado que los espacios de características y los algoritmos de aprendizaje son diferentes, las puntuaciones no son directamente comparables, y la re-normalización tiende a eliminar el beneficio de incorporar las puntuaciones del clasificador. Experimentamos con una variedad de funciones de fusión en el conjunto de desarrollo de consultas (y utilizando un conjunto de interacciones de un período de tiempo diferente de los conjuntos de evaluación final). Encontramos que una combinación heurística simple de fusión de rangos funciona bien y es robusta a variaciones en los valores de puntuación de los clasificadores originales. Para una consulta dada q, se calcula la puntuación implícita ISd para cada resultado d a partir de las características de interacción del usuario disponibles, lo que resulta en la clasificación implícita Id para cada resultado. Calculamos un puntaje combinado SM(d) para d al combinar los rangos obtenidos de la retroalimentación implícita, Id, con el rango original de d, Od: SM(d) = Od + wI * Id, si existe retroalimentación implícita; de lo contrario, SM(d) = Od. Donde el peso wI es un factor de escala ajustado heurísticamente que representa la importancia relativa de la retroalimentación implícita. Los resultados de la consulta se ordenan en valores decrecientes de SM para producir la clasificación final. Un caso especial de este modelo surge al establecer wI en un valor muy grande, lo que efectivamente obliga a que los resultados clicados se clasifiquen por encima de los resultados no clicados, una heurística intuitiva y efectiva que utilizaremos como referencia. Aplicar algoritmos de combinación de clasificadores y ordenadores más sofisticados puede resultar en mejoras adicionales, y es una dirección prometedora para trabajos futuros. El enfoque anterior asume que no hay interacciones entre las características subyacentes que producen la clasificación original de la búsqueda web y las características de retroalimentación implícitas. Ahora relajamos esta suposición al integrar características de retroalimentación implícita directamente en el proceso de clasificación. 3.2 Clasificación con Características de Retroalimentación Implícita Los motores de búsqueda web modernos clasifican los resultados en función de un gran número de características, incluidas las características basadas en el contenido (es decir, qué tan cerca coincide una consulta con el texto, el título o el texto de anclaje del documento) y las características de calidad de página independientes de la consulta (por ejemplo, PageRank del documento o del dominio). En la mayoría de los casos, se desarrollan métodos automáticos (o semiautomáticos) para ajustar la función de clasificación específica que combina estos valores de características. Por lo tanto, un enfoque natural es incorporar características de retroalimentación implícita directamente como características para el algoritmo de clasificación. Durante el entrenamiento o ajuste, el clasificador puede ser ajustado como antes pero con características adicionales. En tiempo de ejecución, el motor de búsqueda recuperaría las características de retroalimentación implícita asociadas con cada par de URL de consulta y resultado. Este modelo requiere que un algoritmo de clasificación sea robusto ante valores faltantes: más del 50% de las consultas a los motores de búsqueda web son únicas, sin disponer de retroalimentación implícita previa. Ahora describimos un clasificador que utilizamos para aprender sobre los conjuntos de características combinadas, incluyendo la retroalimentación implícita. 3.3 Aprendizaje para Clasificar los Resultados de Búsqueda en la Web Un aspecto clave de nuestro enfoque es aprovechar los avances recientes en el aprendizaje automático, en particular los algoritmos de clasificación entrenables para la búsqueda en la web y la recuperación de información (por ejemplo, [5, 11] y resultados clásicos revisados en [3]). En nuestro entorno, contamos con juicios explícitos de relevancia humana (etiquetas) para un conjunto de consultas de búsqueda en la web y resultados. Por lo tanto, una opción atractiva es utilizar una técnica de aprendizaje automático supervisado para aprender una función de clasificación que prediga mejor las evaluaciones de relevancia. RankNet es uno de esos algoritmos. Es un algoritmo de ajuste de red neuronal que optimiza los pesos de las características para que coincidan mejor con las preferencias de los usuarios proporcionadas explícitamente en pares. Si bien los algoritmos de entrenamiento específicos utilizados por RankNet están más allá del alcance de este documento, se describen en detalle en [5] e incluyen una evaluación exhaustiva y comparación con otros métodos de clasificación. Una característica atractiva de RankNet es su eficiencia tanto en el tiempo de entrenamiento como en el de ejecución: el ranking en tiempo de ejecución se puede calcular rápidamente y puede escalarse a la web, y el entrenamiento se puede realizar sobre miles de consultas y resultados juzgados asociados. Utilizamos una implementación de RankNet de 2 capas para modelar relaciones no lineales entre características. Además, RankNet puede aprender con muchas funciones de costo (diferenciables), por lo que puede aprender automáticamente una función de clasificación a partir de etiquetas proporcionadas por humanos, una alternativa atractiva a las técnicas de combinación de características heurísticas. Por lo tanto, también utilizaremos RankNet como un clasificador genérico para explorar la contribución de la retroalimentación implícita en diferentes alternativas de clasificación. 4. MODELO IMPLÍCITO DE RETROALIMENTACIÓN DEL USUARIO Nuestro objetivo es interpretar con precisión la retroalimentación ruidosa del usuario obtenida al rastrear las interacciones del usuario con el motor de búsqueda. Interpretar la retroalimentación implícita en un entorno real de búsqueda en la web no es una tarea fácil. Caracterizamos este problema en detalle en [1], donde motivamos y evaluamos una amplia variedad de modelos de actividades implícitas de usuario. El enfoque general es representar las acciones del usuario para cada resultado de búsqueda como un vector de características, y luego entrenar un clasificador en base a estas características para descubrir los valores de las características que indican resultados de búsqueda relevantes (y no relevantes). Primero resumimos brevemente nuestras características y modelo, y el enfoque de aprendizaje (Sección 4.2) para proporcionar suficiente información para replicar nuestros métodos de clasificación y los experimentos subsiguientes. 4.1 Representación de las acciones del usuario como características. Modelamos los comportamientos observados en la búsqueda web como una combinación de un \"componente de fondo (es decir, ruido independiente de la consulta y relevancia en el comportamiento del usuario, incluidos sesgos posicionales con interacciones de resultados), y un \"componente de relevancia (es decir, comportamiento específico de la consulta indicativo de la relevancia de un resultado para una consulta). Diseñamos nuestras características para aprovechar el comportamiento de usuario agregado. El conjunto de características está compuesto por características observadas directamente (calculadas directamente a partir de observaciones para cada consulta), así como características derivadas específicas de la consulta, calculadas como la desviación de la distribución general de valores independientes de la consulta para los valores de las características observadas directamente correspondientes. Las características utilizadas para representar las interacciones de los usuarios con los resultados de búsqueda en la web se resumen en la Tabla 4.1. Esta información se obtuvo a través de la instrumentación del lado del cliente opt-in de usuarios de un importante motor de búsqueda web. Incluimos las características tradicionales de retroalimentación implícita, como el recuento de clics en los resultados, así como nuestras características derivadas novedosas, como la desviación del número de clics observados para un par de consulta-URL dado del número esperado de clics en un resultado en la posición dada. También modelamos el comportamiento de navegación después de que se hace clic en un resultado, por ejemplo, el tiempo promedio de permanencia en la página para un par de consulta-URL dado, así como su desviación del tiempo de permanencia esperado (promedio). Además, el conjunto de características fue diseñado para proporcionar información esencial sobre la experiencia del usuario y hacer que la interpretación de la retroalimentación sea sólida. Por ejemplo, los usuarios de búsqueda en la web a menudo pueden determinar si un resultado es relevante al mirar el título del resultado, la URL y el resumen; en muchos casos, no es necesario mirar el documento original. Para modelar este aspecto de la experiencia del usuario, incluimos características como la superposición de palabras en el título y las palabras en la consulta (TitleOverlap) y la fracción de palabras compartidas por la consulta y el resumen del resultado. Características de clics Posición Posición de la URL en el ranking actual Frecuencia de clics Número de clics para esta consulta, par URL Probabilidad de clic Probabilidad de un clic para esta consulta y URL Desviación de clic Desviación de la probabilidad de clic esperada ¿Es el siguiente clic? 1 si se hizo clic en la siguiente posición, 0 de lo contrario ¿Es el clic anterior? 1 si se hizo clic en la posición anterior, 0 de lo contrario ¿Hay clic arriba? 1 si hay un clic arriba, 0 de lo contrario ¿Hay clic abajo? 1 si hay un clic abajo, 0 de lo contrario Características de navegación Tiempo en la página Tiempo de permanencia en la página Tiempo acumulado en la página Tiempo acumulado para todas las páginas siguientes después de la búsqueda Tiempo en el dominio Tiempo de permanencia acumulado para este dominio Tiempo en URL corta Tiempo acumulado en el prefijo de la URL, sin parámetros ¿Se siguió el enlace? 1 si se siguió el enlace al resultado, 0 de lo contrario ¿Coincidencia exacta de URL? 0 si se utilizó normalización agresiva, 1 de lo contrario ¿Redirigido? 1 si la URL inicial es la misma que la URL final, 0 de lo contrario ¿Camino desde la búsqueda? 1 si solo se siguieron enlaces después de la consulta, 0 de lo contrario Clics desde la búsqueda Número de saltos para llegar a la página desde la consulta Tiempo promedio de permanencia Tiempo promedio en la página para esta consulta Desviación del tiempo de permanencia Desviación del tiempo de permanencia promedio en la página Desviación acumulada Desviación del tiempo de permanencia acumulado promedio Desviación del dominio Desviación del tiempo de permanencia promedio en el dominio Características de texto de consulta Coincidencia de título Palabras compartidas entre la consulta y el título Coincidencia de resumen Palabras compartidas entre la consulta y el fragmento Coincidencia de URL de consulta Palabras compartidas entre la consulta y la URL Coincidencia de dominio de consulta Palabras compartidas entre la consulta y el dominio de la URL Longitud de la consulta Número de tokens en la consulta Superposición con la siguiente consulta Fracción de palabras compartidas con la siguiente consulta Tabla 4.1: Algunas características utilizadas para representar el historial de navegación posterior a la búsqueda para una consulta dada y una URL de resultado de búsqueda. Habiendo descrito nuestro conjunto de características, revisamos brevemente nuestro método general para derivar un modelo de comportamiento del usuario. 4.2 Derivación de un Modelo de Retroalimentación del Usuario Para aprender a interpretar el comportamiento observado del usuario, correlacionamos las acciones del usuario (es decir, las características en la Tabla 4.1 que representan las acciones) con los juicios explícitos del usuario para un conjunto de consultas de entrenamiento. Encontramos todas las instancias en nuestros registros de sesión donde se enviaron estas consultas al motor de búsqueda, y agregamos las características del comportamiento del usuario para todas las sesiones de búsqueda que involucran estas consultas. Cada par de consulta-URL observado está representado por las características en la Tabla 4.1, con valores promediados en todas las sesiones de búsqueda, y asignado una de las seis etiquetas de relevancia posibles, que van desde Perfecto hasta Malo, según los juicios explícitos de relevancia. Estos vectores de características etiquetados se utilizan como entrada para el algoritmo de entrenamiento RankNet (Sección 3.3), el cual produce un modelo de comportamiento de usuario entrenado. Este enfoque es particularmente atractivo ya que no requiere heurísticas más allá de la ingeniería de características. El modelo de comportamiento del usuario resultante se utiliza para ayudar a clasificar los resultados de búsqueda en la web, ya sea directamente o en combinación con otras características, como se describe a continuación. 5. CONFIGURACIÓN EXPERIMENTAL El objetivo final de incorporar retroalimentación implícita en la clasificación es mejorar la relevancia de los resultados de búsqueda web devueltos. Por lo tanto, comparamos los métodos de clasificación sobre un gran conjunto de consultas evaluadas con etiquetas de relevancia explícitas proporcionadas por jueces humanos. Para que la evaluación sea realista, obtuvimos una muestra aleatoria de consultas de registros de búsqueda web de un motor de búsqueda importante, con resultados asociados y rastros de acciones de usuario. Describimos este conjunto de datos en detalle a continuación. Nuestros indicadores se describen en la Sección 5.2 que utilizamos para evaluar las alternativas de clasificación, enumeradas en la Sección 5.3 en los experimentos de la Sección 6. 5.1 Conjuntos de datos Comparamos nuestros métodos de clasificación en una muestra aleatoria de 3,000 consultas de los registros de consultas del motor de búsqueda. Las consultas fueron extraídas de los registros de manera uniforme al azar por token sin reemplazo, lo que resultó en una muestra de consultas representativa de la distribución general de consultas. En promedio, 30 resultados fueron etiquetados explícitamente por jueces humanos utilizando una escala de seis puntos que va desde Perfecto hasta Malo. En total, hubo más de 83,000 resultados con juicios de relevancia explícitos. Para calcular diversas estadísticas, se considerarán relevantes los documentos con la etiqueta Buena o mejor, y los documentos con etiquetas inferiores se considerarán no relevantes. Se debe tener en cuenta que los experimentos se realizaron sobre los resultados ya altamente clasificados por un motor de búsqueda web, lo cual corresponde a una experiencia de usuario típica que se limita al pequeño número de resultados altamente clasificados para una consulta típica de búsqueda web. Las interacciones de los usuarios fueron recopiladas durante un período de 8 semanas utilizando información voluntaria de participación. En total, se registraron más de 1.2 millones de consultas únicas, lo que resultó en más de 12 millones de interacciones individuales con el motor de búsqueda. Los datos consistían en las interacciones de los usuarios con el motor de búsqueda web (por ejemplo, hacer clic en un enlace de resultado, regresar a los resultados de búsqueda, etc.) realizadas después de enviar una consulta. Estas acciones fueron agregadas entre usuarios y sesiones de búsqueda y convertidas en características en la Tabla 4.1. Para crear los conjuntos de consultas de entrenamiento, validación y prueba, creamos tres divisiones aleatorias diferentes de 1,500 consultas de entrenamiento, 500 de validación y 1000 de prueba. Las divisiones se realizaron aleatoriamente por consulta, de modo que no hubiera superposición en las consultas de entrenamiento, validación y prueba. 5.2 Métricas de Evaluación Evaluamos los algoritmos de clasificación en una variedad de métricas de recuperación de información aceptadas, a saber, Precisión en K (P(K)), Ganancia Acumulada Descontada Normalizada (NDCG) y Precisión Promedio Media (MAP). Cada métrica se enfoca en un aspecto diferente del rendimiento del sistema, como describimos a continuación. • Precisión en K: Como la métrica más intuitiva, P(K) informa la fracción de documentos clasificados en los primeros K resultados que están etiquetados como relevantes. En nuestro entorno, requerimos que un documento relevante sea etiquetado como Bueno o superior. La posición de los documentos relevantes dentro de los primeros K no es relevante, por lo que esta métrica mide la satisfacción general del usuario con los resultados principales de K. • NDCG en K: NDCG es una medida de recuperación diseñada específicamente para la evaluación de búsqueda en la web [10]. Para una consulta dada q, los resultados clasificados se examinan desde el mejor clasificado hacia abajo, y el NDCG se calcula como: = +−= K j jr qq jMN 1 )( )1log(/)12( Donde Mq es una constante de normalización calculada de manera que un orden perfecto obtendría un NDCG de 1; y cada r(j) es una etiqueta de relevancia entera (0=Malo y 5=Perfecto) del resultado devuelto en la posición j. Ten en cuenta que los documentos sin etiquetar y los documentos malos no contribuyen a la suma, pero reducirán el NDCG para la consulta al empujar hacia abajo los documentos etiquetados relevantes, disminuyendo sus contribuciones. NDCG es muy adecuado para la evaluación de búsquedas en la web, ya que recompensa de manera más intensa los documentos relevantes en los resultados mejor clasificados que aquellos clasificados más bajos. • MAP: La precisión promedio para cada consulta se define como la media de la precisión en los valores de K calculados después de que se recuperó cada documento relevante. El valor MAP final se define como la media de las precisiones promedio de todas las consultas en el conjunto de pruebas. Esta métrica es el resumen de un solo valor más comúnmente utilizado de una ejecución sobre un conjunto de consultas. 5.3 Métodos de clasificación comparados. Recuerde que nuestro objetivo es cuantificar la efectividad del comportamiento implícito para la búsqueda web real. Una dimensión es comparar la utilidad de la retroalimentación implícita con otra información disponible para un motor de búsqueda web. Específicamente, comparamos la efectividad de los comportamientos implícitos de los usuarios con la coincidencia basada en el contenido, las características de calidad de la página estática y combinaciones de todas las características. • BM25F: Como referencia sólida de búsqueda web, utilizamos la puntuación BM25F, que se utilizó en uno de los sistemas con mejor rendimiento en la pista web TREC 2004 [23,27]. BM25F y sus variantes han sido extensamente descritas y evaluadas en la literatura de IR, por lo tanto, sirven como una línea base sólida y reproducible. La variante BM25F que utilizamos en nuestros experimentos calcula puntuaciones de coincidencia separadas para cada campo de un documento de resultado (por ejemplo, texto del cuerpo, título y texto del enlace) e incorpora información de enlaces independiente de la consulta (por ejemplo, PageRank, ClickDistance y profundidad de URL). La función de puntuación y la ajuste específico del campo se describen en detalle en [23]. Tenga en cuenta que BM25F no considera directamente la retroalimentación explícita o implícita para ajuste. • RN: La clasificación producida por un clasificador de redes neuronales (RankNet, descrito en la Sección 3.3) que aprende a clasificar los resultados de búsqueda web al incorporar BM25F y un gran número de características estáticas y dinámicas adicionales que describen cada resultado de búsqueda. Este sistema aprende automáticamente los pesos de todas las características (incluido el puntaje BM25F para un documento) basándose en etiquetas humanas explícitas para un gran conjunto de consultas. Un sistema que incorpora una implementación de RankNet actualmente está siendo utilizado por un importante motor de búsqueda y puede considerarse representativo del estado del arte en la búsqueda web. • BM25F-RerankCT: La clasificación producida al incorporar estadísticas de clics para reordenar los resultados de búsqueda web clasificados por BM25F arriba. El clic es un caso especial particularmente importante de retroalimentación implícita, y se ha demostrado que se correlaciona con la relevancia de los resultados. Este es un caso especial del método de clasificación en la Sección 3.1, con el peso wI establecido en 1000 y la clasificación Id es simplemente el número de clics en el resultado correspondiente a d. En efecto, esta clasificación coloca en la parte superior todos los resultados de búsqueda web devueltos con al menos un clic (y los ordena en orden decreciente por número de clics). La clasificación relativa de los resultados restantes no cambia y se insertan debajo de todos los resultados clicados. Este método sirve como nuestro método de reordenamiento de retroalimentación implícita de referencia. La clasificación producida al reordenar los resultados de BM25F utilizando todas las características del comportamiento del usuario (Sección 4). Este método aprende un modelo de preferencias de usuario correlacionando los valores de las características con etiquetas de relevancia explícitas utilizando el algoritmo de red neuronal RankNet (Sección 4.2). En tiempo de ejecución, para una consulta dada se calcula la puntuación implícita Ir para cada resultado r con características de interacción de usuario disponibles, y se produce el ranking implícito. La clasificación combinada se calcula como se describe en la Sección 3.1. Basándonos en los experimentos realizados sobre el conjunto de desarrollo, fijamos el valor de wI en 3 (el efecto del parámetro wI para este clasificador resultó ser insignificante). • BM25F+All: Clasificación derivada al entrenar el aprendiz RankNet (Sección 3.3) sobre el conjunto de características del puntaje BM25F, así como todas las características de retroalimentación implícita (Sección 3.2). Utilizamos la implementación de 2 capas de RankNet [5] entrenada en las consultas y etiquetas de los conjuntos de entrenamiento y validación. • RN+All: Clasificación derivada al entrenar el algoritmo de clasificación RankNet de 2 capas (Sección 3.3) sobre la unión de todas las características de retroalimentación de contenido, dinámicas e implícitas (es decir, todas las características descritas anteriormente, así como todas las nuevas características de retroalimentación implícita que introdujimos). Los métodos de clasificación anteriores abarcan el rango de la información utilizada para clasificar, desde no utilizar la retroalimentación implícita o explícita en absoluto (es decir, BM25F) hasta un motor de búsqueda web moderno que utiliza cientos de características y está ajustado en base a juicios explícitos (RN). Como mostraremos a continuación, incorporar el comportamiento del usuario en estos sistemas de clasificación mejora drásticamente la relevancia de los documentos devueltos. 6. RESULTADOS EXPERIMENTALES El feedback implícito para la clasificación de búsquedas en la web puede ser explotado de diversas formas. Comparamos métodos alternativos para explotar la retroalimentación implícita, tanto reordenando los resultados principales (es decir, los métodos BM25F-RerankCT y BM25F-RerankAll que reordenan los resultados de BM25F), como integrando directamente las características implícitas en el proceso de clasificación (es decir, los métodos RN+ALL y BM25F+All que aprenden a clasificar los resultados sobre la retroalimentación implícita y otras características). Comparamos nuestros métodos con baselines sólidos (BM25F y RN) sobre las medidas NDCG, Precisión en K y MAP definidas en la Sección 5.2. Los resultados se promediaron en tres divisiones aleatorias del conjunto de datos completo. Cada división contenía 1500 consultas de entrenamiento, 500 de validación y 1000 de prueba, todos los conjuntos de consultas disjuntos. Primero presentamos los resultados de las 1000 consultas de prueba (es decir, incluyendo las consultas para las cuales no hay medidas implícitas, por lo que utilizamos las clasificaciones web originales). Luego profundizamos para examinar los efectos en la reorganización de los intentos de búsqueda con más detalle, analizando dónde el feedback implícito resultó más beneficioso. Primero experimentamos con diferentes métodos de volver a clasificar la salida de los resultados de búsqueda de BM25F. Las figuras 6.1 y 6.2 informan sobre NDCG y Precisión para BM25F, así como para las estrategias de volver a clasificar los resultados con retroalimentación del usuario (Sección 3.1). Incorporar todos los comentarios de los usuarios (ya sea en el marco de reordenamiento o como características directamente al aprendiz) resulta en mejoras significativas (utilizando una prueba t de dos colas con p=0.01) tanto sobre la clasificación original de BM25F como sobre el reordenamiento solo con clics. La mejora es consistente en los 10 mejores resultados y es mayor para el mejor resultado: NDCG en 1 para BM25F+All es de 0.622 en comparación con 0.518 de los resultados originales, y la precisión en 1 también aumenta de 0.5 a 0.63. Basándonos en estos resultados, utilizaremos el clasificador de combinación de características directas (es decir, BM25F+All) para comparaciones posteriores que involucren retroalimentación implícita. Curiosamente, el uso solo de clics, aunque proporciona un beneficio significativo sobre la clasificación original de BM25F, no es tan efectivo como considerar el conjunto completo de características en la Tabla 4.1. Mientras analizamos el comportamiento del usuario (y las características de los componentes más efectivos) en un documento separado [1], vale la pena dar un ejemplo concreto del tipo de ruido inherente en la retroalimentación real de los usuarios en el entorno de búsqueda web. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 Posición del resultado Frecuencia de clics relativa PTR=2 PTR=3 PTR=5 Figura 6.3: Frecuencia relativa de clics para consultas con diferentes Posiciones del Resultado Relevante Superior (PTR). Si los usuarios solo consideraran la relevancia de un resultado para su consulta, harían clic en los resultados más relevantes en la parte superior. Desafortunadamente, como han demostrado Joachims y otros, la presentación también influye de manera bastante dramática en los resultados en los que los usuarios hacen clic. Los usuarios a menudo hacen clic en los resultados por encima del relevante, presumiblemente porque los resúmenes cortos no proporcionan suficiente información para hacer evaluaciones precisas de relevancia y han aprendido que, en promedio, los elementos mejor clasificados son relevantes. La Figura 6.3 muestra las frecuencias relativas de clics para consultas con elementos relevantes conocidos en posiciones distintas a la primera posición; la posición del resultado relevante superior (PTR) varía de 2 a 10 en la figura. Por ejemplo, para consultas con el primer resultado relevante en la posición 5 (PTR=5), hay más clics en los resultados no relevantes en posiciones más altas que en el primer resultado relevante en la posición 5. Como veremos, el aprendizaje sobre un conjunto de características de comportamiento más amplio resulta en una mejora sustancial en la precisión en comparación con solo el clic. Ahora consideramos incorporar el comportamiento del usuario en un conjunto de características mucho más amplio, RN (Sección 5.3) utilizado por un importante motor de búsqueda web. RN incorpora BM25F, características basadas en enlaces y cientos de otras características. La Figura 6.4 informa sobre NDCG en K y la Figura 6.5 informa sobre Precisión en K. Curiosamente, aunque las clasificaciones originales de RN son significativamente más precisas que BM25F solo, la incorporación de características de retroalimentación implícita (BM25F+All) da como resultado una clasificación que supera significativamente a las clasificaciones originales de RN. En otras palabras, la retroalimentación implícita incorpora suficiente información para reemplazar las cientos de otras características disponibles para el aprendiz de RankNet entrenado en el conjunto de características RN. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 1 2 3 4 5 6 7 8 9 10K NDCG RN RN+All BM25 BM25+All Figura 6.4: NDCG en K para BM25F, BM25F+All, RN y RN+All para diferentes K Además, enriquecer las características de RN con el conjunto de retroalimentación implícita muestra una ganancia significativa en todas las medidas, permitiendo que RN+All supere a todos los demás métodos. Esto demuestra la naturaleza complementaria de la retroalimentación implícita con otras características disponibles para un motor de búsqueda web de última generación. 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 Precisión RN RN+Todo BM25 BM25+Todo Figura 6.5: Precisión en K para BM25F, BM25F+Todo, RN y RN+Todo para diferentes valores de K. Resumimos el rendimiento de los diferentes métodos de clasificación en la Tabla 6.1. Informamos el puntaje de Precisión Promedio Media (MAP) para cada sistema. Aunque no es intuitivo de interpretar, el MAP permite la comparación cuantitativa en una sola métrica. Las ganancias marcadas con * son significativas a un nivel de p=0.01 utilizando una prueba t de dos colas. Ganancia MAP P(1) Ganancia BM25F 0.184 - 0.503BM25F-Rerank-CT 0.215 0.031* 0.577 0.073* BM25F-RerankImplicit 0.218 0.003 0.605 0.028* BM25F+Implicit 0.222 0.004 0.620 0.015* RN 0.215 - 0.597RN+All 0.248 0.033* 0.629 0.032* Tabla 6.1: Precisión Promedio (MAP) para todas las estrategias. Hasta ahora hemos informado resultados promediados en todas las consultas del conjunto de pruebas. Desafortunadamente, menos de la mitad tuvo interacciones suficientes para intentar un nuevo ranking. De las 1000 consultas en la prueba, entre el 46% y el 49%, dependiendo de la división entre entrenamiento y prueba, tenían suficiente información de interacción para hacer predicciones (es decir, hubo al menos 1 sesión de búsqueda en la que el usuario hizo clic en al menos 1 URL de resultado). Esto no es sorprendente: la búsqueda en la web tiene una distribución de cola pesada y hay muchas consultas únicas. Ahora consideramos el rendimiento en las consultas para las cuales estaban disponibles las interacciones de los usuarios. La Figura 6.6 informa sobre el NDCG para el subconjunto de las consultas de prueba con las características de retroalimentación implícita. Las ganancias en el primer puesto son dramáticas. El NDCG en 1 de BM25F+All aumenta de 0.6 a 0.75 (un aumento relativo del 31%), logrando un rendimiento comparable al de RN+All operando sobre un conjunto de características mucho más rico. 0.6 0.65 0.7 0.75 0.8 1 3 5 10K NDCG RN RN+All BM25 BM25+All Figura 6.6: NDCG en K para BM25F, BM25F+All, RN y RN+All en consultas de prueba con interacciones de usuario. Del mismo modo, las ganancias en precisión en el top 1 son sustanciales (Figura 6.7) y es probable que sean evidentes para los usuarios de búsqueda web. Cuando se dispone de retroalimentación implícita, el sistema BM25F+All devuelve el documento relevante en la parte superior 1 casi el 70% del tiempo, en comparación con el 53% del tiempo cuando la retroalimentación implícita no es considerada por el sistema BM25F original. 0.45 0.5 0.55 0.6 0.65 0.7 1 3 5 10K Precisión RN RN+All BM25 BM25+All Figura 6.7: Precisión en K NDCG en K para BM25F, BM25F+All, RN y RN+All en consultas de prueba con interacciones de usuario Resumimos los resultados en la medida MAP para las consultas intentadas en la Tabla 6.2. Las mejoras en MAP son tanto sustanciales como significativas, siendo más pronunciadas que las mejoras en el clasificador BM25F. Ahora analizamos los casos en los que la retroalimentación implícita resultó ser más útil. La Figura 6.8 informa las mejoras de MAP sobre la ejecución de BM25F base para cada consulta con MAP inferior a 0.6. Ten en cuenta que la mayoría de la mejora es para consultas con bajo rendimiento (es decir, MAP < 0.1). Curiosamente, la incorporación de información sobre el comportamiento del usuario disminuye la precisión para consultas con un puntaje MAP original alto. Una posible explicación es que estas consultas fáciles tienden a ser de navegación (es decir, tener una única respuesta apropiada altamente clasificada), y las interacciones de los usuarios con resultados de menor rango pueden indicar necesidades de información divergentes que son mejor atendidas por los resultados menos populares (con calificaciones de relevancia global correspondientemente bajas). Para resumir nuestros resultados experimentales, la incorporación de retroalimentación implícita en un entorno real de búsqueda web resultó en mejoras significativas sobre las clasificaciones originales, utilizando tanto BM25F como RN como líneas de base. Nuestro amplio conjunto de características implícitas, como el tiempo en la página y las desviaciones del comportamiento promedio, ofrece ventajas sobre el uso exclusivo del clic como indicador de interés. Además, incorporar características de retroalimentación implícita directamente en la función de clasificación aprendida es más efectivo que utilizar la retroalimentación implícita para volver a clasificar. Las mejoras observadas en grandes conjuntos de pruebas de consultas (1,000 en total, entre 466 y 495 con retroalimentación implícita disponible) son tanto sustanciales como estadísticamente significativas. 7. CONCLUSIONES Y TRABAJOS FUTUROS En este artículo exploramos la utilidad de incorporar retroalimentación implícita ruidosa obtenida en un entorno real de búsqueda web para mejorar la clasificación de búsqueda web. Realizamos una evaluación a gran escala con más de 3,000 consultas y más de 12 millones de interacciones de usuarios con un motor de búsqueda importante, estableciendo la utilidad de incorporar retroalimentación implícita ruidosa para mejorar la relevancia de la búsqueda web. Comparamos dos alternativas para incorporar retroalimentación implícita en el proceso de búsqueda, a saber, reordenar con retroalimentación implícita e incorporar directamente características de retroalimentación implícita en la función de clasificación entrenada. Nuestros experimentos mostraron una mejora significativa sobre los métodos que no consideran la retroalimentación implícita. Las ganancias son particularmente dramáticas para el resultado superior K=1 en la clasificación final, con mejoras de precisión de hasta un 31%, y las ganancias son sustanciales para todos los valores de K. Nuestros experimentos mostraron que el feedback implícito del usuario puede mejorar aún más el rendimiento de la búsqueda web, cuando se incorpora directamente con características populares basadas en contenido y enlaces. Curiosamente, la retroalimentación implícita es especialmente valiosa para consultas con una clasificación original deficiente de resultados (por ejemplo, MAP inferior a 0.1). Una dirección prometedora para trabajos futuros es aplicar la investigación reciente sobre la predicción automática de la dificultad de las consultas, e intentar incorporar únicamente retroalimentación implícita para las consultas difíciles. Como otra dirección de investigación, estamos explorando métodos para extender nuestras predicciones a las consultas previamente no vistas (por ejemplo, el agrupamiento de consultas), lo cual debería mejorar aún más la experiencia de búsqueda en la web de los usuarios. AGRADECIMIENTOS Agradecemos a Chris Burges y Matt Richardson por la implementación de RankNet para nuestros experimentos. También agradecemos a Robert Ragno por sus valiosas sugerencias y muchas discusiones. 8. REFERENCIAS [1] E. Agichtein, E. Brill, S. Dumais y R. Ragno, Aprendizaje de modelos de interacción del usuario para predecir las preferencias de resultados de búsqueda web. En Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2006 [2] J. Allan, Resumen de la Pista HARD en TREC 2003, Recuperación de Alta Precisión de Documentos, 2003 [3] R. Baeza-Yates y B. Ribeiro-Neto, Recuperación de Información Moderna, Addison-Wesley, 1999. [4] S. Brin y L. Page, Anatomía de un Motor de Búsqueda Web Hipertextual a Gran Escala, en Actas de WWW, 1997 [5] C.J.C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. Hullender, Aprendizaje para clasificación utilizando descenso de gradiente, en Actas de la Conferencia Internacional sobre Aprendizaje Automático, 2005 [6] D.M. Chickering, The WinMine Toolkit, Informe Técnico de Microsoft MSR-TR-2002-103, 2002 [7] M. Claypool, D. Brown, P. Lee y M. Waseda. Inferir el interés del usuario. IEEE Internet Computing. 2001 [8] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais y T. White. Evaluando medidas implícitas para mejorar la experiencia de búsqueda. En ACM Transactions on Information Systems, 2005 [9] J. Goecks y J. Shavlick. Aprendiendo los intereses de los usuarios observando de manera discreta su comportamiento normal. En Actas del Taller de IJCAI sobre Aprendizaje Automático para Filtrado de Información. 1999. [10] K Jarvelin y J. Kekalainen. Métodos de evaluación IR para recuperar documentos altamente relevantes. En las Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2000 [11] T. Joachims, Optimización de Motores de Búsqueda Utilizando Datos de Clics. En Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (SIGKDD), 2002 [12] T. Joachims, L. Granka, B. Pang, H. Hembrooke y G. Gay, Interpretación precisa de los datos de clics como retroalimentación implícita, Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2005 [13] T. Joachims, Haciendo práctico el aprendizaje SVM a gran escala. Avances en Métodos de Núcleo, en Aprendizaje de Vectores de Soporte, MIT Press, 1999 [14] D. Kelly y J. Teevan, Retroalimentación implícita para inferir preferencias de usuario: una bibliografía. En el Foro SIGIR, 2003 [15] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon y J. Riedl. GroupLens: Aplicando filtrado colaborativo a las noticias de Usenet. En Comunicaciones de ACM, 1997. [16] M. Morita y Y. Shinoda, Filtrado de información basado en análisis del comportamiento del usuario y recuperación de texto de mejor coincidencia. Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 1994 [17] D. Oard y J. Kim. Retroalimentación implícita para sistemas de recomendación. En Actas del Taller de Sistemas de Recomendación de la AAAI. 1998 [18] D. Oard y J. Kim. Modelando el contenido de la información utilizando el comportamiento observable. En Actas de la 64ª Reunión Anual de la Sociedad Americana de Ciencia de la Información y Tecnología. 2001 [19] N. Pharo, N. y K. Järvelin. El método SST: una herramienta para analizar los procesos de búsqueda de información en la web. En Information Processing & Management, 2004 [20] P. Pirolli, El Uso de la Pista de Información Próxima para Buscar Contenido Distal en la World Wide Web. Trabajando con la tecnología en mente: Brunswikiano. Recursos para Ciencia Cognitiva e Ingeniería, Oxford University Press, 2004 [21] F. Radlinski y T. Joachims, Cadenas de Consulta: Aprendizaje para Clasificar a partir de Retroalimentación Implícita. En Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (SIGKDD), 2005. [22] F. Radlinski y T. Joachims, Evaluando la Robustez del Aprendizaje a partir de Retroalimentación Implícita, en Actas del Taller de ICML sobre Aprendizaje en Búsqueda Web, 2005 [23] S. E. Robertson, H. Zaragoza y M. Taylor, Extensión simple de BM25 a múltiples campos ponderados, en Actas de la Conferencia sobre Información y Gestión del Conocimiento (CIKM), 2004 [24] G. Salton y M. McGill. Introducción a la recuperación de información moderna. McGraw-Hill, 1983 [25] E.M. Voorhees, D. Harman, Resumen de TREC, 2001 [26] G.R. Xue, H.J. Zeng, Z. Chen, Y. Yu, W.Y. -> Zeng, Z. Chen, Y. Yu, W.Y. Ma, W.S. Xi, y W.G. Fan, Optimizing web search using web clickthrough data, en Actas de la Conferencia sobre Información y Gestión del Conocimiento (CIKM), 2004 [27] H. Zaragoza, N. Craswell, M. Taylor, S. Saria y S. Robertson. Microsoft Cambridge en TREC 13: Pistas Web y Duras. En Actas de TREC 2004",
    "original_sentences": [
        "Improving Web Search Ranking by Incorporating User Behavior Information Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com ABSTRACT We show that incorporating user behavior data can significantly improve ordering of top results in real web search setting.",
        "We examine alternatives for incorporating feedback into the ranking process and explore the contributions of user feedback compared to other common web search features.",
        "We report results of a large scale evaluation over 3,000 queries and 12 million user interactions with a popular web search engine.",
        "We show that incorporating implicit feedback can augment other features, improving the accuracy of a competitive web search ranking algorithms by as much as 31% relative to the original performance.",
        "Categories and Subject Descriptors H.3.3 Information Search and Retrieval - Relevance feedback, search process; H.3.5 Online Information Services - Web-based services.",
        "General Terms Algorithms, Measurement, Experimentation 1.",
        "INTRODUCTION Millions of users interact with search engines daily.",
        "They issue queries, follow some of the links in the results, click on ads, spend time on pages, reformulate their queries, and perform other actions.",
        "These interactions can serve as a valuable source of information for tuning and improving web search result ranking and can compliment more costly explicit judgments.",
        "Implicit relevance feedback for ranking and personalization has become an active area of research.",
        "Recent work by Joachims and others exploring implicit feedback in controlled environments have shown the value of incorporating implicit feedback into the ranking process.",
        "Our motivation for this work is to understand how implicit feedback can be used in a large-scale operational environment to improve retrieval.",
        "How does it compare to and compliment evidence from page content, anchor text, or link-based features such as inlinks or PageRank?",
        "While it is intuitive that user interactions with the web search engine should reveal at least some information that could be used for ranking, estimating user preferences in real web search settings is a challenging problem, since real user interactions tend to be more noisy than commonly assumed in the controlled settings of previous studies.",
        "Our paper explores whether implicit feedback can be helpful in realistic environments, where user feedback can be noisy (or adversarial) and a web search engine already uses hundreds of features and is heavily tuned.",
        "To this end, we explore different approaches for ranking web search results using real user behavior obtained as part of normal interactions with the web search engine.",
        "The specific contributions of this paper include: • Analysis of alternatives for incorporating user behavior into web search ranking (Section 3). • An application of a robust implicit feedback model derived from mining millions of user interactions with a major web search engine (Section 4). • A large scale evaluation over real user queries and search results, showing significant improvements derived from incorporating user feedback (Section 6).",
        "We summarize our findings and discuss extensions to the current work in Section 7, which concludes the paper. 2.",
        "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
        "Most common approaches primarily focus on similarity of query and a page, as well as the overall page quality [3,4,24].",
        "However, with increasing popularity of search engines, implicit feedback (i.e., the actions users take when interacting with the search engine) can be used to improve the rankings.",
        "Implicit relevance measures have been studied by several research groups.",
        "An overview of implicit measures is compiled in Kelly and Teevan [14].",
        "This research, while developing valuable insights into implicit relevance measures, was not applied to improve the ranking of web search results in realistic settings.",
        "Closely related to our work, Joachims [11] collected implicit measures in place of explicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
        "Fox et al. [8] explored the relationship between implicit and explicit measures in Web search, and developed Bayesian models to correlate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
        "This work considered a wide range of user behaviors (e.g., dwell time, scroll time, reformulation patterns) in addition to the popular clickthrough behavior.",
        "However, the modeling effort was aimed at predicting explicit relevance judgments from implicit user actions and not specifically at learning ranking functions.",
        "Other studies of user behavior in web search include Pharo and Järvelin [19], but were not directly applied to improve ranking.",
        "More recently, Joachims et al. [12] presented an empirical evaluation of interpreting clickthrough evidence.",
        "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthroughs in a controlled, laboratory setting.",
        "Unfortunately, the extent to which previous research applies to real-world web search is unclear.",
        "At the same time, while recent work (e.g., [26]) on using clickthrough information for improving web search ranking is promising, it captures only one aspect of the user interactions with web search engines.",
        "We build on existing research to develop robust user behavior interpretation techniques for the real web search setting.",
        "Instead of treating each user as a reliable expert, we aggregate information from multiple, unreliable, user search session traces, as we describe in the next two sections. 3.",
        "INCORPORATING IMPLICIT FEEDBACK We consider two complementary approaches to ranking with implicit feedback: (1) treating implicit feedback as independent evidence for ranking results, and (2) integrating implicit feedback features directly into the ranking algorithm.",
        "We describe the two general ranking approaches next.",
        "The specific implicit feedback features are described in Section 4, and the algorithms for interpreting and incorporating implicit feedback are described in Section 5. 3.1 Implicit Feedback as Independent Evidence The general approach is to re-rank the results obtained by a web search engine according to observed clickthrough and other user interactions for the query in previous search sessions.",
        "Each result is assigned a score according to expected relevance/user satisfaction based on previous interactions, resulting in some preference ordering based on user interactions alone.",
        "While there has been significant work on merging multiple rankings, we adapt a simple and robust approach of ignoring the original rankers scores, and instead simply merge the rank orders.",
        "The main reason for ignoring the original scores is that since the feature spaces and learning algorithms are different, the scores are not directly comparable, and re-normalization tends to remove the benefit of incorporating classifier scores.",
        "We experimented with a variety of merging functions on the development set of queries (and using a set of interactions from a different time period from final evaluation sets).",
        "We found that a simple rank merging heuristic combination works well, and is robust to variations in score values from original rankers.",
        "For a given query q, the implicit score ISd is computed for each result d from available user interaction features, resulting in the implicit rank Id for each result.",
        "We compute a merged score SM(d) for d by combining the ranks obtained from implicit feedback, Id with the original rank of d, Od:     ¡     ¢ £ + + + + = otherwise O dforexistsfeedbackimplicitif OI w wOIdS d dd I IddM 1 1 1 1 1 1 ),,,( where the weight wI is a heuristically tuned scaling factor representing the relative importance of the implicit feedback.",
        "The query results are ordered in by decreasing values of SM to produce the final ranking.",
        "One special case of this model arises when setting wI to a very large value, effectively forcing clicked results to be ranked higher than un-clicked results - an intuitive and effective heuristic that we will use as a baseline.",
        "Applying more sophisticated classifier and ranker combination algorithms may result in additional improvements, and is a promising direction for future work.",
        "The approach above assumes that there are no interactions between the underlying features producing the original web search ranking and the implicit feedback features.",
        "We now relax this assumption by integrating implicit feedback features directly into the ranking process. 3.2 Ranking with Implicit Feedback Features Modern web search engines rank results based on a large number of features, including content-based features (i.e., how closely a query matches the text or title or anchor text of the document), and query-independent page quality features (e.g., PageRank of the document or the domain).",
        "In most cases, automatic (or semiautomatic) methods are developed for tuning the specific ranking function that combines these feature values.",
        "Hence, a natural approach is to incorporate implicit feedback features directly as features for the ranking algorithm.",
        "During training or tuning, the ranker can be tuned as before but with additional features.",
        "At runtime, the search engine would fetch the implicit feedback features associated with each query-result URL pair.",
        "This model requires a ranking algorithm to be robust to missing values: more than 50% of queries to web search engines are unique, with no previous implicit feedback available.",
        "We now describe such a ranker that we used to learn over the combined feature sets including implicit feedback. 3.3 Learning to Rank Web Search Results A key aspect of our approach is exploiting recent advances in machine learning, namely trainable ranking algorithms for web search and information retrieval (e.g., [5, 11] and classical results reviewed in [3]).",
        "In our setting, explicit human relevance judgments (labels) are available for a set of web search queries and results.",
        "Hence, an attractive choice to use is a supervised machine learning technique to learn a ranking function that best predicts relevance judgments.",
        "RankNet is one such algorithm.",
        "It is a neural net tuning algorithm that optimizes feature weights to best match explicitly provided pairwise user preferences.",
        "While the specific training algorithms used by RankNet are beyond the scope of this paper, it is described in detail in [5] and includes extensive evaluation and comparison with other ranking methods.",
        "An attractive feature of RankNet is both train- and run-time efficiency - runtime ranking can be quickly computed and can scale to the web, and training can be done over thousands of queries and associated judged results.",
        "We use a 2-layer implementation of RankNet in order to model non-linear relationships between features.",
        "Furthermore, RankNet can learn with many (differentiable) cost functions, and hence can automatically learn a ranking function from human-provided labels, an attractive alternative to heuristic feature combination techniques.",
        "Hence, we will also use RankNet as a generic ranker to explore the contribution of implicit feedback for different ranking alternatives. 4.",
        "IMPLICIT USER FEEDBACK MODEL Our goal is to accurately interpret noisy user feedback obtained as by tracing user interactions with the search engine.",
        "Interpreting implicit feedback in real web search setting is not an easy task.",
        "We characterize this problem in detail in [1], where we motivate and evaluate a wide variety of models of implicit user activities.",
        "The general approach is to represent user actions for each search result as a vector of features, and then train a ranker on these features to discover feature values indicative of relevant (and nonrelevant) search results.",
        "We first briefly summarize our features and model, and the learning approach (Section 4.2) in order to provide sufficient information to replicate our ranking methods and the subsequent experiments. 4.1 Representing User Actions as Features We model observed web search behaviors as a combination of a ``background component (i.e., query- and relevance-independent noise in user behavior, including positional biases with result interactions), and a ``relevance component (i.e., query-specific behavior indicative of relevance of a result to a query).",
        "We design our features to take advantage of aggregated user behavior.",
        "The feature set is comprised of directly observed features (computed directly from observations for each query), as well as queryspecific derived features, computed as the deviation from the overall query-independent distribution of values for the corresponding directly observed feature values.",
        "The features used to represent user interactions with web search results are summarized in Table 4.1.",
        "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
        "We include the traditional implicit feedback features such as clickthrough counts for the results, as well as our novel derived features such as the deviation of the observed clickthrough number for a given query-URL pair from the expected number of clicks on a result in the given position.",
        "We also model the browsing behavior after a result was clicked - e.g., the average page dwell time for a given query-URL pair, as well as its deviation from the expected (average) dwell time.",
        "Furthermore, the feature set was designed to provide essential information about the user experience to make feedback interpretation robust.",
        "For example, web search users can often determine whether a result is relevant by looking at the result title, URL, and summary - in many cases, looking at the original document is not necessary.",
        "To model this aspect of user experience we include features such as overlap in words in title and words in query (TitleOverlap) and the fraction of words shared by the query and the result summary.",
        "Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickProbability Probability of a click for this query and URL ClickDeviation Deviation from expected click probability IsNextClicked 1 if clicked on next position, 0 otherwise IsPreviousClicked 1 if clicked on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, no parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from average dwell time on page CumulativeDeviation Deviation from average cumulative dwell time DomainDeviation Deviation from average dwell time on domain Query-text features TitleOverlap Words shared between query and title SummaryOverlap Words shared between query and snippet QueryURLOverlap Words shared between query and URL QueryDomainOverlap Words shared between query and URL domain QueryLength Number of tokens in query QueryNextOverlap Fraction of words shared with next query Table 4.1: Some features used to represent post-search navigation history for a given query and search result URL.",
        "Having described our feature set, we briefly review our general method for deriving a user behavior model. 4.2 Deriving a User Feedback Model To learn to interpret the observed user behavior, we correlate user actions (i.e., the features in Table 4.1 representing the actions) with the explicit user judgments for a set of training queries.",
        "We find all the instances in our session logs where these queries were submitted to the search engine, and aggregate the user behavior features for all search sessions involving these queries.",
        "Each observed query-URL pair is represented by the features in Table 4.1, with values averaged over all search sessions, and assigned one of six possible relevance labels, ranging from Perfect to Bad, as assigned by explicit relevance judgments.",
        "These labeled feature vectors are used as input to the RankNet training algorithm (Section 3.3) which produces a trained user behavior model.",
        "This approach is particularly attractive as it does not require heuristics beyond feature engineering.",
        "The resulting user behavior model is used to help rank web search resultseither directly or in combination with other features, as described below. 5.",
        "EXPERIMENTAL SETUP The ultimate goal of incorporating implicit feedback into ranking is to improve the relevance of the returned web search results.",
        "Hence, we compare the ranking methods over a large set of judged queries with explicit relevance labels provided by human judges.",
        "In order for the evaluation to be realistic we obtained a random sample of queries from web search logs of a major search engine, with associated results and traces for user actions.",
        "We describe this dataset in detail next.",
        "Our metrics are described in Section 5.2 that we use to evaluate the ranking alternatives, listed in Section 5.3 in the experiments of Section 6. 5.1 Datasets We compared our ranking methods over a random sample of 3,000 queries from the search engine query logs.",
        "The queries were drawn from the logs uniformly at random by token without replacement, resulting in a query sample representative of the overall query distribution.",
        "On average, 30 results were explicitly labeled by human judges using a six point scale ranging from Perfect down to Bad.",
        "Overall, there were over 83,000 results with explicit relevance judgments.",
        "In order to compute various statistics, documents with label Good or better will be considered relevant, and with lower labels to be non-relevant.",
        "Note that the experiments were performed over the results already highly ranked by a web search engine, which corresponds to a typical user experience which is limited to the small number of the highly ranked results for a typical web search query.",
        "The user interactions were collected over a period of 8 weeks using voluntary opt-in information.",
        "In total, over 1.2 million unique queries were instrumented, resulting in over 12 million individual interactions with the search engine.",
        "The data consisted of user interactions with the web search engine (e.g., clicking on a result link, going back to search results, etc.) performed after a query was submitted.",
        "These actions were aggregated across users and search sessions and converted to features in Table 4.1.",
        "To create the training, validation, and test query sets, we created three different random splits of 1,500 training, 500 validation, and 1000 test queries.",
        "The splits were done randomly by query, so that there was no overlap in training, validation, and test queries. 5.2 Evaluation Metrics We evaluate the ranking algorithms over a range of accepted information retrieval metrics, namely Precision at K (P(K)), Normalized Discounted Cumulative Gain (NDCG), and Mean Average Precision (MAP).",
        "Each metric focuses on a deferent aspect of system performance, as we describe below. • Precision at K: As the most intuitive metric, P(K) reports the fraction of documents ranked in the top K results that are labeled as relevant.",
        "In our setting, we require a relevant document to be labeled Good or higher.",
        "The position of relevant documents within the top K is irrelevant, and hence this metric measure overall user satisfaction with the top K results. • NDCG at K: NDCG is a retrieval measure devised specifically for web search evaluation [10].",
        "For a given query q, the ranked results are examined from the top ranked down, and the NDCG computed as:   = +−= K j jr qq jMN 1 )( )1log(/)12( Where Mq is a normalization constant calculated so that a perfect ordering would obtain NDCG of 1; and each r(j) is an integer relevance label (0=Bad and 5=Perfect) of result returned at position j.",
        "Note that unlabeled and Bad documents do not contribute to the sum, but will reduce NDCG for the query pushing down the relevant labeled documents, reducing their contributions.",
        "NDCG is well suited to web search evaluation, as it rewards relevant documents in the top ranked results more heavily than those ranked lower. • MAP: Average precision for each query is defined as the mean of the precision at K values computed after each relevant document was retrieved.",
        "The final MAP value is defined as the mean of average precisions of all queries in the test set.",
        "This metric is the most commonly used single-value summary of a run over a set of queries. 5.3 Ranking Methods Compared Recall that our goal is to quantify the effectiveness of implicit behavior for real web search.",
        "One dimension is to compare the utility of implicit feedback with other information available to a web search engine.",
        "Specifically, we compare effectiveness of implicit user behaviors with content-based matching, static page quality features, and combinations of all features. • BM25F: As a strong web search baseline we used the BM25F scoring, which was used in one of the best performing systems in the TREC 2004 Web track [23,27].",
        "BM25F and its variants have been extensively described and evaluated in IR literature, and hence serve as a strong, reproducible baseline.",
        "The BM25F variant we used for our experiments computes separate match scores for each field for a result document (e.g., body text, title, and anchor text), and incorporates query-independent linkbased information (e.g., PageRank, ClickDistance, and URL depth).",
        "The scoring function and field-specific tuning is described in detail in [23].",
        "Note that BM25F does not directly consider explicit or implicit feedback for tuning. • RN: The ranking produced by a neural net ranker (RankNet, described in Section 3.3) that learns to rank web search results by incorporating BM25F and a large number of additional static and dynamic features describing each search result.",
        "This system automatically learns weights for all features (including the BM25F score for a document) based on explicit human labels for a large set of queries.",
        "A system incorporating an implementation of RankNet is currently in use by a major search engine and can be considered representative of the state of the art in web search. • BM25F-RerankCT: The ranking produced by incorporating clickthrough statistics to reorder web search results ranked by BM25F above.",
        "Clickthrough is a particularly important special case of implicit feedback, and has been shown to correlate with result relevance.",
        "This is a special case of the ranking method in Section 3.1, with the weight wI set to 1000 and the ranking Id is simply the number of clicks on the result corresponding to d. In effect, this ranking brings to the top all returned web search results with at least one click (and orders them in decreasing order by number of clicks).",
        "The relative ranking of the remainder of results is unchanged and they are inserted below all clicked results.",
        "This method serves as our baseline implicit feedback reranking method.",
        "BM25F-RerankAll The ranking produced by reordering the BM25F results using all user behavior features (Section 4).",
        "This method learns a model of user preferences by correlating feature values with explicit relevance labels using the RankNet neural net algorithm (Section 4.2).",
        "At runtime, for a given query the implicit score Ir is computed for each result r with available user interaction features, and the implicit ranking is produced.",
        "The merged ranking is computed as described in Section 3.1.",
        "Based on the experiments over the development set we fix the value of wI to 3 (the effect of the wI parameter for this ranker turned out to be negligible). • BM25F+All: Ranking derived by training the RankNet (Section 3.3) learner over the features set of the BM25F score as well as all implicit feedback features (Section 3.2).",
        "We used the 2-layer implementation of RankNet [5] trained on the queries and labels in the training and validation sets. • RN+All: Ranking derived by training the 2-layer RankNet ranking algorithm (Section 3.3) over the union of all content, dynamic, and implicit feedback features (i.e., all of the features described above as well as all of the new implicit feedback features we introduced).",
        "The ranking methods above span the range of the information used for ranking, from not using the implicit or explicit feedback at all (i.e., BM25F) to a modern web search engine using hundreds of features and tuned on explicit judgments (RN).",
        "As we will show next, incorporating user behavior into these ranking systems dramatically improves the relevance of the returned documents. 6.",
        "EXPERIMENTAL RESULTS Implicit feedback for web search ranking can be exploited in a number of ways.",
        "We compare alternative methods of exploiting implicit feedback, both by re-ranking the top results (i.e., the BM25F-RerankCT and BM25F-RerankAll methods that reorder BM25F results), as well as by integrating the implicit features directly into the ranking process (i.e., the RN+ALL and BM25F+All methods which learn to rank results over the implicit feedback and other features).",
        "We compare our methods over strong baselines (BM25F and RN) over the NDCG, Precision at K, and MAP measures defined in Section 5.2.",
        "The results were averaged over three random splits of the overall dataset.",
        "Each split contained 1500 training, 500 validation, and 1000 test queries, all query sets disjoint.",
        "We first present the results over all 1000 test queries (i.e., including queries for which there are no implicit measures so we use the original web rankings).",
        "We then drill down to examine the effects on reranking for the attempted queries in more detail, analyzing where implicit feedback proved most beneficial.",
        "We first experimented with different methods of re-ranking the output of the BM25F search results.",
        "Figures 6.1 and 6.2 report NDCG and Precision for BM25F, as well as for the strategies reranking results with user feedback (Section 3.1).",
        "Incorporating all user feedback (either in reranking framework or as features to the learner directly) results in significant improvements (using two-tailed t-test with p=0.01) over both the original BM25F ranking as well as over reranking with clickthrough alone.",
        "The improvement is consistent across the top 10 results and largest for the top result: NDCG at 1 for BM25F+All is 0.622 compared to 0.518 of the original results, and precision at 1 similarly increases from 0.5 to 0.63.",
        "Based on these results we will use the direct feature combination (i.e., BM25F+All) ranker for subsequent comparisons involving implicit feedback. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 1 2 3 4 5 6 7 8 9 10K NDCG BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.1: NDCG at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K 0.35 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.2: Precision at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K Interestingly, using clickthrough alone, while giving significant benefit over the original BM25F ranking, is not as effective as considering the full set of features in Table 4.1.",
        "While we analyze user behavior (and most effective component features) in a separate paper [1], it is worthwhile to give a concrete example of the kind of noise inherent in real user feedback in web search setting. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 Result position Relativeclickfrequency PTR=2 PTR=3 PTR=5 Figure 6.3: Relative clickthrough frequency for queries with varying Position of Top Relevant result (PTR).",
        "If users considered only the relevance of a result to their query, they would click on the topmost relevant results.",
        "Unfortunately, as Joachims and others have shown, presentation also influences which results users click on quite dramatically.",
        "Users often click on results above the relevant one presumably because the short summaries do not provide enough information to make accurate relevance assessments and they have learned that on average topranked items are relevant.",
        "Figure 6.3 shows relative clickthrough frequencies for queries with known relevant items at positions other than the first position; the position of the top relevant result (PTR) ranges from 2-10 in the figure.",
        "For example, for queries with first relevant result at position 5 (PTR=5), there are more clicks on the non-relevant results in higher ranked positions than on the first relevant result at position 5.",
        "As we will see, learning over a richer behavior feature set, results in substantial accuracy improvement over clickthrough alone.",
        "We now consider incorporating user behavior into a much richer feature set, RN (Section 5.3) used by a major web search engine.",
        "RN incorporates BM25F, link-based features, and hundreds of other features.",
        "Figure 6.4 reports NDCG at K and Figure 6.5 reports Precision at K. Interestingly, while the original RN rankings are significantly more accurate than BM25F alone, incorporating implicit feedback features (BM25F+All) results in ranking that significantly outperforms the original RN rankings.",
        "In other words, implicit feedback incorporates sufficient information to replace the hundreds of other features available to the RankNet learner trained on the RN feature set. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 1 2 3 4 5 6 7 8 9 10K NDCG RN RN+All BM25 BM25+All Figure 6.4: NDCG at K for BM25F, BM25F+All, RN, and RN+All for varying K Furthermore, enriching the RN features with implicit feedback set exhibits significant gain on all measures, allowing RN+All to outperform all other methods.",
        "This demonstrates the complementary nature of implicit feedback with other features available to a state of the art web search engine. 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision RN RN+All BM25 BM25+All Figure 6.5: Precision at K for BM25F, BM25F+All, RN, and RN+All for varying K We summarize the performance of the different ranking methods in Table 6.1.",
        "We report the Mean Average Precision (MAP) score for each system.",
        "While not intuitive to interpret, MAP allows quantitative comparison on a single metric.",
        "The gains marked with * are significant at p=0.01 level using two tailed t-test.",
        "MAP Gain P(1) Gain BM25F 0.184 - 0.503BM25F-Rerank-CT 0.215 0.031* 0.577 0.073* BM25F-RerankImplicit 0.218 0.003 0.605 0.028* BM25F+Implicit 0.222 0.004 0.620 0.015* RN 0.215 - 0.597RN+All 0.248 0.033* 0.629 0.032* Table 6.1: Mean Average Precision (MAP) for all strategies.",
        "So far we reported results averaged across all queries in the test set.",
        "Unfortunately, less than half had sufficient interactions to attempt reranking.",
        "Out of the 1000 queries in test, between 46% and 49%, depending on the train-test split, had sufficient interaction information to make predictions (i.e., there was at least 1 search session in which at least 1 result URL was clicked on by the user).",
        "This is not surprising: web search is heavy-tailed, and there are many unique queries.",
        "We now consider the performance on the queries for which user interactions were available.",
        "Figure 6.6 reports NDCG for the subset of the test queries with the implicit feedback features.",
        "The gains at top 1 are dramatic.",
        "The NDCG at 1 of BM25F+All increases from 0.6 to 0.75 (a 31% relative gain), achieving performance comparable to RN+All operating over a much richer feature set. 0.6 0.65 0.7 0.75 0.8 1 3 5 10K NDCG RN RN+All BM25 BM25+All Figure 6.6: NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions Similarly, gains on precision at top 1 are substantial (Figure 6.7), and are likely to be apparent to web search users.",
        "When implicit feedback is available, the BM25F+All system returns relevant document at top 1 almost 70% of the time, compared 53% of the time when implicit feedback is not considered by the original BM25F system. 0.45 0.5 0.55 0.6 0.65 0.7 1 3 5 10K Precision RN RN+All BM25 BM25+All Figure 6.7: Precision at K NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions We summarize the results on the MAP measure for attempted queries in Table 6.2.",
        "MAP improvements are both substantial and significant, with improvements over the BM25F ranker most pronounced.",
        "Method MAP Gain P(1) Gain RN 0.269 0.632 RN+All 0.321 0.051 (19%) 0.693 0.061(10%) BM25F 0.236 0.525 BM25F+All 0.292 0.056 (24%) 0.687 0.162 (31%) Table 6.2: Mean Average Precision (MAP) on attempted queries for best performing methods We now analyze the cases where implicit feedback was shown most helpful.",
        "Figure 6.8 reports the MAP improvements over the baseline BM25F run for each query with MAP under 0.6.",
        "Note that most of the improvement is for poorly performing queries (i.e., MAP < 0.1).",
        "Interestingly, incorporating user behavior information degrades accuracy for queries with high original MAP score.",
        "One possible explanation is that these easy queries tend to be navigational (i.e., having a single, highly-ranked most appropriate answer), and user interactions with lower-ranked results may indicate divergent information needs that are better served by the less popular results (with correspondingly poor overall relevance ratings). 0 50 100 150 200 250 300 350 0.1 0.2 0.3 0.4 0.5 0.6 -0.4 -0.35 -0.3 -0.25 -0.2 -0.15 -0.1 -0.05 0 0.05 0.1 0.15 0.2 Frequency Average Gain Figure 6.8: Gain of BM25F+All over original BM25F ranking To summarize our experimental results, incorporating implicit feedback in real web search setting resulted in significant improvements over the original rankings, using both BM25F and RN baselines.",
        "Our rich set of implicit features, such as time on page and deviations from the average behavior, provides advantages over using clickthrough alone as an indicator of interest.",
        "Furthermore, incorporating implicit feedback features directly into the learned ranking function is more effective than using implicit feedback for reranking.",
        "The improvements observed over large test sets of queries (1,000 total, between 466 and 495 with implicit feedback available) are both substantial and statistically significant. 7.",
        "CONCLUSIONS AND FUTURE WORK In this paper we explored the utility of incorporating noisy implicit feedback obtained in a real web search setting to improve web search ranking.",
        "We performed a large-scale evaluation over 3,000 queries and more than 12 million user interactions with a major search engine, establishing the utility of incorporating noisy implicit feedback to improve web search relevance.",
        "We compared two alternatives of incorporating implicit feedback into the search process, namely reranking with implicit feedback and incorporating implicit feedback features directly into the trained ranking function.",
        "Our experiments showed significant improvement over methods that do not consider implicit feedback.",
        "The gains are particularly dramatic for the top K=1 result in the final ranking, with precision improvements as high as 31%, and the gains are substantial for all values of K. Our experiments showed that implicit user feedback can further improve web search performance, when incorporated directly with popular content- and link-based features.",
        "Interestingly, implicit feedback is particularly valuable for queries with poor original ranking of results (e.g., MAP lower than 0.1).",
        "One promising direction for future work is to apply recent research on automatically predicting query difficulty, and only attempt to incorporate implicit feedback for the difficult queries.",
        "As another research direction we are exploring methods for extending our predictions to the previously unseen queries (e.g., query clustering), which should further improve the web search experience of users.",
        "ACKNOWLEDGMENTS We thank Chris Burges and Matt Richardson for an implementation of RankNet for our experiments.",
        "We also thank Robert Ragno for his valuable suggestions and many discussions. 8.",
        "REFERENCES [1] E. Agichtein, E. Brill, S. Dumais, and R.Ragno, Learning User Interaction Models for Predicting Web Search Result Preferences.",
        "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan, HARD Track Overview in TREC 2003, High Accuracy Retrieval from Documents, 2003 [3] R. Baeza-Yates and B. Ribeiro-Neto, Modern Information Retrieval, Addison-Wesley, 1999. [4] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine, in Proceedings of WWW, 1997 [5] C.J.C.",
        "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning, 2005 [6] D.M.",
        "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [7] M. Claypool, D. Brown, P. Lee and M. Waseda.",
        "Inferring user interest.",
        "IEEE Internet Computing. 2001 [8] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
        "Evaluating implicit measures to improve the search experience.",
        "In ACM Transactions on Information Systems, 2005 [9] J. Goecks and J. Shavlick.",
        "Learning users interests by unobtrusively observing their normal behavior.",
        "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [10] K Jarvelin and J. Kekalainen.",
        "IR evaluation methods for retrieving highly relevant documents.",
        "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2000 [11] T. Joachims, Optimizing Search Engines Using Clickthrough Data.",
        "In Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [12] T. Joachims, L. Granka, B. Pang, H. Hembrooke, and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [13] T. Joachims, Making Large-Scale SVM Learning Practical.",
        "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [14] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
        "In SIGIR Forum, 2003 [15] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon, and J. Riedl.",
        "GroupLens: Applying collaborative filtering to usenet news.",
        "In Communications of ACM, 1997. [16] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
        "Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [17] D. Oard and J. Kim.",
        "Implicit feedback for recommender systems.",
        "In Proceedings of the AAAI Workshop on Recommender Systems. 1998 [18] D. Oard and J. Kim.",
        "Modeling information content using observable behavior.",
        "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [19] N. Pharo, N. and K. Järvelin.",
        "The SST method: a tool for analyzing web information search processes.",
        "In Information Processing & Management, 2004 [20] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
        "In Working with Technology in Mind: Brunswikian.",
        "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [21] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback.",
        "In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (SIGKDD), 2005. [22] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in Proceedings of the ICML Workshop on Learning in Web Search, 2005 [23] S. E. Robertson, H. Zaragoza, and M. Taylor, Simple BM25 extension to multiple weighted fields, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [24] G. Salton & M. McGill.",
        "Introduction to modern information retrieval.",
        "McGraw-Hill, 1983 [25] E.M. Voorhees, D. Harman, Overview of TREC, 2001 [26] G.R.",
        "Xue, H.J.",
        "Zeng, Z. Chen, Y. Yu, W.Y.",
        "Ma, W.S.",
        "Xi, and W.G.",
        "Fan, Optimizing web search using web clickthrough data, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [27] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
        "Microsoft Cambridge at TREC 13: Web and Hard Tracks.",
        "In Proceedings of TREC 2004"
    ],
    "translated_text_sentences": [
        "Mejorando la clasificación de búsqueda web al incorporar información sobre el comportamiento del usuario. Eugene Agichtein, Microsoft Research eugeneag@microsoft.com Eric Brill, Microsoft Research brill@microsoft.com Susan Dumais, Microsoft Research sdumais@microsoft.com RESUMEN Mostramos que la incorporación de datos sobre el comportamiento del usuario puede mejorar significativamente el orden de los principales resultados en una configuración real de búsqueda web.",
        "Examinamos alternativas para incorporar retroalimentación en el proceso de clasificación y exploramos las contribuciones de la retroalimentación de usuarios en comparación con otras características comunes de búsqueda en la web.",
        "Informamos los resultados de una evaluación a gran escala con más de 3,000 consultas y 12 millones de interacciones de usuarios con un motor de búsqueda web popular.",
        "Mostramos que la incorporación de retroalimentación implícita puede aumentar otras características, mejorando la precisión de los algoritmos de clasificación de búsqueda web competitivos hasta en un 31% en comparación con el rendimiento original.",
        "Categorías y Descriptores de Asignaturas H.3.3 Búsqueda y Recuperación de Información - Retroalimentación de relevancia, proceso de búsqueda; H.3.5 Servicios de Información en Línea - Servicios basados en la web.",
        "Términos generales Algoritmos, Medición, Experimentación 1.",
        "INTRODUCCIÓN Millones de usuarios interactúan con los motores de búsqueda a diario.",
        "Ellos emiten consultas, siguen algunos de los enlaces en los resultados, hacen clic en anuncios, pasan tiempo en páginas, reformulan sus consultas y realizan otras acciones.",
        "Estas interacciones pueden servir como una valiosa fuente de información para ajustar y mejorar la clasificación de los resultados de búsqueda en la web y pueden complementar juicios explícitos más costosos.",
        "La retroalimentación implícita de relevancia para la clasificación y personalización se ha convertido en un área activa de investigación.",
        "El trabajo reciente de Joachims y otros explorando el feedback implícito en entornos controlados ha demostrado el valor de incorporar el feedback implícito en el proceso de clasificación.",
        "Nuestra motivación para este trabajo es comprender cómo se puede utilizar la retroalimentación implícita en un entorno operativo a gran escala para mejorar la recuperación.",
        "¿Cómo se compara y complementa con la evidencia del contenido de la página, el texto del ancla o las características basadas en enlaces como inlinks o PageRank?",
        "Si bien es intuitivo que las interacciones de los usuarios con el motor de búsqueda web deberían revelar al menos alguna información que podría ser utilizada para la clasificación, estimar las preferencias de los usuarios en entornos reales de búsqueda web es un problema desafiante, ya que las interacciones reales tienden a ser más ruidosas de lo que comúnmente se asume en los entornos controlados de estudios previos.",
        "Nuestro artículo explora si la retroalimentación implícita puede ser útil en entornos realistas, donde la retroalimentación del usuario puede ser ruidosa (o adversa) y un motor de búsqueda web ya utiliza cientos de características y está altamente ajustado.",
        "Con este fin, exploramos diferentes enfoques para clasificar los resultados de búsqueda web utilizando el comportamiento real de los usuarios obtenido como parte de las interacciones normales con el motor de búsqueda web.",
        "Las contribuciones específicas de este artículo incluyen: • Análisis de alternativas para incorporar el comportamiento del usuario en la clasificación de búsqueda web (Sección 3). • Aplicación de un modelo robusto de retroalimentación implícita derivado de la minería de millones de interacciones de usuarios con un importante motor de búsqueda web (Sección 4). • Una evaluación a gran escala sobre consultas reales de usuarios y resultados de búsqueda, mostrando mejoras significativas derivadas de la incorporación de la retroalimentación del usuario (Sección 6).",
        "Resumimos nuestros hallazgos y discutimos extensiones al trabajo actual en la Sección 7, que concluye el artículo.",
        "ANTECEDENTES Y TRABAJO RELACIONADO Clasificar los resultados de búsqueda es un problema fundamental en la recuperación de información.",
        "La mayoría de los enfoques comunes se centran principalmente en la similitud entre la consulta y una página, así como en la calidad general de la página [3,4,24].",
        "Sin embargo, con la creciente popularidad de los motores de búsqueda, la retroalimentación implícita (es decir, las acciones que los usuarios realizan al interactuar con el motor de búsqueda) se puede utilizar para mejorar las clasificaciones.",
        "Las medidas de relevancia implícitas han sido estudiadas por varios grupos de investigación.",
        "Un resumen de las medidas implícitas se recopila en Kelly y Teevan [14].",
        "Esta investigación, si bien desarrolló valiosas ideas sobre medidas implícitas de relevancia, no se aplicó para mejorar la clasificación de los resultados de búsqueda en la web en entornos realistas.",
        "Estrechamente relacionado con nuestro trabajo, Joachims [11] recopiló medidas implícitas en lugar de medidas explícitas, introduciendo una técnica basada completamente en datos de clics para aprender funciones de clasificación.",
        "Fox et al. [8] exploraron la relación entre medidas implícitas y explícitas en la búsqueda web, y desarrollaron modelos bayesianos para correlacionar medidas implícitas y juicios explícitos de relevancia tanto para consultas individuales como para sesiones de búsqueda.",
        "Este trabajo consideró una amplia gama de comportamientos de usuario (por ejemplo, tiempo de permanencia, tiempo de desplazamiento, patrones de reformulación) además del popular comportamiento de clics.",
        "Sin embargo, el esfuerzo de modelado se centró en predecir juicios de relevancia explícitos a partir de acciones implícitas de los usuarios y no específicamente en aprender funciones de clasificación.",
        "Otros estudios sobre el comportamiento de los usuarios en la búsqueda web incluyen a Pharo y Järvelin [19], pero no se aplicaron directamente para mejorar la clasificación.",
        "Más recientemente, Joachims et al. [12] presentaron una evaluación empírica de la interpretación de la evidencia de clics.",
        "Al realizar estudios de seguimiento ocular y correlacionar las predicciones de sus estrategias con las calificaciones explícitas, los autores demostraron que es posible interpretar con precisión los clics en un entorno controlado de laboratorio.",
        "Desafortunadamente, no está claro en qué medida la investigación previa se aplica a la búsqueda web del mundo real.",
        "Al mismo tiempo, si bien el trabajo reciente (por ejemplo, [26]) sobre el uso de la información de clics para mejorar la clasificación de búsqueda en la web es prometedor, solo abarca un aspecto de las interacciones de los usuarios con los motores de búsqueda en la web.",
        "Nos basamos en investigaciones existentes para desarrollar técnicas robustas de interpretación del comportamiento del usuario para el entorno real de búsqueda en la web.",
        "En lugar de tratar a cada usuario como un experto confiable, agregamos información de múltiples trazas de sesiones de búsqueda de usuarios no confiables, como describimos en las siguientes dos secciones. 3.",
        "INTEGRANDO LA RETROALIMENTACIÓN IMPLÍCITA Consideramos dos enfoques complementarios para la clasificación con retroalimentación implícita: (1) tratar la retroalimentación implícita como evidencia independiente para clasificar resultados, y (2) integrar características de retroalimentación implícita directamente en el algoritmo de clasificación.",
        "Describimos a continuación los dos enfoques generales de clasificación.",
        "Las características específicas del feedback implícito se describen en la Sección 4, y los algoritmos para interpretar e incorporar el feedback implícito se describen en la Sección 5. 3.1 Feedback Implícito como Evidencia Independiente. El enfoque general es reordenar los resultados obtenidos por un motor de búsqueda web según los clics observados y otras interacciones de usuario para la consulta en sesiones de búsqueda anteriores.",
        "Cada resultado se le asigna una puntuación según la relevancia esperada/ satisfacción del usuario basada en interacciones previas, lo que resulta en un cierto orden de preferencia basado únicamente en las interacciones del usuario.",
        "Si bien ha habido un trabajo significativo en la fusión de múltiples clasificaciones, adaptamos un enfoque simple y robusto de ignorar las puntuaciones de los clasificadores originales, y en su lugar simplemente fusionamos los órdenes de clasificación.",
        "La razón principal para ignorar las puntuaciones originales es que, dado que los espacios de características y los algoritmos de aprendizaje son diferentes, las puntuaciones no son directamente comparables, y la re-normalización tiende a eliminar el beneficio de incorporar las puntuaciones del clasificador.",
        "Experimentamos con una variedad de funciones de fusión en el conjunto de desarrollo de consultas (y utilizando un conjunto de interacciones de un período de tiempo diferente de los conjuntos de evaluación final).",
        "Encontramos que una combinación heurística simple de fusión de rangos funciona bien y es robusta a variaciones en los valores de puntuación de los clasificadores originales.",
        "Para una consulta dada q, se calcula la puntuación implícita ISd para cada resultado d a partir de las características de interacción del usuario disponibles, lo que resulta en la clasificación implícita Id para cada resultado.",
        "Calculamos un puntaje combinado SM(d) para d al combinar los rangos obtenidos de la retroalimentación implícita, Id, con el rango original de d, Od: SM(d) = Od + wI * Id, si existe retroalimentación implícita; de lo contrario, SM(d) = Od. Donde el peso wI es un factor de escala ajustado heurísticamente que representa la importancia relativa de la retroalimentación implícita.",
        "Los resultados de la consulta se ordenan en valores decrecientes de SM para producir la clasificación final.",
        "Un caso especial de este modelo surge al establecer wI en un valor muy grande, lo que efectivamente obliga a que los resultados clicados se clasifiquen por encima de los resultados no clicados, una heurística intuitiva y efectiva que utilizaremos como referencia.",
        "Aplicar algoritmos de combinación de clasificadores y ordenadores más sofisticados puede resultar en mejoras adicionales, y es una dirección prometedora para trabajos futuros.",
        "El enfoque anterior asume que no hay interacciones entre las características subyacentes que producen la clasificación original de la búsqueda web y las características de retroalimentación implícitas.",
        "Ahora relajamos esta suposición al integrar características de retroalimentación implícita directamente en el proceso de clasificación. 3.2 Clasificación con Características de Retroalimentación Implícita Los motores de búsqueda web modernos clasifican los resultados en función de un gran número de características, incluidas las características basadas en el contenido (es decir, qué tan cerca coincide una consulta con el texto, el título o el texto de anclaje del documento) y las características de calidad de página independientes de la consulta (por ejemplo, PageRank del documento o del dominio).",
        "En la mayoría de los casos, se desarrollan métodos automáticos (o semiautomáticos) para ajustar la función de clasificación específica que combina estos valores de características.",
        "Por lo tanto, un enfoque natural es incorporar características de retroalimentación implícita directamente como características para el algoritmo de clasificación.",
        "Durante el entrenamiento o ajuste, el clasificador puede ser ajustado como antes pero con características adicionales.",
        "En tiempo de ejecución, el motor de búsqueda recuperaría las características de retroalimentación implícita asociadas con cada par de URL de consulta y resultado.",
        "Este modelo requiere que un algoritmo de clasificación sea robusto ante valores faltantes: más del 50% de las consultas a los motores de búsqueda web son únicas, sin disponer de retroalimentación implícita previa.",
        "Ahora describimos un clasificador que utilizamos para aprender sobre los conjuntos de características combinadas, incluyendo la retroalimentación implícita. 3.3 Aprendizaje para Clasificar los Resultados de Búsqueda en la Web Un aspecto clave de nuestro enfoque es aprovechar los avances recientes en el aprendizaje automático, en particular los algoritmos de clasificación entrenables para la búsqueda en la web y la recuperación de información (por ejemplo, [5, 11] y resultados clásicos revisados en [3]).",
        "En nuestro entorno, contamos con juicios explícitos de relevancia humana (etiquetas) para un conjunto de consultas de búsqueda en la web y resultados.",
        "Por lo tanto, una opción atractiva es utilizar una técnica de aprendizaje automático supervisado para aprender una función de clasificación que prediga mejor las evaluaciones de relevancia.",
        "RankNet es uno de esos algoritmos.",
        "Es un algoritmo de ajuste de red neuronal que optimiza los pesos de las características para que coincidan mejor con las preferencias de los usuarios proporcionadas explícitamente en pares.",
        "Si bien los algoritmos de entrenamiento específicos utilizados por RankNet están más allá del alcance de este documento, se describen en detalle en [5] e incluyen una evaluación exhaustiva y comparación con otros métodos de clasificación.",
        "Una característica atractiva de RankNet es su eficiencia tanto en el tiempo de entrenamiento como en el de ejecución: el ranking en tiempo de ejecución se puede calcular rápidamente y puede escalarse a la web, y el entrenamiento se puede realizar sobre miles de consultas y resultados juzgados asociados.",
        "Utilizamos una implementación de RankNet de 2 capas para modelar relaciones no lineales entre características.",
        "Además, RankNet puede aprender con muchas funciones de costo (diferenciables), por lo que puede aprender automáticamente una función de clasificación a partir de etiquetas proporcionadas por humanos, una alternativa atractiva a las técnicas de combinación de características heurísticas.",
        "Por lo tanto, también utilizaremos RankNet como un clasificador genérico para explorar la contribución de la retroalimentación implícita en diferentes alternativas de clasificación. 4.",
        "MODELO IMPLÍCITO DE RETROALIMENTACIÓN DEL USUARIO Nuestro objetivo es interpretar con precisión la retroalimentación ruidosa del usuario obtenida al rastrear las interacciones del usuario con el motor de búsqueda.",
        "Interpretar la retroalimentación implícita en un entorno real de búsqueda en la web no es una tarea fácil.",
        "Caracterizamos este problema en detalle en [1], donde motivamos y evaluamos una amplia variedad de modelos de actividades implícitas de usuario.",
        "El enfoque general es representar las acciones del usuario para cada resultado de búsqueda como un vector de características, y luego entrenar un clasificador en base a estas características para descubrir los valores de las características que indican resultados de búsqueda relevantes (y no relevantes).",
        "Primero resumimos brevemente nuestras características y modelo, y el enfoque de aprendizaje (Sección 4.2) para proporcionar suficiente información para replicar nuestros métodos de clasificación y los experimentos subsiguientes. 4.1 Representación de las acciones del usuario como características. Modelamos los comportamientos observados en la búsqueda web como una combinación de un \"componente de fondo (es decir, ruido independiente de la consulta y relevancia en el comportamiento del usuario, incluidos sesgos posicionales con interacciones de resultados), y un \"componente de relevancia (es decir, comportamiento específico de la consulta indicativo de la relevancia de un resultado para una consulta).",
        "Diseñamos nuestras características para aprovechar el comportamiento de usuario agregado.",
        "El conjunto de características está compuesto por características observadas directamente (calculadas directamente a partir de observaciones para cada consulta), así como características derivadas específicas de la consulta, calculadas como la desviación de la distribución general de valores independientes de la consulta para los valores de las características observadas directamente correspondientes.",
        "Las características utilizadas para representar las interacciones de los usuarios con los resultados de búsqueda en la web se resumen en la Tabla 4.1.",
        "Esta información se obtuvo a través de la instrumentación del lado del cliente opt-in de usuarios de un importante motor de búsqueda web.",
        "Incluimos las características tradicionales de retroalimentación implícita, como el recuento de clics en los resultados, así como nuestras características derivadas novedosas, como la desviación del número de clics observados para un par de consulta-URL dado del número esperado de clics en un resultado en la posición dada.",
        "También modelamos el comportamiento de navegación después de que se hace clic en un resultado, por ejemplo, el tiempo promedio de permanencia en la página para un par de consulta-URL dado, así como su desviación del tiempo de permanencia esperado (promedio).",
        "Además, el conjunto de características fue diseñado para proporcionar información esencial sobre la experiencia del usuario y hacer que la interpretación de la retroalimentación sea sólida.",
        "Por ejemplo, los usuarios de búsqueda en la web a menudo pueden determinar si un resultado es relevante al mirar el título del resultado, la URL y el resumen; en muchos casos, no es necesario mirar el documento original.",
        "Para modelar este aspecto de la experiencia del usuario, incluimos características como la superposición de palabras en el título y las palabras en la consulta (TitleOverlap) y la fracción de palabras compartidas por la consulta y el resumen del resultado.",
        "Características de clics Posición Posición de la URL en el ranking actual Frecuencia de clics Número de clics para esta consulta, par URL Probabilidad de clic Probabilidad de un clic para esta consulta y URL Desviación de clic Desviación de la probabilidad de clic esperada ¿Es el siguiente clic? 1 si se hizo clic en la siguiente posición, 0 de lo contrario ¿Es el clic anterior? 1 si se hizo clic en la posición anterior, 0 de lo contrario ¿Hay clic arriba? 1 si hay un clic arriba, 0 de lo contrario ¿Hay clic abajo? 1 si hay un clic abajo, 0 de lo contrario Características de navegación Tiempo en la página Tiempo de permanencia en la página Tiempo acumulado en la página Tiempo acumulado para todas las páginas siguientes después de la búsqueda Tiempo en el dominio Tiempo de permanencia acumulado para este dominio Tiempo en URL corta Tiempo acumulado en el prefijo de la URL, sin parámetros ¿Se siguió el enlace? 1 si se siguió el enlace al resultado, 0 de lo contrario ¿Coincidencia exacta de URL? 0 si se utilizó normalización agresiva, 1 de lo contrario ¿Redirigido? 1 si la URL inicial es la misma que la URL final, 0 de lo contrario ¿Camino desde la búsqueda? 1 si solo se siguieron enlaces después de la consulta, 0 de lo contrario Clics desde la búsqueda Número de saltos para llegar a la página desde la consulta Tiempo promedio de permanencia Tiempo promedio en la página para esta consulta Desviación del tiempo de permanencia Desviación del tiempo de permanencia promedio en la página Desviación acumulada Desviación del tiempo de permanencia acumulado promedio Desviación del dominio Desviación del tiempo de permanencia promedio en el dominio Características de texto de consulta Coincidencia de título Palabras compartidas entre la consulta y el título Coincidencia de resumen Palabras compartidas entre la consulta y el fragmento Coincidencia de URL de consulta Palabras compartidas entre la consulta y la URL Coincidencia de dominio de consulta Palabras compartidas entre la consulta y el dominio de la URL Longitud de la consulta Número de tokens en la consulta Superposición con la siguiente consulta Fracción de palabras compartidas con la siguiente consulta Tabla 4.1: Algunas características utilizadas para representar el historial de navegación posterior a la búsqueda para una consulta dada y una URL de resultado de búsqueda.",
        "Habiendo descrito nuestro conjunto de características, revisamos brevemente nuestro método general para derivar un modelo de comportamiento del usuario. 4.2 Derivación de un Modelo de Retroalimentación del Usuario Para aprender a interpretar el comportamiento observado del usuario, correlacionamos las acciones del usuario (es decir, las características en la Tabla 4.1 que representan las acciones) con los juicios explícitos del usuario para un conjunto de consultas de entrenamiento.",
        "Encontramos todas las instancias en nuestros registros de sesión donde se enviaron estas consultas al motor de búsqueda, y agregamos las características del comportamiento del usuario para todas las sesiones de búsqueda que involucran estas consultas.",
        "Cada par de consulta-URL observado está representado por las características en la Tabla 4.1, con valores promediados en todas las sesiones de búsqueda, y asignado una de las seis etiquetas de relevancia posibles, que van desde Perfecto hasta Malo, según los juicios explícitos de relevancia.",
        "Estos vectores de características etiquetados se utilizan como entrada para el algoritmo de entrenamiento RankNet (Sección 3.3), el cual produce un modelo de comportamiento de usuario entrenado.",
        "Este enfoque es particularmente atractivo ya que no requiere heurísticas más allá de la ingeniería de características.",
        "El modelo de comportamiento del usuario resultante se utiliza para ayudar a clasificar los resultados de búsqueda en la web, ya sea directamente o en combinación con otras características, como se describe a continuación. 5.",
        "CONFIGURACIÓN EXPERIMENTAL El objetivo final de incorporar retroalimentación implícita en la clasificación es mejorar la relevancia de los resultados de búsqueda web devueltos.",
        "Por lo tanto, comparamos los métodos de clasificación sobre un gran conjunto de consultas evaluadas con etiquetas de relevancia explícitas proporcionadas por jueces humanos.",
        "Para que la evaluación sea realista, obtuvimos una muestra aleatoria de consultas de registros de búsqueda web de un motor de búsqueda importante, con resultados asociados y rastros de acciones de usuario.",
        "Describimos este conjunto de datos en detalle a continuación.",
        "Nuestros indicadores se describen en la Sección 5.2 que utilizamos para evaluar las alternativas de clasificación, enumeradas en la Sección 5.3 en los experimentos de la Sección 6. 5.1 Conjuntos de datos Comparamos nuestros métodos de clasificación en una muestra aleatoria de 3,000 consultas de los registros de consultas del motor de búsqueda.",
        "Las consultas fueron extraídas de los registros de manera uniforme al azar por token sin reemplazo, lo que resultó en una muestra de consultas representativa de la distribución general de consultas.",
        "En promedio, 30 resultados fueron etiquetados explícitamente por jueces humanos utilizando una escala de seis puntos que va desde Perfecto hasta Malo.",
        "En total, hubo más de 83,000 resultados con juicios de relevancia explícitos.",
        "Para calcular diversas estadísticas, se considerarán relevantes los documentos con la etiqueta Buena o mejor, y los documentos con etiquetas inferiores se considerarán no relevantes.",
        "Se debe tener en cuenta que los experimentos se realizaron sobre los resultados ya altamente clasificados por un motor de búsqueda web, lo cual corresponde a una experiencia de usuario típica que se limita al pequeño número de resultados altamente clasificados para una consulta típica de búsqueda web.",
        "Las interacciones de los usuarios fueron recopiladas durante un período de 8 semanas utilizando información voluntaria de participación.",
        "En total, se registraron más de 1.2 millones de consultas únicas, lo que resultó en más de 12 millones de interacciones individuales con el motor de búsqueda.",
        "Los datos consistían en las interacciones de los usuarios con el motor de búsqueda web (por ejemplo, hacer clic en un enlace de resultado, regresar a los resultados de búsqueda, etc.) realizadas después de enviar una consulta.",
        "Estas acciones fueron agregadas entre usuarios y sesiones de búsqueda y convertidas en características en la Tabla 4.1.",
        "Para crear los conjuntos de consultas de entrenamiento, validación y prueba, creamos tres divisiones aleatorias diferentes de 1,500 consultas de entrenamiento, 500 de validación y 1000 de prueba.",
        "Las divisiones se realizaron aleatoriamente por consulta, de modo que no hubiera superposición en las consultas de entrenamiento, validación y prueba. 5.2 Métricas de Evaluación Evaluamos los algoritmos de clasificación en una variedad de métricas de recuperación de información aceptadas, a saber, Precisión en K (P(K)), Ganancia Acumulada Descontada Normalizada (NDCG) y Precisión Promedio Media (MAP).",
        "Cada métrica se enfoca en un aspecto diferente del rendimiento del sistema, como describimos a continuación. • Precisión en K: Como la métrica más intuitiva, P(K) informa la fracción de documentos clasificados en los primeros K resultados que están etiquetados como relevantes.",
        "En nuestro entorno, requerimos que un documento relevante sea etiquetado como Bueno o superior.",
        "La posición de los documentos relevantes dentro de los primeros K no es relevante, por lo que esta métrica mide la satisfacción general del usuario con los resultados principales de K. • NDCG en K: NDCG es una medida de recuperación diseñada específicamente para la evaluación de búsqueda en la web [10].",
        "Para una consulta dada q, los resultados clasificados se examinan desde el mejor clasificado hacia abajo, y el NDCG se calcula como: = +−= K j jr qq jMN 1 )( )1log(/)12( Donde Mq es una constante de normalización calculada de manera que un orden perfecto obtendría un NDCG de 1; y cada r(j) es una etiqueta de relevancia entera (0=Malo y 5=Perfecto) del resultado devuelto en la posición j.",
        "Ten en cuenta que los documentos sin etiquetar y los documentos malos no contribuyen a la suma, pero reducirán el NDCG para la consulta al empujar hacia abajo los documentos etiquetados relevantes, disminuyendo sus contribuciones.",
        "NDCG es muy adecuado para la evaluación de búsquedas en la web, ya que recompensa de manera más intensa los documentos relevantes en los resultados mejor clasificados que aquellos clasificados más bajos. • MAP: La precisión promedio para cada consulta se define como la media de la precisión en los valores de K calculados después de que se recuperó cada documento relevante.",
        "El valor MAP final se define como la media de las precisiones promedio de todas las consultas en el conjunto de pruebas.",
        "Esta métrica es el resumen de un solo valor más comúnmente utilizado de una ejecución sobre un conjunto de consultas. 5.3 Métodos de clasificación comparados. Recuerde que nuestro objetivo es cuantificar la efectividad del comportamiento implícito para la búsqueda web real.",
        "Una dimensión es comparar la utilidad de la retroalimentación implícita con otra información disponible para un motor de búsqueda web.",
        "Específicamente, comparamos la efectividad de los comportamientos implícitos de los usuarios con la coincidencia basada en el contenido, las características de calidad de la página estática y combinaciones de todas las características. • BM25F: Como referencia sólida de búsqueda web, utilizamos la puntuación BM25F, que se utilizó en uno de los sistemas con mejor rendimiento en la pista web TREC 2004 [23,27].",
        "BM25F y sus variantes han sido extensamente descritas y evaluadas en la literatura de IR, por lo tanto, sirven como una línea base sólida y reproducible.",
        "La variante BM25F que utilizamos en nuestros experimentos calcula puntuaciones de coincidencia separadas para cada campo de un documento de resultado (por ejemplo, texto del cuerpo, título y texto del enlace) e incorpora información de enlaces independiente de la consulta (por ejemplo, PageRank, ClickDistance y profundidad de URL).",
        "La función de puntuación y la ajuste específico del campo se describen en detalle en [23].",
        "Tenga en cuenta que BM25F no considera directamente la retroalimentación explícita o implícita para ajuste. • RN: La clasificación producida por un clasificador de redes neuronales (RankNet, descrito en la Sección 3.3) que aprende a clasificar los resultados de búsqueda web al incorporar BM25F y un gran número de características estáticas y dinámicas adicionales que describen cada resultado de búsqueda.",
        "Este sistema aprende automáticamente los pesos de todas las características (incluido el puntaje BM25F para un documento) basándose en etiquetas humanas explícitas para un gran conjunto de consultas.",
        "Un sistema que incorpora una implementación de RankNet actualmente está siendo utilizado por un importante motor de búsqueda y puede considerarse representativo del estado del arte en la búsqueda web. • BM25F-RerankCT: La clasificación producida al incorporar estadísticas de clics para reordenar los resultados de búsqueda web clasificados por BM25F arriba.",
        "El clic es un caso especial particularmente importante de retroalimentación implícita, y se ha demostrado que se correlaciona con la relevancia de los resultados.",
        "Este es un caso especial del método de clasificación en la Sección 3.1, con el peso wI establecido en 1000 y la clasificación Id es simplemente el número de clics en el resultado correspondiente a d. En efecto, esta clasificación coloca en la parte superior todos los resultados de búsqueda web devueltos con al menos un clic (y los ordena en orden decreciente por número de clics).",
        "La clasificación relativa de los resultados restantes no cambia y se insertan debajo de todos los resultados clicados.",
        "Este método sirve como nuestro método de reordenamiento de retroalimentación implícita de referencia.",
        "La clasificación producida al reordenar los resultados de BM25F utilizando todas las características del comportamiento del usuario (Sección 4).",
        "Este método aprende un modelo de preferencias de usuario correlacionando los valores de las características con etiquetas de relevancia explícitas utilizando el algoritmo de red neuronal RankNet (Sección 4.2).",
        "En tiempo de ejecución, para una consulta dada se calcula la puntuación implícita Ir para cada resultado r con características de interacción de usuario disponibles, y se produce el ranking implícito.",
        "La clasificación combinada se calcula como se describe en la Sección 3.1.",
        "Basándonos en los experimentos realizados sobre el conjunto de desarrollo, fijamos el valor de wI en 3 (el efecto del parámetro wI para este clasificador resultó ser insignificante). • BM25F+All: Clasificación derivada al entrenar el aprendiz RankNet (Sección 3.3) sobre el conjunto de características del puntaje BM25F, así como todas las características de retroalimentación implícita (Sección 3.2).",
        "Utilizamos la implementación de 2 capas de RankNet [5] entrenada en las consultas y etiquetas de los conjuntos de entrenamiento y validación. • RN+All: Clasificación derivada al entrenar el algoritmo de clasificación RankNet de 2 capas (Sección 3.3) sobre la unión de todas las características de retroalimentación de contenido, dinámicas e implícitas (es decir, todas las características descritas anteriormente, así como todas las nuevas características de retroalimentación implícita que introdujimos).",
        "Los métodos de clasificación anteriores abarcan el rango de la información utilizada para clasificar, desde no utilizar la retroalimentación implícita o explícita en absoluto (es decir, BM25F) hasta un motor de búsqueda web moderno que utiliza cientos de características y está ajustado en base a juicios explícitos (RN).",
        "Como mostraremos a continuación, incorporar el comportamiento del usuario en estos sistemas de clasificación mejora drásticamente la relevancia de los documentos devueltos. 6.",
        "RESULTADOS EXPERIMENTALES El feedback implícito para la clasificación de búsquedas en la web puede ser explotado de diversas formas.",
        "Comparamos métodos alternativos para explotar la retroalimentación implícita, tanto reordenando los resultados principales (es decir, los métodos BM25F-RerankCT y BM25F-RerankAll que reordenan los resultados de BM25F), como integrando directamente las características implícitas en el proceso de clasificación (es decir, los métodos RN+ALL y BM25F+All que aprenden a clasificar los resultados sobre la retroalimentación implícita y otras características).",
        "Comparamos nuestros métodos con baselines sólidos (BM25F y RN) sobre las medidas NDCG, Precisión en K y MAP definidas en la Sección 5.2.",
        "Los resultados se promediaron en tres divisiones aleatorias del conjunto de datos completo.",
        "Cada división contenía 1500 consultas de entrenamiento, 500 de validación y 1000 de prueba, todos los conjuntos de consultas disjuntos.",
        "Primero presentamos los resultados de las 1000 consultas de prueba (es decir, incluyendo las consultas para las cuales no hay medidas implícitas, por lo que utilizamos las clasificaciones web originales).",
        "Luego profundizamos para examinar los efectos en la reorganización de los intentos de búsqueda con más detalle, analizando dónde el feedback implícito resultó más beneficioso.",
        "Primero experimentamos con diferentes métodos de volver a clasificar la salida de los resultados de búsqueda de BM25F.",
        "Las figuras 6.1 y 6.2 informan sobre NDCG y Precisión para BM25F, así como para las estrategias de volver a clasificar los resultados con retroalimentación del usuario (Sección 3.1).",
        "Incorporar todos los comentarios de los usuarios (ya sea en el marco de reordenamiento o como características directamente al aprendiz) resulta en mejoras significativas (utilizando una prueba t de dos colas con p=0.01) tanto sobre la clasificación original de BM25F como sobre el reordenamiento solo con clics.",
        "La mejora es consistente en los 10 mejores resultados y es mayor para el mejor resultado: NDCG en 1 para BM25F+All es de 0.622 en comparación con 0.518 de los resultados originales, y la precisión en 1 también aumenta de 0.5 a 0.63.",
        "Basándonos en estos resultados, utilizaremos el clasificador de combinación de características directas (es decir, BM25F+All) para comparaciones posteriores que involucren retroalimentación implícita. Curiosamente, el uso solo de clics, aunque proporciona un beneficio significativo sobre la clasificación original de BM25F, no es tan efectivo como considerar el conjunto completo de características en la Tabla 4.1.",
        "Mientras analizamos el comportamiento del usuario (y las características de los componentes más efectivos) en un documento separado [1], vale la pena dar un ejemplo concreto del tipo de ruido inherente en la retroalimentación real de los usuarios en el entorno de búsqueda web. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 Posición del resultado Frecuencia de clics relativa PTR=2 PTR=3 PTR=5 Figura 6.3: Frecuencia relativa de clics para consultas con diferentes Posiciones del Resultado Relevante Superior (PTR).",
        "Si los usuarios solo consideraran la relevancia de un resultado para su consulta, harían clic en los resultados más relevantes en la parte superior.",
        "Desafortunadamente, como han demostrado Joachims y otros, la presentación también influye de manera bastante dramática en los resultados en los que los usuarios hacen clic.",
        "Los usuarios a menudo hacen clic en los resultados por encima del relevante, presumiblemente porque los resúmenes cortos no proporcionan suficiente información para hacer evaluaciones precisas de relevancia y han aprendido que, en promedio, los elementos mejor clasificados son relevantes.",
        "La Figura 6.3 muestra las frecuencias relativas de clics para consultas con elementos relevantes conocidos en posiciones distintas a la primera posición; la posición del resultado relevante superior (PTR) varía de 2 a 10 en la figura.",
        "Por ejemplo, para consultas con el primer resultado relevante en la posición 5 (PTR=5), hay más clics en los resultados no relevantes en posiciones más altas que en el primer resultado relevante en la posición 5.",
        "Como veremos, el aprendizaje sobre un conjunto de características de comportamiento más amplio resulta en una mejora sustancial en la precisión en comparación con solo el clic.",
        "Ahora consideramos incorporar el comportamiento del usuario en un conjunto de características mucho más amplio, RN (Sección 5.3) utilizado por un importante motor de búsqueda web.",
        "RN incorpora BM25F, características basadas en enlaces y cientos de otras características.",
        "La Figura 6.4 informa sobre NDCG en K y la Figura 6.5 informa sobre Precisión en K. Curiosamente, aunque las clasificaciones originales de RN son significativamente más precisas que BM25F solo, la incorporación de características de retroalimentación implícita (BM25F+All) da como resultado una clasificación que supera significativamente a las clasificaciones originales de RN.",
        "En otras palabras, la retroalimentación implícita incorpora suficiente información para reemplazar las cientos de otras características disponibles para el aprendiz de RankNet entrenado en el conjunto de características RN. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 1 2 3 4 5 6 7 8 9 10K NDCG RN RN+All BM25 BM25+All Figura 6.4: NDCG en K para BM25F, BM25F+All, RN y RN+All para diferentes K Además, enriquecer las características de RN con el conjunto de retroalimentación implícita muestra una ganancia significativa en todas las medidas, permitiendo que RN+All supere a todos los demás métodos.",
        "Esto demuestra la naturaleza complementaria de la retroalimentación implícita con otras características disponibles para un motor de búsqueda web de última generación. 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 Precisión RN RN+Todo BM25 BM25+Todo Figura 6.5: Precisión en K para BM25F, BM25F+Todo, RN y RN+Todo para diferentes valores de K. Resumimos el rendimiento de los diferentes métodos de clasificación en la Tabla 6.1.",
        "Informamos el puntaje de Precisión Promedio Media (MAP) para cada sistema.",
        "Aunque no es intuitivo de interpretar, el MAP permite la comparación cuantitativa en una sola métrica.",
        "Las ganancias marcadas con * son significativas a un nivel de p=0.01 utilizando una prueba t de dos colas.",
        "Ganancia MAP P(1) Ganancia BM25F 0.184 - 0.503BM25F-Rerank-CT 0.215 0.031* 0.577 0.073* BM25F-RerankImplicit 0.218 0.003 0.605 0.028* BM25F+Implicit 0.222 0.004 0.620 0.015* RN 0.215 - 0.597RN+All 0.248 0.033* 0.629 0.032* Tabla 6.1: Precisión Promedio (MAP) para todas las estrategias.",
        "Hasta ahora hemos informado resultados promediados en todas las consultas del conjunto de pruebas.",
        "Desafortunadamente, menos de la mitad tuvo interacciones suficientes para intentar un nuevo ranking.",
        "De las 1000 consultas en la prueba, entre el 46% y el 49%, dependiendo de la división entre entrenamiento y prueba, tenían suficiente información de interacción para hacer predicciones (es decir, hubo al menos 1 sesión de búsqueda en la que el usuario hizo clic en al menos 1 URL de resultado).",
        "Esto no es sorprendente: la búsqueda en la web tiene una distribución de cola pesada y hay muchas consultas únicas.",
        "Ahora consideramos el rendimiento en las consultas para las cuales estaban disponibles las interacciones de los usuarios.",
        "La Figura 6.6 informa sobre el NDCG para el subconjunto de las consultas de prueba con las características de retroalimentación implícita.",
        "Las ganancias en el primer puesto son dramáticas.",
        "El NDCG en 1 de BM25F+All aumenta de 0.6 a 0.75 (un aumento relativo del 31%), logrando un rendimiento comparable al de RN+All operando sobre un conjunto de características mucho más rico. 0.6 0.65 0.7 0.75 0.8 1 3 5 10K NDCG RN RN+All BM25 BM25+All Figura 6.6: NDCG en K para BM25F, BM25F+All, RN y RN+All en consultas de prueba con interacciones de usuario. Del mismo modo, las ganancias en precisión en el top 1 son sustanciales (Figura 6.7) y es probable que sean evidentes para los usuarios de búsqueda web.",
        "Cuando se dispone de retroalimentación implícita, el sistema BM25F+All devuelve el documento relevante en la parte superior 1 casi el 70% del tiempo, en comparación con el 53% del tiempo cuando la retroalimentación implícita no es considerada por el sistema BM25F original. 0.45 0.5 0.55 0.6 0.65 0.7 1 3 5 10K Precisión RN RN+All BM25 BM25+All Figura 6.7: Precisión en K NDCG en K para BM25F, BM25F+All, RN y RN+All en consultas de prueba con interacciones de usuario Resumimos los resultados en la medida MAP para las consultas intentadas en la Tabla 6.2.",
        "Las mejoras en MAP son tanto sustanciales como significativas, siendo más pronunciadas que las mejoras en el clasificador BM25F.",
        "Ahora analizamos los casos en los que la retroalimentación implícita resultó ser más útil.",
        "La Figura 6.8 informa las mejoras de MAP sobre la ejecución de BM25F base para cada consulta con MAP inferior a 0.6.",
        "Ten en cuenta que la mayoría de la mejora es para consultas con bajo rendimiento (es decir, MAP < 0.1).",
        "Curiosamente, la incorporación de información sobre el comportamiento del usuario disminuye la precisión para consultas con un puntaje MAP original alto.",
        "Una posible explicación es que estas consultas fáciles tienden a ser de navegación (es decir, tener una única respuesta apropiada altamente clasificada), y las interacciones de los usuarios con resultados de menor rango pueden indicar necesidades de información divergentes que son mejor atendidas por los resultados menos populares (con calificaciones de relevancia global correspondientemente bajas). Para resumir nuestros resultados experimentales, la incorporación de retroalimentación implícita en un entorno real de búsqueda web resultó en mejoras significativas sobre las clasificaciones originales, utilizando tanto BM25F como RN como líneas de base.",
        "Nuestro amplio conjunto de características implícitas, como el tiempo en la página y las desviaciones del comportamiento promedio, ofrece ventajas sobre el uso exclusivo del clic como indicador de interés.",
        "Además, incorporar características de retroalimentación implícita directamente en la función de clasificación aprendida es más efectivo que utilizar la retroalimentación implícita para volver a clasificar.",
        "Las mejoras observadas en grandes conjuntos de pruebas de consultas (1,000 en total, entre 466 y 495 con retroalimentación implícita disponible) son tanto sustanciales como estadísticamente significativas. 7.",
        "CONCLUSIONES Y TRABAJOS FUTUROS En este artículo exploramos la utilidad de incorporar retroalimentación implícita ruidosa obtenida en un entorno real de búsqueda web para mejorar la clasificación de búsqueda web.",
        "Realizamos una evaluación a gran escala con más de 3,000 consultas y más de 12 millones de interacciones de usuarios con un motor de búsqueda importante, estableciendo la utilidad de incorporar retroalimentación implícita ruidosa para mejorar la relevancia de la búsqueda web.",
        "Comparamos dos alternativas para incorporar retroalimentación implícita en el proceso de búsqueda, a saber, reordenar con retroalimentación implícita e incorporar directamente características de retroalimentación implícita en la función de clasificación entrenada.",
        "Nuestros experimentos mostraron una mejora significativa sobre los métodos que no consideran la retroalimentación implícita.",
        "Las ganancias son particularmente dramáticas para el resultado superior K=1 en la clasificación final, con mejoras de precisión de hasta un 31%, y las ganancias son sustanciales para todos los valores de K. Nuestros experimentos mostraron que el feedback implícito del usuario puede mejorar aún más el rendimiento de la búsqueda web, cuando se incorpora directamente con características populares basadas en contenido y enlaces.",
        "Curiosamente, la retroalimentación implícita es especialmente valiosa para consultas con una clasificación original deficiente de resultados (por ejemplo, MAP inferior a 0.1).",
        "Una dirección prometedora para trabajos futuros es aplicar la investigación reciente sobre la predicción automática de la dificultad de las consultas, e intentar incorporar únicamente retroalimentación implícita para las consultas difíciles.",
        "Como otra dirección de investigación, estamos explorando métodos para extender nuestras predicciones a las consultas previamente no vistas (por ejemplo, el agrupamiento de consultas), lo cual debería mejorar aún más la experiencia de búsqueda en la web de los usuarios.",
        "AGRADECIMIENTOS Agradecemos a Chris Burges y Matt Richardson por la implementación de RankNet para nuestros experimentos.",
        "También agradecemos a Robert Ragno por sus valiosas sugerencias y muchas discusiones. 8.",
        "REFERENCIAS [1] E. Agichtein, E. Brill, S. Dumais y R. Ragno, Aprendizaje de modelos de interacción del usuario para predecir las preferencias de resultados de búsqueda web.",
        "En Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2006 [2] J. Allan, Resumen de la Pista HARD en TREC 2003, Recuperación de Alta Precisión de Documentos, 2003 [3] R. Baeza-Yates y B. Ribeiro-Neto, Recuperación de Información Moderna, Addison-Wesley, 1999. [4] S. Brin y L. Page, Anatomía de un Motor de Búsqueda Web Hipertextual a Gran Escala, en Actas de WWW, 1997 [5] C.J.C.",
        "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. Hullender, Aprendizaje para clasificación utilizando descenso de gradiente, en Actas de la Conferencia Internacional sobre Aprendizaje Automático, 2005 [6] D.M.",
        "Chickering, The WinMine Toolkit, Informe Técnico de Microsoft MSR-TR-2002-103, 2002 [7] M. Claypool, D. Brown, P. Lee y M. Waseda.",
        "Inferir el interés del usuario.",
        "IEEE Internet Computing. 2001 [8] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais y T. White.",
        "Evaluando medidas implícitas para mejorar la experiencia de búsqueda.",
        "En ACM Transactions on Information Systems, 2005 [9] J. Goecks y J. Shavlick.",
        "Aprendiendo los intereses de los usuarios observando de manera discreta su comportamiento normal.",
        "En Actas del Taller de IJCAI sobre Aprendizaje Automático para Filtrado de Información. 1999. [10] K Jarvelin y J. Kekalainen.",
        "Métodos de evaluación IR para recuperar documentos altamente relevantes.",
        "En las Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2000 [11] T. Joachims, Optimización de Motores de Búsqueda Utilizando Datos de Clics.",
        "En Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (SIGKDD), 2002 [12] T. Joachims, L. Granka, B. Pang, H. Hembrooke y G. Gay, Interpretación precisa de los datos de clics como retroalimentación implícita, Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2005 [13] T. Joachims, Haciendo práctico el aprendizaje SVM a gran escala.",
        "Avances en Métodos de Núcleo, en Aprendizaje de Vectores de Soporte, MIT Press, 1999 [14] D. Kelly y J. Teevan, Retroalimentación implícita para inferir preferencias de usuario: una bibliografía.",
        "En el Foro SIGIR, 2003 [15] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon y J. Riedl.",
        "GroupLens: Aplicando filtrado colaborativo a las noticias de Usenet.",
        "En Comunicaciones de ACM, 1997. [16] M. Morita y Y. Shinoda, Filtrado de información basado en análisis del comportamiento del usuario y recuperación de texto de mejor coincidencia.",
        "Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 1994 [17] D. Oard y J. Kim.",
        "Retroalimentación implícita para sistemas de recomendación.",
        "En Actas del Taller de Sistemas de Recomendación de la AAAI. 1998 [18] D. Oard y J. Kim.",
        "Modelando el contenido de la información utilizando el comportamiento observable.",
        "En Actas de la 64ª Reunión Anual de la Sociedad Americana de Ciencia de la Información y Tecnología. 2001 [19] N. Pharo, N. y K. Järvelin.",
        "El método SST: una herramienta para analizar los procesos de búsqueda de información en la web.",
        "En Information Processing & Management, 2004 [20] P. Pirolli, El Uso de la Pista de Información Próxima para Buscar Contenido Distal en la World Wide Web.",
        "Trabajando con la tecnología en mente: Brunswikiano.",
        "Recursos para Ciencia Cognitiva e Ingeniería, Oxford University Press, 2004 [21] F. Radlinski y T. Joachims, Cadenas de Consulta: Aprendizaje para Clasificar a partir de Retroalimentación Implícita.",
        "En Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (SIGKDD), 2005. [22] F. Radlinski y T. Joachims, Evaluando la Robustez del Aprendizaje a partir de Retroalimentación Implícita, en Actas del Taller de ICML sobre Aprendizaje en Búsqueda Web, 2005 [23] S. E. Robertson, H. Zaragoza y M. Taylor, Extensión simple de BM25 a múltiples campos ponderados, en Actas de la Conferencia sobre Información y Gestión del Conocimiento (CIKM), 2004 [24] G. Salton y M. McGill.",
        "Introducción a la recuperación de información moderna.",
        "McGraw-Hill, 1983 [25] E.M. Voorhees, D. Harman, Resumen de TREC, 2001 [26] G.R.",
        "Xue, H.J.",
        "Zeng, Z. Chen, Y. Yu, W.Y. -> Zeng, Z. Chen, Y. Yu, W.Y.",
        "Ma, W.S.",
        "Xi, y W.G.",
        "Fan, Optimizing web search using web clickthrough data, en Actas de la Conferencia sobre Información y Gestión del Conocimiento (CIKM), 2004 [27] H. Zaragoza, N. Craswell, M. Taylor, S. Saria y S. Robertson.",
        "Microsoft Cambridge en TREC 13: Pistas Web y Duras.",
        "En Actas de TREC 2004"
    ],
    "error_count": 0,
    "keys": {
        "web search": {
            "translated_key": "búsqueda web",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Improving <br>web search</br> Ranking by Incorporating User Behavior Information Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com ABSTRACT We show that incorporating user behavior data can significantly improve ordering of top results in real <br>web search</br> setting.",
                "We examine alternatives for incorporating feedback into the ranking process and explore the contributions of user feedback compared to other common <br>web search</br> features.",
                "We report results of a large scale evaluation over 3,000 queries and 12 million user interactions with a popular <br>web search</br> engine.",
                "We show that incorporating implicit feedback can augment other features, improving the accuracy of a competitive <br>web search</br> ranking algorithms by as much as 31% relative to the original performance.",
                "Categories and Subject Descriptors H.3.3 Information Search and Retrieval - Relevance feedback, search process; H.3.5 Online Information Services - Web-based services.",
                "General Terms Algorithms, Measurement, Experimentation 1.",
                "INTRODUCTION Millions of users interact with search engines daily.",
                "They issue queries, follow some of the links in the results, click on ads, spend time on pages, reformulate their queries, and perform other actions.",
                "These interactions can serve as a valuable source of information for tuning and improving <br>web search</br> result ranking and can compliment more costly explicit judgments.",
                "Implicit relevance feedback for ranking and personalization has become an active area of research.",
                "Recent work by Joachims and others exploring implicit feedback in controlled environments have shown the value of incorporating implicit feedback into the ranking process.",
                "Our motivation for this work is to understand how implicit feedback can be used in a large-scale operational environment to improve retrieval.",
                "How does it compare to and compliment evidence from page content, anchor text, or link-based features such as inlinks or PageRank?",
                "While it is intuitive that user interactions with the <br>web search</br> engine should reveal at least some information that could be used for ranking, estimating user preferences in real <br>web search</br> settings is a challenging problem, since real user interactions tend to be more noisy than commonly assumed in the controlled settings of previous studies.",
                "Our paper explores whether implicit feedback can be helpful in realistic environments, where user feedback can be noisy (or adversarial) and a <br>web search</br> engine already uses hundreds of features and is heavily tuned.",
                "To this end, we explore different approaches for ranking <br>web search</br> results using real user behavior obtained as part of normal interactions with the <br>web search</br> engine.",
                "The specific contributions of this paper include: • Analysis of alternatives for incorporating user behavior into <br>web search</br> ranking (Section 3). • An application of a robust implicit feedback model derived from mining millions of user interactions with a major <br>web search</br> engine (Section 4). • A large scale evaluation over real user queries and search results, showing significant improvements derived from incorporating user feedback (Section 6).",
                "We summarize our findings and discuss extensions to the current work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "Most common approaches primarily focus on similarity of query and a page, as well as the overall page quality [3,4,24].",
                "However, with increasing popularity of search engines, implicit feedback (i.e., the actions users take when interacting with the search engine) can be used to improve the rankings.",
                "Implicit relevance measures have been studied by several research groups.",
                "An overview of implicit measures is compiled in Kelly and Teevan [14].",
                "This research, while developing valuable insights into implicit relevance measures, was not applied to improve the ranking of <br>web search</br> results in realistic settings.",
                "Closely related to our work, Joachims [11] collected implicit measures in place of explicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "Fox et al. [8] explored the relationship between implicit and explicit measures in <br>web search</br>, and developed Bayesian models to correlate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "This work considered a wide range of user behaviors (e.g., dwell time, scroll time, reformulation patterns) in addition to the popular clickthrough behavior.",
                "However, the modeling effort was aimed at predicting explicit relevance judgments from implicit user actions and not specifically at learning ranking functions.",
                "Other studies of user behavior in <br>web search</br> include Pharo and Järvelin [19], but were not directly applied to improve ranking.",
                "More recently, Joachims et al. [12] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthroughs in a controlled, laboratory setting.",
                "Unfortunately, the extent to which previous research applies to real-world <br>web search</br> is unclear.",
                "At the same time, while recent work (e.g., [26]) on using clickthrough information for improving <br>web search</br> ranking is promising, it captures only one aspect of the user interactions with <br>web search</br> engines.",
                "We build on existing research to develop robust user behavior interpretation techniques for the real <br>web search</br> setting.",
                "Instead of treating each user as a reliable expert, we aggregate information from multiple, unreliable, user search session traces, as we describe in the next two sections. 3.",
                "INCORPORATING IMPLICIT FEEDBACK We consider two complementary approaches to ranking with implicit feedback: (1) treating implicit feedback as independent evidence for ranking results, and (2) integrating implicit feedback features directly into the ranking algorithm.",
                "We describe the two general ranking approaches next.",
                "The specific implicit feedback features are described in Section 4, and the algorithms for interpreting and incorporating implicit feedback are described in Section 5. 3.1 Implicit Feedback as Independent Evidence The general approach is to re-rank the results obtained by a <br>web search</br> engine according to observed clickthrough and other user interactions for the query in previous search sessions.",
                "Each result is assigned a score according to expected relevance/user satisfaction based on previous interactions, resulting in some preference ordering based on user interactions alone.",
                "While there has been significant work on merging multiple rankings, we adapt a simple and robust approach of ignoring the original rankers scores, and instead simply merge the rank orders.",
                "The main reason for ignoring the original scores is that since the feature spaces and learning algorithms are different, the scores are not directly comparable, and re-normalization tends to remove the benefit of incorporating classifier scores.",
                "We experimented with a variety of merging functions on the development set of queries (and using a set of interactions from a different time period from final evaluation sets).",
                "We found that a simple rank merging heuristic combination works well, and is robust to variations in score values from original rankers.",
                "For a given query q, the implicit score ISd is computed for each result d from available user interaction features, resulting in the implicit rank Id for each result.",
                "We compute a merged score SM(d) for d by combining the ranks obtained from implicit feedback, Id with the original rank of d, Od:     ¡     ¢ £ + + + + = otherwise O dforexistsfeedbackimplicitif OI w wOIdS d dd I IddM 1 1 1 1 1 1 ),,,( where the weight wI is a heuristically tuned scaling factor representing the relative importance of the implicit feedback.",
                "The query results are ordered in by decreasing values of SM to produce the final ranking.",
                "One special case of this model arises when setting wI to a very large value, effectively forcing clicked results to be ranked higher than un-clicked results - an intuitive and effective heuristic that we will use as a baseline.",
                "Applying more sophisticated classifier and ranker combination algorithms may result in additional improvements, and is a promising direction for future work.",
                "The approach above assumes that there are no interactions between the underlying features producing the original <br>web search</br> ranking and the implicit feedback features.",
                "We now relax this assumption by integrating implicit feedback features directly into the ranking process. 3.2 Ranking with Implicit Feedback Features Modern <br>web search</br> engines rank results based on a large number of features, including content-based features (i.e., how closely a query matches the text or title or anchor text of the document), and query-independent page quality features (e.g., PageRank of the document or the domain).",
                "In most cases, automatic (or semiautomatic) methods are developed for tuning the specific ranking function that combines these feature values.",
                "Hence, a natural approach is to incorporate implicit feedback features directly as features for the ranking algorithm.",
                "During training or tuning, the ranker can be tuned as before but with additional features.",
                "At runtime, the search engine would fetch the implicit feedback features associated with each query-result URL pair.",
                "This model requires a ranking algorithm to be robust to missing values: more than 50% of queries to <br>web search</br> engines are unique, with no previous implicit feedback available.",
                "We now describe such a ranker that we used to learn over the combined feature sets including implicit feedback. 3.3 Learning to Rank <br>web search</br> Results A key aspect of our approach is exploiting recent advances in machine learning, namely trainable ranking algorithms for <br>web search</br> and information retrieval (e.g., [5, 11] and classical results reviewed in [3]).",
                "In our setting, explicit human relevance judgments (labels) are available for a set of <br>web search</br> queries and results.",
                "Hence, an attractive choice to use is a supervised machine learning technique to learn a ranking function that best predicts relevance judgments.",
                "RankNet is one such algorithm.",
                "It is a neural net tuning algorithm that optimizes feature weights to best match explicitly provided pairwise user preferences.",
                "While the specific training algorithms used by RankNet are beyond the scope of this paper, it is described in detail in [5] and includes extensive evaluation and comparison with other ranking methods.",
                "An attractive feature of RankNet is both train- and run-time efficiency - runtime ranking can be quickly computed and can scale to the web, and training can be done over thousands of queries and associated judged results.",
                "We use a 2-layer implementation of RankNet in order to model non-linear relationships between features.",
                "Furthermore, RankNet can learn with many (differentiable) cost functions, and hence can automatically learn a ranking function from human-provided labels, an attractive alternative to heuristic feature combination techniques.",
                "Hence, we will also use RankNet as a generic ranker to explore the contribution of implicit feedback for different ranking alternatives. 4.",
                "IMPLICIT USER FEEDBACK MODEL Our goal is to accurately interpret noisy user feedback obtained as by tracing user interactions with the search engine.",
                "Interpreting implicit feedback in real <br>web search</br> setting is not an easy task.",
                "We characterize this problem in detail in [1], where we motivate and evaluate a wide variety of models of implicit user activities.",
                "The general approach is to represent user actions for each search result as a vector of features, and then train a ranker on these features to discover feature values indicative of relevant (and nonrelevant) search results.",
                "We first briefly summarize our features and model, and the learning approach (Section 4.2) in order to provide sufficient information to replicate our ranking methods and the subsequent experiments. 4.1 Representing User Actions as Features We model observed <br>web search</br> behaviors as a combination of a ``background component (i.e., query- and relevance-independent noise in user behavior, including positional biases with result interactions), and a ``relevance component (i.e., query-specific behavior indicative of relevance of a result to a query).",
                "We design our features to take advantage of aggregated user behavior.",
                "The feature set is comprised of directly observed features (computed directly from observations for each query), as well as queryspecific derived features, computed as the deviation from the overall query-independent distribution of values for the corresponding directly observed feature values.",
                "The features used to represent user interactions with <br>web search</br> results are summarized in Table 4.1.",
                "This information was obtained via opt-in client-side instrumentation from users of a major <br>web search</br> engine.",
                "We include the traditional implicit feedback features such as clickthrough counts for the results, as well as our novel derived features such as the deviation of the observed clickthrough number for a given query-URL pair from the expected number of clicks on a result in the given position.",
                "We also model the browsing behavior after a result was clicked - e.g., the average page dwell time for a given query-URL pair, as well as its deviation from the expected (average) dwell time.",
                "Furthermore, the feature set was designed to provide essential information about the user experience to make feedback interpretation robust.",
                "For example, <br>web search</br> users can often determine whether a result is relevant by looking at the result title, URL, and summary - in many cases, looking at the original document is not necessary.",
                "To model this aspect of user experience we include features such as overlap in words in title and words in query (TitleOverlap) and the fraction of words shared by the query and the result summary.",
                "Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickProbability Probability of a click for this query and URL ClickDeviation Deviation from expected click probability IsNextClicked 1 if clicked on next position, 0 otherwise IsPreviousClicked 1 if clicked on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, no parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from average dwell time on page CumulativeDeviation Deviation from average cumulative dwell time DomainDeviation Deviation from average dwell time on domain Query-text features TitleOverlap Words shared between query and title SummaryOverlap Words shared between query and snippet QueryURLOverlap Words shared between query and URL QueryDomainOverlap Words shared between query and URL domain QueryLength Number of tokens in query QueryNextOverlap Fraction of words shared with next query Table 4.1: Some features used to represent post-search navigation history for a given query and search result URL.",
                "Having described our feature set, we briefly review our general method for deriving a user behavior model. 4.2 Deriving a User Feedback Model To learn to interpret the observed user behavior, we correlate user actions (i.e., the features in Table 4.1 representing the actions) with the explicit user judgments for a set of training queries.",
                "We find all the instances in our session logs where these queries were submitted to the search engine, and aggregate the user behavior features for all search sessions involving these queries.",
                "Each observed query-URL pair is represented by the features in Table 4.1, with values averaged over all search sessions, and assigned one of six possible relevance labels, ranging from Perfect to Bad, as assigned by explicit relevance judgments.",
                "These labeled feature vectors are used as input to the RankNet training algorithm (Section 3.3) which produces a trained user behavior model.",
                "This approach is particularly attractive as it does not require heuristics beyond feature engineering.",
                "The resulting user behavior model is used to help rank <br>web search</br> resultseither directly or in combination with other features, as described below. 5.",
                "EXPERIMENTAL SETUP The ultimate goal of incorporating implicit feedback into ranking is to improve the relevance of the returned <br>web search</br> results.",
                "Hence, we compare the ranking methods over a large set of judged queries with explicit relevance labels provided by human judges.",
                "In order for the evaluation to be realistic we obtained a random sample of queries from <br>web search</br> logs of a major search engine, with associated results and traces for user actions.",
                "We describe this dataset in detail next.",
                "Our metrics are described in Section 5.2 that we use to evaluate the ranking alternatives, listed in Section 5.3 in the experiments of Section 6. 5.1 Datasets We compared our ranking methods over a random sample of 3,000 queries from the search engine query logs.",
                "The queries were drawn from the logs uniformly at random by token without replacement, resulting in a query sample representative of the overall query distribution.",
                "On average, 30 results were explicitly labeled by human judges using a six point scale ranging from Perfect down to Bad.",
                "Overall, there were over 83,000 results with explicit relevance judgments.",
                "In order to compute various statistics, documents with label Good or better will be considered relevant, and with lower labels to be non-relevant.",
                "Note that the experiments were performed over the results already highly ranked by a <br>web search</br> engine, which corresponds to a typical user experience which is limited to the small number of the highly ranked results for a typical <br>web search</br> query.",
                "The user interactions were collected over a period of 8 weeks using voluntary opt-in information.",
                "In total, over 1.2 million unique queries were instrumented, resulting in over 12 million individual interactions with the search engine.",
                "The data consisted of user interactions with the <br>web search</br> engine (e.g., clicking on a result link, going back to search results, etc.) performed after a query was submitted.",
                "These actions were aggregated across users and search sessions and converted to features in Table 4.1.",
                "To create the training, validation, and test query sets, we created three different random splits of 1,500 training, 500 validation, and 1000 test queries.",
                "The splits were done randomly by query, so that there was no overlap in training, validation, and test queries. 5.2 Evaluation Metrics We evaluate the ranking algorithms over a range of accepted information retrieval metrics, namely Precision at K (P(K)), Normalized Discounted Cumulative Gain (NDCG), and Mean Average Precision (MAP).",
                "Each metric focuses on a deferent aspect of system performance, as we describe below. • Precision at K: As the most intuitive metric, P(K) reports the fraction of documents ranked in the top K results that are labeled as relevant.",
                "In our setting, we require a relevant document to be labeled Good or higher.",
                "The position of relevant documents within the top K is irrelevant, and hence this metric measure overall user satisfaction with the top K results. • NDCG at K: NDCG is a retrieval measure devised specifically for <br>web search</br> evaluation [10].",
                "For a given query q, the ranked results are examined from the top ranked down, and the NDCG computed as:   = +−= K j jr qq jMN 1 )( )1log(/)12( Where Mq is a normalization constant calculated so that a perfect ordering would obtain NDCG of 1; and each r(j) is an integer relevance label (0=Bad and 5=Perfect) of result returned at position j.",
                "Note that unlabeled and Bad documents do not contribute to the sum, but will reduce NDCG for the query pushing down the relevant labeled documents, reducing their contributions.",
                "NDCG is well suited to <br>web search</br> evaluation, as it rewards relevant documents in the top ranked results more heavily than those ranked lower. • MAP: Average precision for each query is defined as the mean of the precision at K values computed after each relevant document was retrieved.",
                "The final MAP value is defined as the mean of average precisions of all queries in the test set.",
                "This metric is the most commonly used single-value summary of a run over a set of queries. 5.3 Ranking Methods Compared Recall that our goal is to quantify the effectiveness of implicit behavior for real <br>web search</br>.",
                "One dimension is to compare the utility of implicit feedback with other information available to a <br>web search</br> engine.",
                "Specifically, we compare effectiveness of implicit user behaviors with content-based matching, static page quality features, and combinations of all features. • BM25F: As a strong <br>web search</br> baseline we used the BM25F scoring, which was used in one of the best performing systems in the TREC 2004 Web track [23,27].",
                "BM25F and its variants have been extensively described and evaluated in IR literature, and hence serve as a strong, reproducible baseline.",
                "The BM25F variant we used for our experiments computes separate match scores for each field for a result document (e.g., body text, title, and anchor text), and incorporates query-independent linkbased information (e.g., PageRank, ClickDistance, and URL depth).",
                "The scoring function and field-specific tuning is described in detail in [23].",
                "Note that BM25F does not directly consider explicit or implicit feedback for tuning. • RN: The ranking produced by a neural net ranker (RankNet, described in Section 3.3) that learns to rank <br>web search</br> results by incorporating BM25F and a large number of additional static and dynamic features describing each search result.",
                "This system automatically learns weights for all features (including the BM25F score for a document) based on explicit human labels for a large set of queries.",
                "A system incorporating an implementation of RankNet is currently in use by a major search engine and can be considered representative of the state of the art in <br>web search</br>. • BM25F-RerankCT: The ranking produced by incorporating clickthrough statistics to reorder <br>web search</br> results ranked by BM25F above.",
                "Clickthrough is a particularly important special case of implicit feedback, and has been shown to correlate with result relevance.",
                "This is a special case of the ranking method in Section 3.1, with the weight wI set to 1000 and the ranking Id is simply the number of clicks on the result corresponding to d. In effect, this ranking brings to the top all returned <br>web search</br> results with at least one click (and orders them in decreasing order by number of clicks).",
                "The relative ranking of the remainder of results is unchanged and they are inserted below all clicked results.",
                "This method serves as our baseline implicit feedback reranking method.",
                "BM25F-RerankAll The ranking produced by reordering the BM25F results using all user behavior features (Section 4).",
                "This method learns a model of user preferences by correlating feature values with explicit relevance labels using the RankNet neural net algorithm (Section 4.2).",
                "At runtime, for a given query the implicit score Ir is computed for each result r with available user interaction features, and the implicit ranking is produced.",
                "The merged ranking is computed as described in Section 3.1.",
                "Based on the experiments over the development set we fix the value of wI to 3 (the effect of the wI parameter for this ranker turned out to be negligible). • BM25F+All: Ranking derived by training the RankNet (Section 3.3) learner over the features set of the BM25F score as well as all implicit feedback features (Section 3.2).",
                "We used the 2-layer implementation of RankNet [5] trained on the queries and labels in the training and validation sets. • RN+All: Ranking derived by training the 2-layer RankNet ranking algorithm (Section 3.3) over the union of all content, dynamic, and implicit feedback features (i.e., all of the features described above as well as all of the new implicit feedback features we introduced).",
                "The ranking methods above span the range of the information used for ranking, from not using the implicit or explicit feedback at all (i.e., BM25F) to a modern <br>web search</br> engine using hundreds of features and tuned on explicit judgments (RN).",
                "As we will show next, incorporating user behavior into these ranking systems dramatically improves the relevance of the returned documents. 6.",
                "EXPERIMENTAL RESULTS Implicit feedback for <br>web search</br> ranking can be exploited in a number of ways.",
                "We compare alternative methods of exploiting implicit feedback, both by re-ranking the top results (i.e., the BM25F-RerankCT and BM25F-RerankAll methods that reorder BM25F results), as well as by integrating the implicit features directly into the ranking process (i.e., the RN+ALL and BM25F+All methods which learn to rank results over the implicit feedback and other features).",
                "We compare our methods over strong baselines (BM25F and RN) over the NDCG, Precision at K, and MAP measures defined in Section 5.2.",
                "The results were averaged over three random splits of the overall dataset.",
                "Each split contained 1500 training, 500 validation, and 1000 test queries, all query sets disjoint.",
                "We first present the results over all 1000 test queries (i.e., including queries for which there are no implicit measures so we use the original web rankings).",
                "We then drill down to examine the effects on reranking for the attempted queries in more detail, analyzing where implicit feedback proved most beneficial.",
                "We first experimented with different methods of re-ranking the output of the BM25F search results.",
                "Figures 6.1 and 6.2 report NDCG and Precision for BM25F, as well as for the strategies reranking results with user feedback (Section 3.1).",
                "Incorporating all user feedback (either in reranking framework or as features to the learner directly) results in significant improvements (using two-tailed t-test with p=0.01) over both the original BM25F ranking as well as over reranking with clickthrough alone.",
                "The improvement is consistent across the top 10 results and largest for the top result: NDCG at 1 for BM25F+All is 0.622 compared to 0.518 of the original results, and precision at 1 similarly increases from 0.5 to 0.63.",
                "Based on these results we will use the direct feature combination (i.e., BM25F+All) ranker for subsequent comparisons involving implicit feedback. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 1 2 3 4 5 6 7 8 9 10K NDCG BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.1: NDCG at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K 0.35 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.2: Precision at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K Interestingly, using clickthrough alone, while giving significant benefit over the original BM25F ranking, is not as effective as considering the full set of features in Table 4.1.",
                "While we analyze user behavior (and most effective component features) in a separate paper [1], it is worthwhile to give a concrete example of the kind of noise inherent in real user feedback in <br>web search</br> setting. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 Result position Relativeclickfrequency PTR=2 PTR=3 PTR=5 Figure 6.3: Relative clickthrough frequency for queries with varying Position of Top Relevant result (PTR).",
                "If users considered only the relevance of a result to their query, they would click on the topmost relevant results.",
                "Unfortunately, as Joachims and others have shown, presentation also influences which results users click on quite dramatically.",
                "Users often click on results above the relevant one presumably because the short summaries do not provide enough information to make accurate relevance assessments and they have learned that on average topranked items are relevant.",
                "Figure 6.3 shows relative clickthrough frequencies for queries with known relevant items at positions other than the first position; the position of the top relevant result (PTR) ranges from 2-10 in the figure.",
                "For example, for queries with first relevant result at position 5 (PTR=5), there are more clicks on the non-relevant results in higher ranked positions than on the first relevant result at position 5.",
                "As we will see, learning over a richer behavior feature set, results in substantial accuracy improvement over clickthrough alone.",
                "We now consider incorporating user behavior into a much richer feature set, RN (Section 5.3) used by a major <br>web search</br> engine.",
                "RN incorporates BM25F, link-based features, and hundreds of other features.",
                "Figure 6.4 reports NDCG at K and Figure 6.5 reports Precision at K. Interestingly, while the original RN rankings are significantly more accurate than BM25F alone, incorporating implicit feedback features (BM25F+All) results in ranking that significantly outperforms the original RN rankings.",
                "In other words, implicit feedback incorporates sufficient information to replace the hundreds of other features available to the RankNet learner trained on the RN feature set. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 1 2 3 4 5 6 7 8 9 10K NDCG RN RN+All BM25 BM25+All Figure 6.4: NDCG at K for BM25F, BM25F+All, RN, and RN+All for varying K Furthermore, enriching the RN features with implicit feedback set exhibits significant gain on all measures, allowing RN+All to outperform all other methods.",
                "This demonstrates the complementary nature of implicit feedback with other features available to a state of the art <br>web search</br> engine. 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision RN RN+All BM25 BM25+All Figure 6.5: Precision at K for BM25F, BM25F+All, RN, and RN+All for varying K We summarize the performance of the different ranking methods in Table 6.1.",
                "We report the Mean Average Precision (MAP) score for each system.",
                "While not intuitive to interpret, MAP allows quantitative comparison on a single metric.",
                "The gains marked with * are significant at p=0.01 level using two tailed t-test.",
                "MAP Gain P(1) Gain BM25F 0.184 - 0.503BM25F-Rerank-CT 0.215 0.031* 0.577 0.073* BM25F-RerankImplicit 0.218 0.003 0.605 0.028* BM25F+Implicit 0.222 0.004 0.620 0.015* RN 0.215 - 0.597RN+All 0.248 0.033* 0.629 0.032* Table 6.1: Mean Average Precision (MAP) for all strategies.",
                "So far we reported results averaged across all queries in the test set.",
                "Unfortunately, less than half had sufficient interactions to attempt reranking.",
                "Out of the 1000 queries in test, between 46% and 49%, depending on the train-test split, had sufficient interaction information to make predictions (i.e., there was at least 1 search session in which at least 1 result URL was clicked on by the user).",
                "This is not surprising: <br>web search</br> is heavy-tailed, and there are many unique queries.",
                "We now consider the performance on the queries for which user interactions were available.",
                "Figure 6.6 reports NDCG for the subset of the test queries with the implicit feedback features.",
                "The gains at top 1 are dramatic.",
                "The NDCG at 1 of BM25F+All increases from 0.6 to 0.75 (a 31% relative gain), achieving performance comparable to RN+All operating over a much richer feature set. 0.6 0.65 0.7 0.75 0.8 1 3 5 10K NDCG RN RN+All BM25 BM25+All Figure 6.6: NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions Similarly, gains on precision at top 1 are substantial (Figure 6.7), and are likely to be apparent to <br>web search</br> users.",
                "When implicit feedback is available, the BM25F+All system returns relevant document at top 1 almost 70% of the time, compared 53% of the time when implicit feedback is not considered by the original BM25F system. 0.45 0.5 0.55 0.6 0.65 0.7 1 3 5 10K Precision RN RN+All BM25 BM25+All Figure 6.7: Precision at K NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions We summarize the results on the MAP measure for attempted queries in Table 6.2.",
                "MAP improvements are both substantial and significant, with improvements over the BM25F ranker most pronounced.",
                "Method MAP Gain P(1) Gain RN 0.269 0.632 RN+All 0.321 0.051 (19%) 0.693 0.061(10%) BM25F 0.236 0.525 BM25F+All 0.292 0.056 (24%) 0.687 0.162 (31%) Table 6.2: Mean Average Precision (MAP) on attempted queries for best performing methods We now analyze the cases where implicit feedback was shown most helpful.",
                "Figure 6.8 reports the MAP improvements over the baseline BM25F run for each query with MAP under 0.6.",
                "Note that most of the improvement is for poorly performing queries (i.e., MAP < 0.1).",
                "Interestingly, incorporating user behavior information degrades accuracy for queries with high original MAP score.",
                "One possible explanation is that these easy queries tend to be navigational (i.e., having a single, highly-ranked most appropriate answer), and user interactions with lower-ranked results may indicate divergent information needs that are better served by the less popular results (with correspondingly poor overall relevance ratings). 0 50 100 150 200 250 300 350 0.1 0.2 0.3 0.4 0.5 0.6 -0.4 -0.35 -0.3 -0.25 -0.2 -0.15 -0.1 -0.05 0 0.05 0.1 0.15 0.2 Frequency Average Gain Figure 6.8: Gain of BM25F+All over original BM25F ranking To summarize our experimental results, incorporating implicit feedback in real <br>web search</br> setting resulted in significant improvements over the original rankings, using both BM25F and RN baselines.",
                "Our rich set of implicit features, such as time on page and deviations from the average behavior, provides advantages over using clickthrough alone as an indicator of interest.",
                "Furthermore, incorporating implicit feedback features directly into the learned ranking function is more effective than using implicit feedback for reranking.",
                "The improvements observed over large test sets of queries (1,000 total, between 466 and 495 with implicit feedback available) are both substantial and statistically significant. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper we explored the utility of incorporating noisy implicit feedback obtained in a real <br>web search</br> setting to improve <br>web search</br> ranking.",
                "We performed a large-scale evaluation over 3,000 queries and more than 12 million user interactions with a major search engine, establishing the utility of incorporating noisy implicit feedback to improve <br>web search</br> relevance.",
                "We compared two alternatives of incorporating implicit feedback into the search process, namely reranking with implicit feedback and incorporating implicit feedback features directly into the trained ranking function.",
                "Our experiments showed significant improvement over methods that do not consider implicit feedback.",
                "The gains are particularly dramatic for the top K=1 result in the final ranking, with precision improvements as high as 31%, and the gains are substantial for all values of K. Our experiments showed that implicit user feedback can further improve <br>web search</br> performance, when incorporated directly with popular content- and link-based features.",
                "Interestingly, implicit feedback is particularly valuable for queries with poor original ranking of results (e.g., MAP lower than 0.1).",
                "One promising direction for future work is to apply recent research on automatically predicting query difficulty, and only attempt to incorporate implicit feedback for the difficult queries.",
                "As another research direction we are exploring methods for extending our predictions to the previously unseen queries (e.g., query clustering), which should further improve the <br>web search</br> experience of users.",
                "ACKNOWLEDGMENTS We thank Chris Burges and Matt Richardson for an implementation of RankNet for our experiments.",
                "We also thank Robert Ragno for his valuable suggestions and many discussions. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, S. Dumais, and R.Ragno, Learning User Interaction Models for Predicting <br>web search</br> Result Preferences.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan, HARD Track Overview in TREC 2003, High Accuracy Retrieval from Documents, 2003 [3] R. Baeza-Yates and B. Ribeiro-Neto, Modern Information Retrieval, Addison-Wesley, 1999. [4] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual <br>web search</br> Engine, in Proceedings of WWW, 1997 [5] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning, 2005 [6] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [7] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest.",
                "IEEE Internet Computing. 2001 [8] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [9] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [10] K Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2000 [11] T. Joachims, Optimizing Search Engines Using Clickthrough Data.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [12] T. Joachims, L. Granka, B. Pang, H. Hembrooke, and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [13] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [14] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [15] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [16] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [17] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems.",
                "In Proceedings of the AAAI Workshop on Recommender Systems. 1998 [18] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [19] N. Pharo, N. and K. Järvelin.",
                "The SST method: a tool for analyzing web information search processes.",
                "In Information Processing & Management, 2004 [20] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [21] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (SIGKDD), 2005. [22] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in Proceedings of the ICML Workshop on Learning in <br>web search</br>, 2005 [23] S. E. Robertson, H. Zaragoza, and M. Taylor, Simple BM25 extension to multiple weighted fields, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [24] G. Salton & M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [25] E.M. Voorhees, D. Harman, Overview of TREC, 2001 [26] G.R.",
                "Xue, H.J.",
                "Zeng, Z. Chen, Y. Yu, W.Y.",
                "Ma, W.S.",
                "Xi, and W.G.",
                "Fan, Optimizing <br>web search</br> using web clickthrough data, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [27] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
                "Microsoft Cambridge at TREC 13: Web and Hard Tracks.",
                "In Proceedings of TREC 2004"
            ],
            "original_annotated_samples": [
                "Improving <br>web search</br> Ranking by Incorporating User Behavior Information Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com ABSTRACT We show that incorporating user behavior data can significantly improve ordering of top results in real <br>web search</br> setting.",
                "We examine alternatives for incorporating feedback into the ranking process and explore the contributions of user feedback compared to other common <br>web search</br> features.",
                "We report results of a large scale evaluation over 3,000 queries and 12 million user interactions with a popular <br>web search</br> engine.",
                "We show that incorporating implicit feedback can augment other features, improving the accuracy of a competitive <br>web search</br> ranking algorithms by as much as 31% relative to the original performance.",
                "These interactions can serve as a valuable source of information for tuning and improving <br>web search</br> result ranking and can compliment more costly explicit judgments."
            ],
            "translated_annotated_samples": [
                "Mejorando la clasificación de <br>búsqueda web</br> al incorporar información sobre el comportamiento del usuario. Eugene Agichtein, Microsoft Research eugeneag@microsoft.com Eric Brill, Microsoft Research brill@microsoft.com Susan Dumais, Microsoft Research sdumais@microsoft.com RESUMEN Mostramos que la incorporación de datos sobre el comportamiento del usuario puede mejorar significativamente el orden de los principales resultados en una configuración real de <br>búsqueda web</br>.",
                "Examinamos alternativas para incorporar retroalimentación en el proceso de clasificación y exploramos las contribuciones de la retroalimentación de usuarios en comparación con otras características comunes de <br>búsqueda en la web</br>.",
                "Informamos los resultados de una evaluación a gran escala con más de 3,000 consultas y 12 millones de interacciones de usuarios con un motor de <br>búsqueda web</br> popular.",
                "Mostramos que la incorporación de retroalimentación implícita puede aumentar otras características, mejorando la precisión de los algoritmos de clasificación de <br>búsqueda web</br> competitivos hasta en un 31% en comparación con el rendimiento original.",
                "Estas interacciones pueden servir como una valiosa fuente de información para ajustar y mejorar la clasificación de los resultados de <br>búsqueda en la web</br> y pueden complementar juicios explícitos más costosos."
            ],
            "translated_text": "Mejorando la clasificación de <br>búsqueda web</br> al incorporar información sobre el comportamiento del usuario. Eugene Agichtein, Microsoft Research eugeneag@microsoft.com Eric Brill, Microsoft Research brill@microsoft.com Susan Dumais, Microsoft Research sdumais@microsoft.com RESUMEN Mostramos que la incorporación de datos sobre el comportamiento del usuario puede mejorar significativamente el orden de los principales resultados en una configuración real de <br>búsqueda web</br>. Examinamos alternativas para incorporar retroalimentación en el proceso de clasificación y exploramos las contribuciones de la retroalimentación de usuarios en comparación con otras características comunes de <br>búsqueda en la web</br>. Informamos los resultados de una evaluación a gran escala con más de 3,000 consultas y 12 millones de interacciones de usuarios con un motor de <br>búsqueda web</br> popular. Mostramos que la incorporación de retroalimentación implícita puede aumentar otras características, mejorando la precisión de los algoritmos de clasificación de <br>búsqueda web</br> competitivos hasta en un 31% en comparación con el rendimiento original. Categorías y Descriptores de Asignaturas H.3.3 Búsqueda y Recuperación de Información - Retroalimentación de relevancia, proceso de búsqueda; H.3.5 Servicios de Información en Línea - Servicios basados en la web. Términos generales Algoritmos, Medición, Experimentación 1. INTRODUCCIÓN Millones de usuarios interactúan con los motores de búsqueda a diario. Ellos emiten consultas, siguen algunos de los enlaces en los resultados, hacen clic en anuncios, pasan tiempo en páginas, reformulan sus consultas y realizan otras acciones. Estas interacciones pueden servir como una valiosa fuente de información para ajustar y mejorar la clasificación de los resultados de <br>búsqueda en la web</br> y pueden complementar juicios explícitos más costosos. ",
            "candidates": [],
            "error": [
                [
                    "búsqueda web",
                    "búsqueda web",
                    "búsqueda en la web",
                    "búsqueda web",
                    "búsqueda web",
                    "búsqueda en la web"
                ]
            ]
        },
        "ranking": {
            "translated_key": "clasificación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Improving Web Search <br>ranking</br> by Incorporating User Behavior Information Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com ABSTRACT We show that incorporating user behavior data can significantly improve ordering of top results in real web search setting.",
                "We examine alternatives for incorporating feedback into the <br>ranking</br> process and explore the contributions of user feedback compared to other common web search features.",
                "We report results of a large scale evaluation over 3,000 queries and 12 million user interactions with a popular web search engine.",
                "We show that incorporating implicit feedback can augment other features, improving the accuracy of a competitive web search <br>ranking</br> algorithms by as much as 31% relative to the original performance.",
                "Categories and Subject Descriptors H.3.3 Information Search and Retrieval - Relevance feedback, search process; H.3.5 Online Information Services - Web-based services.",
                "General Terms Algorithms, Measurement, Experimentation 1.",
                "INTRODUCTION Millions of users interact with search engines daily.",
                "They issue queries, follow some of the links in the results, click on ads, spend time on pages, reformulate their queries, and perform other actions.",
                "These interactions can serve as a valuable source of information for tuning and improving web search result <br>ranking</br> and can compliment more costly explicit judgments.",
                "Implicit relevance feedback for <br>ranking</br> and personalization has become an active area of research.",
                "Recent work by Joachims and others exploring implicit feedback in controlled environments have shown the value of incorporating implicit feedback into the <br>ranking</br> process.",
                "Our motivation for this work is to understand how implicit feedback can be used in a large-scale operational environment to improve retrieval.",
                "How does it compare to and compliment evidence from page content, anchor text, or link-based features such as inlinks or PageRank?",
                "While it is intuitive that user interactions with the web search engine should reveal at least some information that could be used for <br>ranking</br>, estimating user preferences in real web search settings is a challenging problem, since real user interactions tend to be more noisy than commonly assumed in the controlled settings of previous studies.",
                "Our paper explores whether implicit feedback can be helpful in realistic environments, where user feedback can be noisy (or adversarial) and a web search engine already uses hundreds of features and is heavily tuned.",
                "To this end, we explore different approaches for <br>ranking</br> web search results using real user behavior obtained as part of normal interactions with the web search engine.",
                "The specific contributions of this paper include: • Analysis of alternatives for incorporating user behavior into web search <br>ranking</br> (Section 3). • An application of a robust implicit feedback model derived from mining millions of user interactions with a major web search engine (Section 4). • A large scale evaluation over real user queries and search results, showing significant improvements derived from incorporating user feedback (Section 6).",
                "We summarize our findings and discuss extensions to the current work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK <br>ranking</br> search results is a fundamental problem in information retrieval.",
                "Most common approaches primarily focus on similarity of query and a page, as well as the overall page quality [3,4,24].",
                "However, with increasing popularity of search engines, implicit feedback (i.e., the actions users take when interacting with the search engine) can be used to improve the rankings.",
                "Implicit relevance measures have been studied by several research groups.",
                "An overview of implicit measures is compiled in Kelly and Teevan [14].",
                "This research, while developing valuable insights into implicit relevance measures, was not applied to improve the <br>ranking</br> of web search results in realistic settings.",
                "Closely related to our work, Joachims [11] collected implicit measures in place of explicit measures, introducing a technique based entirely on clickthrough data to learn <br>ranking</br> functions.",
                "Fox et al. [8] explored the relationship between implicit and explicit measures in Web search, and developed Bayesian models to correlate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "This work considered a wide range of user behaviors (e.g., dwell time, scroll time, reformulation patterns) in addition to the popular clickthrough behavior.",
                "However, the modeling effort was aimed at predicting explicit relevance judgments from implicit user actions and not specifically at learning <br>ranking</br> functions.",
                "Other studies of user behavior in web search include Pharo and Järvelin [19], but were not directly applied to improve <br>ranking</br>.",
                "More recently, Joachims et al. [12] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthroughs in a controlled, laboratory setting.",
                "Unfortunately, the extent to which previous research applies to real-world web search is unclear.",
                "At the same time, while recent work (e.g., [26]) on using clickthrough information for improving web search <br>ranking</br> is promising, it captures only one aspect of the user interactions with web search engines.",
                "We build on existing research to develop robust user behavior interpretation techniques for the real web search setting.",
                "Instead of treating each user as a reliable expert, we aggregate information from multiple, unreliable, user search session traces, as we describe in the next two sections. 3.",
                "INCORPORATING IMPLICIT FEEDBACK We consider two complementary approaches to <br>ranking</br> with implicit feedback: (1) treating implicit feedback as independent evidence for <br>ranking</br> results, and (2) integrating implicit feedback features directly into the ranking algorithm.",
                "We describe the two general <br>ranking</br> approaches next.",
                "The specific implicit feedback features are described in Section 4, and the algorithms for interpreting and incorporating implicit feedback are described in Section 5. 3.1 Implicit Feedback as Independent Evidence The general approach is to re-rank the results obtained by a web search engine according to observed clickthrough and other user interactions for the query in previous search sessions.",
                "Each result is assigned a score according to expected relevance/user satisfaction based on previous interactions, resulting in some preference ordering based on user interactions alone.",
                "While there has been significant work on merging multiple rankings, we adapt a simple and robust approach of ignoring the original rankers scores, and instead simply merge the rank orders.",
                "The main reason for ignoring the original scores is that since the feature spaces and learning algorithms are different, the scores are not directly comparable, and re-normalization tends to remove the benefit of incorporating classifier scores.",
                "We experimented with a variety of merging functions on the development set of queries (and using a set of interactions from a different time period from final evaluation sets).",
                "We found that a simple rank merging heuristic combination works well, and is robust to variations in score values from original rankers.",
                "For a given query q, the implicit score ISd is computed for each result d from available user interaction features, resulting in the implicit rank Id for each result.",
                "We compute a merged score SM(d) for d by combining the ranks obtained from implicit feedback, Id with the original rank of d, Od:     ¡     ¢ £ + + + + = otherwise O dforexistsfeedbackimplicitif OI w wOIdS d dd I IddM 1 1 1 1 1 1 ),,,( where the weight wI is a heuristically tuned scaling factor representing the relative importance of the implicit feedback.",
                "The query results are ordered in by decreasing values of SM to produce the final <br>ranking</br>.",
                "One special case of this model arises when setting wI to a very large value, effectively forcing clicked results to be ranked higher than un-clicked results - an intuitive and effective heuristic that we will use as a baseline.",
                "Applying more sophisticated classifier and ranker combination algorithms may result in additional improvements, and is a promising direction for future work.",
                "The approach above assumes that there are no interactions between the underlying features producing the original web search <br>ranking</br> and the implicit feedback features.",
                "We now relax this assumption by integrating implicit feedback features directly into the <br>ranking</br> process. 3.2 <br>ranking</br> with Implicit Feedback Features Modern web search engines rank results based on a large number of features, including content-based features (i.e., how closely a query matches the text or title or anchor text of the document), and query-independent page quality features (e.g., PageRank of the document or the domain).",
                "In most cases, automatic (or semiautomatic) methods are developed for tuning the specific <br>ranking</br> function that combines these feature values.",
                "Hence, a natural approach is to incorporate implicit feedback features directly as features for the <br>ranking</br> algorithm.",
                "During training or tuning, the ranker can be tuned as before but with additional features.",
                "At runtime, the search engine would fetch the implicit feedback features associated with each query-result URL pair.",
                "This model requires a <br>ranking</br> algorithm to be robust to missing values: more than 50% of queries to web search engines are unique, with no previous implicit feedback available.",
                "We now describe such a ranker that we used to learn over the combined feature sets including implicit feedback. 3.3 Learning to Rank Web Search Results A key aspect of our approach is exploiting recent advances in machine learning, namely trainable <br>ranking</br> algorithms for web search and information retrieval (e.g., [5, 11] and classical results reviewed in [3]).",
                "In our setting, explicit human relevance judgments (labels) are available for a set of web search queries and results.",
                "Hence, an attractive choice to use is a supervised machine learning technique to learn a <br>ranking</br> function that best predicts relevance judgments.",
                "RankNet is one such algorithm.",
                "It is a neural net tuning algorithm that optimizes feature weights to best match explicitly provided pairwise user preferences.",
                "While the specific training algorithms used by RankNet are beyond the scope of this paper, it is described in detail in [5] and includes extensive evaluation and comparison with other <br>ranking</br> methods.",
                "An attractive feature of RankNet is both train- and run-time efficiency - runtime <br>ranking</br> can be quickly computed and can scale to the web, and training can be done over thousands of queries and associated judged results.",
                "We use a 2-layer implementation of RankNet in order to model non-linear relationships between features.",
                "Furthermore, RankNet can learn with many (differentiable) cost functions, and hence can automatically learn a <br>ranking</br> function from human-provided labels, an attractive alternative to heuristic feature combination techniques.",
                "Hence, we will also use RankNet as a generic ranker to explore the contribution of implicit feedback for different <br>ranking</br> alternatives. 4.",
                "IMPLICIT USER FEEDBACK MODEL Our goal is to accurately interpret noisy user feedback obtained as by tracing user interactions with the search engine.",
                "Interpreting implicit feedback in real web search setting is not an easy task.",
                "We characterize this problem in detail in [1], where we motivate and evaluate a wide variety of models of implicit user activities.",
                "The general approach is to represent user actions for each search result as a vector of features, and then train a ranker on these features to discover feature values indicative of relevant (and nonrelevant) search results.",
                "We first briefly summarize our features and model, and the learning approach (Section 4.2) in order to provide sufficient information to replicate our <br>ranking</br> methods and the subsequent experiments. 4.1 Representing User Actions as Features We model observed web search behaviors as a combination of a ``background component (i.e., query- and relevance-independent noise in user behavior, including positional biases with result interactions), and a ``relevance component (i.e., query-specific behavior indicative of relevance of a result to a query).",
                "We design our features to take advantage of aggregated user behavior.",
                "The feature set is comprised of directly observed features (computed directly from observations for each query), as well as queryspecific derived features, computed as the deviation from the overall query-independent distribution of values for the corresponding directly observed feature values.",
                "The features used to represent user interactions with web search results are summarized in Table 4.1.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "We include the traditional implicit feedback features such as clickthrough counts for the results, as well as our novel derived features such as the deviation of the observed clickthrough number for a given query-URL pair from the expected number of clicks on a result in the given position.",
                "We also model the browsing behavior after a result was clicked - e.g., the average page dwell time for a given query-URL pair, as well as its deviation from the expected (average) dwell time.",
                "Furthermore, the feature set was designed to provide essential information about the user experience to make feedback interpretation robust.",
                "For example, web search users can often determine whether a result is relevant by looking at the result title, URL, and summary - in many cases, looking at the original document is not necessary.",
                "To model this aspect of user experience we include features such as overlap in words in title and words in query (TitleOverlap) and the fraction of words shared by the query and the result summary.",
                "Clickthrough features Position Position of the URL in Current <br>ranking</br> ClickFrequency Number of clicks for this query, URL pair ClickProbability Probability of a click for this query and URL ClickDeviation Deviation from expected click probability IsNextClicked 1 if clicked on next position, 0 otherwise IsPreviousClicked 1 if clicked on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, no parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from average dwell time on page CumulativeDeviation Deviation from average cumulative dwell time DomainDeviation Deviation from average dwell time on domain Query-text features TitleOverlap Words shared between query and title SummaryOverlap Words shared between query and snippet QueryURLOverlap Words shared between query and URL QueryDomainOverlap Words shared between query and URL domain QueryLength Number of tokens in query QueryNextOverlap Fraction of words shared with next query Table 4.1: Some features used to represent post-search navigation history for a given query and search result URL.",
                "Having described our feature set, we briefly review our general method for deriving a user behavior model. 4.2 Deriving a User Feedback Model To learn to interpret the observed user behavior, we correlate user actions (i.e., the features in Table 4.1 representing the actions) with the explicit user judgments for a set of training queries.",
                "We find all the instances in our session logs where these queries were submitted to the search engine, and aggregate the user behavior features for all search sessions involving these queries.",
                "Each observed query-URL pair is represented by the features in Table 4.1, with values averaged over all search sessions, and assigned one of six possible relevance labels, ranging from Perfect to Bad, as assigned by explicit relevance judgments.",
                "These labeled feature vectors are used as input to the RankNet training algorithm (Section 3.3) which produces a trained user behavior model.",
                "This approach is particularly attractive as it does not require heuristics beyond feature engineering.",
                "The resulting user behavior model is used to help rank web search resultseither directly or in combination with other features, as described below. 5.",
                "EXPERIMENTAL SETUP The ultimate goal of incorporating implicit feedback into <br>ranking</br> is to improve the relevance of the returned web search results.",
                "Hence, we compare the <br>ranking</br> methods over a large set of judged queries with explicit relevance labels provided by human judges.",
                "In order for the evaluation to be realistic we obtained a random sample of queries from web search logs of a major search engine, with associated results and traces for user actions.",
                "We describe this dataset in detail next.",
                "Our metrics are described in Section 5.2 that we use to evaluate the <br>ranking</br> alternatives, listed in Section 5.3 in the experiments of Section 6. 5.1 Datasets We compared our <br>ranking</br> methods over a random sample of 3,000 queries from the search engine query logs.",
                "The queries were drawn from the logs uniformly at random by token without replacement, resulting in a query sample representative of the overall query distribution.",
                "On average, 30 results were explicitly labeled by human judges using a six point scale ranging from Perfect down to Bad.",
                "Overall, there were over 83,000 results with explicit relevance judgments.",
                "In order to compute various statistics, documents with label Good or better will be considered relevant, and with lower labels to be non-relevant.",
                "Note that the experiments were performed over the results already highly ranked by a web search engine, which corresponds to a typical user experience which is limited to the small number of the highly ranked results for a typical web search query.",
                "The user interactions were collected over a period of 8 weeks using voluntary opt-in information.",
                "In total, over 1.2 million unique queries were instrumented, resulting in over 12 million individual interactions with the search engine.",
                "The data consisted of user interactions with the web search engine (e.g., clicking on a result link, going back to search results, etc.) performed after a query was submitted.",
                "These actions were aggregated across users and search sessions and converted to features in Table 4.1.",
                "To create the training, validation, and test query sets, we created three different random splits of 1,500 training, 500 validation, and 1000 test queries.",
                "The splits were done randomly by query, so that there was no overlap in training, validation, and test queries. 5.2 Evaluation Metrics We evaluate the <br>ranking</br> algorithms over a range of accepted information retrieval metrics, namely Precision at K (P(K)), Normalized Discounted Cumulative Gain (NDCG), and Mean Average Precision (MAP).",
                "Each metric focuses on a deferent aspect of system performance, as we describe below. • Precision at K: As the most intuitive metric, P(K) reports the fraction of documents ranked in the top K results that are labeled as relevant.",
                "In our setting, we require a relevant document to be labeled Good or higher.",
                "The position of relevant documents within the top K is irrelevant, and hence this metric measure overall user satisfaction with the top K results. • NDCG at K: NDCG is a retrieval measure devised specifically for web search evaluation [10].",
                "For a given query q, the ranked results are examined from the top ranked down, and the NDCG computed as:   = +−= K j jr qq jMN 1 )( )1log(/)12( Where Mq is a normalization constant calculated so that a perfect ordering would obtain NDCG of 1; and each r(j) is an integer relevance label (0=Bad and 5=Perfect) of result returned at position j.",
                "Note that unlabeled and Bad documents do not contribute to the sum, but will reduce NDCG for the query pushing down the relevant labeled documents, reducing their contributions.",
                "NDCG is well suited to web search evaluation, as it rewards relevant documents in the top ranked results more heavily than those ranked lower. • MAP: Average precision for each query is defined as the mean of the precision at K values computed after each relevant document was retrieved.",
                "The final MAP value is defined as the mean of average precisions of all queries in the test set.",
                "This metric is the most commonly used single-value summary of a run over a set of queries. 5.3 <br>ranking</br> Methods Compared Recall that our goal is to quantify the effectiveness of implicit behavior for real web search.",
                "One dimension is to compare the utility of implicit feedback with other information available to a web search engine.",
                "Specifically, we compare effectiveness of implicit user behaviors with content-based matching, static page quality features, and combinations of all features. • BM25F: As a strong web search baseline we used the BM25F scoring, which was used in one of the best performing systems in the TREC 2004 Web track [23,27].",
                "BM25F and its variants have been extensively described and evaluated in IR literature, and hence serve as a strong, reproducible baseline.",
                "The BM25F variant we used for our experiments computes separate match scores for each field for a result document (e.g., body text, title, and anchor text), and incorporates query-independent linkbased information (e.g., PageRank, ClickDistance, and URL depth).",
                "The scoring function and field-specific tuning is described in detail in [23].",
                "Note that BM25F does not directly consider explicit or implicit feedback for tuning. • RN: The <br>ranking</br> produced by a neural net ranker (RankNet, described in Section 3.3) that learns to rank web search results by incorporating BM25F and a large number of additional static and dynamic features describing each search result.",
                "This system automatically learns weights for all features (including the BM25F score for a document) based on explicit human labels for a large set of queries.",
                "A system incorporating an implementation of RankNet is currently in use by a major search engine and can be considered representative of the state of the art in web search. • BM25F-RerankCT: The <br>ranking</br> produced by incorporating clickthrough statistics to reorder web search results ranked by BM25F above.",
                "Clickthrough is a particularly important special case of implicit feedback, and has been shown to correlate with result relevance.",
                "This is a special case of the <br>ranking</br> method in Section 3.1, with the weight wI set to 1000 and the <br>ranking</br> Id is simply the number of clicks on the result corresponding to d. In effect, this ranking brings to the top all returned web search results with at least one click (and orders them in decreasing order by number of clicks).",
                "The relative <br>ranking</br> of the remainder of results is unchanged and they are inserted below all clicked results.",
                "This method serves as our baseline implicit feedback reranking method.",
                "BM25F-RerankAll The <br>ranking</br> produced by reordering the BM25F results using all user behavior features (Section 4).",
                "This method learns a model of user preferences by correlating feature values with explicit relevance labels using the RankNet neural net algorithm (Section 4.2).",
                "At runtime, for a given query the implicit score Ir is computed for each result r with available user interaction features, and the implicit <br>ranking</br> is produced.",
                "The merged <br>ranking</br> is computed as described in Section 3.1.",
                "Based on the experiments over the development set we fix the value of wI to 3 (the effect of the wI parameter for this ranker turned out to be negligible). • BM25F+All: <br>ranking</br> derived by training the RankNet (Section 3.3) learner over the features set of the BM25F score as well as all implicit feedback features (Section 3.2).",
                "We used the 2-layer implementation of RankNet [5] trained on the queries and labels in the training and validation sets. • RN+All: <br>ranking</br> derived by training the 2-layer RankNet <br>ranking</br> algorithm (Section 3.3) over the union of all content, dynamic, and implicit feedback features (i.e., all of the features described above as well as all of the new implicit feedback features we introduced).",
                "The <br>ranking</br> methods above span the range of the information used for <br>ranking</br>, from not using the implicit or explicit feedback at all (i.e., BM25F) to a modern web search engine using hundreds of features and tuned on explicit judgments (RN).",
                "As we will show next, incorporating user behavior into these <br>ranking</br> systems dramatically improves the relevance of the returned documents. 6.",
                "EXPERIMENTAL RESULTS Implicit feedback for web search <br>ranking</br> can be exploited in a number of ways.",
                "We compare alternative methods of exploiting implicit feedback, both by re-<br>ranking</br> the top results (i.e., the BM25F-RerankCT and BM25F-RerankAll methods that reorder BM25F results), as well as by integrating the implicit features directly into the <br>ranking</br> process (i.e., the RN+ALL and BM25F+All methods which learn to rank results over the implicit feedback and other features).",
                "We compare our methods over strong baselines (BM25F and RN) over the NDCG, Precision at K, and MAP measures defined in Section 5.2.",
                "The results were averaged over three random splits of the overall dataset.",
                "Each split contained 1500 training, 500 validation, and 1000 test queries, all query sets disjoint.",
                "We first present the results over all 1000 test queries (i.e., including queries for which there are no implicit measures so we use the original web rankings).",
                "We then drill down to examine the effects on reranking for the attempted queries in more detail, analyzing where implicit feedback proved most beneficial.",
                "We first experimented with different methods of re-<br>ranking</br> the output of the BM25F search results.",
                "Figures 6.1 and 6.2 report NDCG and Precision for BM25F, as well as for the strategies reranking results with user feedback (Section 3.1).",
                "Incorporating all user feedback (either in reranking framework or as features to the learner directly) results in significant improvements (using two-tailed t-test with p=0.01) over both the original BM25F <br>ranking</br> as well as over reranking with clickthrough alone.",
                "The improvement is consistent across the top 10 results and largest for the top result: NDCG at 1 for BM25F+All is 0.622 compared to 0.518 of the original results, and precision at 1 similarly increases from 0.5 to 0.63.",
                "Based on these results we will use the direct feature combination (i.e., BM25F+All) ranker for subsequent comparisons involving implicit feedback. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 1 2 3 4 5 6 7 8 9 10K NDCG BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.1: NDCG at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K 0.35 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.2: Precision at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K Interestingly, using clickthrough alone, while giving significant benefit over the original BM25F <br>ranking</br>, is not as effective as considering the full set of features in Table 4.1.",
                "While we analyze user behavior (and most effective component features) in a separate paper [1], it is worthwhile to give a concrete example of the kind of noise inherent in real user feedback in web search setting. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 Result position Relativeclickfrequency PTR=2 PTR=3 PTR=5 Figure 6.3: Relative clickthrough frequency for queries with varying Position of Top Relevant result (PTR).",
                "If users considered only the relevance of a result to their query, they would click on the topmost relevant results.",
                "Unfortunately, as Joachims and others have shown, presentation also influences which results users click on quite dramatically.",
                "Users often click on results above the relevant one presumably because the short summaries do not provide enough information to make accurate relevance assessments and they have learned that on average topranked items are relevant.",
                "Figure 6.3 shows relative clickthrough frequencies for queries with known relevant items at positions other than the first position; the position of the top relevant result (PTR) ranges from 2-10 in the figure.",
                "For example, for queries with first relevant result at position 5 (PTR=5), there are more clicks on the non-relevant results in higher ranked positions than on the first relevant result at position 5.",
                "As we will see, learning over a richer behavior feature set, results in substantial accuracy improvement over clickthrough alone.",
                "We now consider incorporating user behavior into a much richer feature set, RN (Section 5.3) used by a major web search engine.",
                "RN incorporates BM25F, link-based features, and hundreds of other features.",
                "Figure 6.4 reports NDCG at K and Figure 6.5 reports Precision at K. Interestingly, while the original RN rankings are significantly more accurate than BM25F alone, incorporating implicit feedback features (BM25F+All) results in <br>ranking</br> that significantly outperforms the original RN rankings.",
                "In other words, implicit feedback incorporates sufficient information to replace the hundreds of other features available to the RankNet learner trained on the RN feature set. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 1 2 3 4 5 6 7 8 9 10K NDCG RN RN+All BM25 BM25+All Figure 6.4: NDCG at K for BM25F, BM25F+All, RN, and RN+All for varying K Furthermore, enriching the RN features with implicit feedback set exhibits significant gain on all measures, allowing RN+All to outperform all other methods.",
                "This demonstrates the complementary nature of implicit feedback with other features available to a state of the art web search engine. 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision RN RN+All BM25 BM25+All Figure 6.5: Precision at K for BM25F, BM25F+All, RN, and RN+All for varying K We summarize the performance of the different <br>ranking</br> methods in Table 6.1.",
                "We report the Mean Average Precision (MAP) score for each system.",
                "While not intuitive to interpret, MAP allows quantitative comparison on a single metric.",
                "The gains marked with * are significant at p=0.01 level using two tailed t-test.",
                "MAP Gain P(1) Gain BM25F 0.184 - 0.503BM25F-Rerank-CT 0.215 0.031* 0.577 0.073* BM25F-RerankImplicit 0.218 0.003 0.605 0.028* BM25F+Implicit 0.222 0.004 0.620 0.015* RN 0.215 - 0.597RN+All 0.248 0.033* 0.629 0.032* Table 6.1: Mean Average Precision (MAP) for all strategies.",
                "So far we reported results averaged across all queries in the test set.",
                "Unfortunately, less than half had sufficient interactions to attempt reranking.",
                "Out of the 1000 queries in test, between 46% and 49%, depending on the train-test split, had sufficient interaction information to make predictions (i.e., there was at least 1 search session in which at least 1 result URL was clicked on by the user).",
                "This is not surprising: web search is heavy-tailed, and there are many unique queries.",
                "We now consider the performance on the queries for which user interactions were available.",
                "Figure 6.6 reports NDCG for the subset of the test queries with the implicit feedback features.",
                "The gains at top 1 are dramatic.",
                "The NDCG at 1 of BM25F+All increases from 0.6 to 0.75 (a 31% relative gain), achieving performance comparable to RN+All operating over a much richer feature set. 0.6 0.65 0.7 0.75 0.8 1 3 5 10K NDCG RN RN+All BM25 BM25+All Figure 6.6: NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions Similarly, gains on precision at top 1 are substantial (Figure 6.7), and are likely to be apparent to web search users.",
                "When implicit feedback is available, the BM25F+All system returns relevant document at top 1 almost 70% of the time, compared 53% of the time when implicit feedback is not considered by the original BM25F system. 0.45 0.5 0.55 0.6 0.65 0.7 1 3 5 10K Precision RN RN+All BM25 BM25+All Figure 6.7: Precision at K NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions We summarize the results on the MAP measure for attempted queries in Table 6.2.",
                "MAP improvements are both substantial and significant, with improvements over the BM25F ranker most pronounced.",
                "Method MAP Gain P(1) Gain RN 0.269 0.632 RN+All 0.321 0.051 (19%) 0.693 0.061(10%) BM25F 0.236 0.525 BM25F+All 0.292 0.056 (24%) 0.687 0.162 (31%) Table 6.2: Mean Average Precision (MAP) on attempted queries for best performing methods We now analyze the cases where implicit feedback was shown most helpful.",
                "Figure 6.8 reports the MAP improvements over the baseline BM25F run for each query with MAP under 0.6.",
                "Note that most of the improvement is for poorly performing queries (i.e., MAP < 0.1).",
                "Interestingly, incorporating user behavior information degrades accuracy for queries with high original MAP score.",
                "One possible explanation is that these easy queries tend to be navigational (i.e., having a single, highly-ranked most appropriate answer), and user interactions with lower-ranked results may indicate divergent information needs that are better served by the less popular results (with correspondingly poor overall relevance ratings). 0 50 100 150 200 250 300 350 0.1 0.2 0.3 0.4 0.5 0.6 -0.4 -0.35 -0.3 -0.25 -0.2 -0.15 -0.1 -0.05 0 0.05 0.1 0.15 0.2 Frequency Average Gain Figure 6.8: Gain of BM25F+All over original BM25F <br>ranking</br> To summarize our experimental results, incorporating implicit feedback in real web search setting resulted in significant improvements over the original rankings, using both BM25F and RN baselines.",
                "Our rich set of implicit features, such as time on page and deviations from the average behavior, provides advantages over using clickthrough alone as an indicator of interest.",
                "Furthermore, incorporating implicit feedback features directly into the learned <br>ranking</br> function is more effective than using implicit feedback for reranking.",
                "The improvements observed over large test sets of queries (1,000 total, between 466 and 495 with implicit feedback available) are both substantial and statistically significant. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper we explored the utility of incorporating noisy implicit feedback obtained in a real web search setting to improve web search <br>ranking</br>.",
                "We performed a large-scale evaluation over 3,000 queries and more than 12 million user interactions with a major search engine, establishing the utility of incorporating noisy implicit feedback to improve web search relevance.",
                "We compared two alternatives of incorporating implicit feedback into the search process, namely reranking with implicit feedback and incorporating implicit feedback features directly into the trained <br>ranking</br> function.",
                "Our experiments showed significant improvement over methods that do not consider implicit feedback.",
                "The gains are particularly dramatic for the top K=1 result in the final <br>ranking</br>, with precision improvements as high as 31%, and the gains are substantial for all values of K. Our experiments showed that implicit user feedback can further improve web search performance, when incorporated directly with popular content- and link-based features.",
                "Interestingly, implicit feedback is particularly valuable for queries with poor original <br>ranking</br> of results (e.g., MAP lower than 0.1).",
                "One promising direction for future work is to apply recent research on automatically predicting query difficulty, and only attempt to incorporate implicit feedback for the difficult queries.",
                "As another research direction we are exploring methods for extending our predictions to the previously unseen queries (e.g., query clustering), which should further improve the web search experience of users.",
                "ACKNOWLEDGMENTS We thank Chris Burges and Matt Richardson for an implementation of RankNet for our experiments.",
                "We also thank Robert Ragno for his valuable suggestions and many discussions. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, S. Dumais, and R.Ragno, Learning User Interaction Models for Predicting Web Search Result Preferences.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan, HARD Track Overview in TREC 2003, High Accuracy Retrieval from Documents, 2003 [3] R. Baeza-Yates and B. Ribeiro-Neto, Modern Information Retrieval, Addison-Wesley, 1999. [4] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine, in Proceedings of WWW, 1997 [5] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning, 2005 [6] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [7] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest.",
                "IEEE Internet Computing. 2001 [8] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [9] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [10] K Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2000 [11] T. Joachims, Optimizing Search Engines Using Clickthrough Data.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [12] T. Joachims, L. Granka, B. Pang, H. Hembrooke, and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [13] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [14] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [15] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [16] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [17] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems.",
                "In Proceedings of the AAAI Workshop on Recommender Systems. 1998 [18] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [19] N. Pharo, N. and K. Järvelin.",
                "The SST method: a tool for analyzing web information search processes.",
                "In Information Processing & Management, 2004 [20] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [21] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (SIGKDD), 2005. [22] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in Proceedings of the ICML Workshop on Learning in Web Search, 2005 [23] S. E. Robertson, H. Zaragoza, and M. Taylor, Simple BM25 extension to multiple weighted fields, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [24] G. Salton & M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [25] E.M. Voorhees, D. Harman, Overview of TREC, 2001 [26] G.R.",
                "Xue, H.J.",
                "Zeng, Z. Chen, Y. Yu, W.Y.",
                "Ma, W.S.",
                "Xi, and W.G.",
                "Fan, Optimizing web search using web clickthrough data, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [27] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
                "Microsoft Cambridge at TREC 13: Web and Hard Tracks.",
                "In Proceedings of TREC 2004"
            ],
            "original_annotated_samples": [
                "Improving Web Search <br>ranking</br> by Incorporating User Behavior Information Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com ABSTRACT We show that incorporating user behavior data can significantly improve ordering of top results in real web search setting.",
                "We examine alternatives for incorporating feedback into the <br>ranking</br> process and explore the contributions of user feedback compared to other common web search features.",
                "We show that incorporating implicit feedback can augment other features, improving the accuracy of a competitive web search <br>ranking</br> algorithms by as much as 31% relative to the original performance.",
                "These interactions can serve as a valuable source of information for tuning and improving web search result <br>ranking</br> and can compliment more costly explicit judgments.",
                "Implicit relevance feedback for <br>ranking</br> and personalization has become an active area of research."
            ],
            "translated_annotated_samples": [
                "Mejorando la <br>clasificación</br> de búsqueda web al incorporar información sobre el comportamiento del usuario. Eugene Agichtein, Microsoft Research eugeneag@microsoft.com Eric Brill, Microsoft Research brill@microsoft.com Susan Dumais, Microsoft Research sdumais@microsoft.com RESUMEN Mostramos que la incorporación de datos sobre el comportamiento del usuario puede mejorar significativamente el orden de los principales resultados en una configuración real de búsqueda web.",
                "Examinamos alternativas para incorporar retroalimentación en el proceso de <br>clasificación</br> y exploramos las contribuciones de la retroalimentación de usuarios en comparación con otras características comunes de búsqueda en la web.",
                "Mostramos que la incorporación de retroalimentación implícita puede aumentar otras características, mejorando la precisión de los algoritmos de <br>clasificación</br> de búsqueda web competitivos hasta en un 31% en comparación con el rendimiento original.",
                "Estas interacciones pueden servir como una valiosa fuente de información para ajustar y mejorar la <br>clasificación</br> de los resultados de búsqueda en la web y pueden complementar juicios explícitos más costosos.",
                "La retroalimentación implícita de relevancia para la <br>clasificación</br> y personalización se ha convertido en un área activa de investigación."
            ],
            "translated_text": "Mejorando la <br>clasificación</br> de búsqueda web al incorporar información sobre el comportamiento del usuario. Eugene Agichtein, Microsoft Research eugeneag@microsoft.com Eric Brill, Microsoft Research brill@microsoft.com Susan Dumais, Microsoft Research sdumais@microsoft.com RESUMEN Mostramos que la incorporación de datos sobre el comportamiento del usuario puede mejorar significativamente el orden de los principales resultados en una configuración real de búsqueda web. Examinamos alternativas para incorporar retroalimentación en el proceso de <br>clasificación</br> y exploramos las contribuciones de la retroalimentación de usuarios en comparación con otras características comunes de búsqueda en la web. Informamos los resultados de una evaluación a gran escala con más de 3,000 consultas y 12 millones de interacciones de usuarios con un motor de búsqueda web popular. Mostramos que la incorporación de retroalimentación implícita puede aumentar otras características, mejorando la precisión de los algoritmos de <br>clasificación</br> de búsqueda web competitivos hasta en un 31% en comparación con el rendimiento original. Categorías y Descriptores de Asignaturas H.3.3 Búsqueda y Recuperación de Información - Retroalimentación de relevancia, proceso de búsqueda; H.3.5 Servicios de Información en Línea - Servicios basados en la web. Términos generales Algoritmos, Medición, Experimentación 1. INTRODUCCIÓN Millones de usuarios interactúan con los motores de búsqueda a diario. Ellos emiten consultas, siguen algunos de los enlaces en los resultados, hacen clic en anuncios, pasan tiempo en páginas, reformulan sus consultas y realizan otras acciones. Estas interacciones pueden servir como una valiosa fuente de información para ajustar y mejorar la <br>clasificación</br> de los resultados de búsqueda en la web y pueden complementar juicios explícitos más costosos. La retroalimentación implícita de relevancia para la <br>clasificación</br> y personalización se ha convertido en un área activa de investigación. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "information retrieval": {
            "translated_key": "recuperación de información",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Improving Web Search Ranking by Incorporating User Behavior Information Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com ABSTRACT We show that incorporating user behavior data can significantly improve ordering of top results in real web search setting.",
                "We examine alternatives for incorporating feedback into the ranking process and explore the contributions of user feedback compared to other common web search features.",
                "We report results of a large scale evaluation over 3,000 queries and 12 million user interactions with a popular web search engine.",
                "We show that incorporating implicit feedback can augment other features, improving the accuracy of a competitive web search ranking algorithms by as much as 31% relative to the original performance.",
                "Categories and Subject Descriptors H.3.3 Information Search and Retrieval - Relevance feedback, search process; H.3.5 Online Information Services - Web-based services.",
                "General Terms Algorithms, Measurement, Experimentation 1.",
                "INTRODUCTION Millions of users interact with search engines daily.",
                "They issue queries, follow some of the links in the results, click on ads, spend time on pages, reformulate their queries, and perform other actions.",
                "These interactions can serve as a valuable source of information for tuning and improving web search result ranking and can compliment more costly explicit judgments.",
                "Implicit relevance feedback for ranking and personalization has become an active area of research.",
                "Recent work by Joachims and others exploring implicit feedback in controlled environments have shown the value of incorporating implicit feedback into the ranking process.",
                "Our motivation for this work is to understand how implicit feedback can be used in a large-scale operational environment to improve retrieval.",
                "How does it compare to and compliment evidence from page content, anchor text, or link-based features such as inlinks or PageRank?",
                "While it is intuitive that user interactions with the web search engine should reveal at least some information that could be used for ranking, estimating user preferences in real web search settings is a challenging problem, since real user interactions tend to be more noisy than commonly assumed in the controlled settings of previous studies.",
                "Our paper explores whether implicit feedback can be helpful in realistic environments, where user feedback can be noisy (or adversarial) and a web search engine already uses hundreds of features and is heavily tuned.",
                "To this end, we explore different approaches for ranking web search results using real user behavior obtained as part of normal interactions with the web search engine.",
                "The specific contributions of this paper include: • Analysis of alternatives for incorporating user behavior into web search ranking (Section 3). • An application of a robust implicit feedback model derived from mining millions of user interactions with a major web search engine (Section 4). • A large scale evaluation over real user queries and search results, showing significant improvements derived from incorporating user feedback (Section 6).",
                "We summarize our findings and discuss extensions to the current work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in <br>information retrieval</br>.",
                "Most common approaches primarily focus on similarity of query and a page, as well as the overall page quality [3,4,24].",
                "However, with increasing popularity of search engines, implicit feedback (i.e., the actions users take when interacting with the search engine) can be used to improve the rankings.",
                "Implicit relevance measures have been studied by several research groups.",
                "An overview of implicit measures is compiled in Kelly and Teevan [14].",
                "This research, while developing valuable insights into implicit relevance measures, was not applied to improve the ranking of web search results in realistic settings.",
                "Closely related to our work, Joachims [11] collected implicit measures in place of explicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "Fox et al. [8] explored the relationship between implicit and explicit measures in Web search, and developed Bayesian models to correlate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "This work considered a wide range of user behaviors (e.g., dwell time, scroll time, reformulation patterns) in addition to the popular clickthrough behavior.",
                "However, the modeling effort was aimed at predicting explicit relevance judgments from implicit user actions and not specifically at learning ranking functions.",
                "Other studies of user behavior in web search include Pharo and Järvelin [19], but were not directly applied to improve ranking.",
                "More recently, Joachims et al. [12] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthroughs in a controlled, laboratory setting.",
                "Unfortunately, the extent to which previous research applies to real-world web search is unclear.",
                "At the same time, while recent work (e.g., [26]) on using clickthrough information for improving web search ranking is promising, it captures only one aspect of the user interactions with web search engines.",
                "We build on existing research to develop robust user behavior interpretation techniques for the real web search setting.",
                "Instead of treating each user as a reliable expert, we aggregate information from multiple, unreliable, user search session traces, as we describe in the next two sections. 3.",
                "INCORPORATING IMPLICIT FEEDBACK We consider two complementary approaches to ranking with implicit feedback: (1) treating implicit feedback as independent evidence for ranking results, and (2) integrating implicit feedback features directly into the ranking algorithm.",
                "We describe the two general ranking approaches next.",
                "The specific implicit feedback features are described in Section 4, and the algorithms for interpreting and incorporating implicit feedback are described in Section 5. 3.1 Implicit Feedback as Independent Evidence The general approach is to re-rank the results obtained by a web search engine according to observed clickthrough and other user interactions for the query in previous search sessions.",
                "Each result is assigned a score according to expected relevance/user satisfaction based on previous interactions, resulting in some preference ordering based on user interactions alone.",
                "While there has been significant work on merging multiple rankings, we adapt a simple and robust approach of ignoring the original rankers scores, and instead simply merge the rank orders.",
                "The main reason for ignoring the original scores is that since the feature spaces and learning algorithms are different, the scores are not directly comparable, and re-normalization tends to remove the benefit of incorporating classifier scores.",
                "We experimented with a variety of merging functions on the development set of queries (and using a set of interactions from a different time period from final evaluation sets).",
                "We found that a simple rank merging heuristic combination works well, and is robust to variations in score values from original rankers.",
                "For a given query q, the implicit score ISd is computed for each result d from available user interaction features, resulting in the implicit rank Id for each result.",
                "We compute a merged score SM(d) for d by combining the ranks obtained from implicit feedback, Id with the original rank of d, Od:     ¡     ¢ £ + + + + = otherwise O dforexistsfeedbackimplicitif OI w wOIdS d dd I IddM 1 1 1 1 1 1 ),,,( where the weight wI is a heuristically tuned scaling factor representing the relative importance of the implicit feedback.",
                "The query results are ordered in by decreasing values of SM to produce the final ranking.",
                "One special case of this model arises when setting wI to a very large value, effectively forcing clicked results to be ranked higher than un-clicked results - an intuitive and effective heuristic that we will use as a baseline.",
                "Applying more sophisticated classifier and ranker combination algorithms may result in additional improvements, and is a promising direction for future work.",
                "The approach above assumes that there are no interactions between the underlying features producing the original web search ranking and the implicit feedback features.",
                "We now relax this assumption by integrating implicit feedback features directly into the ranking process. 3.2 Ranking with Implicit Feedback Features Modern web search engines rank results based on a large number of features, including content-based features (i.e., how closely a query matches the text or title or anchor text of the document), and query-independent page quality features (e.g., PageRank of the document or the domain).",
                "In most cases, automatic (or semiautomatic) methods are developed for tuning the specific ranking function that combines these feature values.",
                "Hence, a natural approach is to incorporate implicit feedback features directly as features for the ranking algorithm.",
                "During training or tuning, the ranker can be tuned as before but with additional features.",
                "At runtime, the search engine would fetch the implicit feedback features associated with each query-result URL pair.",
                "This model requires a ranking algorithm to be robust to missing values: more than 50% of queries to web search engines are unique, with no previous implicit feedback available.",
                "We now describe such a ranker that we used to learn over the combined feature sets including implicit feedback. 3.3 Learning to Rank Web Search Results A key aspect of our approach is exploiting recent advances in machine learning, namely trainable ranking algorithms for web search and <br>information retrieval</br> (e.g., [5, 11] and classical results reviewed in [3]).",
                "In our setting, explicit human relevance judgments (labels) are available for a set of web search queries and results.",
                "Hence, an attractive choice to use is a supervised machine learning technique to learn a ranking function that best predicts relevance judgments.",
                "RankNet is one such algorithm.",
                "It is a neural net tuning algorithm that optimizes feature weights to best match explicitly provided pairwise user preferences.",
                "While the specific training algorithms used by RankNet are beyond the scope of this paper, it is described in detail in [5] and includes extensive evaluation and comparison with other ranking methods.",
                "An attractive feature of RankNet is both train- and run-time efficiency - runtime ranking can be quickly computed and can scale to the web, and training can be done over thousands of queries and associated judged results.",
                "We use a 2-layer implementation of RankNet in order to model non-linear relationships between features.",
                "Furthermore, RankNet can learn with many (differentiable) cost functions, and hence can automatically learn a ranking function from human-provided labels, an attractive alternative to heuristic feature combination techniques.",
                "Hence, we will also use RankNet as a generic ranker to explore the contribution of implicit feedback for different ranking alternatives. 4.",
                "IMPLICIT USER FEEDBACK MODEL Our goal is to accurately interpret noisy user feedback obtained as by tracing user interactions with the search engine.",
                "Interpreting implicit feedback in real web search setting is not an easy task.",
                "We characterize this problem in detail in [1], where we motivate and evaluate a wide variety of models of implicit user activities.",
                "The general approach is to represent user actions for each search result as a vector of features, and then train a ranker on these features to discover feature values indicative of relevant (and nonrelevant) search results.",
                "We first briefly summarize our features and model, and the learning approach (Section 4.2) in order to provide sufficient information to replicate our ranking methods and the subsequent experiments. 4.1 Representing User Actions as Features We model observed web search behaviors as a combination of a ``background component (i.e., query- and relevance-independent noise in user behavior, including positional biases with result interactions), and a ``relevance component (i.e., query-specific behavior indicative of relevance of a result to a query).",
                "We design our features to take advantage of aggregated user behavior.",
                "The feature set is comprised of directly observed features (computed directly from observations for each query), as well as queryspecific derived features, computed as the deviation from the overall query-independent distribution of values for the corresponding directly observed feature values.",
                "The features used to represent user interactions with web search results are summarized in Table 4.1.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "We include the traditional implicit feedback features such as clickthrough counts for the results, as well as our novel derived features such as the deviation of the observed clickthrough number for a given query-URL pair from the expected number of clicks on a result in the given position.",
                "We also model the browsing behavior after a result was clicked - e.g., the average page dwell time for a given query-URL pair, as well as its deviation from the expected (average) dwell time.",
                "Furthermore, the feature set was designed to provide essential information about the user experience to make feedback interpretation robust.",
                "For example, web search users can often determine whether a result is relevant by looking at the result title, URL, and summary - in many cases, looking at the original document is not necessary.",
                "To model this aspect of user experience we include features such as overlap in words in title and words in query (TitleOverlap) and the fraction of words shared by the query and the result summary.",
                "Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickProbability Probability of a click for this query and URL ClickDeviation Deviation from expected click probability IsNextClicked 1 if clicked on next position, 0 otherwise IsPreviousClicked 1 if clicked on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, no parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from average dwell time on page CumulativeDeviation Deviation from average cumulative dwell time DomainDeviation Deviation from average dwell time on domain Query-text features TitleOverlap Words shared between query and title SummaryOverlap Words shared between query and snippet QueryURLOverlap Words shared between query and URL QueryDomainOverlap Words shared between query and URL domain QueryLength Number of tokens in query QueryNextOverlap Fraction of words shared with next query Table 4.1: Some features used to represent post-search navigation history for a given query and search result URL.",
                "Having described our feature set, we briefly review our general method for deriving a user behavior model. 4.2 Deriving a User Feedback Model To learn to interpret the observed user behavior, we correlate user actions (i.e., the features in Table 4.1 representing the actions) with the explicit user judgments for a set of training queries.",
                "We find all the instances in our session logs where these queries were submitted to the search engine, and aggregate the user behavior features for all search sessions involving these queries.",
                "Each observed query-URL pair is represented by the features in Table 4.1, with values averaged over all search sessions, and assigned one of six possible relevance labels, ranging from Perfect to Bad, as assigned by explicit relevance judgments.",
                "These labeled feature vectors are used as input to the RankNet training algorithm (Section 3.3) which produces a trained user behavior model.",
                "This approach is particularly attractive as it does not require heuristics beyond feature engineering.",
                "The resulting user behavior model is used to help rank web search resultseither directly or in combination with other features, as described below. 5.",
                "EXPERIMENTAL SETUP The ultimate goal of incorporating implicit feedback into ranking is to improve the relevance of the returned web search results.",
                "Hence, we compare the ranking methods over a large set of judged queries with explicit relevance labels provided by human judges.",
                "In order for the evaluation to be realistic we obtained a random sample of queries from web search logs of a major search engine, with associated results and traces for user actions.",
                "We describe this dataset in detail next.",
                "Our metrics are described in Section 5.2 that we use to evaluate the ranking alternatives, listed in Section 5.3 in the experiments of Section 6. 5.1 Datasets We compared our ranking methods over a random sample of 3,000 queries from the search engine query logs.",
                "The queries were drawn from the logs uniformly at random by token without replacement, resulting in a query sample representative of the overall query distribution.",
                "On average, 30 results were explicitly labeled by human judges using a six point scale ranging from Perfect down to Bad.",
                "Overall, there were over 83,000 results with explicit relevance judgments.",
                "In order to compute various statistics, documents with label Good or better will be considered relevant, and with lower labels to be non-relevant.",
                "Note that the experiments were performed over the results already highly ranked by a web search engine, which corresponds to a typical user experience which is limited to the small number of the highly ranked results for a typical web search query.",
                "The user interactions were collected over a period of 8 weeks using voluntary opt-in information.",
                "In total, over 1.2 million unique queries were instrumented, resulting in over 12 million individual interactions with the search engine.",
                "The data consisted of user interactions with the web search engine (e.g., clicking on a result link, going back to search results, etc.) performed after a query was submitted.",
                "These actions were aggregated across users and search sessions and converted to features in Table 4.1.",
                "To create the training, validation, and test query sets, we created three different random splits of 1,500 training, 500 validation, and 1000 test queries.",
                "The splits were done randomly by query, so that there was no overlap in training, validation, and test queries. 5.2 Evaluation Metrics We evaluate the ranking algorithms over a range of accepted <br>information retrieval</br> metrics, namely Precision at K (P(K)), Normalized Discounted Cumulative Gain (NDCG), and Mean Average Precision (MAP).",
                "Each metric focuses on a deferent aspect of system performance, as we describe below. • Precision at K: As the most intuitive metric, P(K) reports the fraction of documents ranked in the top K results that are labeled as relevant.",
                "In our setting, we require a relevant document to be labeled Good or higher.",
                "The position of relevant documents within the top K is irrelevant, and hence this metric measure overall user satisfaction with the top K results. • NDCG at K: NDCG is a retrieval measure devised specifically for web search evaluation [10].",
                "For a given query q, the ranked results are examined from the top ranked down, and the NDCG computed as:   = +−= K j jr qq jMN 1 )( )1log(/)12( Where Mq is a normalization constant calculated so that a perfect ordering would obtain NDCG of 1; and each r(j) is an integer relevance label (0=Bad and 5=Perfect) of result returned at position j.",
                "Note that unlabeled and Bad documents do not contribute to the sum, but will reduce NDCG for the query pushing down the relevant labeled documents, reducing their contributions.",
                "NDCG is well suited to web search evaluation, as it rewards relevant documents in the top ranked results more heavily than those ranked lower. • MAP: Average precision for each query is defined as the mean of the precision at K values computed after each relevant document was retrieved.",
                "The final MAP value is defined as the mean of average precisions of all queries in the test set.",
                "This metric is the most commonly used single-value summary of a run over a set of queries. 5.3 Ranking Methods Compared Recall that our goal is to quantify the effectiveness of implicit behavior for real web search.",
                "One dimension is to compare the utility of implicit feedback with other information available to a web search engine.",
                "Specifically, we compare effectiveness of implicit user behaviors with content-based matching, static page quality features, and combinations of all features. • BM25F: As a strong web search baseline we used the BM25F scoring, which was used in one of the best performing systems in the TREC 2004 Web track [23,27].",
                "BM25F and its variants have been extensively described and evaluated in IR literature, and hence serve as a strong, reproducible baseline.",
                "The BM25F variant we used for our experiments computes separate match scores for each field for a result document (e.g., body text, title, and anchor text), and incorporates query-independent linkbased information (e.g., PageRank, ClickDistance, and URL depth).",
                "The scoring function and field-specific tuning is described in detail in [23].",
                "Note that BM25F does not directly consider explicit or implicit feedback for tuning. • RN: The ranking produced by a neural net ranker (RankNet, described in Section 3.3) that learns to rank web search results by incorporating BM25F and a large number of additional static and dynamic features describing each search result.",
                "This system automatically learns weights for all features (including the BM25F score for a document) based on explicit human labels for a large set of queries.",
                "A system incorporating an implementation of RankNet is currently in use by a major search engine and can be considered representative of the state of the art in web search. • BM25F-RerankCT: The ranking produced by incorporating clickthrough statistics to reorder web search results ranked by BM25F above.",
                "Clickthrough is a particularly important special case of implicit feedback, and has been shown to correlate with result relevance.",
                "This is a special case of the ranking method in Section 3.1, with the weight wI set to 1000 and the ranking Id is simply the number of clicks on the result corresponding to d. In effect, this ranking brings to the top all returned web search results with at least one click (and orders them in decreasing order by number of clicks).",
                "The relative ranking of the remainder of results is unchanged and they are inserted below all clicked results.",
                "This method serves as our baseline implicit feedback reranking method.",
                "BM25F-RerankAll The ranking produced by reordering the BM25F results using all user behavior features (Section 4).",
                "This method learns a model of user preferences by correlating feature values with explicit relevance labels using the RankNet neural net algorithm (Section 4.2).",
                "At runtime, for a given query the implicit score Ir is computed for each result r with available user interaction features, and the implicit ranking is produced.",
                "The merged ranking is computed as described in Section 3.1.",
                "Based on the experiments over the development set we fix the value of wI to 3 (the effect of the wI parameter for this ranker turned out to be negligible). • BM25F+All: Ranking derived by training the RankNet (Section 3.3) learner over the features set of the BM25F score as well as all implicit feedback features (Section 3.2).",
                "We used the 2-layer implementation of RankNet [5] trained on the queries and labels in the training and validation sets. • RN+All: Ranking derived by training the 2-layer RankNet ranking algorithm (Section 3.3) over the union of all content, dynamic, and implicit feedback features (i.e., all of the features described above as well as all of the new implicit feedback features we introduced).",
                "The ranking methods above span the range of the information used for ranking, from not using the implicit or explicit feedback at all (i.e., BM25F) to a modern web search engine using hundreds of features and tuned on explicit judgments (RN).",
                "As we will show next, incorporating user behavior into these ranking systems dramatically improves the relevance of the returned documents. 6.",
                "EXPERIMENTAL RESULTS Implicit feedback for web search ranking can be exploited in a number of ways.",
                "We compare alternative methods of exploiting implicit feedback, both by re-ranking the top results (i.e., the BM25F-RerankCT and BM25F-RerankAll methods that reorder BM25F results), as well as by integrating the implicit features directly into the ranking process (i.e., the RN+ALL and BM25F+All methods which learn to rank results over the implicit feedback and other features).",
                "We compare our methods over strong baselines (BM25F and RN) over the NDCG, Precision at K, and MAP measures defined in Section 5.2.",
                "The results were averaged over three random splits of the overall dataset.",
                "Each split contained 1500 training, 500 validation, and 1000 test queries, all query sets disjoint.",
                "We first present the results over all 1000 test queries (i.e., including queries for which there are no implicit measures so we use the original web rankings).",
                "We then drill down to examine the effects on reranking for the attempted queries in more detail, analyzing where implicit feedback proved most beneficial.",
                "We first experimented with different methods of re-ranking the output of the BM25F search results.",
                "Figures 6.1 and 6.2 report NDCG and Precision for BM25F, as well as for the strategies reranking results with user feedback (Section 3.1).",
                "Incorporating all user feedback (either in reranking framework or as features to the learner directly) results in significant improvements (using two-tailed t-test with p=0.01) over both the original BM25F ranking as well as over reranking with clickthrough alone.",
                "The improvement is consistent across the top 10 results and largest for the top result: NDCG at 1 for BM25F+All is 0.622 compared to 0.518 of the original results, and precision at 1 similarly increases from 0.5 to 0.63.",
                "Based on these results we will use the direct feature combination (i.e., BM25F+All) ranker for subsequent comparisons involving implicit feedback. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 1 2 3 4 5 6 7 8 9 10K NDCG BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.1: NDCG at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K 0.35 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.2: Precision at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K Interestingly, using clickthrough alone, while giving significant benefit over the original BM25F ranking, is not as effective as considering the full set of features in Table 4.1.",
                "While we analyze user behavior (and most effective component features) in a separate paper [1], it is worthwhile to give a concrete example of the kind of noise inherent in real user feedback in web search setting. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 Result position Relativeclickfrequency PTR=2 PTR=3 PTR=5 Figure 6.3: Relative clickthrough frequency for queries with varying Position of Top Relevant result (PTR).",
                "If users considered only the relevance of a result to their query, they would click on the topmost relevant results.",
                "Unfortunately, as Joachims and others have shown, presentation also influences which results users click on quite dramatically.",
                "Users often click on results above the relevant one presumably because the short summaries do not provide enough information to make accurate relevance assessments and they have learned that on average topranked items are relevant.",
                "Figure 6.3 shows relative clickthrough frequencies for queries with known relevant items at positions other than the first position; the position of the top relevant result (PTR) ranges from 2-10 in the figure.",
                "For example, for queries with first relevant result at position 5 (PTR=5), there are more clicks on the non-relevant results in higher ranked positions than on the first relevant result at position 5.",
                "As we will see, learning over a richer behavior feature set, results in substantial accuracy improvement over clickthrough alone.",
                "We now consider incorporating user behavior into a much richer feature set, RN (Section 5.3) used by a major web search engine.",
                "RN incorporates BM25F, link-based features, and hundreds of other features.",
                "Figure 6.4 reports NDCG at K and Figure 6.5 reports Precision at K. Interestingly, while the original RN rankings are significantly more accurate than BM25F alone, incorporating implicit feedback features (BM25F+All) results in ranking that significantly outperforms the original RN rankings.",
                "In other words, implicit feedback incorporates sufficient information to replace the hundreds of other features available to the RankNet learner trained on the RN feature set. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 1 2 3 4 5 6 7 8 9 10K NDCG RN RN+All BM25 BM25+All Figure 6.4: NDCG at K for BM25F, BM25F+All, RN, and RN+All for varying K Furthermore, enriching the RN features with implicit feedback set exhibits significant gain on all measures, allowing RN+All to outperform all other methods.",
                "This demonstrates the complementary nature of implicit feedback with other features available to a state of the art web search engine. 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision RN RN+All BM25 BM25+All Figure 6.5: Precision at K for BM25F, BM25F+All, RN, and RN+All for varying K We summarize the performance of the different ranking methods in Table 6.1.",
                "We report the Mean Average Precision (MAP) score for each system.",
                "While not intuitive to interpret, MAP allows quantitative comparison on a single metric.",
                "The gains marked with * are significant at p=0.01 level using two tailed t-test.",
                "MAP Gain P(1) Gain BM25F 0.184 - 0.503BM25F-Rerank-CT 0.215 0.031* 0.577 0.073* BM25F-RerankImplicit 0.218 0.003 0.605 0.028* BM25F+Implicit 0.222 0.004 0.620 0.015* RN 0.215 - 0.597RN+All 0.248 0.033* 0.629 0.032* Table 6.1: Mean Average Precision (MAP) for all strategies.",
                "So far we reported results averaged across all queries in the test set.",
                "Unfortunately, less than half had sufficient interactions to attempt reranking.",
                "Out of the 1000 queries in test, between 46% and 49%, depending on the train-test split, had sufficient interaction information to make predictions (i.e., there was at least 1 search session in which at least 1 result URL was clicked on by the user).",
                "This is not surprising: web search is heavy-tailed, and there are many unique queries.",
                "We now consider the performance on the queries for which user interactions were available.",
                "Figure 6.6 reports NDCG for the subset of the test queries with the implicit feedback features.",
                "The gains at top 1 are dramatic.",
                "The NDCG at 1 of BM25F+All increases from 0.6 to 0.75 (a 31% relative gain), achieving performance comparable to RN+All operating over a much richer feature set. 0.6 0.65 0.7 0.75 0.8 1 3 5 10K NDCG RN RN+All BM25 BM25+All Figure 6.6: NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions Similarly, gains on precision at top 1 are substantial (Figure 6.7), and are likely to be apparent to web search users.",
                "When implicit feedback is available, the BM25F+All system returns relevant document at top 1 almost 70% of the time, compared 53% of the time when implicit feedback is not considered by the original BM25F system. 0.45 0.5 0.55 0.6 0.65 0.7 1 3 5 10K Precision RN RN+All BM25 BM25+All Figure 6.7: Precision at K NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions We summarize the results on the MAP measure for attempted queries in Table 6.2.",
                "MAP improvements are both substantial and significant, with improvements over the BM25F ranker most pronounced.",
                "Method MAP Gain P(1) Gain RN 0.269 0.632 RN+All 0.321 0.051 (19%) 0.693 0.061(10%) BM25F 0.236 0.525 BM25F+All 0.292 0.056 (24%) 0.687 0.162 (31%) Table 6.2: Mean Average Precision (MAP) on attempted queries for best performing methods We now analyze the cases where implicit feedback was shown most helpful.",
                "Figure 6.8 reports the MAP improvements over the baseline BM25F run for each query with MAP under 0.6.",
                "Note that most of the improvement is for poorly performing queries (i.e., MAP < 0.1).",
                "Interestingly, incorporating user behavior information degrades accuracy for queries with high original MAP score.",
                "One possible explanation is that these easy queries tend to be navigational (i.e., having a single, highly-ranked most appropriate answer), and user interactions with lower-ranked results may indicate divergent information needs that are better served by the less popular results (with correspondingly poor overall relevance ratings). 0 50 100 150 200 250 300 350 0.1 0.2 0.3 0.4 0.5 0.6 -0.4 -0.35 -0.3 -0.25 -0.2 -0.15 -0.1 -0.05 0 0.05 0.1 0.15 0.2 Frequency Average Gain Figure 6.8: Gain of BM25F+All over original BM25F ranking To summarize our experimental results, incorporating implicit feedback in real web search setting resulted in significant improvements over the original rankings, using both BM25F and RN baselines.",
                "Our rich set of implicit features, such as time on page and deviations from the average behavior, provides advantages over using clickthrough alone as an indicator of interest.",
                "Furthermore, incorporating implicit feedback features directly into the learned ranking function is more effective than using implicit feedback for reranking.",
                "The improvements observed over large test sets of queries (1,000 total, between 466 and 495 with implicit feedback available) are both substantial and statistically significant. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper we explored the utility of incorporating noisy implicit feedback obtained in a real web search setting to improve web search ranking.",
                "We performed a large-scale evaluation over 3,000 queries and more than 12 million user interactions with a major search engine, establishing the utility of incorporating noisy implicit feedback to improve web search relevance.",
                "We compared two alternatives of incorporating implicit feedback into the search process, namely reranking with implicit feedback and incorporating implicit feedback features directly into the trained ranking function.",
                "Our experiments showed significant improvement over methods that do not consider implicit feedback.",
                "The gains are particularly dramatic for the top K=1 result in the final ranking, with precision improvements as high as 31%, and the gains are substantial for all values of K. Our experiments showed that implicit user feedback can further improve web search performance, when incorporated directly with popular content- and link-based features.",
                "Interestingly, implicit feedback is particularly valuable for queries with poor original ranking of results (e.g., MAP lower than 0.1).",
                "One promising direction for future work is to apply recent research on automatically predicting query difficulty, and only attempt to incorporate implicit feedback for the difficult queries.",
                "As another research direction we are exploring methods for extending our predictions to the previously unseen queries (e.g., query clustering), which should further improve the web search experience of users.",
                "ACKNOWLEDGMENTS We thank Chris Burges and Matt Richardson for an implementation of RankNet for our experiments.",
                "We also thank Robert Ragno for his valuable suggestions and many discussions. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, S. Dumais, and R.Ragno, Learning User Interaction Models for Predicting Web Search Result Preferences.",
                "In Proceedings of the ACM Conference on Research and Development on <br>information retrieval</br> (SIGIR), 2006 [2] J. Allan, HARD Track Overview in TREC 2003, High Accuracy Retrieval from Documents, 2003 [3] R. Baeza-Yates and B. Ribeiro-Neto, Modern <br>information retrieval</br>, Addison-Wesley, 1999. [4] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine, in Proceedings of WWW, 1997 [5] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning, 2005 [6] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [7] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest.",
                "IEEE Internet Computing. 2001 [8] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [9] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [10] K Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development on <br>information retrieval</br> (SIGIR), 2000 [11] T. Joachims, Optimizing Search Engines Using Clickthrough Data.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [12] T. Joachims, L. Granka, B. Pang, H. Hembrooke, and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, Proceedings of the ACM Conference on Research and Development on <br>information retrieval</br> (SIGIR), 2005 [13] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [14] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [15] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [16] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "Proceedings of the ACM Conference on Research and Development on <br>information retrieval</br> (SIGIR), 1994 [17] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems.",
                "In Proceedings of the AAAI Workshop on Recommender Systems. 1998 [18] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [19] N. Pharo, N. and K. Järvelin.",
                "The SST method: a tool for analyzing web information search processes.",
                "In Information Processing & Management, 2004 [20] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [21] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (SIGKDD), 2005. [22] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in Proceedings of the ICML Workshop on Learning in Web Search, 2005 [23] S. E. Robertson, H. Zaragoza, and M. Taylor, Simple BM25 extension to multiple weighted fields, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [24] G. Salton & M. McGill.",
                "Introduction to modern <br>information retrieval</br>.",
                "McGraw-Hill, 1983 [25] E.M. Voorhees, D. Harman, Overview of TREC, 2001 [26] G.R.",
                "Xue, H.J.",
                "Zeng, Z. Chen, Y. Yu, W.Y.",
                "Ma, W.S.",
                "Xi, and W.G.",
                "Fan, Optimizing web search using web clickthrough data, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [27] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
                "Microsoft Cambridge at TREC 13: Web and Hard Tracks.",
                "In Proceedings of TREC 2004"
            ],
            "original_annotated_samples": [
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in <br>information retrieval</br>.",
                "We now describe such a ranker that we used to learn over the combined feature sets including implicit feedback. 3.3 Learning to Rank Web Search Results A key aspect of our approach is exploiting recent advances in machine learning, namely trainable ranking algorithms for web search and <br>information retrieval</br> (e.g., [5, 11] and classical results reviewed in [3]).",
                "The splits were done randomly by query, so that there was no overlap in training, validation, and test queries. 5.2 Evaluation Metrics We evaluate the ranking algorithms over a range of accepted <br>information retrieval</br> metrics, namely Precision at K (P(K)), Normalized Discounted Cumulative Gain (NDCG), and Mean Average Precision (MAP).",
                "In Proceedings of the ACM Conference on Research and Development on <br>information retrieval</br> (SIGIR), 2006 [2] J. Allan, HARD Track Overview in TREC 2003, High Accuracy Retrieval from Documents, 2003 [3] R. Baeza-Yates and B. Ribeiro-Neto, Modern <br>information retrieval</br>, Addison-Wesley, 1999. [4] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine, in Proceedings of WWW, 1997 [5] C.J.C.",
                "In Proceedings of the ACM Conference on Research and Development on <br>information retrieval</br> (SIGIR), 2000 [11] T. Joachims, Optimizing Search Engines Using Clickthrough Data."
            ],
            "translated_annotated_samples": [
                "ANTECEDENTES Y TRABAJO RELACIONADO Clasificar los resultados de búsqueda es un problema fundamental en la <br>recuperación de información</br>.",
                "Ahora describimos un clasificador que utilizamos para aprender sobre los conjuntos de características combinadas, incluyendo la retroalimentación implícita. 3.3 Aprendizaje para Clasificar los Resultados de Búsqueda en la Web Un aspecto clave de nuestro enfoque es aprovechar los avances recientes en el aprendizaje automático, en particular los algoritmos de clasificación entrenables para la búsqueda en la web y la <br>recuperación de información</br> (por ejemplo, [5, 11] y resultados clásicos revisados en [3]).",
                "Las divisiones se realizaron aleatoriamente por consulta, de modo que no hubiera superposición en las consultas de entrenamiento, validación y prueba. 5.2 Métricas de Evaluación Evaluamos los algoritmos de clasificación en una variedad de métricas de <br>recuperación de información</br> aceptadas, a saber, Precisión en K (P(K)), Ganancia Acumulada Descontada Normalizada (NDCG) y Precisión Promedio Media (MAP).",
                "En Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2006 [2] J. Allan, Resumen de la Pista HARD en TREC 2003, Recuperación de Alta Precisión de Documentos, 2003 [3] R. Baeza-Yates y B. Ribeiro-Neto, Recuperación de Información Moderna, Addison-Wesley, 1999. [4] S. Brin y L. Page, Anatomía de un Motor de Búsqueda Web Hipertextual a Gran Escala, en Actas de WWW, 1997 [5] C.J.C.",
                "En las Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2000 [11] T. Joachims, Optimización de Motores de Búsqueda Utilizando Datos de Clics."
            ],
            "translated_text": "Mejorando la clasificación de búsqueda web al incorporar información sobre el comportamiento del usuario. Eugene Agichtein, Microsoft Research eugeneag@microsoft.com Eric Brill, Microsoft Research brill@microsoft.com Susan Dumais, Microsoft Research sdumais@microsoft.com RESUMEN Mostramos que la incorporación de datos sobre el comportamiento del usuario puede mejorar significativamente el orden de los principales resultados en una configuración real de búsqueda web. Examinamos alternativas para incorporar retroalimentación en el proceso de clasificación y exploramos las contribuciones de la retroalimentación de usuarios en comparación con otras características comunes de búsqueda en la web. Informamos los resultados de una evaluación a gran escala con más de 3,000 consultas y 12 millones de interacciones de usuarios con un motor de búsqueda web popular. Mostramos que la incorporación de retroalimentación implícita puede aumentar otras características, mejorando la precisión de los algoritmos de clasificación de búsqueda web competitivos hasta en un 31% en comparación con el rendimiento original. Categorías y Descriptores de Asignaturas H.3.3 Búsqueda y Recuperación de Información - Retroalimentación de relevancia, proceso de búsqueda; H.3.5 Servicios de Información en Línea - Servicios basados en la web. Términos generales Algoritmos, Medición, Experimentación 1. INTRODUCCIÓN Millones de usuarios interactúan con los motores de búsqueda a diario. Ellos emiten consultas, siguen algunos de los enlaces en los resultados, hacen clic en anuncios, pasan tiempo en páginas, reformulan sus consultas y realizan otras acciones. Estas interacciones pueden servir como una valiosa fuente de información para ajustar y mejorar la clasificación de los resultados de búsqueda en la web y pueden complementar juicios explícitos más costosos. La retroalimentación implícita de relevancia para la clasificación y personalización se ha convertido en un área activa de investigación. El trabajo reciente de Joachims y otros explorando el feedback implícito en entornos controlados ha demostrado el valor de incorporar el feedback implícito en el proceso de clasificación. Nuestra motivación para este trabajo es comprender cómo se puede utilizar la retroalimentación implícita en un entorno operativo a gran escala para mejorar la recuperación. ¿Cómo se compara y complementa con la evidencia del contenido de la página, el texto del ancla o las características basadas en enlaces como inlinks o PageRank? Si bien es intuitivo que las interacciones de los usuarios con el motor de búsqueda web deberían revelar al menos alguna información que podría ser utilizada para la clasificación, estimar las preferencias de los usuarios en entornos reales de búsqueda web es un problema desafiante, ya que las interacciones reales tienden a ser más ruidosas de lo que comúnmente se asume en los entornos controlados de estudios previos. Nuestro artículo explora si la retroalimentación implícita puede ser útil en entornos realistas, donde la retroalimentación del usuario puede ser ruidosa (o adversa) y un motor de búsqueda web ya utiliza cientos de características y está altamente ajustado. Con este fin, exploramos diferentes enfoques para clasificar los resultados de búsqueda web utilizando el comportamiento real de los usuarios obtenido como parte de las interacciones normales con el motor de búsqueda web. Las contribuciones específicas de este artículo incluyen: • Análisis de alternativas para incorporar el comportamiento del usuario en la clasificación de búsqueda web (Sección 3). • Aplicación de un modelo robusto de retroalimentación implícita derivado de la minería de millones de interacciones de usuarios con un importante motor de búsqueda web (Sección 4). • Una evaluación a gran escala sobre consultas reales de usuarios y resultados de búsqueda, mostrando mejoras significativas derivadas de la incorporación de la retroalimentación del usuario (Sección 6). Resumimos nuestros hallazgos y discutimos extensiones al trabajo actual en la Sección 7, que concluye el artículo. ANTECEDENTES Y TRABAJO RELACIONADO Clasificar los resultados de búsqueda es un problema fundamental en la <br>recuperación de información</br>. La mayoría de los enfoques comunes se centran principalmente en la similitud entre la consulta y una página, así como en la calidad general de la página [3,4,24]. Sin embargo, con la creciente popularidad de los motores de búsqueda, la retroalimentación implícita (es decir, las acciones que los usuarios realizan al interactuar con el motor de búsqueda) se puede utilizar para mejorar las clasificaciones. Las medidas de relevancia implícitas han sido estudiadas por varios grupos de investigación. Un resumen de las medidas implícitas se recopila en Kelly y Teevan [14]. Esta investigación, si bien desarrolló valiosas ideas sobre medidas implícitas de relevancia, no se aplicó para mejorar la clasificación de los resultados de búsqueda en la web en entornos realistas. Estrechamente relacionado con nuestro trabajo, Joachims [11] recopiló medidas implícitas en lugar de medidas explícitas, introduciendo una técnica basada completamente en datos de clics para aprender funciones de clasificación. Fox et al. [8] exploraron la relación entre medidas implícitas y explícitas en la búsqueda web, y desarrollaron modelos bayesianos para correlacionar medidas implícitas y juicios explícitos de relevancia tanto para consultas individuales como para sesiones de búsqueda. Este trabajo consideró una amplia gama de comportamientos de usuario (por ejemplo, tiempo de permanencia, tiempo de desplazamiento, patrones de reformulación) además del popular comportamiento de clics. Sin embargo, el esfuerzo de modelado se centró en predecir juicios de relevancia explícitos a partir de acciones implícitas de los usuarios y no específicamente en aprender funciones de clasificación. Otros estudios sobre el comportamiento de los usuarios en la búsqueda web incluyen a Pharo y Järvelin [19], pero no se aplicaron directamente para mejorar la clasificación. Más recientemente, Joachims et al. [12] presentaron una evaluación empírica de la interpretación de la evidencia de clics. Al realizar estudios de seguimiento ocular y correlacionar las predicciones de sus estrategias con las calificaciones explícitas, los autores demostraron que es posible interpretar con precisión los clics en un entorno controlado de laboratorio. Desafortunadamente, no está claro en qué medida la investigación previa se aplica a la búsqueda web del mundo real. Al mismo tiempo, si bien el trabajo reciente (por ejemplo, [26]) sobre el uso de la información de clics para mejorar la clasificación de búsqueda en la web es prometedor, solo abarca un aspecto de las interacciones de los usuarios con los motores de búsqueda en la web. Nos basamos en investigaciones existentes para desarrollar técnicas robustas de interpretación del comportamiento del usuario para el entorno real de búsqueda en la web. En lugar de tratar a cada usuario como un experto confiable, agregamos información de múltiples trazas de sesiones de búsqueda de usuarios no confiables, como describimos en las siguientes dos secciones. 3. INTEGRANDO LA RETROALIMENTACIÓN IMPLÍCITA Consideramos dos enfoques complementarios para la clasificación con retroalimentación implícita: (1) tratar la retroalimentación implícita como evidencia independiente para clasificar resultados, y (2) integrar características de retroalimentación implícita directamente en el algoritmo de clasificación. Describimos a continuación los dos enfoques generales de clasificación. Las características específicas del feedback implícito se describen en la Sección 4, y los algoritmos para interpretar e incorporar el feedback implícito se describen en la Sección 5. 3.1 Feedback Implícito como Evidencia Independiente. El enfoque general es reordenar los resultados obtenidos por un motor de búsqueda web según los clics observados y otras interacciones de usuario para la consulta en sesiones de búsqueda anteriores. Cada resultado se le asigna una puntuación según la relevancia esperada/ satisfacción del usuario basada en interacciones previas, lo que resulta en un cierto orden de preferencia basado únicamente en las interacciones del usuario. Si bien ha habido un trabajo significativo en la fusión de múltiples clasificaciones, adaptamos un enfoque simple y robusto de ignorar las puntuaciones de los clasificadores originales, y en su lugar simplemente fusionamos los órdenes de clasificación. La razón principal para ignorar las puntuaciones originales es que, dado que los espacios de características y los algoritmos de aprendizaje son diferentes, las puntuaciones no son directamente comparables, y la re-normalización tiende a eliminar el beneficio de incorporar las puntuaciones del clasificador. Experimentamos con una variedad de funciones de fusión en el conjunto de desarrollo de consultas (y utilizando un conjunto de interacciones de un período de tiempo diferente de los conjuntos de evaluación final). Encontramos que una combinación heurística simple de fusión de rangos funciona bien y es robusta a variaciones en los valores de puntuación de los clasificadores originales. Para una consulta dada q, se calcula la puntuación implícita ISd para cada resultado d a partir de las características de interacción del usuario disponibles, lo que resulta en la clasificación implícita Id para cada resultado. Calculamos un puntaje combinado SM(d) para d al combinar los rangos obtenidos de la retroalimentación implícita, Id, con el rango original de d, Od: SM(d) = Od + wI * Id, si existe retroalimentación implícita; de lo contrario, SM(d) = Od. Donde el peso wI es un factor de escala ajustado heurísticamente que representa la importancia relativa de la retroalimentación implícita. Los resultados de la consulta se ordenan en valores decrecientes de SM para producir la clasificación final. Un caso especial de este modelo surge al establecer wI en un valor muy grande, lo que efectivamente obliga a que los resultados clicados se clasifiquen por encima de los resultados no clicados, una heurística intuitiva y efectiva que utilizaremos como referencia. Aplicar algoritmos de combinación de clasificadores y ordenadores más sofisticados puede resultar en mejoras adicionales, y es una dirección prometedora para trabajos futuros. El enfoque anterior asume que no hay interacciones entre las características subyacentes que producen la clasificación original de la búsqueda web y las características de retroalimentación implícitas. Ahora relajamos esta suposición al integrar características de retroalimentación implícita directamente en el proceso de clasificación. 3.2 Clasificación con Características de Retroalimentación Implícita Los motores de búsqueda web modernos clasifican los resultados en función de un gran número de características, incluidas las características basadas en el contenido (es decir, qué tan cerca coincide una consulta con el texto, el título o el texto de anclaje del documento) y las características de calidad de página independientes de la consulta (por ejemplo, PageRank del documento o del dominio). En la mayoría de los casos, se desarrollan métodos automáticos (o semiautomáticos) para ajustar la función de clasificación específica que combina estos valores de características. Por lo tanto, un enfoque natural es incorporar características de retroalimentación implícita directamente como características para el algoritmo de clasificación. Durante el entrenamiento o ajuste, el clasificador puede ser ajustado como antes pero con características adicionales. En tiempo de ejecución, el motor de búsqueda recuperaría las características de retroalimentación implícita asociadas con cada par de URL de consulta y resultado. Este modelo requiere que un algoritmo de clasificación sea robusto ante valores faltantes: más del 50% de las consultas a los motores de búsqueda web son únicas, sin disponer de retroalimentación implícita previa. Ahora describimos un clasificador que utilizamos para aprender sobre los conjuntos de características combinadas, incluyendo la retroalimentación implícita. 3.3 Aprendizaje para Clasificar los Resultados de Búsqueda en la Web Un aspecto clave de nuestro enfoque es aprovechar los avances recientes en el aprendizaje automático, en particular los algoritmos de clasificación entrenables para la búsqueda en la web y la <br>recuperación de información</br> (por ejemplo, [5, 11] y resultados clásicos revisados en [3]). En nuestro entorno, contamos con juicios explícitos de relevancia humana (etiquetas) para un conjunto de consultas de búsqueda en la web y resultados. Por lo tanto, una opción atractiva es utilizar una técnica de aprendizaje automático supervisado para aprender una función de clasificación que prediga mejor las evaluaciones de relevancia. RankNet es uno de esos algoritmos. Es un algoritmo de ajuste de red neuronal que optimiza los pesos de las características para que coincidan mejor con las preferencias de los usuarios proporcionadas explícitamente en pares. Si bien los algoritmos de entrenamiento específicos utilizados por RankNet están más allá del alcance de este documento, se describen en detalle en [5] e incluyen una evaluación exhaustiva y comparación con otros métodos de clasificación. Una característica atractiva de RankNet es su eficiencia tanto en el tiempo de entrenamiento como en el de ejecución: el ranking en tiempo de ejecución se puede calcular rápidamente y puede escalarse a la web, y el entrenamiento se puede realizar sobre miles de consultas y resultados juzgados asociados. Utilizamos una implementación de RankNet de 2 capas para modelar relaciones no lineales entre características. Además, RankNet puede aprender con muchas funciones de costo (diferenciables), por lo que puede aprender automáticamente una función de clasificación a partir de etiquetas proporcionadas por humanos, una alternativa atractiva a las técnicas de combinación de características heurísticas. Por lo tanto, también utilizaremos RankNet como un clasificador genérico para explorar la contribución de la retroalimentación implícita en diferentes alternativas de clasificación. 4. MODELO IMPLÍCITO DE RETROALIMENTACIÓN DEL USUARIO Nuestro objetivo es interpretar con precisión la retroalimentación ruidosa del usuario obtenida al rastrear las interacciones del usuario con el motor de búsqueda. Interpretar la retroalimentación implícita en un entorno real de búsqueda en la web no es una tarea fácil. Caracterizamos este problema en detalle en [1], donde motivamos y evaluamos una amplia variedad de modelos de actividades implícitas de usuario. El enfoque general es representar las acciones del usuario para cada resultado de búsqueda como un vector de características, y luego entrenar un clasificador en base a estas características para descubrir los valores de las características que indican resultados de búsqueda relevantes (y no relevantes). Primero resumimos brevemente nuestras características y modelo, y el enfoque de aprendizaje (Sección 4.2) para proporcionar suficiente información para replicar nuestros métodos de clasificación y los experimentos subsiguientes. 4.1 Representación de las acciones del usuario como características. Modelamos los comportamientos observados en la búsqueda web como una combinación de un \"componente de fondo (es decir, ruido independiente de la consulta y relevancia en el comportamiento del usuario, incluidos sesgos posicionales con interacciones de resultados), y un \"componente de relevancia (es decir, comportamiento específico de la consulta indicativo de la relevancia de un resultado para una consulta). Diseñamos nuestras características para aprovechar el comportamiento de usuario agregado. El conjunto de características está compuesto por características observadas directamente (calculadas directamente a partir de observaciones para cada consulta), así como características derivadas específicas de la consulta, calculadas como la desviación de la distribución general de valores independientes de la consulta para los valores de las características observadas directamente correspondientes. Las características utilizadas para representar las interacciones de los usuarios con los resultados de búsqueda en la web se resumen en la Tabla 4.1. Esta información se obtuvo a través de la instrumentación del lado del cliente opt-in de usuarios de un importante motor de búsqueda web. Incluimos las características tradicionales de retroalimentación implícita, como el recuento de clics en los resultados, así como nuestras características derivadas novedosas, como la desviación del número de clics observados para un par de consulta-URL dado del número esperado de clics en un resultado en la posición dada. También modelamos el comportamiento de navegación después de que se hace clic en un resultado, por ejemplo, el tiempo promedio de permanencia en la página para un par de consulta-URL dado, así como su desviación del tiempo de permanencia esperado (promedio). Además, el conjunto de características fue diseñado para proporcionar información esencial sobre la experiencia del usuario y hacer que la interpretación de la retroalimentación sea sólida. Por ejemplo, los usuarios de búsqueda en la web a menudo pueden determinar si un resultado es relevante al mirar el título del resultado, la URL y el resumen; en muchos casos, no es necesario mirar el documento original. Para modelar este aspecto de la experiencia del usuario, incluimos características como la superposición de palabras en el título y las palabras en la consulta (TitleOverlap) y la fracción de palabras compartidas por la consulta y el resumen del resultado. Características de clics Posición Posición de la URL en el ranking actual Frecuencia de clics Número de clics para esta consulta, par URL Probabilidad de clic Probabilidad de un clic para esta consulta y URL Desviación de clic Desviación de la probabilidad de clic esperada ¿Es el siguiente clic? 1 si se hizo clic en la siguiente posición, 0 de lo contrario ¿Es el clic anterior? 1 si se hizo clic en la posición anterior, 0 de lo contrario ¿Hay clic arriba? 1 si hay un clic arriba, 0 de lo contrario ¿Hay clic abajo? 1 si hay un clic abajo, 0 de lo contrario Características de navegación Tiempo en la página Tiempo de permanencia en la página Tiempo acumulado en la página Tiempo acumulado para todas las páginas siguientes después de la búsqueda Tiempo en el dominio Tiempo de permanencia acumulado para este dominio Tiempo en URL corta Tiempo acumulado en el prefijo de la URL, sin parámetros ¿Se siguió el enlace? 1 si se siguió el enlace al resultado, 0 de lo contrario ¿Coincidencia exacta de URL? 0 si se utilizó normalización agresiva, 1 de lo contrario ¿Redirigido? 1 si la URL inicial es la misma que la URL final, 0 de lo contrario ¿Camino desde la búsqueda? 1 si solo se siguieron enlaces después de la consulta, 0 de lo contrario Clics desde la búsqueda Número de saltos para llegar a la página desde la consulta Tiempo promedio de permanencia Tiempo promedio en la página para esta consulta Desviación del tiempo de permanencia Desviación del tiempo de permanencia promedio en la página Desviación acumulada Desviación del tiempo de permanencia acumulado promedio Desviación del dominio Desviación del tiempo de permanencia promedio en el dominio Características de texto de consulta Coincidencia de título Palabras compartidas entre la consulta y el título Coincidencia de resumen Palabras compartidas entre la consulta y el fragmento Coincidencia de URL de consulta Palabras compartidas entre la consulta y la URL Coincidencia de dominio de consulta Palabras compartidas entre la consulta y el dominio de la URL Longitud de la consulta Número de tokens en la consulta Superposición con la siguiente consulta Fracción de palabras compartidas con la siguiente consulta Tabla 4.1: Algunas características utilizadas para representar el historial de navegación posterior a la búsqueda para una consulta dada y una URL de resultado de búsqueda. Habiendo descrito nuestro conjunto de características, revisamos brevemente nuestro método general para derivar un modelo de comportamiento del usuario. 4.2 Derivación de un Modelo de Retroalimentación del Usuario Para aprender a interpretar el comportamiento observado del usuario, correlacionamos las acciones del usuario (es decir, las características en la Tabla 4.1 que representan las acciones) con los juicios explícitos del usuario para un conjunto de consultas de entrenamiento. Encontramos todas las instancias en nuestros registros de sesión donde se enviaron estas consultas al motor de búsqueda, y agregamos las características del comportamiento del usuario para todas las sesiones de búsqueda que involucran estas consultas. Cada par de consulta-URL observado está representado por las características en la Tabla 4.1, con valores promediados en todas las sesiones de búsqueda, y asignado una de las seis etiquetas de relevancia posibles, que van desde Perfecto hasta Malo, según los juicios explícitos de relevancia. Estos vectores de características etiquetados se utilizan como entrada para el algoritmo de entrenamiento RankNet (Sección 3.3), el cual produce un modelo de comportamiento de usuario entrenado. Este enfoque es particularmente atractivo ya que no requiere heurísticas más allá de la ingeniería de características. El modelo de comportamiento del usuario resultante se utiliza para ayudar a clasificar los resultados de búsqueda en la web, ya sea directamente o en combinación con otras características, como se describe a continuación. 5. CONFIGURACIÓN EXPERIMENTAL El objetivo final de incorporar retroalimentación implícita en la clasificación es mejorar la relevancia de los resultados de búsqueda web devueltos. Por lo tanto, comparamos los métodos de clasificación sobre un gran conjunto de consultas evaluadas con etiquetas de relevancia explícitas proporcionadas por jueces humanos. Para que la evaluación sea realista, obtuvimos una muestra aleatoria de consultas de registros de búsqueda web de un motor de búsqueda importante, con resultados asociados y rastros de acciones de usuario. Describimos este conjunto de datos en detalle a continuación. Nuestros indicadores se describen en la Sección 5.2 que utilizamos para evaluar las alternativas de clasificación, enumeradas en la Sección 5.3 en los experimentos de la Sección 6. 5.1 Conjuntos de datos Comparamos nuestros métodos de clasificación en una muestra aleatoria de 3,000 consultas de los registros de consultas del motor de búsqueda. Las consultas fueron extraídas de los registros de manera uniforme al azar por token sin reemplazo, lo que resultó en una muestra de consultas representativa de la distribución general de consultas. En promedio, 30 resultados fueron etiquetados explícitamente por jueces humanos utilizando una escala de seis puntos que va desde Perfecto hasta Malo. En total, hubo más de 83,000 resultados con juicios de relevancia explícitos. Para calcular diversas estadísticas, se considerarán relevantes los documentos con la etiqueta Buena o mejor, y los documentos con etiquetas inferiores se considerarán no relevantes. Se debe tener en cuenta que los experimentos se realizaron sobre los resultados ya altamente clasificados por un motor de búsqueda web, lo cual corresponde a una experiencia de usuario típica que se limita al pequeño número de resultados altamente clasificados para una consulta típica de búsqueda web. Las interacciones de los usuarios fueron recopiladas durante un período de 8 semanas utilizando información voluntaria de participación. En total, se registraron más de 1.2 millones de consultas únicas, lo que resultó en más de 12 millones de interacciones individuales con el motor de búsqueda. Los datos consistían en las interacciones de los usuarios con el motor de búsqueda web (por ejemplo, hacer clic en un enlace de resultado, regresar a los resultados de búsqueda, etc.) realizadas después de enviar una consulta. Estas acciones fueron agregadas entre usuarios y sesiones de búsqueda y convertidas en características en la Tabla 4.1. Para crear los conjuntos de consultas de entrenamiento, validación y prueba, creamos tres divisiones aleatorias diferentes de 1,500 consultas de entrenamiento, 500 de validación y 1000 de prueba. Las divisiones se realizaron aleatoriamente por consulta, de modo que no hubiera superposición en las consultas de entrenamiento, validación y prueba. 5.2 Métricas de Evaluación Evaluamos los algoritmos de clasificación en una variedad de métricas de <br>recuperación de información</br> aceptadas, a saber, Precisión en K (P(K)), Ganancia Acumulada Descontada Normalizada (NDCG) y Precisión Promedio Media (MAP). Cada métrica se enfoca en un aspecto diferente del rendimiento del sistema, como describimos a continuación. • Precisión en K: Como la métrica más intuitiva, P(K) informa la fracción de documentos clasificados en los primeros K resultados que están etiquetados como relevantes. En nuestro entorno, requerimos que un documento relevante sea etiquetado como Bueno o superior. La posición de los documentos relevantes dentro de los primeros K no es relevante, por lo que esta métrica mide la satisfacción general del usuario con los resultados principales de K. • NDCG en K: NDCG es una medida de recuperación diseñada específicamente para la evaluación de búsqueda en la web [10]. Para una consulta dada q, los resultados clasificados se examinan desde el mejor clasificado hacia abajo, y el NDCG se calcula como: = +−= K j jr qq jMN 1 )( )1log(/)12( Donde Mq es una constante de normalización calculada de manera que un orden perfecto obtendría un NDCG de 1; y cada r(j) es una etiqueta de relevancia entera (0=Malo y 5=Perfecto) del resultado devuelto en la posición j. Ten en cuenta que los documentos sin etiquetar y los documentos malos no contribuyen a la suma, pero reducirán el NDCG para la consulta al empujar hacia abajo los documentos etiquetados relevantes, disminuyendo sus contribuciones. NDCG es muy adecuado para la evaluación de búsquedas en la web, ya que recompensa de manera más intensa los documentos relevantes en los resultados mejor clasificados que aquellos clasificados más bajos. • MAP: La precisión promedio para cada consulta se define como la media de la precisión en los valores de K calculados después de que se recuperó cada documento relevante. El valor MAP final se define como la media de las precisiones promedio de todas las consultas en el conjunto de pruebas. Esta métrica es el resumen de un solo valor más comúnmente utilizado de una ejecución sobre un conjunto de consultas. 5.3 Métodos de clasificación comparados. Recuerde que nuestro objetivo es cuantificar la efectividad del comportamiento implícito para la búsqueda web real. Una dimensión es comparar la utilidad de la retroalimentación implícita con otra información disponible para un motor de búsqueda web. Específicamente, comparamos la efectividad de los comportamientos implícitos de los usuarios con la coincidencia basada en el contenido, las características de calidad de la página estática y combinaciones de todas las características. • BM25F: Como referencia sólida de búsqueda web, utilizamos la puntuación BM25F, que se utilizó en uno de los sistemas con mejor rendimiento en la pista web TREC 2004 [23,27]. BM25F y sus variantes han sido extensamente descritas y evaluadas en la literatura de IR, por lo tanto, sirven como una línea base sólida y reproducible. La variante BM25F que utilizamos en nuestros experimentos calcula puntuaciones de coincidencia separadas para cada campo de un documento de resultado (por ejemplo, texto del cuerpo, título y texto del enlace) e incorpora información de enlaces independiente de la consulta (por ejemplo, PageRank, ClickDistance y profundidad de URL). La función de puntuación y la ajuste específico del campo se describen en detalle en [23]. Tenga en cuenta que BM25F no considera directamente la retroalimentación explícita o implícita para ajuste. • RN: La clasificación producida por un clasificador de redes neuronales (RankNet, descrito en la Sección 3.3) que aprende a clasificar los resultados de búsqueda web al incorporar BM25F y un gran número de características estáticas y dinámicas adicionales que describen cada resultado de búsqueda. Este sistema aprende automáticamente los pesos de todas las características (incluido el puntaje BM25F para un documento) basándose en etiquetas humanas explícitas para un gran conjunto de consultas. Un sistema que incorpora una implementación de RankNet actualmente está siendo utilizado por un importante motor de búsqueda y puede considerarse representativo del estado del arte en la búsqueda web. • BM25F-RerankCT: La clasificación producida al incorporar estadísticas de clics para reordenar los resultados de búsqueda web clasificados por BM25F arriba. El clic es un caso especial particularmente importante de retroalimentación implícita, y se ha demostrado que se correlaciona con la relevancia de los resultados. Este es un caso especial del método de clasificación en la Sección 3.1, con el peso wI establecido en 1000 y la clasificación Id es simplemente el número de clics en el resultado correspondiente a d. En efecto, esta clasificación coloca en la parte superior todos los resultados de búsqueda web devueltos con al menos un clic (y los ordena en orden decreciente por número de clics). La clasificación relativa de los resultados restantes no cambia y se insertan debajo de todos los resultados clicados. Este método sirve como nuestro método de reordenamiento de retroalimentación implícita de referencia. La clasificación producida al reordenar los resultados de BM25F utilizando todas las características del comportamiento del usuario (Sección 4). Este método aprende un modelo de preferencias de usuario correlacionando los valores de las características con etiquetas de relevancia explícitas utilizando el algoritmo de red neuronal RankNet (Sección 4.2). En tiempo de ejecución, para una consulta dada se calcula la puntuación implícita Ir para cada resultado r con características de interacción de usuario disponibles, y se produce el ranking implícito. La clasificación combinada se calcula como se describe en la Sección 3.1. Basándonos en los experimentos realizados sobre el conjunto de desarrollo, fijamos el valor de wI en 3 (el efecto del parámetro wI para este clasificador resultó ser insignificante). • BM25F+All: Clasificación derivada al entrenar el aprendiz RankNet (Sección 3.3) sobre el conjunto de características del puntaje BM25F, así como todas las características de retroalimentación implícita (Sección 3.2). Utilizamos la implementación de 2 capas de RankNet [5] entrenada en las consultas y etiquetas de los conjuntos de entrenamiento y validación. • RN+All: Clasificación derivada al entrenar el algoritmo de clasificación RankNet de 2 capas (Sección 3.3) sobre la unión de todas las características de retroalimentación de contenido, dinámicas e implícitas (es decir, todas las características descritas anteriormente, así como todas las nuevas características de retroalimentación implícita que introdujimos). Los métodos de clasificación anteriores abarcan el rango de la información utilizada para clasificar, desde no utilizar la retroalimentación implícita o explícita en absoluto (es decir, BM25F) hasta un motor de búsqueda web moderno que utiliza cientos de características y está ajustado en base a juicios explícitos (RN). Como mostraremos a continuación, incorporar el comportamiento del usuario en estos sistemas de clasificación mejora drásticamente la relevancia de los documentos devueltos. 6. RESULTADOS EXPERIMENTALES El feedback implícito para la clasificación de búsquedas en la web puede ser explotado de diversas formas. Comparamos métodos alternativos para explotar la retroalimentación implícita, tanto reordenando los resultados principales (es decir, los métodos BM25F-RerankCT y BM25F-RerankAll que reordenan los resultados de BM25F), como integrando directamente las características implícitas en el proceso de clasificación (es decir, los métodos RN+ALL y BM25F+All que aprenden a clasificar los resultados sobre la retroalimentación implícita y otras características). Comparamos nuestros métodos con baselines sólidos (BM25F y RN) sobre las medidas NDCG, Precisión en K y MAP definidas en la Sección 5.2. Los resultados se promediaron en tres divisiones aleatorias del conjunto de datos completo. Cada división contenía 1500 consultas de entrenamiento, 500 de validación y 1000 de prueba, todos los conjuntos de consultas disjuntos. Primero presentamos los resultados de las 1000 consultas de prueba (es decir, incluyendo las consultas para las cuales no hay medidas implícitas, por lo que utilizamos las clasificaciones web originales). Luego profundizamos para examinar los efectos en la reorganización de los intentos de búsqueda con más detalle, analizando dónde el feedback implícito resultó más beneficioso. Primero experimentamos con diferentes métodos de volver a clasificar la salida de los resultados de búsqueda de BM25F. Las figuras 6.1 y 6.2 informan sobre NDCG y Precisión para BM25F, así como para las estrategias de volver a clasificar los resultados con retroalimentación del usuario (Sección 3.1). Incorporar todos los comentarios de los usuarios (ya sea en el marco de reordenamiento o como características directamente al aprendiz) resulta en mejoras significativas (utilizando una prueba t de dos colas con p=0.01) tanto sobre la clasificación original de BM25F como sobre el reordenamiento solo con clics. La mejora es consistente en los 10 mejores resultados y es mayor para el mejor resultado: NDCG en 1 para BM25F+All es de 0.622 en comparación con 0.518 de los resultados originales, y la precisión en 1 también aumenta de 0.5 a 0.63. Basándonos en estos resultados, utilizaremos el clasificador de combinación de características directas (es decir, BM25F+All) para comparaciones posteriores que involucren retroalimentación implícita. Curiosamente, el uso solo de clics, aunque proporciona un beneficio significativo sobre la clasificación original de BM25F, no es tan efectivo como considerar el conjunto completo de características en la Tabla 4.1. Mientras analizamos el comportamiento del usuario (y las características de los componentes más efectivos) en un documento separado [1], vale la pena dar un ejemplo concreto del tipo de ruido inherente en la retroalimentación real de los usuarios en el entorno de búsqueda web. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 Posición del resultado Frecuencia de clics relativa PTR=2 PTR=3 PTR=5 Figura 6.3: Frecuencia relativa de clics para consultas con diferentes Posiciones del Resultado Relevante Superior (PTR). Si los usuarios solo consideraran la relevancia de un resultado para su consulta, harían clic en los resultados más relevantes en la parte superior. Desafortunadamente, como han demostrado Joachims y otros, la presentación también influye de manera bastante dramática en los resultados en los que los usuarios hacen clic. Los usuarios a menudo hacen clic en los resultados por encima del relevante, presumiblemente porque los resúmenes cortos no proporcionan suficiente información para hacer evaluaciones precisas de relevancia y han aprendido que, en promedio, los elementos mejor clasificados son relevantes. La Figura 6.3 muestra las frecuencias relativas de clics para consultas con elementos relevantes conocidos en posiciones distintas a la primera posición; la posición del resultado relevante superior (PTR) varía de 2 a 10 en la figura. Por ejemplo, para consultas con el primer resultado relevante en la posición 5 (PTR=5), hay más clics en los resultados no relevantes en posiciones más altas que en el primer resultado relevante en la posición 5. Como veremos, el aprendizaje sobre un conjunto de características de comportamiento más amplio resulta en una mejora sustancial en la precisión en comparación con solo el clic. Ahora consideramos incorporar el comportamiento del usuario en un conjunto de características mucho más amplio, RN (Sección 5.3) utilizado por un importante motor de búsqueda web. RN incorpora BM25F, características basadas en enlaces y cientos de otras características. La Figura 6.4 informa sobre NDCG en K y la Figura 6.5 informa sobre Precisión en K. Curiosamente, aunque las clasificaciones originales de RN son significativamente más precisas que BM25F solo, la incorporación de características de retroalimentación implícita (BM25F+All) da como resultado una clasificación que supera significativamente a las clasificaciones originales de RN. En otras palabras, la retroalimentación implícita incorpora suficiente información para reemplazar las cientos de otras características disponibles para el aprendiz de RankNet entrenado en el conjunto de características RN. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 1 2 3 4 5 6 7 8 9 10K NDCG RN RN+All BM25 BM25+All Figura 6.4: NDCG en K para BM25F, BM25F+All, RN y RN+All para diferentes K Además, enriquecer las características de RN con el conjunto de retroalimentación implícita muestra una ganancia significativa en todas las medidas, permitiendo que RN+All supere a todos los demás métodos. Esto demuestra la naturaleza complementaria de la retroalimentación implícita con otras características disponibles para un motor de búsqueda web de última generación. 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 Precisión RN RN+Todo BM25 BM25+Todo Figura 6.5: Precisión en K para BM25F, BM25F+Todo, RN y RN+Todo para diferentes valores de K. Resumimos el rendimiento de los diferentes métodos de clasificación en la Tabla 6.1. Informamos el puntaje de Precisión Promedio Media (MAP) para cada sistema. Aunque no es intuitivo de interpretar, el MAP permite la comparación cuantitativa en una sola métrica. Las ganancias marcadas con * son significativas a un nivel de p=0.01 utilizando una prueba t de dos colas. Ganancia MAP P(1) Ganancia BM25F 0.184 - 0.503BM25F-Rerank-CT 0.215 0.031* 0.577 0.073* BM25F-RerankImplicit 0.218 0.003 0.605 0.028* BM25F+Implicit 0.222 0.004 0.620 0.015* RN 0.215 - 0.597RN+All 0.248 0.033* 0.629 0.032* Tabla 6.1: Precisión Promedio (MAP) para todas las estrategias. Hasta ahora hemos informado resultados promediados en todas las consultas del conjunto de pruebas. Desafortunadamente, menos de la mitad tuvo interacciones suficientes para intentar un nuevo ranking. De las 1000 consultas en la prueba, entre el 46% y el 49%, dependiendo de la división entre entrenamiento y prueba, tenían suficiente información de interacción para hacer predicciones (es decir, hubo al menos 1 sesión de búsqueda en la que el usuario hizo clic en al menos 1 URL de resultado). Esto no es sorprendente: la búsqueda en la web tiene una distribución de cola pesada y hay muchas consultas únicas. Ahora consideramos el rendimiento en las consultas para las cuales estaban disponibles las interacciones de los usuarios. La Figura 6.6 informa sobre el NDCG para el subconjunto de las consultas de prueba con las características de retroalimentación implícita. Las ganancias en el primer puesto son dramáticas. El NDCG en 1 de BM25F+All aumenta de 0.6 a 0.75 (un aumento relativo del 31%), logrando un rendimiento comparable al de RN+All operando sobre un conjunto de características mucho más rico. 0.6 0.65 0.7 0.75 0.8 1 3 5 10K NDCG RN RN+All BM25 BM25+All Figura 6.6: NDCG en K para BM25F, BM25F+All, RN y RN+All en consultas de prueba con interacciones de usuario. Del mismo modo, las ganancias en precisión en el top 1 son sustanciales (Figura 6.7) y es probable que sean evidentes para los usuarios de búsqueda web. Cuando se dispone de retroalimentación implícita, el sistema BM25F+All devuelve el documento relevante en la parte superior 1 casi el 70% del tiempo, en comparación con el 53% del tiempo cuando la retroalimentación implícita no es considerada por el sistema BM25F original. 0.45 0.5 0.55 0.6 0.65 0.7 1 3 5 10K Precisión RN RN+All BM25 BM25+All Figura 6.7: Precisión en K NDCG en K para BM25F, BM25F+All, RN y RN+All en consultas de prueba con interacciones de usuario Resumimos los resultados en la medida MAP para las consultas intentadas en la Tabla 6.2. Las mejoras en MAP son tanto sustanciales como significativas, siendo más pronunciadas que las mejoras en el clasificador BM25F. Ahora analizamos los casos en los que la retroalimentación implícita resultó ser más útil. La Figura 6.8 informa las mejoras de MAP sobre la ejecución de BM25F base para cada consulta con MAP inferior a 0.6. Ten en cuenta que la mayoría de la mejora es para consultas con bajo rendimiento (es decir, MAP < 0.1). Curiosamente, la incorporación de información sobre el comportamiento del usuario disminuye la precisión para consultas con un puntaje MAP original alto. Una posible explicación es que estas consultas fáciles tienden a ser de navegación (es decir, tener una única respuesta apropiada altamente clasificada), y las interacciones de los usuarios con resultados de menor rango pueden indicar necesidades de información divergentes que son mejor atendidas por los resultados menos populares (con calificaciones de relevancia global correspondientemente bajas). Para resumir nuestros resultados experimentales, la incorporación de retroalimentación implícita en un entorno real de búsqueda web resultó en mejoras significativas sobre las clasificaciones originales, utilizando tanto BM25F como RN como líneas de base. Nuestro amplio conjunto de características implícitas, como el tiempo en la página y las desviaciones del comportamiento promedio, ofrece ventajas sobre el uso exclusivo del clic como indicador de interés. Además, incorporar características de retroalimentación implícita directamente en la función de clasificación aprendida es más efectivo que utilizar la retroalimentación implícita para volver a clasificar. Las mejoras observadas en grandes conjuntos de pruebas de consultas (1,000 en total, entre 466 y 495 con retroalimentación implícita disponible) son tanto sustanciales como estadísticamente significativas. 7. CONCLUSIONES Y TRABAJOS FUTUROS En este artículo exploramos la utilidad de incorporar retroalimentación implícita ruidosa obtenida en un entorno real de búsqueda web para mejorar la clasificación de búsqueda web. Realizamos una evaluación a gran escala con más de 3,000 consultas y más de 12 millones de interacciones de usuarios con un motor de búsqueda importante, estableciendo la utilidad de incorporar retroalimentación implícita ruidosa para mejorar la relevancia de la búsqueda web. Comparamos dos alternativas para incorporar retroalimentación implícita en el proceso de búsqueda, a saber, reordenar con retroalimentación implícita e incorporar directamente características de retroalimentación implícita en la función de clasificación entrenada. Nuestros experimentos mostraron una mejora significativa sobre los métodos que no consideran la retroalimentación implícita. Las ganancias son particularmente dramáticas para el resultado superior K=1 en la clasificación final, con mejoras de precisión de hasta un 31%, y las ganancias son sustanciales para todos los valores de K. Nuestros experimentos mostraron que el feedback implícito del usuario puede mejorar aún más el rendimiento de la búsqueda web, cuando se incorpora directamente con características populares basadas en contenido y enlaces. Curiosamente, la retroalimentación implícita es especialmente valiosa para consultas con una clasificación original deficiente de resultados (por ejemplo, MAP inferior a 0.1). Una dirección prometedora para trabajos futuros es aplicar la investigación reciente sobre la predicción automática de la dificultad de las consultas, e intentar incorporar únicamente retroalimentación implícita para las consultas difíciles. Como otra dirección de investigación, estamos explorando métodos para extender nuestras predicciones a las consultas previamente no vistas (por ejemplo, el agrupamiento de consultas), lo cual debería mejorar aún más la experiencia de búsqueda en la web de los usuarios. AGRADECIMIENTOS Agradecemos a Chris Burges y Matt Richardson por la implementación de RankNet para nuestros experimentos. También agradecemos a Robert Ragno por sus valiosas sugerencias y muchas discusiones. 8. REFERENCIAS [1] E. Agichtein, E. Brill, S. Dumais y R. Ragno, Aprendizaje de modelos de interacción del usuario para predecir las preferencias de resultados de búsqueda web. En Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2006 [2] J. Allan, Resumen de la Pista HARD en TREC 2003, Recuperación de Alta Precisión de Documentos, 2003 [3] R. Baeza-Yates y B. Ribeiro-Neto, Recuperación de Información Moderna, Addison-Wesley, 1999. [4] S. Brin y L. Page, Anatomía de un Motor de Búsqueda Web Hipertextual a Gran Escala, en Actas de WWW, 1997 [5] C.J.C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. Hullender, Aprendizaje para clasificación utilizando descenso de gradiente, en Actas de la Conferencia Internacional sobre Aprendizaje Automático, 2005 [6] D.M. Chickering, The WinMine Toolkit, Informe Técnico de Microsoft MSR-TR-2002-103, 2002 [7] M. Claypool, D. Brown, P. Lee y M. Waseda. Inferir el interés del usuario. IEEE Internet Computing. 2001 [8] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais y T. White. Evaluando medidas implícitas para mejorar la experiencia de búsqueda. En ACM Transactions on Information Systems, 2005 [9] J. Goecks y J. Shavlick. Aprendiendo los intereses de los usuarios observando de manera discreta su comportamiento normal. En Actas del Taller de IJCAI sobre Aprendizaje Automático para Filtrado de Información. 1999. [10] K Jarvelin y J. Kekalainen. Métodos de evaluación IR para recuperar documentos altamente relevantes. En las Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2000 [11] T. Joachims, Optimización de Motores de Búsqueda Utilizando Datos de Clics. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "user behavior": {
            "translated_key": "comportamiento del usuario",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Improving Web Search Ranking by Incorporating <br>user behavior</br> Information Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com ABSTRACT We show that incorporating <br>user behavior</br> data can significantly improve ordering of top results in real web search setting.",
                "We examine alternatives for incorporating feedback into the ranking process and explore the contributions of user feedback compared to other common web search features.",
                "We report results of a large scale evaluation over 3,000 queries and 12 million user interactions with a popular web search engine.",
                "We show that incorporating implicit feedback can augment other features, improving the accuracy of a competitive web search ranking algorithms by as much as 31% relative to the original performance.",
                "Categories and Subject Descriptors H.3.3 Information Search and Retrieval - Relevance feedback, search process; H.3.5 Online Information Services - Web-based services.",
                "General Terms Algorithms, Measurement, Experimentation 1.",
                "INTRODUCTION Millions of users interact with search engines daily.",
                "They issue queries, follow some of the links in the results, click on ads, spend time on pages, reformulate their queries, and perform other actions.",
                "These interactions can serve as a valuable source of information for tuning and improving web search result ranking and can compliment more costly explicit judgments.",
                "Implicit relevance feedback for ranking and personalization has become an active area of research.",
                "Recent work by Joachims and others exploring implicit feedback in controlled environments have shown the value of incorporating implicit feedback into the ranking process.",
                "Our motivation for this work is to understand how implicit feedback can be used in a large-scale operational environment to improve retrieval.",
                "How does it compare to and compliment evidence from page content, anchor text, or link-based features such as inlinks or PageRank?",
                "While it is intuitive that user interactions with the web search engine should reveal at least some information that could be used for ranking, estimating user preferences in real web search settings is a challenging problem, since real user interactions tend to be more noisy than commonly assumed in the controlled settings of previous studies.",
                "Our paper explores whether implicit feedback can be helpful in realistic environments, where user feedback can be noisy (or adversarial) and a web search engine already uses hundreds of features and is heavily tuned.",
                "To this end, we explore different approaches for ranking web search results using real <br>user behavior</br> obtained as part of normal interactions with the web search engine.",
                "The specific contributions of this paper include: • Analysis of alternatives for incorporating <br>user behavior</br> into web search ranking (Section 3). • An application of a robust implicit feedback model derived from mining millions of user interactions with a major web search engine (Section 4). • A large scale evaluation over real user queries and search results, showing significant improvements derived from incorporating user feedback (Section 6).",
                "We summarize our findings and discuss extensions to the current work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "Most common approaches primarily focus on similarity of query and a page, as well as the overall page quality [3,4,24].",
                "However, with increasing popularity of search engines, implicit feedback (i.e., the actions users take when interacting with the search engine) can be used to improve the rankings.",
                "Implicit relevance measures have been studied by several research groups.",
                "An overview of implicit measures is compiled in Kelly and Teevan [14].",
                "This research, while developing valuable insights into implicit relevance measures, was not applied to improve the ranking of web search results in realistic settings.",
                "Closely related to our work, Joachims [11] collected implicit measures in place of explicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "Fox et al. [8] explored the relationship between implicit and explicit measures in Web search, and developed Bayesian models to correlate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "This work considered a wide range of user behaviors (e.g., dwell time, scroll time, reformulation patterns) in addition to the popular clickthrough behavior.",
                "However, the modeling effort was aimed at predicting explicit relevance judgments from implicit user actions and not specifically at learning ranking functions.",
                "Other studies of <br>user behavior</br> in web search include Pharo and Järvelin [19], but were not directly applied to improve ranking.",
                "More recently, Joachims et al. [12] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthroughs in a controlled, laboratory setting.",
                "Unfortunately, the extent to which previous research applies to real-world web search is unclear.",
                "At the same time, while recent work (e.g., [26]) on using clickthrough information for improving web search ranking is promising, it captures only one aspect of the user interactions with web search engines.",
                "We build on existing research to develop robust <br>user behavior</br> interpretation techniques for the real web search setting.",
                "Instead of treating each user as a reliable expert, we aggregate information from multiple, unreliable, user search session traces, as we describe in the next two sections. 3.",
                "INCORPORATING IMPLICIT FEEDBACK We consider two complementary approaches to ranking with implicit feedback: (1) treating implicit feedback as independent evidence for ranking results, and (2) integrating implicit feedback features directly into the ranking algorithm.",
                "We describe the two general ranking approaches next.",
                "The specific implicit feedback features are described in Section 4, and the algorithms for interpreting and incorporating implicit feedback are described in Section 5. 3.1 Implicit Feedback as Independent Evidence The general approach is to re-rank the results obtained by a web search engine according to observed clickthrough and other user interactions for the query in previous search sessions.",
                "Each result is assigned a score according to expected relevance/user satisfaction based on previous interactions, resulting in some preference ordering based on user interactions alone.",
                "While there has been significant work on merging multiple rankings, we adapt a simple and robust approach of ignoring the original rankers scores, and instead simply merge the rank orders.",
                "The main reason for ignoring the original scores is that since the feature spaces and learning algorithms are different, the scores are not directly comparable, and re-normalization tends to remove the benefit of incorporating classifier scores.",
                "We experimented with a variety of merging functions on the development set of queries (and using a set of interactions from a different time period from final evaluation sets).",
                "We found that a simple rank merging heuristic combination works well, and is robust to variations in score values from original rankers.",
                "For a given query q, the implicit score ISd is computed for each result d from available user interaction features, resulting in the implicit rank Id for each result.",
                "We compute a merged score SM(d) for d by combining the ranks obtained from implicit feedback, Id with the original rank of d, Od:     ¡     ¢ £ + + + + = otherwise O dforexistsfeedbackimplicitif OI w wOIdS d dd I IddM 1 1 1 1 1 1 ),,,( where the weight wI is a heuristically tuned scaling factor representing the relative importance of the implicit feedback.",
                "The query results are ordered in by decreasing values of SM to produce the final ranking.",
                "One special case of this model arises when setting wI to a very large value, effectively forcing clicked results to be ranked higher than un-clicked results - an intuitive and effective heuristic that we will use as a baseline.",
                "Applying more sophisticated classifier and ranker combination algorithms may result in additional improvements, and is a promising direction for future work.",
                "The approach above assumes that there are no interactions between the underlying features producing the original web search ranking and the implicit feedback features.",
                "We now relax this assumption by integrating implicit feedback features directly into the ranking process. 3.2 Ranking with Implicit Feedback Features Modern web search engines rank results based on a large number of features, including content-based features (i.e., how closely a query matches the text or title or anchor text of the document), and query-independent page quality features (e.g., PageRank of the document or the domain).",
                "In most cases, automatic (or semiautomatic) methods are developed for tuning the specific ranking function that combines these feature values.",
                "Hence, a natural approach is to incorporate implicit feedback features directly as features for the ranking algorithm.",
                "During training or tuning, the ranker can be tuned as before but with additional features.",
                "At runtime, the search engine would fetch the implicit feedback features associated with each query-result URL pair.",
                "This model requires a ranking algorithm to be robust to missing values: more than 50% of queries to web search engines are unique, with no previous implicit feedback available.",
                "We now describe such a ranker that we used to learn over the combined feature sets including implicit feedback. 3.3 Learning to Rank Web Search Results A key aspect of our approach is exploiting recent advances in machine learning, namely trainable ranking algorithms for web search and information retrieval (e.g., [5, 11] and classical results reviewed in [3]).",
                "In our setting, explicit human relevance judgments (labels) are available for a set of web search queries and results.",
                "Hence, an attractive choice to use is a supervised machine learning technique to learn a ranking function that best predicts relevance judgments.",
                "RankNet is one such algorithm.",
                "It is a neural net tuning algorithm that optimizes feature weights to best match explicitly provided pairwise user preferences.",
                "While the specific training algorithms used by RankNet are beyond the scope of this paper, it is described in detail in [5] and includes extensive evaluation and comparison with other ranking methods.",
                "An attractive feature of RankNet is both train- and run-time efficiency - runtime ranking can be quickly computed and can scale to the web, and training can be done over thousands of queries and associated judged results.",
                "We use a 2-layer implementation of RankNet in order to model non-linear relationships between features.",
                "Furthermore, RankNet can learn with many (differentiable) cost functions, and hence can automatically learn a ranking function from human-provided labels, an attractive alternative to heuristic feature combination techniques.",
                "Hence, we will also use RankNet as a generic ranker to explore the contribution of implicit feedback for different ranking alternatives. 4.",
                "IMPLICIT USER FEEDBACK MODEL Our goal is to accurately interpret noisy user feedback obtained as by tracing user interactions with the search engine.",
                "Interpreting implicit feedback in real web search setting is not an easy task.",
                "We characterize this problem in detail in [1], where we motivate and evaluate a wide variety of models of implicit user activities.",
                "The general approach is to represent user actions for each search result as a vector of features, and then train a ranker on these features to discover feature values indicative of relevant (and nonrelevant) search results.",
                "We first briefly summarize our features and model, and the learning approach (Section 4.2) in order to provide sufficient information to replicate our ranking methods and the subsequent experiments. 4.1 Representing User Actions as Features We model observed web search behaviors as a combination of a ``background component (i.e., query- and relevance-independent noise in <br>user behavior</br>, including positional biases with result interactions), and a ``relevance component (i.e., query-specific behavior indicative of relevance of a result to a query).",
                "We design our features to take advantage of aggregated <br>user behavior</br>.",
                "The feature set is comprised of directly observed features (computed directly from observations for each query), as well as queryspecific derived features, computed as the deviation from the overall query-independent distribution of values for the corresponding directly observed feature values.",
                "The features used to represent user interactions with web search results are summarized in Table 4.1.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "We include the traditional implicit feedback features such as clickthrough counts for the results, as well as our novel derived features such as the deviation of the observed clickthrough number for a given query-URL pair from the expected number of clicks on a result in the given position.",
                "We also model the browsing behavior after a result was clicked - e.g., the average page dwell time for a given query-URL pair, as well as its deviation from the expected (average) dwell time.",
                "Furthermore, the feature set was designed to provide essential information about the user experience to make feedback interpretation robust.",
                "For example, web search users can often determine whether a result is relevant by looking at the result title, URL, and summary - in many cases, looking at the original document is not necessary.",
                "To model this aspect of user experience we include features such as overlap in words in title and words in query (TitleOverlap) and the fraction of words shared by the query and the result summary.",
                "Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickProbability Probability of a click for this query and URL ClickDeviation Deviation from expected click probability IsNextClicked 1 if clicked on next position, 0 otherwise IsPreviousClicked 1 if clicked on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, no parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from average dwell time on page CumulativeDeviation Deviation from average cumulative dwell time DomainDeviation Deviation from average dwell time on domain Query-text features TitleOverlap Words shared between query and title SummaryOverlap Words shared between query and snippet QueryURLOverlap Words shared between query and URL QueryDomainOverlap Words shared between query and URL domain QueryLength Number of tokens in query QueryNextOverlap Fraction of words shared with next query Table 4.1: Some features used to represent post-search navigation history for a given query and search result URL.",
                "Having described our feature set, we briefly review our general method for deriving a <br>user behavior</br> model. 4.2 Deriving a User Feedback Model To learn to interpret the observed <br>user behavior</br>, we correlate user actions (i.e., the features in Table 4.1 representing the actions) with the explicit user judgments for a set of training queries.",
                "We find all the instances in our session logs where these queries were submitted to the search engine, and aggregate the <br>user behavior</br> features for all search sessions involving these queries.",
                "Each observed query-URL pair is represented by the features in Table 4.1, with values averaged over all search sessions, and assigned one of six possible relevance labels, ranging from Perfect to Bad, as assigned by explicit relevance judgments.",
                "These labeled feature vectors are used as input to the RankNet training algorithm (Section 3.3) which produces a trained <br>user behavior</br> model.",
                "This approach is particularly attractive as it does not require heuristics beyond feature engineering.",
                "The resulting <br>user behavior</br> model is used to help rank web search resultseither directly or in combination with other features, as described below. 5.",
                "EXPERIMENTAL SETUP The ultimate goal of incorporating implicit feedback into ranking is to improve the relevance of the returned web search results.",
                "Hence, we compare the ranking methods over a large set of judged queries with explicit relevance labels provided by human judges.",
                "In order for the evaluation to be realistic we obtained a random sample of queries from web search logs of a major search engine, with associated results and traces for user actions.",
                "We describe this dataset in detail next.",
                "Our metrics are described in Section 5.2 that we use to evaluate the ranking alternatives, listed in Section 5.3 in the experiments of Section 6. 5.1 Datasets We compared our ranking methods over a random sample of 3,000 queries from the search engine query logs.",
                "The queries were drawn from the logs uniformly at random by token without replacement, resulting in a query sample representative of the overall query distribution.",
                "On average, 30 results were explicitly labeled by human judges using a six point scale ranging from Perfect down to Bad.",
                "Overall, there were over 83,000 results with explicit relevance judgments.",
                "In order to compute various statistics, documents with label Good or better will be considered relevant, and with lower labels to be non-relevant.",
                "Note that the experiments were performed over the results already highly ranked by a web search engine, which corresponds to a typical user experience which is limited to the small number of the highly ranked results for a typical web search query.",
                "The user interactions were collected over a period of 8 weeks using voluntary opt-in information.",
                "In total, over 1.2 million unique queries were instrumented, resulting in over 12 million individual interactions with the search engine.",
                "The data consisted of user interactions with the web search engine (e.g., clicking on a result link, going back to search results, etc.) performed after a query was submitted.",
                "These actions were aggregated across users and search sessions and converted to features in Table 4.1.",
                "To create the training, validation, and test query sets, we created three different random splits of 1,500 training, 500 validation, and 1000 test queries.",
                "The splits were done randomly by query, so that there was no overlap in training, validation, and test queries. 5.2 Evaluation Metrics We evaluate the ranking algorithms over a range of accepted information retrieval metrics, namely Precision at K (P(K)), Normalized Discounted Cumulative Gain (NDCG), and Mean Average Precision (MAP).",
                "Each metric focuses on a deferent aspect of system performance, as we describe below. • Precision at K: As the most intuitive metric, P(K) reports the fraction of documents ranked in the top K results that are labeled as relevant.",
                "In our setting, we require a relevant document to be labeled Good or higher.",
                "The position of relevant documents within the top K is irrelevant, and hence this metric measure overall user satisfaction with the top K results. • NDCG at K: NDCG is a retrieval measure devised specifically for web search evaluation [10].",
                "For a given query q, the ranked results are examined from the top ranked down, and the NDCG computed as:   = +−= K j jr qq jMN 1 )( )1log(/)12( Where Mq is a normalization constant calculated so that a perfect ordering would obtain NDCG of 1; and each r(j) is an integer relevance label (0=Bad and 5=Perfect) of result returned at position j.",
                "Note that unlabeled and Bad documents do not contribute to the sum, but will reduce NDCG for the query pushing down the relevant labeled documents, reducing their contributions.",
                "NDCG is well suited to web search evaluation, as it rewards relevant documents in the top ranked results more heavily than those ranked lower. • MAP: Average precision for each query is defined as the mean of the precision at K values computed after each relevant document was retrieved.",
                "The final MAP value is defined as the mean of average precisions of all queries in the test set.",
                "This metric is the most commonly used single-value summary of a run over a set of queries. 5.3 Ranking Methods Compared Recall that our goal is to quantify the effectiveness of implicit behavior for real web search.",
                "One dimension is to compare the utility of implicit feedback with other information available to a web search engine.",
                "Specifically, we compare effectiveness of implicit user behaviors with content-based matching, static page quality features, and combinations of all features. • BM25F: As a strong web search baseline we used the BM25F scoring, which was used in one of the best performing systems in the TREC 2004 Web track [23,27].",
                "BM25F and its variants have been extensively described and evaluated in IR literature, and hence serve as a strong, reproducible baseline.",
                "The BM25F variant we used for our experiments computes separate match scores for each field for a result document (e.g., body text, title, and anchor text), and incorporates query-independent linkbased information (e.g., PageRank, ClickDistance, and URL depth).",
                "The scoring function and field-specific tuning is described in detail in [23].",
                "Note that BM25F does not directly consider explicit or implicit feedback for tuning. • RN: The ranking produced by a neural net ranker (RankNet, described in Section 3.3) that learns to rank web search results by incorporating BM25F and a large number of additional static and dynamic features describing each search result.",
                "This system automatically learns weights for all features (including the BM25F score for a document) based on explicit human labels for a large set of queries.",
                "A system incorporating an implementation of RankNet is currently in use by a major search engine and can be considered representative of the state of the art in web search. • BM25F-RerankCT: The ranking produced by incorporating clickthrough statistics to reorder web search results ranked by BM25F above.",
                "Clickthrough is a particularly important special case of implicit feedback, and has been shown to correlate with result relevance.",
                "This is a special case of the ranking method in Section 3.1, with the weight wI set to 1000 and the ranking Id is simply the number of clicks on the result corresponding to d. In effect, this ranking brings to the top all returned web search results with at least one click (and orders them in decreasing order by number of clicks).",
                "The relative ranking of the remainder of results is unchanged and they are inserted below all clicked results.",
                "This method serves as our baseline implicit feedback reranking method.",
                "BM25F-RerankAll The ranking produced by reordering the BM25F results using all <br>user behavior</br> features (Section 4).",
                "This method learns a model of user preferences by correlating feature values with explicit relevance labels using the RankNet neural net algorithm (Section 4.2).",
                "At runtime, for a given query the implicit score Ir is computed for each result r with available user interaction features, and the implicit ranking is produced.",
                "The merged ranking is computed as described in Section 3.1.",
                "Based on the experiments over the development set we fix the value of wI to 3 (the effect of the wI parameter for this ranker turned out to be negligible). • BM25F+All: Ranking derived by training the RankNet (Section 3.3) learner over the features set of the BM25F score as well as all implicit feedback features (Section 3.2).",
                "We used the 2-layer implementation of RankNet [5] trained on the queries and labels in the training and validation sets. • RN+All: Ranking derived by training the 2-layer RankNet ranking algorithm (Section 3.3) over the union of all content, dynamic, and implicit feedback features (i.e., all of the features described above as well as all of the new implicit feedback features we introduced).",
                "The ranking methods above span the range of the information used for ranking, from not using the implicit or explicit feedback at all (i.e., BM25F) to a modern web search engine using hundreds of features and tuned on explicit judgments (RN).",
                "As we will show next, incorporating <br>user behavior</br> into these ranking systems dramatically improves the relevance of the returned documents. 6.",
                "EXPERIMENTAL RESULTS Implicit feedback for web search ranking can be exploited in a number of ways.",
                "We compare alternative methods of exploiting implicit feedback, both by re-ranking the top results (i.e., the BM25F-RerankCT and BM25F-RerankAll methods that reorder BM25F results), as well as by integrating the implicit features directly into the ranking process (i.e., the RN+ALL and BM25F+All methods which learn to rank results over the implicit feedback and other features).",
                "We compare our methods over strong baselines (BM25F and RN) over the NDCG, Precision at K, and MAP measures defined in Section 5.2.",
                "The results were averaged over three random splits of the overall dataset.",
                "Each split contained 1500 training, 500 validation, and 1000 test queries, all query sets disjoint.",
                "We first present the results over all 1000 test queries (i.e., including queries for which there are no implicit measures so we use the original web rankings).",
                "We then drill down to examine the effects on reranking for the attempted queries in more detail, analyzing where implicit feedback proved most beneficial.",
                "We first experimented with different methods of re-ranking the output of the BM25F search results.",
                "Figures 6.1 and 6.2 report NDCG and Precision for BM25F, as well as for the strategies reranking results with user feedback (Section 3.1).",
                "Incorporating all user feedback (either in reranking framework or as features to the learner directly) results in significant improvements (using two-tailed t-test with p=0.01) over both the original BM25F ranking as well as over reranking with clickthrough alone.",
                "The improvement is consistent across the top 10 results and largest for the top result: NDCG at 1 for BM25F+All is 0.622 compared to 0.518 of the original results, and precision at 1 similarly increases from 0.5 to 0.63.",
                "Based on these results we will use the direct feature combination (i.e., BM25F+All) ranker for subsequent comparisons involving implicit feedback. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 1 2 3 4 5 6 7 8 9 10K NDCG BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.1: NDCG at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K 0.35 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.2: Precision at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K Interestingly, using clickthrough alone, while giving significant benefit over the original BM25F ranking, is not as effective as considering the full set of features in Table 4.1.",
                "While we analyze <br>user behavior</br> (and most effective component features) in a separate paper [1], it is worthwhile to give a concrete example of the kind of noise inherent in real user feedback in web search setting. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 Result position Relativeclickfrequency PTR=2 PTR=3 PTR=5 Figure 6.3: Relative clickthrough frequency for queries with varying Position of Top Relevant result (PTR).",
                "If users considered only the relevance of a result to their query, they would click on the topmost relevant results.",
                "Unfortunately, as Joachims and others have shown, presentation also influences which results users click on quite dramatically.",
                "Users often click on results above the relevant one presumably because the short summaries do not provide enough information to make accurate relevance assessments and they have learned that on average topranked items are relevant.",
                "Figure 6.3 shows relative clickthrough frequencies for queries with known relevant items at positions other than the first position; the position of the top relevant result (PTR) ranges from 2-10 in the figure.",
                "For example, for queries with first relevant result at position 5 (PTR=5), there are more clicks on the non-relevant results in higher ranked positions than on the first relevant result at position 5.",
                "As we will see, learning over a richer behavior feature set, results in substantial accuracy improvement over clickthrough alone.",
                "We now consider incorporating <br>user behavior</br> into a much richer feature set, RN (Section 5.3) used by a major web search engine.",
                "RN incorporates BM25F, link-based features, and hundreds of other features.",
                "Figure 6.4 reports NDCG at K and Figure 6.5 reports Precision at K. Interestingly, while the original RN rankings are significantly more accurate than BM25F alone, incorporating implicit feedback features (BM25F+All) results in ranking that significantly outperforms the original RN rankings.",
                "In other words, implicit feedback incorporates sufficient information to replace the hundreds of other features available to the RankNet learner trained on the RN feature set. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 1 2 3 4 5 6 7 8 9 10K NDCG RN RN+All BM25 BM25+All Figure 6.4: NDCG at K for BM25F, BM25F+All, RN, and RN+All for varying K Furthermore, enriching the RN features with implicit feedback set exhibits significant gain on all measures, allowing RN+All to outperform all other methods.",
                "This demonstrates the complementary nature of implicit feedback with other features available to a state of the art web search engine. 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision RN RN+All BM25 BM25+All Figure 6.5: Precision at K for BM25F, BM25F+All, RN, and RN+All for varying K We summarize the performance of the different ranking methods in Table 6.1.",
                "We report the Mean Average Precision (MAP) score for each system.",
                "While not intuitive to interpret, MAP allows quantitative comparison on a single metric.",
                "The gains marked with * are significant at p=0.01 level using two tailed t-test.",
                "MAP Gain P(1) Gain BM25F 0.184 - 0.503BM25F-Rerank-CT 0.215 0.031* 0.577 0.073* BM25F-RerankImplicit 0.218 0.003 0.605 0.028* BM25F+Implicit 0.222 0.004 0.620 0.015* RN 0.215 - 0.597RN+All 0.248 0.033* 0.629 0.032* Table 6.1: Mean Average Precision (MAP) for all strategies.",
                "So far we reported results averaged across all queries in the test set.",
                "Unfortunately, less than half had sufficient interactions to attempt reranking.",
                "Out of the 1000 queries in test, between 46% and 49%, depending on the train-test split, had sufficient interaction information to make predictions (i.e., there was at least 1 search session in which at least 1 result URL was clicked on by the user).",
                "This is not surprising: web search is heavy-tailed, and there are many unique queries.",
                "We now consider the performance on the queries for which user interactions were available.",
                "Figure 6.6 reports NDCG for the subset of the test queries with the implicit feedback features.",
                "The gains at top 1 are dramatic.",
                "The NDCG at 1 of BM25F+All increases from 0.6 to 0.75 (a 31% relative gain), achieving performance comparable to RN+All operating over a much richer feature set. 0.6 0.65 0.7 0.75 0.8 1 3 5 10K NDCG RN RN+All BM25 BM25+All Figure 6.6: NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions Similarly, gains on precision at top 1 are substantial (Figure 6.7), and are likely to be apparent to web search users.",
                "When implicit feedback is available, the BM25F+All system returns relevant document at top 1 almost 70% of the time, compared 53% of the time when implicit feedback is not considered by the original BM25F system. 0.45 0.5 0.55 0.6 0.65 0.7 1 3 5 10K Precision RN RN+All BM25 BM25+All Figure 6.7: Precision at K NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions We summarize the results on the MAP measure for attempted queries in Table 6.2.",
                "MAP improvements are both substantial and significant, with improvements over the BM25F ranker most pronounced.",
                "Method MAP Gain P(1) Gain RN 0.269 0.632 RN+All 0.321 0.051 (19%) 0.693 0.061(10%) BM25F 0.236 0.525 BM25F+All 0.292 0.056 (24%) 0.687 0.162 (31%) Table 6.2: Mean Average Precision (MAP) on attempted queries for best performing methods We now analyze the cases where implicit feedback was shown most helpful.",
                "Figure 6.8 reports the MAP improvements over the baseline BM25F run for each query with MAP under 0.6.",
                "Note that most of the improvement is for poorly performing queries (i.e., MAP < 0.1).",
                "Interestingly, incorporating <br>user behavior</br> information degrades accuracy for queries with high original MAP score.",
                "One possible explanation is that these easy queries tend to be navigational (i.e., having a single, highly-ranked most appropriate answer), and user interactions with lower-ranked results may indicate divergent information needs that are better served by the less popular results (with correspondingly poor overall relevance ratings). 0 50 100 150 200 250 300 350 0.1 0.2 0.3 0.4 0.5 0.6 -0.4 -0.35 -0.3 -0.25 -0.2 -0.15 -0.1 -0.05 0 0.05 0.1 0.15 0.2 Frequency Average Gain Figure 6.8: Gain of BM25F+All over original BM25F ranking To summarize our experimental results, incorporating implicit feedback in real web search setting resulted in significant improvements over the original rankings, using both BM25F and RN baselines.",
                "Our rich set of implicit features, such as time on page and deviations from the average behavior, provides advantages over using clickthrough alone as an indicator of interest.",
                "Furthermore, incorporating implicit feedback features directly into the learned ranking function is more effective than using implicit feedback for reranking.",
                "The improvements observed over large test sets of queries (1,000 total, between 466 and 495 with implicit feedback available) are both substantial and statistically significant. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper we explored the utility of incorporating noisy implicit feedback obtained in a real web search setting to improve web search ranking.",
                "We performed a large-scale evaluation over 3,000 queries and more than 12 million user interactions with a major search engine, establishing the utility of incorporating noisy implicit feedback to improve web search relevance.",
                "We compared two alternatives of incorporating implicit feedback into the search process, namely reranking with implicit feedback and incorporating implicit feedback features directly into the trained ranking function.",
                "Our experiments showed significant improvement over methods that do not consider implicit feedback.",
                "The gains are particularly dramatic for the top K=1 result in the final ranking, with precision improvements as high as 31%, and the gains are substantial for all values of K. Our experiments showed that implicit user feedback can further improve web search performance, when incorporated directly with popular content- and link-based features.",
                "Interestingly, implicit feedback is particularly valuable for queries with poor original ranking of results (e.g., MAP lower than 0.1).",
                "One promising direction for future work is to apply recent research on automatically predicting query difficulty, and only attempt to incorporate implicit feedback for the difficult queries.",
                "As another research direction we are exploring methods for extending our predictions to the previously unseen queries (e.g., query clustering), which should further improve the web search experience of users.",
                "ACKNOWLEDGMENTS We thank Chris Burges and Matt Richardson for an implementation of RankNet for our experiments.",
                "We also thank Robert Ragno for his valuable suggestions and many discussions. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, S. Dumais, and R.Ragno, Learning User Interaction Models for Predicting Web Search Result Preferences.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan, HARD Track Overview in TREC 2003, High Accuracy Retrieval from Documents, 2003 [3] R. Baeza-Yates and B. Ribeiro-Neto, Modern Information Retrieval, Addison-Wesley, 1999. [4] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine, in Proceedings of WWW, 1997 [5] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning, 2005 [6] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [7] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest.",
                "IEEE Internet Computing. 2001 [8] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [9] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [10] K Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2000 [11] T. Joachims, Optimizing Search Engines Using Clickthrough Data.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [12] T. Joachims, L. Granka, B. Pang, H. Hembrooke, and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [13] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [14] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [15] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [16] M. Morita, and Y. Shinoda, Information filtering based on <br>user behavior</br> analysis and best match text retrieval.",
                "Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [17] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems.",
                "In Proceedings of the AAAI Workshop on Recommender Systems. 1998 [18] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [19] N. Pharo, N. and K. Järvelin.",
                "The SST method: a tool for analyzing web information search processes.",
                "In Information Processing & Management, 2004 [20] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [21] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (SIGKDD), 2005. [22] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in Proceedings of the ICML Workshop on Learning in Web Search, 2005 [23] S. E. Robertson, H. Zaragoza, and M. Taylor, Simple BM25 extension to multiple weighted fields, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [24] G. Salton & M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [25] E.M. Voorhees, D. Harman, Overview of TREC, 2001 [26] G.R.",
                "Xue, H.J.",
                "Zeng, Z. Chen, Y. Yu, W.Y.",
                "Ma, W.S.",
                "Xi, and W.G.",
                "Fan, Optimizing web search using web clickthrough data, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [27] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
                "Microsoft Cambridge at TREC 13: Web and Hard Tracks.",
                "In Proceedings of TREC 2004"
            ],
            "original_annotated_samples": [
                "Improving Web Search Ranking by Incorporating <br>user behavior</br> Information Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com ABSTRACT We show that incorporating <br>user behavior</br> data can significantly improve ordering of top results in real web search setting.",
                "To this end, we explore different approaches for ranking web search results using real <br>user behavior</br> obtained as part of normal interactions with the web search engine.",
                "The specific contributions of this paper include: • Analysis of alternatives for incorporating <br>user behavior</br> into web search ranking (Section 3). • An application of a robust implicit feedback model derived from mining millions of user interactions with a major web search engine (Section 4). • A large scale evaluation over real user queries and search results, showing significant improvements derived from incorporating user feedback (Section 6).",
                "Other studies of <br>user behavior</br> in web search include Pharo and Järvelin [19], but were not directly applied to improve ranking.",
                "We build on existing research to develop robust <br>user behavior</br> interpretation techniques for the real web search setting."
            ],
            "translated_annotated_samples": [
                "Mejorando la clasificación de búsqueda web al incorporar información sobre el <br>comportamiento del usuario</br>. Eugene Agichtein, Microsoft Research eugeneag@microsoft.com Eric Brill, Microsoft Research brill@microsoft.com Susan Dumais, Microsoft Research sdumais@microsoft.com RESUMEN Mostramos que la incorporación de datos sobre el <br>comportamiento del usuario</br> puede mejorar significativamente el orden de los principales resultados en una configuración real de búsqueda web.",
                "Con este fin, exploramos diferentes enfoques para clasificar los resultados de búsqueda web utilizando el <br>comportamiento real de los usuarios</br> obtenido como parte de las interacciones normales con el motor de búsqueda web.",
                "Las contribuciones específicas de este artículo incluyen: • Análisis de alternativas para incorporar el <br>comportamiento del usuario</br> en la clasificación de búsqueda web (Sección 3). • Aplicación de un modelo robusto de retroalimentación implícita derivado de la minería de millones de interacciones de usuarios con un importante motor de búsqueda web (Sección 4). • Una evaluación a gran escala sobre consultas reales de usuarios y resultados de búsqueda, mostrando mejoras significativas derivadas de la incorporación de la retroalimentación del usuario (Sección 6).",
                "Otros estudios sobre el <br>comportamiento de los usuarios</br> en la búsqueda web incluyen a Pharo y Järvelin [19], pero no se aplicaron directamente para mejorar la clasificación.",
                "Nos basamos en investigaciones existentes para desarrollar técnicas robustas de interpretación del <br>comportamiento del usuario</br> para el entorno real de búsqueda en la web."
            ],
            "translated_text": "Mejorando la clasificación de búsqueda web al incorporar información sobre el <br>comportamiento del usuario</br>. Eugene Agichtein, Microsoft Research eugeneag@microsoft.com Eric Brill, Microsoft Research brill@microsoft.com Susan Dumais, Microsoft Research sdumais@microsoft.com RESUMEN Mostramos que la incorporación de datos sobre el <br>comportamiento del usuario</br> puede mejorar significativamente el orden de los principales resultados en una configuración real de búsqueda web. Examinamos alternativas para incorporar retroalimentación en el proceso de clasificación y exploramos las contribuciones de la retroalimentación de usuarios en comparación con otras características comunes de búsqueda en la web. Informamos los resultados de una evaluación a gran escala con más de 3,000 consultas y 12 millones de interacciones de usuarios con un motor de búsqueda web popular. Mostramos que la incorporación de retroalimentación implícita puede aumentar otras características, mejorando la precisión de los algoritmos de clasificación de búsqueda web competitivos hasta en un 31% en comparación con el rendimiento original. Categorías y Descriptores de Asignaturas H.3.3 Búsqueda y Recuperación de Información - Retroalimentación de relevancia, proceso de búsqueda; H.3.5 Servicios de Información en Línea - Servicios basados en la web. Términos generales Algoritmos, Medición, Experimentación 1. INTRODUCCIÓN Millones de usuarios interactúan con los motores de búsqueda a diario. Ellos emiten consultas, siguen algunos de los enlaces en los resultados, hacen clic en anuncios, pasan tiempo en páginas, reformulan sus consultas y realizan otras acciones. Estas interacciones pueden servir como una valiosa fuente de información para ajustar y mejorar la clasificación de los resultados de búsqueda en la web y pueden complementar juicios explícitos más costosos. La retroalimentación implícita de relevancia para la clasificación y personalización se ha convertido en un área activa de investigación. El trabajo reciente de Joachims y otros explorando el feedback implícito en entornos controlados ha demostrado el valor de incorporar el feedback implícito en el proceso de clasificación. Nuestra motivación para este trabajo es comprender cómo se puede utilizar la retroalimentación implícita en un entorno operativo a gran escala para mejorar la recuperación. ¿Cómo se compara y complementa con la evidencia del contenido de la página, el texto del ancla o las características basadas en enlaces como inlinks o PageRank? Si bien es intuitivo que las interacciones de los usuarios con el motor de búsqueda web deberían revelar al menos alguna información que podría ser utilizada para la clasificación, estimar las preferencias de los usuarios en entornos reales de búsqueda web es un problema desafiante, ya que las interacciones reales tienden a ser más ruidosas de lo que comúnmente se asume en los entornos controlados de estudios previos. Nuestro artículo explora si la retroalimentación implícita puede ser útil en entornos realistas, donde la retroalimentación del usuario puede ser ruidosa (o adversa) y un motor de búsqueda web ya utiliza cientos de características y está altamente ajustado. Con este fin, exploramos diferentes enfoques para clasificar los resultados de búsqueda web utilizando el <br>comportamiento real de los usuarios</br> obtenido como parte de las interacciones normales con el motor de búsqueda web. Las contribuciones específicas de este artículo incluyen: • Análisis de alternativas para incorporar el <br>comportamiento del usuario</br> en la clasificación de búsqueda web (Sección 3). • Aplicación de un modelo robusto de retroalimentación implícita derivado de la minería de millones de interacciones de usuarios con un importante motor de búsqueda web (Sección 4). • Una evaluación a gran escala sobre consultas reales de usuarios y resultados de búsqueda, mostrando mejoras significativas derivadas de la incorporación de la retroalimentación del usuario (Sección 6). Resumimos nuestros hallazgos y discutimos extensiones al trabajo actual en la Sección 7, que concluye el artículo. ANTECEDENTES Y TRABAJO RELACIONADO Clasificar los resultados de búsqueda es un problema fundamental en la recuperación de información. La mayoría de los enfoques comunes se centran principalmente en la similitud entre la consulta y una página, así como en la calidad general de la página [3,4,24]. Sin embargo, con la creciente popularidad de los motores de búsqueda, la retroalimentación implícita (es decir, las acciones que los usuarios realizan al interactuar con el motor de búsqueda) se puede utilizar para mejorar las clasificaciones. Las medidas de relevancia implícitas han sido estudiadas por varios grupos de investigación. Un resumen de las medidas implícitas se recopila en Kelly y Teevan [14]. Esta investigación, si bien desarrolló valiosas ideas sobre medidas implícitas de relevancia, no se aplicó para mejorar la clasificación de los resultados de búsqueda en la web en entornos realistas. Estrechamente relacionado con nuestro trabajo, Joachims [11] recopiló medidas implícitas en lugar de medidas explícitas, introduciendo una técnica basada completamente en datos de clics para aprender funciones de clasificación. Fox et al. [8] exploraron la relación entre medidas implícitas y explícitas en la búsqueda web, y desarrollaron modelos bayesianos para correlacionar medidas implícitas y juicios explícitos de relevancia tanto para consultas individuales como para sesiones de búsqueda. Este trabajo consideró una amplia gama de comportamientos de usuario (por ejemplo, tiempo de permanencia, tiempo de desplazamiento, patrones de reformulación) además del popular comportamiento de clics. Sin embargo, el esfuerzo de modelado se centró en predecir juicios de relevancia explícitos a partir de acciones implícitas de los usuarios y no específicamente en aprender funciones de clasificación. Otros estudios sobre el <br>comportamiento de los usuarios</br> en la búsqueda web incluyen a Pharo y Järvelin [19], pero no se aplicaron directamente para mejorar la clasificación. Más recientemente, Joachims et al. [12] presentaron una evaluación empírica de la interpretación de la evidencia de clics. Al realizar estudios de seguimiento ocular y correlacionar las predicciones de sus estrategias con las calificaciones explícitas, los autores demostraron que es posible interpretar con precisión los clics en un entorno controlado de laboratorio. Desafortunadamente, no está claro en qué medida la investigación previa se aplica a la búsqueda web del mundo real. Al mismo tiempo, si bien el trabajo reciente (por ejemplo, [26]) sobre el uso de la información de clics para mejorar la clasificación de búsqueda en la web es prometedor, solo abarca un aspecto de las interacciones de los usuarios con los motores de búsqueda en la web. Nos basamos en investigaciones existentes para desarrollar técnicas robustas de interpretación del <br>comportamiento del usuario</br> para el entorno real de búsqueda en la web. ",
            "candidates": [],
            "error": [
                [
                    "comportamiento del usuario",
                    "comportamiento del usuario",
                    "comportamiento real de los usuarios",
                    "comportamiento del usuario",
                    "comportamiento de los usuarios",
                    "comportamiento del usuario"
                ]
            ]
        },
        "feedback": {
            "translated_key": "retroalimentación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Improving Web Search Ranking by Incorporating User Behavior Information Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com ABSTRACT We show that incorporating user behavior data can significantly improve ordering of top results in real web search setting.",
                "We examine alternatives for incorporating <br>feedback</br> into the ranking process and explore the contributions of user <br>feedback</br> compared to other common web search features.",
                "We report results of a large scale evaluation over 3,000 queries and 12 million user interactions with a popular web search engine.",
                "We show that incorporating implicit <br>feedback</br> can augment other features, improving the accuracy of a competitive web search ranking algorithms by as much as 31% relative to the original performance.",
                "Categories and Subject Descriptors H.3.3 Information Search and Retrieval - Relevance <br>feedback</br>, search process; H.3.5 Online Information Services - Web-based services.",
                "General Terms Algorithms, Measurement, Experimentation 1.",
                "INTRODUCTION Millions of users interact with search engines daily.",
                "They issue queries, follow some of the links in the results, click on ads, spend time on pages, reformulate their queries, and perform other actions.",
                "These interactions can serve as a valuable source of information for tuning and improving web search result ranking and can compliment more costly explicit judgments.",
                "Implicit relevance <br>feedback</br> for ranking and personalization has become an active area of research.",
                "Recent work by Joachims and others exploring implicit <br>feedback</br> in controlled environments have shown the value of incorporating implicit <br>feedback</br> into the ranking process.",
                "Our motivation for this work is to understand how implicit <br>feedback</br> can be used in a large-scale operational environment to improve retrieval.",
                "How does it compare to and compliment evidence from page content, anchor text, or link-based features such as inlinks or PageRank?",
                "While it is intuitive that user interactions with the web search engine should reveal at least some information that could be used for ranking, estimating user preferences in real web search settings is a challenging problem, since real user interactions tend to be more noisy than commonly assumed in the controlled settings of previous studies.",
                "Our paper explores whether implicit <br>feedback</br> can be helpful in realistic environments, where user <br>feedback</br> can be noisy (or adversarial) and a web search engine already uses hundreds of features and is heavily tuned.",
                "To this end, we explore different approaches for ranking web search results using real user behavior obtained as part of normal interactions with the web search engine.",
                "The specific contributions of this paper include: • Analysis of alternatives for incorporating user behavior into web search ranking (Section 3). • An application of a robust implicit <br>feedback</br> model derived from mining millions of user interactions with a major web search engine (Section 4). • A large scale evaluation over real user queries and search results, showing significant improvements derived from incorporating user <br>feedback</br> (Section 6).",
                "We summarize our findings and discuss extensions to the current work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "Most common approaches primarily focus on similarity of query and a page, as well as the overall page quality [3,4,24].",
                "However, with increasing popularity of search engines, implicit <br>feedback</br> (i.e., the actions users take when interacting with the search engine) can be used to improve the rankings.",
                "Implicit relevance measures have been studied by several research groups.",
                "An overview of implicit measures is compiled in Kelly and Teevan [14].",
                "This research, while developing valuable insights into implicit relevance measures, was not applied to improve the ranking of web search results in realistic settings.",
                "Closely related to our work, Joachims [11] collected implicit measures in place of explicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "Fox et al. [8] explored the relationship between implicit and explicit measures in Web search, and developed Bayesian models to correlate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "This work considered a wide range of user behaviors (e.g., dwell time, scroll time, reformulation patterns) in addition to the popular clickthrough behavior.",
                "However, the modeling effort was aimed at predicting explicit relevance judgments from implicit user actions and not specifically at learning ranking functions.",
                "Other studies of user behavior in web search include Pharo and Järvelin [19], but were not directly applied to improve ranking.",
                "More recently, Joachims et al. [12] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthroughs in a controlled, laboratory setting.",
                "Unfortunately, the extent to which previous research applies to real-world web search is unclear.",
                "At the same time, while recent work (e.g., [26]) on using clickthrough information for improving web search ranking is promising, it captures only one aspect of the user interactions with web search engines.",
                "We build on existing research to develop robust user behavior interpretation techniques for the real web search setting.",
                "Instead of treating each user as a reliable expert, we aggregate information from multiple, unreliable, user search session traces, as we describe in the next two sections. 3.",
                "INCORPORATING IMPLICIT <br>feedback</br> We consider two complementary approaches to ranking with implicit <br>feedback</br>: (1) treating implicit feedback as independent evidence for ranking results, and (2) integrating implicit feedback features directly into the ranking algorithm.",
                "We describe the two general ranking approaches next.",
                "The specific implicit <br>feedback</br> features are described in Section 4, and the algorithms for interpreting and incorporating implicit <br>feedback</br> are described in Section 5. 3.1 Implicit Feedback as Independent Evidence The general approach is to re-rank the results obtained by a web search engine according to observed clickthrough and other user interactions for the query in previous search sessions.",
                "Each result is assigned a score according to expected relevance/user satisfaction based on previous interactions, resulting in some preference ordering based on user interactions alone.",
                "While there has been significant work on merging multiple rankings, we adapt a simple and robust approach of ignoring the original rankers scores, and instead simply merge the rank orders.",
                "The main reason for ignoring the original scores is that since the feature spaces and learning algorithms are different, the scores are not directly comparable, and re-normalization tends to remove the benefit of incorporating classifier scores.",
                "We experimented with a variety of merging functions on the development set of queries (and using a set of interactions from a different time period from final evaluation sets).",
                "We found that a simple rank merging heuristic combination works well, and is robust to variations in score values from original rankers.",
                "For a given query q, the implicit score ISd is computed for each result d from available user interaction features, resulting in the implicit rank Id for each result.",
                "We compute a merged score SM(d) for d by combining the ranks obtained from implicit <br>feedback</br>, Id with the original rank of d, Od:     ¡     ¢ £ + + + + = otherwise O dforexistsfeedbackimplicitif OI w wOIdS d dd I IddM 1 1 1 1 1 1 ),,,( where the weight wI is a heuristically tuned scaling factor representing the relative importance of the implicit <br>feedback</br>.",
                "The query results are ordered in by decreasing values of SM to produce the final ranking.",
                "One special case of this model arises when setting wI to a very large value, effectively forcing clicked results to be ranked higher than un-clicked results - an intuitive and effective heuristic that we will use as a baseline.",
                "Applying more sophisticated classifier and ranker combination algorithms may result in additional improvements, and is a promising direction for future work.",
                "The approach above assumes that there are no interactions between the underlying features producing the original web search ranking and the implicit <br>feedback</br> features.",
                "We now relax this assumption by integrating implicit <br>feedback</br> features directly into the ranking process. 3.2 Ranking with Implicit <br>feedback</br> Features Modern web search engines rank results based on a large number of features, including content-based features (i.e., how closely a query matches the text or title or anchor text of the document), and query-independent page quality features (e.g., PageRank of the document or the domain).",
                "In most cases, automatic (or semiautomatic) methods are developed for tuning the specific ranking function that combines these feature values.",
                "Hence, a natural approach is to incorporate implicit <br>feedback</br> features directly as features for the ranking algorithm.",
                "During training or tuning, the ranker can be tuned as before but with additional features.",
                "At runtime, the search engine would fetch the implicit <br>feedback</br> features associated with each query-result URL pair.",
                "This model requires a ranking algorithm to be robust to missing values: more than 50% of queries to web search engines are unique, with no previous implicit <br>feedback</br> available.",
                "We now describe such a ranker that we used to learn over the combined feature sets including implicit <br>feedback</br>. 3.3 Learning to Rank Web Search Results A key aspect of our approach is exploiting recent advances in machine learning, namely trainable ranking algorithms for web search and information retrieval (e.g., [5, 11] and classical results reviewed in [3]).",
                "In our setting, explicit human relevance judgments (labels) are available for a set of web search queries and results.",
                "Hence, an attractive choice to use is a supervised machine learning technique to learn a ranking function that best predicts relevance judgments.",
                "RankNet is one such algorithm.",
                "It is a neural net tuning algorithm that optimizes feature weights to best match explicitly provided pairwise user preferences.",
                "While the specific training algorithms used by RankNet are beyond the scope of this paper, it is described in detail in [5] and includes extensive evaluation and comparison with other ranking methods.",
                "An attractive feature of RankNet is both train- and run-time efficiency - runtime ranking can be quickly computed and can scale to the web, and training can be done over thousands of queries and associated judged results.",
                "We use a 2-layer implementation of RankNet in order to model non-linear relationships between features.",
                "Furthermore, RankNet can learn with many (differentiable) cost functions, and hence can automatically learn a ranking function from human-provided labels, an attractive alternative to heuristic feature combination techniques.",
                "Hence, we will also use RankNet as a generic ranker to explore the contribution of implicit <br>feedback</br> for different ranking alternatives. 4.",
                "IMPLICIT USER <br>feedback</br> MODEL Our goal is to accurately interpret noisy user <br>feedback</br> obtained as by tracing user interactions with the search engine.",
                "Interpreting implicit <br>feedback</br> in real web search setting is not an easy task.",
                "We characterize this problem in detail in [1], where we motivate and evaluate a wide variety of models of implicit user activities.",
                "The general approach is to represent user actions for each search result as a vector of features, and then train a ranker on these features to discover feature values indicative of relevant (and nonrelevant) search results.",
                "We first briefly summarize our features and model, and the learning approach (Section 4.2) in order to provide sufficient information to replicate our ranking methods and the subsequent experiments. 4.1 Representing User Actions as Features We model observed web search behaviors as a combination of a ``background component (i.e., query- and relevance-independent noise in user behavior, including positional biases with result interactions), and a ``relevance component (i.e., query-specific behavior indicative of relevance of a result to a query).",
                "We design our features to take advantage of aggregated user behavior.",
                "The feature set is comprised of directly observed features (computed directly from observations for each query), as well as queryspecific derived features, computed as the deviation from the overall query-independent distribution of values for the corresponding directly observed feature values.",
                "The features used to represent user interactions with web search results are summarized in Table 4.1.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "We include the traditional implicit <br>feedback</br> features such as clickthrough counts for the results, as well as our novel derived features such as the deviation of the observed clickthrough number for a given query-URL pair from the expected number of clicks on a result in the given position.",
                "We also model the browsing behavior after a result was clicked - e.g., the average page dwell time for a given query-URL pair, as well as its deviation from the expected (average) dwell time.",
                "Furthermore, the feature set was designed to provide essential information about the user experience to make <br>feedback</br> interpretation robust.",
                "For example, web search users can often determine whether a result is relevant by looking at the result title, URL, and summary - in many cases, looking at the original document is not necessary.",
                "To model this aspect of user experience we include features such as overlap in words in title and words in query (TitleOverlap) and the fraction of words shared by the query and the result summary.",
                "Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickProbability Probability of a click for this query and URL ClickDeviation Deviation from expected click probability IsNextClicked 1 if clicked on next position, 0 otherwise IsPreviousClicked 1 if clicked on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, no parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from average dwell time on page CumulativeDeviation Deviation from average cumulative dwell time DomainDeviation Deviation from average dwell time on domain Query-text features TitleOverlap Words shared between query and title SummaryOverlap Words shared between query and snippet QueryURLOverlap Words shared between query and URL QueryDomainOverlap Words shared between query and URL domain QueryLength Number of tokens in query QueryNextOverlap Fraction of words shared with next query Table 4.1: Some features used to represent post-search navigation history for a given query and search result URL.",
                "Having described our feature set, we briefly review our general method for deriving a user behavior model. 4.2 Deriving a User <br>feedback</br> Model To learn to interpret the observed user behavior, we correlate user actions (i.e., the features in Table 4.1 representing the actions) with the explicit user judgments for a set of training queries.",
                "We find all the instances in our session logs where these queries were submitted to the search engine, and aggregate the user behavior features for all search sessions involving these queries.",
                "Each observed query-URL pair is represented by the features in Table 4.1, with values averaged over all search sessions, and assigned one of six possible relevance labels, ranging from Perfect to Bad, as assigned by explicit relevance judgments.",
                "These labeled feature vectors are used as input to the RankNet training algorithm (Section 3.3) which produces a trained user behavior model.",
                "This approach is particularly attractive as it does not require heuristics beyond feature engineering.",
                "The resulting user behavior model is used to help rank web search resultseither directly or in combination with other features, as described below. 5.",
                "EXPERIMENTAL SETUP The ultimate goal of incorporating implicit <br>feedback</br> into ranking is to improve the relevance of the returned web search results.",
                "Hence, we compare the ranking methods over a large set of judged queries with explicit relevance labels provided by human judges.",
                "In order for the evaluation to be realistic we obtained a random sample of queries from web search logs of a major search engine, with associated results and traces for user actions.",
                "We describe this dataset in detail next.",
                "Our metrics are described in Section 5.2 that we use to evaluate the ranking alternatives, listed in Section 5.3 in the experiments of Section 6. 5.1 Datasets We compared our ranking methods over a random sample of 3,000 queries from the search engine query logs.",
                "The queries were drawn from the logs uniformly at random by token without replacement, resulting in a query sample representative of the overall query distribution.",
                "On average, 30 results were explicitly labeled by human judges using a six point scale ranging from Perfect down to Bad.",
                "Overall, there were over 83,000 results with explicit relevance judgments.",
                "In order to compute various statistics, documents with label Good or better will be considered relevant, and with lower labels to be non-relevant.",
                "Note that the experiments were performed over the results already highly ranked by a web search engine, which corresponds to a typical user experience which is limited to the small number of the highly ranked results for a typical web search query.",
                "The user interactions were collected over a period of 8 weeks using voluntary opt-in information.",
                "In total, over 1.2 million unique queries were instrumented, resulting in over 12 million individual interactions with the search engine.",
                "The data consisted of user interactions with the web search engine (e.g., clicking on a result link, going back to search results, etc.) performed after a query was submitted.",
                "These actions were aggregated across users and search sessions and converted to features in Table 4.1.",
                "To create the training, validation, and test query sets, we created three different random splits of 1,500 training, 500 validation, and 1000 test queries.",
                "The splits were done randomly by query, so that there was no overlap in training, validation, and test queries. 5.2 Evaluation Metrics We evaluate the ranking algorithms over a range of accepted information retrieval metrics, namely Precision at K (P(K)), Normalized Discounted Cumulative Gain (NDCG), and Mean Average Precision (MAP).",
                "Each metric focuses on a deferent aspect of system performance, as we describe below. • Precision at K: As the most intuitive metric, P(K) reports the fraction of documents ranked in the top K results that are labeled as relevant.",
                "In our setting, we require a relevant document to be labeled Good or higher.",
                "The position of relevant documents within the top K is irrelevant, and hence this metric measure overall user satisfaction with the top K results. • NDCG at K: NDCG is a retrieval measure devised specifically for web search evaluation [10].",
                "For a given query q, the ranked results are examined from the top ranked down, and the NDCG computed as:   = +−= K j jr qq jMN 1 )( )1log(/)12( Where Mq is a normalization constant calculated so that a perfect ordering would obtain NDCG of 1; and each r(j) is an integer relevance label (0=Bad and 5=Perfect) of result returned at position j.",
                "Note that unlabeled and Bad documents do not contribute to the sum, but will reduce NDCG for the query pushing down the relevant labeled documents, reducing their contributions.",
                "NDCG is well suited to web search evaluation, as it rewards relevant documents in the top ranked results more heavily than those ranked lower. • MAP: Average precision for each query is defined as the mean of the precision at K values computed after each relevant document was retrieved.",
                "The final MAP value is defined as the mean of average precisions of all queries in the test set.",
                "This metric is the most commonly used single-value summary of a run over a set of queries. 5.3 Ranking Methods Compared Recall that our goal is to quantify the effectiveness of implicit behavior for real web search.",
                "One dimension is to compare the utility of implicit <br>feedback</br> with other information available to a web search engine.",
                "Specifically, we compare effectiveness of implicit user behaviors with content-based matching, static page quality features, and combinations of all features. • BM25F: As a strong web search baseline we used the BM25F scoring, which was used in one of the best performing systems in the TREC 2004 Web track [23,27].",
                "BM25F and its variants have been extensively described and evaluated in IR literature, and hence serve as a strong, reproducible baseline.",
                "The BM25F variant we used for our experiments computes separate match scores for each field for a result document (e.g., body text, title, and anchor text), and incorporates query-independent linkbased information (e.g., PageRank, ClickDistance, and URL depth).",
                "The scoring function and field-specific tuning is described in detail in [23].",
                "Note that BM25F does not directly consider explicit or implicit <br>feedback</br> for tuning. • RN: The ranking produced by a neural net ranker (RankNet, described in Section 3.3) that learns to rank web search results by incorporating BM25F and a large number of additional static and dynamic features describing each search result.",
                "This system automatically learns weights for all features (including the BM25F score for a document) based on explicit human labels for a large set of queries.",
                "A system incorporating an implementation of RankNet is currently in use by a major search engine and can be considered representative of the state of the art in web search. • BM25F-RerankCT: The ranking produced by incorporating clickthrough statistics to reorder web search results ranked by BM25F above.",
                "Clickthrough is a particularly important special case of implicit <br>feedback</br>, and has been shown to correlate with result relevance.",
                "This is a special case of the ranking method in Section 3.1, with the weight wI set to 1000 and the ranking Id is simply the number of clicks on the result corresponding to d. In effect, this ranking brings to the top all returned web search results with at least one click (and orders them in decreasing order by number of clicks).",
                "The relative ranking of the remainder of results is unchanged and they are inserted below all clicked results.",
                "This method serves as our baseline implicit <br>feedback</br> reranking method.",
                "BM25F-RerankAll The ranking produced by reordering the BM25F results using all user behavior features (Section 4).",
                "This method learns a model of user preferences by correlating feature values with explicit relevance labels using the RankNet neural net algorithm (Section 4.2).",
                "At runtime, for a given query the implicit score Ir is computed for each result r with available user interaction features, and the implicit ranking is produced.",
                "The merged ranking is computed as described in Section 3.1.",
                "Based on the experiments over the development set we fix the value of wI to 3 (the effect of the wI parameter for this ranker turned out to be negligible). • BM25F+All: Ranking derived by training the RankNet (Section 3.3) learner over the features set of the BM25F score as well as all implicit <br>feedback</br> features (Section 3.2).",
                "We used the 2-layer implementation of RankNet [5] trained on the queries and labels in the training and validation sets. • RN+All: Ranking derived by training the 2-layer RankNet ranking algorithm (Section 3.3) over the union of all content, dynamic, and implicit <br>feedback</br> features (i.e., all of the features described above as well as all of the new implicit <br>feedback</br> features we introduced).",
                "The ranking methods above span the range of the information used for ranking, from not using the implicit or explicit <br>feedback</br> at all (i.e., BM25F) to a modern web search engine using hundreds of features and tuned on explicit judgments (RN).",
                "As we will show next, incorporating user behavior into these ranking systems dramatically improves the relevance of the returned documents. 6.",
                "EXPERIMENTAL RESULTS Implicit <br>feedback</br> for web search ranking can be exploited in a number of ways.",
                "We compare alternative methods of exploiting implicit <br>feedback</br>, both by re-ranking the top results (i.e., the BM25F-RerankCT and BM25F-RerankAll methods that reorder BM25F results), as well as by integrating the implicit features directly into the ranking process (i.e., the RN+ALL and BM25F+All methods which learn to rank results over the implicit <br>feedback</br> and other features).",
                "We compare our methods over strong baselines (BM25F and RN) over the NDCG, Precision at K, and MAP measures defined in Section 5.2.",
                "The results were averaged over three random splits of the overall dataset.",
                "Each split contained 1500 training, 500 validation, and 1000 test queries, all query sets disjoint.",
                "We first present the results over all 1000 test queries (i.e., including queries for which there are no implicit measures so we use the original web rankings).",
                "We then drill down to examine the effects on reranking for the attempted queries in more detail, analyzing where implicit <br>feedback</br> proved most beneficial.",
                "We first experimented with different methods of re-ranking the output of the BM25F search results.",
                "Figures 6.1 and 6.2 report NDCG and Precision for BM25F, as well as for the strategies reranking results with user <br>feedback</br> (Section 3.1).",
                "Incorporating all user <br>feedback</br> (either in reranking framework or as features to the learner directly) results in significant improvements (using two-tailed t-test with p=0.01) over both the original BM25F ranking as well as over reranking with clickthrough alone.",
                "The improvement is consistent across the top 10 results and largest for the top result: NDCG at 1 for BM25F+All is 0.622 compared to 0.518 of the original results, and precision at 1 similarly increases from 0.5 to 0.63.",
                "Based on these results we will use the direct feature combination (i.e., BM25F+All) ranker for subsequent comparisons involving implicit <br>feedback</br>. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 1 2 3 4 5 6 7 8 9 10K NDCG BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.1: NDCG at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K 0.35 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.2: Precision at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K Interestingly, using clickthrough alone, while giving significant benefit over the original BM25F ranking, is not as effective as considering the full set of features in Table 4.1.",
                "While we analyze user behavior (and most effective component features) in a separate paper [1], it is worthwhile to give a concrete example of the kind of noise inherent in real user <br>feedback</br> in web search setting. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 Result position Relativeclickfrequency PTR=2 PTR=3 PTR=5 Figure 6.3: Relative clickthrough frequency for queries with varying Position of Top Relevant result (PTR).",
                "If users considered only the relevance of a result to their query, they would click on the topmost relevant results.",
                "Unfortunately, as Joachims and others have shown, presentation also influences which results users click on quite dramatically.",
                "Users often click on results above the relevant one presumably because the short summaries do not provide enough information to make accurate relevance assessments and they have learned that on average topranked items are relevant.",
                "Figure 6.3 shows relative clickthrough frequencies for queries with known relevant items at positions other than the first position; the position of the top relevant result (PTR) ranges from 2-10 in the figure.",
                "For example, for queries with first relevant result at position 5 (PTR=5), there are more clicks on the non-relevant results in higher ranked positions than on the first relevant result at position 5.",
                "As we will see, learning over a richer behavior feature set, results in substantial accuracy improvement over clickthrough alone.",
                "We now consider incorporating user behavior into a much richer feature set, RN (Section 5.3) used by a major web search engine.",
                "RN incorporates BM25F, link-based features, and hundreds of other features.",
                "Figure 6.4 reports NDCG at K and Figure 6.5 reports Precision at K. Interestingly, while the original RN rankings are significantly more accurate than BM25F alone, incorporating implicit <br>feedback</br> features (BM25F+All) results in ranking that significantly outperforms the original RN rankings.",
                "In other words, implicit <br>feedback</br> incorporates sufficient information to replace the hundreds of other features available to the RankNet learner trained on the RN feature set. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 1 2 3 4 5 6 7 8 9 10K NDCG RN RN+All BM25 BM25+All Figure 6.4: NDCG at K for BM25F, BM25F+All, RN, and RN+All for varying K Furthermore, enriching the RN features with implicit <br>feedback</br> set exhibits significant gain on all measures, allowing RN+All to outperform all other methods.",
                "This demonstrates the complementary nature of implicit <br>feedback</br> with other features available to a state of the art web search engine. 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision RN RN+All BM25 BM25+All Figure 6.5: Precision at K for BM25F, BM25F+All, RN, and RN+All for varying K We summarize the performance of the different ranking methods in Table 6.1.",
                "We report the Mean Average Precision (MAP) score for each system.",
                "While not intuitive to interpret, MAP allows quantitative comparison on a single metric.",
                "The gains marked with * are significant at p=0.01 level using two tailed t-test.",
                "MAP Gain P(1) Gain BM25F 0.184 - 0.503BM25F-Rerank-CT 0.215 0.031* 0.577 0.073* BM25F-RerankImplicit 0.218 0.003 0.605 0.028* BM25F+Implicit 0.222 0.004 0.620 0.015* RN 0.215 - 0.597RN+All 0.248 0.033* 0.629 0.032* Table 6.1: Mean Average Precision (MAP) for all strategies.",
                "So far we reported results averaged across all queries in the test set.",
                "Unfortunately, less than half had sufficient interactions to attempt reranking.",
                "Out of the 1000 queries in test, between 46% and 49%, depending on the train-test split, had sufficient interaction information to make predictions (i.e., there was at least 1 search session in which at least 1 result URL was clicked on by the user).",
                "This is not surprising: web search is heavy-tailed, and there are many unique queries.",
                "We now consider the performance on the queries for which user interactions were available.",
                "Figure 6.6 reports NDCG for the subset of the test queries with the implicit <br>feedback</br> features.",
                "The gains at top 1 are dramatic.",
                "The NDCG at 1 of BM25F+All increases from 0.6 to 0.75 (a 31% relative gain), achieving performance comparable to RN+All operating over a much richer feature set. 0.6 0.65 0.7 0.75 0.8 1 3 5 10K NDCG RN RN+All BM25 BM25+All Figure 6.6: NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions Similarly, gains on precision at top 1 are substantial (Figure 6.7), and are likely to be apparent to web search users.",
                "When implicit <br>feedback</br> is available, the BM25F+All system returns relevant document at top 1 almost 70% of the time, compared 53% of the time when implicit <br>feedback</br> is not considered by the original BM25F system. 0.45 0.5 0.55 0.6 0.65 0.7 1 3 5 10K Precision RN RN+All BM25 BM25+All Figure 6.7: Precision at K NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions We summarize the results on the MAP measure for attempted queries in Table 6.2.",
                "MAP improvements are both substantial and significant, with improvements over the BM25F ranker most pronounced.",
                "Method MAP Gain P(1) Gain RN 0.269 0.632 RN+All 0.321 0.051 (19%) 0.693 0.061(10%) BM25F 0.236 0.525 BM25F+All 0.292 0.056 (24%) 0.687 0.162 (31%) Table 6.2: Mean Average Precision (MAP) on attempted queries for best performing methods We now analyze the cases where implicit <br>feedback</br> was shown most helpful.",
                "Figure 6.8 reports the MAP improvements over the baseline BM25F run for each query with MAP under 0.6.",
                "Note that most of the improvement is for poorly performing queries (i.e., MAP < 0.1).",
                "Interestingly, incorporating user behavior information degrades accuracy for queries with high original MAP score.",
                "One possible explanation is that these easy queries tend to be navigational (i.e., having a single, highly-ranked most appropriate answer), and user interactions with lower-ranked results may indicate divergent information needs that are better served by the less popular results (with correspondingly poor overall relevance ratings). 0 50 100 150 200 250 300 350 0.1 0.2 0.3 0.4 0.5 0.6 -0.4 -0.35 -0.3 -0.25 -0.2 -0.15 -0.1 -0.05 0 0.05 0.1 0.15 0.2 Frequency Average Gain Figure 6.8: Gain of BM25F+All over original BM25F ranking To summarize our experimental results, incorporating implicit <br>feedback</br> in real web search setting resulted in significant improvements over the original rankings, using both BM25F and RN baselines.",
                "Our rich set of implicit features, such as time on page and deviations from the average behavior, provides advantages over using clickthrough alone as an indicator of interest.",
                "Furthermore, incorporating implicit <br>feedback</br> features directly into the learned ranking function is more effective than using implicit <br>feedback</br> for reranking.",
                "The improvements observed over large test sets of queries (1,000 total, between 466 and 495 with implicit <br>feedback</br> available) are both substantial and statistically significant. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper we explored the utility of incorporating noisy implicit <br>feedback</br> obtained in a real web search setting to improve web search ranking.",
                "We performed a large-scale evaluation over 3,000 queries and more than 12 million user interactions with a major search engine, establishing the utility of incorporating noisy implicit <br>feedback</br> to improve web search relevance.",
                "We compared two alternatives of incorporating implicit <br>feedback</br> into the search process, namely reranking with implicit <br>feedback</br> and incorporating implicit feedback features directly into the trained ranking function.",
                "Our experiments showed significant improvement over methods that do not consider implicit <br>feedback</br>.",
                "The gains are particularly dramatic for the top K=1 result in the final ranking, with precision improvements as high as 31%, and the gains are substantial for all values of K. Our experiments showed that implicit user <br>feedback</br> can further improve web search performance, when incorporated directly with popular content- and link-based features.",
                "Interestingly, implicit <br>feedback</br> is particularly valuable for queries with poor original ranking of results (e.g., MAP lower than 0.1).",
                "One promising direction for future work is to apply recent research on automatically predicting query difficulty, and only attempt to incorporate implicit <br>feedback</br> for the difficult queries.",
                "As another research direction we are exploring methods for extending our predictions to the previously unseen queries (e.g., query clustering), which should further improve the web search experience of users.",
                "ACKNOWLEDGMENTS We thank Chris Burges and Matt Richardson for an implementation of RankNet for our experiments.",
                "We also thank Robert Ragno for his valuable suggestions and many discussions. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, S. Dumais, and R.Ragno, Learning User Interaction Models for Predicting Web Search Result Preferences.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan, HARD Track Overview in TREC 2003, High Accuracy Retrieval from Documents, 2003 [3] R. Baeza-Yates and B. Ribeiro-Neto, Modern Information Retrieval, Addison-Wesley, 1999. [4] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine, in Proceedings of WWW, 1997 [5] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning, 2005 [6] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [7] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest.",
                "IEEE Internet Computing. 2001 [8] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [9] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [10] K Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2000 [11] T. Joachims, Optimizing Search Engines Using Clickthrough Data.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [12] T. Joachims, L. Granka, B. Pang, H. Hembrooke, and G. Gay, Accurately Interpreting Clickthrough Data as Implicit <br>feedback</br>, Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [13] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [14] D. Kelly and J. Teevan, Implicit <br>feedback</br> for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [15] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [16] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [17] D. Oard and J. Kim.",
                "Implicit <br>feedback</br> for recommender systems.",
                "In Proceedings of the AAAI Workshop on Recommender Systems. 1998 [18] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [19] N. Pharo, N. and K. Järvelin.",
                "The SST method: a tool for analyzing web information search processes.",
                "In Information Processing & Management, 2004 [20] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [21] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit <br>feedback</br>.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (SIGKDD), 2005. [22] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit <br>feedback</br>, in Proceedings of the ICML Workshop on Learning in Web Search, 2005 [23] S. E. Robertson, H. Zaragoza, and M. Taylor, Simple BM25 extension to multiple weighted fields, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [24] G. Salton & M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [25] E.M. Voorhees, D. Harman, Overview of TREC, 2001 [26] G.R.",
                "Xue, H.J.",
                "Zeng, Z. Chen, Y. Yu, W.Y.",
                "Ma, W.S.",
                "Xi, and W.G.",
                "Fan, Optimizing web search using web clickthrough data, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [27] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
                "Microsoft Cambridge at TREC 13: Web and Hard Tracks.",
                "In Proceedings of TREC 2004"
            ],
            "original_annotated_samples": [
                "We examine alternatives for incorporating <br>feedback</br> into the ranking process and explore the contributions of user <br>feedback</br> compared to other common web search features.",
                "We show that incorporating implicit <br>feedback</br> can augment other features, improving the accuracy of a competitive web search ranking algorithms by as much as 31% relative to the original performance.",
                "Categories and Subject Descriptors H.3.3 Information Search and Retrieval - Relevance <br>feedback</br>, search process; H.3.5 Online Information Services - Web-based services.",
                "Implicit relevance <br>feedback</br> for ranking and personalization has become an active area of research.",
                "Recent work by Joachims and others exploring implicit <br>feedback</br> in controlled environments have shown the value of incorporating implicit <br>feedback</br> into the ranking process."
            ],
            "translated_annotated_samples": [
                "Examinamos alternativas para incorporar <br>retroalimentación</br> en el proceso de clasificación y exploramos las contribuciones de la <br>retroalimentación</br> de usuarios en comparación con otras características comunes de búsqueda en la web.",
                "Mostramos que la incorporación de <br>retroalimentación</br> implícita puede aumentar otras características, mejorando la precisión de los algoritmos de clasificación de búsqueda web competitivos hasta en un 31% en comparación con el rendimiento original.",
                "Categorías y Descriptores de Asignaturas H.3.3 Búsqueda y Recuperación de Información - Retroalimentación de relevancia, proceso de búsqueda; H.3.5 Servicios de Información en Línea - Servicios basados en la web.",
                "La <br>retroalimentación</br> implícita de relevancia para la clasificación y personalización se ha convertido en un área activa de investigación.",
                "El trabajo reciente de Joachims y otros explorando el <br>feedback implícito</br> en entornos controlados ha demostrado el valor de incorporar el <br>feedback implícito</br> en el proceso de clasificación."
            ],
            "translated_text": "Mejorando la clasificación de búsqueda web al incorporar información sobre el comportamiento del usuario. Eugene Agichtein, Microsoft Research eugeneag@microsoft.com Eric Brill, Microsoft Research brill@microsoft.com Susan Dumais, Microsoft Research sdumais@microsoft.com RESUMEN Mostramos que la incorporación de datos sobre el comportamiento del usuario puede mejorar significativamente el orden de los principales resultados en una configuración real de búsqueda web. Examinamos alternativas para incorporar <br>retroalimentación</br> en el proceso de clasificación y exploramos las contribuciones de la <br>retroalimentación</br> de usuarios en comparación con otras características comunes de búsqueda en la web. Informamos los resultados de una evaluación a gran escala con más de 3,000 consultas y 12 millones de interacciones de usuarios con un motor de búsqueda web popular. Mostramos que la incorporación de <br>retroalimentación</br> implícita puede aumentar otras características, mejorando la precisión de los algoritmos de clasificación de búsqueda web competitivos hasta en un 31% en comparación con el rendimiento original. Categorías y Descriptores de Asignaturas H.3.3 Búsqueda y Recuperación de Información - Retroalimentación de relevancia, proceso de búsqueda; H.3.5 Servicios de Información en Línea - Servicios basados en la web. Términos generales Algoritmos, Medición, Experimentación 1. INTRODUCCIÓN Millones de usuarios interactúan con los motores de búsqueda a diario. Ellos emiten consultas, siguen algunos de los enlaces en los resultados, hacen clic en anuncios, pasan tiempo en páginas, reformulan sus consultas y realizan otras acciones. Estas interacciones pueden servir como una valiosa fuente de información para ajustar y mejorar la clasificación de los resultados de búsqueda en la web y pueden complementar juicios explícitos más costosos. La <br>retroalimentación</br> implícita de relevancia para la clasificación y personalización se ha convertido en un área activa de investigación. El trabajo reciente de Joachims y otros explorando el <br>feedback implícito</br> en entornos controlados ha demostrado el valor de incorporar el <br>feedback implícito</br> en el proceso de clasificación. ",
            "candidates": [],
            "error": [
                [
                    "retroalimentación",
                    "retroalimentación",
                    "retroalimentación",
                    "retroalimentación",
                    "feedback implícito",
                    "feedback implícito"
                ]
            ]
        },
        "result": {
            "translated_key": "resultado",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Improving Web Search Ranking by Incorporating User Behavior Information Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com ABSTRACT We show that incorporating user behavior data can significantly improve ordering of top results in real web search setting.",
                "We examine alternatives for incorporating feedback into the ranking process and explore the contributions of user feedback compared to other common web search features.",
                "We report results of a large scale evaluation over 3,000 queries and 12 million user interactions with a popular web search engine.",
                "We show that incorporating implicit feedback can augment other features, improving the accuracy of a competitive web search ranking algorithms by as much as 31% relative to the original performance.",
                "Categories and Subject Descriptors H.3.3 Information Search and Retrieval - Relevance feedback, search process; H.3.5 Online Information Services - Web-based services.",
                "General Terms Algorithms, Measurement, Experimentation 1.",
                "INTRODUCTION Millions of users interact with search engines daily.",
                "They issue queries, follow some of the links in the results, click on ads, spend time on pages, reformulate their queries, and perform other actions.",
                "These interactions can serve as a valuable source of information for tuning and improving web search <br>result</br> ranking and can compliment more costly explicit judgments.",
                "Implicit relevance feedback for ranking and personalization has become an active area of research.",
                "Recent work by Joachims and others exploring implicit feedback in controlled environments have shown the value of incorporating implicit feedback into the ranking process.",
                "Our motivation for this work is to understand how implicit feedback can be used in a large-scale operational environment to improve retrieval.",
                "How does it compare to and compliment evidence from page content, anchor text, or link-based features such as inlinks or PageRank?",
                "While it is intuitive that user interactions with the web search engine should reveal at least some information that could be used for ranking, estimating user preferences in real web search settings is a challenging problem, since real user interactions tend to be more noisy than commonly assumed in the controlled settings of previous studies.",
                "Our paper explores whether implicit feedback can be helpful in realistic environments, where user feedback can be noisy (or adversarial) and a web search engine already uses hundreds of features and is heavily tuned.",
                "To this end, we explore different approaches for ranking web search results using real user behavior obtained as part of normal interactions with the web search engine.",
                "The specific contributions of this paper include: • Analysis of alternatives for incorporating user behavior into web search ranking (Section 3). • An application of a robust implicit feedback model derived from mining millions of user interactions with a major web search engine (Section 4). • A large scale evaluation over real user queries and search results, showing significant improvements derived from incorporating user feedback (Section 6).",
                "We summarize our findings and discuss extensions to the current work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "Most common approaches primarily focus on similarity of query and a page, as well as the overall page quality [3,4,24].",
                "However, with increasing popularity of search engines, implicit feedback (i.e., the actions users take when interacting with the search engine) can be used to improve the rankings.",
                "Implicit relevance measures have been studied by several research groups.",
                "An overview of implicit measures is compiled in Kelly and Teevan [14].",
                "This research, while developing valuable insights into implicit relevance measures, was not applied to improve the ranking of web search results in realistic settings.",
                "Closely related to our work, Joachims [11] collected implicit measures in place of explicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "Fox et al. [8] explored the relationship between implicit and explicit measures in Web search, and developed Bayesian models to correlate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "This work considered a wide range of user behaviors (e.g., dwell time, scroll time, reformulation patterns) in addition to the popular clickthrough behavior.",
                "However, the modeling effort was aimed at predicting explicit relevance judgments from implicit user actions and not specifically at learning ranking functions.",
                "Other studies of user behavior in web search include Pharo and Järvelin [19], but were not directly applied to improve ranking.",
                "More recently, Joachims et al. [12] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthroughs in a controlled, laboratory setting.",
                "Unfortunately, the extent to which previous research applies to real-world web search is unclear.",
                "At the same time, while recent work (e.g., [26]) on using clickthrough information for improving web search ranking is promising, it captures only one aspect of the user interactions with web search engines.",
                "We build on existing research to develop robust user behavior interpretation techniques for the real web search setting.",
                "Instead of treating each user as a reliable expert, we aggregate information from multiple, unreliable, user search session traces, as we describe in the next two sections. 3.",
                "INCORPORATING IMPLICIT FEEDBACK We consider two complementary approaches to ranking with implicit feedback: (1) treating implicit feedback as independent evidence for ranking results, and (2) integrating implicit feedback features directly into the ranking algorithm.",
                "We describe the two general ranking approaches next.",
                "The specific implicit feedback features are described in Section 4, and the algorithms for interpreting and incorporating implicit feedback are described in Section 5. 3.1 Implicit Feedback as Independent Evidence The general approach is to re-rank the results obtained by a web search engine according to observed clickthrough and other user interactions for the query in previous search sessions.",
                "Each <br>result</br> is assigned a score according to expected relevance/user satisfaction based on previous interactions, resulting in some preference ordering based on user interactions alone.",
                "While there has been significant work on merging multiple rankings, we adapt a simple and robust approach of ignoring the original rankers scores, and instead simply merge the rank orders.",
                "The main reason for ignoring the original scores is that since the feature spaces and learning algorithms are different, the scores are not directly comparable, and re-normalization tends to remove the benefit of incorporating classifier scores.",
                "We experimented with a variety of merging functions on the development set of queries (and using a set of interactions from a different time period from final evaluation sets).",
                "We found that a simple rank merging heuristic combination works well, and is robust to variations in score values from original rankers.",
                "For a given query q, the implicit score ISd is computed for each <br>result</br> d from available user interaction features, resulting in the implicit rank Id for each <br>result</br>.",
                "We compute a merged score SM(d) for d by combining the ranks obtained from implicit feedback, Id with the original rank of d, Od:     ¡     ¢ £ + + + + = otherwise O dforexistsfeedbackimplicitif OI w wOIdS d dd I IddM 1 1 1 1 1 1 ),,,( where the weight wI is a heuristically tuned scaling factor representing the relative importance of the implicit feedback.",
                "The query results are ordered in by decreasing values of SM to produce the final ranking.",
                "One special case of this model arises when setting wI to a very large value, effectively forcing clicked results to be ranked higher than un-clicked results - an intuitive and effective heuristic that we will use as a baseline.",
                "Applying more sophisticated classifier and ranker combination algorithms may <br>result</br> in additional improvements, and is a promising direction for future work.",
                "The approach above assumes that there are no interactions between the underlying features producing the original web search ranking and the implicit feedback features.",
                "We now relax this assumption by integrating implicit feedback features directly into the ranking process. 3.2 Ranking with Implicit Feedback Features Modern web search engines rank results based on a large number of features, including content-based features (i.e., how closely a query matches the text or title or anchor text of the document), and query-independent page quality features (e.g., PageRank of the document or the domain).",
                "In most cases, automatic (or semiautomatic) methods are developed for tuning the specific ranking function that combines these feature values.",
                "Hence, a natural approach is to incorporate implicit feedback features directly as features for the ranking algorithm.",
                "During training or tuning, the ranker can be tuned as before but with additional features.",
                "At runtime, the search engine would fetch the implicit feedback features associated with each query-<br>result</br> URL pair.",
                "This model requires a ranking algorithm to be robust to missing values: more than 50% of queries to web search engines are unique, with no previous implicit feedback available.",
                "We now describe such a ranker that we used to learn over the combined feature sets including implicit feedback. 3.3 Learning to Rank Web Search Results A key aspect of our approach is exploiting recent advances in machine learning, namely trainable ranking algorithms for web search and information retrieval (e.g., [5, 11] and classical results reviewed in [3]).",
                "In our setting, explicit human relevance judgments (labels) are available for a set of web search queries and results.",
                "Hence, an attractive choice to use is a supervised machine learning technique to learn a ranking function that best predicts relevance judgments.",
                "RankNet is one such algorithm.",
                "It is a neural net tuning algorithm that optimizes feature weights to best match explicitly provided pairwise user preferences.",
                "While the specific training algorithms used by RankNet are beyond the scope of this paper, it is described in detail in [5] and includes extensive evaluation and comparison with other ranking methods.",
                "An attractive feature of RankNet is both train- and run-time efficiency - runtime ranking can be quickly computed and can scale to the web, and training can be done over thousands of queries and associated judged results.",
                "We use a 2-layer implementation of RankNet in order to model non-linear relationships between features.",
                "Furthermore, RankNet can learn with many (differentiable) cost functions, and hence can automatically learn a ranking function from human-provided labels, an attractive alternative to heuristic feature combination techniques.",
                "Hence, we will also use RankNet as a generic ranker to explore the contribution of implicit feedback for different ranking alternatives. 4.",
                "IMPLICIT USER FEEDBACK MODEL Our goal is to accurately interpret noisy user feedback obtained as by tracing user interactions with the search engine.",
                "Interpreting implicit feedback in real web search setting is not an easy task.",
                "We characterize this problem in detail in [1], where we motivate and evaluate a wide variety of models of implicit user activities.",
                "The general approach is to represent user actions for each search <br>result</br> as a vector of features, and then train a ranker on these features to discover feature values indicative of relevant (and nonrelevant) search results.",
                "We first briefly summarize our features and model, and the learning approach (Section 4.2) in order to provide sufficient information to replicate our ranking methods and the subsequent experiments. 4.1 Representing User Actions as Features We model observed web search behaviors as a combination of a ``background component (i.e., query- and relevance-independent noise in user behavior, including positional biases with <br>result</br> interactions), and a ``relevance component (i.e., query-specific behavior indicative of relevance of a <br>result</br> to a query).",
                "We design our features to take advantage of aggregated user behavior.",
                "The feature set is comprised of directly observed features (computed directly from observations for each query), as well as queryspecific derived features, computed as the deviation from the overall query-independent distribution of values for the corresponding directly observed feature values.",
                "The features used to represent user interactions with web search results are summarized in Table 4.1.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "We include the traditional implicit feedback features such as clickthrough counts for the results, as well as our novel derived features such as the deviation of the observed clickthrough number for a given query-URL pair from the expected number of clicks on a <br>result</br> in the given position.",
                "We also model the browsing behavior after a <br>result</br> was clicked - e.g., the average page dwell time for a given query-URL pair, as well as its deviation from the expected (average) dwell time.",
                "Furthermore, the feature set was designed to provide essential information about the user experience to make feedback interpretation robust.",
                "For example, web search users can often determine whether a <br>result</br> is relevant by looking at the <br>result</br> title, URL, and summary - in many cases, looking at the original document is not necessary.",
                "To model this aspect of user experience we include features such as overlap in words in title and words in query (TitleOverlap) and the fraction of words shared by the query and the <br>result</br> summary.",
                "Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickProbability Probability of a click for this query and URL ClickDeviation Deviation from expected click probability IsNextClicked 1 if clicked on next position, 0 otherwise IsPreviousClicked 1 if clicked on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, no parameters IsFollowedLink 1 if followed link to <br>result</br>, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from average dwell time on page CumulativeDeviation Deviation from average cumulative dwell time DomainDeviation Deviation from average dwell time on domain Query-text features TitleOverlap Words shared between query and title SummaryOverlap Words shared between query and snippet QueryURLOverlap Words shared between query and URL QueryDomainOverlap Words shared between query and URL domain QueryLength Number of tokens in query QueryNextOverlap Fraction of words shared with next query Table 4.1: Some features used to represent post-search navigation history for a given query and search <br>result</br> URL.",
                "Having described our feature set, we briefly review our general method for deriving a user behavior model. 4.2 Deriving a User Feedback Model To learn to interpret the observed user behavior, we correlate user actions (i.e., the features in Table 4.1 representing the actions) with the explicit user judgments for a set of training queries.",
                "We find all the instances in our session logs where these queries were submitted to the search engine, and aggregate the user behavior features for all search sessions involving these queries.",
                "Each observed query-URL pair is represented by the features in Table 4.1, with values averaged over all search sessions, and assigned one of six possible relevance labels, ranging from Perfect to Bad, as assigned by explicit relevance judgments.",
                "These labeled feature vectors are used as input to the RankNet training algorithm (Section 3.3) which produces a trained user behavior model.",
                "This approach is particularly attractive as it does not require heuristics beyond feature engineering.",
                "The resulting user behavior model is used to help rank web search resultseither directly or in combination with other features, as described below. 5.",
                "EXPERIMENTAL SETUP The ultimate goal of incorporating implicit feedback into ranking is to improve the relevance of the returned web search results.",
                "Hence, we compare the ranking methods over a large set of judged queries with explicit relevance labels provided by human judges.",
                "In order for the evaluation to be realistic we obtained a random sample of queries from web search logs of a major search engine, with associated results and traces for user actions.",
                "We describe this dataset in detail next.",
                "Our metrics are described in Section 5.2 that we use to evaluate the ranking alternatives, listed in Section 5.3 in the experiments of Section 6. 5.1 Datasets We compared our ranking methods over a random sample of 3,000 queries from the search engine query logs.",
                "The queries were drawn from the logs uniformly at random by token without replacement, resulting in a query sample representative of the overall query distribution.",
                "On average, 30 results were explicitly labeled by human judges using a six point scale ranging from Perfect down to Bad.",
                "Overall, there were over 83,000 results with explicit relevance judgments.",
                "In order to compute various statistics, documents with label Good or better will be considered relevant, and with lower labels to be non-relevant.",
                "Note that the experiments were performed over the results already highly ranked by a web search engine, which corresponds to a typical user experience which is limited to the small number of the highly ranked results for a typical web search query.",
                "The user interactions were collected over a period of 8 weeks using voluntary opt-in information.",
                "In total, over 1.2 million unique queries were instrumented, resulting in over 12 million individual interactions with the search engine.",
                "The data consisted of user interactions with the web search engine (e.g., clicking on a <br>result</br> link, going back to search results, etc.) performed after a query was submitted.",
                "These actions were aggregated across users and search sessions and converted to features in Table 4.1.",
                "To create the training, validation, and test query sets, we created three different random splits of 1,500 training, 500 validation, and 1000 test queries.",
                "The splits were done randomly by query, so that there was no overlap in training, validation, and test queries. 5.2 Evaluation Metrics We evaluate the ranking algorithms over a range of accepted information retrieval metrics, namely Precision at K (P(K)), Normalized Discounted Cumulative Gain (NDCG), and Mean Average Precision (MAP).",
                "Each metric focuses on a deferent aspect of system performance, as we describe below. • Precision at K: As the most intuitive metric, P(K) reports the fraction of documents ranked in the top K results that are labeled as relevant.",
                "In our setting, we require a relevant document to be labeled Good or higher.",
                "The position of relevant documents within the top K is irrelevant, and hence this metric measure overall user satisfaction with the top K results. • NDCG at K: NDCG is a retrieval measure devised specifically for web search evaluation [10].",
                "For a given query q, the ranked results are examined from the top ranked down, and the NDCG computed as:   = +−= K j jr qq jMN 1 )( )1log(/)12( Where Mq is a normalization constant calculated so that a perfect ordering would obtain NDCG of 1; and each r(j) is an integer relevance label (0=Bad and 5=Perfect) of <br>result</br> returned at position j.",
                "Note that unlabeled and Bad documents do not contribute to the sum, but will reduce NDCG for the query pushing down the relevant labeled documents, reducing their contributions.",
                "NDCG is well suited to web search evaluation, as it rewards relevant documents in the top ranked results more heavily than those ranked lower. • MAP: Average precision for each query is defined as the mean of the precision at K values computed after each relevant document was retrieved.",
                "The final MAP value is defined as the mean of average precisions of all queries in the test set.",
                "This metric is the most commonly used single-value summary of a run over a set of queries. 5.3 Ranking Methods Compared Recall that our goal is to quantify the effectiveness of implicit behavior for real web search.",
                "One dimension is to compare the utility of implicit feedback with other information available to a web search engine.",
                "Specifically, we compare effectiveness of implicit user behaviors with content-based matching, static page quality features, and combinations of all features. • BM25F: As a strong web search baseline we used the BM25F scoring, which was used in one of the best performing systems in the TREC 2004 Web track [23,27].",
                "BM25F and its variants have been extensively described and evaluated in IR literature, and hence serve as a strong, reproducible baseline.",
                "The BM25F variant we used for our experiments computes separate match scores for each field for a <br>result</br> document (e.g., body text, title, and anchor text), and incorporates query-independent linkbased information (e.g., PageRank, ClickDistance, and URL depth).",
                "The scoring function and field-specific tuning is described in detail in [23].",
                "Note that BM25F does not directly consider explicit or implicit feedback for tuning. • RN: The ranking produced by a neural net ranker (RankNet, described in Section 3.3) that learns to rank web search results by incorporating BM25F and a large number of additional static and dynamic features describing each search <br>result</br>.",
                "This system automatically learns weights for all features (including the BM25F score for a document) based on explicit human labels for a large set of queries.",
                "A system incorporating an implementation of RankNet is currently in use by a major search engine and can be considered representative of the state of the art in web search. • BM25F-RerankCT: The ranking produced by incorporating clickthrough statistics to reorder web search results ranked by BM25F above.",
                "Clickthrough is a particularly important special case of implicit feedback, and has been shown to correlate with <br>result</br> relevance.",
                "This is a special case of the ranking method in Section 3.1, with the weight wI set to 1000 and the ranking Id is simply the number of clicks on the <br>result</br> corresponding to d. In effect, this ranking brings to the top all returned web search results with at least one click (and orders them in decreasing order by number of clicks).",
                "The relative ranking of the remainder of results is unchanged and they are inserted below all clicked results.",
                "This method serves as our baseline implicit feedback reranking method.",
                "BM25F-RerankAll The ranking produced by reordering the BM25F results using all user behavior features (Section 4).",
                "This method learns a model of user preferences by correlating feature values with explicit relevance labels using the RankNet neural net algorithm (Section 4.2).",
                "At runtime, for a given query the implicit score Ir is computed for each <br>result</br> r with available user interaction features, and the implicit ranking is produced.",
                "The merged ranking is computed as described in Section 3.1.",
                "Based on the experiments over the development set we fix the value of wI to 3 (the effect of the wI parameter for this ranker turned out to be negligible). • BM25F+All: Ranking derived by training the RankNet (Section 3.3) learner over the features set of the BM25F score as well as all implicit feedback features (Section 3.2).",
                "We used the 2-layer implementation of RankNet [5] trained on the queries and labels in the training and validation sets. • RN+All: Ranking derived by training the 2-layer RankNet ranking algorithm (Section 3.3) over the union of all content, dynamic, and implicit feedback features (i.e., all of the features described above as well as all of the new implicit feedback features we introduced).",
                "The ranking methods above span the range of the information used for ranking, from not using the implicit or explicit feedback at all (i.e., BM25F) to a modern web search engine using hundreds of features and tuned on explicit judgments (RN).",
                "As we will show next, incorporating user behavior into these ranking systems dramatically improves the relevance of the returned documents. 6.",
                "EXPERIMENTAL RESULTS Implicit feedback for web search ranking can be exploited in a number of ways.",
                "We compare alternative methods of exploiting implicit feedback, both by re-ranking the top results (i.e., the BM25F-RerankCT and BM25F-RerankAll methods that reorder BM25F results), as well as by integrating the implicit features directly into the ranking process (i.e., the RN+ALL and BM25F+All methods which learn to rank results over the implicit feedback and other features).",
                "We compare our methods over strong baselines (BM25F and RN) over the NDCG, Precision at K, and MAP measures defined in Section 5.2.",
                "The results were averaged over three random splits of the overall dataset.",
                "Each split contained 1500 training, 500 validation, and 1000 test queries, all query sets disjoint.",
                "We first present the results over all 1000 test queries (i.e., including queries for which there are no implicit measures so we use the original web rankings).",
                "We then drill down to examine the effects on reranking for the attempted queries in more detail, analyzing where implicit feedback proved most beneficial.",
                "We first experimented with different methods of re-ranking the output of the BM25F search results.",
                "Figures 6.1 and 6.2 report NDCG and Precision for BM25F, as well as for the strategies reranking results with user feedback (Section 3.1).",
                "Incorporating all user feedback (either in reranking framework or as features to the learner directly) results in significant improvements (using two-tailed t-test with p=0.01) over both the original BM25F ranking as well as over reranking with clickthrough alone.",
                "The improvement is consistent across the top 10 results and largest for the top <br>result</br>: NDCG at 1 for BM25F+All is 0.622 compared to 0.518 of the original results, and precision at 1 similarly increases from 0.5 to 0.63.",
                "Based on these results we will use the direct feature combination (i.e., BM25F+All) ranker for subsequent comparisons involving implicit feedback. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 1 2 3 4 5 6 7 8 9 10K NDCG BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.1: NDCG at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K 0.35 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.2: Precision at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K Interestingly, using clickthrough alone, while giving significant benefit over the original BM25F ranking, is not as effective as considering the full set of features in Table 4.1.",
                "While we analyze user behavior (and most effective component features) in a separate paper [1], it is worthwhile to give a concrete example of the kind of noise inherent in real user feedback in web search setting. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 <br>result</br> position Relativeclickfrequency PTR=2 PTR=3 PTR=5 Figure 6.3: Relative clickthrough frequency for queries with varying Position of Top Relevant <br>result</br> (PTR).",
                "If users considered only the relevance of a <br>result</br> to their query, they would click on the topmost relevant results.",
                "Unfortunately, as Joachims and others have shown, presentation also influences which results users click on quite dramatically.",
                "Users often click on results above the relevant one presumably because the short summaries do not provide enough information to make accurate relevance assessments and they have learned that on average topranked items are relevant.",
                "Figure 6.3 shows relative clickthrough frequencies for queries with known relevant items at positions other than the first position; the position of the top relevant <br>result</br> (PTR) ranges from 2-10 in the figure.",
                "For example, for queries with first relevant <br>result</br> at position 5 (PTR=5), there are more clicks on the non-relevant results in higher ranked positions than on the first relevant <br>result</br> at position 5.",
                "As we will see, learning over a richer behavior feature set, results in substantial accuracy improvement over clickthrough alone.",
                "We now consider incorporating user behavior into a much richer feature set, RN (Section 5.3) used by a major web search engine.",
                "RN incorporates BM25F, link-based features, and hundreds of other features.",
                "Figure 6.4 reports NDCG at K and Figure 6.5 reports Precision at K. Interestingly, while the original RN rankings are significantly more accurate than BM25F alone, incorporating implicit feedback features (BM25F+All) results in ranking that significantly outperforms the original RN rankings.",
                "In other words, implicit feedback incorporates sufficient information to replace the hundreds of other features available to the RankNet learner trained on the RN feature set. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 1 2 3 4 5 6 7 8 9 10K NDCG RN RN+All BM25 BM25+All Figure 6.4: NDCG at K for BM25F, BM25F+All, RN, and RN+All for varying K Furthermore, enriching the RN features with implicit feedback set exhibits significant gain on all measures, allowing RN+All to outperform all other methods.",
                "This demonstrates the complementary nature of implicit feedback with other features available to a state of the art web search engine. 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision RN RN+All BM25 BM25+All Figure 6.5: Precision at K for BM25F, BM25F+All, RN, and RN+All for varying K We summarize the performance of the different ranking methods in Table 6.1.",
                "We report the Mean Average Precision (MAP) score for each system.",
                "While not intuitive to interpret, MAP allows quantitative comparison on a single metric.",
                "The gains marked with * are significant at p=0.01 level using two tailed t-test.",
                "MAP Gain P(1) Gain BM25F 0.184 - 0.503BM25F-Rerank-CT 0.215 0.031* 0.577 0.073* BM25F-RerankImplicit 0.218 0.003 0.605 0.028* BM25F+Implicit 0.222 0.004 0.620 0.015* RN 0.215 - 0.597RN+All 0.248 0.033* 0.629 0.032* Table 6.1: Mean Average Precision (MAP) for all strategies.",
                "So far we reported results averaged across all queries in the test set.",
                "Unfortunately, less than half had sufficient interactions to attempt reranking.",
                "Out of the 1000 queries in test, between 46% and 49%, depending on the train-test split, had sufficient interaction information to make predictions (i.e., there was at least 1 search session in which at least 1 <br>result</br> URL was clicked on by the user).",
                "This is not surprising: web search is heavy-tailed, and there are many unique queries.",
                "We now consider the performance on the queries for which user interactions were available.",
                "Figure 6.6 reports NDCG for the subset of the test queries with the implicit feedback features.",
                "The gains at top 1 are dramatic.",
                "The NDCG at 1 of BM25F+All increases from 0.6 to 0.75 (a 31% relative gain), achieving performance comparable to RN+All operating over a much richer feature set. 0.6 0.65 0.7 0.75 0.8 1 3 5 10K NDCG RN RN+All BM25 BM25+All Figure 6.6: NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions Similarly, gains on precision at top 1 are substantial (Figure 6.7), and are likely to be apparent to web search users.",
                "When implicit feedback is available, the BM25F+All system returns relevant document at top 1 almost 70% of the time, compared 53% of the time when implicit feedback is not considered by the original BM25F system. 0.45 0.5 0.55 0.6 0.65 0.7 1 3 5 10K Precision RN RN+All BM25 BM25+All Figure 6.7: Precision at K NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions We summarize the results on the MAP measure for attempted queries in Table 6.2.",
                "MAP improvements are both substantial and significant, with improvements over the BM25F ranker most pronounced.",
                "Method MAP Gain P(1) Gain RN 0.269 0.632 RN+All 0.321 0.051 (19%) 0.693 0.061(10%) BM25F 0.236 0.525 BM25F+All 0.292 0.056 (24%) 0.687 0.162 (31%) Table 6.2: Mean Average Precision (MAP) on attempted queries for best performing methods We now analyze the cases where implicit feedback was shown most helpful.",
                "Figure 6.8 reports the MAP improvements over the baseline BM25F run for each query with MAP under 0.6.",
                "Note that most of the improvement is for poorly performing queries (i.e., MAP < 0.1).",
                "Interestingly, incorporating user behavior information degrades accuracy for queries with high original MAP score.",
                "One possible explanation is that these easy queries tend to be navigational (i.e., having a single, highly-ranked most appropriate answer), and user interactions with lower-ranked results may indicate divergent information needs that are better served by the less popular results (with correspondingly poor overall relevance ratings). 0 50 100 150 200 250 300 350 0.1 0.2 0.3 0.4 0.5 0.6 -0.4 -0.35 -0.3 -0.25 -0.2 -0.15 -0.1 -0.05 0 0.05 0.1 0.15 0.2 Frequency Average Gain Figure 6.8: Gain of BM25F+All over original BM25F ranking To summarize our experimental results, incorporating implicit feedback in real web search setting resulted in significant improvements over the original rankings, using both BM25F and RN baselines.",
                "Our rich set of implicit features, such as time on page and deviations from the average behavior, provides advantages over using clickthrough alone as an indicator of interest.",
                "Furthermore, incorporating implicit feedback features directly into the learned ranking function is more effective than using implicit feedback for reranking.",
                "The improvements observed over large test sets of queries (1,000 total, between 466 and 495 with implicit feedback available) are both substantial and statistically significant. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper we explored the utility of incorporating noisy implicit feedback obtained in a real web search setting to improve web search ranking.",
                "We performed a large-scale evaluation over 3,000 queries and more than 12 million user interactions with a major search engine, establishing the utility of incorporating noisy implicit feedback to improve web search relevance.",
                "We compared two alternatives of incorporating implicit feedback into the search process, namely reranking with implicit feedback and incorporating implicit feedback features directly into the trained ranking function.",
                "Our experiments showed significant improvement over methods that do not consider implicit feedback.",
                "The gains are particularly dramatic for the top K=1 <br>result</br> in the final ranking, with precision improvements as high as 31%, and the gains are substantial for all values of K. Our experiments showed that implicit user feedback can further improve web search performance, when incorporated directly with popular content- and link-based features.",
                "Interestingly, implicit feedback is particularly valuable for queries with poor original ranking of results (e.g., MAP lower than 0.1).",
                "One promising direction for future work is to apply recent research on automatically predicting query difficulty, and only attempt to incorporate implicit feedback for the difficult queries.",
                "As another research direction we are exploring methods for extending our predictions to the previously unseen queries (e.g., query clustering), which should further improve the web search experience of users.",
                "ACKNOWLEDGMENTS We thank Chris Burges and Matt Richardson for an implementation of RankNet for our experiments.",
                "We also thank Robert Ragno for his valuable suggestions and many discussions. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, S. Dumais, and R.Ragno, Learning User Interaction Models for Predicting Web Search <br>result</br> Preferences.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan, HARD Track Overview in TREC 2003, High Accuracy Retrieval from Documents, 2003 [3] R. Baeza-Yates and B. Ribeiro-Neto, Modern Information Retrieval, Addison-Wesley, 1999. [4] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine, in Proceedings of WWW, 1997 [5] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning, 2005 [6] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [7] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest.",
                "IEEE Internet Computing. 2001 [8] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [9] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [10] K Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2000 [11] T. Joachims, Optimizing Search Engines Using Clickthrough Data.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [12] T. Joachims, L. Granka, B. Pang, H. Hembrooke, and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [13] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [14] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [15] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [16] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [17] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems.",
                "In Proceedings of the AAAI Workshop on Recommender Systems. 1998 [18] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [19] N. Pharo, N. and K. Järvelin.",
                "The SST method: a tool for analyzing web information search processes.",
                "In Information Processing & Management, 2004 [20] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [21] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (SIGKDD), 2005. [22] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in Proceedings of the ICML Workshop on Learning in Web Search, 2005 [23] S. E. Robertson, H. Zaragoza, and M. Taylor, Simple BM25 extension to multiple weighted fields, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [24] G. Salton & M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [25] E.M. Voorhees, D. Harman, Overview of TREC, 2001 [26] G.R.",
                "Xue, H.J.",
                "Zeng, Z. Chen, Y. Yu, W.Y.",
                "Ma, W.S.",
                "Xi, and W.G.",
                "Fan, Optimizing web search using web clickthrough data, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [27] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
                "Microsoft Cambridge at TREC 13: Web and Hard Tracks.",
                "In Proceedings of TREC 2004"
            ],
            "original_annotated_samples": [
                "These interactions can serve as a valuable source of information for tuning and improving web search <br>result</br> ranking and can compliment more costly explicit judgments.",
                "Each <br>result</br> is assigned a score according to expected relevance/user satisfaction based on previous interactions, resulting in some preference ordering based on user interactions alone.",
                "For a given query q, the implicit score ISd is computed for each <br>result</br> d from available user interaction features, resulting in the implicit rank Id for each <br>result</br>.",
                "Applying more sophisticated classifier and ranker combination algorithms may <br>result</br> in additional improvements, and is a promising direction for future work.",
                "At runtime, the search engine would fetch the implicit feedback features associated with each query-<br>result</br> URL pair."
            ],
            "translated_annotated_samples": [
                "Estas interacciones pueden servir como una valiosa fuente de información para ajustar y mejorar la clasificación de los <br>resultados de búsqueda</br> en la web y pueden complementar juicios explícitos más costosos.",
                "Cada <br>resultado</br> se le asigna una puntuación según la relevancia esperada/ satisfacción del usuario basada en interacciones previas, lo que resulta en un cierto orden de preferencia basado únicamente en las interacciones del usuario.",
                "Para una consulta dada q, se calcula la puntuación implícita ISd para cada <br>resultado</br> d a partir de las características de interacción del usuario disponibles, lo que resulta en la clasificación implícita Id para cada <br>resultado</br>.",
                "Aplicar algoritmos de combinación de clasificadores y ordenadores más sofisticados puede <br>resultar</br> en mejoras adicionales, y es una dirección prometedora para trabajos futuros.",
                "En tiempo de ejecución, el motor de búsqueda recuperaría las características de retroalimentación implícita asociadas con cada par de URL de consulta y <br>resultado</br>."
            ],
            "translated_text": "Mejorando la clasificación de búsqueda web al incorporar información sobre el comportamiento del usuario. Eugene Agichtein, Microsoft Research eugeneag@microsoft.com Eric Brill, Microsoft Research brill@microsoft.com Susan Dumais, Microsoft Research sdumais@microsoft.com RESUMEN Mostramos que la incorporación de datos sobre el comportamiento del usuario puede mejorar significativamente el orden de los principales resultados en una configuración real de búsqueda web. Examinamos alternativas para incorporar retroalimentación en el proceso de clasificación y exploramos las contribuciones de la retroalimentación de usuarios en comparación con otras características comunes de búsqueda en la web. Informamos los resultados de una evaluación a gran escala con más de 3,000 consultas y 12 millones de interacciones de usuarios con un motor de búsqueda web popular. Mostramos que la incorporación de retroalimentación implícita puede aumentar otras características, mejorando la precisión de los algoritmos de clasificación de búsqueda web competitivos hasta en un 31% en comparación con el rendimiento original. Categorías y Descriptores de Asignaturas H.3.3 Búsqueda y Recuperación de Información - Retroalimentación de relevancia, proceso de búsqueda; H.3.5 Servicios de Información en Línea - Servicios basados en la web. Términos generales Algoritmos, Medición, Experimentación 1. INTRODUCCIÓN Millones de usuarios interactúan con los motores de búsqueda a diario. Ellos emiten consultas, siguen algunos de los enlaces en los resultados, hacen clic en anuncios, pasan tiempo en páginas, reformulan sus consultas y realizan otras acciones. Estas interacciones pueden servir como una valiosa fuente de información para ajustar y mejorar la clasificación de los <br>resultados de búsqueda</br> en la web y pueden complementar juicios explícitos más costosos. La retroalimentación implícita de relevancia para la clasificación y personalización se ha convertido en un área activa de investigación. El trabajo reciente de Joachims y otros explorando el feedback implícito en entornos controlados ha demostrado el valor de incorporar el feedback implícito en el proceso de clasificación. Nuestra motivación para este trabajo es comprender cómo se puede utilizar la retroalimentación implícita en un entorno operativo a gran escala para mejorar la recuperación. ¿Cómo se compara y complementa con la evidencia del contenido de la página, el texto del ancla o las características basadas en enlaces como inlinks o PageRank? Si bien es intuitivo que las interacciones de los usuarios con el motor de búsqueda web deberían revelar al menos alguna información que podría ser utilizada para la clasificación, estimar las preferencias de los usuarios en entornos reales de búsqueda web es un problema desafiante, ya que las interacciones reales tienden a ser más ruidosas de lo que comúnmente se asume en los entornos controlados de estudios previos. Nuestro artículo explora si la retroalimentación implícita puede ser útil en entornos realistas, donde la retroalimentación del usuario puede ser ruidosa (o adversa) y un motor de búsqueda web ya utiliza cientos de características y está altamente ajustado. Con este fin, exploramos diferentes enfoques para clasificar los resultados de búsqueda web utilizando el comportamiento real de los usuarios obtenido como parte de las interacciones normales con el motor de búsqueda web. Las contribuciones específicas de este artículo incluyen: • Análisis de alternativas para incorporar el comportamiento del usuario en la clasificación de búsqueda web (Sección 3). • Aplicación de un modelo robusto de retroalimentación implícita derivado de la minería de millones de interacciones de usuarios con un importante motor de búsqueda web (Sección 4). • Una evaluación a gran escala sobre consultas reales de usuarios y resultados de búsqueda, mostrando mejoras significativas derivadas de la incorporación de la retroalimentación del usuario (Sección 6). Resumimos nuestros hallazgos y discutimos extensiones al trabajo actual en la Sección 7, que concluye el artículo. ANTECEDENTES Y TRABAJO RELACIONADO Clasificar los resultados de búsqueda es un problema fundamental en la recuperación de información. La mayoría de los enfoques comunes se centran principalmente en la similitud entre la consulta y una página, así como en la calidad general de la página [3,4,24]. Sin embargo, con la creciente popularidad de los motores de búsqueda, la retroalimentación implícita (es decir, las acciones que los usuarios realizan al interactuar con el motor de búsqueda) se puede utilizar para mejorar las clasificaciones. Las medidas de relevancia implícitas han sido estudiadas por varios grupos de investigación. Un resumen de las medidas implícitas se recopila en Kelly y Teevan [14]. Esta investigación, si bien desarrolló valiosas ideas sobre medidas implícitas de relevancia, no se aplicó para mejorar la clasificación de los resultados de búsqueda en la web en entornos realistas. Estrechamente relacionado con nuestro trabajo, Joachims [11] recopiló medidas implícitas en lugar de medidas explícitas, introduciendo una técnica basada completamente en datos de clics para aprender funciones de clasificación. Fox et al. [8] exploraron la relación entre medidas implícitas y explícitas en la búsqueda web, y desarrollaron modelos bayesianos para correlacionar medidas implícitas y juicios explícitos de relevancia tanto para consultas individuales como para sesiones de búsqueda. Este trabajo consideró una amplia gama de comportamientos de usuario (por ejemplo, tiempo de permanencia, tiempo de desplazamiento, patrones de reformulación) además del popular comportamiento de clics. Sin embargo, el esfuerzo de modelado se centró en predecir juicios de relevancia explícitos a partir de acciones implícitas de los usuarios y no específicamente en aprender funciones de clasificación. Otros estudios sobre el comportamiento de los usuarios en la búsqueda web incluyen a Pharo y Järvelin [19], pero no se aplicaron directamente para mejorar la clasificación. Más recientemente, Joachims et al. [12] presentaron una evaluación empírica de la interpretación de la evidencia de clics. Al realizar estudios de seguimiento ocular y correlacionar las predicciones de sus estrategias con las calificaciones explícitas, los autores demostraron que es posible interpretar con precisión los clics en un entorno controlado de laboratorio. Desafortunadamente, no está claro en qué medida la investigación previa se aplica a la búsqueda web del mundo real. Al mismo tiempo, si bien el trabajo reciente (por ejemplo, [26]) sobre el uso de la información de clics para mejorar la clasificación de búsqueda en la web es prometedor, solo abarca un aspecto de las interacciones de los usuarios con los motores de búsqueda en la web. Nos basamos en investigaciones existentes para desarrollar técnicas robustas de interpretación del comportamiento del usuario para el entorno real de búsqueda en la web. En lugar de tratar a cada usuario como un experto confiable, agregamos información de múltiples trazas de sesiones de búsqueda de usuarios no confiables, como describimos en las siguientes dos secciones. 3. INTEGRANDO LA RETROALIMENTACIÓN IMPLÍCITA Consideramos dos enfoques complementarios para la clasificación con retroalimentación implícita: (1) tratar la retroalimentación implícita como evidencia independiente para clasificar resultados, y (2) integrar características de retroalimentación implícita directamente en el algoritmo de clasificación. Describimos a continuación los dos enfoques generales de clasificación. Las características específicas del feedback implícito se describen en la Sección 4, y los algoritmos para interpretar e incorporar el feedback implícito se describen en la Sección 5. 3.1 Feedback Implícito como Evidencia Independiente. El enfoque general es reordenar los resultados obtenidos por un motor de búsqueda web según los clics observados y otras interacciones de usuario para la consulta en sesiones de búsqueda anteriores. Cada <br>resultado</br> se le asigna una puntuación según la relevancia esperada/ satisfacción del usuario basada en interacciones previas, lo que resulta en un cierto orden de preferencia basado únicamente en las interacciones del usuario. Si bien ha habido un trabajo significativo en la fusión de múltiples clasificaciones, adaptamos un enfoque simple y robusto de ignorar las puntuaciones de los clasificadores originales, y en su lugar simplemente fusionamos los órdenes de clasificación. La razón principal para ignorar las puntuaciones originales es que, dado que los espacios de características y los algoritmos de aprendizaje son diferentes, las puntuaciones no son directamente comparables, y la re-normalización tiende a eliminar el beneficio de incorporar las puntuaciones del clasificador. Experimentamos con una variedad de funciones de fusión en el conjunto de desarrollo de consultas (y utilizando un conjunto de interacciones de un período de tiempo diferente de los conjuntos de evaluación final). Encontramos que una combinación heurística simple de fusión de rangos funciona bien y es robusta a variaciones en los valores de puntuación de los clasificadores originales. Para una consulta dada q, se calcula la puntuación implícita ISd para cada <br>resultado</br> d a partir de las características de interacción del usuario disponibles, lo que resulta en la clasificación implícita Id para cada <br>resultado</br>. Calculamos un puntaje combinado SM(d) para d al combinar los rangos obtenidos de la retroalimentación implícita, Id, con el rango original de d, Od: SM(d) = Od + wI * Id, si existe retroalimentación implícita; de lo contrario, SM(d) = Od. Donde el peso wI es un factor de escala ajustado heurísticamente que representa la importancia relativa de la retroalimentación implícita. Los resultados de la consulta se ordenan en valores decrecientes de SM para producir la clasificación final. Un caso especial de este modelo surge al establecer wI en un valor muy grande, lo que efectivamente obliga a que los resultados clicados se clasifiquen por encima de los resultados no clicados, una heurística intuitiva y efectiva que utilizaremos como referencia. Aplicar algoritmos de combinación de clasificadores y ordenadores más sofisticados puede <br>resultar</br> en mejoras adicionales, y es una dirección prometedora para trabajos futuros. El enfoque anterior asume que no hay interacciones entre las características subyacentes que producen la clasificación original de la búsqueda web y las características de retroalimentación implícitas. Ahora relajamos esta suposición al integrar características de retroalimentación implícita directamente en el proceso de clasificación. 3.2 Clasificación con Características de Retroalimentación Implícita Los motores de búsqueda web modernos clasifican los resultados en función de un gran número de características, incluidas las características basadas en el contenido (es decir, qué tan cerca coincide una consulta con el texto, el título o el texto de anclaje del documento) y las características de calidad de página independientes de la consulta (por ejemplo, PageRank del documento o del dominio). En la mayoría de los casos, se desarrollan métodos automáticos (o semiautomáticos) para ajustar la función de clasificación específica que combina estos valores de características. Por lo tanto, un enfoque natural es incorporar características de retroalimentación implícita directamente como características para el algoritmo de clasificación. Durante el entrenamiento o ajuste, el clasificador puede ser ajustado como antes pero con características adicionales. En tiempo de ejecución, el motor de búsqueda recuperaría las características de retroalimentación implícita asociadas con cada par de URL de consulta y <br>resultado</br>. ",
            "candidates": [],
            "error": [
                [
                    "resultados de búsqueda",
                    "resultado",
                    "resultado",
                    "resultado",
                    "resultar",
                    "resultado"
                ]
            ]
        },
        "relevance feedback": {
            "translated_key": "retroalimentación de relevancia",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Improving Web Search Ranking by Incorporating User Behavior Information Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com ABSTRACT We show that incorporating user behavior data can significantly improve ordering of top results in real web search setting.",
                "We examine alternatives for incorporating feedback into the ranking process and explore the contributions of user feedback compared to other common web search features.",
                "We report results of a large scale evaluation over 3,000 queries and 12 million user interactions with a popular web search engine.",
                "We show that incorporating implicit feedback can augment other features, improving the accuracy of a competitive web search ranking algorithms by as much as 31% relative to the original performance.",
                "Categories and Subject Descriptors H.3.3 Information Search and Retrieval - <br>relevance feedback</br>, search process; H.3.5 Online Information Services - Web-based services.",
                "General Terms Algorithms, Measurement, Experimentation 1.",
                "INTRODUCTION Millions of users interact with search engines daily.",
                "They issue queries, follow some of the links in the results, click on ads, spend time on pages, reformulate their queries, and perform other actions.",
                "These interactions can serve as a valuable source of information for tuning and improving web search result ranking and can compliment more costly explicit judgments.",
                "Implicit <br>relevance feedback</br> for ranking and personalization has become an active area of research.",
                "Recent work by Joachims and others exploring implicit feedback in controlled environments have shown the value of incorporating implicit feedback into the ranking process.",
                "Our motivation for this work is to understand how implicit feedback can be used in a large-scale operational environment to improve retrieval.",
                "How does it compare to and compliment evidence from page content, anchor text, or link-based features such as inlinks or PageRank?",
                "While it is intuitive that user interactions with the web search engine should reveal at least some information that could be used for ranking, estimating user preferences in real web search settings is a challenging problem, since real user interactions tend to be more noisy than commonly assumed in the controlled settings of previous studies.",
                "Our paper explores whether implicit feedback can be helpful in realistic environments, where user feedback can be noisy (or adversarial) and a web search engine already uses hundreds of features and is heavily tuned.",
                "To this end, we explore different approaches for ranking web search results using real user behavior obtained as part of normal interactions with the web search engine.",
                "The specific contributions of this paper include: • Analysis of alternatives for incorporating user behavior into web search ranking (Section 3). • An application of a robust implicit feedback model derived from mining millions of user interactions with a major web search engine (Section 4). • A large scale evaluation over real user queries and search results, showing significant improvements derived from incorporating user feedback (Section 6).",
                "We summarize our findings and discuss extensions to the current work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "Most common approaches primarily focus on similarity of query and a page, as well as the overall page quality [3,4,24].",
                "However, with increasing popularity of search engines, implicit feedback (i.e., the actions users take when interacting with the search engine) can be used to improve the rankings.",
                "Implicit relevance measures have been studied by several research groups.",
                "An overview of implicit measures is compiled in Kelly and Teevan [14].",
                "This research, while developing valuable insights into implicit relevance measures, was not applied to improve the ranking of web search results in realistic settings.",
                "Closely related to our work, Joachims [11] collected implicit measures in place of explicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "Fox et al. [8] explored the relationship between implicit and explicit measures in Web search, and developed Bayesian models to correlate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "This work considered a wide range of user behaviors (e.g., dwell time, scroll time, reformulation patterns) in addition to the popular clickthrough behavior.",
                "However, the modeling effort was aimed at predicting explicit relevance judgments from implicit user actions and not specifically at learning ranking functions.",
                "Other studies of user behavior in web search include Pharo and Järvelin [19], but were not directly applied to improve ranking.",
                "More recently, Joachims et al. [12] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthroughs in a controlled, laboratory setting.",
                "Unfortunately, the extent to which previous research applies to real-world web search is unclear.",
                "At the same time, while recent work (e.g., [26]) on using clickthrough information for improving web search ranking is promising, it captures only one aspect of the user interactions with web search engines.",
                "We build on existing research to develop robust user behavior interpretation techniques for the real web search setting.",
                "Instead of treating each user as a reliable expert, we aggregate information from multiple, unreliable, user search session traces, as we describe in the next two sections. 3.",
                "INCORPORATING IMPLICIT FEEDBACK We consider two complementary approaches to ranking with implicit feedback: (1) treating implicit feedback as independent evidence for ranking results, and (2) integrating implicit feedback features directly into the ranking algorithm.",
                "We describe the two general ranking approaches next.",
                "The specific implicit feedback features are described in Section 4, and the algorithms for interpreting and incorporating implicit feedback are described in Section 5. 3.1 Implicit Feedback as Independent Evidence The general approach is to re-rank the results obtained by a web search engine according to observed clickthrough and other user interactions for the query in previous search sessions.",
                "Each result is assigned a score according to expected relevance/user satisfaction based on previous interactions, resulting in some preference ordering based on user interactions alone.",
                "While there has been significant work on merging multiple rankings, we adapt a simple and robust approach of ignoring the original rankers scores, and instead simply merge the rank orders.",
                "The main reason for ignoring the original scores is that since the feature spaces and learning algorithms are different, the scores are not directly comparable, and re-normalization tends to remove the benefit of incorporating classifier scores.",
                "We experimented with a variety of merging functions on the development set of queries (and using a set of interactions from a different time period from final evaluation sets).",
                "We found that a simple rank merging heuristic combination works well, and is robust to variations in score values from original rankers.",
                "For a given query q, the implicit score ISd is computed for each result d from available user interaction features, resulting in the implicit rank Id for each result.",
                "We compute a merged score SM(d) for d by combining the ranks obtained from implicit feedback, Id with the original rank of d, Od:     ¡     ¢ £ + + + + = otherwise O dforexistsfeedbackimplicitif OI w wOIdS d dd I IddM 1 1 1 1 1 1 ),,,( where the weight wI is a heuristically tuned scaling factor representing the relative importance of the implicit feedback.",
                "The query results are ordered in by decreasing values of SM to produce the final ranking.",
                "One special case of this model arises when setting wI to a very large value, effectively forcing clicked results to be ranked higher than un-clicked results - an intuitive and effective heuristic that we will use as a baseline.",
                "Applying more sophisticated classifier and ranker combination algorithms may result in additional improvements, and is a promising direction for future work.",
                "The approach above assumes that there are no interactions between the underlying features producing the original web search ranking and the implicit feedback features.",
                "We now relax this assumption by integrating implicit feedback features directly into the ranking process. 3.2 Ranking with Implicit Feedback Features Modern web search engines rank results based on a large number of features, including content-based features (i.e., how closely a query matches the text or title or anchor text of the document), and query-independent page quality features (e.g., PageRank of the document or the domain).",
                "In most cases, automatic (or semiautomatic) methods are developed for tuning the specific ranking function that combines these feature values.",
                "Hence, a natural approach is to incorporate implicit feedback features directly as features for the ranking algorithm.",
                "During training or tuning, the ranker can be tuned as before but with additional features.",
                "At runtime, the search engine would fetch the implicit feedback features associated with each query-result URL pair.",
                "This model requires a ranking algorithm to be robust to missing values: more than 50% of queries to web search engines are unique, with no previous implicit feedback available.",
                "We now describe such a ranker that we used to learn over the combined feature sets including implicit feedback. 3.3 Learning to Rank Web Search Results A key aspect of our approach is exploiting recent advances in machine learning, namely trainable ranking algorithms for web search and information retrieval (e.g., [5, 11] and classical results reviewed in [3]).",
                "In our setting, explicit human relevance judgments (labels) are available for a set of web search queries and results.",
                "Hence, an attractive choice to use is a supervised machine learning technique to learn a ranking function that best predicts relevance judgments.",
                "RankNet is one such algorithm.",
                "It is a neural net tuning algorithm that optimizes feature weights to best match explicitly provided pairwise user preferences.",
                "While the specific training algorithms used by RankNet are beyond the scope of this paper, it is described in detail in [5] and includes extensive evaluation and comparison with other ranking methods.",
                "An attractive feature of RankNet is both train- and run-time efficiency - runtime ranking can be quickly computed and can scale to the web, and training can be done over thousands of queries and associated judged results.",
                "We use a 2-layer implementation of RankNet in order to model non-linear relationships between features.",
                "Furthermore, RankNet can learn with many (differentiable) cost functions, and hence can automatically learn a ranking function from human-provided labels, an attractive alternative to heuristic feature combination techniques.",
                "Hence, we will also use RankNet as a generic ranker to explore the contribution of implicit feedback for different ranking alternatives. 4.",
                "IMPLICIT USER FEEDBACK MODEL Our goal is to accurately interpret noisy user feedback obtained as by tracing user interactions with the search engine.",
                "Interpreting implicit feedback in real web search setting is not an easy task.",
                "We characterize this problem in detail in [1], where we motivate and evaluate a wide variety of models of implicit user activities.",
                "The general approach is to represent user actions for each search result as a vector of features, and then train a ranker on these features to discover feature values indicative of relevant (and nonrelevant) search results.",
                "We first briefly summarize our features and model, and the learning approach (Section 4.2) in order to provide sufficient information to replicate our ranking methods and the subsequent experiments. 4.1 Representing User Actions as Features We model observed web search behaviors as a combination of a ``background component (i.e., query- and relevance-independent noise in user behavior, including positional biases with result interactions), and a ``relevance component (i.e., query-specific behavior indicative of relevance of a result to a query).",
                "We design our features to take advantage of aggregated user behavior.",
                "The feature set is comprised of directly observed features (computed directly from observations for each query), as well as queryspecific derived features, computed as the deviation from the overall query-independent distribution of values for the corresponding directly observed feature values.",
                "The features used to represent user interactions with web search results are summarized in Table 4.1.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "We include the traditional implicit feedback features such as clickthrough counts for the results, as well as our novel derived features such as the deviation of the observed clickthrough number for a given query-URL pair from the expected number of clicks on a result in the given position.",
                "We also model the browsing behavior after a result was clicked - e.g., the average page dwell time for a given query-URL pair, as well as its deviation from the expected (average) dwell time.",
                "Furthermore, the feature set was designed to provide essential information about the user experience to make feedback interpretation robust.",
                "For example, web search users can often determine whether a result is relevant by looking at the result title, URL, and summary - in many cases, looking at the original document is not necessary.",
                "To model this aspect of user experience we include features such as overlap in words in title and words in query (TitleOverlap) and the fraction of words shared by the query and the result summary.",
                "Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickProbability Probability of a click for this query and URL ClickDeviation Deviation from expected click probability IsNextClicked 1 if clicked on next position, 0 otherwise IsPreviousClicked 1 if clicked on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, no parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from average dwell time on page CumulativeDeviation Deviation from average cumulative dwell time DomainDeviation Deviation from average dwell time on domain Query-text features TitleOverlap Words shared between query and title SummaryOverlap Words shared between query and snippet QueryURLOverlap Words shared between query and URL QueryDomainOverlap Words shared between query and URL domain QueryLength Number of tokens in query QueryNextOverlap Fraction of words shared with next query Table 4.1: Some features used to represent post-search navigation history for a given query and search result URL.",
                "Having described our feature set, we briefly review our general method for deriving a user behavior model. 4.2 Deriving a User Feedback Model To learn to interpret the observed user behavior, we correlate user actions (i.e., the features in Table 4.1 representing the actions) with the explicit user judgments for a set of training queries.",
                "We find all the instances in our session logs where these queries were submitted to the search engine, and aggregate the user behavior features for all search sessions involving these queries.",
                "Each observed query-URL pair is represented by the features in Table 4.1, with values averaged over all search sessions, and assigned one of six possible relevance labels, ranging from Perfect to Bad, as assigned by explicit relevance judgments.",
                "These labeled feature vectors are used as input to the RankNet training algorithm (Section 3.3) which produces a trained user behavior model.",
                "This approach is particularly attractive as it does not require heuristics beyond feature engineering.",
                "The resulting user behavior model is used to help rank web search resultseither directly or in combination with other features, as described below. 5.",
                "EXPERIMENTAL SETUP The ultimate goal of incorporating implicit feedback into ranking is to improve the relevance of the returned web search results.",
                "Hence, we compare the ranking methods over a large set of judged queries with explicit relevance labels provided by human judges.",
                "In order for the evaluation to be realistic we obtained a random sample of queries from web search logs of a major search engine, with associated results and traces for user actions.",
                "We describe this dataset in detail next.",
                "Our metrics are described in Section 5.2 that we use to evaluate the ranking alternatives, listed in Section 5.3 in the experiments of Section 6. 5.1 Datasets We compared our ranking methods over a random sample of 3,000 queries from the search engine query logs.",
                "The queries were drawn from the logs uniformly at random by token without replacement, resulting in a query sample representative of the overall query distribution.",
                "On average, 30 results were explicitly labeled by human judges using a six point scale ranging from Perfect down to Bad.",
                "Overall, there were over 83,000 results with explicit relevance judgments.",
                "In order to compute various statistics, documents with label Good or better will be considered relevant, and with lower labels to be non-relevant.",
                "Note that the experiments were performed over the results already highly ranked by a web search engine, which corresponds to a typical user experience which is limited to the small number of the highly ranked results for a typical web search query.",
                "The user interactions were collected over a period of 8 weeks using voluntary opt-in information.",
                "In total, over 1.2 million unique queries were instrumented, resulting in over 12 million individual interactions with the search engine.",
                "The data consisted of user interactions with the web search engine (e.g., clicking on a result link, going back to search results, etc.) performed after a query was submitted.",
                "These actions were aggregated across users and search sessions and converted to features in Table 4.1.",
                "To create the training, validation, and test query sets, we created three different random splits of 1,500 training, 500 validation, and 1000 test queries.",
                "The splits were done randomly by query, so that there was no overlap in training, validation, and test queries. 5.2 Evaluation Metrics We evaluate the ranking algorithms over a range of accepted information retrieval metrics, namely Precision at K (P(K)), Normalized Discounted Cumulative Gain (NDCG), and Mean Average Precision (MAP).",
                "Each metric focuses on a deferent aspect of system performance, as we describe below. • Precision at K: As the most intuitive metric, P(K) reports the fraction of documents ranked in the top K results that are labeled as relevant.",
                "In our setting, we require a relevant document to be labeled Good or higher.",
                "The position of relevant documents within the top K is irrelevant, and hence this metric measure overall user satisfaction with the top K results. • NDCG at K: NDCG is a retrieval measure devised specifically for web search evaluation [10].",
                "For a given query q, the ranked results are examined from the top ranked down, and the NDCG computed as:   = +−= K j jr qq jMN 1 )( )1log(/)12( Where Mq is a normalization constant calculated so that a perfect ordering would obtain NDCG of 1; and each r(j) is an integer relevance label (0=Bad and 5=Perfect) of result returned at position j.",
                "Note that unlabeled and Bad documents do not contribute to the sum, but will reduce NDCG for the query pushing down the relevant labeled documents, reducing their contributions.",
                "NDCG is well suited to web search evaluation, as it rewards relevant documents in the top ranked results more heavily than those ranked lower. • MAP: Average precision for each query is defined as the mean of the precision at K values computed after each relevant document was retrieved.",
                "The final MAP value is defined as the mean of average precisions of all queries in the test set.",
                "This metric is the most commonly used single-value summary of a run over a set of queries. 5.3 Ranking Methods Compared Recall that our goal is to quantify the effectiveness of implicit behavior for real web search.",
                "One dimension is to compare the utility of implicit feedback with other information available to a web search engine.",
                "Specifically, we compare effectiveness of implicit user behaviors with content-based matching, static page quality features, and combinations of all features. • BM25F: As a strong web search baseline we used the BM25F scoring, which was used in one of the best performing systems in the TREC 2004 Web track [23,27].",
                "BM25F and its variants have been extensively described and evaluated in IR literature, and hence serve as a strong, reproducible baseline.",
                "The BM25F variant we used for our experiments computes separate match scores for each field for a result document (e.g., body text, title, and anchor text), and incorporates query-independent linkbased information (e.g., PageRank, ClickDistance, and URL depth).",
                "The scoring function and field-specific tuning is described in detail in [23].",
                "Note that BM25F does not directly consider explicit or implicit feedback for tuning. • RN: The ranking produced by a neural net ranker (RankNet, described in Section 3.3) that learns to rank web search results by incorporating BM25F and a large number of additional static and dynamic features describing each search result.",
                "This system automatically learns weights for all features (including the BM25F score for a document) based on explicit human labels for a large set of queries.",
                "A system incorporating an implementation of RankNet is currently in use by a major search engine and can be considered representative of the state of the art in web search. • BM25F-RerankCT: The ranking produced by incorporating clickthrough statistics to reorder web search results ranked by BM25F above.",
                "Clickthrough is a particularly important special case of implicit feedback, and has been shown to correlate with result relevance.",
                "This is a special case of the ranking method in Section 3.1, with the weight wI set to 1000 and the ranking Id is simply the number of clicks on the result corresponding to d. In effect, this ranking brings to the top all returned web search results with at least one click (and orders them in decreasing order by number of clicks).",
                "The relative ranking of the remainder of results is unchanged and they are inserted below all clicked results.",
                "This method serves as our baseline implicit feedback reranking method.",
                "BM25F-RerankAll The ranking produced by reordering the BM25F results using all user behavior features (Section 4).",
                "This method learns a model of user preferences by correlating feature values with explicit relevance labels using the RankNet neural net algorithm (Section 4.2).",
                "At runtime, for a given query the implicit score Ir is computed for each result r with available user interaction features, and the implicit ranking is produced.",
                "The merged ranking is computed as described in Section 3.1.",
                "Based on the experiments over the development set we fix the value of wI to 3 (the effect of the wI parameter for this ranker turned out to be negligible). • BM25F+All: Ranking derived by training the RankNet (Section 3.3) learner over the features set of the BM25F score as well as all implicit feedback features (Section 3.2).",
                "We used the 2-layer implementation of RankNet [5] trained on the queries and labels in the training and validation sets. • RN+All: Ranking derived by training the 2-layer RankNet ranking algorithm (Section 3.3) over the union of all content, dynamic, and implicit feedback features (i.e., all of the features described above as well as all of the new implicit feedback features we introduced).",
                "The ranking methods above span the range of the information used for ranking, from not using the implicit or explicit feedback at all (i.e., BM25F) to a modern web search engine using hundreds of features and tuned on explicit judgments (RN).",
                "As we will show next, incorporating user behavior into these ranking systems dramatically improves the relevance of the returned documents. 6.",
                "EXPERIMENTAL RESULTS Implicit feedback for web search ranking can be exploited in a number of ways.",
                "We compare alternative methods of exploiting implicit feedback, both by re-ranking the top results (i.e., the BM25F-RerankCT and BM25F-RerankAll methods that reorder BM25F results), as well as by integrating the implicit features directly into the ranking process (i.e., the RN+ALL and BM25F+All methods which learn to rank results over the implicit feedback and other features).",
                "We compare our methods over strong baselines (BM25F and RN) over the NDCG, Precision at K, and MAP measures defined in Section 5.2.",
                "The results were averaged over three random splits of the overall dataset.",
                "Each split contained 1500 training, 500 validation, and 1000 test queries, all query sets disjoint.",
                "We first present the results over all 1000 test queries (i.e., including queries for which there are no implicit measures so we use the original web rankings).",
                "We then drill down to examine the effects on reranking for the attempted queries in more detail, analyzing where implicit feedback proved most beneficial.",
                "We first experimented with different methods of re-ranking the output of the BM25F search results.",
                "Figures 6.1 and 6.2 report NDCG and Precision for BM25F, as well as for the strategies reranking results with user feedback (Section 3.1).",
                "Incorporating all user feedback (either in reranking framework or as features to the learner directly) results in significant improvements (using two-tailed t-test with p=0.01) over both the original BM25F ranking as well as over reranking with clickthrough alone.",
                "The improvement is consistent across the top 10 results and largest for the top result: NDCG at 1 for BM25F+All is 0.622 compared to 0.518 of the original results, and precision at 1 similarly increases from 0.5 to 0.63.",
                "Based on these results we will use the direct feature combination (i.e., BM25F+All) ranker for subsequent comparisons involving implicit feedback. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 1 2 3 4 5 6 7 8 9 10K NDCG BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.1: NDCG at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K 0.35 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.2: Precision at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K Interestingly, using clickthrough alone, while giving significant benefit over the original BM25F ranking, is not as effective as considering the full set of features in Table 4.1.",
                "While we analyze user behavior (and most effective component features) in a separate paper [1], it is worthwhile to give a concrete example of the kind of noise inherent in real user feedback in web search setting. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 Result position Relativeclickfrequency PTR=2 PTR=3 PTR=5 Figure 6.3: Relative clickthrough frequency for queries with varying Position of Top Relevant result (PTR).",
                "If users considered only the relevance of a result to their query, they would click on the topmost relevant results.",
                "Unfortunately, as Joachims and others have shown, presentation also influences which results users click on quite dramatically.",
                "Users often click on results above the relevant one presumably because the short summaries do not provide enough information to make accurate relevance assessments and they have learned that on average topranked items are relevant.",
                "Figure 6.3 shows relative clickthrough frequencies for queries with known relevant items at positions other than the first position; the position of the top relevant result (PTR) ranges from 2-10 in the figure.",
                "For example, for queries with first relevant result at position 5 (PTR=5), there are more clicks on the non-relevant results in higher ranked positions than on the first relevant result at position 5.",
                "As we will see, learning over a richer behavior feature set, results in substantial accuracy improvement over clickthrough alone.",
                "We now consider incorporating user behavior into a much richer feature set, RN (Section 5.3) used by a major web search engine.",
                "RN incorporates BM25F, link-based features, and hundreds of other features.",
                "Figure 6.4 reports NDCG at K and Figure 6.5 reports Precision at K. Interestingly, while the original RN rankings are significantly more accurate than BM25F alone, incorporating implicit feedback features (BM25F+All) results in ranking that significantly outperforms the original RN rankings.",
                "In other words, implicit feedback incorporates sufficient information to replace the hundreds of other features available to the RankNet learner trained on the RN feature set. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 1 2 3 4 5 6 7 8 9 10K NDCG RN RN+All BM25 BM25+All Figure 6.4: NDCG at K for BM25F, BM25F+All, RN, and RN+All for varying K Furthermore, enriching the RN features with implicit feedback set exhibits significant gain on all measures, allowing RN+All to outperform all other methods.",
                "This demonstrates the complementary nature of implicit feedback with other features available to a state of the art web search engine. 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision RN RN+All BM25 BM25+All Figure 6.5: Precision at K for BM25F, BM25F+All, RN, and RN+All for varying K We summarize the performance of the different ranking methods in Table 6.1.",
                "We report the Mean Average Precision (MAP) score for each system.",
                "While not intuitive to interpret, MAP allows quantitative comparison on a single metric.",
                "The gains marked with * are significant at p=0.01 level using two tailed t-test.",
                "MAP Gain P(1) Gain BM25F 0.184 - 0.503BM25F-Rerank-CT 0.215 0.031* 0.577 0.073* BM25F-RerankImplicit 0.218 0.003 0.605 0.028* BM25F+Implicit 0.222 0.004 0.620 0.015* RN 0.215 - 0.597RN+All 0.248 0.033* 0.629 0.032* Table 6.1: Mean Average Precision (MAP) for all strategies.",
                "So far we reported results averaged across all queries in the test set.",
                "Unfortunately, less than half had sufficient interactions to attempt reranking.",
                "Out of the 1000 queries in test, between 46% and 49%, depending on the train-test split, had sufficient interaction information to make predictions (i.e., there was at least 1 search session in which at least 1 result URL was clicked on by the user).",
                "This is not surprising: web search is heavy-tailed, and there are many unique queries.",
                "We now consider the performance on the queries for which user interactions were available.",
                "Figure 6.6 reports NDCG for the subset of the test queries with the implicit feedback features.",
                "The gains at top 1 are dramatic.",
                "The NDCG at 1 of BM25F+All increases from 0.6 to 0.75 (a 31% relative gain), achieving performance comparable to RN+All operating over a much richer feature set. 0.6 0.65 0.7 0.75 0.8 1 3 5 10K NDCG RN RN+All BM25 BM25+All Figure 6.6: NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions Similarly, gains on precision at top 1 are substantial (Figure 6.7), and are likely to be apparent to web search users.",
                "When implicit feedback is available, the BM25F+All system returns relevant document at top 1 almost 70% of the time, compared 53% of the time when implicit feedback is not considered by the original BM25F system. 0.45 0.5 0.55 0.6 0.65 0.7 1 3 5 10K Precision RN RN+All BM25 BM25+All Figure 6.7: Precision at K NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions We summarize the results on the MAP measure for attempted queries in Table 6.2.",
                "MAP improvements are both substantial and significant, with improvements over the BM25F ranker most pronounced.",
                "Method MAP Gain P(1) Gain RN 0.269 0.632 RN+All 0.321 0.051 (19%) 0.693 0.061(10%) BM25F 0.236 0.525 BM25F+All 0.292 0.056 (24%) 0.687 0.162 (31%) Table 6.2: Mean Average Precision (MAP) on attempted queries for best performing methods We now analyze the cases where implicit feedback was shown most helpful.",
                "Figure 6.8 reports the MAP improvements over the baseline BM25F run for each query with MAP under 0.6.",
                "Note that most of the improvement is for poorly performing queries (i.e., MAP < 0.1).",
                "Interestingly, incorporating user behavior information degrades accuracy for queries with high original MAP score.",
                "One possible explanation is that these easy queries tend to be navigational (i.e., having a single, highly-ranked most appropriate answer), and user interactions with lower-ranked results may indicate divergent information needs that are better served by the less popular results (with correspondingly poor overall relevance ratings). 0 50 100 150 200 250 300 350 0.1 0.2 0.3 0.4 0.5 0.6 -0.4 -0.35 -0.3 -0.25 -0.2 -0.15 -0.1 -0.05 0 0.05 0.1 0.15 0.2 Frequency Average Gain Figure 6.8: Gain of BM25F+All over original BM25F ranking To summarize our experimental results, incorporating implicit feedback in real web search setting resulted in significant improvements over the original rankings, using both BM25F and RN baselines.",
                "Our rich set of implicit features, such as time on page and deviations from the average behavior, provides advantages over using clickthrough alone as an indicator of interest.",
                "Furthermore, incorporating implicit feedback features directly into the learned ranking function is more effective than using implicit feedback for reranking.",
                "The improvements observed over large test sets of queries (1,000 total, between 466 and 495 with implicit feedback available) are both substantial and statistically significant. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper we explored the utility of incorporating noisy implicit feedback obtained in a real web search setting to improve web search ranking.",
                "We performed a large-scale evaluation over 3,000 queries and more than 12 million user interactions with a major search engine, establishing the utility of incorporating noisy implicit feedback to improve web search relevance.",
                "We compared two alternatives of incorporating implicit feedback into the search process, namely reranking with implicit feedback and incorporating implicit feedback features directly into the trained ranking function.",
                "Our experiments showed significant improvement over methods that do not consider implicit feedback.",
                "The gains are particularly dramatic for the top K=1 result in the final ranking, with precision improvements as high as 31%, and the gains are substantial for all values of K. Our experiments showed that implicit user feedback can further improve web search performance, when incorporated directly with popular content- and link-based features.",
                "Interestingly, implicit feedback is particularly valuable for queries with poor original ranking of results (e.g., MAP lower than 0.1).",
                "One promising direction for future work is to apply recent research on automatically predicting query difficulty, and only attempt to incorporate implicit feedback for the difficult queries.",
                "As another research direction we are exploring methods for extending our predictions to the previously unseen queries (e.g., query clustering), which should further improve the web search experience of users.",
                "ACKNOWLEDGMENTS We thank Chris Burges and Matt Richardson for an implementation of RankNet for our experiments.",
                "We also thank Robert Ragno for his valuable suggestions and many discussions. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, S. Dumais, and R.Ragno, Learning User Interaction Models for Predicting Web Search Result Preferences.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan, HARD Track Overview in TREC 2003, High Accuracy Retrieval from Documents, 2003 [3] R. Baeza-Yates and B. Ribeiro-Neto, Modern Information Retrieval, Addison-Wesley, 1999. [4] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine, in Proceedings of WWW, 1997 [5] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning, 2005 [6] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [7] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest.",
                "IEEE Internet Computing. 2001 [8] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [9] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [10] K Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2000 [11] T. Joachims, Optimizing Search Engines Using Clickthrough Data.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [12] T. Joachims, L. Granka, B. Pang, H. Hembrooke, and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [13] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [14] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [15] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [16] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [17] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems.",
                "In Proceedings of the AAAI Workshop on Recommender Systems. 1998 [18] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [19] N. Pharo, N. and K. Järvelin.",
                "The SST method: a tool for analyzing web information search processes.",
                "In Information Processing & Management, 2004 [20] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [21] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (SIGKDD), 2005. [22] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in Proceedings of the ICML Workshop on Learning in Web Search, 2005 [23] S. E. Robertson, H. Zaragoza, and M. Taylor, Simple BM25 extension to multiple weighted fields, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [24] G. Salton & M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [25] E.M. Voorhees, D. Harman, Overview of TREC, 2001 [26] G.R.",
                "Xue, H.J.",
                "Zeng, Z. Chen, Y. Yu, W.Y.",
                "Ma, W.S.",
                "Xi, and W.G.",
                "Fan, Optimizing web search using web clickthrough data, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [27] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
                "Microsoft Cambridge at TREC 13: Web and Hard Tracks.",
                "In Proceedings of TREC 2004"
            ],
            "original_annotated_samples": [
                "Categories and Subject Descriptors H.3.3 Information Search and Retrieval - <br>relevance feedback</br>, search process; H.3.5 Online Information Services - Web-based services.",
                "Implicit <br>relevance feedback</br> for ranking and personalization has become an active area of research."
            ],
            "translated_annotated_samples": [
                "Categorías y Descriptores de Asignaturas H.3.3 Búsqueda y Recuperación de Información - Retroalimentación de relevancia, proceso de búsqueda; H.3.5 Servicios de Información en Línea - Servicios basados en la web.",
                "La retroalimentación implícita de relevancia para la clasificación y personalización se ha convertido en un área activa de investigación."
            ],
            "translated_text": "Mejorando la clasificación de búsqueda web al incorporar información sobre el comportamiento del usuario. Eugene Agichtein, Microsoft Research eugeneag@microsoft.com Eric Brill, Microsoft Research brill@microsoft.com Susan Dumais, Microsoft Research sdumais@microsoft.com RESUMEN Mostramos que la incorporación de datos sobre el comportamiento del usuario puede mejorar significativamente el orden de los principales resultados en una configuración real de búsqueda web. Examinamos alternativas para incorporar retroalimentación en el proceso de clasificación y exploramos las contribuciones de la retroalimentación de usuarios en comparación con otras características comunes de búsqueda en la web. Informamos los resultados de una evaluación a gran escala con más de 3,000 consultas y 12 millones de interacciones de usuarios con un motor de búsqueda web popular. Mostramos que la incorporación de retroalimentación implícita puede aumentar otras características, mejorando la precisión de los algoritmos de clasificación de búsqueda web competitivos hasta en un 31% en comparación con el rendimiento original. Categorías y Descriptores de Asignaturas H.3.3 Búsqueda y Recuperación de Información - Retroalimentación de relevancia, proceso de búsqueda; H.3.5 Servicios de Información en Línea - Servicios basados en la web. Términos generales Algoritmos, Medición, Experimentación 1. INTRODUCCIÓN Millones de usuarios interactúan con los motores de búsqueda a diario. Ellos emiten consultas, siguen algunos de los enlaces en los resultados, hacen clic en anuncios, pasan tiempo en páginas, reformulan sus consultas y realizan otras acciones. Estas interacciones pueden servir como una valiosa fuente de información para ajustar y mejorar la clasificación de los resultados de búsqueda en la web y pueden complementar juicios explícitos más costosos. La retroalimentación implícita de relevancia para la clasificación y personalización se ha convertido en un área activa de investigación. El trabajo reciente de Joachims y otros explorando el feedback implícito en entornos controlados ha demostrado el valor de incorporar el feedback implícito en el proceso de clasificación. Nuestra motivación para este trabajo es comprender cómo se puede utilizar la retroalimentación implícita en un entorno operativo a gran escala para mejorar la recuperación. ¿Cómo se compara y complementa con la evidencia del contenido de la página, el texto del ancla o las características basadas en enlaces como inlinks o PageRank? Si bien es intuitivo que las interacciones de los usuarios con el motor de búsqueda web deberían revelar al menos alguna información que podría ser utilizada para la clasificación, estimar las preferencias de los usuarios en entornos reales de búsqueda web es un problema desafiante, ya que las interacciones reales tienden a ser más ruidosas de lo que comúnmente se asume en los entornos controlados de estudios previos. Nuestro artículo explora si la retroalimentación implícita puede ser útil en entornos realistas, donde la retroalimentación del usuario puede ser ruidosa (o adversa) y un motor de búsqueda web ya utiliza cientos de características y está altamente ajustado. Con este fin, exploramos diferentes enfoques para clasificar los resultados de búsqueda web utilizando el comportamiento real de los usuarios obtenido como parte de las interacciones normales con el motor de búsqueda web. Las contribuciones específicas de este artículo incluyen: • Análisis de alternativas para incorporar el comportamiento del usuario en la clasificación de búsqueda web (Sección 3). • Aplicación de un modelo robusto de retroalimentación implícita derivado de la minería de millones de interacciones de usuarios con un importante motor de búsqueda web (Sección 4). • Una evaluación a gran escala sobre consultas reales de usuarios y resultados de búsqueda, mostrando mejoras significativas derivadas de la incorporación de la retroalimentación del usuario (Sección 6). Resumimos nuestros hallazgos y discutimos extensiones al trabajo actual en la Sección 7, que concluye el artículo. ANTECEDENTES Y TRABAJO RELACIONADO Clasificar los resultados de búsqueda es un problema fundamental en la recuperación de información. La mayoría de los enfoques comunes se centran principalmente en la similitud entre la consulta y una página, así como en la calidad general de la página [3,4,24]. Sin embargo, con la creciente popularidad de los motores de búsqueda, la retroalimentación implícita (es decir, las acciones que los usuarios realizan al interactuar con el motor de búsqueda) se puede utilizar para mejorar las clasificaciones. Las medidas de relevancia implícitas han sido estudiadas por varios grupos de investigación. Un resumen de las medidas implícitas se recopila en Kelly y Teevan [14]. Esta investigación, si bien desarrolló valiosas ideas sobre medidas implícitas de relevancia, no se aplicó para mejorar la clasificación de los resultados de búsqueda en la web en entornos realistas. Estrechamente relacionado con nuestro trabajo, Joachims [11] recopiló medidas implícitas en lugar de medidas explícitas, introduciendo una técnica basada completamente en datos de clics para aprender funciones de clasificación. Fox et al. [8] exploraron la relación entre medidas implícitas y explícitas en la búsqueda web, y desarrollaron modelos bayesianos para correlacionar medidas implícitas y juicios explícitos de relevancia tanto para consultas individuales como para sesiones de búsqueda. Este trabajo consideró una amplia gama de comportamientos de usuario (por ejemplo, tiempo de permanencia, tiempo de desplazamiento, patrones de reformulación) además del popular comportamiento de clics. Sin embargo, el esfuerzo de modelado se centró en predecir juicios de relevancia explícitos a partir de acciones implícitas de los usuarios y no específicamente en aprender funciones de clasificación. Otros estudios sobre el comportamiento de los usuarios en la búsqueda web incluyen a Pharo y Järvelin [19], pero no se aplicaron directamente para mejorar la clasificación. Más recientemente, Joachims et al. [12] presentaron una evaluación empírica de la interpretación de la evidencia de clics. Al realizar estudios de seguimiento ocular y correlacionar las predicciones de sus estrategias con las calificaciones explícitas, los autores demostraron que es posible interpretar con precisión los clics en un entorno controlado de laboratorio. Desafortunadamente, no está claro en qué medida la investigación previa se aplica a la búsqueda web del mundo real. Al mismo tiempo, si bien el trabajo reciente (por ejemplo, [26]) sobre el uso de la información de clics para mejorar la clasificación de búsqueda en la web es prometedor, solo abarca un aspecto de las interacciones de los usuarios con los motores de búsqueda en la web. Nos basamos en investigaciones existentes para desarrollar técnicas robustas de interpretación del comportamiento del usuario para el entorno real de búsqueda en la web. En lugar de tratar a cada usuario como un experto confiable, agregamos información de múltiples trazas de sesiones de búsqueda de usuarios no confiables, como describimos en las siguientes dos secciones. 3. INTEGRANDO LA RETROALIMENTACIÓN IMPLÍCITA Consideramos dos enfoques complementarios para la clasificación con retroalimentación implícita: (1) tratar la retroalimentación implícita como evidencia independiente para clasificar resultados, y (2) integrar características de retroalimentación implícita directamente en el algoritmo de clasificación. Describimos a continuación los dos enfoques generales de clasificación. Las características específicas del feedback implícito se describen en la Sección 4, y los algoritmos para interpretar e incorporar el feedback implícito se describen en la Sección 5. 3.1 Feedback Implícito como Evidencia Independiente. El enfoque general es reordenar los resultados obtenidos por un motor de búsqueda web según los clics observados y otras interacciones de usuario para la consulta en sesiones de búsqueda anteriores. Cada resultado se le asigna una puntuación según la relevancia esperada/ satisfacción del usuario basada en interacciones previas, lo que resulta en un cierto orden de preferencia basado únicamente en las interacciones del usuario. Si bien ha habido un trabajo significativo en la fusión de múltiples clasificaciones, adaptamos un enfoque simple y robusto de ignorar las puntuaciones de los clasificadores originales, y en su lugar simplemente fusionamos los órdenes de clasificación. La razón principal para ignorar las puntuaciones originales es que, dado que los espacios de características y los algoritmos de aprendizaje son diferentes, las puntuaciones no son directamente comparables, y la re-normalización tiende a eliminar el beneficio de incorporar las puntuaciones del clasificador. Experimentamos con una variedad de funciones de fusión en el conjunto de desarrollo de consultas (y utilizando un conjunto de interacciones de un período de tiempo diferente de los conjuntos de evaluación final). Encontramos que una combinación heurística simple de fusión de rangos funciona bien y es robusta a variaciones en los valores de puntuación de los clasificadores originales. Para una consulta dada q, se calcula la puntuación implícita ISd para cada resultado d a partir de las características de interacción del usuario disponibles, lo que resulta en la clasificación implícita Id para cada resultado. Calculamos un puntaje combinado SM(d) para d al combinar los rangos obtenidos de la retroalimentación implícita, Id, con el rango original de d, Od: SM(d) = Od + wI * Id, si existe retroalimentación implícita; de lo contrario, SM(d) = Od. Donde el peso wI es un factor de escala ajustado heurísticamente que representa la importancia relativa de la retroalimentación implícita. Los resultados de la consulta se ordenan en valores decrecientes de SM para producir la clasificación final. Un caso especial de este modelo surge al establecer wI en un valor muy grande, lo que efectivamente obliga a que los resultados clicados se clasifiquen por encima de los resultados no clicados, una heurística intuitiva y efectiva que utilizaremos como referencia. Aplicar algoritmos de combinación de clasificadores y ordenadores más sofisticados puede resultar en mejoras adicionales, y es una dirección prometedora para trabajos futuros. El enfoque anterior asume que no hay interacciones entre las características subyacentes que producen la clasificación original de la búsqueda web y las características de retroalimentación implícitas. Ahora relajamos esta suposición al integrar características de retroalimentación implícita directamente en el proceso de clasificación. 3.2 Clasificación con Características de Retroalimentación Implícita Los motores de búsqueda web modernos clasifican los resultados en función de un gran número de características, incluidas las características basadas en el contenido (es decir, qué tan cerca coincide una consulta con el texto, el título o el texto de anclaje del documento) y las características de calidad de página independientes de la consulta (por ejemplo, PageRank del documento o del dominio). En la mayoría de los casos, se desarrollan métodos automáticos (o semiautomáticos) para ajustar la función de clasificación específica que combina estos valores de características. Por lo tanto, un enfoque natural es incorporar características de retroalimentación implícita directamente como características para el algoritmo de clasificación. Durante el entrenamiento o ajuste, el clasificador puede ser ajustado como antes pero con características adicionales. En tiempo de ejecución, el motor de búsqueda recuperaría las características de retroalimentación implícita asociadas con cada par de URL de consulta y resultado. Este modelo requiere que un algoritmo de clasificación sea robusto ante valores faltantes: más del 50% de las consultas a los motores de búsqueda web son únicas, sin disponer de retroalimentación implícita previa. Ahora describimos un clasificador que utilizamos para aprender sobre los conjuntos de características combinadas, incluyendo la retroalimentación implícita. 3.3 Aprendizaje para Clasificar los Resultados de Búsqueda en la Web Un aspecto clave de nuestro enfoque es aprovechar los avances recientes en el aprendizaje automático, en particular los algoritmos de clasificación entrenables para la búsqueda en la web y la recuperación de información (por ejemplo, [5, 11] y resultados clásicos revisados en [3]). En nuestro entorno, contamos con juicios explícitos de relevancia humana (etiquetas) para un conjunto de consultas de búsqueda en la web y resultados. Por lo tanto, una opción atractiva es utilizar una técnica de aprendizaje automático supervisado para aprender una función de clasificación que prediga mejor las evaluaciones de relevancia. RankNet es uno de esos algoritmos. Es un algoritmo de ajuste de red neuronal que optimiza los pesos de las características para que coincidan mejor con las preferencias de los usuarios proporcionadas explícitamente en pares. Si bien los algoritmos de entrenamiento específicos utilizados por RankNet están más allá del alcance de este documento, se describen en detalle en [5] e incluyen una evaluación exhaustiva y comparación con otros métodos de clasificación. Una característica atractiva de RankNet es su eficiencia tanto en el tiempo de entrenamiento como en el de ejecución: el ranking en tiempo de ejecución se puede calcular rápidamente y puede escalarse a la web, y el entrenamiento se puede realizar sobre miles de consultas y resultados juzgados asociados. Utilizamos una implementación de RankNet de 2 capas para modelar relaciones no lineales entre características. Además, RankNet puede aprender con muchas funciones de costo (diferenciables), por lo que puede aprender automáticamente una función de clasificación a partir de etiquetas proporcionadas por humanos, una alternativa atractiva a las técnicas de combinación de características heurísticas. Por lo tanto, también utilizaremos RankNet como un clasificador genérico para explorar la contribución de la retroalimentación implícita en diferentes alternativas de clasificación. 4. MODELO IMPLÍCITO DE RETROALIMENTACIÓN DEL USUARIO Nuestro objetivo es interpretar con precisión la retroalimentación ruidosa del usuario obtenida al rastrear las interacciones del usuario con el motor de búsqueda. Interpretar la retroalimentación implícita en un entorno real de búsqueda en la web no es una tarea fácil. Caracterizamos este problema en detalle en [1], donde motivamos y evaluamos una amplia variedad de modelos de actividades implícitas de usuario. El enfoque general es representar las acciones del usuario para cada resultado de búsqueda como un vector de características, y luego entrenar un clasificador en base a estas características para descubrir los valores de las características que indican resultados de búsqueda relevantes (y no relevantes). Primero resumimos brevemente nuestras características y modelo, y el enfoque de aprendizaje (Sección 4.2) para proporcionar suficiente información para replicar nuestros métodos de clasificación y los experimentos subsiguientes. 4.1 Representación de las acciones del usuario como características. Modelamos los comportamientos observados en la búsqueda web como una combinación de un \"componente de fondo (es decir, ruido independiente de la consulta y relevancia en el comportamiento del usuario, incluidos sesgos posicionales con interacciones de resultados), y un \"componente de relevancia (es decir, comportamiento específico de la consulta indicativo de la relevancia de un resultado para una consulta). Diseñamos nuestras características para aprovechar el comportamiento de usuario agregado. El conjunto de características está compuesto por características observadas directamente (calculadas directamente a partir de observaciones para cada consulta), así como características derivadas específicas de la consulta, calculadas como la desviación de la distribución general de valores independientes de la consulta para los valores de las características observadas directamente correspondientes. Las características utilizadas para representar las interacciones de los usuarios con los resultados de búsqueda en la web se resumen en la Tabla 4.1. Esta información se obtuvo a través de la instrumentación del lado del cliente opt-in de usuarios de un importante motor de búsqueda web. Incluimos las características tradicionales de retroalimentación implícita, como el recuento de clics en los resultados, así como nuestras características derivadas novedosas, como la desviación del número de clics observados para un par de consulta-URL dado del número esperado de clics en un resultado en la posición dada. También modelamos el comportamiento de navegación después de que se hace clic en un resultado, por ejemplo, el tiempo promedio de permanencia en la página para un par de consulta-URL dado, así como su desviación del tiempo de permanencia esperado (promedio). Además, el conjunto de características fue diseñado para proporcionar información esencial sobre la experiencia del usuario y hacer que la interpretación de la retroalimentación sea sólida. Por ejemplo, los usuarios de búsqueda en la web a menudo pueden determinar si un resultado es relevante al mirar el título del resultado, la URL y el resumen; en muchos casos, no es necesario mirar el documento original. Para modelar este aspecto de la experiencia del usuario, incluimos características como la superposición de palabras en el título y las palabras en la consulta (TitleOverlap) y la fracción de palabras compartidas por la consulta y el resumen del resultado. Características de clics Posición Posición de la URL en el ranking actual Frecuencia de clics Número de clics para esta consulta, par URL Probabilidad de clic Probabilidad de un clic para esta consulta y URL Desviación de clic Desviación de la probabilidad de clic esperada ¿Es el siguiente clic? 1 si se hizo clic en la siguiente posición, 0 de lo contrario ¿Es el clic anterior? 1 si se hizo clic en la posición anterior, 0 de lo contrario ¿Hay clic arriba? 1 si hay un clic arriba, 0 de lo contrario ¿Hay clic abajo? 1 si hay un clic abajo, 0 de lo contrario Características de navegación Tiempo en la página Tiempo de permanencia en la página Tiempo acumulado en la página Tiempo acumulado para todas las páginas siguientes después de la búsqueda Tiempo en el dominio Tiempo de permanencia acumulado para este dominio Tiempo en URL corta Tiempo acumulado en el prefijo de la URL, sin parámetros ¿Se siguió el enlace? 1 si se siguió el enlace al resultado, 0 de lo contrario ¿Coincidencia exacta de URL? 0 si se utilizó normalización agresiva, 1 de lo contrario ¿Redirigido? 1 si la URL inicial es la misma que la URL final, 0 de lo contrario ¿Camino desde la búsqueda? 1 si solo se siguieron enlaces después de la consulta, 0 de lo contrario Clics desde la búsqueda Número de saltos para llegar a la página desde la consulta Tiempo promedio de permanencia Tiempo promedio en la página para esta consulta Desviación del tiempo de permanencia Desviación del tiempo de permanencia promedio en la página Desviación acumulada Desviación del tiempo de permanencia acumulado promedio Desviación del dominio Desviación del tiempo de permanencia promedio en el dominio Características de texto de consulta Coincidencia de título Palabras compartidas entre la consulta y el título Coincidencia de resumen Palabras compartidas entre la consulta y el fragmento Coincidencia de URL de consulta Palabras compartidas entre la consulta y la URL Coincidencia de dominio de consulta Palabras compartidas entre la consulta y el dominio de la URL Longitud de la consulta Número de tokens en la consulta Superposición con la siguiente consulta Fracción de palabras compartidas con la siguiente consulta Tabla 4.1: Algunas características utilizadas para representar el historial de navegación posterior a la búsqueda para una consulta dada y una URL de resultado de búsqueda. Habiendo descrito nuestro conjunto de características, revisamos brevemente nuestro método general para derivar un modelo de comportamiento del usuario. 4.2 Derivación de un Modelo de Retroalimentación del Usuario Para aprender a interpretar el comportamiento observado del usuario, correlacionamos las acciones del usuario (es decir, las características en la Tabla 4.1 que representan las acciones) con los juicios explícitos del usuario para un conjunto de consultas de entrenamiento. Encontramos todas las instancias en nuestros registros de sesión donde se enviaron estas consultas al motor de búsqueda, y agregamos las características del comportamiento del usuario para todas las sesiones de búsqueda que involucran estas consultas. Cada par de consulta-URL observado está representado por las características en la Tabla 4.1, con valores promediados en todas las sesiones de búsqueda, y asignado una de las seis etiquetas de relevancia posibles, que van desde Perfecto hasta Malo, según los juicios explícitos de relevancia. Estos vectores de características etiquetados se utilizan como entrada para el algoritmo de entrenamiento RankNet (Sección 3.3), el cual produce un modelo de comportamiento de usuario entrenado. Este enfoque es particularmente atractivo ya que no requiere heurísticas más allá de la ingeniería de características. El modelo de comportamiento del usuario resultante se utiliza para ayudar a clasificar los resultados de búsqueda en la web, ya sea directamente o en combinación con otras características, como se describe a continuación. 5. CONFIGURACIÓN EXPERIMENTAL El objetivo final de incorporar retroalimentación implícita en la clasificación es mejorar la relevancia de los resultados de búsqueda web devueltos. Por lo tanto, comparamos los métodos de clasificación sobre un gran conjunto de consultas evaluadas con etiquetas de relevancia explícitas proporcionadas por jueces humanos. Para que la evaluación sea realista, obtuvimos una muestra aleatoria de consultas de registros de búsqueda web de un motor de búsqueda importante, con resultados asociados y rastros de acciones de usuario. Describimos este conjunto de datos en detalle a continuación. Nuestros indicadores se describen en la Sección 5.2 que utilizamos para evaluar las alternativas de clasificación, enumeradas en la Sección 5.3 en los experimentos de la Sección 6. 5.1 Conjuntos de datos Comparamos nuestros métodos de clasificación en una muestra aleatoria de 3,000 consultas de los registros de consultas del motor de búsqueda. Las consultas fueron extraídas de los registros de manera uniforme al azar por token sin reemplazo, lo que resultó en una muestra de consultas representativa de la distribución general de consultas. En promedio, 30 resultados fueron etiquetados explícitamente por jueces humanos utilizando una escala de seis puntos que va desde Perfecto hasta Malo. En total, hubo más de 83,000 resultados con juicios de relevancia explícitos. Para calcular diversas estadísticas, se considerarán relevantes los documentos con la etiqueta Buena o mejor, y los documentos con etiquetas inferiores se considerarán no relevantes. Se debe tener en cuenta que los experimentos se realizaron sobre los resultados ya altamente clasificados por un motor de búsqueda web, lo cual corresponde a una experiencia de usuario típica que se limita al pequeño número de resultados altamente clasificados para una consulta típica de búsqueda web. Las interacciones de los usuarios fueron recopiladas durante un período de 8 semanas utilizando información voluntaria de participación. En total, se registraron más de 1.2 millones de consultas únicas, lo que resultó en más de 12 millones de interacciones individuales con el motor de búsqueda. Los datos consistían en las interacciones de los usuarios con el motor de búsqueda web (por ejemplo, hacer clic en un enlace de resultado, regresar a los resultados de búsqueda, etc.) realizadas después de enviar una consulta. Estas acciones fueron agregadas entre usuarios y sesiones de búsqueda y convertidas en características en la Tabla 4.1. Para crear los conjuntos de consultas de entrenamiento, validación y prueba, creamos tres divisiones aleatorias diferentes de 1,500 consultas de entrenamiento, 500 de validación y 1000 de prueba. Las divisiones se realizaron aleatoriamente por consulta, de modo que no hubiera superposición en las consultas de entrenamiento, validación y prueba. 5.2 Métricas de Evaluación Evaluamos los algoritmos de clasificación en una variedad de métricas de recuperación de información aceptadas, a saber, Precisión en K (P(K)), Ganancia Acumulada Descontada Normalizada (NDCG) y Precisión Promedio Media (MAP). Cada métrica se enfoca en un aspecto diferente del rendimiento del sistema, como describimos a continuación. • Precisión en K: Como la métrica más intuitiva, P(K) informa la fracción de documentos clasificados en los primeros K resultados que están etiquetados como relevantes. En nuestro entorno, requerimos que un documento relevante sea etiquetado como Bueno o superior. La posición de los documentos relevantes dentro de los primeros K no es relevante, por lo que esta métrica mide la satisfacción general del usuario con los resultados principales de K. • NDCG en K: NDCG es una medida de recuperación diseñada específicamente para la evaluación de búsqueda en la web [10]. Para una consulta dada q, los resultados clasificados se examinan desde el mejor clasificado hacia abajo, y el NDCG se calcula como: = +−= K j jr qq jMN 1 )( )1log(/)12( Donde Mq es una constante de normalización calculada de manera que un orden perfecto obtendría un NDCG de 1; y cada r(j) es una etiqueta de relevancia entera (0=Malo y 5=Perfecto) del resultado devuelto en la posición j. Ten en cuenta que los documentos sin etiquetar y los documentos malos no contribuyen a la suma, pero reducirán el NDCG para la consulta al empujar hacia abajo los documentos etiquetados relevantes, disminuyendo sus contribuciones. NDCG es muy adecuado para la evaluación de búsquedas en la web, ya que recompensa de manera más intensa los documentos relevantes en los resultados mejor clasificados que aquellos clasificados más bajos. • MAP: La precisión promedio para cada consulta se define como la media de la precisión en los valores de K calculados después de que se recuperó cada documento relevante. El valor MAP final se define como la media de las precisiones promedio de todas las consultas en el conjunto de pruebas. Esta métrica es el resumen de un solo valor más comúnmente utilizado de una ejecución sobre un conjunto de consultas. 5.3 Métodos de clasificación comparados. Recuerde que nuestro objetivo es cuantificar la efectividad del comportamiento implícito para la búsqueda web real. Una dimensión es comparar la utilidad de la retroalimentación implícita con otra información disponible para un motor de búsqueda web. Específicamente, comparamos la efectividad de los comportamientos implícitos de los usuarios con la coincidencia basada en el contenido, las características de calidad de la página estática y combinaciones de todas las características. • BM25F: Como referencia sólida de búsqueda web, utilizamos la puntuación BM25F, que se utilizó en uno de los sistemas con mejor rendimiento en la pista web TREC 2004 [23,27]. BM25F y sus variantes han sido extensamente descritas y evaluadas en la literatura de IR, por lo tanto, sirven como una línea base sólida y reproducible. La variante BM25F que utilizamos en nuestros experimentos calcula puntuaciones de coincidencia separadas para cada campo de un documento de resultado (por ejemplo, texto del cuerpo, título y texto del enlace) e incorpora información de enlaces independiente de la consulta (por ejemplo, PageRank, ClickDistance y profundidad de URL). La función de puntuación y la ajuste específico del campo se describen en detalle en [23]. Tenga en cuenta que BM25F no considera directamente la retroalimentación explícita o implícita para ajuste. • RN: La clasificación producida por un clasificador de redes neuronales (RankNet, descrito en la Sección 3.3) que aprende a clasificar los resultados de búsqueda web al incorporar BM25F y un gran número de características estáticas y dinámicas adicionales que describen cada resultado de búsqueda. Este sistema aprende automáticamente los pesos de todas las características (incluido el puntaje BM25F para un documento) basándose en etiquetas humanas explícitas para un gran conjunto de consultas. Un sistema que incorpora una implementación de RankNet actualmente está siendo utilizado por un importante motor de búsqueda y puede considerarse representativo del estado del arte en la búsqueda web. • BM25F-RerankCT: La clasificación producida al incorporar estadísticas de clics para reordenar los resultados de búsqueda web clasificados por BM25F arriba. El clic es un caso especial particularmente importante de retroalimentación implícita, y se ha demostrado que se correlaciona con la relevancia de los resultados. Este es un caso especial del método de clasificación en la Sección 3.1, con el peso wI establecido en 1000 y la clasificación Id es simplemente el número de clics en el resultado correspondiente a d. En efecto, esta clasificación coloca en la parte superior todos los resultados de búsqueda web devueltos con al menos un clic (y los ordena en orden decreciente por número de clics). La clasificación relativa de los resultados restantes no cambia y se insertan debajo de todos los resultados clicados. Este método sirve como nuestro método de reordenamiento de retroalimentación implícita de referencia. La clasificación producida al reordenar los resultados de BM25F utilizando todas las características del comportamiento del usuario (Sección 4). Este método aprende un modelo de preferencias de usuario correlacionando los valores de las características con etiquetas de relevancia explícitas utilizando el algoritmo de red neuronal RankNet (Sección 4.2). En tiempo de ejecución, para una consulta dada se calcula la puntuación implícita Ir para cada resultado r con características de interacción de usuario disponibles, y se produce el ranking implícito. La clasificación combinada se calcula como se describe en la Sección 3.1. Basándonos en los experimentos realizados sobre el conjunto de desarrollo, fijamos el valor de wI en 3 (el efecto del parámetro wI para este clasificador resultó ser insignificante). • BM25F+All: Clasificación derivada al entrenar el aprendiz RankNet (Sección 3.3) sobre el conjunto de características del puntaje BM25F, así como todas las características de retroalimentación implícita (Sección 3.2). Utilizamos la implementación de 2 capas de RankNet [5] entrenada en las consultas y etiquetas de los conjuntos de entrenamiento y validación. • RN+All: Clasificación derivada al entrenar el algoritmo de clasificación RankNet de 2 capas (Sección 3.3) sobre la unión de todas las características de retroalimentación de contenido, dinámicas e implícitas (es decir, todas las características descritas anteriormente, así como todas las nuevas características de retroalimentación implícita que introdujimos). Los métodos de clasificación anteriores abarcan el rango de la información utilizada para clasificar, desde no utilizar la retroalimentación implícita o explícita en absoluto (es decir, BM25F) hasta un motor de búsqueda web moderno que utiliza cientos de características y está ajustado en base a juicios explícitos (RN). Como mostraremos a continuación, incorporar el comportamiento del usuario en estos sistemas de clasificación mejora drásticamente la relevancia de los documentos devueltos. 6. RESULTADOS EXPERIMENTALES El feedback implícito para la clasificación de búsquedas en la web puede ser explotado de diversas formas. Comparamos métodos alternativos para explotar la retroalimentación implícita, tanto reordenando los resultados principales (es decir, los métodos BM25F-RerankCT y BM25F-RerankAll que reordenan los resultados de BM25F), como integrando directamente las características implícitas en el proceso de clasificación (es decir, los métodos RN+ALL y BM25F+All que aprenden a clasificar los resultados sobre la retroalimentación implícita y otras características). Comparamos nuestros métodos con baselines sólidos (BM25F y RN) sobre las medidas NDCG, Precisión en K y MAP definidas en la Sección 5.2. Los resultados se promediaron en tres divisiones aleatorias del conjunto de datos completo. Cada división contenía 1500 consultas de entrenamiento, 500 de validación y 1000 de prueba, todos los conjuntos de consultas disjuntos. Primero presentamos los resultados de las 1000 consultas de prueba (es decir, incluyendo las consultas para las cuales no hay medidas implícitas, por lo que utilizamos las clasificaciones web originales). Luego profundizamos para examinar los efectos en la reorganización de los intentos de búsqueda con más detalle, analizando dónde el feedback implícito resultó más beneficioso. Primero experimentamos con diferentes métodos de volver a clasificar la salida de los resultados de búsqueda de BM25F. Las figuras 6.1 y 6.2 informan sobre NDCG y Precisión para BM25F, así como para las estrategias de volver a clasificar los resultados con retroalimentación del usuario (Sección 3.1). Incorporar todos los comentarios de los usuarios (ya sea en el marco de reordenamiento o como características directamente al aprendiz) resulta en mejoras significativas (utilizando una prueba t de dos colas con p=0.01) tanto sobre la clasificación original de BM25F como sobre el reordenamiento solo con clics. La mejora es consistente en los 10 mejores resultados y es mayor para el mejor resultado: NDCG en 1 para BM25F+All es de 0.622 en comparación con 0.518 de los resultados originales, y la precisión en 1 también aumenta de 0.5 a 0.63. Basándonos en estos resultados, utilizaremos el clasificador de combinación de características directas (es decir, BM25F+All) para comparaciones posteriores que involucren retroalimentación implícita. Curiosamente, el uso solo de clics, aunque proporciona un beneficio significativo sobre la clasificación original de BM25F, no es tan efectivo como considerar el conjunto completo de características en la Tabla 4.1. Mientras analizamos el comportamiento del usuario (y las características de los componentes más efectivos) en un documento separado [1], vale la pena dar un ejemplo concreto del tipo de ruido inherente en la retroalimentación real de los usuarios en el entorno de búsqueda web. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 Posición del resultado Frecuencia de clics relativa PTR=2 PTR=3 PTR=5 Figura 6.3: Frecuencia relativa de clics para consultas con diferentes Posiciones del Resultado Relevante Superior (PTR). Si los usuarios solo consideraran la relevancia de un resultado para su consulta, harían clic en los resultados más relevantes en la parte superior. Desafortunadamente, como han demostrado Joachims y otros, la presentación también influye de manera bastante dramática en los resultados en los que los usuarios hacen clic. Los usuarios a menudo hacen clic en los resultados por encima del relevante, presumiblemente porque los resúmenes cortos no proporcionan suficiente información para hacer evaluaciones precisas de relevancia y han aprendido que, en promedio, los elementos mejor clasificados son relevantes. La Figura 6.3 muestra las frecuencias relativas de clics para consultas con elementos relevantes conocidos en posiciones distintas a la primera posición; la posición del resultado relevante superior (PTR) varía de 2 a 10 en la figura. Por ejemplo, para consultas con el primer resultado relevante en la posición 5 (PTR=5), hay más clics en los resultados no relevantes en posiciones más altas que en el primer resultado relevante en la posición 5. Como veremos, el aprendizaje sobre un conjunto de características de comportamiento más amplio resulta en una mejora sustancial en la precisión en comparación con solo el clic. Ahora consideramos incorporar el comportamiento del usuario en un conjunto de características mucho más amplio, RN (Sección 5.3) utilizado por un importante motor de búsqueda web. RN incorpora BM25F, características basadas en enlaces y cientos de otras características. La Figura 6.4 informa sobre NDCG en K y la Figura 6.5 informa sobre Precisión en K. Curiosamente, aunque las clasificaciones originales de RN son significativamente más precisas que BM25F solo, la incorporación de características de retroalimentación implícita (BM25F+All) da como resultado una clasificación que supera significativamente a las clasificaciones originales de RN. En otras palabras, la retroalimentación implícita incorpora suficiente información para reemplazar las cientos de otras características disponibles para el aprendiz de RankNet entrenado en el conjunto de características RN. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 1 2 3 4 5 6 7 8 9 10K NDCG RN RN+All BM25 BM25+All Figura 6.4: NDCG en K para BM25F, BM25F+All, RN y RN+All para diferentes K Además, enriquecer las características de RN con el conjunto de retroalimentación implícita muestra una ganancia significativa en todas las medidas, permitiendo que RN+All supere a todos los demás métodos. Esto demuestra la naturaleza complementaria de la retroalimentación implícita con otras características disponibles para un motor de búsqueda web de última generación. 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 Precisión RN RN+Todo BM25 BM25+Todo Figura 6.5: Precisión en K para BM25F, BM25F+Todo, RN y RN+Todo para diferentes valores de K. Resumimos el rendimiento de los diferentes métodos de clasificación en la Tabla 6.1. Informamos el puntaje de Precisión Promedio Media (MAP) para cada sistema. Aunque no es intuitivo de interpretar, el MAP permite la comparación cuantitativa en una sola métrica. Las ganancias marcadas con * son significativas a un nivel de p=0.01 utilizando una prueba t de dos colas. Ganancia MAP P(1) Ganancia BM25F 0.184 - 0.503BM25F-Rerank-CT 0.215 0.031* 0.577 0.073* BM25F-RerankImplicit 0.218 0.003 0.605 0.028* BM25F+Implicit 0.222 0.004 0.620 0.015* RN 0.215 - 0.597RN+All 0.248 0.033* 0.629 0.032* Tabla 6.1: Precisión Promedio (MAP) para todas las estrategias. Hasta ahora hemos informado resultados promediados en todas las consultas del conjunto de pruebas. Desafortunadamente, menos de la mitad tuvo interacciones suficientes para intentar un nuevo ranking. De las 1000 consultas en la prueba, entre el 46% y el 49%, dependiendo de la división entre entrenamiento y prueba, tenían suficiente información de interacción para hacer predicciones (es decir, hubo al menos 1 sesión de búsqueda en la que el usuario hizo clic en al menos 1 URL de resultado). Esto no es sorprendente: la búsqueda en la web tiene una distribución de cola pesada y hay muchas consultas únicas. Ahora consideramos el rendimiento en las consultas para las cuales estaban disponibles las interacciones de los usuarios. La Figura 6.6 informa sobre el NDCG para el subconjunto de las consultas de prueba con las características de retroalimentación implícita. Las ganancias en el primer puesto son dramáticas. El NDCG en 1 de BM25F+All aumenta de 0.6 a 0.75 (un aumento relativo del 31%), logrando un rendimiento comparable al de RN+All operando sobre un conjunto de características mucho más rico. 0.6 0.65 0.7 0.75 0.8 1 3 5 10K NDCG RN RN+All BM25 BM25+All Figura 6.6: NDCG en K para BM25F, BM25F+All, RN y RN+All en consultas de prueba con interacciones de usuario. Del mismo modo, las ganancias en precisión en el top 1 son sustanciales (Figura 6.7) y es probable que sean evidentes para los usuarios de búsqueda web. Cuando se dispone de retroalimentación implícita, el sistema BM25F+All devuelve el documento relevante en la parte superior 1 casi el 70% del tiempo, en comparación con el 53% del tiempo cuando la retroalimentación implícita no es considerada por el sistema BM25F original. 0.45 0.5 0.55 0.6 0.65 0.7 1 3 5 10K Precisión RN RN+All BM25 BM25+All Figura 6.7: Precisión en K NDCG en K para BM25F, BM25F+All, RN y RN+All en consultas de prueba con interacciones de usuario Resumimos los resultados en la medida MAP para las consultas intentadas en la Tabla 6.2. Las mejoras en MAP son tanto sustanciales como significativas, siendo más pronunciadas que las mejoras en el clasificador BM25F. Ahora analizamos los casos en los que la retroalimentación implícita resultó ser más útil. La Figura 6.8 informa las mejoras de MAP sobre la ejecución de BM25F base para cada consulta con MAP inferior a 0.6. Ten en cuenta que la mayoría de la mejora es para consultas con bajo rendimiento (es decir, MAP < 0.1). Curiosamente, la incorporación de información sobre el comportamiento del usuario disminuye la precisión para consultas con un puntaje MAP original alto. Una posible explicación es que estas consultas fáciles tienden a ser de navegación (es decir, tener una única respuesta apropiada altamente clasificada), y las interacciones de los usuarios con resultados de menor rango pueden indicar necesidades de información divergentes que son mejor atendidas por los resultados menos populares (con calificaciones de relevancia global correspondientemente bajas). Para resumir nuestros resultados experimentales, la incorporación de retroalimentación implícita en un entorno real de búsqueda web resultó en mejoras significativas sobre las clasificaciones originales, utilizando tanto BM25F como RN como líneas de base. Nuestro amplio conjunto de características implícitas, como el tiempo en la página y las desviaciones del comportamiento promedio, ofrece ventajas sobre el uso exclusivo del clic como indicador de interés. Además, incorporar características de retroalimentación implícita directamente en la función de clasificación aprendida es más efectivo que utilizar la retroalimentación implícita para volver a clasificar. Las mejoras observadas en grandes conjuntos de pruebas de consultas (1,000 en total, entre 466 y 495 con retroalimentación implícita disponible) son tanto sustanciales como estadísticamente significativas. 7. CONCLUSIONES Y TRABAJOS FUTUROS En este artículo exploramos la utilidad de incorporar retroalimentación implícita ruidosa obtenida en un entorno real de búsqueda web para mejorar la clasificación de búsqueda web. Realizamos una evaluación a gran escala con más de 3,000 consultas y más de 12 millones de interacciones de usuarios con un motor de búsqueda importante, estableciendo la utilidad de incorporar retroalimentación implícita ruidosa para mejorar la relevancia de la búsqueda web. Comparamos dos alternativas para incorporar retroalimentación implícita en el proceso de búsqueda, a saber, reordenar con retroalimentación implícita e incorporar directamente características de retroalimentación implícita en la función de clasificación entrenada. Nuestros experimentos mostraron una mejora significativa sobre los métodos que no consideran la retroalimentación implícita. Las ganancias son particularmente dramáticas para el resultado superior K=1 en la clasificación final, con mejoras de precisión de hasta un 31%, y las ganancias son sustanciales para todos los valores de K. Nuestros experimentos mostraron que el feedback implícito del usuario puede mejorar aún más el rendimiento de la búsqueda web, cuando se incorpora directamente con características populares basadas en contenido y enlaces. Curiosamente, la retroalimentación implícita es especialmente valiosa para consultas con una clasificación original deficiente de resultados (por ejemplo, MAP inferior a 0.1). Una dirección prometedora para trabajos futuros es aplicar la investigación reciente sobre la predicción automática de la dificultad de las consultas, e intentar incorporar únicamente retroalimentación implícita para las consultas difíciles. Como otra dirección de investigación, estamos explorando métodos para extender nuestras predicciones a las consultas previamente no vistas (por ejemplo, el agrupamiento de consultas), lo cual debería mejorar aún más la experiencia de búsqueda en la web de los usuarios. AGRADECIMIENTOS Agradecemos a Chris Burges y Matt Richardson por la implementación de RankNet para nuestros experimentos. También agradecemos a Robert Ragno por sus valiosas sugerencias y muchas discusiones. 8. REFERENCIAS [1] E. Agichtein, E. Brill, S. Dumais y R. Ragno, Aprendizaje de modelos de interacción del usuario para predecir las preferencias de resultados de búsqueda web. En Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2006 [2] J. Allan, Resumen de la Pista HARD en TREC 2003, Recuperación de Alta Precisión de Documentos, 2003 [3] R. Baeza-Yates y B. Ribeiro-Neto, Recuperación de Información Moderna, Addison-Wesley, 1999. [4] S. Brin y L. Page, Anatomía de un Motor de Búsqueda Web Hipertextual a Gran Escala, en Actas de WWW, 1997 [5] C.J.C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. Hullender, Aprendizaje para clasificación utilizando descenso de gradiente, en Actas de la Conferencia Internacional sobre Aprendizaje Automático, 2005 [6] D.M. Chickering, The WinMine Toolkit, Informe Técnico de Microsoft MSR-TR-2002-103, 2002 [7] M. Claypool, D. Brown, P. Lee y M. Waseda. Inferir el interés del usuario. IEEE Internet Computing. 2001 [8] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais y T. White. Evaluando medidas implícitas para mejorar la experiencia de búsqueda. En ACM Transactions on Information Systems, 2005 [9] J. Goecks y J. Shavlick. Aprendiendo los intereses de los usuarios observando de manera discreta su comportamiento normal. En Actas del Taller de IJCAI sobre Aprendizaje Automático para Filtrado de Información. 1999. [10] K Jarvelin y J. Kekalainen. Métodos de evaluación IR para recuperar documentos altamente relevantes. En las Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2000 [11] T. Joachims, Optimización de Motores de Búsqueda Utilizando Datos de Clics. En Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (SIGKDD), 2002 [12] T. Joachims, L. Granka, B. Pang, H. Hembrooke y G. Gay, Interpretación precisa de los datos de clics como retroalimentación implícita, Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2005 [13] T. Joachims, Haciendo práctico el aprendizaje SVM a gran escala. Avances en Métodos de Núcleo, en Aprendizaje de Vectores de Soporte, MIT Press, 1999 [14] D. Kelly y J. Teevan, Retroalimentación implícita para inferir preferencias de usuario: una bibliografía. En el Foro SIGIR, 2003 [15] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon y J. Riedl. GroupLens: Aplicando filtrado colaborativo a las noticias de Usenet. En Comunicaciones de ACM, 1997. [16] M. Morita y Y. Shinoda, Filtrado de información basado en análisis del comportamiento del usuario y recuperación de texto de mejor coincidencia. Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 1994 [17] D. Oard y J. Kim. Retroalimentación implícita para sistemas de recomendación. En Actas del Taller de Sistemas de Recomendación de la AAAI. 1998 [18] D. Oard y J. Kim. Modelando el contenido de la información utilizando el comportamiento observable. En Actas de la 64ª Reunión Anual de la Sociedad Americana de Ciencia de la Información y Tecnología. 2001 [19] N. Pharo, N. y K. Järvelin. El método SST: una herramienta para analizar los procesos de búsqueda de información en la web. En Information Processing & Management, 2004 [20] P. Pirolli, El Uso de la Pista de Información Próxima para Buscar Contenido Distal en la World Wide Web. Trabajando con la tecnología en mente: Brunswikiano. Recursos para Ciencia Cognitiva e Ingeniería, Oxford University Press, 2004 [21] F. Radlinski y T. Joachims, Cadenas de Consulta: Aprendizaje para Clasificar a partir de Retroalimentación Implícita. En Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (SIGKDD), 2005. [22] F. Radlinski y T. Joachims, Evaluando la Robustez del Aprendizaje a partir de Retroalimentación Implícita, en Actas del Taller de ICML sobre Aprendizaje en Búsqueda Web, 2005 [23] S. E. Robertson, H. Zaragoza y M. Taylor, Extensión simple de BM25 a múltiples campos ponderados, en Actas de la Conferencia sobre Información y Gestión del Conocimiento (CIKM), 2004 [24] G. Salton y M. McGill. Introducción a la recuperación de información moderna. McGraw-Hill, 1983 [25] E.M. Voorhees, D. Harman, Resumen de TREC, 2001 [26] G.R. Xue, H.J. Zeng, Z. Chen, Y. Yu, W.Y. -> Zeng, Z. Chen, Y. Yu, W.Y. Ma, W.S. Xi, y W.G. Fan, Optimizing web search using web clickthrough data, en Actas de la Conferencia sobre Información y Gestión del Conocimiento (CIKM), 2004 [27] H. Zaragoza, N. Craswell, M. Taylor, S. Saria y S. Robertson. Microsoft Cambridge en TREC 13: Pistas Web y Duras. En Actas de TREC 2004 ",
            "candidates": [],
            "error": [
                []
            ]
        },
        "user interaction": {
            "translated_key": "características de interacción del usuario",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Improving Web Search Ranking by Incorporating User Behavior Information Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com ABSTRACT We show that incorporating user behavior data can significantly improve ordering of top results in real web search setting.",
                "We examine alternatives for incorporating feedback into the ranking process and explore the contributions of user feedback compared to other common web search features.",
                "We report results of a large scale evaluation over 3,000 queries and 12 million user interactions with a popular web search engine.",
                "We show that incorporating implicit feedback can augment other features, improving the accuracy of a competitive web search ranking algorithms by as much as 31% relative to the original performance.",
                "Categories and Subject Descriptors H.3.3 Information Search and Retrieval - Relevance feedback, search process; H.3.5 Online Information Services - Web-based services.",
                "General Terms Algorithms, Measurement, Experimentation 1.",
                "INTRODUCTION Millions of users interact with search engines daily.",
                "They issue queries, follow some of the links in the results, click on ads, spend time on pages, reformulate their queries, and perform other actions.",
                "These interactions can serve as a valuable source of information for tuning and improving web search result ranking and can compliment more costly explicit judgments.",
                "Implicit relevance feedback for ranking and personalization has become an active area of research.",
                "Recent work by Joachims and others exploring implicit feedback in controlled environments have shown the value of incorporating implicit feedback into the ranking process.",
                "Our motivation for this work is to understand how implicit feedback can be used in a large-scale operational environment to improve retrieval.",
                "How does it compare to and compliment evidence from page content, anchor text, or link-based features such as inlinks or PageRank?",
                "While it is intuitive that user interactions with the web search engine should reveal at least some information that could be used for ranking, estimating user preferences in real web search settings is a challenging problem, since real user interactions tend to be more noisy than commonly assumed in the controlled settings of previous studies.",
                "Our paper explores whether implicit feedback can be helpful in realistic environments, where user feedback can be noisy (or adversarial) and a web search engine already uses hundreds of features and is heavily tuned.",
                "To this end, we explore different approaches for ranking web search results using real user behavior obtained as part of normal interactions with the web search engine.",
                "The specific contributions of this paper include: • Analysis of alternatives for incorporating user behavior into web search ranking (Section 3). • An application of a robust implicit feedback model derived from mining millions of user interactions with a major web search engine (Section 4). • A large scale evaluation over real user queries and search results, showing significant improvements derived from incorporating user feedback (Section 6).",
                "We summarize our findings and discuss extensions to the current work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "Most common approaches primarily focus on similarity of query and a page, as well as the overall page quality [3,4,24].",
                "However, with increasing popularity of search engines, implicit feedback (i.e., the actions users take when interacting with the search engine) can be used to improve the rankings.",
                "Implicit relevance measures have been studied by several research groups.",
                "An overview of implicit measures is compiled in Kelly and Teevan [14].",
                "This research, while developing valuable insights into implicit relevance measures, was not applied to improve the ranking of web search results in realistic settings.",
                "Closely related to our work, Joachims [11] collected implicit measures in place of explicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "Fox et al. [8] explored the relationship between implicit and explicit measures in Web search, and developed Bayesian models to correlate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "This work considered a wide range of user behaviors (e.g., dwell time, scroll time, reformulation patterns) in addition to the popular clickthrough behavior.",
                "However, the modeling effort was aimed at predicting explicit relevance judgments from implicit user actions and not specifically at learning ranking functions.",
                "Other studies of user behavior in web search include Pharo and Järvelin [19], but were not directly applied to improve ranking.",
                "More recently, Joachims et al. [12] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthroughs in a controlled, laboratory setting.",
                "Unfortunately, the extent to which previous research applies to real-world web search is unclear.",
                "At the same time, while recent work (e.g., [26]) on using clickthrough information for improving web search ranking is promising, it captures only one aspect of the user interactions with web search engines.",
                "We build on existing research to develop robust user behavior interpretation techniques for the real web search setting.",
                "Instead of treating each user as a reliable expert, we aggregate information from multiple, unreliable, user search session traces, as we describe in the next two sections. 3.",
                "INCORPORATING IMPLICIT FEEDBACK We consider two complementary approaches to ranking with implicit feedback: (1) treating implicit feedback as independent evidence for ranking results, and (2) integrating implicit feedback features directly into the ranking algorithm.",
                "We describe the two general ranking approaches next.",
                "The specific implicit feedback features are described in Section 4, and the algorithms for interpreting and incorporating implicit feedback are described in Section 5. 3.1 Implicit Feedback as Independent Evidence The general approach is to re-rank the results obtained by a web search engine according to observed clickthrough and other user interactions for the query in previous search sessions.",
                "Each result is assigned a score according to expected relevance/user satisfaction based on previous interactions, resulting in some preference ordering based on user interactions alone.",
                "While there has been significant work on merging multiple rankings, we adapt a simple and robust approach of ignoring the original rankers scores, and instead simply merge the rank orders.",
                "The main reason for ignoring the original scores is that since the feature spaces and learning algorithms are different, the scores are not directly comparable, and re-normalization tends to remove the benefit of incorporating classifier scores.",
                "We experimented with a variety of merging functions on the development set of queries (and using a set of interactions from a different time period from final evaluation sets).",
                "We found that a simple rank merging heuristic combination works well, and is robust to variations in score values from original rankers.",
                "For a given query q, the implicit score ISd is computed for each result d from available <br>user interaction</br> features, resulting in the implicit rank Id for each result.",
                "We compute a merged score SM(d) for d by combining the ranks obtained from implicit feedback, Id with the original rank of d, Od:     ¡     ¢ £ + + + + = otherwise O dforexistsfeedbackimplicitif OI w wOIdS d dd I IddM 1 1 1 1 1 1 ),,,( where the weight wI is a heuristically tuned scaling factor representing the relative importance of the implicit feedback.",
                "The query results are ordered in by decreasing values of SM to produce the final ranking.",
                "One special case of this model arises when setting wI to a very large value, effectively forcing clicked results to be ranked higher than un-clicked results - an intuitive and effective heuristic that we will use as a baseline.",
                "Applying more sophisticated classifier and ranker combination algorithms may result in additional improvements, and is a promising direction for future work.",
                "The approach above assumes that there are no interactions between the underlying features producing the original web search ranking and the implicit feedback features.",
                "We now relax this assumption by integrating implicit feedback features directly into the ranking process. 3.2 Ranking with Implicit Feedback Features Modern web search engines rank results based on a large number of features, including content-based features (i.e., how closely a query matches the text or title or anchor text of the document), and query-independent page quality features (e.g., PageRank of the document or the domain).",
                "In most cases, automatic (or semiautomatic) methods are developed for tuning the specific ranking function that combines these feature values.",
                "Hence, a natural approach is to incorporate implicit feedback features directly as features for the ranking algorithm.",
                "During training or tuning, the ranker can be tuned as before but with additional features.",
                "At runtime, the search engine would fetch the implicit feedback features associated with each query-result URL pair.",
                "This model requires a ranking algorithm to be robust to missing values: more than 50% of queries to web search engines are unique, with no previous implicit feedback available.",
                "We now describe such a ranker that we used to learn over the combined feature sets including implicit feedback. 3.3 Learning to Rank Web Search Results A key aspect of our approach is exploiting recent advances in machine learning, namely trainable ranking algorithms for web search and information retrieval (e.g., [5, 11] and classical results reviewed in [3]).",
                "In our setting, explicit human relevance judgments (labels) are available for a set of web search queries and results.",
                "Hence, an attractive choice to use is a supervised machine learning technique to learn a ranking function that best predicts relevance judgments.",
                "RankNet is one such algorithm.",
                "It is a neural net tuning algorithm that optimizes feature weights to best match explicitly provided pairwise user preferences.",
                "While the specific training algorithms used by RankNet are beyond the scope of this paper, it is described in detail in [5] and includes extensive evaluation and comparison with other ranking methods.",
                "An attractive feature of RankNet is both train- and run-time efficiency - runtime ranking can be quickly computed and can scale to the web, and training can be done over thousands of queries and associated judged results.",
                "We use a 2-layer implementation of RankNet in order to model non-linear relationships between features.",
                "Furthermore, RankNet can learn with many (differentiable) cost functions, and hence can automatically learn a ranking function from human-provided labels, an attractive alternative to heuristic feature combination techniques.",
                "Hence, we will also use RankNet as a generic ranker to explore the contribution of implicit feedback for different ranking alternatives. 4.",
                "IMPLICIT USER FEEDBACK MODEL Our goal is to accurately interpret noisy user feedback obtained as by tracing user interactions with the search engine.",
                "Interpreting implicit feedback in real web search setting is not an easy task.",
                "We characterize this problem in detail in [1], where we motivate and evaluate a wide variety of models of implicit user activities.",
                "The general approach is to represent user actions for each search result as a vector of features, and then train a ranker on these features to discover feature values indicative of relevant (and nonrelevant) search results.",
                "We first briefly summarize our features and model, and the learning approach (Section 4.2) in order to provide sufficient information to replicate our ranking methods and the subsequent experiments. 4.1 Representing User Actions as Features We model observed web search behaviors as a combination of a ``background component (i.e., query- and relevance-independent noise in user behavior, including positional biases with result interactions), and a ``relevance component (i.e., query-specific behavior indicative of relevance of a result to a query).",
                "We design our features to take advantage of aggregated user behavior.",
                "The feature set is comprised of directly observed features (computed directly from observations for each query), as well as queryspecific derived features, computed as the deviation from the overall query-independent distribution of values for the corresponding directly observed feature values.",
                "The features used to represent user interactions with web search results are summarized in Table 4.1.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "We include the traditional implicit feedback features such as clickthrough counts for the results, as well as our novel derived features such as the deviation of the observed clickthrough number for a given query-URL pair from the expected number of clicks on a result in the given position.",
                "We also model the browsing behavior after a result was clicked - e.g., the average page dwell time for a given query-URL pair, as well as its deviation from the expected (average) dwell time.",
                "Furthermore, the feature set was designed to provide essential information about the user experience to make feedback interpretation robust.",
                "For example, web search users can often determine whether a result is relevant by looking at the result title, URL, and summary - in many cases, looking at the original document is not necessary.",
                "To model this aspect of user experience we include features such as overlap in words in title and words in query (TitleOverlap) and the fraction of words shared by the query and the result summary.",
                "Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickProbability Probability of a click for this query and URL ClickDeviation Deviation from expected click probability IsNextClicked 1 if clicked on next position, 0 otherwise IsPreviousClicked 1 if clicked on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, no parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from average dwell time on page CumulativeDeviation Deviation from average cumulative dwell time DomainDeviation Deviation from average dwell time on domain Query-text features TitleOverlap Words shared between query and title SummaryOverlap Words shared between query and snippet QueryURLOverlap Words shared between query and URL QueryDomainOverlap Words shared between query and URL domain QueryLength Number of tokens in query QueryNextOverlap Fraction of words shared with next query Table 4.1: Some features used to represent post-search navigation history for a given query and search result URL.",
                "Having described our feature set, we briefly review our general method for deriving a user behavior model. 4.2 Deriving a User Feedback Model To learn to interpret the observed user behavior, we correlate user actions (i.e., the features in Table 4.1 representing the actions) with the explicit user judgments for a set of training queries.",
                "We find all the instances in our session logs where these queries were submitted to the search engine, and aggregate the user behavior features for all search sessions involving these queries.",
                "Each observed query-URL pair is represented by the features in Table 4.1, with values averaged over all search sessions, and assigned one of six possible relevance labels, ranging from Perfect to Bad, as assigned by explicit relevance judgments.",
                "These labeled feature vectors are used as input to the RankNet training algorithm (Section 3.3) which produces a trained user behavior model.",
                "This approach is particularly attractive as it does not require heuristics beyond feature engineering.",
                "The resulting user behavior model is used to help rank web search resultseither directly or in combination with other features, as described below. 5.",
                "EXPERIMENTAL SETUP The ultimate goal of incorporating implicit feedback into ranking is to improve the relevance of the returned web search results.",
                "Hence, we compare the ranking methods over a large set of judged queries with explicit relevance labels provided by human judges.",
                "In order for the evaluation to be realistic we obtained a random sample of queries from web search logs of a major search engine, with associated results and traces for user actions.",
                "We describe this dataset in detail next.",
                "Our metrics are described in Section 5.2 that we use to evaluate the ranking alternatives, listed in Section 5.3 in the experiments of Section 6. 5.1 Datasets We compared our ranking methods over a random sample of 3,000 queries from the search engine query logs.",
                "The queries were drawn from the logs uniformly at random by token without replacement, resulting in a query sample representative of the overall query distribution.",
                "On average, 30 results were explicitly labeled by human judges using a six point scale ranging from Perfect down to Bad.",
                "Overall, there were over 83,000 results with explicit relevance judgments.",
                "In order to compute various statistics, documents with label Good or better will be considered relevant, and with lower labels to be non-relevant.",
                "Note that the experiments were performed over the results already highly ranked by a web search engine, which corresponds to a typical user experience which is limited to the small number of the highly ranked results for a typical web search query.",
                "The user interactions were collected over a period of 8 weeks using voluntary opt-in information.",
                "In total, over 1.2 million unique queries were instrumented, resulting in over 12 million individual interactions with the search engine.",
                "The data consisted of user interactions with the web search engine (e.g., clicking on a result link, going back to search results, etc.) performed after a query was submitted.",
                "These actions were aggregated across users and search sessions and converted to features in Table 4.1.",
                "To create the training, validation, and test query sets, we created three different random splits of 1,500 training, 500 validation, and 1000 test queries.",
                "The splits were done randomly by query, so that there was no overlap in training, validation, and test queries. 5.2 Evaluation Metrics We evaluate the ranking algorithms over a range of accepted information retrieval metrics, namely Precision at K (P(K)), Normalized Discounted Cumulative Gain (NDCG), and Mean Average Precision (MAP).",
                "Each metric focuses on a deferent aspect of system performance, as we describe below. • Precision at K: As the most intuitive metric, P(K) reports the fraction of documents ranked in the top K results that are labeled as relevant.",
                "In our setting, we require a relevant document to be labeled Good or higher.",
                "The position of relevant documents within the top K is irrelevant, and hence this metric measure overall user satisfaction with the top K results. • NDCG at K: NDCG is a retrieval measure devised specifically for web search evaluation [10].",
                "For a given query q, the ranked results are examined from the top ranked down, and the NDCG computed as:   = +−= K j jr qq jMN 1 )( )1log(/)12( Where Mq is a normalization constant calculated so that a perfect ordering would obtain NDCG of 1; and each r(j) is an integer relevance label (0=Bad and 5=Perfect) of result returned at position j.",
                "Note that unlabeled and Bad documents do not contribute to the sum, but will reduce NDCG for the query pushing down the relevant labeled documents, reducing their contributions.",
                "NDCG is well suited to web search evaluation, as it rewards relevant documents in the top ranked results more heavily than those ranked lower. • MAP: Average precision for each query is defined as the mean of the precision at K values computed after each relevant document was retrieved.",
                "The final MAP value is defined as the mean of average precisions of all queries in the test set.",
                "This metric is the most commonly used single-value summary of a run over a set of queries. 5.3 Ranking Methods Compared Recall that our goal is to quantify the effectiveness of implicit behavior for real web search.",
                "One dimension is to compare the utility of implicit feedback with other information available to a web search engine.",
                "Specifically, we compare effectiveness of implicit user behaviors with content-based matching, static page quality features, and combinations of all features. • BM25F: As a strong web search baseline we used the BM25F scoring, which was used in one of the best performing systems in the TREC 2004 Web track [23,27].",
                "BM25F and its variants have been extensively described and evaluated in IR literature, and hence serve as a strong, reproducible baseline.",
                "The BM25F variant we used for our experiments computes separate match scores for each field for a result document (e.g., body text, title, and anchor text), and incorporates query-independent linkbased information (e.g., PageRank, ClickDistance, and URL depth).",
                "The scoring function and field-specific tuning is described in detail in [23].",
                "Note that BM25F does not directly consider explicit or implicit feedback for tuning. • RN: The ranking produced by a neural net ranker (RankNet, described in Section 3.3) that learns to rank web search results by incorporating BM25F and a large number of additional static and dynamic features describing each search result.",
                "This system automatically learns weights for all features (including the BM25F score for a document) based on explicit human labels for a large set of queries.",
                "A system incorporating an implementation of RankNet is currently in use by a major search engine and can be considered representative of the state of the art in web search. • BM25F-RerankCT: The ranking produced by incorporating clickthrough statistics to reorder web search results ranked by BM25F above.",
                "Clickthrough is a particularly important special case of implicit feedback, and has been shown to correlate with result relevance.",
                "This is a special case of the ranking method in Section 3.1, with the weight wI set to 1000 and the ranking Id is simply the number of clicks on the result corresponding to d. In effect, this ranking brings to the top all returned web search results with at least one click (and orders them in decreasing order by number of clicks).",
                "The relative ranking of the remainder of results is unchanged and they are inserted below all clicked results.",
                "This method serves as our baseline implicit feedback reranking method.",
                "BM25F-RerankAll The ranking produced by reordering the BM25F results using all user behavior features (Section 4).",
                "This method learns a model of user preferences by correlating feature values with explicit relevance labels using the RankNet neural net algorithm (Section 4.2).",
                "At runtime, for a given query the implicit score Ir is computed for each result r with available <br>user interaction</br> features, and the implicit ranking is produced.",
                "The merged ranking is computed as described in Section 3.1.",
                "Based on the experiments over the development set we fix the value of wI to 3 (the effect of the wI parameter for this ranker turned out to be negligible). • BM25F+All: Ranking derived by training the RankNet (Section 3.3) learner over the features set of the BM25F score as well as all implicit feedback features (Section 3.2).",
                "We used the 2-layer implementation of RankNet [5] trained on the queries and labels in the training and validation sets. • RN+All: Ranking derived by training the 2-layer RankNet ranking algorithm (Section 3.3) over the union of all content, dynamic, and implicit feedback features (i.e., all of the features described above as well as all of the new implicit feedback features we introduced).",
                "The ranking methods above span the range of the information used for ranking, from not using the implicit or explicit feedback at all (i.e., BM25F) to a modern web search engine using hundreds of features and tuned on explicit judgments (RN).",
                "As we will show next, incorporating user behavior into these ranking systems dramatically improves the relevance of the returned documents. 6.",
                "EXPERIMENTAL RESULTS Implicit feedback for web search ranking can be exploited in a number of ways.",
                "We compare alternative methods of exploiting implicit feedback, both by re-ranking the top results (i.e., the BM25F-RerankCT and BM25F-RerankAll methods that reorder BM25F results), as well as by integrating the implicit features directly into the ranking process (i.e., the RN+ALL and BM25F+All methods which learn to rank results over the implicit feedback and other features).",
                "We compare our methods over strong baselines (BM25F and RN) over the NDCG, Precision at K, and MAP measures defined in Section 5.2.",
                "The results were averaged over three random splits of the overall dataset.",
                "Each split contained 1500 training, 500 validation, and 1000 test queries, all query sets disjoint.",
                "We first present the results over all 1000 test queries (i.e., including queries for which there are no implicit measures so we use the original web rankings).",
                "We then drill down to examine the effects on reranking for the attempted queries in more detail, analyzing where implicit feedback proved most beneficial.",
                "We first experimented with different methods of re-ranking the output of the BM25F search results.",
                "Figures 6.1 and 6.2 report NDCG and Precision for BM25F, as well as for the strategies reranking results with user feedback (Section 3.1).",
                "Incorporating all user feedback (either in reranking framework or as features to the learner directly) results in significant improvements (using two-tailed t-test with p=0.01) over both the original BM25F ranking as well as over reranking with clickthrough alone.",
                "The improvement is consistent across the top 10 results and largest for the top result: NDCG at 1 for BM25F+All is 0.622 compared to 0.518 of the original results, and precision at 1 similarly increases from 0.5 to 0.63.",
                "Based on these results we will use the direct feature combination (i.e., BM25F+All) ranker for subsequent comparisons involving implicit feedback. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 1 2 3 4 5 6 7 8 9 10K NDCG BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.1: NDCG at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K 0.35 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.2: Precision at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K Interestingly, using clickthrough alone, while giving significant benefit over the original BM25F ranking, is not as effective as considering the full set of features in Table 4.1.",
                "While we analyze user behavior (and most effective component features) in a separate paper [1], it is worthwhile to give a concrete example of the kind of noise inherent in real user feedback in web search setting. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 Result position Relativeclickfrequency PTR=2 PTR=3 PTR=5 Figure 6.3: Relative clickthrough frequency for queries with varying Position of Top Relevant result (PTR).",
                "If users considered only the relevance of a result to their query, they would click on the topmost relevant results.",
                "Unfortunately, as Joachims and others have shown, presentation also influences which results users click on quite dramatically.",
                "Users often click on results above the relevant one presumably because the short summaries do not provide enough information to make accurate relevance assessments and they have learned that on average topranked items are relevant.",
                "Figure 6.3 shows relative clickthrough frequencies for queries with known relevant items at positions other than the first position; the position of the top relevant result (PTR) ranges from 2-10 in the figure.",
                "For example, for queries with first relevant result at position 5 (PTR=5), there are more clicks on the non-relevant results in higher ranked positions than on the first relevant result at position 5.",
                "As we will see, learning over a richer behavior feature set, results in substantial accuracy improvement over clickthrough alone.",
                "We now consider incorporating user behavior into a much richer feature set, RN (Section 5.3) used by a major web search engine.",
                "RN incorporates BM25F, link-based features, and hundreds of other features.",
                "Figure 6.4 reports NDCG at K and Figure 6.5 reports Precision at K. Interestingly, while the original RN rankings are significantly more accurate than BM25F alone, incorporating implicit feedback features (BM25F+All) results in ranking that significantly outperforms the original RN rankings.",
                "In other words, implicit feedback incorporates sufficient information to replace the hundreds of other features available to the RankNet learner trained on the RN feature set. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 1 2 3 4 5 6 7 8 9 10K NDCG RN RN+All BM25 BM25+All Figure 6.4: NDCG at K for BM25F, BM25F+All, RN, and RN+All for varying K Furthermore, enriching the RN features with implicit feedback set exhibits significant gain on all measures, allowing RN+All to outperform all other methods.",
                "This demonstrates the complementary nature of implicit feedback with other features available to a state of the art web search engine. 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision RN RN+All BM25 BM25+All Figure 6.5: Precision at K for BM25F, BM25F+All, RN, and RN+All for varying K We summarize the performance of the different ranking methods in Table 6.1.",
                "We report the Mean Average Precision (MAP) score for each system.",
                "While not intuitive to interpret, MAP allows quantitative comparison on a single metric.",
                "The gains marked with * are significant at p=0.01 level using two tailed t-test.",
                "MAP Gain P(1) Gain BM25F 0.184 - 0.503BM25F-Rerank-CT 0.215 0.031* 0.577 0.073* BM25F-RerankImplicit 0.218 0.003 0.605 0.028* BM25F+Implicit 0.222 0.004 0.620 0.015* RN 0.215 - 0.597RN+All 0.248 0.033* 0.629 0.032* Table 6.1: Mean Average Precision (MAP) for all strategies.",
                "So far we reported results averaged across all queries in the test set.",
                "Unfortunately, less than half had sufficient interactions to attempt reranking.",
                "Out of the 1000 queries in test, between 46% and 49%, depending on the train-test split, had sufficient interaction information to make predictions (i.e., there was at least 1 search session in which at least 1 result URL was clicked on by the user).",
                "This is not surprising: web search is heavy-tailed, and there are many unique queries.",
                "We now consider the performance on the queries for which user interactions were available.",
                "Figure 6.6 reports NDCG for the subset of the test queries with the implicit feedback features.",
                "The gains at top 1 are dramatic.",
                "The NDCG at 1 of BM25F+All increases from 0.6 to 0.75 (a 31% relative gain), achieving performance comparable to RN+All operating over a much richer feature set. 0.6 0.65 0.7 0.75 0.8 1 3 5 10K NDCG RN RN+All BM25 BM25+All Figure 6.6: NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions Similarly, gains on precision at top 1 are substantial (Figure 6.7), and are likely to be apparent to web search users.",
                "When implicit feedback is available, the BM25F+All system returns relevant document at top 1 almost 70% of the time, compared 53% of the time when implicit feedback is not considered by the original BM25F system. 0.45 0.5 0.55 0.6 0.65 0.7 1 3 5 10K Precision RN RN+All BM25 BM25+All Figure 6.7: Precision at K NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions We summarize the results on the MAP measure for attempted queries in Table 6.2.",
                "MAP improvements are both substantial and significant, with improvements over the BM25F ranker most pronounced.",
                "Method MAP Gain P(1) Gain RN 0.269 0.632 RN+All 0.321 0.051 (19%) 0.693 0.061(10%) BM25F 0.236 0.525 BM25F+All 0.292 0.056 (24%) 0.687 0.162 (31%) Table 6.2: Mean Average Precision (MAP) on attempted queries for best performing methods We now analyze the cases where implicit feedback was shown most helpful.",
                "Figure 6.8 reports the MAP improvements over the baseline BM25F run for each query with MAP under 0.6.",
                "Note that most of the improvement is for poorly performing queries (i.e., MAP < 0.1).",
                "Interestingly, incorporating user behavior information degrades accuracy for queries with high original MAP score.",
                "One possible explanation is that these easy queries tend to be navigational (i.e., having a single, highly-ranked most appropriate answer), and user interactions with lower-ranked results may indicate divergent information needs that are better served by the less popular results (with correspondingly poor overall relevance ratings). 0 50 100 150 200 250 300 350 0.1 0.2 0.3 0.4 0.5 0.6 -0.4 -0.35 -0.3 -0.25 -0.2 -0.15 -0.1 -0.05 0 0.05 0.1 0.15 0.2 Frequency Average Gain Figure 6.8: Gain of BM25F+All over original BM25F ranking To summarize our experimental results, incorporating implicit feedback in real web search setting resulted in significant improvements over the original rankings, using both BM25F and RN baselines.",
                "Our rich set of implicit features, such as time on page and deviations from the average behavior, provides advantages over using clickthrough alone as an indicator of interest.",
                "Furthermore, incorporating implicit feedback features directly into the learned ranking function is more effective than using implicit feedback for reranking.",
                "The improvements observed over large test sets of queries (1,000 total, between 466 and 495 with implicit feedback available) are both substantial and statistically significant. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper we explored the utility of incorporating noisy implicit feedback obtained in a real web search setting to improve web search ranking.",
                "We performed a large-scale evaluation over 3,000 queries and more than 12 million user interactions with a major search engine, establishing the utility of incorporating noisy implicit feedback to improve web search relevance.",
                "We compared two alternatives of incorporating implicit feedback into the search process, namely reranking with implicit feedback and incorporating implicit feedback features directly into the trained ranking function.",
                "Our experiments showed significant improvement over methods that do not consider implicit feedback.",
                "The gains are particularly dramatic for the top K=1 result in the final ranking, with precision improvements as high as 31%, and the gains are substantial for all values of K. Our experiments showed that implicit user feedback can further improve web search performance, when incorporated directly with popular content- and link-based features.",
                "Interestingly, implicit feedback is particularly valuable for queries with poor original ranking of results (e.g., MAP lower than 0.1).",
                "One promising direction for future work is to apply recent research on automatically predicting query difficulty, and only attempt to incorporate implicit feedback for the difficult queries.",
                "As another research direction we are exploring methods for extending our predictions to the previously unseen queries (e.g., query clustering), which should further improve the web search experience of users.",
                "ACKNOWLEDGMENTS We thank Chris Burges and Matt Richardson for an implementation of RankNet for our experiments.",
                "We also thank Robert Ragno for his valuable suggestions and many discussions. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, S. Dumais, and R.Ragno, Learning <br>user interaction</br> Models for Predicting Web Search Result Preferences.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan, HARD Track Overview in TREC 2003, High Accuracy Retrieval from Documents, 2003 [3] R. Baeza-Yates and B. Ribeiro-Neto, Modern Information Retrieval, Addison-Wesley, 1999. [4] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine, in Proceedings of WWW, 1997 [5] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning, 2005 [6] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [7] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest.",
                "IEEE Internet Computing. 2001 [8] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [9] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [10] K Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2000 [11] T. Joachims, Optimizing Search Engines Using Clickthrough Data.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [12] T. Joachims, L. Granka, B. Pang, H. Hembrooke, and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [13] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [14] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [15] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [16] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [17] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems.",
                "In Proceedings of the AAAI Workshop on Recommender Systems. 1998 [18] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [19] N. Pharo, N. and K. Järvelin.",
                "The SST method: a tool for analyzing web information search processes.",
                "In Information Processing & Management, 2004 [20] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [21] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (SIGKDD), 2005. [22] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in Proceedings of the ICML Workshop on Learning in Web Search, 2005 [23] S. E. Robertson, H. Zaragoza, and M. Taylor, Simple BM25 extension to multiple weighted fields, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [24] G. Salton & M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [25] E.M. Voorhees, D. Harman, Overview of TREC, 2001 [26] G.R.",
                "Xue, H.J.",
                "Zeng, Z. Chen, Y. Yu, W.Y.",
                "Ma, W.S.",
                "Xi, and W.G.",
                "Fan, Optimizing web search using web clickthrough data, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [27] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
                "Microsoft Cambridge at TREC 13: Web and Hard Tracks.",
                "In Proceedings of TREC 2004"
            ],
            "original_annotated_samples": [
                "For a given query q, the implicit score ISd is computed for each result d from available <br>user interaction</br> features, resulting in the implicit rank Id for each result.",
                "At runtime, for a given query the implicit score Ir is computed for each result r with available <br>user interaction</br> features, and the implicit ranking is produced.",
                "REFERENCES [1] E. Agichtein, E. Brill, S. Dumais, and R.Ragno, Learning <br>user interaction</br> Models for Predicting Web Search Result Preferences."
            ],
            "translated_annotated_samples": [
                "Para una consulta dada q, se calcula la puntuación implícita ISd para cada resultado d a partir de las <br>características de interacción del usuario</br> disponibles, lo que resulta en la clasificación implícita Id para cada resultado.",
                "En tiempo de ejecución, para una consulta dada se calcula la puntuación implícita Ir para cada resultado r con <br>características de interacción de usuario</br> disponibles, y se produce el ranking implícito.",
                "REFERENCIAS [1] E. Agichtein, E. Brill, S. Dumais y R. Ragno, Aprendizaje de modelos de <br>interacción del usuario</br> para predecir las preferencias de resultados de búsqueda web."
            ],
            "translated_text": "Mejorando la clasificación de búsqueda web al incorporar información sobre el comportamiento del usuario. Eugene Agichtein, Microsoft Research eugeneag@microsoft.com Eric Brill, Microsoft Research brill@microsoft.com Susan Dumais, Microsoft Research sdumais@microsoft.com RESUMEN Mostramos que la incorporación de datos sobre el comportamiento del usuario puede mejorar significativamente el orden de los principales resultados en una configuración real de búsqueda web. Examinamos alternativas para incorporar retroalimentación en el proceso de clasificación y exploramos las contribuciones de la retroalimentación de usuarios en comparación con otras características comunes de búsqueda en la web. Informamos los resultados de una evaluación a gran escala con más de 3,000 consultas y 12 millones de interacciones de usuarios con un motor de búsqueda web popular. Mostramos que la incorporación de retroalimentación implícita puede aumentar otras características, mejorando la precisión de los algoritmos de clasificación de búsqueda web competitivos hasta en un 31% en comparación con el rendimiento original. Categorías y Descriptores de Asignaturas H.3.3 Búsqueda y Recuperación de Información - Retroalimentación de relevancia, proceso de búsqueda; H.3.5 Servicios de Información en Línea - Servicios basados en la web. Términos generales Algoritmos, Medición, Experimentación 1. INTRODUCCIÓN Millones de usuarios interactúan con los motores de búsqueda a diario. Ellos emiten consultas, siguen algunos de los enlaces en los resultados, hacen clic en anuncios, pasan tiempo en páginas, reformulan sus consultas y realizan otras acciones. Estas interacciones pueden servir como una valiosa fuente de información para ajustar y mejorar la clasificación de los resultados de búsqueda en la web y pueden complementar juicios explícitos más costosos. La retroalimentación implícita de relevancia para la clasificación y personalización se ha convertido en un área activa de investigación. El trabajo reciente de Joachims y otros explorando el feedback implícito en entornos controlados ha demostrado el valor de incorporar el feedback implícito en el proceso de clasificación. Nuestra motivación para este trabajo es comprender cómo se puede utilizar la retroalimentación implícita en un entorno operativo a gran escala para mejorar la recuperación. ¿Cómo se compara y complementa con la evidencia del contenido de la página, el texto del ancla o las características basadas en enlaces como inlinks o PageRank? Si bien es intuitivo que las interacciones de los usuarios con el motor de búsqueda web deberían revelar al menos alguna información que podría ser utilizada para la clasificación, estimar las preferencias de los usuarios en entornos reales de búsqueda web es un problema desafiante, ya que las interacciones reales tienden a ser más ruidosas de lo que comúnmente se asume en los entornos controlados de estudios previos. Nuestro artículo explora si la retroalimentación implícita puede ser útil en entornos realistas, donde la retroalimentación del usuario puede ser ruidosa (o adversa) y un motor de búsqueda web ya utiliza cientos de características y está altamente ajustado. Con este fin, exploramos diferentes enfoques para clasificar los resultados de búsqueda web utilizando el comportamiento real de los usuarios obtenido como parte de las interacciones normales con el motor de búsqueda web. Las contribuciones específicas de este artículo incluyen: • Análisis de alternativas para incorporar el comportamiento del usuario en la clasificación de búsqueda web (Sección 3). • Aplicación de un modelo robusto de retroalimentación implícita derivado de la minería de millones de interacciones de usuarios con un importante motor de búsqueda web (Sección 4). • Una evaluación a gran escala sobre consultas reales de usuarios y resultados de búsqueda, mostrando mejoras significativas derivadas de la incorporación de la retroalimentación del usuario (Sección 6). Resumimos nuestros hallazgos y discutimos extensiones al trabajo actual en la Sección 7, que concluye el artículo. ANTECEDENTES Y TRABAJO RELACIONADO Clasificar los resultados de búsqueda es un problema fundamental en la recuperación de información. La mayoría de los enfoques comunes se centran principalmente en la similitud entre la consulta y una página, así como en la calidad general de la página [3,4,24]. Sin embargo, con la creciente popularidad de los motores de búsqueda, la retroalimentación implícita (es decir, las acciones que los usuarios realizan al interactuar con el motor de búsqueda) se puede utilizar para mejorar las clasificaciones. Las medidas de relevancia implícitas han sido estudiadas por varios grupos de investigación. Un resumen de las medidas implícitas se recopila en Kelly y Teevan [14]. Esta investigación, si bien desarrolló valiosas ideas sobre medidas implícitas de relevancia, no se aplicó para mejorar la clasificación de los resultados de búsqueda en la web en entornos realistas. Estrechamente relacionado con nuestro trabajo, Joachims [11] recopiló medidas implícitas en lugar de medidas explícitas, introduciendo una técnica basada completamente en datos de clics para aprender funciones de clasificación. Fox et al. [8] exploraron la relación entre medidas implícitas y explícitas en la búsqueda web, y desarrollaron modelos bayesianos para correlacionar medidas implícitas y juicios explícitos de relevancia tanto para consultas individuales como para sesiones de búsqueda. Este trabajo consideró una amplia gama de comportamientos de usuario (por ejemplo, tiempo de permanencia, tiempo de desplazamiento, patrones de reformulación) además del popular comportamiento de clics. Sin embargo, el esfuerzo de modelado se centró en predecir juicios de relevancia explícitos a partir de acciones implícitas de los usuarios y no específicamente en aprender funciones de clasificación. Otros estudios sobre el comportamiento de los usuarios en la búsqueda web incluyen a Pharo y Järvelin [19], pero no se aplicaron directamente para mejorar la clasificación. Más recientemente, Joachims et al. [12] presentaron una evaluación empírica de la interpretación de la evidencia de clics. Al realizar estudios de seguimiento ocular y correlacionar las predicciones de sus estrategias con las calificaciones explícitas, los autores demostraron que es posible interpretar con precisión los clics en un entorno controlado de laboratorio. Desafortunadamente, no está claro en qué medida la investigación previa se aplica a la búsqueda web del mundo real. Al mismo tiempo, si bien el trabajo reciente (por ejemplo, [26]) sobre el uso de la información de clics para mejorar la clasificación de búsqueda en la web es prometedor, solo abarca un aspecto de las interacciones de los usuarios con los motores de búsqueda en la web. Nos basamos en investigaciones existentes para desarrollar técnicas robustas de interpretación del comportamiento del usuario para el entorno real de búsqueda en la web. En lugar de tratar a cada usuario como un experto confiable, agregamos información de múltiples trazas de sesiones de búsqueda de usuarios no confiables, como describimos en las siguientes dos secciones. 3. INTEGRANDO LA RETROALIMENTACIÓN IMPLÍCITA Consideramos dos enfoques complementarios para la clasificación con retroalimentación implícita: (1) tratar la retroalimentación implícita como evidencia independiente para clasificar resultados, y (2) integrar características de retroalimentación implícita directamente en el algoritmo de clasificación. Describimos a continuación los dos enfoques generales de clasificación. Las características específicas del feedback implícito se describen en la Sección 4, y los algoritmos para interpretar e incorporar el feedback implícito se describen en la Sección 5. 3.1 Feedback Implícito como Evidencia Independiente. El enfoque general es reordenar los resultados obtenidos por un motor de búsqueda web según los clics observados y otras interacciones de usuario para la consulta en sesiones de búsqueda anteriores. Cada resultado se le asigna una puntuación según la relevancia esperada/ satisfacción del usuario basada en interacciones previas, lo que resulta en un cierto orden de preferencia basado únicamente en las interacciones del usuario. Si bien ha habido un trabajo significativo en la fusión de múltiples clasificaciones, adaptamos un enfoque simple y robusto de ignorar las puntuaciones de los clasificadores originales, y en su lugar simplemente fusionamos los órdenes de clasificación. La razón principal para ignorar las puntuaciones originales es que, dado que los espacios de características y los algoritmos de aprendizaje son diferentes, las puntuaciones no son directamente comparables, y la re-normalización tiende a eliminar el beneficio de incorporar las puntuaciones del clasificador. Experimentamos con una variedad de funciones de fusión en el conjunto de desarrollo de consultas (y utilizando un conjunto de interacciones de un período de tiempo diferente de los conjuntos de evaluación final). Encontramos que una combinación heurística simple de fusión de rangos funciona bien y es robusta a variaciones en los valores de puntuación de los clasificadores originales. Para una consulta dada q, se calcula la puntuación implícita ISd para cada resultado d a partir de las <br>características de interacción del usuario</br> disponibles, lo que resulta en la clasificación implícita Id para cada resultado. Calculamos un puntaje combinado SM(d) para d al combinar los rangos obtenidos de la retroalimentación implícita, Id, con el rango original de d, Od: SM(d) = Od + wI * Id, si existe retroalimentación implícita; de lo contrario, SM(d) = Od. Donde el peso wI es un factor de escala ajustado heurísticamente que representa la importancia relativa de la retroalimentación implícita. Los resultados de la consulta se ordenan en valores decrecientes de SM para producir la clasificación final. Un caso especial de este modelo surge al establecer wI en un valor muy grande, lo que efectivamente obliga a que los resultados clicados se clasifiquen por encima de los resultados no clicados, una heurística intuitiva y efectiva que utilizaremos como referencia. Aplicar algoritmos de combinación de clasificadores y ordenadores más sofisticados puede resultar en mejoras adicionales, y es una dirección prometedora para trabajos futuros. El enfoque anterior asume que no hay interacciones entre las características subyacentes que producen la clasificación original de la búsqueda web y las características de retroalimentación implícitas. Ahora relajamos esta suposición al integrar características de retroalimentación implícita directamente en el proceso de clasificación. 3.2 Clasificación con Características de Retroalimentación Implícita Los motores de búsqueda web modernos clasifican los resultados en función de un gran número de características, incluidas las características basadas en el contenido (es decir, qué tan cerca coincide una consulta con el texto, el título o el texto de anclaje del documento) y las características de calidad de página independientes de la consulta (por ejemplo, PageRank del documento o del dominio). En la mayoría de los casos, se desarrollan métodos automáticos (o semiautomáticos) para ajustar la función de clasificación específica que combina estos valores de características. Por lo tanto, un enfoque natural es incorporar características de retroalimentación implícita directamente como características para el algoritmo de clasificación. Durante el entrenamiento o ajuste, el clasificador puede ser ajustado como antes pero con características adicionales. En tiempo de ejecución, el motor de búsqueda recuperaría las características de retroalimentación implícita asociadas con cada par de URL de consulta y resultado. Este modelo requiere que un algoritmo de clasificación sea robusto ante valores faltantes: más del 50% de las consultas a los motores de búsqueda web son únicas, sin disponer de retroalimentación implícita previa. Ahora describimos un clasificador que utilizamos para aprender sobre los conjuntos de características combinadas, incluyendo la retroalimentación implícita. 3.3 Aprendizaje para Clasificar los Resultados de Búsqueda en la Web Un aspecto clave de nuestro enfoque es aprovechar los avances recientes en el aprendizaje automático, en particular los algoritmos de clasificación entrenables para la búsqueda en la web y la recuperación de información (por ejemplo, [5, 11] y resultados clásicos revisados en [3]). En nuestro entorno, contamos con juicios explícitos de relevancia humana (etiquetas) para un conjunto de consultas de búsqueda en la web y resultados. Por lo tanto, una opción atractiva es utilizar una técnica de aprendizaje automático supervisado para aprender una función de clasificación que prediga mejor las evaluaciones de relevancia. RankNet es uno de esos algoritmos. Es un algoritmo de ajuste de red neuronal que optimiza los pesos de las características para que coincidan mejor con las preferencias de los usuarios proporcionadas explícitamente en pares. Si bien los algoritmos de entrenamiento específicos utilizados por RankNet están más allá del alcance de este documento, se describen en detalle en [5] e incluyen una evaluación exhaustiva y comparación con otros métodos de clasificación. Una característica atractiva de RankNet es su eficiencia tanto en el tiempo de entrenamiento como en el de ejecución: el ranking en tiempo de ejecución se puede calcular rápidamente y puede escalarse a la web, y el entrenamiento se puede realizar sobre miles de consultas y resultados juzgados asociados. Utilizamos una implementación de RankNet de 2 capas para modelar relaciones no lineales entre características. Además, RankNet puede aprender con muchas funciones de costo (diferenciables), por lo que puede aprender automáticamente una función de clasificación a partir de etiquetas proporcionadas por humanos, una alternativa atractiva a las técnicas de combinación de características heurísticas. Por lo tanto, también utilizaremos RankNet como un clasificador genérico para explorar la contribución de la retroalimentación implícita en diferentes alternativas de clasificación. 4. MODELO IMPLÍCITO DE RETROALIMENTACIÓN DEL USUARIO Nuestro objetivo es interpretar con precisión la retroalimentación ruidosa del usuario obtenida al rastrear las interacciones del usuario con el motor de búsqueda. Interpretar la retroalimentación implícita en un entorno real de búsqueda en la web no es una tarea fácil. Caracterizamos este problema en detalle en [1], donde motivamos y evaluamos una amplia variedad de modelos de actividades implícitas de usuario. El enfoque general es representar las acciones del usuario para cada resultado de búsqueda como un vector de características, y luego entrenar un clasificador en base a estas características para descubrir los valores de las características que indican resultados de búsqueda relevantes (y no relevantes). Primero resumimos brevemente nuestras características y modelo, y el enfoque de aprendizaje (Sección 4.2) para proporcionar suficiente información para replicar nuestros métodos de clasificación y los experimentos subsiguientes. 4.1 Representación de las acciones del usuario como características. Modelamos los comportamientos observados en la búsqueda web como una combinación de un \"componente de fondo (es decir, ruido independiente de la consulta y relevancia en el comportamiento del usuario, incluidos sesgos posicionales con interacciones de resultados), y un \"componente de relevancia (es decir, comportamiento específico de la consulta indicativo de la relevancia de un resultado para una consulta). Diseñamos nuestras características para aprovechar el comportamiento de usuario agregado. El conjunto de características está compuesto por características observadas directamente (calculadas directamente a partir de observaciones para cada consulta), así como características derivadas específicas de la consulta, calculadas como la desviación de la distribución general de valores independientes de la consulta para los valores de las características observadas directamente correspondientes. Las características utilizadas para representar las interacciones de los usuarios con los resultados de búsqueda en la web se resumen en la Tabla 4.1. Esta información se obtuvo a través de la instrumentación del lado del cliente opt-in de usuarios de un importante motor de búsqueda web. Incluimos las características tradicionales de retroalimentación implícita, como el recuento de clics en los resultados, así como nuestras características derivadas novedosas, como la desviación del número de clics observados para un par de consulta-URL dado del número esperado de clics en un resultado en la posición dada. También modelamos el comportamiento de navegación después de que se hace clic en un resultado, por ejemplo, el tiempo promedio de permanencia en la página para un par de consulta-URL dado, así como su desviación del tiempo de permanencia esperado (promedio). Además, el conjunto de características fue diseñado para proporcionar información esencial sobre la experiencia del usuario y hacer que la interpretación de la retroalimentación sea sólida. Por ejemplo, los usuarios de búsqueda en la web a menudo pueden determinar si un resultado es relevante al mirar el título del resultado, la URL y el resumen; en muchos casos, no es necesario mirar el documento original. Para modelar este aspecto de la experiencia del usuario, incluimos características como la superposición de palabras en el título y las palabras en la consulta (TitleOverlap) y la fracción de palabras compartidas por la consulta y el resumen del resultado. Características de clics Posición Posición de la URL en el ranking actual Frecuencia de clics Número de clics para esta consulta, par URL Probabilidad de clic Probabilidad de un clic para esta consulta y URL Desviación de clic Desviación de la probabilidad de clic esperada ¿Es el siguiente clic? 1 si se hizo clic en la siguiente posición, 0 de lo contrario ¿Es el clic anterior? 1 si se hizo clic en la posición anterior, 0 de lo contrario ¿Hay clic arriba? 1 si hay un clic arriba, 0 de lo contrario ¿Hay clic abajo? 1 si hay un clic abajo, 0 de lo contrario Características de navegación Tiempo en la página Tiempo de permanencia en la página Tiempo acumulado en la página Tiempo acumulado para todas las páginas siguientes después de la búsqueda Tiempo en el dominio Tiempo de permanencia acumulado para este dominio Tiempo en URL corta Tiempo acumulado en el prefijo de la URL, sin parámetros ¿Se siguió el enlace? 1 si se siguió el enlace al resultado, 0 de lo contrario ¿Coincidencia exacta de URL? 0 si se utilizó normalización agresiva, 1 de lo contrario ¿Redirigido? 1 si la URL inicial es la misma que la URL final, 0 de lo contrario ¿Camino desde la búsqueda? 1 si solo se siguieron enlaces después de la consulta, 0 de lo contrario Clics desde la búsqueda Número de saltos para llegar a la página desde la consulta Tiempo promedio de permanencia Tiempo promedio en la página para esta consulta Desviación del tiempo de permanencia Desviación del tiempo de permanencia promedio en la página Desviación acumulada Desviación del tiempo de permanencia acumulado promedio Desviación del dominio Desviación del tiempo de permanencia promedio en el dominio Características de texto de consulta Coincidencia de título Palabras compartidas entre la consulta y el título Coincidencia de resumen Palabras compartidas entre la consulta y el fragmento Coincidencia de URL de consulta Palabras compartidas entre la consulta y la URL Coincidencia de dominio de consulta Palabras compartidas entre la consulta y el dominio de la URL Longitud de la consulta Número de tokens en la consulta Superposición con la siguiente consulta Fracción de palabras compartidas con la siguiente consulta Tabla 4.1: Algunas características utilizadas para representar el historial de navegación posterior a la búsqueda para una consulta dada y una URL de resultado de búsqueda. Habiendo descrito nuestro conjunto de características, revisamos brevemente nuestro método general para derivar un modelo de comportamiento del usuario. 4.2 Derivación de un Modelo de Retroalimentación del Usuario Para aprender a interpretar el comportamiento observado del usuario, correlacionamos las acciones del usuario (es decir, las características en la Tabla 4.1 que representan las acciones) con los juicios explícitos del usuario para un conjunto de consultas de entrenamiento. Encontramos todas las instancias en nuestros registros de sesión donde se enviaron estas consultas al motor de búsqueda, y agregamos las características del comportamiento del usuario para todas las sesiones de búsqueda que involucran estas consultas. Cada par de consulta-URL observado está representado por las características en la Tabla 4.1, con valores promediados en todas las sesiones de búsqueda, y asignado una de las seis etiquetas de relevancia posibles, que van desde Perfecto hasta Malo, según los juicios explícitos de relevancia. Estos vectores de características etiquetados se utilizan como entrada para el algoritmo de entrenamiento RankNet (Sección 3.3), el cual produce un modelo de comportamiento de usuario entrenado. Este enfoque es particularmente atractivo ya que no requiere heurísticas más allá de la ingeniería de características. El modelo de comportamiento del usuario resultante se utiliza para ayudar a clasificar los resultados de búsqueda en la web, ya sea directamente o en combinación con otras características, como se describe a continuación. 5. CONFIGURACIÓN EXPERIMENTAL El objetivo final de incorporar retroalimentación implícita en la clasificación es mejorar la relevancia de los resultados de búsqueda web devueltos. Por lo tanto, comparamos los métodos de clasificación sobre un gran conjunto de consultas evaluadas con etiquetas de relevancia explícitas proporcionadas por jueces humanos. Para que la evaluación sea realista, obtuvimos una muestra aleatoria de consultas de registros de búsqueda web de un motor de búsqueda importante, con resultados asociados y rastros de acciones de usuario. Describimos este conjunto de datos en detalle a continuación. Nuestros indicadores se describen en la Sección 5.2 que utilizamos para evaluar las alternativas de clasificación, enumeradas en la Sección 5.3 en los experimentos de la Sección 6. 5.1 Conjuntos de datos Comparamos nuestros métodos de clasificación en una muestra aleatoria de 3,000 consultas de los registros de consultas del motor de búsqueda. Las consultas fueron extraídas de los registros de manera uniforme al azar por token sin reemplazo, lo que resultó en una muestra de consultas representativa de la distribución general de consultas. En promedio, 30 resultados fueron etiquetados explícitamente por jueces humanos utilizando una escala de seis puntos que va desde Perfecto hasta Malo. En total, hubo más de 83,000 resultados con juicios de relevancia explícitos. Para calcular diversas estadísticas, se considerarán relevantes los documentos con la etiqueta Buena o mejor, y los documentos con etiquetas inferiores se considerarán no relevantes. Se debe tener en cuenta que los experimentos se realizaron sobre los resultados ya altamente clasificados por un motor de búsqueda web, lo cual corresponde a una experiencia de usuario típica que se limita al pequeño número de resultados altamente clasificados para una consulta típica de búsqueda web. Las interacciones de los usuarios fueron recopiladas durante un período de 8 semanas utilizando información voluntaria de participación. En total, se registraron más de 1.2 millones de consultas únicas, lo que resultó en más de 12 millones de interacciones individuales con el motor de búsqueda. Los datos consistían en las interacciones de los usuarios con el motor de búsqueda web (por ejemplo, hacer clic en un enlace de resultado, regresar a los resultados de búsqueda, etc.) realizadas después de enviar una consulta. Estas acciones fueron agregadas entre usuarios y sesiones de búsqueda y convertidas en características en la Tabla 4.1. Para crear los conjuntos de consultas de entrenamiento, validación y prueba, creamos tres divisiones aleatorias diferentes de 1,500 consultas de entrenamiento, 500 de validación y 1000 de prueba. Las divisiones se realizaron aleatoriamente por consulta, de modo que no hubiera superposición en las consultas de entrenamiento, validación y prueba. 5.2 Métricas de Evaluación Evaluamos los algoritmos de clasificación en una variedad de métricas de recuperación de información aceptadas, a saber, Precisión en K (P(K)), Ganancia Acumulada Descontada Normalizada (NDCG) y Precisión Promedio Media (MAP). Cada métrica se enfoca en un aspecto diferente del rendimiento del sistema, como describimos a continuación. • Precisión en K: Como la métrica más intuitiva, P(K) informa la fracción de documentos clasificados en los primeros K resultados que están etiquetados como relevantes. En nuestro entorno, requerimos que un documento relevante sea etiquetado como Bueno o superior. La posición de los documentos relevantes dentro de los primeros K no es relevante, por lo que esta métrica mide la satisfacción general del usuario con los resultados principales de K. • NDCG en K: NDCG es una medida de recuperación diseñada específicamente para la evaluación de búsqueda en la web [10]. Para una consulta dada q, los resultados clasificados se examinan desde el mejor clasificado hacia abajo, y el NDCG se calcula como: = +−= K j jr qq jMN 1 )( )1log(/)12( Donde Mq es una constante de normalización calculada de manera que un orden perfecto obtendría un NDCG de 1; y cada r(j) es una etiqueta de relevancia entera (0=Malo y 5=Perfecto) del resultado devuelto en la posición j. Ten en cuenta que los documentos sin etiquetar y los documentos malos no contribuyen a la suma, pero reducirán el NDCG para la consulta al empujar hacia abajo los documentos etiquetados relevantes, disminuyendo sus contribuciones. NDCG es muy adecuado para la evaluación de búsquedas en la web, ya que recompensa de manera más intensa los documentos relevantes en los resultados mejor clasificados que aquellos clasificados más bajos. • MAP: La precisión promedio para cada consulta se define como la media de la precisión en los valores de K calculados después de que se recuperó cada documento relevante. El valor MAP final se define como la media de las precisiones promedio de todas las consultas en el conjunto de pruebas. Esta métrica es el resumen de un solo valor más comúnmente utilizado de una ejecución sobre un conjunto de consultas. 5.3 Métodos de clasificación comparados. Recuerde que nuestro objetivo es cuantificar la efectividad del comportamiento implícito para la búsqueda web real. Una dimensión es comparar la utilidad de la retroalimentación implícita con otra información disponible para un motor de búsqueda web. Específicamente, comparamos la efectividad de los comportamientos implícitos de los usuarios con la coincidencia basada en el contenido, las características de calidad de la página estática y combinaciones de todas las características. • BM25F: Como referencia sólida de búsqueda web, utilizamos la puntuación BM25F, que se utilizó en uno de los sistemas con mejor rendimiento en la pista web TREC 2004 [23,27]. BM25F y sus variantes han sido extensamente descritas y evaluadas en la literatura de IR, por lo tanto, sirven como una línea base sólida y reproducible. La variante BM25F que utilizamos en nuestros experimentos calcula puntuaciones de coincidencia separadas para cada campo de un documento de resultado (por ejemplo, texto del cuerpo, título y texto del enlace) e incorpora información de enlaces independiente de la consulta (por ejemplo, PageRank, ClickDistance y profundidad de URL). La función de puntuación y la ajuste específico del campo se describen en detalle en [23]. Tenga en cuenta que BM25F no considera directamente la retroalimentación explícita o implícita para ajuste. • RN: La clasificación producida por un clasificador de redes neuronales (RankNet, descrito en la Sección 3.3) que aprende a clasificar los resultados de búsqueda web al incorporar BM25F y un gran número de características estáticas y dinámicas adicionales que describen cada resultado de búsqueda. Este sistema aprende automáticamente los pesos de todas las características (incluido el puntaje BM25F para un documento) basándose en etiquetas humanas explícitas para un gran conjunto de consultas. Un sistema que incorpora una implementación de RankNet actualmente está siendo utilizado por un importante motor de búsqueda y puede considerarse representativo del estado del arte en la búsqueda web. • BM25F-RerankCT: La clasificación producida al incorporar estadísticas de clics para reordenar los resultados de búsqueda web clasificados por BM25F arriba. El clic es un caso especial particularmente importante de retroalimentación implícita, y se ha demostrado que se correlaciona con la relevancia de los resultados. Este es un caso especial del método de clasificación en la Sección 3.1, con el peso wI establecido en 1000 y la clasificación Id es simplemente el número de clics en el resultado correspondiente a d. En efecto, esta clasificación coloca en la parte superior todos los resultados de búsqueda web devueltos con al menos un clic (y los ordena en orden decreciente por número de clics). La clasificación relativa de los resultados restantes no cambia y se insertan debajo de todos los resultados clicados. Este método sirve como nuestro método de reordenamiento de retroalimentación implícita de referencia. La clasificación producida al reordenar los resultados de BM25F utilizando todas las características del comportamiento del usuario (Sección 4). Este método aprende un modelo de preferencias de usuario correlacionando los valores de las características con etiquetas de relevancia explícitas utilizando el algoritmo de red neuronal RankNet (Sección 4.2). En tiempo de ejecución, para una consulta dada se calcula la puntuación implícita Ir para cada resultado r con <br>características de interacción de usuario</br> disponibles, y se produce el ranking implícito. La clasificación combinada se calcula como se describe en la Sección 3.1. Basándonos en los experimentos realizados sobre el conjunto de desarrollo, fijamos el valor de wI en 3 (el efecto del parámetro wI para este clasificador resultó ser insignificante). • BM25F+All: Clasificación derivada al entrenar el aprendiz RankNet (Sección 3.3) sobre el conjunto de características del puntaje BM25F, así como todas las características de retroalimentación implícita (Sección 3.2). Utilizamos la implementación de 2 capas de RankNet [5] entrenada en las consultas y etiquetas de los conjuntos de entrenamiento y validación. • RN+All: Clasificación derivada al entrenar el algoritmo de clasificación RankNet de 2 capas (Sección 3.3) sobre la unión de todas las características de retroalimentación de contenido, dinámicas e implícitas (es decir, todas las características descritas anteriormente, así como todas las nuevas características de retroalimentación implícita que introdujimos). Los métodos de clasificación anteriores abarcan el rango de la información utilizada para clasificar, desde no utilizar la retroalimentación implícita o explícita en absoluto (es decir, BM25F) hasta un motor de búsqueda web moderno que utiliza cientos de características y está ajustado en base a juicios explícitos (RN). Como mostraremos a continuación, incorporar el comportamiento del usuario en estos sistemas de clasificación mejora drásticamente la relevancia de los documentos devueltos. 6. RESULTADOS EXPERIMENTALES El feedback implícito para la clasificación de búsquedas en la web puede ser explotado de diversas formas. Comparamos métodos alternativos para explotar la retroalimentación implícita, tanto reordenando los resultados principales (es decir, los métodos BM25F-RerankCT y BM25F-RerankAll que reordenan los resultados de BM25F), como integrando directamente las características implícitas en el proceso de clasificación (es decir, los métodos RN+ALL y BM25F+All que aprenden a clasificar los resultados sobre la retroalimentación implícita y otras características). Comparamos nuestros métodos con baselines sólidos (BM25F y RN) sobre las medidas NDCG, Precisión en K y MAP definidas en la Sección 5.2. Los resultados se promediaron en tres divisiones aleatorias del conjunto de datos completo. Cada división contenía 1500 consultas de entrenamiento, 500 de validación y 1000 de prueba, todos los conjuntos de consultas disjuntos. Primero presentamos los resultados de las 1000 consultas de prueba (es decir, incluyendo las consultas para las cuales no hay medidas implícitas, por lo que utilizamos las clasificaciones web originales). Luego profundizamos para examinar los efectos en la reorganización de los intentos de búsqueda con más detalle, analizando dónde el feedback implícito resultó más beneficioso. Primero experimentamos con diferentes métodos de volver a clasificar la salida de los resultados de búsqueda de BM25F. Las figuras 6.1 y 6.2 informan sobre NDCG y Precisión para BM25F, así como para las estrategias de volver a clasificar los resultados con retroalimentación del usuario (Sección 3.1). Incorporar todos los comentarios de los usuarios (ya sea en el marco de reordenamiento o como características directamente al aprendiz) resulta en mejoras significativas (utilizando una prueba t de dos colas con p=0.01) tanto sobre la clasificación original de BM25F como sobre el reordenamiento solo con clics. La mejora es consistente en los 10 mejores resultados y es mayor para el mejor resultado: NDCG en 1 para BM25F+All es de 0.622 en comparación con 0.518 de los resultados originales, y la precisión en 1 también aumenta de 0.5 a 0.63. Basándonos en estos resultados, utilizaremos el clasificador de combinación de características directas (es decir, BM25F+All) para comparaciones posteriores que involucren retroalimentación implícita. Curiosamente, el uso solo de clics, aunque proporciona un beneficio significativo sobre la clasificación original de BM25F, no es tan efectivo como considerar el conjunto completo de características en la Tabla 4.1. Mientras analizamos el comportamiento del usuario (y las características de los componentes más efectivos) en un documento separado [1], vale la pena dar un ejemplo concreto del tipo de ruido inherente en la retroalimentación real de los usuarios en el entorno de búsqueda web. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 Posición del resultado Frecuencia de clics relativa PTR=2 PTR=3 PTR=5 Figura 6.3: Frecuencia relativa de clics para consultas con diferentes Posiciones del Resultado Relevante Superior (PTR). Si los usuarios solo consideraran la relevancia de un resultado para su consulta, harían clic en los resultados más relevantes en la parte superior. Desafortunadamente, como han demostrado Joachims y otros, la presentación también influye de manera bastante dramática en los resultados en los que los usuarios hacen clic. Los usuarios a menudo hacen clic en los resultados por encima del relevante, presumiblemente porque los resúmenes cortos no proporcionan suficiente información para hacer evaluaciones precisas de relevancia y han aprendido que, en promedio, los elementos mejor clasificados son relevantes. La Figura 6.3 muestra las frecuencias relativas de clics para consultas con elementos relevantes conocidos en posiciones distintas a la primera posición; la posición del resultado relevante superior (PTR) varía de 2 a 10 en la figura. Por ejemplo, para consultas con el primer resultado relevante en la posición 5 (PTR=5), hay más clics en los resultados no relevantes en posiciones más altas que en el primer resultado relevante en la posición 5. Como veremos, el aprendizaje sobre un conjunto de características de comportamiento más amplio resulta en una mejora sustancial en la precisión en comparación con solo el clic. Ahora consideramos incorporar el comportamiento del usuario en un conjunto de características mucho más amplio, RN (Sección 5.3) utilizado por un importante motor de búsqueda web. RN incorpora BM25F, características basadas en enlaces y cientos de otras características. La Figura 6.4 informa sobre NDCG en K y la Figura 6.5 informa sobre Precisión en K. Curiosamente, aunque las clasificaciones originales de RN son significativamente más precisas que BM25F solo, la incorporación de características de retroalimentación implícita (BM25F+All) da como resultado una clasificación que supera significativamente a las clasificaciones originales de RN. En otras palabras, la retroalimentación implícita incorpora suficiente información para reemplazar las cientos de otras características disponibles para el aprendiz de RankNet entrenado en el conjunto de características RN. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 1 2 3 4 5 6 7 8 9 10K NDCG RN RN+All BM25 BM25+All Figura 6.4: NDCG en K para BM25F, BM25F+All, RN y RN+All para diferentes K Además, enriquecer las características de RN con el conjunto de retroalimentación implícita muestra una ganancia significativa en todas las medidas, permitiendo que RN+All supere a todos los demás métodos. Esto demuestra la naturaleza complementaria de la retroalimentación implícita con otras características disponibles para un motor de búsqueda web de última generación. 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 Precisión RN RN+Todo BM25 BM25+Todo Figura 6.5: Precisión en K para BM25F, BM25F+Todo, RN y RN+Todo para diferentes valores de K. Resumimos el rendimiento de los diferentes métodos de clasificación en la Tabla 6.1. Informamos el puntaje de Precisión Promedio Media (MAP) para cada sistema. Aunque no es intuitivo de interpretar, el MAP permite la comparación cuantitativa en una sola métrica. Las ganancias marcadas con * son significativas a un nivel de p=0.01 utilizando una prueba t de dos colas. Ganancia MAP P(1) Ganancia BM25F 0.184 - 0.503BM25F-Rerank-CT 0.215 0.031* 0.577 0.073* BM25F-RerankImplicit 0.218 0.003 0.605 0.028* BM25F+Implicit 0.222 0.004 0.620 0.015* RN 0.215 - 0.597RN+All 0.248 0.033* 0.629 0.032* Tabla 6.1: Precisión Promedio (MAP) para todas las estrategias. Hasta ahora hemos informado resultados promediados en todas las consultas del conjunto de pruebas. Desafortunadamente, menos de la mitad tuvo interacciones suficientes para intentar un nuevo ranking. De las 1000 consultas en la prueba, entre el 46% y el 49%, dependiendo de la división entre entrenamiento y prueba, tenían suficiente información de interacción para hacer predicciones (es decir, hubo al menos 1 sesión de búsqueda en la que el usuario hizo clic en al menos 1 URL de resultado). Esto no es sorprendente: la búsqueda en la web tiene una distribución de cola pesada y hay muchas consultas únicas. Ahora consideramos el rendimiento en las consultas para las cuales estaban disponibles las interacciones de los usuarios. La Figura 6.6 informa sobre el NDCG para el subconjunto de las consultas de prueba con las características de retroalimentación implícita. Las ganancias en el primer puesto son dramáticas. El NDCG en 1 de BM25F+All aumenta de 0.6 a 0.75 (un aumento relativo del 31%), logrando un rendimiento comparable al de RN+All operando sobre un conjunto de características mucho más rico. 0.6 0.65 0.7 0.75 0.8 1 3 5 10K NDCG RN RN+All BM25 BM25+All Figura 6.6: NDCG en K para BM25F, BM25F+All, RN y RN+All en consultas de prueba con interacciones de usuario. Del mismo modo, las ganancias en precisión en el top 1 son sustanciales (Figura 6.7) y es probable que sean evidentes para los usuarios de búsqueda web. Cuando se dispone de retroalimentación implícita, el sistema BM25F+All devuelve el documento relevante en la parte superior 1 casi el 70% del tiempo, en comparación con el 53% del tiempo cuando la retroalimentación implícita no es considerada por el sistema BM25F original. 0.45 0.5 0.55 0.6 0.65 0.7 1 3 5 10K Precisión RN RN+All BM25 BM25+All Figura 6.7: Precisión en K NDCG en K para BM25F, BM25F+All, RN y RN+All en consultas de prueba con interacciones de usuario Resumimos los resultados en la medida MAP para las consultas intentadas en la Tabla 6.2. Las mejoras en MAP son tanto sustanciales como significativas, siendo más pronunciadas que las mejoras en el clasificador BM25F. Ahora analizamos los casos en los que la retroalimentación implícita resultó ser más útil. La Figura 6.8 informa las mejoras de MAP sobre la ejecución de BM25F base para cada consulta con MAP inferior a 0.6. Ten en cuenta que la mayoría de la mejora es para consultas con bajo rendimiento (es decir, MAP < 0.1). Curiosamente, la incorporación de información sobre el comportamiento del usuario disminuye la precisión para consultas con un puntaje MAP original alto. Una posible explicación es que estas consultas fáciles tienden a ser de navegación (es decir, tener una única respuesta apropiada altamente clasificada), y las interacciones de los usuarios con resultados de menor rango pueden indicar necesidades de información divergentes que son mejor atendidas por los resultados menos populares (con calificaciones de relevancia global correspondientemente bajas). Para resumir nuestros resultados experimentales, la incorporación de retroalimentación implícita en un entorno real de búsqueda web resultó en mejoras significativas sobre las clasificaciones originales, utilizando tanto BM25F como RN como líneas de base. Nuestro amplio conjunto de características implícitas, como el tiempo en la página y las desviaciones del comportamiento promedio, ofrece ventajas sobre el uso exclusivo del clic como indicador de interés. Además, incorporar características de retroalimentación implícita directamente en la función de clasificación aprendida es más efectivo que utilizar la retroalimentación implícita para volver a clasificar. Las mejoras observadas en grandes conjuntos de pruebas de consultas (1,000 en total, entre 466 y 495 con retroalimentación implícita disponible) son tanto sustanciales como estadísticamente significativas. 7. CONCLUSIONES Y TRABAJOS FUTUROS En este artículo exploramos la utilidad de incorporar retroalimentación implícita ruidosa obtenida en un entorno real de búsqueda web para mejorar la clasificación de búsqueda web. Realizamos una evaluación a gran escala con más de 3,000 consultas y más de 12 millones de interacciones de usuarios con un motor de búsqueda importante, estableciendo la utilidad de incorporar retroalimentación implícita ruidosa para mejorar la relevancia de la búsqueda web. Comparamos dos alternativas para incorporar retroalimentación implícita en el proceso de búsqueda, a saber, reordenar con retroalimentación implícita e incorporar directamente características de retroalimentación implícita en la función de clasificación entrenada. Nuestros experimentos mostraron una mejora significativa sobre los métodos que no consideran la retroalimentación implícita. Las ganancias son particularmente dramáticas para el resultado superior K=1 en la clasificación final, con mejoras de precisión de hasta un 31%, y las ganancias son sustanciales para todos los valores de K. Nuestros experimentos mostraron que el feedback implícito del usuario puede mejorar aún más el rendimiento de la búsqueda web, cuando se incorpora directamente con características populares basadas en contenido y enlaces. Curiosamente, la retroalimentación implícita es especialmente valiosa para consultas con una clasificación original deficiente de resultados (por ejemplo, MAP inferior a 0.1). Una dirección prometedora para trabajos futuros es aplicar la investigación reciente sobre la predicción automática de la dificultad de las consultas, e intentar incorporar únicamente retroalimentación implícita para las consultas difíciles. Como otra dirección de investigación, estamos explorando métodos para extender nuestras predicciones a las consultas previamente no vistas (por ejemplo, el agrupamiento de consultas), lo cual debería mejorar aún más la experiencia de búsqueda en la web de los usuarios. AGRADECIMIENTOS Agradecemos a Chris Burges y Matt Richardson por la implementación de RankNet para nuestros experimentos. También agradecemos a Robert Ragno por sus valiosas sugerencias y muchas discusiones. 8. REFERENCIAS [1] E. Agichtein, E. Brill, S. Dumais y R. Ragno, Aprendizaje de modelos de <br>interacción del usuario</br> para predecir las preferencias de resultados de búsqueda web. En Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2006 [2] J. Allan, Resumen de la Pista HARD en TREC 2003, Recuperación de Alta Precisión de Documentos, 2003 [3] R. Baeza-Yates y B. Ribeiro-Neto, Recuperación de Información Moderna, Addison-Wesley, 1999. [4] S. Brin y L. Page, Anatomía de un Motor de Búsqueda Web Hipertextual a Gran Escala, en Actas de WWW, 1997 [5] C.J.C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. Hullender, Aprendizaje para clasificación utilizando descenso de gradiente, en Actas de la Conferencia Internacional sobre Aprendizaje Automático, 2005 [6] D.M. Chickering, The WinMine Toolkit, Informe Técnico de Microsoft MSR-TR-2002-103, 2002 [7] M. Claypool, D. Brown, P. Lee y M. Waseda. Inferir el interés del usuario. IEEE Internet Computing. 2001 [8] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais y T. White. Evaluando medidas implícitas para mejorar la experiencia de búsqueda. En ACM Transactions on Information Systems, 2005 [9] J. Goecks y J. Shavlick. Aprendiendo los intereses de los usuarios observando de manera discreta su comportamiento normal. En Actas del Taller de IJCAI sobre Aprendizaje Automático para Filtrado de Información. 1999. [10] K Jarvelin y J. Kekalainen. Métodos de evaluación IR para recuperar documentos altamente relevantes. En las Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2000 [11] T. Joachims, Optimización de Motores de Búsqueda Utilizando Datos de Clics. En Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (SIGKDD), 2002 [12] T. Joachims, L. Granka, B. Pang, H. Hembrooke y G. Gay, Interpretación precisa de los datos de clics como retroalimentación implícita, Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2005 [13] T. Joachims, Haciendo práctico el aprendizaje SVM a gran escala. Avances en Métodos de Núcleo, en Aprendizaje de Vectores de Soporte, MIT Press, 1999 [14] D. Kelly y J. Teevan, Retroalimentación implícita para inferir preferencias de usuario: una bibliografía. En el Foro SIGIR, 2003 [15] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon y J. Riedl. GroupLens: Aplicando filtrado colaborativo a las noticias de Usenet. En Comunicaciones de ACM, 1997. [16] M. Morita y Y. Shinoda, Filtrado de información basado en análisis del comportamiento del usuario y recuperación de texto de mejor coincidencia. Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 1994 [17] D. Oard y J. Kim. Retroalimentación implícita para sistemas de recomendación. En Actas del Taller de Sistemas de Recomendación de la AAAI. 1998 [18] D. Oard y J. Kim. Modelando el contenido de la información utilizando el comportamiento observable. En Actas de la 64ª Reunión Anual de la Sociedad Americana de Ciencia de la Información y Tecnología. 2001 [19] N. Pharo, N. y K. Järvelin. El método SST: una herramienta para analizar los procesos de búsqueda de información en la web. En Information Processing & Management, 2004 [20] P. Pirolli, El Uso de la Pista de Información Próxima para Buscar Contenido Distal en la World Wide Web. Trabajando con la tecnología en mente: Brunswikiano. Recursos para Ciencia Cognitiva e Ingeniería, Oxford University Press, 2004 [21] F. Radlinski y T. Joachims, Cadenas de Consulta: Aprendizaje para Clasificar a partir de Retroalimentación Implícita. En Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (SIGKDD), 2005. [22] F. Radlinski y T. Joachims, Evaluando la Robustez del Aprendizaje a partir de Retroalimentación Implícita, en Actas del Taller de ICML sobre Aprendizaje en Búsqueda Web, 2005 [23] S. E. Robertson, H. Zaragoza y M. Taylor, Extensión simple de BM25 a múltiples campos ponderados, en Actas de la Conferencia sobre Información y Gestión del Conocimiento (CIKM), 2004 [24] G. Salton y M. McGill. Introducción a la recuperación de información moderna. McGraw-Hill, 1983 [25] E.M. Voorhees, D. Harman, Resumen de TREC, 2001 [26] G.R. Xue, H.J. Zeng, Z. Chen, Y. Yu, W.Y. -> Zeng, Z. Chen, Y. Yu, W.Y. Ma, W.S. Xi, y W.G. Fan, Optimizing web search using web clickthrough data, en Actas de la Conferencia sobre Información y Gestión del Conocimiento (CIKM), 2004 [27] H. Zaragoza, N. Craswell, M. Taylor, S. Saria y S. Robertson. Microsoft Cambridge en TREC 13: Pistas Web y Duras. En Actas de TREC 2004 ",
            "candidates": [],
            "error": [
                [
                    "características de interacción del usuario",
                    "características de interacción de usuario",
                    "interacción del usuario"
                ]
            ]
        },
        "information": {
            "translated_key": "información",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Improving Web Search Ranking by Incorporating User Behavior <br>information</br> Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com ABSTRACT We show that incorporating user behavior data can significantly improve ordering of top results in real web search setting.",
                "We examine alternatives for incorporating feedback into the ranking process and explore the contributions of user feedback compared to other common web search features.",
                "We report results of a large scale evaluation over 3,000 queries and 12 million user interactions with a popular web search engine.",
                "We show that incorporating implicit feedback can augment other features, improving the accuracy of a competitive web search ranking algorithms by as much as 31% relative to the original performance.",
                "Categories and Subject Descriptors H.3.3 <br>information</br> Search and Retrieval - Relevance feedback, search process; H.3.5 Online <br>information</br> Services - Web-based services.",
                "General Terms Algorithms, Measurement, Experimentation 1.",
                "INTRODUCTION Millions of users interact with search engines daily.",
                "They issue queries, follow some of the links in the results, click on ads, spend time on pages, reformulate their queries, and perform other actions.",
                "These interactions can serve as a valuable source of <br>information</br> for tuning and improving web search result ranking and can compliment more costly explicit judgments.",
                "Implicit relevance feedback for ranking and personalization has become an active area of research.",
                "Recent work by Joachims and others exploring implicit feedback in controlled environments have shown the value of incorporating implicit feedback into the ranking process.",
                "Our motivation for this work is to understand how implicit feedback can be used in a large-scale operational environment to improve retrieval.",
                "How does it compare to and compliment evidence from page content, anchor text, or link-based features such as inlinks or PageRank?",
                "While it is intuitive that user interactions with the web search engine should reveal at least some <br>information</br> that could be used for ranking, estimating user preferences in real web search settings is a challenging problem, since real user interactions tend to be more noisy than commonly assumed in the controlled settings of previous studies.",
                "Our paper explores whether implicit feedback can be helpful in realistic environments, where user feedback can be noisy (or adversarial) and a web search engine already uses hundreds of features and is heavily tuned.",
                "To this end, we explore different approaches for ranking web search results using real user behavior obtained as part of normal interactions with the web search engine.",
                "The specific contributions of this paper include: • Analysis of alternatives for incorporating user behavior into web search ranking (Section 3). • An application of a robust implicit feedback model derived from mining millions of user interactions with a major web search engine (Section 4). • A large scale evaluation over real user queries and search results, showing significant improvements derived from incorporating user feedback (Section 6).",
                "We summarize our findings and discuss extensions to the current work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in <br>information</br> retrieval.",
                "Most common approaches primarily focus on similarity of query and a page, as well as the overall page quality [3,4,24].",
                "However, with increasing popularity of search engines, implicit feedback (i.e., the actions users take when interacting with the search engine) can be used to improve the rankings.",
                "Implicit relevance measures have been studied by several research groups.",
                "An overview of implicit measures is compiled in Kelly and Teevan [14].",
                "This research, while developing valuable insights into implicit relevance measures, was not applied to improve the ranking of web search results in realistic settings.",
                "Closely related to our work, Joachims [11] collected implicit measures in place of explicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "Fox et al. [8] explored the relationship between implicit and explicit measures in Web search, and developed Bayesian models to correlate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "This work considered a wide range of user behaviors (e.g., dwell time, scroll time, reformulation patterns) in addition to the popular clickthrough behavior.",
                "However, the modeling effort was aimed at predicting explicit relevance judgments from implicit user actions and not specifically at learning ranking functions.",
                "Other studies of user behavior in web search include Pharo and Järvelin [19], but were not directly applied to improve ranking.",
                "More recently, Joachims et al. [12] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthroughs in a controlled, laboratory setting.",
                "Unfortunately, the extent to which previous research applies to real-world web search is unclear.",
                "At the same time, while recent work (e.g., [26]) on using clickthrough <br>information</br> for improving web search ranking is promising, it captures only one aspect of the user interactions with web search engines.",
                "We build on existing research to develop robust user behavior interpretation techniques for the real web search setting.",
                "Instead of treating each user as a reliable expert, we aggregate <br>information</br> from multiple, unreliable, user search session traces, as we describe in the next two sections. 3.",
                "INCORPORATING IMPLICIT FEEDBACK We consider two complementary approaches to ranking with implicit feedback: (1) treating implicit feedback as independent evidence for ranking results, and (2) integrating implicit feedback features directly into the ranking algorithm.",
                "We describe the two general ranking approaches next.",
                "The specific implicit feedback features are described in Section 4, and the algorithms for interpreting and incorporating implicit feedback are described in Section 5. 3.1 Implicit Feedback as Independent Evidence The general approach is to re-rank the results obtained by a web search engine according to observed clickthrough and other user interactions for the query in previous search sessions.",
                "Each result is assigned a score according to expected relevance/user satisfaction based on previous interactions, resulting in some preference ordering based on user interactions alone.",
                "While there has been significant work on merging multiple rankings, we adapt a simple and robust approach of ignoring the original rankers scores, and instead simply merge the rank orders.",
                "The main reason for ignoring the original scores is that since the feature spaces and learning algorithms are different, the scores are not directly comparable, and re-normalization tends to remove the benefit of incorporating classifier scores.",
                "We experimented with a variety of merging functions on the development set of queries (and using a set of interactions from a different time period from final evaluation sets).",
                "We found that a simple rank merging heuristic combination works well, and is robust to variations in score values from original rankers.",
                "For a given query q, the implicit score ISd is computed for each result d from available user interaction features, resulting in the implicit rank Id for each result.",
                "We compute a merged score SM(d) for d by combining the ranks obtained from implicit feedback, Id with the original rank of d, Od:     ¡     ¢ £ + + + + = otherwise O dforexistsfeedbackimplicitif OI w wOIdS d dd I IddM 1 1 1 1 1 1 ),,,( where the weight wI is a heuristically tuned scaling factor representing the relative importance of the implicit feedback.",
                "The query results are ordered in by decreasing values of SM to produce the final ranking.",
                "One special case of this model arises when setting wI to a very large value, effectively forcing clicked results to be ranked higher than un-clicked results - an intuitive and effective heuristic that we will use as a baseline.",
                "Applying more sophisticated classifier and ranker combination algorithms may result in additional improvements, and is a promising direction for future work.",
                "The approach above assumes that there are no interactions between the underlying features producing the original web search ranking and the implicit feedback features.",
                "We now relax this assumption by integrating implicit feedback features directly into the ranking process. 3.2 Ranking with Implicit Feedback Features Modern web search engines rank results based on a large number of features, including content-based features (i.e., how closely a query matches the text or title or anchor text of the document), and query-independent page quality features (e.g., PageRank of the document or the domain).",
                "In most cases, automatic (or semiautomatic) methods are developed for tuning the specific ranking function that combines these feature values.",
                "Hence, a natural approach is to incorporate implicit feedback features directly as features for the ranking algorithm.",
                "During training or tuning, the ranker can be tuned as before but with additional features.",
                "At runtime, the search engine would fetch the implicit feedback features associated with each query-result URL pair.",
                "This model requires a ranking algorithm to be robust to missing values: more than 50% of queries to web search engines are unique, with no previous implicit feedback available.",
                "We now describe such a ranker that we used to learn over the combined feature sets including implicit feedback. 3.3 Learning to Rank Web Search Results A key aspect of our approach is exploiting recent advances in machine learning, namely trainable ranking algorithms for web search and <br>information</br> retrieval (e.g., [5, 11] and classical results reviewed in [3]).",
                "In our setting, explicit human relevance judgments (labels) are available for a set of web search queries and results.",
                "Hence, an attractive choice to use is a supervised machine learning technique to learn a ranking function that best predicts relevance judgments.",
                "RankNet is one such algorithm.",
                "It is a neural net tuning algorithm that optimizes feature weights to best match explicitly provided pairwise user preferences.",
                "While the specific training algorithms used by RankNet are beyond the scope of this paper, it is described in detail in [5] and includes extensive evaluation and comparison with other ranking methods.",
                "An attractive feature of RankNet is both train- and run-time efficiency - runtime ranking can be quickly computed and can scale to the web, and training can be done over thousands of queries and associated judged results.",
                "We use a 2-layer implementation of RankNet in order to model non-linear relationships between features.",
                "Furthermore, RankNet can learn with many (differentiable) cost functions, and hence can automatically learn a ranking function from human-provided labels, an attractive alternative to heuristic feature combination techniques.",
                "Hence, we will also use RankNet as a generic ranker to explore the contribution of implicit feedback for different ranking alternatives. 4.",
                "IMPLICIT USER FEEDBACK MODEL Our goal is to accurately interpret noisy user feedback obtained as by tracing user interactions with the search engine.",
                "Interpreting implicit feedback in real web search setting is not an easy task.",
                "We characterize this problem in detail in [1], where we motivate and evaluate a wide variety of models of implicit user activities.",
                "The general approach is to represent user actions for each search result as a vector of features, and then train a ranker on these features to discover feature values indicative of relevant (and nonrelevant) search results.",
                "We first briefly summarize our features and model, and the learning approach (Section 4.2) in order to provide sufficient <br>information</br> to replicate our ranking methods and the subsequent experiments. 4.1 Representing User Actions as Features We model observed web search behaviors as a combination of a ``background component (i.e., query- and relevance-independent noise in user behavior, including positional biases with result interactions), and a ``relevance component (i.e., query-specific behavior indicative of relevance of a result to a query).",
                "We design our features to take advantage of aggregated user behavior.",
                "The feature set is comprised of directly observed features (computed directly from observations for each query), as well as queryspecific derived features, computed as the deviation from the overall query-independent distribution of values for the corresponding directly observed feature values.",
                "The features used to represent user interactions with web search results are summarized in Table 4.1.",
                "This <br>information</br> was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "We include the traditional implicit feedback features such as clickthrough counts for the results, as well as our novel derived features such as the deviation of the observed clickthrough number for a given query-URL pair from the expected number of clicks on a result in the given position.",
                "We also model the browsing behavior after a result was clicked - e.g., the average page dwell time for a given query-URL pair, as well as its deviation from the expected (average) dwell time.",
                "Furthermore, the feature set was designed to provide essential <br>information</br> about the user experience to make feedback interpretation robust.",
                "For example, web search users can often determine whether a result is relevant by looking at the result title, URL, and summary - in many cases, looking at the original document is not necessary.",
                "To model this aspect of user experience we include features such as overlap in words in title and words in query (TitleOverlap) and the fraction of words shared by the query and the result summary.",
                "Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickProbability Probability of a click for this query and URL ClickDeviation Deviation from expected click probability IsNextClicked 1 if clicked on next position, 0 otherwise IsPreviousClicked 1 if clicked on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, no parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from average dwell time on page CumulativeDeviation Deviation from average cumulative dwell time DomainDeviation Deviation from average dwell time on domain Query-text features TitleOverlap Words shared between query and title SummaryOverlap Words shared between query and snippet QueryURLOverlap Words shared between query and URL QueryDomainOverlap Words shared between query and URL domain QueryLength Number of tokens in query QueryNextOverlap Fraction of words shared with next query Table 4.1: Some features used to represent post-search navigation history for a given query and search result URL.",
                "Having described our feature set, we briefly review our general method for deriving a user behavior model. 4.2 Deriving a User Feedback Model To learn to interpret the observed user behavior, we correlate user actions (i.e., the features in Table 4.1 representing the actions) with the explicit user judgments for a set of training queries.",
                "We find all the instances in our session logs where these queries were submitted to the search engine, and aggregate the user behavior features for all search sessions involving these queries.",
                "Each observed query-URL pair is represented by the features in Table 4.1, with values averaged over all search sessions, and assigned one of six possible relevance labels, ranging from Perfect to Bad, as assigned by explicit relevance judgments.",
                "These labeled feature vectors are used as input to the RankNet training algorithm (Section 3.3) which produces a trained user behavior model.",
                "This approach is particularly attractive as it does not require heuristics beyond feature engineering.",
                "The resulting user behavior model is used to help rank web search resultseither directly or in combination with other features, as described below. 5.",
                "EXPERIMENTAL SETUP The ultimate goal of incorporating implicit feedback into ranking is to improve the relevance of the returned web search results.",
                "Hence, we compare the ranking methods over a large set of judged queries with explicit relevance labels provided by human judges.",
                "In order for the evaluation to be realistic we obtained a random sample of queries from web search logs of a major search engine, with associated results and traces for user actions.",
                "We describe this dataset in detail next.",
                "Our metrics are described in Section 5.2 that we use to evaluate the ranking alternatives, listed in Section 5.3 in the experiments of Section 6. 5.1 Datasets We compared our ranking methods over a random sample of 3,000 queries from the search engine query logs.",
                "The queries were drawn from the logs uniformly at random by token without replacement, resulting in a query sample representative of the overall query distribution.",
                "On average, 30 results were explicitly labeled by human judges using a six point scale ranging from Perfect down to Bad.",
                "Overall, there were over 83,000 results with explicit relevance judgments.",
                "In order to compute various statistics, documents with label Good or better will be considered relevant, and with lower labels to be non-relevant.",
                "Note that the experiments were performed over the results already highly ranked by a web search engine, which corresponds to a typical user experience which is limited to the small number of the highly ranked results for a typical web search query.",
                "The user interactions were collected over a period of 8 weeks using voluntary opt-in <br>information</br>.",
                "In total, over 1.2 million unique queries were instrumented, resulting in over 12 million individual interactions with the search engine.",
                "The data consisted of user interactions with the web search engine (e.g., clicking on a result link, going back to search results, etc.) performed after a query was submitted.",
                "These actions were aggregated across users and search sessions and converted to features in Table 4.1.",
                "To create the training, validation, and test query sets, we created three different random splits of 1,500 training, 500 validation, and 1000 test queries.",
                "The splits were done randomly by query, so that there was no overlap in training, validation, and test queries. 5.2 Evaluation Metrics We evaluate the ranking algorithms over a range of accepted <br>information</br> retrieval metrics, namely Precision at K (P(K)), Normalized Discounted Cumulative Gain (NDCG), and Mean Average Precision (MAP).",
                "Each metric focuses on a deferent aspect of system performance, as we describe below. • Precision at K: As the most intuitive metric, P(K) reports the fraction of documents ranked in the top K results that are labeled as relevant.",
                "In our setting, we require a relevant document to be labeled Good or higher.",
                "The position of relevant documents within the top K is irrelevant, and hence this metric measure overall user satisfaction with the top K results. • NDCG at K: NDCG is a retrieval measure devised specifically for web search evaluation [10].",
                "For a given query q, the ranked results are examined from the top ranked down, and the NDCG computed as:   = +−= K j jr qq jMN 1 )( )1log(/)12( Where Mq is a normalization constant calculated so that a perfect ordering would obtain NDCG of 1; and each r(j) is an integer relevance label (0=Bad and 5=Perfect) of result returned at position j.",
                "Note that unlabeled and Bad documents do not contribute to the sum, but will reduce NDCG for the query pushing down the relevant labeled documents, reducing their contributions.",
                "NDCG is well suited to web search evaluation, as it rewards relevant documents in the top ranked results more heavily than those ranked lower. • MAP: Average precision for each query is defined as the mean of the precision at K values computed after each relevant document was retrieved.",
                "The final MAP value is defined as the mean of average precisions of all queries in the test set.",
                "This metric is the most commonly used single-value summary of a run over a set of queries. 5.3 Ranking Methods Compared Recall that our goal is to quantify the effectiveness of implicit behavior for real web search.",
                "One dimension is to compare the utility of implicit feedback with other <br>information</br> available to a web search engine.",
                "Specifically, we compare effectiveness of implicit user behaviors with content-based matching, static page quality features, and combinations of all features. • BM25F: As a strong web search baseline we used the BM25F scoring, which was used in one of the best performing systems in the TREC 2004 Web track [23,27].",
                "BM25F and its variants have been extensively described and evaluated in IR literature, and hence serve as a strong, reproducible baseline.",
                "The BM25F variant we used for our experiments computes separate match scores for each field for a result document (e.g., body text, title, and anchor text), and incorporates query-independent linkbased <br>information</br> (e.g., PageRank, ClickDistance, and URL depth).",
                "The scoring function and field-specific tuning is described in detail in [23].",
                "Note that BM25F does not directly consider explicit or implicit feedback for tuning. • RN: The ranking produced by a neural net ranker (RankNet, described in Section 3.3) that learns to rank web search results by incorporating BM25F and a large number of additional static and dynamic features describing each search result.",
                "This system automatically learns weights for all features (including the BM25F score for a document) based on explicit human labels for a large set of queries.",
                "A system incorporating an implementation of RankNet is currently in use by a major search engine and can be considered representative of the state of the art in web search. • BM25F-RerankCT: The ranking produced by incorporating clickthrough statistics to reorder web search results ranked by BM25F above.",
                "Clickthrough is a particularly important special case of implicit feedback, and has been shown to correlate with result relevance.",
                "This is a special case of the ranking method in Section 3.1, with the weight wI set to 1000 and the ranking Id is simply the number of clicks on the result corresponding to d. In effect, this ranking brings to the top all returned web search results with at least one click (and orders them in decreasing order by number of clicks).",
                "The relative ranking of the remainder of results is unchanged and they are inserted below all clicked results.",
                "This method serves as our baseline implicit feedback reranking method.",
                "BM25F-RerankAll The ranking produced by reordering the BM25F results using all user behavior features (Section 4).",
                "This method learns a model of user preferences by correlating feature values with explicit relevance labels using the RankNet neural net algorithm (Section 4.2).",
                "At runtime, for a given query the implicit score Ir is computed for each result r with available user interaction features, and the implicit ranking is produced.",
                "The merged ranking is computed as described in Section 3.1.",
                "Based on the experiments over the development set we fix the value of wI to 3 (the effect of the wI parameter for this ranker turned out to be negligible). • BM25F+All: Ranking derived by training the RankNet (Section 3.3) learner over the features set of the BM25F score as well as all implicit feedback features (Section 3.2).",
                "We used the 2-layer implementation of RankNet [5] trained on the queries and labels in the training and validation sets. • RN+All: Ranking derived by training the 2-layer RankNet ranking algorithm (Section 3.3) over the union of all content, dynamic, and implicit feedback features (i.e., all of the features described above as well as all of the new implicit feedback features we introduced).",
                "The ranking methods above span the range of the <br>information</br> used for ranking, from not using the implicit or explicit feedback at all (i.e., BM25F) to a modern web search engine using hundreds of features and tuned on explicit judgments (RN).",
                "As we will show next, incorporating user behavior into these ranking systems dramatically improves the relevance of the returned documents. 6.",
                "EXPERIMENTAL RESULTS Implicit feedback for web search ranking can be exploited in a number of ways.",
                "We compare alternative methods of exploiting implicit feedback, both by re-ranking the top results (i.e., the BM25F-RerankCT and BM25F-RerankAll methods that reorder BM25F results), as well as by integrating the implicit features directly into the ranking process (i.e., the RN+ALL and BM25F+All methods which learn to rank results over the implicit feedback and other features).",
                "We compare our methods over strong baselines (BM25F and RN) over the NDCG, Precision at K, and MAP measures defined in Section 5.2.",
                "The results were averaged over three random splits of the overall dataset.",
                "Each split contained 1500 training, 500 validation, and 1000 test queries, all query sets disjoint.",
                "We first present the results over all 1000 test queries (i.e., including queries for which there are no implicit measures so we use the original web rankings).",
                "We then drill down to examine the effects on reranking for the attempted queries in more detail, analyzing where implicit feedback proved most beneficial.",
                "We first experimented with different methods of re-ranking the output of the BM25F search results.",
                "Figures 6.1 and 6.2 report NDCG and Precision for BM25F, as well as for the strategies reranking results with user feedback (Section 3.1).",
                "Incorporating all user feedback (either in reranking framework or as features to the learner directly) results in significant improvements (using two-tailed t-test with p=0.01) over both the original BM25F ranking as well as over reranking with clickthrough alone.",
                "The improvement is consistent across the top 10 results and largest for the top result: NDCG at 1 for BM25F+All is 0.622 compared to 0.518 of the original results, and precision at 1 similarly increases from 0.5 to 0.63.",
                "Based on these results we will use the direct feature combination (i.e., BM25F+All) ranker for subsequent comparisons involving implicit feedback. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 1 2 3 4 5 6 7 8 9 10K NDCG BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.1: NDCG at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K 0.35 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.2: Precision at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K Interestingly, using clickthrough alone, while giving significant benefit over the original BM25F ranking, is not as effective as considering the full set of features in Table 4.1.",
                "While we analyze user behavior (and most effective component features) in a separate paper [1], it is worthwhile to give a concrete example of the kind of noise inherent in real user feedback in web search setting. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 Result position Relativeclickfrequency PTR=2 PTR=3 PTR=5 Figure 6.3: Relative clickthrough frequency for queries with varying Position of Top Relevant result (PTR).",
                "If users considered only the relevance of a result to their query, they would click on the topmost relevant results.",
                "Unfortunately, as Joachims and others have shown, presentation also influences which results users click on quite dramatically.",
                "Users often click on results above the relevant one presumably because the short summaries do not provide enough <br>information</br> to make accurate relevance assessments and they have learned that on average topranked items are relevant.",
                "Figure 6.3 shows relative clickthrough frequencies for queries with known relevant items at positions other than the first position; the position of the top relevant result (PTR) ranges from 2-10 in the figure.",
                "For example, for queries with first relevant result at position 5 (PTR=5), there are more clicks on the non-relevant results in higher ranked positions than on the first relevant result at position 5.",
                "As we will see, learning over a richer behavior feature set, results in substantial accuracy improvement over clickthrough alone.",
                "We now consider incorporating user behavior into a much richer feature set, RN (Section 5.3) used by a major web search engine.",
                "RN incorporates BM25F, link-based features, and hundreds of other features.",
                "Figure 6.4 reports NDCG at K and Figure 6.5 reports Precision at K. Interestingly, while the original RN rankings are significantly more accurate than BM25F alone, incorporating implicit feedback features (BM25F+All) results in ranking that significantly outperforms the original RN rankings.",
                "In other words, implicit feedback incorporates sufficient <br>information</br> to replace the hundreds of other features available to the RankNet learner trained on the RN feature set. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 1 2 3 4 5 6 7 8 9 10K NDCG RN RN+All BM25 BM25+All Figure 6.4: NDCG at K for BM25F, BM25F+All, RN, and RN+All for varying K Furthermore, enriching the RN features with implicit feedback set exhibits significant gain on all measures, allowing RN+All to outperform all other methods.",
                "This demonstrates the complementary nature of implicit feedback with other features available to a state of the art web search engine. 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision RN RN+All BM25 BM25+All Figure 6.5: Precision at K for BM25F, BM25F+All, RN, and RN+All for varying K We summarize the performance of the different ranking methods in Table 6.1.",
                "We report the Mean Average Precision (MAP) score for each system.",
                "While not intuitive to interpret, MAP allows quantitative comparison on a single metric.",
                "The gains marked with * are significant at p=0.01 level using two tailed t-test.",
                "MAP Gain P(1) Gain BM25F 0.184 - 0.503BM25F-Rerank-CT 0.215 0.031* 0.577 0.073* BM25F-RerankImplicit 0.218 0.003 0.605 0.028* BM25F+Implicit 0.222 0.004 0.620 0.015* RN 0.215 - 0.597RN+All 0.248 0.033* 0.629 0.032* Table 6.1: Mean Average Precision (MAP) for all strategies.",
                "So far we reported results averaged across all queries in the test set.",
                "Unfortunately, less than half had sufficient interactions to attempt reranking.",
                "Out of the 1000 queries in test, between 46% and 49%, depending on the train-test split, had sufficient interaction <br>information</br> to make predictions (i.e., there was at least 1 search session in which at least 1 result URL was clicked on by the user).",
                "This is not surprising: web search is heavy-tailed, and there are many unique queries.",
                "We now consider the performance on the queries for which user interactions were available.",
                "Figure 6.6 reports NDCG for the subset of the test queries with the implicit feedback features.",
                "The gains at top 1 are dramatic.",
                "The NDCG at 1 of BM25F+All increases from 0.6 to 0.75 (a 31% relative gain), achieving performance comparable to RN+All operating over a much richer feature set. 0.6 0.65 0.7 0.75 0.8 1 3 5 10K NDCG RN RN+All BM25 BM25+All Figure 6.6: NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions Similarly, gains on precision at top 1 are substantial (Figure 6.7), and are likely to be apparent to web search users.",
                "When implicit feedback is available, the BM25F+All system returns relevant document at top 1 almost 70% of the time, compared 53% of the time when implicit feedback is not considered by the original BM25F system. 0.45 0.5 0.55 0.6 0.65 0.7 1 3 5 10K Precision RN RN+All BM25 BM25+All Figure 6.7: Precision at K NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions We summarize the results on the MAP measure for attempted queries in Table 6.2.",
                "MAP improvements are both substantial and significant, with improvements over the BM25F ranker most pronounced.",
                "Method MAP Gain P(1) Gain RN 0.269 0.632 RN+All 0.321 0.051 (19%) 0.693 0.061(10%) BM25F 0.236 0.525 BM25F+All 0.292 0.056 (24%) 0.687 0.162 (31%) Table 6.2: Mean Average Precision (MAP) on attempted queries for best performing methods We now analyze the cases where implicit feedback was shown most helpful.",
                "Figure 6.8 reports the MAP improvements over the baseline BM25F run for each query with MAP under 0.6.",
                "Note that most of the improvement is for poorly performing queries (i.e., MAP < 0.1).",
                "Interestingly, incorporating user behavior <br>information</br> degrades accuracy for queries with high original MAP score.",
                "One possible explanation is that these easy queries tend to be navigational (i.e., having a single, highly-ranked most appropriate answer), and user interactions with lower-ranked results may indicate divergent <br>information</br> needs that are better served by the less popular results (with correspondingly poor overall relevance ratings). 0 50 100 150 200 250 300 350 0.1 0.2 0.3 0.4 0.5 0.6 -0.4 -0.35 -0.3 -0.25 -0.2 -0.15 -0.1 -0.05 0 0.05 0.1 0.15 0.2 Frequency Average Gain Figure 6.8: Gain of BM25F+All over original BM25F ranking To summarize our experimental results, incorporating implicit feedback in real web search setting resulted in significant improvements over the original rankings, using both BM25F and RN baselines.",
                "Our rich set of implicit features, such as time on page and deviations from the average behavior, provides advantages over using clickthrough alone as an indicator of interest.",
                "Furthermore, incorporating implicit feedback features directly into the learned ranking function is more effective than using implicit feedback for reranking.",
                "The improvements observed over large test sets of queries (1,000 total, between 466 and 495 with implicit feedback available) are both substantial and statistically significant. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper we explored the utility of incorporating noisy implicit feedback obtained in a real web search setting to improve web search ranking.",
                "We performed a large-scale evaluation over 3,000 queries and more than 12 million user interactions with a major search engine, establishing the utility of incorporating noisy implicit feedback to improve web search relevance.",
                "We compared two alternatives of incorporating implicit feedback into the search process, namely reranking with implicit feedback and incorporating implicit feedback features directly into the trained ranking function.",
                "Our experiments showed significant improvement over methods that do not consider implicit feedback.",
                "The gains are particularly dramatic for the top K=1 result in the final ranking, with precision improvements as high as 31%, and the gains are substantial for all values of K. Our experiments showed that implicit user feedback can further improve web search performance, when incorporated directly with popular content- and link-based features.",
                "Interestingly, implicit feedback is particularly valuable for queries with poor original ranking of results (e.g., MAP lower than 0.1).",
                "One promising direction for future work is to apply recent research on automatically predicting query difficulty, and only attempt to incorporate implicit feedback for the difficult queries.",
                "As another research direction we are exploring methods for extending our predictions to the previously unseen queries (e.g., query clustering), which should further improve the web search experience of users.",
                "ACKNOWLEDGMENTS We thank Chris Burges and Matt Richardson for an implementation of RankNet for our experiments.",
                "We also thank Robert Ragno for his valuable suggestions and many discussions. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, S. Dumais, and R.Ragno, Learning User Interaction Models for Predicting Web Search Result Preferences.",
                "In Proceedings of the ACM Conference on Research and Development on <br>information</br> Retrieval (SIGIR), 2006 [2] J. Allan, HARD Track Overview in TREC 2003, High Accuracy Retrieval from Documents, 2003 [3] R. Baeza-Yates and B. Ribeiro-Neto, Modern <br>information</br> Retrieval, Addison-Wesley, 1999. [4] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine, in Proceedings of WWW, 1997 [5] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning, 2005 [6] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [7] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest.",
                "IEEE Internet Computing. 2001 [8] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on <br>information</br> Systems, 2005 [9] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for <br>information</br> Filtering. 1999. [10] K Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development on <br>information</br> Retrieval (SIGIR), 2000 [11] T. Joachims, Optimizing Search Engines Using Clickthrough Data.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [12] T. Joachims, L. Granka, B. Pang, H. Hembrooke, and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, Proceedings of the ACM Conference on Research and Development on <br>information</br> Retrieval (SIGIR), 2005 [13] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [14] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [15] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [16] M. Morita, and Y. Shinoda, <br>information</br> filtering based on user behavior analysis and best match text retrieval.",
                "Proceedings of the ACM Conference on Research and Development on <br>information</br> Retrieval (SIGIR), 1994 [17] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems.",
                "In Proceedings of the AAAI Workshop on Recommender Systems. 1998 [18] D. Oard and J. Kim.",
                "Modeling <br>information</br> content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for <br>information</br> Science and Technology. 2001 [19] N. Pharo, N. and K. Järvelin.",
                "The SST method: a tool for analyzing web <br>information</br> search processes.",
                "In <br>information</br> Processing & Management, 2004 [20] P. Pirolli, The Use of Proximal <br>information</br> Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [21] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (SIGKDD), 2005. [22] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in Proceedings of the ICML Workshop on Learning in Web Search, 2005 [23] S. E. Robertson, H. Zaragoza, and M. Taylor, Simple BM25 extension to multiple weighted fields, in Proceedings of the Conference on <br>information</br> and Knowledge Management (CIKM), 2004 [24] G. Salton & M. McGill.",
                "Introduction to modern <br>information</br> retrieval.",
                "McGraw-Hill, 1983 [25] E.M. Voorhees, D. Harman, Overview of TREC, 2001 [26] G.R.",
                "Xue, H.J.",
                "Zeng, Z. Chen, Y. Yu, W.Y.",
                "Ma, W.S.",
                "Xi, and W.G.",
                "Fan, Optimizing web search using web clickthrough data, in Proceedings of the Conference on <br>information</br> and Knowledge Management (CIKM), 2004 [27] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
                "Microsoft Cambridge at TREC 13: Web and Hard Tracks.",
                "In Proceedings of TREC 2004"
            ],
            "original_annotated_samples": [
                "Improving Web Search Ranking by Incorporating User Behavior <br>information</br> Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com ABSTRACT We show that incorporating user behavior data can significantly improve ordering of top results in real web search setting.",
                "Categories and Subject Descriptors H.3.3 <br>information</br> Search and Retrieval - Relevance feedback, search process; H.3.5 Online <br>information</br> Services - Web-based services.",
                "These interactions can serve as a valuable source of <br>information</br> for tuning and improving web search result ranking and can compliment more costly explicit judgments.",
                "While it is intuitive that user interactions with the web search engine should reveal at least some <br>information</br> that could be used for ranking, estimating user preferences in real web search settings is a challenging problem, since real user interactions tend to be more noisy than commonly assumed in the controlled settings of previous studies.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in <br>information</br> retrieval."
            ],
            "translated_annotated_samples": [
                "Mejorando la clasificación de búsqueda web al incorporar <br>información sobre el comportamiento del usuario</br>. Eugene Agichtein, Microsoft Research eugeneag@microsoft.com Eric Brill, Microsoft Research brill@microsoft.com Susan Dumais, Microsoft Research sdumais@microsoft.com RESUMEN Mostramos que la incorporación de datos sobre el comportamiento del usuario puede mejorar significativamente el orden de los principales resultados en una configuración real de búsqueda web.",
                "Categorías y Descriptores de Asignaturas H.3.3 Búsqueda y Recuperación de Información - Retroalimentación de relevancia, proceso de búsqueda; H.3.5 Servicios de Información en Línea - Servicios basados en la web.",
                "Estas interacciones pueden servir como una valiosa fuente de <br>información</br> para ajustar y mejorar la clasificación de los resultados de búsqueda en la web y pueden complementar juicios explícitos más costosos.",
                "Si bien es intuitivo que las interacciones de los usuarios con el motor de búsqueda web deberían revelar al menos alguna <br>información</br> que podría ser utilizada para la clasificación, estimar las preferencias de los usuarios en entornos reales de búsqueda web es un problema desafiante, ya que las interacciones reales tienden a ser más ruidosas de lo que comúnmente se asume en los entornos controlados de estudios previos.",
                "ANTECEDENTES Y TRABAJO RELACIONADO Clasificar los resultados de búsqueda es un problema fundamental en la recuperación de <br>información</br>."
            ],
            "translated_text": "Mejorando la clasificación de búsqueda web al incorporar <br>información sobre el comportamiento del usuario</br>. Eugene Agichtein, Microsoft Research eugeneag@microsoft.com Eric Brill, Microsoft Research brill@microsoft.com Susan Dumais, Microsoft Research sdumais@microsoft.com RESUMEN Mostramos que la incorporación de datos sobre el comportamiento del usuario puede mejorar significativamente el orden de los principales resultados en una configuración real de búsqueda web. Examinamos alternativas para incorporar retroalimentación en el proceso de clasificación y exploramos las contribuciones de la retroalimentación de usuarios en comparación con otras características comunes de búsqueda en la web. Informamos los resultados de una evaluación a gran escala con más de 3,000 consultas y 12 millones de interacciones de usuarios con un motor de búsqueda web popular. Mostramos que la incorporación de retroalimentación implícita puede aumentar otras características, mejorando la precisión de los algoritmos de clasificación de búsqueda web competitivos hasta en un 31% en comparación con el rendimiento original. Categorías y Descriptores de Asignaturas H.3.3 Búsqueda y Recuperación de Información - Retroalimentación de relevancia, proceso de búsqueda; H.3.5 Servicios de Información en Línea - Servicios basados en la web. Términos generales Algoritmos, Medición, Experimentación 1. INTRODUCCIÓN Millones de usuarios interactúan con los motores de búsqueda a diario. Ellos emiten consultas, siguen algunos de los enlaces en los resultados, hacen clic en anuncios, pasan tiempo en páginas, reformulan sus consultas y realizan otras acciones. Estas interacciones pueden servir como una valiosa fuente de <br>información</br> para ajustar y mejorar la clasificación de los resultados de búsqueda en la web y pueden complementar juicios explícitos más costosos. La retroalimentación implícita de relevancia para la clasificación y personalización se ha convertido en un área activa de investigación. El trabajo reciente de Joachims y otros explorando el feedback implícito en entornos controlados ha demostrado el valor de incorporar el feedback implícito en el proceso de clasificación. Nuestra motivación para este trabajo es comprender cómo se puede utilizar la retroalimentación implícita en un entorno operativo a gran escala para mejorar la recuperación. ¿Cómo se compara y complementa con la evidencia del contenido de la página, el texto del ancla o las características basadas en enlaces como inlinks o PageRank? Si bien es intuitivo que las interacciones de los usuarios con el motor de búsqueda web deberían revelar al menos alguna <br>información</br> que podría ser utilizada para la clasificación, estimar las preferencias de los usuarios en entornos reales de búsqueda web es un problema desafiante, ya que las interacciones reales tienden a ser más ruidosas de lo que comúnmente se asume en los entornos controlados de estudios previos. Nuestro artículo explora si la retroalimentación implícita puede ser útil en entornos realistas, donde la retroalimentación del usuario puede ser ruidosa (o adversa) y un motor de búsqueda web ya utiliza cientos de características y está altamente ajustado. Con este fin, exploramos diferentes enfoques para clasificar los resultados de búsqueda web utilizando el comportamiento real de los usuarios obtenido como parte de las interacciones normales con el motor de búsqueda web. Las contribuciones específicas de este artículo incluyen: • Análisis de alternativas para incorporar el comportamiento del usuario en la clasificación de búsqueda web (Sección 3). • Aplicación de un modelo robusto de retroalimentación implícita derivado de la minería de millones de interacciones de usuarios con un importante motor de búsqueda web (Sección 4). • Una evaluación a gran escala sobre consultas reales de usuarios y resultados de búsqueda, mostrando mejoras significativas derivadas de la incorporación de la retroalimentación del usuario (Sección 6). Resumimos nuestros hallazgos y discutimos extensiones al trabajo actual en la Sección 7, que concluye el artículo. ANTECEDENTES Y TRABAJO RELACIONADO Clasificar los resultados de búsqueda es un problema fundamental en la recuperación de <br>información</br>. ",
            "candidates": [],
            "error": [
                [
                    "información sobre el comportamiento del usuario",
                    "información",
                    "información",
                    "información"
                ]
            ]
        },
        "score": {
            "translated_key": "puntuación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Improving Web Search Ranking by Incorporating User Behavior Information Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com ABSTRACT We show that incorporating user behavior data can significantly improve ordering of top results in real web search setting.",
                "We examine alternatives for incorporating feedback into the ranking process and explore the contributions of user feedback compared to other common web search features.",
                "We report results of a large scale evaluation over 3,000 queries and 12 million user interactions with a popular web search engine.",
                "We show that incorporating implicit feedback can augment other features, improving the accuracy of a competitive web search ranking algorithms by as much as 31% relative to the original performance.",
                "Categories and Subject Descriptors H.3.3 Information Search and Retrieval - Relevance feedback, search process; H.3.5 Online Information Services - Web-based services.",
                "General Terms Algorithms, Measurement, Experimentation 1.",
                "INTRODUCTION Millions of users interact with search engines daily.",
                "They issue queries, follow some of the links in the results, click on ads, spend time on pages, reformulate their queries, and perform other actions.",
                "These interactions can serve as a valuable source of information for tuning and improving web search result ranking and can compliment more costly explicit judgments.",
                "Implicit relevance feedback for ranking and personalization has become an active area of research.",
                "Recent work by Joachims and others exploring implicit feedback in controlled environments have shown the value of incorporating implicit feedback into the ranking process.",
                "Our motivation for this work is to understand how implicit feedback can be used in a large-scale operational environment to improve retrieval.",
                "How does it compare to and compliment evidence from page content, anchor text, or link-based features such as inlinks or PageRank?",
                "While it is intuitive that user interactions with the web search engine should reveal at least some information that could be used for ranking, estimating user preferences in real web search settings is a challenging problem, since real user interactions tend to be more noisy than commonly assumed in the controlled settings of previous studies.",
                "Our paper explores whether implicit feedback can be helpful in realistic environments, where user feedback can be noisy (or adversarial) and a web search engine already uses hundreds of features and is heavily tuned.",
                "To this end, we explore different approaches for ranking web search results using real user behavior obtained as part of normal interactions with the web search engine.",
                "The specific contributions of this paper include: • Analysis of alternatives for incorporating user behavior into web search ranking (Section 3). • An application of a robust implicit feedback model derived from mining millions of user interactions with a major web search engine (Section 4). • A large scale evaluation over real user queries and search results, showing significant improvements derived from incorporating user feedback (Section 6).",
                "We summarize our findings and discuss extensions to the current work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "Most common approaches primarily focus on similarity of query and a page, as well as the overall page quality [3,4,24].",
                "However, with increasing popularity of search engines, implicit feedback (i.e., the actions users take when interacting with the search engine) can be used to improve the rankings.",
                "Implicit relevance measures have been studied by several research groups.",
                "An overview of implicit measures is compiled in Kelly and Teevan [14].",
                "This research, while developing valuable insights into implicit relevance measures, was not applied to improve the ranking of web search results in realistic settings.",
                "Closely related to our work, Joachims [11] collected implicit measures in place of explicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "Fox et al. [8] explored the relationship between implicit and explicit measures in Web search, and developed Bayesian models to correlate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "This work considered a wide range of user behaviors (e.g., dwell time, scroll time, reformulation patterns) in addition to the popular clickthrough behavior.",
                "However, the modeling effort was aimed at predicting explicit relevance judgments from implicit user actions and not specifically at learning ranking functions.",
                "Other studies of user behavior in web search include Pharo and Järvelin [19], but were not directly applied to improve ranking.",
                "More recently, Joachims et al. [12] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthroughs in a controlled, laboratory setting.",
                "Unfortunately, the extent to which previous research applies to real-world web search is unclear.",
                "At the same time, while recent work (e.g., [26]) on using clickthrough information for improving web search ranking is promising, it captures only one aspect of the user interactions with web search engines.",
                "We build on existing research to develop robust user behavior interpretation techniques for the real web search setting.",
                "Instead of treating each user as a reliable expert, we aggregate information from multiple, unreliable, user search session traces, as we describe in the next two sections. 3.",
                "INCORPORATING IMPLICIT FEEDBACK We consider two complementary approaches to ranking with implicit feedback: (1) treating implicit feedback as independent evidence for ranking results, and (2) integrating implicit feedback features directly into the ranking algorithm.",
                "We describe the two general ranking approaches next.",
                "The specific implicit feedback features are described in Section 4, and the algorithms for interpreting and incorporating implicit feedback are described in Section 5. 3.1 Implicit Feedback as Independent Evidence The general approach is to re-rank the results obtained by a web search engine according to observed clickthrough and other user interactions for the query in previous search sessions.",
                "Each result is assigned a <br>score</br> according to expected relevance/user satisfaction based on previous interactions, resulting in some preference ordering based on user interactions alone.",
                "While there has been significant work on merging multiple rankings, we adapt a simple and robust approach of ignoring the original rankers scores, and instead simply merge the rank orders.",
                "The main reason for ignoring the original scores is that since the feature spaces and learning algorithms are different, the scores are not directly comparable, and re-normalization tends to remove the benefit of incorporating classifier scores.",
                "We experimented with a variety of merging functions on the development set of queries (and using a set of interactions from a different time period from final evaluation sets).",
                "We found that a simple rank merging heuristic combination works well, and is robust to variations in <br>score</br> values from original rankers.",
                "For a given query q, the implicit <br>score</br> ISd is computed for each result d from available user interaction features, resulting in the implicit rank Id for each result.",
                "We compute a merged <br>score</br> SM(d) for d by combining the ranks obtained from implicit feedback, Id with the original rank of d, Od:     ¡     ¢ £ + + + + = otherwise O dforexistsfeedbackimplicitif OI w wOIdS d dd I IddM 1 1 1 1 1 1 ),,,( where the weight wI is a heuristically tuned scaling factor representing the relative importance of the implicit feedback.",
                "The query results are ordered in by decreasing values of SM to produce the final ranking.",
                "One special case of this model arises when setting wI to a very large value, effectively forcing clicked results to be ranked higher than un-clicked results - an intuitive and effective heuristic that we will use as a baseline.",
                "Applying more sophisticated classifier and ranker combination algorithms may result in additional improvements, and is a promising direction for future work.",
                "The approach above assumes that there are no interactions between the underlying features producing the original web search ranking and the implicit feedback features.",
                "We now relax this assumption by integrating implicit feedback features directly into the ranking process. 3.2 Ranking with Implicit Feedback Features Modern web search engines rank results based on a large number of features, including content-based features (i.e., how closely a query matches the text or title or anchor text of the document), and query-independent page quality features (e.g., PageRank of the document or the domain).",
                "In most cases, automatic (or semiautomatic) methods are developed for tuning the specific ranking function that combines these feature values.",
                "Hence, a natural approach is to incorporate implicit feedback features directly as features for the ranking algorithm.",
                "During training or tuning, the ranker can be tuned as before but with additional features.",
                "At runtime, the search engine would fetch the implicit feedback features associated with each query-result URL pair.",
                "This model requires a ranking algorithm to be robust to missing values: more than 50% of queries to web search engines are unique, with no previous implicit feedback available.",
                "We now describe such a ranker that we used to learn over the combined feature sets including implicit feedback. 3.3 Learning to Rank Web Search Results A key aspect of our approach is exploiting recent advances in machine learning, namely trainable ranking algorithms for web search and information retrieval (e.g., [5, 11] and classical results reviewed in [3]).",
                "In our setting, explicit human relevance judgments (labels) are available for a set of web search queries and results.",
                "Hence, an attractive choice to use is a supervised machine learning technique to learn a ranking function that best predicts relevance judgments.",
                "RankNet is one such algorithm.",
                "It is a neural net tuning algorithm that optimizes feature weights to best match explicitly provided pairwise user preferences.",
                "While the specific training algorithms used by RankNet are beyond the scope of this paper, it is described in detail in [5] and includes extensive evaluation and comparison with other ranking methods.",
                "An attractive feature of RankNet is both train- and run-time efficiency - runtime ranking can be quickly computed and can scale to the web, and training can be done over thousands of queries and associated judged results.",
                "We use a 2-layer implementation of RankNet in order to model non-linear relationships between features.",
                "Furthermore, RankNet can learn with many (differentiable) cost functions, and hence can automatically learn a ranking function from human-provided labels, an attractive alternative to heuristic feature combination techniques.",
                "Hence, we will also use RankNet as a generic ranker to explore the contribution of implicit feedback for different ranking alternatives. 4.",
                "IMPLICIT USER FEEDBACK MODEL Our goal is to accurately interpret noisy user feedback obtained as by tracing user interactions with the search engine.",
                "Interpreting implicit feedback in real web search setting is not an easy task.",
                "We characterize this problem in detail in [1], where we motivate and evaluate a wide variety of models of implicit user activities.",
                "The general approach is to represent user actions for each search result as a vector of features, and then train a ranker on these features to discover feature values indicative of relevant (and nonrelevant) search results.",
                "We first briefly summarize our features and model, and the learning approach (Section 4.2) in order to provide sufficient information to replicate our ranking methods and the subsequent experiments. 4.1 Representing User Actions as Features We model observed web search behaviors as a combination of a ``background component (i.e., query- and relevance-independent noise in user behavior, including positional biases with result interactions), and a ``relevance component (i.e., query-specific behavior indicative of relevance of a result to a query).",
                "We design our features to take advantage of aggregated user behavior.",
                "The feature set is comprised of directly observed features (computed directly from observations for each query), as well as queryspecific derived features, computed as the deviation from the overall query-independent distribution of values for the corresponding directly observed feature values.",
                "The features used to represent user interactions with web search results are summarized in Table 4.1.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "We include the traditional implicit feedback features such as clickthrough counts for the results, as well as our novel derived features such as the deviation of the observed clickthrough number for a given query-URL pair from the expected number of clicks on a result in the given position.",
                "We also model the browsing behavior after a result was clicked - e.g., the average page dwell time for a given query-URL pair, as well as its deviation from the expected (average) dwell time.",
                "Furthermore, the feature set was designed to provide essential information about the user experience to make feedback interpretation robust.",
                "For example, web search users can often determine whether a result is relevant by looking at the result title, URL, and summary - in many cases, looking at the original document is not necessary.",
                "To model this aspect of user experience we include features such as overlap in words in title and words in query (TitleOverlap) and the fraction of words shared by the query and the result summary.",
                "Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickProbability Probability of a click for this query and URL ClickDeviation Deviation from expected click probability IsNextClicked 1 if clicked on next position, 0 otherwise IsPreviousClicked 1 if clicked on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, no parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from average dwell time on page CumulativeDeviation Deviation from average cumulative dwell time DomainDeviation Deviation from average dwell time on domain Query-text features TitleOverlap Words shared between query and title SummaryOverlap Words shared between query and snippet QueryURLOverlap Words shared between query and URL QueryDomainOverlap Words shared between query and URL domain QueryLength Number of tokens in query QueryNextOverlap Fraction of words shared with next query Table 4.1: Some features used to represent post-search navigation history for a given query and search result URL.",
                "Having described our feature set, we briefly review our general method for deriving a user behavior model. 4.2 Deriving a User Feedback Model To learn to interpret the observed user behavior, we correlate user actions (i.e., the features in Table 4.1 representing the actions) with the explicit user judgments for a set of training queries.",
                "We find all the instances in our session logs where these queries were submitted to the search engine, and aggregate the user behavior features for all search sessions involving these queries.",
                "Each observed query-URL pair is represented by the features in Table 4.1, with values averaged over all search sessions, and assigned one of six possible relevance labels, ranging from Perfect to Bad, as assigned by explicit relevance judgments.",
                "These labeled feature vectors are used as input to the RankNet training algorithm (Section 3.3) which produces a trained user behavior model.",
                "This approach is particularly attractive as it does not require heuristics beyond feature engineering.",
                "The resulting user behavior model is used to help rank web search resultseither directly or in combination with other features, as described below. 5.",
                "EXPERIMENTAL SETUP The ultimate goal of incorporating implicit feedback into ranking is to improve the relevance of the returned web search results.",
                "Hence, we compare the ranking methods over a large set of judged queries with explicit relevance labels provided by human judges.",
                "In order for the evaluation to be realistic we obtained a random sample of queries from web search logs of a major search engine, with associated results and traces for user actions.",
                "We describe this dataset in detail next.",
                "Our metrics are described in Section 5.2 that we use to evaluate the ranking alternatives, listed in Section 5.3 in the experiments of Section 6. 5.1 Datasets We compared our ranking methods over a random sample of 3,000 queries from the search engine query logs.",
                "The queries were drawn from the logs uniformly at random by token without replacement, resulting in a query sample representative of the overall query distribution.",
                "On average, 30 results were explicitly labeled by human judges using a six point scale ranging from Perfect down to Bad.",
                "Overall, there were over 83,000 results with explicit relevance judgments.",
                "In order to compute various statistics, documents with label Good or better will be considered relevant, and with lower labels to be non-relevant.",
                "Note that the experiments were performed over the results already highly ranked by a web search engine, which corresponds to a typical user experience which is limited to the small number of the highly ranked results for a typical web search query.",
                "The user interactions were collected over a period of 8 weeks using voluntary opt-in information.",
                "In total, over 1.2 million unique queries were instrumented, resulting in over 12 million individual interactions with the search engine.",
                "The data consisted of user interactions with the web search engine (e.g., clicking on a result link, going back to search results, etc.) performed after a query was submitted.",
                "These actions were aggregated across users and search sessions and converted to features in Table 4.1.",
                "To create the training, validation, and test query sets, we created three different random splits of 1,500 training, 500 validation, and 1000 test queries.",
                "The splits were done randomly by query, so that there was no overlap in training, validation, and test queries. 5.2 Evaluation Metrics We evaluate the ranking algorithms over a range of accepted information retrieval metrics, namely Precision at K (P(K)), Normalized Discounted Cumulative Gain (NDCG), and Mean Average Precision (MAP).",
                "Each metric focuses on a deferent aspect of system performance, as we describe below. • Precision at K: As the most intuitive metric, P(K) reports the fraction of documents ranked in the top K results that are labeled as relevant.",
                "In our setting, we require a relevant document to be labeled Good or higher.",
                "The position of relevant documents within the top K is irrelevant, and hence this metric measure overall user satisfaction with the top K results. • NDCG at K: NDCG is a retrieval measure devised specifically for web search evaluation [10].",
                "For a given query q, the ranked results are examined from the top ranked down, and the NDCG computed as:   = +−= K j jr qq jMN 1 )( )1log(/)12( Where Mq is a normalization constant calculated so that a perfect ordering would obtain NDCG of 1; and each r(j) is an integer relevance label (0=Bad and 5=Perfect) of result returned at position j.",
                "Note that unlabeled and Bad documents do not contribute to the sum, but will reduce NDCG for the query pushing down the relevant labeled documents, reducing their contributions.",
                "NDCG is well suited to web search evaluation, as it rewards relevant documents in the top ranked results more heavily than those ranked lower. • MAP: Average precision for each query is defined as the mean of the precision at K values computed after each relevant document was retrieved.",
                "The final MAP value is defined as the mean of average precisions of all queries in the test set.",
                "This metric is the most commonly used single-value summary of a run over a set of queries. 5.3 Ranking Methods Compared Recall that our goal is to quantify the effectiveness of implicit behavior for real web search.",
                "One dimension is to compare the utility of implicit feedback with other information available to a web search engine.",
                "Specifically, we compare effectiveness of implicit user behaviors with content-based matching, static page quality features, and combinations of all features. • BM25F: As a strong web search baseline we used the BM25F scoring, which was used in one of the best performing systems in the TREC 2004 Web track [23,27].",
                "BM25F and its variants have been extensively described and evaluated in IR literature, and hence serve as a strong, reproducible baseline.",
                "The BM25F variant we used for our experiments computes separate match scores for each field for a result document (e.g., body text, title, and anchor text), and incorporates query-independent linkbased information (e.g., PageRank, ClickDistance, and URL depth).",
                "The scoring function and field-specific tuning is described in detail in [23].",
                "Note that BM25F does not directly consider explicit or implicit feedback for tuning. • RN: The ranking produced by a neural net ranker (RankNet, described in Section 3.3) that learns to rank web search results by incorporating BM25F and a large number of additional static and dynamic features describing each search result.",
                "This system automatically learns weights for all features (including the BM25F <br>score</br> for a document) based on explicit human labels for a large set of queries.",
                "A system incorporating an implementation of RankNet is currently in use by a major search engine and can be considered representative of the state of the art in web search. • BM25F-RerankCT: The ranking produced by incorporating clickthrough statistics to reorder web search results ranked by BM25F above.",
                "Clickthrough is a particularly important special case of implicit feedback, and has been shown to correlate with result relevance.",
                "This is a special case of the ranking method in Section 3.1, with the weight wI set to 1000 and the ranking Id is simply the number of clicks on the result corresponding to d. In effect, this ranking brings to the top all returned web search results with at least one click (and orders them in decreasing order by number of clicks).",
                "The relative ranking of the remainder of results is unchanged and they are inserted below all clicked results.",
                "This method serves as our baseline implicit feedback reranking method.",
                "BM25F-RerankAll The ranking produced by reordering the BM25F results using all user behavior features (Section 4).",
                "This method learns a model of user preferences by correlating feature values with explicit relevance labels using the RankNet neural net algorithm (Section 4.2).",
                "At runtime, for a given query the implicit <br>score</br> Ir is computed for each result r with available user interaction features, and the implicit ranking is produced.",
                "The merged ranking is computed as described in Section 3.1.",
                "Based on the experiments over the development set we fix the value of wI to 3 (the effect of the wI parameter for this ranker turned out to be negligible). • BM25F+All: Ranking derived by training the RankNet (Section 3.3) learner over the features set of the BM25F <br>score</br> as well as all implicit feedback features (Section 3.2).",
                "We used the 2-layer implementation of RankNet [5] trained on the queries and labels in the training and validation sets. • RN+All: Ranking derived by training the 2-layer RankNet ranking algorithm (Section 3.3) over the union of all content, dynamic, and implicit feedback features (i.e., all of the features described above as well as all of the new implicit feedback features we introduced).",
                "The ranking methods above span the range of the information used for ranking, from not using the implicit or explicit feedback at all (i.e., BM25F) to a modern web search engine using hundreds of features and tuned on explicit judgments (RN).",
                "As we will show next, incorporating user behavior into these ranking systems dramatically improves the relevance of the returned documents. 6.",
                "EXPERIMENTAL RESULTS Implicit feedback for web search ranking can be exploited in a number of ways.",
                "We compare alternative methods of exploiting implicit feedback, both by re-ranking the top results (i.e., the BM25F-RerankCT and BM25F-RerankAll methods that reorder BM25F results), as well as by integrating the implicit features directly into the ranking process (i.e., the RN+ALL and BM25F+All methods which learn to rank results over the implicit feedback and other features).",
                "We compare our methods over strong baselines (BM25F and RN) over the NDCG, Precision at K, and MAP measures defined in Section 5.2.",
                "The results were averaged over three random splits of the overall dataset.",
                "Each split contained 1500 training, 500 validation, and 1000 test queries, all query sets disjoint.",
                "We first present the results over all 1000 test queries (i.e., including queries for which there are no implicit measures so we use the original web rankings).",
                "We then drill down to examine the effects on reranking for the attempted queries in more detail, analyzing where implicit feedback proved most beneficial.",
                "We first experimented with different methods of re-ranking the output of the BM25F search results.",
                "Figures 6.1 and 6.2 report NDCG and Precision for BM25F, as well as for the strategies reranking results with user feedback (Section 3.1).",
                "Incorporating all user feedback (either in reranking framework or as features to the learner directly) results in significant improvements (using two-tailed t-test with p=0.01) over both the original BM25F ranking as well as over reranking with clickthrough alone.",
                "The improvement is consistent across the top 10 results and largest for the top result: NDCG at 1 for BM25F+All is 0.622 compared to 0.518 of the original results, and precision at 1 similarly increases from 0.5 to 0.63.",
                "Based on these results we will use the direct feature combination (i.e., BM25F+All) ranker for subsequent comparisons involving implicit feedback. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 1 2 3 4 5 6 7 8 9 10K NDCG BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.1: NDCG at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K 0.35 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.2: Precision at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K Interestingly, using clickthrough alone, while giving significant benefit over the original BM25F ranking, is not as effective as considering the full set of features in Table 4.1.",
                "While we analyze user behavior (and most effective component features) in a separate paper [1], it is worthwhile to give a concrete example of the kind of noise inherent in real user feedback in web search setting. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 Result position Relativeclickfrequency PTR=2 PTR=3 PTR=5 Figure 6.3: Relative clickthrough frequency for queries with varying Position of Top Relevant result (PTR).",
                "If users considered only the relevance of a result to their query, they would click on the topmost relevant results.",
                "Unfortunately, as Joachims and others have shown, presentation also influences which results users click on quite dramatically.",
                "Users often click on results above the relevant one presumably because the short summaries do not provide enough information to make accurate relevance assessments and they have learned that on average topranked items are relevant.",
                "Figure 6.3 shows relative clickthrough frequencies for queries with known relevant items at positions other than the first position; the position of the top relevant result (PTR) ranges from 2-10 in the figure.",
                "For example, for queries with first relevant result at position 5 (PTR=5), there are more clicks on the non-relevant results in higher ranked positions than on the first relevant result at position 5.",
                "As we will see, learning over a richer behavior feature set, results in substantial accuracy improvement over clickthrough alone.",
                "We now consider incorporating user behavior into a much richer feature set, RN (Section 5.3) used by a major web search engine.",
                "RN incorporates BM25F, link-based features, and hundreds of other features.",
                "Figure 6.4 reports NDCG at K and Figure 6.5 reports Precision at K. Interestingly, while the original RN rankings are significantly more accurate than BM25F alone, incorporating implicit feedback features (BM25F+All) results in ranking that significantly outperforms the original RN rankings.",
                "In other words, implicit feedback incorporates sufficient information to replace the hundreds of other features available to the RankNet learner trained on the RN feature set. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 1 2 3 4 5 6 7 8 9 10K NDCG RN RN+All BM25 BM25+All Figure 6.4: NDCG at K for BM25F, BM25F+All, RN, and RN+All for varying K Furthermore, enriching the RN features with implicit feedback set exhibits significant gain on all measures, allowing RN+All to outperform all other methods.",
                "This demonstrates the complementary nature of implicit feedback with other features available to a state of the art web search engine. 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision RN RN+All BM25 BM25+All Figure 6.5: Precision at K for BM25F, BM25F+All, RN, and RN+All for varying K We summarize the performance of the different ranking methods in Table 6.1.",
                "We report the Mean Average Precision (MAP) <br>score</br> for each system.",
                "While not intuitive to interpret, MAP allows quantitative comparison on a single metric.",
                "The gains marked with * are significant at p=0.01 level using two tailed t-test.",
                "MAP Gain P(1) Gain BM25F 0.184 - 0.503BM25F-Rerank-CT 0.215 0.031* 0.577 0.073* BM25F-RerankImplicit 0.218 0.003 0.605 0.028* BM25F+Implicit 0.222 0.004 0.620 0.015* RN 0.215 - 0.597RN+All 0.248 0.033* 0.629 0.032* Table 6.1: Mean Average Precision (MAP) for all strategies.",
                "So far we reported results averaged across all queries in the test set.",
                "Unfortunately, less than half had sufficient interactions to attempt reranking.",
                "Out of the 1000 queries in test, between 46% and 49%, depending on the train-test split, had sufficient interaction information to make predictions (i.e., there was at least 1 search session in which at least 1 result URL was clicked on by the user).",
                "This is not surprising: web search is heavy-tailed, and there are many unique queries.",
                "We now consider the performance on the queries for which user interactions were available.",
                "Figure 6.6 reports NDCG for the subset of the test queries with the implicit feedback features.",
                "The gains at top 1 are dramatic.",
                "The NDCG at 1 of BM25F+All increases from 0.6 to 0.75 (a 31% relative gain), achieving performance comparable to RN+All operating over a much richer feature set. 0.6 0.65 0.7 0.75 0.8 1 3 5 10K NDCG RN RN+All BM25 BM25+All Figure 6.6: NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions Similarly, gains on precision at top 1 are substantial (Figure 6.7), and are likely to be apparent to web search users.",
                "When implicit feedback is available, the BM25F+All system returns relevant document at top 1 almost 70% of the time, compared 53% of the time when implicit feedback is not considered by the original BM25F system. 0.45 0.5 0.55 0.6 0.65 0.7 1 3 5 10K Precision RN RN+All BM25 BM25+All Figure 6.7: Precision at K NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions We summarize the results on the MAP measure for attempted queries in Table 6.2.",
                "MAP improvements are both substantial and significant, with improvements over the BM25F ranker most pronounced.",
                "Method MAP Gain P(1) Gain RN 0.269 0.632 RN+All 0.321 0.051 (19%) 0.693 0.061(10%) BM25F 0.236 0.525 BM25F+All 0.292 0.056 (24%) 0.687 0.162 (31%) Table 6.2: Mean Average Precision (MAP) on attempted queries for best performing methods We now analyze the cases where implicit feedback was shown most helpful.",
                "Figure 6.8 reports the MAP improvements over the baseline BM25F run for each query with MAP under 0.6.",
                "Note that most of the improvement is for poorly performing queries (i.e., MAP < 0.1).",
                "Interestingly, incorporating user behavior information degrades accuracy for queries with high original MAP <br>score</br>.",
                "One possible explanation is that these easy queries tend to be navigational (i.e., having a single, highly-ranked most appropriate answer), and user interactions with lower-ranked results may indicate divergent information needs that are better served by the less popular results (with correspondingly poor overall relevance ratings). 0 50 100 150 200 250 300 350 0.1 0.2 0.3 0.4 0.5 0.6 -0.4 -0.35 -0.3 -0.25 -0.2 -0.15 -0.1 -0.05 0 0.05 0.1 0.15 0.2 Frequency Average Gain Figure 6.8: Gain of BM25F+All over original BM25F ranking To summarize our experimental results, incorporating implicit feedback in real web search setting resulted in significant improvements over the original rankings, using both BM25F and RN baselines.",
                "Our rich set of implicit features, such as time on page and deviations from the average behavior, provides advantages over using clickthrough alone as an indicator of interest.",
                "Furthermore, incorporating implicit feedback features directly into the learned ranking function is more effective than using implicit feedback for reranking.",
                "The improvements observed over large test sets of queries (1,000 total, between 466 and 495 with implicit feedback available) are both substantial and statistically significant. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper we explored the utility of incorporating noisy implicit feedback obtained in a real web search setting to improve web search ranking.",
                "We performed a large-scale evaluation over 3,000 queries and more than 12 million user interactions with a major search engine, establishing the utility of incorporating noisy implicit feedback to improve web search relevance.",
                "We compared two alternatives of incorporating implicit feedback into the search process, namely reranking with implicit feedback and incorporating implicit feedback features directly into the trained ranking function.",
                "Our experiments showed significant improvement over methods that do not consider implicit feedback.",
                "The gains are particularly dramatic for the top K=1 result in the final ranking, with precision improvements as high as 31%, and the gains are substantial for all values of K. Our experiments showed that implicit user feedback can further improve web search performance, when incorporated directly with popular content- and link-based features.",
                "Interestingly, implicit feedback is particularly valuable for queries with poor original ranking of results (e.g., MAP lower than 0.1).",
                "One promising direction for future work is to apply recent research on automatically predicting query difficulty, and only attempt to incorporate implicit feedback for the difficult queries.",
                "As another research direction we are exploring methods for extending our predictions to the previously unseen queries (e.g., query clustering), which should further improve the web search experience of users.",
                "ACKNOWLEDGMENTS We thank Chris Burges and Matt Richardson for an implementation of RankNet for our experiments.",
                "We also thank Robert Ragno for his valuable suggestions and many discussions. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, S. Dumais, and R.Ragno, Learning User Interaction Models for Predicting Web Search Result Preferences.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan, HARD Track Overview in TREC 2003, High Accuracy Retrieval from Documents, 2003 [3] R. Baeza-Yates and B. Ribeiro-Neto, Modern Information Retrieval, Addison-Wesley, 1999. [4] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine, in Proceedings of WWW, 1997 [5] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning, 2005 [6] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [7] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest.",
                "IEEE Internet Computing. 2001 [8] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [9] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [10] K Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2000 [11] T. Joachims, Optimizing Search Engines Using Clickthrough Data.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [12] T. Joachims, L. Granka, B. Pang, H. Hembrooke, and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [13] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [14] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [15] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [16] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [17] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems.",
                "In Proceedings of the AAAI Workshop on Recommender Systems. 1998 [18] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [19] N. Pharo, N. and K. Järvelin.",
                "The SST method: a tool for analyzing web information search processes.",
                "In Information Processing & Management, 2004 [20] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [21] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (SIGKDD), 2005. [22] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in Proceedings of the ICML Workshop on Learning in Web Search, 2005 [23] S. E. Robertson, H. Zaragoza, and M. Taylor, Simple BM25 extension to multiple weighted fields, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [24] G. Salton & M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [25] E.M. Voorhees, D. Harman, Overview of TREC, 2001 [26] G.R.",
                "Xue, H.J.",
                "Zeng, Z. Chen, Y. Yu, W.Y.",
                "Ma, W.S.",
                "Xi, and W.G.",
                "Fan, Optimizing web search using web clickthrough data, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [27] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
                "Microsoft Cambridge at TREC 13: Web and Hard Tracks.",
                "In Proceedings of TREC 2004"
            ],
            "original_annotated_samples": [
                "Each result is assigned a <br>score</br> according to expected relevance/user satisfaction based on previous interactions, resulting in some preference ordering based on user interactions alone.",
                "We found that a simple rank merging heuristic combination works well, and is robust to variations in <br>score</br> values from original rankers.",
                "For a given query q, the implicit <br>score</br> ISd is computed for each result d from available user interaction features, resulting in the implicit rank Id for each result.",
                "We compute a merged <br>score</br> SM(d) for d by combining the ranks obtained from implicit feedback, Id with the original rank of d, Od:     ¡     ¢ £ + + + + = otherwise O dforexistsfeedbackimplicitif OI w wOIdS d dd I IddM 1 1 1 1 1 1 ),,,( where the weight wI is a heuristically tuned scaling factor representing the relative importance of the implicit feedback.",
                "This system automatically learns weights for all features (including the BM25F <br>score</br> for a document) based on explicit human labels for a large set of queries."
            ],
            "translated_annotated_samples": [
                "Cada resultado se le asigna una <br>puntuación</br> según la relevancia esperada/ satisfacción del usuario basada en interacciones previas, lo que resulta en un cierto orden de preferencia basado únicamente en las interacciones del usuario.",
                "Encontramos que una combinación heurística simple de fusión de rangos funciona bien y es robusta a variaciones en los valores de <br>puntuación</br> de los clasificadores originales.",
                "Para una consulta dada q, se calcula la <br>puntuación</br> implícita ISd para cada resultado d a partir de las características de interacción del usuario disponibles, lo que resulta en la clasificación implícita Id para cada resultado.",
                "Calculamos un <br>puntaje</br> combinado SM(d) para d al combinar los rangos obtenidos de la retroalimentación implícita, Id, con el rango original de d, Od: SM(d) = Od + wI * Id, si existe retroalimentación implícita; de lo contrario, SM(d) = Od. Donde el peso wI es un factor de escala ajustado heurísticamente que representa la importancia relativa de la retroalimentación implícita.",
                "Este sistema aprende automáticamente los pesos de todas las características (incluido el <br>puntaje BM25F</br> para un documento) basándose en etiquetas humanas explícitas para un gran conjunto de consultas."
            ],
            "translated_text": "Mejorando la clasificación de búsqueda web al incorporar información sobre el comportamiento del usuario. Eugene Agichtein, Microsoft Research eugeneag@microsoft.com Eric Brill, Microsoft Research brill@microsoft.com Susan Dumais, Microsoft Research sdumais@microsoft.com RESUMEN Mostramos que la incorporación de datos sobre el comportamiento del usuario puede mejorar significativamente el orden de los principales resultados en una configuración real de búsqueda web. Examinamos alternativas para incorporar retroalimentación en el proceso de clasificación y exploramos las contribuciones de la retroalimentación de usuarios en comparación con otras características comunes de búsqueda en la web. Informamos los resultados de una evaluación a gran escala con más de 3,000 consultas y 12 millones de interacciones de usuarios con un motor de búsqueda web popular. Mostramos que la incorporación de retroalimentación implícita puede aumentar otras características, mejorando la precisión de los algoritmos de clasificación de búsqueda web competitivos hasta en un 31% en comparación con el rendimiento original. Categorías y Descriptores de Asignaturas H.3.3 Búsqueda y Recuperación de Información - Retroalimentación de relevancia, proceso de búsqueda; H.3.5 Servicios de Información en Línea - Servicios basados en la web. Términos generales Algoritmos, Medición, Experimentación 1. INTRODUCCIÓN Millones de usuarios interactúan con los motores de búsqueda a diario. Ellos emiten consultas, siguen algunos de los enlaces en los resultados, hacen clic en anuncios, pasan tiempo en páginas, reformulan sus consultas y realizan otras acciones. Estas interacciones pueden servir como una valiosa fuente de información para ajustar y mejorar la clasificación de los resultados de búsqueda en la web y pueden complementar juicios explícitos más costosos. La retroalimentación implícita de relevancia para la clasificación y personalización se ha convertido en un área activa de investigación. El trabajo reciente de Joachims y otros explorando el feedback implícito en entornos controlados ha demostrado el valor de incorporar el feedback implícito en el proceso de clasificación. Nuestra motivación para este trabajo es comprender cómo se puede utilizar la retroalimentación implícita en un entorno operativo a gran escala para mejorar la recuperación. ¿Cómo se compara y complementa con la evidencia del contenido de la página, el texto del ancla o las características basadas en enlaces como inlinks o PageRank? Si bien es intuitivo que las interacciones de los usuarios con el motor de búsqueda web deberían revelar al menos alguna información que podría ser utilizada para la clasificación, estimar las preferencias de los usuarios en entornos reales de búsqueda web es un problema desafiante, ya que las interacciones reales tienden a ser más ruidosas de lo que comúnmente se asume en los entornos controlados de estudios previos. Nuestro artículo explora si la retroalimentación implícita puede ser útil en entornos realistas, donde la retroalimentación del usuario puede ser ruidosa (o adversa) y un motor de búsqueda web ya utiliza cientos de características y está altamente ajustado. Con este fin, exploramos diferentes enfoques para clasificar los resultados de búsqueda web utilizando el comportamiento real de los usuarios obtenido como parte de las interacciones normales con el motor de búsqueda web. Las contribuciones específicas de este artículo incluyen: • Análisis de alternativas para incorporar el comportamiento del usuario en la clasificación de búsqueda web (Sección 3). • Aplicación de un modelo robusto de retroalimentación implícita derivado de la minería de millones de interacciones de usuarios con un importante motor de búsqueda web (Sección 4). • Una evaluación a gran escala sobre consultas reales de usuarios y resultados de búsqueda, mostrando mejoras significativas derivadas de la incorporación de la retroalimentación del usuario (Sección 6). Resumimos nuestros hallazgos y discutimos extensiones al trabajo actual en la Sección 7, que concluye el artículo. ANTECEDENTES Y TRABAJO RELACIONADO Clasificar los resultados de búsqueda es un problema fundamental en la recuperación de información. La mayoría de los enfoques comunes se centran principalmente en la similitud entre la consulta y una página, así como en la calidad general de la página [3,4,24]. Sin embargo, con la creciente popularidad de los motores de búsqueda, la retroalimentación implícita (es decir, las acciones que los usuarios realizan al interactuar con el motor de búsqueda) se puede utilizar para mejorar las clasificaciones. Las medidas de relevancia implícitas han sido estudiadas por varios grupos de investigación. Un resumen de las medidas implícitas se recopila en Kelly y Teevan [14]. Esta investigación, si bien desarrolló valiosas ideas sobre medidas implícitas de relevancia, no se aplicó para mejorar la clasificación de los resultados de búsqueda en la web en entornos realistas. Estrechamente relacionado con nuestro trabajo, Joachims [11] recopiló medidas implícitas en lugar de medidas explícitas, introduciendo una técnica basada completamente en datos de clics para aprender funciones de clasificación. Fox et al. [8] exploraron la relación entre medidas implícitas y explícitas en la búsqueda web, y desarrollaron modelos bayesianos para correlacionar medidas implícitas y juicios explícitos de relevancia tanto para consultas individuales como para sesiones de búsqueda. Este trabajo consideró una amplia gama de comportamientos de usuario (por ejemplo, tiempo de permanencia, tiempo de desplazamiento, patrones de reformulación) además del popular comportamiento de clics. Sin embargo, el esfuerzo de modelado se centró en predecir juicios de relevancia explícitos a partir de acciones implícitas de los usuarios y no específicamente en aprender funciones de clasificación. Otros estudios sobre el comportamiento de los usuarios en la búsqueda web incluyen a Pharo y Järvelin [19], pero no se aplicaron directamente para mejorar la clasificación. Más recientemente, Joachims et al. [12] presentaron una evaluación empírica de la interpretación de la evidencia de clics. Al realizar estudios de seguimiento ocular y correlacionar las predicciones de sus estrategias con las calificaciones explícitas, los autores demostraron que es posible interpretar con precisión los clics en un entorno controlado de laboratorio. Desafortunadamente, no está claro en qué medida la investigación previa se aplica a la búsqueda web del mundo real. Al mismo tiempo, si bien el trabajo reciente (por ejemplo, [26]) sobre el uso de la información de clics para mejorar la clasificación de búsqueda en la web es prometedor, solo abarca un aspecto de las interacciones de los usuarios con los motores de búsqueda en la web. Nos basamos en investigaciones existentes para desarrollar técnicas robustas de interpretación del comportamiento del usuario para el entorno real de búsqueda en la web. En lugar de tratar a cada usuario como un experto confiable, agregamos información de múltiples trazas de sesiones de búsqueda de usuarios no confiables, como describimos en las siguientes dos secciones. 3. INTEGRANDO LA RETROALIMENTACIÓN IMPLÍCITA Consideramos dos enfoques complementarios para la clasificación con retroalimentación implícita: (1) tratar la retroalimentación implícita como evidencia independiente para clasificar resultados, y (2) integrar características de retroalimentación implícita directamente en el algoritmo de clasificación. Describimos a continuación los dos enfoques generales de clasificación. Las características específicas del feedback implícito se describen en la Sección 4, y los algoritmos para interpretar e incorporar el feedback implícito se describen en la Sección 5. 3.1 Feedback Implícito como Evidencia Independiente. El enfoque general es reordenar los resultados obtenidos por un motor de búsqueda web según los clics observados y otras interacciones de usuario para la consulta en sesiones de búsqueda anteriores. Cada resultado se le asigna una <br>puntuación</br> según la relevancia esperada/ satisfacción del usuario basada en interacciones previas, lo que resulta en un cierto orden de preferencia basado únicamente en las interacciones del usuario. Si bien ha habido un trabajo significativo en la fusión de múltiples clasificaciones, adaptamos un enfoque simple y robusto de ignorar las puntuaciones de los clasificadores originales, y en su lugar simplemente fusionamos los órdenes de clasificación. La razón principal para ignorar las puntuaciones originales es que, dado que los espacios de características y los algoritmos de aprendizaje son diferentes, las puntuaciones no son directamente comparables, y la re-normalización tiende a eliminar el beneficio de incorporar las puntuaciones del clasificador. Experimentamos con una variedad de funciones de fusión en el conjunto de desarrollo de consultas (y utilizando un conjunto de interacciones de un período de tiempo diferente de los conjuntos de evaluación final). Encontramos que una combinación heurística simple de fusión de rangos funciona bien y es robusta a variaciones en los valores de <br>puntuación</br> de los clasificadores originales. Para una consulta dada q, se calcula la <br>puntuación</br> implícita ISd para cada resultado d a partir de las características de interacción del usuario disponibles, lo que resulta en la clasificación implícita Id para cada resultado. Calculamos un <br>puntaje</br> combinado SM(d) para d al combinar los rangos obtenidos de la retroalimentación implícita, Id, con el rango original de d, Od: SM(d) = Od + wI * Id, si existe retroalimentación implícita; de lo contrario, SM(d) = Od. Donde el peso wI es un factor de escala ajustado heurísticamente que representa la importancia relativa de la retroalimentación implícita. Los resultados de la consulta se ordenan en valores decrecientes de SM para producir la clasificación final. Un caso especial de este modelo surge al establecer wI en un valor muy grande, lo que efectivamente obliga a que los resultados clicados se clasifiquen por encima de los resultados no clicados, una heurística intuitiva y efectiva que utilizaremos como referencia. Aplicar algoritmos de combinación de clasificadores y ordenadores más sofisticados puede resultar en mejoras adicionales, y es una dirección prometedora para trabajos futuros. El enfoque anterior asume que no hay interacciones entre las características subyacentes que producen la clasificación original de la búsqueda web y las características de retroalimentación implícitas. Ahora relajamos esta suposición al integrar características de retroalimentación implícita directamente en el proceso de clasificación. 3.2 Clasificación con Características de Retroalimentación Implícita Los motores de búsqueda web modernos clasifican los resultados en función de un gran número de características, incluidas las características basadas en el contenido (es decir, qué tan cerca coincide una consulta con el texto, el título o el texto de anclaje del documento) y las características de calidad de página independientes de la consulta (por ejemplo, PageRank del documento o del dominio). En la mayoría de los casos, se desarrollan métodos automáticos (o semiautomáticos) para ajustar la función de clasificación específica que combina estos valores de características. Por lo tanto, un enfoque natural es incorporar características de retroalimentación implícita directamente como características para el algoritmo de clasificación. Durante el entrenamiento o ajuste, el clasificador puede ser ajustado como antes pero con características adicionales. En tiempo de ejecución, el motor de búsqueda recuperaría las características de retroalimentación implícita asociadas con cada par de URL de consulta y resultado. Este modelo requiere que un algoritmo de clasificación sea robusto ante valores faltantes: más del 50% de las consultas a los motores de búsqueda web son únicas, sin disponer de retroalimentación implícita previa. Ahora describimos un clasificador que utilizamos para aprender sobre los conjuntos de características combinadas, incluyendo la retroalimentación implícita. 3.3 Aprendizaje para Clasificar los Resultados de Búsqueda en la Web Un aspecto clave de nuestro enfoque es aprovechar los avances recientes en el aprendizaje automático, en particular los algoritmos de clasificación entrenables para la búsqueda en la web y la recuperación de información (por ejemplo, [5, 11] y resultados clásicos revisados en [3]). En nuestro entorno, contamos con juicios explícitos de relevancia humana (etiquetas) para un conjunto de consultas de búsqueda en la web y resultados. Por lo tanto, una opción atractiva es utilizar una técnica de aprendizaje automático supervisado para aprender una función de clasificación que prediga mejor las evaluaciones de relevancia. RankNet es uno de esos algoritmos. Es un algoritmo de ajuste de red neuronal que optimiza los pesos de las características para que coincidan mejor con las preferencias de los usuarios proporcionadas explícitamente en pares. Si bien los algoritmos de entrenamiento específicos utilizados por RankNet están más allá del alcance de este documento, se describen en detalle en [5] e incluyen una evaluación exhaustiva y comparación con otros métodos de clasificación. Una característica atractiva de RankNet es su eficiencia tanto en el tiempo de entrenamiento como en el de ejecución: el ranking en tiempo de ejecución se puede calcular rápidamente y puede escalarse a la web, y el entrenamiento se puede realizar sobre miles de consultas y resultados juzgados asociados. Utilizamos una implementación de RankNet de 2 capas para modelar relaciones no lineales entre características. Además, RankNet puede aprender con muchas funciones de costo (diferenciables), por lo que puede aprender automáticamente una función de clasificación a partir de etiquetas proporcionadas por humanos, una alternativa atractiva a las técnicas de combinación de características heurísticas. Por lo tanto, también utilizaremos RankNet como un clasificador genérico para explorar la contribución de la retroalimentación implícita en diferentes alternativas de clasificación. 4. MODELO IMPLÍCITO DE RETROALIMENTACIÓN DEL USUARIO Nuestro objetivo es interpretar con precisión la retroalimentación ruidosa del usuario obtenida al rastrear las interacciones del usuario con el motor de búsqueda. Interpretar la retroalimentación implícita en un entorno real de búsqueda en la web no es una tarea fácil. Caracterizamos este problema en detalle en [1], donde motivamos y evaluamos una amplia variedad de modelos de actividades implícitas de usuario. El enfoque general es representar las acciones del usuario para cada resultado de búsqueda como un vector de características, y luego entrenar un clasificador en base a estas características para descubrir los valores de las características que indican resultados de búsqueda relevantes (y no relevantes). Primero resumimos brevemente nuestras características y modelo, y el enfoque de aprendizaje (Sección 4.2) para proporcionar suficiente información para replicar nuestros métodos de clasificación y los experimentos subsiguientes. 4.1 Representación de las acciones del usuario como características. Modelamos los comportamientos observados en la búsqueda web como una combinación de un \"componente de fondo (es decir, ruido independiente de la consulta y relevancia en el comportamiento del usuario, incluidos sesgos posicionales con interacciones de resultados), y un \"componente de relevancia (es decir, comportamiento específico de la consulta indicativo de la relevancia de un resultado para una consulta). Diseñamos nuestras características para aprovechar el comportamiento de usuario agregado. El conjunto de características está compuesto por características observadas directamente (calculadas directamente a partir de observaciones para cada consulta), así como características derivadas específicas de la consulta, calculadas como la desviación de la distribución general de valores independientes de la consulta para los valores de las características observadas directamente correspondientes. Las características utilizadas para representar las interacciones de los usuarios con los resultados de búsqueda en la web se resumen en la Tabla 4.1. Esta información se obtuvo a través de la instrumentación del lado del cliente opt-in de usuarios de un importante motor de búsqueda web. Incluimos las características tradicionales de retroalimentación implícita, como el recuento de clics en los resultados, así como nuestras características derivadas novedosas, como la desviación del número de clics observados para un par de consulta-URL dado del número esperado de clics en un resultado en la posición dada. También modelamos el comportamiento de navegación después de que se hace clic en un resultado, por ejemplo, el tiempo promedio de permanencia en la página para un par de consulta-URL dado, así como su desviación del tiempo de permanencia esperado (promedio). Además, el conjunto de características fue diseñado para proporcionar información esencial sobre la experiencia del usuario y hacer que la interpretación de la retroalimentación sea sólida. Por ejemplo, los usuarios de búsqueda en la web a menudo pueden determinar si un resultado es relevante al mirar el título del resultado, la URL y el resumen; en muchos casos, no es necesario mirar el documento original. Para modelar este aspecto de la experiencia del usuario, incluimos características como la superposición de palabras en el título y las palabras en la consulta (TitleOverlap) y la fracción de palabras compartidas por la consulta y el resumen del resultado. Características de clics Posición Posición de la URL en el ranking actual Frecuencia de clics Número de clics para esta consulta, par URL Probabilidad de clic Probabilidad de un clic para esta consulta y URL Desviación de clic Desviación de la probabilidad de clic esperada ¿Es el siguiente clic? 1 si se hizo clic en la siguiente posición, 0 de lo contrario ¿Es el clic anterior? 1 si se hizo clic en la posición anterior, 0 de lo contrario ¿Hay clic arriba? 1 si hay un clic arriba, 0 de lo contrario ¿Hay clic abajo? 1 si hay un clic abajo, 0 de lo contrario Características de navegación Tiempo en la página Tiempo de permanencia en la página Tiempo acumulado en la página Tiempo acumulado para todas las páginas siguientes después de la búsqueda Tiempo en el dominio Tiempo de permanencia acumulado para este dominio Tiempo en URL corta Tiempo acumulado en el prefijo de la URL, sin parámetros ¿Se siguió el enlace? 1 si se siguió el enlace al resultado, 0 de lo contrario ¿Coincidencia exacta de URL? 0 si se utilizó normalización agresiva, 1 de lo contrario ¿Redirigido? 1 si la URL inicial es la misma que la URL final, 0 de lo contrario ¿Camino desde la búsqueda? 1 si solo se siguieron enlaces después de la consulta, 0 de lo contrario Clics desde la búsqueda Número de saltos para llegar a la página desde la consulta Tiempo promedio de permanencia Tiempo promedio en la página para esta consulta Desviación del tiempo de permanencia Desviación del tiempo de permanencia promedio en la página Desviación acumulada Desviación del tiempo de permanencia acumulado promedio Desviación del dominio Desviación del tiempo de permanencia promedio en el dominio Características de texto de consulta Coincidencia de título Palabras compartidas entre la consulta y el título Coincidencia de resumen Palabras compartidas entre la consulta y el fragmento Coincidencia de URL de consulta Palabras compartidas entre la consulta y la URL Coincidencia de dominio de consulta Palabras compartidas entre la consulta y el dominio de la URL Longitud de la consulta Número de tokens en la consulta Superposición con la siguiente consulta Fracción de palabras compartidas con la siguiente consulta Tabla 4.1: Algunas características utilizadas para representar el historial de navegación posterior a la búsqueda para una consulta dada y una URL de resultado de búsqueda. Habiendo descrito nuestro conjunto de características, revisamos brevemente nuestro método general para derivar un modelo de comportamiento del usuario. 4.2 Derivación de un Modelo de Retroalimentación del Usuario Para aprender a interpretar el comportamiento observado del usuario, correlacionamos las acciones del usuario (es decir, las características en la Tabla 4.1 que representan las acciones) con los juicios explícitos del usuario para un conjunto de consultas de entrenamiento. Encontramos todas las instancias en nuestros registros de sesión donde se enviaron estas consultas al motor de búsqueda, y agregamos las características del comportamiento del usuario para todas las sesiones de búsqueda que involucran estas consultas. Cada par de consulta-URL observado está representado por las características en la Tabla 4.1, con valores promediados en todas las sesiones de búsqueda, y asignado una de las seis etiquetas de relevancia posibles, que van desde Perfecto hasta Malo, según los juicios explícitos de relevancia. Estos vectores de características etiquetados se utilizan como entrada para el algoritmo de entrenamiento RankNet (Sección 3.3), el cual produce un modelo de comportamiento de usuario entrenado. Este enfoque es particularmente atractivo ya que no requiere heurísticas más allá de la ingeniería de características. El modelo de comportamiento del usuario resultante se utiliza para ayudar a clasificar los resultados de búsqueda en la web, ya sea directamente o en combinación con otras características, como se describe a continuación. 5. CONFIGURACIÓN EXPERIMENTAL El objetivo final de incorporar retroalimentación implícita en la clasificación es mejorar la relevancia de los resultados de búsqueda web devueltos. Por lo tanto, comparamos los métodos de clasificación sobre un gran conjunto de consultas evaluadas con etiquetas de relevancia explícitas proporcionadas por jueces humanos. Para que la evaluación sea realista, obtuvimos una muestra aleatoria de consultas de registros de búsqueda web de un motor de búsqueda importante, con resultados asociados y rastros de acciones de usuario. Describimos este conjunto de datos en detalle a continuación. Nuestros indicadores se describen en la Sección 5.2 que utilizamos para evaluar las alternativas de clasificación, enumeradas en la Sección 5.3 en los experimentos de la Sección 6. 5.1 Conjuntos de datos Comparamos nuestros métodos de clasificación en una muestra aleatoria de 3,000 consultas de los registros de consultas del motor de búsqueda. Las consultas fueron extraídas de los registros de manera uniforme al azar por token sin reemplazo, lo que resultó en una muestra de consultas representativa de la distribución general de consultas. En promedio, 30 resultados fueron etiquetados explícitamente por jueces humanos utilizando una escala de seis puntos que va desde Perfecto hasta Malo. En total, hubo más de 83,000 resultados con juicios de relevancia explícitos. Para calcular diversas estadísticas, se considerarán relevantes los documentos con la etiqueta Buena o mejor, y los documentos con etiquetas inferiores se considerarán no relevantes. Se debe tener en cuenta que los experimentos se realizaron sobre los resultados ya altamente clasificados por un motor de búsqueda web, lo cual corresponde a una experiencia de usuario típica que se limita al pequeño número de resultados altamente clasificados para una consulta típica de búsqueda web. Las interacciones de los usuarios fueron recopiladas durante un período de 8 semanas utilizando información voluntaria de participación. En total, se registraron más de 1.2 millones de consultas únicas, lo que resultó en más de 12 millones de interacciones individuales con el motor de búsqueda. Los datos consistían en las interacciones de los usuarios con el motor de búsqueda web (por ejemplo, hacer clic en un enlace de resultado, regresar a los resultados de búsqueda, etc.) realizadas después de enviar una consulta. Estas acciones fueron agregadas entre usuarios y sesiones de búsqueda y convertidas en características en la Tabla 4.1. Para crear los conjuntos de consultas de entrenamiento, validación y prueba, creamos tres divisiones aleatorias diferentes de 1,500 consultas de entrenamiento, 500 de validación y 1000 de prueba. Las divisiones se realizaron aleatoriamente por consulta, de modo que no hubiera superposición en las consultas de entrenamiento, validación y prueba. 5.2 Métricas de Evaluación Evaluamos los algoritmos de clasificación en una variedad de métricas de recuperación de información aceptadas, a saber, Precisión en K (P(K)), Ganancia Acumulada Descontada Normalizada (NDCG) y Precisión Promedio Media (MAP). Cada métrica se enfoca en un aspecto diferente del rendimiento del sistema, como describimos a continuación. • Precisión en K: Como la métrica más intuitiva, P(K) informa la fracción de documentos clasificados en los primeros K resultados que están etiquetados como relevantes. En nuestro entorno, requerimos que un documento relevante sea etiquetado como Bueno o superior. La posición de los documentos relevantes dentro de los primeros K no es relevante, por lo que esta métrica mide la satisfacción general del usuario con los resultados principales de K. • NDCG en K: NDCG es una medida de recuperación diseñada específicamente para la evaluación de búsqueda en la web [10]. Para una consulta dada q, los resultados clasificados se examinan desde el mejor clasificado hacia abajo, y el NDCG se calcula como: = +−= K j jr qq jMN 1 )( )1log(/)12( Donde Mq es una constante de normalización calculada de manera que un orden perfecto obtendría un NDCG de 1; y cada r(j) es una etiqueta de relevancia entera (0=Malo y 5=Perfecto) del resultado devuelto en la posición j. Ten en cuenta que los documentos sin etiquetar y los documentos malos no contribuyen a la suma, pero reducirán el NDCG para la consulta al empujar hacia abajo los documentos etiquetados relevantes, disminuyendo sus contribuciones. NDCG es muy adecuado para la evaluación de búsquedas en la web, ya que recompensa de manera más intensa los documentos relevantes en los resultados mejor clasificados que aquellos clasificados más bajos. • MAP: La precisión promedio para cada consulta se define como la media de la precisión en los valores de K calculados después de que se recuperó cada documento relevante. El valor MAP final se define como la media de las precisiones promedio de todas las consultas en el conjunto de pruebas. Esta métrica es el resumen de un solo valor más comúnmente utilizado de una ejecución sobre un conjunto de consultas. 5.3 Métodos de clasificación comparados. Recuerde que nuestro objetivo es cuantificar la efectividad del comportamiento implícito para la búsqueda web real. Una dimensión es comparar la utilidad de la retroalimentación implícita con otra información disponible para un motor de búsqueda web. Específicamente, comparamos la efectividad de los comportamientos implícitos de los usuarios con la coincidencia basada en el contenido, las características de calidad de la página estática y combinaciones de todas las características. • BM25F: Como referencia sólida de búsqueda web, utilizamos la puntuación BM25F, que se utilizó en uno de los sistemas con mejor rendimiento en la pista web TREC 2004 [23,27]. BM25F y sus variantes han sido extensamente descritas y evaluadas en la literatura de IR, por lo tanto, sirven como una línea base sólida y reproducible. La variante BM25F que utilizamos en nuestros experimentos calcula puntuaciones de coincidencia separadas para cada campo de un documento de resultado (por ejemplo, texto del cuerpo, título y texto del enlace) e incorpora información de enlaces independiente de la consulta (por ejemplo, PageRank, ClickDistance y profundidad de URL). La función de puntuación y la ajuste específico del campo se describen en detalle en [23]. Tenga en cuenta que BM25F no considera directamente la retroalimentación explícita o implícita para ajuste. • RN: La clasificación producida por un clasificador de redes neuronales (RankNet, descrito en la Sección 3.3) que aprende a clasificar los resultados de búsqueda web al incorporar BM25F y un gran número de características estáticas y dinámicas adicionales que describen cada resultado de búsqueda. Este sistema aprende automáticamente los pesos de todas las características (incluido el <br>puntaje BM25F</br> para un documento) basándose en etiquetas humanas explícitas para un gran conjunto de consultas. ",
            "candidates": [],
            "error": [
                [
                    "puntuación",
                    "puntuación",
                    "puntuación",
                    "puntaje",
                    "puntaje BM25F"
                ]
            ]
        },
        "document": {
            "translated_key": "documento",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Improving Web Search Ranking by Incorporating User Behavior Information Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com ABSTRACT We show that incorporating user behavior data can significantly improve ordering of top results in real web search setting.",
                "We examine alternatives for incorporating feedback into the ranking process and explore the contributions of user feedback compared to other common web search features.",
                "We report results of a large scale evaluation over 3,000 queries and 12 million user interactions with a popular web search engine.",
                "We show that incorporating implicit feedback can augment other features, improving the accuracy of a competitive web search ranking algorithms by as much as 31% relative to the original performance.",
                "Categories and Subject Descriptors H.3.3 Information Search and Retrieval - Relevance feedback, search process; H.3.5 Online Information Services - Web-based services.",
                "General Terms Algorithms, Measurement, Experimentation 1.",
                "INTRODUCTION Millions of users interact with search engines daily.",
                "They issue queries, follow some of the links in the results, click on ads, spend time on pages, reformulate their queries, and perform other actions.",
                "These interactions can serve as a valuable source of information for tuning and improving web search result ranking and can compliment more costly explicit judgments.",
                "Implicit relevance feedback for ranking and personalization has become an active area of research.",
                "Recent work by Joachims and others exploring implicit feedback in controlled environments have shown the value of incorporating implicit feedback into the ranking process.",
                "Our motivation for this work is to understand how implicit feedback can be used in a large-scale operational environment to improve retrieval.",
                "How does it compare to and compliment evidence from page content, anchor text, or link-based features such as inlinks or PageRank?",
                "While it is intuitive that user interactions with the web search engine should reveal at least some information that could be used for ranking, estimating user preferences in real web search settings is a challenging problem, since real user interactions tend to be more noisy than commonly assumed in the controlled settings of previous studies.",
                "Our paper explores whether implicit feedback can be helpful in realistic environments, where user feedback can be noisy (or adversarial) and a web search engine already uses hundreds of features and is heavily tuned.",
                "To this end, we explore different approaches for ranking web search results using real user behavior obtained as part of normal interactions with the web search engine.",
                "The specific contributions of this paper include: • Analysis of alternatives for incorporating user behavior into web search ranking (Section 3). • An application of a robust implicit feedback model derived from mining millions of user interactions with a major web search engine (Section 4). • A large scale evaluation over real user queries and search results, showing significant improvements derived from incorporating user feedback (Section 6).",
                "We summarize our findings and discuss extensions to the current work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "Most common approaches primarily focus on similarity of query and a page, as well as the overall page quality [3,4,24].",
                "However, with increasing popularity of search engines, implicit feedback (i.e., the actions users take when interacting with the search engine) can be used to improve the rankings.",
                "Implicit relevance measures have been studied by several research groups.",
                "An overview of implicit measures is compiled in Kelly and Teevan [14].",
                "This research, while developing valuable insights into implicit relevance measures, was not applied to improve the ranking of web search results in realistic settings.",
                "Closely related to our work, Joachims [11] collected implicit measures in place of explicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "Fox et al. [8] explored the relationship between implicit and explicit measures in Web search, and developed Bayesian models to correlate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "This work considered a wide range of user behaviors (e.g., dwell time, scroll time, reformulation patterns) in addition to the popular clickthrough behavior.",
                "However, the modeling effort was aimed at predicting explicit relevance judgments from implicit user actions and not specifically at learning ranking functions.",
                "Other studies of user behavior in web search include Pharo and Järvelin [19], but were not directly applied to improve ranking.",
                "More recently, Joachims et al. [12] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthroughs in a controlled, laboratory setting.",
                "Unfortunately, the extent to which previous research applies to real-world web search is unclear.",
                "At the same time, while recent work (e.g., [26]) on using clickthrough information for improving web search ranking is promising, it captures only one aspect of the user interactions with web search engines.",
                "We build on existing research to develop robust user behavior interpretation techniques for the real web search setting.",
                "Instead of treating each user as a reliable expert, we aggregate information from multiple, unreliable, user search session traces, as we describe in the next two sections. 3.",
                "INCORPORATING IMPLICIT FEEDBACK We consider two complementary approaches to ranking with implicit feedback: (1) treating implicit feedback as independent evidence for ranking results, and (2) integrating implicit feedback features directly into the ranking algorithm.",
                "We describe the two general ranking approaches next.",
                "The specific implicit feedback features are described in Section 4, and the algorithms for interpreting and incorporating implicit feedback are described in Section 5. 3.1 Implicit Feedback as Independent Evidence The general approach is to re-rank the results obtained by a web search engine according to observed clickthrough and other user interactions for the query in previous search sessions.",
                "Each result is assigned a score according to expected relevance/user satisfaction based on previous interactions, resulting in some preference ordering based on user interactions alone.",
                "While there has been significant work on merging multiple rankings, we adapt a simple and robust approach of ignoring the original rankers scores, and instead simply merge the rank orders.",
                "The main reason for ignoring the original scores is that since the feature spaces and learning algorithms are different, the scores are not directly comparable, and re-normalization tends to remove the benefit of incorporating classifier scores.",
                "We experimented with a variety of merging functions on the development set of queries (and using a set of interactions from a different time period from final evaluation sets).",
                "We found that a simple rank merging heuristic combination works well, and is robust to variations in score values from original rankers.",
                "For a given query q, the implicit score ISd is computed for each result d from available user interaction features, resulting in the implicit rank Id for each result.",
                "We compute a merged score SM(d) for d by combining the ranks obtained from implicit feedback, Id with the original rank of d, Od:     ¡     ¢ £ + + + + = otherwise O dforexistsfeedbackimplicitif OI w wOIdS d dd I IddM 1 1 1 1 1 1 ),,,( where the weight wI is a heuristically tuned scaling factor representing the relative importance of the implicit feedback.",
                "The query results are ordered in by decreasing values of SM to produce the final ranking.",
                "One special case of this model arises when setting wI to a very large value, effectively forcing clicked results to be ranked higher than un-clicked results - an intuitive and effective heuristic that we will use as a baseline.",
                "Applying more sophisticated classifier and ranker combination algorithms may result in additional improvements, and is a promising direction for future work.",
                "The approach above assumes that there are no interactions between the underlying features producing the original web search ranking and the implicit feedback features.",
                "We now relax this assumption by integrating implicit feedback features directly into the ranking process. 3.2 Ranking with Implicit Feedback Features Modern web search engines rank results based on a large number of features, including content-based features (i.e., how closely a query matches the text or title or anchor text of the <br>document</br>), and query-independent page quality features (e.g., PageRank of the <br>document</br> or the domain).",
                "In most cases, automatic (or semiautomatic) methods are developed for tuning the specific ranking function that combines these feature values.",
                "Hence, a natural approach is to incorporate implicit feedback features directly as features for the ranking algorithm.",
                "During training or tuning, the ranker can be tuned as before but with additional features.",
                "At runtime, the search engine would fetch the implicit feedback features associated with each query-result URL pair.",
                "This model requires a ranking algorithm to be robust to missing values: more than 50% of queries to web search engines are unique, with no previous implicit feedback available.",
                "We now describe such a ranker that we used to learn over the combined feature sets including implicit feedback. 3.3 Learning to Rank Web Search Results A key aspect of our approach is exploiting recent advances in machine learning, namely trainable ranking algorithms for web search and information retrieval (e.g., [5, 11] and classical results reviewed in [3]).",
                "In our setting, explicit human relevance judgments (labels) are available for a set of web search queries and results.",
                "Hence, an attractive choice to use is a supervised machine learning technique to learn a ranking function that best predicts relevance judgments.",
                "RankNet is one such algorithm.",
                "It is a neural net tuning algorithm that optimizes feature weights to best match explicitly provided pairwise user preferences.",
                "While the specific training algorithms used by RankNet are beyond the scope of this paper, it is described in detail in [5] and includes extensive evaluation and comparison with other ranking methods.",
                "An attractive feature of RankNet is both train- and run-time efficiency - runtime ranking can be quickly computed and can scale to the web, and training can be done over thousands of queries and associated judged results.",
                "We use a 2-layer implementation of RankNet in order to model non-linear relationships between features.",
                "Furthermore, RankNet can learn with many (differentiable) cost functions, and hence can automatically learn a ranking function from human-provided labels, an attractive alternative to heuristic feature combination techniques.",
                "Hence, we will also use RankNet as a generic ranker to explore the contribution of implicit feedback for different ranking alternatives. 4.",
                "IMPLICIT USER FEEDBACK MODEL Our goal is to accurately interpret noisy user feedback obtained as by tracing user interactions with the search engine.",
                "Interpreting implicit feedback in real web search setting is not an easy task.",
                "We characterize this problem in detail in [1], where we motivate and evaluate a wide variety of models of implicit user activities.",
                "The general approach is to represent user actions for each search result as a vector of features, and then train a ranker on these features to discover feature values indicative of relevant (and nonrelevant) search results.",
                "We first briefly summarize our features and model, and the learning approach (Section 4.2) in order to provide sufficient information to replicate our ranking methods and the subsequent experiments. 4.1 Representing User Actions as Features We model observed web search behaviors as a combination of a ``background component (i.e., query- and relevance-independent noise in user behavior, including positional biases with result interactions), and a ``relevance component (i.e., query-specific behavior indicative of relevance of a result to a query).",
                "We design our features to take advantage of aggregated user behavior.",
                "The feature set is comprised of directly observed features (computed directly from observations for each query), as well as queryspecific derived features, computed as the deviation from the overall query-independent distribution of values for the corresponding directly observed feature values.",
                "The features used to represent user interactions with web search results are summarized in Table 4.1.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "We include the traditional implicit feedback features such as clickthrough counts for the results, as well as our novel derived features such as the deviation of the observed clickthrough number for a given query-URL pair from the expected number of clicks on a result in the given position.",
                "We also model the browsing behavior after a result was clicked - e.g., the average page dwell time for a given query-URL pair, as well as its deviation from the expected (average) dwell time.",
                "Furthermore, the feature set was designed to provide essential information about the user experience to make feedback interpretation robust.",
                "For example, web search users can often determine whether a result is relevant by looking at the result title, URL, and summary - in many cases, looking at the original <br>document</br> is not necessary.",
                "To model this aspect of user experience we include features such as overlap in words in title and words in query (TitleOverlap) and the fraction of words shared by the query and the result summary.",
                "Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickProbability Probability of a click for this query and URL ClickDeviation Deviation from expected click probability IsNextClicked 1 if clicked on next position, 0 otherwise IsPreviousClicked 1 if clicked on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, no parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from average dwell time on page CumulativeDeviation Deviation from average cumulative dwell time DomainDeviation Deviation from average dwell time on domain Query-text features TitleOverlap Words shared between query and title SummaryOverlap Words shared between query and snippet QueryURLOverlap Words shared between query and URL QueryDomainOverlap Words shared between query and URL domain QueryLength Number of tokens in query QueryNextOverlap Fraction of words shared with next query Table 4.1: Some features used to represent post-search navigation history for a given query and search result URL.",
                "Having described our feature set, we briefly review our general method for deriving a user behavior model. 4.2 Deriving a User Feedback Model To learn to interpret the observed user behavior, we correlate user actions (i.e., the features in Table 4.1 representing the actions) with the explicit user judgments for a set of training queries.",
                "We find all the instances in our session logs where these queries were submitted to the search engine, and aggregate the user behavior features for all search sessions involving these queries.",
                "Each observed query-URL pair is represented by the features in Table 4.1, with values averaged over all search sessions, and assigned one of six possible relevance labels, ranging from Perfect to Bad, as assigned by explicit relevance judgments.",
                "These labeled feature vectors are used as input to the RankNet training algorithm (Section 3.3) which produces a trained user behavior model.",
                "This approach is particularly attractive as it does not require heuristics beyond feature engineering.",
                "The resulting user behavior model is used to help rank web search resultseither directly or in combination with other features, as described below. 5.",
                "EXPERIMENTAL SETUP The ultimate goal of incorporating implicit feedback into ranking is to improve the relevance of the returned web search results.",
                "Hence, we compare the ranking methods over a large set of judged queries with explicit relevance labels provided by human judges.",
                "In order for the evaluation to be realistic we obtained a random sample of queries from web search logs of a major search engine, with associated results and traces for user actions.",
                "We describe this dataset in detail next.",
                "Our metrics are described in Section 5.2 that we use to evaluate the ranking alternatives, listed in Section 5.3 in the experiments of Section 6. 5.1 Datasets We compared our ranking methods over a random sample of 3,000 queries from the search engine query logs.",
                "The queries were drawn from the logs uniformly at random by token without replacement, resulting in a query sample representative of the overall query distribution.",
                "On average, 30 results were explicitly labeled by human judges using a six point scale ranging from Perfect down to Bad.",
                "Overall, there were over 83,000 results with explicit relevance judgments.",
                "In order to compute various statistics, documents with label Good or better will be considered relevant, and with lower labels to be non-relevant.",
                "Note that the experiments were performed over the results already highly ranked by a web search engine, which corresponds to a typical user experience which is limited to the small number of the highly ranked results for a typical web search query.",
                "The user interactions were collected over a period of 8 weeks using voluntary opt-in information.",
                "In total, over 1.2 million unique queries were instrumented, resulting in over 12 million individual interactions with the search engine.",
                "The data consisted of user interactions with the web search engine (e.g., clicking on a result link, going back to search results, etc.) performed after a query was submitted.",
                "These actions were aggregated across users and search sessions and converted to features in Table 4.1.",
                "To create the training, validation, and test query sets, we created three different random splits of 1,500 training, 500 validation, and 1000 test queries.",
                "The splits were done randomly by query, so that there was no overlap in training, validation, and test queries. 5.2 Evaluation Metrics We evaluate the ranking algorithms over a range of accepted information retrieval metrics, namely Precision at K (P(K)), Normalized Discounted Cumulative Gain (NDCG), and Mean Average Precision (MAP).",
                "Each metric focuses on a deferent aspect of system performance, as we describe below. • Precision at K: As the most intuitive metric, P(K) reports the fraction of documents ranked in the top K results that are labeled as relevant.",
                "In our setting, we require a relevant <br>document</br> to be labeled Good or higher.",
                "The position of relevant documents within the top K is irrelevant, and hence this metric measure overall user satisfaction with the top K results. • NDCG at K: NDCG is a retrieval measure devised specifically for web search evaluation [10].",
                "For a given query q, the ranked results are examined from the top ranked down, and the NDCG computed as:   = +−= K j jr qq jMN 1 )( )1log(/)12( Where Mq is a normalization constant calculated so that a perfect ordering would obtain NDCG of 1; and each r(j) is an integer relevance label (0=Bad and 5=Perfect) of result returned at position j.",
                "Note that unlabeled and Bad documents do not contribute to the sum, but will reduce NDCG for the query pushing down the relevant labeled documents, reducing their contributions.",
                "NDCG is well suited to web search evaluation, as it rewards relevant documents in the top ranked results more heavily than those ranked lower. • MAP: Average precision for each query is defined as the mean of the precision at K values computed after each relevant <br>document</br> was retrieved.",
                "The final MAP value is defined as the mean of average precisions of all queries in the test set.",
                "This metric is the most commonly used single-value summary of a run over a set of queries. 5.3 Ranking Methods Compared Recall that our goal is to quantify the effectiveness of implicit behavior for real web search.",
                "One dimension is to compare the utility of implicit feedback with other information available to a web search engine.",
                "Specifically, we compare effectiveness of implicit user behaviors with content-based matching, static page quality features, and combinations of all features. • BM25F: As a strong web search baseline we used the BM25F scoring, which was used in one of the best performing systems in the TREC 2004 Web track [23,27].",
                "BM25F and its variants have been extensively described and evaluated in IR literature, and hence serve as a strong, reproducible baseline.",
                "The BM25F variant we used for our experiments computes separate match scores for each field for a result <br>document</br> (e.g., body text, title, and anchor text), and incorporates query-independent linkbased information (e.g., PageRank, ClickDistance, and URL depth).",
                "The scoring function and field-specific tuning is described in detail in [23].",
                "Note that BM25F does not directly consider explicit or implicit feedback for tuning. • RN: The ranking produced by a neural net ranker (RankNet, described in Section 3.3) that learns to rank web search results by incorporating BM25F and a large number of additional static and dynamic features describing each search result.",
                "This system automatically learns weights for all features (including the BM25F score for a <br>document</br>) based on explicit human labels for a large set of queries.",
                "A system incorporating an implementation of RankNet is currently in use by a major search engine and can be considered representative of the state of the art in web search. • BM25F-RerankCT: The ranking produced by incorporating clickthrough statistics to reorder web search results ranked by BM25F above.",
                "Clickthrough is a particularly important special case of implicit feedback, and has been shown to correlate with result relevance.",
                "This is a special case of the ranking method in Section 3.1, with the weight wI set to 1000 and the ranking Id is simply the number of clicks on the result corresponding to d. In effect, this ranking brings to the top all returned web search results with at least one click (and orders them in decreasing order by number of clicks).",
                "The relative ranking of the remainder of results is unchanged and they are inserted below all clicked results.",
                "This method serves as our baseline implicit feedback reranking method.",
                "BM25F-RerankAll The ranking produced by reordering the BM25F results using all user behavior features (Section 4).",
                "This method learns a model of user preferences by correlating feature values with explicit relevance labels using the RankNet neural net algorithm (Section 4.2).",
                "At runtime, for a given query the implicit score Ir is computed for each result r with available user interaction features, and the implicit ranking is produced.",
                "The merged ranking is computed as described in Section 3.1.",
                "Based on the experiments over the development set we fix the value of wI to 3 (the effect of the wI parameter for this ranker turned out to be negligible). • BM25F+All: Ranking derived by training the RankNet (Section 3.3) learner over the features set of the BM25F score as well as all implicit feedback features (Section 3.2).",
                "We used the 2-layer implementation of RankNet [5] trained on the queries and labels in the training and validation sets. • RN+All: Ranking derived by training the 2-layer RankNet ranking algorithm (Section 3.3) over the union of all content, dynamic, and implicit feedback features (i.e., all of the features described above as well as all of the new implicit feedback features we introduced).",
                "The ranking methods above span the range of the information used for ranking, from not using the implicit or explicit feedback at all (i.e., BM25F) to a modern web search engine using hundreds of features and tuned on explicit judgments (RN).",
                "As we will show next, incorporating user behavior into these ranking systems dramatically improves the relevance of the returned documents. 6.",
                "EXPERIMENTAL RESULTS Implicit feedback for web search ranking can be exploited in a number of ways.",
                "We compare alternative methods of exploiting implicit feedback, both by re-ranking the top results (i.e., the BM25F-RerankCT and BM25F-RerankAll methods that reorder BM25F results), as well as by integrating the implicit features directly into the ranking process (i.e., the RN+ALL and BM25F+All methods which learn to rank results over the implicit feedback and other features).",
                "We compare our methods over strong baselines (BM25F and RN) over the NDCG, Precision at K, and MAP measures defined in Section 5.2.",
                "The results were averaged over three random splits of the overall dataset.",
                "Each split contained 1500 training, 500 validation, and 1000 test queries, all query sets disjoint.",
                "We first present the results over all 1000 test queries (i.e., including queries for which there are no implicit measures so we use the original web rankings).",
                "We then drill down to examine the effects on reranking for the attempted queries in more detail, analyzing where implicit feedback proved most beneficial.",
                "We first experimented with different methods of re-ranking the output of the BM25F search results.",
                "Figures 6.1 and 6.2 report NDCG and Precision for BM25F, as well as for the strategies reranking results with user feedback (Section 3.1).",
                "Incorporating all user feedback (either in reranking framework or as features to the learner directly) results in significant improvements (using two-tailed t-test with p=0.01) over both the original BM25F ranking as well as over reranking with clickthrough alone.",
                "The improvement is consistent across the top 10 results and largest for the top result: NDCG at 1 for BM25F+All is 0.622 compared to 0.518 of the original results, and precision at 1 similarly increases from 0.5 to 0.63.",
                "Based on these results we will use the direct feature combination (i.e., BM25F+All) ranker for subsequent comparisons involving implicit feedback. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 1 2 3 4 5 6 7 8 9 10K NDCG BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.1: NDCG at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K 0.35 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.2: Precision at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K Interestingly, using clickthrough alone, while giving significant benefit over the original BM25F ranking, is not as effective as considering the full set of features in Table 4.1.",
                "While we analyze user behavior (and most effective component features) in a separate paper [1], it is worthwhile to give a concrete example of the kind of noise inherent in real user feedback in web search setting. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 Result position Relativeclickfrequency PTR=2 PTR=3 PTR=5 Figure 6.3: Relative clickthrough frequency for queries with varying Position of Top Relevant result (PTR).",
                "If users considered only the relevance of a result to their query, they would click on the topmost relevant results.",
                "Unfortunately, as Joachims and others have shown, presentation also influences which results users click on quite dramatically.",
                "Users often click on results above the relevant one presumably because the short summaries do not provide enough information to make accurate relevance assessments and they have learned that on average topranked items are relevant.",
                "Figure 6.3 shows relative clickthrough frequencies for queries with known relevant items at positions other than the first position; the position of the top relevant result (PTR) ranges from 2-10 in the figure.",
                "For example, for queries with first relevant result at position 5 (PTR=5), there are more clicks on the non-relevant results in higher ranked positions than on the first relevant result at position 5.",
                "As we will see, learning over a richer behavior feature set, results in substantial accuracy improvement over clickthrough alone.",
                "We now consider incorporating user behavior into a much richer feature set, RN (Section 5.3) used by a major web search engine.",
                "RN incorporates BM25F, link-based features, and hundreds of other features.",
                "Figure 6.4 reports NDCG at K and Figure 6.5 reports Precision at K. Interestingly, while the original RN rankings are significantly more accurate than BM25F alone, incorporating implicit feedback features (BM25F+All) results in ranking that significantly outperforms the original RN rankings.",
                "In other words, implicit feedback incorporates sufficient information to replace the hundreds of other features available to the RankNet learner trained on the RN feature set. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 1 2 3 4 5 6 7 8 9 10K NDCG RN RN+All BM25 BM25+All Figure 6.4: NDCG at K for BM25F, BM25F+All, RN, and RN+All for varying K Furthermore, enriching the RN features with implicit feedback set exhibits significant gain on all measures, allowing RN+All to outperform all other methods.",
                "This demonstrates the complementary nature of implicit feedback with other features available to a state of the art web search engine. 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision RN RN+All BM25 BM25+All Figure 6.5: Precision at K for BM25F, BM25F+All, RN, and RN+All for varying K We summarize the performance of the different ranking methods in Table 6.1.",
                "We report the Mean Average Precision (MAP) score for each system.",
                "While not intuitive to interpret, MAP allows quantitative comparison on a single metric.",
                "The gains marked with * are significant at p=0.01 level using two tailed t-test.",
                "MAP Gain P(1) Gain BM25F 0.184 - 0.503BM25F-Rerank-CT 0.215 0.031* 0.577 0.073* BM25F-RerankImplicit 0.218 0.003 0.605 0.028* BM25F+Implicit 0.222 0.004 0.620 0.015* RN 0.215 - 0.597RN+All 0.248 0.033* 0.629 0.032* Table 6.1: Mean Average Precision (MAP) for all strategies.",
                "So far we reported results averaged across all queries in the test set.",
                "Unfortunately, less than half had sufficient interactions to attempt reranking.",
                "Out of the 1000 queries in test, between 46% and 49%, depending on the train-test split, had sufficient interaction information to make predictions (i.e., there was at least 1 search session in which at least 1 result URL was clicked on by the user).",
                "This is not surprising: web search is heavy-tailed, and there are many unique queries.",
                "We now consider the performance on the queries for which user interactions were available.",
                "Figure 6.6 reports NDCG for the subset of the test queries with the implicit feedback features.",
                "The gains at top 1 are dramatic.",
                "The NDCG at 1 of BM25F+All increases from 0.6 to 0.75 (a 31% relative gain), achieving performance comparable to RN+All operating over a much richer feature set. 0.6 0.65 0.7 0.75 0.8 1 3 5 10K NDCG RN RN+All BM25 BM25+All Figure 6.6: NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions Similarly, gains on precision at top 1 are substantial (Figure 6.7), and are likely to be apparent to web search users.",
                "When implicit feedback is available, the BM25F+All system returns relevant <br>document</br> at top 1 almost 70% of the time, compared 53% of the time when implicit feedback is not considered by the original BM25F system. 0.45 0.5 0.55 0.6 0.65 0.7 1 3 5 10K Precision RN RN+All BM25 BM25+All Figure 6.7: Precision at K NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions We summarize the results on the MAP measure for attempted queries in Table 6.2.",
                "MAP improvements are both substantial and significant, with improvements over the BM25F ranker most pronounced.",
                "Method MAP Gain P(1) Gain RN 0.269 0.632 RN+All 0.321 0.051 (19%) 0.693 0.061(10%) BM25F 0.236 0.525 BM25F+All 0.292 0.056 (24%) 0.687 0.162 (31%) Table 6.2: Mean Average Precision (MAP) on attempted queries for best performing methods We now analyze the cases where implicit feedback was shown most helpful.",
                "Figure 6.8 reports the MAP improvements over the baseline BM25F run for each query with MAP under 0.6.",
                "Note that most of the improvement is for poorly performing queries (i.e., MAP < 0.1).",
                "Interestingly, incorporating user behavior information degrades accuracy for queries with high original MAP score.",
                "One possible explanation is that these easy queries tend to be navigational (i.e., having a single, highly-ranked most appropriate answer), and user interactions with lower-ranked results may indicate divergent information needs that are better served by the less popular results (with correspondingly poor overall relevance ratings). 0 50 100 150 200 250 300 350 0.1 0.2 0.3 0.4 0.5 0.6 -0.4 -0.35 -0.3 -0.25 -0.2 -0.15 -0.1 -0.05 0 0.05 0.1 0.15 0.2 Frequency Average Gain Figure 6.8: Gain of BM25F+All over original BM25F ranking To summarize our experimental results, incorporating implicit feedback in real web search setting resulted in significant improvements over the original rankings, using both BM25F and RN baselines.",
                "Our rich set of implicit features, such as time on page and deviations from the average behavior, provides advantages over using clickthrough alone as an indicator of interest.",
                "Furthermore, incorporating implicit feedback features directly into the learned ranking function is more effective than using implicit feedback for reranking.",
                "The improvements observed over large test sets of queries (1,000 total, between 466 and 495 with implicit feedback available) are both substantial and statistically significant. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper we explored the utility of incorporating noisy implicit feedback obtained in a real web search setting to improve web search ranking.",
                "We performed a large-scale evaluation over 3,000 queries and more than 12 million user interactions with a major search engine, establishing the utility of incorporating noisy implicit feedback to improve web search relevance.",
                "We compared two alternatives of incorporating implicit feedback into the search process, namely reranking with implicit feedback and incorporating implicit feedback features directly into the trained ranking function.",
                "Our experiments showed significant improvement over methods that do not consider implicit feedback.",
                "The gains are particularly dramatic for the top K=1 result in the final ranking, with precision improvements as high as 31%, and the gains are substantial for all values of K. Our experiments showed that implicit user feedback can further improve web search performance, when incorporated directly with popular content- and link-based features.",
                "Interestingly, implicit feedback is particularly valuable for queries with poor original ranking of results (e.g., MAP lower than 0.1).",
                "One promising direction for future work is to apply recent research on automatically predicting query difficulty, and only attempt to incorporate implicit feedback for the difficult queries.",
                "As another research direction we are exploring methods for extending our predictions to the previously unseen queries (e.g., query clustering), which should further improve the web search experience of users.",
                "ACKNOWLEDGMENTS We thank Chris Burges and Matt Richardson for an implementation of RankNet for our experiments.",
                "We also thank Robert Ragno for his valuable suggestions and many discussions. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, S. Dumais, and R.Ragno, Learning User Interaction Models for Predicting Web Search Result Preferences.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan, HARD Track Overview in TREC 2003, High Accuracy Retrieval from Documents, 2003 [3] R. Baeza-Yates and B. Ribeiro-Neto, Modern Information Retrieval, Addison-Wesley, 1999. [4] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine, in Proceedings of WWW, 1997 [5] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning, 2005 [6] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [7] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest.",
                "IEEE Internet Computing. 2001 [8] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [9] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [10] K Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2000 [11] T. Joachims, Optimizing Search Engines Using Clickthrough Data.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [12] T. Joachims, L. Granka, B. Pang, H. Hembrooke, and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [13] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [14] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [15] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [16] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [17] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems.",
                "In Proceedings of the AAAI Workshop on Recommender Systems. 1998 [18] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [19] N. Pharo, N. and K. Järvelin.",
                "The SST method: a tool for analyzing web information search processes.",
                "In Information Processing & Management, 2004 [20] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [21] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (SIGKDD), 2005. [22] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in Proceedings of the ICML Workshop on Learning in Web Search, 2005 [23] S. E. Robertson, H. Zaragoza, and M. Taylor, Simple BM25 extension to multiple weighted fields, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [24] G. Salton & M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [25] E.M. Voorhees, D. Harman, Overview of TREC, 2001 [26] G.R.",
                "Xue, H.J.",
                "Zeng, Z. Chen, Y. Yu, W.Y.",
                "Ma, W.S.",
                "Xi, and W.G.",
                "Fan, Optimizing web search using web clickthrough data, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [27] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
                "Microsoft Cambridge at TREC 13: Web and Hard Tracks.",
                "In Proceedings of TREC 2004"
            ],
            "original_annotated_samples": [
                "We now relax this assumption by integrating implicit feedback features directly into the ranking process. 3.2 Ranking with Implicit Feedback Features Modern web search engines rank results based on a large number of features, including content-based features (i.e., how closely a query matches the text or title or anchor text of the <br>document</br>), and query-independent page quality features (e.g., PageRank of the <br>document</br> or the domain).",
                "For example, web search users can often determine whether a result is relevant by looking at the result title, URL, and summary - in many cases, looking at the original <br>document</br> is not necessary.",
                "In our setting, we require a relevant <br>document</br> to be labeled Good or higher.",
                "NDCG is well suited to web search evaluation, as it rewards relevant documents in the top ranked results more heavily than those ranked lower. • MAP: Average precision for each query is defined as the mean of the precision at K values computed after each relevant <br>document</br> was retrieved.",
                "The BM25F variant we used for our experiments computes separate match scores for each field for a result <br>document</br> (e.g., body text, title, and anchor text), and incorporates query-independent linkbased information (e.g., PageRank, ClickDistance, and URL depth)."
            ],
            "translated_annotated_samples": [
                "Ahora relajamos esta suposición al integrar características de retroalimentación implícita directamente en el proceso de clasificación. 3.2 Clasificación con Características de Retroalimentación Implícita Los motores de búsqueda web modernos clasifican los resultados en función de un gran número de características, incluidas las características basadas en el contenido (es decir, qué tan cerca coincide una consulta con el texto, el título o el texto de anclaje del <br>documento</br>) y las características de calidad de página independientes de la consulta (por ejemplo, PageRank del <br>documento</br> o del dominio).",
                "Por ejemplo, los usuarios de búsqueda en la web a menudo pueden determinar si un resultado es relevante al mirar el título del resultado, la URL y el resumen; en muchos casos, no es necesario mirar el <br>documento</br> original.",
                "En nuestro entorno, requerimos que un <br>documento</br> relevante sea etiquetado como Bueno o superior.",
                "NDCG es muy adecuado para la evaluación de búsquedas en la web, ya que recompensa de manera más intensa los <br>documento</br>s relevantes en los resultados mejor clasificados que aquellos clasificados más bajos. • MAP: La precisión promedio para cada consulta se define como la media de la precisión en los valores de K calculados después de que se recuperó cada <br>documento</br> relevante.",
                "La variante BM25F que utilizamos en nuestros experimentos calcula puntuaciones de coincidencia separadas para cada campo de un <br>documento</br> de resultado (por ejemplo, texto del cuerpo, título y texto del enlace) e incorpora información de enlaces independiente de la consulta (por ejemplo, PageRank, ClickDistance y profundidad de URL)."
            ],
            "translated_text": "Mejorando la clasificación de búsqueda web al incorporar información sobre el comportamiento del usuario. Eugene Agichtein, Microsoft Research eugeneag@microsoft.com Eric Brill, Microsoft Research brill@microsoft.com Susan Dumais, Microsoft Research sdumais@microsoft.com RESUMEN Mostramos que la incorporación de datos sobre el comportamiento del usuario puede mejorar significativamente el orden de los principales resultados en una configuración real de búsqueda web. Examinamos alternativas para incorporar retroalimentación en el proceso de clasificación y exploramos las contribuciones de la retroalimentación de usuarios en comparación con otras características comunes de búsqueda en la web. Informamos los resultados de una evaluación a gran escala con más de 3,000 consultas y 12 millones de interacciones de usuarios con un motor de búsqueda web popular. Mostramos que la incorporación de retroalimentación implícita puede aumentar otras características, mejorando la precisión de los algoritmos de clasificación de búsqueda web competitivos hasta en un 31% en comparación con el rendimiento original. Categorías y Descriptores de Asignaturas H.3.3 Búsqueda y Recuperación de Información - Retroalimentación de relevancia, proceso de búsqueda; H.3.5 Servicios de Información en Línea - Servicios basados en la web. Términos generales Algoritmos, Medición, Experimentación 1. INTRODUCCIÓN Millones de usuarios interactúan con los motores de búsqueda a diario. Ellos emiten consultas, siguen algunos de los enlaces en los resultados, hacen clic en anuncios, pasan tiempo en páginas, reformulan sus consultas y realizan otras acciones. Estas interacciones pueden servir como una valiosa fuente de información para ajustar y mejorar la clasificación de los resultados de búsqueda en la web y pueden complementar juicios explícitos más costosos. La retroalimentación implícita de relevancia para la clasificación y personalización se ha convertido en un área activa de investigación. El trabajo reciente de Joachims y otros explorando el feedback implícito en entornos controlados ha demostrado el valor de incorporar el feedback implícito en el proceso de clasificación. Nuestra motivación para este trabajo es comprender cómo se puede utilizar la retroalimentación implícita en un entorno operativo a gran escala para mejorar la recuperación. ¿Cómo se compara y complementa con la evidencia del contenido de la página, el texto del ancla o las características basadas en enlaces como inlinks o PageRank? Si bien es intuitivo que las interacciones de los usuarios con el motor de búsqueda web deberían revelar al menos alguna información que podría ser utilizada para la clasificación, estimar las preferencias de los usuarios en entornos reales de búsqueda web es un problema desafiante, ya que las interacciones reales tienden a ser más ruidosas de lo que comúnmente se asume en los entornos controlados de estudios previos. Nuestro artículo explora si la retroalimentación implícita puede ser útil en entornos realistas, donde la retroalimentación del usuario puede ser ruidosa (o adversa) y un motor de búsqueda web ya utiliza cientos de características y está altamente ajustado. Con este fin, exploramos diferentes enfoques para clasificar los resultados de búsqueda web utilizando el comportamiento real de los usuarios obtenido como parte de las interacciones normales con el motor de búsqueda web. Las contribuciones específicas de este artículo incluyen: • Análisis de alternativas para incorporar el comportamiento del usuario en la clasificación de búsqueda web (Sección 3). • Aplicación de un modelo robusto de retroalimentación implícita derivado de la minería de millones de interacciones de usuarios con un importante motor de búsqueda web (Sección 4). • Una evaluación a gran escala sobre consultas reales de usuarios y resultados de búsqueda, mostrando mejoras significativas derivadas de la incorporación de la retroalimentación del usuario (Sección 6). Resumimos nuestros hallazgos y discutimos extensiones al trabajo actual en la Sección 7, que concluye el artículo. ANTECEDENTES Y TRABAJO RELACIONADO Clasificar los resultados de búsqueda es un problema fundamental en la recuperación de información. La mayoría de los enfoques comunes se centran principalmente en la similitud entre la consulta y una página, así como en la calidad general de la página [3,4,24]. Sin embargo, con la creciente popularidad de los motores de búsqueda, la retroalimentación implícita (es decir, las acciones que los usuarios realizan al interactuar con el motor de búsqueda) se puede utilizar para mejorar las clasificaciones. Las medidas de relevancia implícitas han sido estudiadas por varios grupos de investigación. Un resumen de las medidas implícitas se recopila en Kelly y Teevan [14]. Esta investigación, si bien desarrolló valiosas ideas sobre medidas implícitas de relevancia, no se aplicó para mejorar la clasificación de los resultados de búsqueda en la web en entornos realistas. Estrechamente relacionado con nuestro trabajo, Joachims [11] recopiló medidas implícitas en lugar de medidas explícitas, introduciendo una técnica basada completamente en datos de clics para aprender funciones de clasificación. Fox et al. [8] exploraron la relación entre medidas implícitas y explícitas en la búsqueda web, y desarrollaron modelos bayesianos para correlacionar medidas implícitas y juicios explícitos de relevancia tanto para consultas individuales como para sesiones de búsqueda. Este trabajo consideró una amplia gama de comportamientos de usuario (por ejemplo, tiempo de permanencia, tiempo de desplazamiento, patrones de reformulación) además del popular comportamiento de clics. Sin embargo, el esfuerzo de modelado se centró en predecir juicios de relevancia explícitos a partir de acciones implícitas de los usuarios y no específicamente en aprender funciones de clasificación. Otros estudios sobre el comportamiento de los usuarios en la búsqueda web incluyen a Pharo y Järvelin [19], pero no se aplicaron directamente para mejorar la clasificación. Más recientemente, Joachims et al. [12] presentaron una evaluación empírica de la interpretación de la evidencia de clics. Al realizar estudios de seguimiento ocular y correlacionar las predicciones de sus estrategias con las calificaciones explícitas, los autores demostraron que es posible interpretar con precisión los clics en un entorno controlado de laboratorio. Desafortunadamente, no está claro en qué medida la investigación previa se aplica a la búsqueda web del mundo real. Al mismo tiempo, si bien el trabajo reciente (por ejemplo, [26]) sobre el uso de la información de clics para mejorar la clasificación de búsqueda en la web es prometedor, solo abarca un aspecto de las interacciones de los usuarios con los motores de búsqueda en la web. Nos basamos en investigaciones existentes para desarrollar técnicas robustas de interpretación del comportamiento del usuario para el entorno real de búsqueda en la web. En lugar de tratar a cada usuario como un experto confiable, agregamos información de múltiples trazas de sesiones de búsqueda de usuarios no confiables, como describimos en las siguientes dos secciones. 3. INTEGRANDO LA RETROALIMENTACIÓN IMPLÍCITA Consideramos dos enfoques complementarios para la clasificación con retroalimentación implícita: (1) tratar la retroalimentación implícita como evidencia independiente para clasificar resultados, y (2) integrar características de retroalimentación implícita directamente en el algoritmo de clasificación. Describimos a continuación los dos enfoques generales de clasificación. Las características específicas del feedback implícito se describen en la Sección 4, y los algoritmos para interpretar e incorporar el feedback implícito se describen en la Sección 5. 3.1 Feedback Implícito como Evidencia Independiente. El enfoque general es reordenar los resultados obtenidos por un motor de búsqueda web según los clics observados y otras interacciones de usuario para la consulta en sesiones de búsqueda anteriores. Cada resultado se le asigna una puntuación según la relevancia esperada/ satisfacción del usuario basada en interacciones previas, lo que resulta en un cierto orden de preferencia basado únicamente en las interacciones del usuario. Si bien ha habido un trabajo significativo en la fusión de múltiples clasificaciones, adaptamos un enfoque simple y robusto de ignorar las puntuaciones de los clasificadores originales, y en su lugar simplemente fusionamos los órdenes de clasificación. La razón principal para ignorar las puntuaciones originales es que, dado que los espacios de características y los algoritmos de aprendizaje son diferentes, las puntuaciones no son directamente comparables, y la re-normalización tiende a eliminar el beneficio de incorporar las puntuaciones del clasificador. Experimentamos con una variedad de funciones de fusión en el conjunto de desarrollo de consultas (y utilizando un conjunto de interacciones de un período de tiempo diferente de los conjuntos de evaluación final). Encontramos que una combinación heurística simple de fusión de rangos funciona bien y es robusta a variaciones en los valores de puntuación de los clasificadores originales. Para una consulta dada q, se calcula la puntuación implícita ISd para cada resultado d a partir de las características de interacción del usuario disponibles, lo que resulta en la clasificación implícita Id para cada resultado. Calculamos un puntaje combinado SM(d) para d al combinar los rangos obtenidos de la retroalimentación implícita, Id, con el rango original de d, Od: SM(d) = Od + wI * Id, si existe retroalimentación implícita; de lo contrario, SM(d) = Od. Donde el peso wI es un factor de escala ajustado heurísticamente que representa la importancia relativa de la retroalimentación implícita. Los resultados de la consulta se ordenan en valores decrecientes de SM para producir la clasificación final. Un caso especial de este modelo surge al establecer wI en un valor muy grande, lo que efectivamente obliga a que los resultados clicados se clasifiquen por encima de los resultados no clicados, una heurística intuitiva y efectiva que utilizaremos como referencia. Aplicar algoritmos de combinación de clasificadores y ordenadores más sofisticados puede resultar en mejoras adicionales, y es una dirección prometedora para trabajos futuros. El enfoque anterior asume que no hay interacciones entre las características subyacentes que producen la clasificación original de la búsqueda web y las características de retroalimentación implícitas. Ahora relajamos esta suposición al integrar características de retroalimentación implícita directamente en el proceso de clasificación. 3.2 Clasificación con Características de Retroalimentación Implícita Los motores de búsqueda web modernos clasifican los resultados en función de un gran número de características, incluidas las características basadas en el contenido (es decir, qué tan cerca coincide una consulta con el texto, el título o el texto de anclaje del <br>documento</br>) y las características de calidad de página independientes de la consulta (por ejemplo, PageRank del <br>documento</br> o del dominio). En la mayoría de los casos, se desarrollan métodos automáticos (o semiautomáticos) para ajustar la función de clasificación específica que combina estos valores de características. Por lo tanto, un enfoque natural es incorporar características de retroalimentación implícita directamente como características para el algoritmo de clasificación. Durante el entrenamiento o ajuste, el clasificador puede ser ajustado como antes pero con características adicionales. En tiempo de ejecución, el motor de búsqueda recuperaría las características de retroalimentación implícita asociadas con cada par de URL de consulta y resultado. Este modelo requiere que un algoritmo de clasificación sea robusto ante valores faltantes: más del 50% de las consultas a los motores de búsqueda web son únicas, sin disponer de retroalimentación implícita previa. Ahora describimos un clasificador que utilizamos para aprender sobre los conjuntos de características combinadas, incluyendo la retroalimentación implícita. 3.3 Aprendizaje para Clasificar los Resultados de Búsqueda en la Web Un aspecto clave de nuestro enfoque es aprovechar los avances recientes en el aprendizaje automático, en particular los algoritmos de clasificación entrenables para la búsqueda en la web y la recuperación de información (por ejemplo, [5, 11] y resultados clásicos revisados en [3]). En nuestro entorno, contamos con juicios explícitos de relevancia humana (etiquetas) para un conjunto de consultas de búsqueda en la web y resultados. Por lo tanto, una opción atractiva es utilizar una técnica de aprendizaje automático supervisado para aprender una función de clasificación que prediga mejor las evaluaciones de relevancia. RankNet es uno de esos algoritmos. Es un algoritmo de ajuste de red neuronal que optimiza los pesos de las características para que coincidan mejor con las preferencias de los usuarios proporcionadas explícitamente en pares. Si bien los algoritmos de entrenamiento específicos utilizados por RankNet están más allá del alcance de este documento, se describen en detalle en [5] e incluyen una evaluación exhaustiva y comparación con otros métodos de clasificación. Una característica atractiva de RankNet es su eficiencia tanto en el tiempo de entrenamiento como en el de ejecución: el ranking en tiempo de ejecución se puede calcular rápidamente y puede escalarse a la web, y el entrenamiento se puede realizar sobre miles de consultas y resultados juzgados asociados. Utilizamos una implementación de RankNet de 2 capas para modelar relaciones no lineales entre características. Además, RankNet puede aprender con muchas funciones de costo (diferenciables), por lo que puede aprender automáticamente una función de clasificación a partir de etiquetas proporcionadas por humanos, una alternativa atractiva a las técnicas de combinación de características heurísticas. Por lo tanto, también utilizaremos RankNet como un clasificador genérico para explorar la contribución de la retroalimentación implícita en diferentes alternativas de clasificación. 4. MODELO IMPLÍCITO DE RETROALIMENTACIÓN DEL USUARIO Nuestro objetivo es interpretar con precisión la retroalimentación ruidosa del usuario obtenida al rastrear las interacciones del usuario con el motor de búsqueda. Interpretar la retroalimentación implícita en un entorno real de búsqueda en la web no es una tarea fácil. Caracterizamos este problema en detalle en [1], donde motivamos y evaluamos una amplia variedad de modelos de actividades implícitas de usuario. El enfoque general es representar las acciones del usuario para cada resultado de búsqueda como un vector de características, y luego entrenar un clasificador en base a estas características para descubrir los valores de las características que indican resultados de búsqueda relevantes (y no relevantes). Primero resumimos brevemente nuestras características y modelo, y el enfoque de aprendizaje (Sección 4.2) para proporcionar suficiente información para replicar nuestros métodos de clasificación y los experimentos subsiguientes. 4.1 Representación de las acciones del usuario como características. Modelamos los comportamientos observados en la búsqueda web como una combinación de un \"componente de fondo (es decir, ruido independiente de la consulta y relevancia en el comportamiento del usuario, incluidos sesgos posicionales con interacciones de resultados), y un \"componente de relevancia (es decir, comportamiento específico de la consulta indicativo de la relevancia de un resultado para una consulta). Diseñamos nuestras características para aprovechar el comportamiento de usuario agregado. El conjunto de características está compuesto por características observadas directamente (calculadas directamente a partir de observaciones para cada consulta), así como características derivadas específicas de la consulta, calculadas como la desviación de la distribución general de valores independientes de la consulta para los valores de las características observadas directamente correspondientes. Las características utilizadas para representar las interacciones de los usuarios con los resultados de búsqueda en la web se resumen en la Tabla 4.1. Esta información se obtuvo a través de la instrumentación del lado del cliente opt-in de usuarios de un importante motor de búsqueda web. Incluimos las características tradicionales de retroalimentación implícita, como el recuento de clics en los resultados, así como nuestras características derivadas novedosas, como la desviación del número de clics observados para un par de consulta-URL dado del número esperado de clics en un resultado en la posición dada. También modelamos el comportamiento de navegación después de que se hace clic en un resultado, por ejemplo, el tiempo promedio de permanencia en la página para un par de consulta-URL dado, así como su desviación del tiempo de permanencia esperado (promedio). Además, el conjunto de características fue diseñado para proporcionar información esencial sobre la experiencia del usuario y hacer que la interpretación de la retroalimentación sea sólida. Por ejemplo, los usuarios de búsqueda en la web a menudo pueden determinar si un resultado es relevante al mirar el título del resultado, la URL y el resumen; en muchos casos, no es necesario mirar el <br>documento</br> original. Para modelar este aspecto de la experiencia del usuario, incluimos características como la superposición de palabras en el título y las palabras en la consulta (TitleOverlap) y la fracción de palabras compartidas por la consulta y el resumen del resultado. Características de clics Posición Posición de la URL en el ranking actual Frecuencia de clics Número de clics para esta consulta, par URL Probabilidad de clic Probabilidad de un clic para esta consulta y URL Desviación de clic Desviación de la probabilidad de clic esperada ¿Es el siguiente clic? 1 si se hizo clic en la siguiente posición, 0 de lo contrario ¿Es el clic anterior? 1 si se hizo clic en la posición anterior, 0 de lo contrario ¿Hay clic arriba? 1 si hay un clic arriba, 0 de lo contrario ¿Hay clic abajo? 1 si hay un clic abajo, 0 de lo contrario Características de navegación Tiempo en la página Tiempo de permanencia en la página Tiempo acumulado en la página Tiempo acumulado para todas las páginas siguientes después de la búsqueda Tiempo en el dominio Tiempo de permanencia acumulado para este dominio Tiempo en URL corta Tiempo acumulado en el prefijo de la URL, sin parámetros ¿Se siguió el enlace? 1 si se siguió el enlace al resultado, 0 de lo contrario ¿Coincidencia exacta de URL? 0 si se utilizó normalización agresiva, 1 de lo contrario ¿Redirigido? 1 si la URL inicial es la misma que la URL final, 0 de lo contrario ¿Camino desde la búsqueda? 1 si solo se siguieron enlaces después de la consulta, 0 de lo contrario Clics desde la búsqueda Número de saltos para llegar a la página desde la consulta Tiempo promedio de permanencia Tiempo promedio en la página para esta consulta Desviación del tiempo de permanencia Desviación del tiempo de permanencia promedio en la página Desviación acumulada Desviación del tiempo de permanencia acumulado promedio Desviación del dominio Desviación del tiempo de permanencia promedio en el dominio Características de texto de consulta Coincidencia de título Palabras compartidas entre la consulta y el título Coincidencia de resumen Palabras compartidas entre la consulta y el fragmento Coincidencia de URL de consulta Palabras compartidas entre la consulta y la URL Coincidencia de dominio de consulta Palabras compartidas entre la consulta y el dominio de la URL Longitud de la consulta Número de tokens en la consulta Superposición con la siguiente consulta Fracción de palabras compartidas con la siguiente consulta Tabla 4.1: Algunas características utilizadas para representar el historial de navegación posterior a la búsqueda para una consulta dada y una URL de resultado de búsqueda. Habiendo descrito nuestro conjunto de características, revisamos brevemente nuestro método general para derivar un modelo de comportamiento del usuario. 4.2 Derivación de un Modelo de Retroalimentación del Usuario Para aprender a interpretar el comportamiento observado del usuario, correlacionamos las acciones del usuario (es decir, las características en la Tabla 4.1 que representan las acciones) con los juicios explícitos del usuario para un conjunto de consultas de entrenamiento. Encontramos todas las instancias en nuestros registros de sesión donde se enviaron estas consultas al motor de búsqueda, y agregamos las características del comportamiento del usuario para todas las sesiones de búsqueda que involucran estas consultas. Cada par de consulta-URL observado está representado por las características en la Tabla 4.1, con valores promediados en todas las sesiones de búsqueda, y asignado una de las seis etiquetas de relevancia posibles, que van desde Perfecto hasta Malo, según los juicios explícitos de relevancia. Estos vectores de características etiquetados se utilizan como entrada para el algoritmo de entrenamiento RankNet (Sección 3.3), el cual produce un modelo de comportamiento de usuario entrenado. Este enfoque es particularmente atractivo ya que no requiere heurísticas más allá de la ingeniería de características. El modelo de comportamiento del usuario resultante se utiliza para ayudar a clasificar los resultados de búsqueda en la web, ya sea directamente o en combinación con otras características, como se describe a continuación. 5. CONFIGURACIÓN EXPERIMENTAL El objetivo final de incorporar retroalimentación implícita en la clasificación es mejorar la relevancia de los resultados de búsqueda web devueltos. Por lo tanto, comparamos los métodos de clasificación sobre un gran conjunto de consultas evaluadas con etiquetas de relevancia explícitas proporcionadas por jueces humanos. Para que la evaluación sea realista, obtuvimos una muestra aleatoria de consultas de registros de búsqueda web de un motor de búsqueda importante, con resultados asociados y rastros de acciones de usuario. Describimos este conjunto de datos en detalle a continuación. Nuestros indicadores se describen en la Sección 5.2 que utilizamos para evaluar las alternativas de clasificación, enumeradas en la Sección 5.3 en los experimentos de la Sección 6. 5.1 Conjuntos de datos Comparamos nuestros métodos de clasificación en una muestra aleatoria de 3,000 consultas de los registros de consultas del motor de búsqueda. Las consultas fueron extraídas de los registros de manera uniforme al azar por token sin reemplazo, lo que resultó en una muestra de consultas representativa de la distribución general de consultas. En promedio, 30 resultados fueron etiquetados explícitamente por jueces humanos utilizando una escala de seis puntos que va desde Perfecto hasta Malo. En total, hubo más de 83,000 resultados con juicios de relevancia explícitos. Para calcular diversas estadísticas, se considerarán relevantes los documentos con la etiqueta Buena o mejor, y los documentos con etiquetas inferiores se considerarán no relevantes. Se debe tener en cuenta que los experimentos se realizaron sobre los resultados ya altamente clasificados por un motor de búsqueda web, lo cual corresponde a una experiencia de usuario típica que se limita al pequeño número de resultados altamente clasificados para una consulta típica de búsqueda web. Las interacciones de los usuarios fueron recopiladas durante un período de 8 semanas utilizando información voluntaria de participación. En total, se registraron más de 1.2 millones de consultas únicas, lo que resultó en más de 12 millones de interacciones individuales con el motor de búsqueda. Los datos consistían en las interacciones de los usuarios con el motor de búsqueda web (por ejemplo, hacer clic en un enlace de resultado, regresar a los resultados de búsqueda, etc.) realizadas después de enviar una consulta. Estas acciones fueron agregadas entre usuarios y sesiones de búsqueda y convertidas en características en la Tabla 4.1. Para crear los conjuntos de consultas de entrenamiento, validación y prueba, creamos tres divisiones aleatorias diferentes de 1,500 consultas de entrenamiento, 500 de validación y 1000 de prueba. Las divisiones se realizaron aleatoriamente por consulta, de modo que no hubiera superposición en las consultas de entrenamiento, validación y prueba. 5.2 Métricas de Evaluación Evaluamos los algoritmos de clasificación en una variedad de métricas de recuperación de información aceptadas, a saber, Precisión en K (P(K)), Ganancia Acumulada Descontada Normalizada (NDCG) y Precisión Promedio Media (MAP). Cada métrica se enfoca en un aspecto diferente del rendimiento del sistema, como describimos a continuación. • Precisión en K: Como la métrica más intuitiva, P(K) informa la fracción de documentos clasificados en los primeros K resultados que están etiquetados como relevantes. En nuestro entorno, requerimos que un <br>documento</br> relevante sea etiquetado como Bueno o superior. La posición de los documentos relevantes dentro de los primeros K no es relevante, por lo que esta métrica mide la satisfacción general del usuario con los resultados principales de K. • NDCG en K: NDCG es una medida de recuperación diseñada específicamente para la evaluación de búsqueda en la web [10]. Para una consulta dada q, los resultados clasificados se examinan desde el mejor clasificado hacia abajo, y el NDCG se calcula como: = +−= K j jr qq jMN 1 )( )1log(/)12( Donde Mq es una constante de normalización calculada de manera que un orden perfecto obtendría un NDCG de 1; y cada r(j) es una etiqueta de relevancia entera (0=Malo y 5=Perfecto) del resultado devuelto en la posición j. Ten en cuenta que los documentos sin etiquetar y los documentos malos no contribuyen a la suma, pero reducirán el NDCG para la consulta al empujar hacia abajo los documentos etiquetados relevantes, disminuyendo sus contribuciones. NDCG es muy adecuado para la evaluación de búsquedas en la web, ya que recompensa de manera más intensa los <br>documento</br>s relevantes en los resultados mejor clasificados que aquellos clasificados más bajos. • MAP: La precisión promedio para cada consulta se define como la media de la precisión en los valores de K calculados después de que se recuperó cada <br>documento</br> relevante. El valor MAP final se define como la media de las precisiones promedio de todas las consultas en el conjunto de pruebas. Esta métrica es el resumen de un solo valor más comúnmente utilizado de una ejecución sobre un conjunto de consultas. 5.3 Métodos de clasificación comparados. Recuerde que nuestro objetivo es cuantificar la efectividad del comportamiento implícito para la búsqueda web real. Una dimensión es comparar la utilidad de la retroalimentación implícita con otra información disponible para un motor de búsqueda web. Específicamente, comparamos la efectividad de los comportamientos implícitos de los usuarios con la coincidencia basada en el contenido, las características de calidad de la página estática y combinaciones de todas las características. • BM25F: Como referencia sólida de búsqueda web, utilizamos la puntuación BM25F, que se utilizó en uno de los sistemas con mejor rendimiento en la pista web TREC 2004 [23,27]. BM25F y sus variantes han sido extensamente descritas y evaluadas en la literatura de IR, por lo tanto, sirven como una línea base sólida y reproducible. La variante BM25F que utilizamos en nuestros experimentos calcula puntuaciones de coincidencia separadas para cada campo de un <br>documento</br> de resultado (por ejemplo, texto del cuerpo, título y texto del enlace) e incorpora información de enlaces independiente de la consulta (por ejemplo, PageRank, ClickDistance y profundidad de URL). ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "implicit relevance feedback": {
            "translated_key": "retroalimentación implícita",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Improving Web Search Ranking by Incorporating User Behavior Information Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com ABSTRACT We show that incorporating user behavior data can significantly improve ordering of top results in real web search setting.",
                "We examine alternatives for incorporating feedback into the ranking process and explore the contributions of user feedback compared to other common web search features.",
                "We report results of a large scale evaluation over 3,000 queries and 12 million user interactions with a popular web search engine.",
                "We show that incorporating implicit feedback can augment other features, improving the accuracy of a competitive web search ranking algorithms by as much as 31% relative to the original performance.",
                "Categories and Subject Descriptors H.3.3 Information Search and Retrieval - Relevance feedback, search process; H.3.5 Online Information Services - Web-based services.",
                "General Terms Algorithms, Measurement, Experimentation 1.",
                "INTRODUCTION Millions of users interact with search engines daily.",
                "They issue queries, follow some of the links in the results, click on ads, spend time on pages, reformulate their queries, and perform other actions.",
                "These interactions can serve as a valuable source of information for tuning and improving web search result ranking and can compliment more costly explicit judgments.",
                "<br>implicit relevance feedback</br> for ranking and personalization has become an active area of research.",
                "Recent work by Joachims and others exploring implicit feedback in controlled environments have shown the value of incorporating implicit feedback into the ranking process.",
                "Our motivation for this work is to understand how implicit feedback can be used in a large-scale operational environment to improve retrieval.",
                "How does it compare to and compliment evidence from page content, anchor text, or link-based features such as inlinks or PageRank?",
                "While it is intuitive that user interactions with the web search engine should reveal at least some information that could be used for ranking, estimating user preferences in real web search settings is a challenging problem, since real user interactions tend to be more noisy than commonly assumed in the controlled settings of previous studies.",
                "Our paper explores whether implicit feedback can be helpful in realistic environments, where user feedback can be noisy (or adversarial) and a web search engine already uses hundreds of features and is heavily tuned.",
                "To this end, we explore different approaches for ranking web search results using real user behavior obtained as part of normal interactions with the web search engine.",
                "The specific contributions of this paper include: • Analysis of alternatives for incorporating user behavior into web search ranking (Section 3). • An application of a robust implicit feedback model derived from mining millions of user interactions with a major web search engine (Section 4). • A large scale evaluation over real user queries and search results, showing significant improvements derived from incorporating user feedback (Section 6).",
                "We summarize our findings and discuss extensions to the current work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "Most common approaches primarily focus on similarity of query and a page, as well as the overall page quality [3,4,24].",
                "However, with increasing popularity of search engines, implicit feedback (i.e., the actions users take when interacting with the search engine) can be used to improve the rankings.",
                "Implicit relevance measures have been studied by several research groups.",
                "An overview of implicit measures is compiled in Kelly and Teevan [14].",
                "This research, while developing valuable insights into implicit relevance measures, was not applied to improve the ranking of web search results in realistic settings.",
                "Closely related to our work, Joachims [11] collected implicit measures in place of explicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "Fox et al. [8] explored the relationship between implicit and explicit measures in Web search, and developed Bayesian models to correlate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "This work considered a wide range of user behaviors (e.g., dwell time, scroll time, reformulation patterns) in addition to the popular clickthrough behavior.",
                "However, the modeling effort was aimed at predicting explicit relevance judgments from implicit user actions and not specifically at learning ranking functions.",
                "Other studies of user behavior in web search include Pharo and Järvelin [19], but were not directly applied to improve ranking.",
                "More recently, Joachims et al. [12] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthroughs in a controlled, laboratory setting.",
                "Unfortunately, the extent to which previous research applies to real-world web search is unclear.",
                "At the same time, while recent work (e.g., [26]) on using clickthrough information for improving web search ranking is promising, it captures only one aspect of the user interactions with web search engines.",
                "We build on existing research to develop robust user behavior interpretation techniques for the real web search setting.",
                "Instead of treating each user as a reliable expert, we aggregate information from multiple, unreliable, user search session traces, as we describe in the next two sections. 3.",
                "INCORPORATING IMPLICIT FEEDBACK We consider two complementary approaches to ranking with implicit feedback: (1) treating implicit feedback as independent evidence for ranking results, and (2) integrating implicit feedback features directly into the ranking algorithm.",
                "We describe the two general ranking approaches next.",
                "The specific implicit feedback features are described in Section 4, and the algorithms for interpreting and incorporating implicit feedback are described in Section 5. 3.1 Implicit Feedback as Independent Evidence The general approach is to re-rank the results obtained by a web search engine according to observed clickthrough and other user interactions for the query in previous search sessions.",
                "Each result is assigned a score according to expected relevance/user satisfaction based on previous interactions, resulting in some preference ordering based on user interactions alone.",
                "While there has been significant work on merging multiple rankings, we adapt a simple and robust approach of ignoring the original rankers scores, and instead simply merge the rank orders.",
                "The main reason for ignoring the original scores is that since the feature spaces and learning algorithms are different, the scores are not directly comparable, and re-normalization tends to remove the benefit of incorporating classifier scores.",
                "We experimented with a variety of merging functions on the development set of queries (and using a set of interactions from a different time period from final evaluation sets).",
                "We found that a simple rank merging heuristic combination works well, and is robust to variations in score values from original rankers.",
                "For a given query q, the implicit score ISd is computed for each result d from available user interaction features, resulting in the implicit rank Id for each result.",
                "We compute a merged score SM(d) for d by combining the ranks obtained from implicit feedback, Id with the original rank of d, Od:     ¡     ¢ £ + + + + = otherwise O dforexistsfeedbackimplicitif OI w wOIdS d dd I IddM 1 1 1 1 1 1 ),,,( where the weight wI is a heuristically tuned scaling factor representing the relative importance of the implicit feedback.",
                "The query results are ordered in by decreasing values of SM to produce the final ranking.",
                "One special case of this model arises when setting wI to a very large value, effectively forcing clicked results to be ranked higher than un-clicked results - an intuitive and effective heuristic that we will use as a baseline.",
                "Applying more sophisticated classifier and ranker combination algorithms may result in additional improvements, and is a promising direction for future work.",
                "The approach above assumes that there are no interactions between the underlying features producing the original web search ranking and the implicit feedback features.",
                "We now relax this assumption by integrating implicit feedback features directly into the ranking process. 3.2 Ranking with Implicit Feedback Features Modern web search engines rank results based on a large number of features, including content-based features (i.e., how closely a query matches the text or title or anchor text of the document), and query-independent page quality features (e.g., PageRank of the document or the domain).",
                "In most cases, automatic (or semiautomatic) methods are developed for tuning the specific ranking function that combines these feature values.",
                "Hence, a natural approach is to incorporate implicit feedback features directly as features for the ranking algorithm.",
                "During training or tuning, the ranker can be tuned as before but with additional features.",
                "At runtime, the search engine would fetch the implicit feedback features associated with each query-result URL pair.",
                "This model requires a ranking algorithm to be robust to missing values: more than 50% of queries to web search engines are unique, with no previous implicit feedback available.",
                "We now describe such a ranker that we used to learn over the combined feature sets including implicit feedback. 3.3 Learning to Rank Web Search Results A key aspect of our approach is exploiting recent advances in machine learning, namely trainable ranking algorithms for web search and information retrieval (e.g., [5, 11] and classical results reviewed in [3]).",
                "In our setting, explicit human relevance judgments (labels) are available for a set of web search queries and results.",
                "Hence, an attractive choice to use is a supervised machine learning technique to learn a ranking function that best predicts relevance judgments.",
                "RankNet is one such algorithm.",
                "It is a neural net tuning algorithm that optimizes feature weights to best match explicitly provided pairwise user preferences.",
                "While the specific training algorithms used by RankNet are beyond the scope of this paper, it is described in detail in [5] and includes extensive evaluation and comparison with other ranking methods.",
                "An attractive feature of RankNet is both train- and run-time efficiency - runtime ranking can be quickly computed and can scale to the web, and training can be done over thousands of queries and associated judged results.",
                "We use a 2-layer implementation of RankNet in order to model non-linear relationships between features.",
                "Furthermore, RankNet can learn with many (differentiable) cost functions, and hence can automatically learn a ranking function from human-provided labels, an attractive alternative to heuristic feature combination techniques.",
                "Hence, we will also use RankNet as a generic ranker to explore the contribution of implicit feedback for different ranking alternatives. 4.",
                "IMPLICIT USER FEEDBACK MODEL Our goal is to accurately interpret noisy user feedback obtained as by tracing user interactions with the search engine.",
                "Interpreting implicit feedback in real web search setting is not an easy task.",
                "We characterize this problem in detail in [1], where we motivate and evaluate a wide variety of models of implicit user activities.",
                "The general approach is to represent user actions for each search result as a vector of features, and then train a ranker on these features to discover feature values indicative of relevant (and nonrelevant) search results.",
                "We first briefly summarize our features and model, and the learning approach (Section 4.2) in order to provide sufficient information to replicate our ranking methods and the subsequent experiments. 4.1 Representing User Actions as Features We model observed web search behaviors as a combination of a ``background component (i.e., query- and relevance-independent noise in user behavior, including positional biases with result interactions), and a ``relevance component (i.e., query-specific behavior indicative of relevance of a result to a query).",
                "We design our features to take advantage of aggregated user behavior.",
                "The feature set is comprised of directly observed features (computed directly from observations for each query), as well as queryspecific derived features, computed as the deviation from the overall query-independent distribution of values for the corresponding directly observed feature values.",
                "The features used to represent user interactions with web search results are summarized in Table 4.1.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "We include the traditional implicit feedback features such as clickthrough counts for the results, as well as our novel derived features such as the deviation of the observed clickthrough number for a given query-URL pair from the expected number of clicks on a result in the given position.",
                "We also model the browsing behavior after a result was clicked - e.g., the average page dwell time for a given query-URL pair, as well as its deviation from the expected (average) dwell time.",
                "Furthermore, the feature set was designed to provide essential information about the user experience to make feedback interpretation robust.",
                "For example, web search users can often determine whether a result is relevant by looking at the result title, URL, and summary - in many cases, looking at the original document is not necessary.",
                "To model this aspect of user experience we include features such as overlap in words in title and words in query (TitleOverlap) and the fraction of words shared by the query and the result summary.",
                "Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickProbability Probability of a click for this query and URL ClickDeviation Deviation from expected click probability IsNextClicked 1 if clicked on next position, 0 otherwise IsPreviousClicked 1 if clicked on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, no parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from average dwell time on page CumulativeDeviation Deviation from average cumulative dwell time DomainDeviation Deviation from average dwell time on domain Query-text features TitleOverlap Words shared between query and title SummaryOverlap Words shared between query and snippet QueryURLOverlap Words shared between query and URL QueryDomainOverlap Words shared between query and URL domain QueryLength Number of tokens in query QueryNextOverlap Fraction of words shared with next query Table 4.1: Some features used to represent post-search navigation history for a given query and search result URL.",
                "Having described our feature set, we briefly review our general method for deriving a user behavior model. 4.2 Deriving a User Feedback Model To learn to interpret the observed user behavior, we correlate user actions (i.e., the features in Table 4.1 representing the actions) with the explicit user judgments for a set of training queries.",
                "We find all the instances in our session logs where these queries were submitted to the search engine, and aggregate the user behavior features for all search sessions involving these queries.",
                "Each observed query-URL pair is represented by the features in Table 4.1, with values averaged over all search sessions, and assigned one of six possible relevance labels, ranging from Perfect to Bad, as assigned by explicit relevance judgments.",
                "These labeled feature vectors are used as input to the RankNet training algorithm (Section 3.3) which produces a trained user behavior model.",
                "This approach is particularly attractive as it does not require heuristics beyond feature engineering.",
                "The resulting user behavior model is used to help rank web search resultseither directly or in combination with other features, as described below. 5.",
                "EXPERIMENTAL SETUP The ultimate goal of incorporating implicit feedback into ranking is to improve the relevance of the returned web search results.",
                "Hence, we compare the ranking methods over a large set of judged queries with explicit relevance labels provided by human judges.",
                "In order for the evaluation to be realistic we obtained a random sample of queries from web search logs of a major search engine, with associated results and traces for user actions.",
                "We describe this dataset in detail next.",
                "Our metrics are described in Section 5.2 that we use to evaluate the ranking alternatives, listed in Section 5.3 in the experiments of Section 6. 5.1 Datasets We compared our ranking methods over a random sample of 3,000 queries from the search engine query logs.",
                "The queries were drawn from the logs uniformly at random by token without replacement, resulting in a query sample representative of the overall query distribution.",
                "On average, 30 results were explicitly labeled by human judges using a six point scale ranging from Perfect down to Bad.",
                "Overall, there were over 83,000 results with explicit relevance judgments.",
                "In order to compute various statistics, documents with label Good or better will be considered relevant, and with lower labels to be non-relevant.",
                "Note that the experiments were performed over the results already highly ranked by a web search engine, which corresponds to a typical user experience which is limited to the small number of the highly ranked results for a typical web search query.",
                "The user interactions were collected over a period of 8 weeks using voluntary opt-in information.",
                "In total, over 1.2 million unique queries were instrumented, resulting in over 12 million individual interactions with the search engine.",
                "The data consisted of user interactions with the web search engine (e.g., clicking on a result link, going back to search results, etc.) performed after a query was submitted.",
                "These actions were aggregated across users and search sessions and converted to features in Table 4.1.",
                "To create the training, validation, and test query sets, we created three different random splits of 1,500 training, 500 validation, and 1000 test queries.",
                "The splits were done randomly by query, so that there was no overlap in training, validation, and test queries. 5.2 Evaluation Metrics We evaluate the ranking algorithms over a range of accepted information retrieval metrics, namely Precision at K (P(K)), Normalized Discounted Cumulative Gain (NDCG), and Mean Average Precision (MAP).",
                "Each metric focuses on a deferent aspect of system performance, as we describe below. • Precision at K: As the most intuitive metric, P(K) reports the fraction of documents ranked in the top K results that are labeled as relevant.",
                "In our setting, we require a relevant document to be labeled Good or higher.",
                "The position of relevant documents within the top K is irrelevant, and hence this metric measure overall user satisfaction with the top K results. • NDCG at K: NDCG is a retrieval measure devised specifically for web search evaluation [10].",
                "For a given query q, the ranked results are examined from the top ranked down, and the NDCG computed as:   = +−= K j jr qq jMN 1 )( )1log(/)12( Where Mq is a normalization constant calculated so that a perfect ordering would obtain NDCG of 1; and each r(j) is an integer relevance label (0=Bad and 5=Perfect) of result returned at position j.",
                "Note that unlabeled and Bad documents do not contribute to the sum, but will reduce NDCG for the query pushing down the relevant labeled documents, reducing their contributions.",
                "NDCG is well suited to web search evaluation, as it rewards relevant documents in the top ranked results more heavily than those ranked lower. • MAP: Average precision for each query is defined as the mean of the precision at K values computed after each relevant document was retrieved.",
                "The final MAP value is defined as the mean of average precisions of all queries in the test set.",
                "This metric is the most commonly used single-value summary of a run over a set of queries. 5.3 Ranking Methods Compared Recall that our goal is to quantify the effectiveness of implicit behavior for real web search.",
                "One dimension is to compare the utility of implicit feedback with other information available to a web search engine.",
                "Specifically, we compare effectiveness of implicit user behaviors with content-based matching, static page quality features, and combinations of all features. • BM25F: As a strong web search baseline we used the BM25F scoring, which was used in one of the best performing systems in the TREC 2004 Web track [23,27].",
                "BM25F and its variants have been extensively described and evaluated in IR literature, and hence serve as a strong, reproducible baseline.",
                "The BM25F variant we used for our experiments computes separate match scores for each field for a result document (e.g., body text, title, and anchor text), and incorporates query-independent linkbased information (e.g., PageRank, ClickDistance, and URL depth).",
                "The scoring function and field-specific tuning is described in detail in [23].",
                "Note that BM25F does not directly consider explicit or implicit feedback for tuning. • RN: The ranking produced by a neural net ranker (RankNet, described in Section 3.3) that learns to rank web search results by incorporating BM25F and a large number of additional static and dynamic features describing each search result.",
                "This system automatically learns weights for all features (including the BM25F score for a document) based on explicit human labels for a large set of queries.",
                "A system incorporating an implementation of RankNet is currently in use by a major search engine and can be considered representative of the state of the art in web search. • BM25F-RerankCT: The ranking produced by incorporating clickthrough statistics to reorder web search results ranked by BM25F above.",
                "Clickthrough is a particularly important special case of implicit feedback, and has been shown to correlate with result relevance.",
                "This is a special case of the ranking method in Section 3.1, with the weight wI set to 1000 and the ranking Id is simply the number of clicks on the result corresponding to d. In effect, this ranking brings to the top all returned web search results with at least one click (and orders them in decreasing order by number of clicks).",
                "The relative ranking of the remainder of results is unchanged and they are inserted below all clicked results.",
                "This method serves as our baseline implicit feedback reranking method.",
                "BM25F-RerankAll The ranking produced by reordering the BM25F results using all user behavior features (Section 4).",
                "This method learns a model of user preferences by correlating feature values with explicit relevance labels using the RankNet neural net algorithm (Section 4.2).",
                "At runtime, for a given query the implicit score Ir is computed for each result r with available user interaction features, and the implicit ranking is produced.",
                "The merged ranking is computed as described in Section 3.1.",
                "Based on the experiments over the development set we fix the value of wI to 3 (the effect of the wI parameter for this ranker turned out to be negligible). • BM25F+All: Ranking derived by training the RankNet (Section 3.3) learner over the features set of the BM25F score as well as all implicit feedback features (Section 3.2).",
                "We used the 2-layer implementation of RankNet [5] trained on the queries and labels in the training and validation sets. • RN+All: Ranking derived by training the 2-layer RankNet ranking algorithm (Section 3.3) over the union of all content, dynamic, and implicit feedback features (i.e., all of the features described above as well as all of the new implicit feedback features we introduced).",
                "The ranking methods above span the range of the information used for ranking, from not using the implicit or explicit feedback at all (i.e., BM25F) to a modern web search engine using hundreds of features and tuned on explicit judgments (RN).",
                "As we will show next, incorporating user behavior into these ranking systems dramatically improves the relevance of the returned documents. 6.",
                "EXPERIMENTAL RESULTS Implicit feedback for web search ranking can be exploited in a number of ways.",
                "We compare alternative methods of exploiting implicit feedback, both by re-ranking the top results (i.e., the BM25F-RerankCT and BM25F-RerankAll methods that reorder BM25F results), as well as by integrating the implicit features directly into the ranking process (i.e., the RN+ALL and BM25F+All methods which learn to rank results over the implicit feedback and other features).",
                "We compare our methods over strong baselines (BM25F and RN) over the NDCG, Precision at K, and MAP measures defined in Section 5.2.",
                "The results were averaged over three random splits of the overall dataset.",
                "Each split contained 1500 training, 500 validation, and 1000 test queries, all query sets disjoint.",
                "We first present the results over all 1000 test queries (i.e., including queries for which there are no implicit measures so we use the original web rankings).",
                "We then drill down to examine the effects on reranking for the attempted queries in more detail, analyzing where implicit feedback proved most beneficial.",
                "We first experimented with different methods of re-ranking the output of the BM25F search results.",
                "Figures 6.1 and 6.2 report NDCG and Precision for BM25F, as well as for the strategies reranking results with user feedback (Section 3.1).",
                "Incorporating all user feedback (either in reranking framework or as features to the learner directly) results in significant improvements (using two-tailed t-test with p=0.01) over both the original BM25F ranking as well as over reranking with clickthrough alone.",
                "The improvement is consistent across the top 10 results and largest for the top result: NDCG at 1 for BM25F+All is 0.622 compared to 0.518 of the original results, and precision at 1 similarly increases from 0.5 to 0.63.",
                "Based on these results we will use the direct feature combination (i.e., BM25F+All) ranker for subsequent comparisons involving implicit feedback. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 1 2 3 4 5 6 7 8 9 10K NDCG BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.1: NDCG at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K 0.35 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.2: Precision at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K Interestingly, using clickthrough alone, while giving significant benefit over the original BM25F ranking, is not as effective as considering the full set of features in Table 4.1.",
                "While we analyze user behavior (and most effective component features) in a separate paper [1], it is worthwhile to give a concrete example of the kind of noise inherent in real user feedback in web search setting. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 Result position Relativeclickfrequency PTR=2 PTR=3 PTR=5 Figure 6.3: Relative clickthrough frequency for queries with varying Position of Top Relevant result (PTR).",
                "If users considered only the relevance of a result to their query, they would click on the topmost relevant results.",
                "Unfortunately, as Joachims and others have shown, presentation also influences which results users click on quite dramatically.",
                "Users often click on results above the relevant one presumably because the short summaries do not provide enough information to make accurate relevance assessments and they have learned that on average topranked items are relevant.",
                "Figure 6.3 shows relative clickthrough frequencies for queries with known relevant items at positions other than the first position; the position of the top relevant result (PTR) ranges from 2-10 in the figure.",
                "For example, for queries with first relevant result at position 5 (PTR=5), there are more clicks on the non-relevant results in higher ranked positions than on the first relevant result at position 5.",
                "As we will see, learning over a richer behavior feature set, results in substantial accuracy improvement over clickthrough alone.",
                "We now consider incorporating user behavior into a much richer feature set, RN (Section 5.3) used by a major web search engine.",
                "RN incorporates BM25F, link-based features, and hundreds of other features.",
                "Figure 6.4 reports NDCG at K and Figure 6.5 reports Precision at K. Interestingly, while the original RN rankings are significantly more accurate than BM25F alone, incorporating implicit feedback features (BM25F+All) results in ranking that significantly outperforms the original RN rankings.",
                "In other words, implicit feedback incorporates sufficient information to replace the hundreds of other features available to the RankNet learner trained on the RN feature set. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 1 2 3 4 5 6 7 8 9 10K NDCG RN RN+All BM25 BM25+All Figure 6.4: NDCG at K for BM25F, BM25F+All, RN, and RN+All for varying K Furthermore, enriching the RN features with implicit feedback set exhibits significant gain on all measures, allowing RN+All to outperform all other methods.",
                "This demonstrates the complementary nature of implicit feedback with other features available to a state of the art web search engine. 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision RN RN+All BM25 BM25+All Figure 6.5: Precision at K for BM25F, BM25F+All, RN, and RN+All for varying K We summarize the performance of the different ranking methods in Table 6.1.",
                "We report the Mean Average Precision (MAP) score for each system.",
                "While not intuitive to interpret, MAP allows quantitative comparison on a single metric.",
                "The gains marked with * are significant at p=0.01 level using two tailed t-test.",
                "MAP Gain P(1) Gain BM25F 0.184 - 0.503BM25F-Rerank-CT 0.215 0.031* 0.577 0.073* BM25F-RerankImplicit 0.218 0.003 0.605 0.028* BM25F+Implicit 0.222 0.004 0.620 0.015* RN 0.215 - 0.597RN+All 0.248 0.033* 0.629 0.032* Table 6.1: Mean Average Precision (MAP) for all strategies.",
                "So far we reported results averaged across all queries in the test set.",
                "Unfortunately, less than half had sufficient interactions to attempt reranking.",
                "Out of the 1000 queries in test, between 46% and 49%, depending on the train-test split, had sufficient interaction information to make predictions (i.e., there was at least 1 search session in which at least 1 result URL was clicked on by the user).",
                "This is not surprising: web search is heavy-tailed, and there are many unique queries.",
                "We now consider the performance on the queries for which user interactions were available.",
                "Figure 6.6 reports NDCG for the subset of the test queries with the implicit feedback features.",
                "The gains at top 1 are dramatic.",
                "The NDCG at 1 of BM25F+All increases from 0.6 to 0.75 (a 31% relative gain), achieving performance comparable to RN+All operating over a much richer feature set. 0.6 0.65 0.7 0.75 0.8 1 3 5 10K NDCG RN RN+All BM25 BM25+All Figure 6.6: NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions Similarly, gains on precision at top 1 are substantial (Figure 6.7), and are likely to be apparent to web search users.",
                "When implicit feedback is available, the BM25F+All system returns relevant document at top 1 almost 70% of the time, compared 53% of the time when implicit feedback is not considered by the original BM25F system. 0.45 0.5 0.55 0.6 0.65 0.7 1 3 5 10K Precision RN RN+All BM25 BM25+All Figure 6.7: Precision at K NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions We summarize the results on the MAP measure for attempted queries in Table 6.2.",
                "MAP improvements are both substantial and significant, with improvements over the BM25F ranker most pronounced.",
                "Method MAP Gain P(1) Gain RN 0.269 0.632 RN+All 0.321 0.051 (19%) 0.693 0.061(10%) BM25F 0.236 0.525 BM25F+All 0.292 0.056 (24%) 0.687 0.162 (31%) Table 6.2: Mean Average Precision (MAP) on attempted queries for best performing methods We now analyze the cases where implicit feedback was shown most helpful.",
                "Figure 6.8 reports the MAP improvements over the baseline BM25F run for each query with MAP under 0.6.",
                "Note that most of the improvement is for poorly performing queries (i.e., MAP < 0.1).",
                "Interestingly, incorporating user behavior information degrades accuracy for queries with high original MAP score.",
                "One possible explanation is that these easy queries tend to be navigational (i.e., having a single, highly-ranked most appropriate answer), and user interactions with lower-ranked results may indicate divergent information needs that are better served by the less popular results (with correspondingly poor overall relevance ratings). 0 50 100 150 200 250 300 350 0.1 0.2 0.3 0.4 0.5 0.6 -0.4 -0.35 -0.3 -0.25 -0.2 -0.15 -0.1 -0.05 0 0.05 0.1 0.15 0.2 Frequency Average Gain Figure 6.8: Gain of BM25F+All over original BM25F ranking To summarize our experimental results, incorporating implicit feedback in real web search setting resulted in significant improvements over the original rankings, using both BM25F and RN baselines.",
                "Our rich set of implicit features, such as time on page and deviations from the average behavior, provides advantages over using clickthrough alone as an indicator of interest.",
                "Furthermore, incorporating implicit feedback features directly into the learned ranking function is more effective than using implicit feedback for reranking.",
                "The improvements observed over large test sets of queries (1,000 total, between 466 and 495 with implicit feedback available) are both substantial and statistically significant. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper we explored the utility of incorporating noisy implicit feedback obtained in a real web search setting to improve web search ranking.",
                "We performed a large-scale evaluation over 3,000 queries and more than 12 million user interactions with a major search engine, establishing the utility of incorporating noisy implicit feedback to improve web search relevance.",
                "We compared two alternatives of incorporating implicit feedback into the search process, namely reranking with implicit feedback and incorporating implicit feedback features directly into the trained ranking function.",
                "Our experiments showed significant improvement over methods that do not consider implicit feedback.",
                "The gains are particularly dramatic for the top K=1 result in the final ranking, with precision improvements as high as 31%, and the gains are substantial for all values of K. Our experiments showed that implicit user feedback can further improve web search performance, when incorporated directly with popular content- and link-based features.",
                "Interestingly, implicit feedback is particularly valuable for queries with poor original ranking of results (e.g., MAP lower than 0.1).",
                "One promising direction for future work is to apply recent research on automatically predicting query difficulty, and only attempt to incorporate implicit feedback for the difficult queries.",
                "As another research direction we are exploring methods for extending our predictions to the previously unseen queries (e.g., query clustering), which should further improve the web search experience of users.",
                "ACKNOWLEDGMENTS We thank Chris Burges and Matt Richardson for an implementation of RankNet for our experiments.",
                "We also thank Robert Ragno for his valuable suggestions and many discussions. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, S. Dumais, and R.Ragno, Learning User Interaction Models for Predicting Web Search Result Preferences.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan, HARD Track Overview in TREC 2003, High Accuracy Retrieval from Documents, 2003 [3] R. Baeza-Yates and B. Ribeiro-Neto, Modern Information Retrieval, Addison-Wesley, 1999. [4] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine, in Proceedings of WWW, 1997 [5] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning, 2005 [6] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [7] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest.",
                "IEEE Internet Computing. 2001 [8] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [9] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [10] K Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2000 [11] T. Joachims, Optimizing Search Engines Using Clickthrough Data.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [12] T. Joachims, L. Granka, B. Pang, H. Hembrooke, and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [13] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [14] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [15] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [16] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [17] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems.",
                "In Proceedings of the AAAI Workshop on Recommender Systems. 1998 [18] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [19] N. Pharo, N. and K. Järvelin.",
                "The SST method: a tool for analyzing web information search processes.",
                "In Information Processing & Management, 2004 [20] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [21] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (SIGKDD), 2005. [22] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in Proceedings of the ICML Workshop on Learning in Web Search, 2005 [23] S. E. Robertson, H. Zaragoza, and M. Taylor, Simple BM25 extension to multiple weighted fields, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [24] G. Salton & M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [25] E.M. Voorhees, D. Harman, Overview of TREC, 2001 [26] G.R.",
                "Xue, H.J.",
                "Zeng, Z. Chen, Y. Yu, W.Y.",
                "Ma, W.S.",
                "Xi, and W.G.",
                "Fan, Optimizing web search using web clickthrough data, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [27] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
                "Microsoft Cambridge at TREC 13: Web and Hard Tracks.",
                "In Proceedings of TREC 2004"
            ],
            "original_annotated_samples": [
                "<br>implicit relevance feedback</br> for ranking and personalization has become an active area of research."
            ],
            "translated_annotated_samples": [
                "La <br>retroalimentación implícita</br> de relevancia para la clasificación y personalización se ha convertido en un área activa de investigación."
            ],
            "translated_text": "Mejorando la clasificación de búsqueda web al incorporar información sobre el comportamiento del usuario. Eugene Agichtein, Microsoft Research eugeneag@microsoft.com Eric Brill, Microsoft Research brill@microsoft.com Susan Dumais, Microsoft Research sdumais@microsoft.com RESUMEN Mostramos que la incorporación de datos sobre el comportamiento del usuario puede mejorar significativamente el orden de los principales resultados en una configuración real de búsqueda web. Examinamos alternativas para incorporar retroalimentación en el proceso de clasificación y exploramos las contribuciones de la retroalimentación de usuarios en comparación con otras características comunes de búsqueda en la web. Informamos los resultados de una evaluación a gran escala con más de 3,000 consultas y 12 millones de interacciones de usuarios con un motor de búsqueda web popular. Mostramos que la incorporación de retroalimentación implícita puede aumentar otras características, mejorando la precisión de los algoritmos de clasificación de búsqueda web competitivos hasta en un 31% en comparación con el rendimiento original. Categorías y Descriptores de Asignaturas H.3.3 Búsqueda y Recuperación de Información - Retroalimentación de relevancia, proceso de búsqueda; H.3.5 Servicios de Información en Línea - Servicios basados en la web. Términos generales Algoritmos, Medición, Experimentación 1. INTRODUCCIÓN Millones de usuarios interactúan con los motores de búsqueda a diario. Ellos emiten consultas, siguen algunos de los enlaces en los resultados, hacen clic en anuncios, pasan tiempo en páginas, reformulan sus consultas y realizan otras acciones. Estas interacciones pueden servir como una valiosa fuente de información para ajustar y mejorar la clasificación de los resultados de búsqueda en la web y pueden complementar juicios explícitos más costosos. La <br>retroalimentación implícita</br> de relevancia para la clasificación y personalización se ha convertido en un área activa de investigación. El trabajo reciente de Joachims y otros explorando el feedback implícito en entornos controlados ha demostrado el valor de incorporar el feedback implícito en el proceso de clasificación. Nuestra motivación para este trabajo es comprender cómo se puede utilizar la retroalimentación implícita en un entorno operativo a gran escala para mejorar la recuperación. ¿Cómo se compara y complementa con la evidencia del contenido de la página, el texto del ancla o las características basadas en enlaces como inlinks o PageRank? Si bien es intuitivo que las interacciones de los usuarios con el motor de búsqueda web deberían revelar al menos alguna información que podría ser utilizada para la clasificación, estimar las preferencias de los usuarios en entornos reales de búsqueda web es un problema desafiante, ya que las interacciones reales tienden a ser más ruidosas de lo que comúnmente se asume en los entornos controlados de estudios previos. Nuestro artículo explora si la retroalimentación implícita puede ser útil en entornos realistas, donde la retroalimentación del usuario puede ser ruidosa (o adversa) y un motor de búsqueda web ya utiliza cientos de características y está altamente ajustado. Con este fin, exploramos diferentes enfoques para clasificar los resultados de búsqueda web utilizando el comportamiento real de los usuarios obtenido como parte de las interacciones normales con el motor de búsqueda web. Las contribuciones específicas de este artículo incluyen: • Análisis de alternativas para incorporar el comportamiento del usuario en la clasificación de búsqueda web (Sección 3). • Aplicación de un modelo robusto de retroalimentación implícita derivado de la minería de millones de interacciones de usuarios con un importante motor de búsqueda web (Sección 4). • Una evaluación a gran escala sobre consultas reales de usuarios y resultados de búsqueda, mostrando mejoras significativas derivadas de la incorporación de la retroalimentación del usuario (Sección 6). Resumimos nuestros hallazgos y discutimos extensiones al trabajo actual en la Sección 7, que concluye el artículo. ANTECEDENTES Y TRABAJO RELACIONADO Clasificar los resultados de búsqueda es un problema fundamental en la recuperación de información. La mayoría de los enfoques comunes se centran principalmente en la similitud entre la consulta y una página, así como en la calidad general de la página [3,4,24]. Sin embargo, con la creciente popularidad de los motores de búsqueda, la retroalimentación implícita (es decir, las acciones que los usuarios realizan al interactuar con el motor de búsqueda) se puede utilizar para mejorar las clasificaciones. Las medidas de relevancia implícitas han sido estudiadas por varios grupos de investigación. Un resumen de las medidas implícitas se recopila en Kelly y Teevan [14]. Esta investigación, si bien desarrolló valiosas ideas sobre medidas implícitas de relevancia, no se aplicó para mejorar la clasificación de los resultados de búsqueda en la web en entornos realistas. Estrechamente relacionado con nuestro trabajo, Joachims [11] recopiló medidas implícitas en lugar de medidas explícitas, introduciendo una técnica basada completamente en datos de clics para aprender funciones de clasificación. Fox et al. [8] exploraron la relación entre medidas implícitas y explícitas en la búsqueda web, y desarrollaron modelos bayesianos para correlacionar medidas implícitas y juicios explícitos de relevancia tanto para consultas individuales como para sesiones de búsqueda. Este trabajo consideró una amplia gama de comportamientos de usuario (por ejemplo, tiempo de permanencia, tiempo de desplazamiento, patrones de reformulación) además del popular comportamiento de clics. Sin embargo, el esfuerzo de modelado se centró en predecir juicios de relevancia explícitos a partir de acciones implícitas de los usuarios y no específicamente en aprender funciones de clasificación. Otros estudios sobre el comportamiento de los usuarios en la búsqueda web incluyen a Pharo y Järvelin [19], pero no se aplicaron directamente para mejorar la clasificación. Más recientemente, Joachims et al. [12] presentaron una evaluación empírica de la interpretación de la evidencia de clics. Al realizar estudios de seguimiento ocular y correlacionar las predicciones de sus estrategias con las calificaciones explícitas, los autores demostraron que es posible interpretar con precisión los clics en un entorno controlado de laboratorio. Desafortunadamente, no está claro en qué medida la investigación previa se aplica a la búsqueda web del mundo real. Al mismo tiempo, si bien el trabajo reciente (por ejemplo, [26]) sobre el uso de la información de clics para mejorar la clasificación de búsqueda en la web es prometedor, solo abarca un aspecto de las interacciones de los usuarios con los motores de búsqueda en la web. Nos basamos en investigaciones existentes para desarrollar técnicas robustas de interpretación del comportamiento del usuario para el entorno real de búsqueda en la web. En lugar de tratar a cada usuario como un experto confiable, agregamos información de múltiples trazas de sesiones de búsqueda de usuarios no confiables, como describimos en las siguientes dos secciones. 3. INTEGRANDO LA RETROALIMENTACIÓN IMPLÍCITA Consideramos dos enfoques complementarios para la clasificación con retroalimentación implícita: (1) tratar la retroalimentación implícita como evidencia independiente para clasificar resultados, y (2) integrar características de retroalimentación implícita directamente en el algoritmo de clasificación. Describimos a continuación los dos enfoques generales de clasificación. Las características específicas del feedback implícito se describen en la Sección 4, y los algoritmos para interpretar e incorporar el feedback implícito se describen en la Sección 5. 3.1 Feedback Implícito como Evidencia Independiente. El enfoque general es reordenar los resultados obtenidos por un motor de búsqueda web según los clics observados y otras interacciones de usuario para la consulta en sesiones de búsqueda anteriores. Cada resultado se le asigna una puntuación según la relevancia esperada/ satisfacción del usuario basada en interacciones previas, lo que resulta en un cierto orden de preferencia basado únicamente en las interacciones del usuario. Si bien ha habido un trabajo significativo en la fusión de múltiples clasificaciones, adaptamos un enfoque simple y robusto de ignorar las puntuaciones de los clasificadores originales, y en su lugar simplemente fusionamos los órdenes de clasificación. La razón principal para ignorar las puntuaciones originales es que, dado que los espacios de características y los algoritmos de aprendizaje son diferentes, las puntuaciones no son directamente comparables, y la re-normalización tiende a eliminar el beneficio de incorporar las puntuaciones del clasificador. Experimentamos con una variedad de funciones de fusión en el conjunto de desarrollo de consultas (y utilizando un conjunto de interacciones de un período de tiempo diferente de los conjuntos de evaluación final). Encontramos que una combinación heurística simple de fusión de rangos funciona bien y es robusta a variaciones en los valores de puntuación de los clasificadores originales. Para una consulta dada q, se calcula la puntuación implícita ISd para cada resultado d a partir de las características de interacción del usuario disponibles, lo que resulta en la clasificación implícita Id para cada resultado. Calculamos un puntaje combinado SM(d) para d al combinar los rangos obtenidos de la retroalimentación implícita, Id, con el rango original de d, Od: SM(d) = Od + wI * Id, si existe retroalimentación implícita; de lo contrario, SM(d) = Od. Donde el peso wI es un factor de escala ajustado heurísticamente que representa la importancia relativa de la retroalimentación implícita. Los resultados de la consulta se ordenan en valores decrecientes de SM para producir la clasificación final. Un caso especial de este modelo surge al establecer wI en un valor muy grande, lo que efectivamente obliga a que los resultados clicados se clasifiquen por encima de los resultados no clicados, una heurística intuitiva y efectiva que utilizaremos como referencia. Aplicar algoritmos de combinación de clasificadores y ordenadores más sofisticados puede resultar en mejoras adicionales, y es una dirección prometedora para trabajos futuros. El enfoque anterior asume que no hay interacciones entre las características subyacentes que producen la clasificación original de la búsqueda web y las características de retroalimentación implícitas. Ahora relajamos esta suposición al integrar características de retroalimentación implícita directamente en el proceso de clasificación. 3.2 Clasificación con Características de Retroalimentación Implícita Los motores de búsqueda web modernos clasifican los resultados en función de un gran número de características, incluidas las características basadas en el contenido (es decir, qué tan cerca coincide una consulta con el texto, el título o el texto de anclaje del documento) y las características de calidad de página independientes de la consulta (por ejemplo, PageRank del documento o del dominio). En la mayoría de los casos, se desarrollan métodos automáticos (o semiautomáticos) para ajustar la función de clasificación específica que combina estos valores de características. Por lo tanto, un enfoque natural es incorporar características de retroalimentación implícita directamente como características para el algoritmo de clasificación. Durante el entrenamiento o ajuste, el clasificador puede ser ajustado como antes pero con características adicionales. En tiempo de ejecución, el motor de búsqueda recuperaría las características de retroalimentación implícita asociadas con cada par de URL de consulta y resultado. Este modelo requiere que un algoritmo de clasificación sea robusto ante valores faltantes: más del 50% de las consultas a los motores de búsqueda web son únicas, sin disponer de retroalimentación implícita previa. Ahora describimos un clasificador que utilizamos para aprender sobre los conjuntos de características combinadas, incluyendo la retroalimentación implícita. 3.3 Aprendizaje para Clasificar los Resultados de Búsqueda en la Web Un aspecto clave de nuestro enfoque es aprovechar los avances recientes en el aprendizaje automático, en particular los algoritmos de clasificación entrenables para la búsqueda en la web y la recuperación de información (por ejemplo, [5, 11] y resultados clásicos revisados en [3]). En nuestro entorno, contamos con juicios explícitos de relevancia humana (etiquetas) para un conjunto de consultas de búsqueda en la web y resultados. Por lo tanto, una opción atractiva es utilizar una técnica de aprendizaje automático supervisado para aprender una función de clasificación que prediga mejor las evaluaciones de relevancia. RankNet es uno de esos algoritmos. Es un algoritmo de ajuste de red neuronal que optimiza los pesos de las características para que coincidan mejor con las preferencias de los usuarios proporcionadas explícitamente en pares. Si bien los algoritmos de entrenamiento específicos utilizados por RankNet están más allá del alcance de este documento, se describen en detalle en [5] e incluyen una evaluación exhaustiva y comparación con otros métodos de clasificación. Una característica atractiva de RankNet es su eficiencia tanto en el tiempo de entrenamiento como en el de ejecución: el ranking en tiempo de ejecución se puede calcular rápidamente y puede escalarse a la web, y el entrenamiento se puede realizar sobre miles de consultas y resultados juzgados asociados. Utilizamos una implementación de RankNet de 2 capas para modelar relaciones no lineales entre características. Además, RankNet puede aprender con muchas funciones de costo (diferenciables), por lo que puede aprender automáticamente una función de clasificación a partir de etiquetas proporcionadas por humanos, una alternativa atractiva a las técnicas de combinación de características heurísticas. Por lo tanto, también utilizaremos RankNet como un clasificador genérico para explorar la contribución de la retroalimentación implícita en diferentes alternativas de clasificación. 4. MODELO IMPLÍCITO DE RETROALIMENTACIÓN DEL USUARIO Nuestro objetivo es interpretar con precisión la retroalimentación ruidosa del usuario obtenida al rastrear las interacciones del usuario con el motor de búsqueda. Interpretar la retroalimentación implícita en un entorno real de búsqueda en la web no es una tarea fácil. Caracterizamos este problema en detalle en [1], donde motivamos y evaluamos una amplia variedad de modelos de actividades implícitas de usuario. El enfoque general es representar las acciones del usuario para cada resultado de búsqueda como un vector de características, y luego entrenar un clasificador en base a estas características para descubrir los valores de las características que indican resultados de búsqueda relevantes (y no relevantes). Primero resumimos brevemente nuestras características y modelo, y el enfoque de aprendizaje (Sección 4.2) para proporcionar suficiente información para replicar nuestros métodos de clasificación y los experimentos subsiguientes. 4.1 Representación de las acciones del usuario como características. Modelamos los comportamientos observados en la búsqueda web como una combinación de un \"componente de fondo (es decir, ruido independiente de la consulta y relevancia en el comportamiento del usuario, incluidos sesgos posicionales con interacciones de resultados), y un \"componente de relevancia (es decir, comportamiento específico de la consulta indicativo de la relevancia de un resultado para una consulta). Diseñamos nuestras características para aprovechar el comportamiento de usuario agregado. El conjunto de características está compuesto por características observadas directamente (calculadas directamente a partir de observaciones para cada consulta), así como características derivadas específicas de la consulta, calculadas como la desviación de la distribución general de valores independientes de la consulta para los valores de las características observadas directamente correspondientes. Las características utilizadas para representar las interacciones de los usuarios con los resultados de búsqueda en la web se resumen en la Tabla 4.1. Esta información se obtuvo a través de la instrumentación del lado del cliente opt-in de usuarios de un importante motor de búsqueda web. Incluimos las características tradicionales de retroalimentación implícita, como el recuento de clics en los resultados, así como nuestras características derivadas novedosas, como la desviación del número de clics observados para un par de consulta-URL dado del número esperado de clics en un resultado en la posición dada. También modelamos el comportamiento de navegación después de que se hace clic en un resultado, por ejemplo, el tiempo promedio de permanencia en la página para un par de consulta-URL dado, así como su desviación del tiempo de permanencia esperado (promedio). Además, el conjunto de características fue diseñado para proporcionar información esencial sobre la experiencia del usuario y hacer que la interpretación de la retroalimentación sea sólida. Por ejemplo, los usuarios de búsqueda en la web a menudo pueden determinar si un resultado es relevante al mirar el título del resultado, la URL y el resumen; en muchos casos, no es necesario mirar el documento original. Para modelar este aspecto de la experiencia del usuario, incluimos características como la superposición de palabras en el título y las palabras en la consulta (TitleOverlap) y la fracción de palabras compartidas por la consulta y el resumen del resultado. Características de clics Posición Posición de la URL en el ranking actual Frecuencia de clics Número de clics para esta consulta, par URL Probabilidad de clic Probabilidad de un clic para esta consulta y URL Desviación de clic Desviación de la probabilidad de clic esperada ¿Es el siguiente clic? 1 si se hizo clic en la siguiente posición, 0 de lo contrario ¿Es el clic anterior? 1 si se hizo clic en la posición anterior, 0 de lo contrario ¿Hay clic arriba? 1 si hay un clic arriba, 0 de lo contrario ¿Hay clic abajo? 1 si hay un clic abajo, 0 de lo contrario Características de navegación Tiempo en la página Tiempo de permanencia en la página Tiempo acumulado en la página Tiempo acumulado para todas las páginas siguientes después de la búsqueda Tiempo en el dominio Tiempo de permanencia acumulado para este dominio Tiempo en URL corta Tiempo acumulado en el prefijo de la URL, sin parámetros ¿Se siguió el enlace? 1 si se siguió el enlace al resultado, 0 de lo contrario ¿Coincidencia exacta de URL? 0 si se utilizó normalización agresiva, 1 de lo contrario ¿Redirigido? 1 si la URL inicial es la misma que la URL final, 0 de lo contrario ¿Camino desde la búsqueda? 1 si solo se siguieron enlaces después de la consulta, 0 de lo contrario Clics desde la búsqueda Número de saltos para llegar a la página desde la consulta Tiempo promedio de permanencia Tiempo promedio en la página para esta consulta Desviación del tiempo de permanencia Desviación del tiempo de permanencia promedio en la página Desviación acumulada Desviación del tiempo de permanencia acumulado promedio Desviación del dominio Desviación del tiempo de permanencia promedio en el dominio Características de texto de consulta Coincidencia de título Palabras compartidas entre la consulta y el título Coincidencia de resumen Palabras compartidas entre la consulta y el fragmento Coincidencia de URL de consulta Palabras compartidas entre la consulta y la URL Coincidencia de dominio de consulta Palabras compartidas entre la consulta y el dominio de la URL Longitud de la consulta Número de tokens en la consulta Superposición con la siguiente consulta Fracción de palabras compartidas con la siguiente consulta Tabla 4.1: Algunas características utilizadas para representar el historial de navegación posterior a la búsqueda para una consulta dada y una URL de resultado de búsqueda. Habiendo descrito nuestro conjunto de características, revisamos brevemente nuestro método general para derivar un modelo de comportamiento del usuario. 4.2 Derivación de un Modelo de Retroalimentación del Usuario Para aprender a interpretar el comportamiento observado del usuario, correlacionamos las acciones del usuario (es decir, las características en la Tabla 4.1 que representan las acciones) con los juicios explícitos del usuario para un conjunto de consultas de entrenamiento. Encontramos todas las instancias en nuestros registros de sesión donde se enviaron estas consultas al motor de búsqueda, y agregamos las características del comportamiento del usuario para todas las sesiones de búsqueda que involucran estas consultas. Cada par de consulta-URL observado está representado por las características en la Tabla 4.1, con valores promediados en todas las sesiones de búsqueda, y asignado una de las seis etiquetas de relevancia posibles, que van desde Perfecto hasta Malo, según los juicios explícitos de relevancia. Estos vectores de características etiquetados se utilizan como entrada para el algoritmo de entrenamiento RankNet (Sección 3.3), el cual produce un modelo de comportamiento de usuario entrenado. Este enfoque es particularmente atractivo ya que no requiere heurísticas más allá de la ingeniería de características. El modelo de comportamiento del usuario resultante se utiliza para ayudar a clasificar los resultados de búsqueda en la web, ya sea directamente o en combinación con otras características, como se describe a continuación. 5. CONFIGURACIÓN EXPERIMENTAL El objetivo final de incorporar retroalimentación implícita en la clasificación es mejorar la relevancia de los resultados de búsqueda web devueltos. Por lo tanto, comparamos los métodos de clasificación sobre un gran conjunto de consultas evaluadas con etiquetas de relevancia explícitas proporcionadas por jueces humanos. Para que la evaluación sea realista, obtuvimos una muestra aleatoria de consultas de registros de búsqueda web de un motor de búsqueda importante, con resultados asociados y rastros de acciones de usuario. Describimos este conjunto de datos en detalle a continuación. Nuestros indicadores se describen en la Sección 5.2 que utilizamos para evaluar las alternativas de clasificación, enumeradas en la Sección 5.3 en los experimentos de la Sección 6. 5.1 Conjuntos de datos Comparamos nuestros métodos de clasificación en una muestra aleatoria de 3,000 consultas de los registros de consultas del motor de búsqueda. Las consultas fueron extraídas de los registros de manera uniforme al azar por token sin reemplazo, lo que resultó en una muestra de consultas representativa de la distribución general de consultas. En promedio, 30 resultados fueron etiquetados explícitamente por jueces humanos utilizando una escala de seis puntos que va desde Perfecto hasta Malo. En total, hubo más de 83,000 resultados con juicios de relevancia explícitos. Para calcular diversas estadísticas, se considerarán relevantes los documentos con la etiqueta Buena o mejor, y los documentos con etiquetas inferiores se considerarán no relevantes. Se debe tener en cuenta que los experimentos se realizaron sobre los resultados ya altamente clasificados por un motor de búsqueda web, lo cual corresponde a una experiencia de usuario típica que se limita al pequeño número de resultados altamente clasificados para una consulta típica de búsqueda web. Las interacciones de los usuarios fueron recopiladas durante un período de 8 semanas utilizando información voluntaria de participación. En total, se registraron más de 1.2 millones de consultas únicas, lo que resultó en más de 12 millones de interacciones individuales con el motor de búsqueda. Los datos consistían en las interacciones de los usuarios con el motor de búsqueda web (por ejemplo, hacer clic en un enlace de resultado, regresar a los resultados de búsqueda, etc.) realizadas después de enviar una consulta. Estas acciones fueron agregadas entre usuarios y sesiones de búsqueda y convertidas en características en la Tabla 4.1. Para crear los conjuntos de consultas de entrenamiento, validación y prueba, creamos tres divisiones aleatorias diferentes de 1,500 consultas de entrenamiento, 500 de validación y 1000 de prueba. Las divisiones se realizaron aleatoriamente por consulta, de modo que no hubiera superposición en las consultas de entrenamiento, validación y prueba. 5.2 Métricas de Evaluación Evaluamos los algoritmos de clasificación en una variedad de métricas de recuperación de información aceptadas, a saber, Precisión en K (P(K)), Ganancia Acumulada Descontada Normalizada (NDCG) y Precisión Promedio Media (MAP). Cada métrica se enfoca en un aspecto diferente del rendimiento del sistema, como describimos a continuación. • Precisión en K: Como la métrica más intuitiva, P(K) informa la fracción de documentos clasificados en los primeros K resultados que están etiquetados como relevantes. En nuestro entorno, requerimos que un documento relevante sea etiquetado como Bueno o superior. La posición de los documentos relevantes dentro de los primeros K no es relevante, por lo que esta métrica mide la satisfacción general del usuario con los resultados principales de K. • NDCG en K: NDCG es una medida de recuperación diseñada específicamente para la evaluación de búsqueda en la web [10]. Para una consulta dada q, los resultados clasificados se examinan desde el mejor clasificado hacia abajo, y el NDCG se calcula como: = +−= K j jr qq jMN 1 )( )1log(/)12( Donde Mq es una constante de normalización calculada de manera que un orden perfecto obtendría un NDCG de 1; y cada r(j) es una etiqueta de relevancia entera (0=Malo y 5=Perfecto) del resultado devuelto en la posición j. Ten en cuenta que los documentos sin etiquetar y los documentos malos no contribuyen a la suma, pero reducirán el NDCG para la consulta al empujar hacia abajo los documentos etiquetados relevantes, disminuyendo sus contribuciones. NDCG es muy adecuado para la evaluación de búsquedas en la web, ya que recompensa de manera más intensa los documentos relevantes en los resultados mejor clasificados que aquellos clasificados más bajos. • MAP: La precisión promedio para cada consulta se define como la media de la precisión en los valores de K calculados después de que se recuperó cada documento relevante. El valor MAP final se define como la media de las precisiones promedio de todas las consultas en el conjunto de pruebas. Esta métrica es el resumen de un solo valor más comúnmente utilizado de una ejecución sobre un conjunto de consultas. 5.3 Métodos de clasificación comparados. Recuerde que nuestro objetivo es cuantificar la efectividad del comportamiento implícito para la búsqueda web real. Una dimensión es comparar la utilidad de la retroalimentación implícita con otra información disponible para un motor de búsqueda web. Específicamente, comparamos la efectividad de los comportamientos implícitos de los usuarios con la coincidencia basada en el contenido, las características de calidad de la página estática y combinaciones de todas las características. • BM25F: Como referencia sólida de búsqueda web, utilizamos la puntuación BM25F, que se utilizó en uno de los sistemas con mejor rendimiento en la pista web TREC 2004 [23,27]. BM25F y sus variantes han sido extensamente descritas y evaluadas en la literatura de IR, por lo tanto, sirven como una línea base sólida y reproducible. La variante BM25F que utilizamos en nuestros experimentos calcula puntuaciones de coincidencia separadas para cada campo de un documento de resultado (por ejemplo, texto del cuerpo, título y texto del enlace) e incorpora información de enlaces independiente de la consulta (por ejemplo, PageRank, ClickDistance y profundidad de URL). La función de puntuación y la ajuste específico del campo se describen en detalle en [23]. Tenga en cuenta que BM25F no considera directamente la retroalimentación explícita o implícita para ajuste. • RN: La clasificación producida por un clasificador de redes neuronales (RankNet, descrito en la Sección 3.3) que aprende a clasificar los resultados de búsqueda web al incorporar BM25F y un gran número de características estáticas y dinámicas adicionales que describen cada resultado de búsqueda. Este sistema aprende automáticamente los pesos de todas las características (incluido el puntaje BM25F para un documento) basándose en etiquetas humanas explícitas para un gran conjunto de consultas. Un sistema que incorpora una implementación de RankNet actualmente está siendo utilizado por un importante motor de búsqueda y puede considerarse representativo del estado del arte en la búsqueda web. • BM25F-RerankCT: La clasificación producida al incorporar estadísticas de clics para reordenar los resultados de búsqueda web clasificados por BM25F arriba. El clic es un caso especial particularmente importante de retroalimentación implícita, y se ha demostrado que se correlaciona con la relevancia de los resultados. Este es un caso especial del método de clasificación en la Sección 3.1, con el peso wI establecido en 1000 y la clasificación Id es simplemente el número de clics en el resultado correspondiente a d. En efecto, esta clasificación coloca en la parte superior todos los resultados de búsqueda web devueltos con al menos un clic (y los ordena en orden decreciente por número de clics). La clasificación relativa de los resultados restantes no cambia y se insertan debajo de todos los resultados clicados. Este método sirve como nuestro método de reordenamiento de retroalimentación implícita de referencia. La clasificación producida al reordenar los resultados de BM25F utilizando todas las características del comportamiento del usuario (Sección 4). Este método aprende un modelo de preferencias de usuario correlacionando los valores de las características con etiquetas de relevancia explícitas utilizando el algoritmo de red neuronal RankNet (Sección 4.2). En tiempo de ejecución, para una consulta dada se calcula la puntuación implícita Ir para cada resultado r con características de interacción de usuario disponibles, y se produce el ranking implícito. La clasificación combinada se calcula como se describe en la Sección 3.1. Basándonos en los experimentos realizados sobre el conjunto de desarrollo, fijamos el valor de wI en 3 (el efecto del parámetro wI para este clasificador resultó ser insignificante). • BM25F+All: Clasificación derivada al entrenar el aprendiz RankNet (Sección 3.3) sobre el conjunto de características del puntaje BM25F, así como todas las características de retroalimentación implícita (Sección 3.2). Utilizamos la implementación de 2 capas de RankNet [5] entrenada en las consultas y etiquetas de los conjuntos de entrenamiento y validación. • RN+All: Clasificación derivada al entrenar el algoritmo de clasificación RankNet de 2 capas (Sección 3.3) sobre la unión de todas las características de retroalimentación de contenido, dinámicas e implícitas (es decir, todas las características descritas anteriormente, así como todas las nuevas características de retroalimentación implícita que introdujimos). Los métodos de clasificación anteriores abarcan el rango de la información utilizada para clasificar, desde no utilizar la retroalimentación implícita o explícita en absoluto (es decir, BM25F) hasta un motor de búsqueda web moderno que utiliza cientos de características y está ajustado en base a juicios explícitos (RN). Como mostraremos a continuación, incorporar el comportamiento del usuario en estos sistemas de clasificación mejora drásticamente la relevancia de los documentos devueltos. 6. RESULTADOS EXPERIMENTALES El feedback implícito para la clasificación de búsquedas en la web puede ser explotado de diversas formas. Comparamos métodos alternativos para explotar la retroalimentación implícita, tanto reordenando los resultados principales (es decir, los métodos BM25F-RerankCT y BM25F-RerankAll que reordenan los resultados de BM25F), como integrando directamente las características implícitas en el proceso de clasificación (es decir, los métodos RN+ALL y BM25F+All que aprenden a clasificar los resultados sobre la retroalimentación implícita y otras características). Comparamos nuestros métodos con baselines sólidos (BM25F y RN) sobre las medidas NDCG, Precisión en K y MAP definidas en la Sección 5.2. Los resultados se promediaron en tres divisiones aleatorias del conjunto de datos completo. Cada división contenía 1500 consultas de entrenamiento, 500 de validación y 1000 de prueba, todos los conjuntos de consultas disjuntos. Primero presentamos los resultados de las 1000 consultas de prueba (es decir, incluyendo las consultas para las cuales no hay medidas implícitas, por lo que utilizamos las clasificaciones web originales). Luego profundizamos para examinar los efectos en la reorganización de los intentos de búsqueda con más detalle, analizando dónde el feedback implícito resultó más beneficioso. Primero experimentamos con diferentes métodos de volver a clasificar la salida de los resultados de búsqueda de BM25F. Las figuras 6.1 y 6.2 informan sobre NDCG y Precisión para BM25F, así como para las estrategias de volver a clasificar los resultados con retroalimentación del usuario (Sección 3.1). Incorporar todos los comentarios de los usuarios (ya sea en el marco de reordenamiento o como características directamente al aprendiz) resulta en mejoras significativas (utilizando una prueba t de dos colas con p=0.01) tanto sobre la clasificación original de BM25F como sobre el reordenamiento solo con clics. La mejora es consistente en los 10 mejores resultados y es mayor para el mejor resultado: NDCG en 1 para BM25F+All es de 0.622 en comparación con 0.518 de los resultados originales, y la precisión en 1 también aumenta de 0.5 a 0.63. Basándonos en estos resultados, utilizaremos el clasificador de combinación de características directas (es decir, BM25F+All) para comparaciones posteriores que involucren retroalimentación implícita. Curiosamente, el uso solo de clics, aunque proporciona un beneficio significativo sobre la clasificación original de BM25F, no es tan efectivo como considerar el conjunto completo de características en la Tabla 4.1. Mientras analizamos el comportamiento del usuario (y las características de los componentes más efectivos) en un documento separado [1], vale la pena dar un ejemplo concreto del tipo de ruido inherente en la retroalimentación real de los usuarios en el entorno de búsqueda web. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 Posición del resultado Frecuencia de clics relativa PTR=2 PTR=3 PTR=5 Figura 6.3: Frecuencia relativa de clics para consultas con diferentes Posiciones del Resultado Relevante Superior (PTR). Si los usuarios solo consideraran la relevancia de un resultado para su consulta, harían clic en los resultados más relevantes en la parte superior. Desafortunadamente, como han demostrado Joachims y otros, la presentación también influye de manera bastante dramática en los resultados en los que los usuarios hacen clic. Los usuarios a menudo hacen clic en los resultados por encima del relevante, presumiblemente porque los resúmenes cortos no proporcionan suficiente información para hacer evaluaciones precisas de relevancia y han aprendido que, en promedio, los elementos mejor clasificados son relevantes. La Figura 6.3 muestra las frecuencias relativas de clics para consultas con elementos relevantes conocidos en posiciones distintas a la primera posición; la posición del resultado relevante superior (PTR) varía de 2 a 10 en la figura. Por ejemplo, para consultas con el primer resultado relevante en la posición 5 (PTR=5), hay más clics en los resultados no relevantes en posiciones más altas que en el primer resultado relevante en la posición 5. Como veremos, el aprendizaje sobre un conjunto de características de comportamiento más amplio resulta en una mejora sustancial en la precisión en comparación con solo el clic. Ahora consideramos incorporar el comportamiento del usuario en un conjunto de características mucho más amplio, RN (Sección 5.3) utilizado por un importante motor de búsqueda web. RN incorpora BM25F, características basadas en enlaces y cientos de otras características. La Figura 6.4 informa sobre NDCG en K y la Figura 6.5 informa sobre Precisión en K. Curiosamente, aunque las clasificaciones originales de RN son significativamente más precisas que BM25F solo, la incorporación de características de retroalimentación implícita (BM25F+All) da como resultado una clasificación que supera significativamente a las clasificaciones originales de RN. En otras palabras, la retroalimentación implícita incorpora suficiente información para reemplazar las cientos de otras características disponibles para el aprendiz de RankNet entrenado en el conjunto de características RN. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 1 2 3 4 5 6 7 8 9 10K NDCG RN RN+All BM25 BM25+All Figura 6.4: NDCG en K para BM25F, BM25F+All, RN y RN+All para diferentes K Además, enriquecer las características de RN con el conjunto de retroalimentación implícita muestra una ganancia significativa en todas las medidas, permitiendo que RN+All supere a todos los demás métodos. Esto demuestra la naturaleza complementaria de la retroalimentación implícita con otras características disponibles para un motor de búsqueda web de última generación. 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 Precisión RN RN+Todo BM25 BM25+Todo Figura 6.5: Precisión en K para BM25F, BM25F+Todo, RN y RN+Todo para diferentes valores de K. Resumimos el rendimiento de los diferentes métodos de clasificación en la Tabla 6.1. Informamos el puntaje de Precisión Promedio Media (MAP) para cada sistema. Aunque no es intuitivo de interpretar, el MAP permite la comparación cuantitativa en una sola métrica. Las ganancias marcadas con * son significativas a un nivel de p=0.01 utilizando una prueba t de dos colas. Ganancia MAP P(1) Ganancia BM25F 0.184 - 0.503BM25F-Rerank-CT 0.215 0.031* 0.577 0.073* BM25F-RerankImplicit 0.218 0.003 0.605 0.028* BM25F+Implicit 0.222 0.004 0.620 0.015* RN 0.215 - 0.597RN+All 0.248 0.033* 0.629 0.032* Tabla 6.1: Precisión Promedio (MAP) para todas las estrategias. Hasta ahora hemos informado resultados promediados en todas las consultas del conjunto de pruebas. Desafortunadamente, menos de la mitad tuvo interacciones suficientes para intentar un nuevo ranking. De las 1000 consultas en la prueba, entre el 46% y el 49%, dependiendo de la división entre entrenamiento y prueba, tenían suficiente información de interacción para hacer predicciones (es decir, hubo al menos 1 sesión de búsqueda en la que el usuario hizo clic en al menos 1 URL de resultado). Esto no es sorprendente: la búsqueda en la web tiene una distribución de cola pesada y hay muchas consultas únicas. Ahora consideramos el rendimiento en las consultas para las cuales estaban disponibles las interacciones de los usuarios. La Figura 6.6 informa sobre el NDCG para el subconjunto de las consultas de prueba con las características de retroalimentación implícita. Las ganancias en el primer puesto son dramáticas. El NDCG en 1 de BM25F+All aumenta de 0.6 a 0.75 (un aumento relativo del 31%), logrando un rendimiento comparable al de RN+All operando sobre un conjunto de características mucho más rico. 0.6 0.65 0.7 0.75 0.8 1 3 5 10K NDCG RN RN+All BM25 BM25+All Figura 6.6: NDCG en K para BM25F, BM25F+All, RN y RN+All en consultas de prueba con interacciones de usuario. Del mismo modo, las ganancias en precisión en el top 1 son sustanciales (Figura 6.7) y es probable que sean evidentes para los usuarios de búsqueda web. Cuando se dispone de retroalimentación implícita, el sistema BM25F+All devuelve el documento relevante en la parte superior 1 casi el 70% del tiempo, en comparación con el 53% del tiempo cuando la retroalimentación implícita no es considerada por el sistema BM25F original. 0.45 0.5 0.55 0.6 0.65 0.7 1 3 5 10K Precisión RN RN+All BM25 BM25+All Figura 6.7: Precisión en K NDCG en K para BM25F, BM25F+All, RN y RN+All en consultas de prueba con interacciones de usuario Resumimos los resultados en la medida MAP para las consultas intentadas en la Tabla 6.2. Las mejoras en MAP son tanto sustanciales como significativas, siendo más pronunciadas que las mejoras en el clasificador BM25F. Ahora analizamos los casos en los que la retroalimentación implícita resultó ser más útil. La Figura 6.8 informa las mejoras de MAP sobre la ejecución de BM25F base para cada consulta con MAP inferior a 0.6. Ten en cuenta que la mayoría de la mejora es para consultas con bajo rendimiento (es decir, MAP < 0.1). Curiosamente, la incorporación de información sobre el comportamiento del usuario disminuye la precisión para consultas con un puntaje MAP original alto. Una posible explicación es que estas consultas fáciles tienden a ser de navegación (es decir, tener una única respuesta apropiada altamente clasificada), y las interacciones de los usuarios con resultados de menor rango pueden indicar necesidades de información divergentes que son mejor atendidas por los resultados menos populares (con calificaciones de relevancia global correspondientemente bajas). Para resumir nuestros resultados experimentales, la incorporación de retroalimentación implícita en un entorno real de búsqueda web resultó en mejoras significativas sobre las clasificaciones originales, utilizando tanto BM25F como RN como líneas de base. Nuestro amplio conjunto de características implícitas, como el tiempo en la página y las desviaciones del comportamiento promedio, ofrece ventajas sobre el uso exclusivo del clic como indicador de interés. Además, incorporar características de retroalimentación implícita directamente en la función de clasificación aprendida es más efectivo que utilizar la retroalimentación implícita para volver a clasificar. Las mejoras observadas en grandes conjuntos de pruebas de consultas (1,000 en total, entre 466 y 495 con retroalimentación implícita disponible) son tanto sustanciales como estadísticamente significativas. 7. CONCLUSIONES Y TRABAJOS FUTUROS En este artículo exploramos la utilidad de incorporar retroalimentación implícita ruidosa obtenida en un entorno real de búsqueda web para mejorar la clasificación de búsqueda web. Realizamos una evaluación a gran escala con más de 3,000 consultas y más de 12 millones de interacciones de usuarios con un motor de búsqueda importante, estableciendo la utilidad de incorporar retroalimentación implícita ruidosa para mejorar la relevancia de la búsqueda web. Comparamos dos alternativas para incorporar retroalimentación implícita en el proceso de búsqueda, a saber, reordenar con retroalimentación implícita e incorporar directamente características de retroalimentación implícita en la función de clasificación entrenada. Nuestros experimentos mostraron una mejora significativa sobre los métodos que no consideran la retroalimentación implícita. Las ganancias son particularmente dramáticas para el resultado superior K=1 en la clasificación final, con mejoras de precisión de hasta un 31%, y las ganancias son sustanciales para todos los valores de K. Nuestros experimentos mostraron que el feedback implícito del usuario puede mejorar aún más el rendimiento de la búsqueda web, cuando se incorpora directamente con características populares basadas en contenido y enlaces. Curiosamente, la retroalimentación implícita es especialmente valiosa para consultas con una clasificación original deficiente de resultados (por ejemplo, MAP inferior a 0.1). Una dirección prometedora para trabajos futuros es aplicar la investigación reciente sobre la predicción automática de la dificultad de las consultas, e intentar incorporar únicamente retroalimentación implícita para las consultas difíciles. Como otra dirección de investigación, estamos explorando métodos para extender nuestras predicciones a las consultas previamente no vistas (por ejemplo, el agrupamiento de consultas), lo cual debería mejorar aún más la experiencia de búsqueda en la web de los usuarios. AGRADECIMIENTOS Agradecemos a Chris Burges y Matt Richardson por la implementación de RankNet para nuestros experimentos. También agradecemos a Robert Ragno por sus valiosas sugerencias y muchas discusiones. 8. REFERENCIAS [1] E. Agichtein, E. Brill, S. Dumais y R. Ragno, Aprendizaje de modelos de interacción del usuario para predecir las preferencias de resultados de búsqueda web. En Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2006 [2] J. Allan, Resumen de la Pista HARD en TREC 2003, Recuperación de Alta Precisión de Documentos, 2003 [3] R. Baeza-Yates y B. Ribeiro-Neto, Recuperación de Información Moderna, Addison-Wesley, 1999. [4] S. Brin y L. Page, Anatomía de un Motor de Búsqueda Web Hipertextual a Gran Escala, en Actas de WWW, 1997 [5] C.J.C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. Hullender, Aprendizaje para clasificación utilizando descenso de gradiente, en Actas de la Conferencia Internacional sobre Aprendizaje Automático, 2005 [6] D.M. Chickering, The WinMine Toolkit, Informe Técnico de Microsoft MSR-TR-2002-103, 2002 [7] M. Claypool, D. Brown, P. Lee y M. Waseda. Inferir el interés del usuario. IEEE Internet Computing. 2001 [8] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais y T. White. Evaluando medidas implícitas para mejorar la experiencia de búsqueda. En ACM Transactions on Information Systems, 2005 [9] J. Goecks y J. Shavlick. Aprendiendo los intereses de los usuarios observando de manera discreta su comportamiento normal. En Actas del Taller de IJCAI sobre Aprendizaje Automático para Filtrado de Información. 1999. [10] K Jarvelin y J. Kekalainen. Métodos de evaluación IR para recuperar documentos altamente relevantes. En las Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2000 [11] T. Joachims, Optimización de Motores de Búsqueda Utilizando Datos de Clics. En Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (SIGKDD), 2002 [12] T. Joachims, L. Granka, B. Pang, H. Hembrooke y G. Gay, Interpretación precisa de los datos de clics como retroalimentación implícita, Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2005 [13] T. Joachims, Haciendo práctico el aprendizaje SVM a gran escala. Avances en Métodos de Núcleo, en Aprendizaje de Vectores de Soporte, MIT Press, 1999 [14] D. Kelly y J. Teevan, Retroalimentación implícita para inferir preferencias de usuario: una bibliografía. En el Foro SIGIR, 2003 [15] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon y J. Riedl. GroupLens: Aplicando filtrado colaborativo a las noticias de Usenet. En Comunicaciones de ACM, 1997. [16] M. Morita y Y. Shinoda, Filtrado de información basado en análisis del comportamiento del usuario y recuperación de texto de mejor coincidencia. Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 1994 [17] D. Oard y J. Kim. Retroalimentación implícita para sistemas de recomendación. En Actas del Taller de Sistemas de Recomendación de la AAAI. 1998 [18] D. Oard y J. Kim. Modelando el contenido de la información utilizando el comportamiento observable. En Actas de la 64ª Reunión Anual de la Sociedad Americana de Ciencia de la Información y Tecnología. 2001 [19] N. Pharo, N. y K. Järvelin. El método SST: una herramienta para analizar los procesos de búsqueda de información en la web. En Information Processing & Management, 2004 [20] P. Pirolli, El Uso de la Pista de Información Próxima para Buscar Contenido Distal en la World Wide Web. Trabajando con la tecnología en mente: Brunswikiano. Recursos para Ciencia Cognitiva e Ingeniería, Oxford University Press, 2004 [21] F. Radlinski y T. Joachims, Cadenas de Consulta: Aprendizaje para Clasificar a partir de Retroalimentación Implícita. En Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (SIGKDD), 2005. [22] F. Radlinski y T. Joachims, Evaluando la Robustez del Aprendizaje a partir de Retroalimentación Implícita, en Actas del Taller de ICML sobre Aprendizaje en Búsqueda Web, 2005 [23] S. E. Robertson, H. Zaragoza y M. Taylor, Extensión simple de BM25 a múltiples campos ponderados, en Actas de la Conferencia sobre Información y Gestión del Conocimiento (CIKM), 2004 [24] G. Salton y M. McGill. Introducción a la recuperación de información moderna. McGraw-Hill, 1983 [25] E.M. Voorhees, D. Harman, Resumen de TREC, 2001 [26] G.R. Xue, H.J. Zeng, Z. Chen, Y. Yu, W.Y. -> Zeng, Z. Chen, Y. Yu, W.Y. Ma, W.S. Xi, y W.G. Fan, Optimizing web search using web clickthrough data, en Actas de la Conferencia sobre Información y Gestión del Conocimiento (CIKM), 2004 [27] H. Zaragoza, N. Craswell, M. Taylor, S. Saria y S. Robertson. Microsoft Cambridge en TREC 13: Pistas Web y Duras. En Actas de TREC 2004 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "web search rank": {
            "translated_key": "clasificación de búsqueda web",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Improving Web Search Ranking by Incorporating User Behavior Information Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com ABSTRACT We show that incorporating user behavior data can significantly improve ordering of top results in real web search setting.",
                "We examine alternatives for incorporating feedback into the ranking process and explore the contributions of user feedback compared to other common web search features.",
                "We report results of a large scale evaluation over 3,000 queries and 12 million user interactions with a popular web search engine.",
                "We show that incorporating implicit feedback can augment other features, improving the accuracy of a competitive <br>web search rank</br>ing algorithms by as much as 31% relative to the original performance.",
                "Categories and Subject Descriptors H.3.3 Information Search and Retrieval - Relevance feedback, search process; H.3.5 Online Information Services - Web-based services.",
                "General Terms Algorithms, Measurement, Experimentation 1.",
                "INTRODUCTION Millions of users interact with search engines daily.",
                "They issue queries, follow some of the links in the results, click on ads, spend time on pages, reformulate their queries, and perform other actions.",
                "These interactions can serve as a valuable source of information for tuning and improving web search result ranking and can compliment more costly explicit judgments.",
                "Implicit relevance feedback for ranking and personalization has become an active area of research.",
                "Recent work by Joachims and others exploring implicit feedback in controlled environments have shown the value of incorporating implicit feedback into the ranking process.",
                "Our motivation for this work is to understand how implicit feedback can be used in a large-scale operational environment to improve retrieval.",
                "How does it compare to and compliment evidence from page content, anchor text, or link-based features such as inlinks or PageRank?",
                "While it is intuitive that user interactions with the web search engine should reveal at least some information that could be used for ranking, estimating user preferences in real web search settings is a challenging problem, since real user interactions tend to be more noisy than commonly assumed in the controlled settings of previous studies.",
                "Our paper explores whether implicit feedback can be helpful in realistic environments, where user feedback can be noisy (or adversarial) and a web search engine already uses hundreds of features and is heavily tuned.",
                "To this end, we explore different approaches for ranking web search results using real user behavior obtained as part of normal interactions with the web search engine.",
                "The specific contributions of this paper include: • Analysis of alternatives for incorporating user behavior into <br>web search rank</br>ing (Section 3). • An application of a robust implicit feedback model derived from mining millions of user interactions with a major web search engine (Section 4). • A large scale evaluation over real user queries and search results, showing significant improvements derived from incorporating user feedback (Section 6).",
                "We summarize our findings and discuss extensions to the current work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "Most common approaches primarily focus on similarity of query and a page, as well as the overall page quality [3,4,24].",
                "However, with increasing popularity of search engines, implicit feedback (i.e., the actions users take when interacting with the search engine) can be used to improve the rankings.",
                "Implicit relevance measures have been studied by several research groups.",
                "An overview of implicit measures is compiled in Kelly and Teevan [14].",
                "This research, while developing valuable insights into implicit relevance measures, was not applied to improve the ranking of web search results in realistic settings.",
                "Closely related to our work, Joachims [11] collected implicit measures in place of explicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "Fox et al. [8] explored the relationship between implicit and explicit measures in Web search, and developed Bayesian models to correlate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "This work considered a wide range of user behaviors (e.g., dwell time, scroll time, reformulation patterns) in addition to the popular clickthrough behavior.",
                "However, the modeling effort was aimed at predicting explicit relevance judgments from implicit user actions and not specifically at learning ranking functions.",
                "Other studies of user behavior in web search include Pharo and Järvelin [19], but were not directly applied to improve ranking.",
                "More recently, Joachims et al. [12] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthroughs in a controlled, laboratory setting.",
                "Unfortunately, the extent to which previous research applies to real-world web search is unclear.",
                "At the same time, while recent work (e.g., [26]) on using clickthrough information for improving <br>web search rank</br>ing is promising, it captures only one aspect of the user interactions with web search engines.",
                "We build on existing research to develop robust user behavior interpretation techniques for the real web search setting.",
                "Instead of treating each user as a reliable expert, we aggregate information from multiple, unreliable, user search session traces, as we describe in the next two sections. 3.",
                "INCORPORATING IMPLICIT FEEDBACK We consider two complementary approaches to ranking with implicit feedback: (1) treating implicit feedback as independent evidence for ranking results, and (2) integrating implicit feedback features directly into the ranking algorithm.",
                "We describe the two general ranking approaches next.",
                "The specific implicit feedback features are described in Section 4, and the algorithms for interpreting and incorporating implicit feedback are described in Section 5. 3.1 Implicit Feedback as Independent Evidence The general approach is to re-rank the results obtained by a web search engine according to observed clickthrough and other user interactions for the query in previous search sessions.",
                "Each result is assigned a score according to expected relevance/user satisfaction based on previous interactions, resulting in some preference ordering based on user interactions alone.",
                "While there has been significant work on merging multiple rankings, we adapt a simple and robust approach of ignoring the original rankers scores, and instead simply merge the rank orders.",
                "The main reason for ignoring the original scores is that since the feature spaces and learning algorithms are different, the scores are not directly comparable, and re-normalization tends to remove the benefit of incorporating classifier scores.",
                "We experimented with a variety of merging functions on the development set of queries (and using a set of interactions from a different time period from final evaluation sets).",
                "We found that a simple rank merging heuristic combination works well, and is robust to variations in score values from original rankers.",
                "For a given query q, the implicit score ISd is computed for each result d from available user interaction features, resulting in the implicit rank Id for each result.",
                "We compute a merged score SM(d) for d by combining the ranks obtained from implicit feedback, Id with the original rank of d, Od:     ¡     ¢ £ + + + + = otherwise O dforexistsfeedbackimplicitif OI w wOIdS d dd I IddM 1 1 1 1 1 1 ),,,( where the weight wI is a heuristically tuned scaling factor representing the relative importance of the implicit feedback.",
                "The query results are ordered in by decreasing values of SM to produce the final ranking.",
                "One special case of this model arises when setting wI to a very large value, effectively forcing clicked results to be ranked higher than un-clicked results - an intuitive and effective heuristic that we will use as a baseline.",
                "Applying more sophisticated classifier and ranker combination algorithms may result in additional improvements, and is a promising direction for future work.",
                "The approach above assumes that there are no interactions between the underlying features producing the original <br>web search rank</br>ing and the implicit feedback features.",
                "We now relax this assumption by integrating implicit feedback features directly into the ranking process. 3.2 Ranking with Implicit Feedback Features Modern web search engines rank results based on a large number of features, including content-based features (i.e., how closely a query matches the text or title or anchor text of the document), and query-independent page quality features (e.g., PageRank of the document or the domain).",
                "In most cases, automatic (or semiautomatic) methods are developed for tuning the specific ranking function that combines these feature values.",
                "Hence, a natural approach is to incorporate implicit feedback features directly as features for the ranking algorithm.",
                "During training or tuning, the ranker can be tuned as before but with additional features.",
                "At runtime, the search engine would fetch the implicit feedback features associated with each query-result URL pair.",
                "This model requires a ranking algorithm to be robust to missing values: more than 50% of queries to web search engines are unique, with no previous implicit feedback available.",
                "We now describe such a ranker that we used to learn over the combined feature sets including implicit feedback. 3.3 Learning to Rank Web Search Results A key aspect of our approach is exploiting recent advances in machine learning, namely trainable ranking algorithms for web search and information retrieval (e.g., [5, 11] and classical results reviewed in [3]).",
                "In our setting, explicit human relevance judgments (labels) are available for a set of web search queries and results.",
                "Hence, an attractive choice to use is a supervised machine learning technique to learn a ranking function that best predicts relevance judgments.",
                "RankNet is one such algorithm.",
                "It is a neural net tuning algorithm that optimizes feature weights to best match explicitly provided pairwise user preferences.",
                "While the specific training algorithms used by RankNet are beyond the scope of this paper, it is described in detail in [5] and includes extensive evaluation and comparison with other ranking methods.",
                "An attractive feature of RankNet is both train- and run-time efficiency - runtime ranking can be quickly computed and can scale to the web, and training can be done over thousands of queries and associated judged results.",
                "We use a 2-layer implementation of RankNet in order to model non-linear relationships between features.",
                "Furthermore, RankNet can learn with many (differentiable) cost functions, and hence can automatically learn a ranking function from human-provided labels, an attractive alternative to heuristic feature combination techniques.",
                "Hence, we will also use RankNet as a generic ranker to explore the contribution of implicit feedback for different ranking alternatives. 4.",
                "IMPLICIT USER FEEDBACK MODEL Our goal is to accurately interpret noisy user feedback obtained as by tracing user interactions with the search engine.",
                "Interpreting implicit feedback in real web search setting is not an easy task.",
                "We characterize this problem in detail in [1], where we motivate and evaluate a wide variety of models of implicit user activities.",
                "The general approach is to represent user actions for each search result as a vector of features, and then train a ranker on these features to discover feature values indicative of relevant (and nonrelevant) search results.",
                "We first briefly summarize our features and model, and the learning approach (Section 4.2) in order to provide sufficient information to replicate our ranking methods and the subsequent experiments. 4.1 Representing User Actions as Features We model observed web search behaviors as a combination of a ``background component (i.e., query- and relevance-independent noise in user behavior, including positional biases with result interactions), and a ``relevance component (i.e., query-specific behavior indicative of relevance of a result to a query).",
                "We design our features to take advantage of aggregated user behavior.",
                "The feature set is comprised of directly observed features (computed directly from observations for each query), as well as queryspecific derived features, computed as the deviation from the overall query-independent distribution of values for the corresponding directly observed feature values.",
                "The features used to represent user interactions with web search results are summarized in Table 4.1.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "We include the traditional implicit feedback features such as clickthrough counts for the results, as well as our novel derived features such as the deviation of the observed clickthrough number for a given query-URL pair from the expected number of clicks on a result in the given position.",
                "We also model the browsing behavior after a result was clicked - e.g., the average page dwell time for a given query-URL pair, as well as its deviation from the expected (average) dwell time.",
                "Furthermore, the feature set was designed to provide essential information about the user experience to make feedback interpretation robust.",
                "For example, web search users can often determine whether a result is relevant by looking at the result title, URL, and summary - in many cases, looking at the original document is not necessary.",
                "To model this aspect of user experience we include features such as overlap in words in title and words in query (TitleOverlap) and the fraction of words shared by the query and the result summary.",
                "Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickProbability Probability of a click for this query and URL ClickDeviation Deviation from expected click probability IsNextClicked 1 if clicked on next position, 0 otherwise IsPreviousClicked 1 if clicked on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, no parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from average dwell time on page CumulativeDeviation Deviation from average cumulative dwell time DomainDeviation Deviation from average dwell time on domain Query-text features TitleOverlap Words shared between query and title SummaryOverlap Words shared between query and snippet QueryURLOverlap Words shared between query and URL QueryDomainOverlap Words shared between query and URL domain QueryLength Number of tokens in query QueryNextOverlap Fraction of words shared with next query Table 4.1: Some features used to represent post-search navigation history for a given query and search result URL.",
                "Having described our feature set, we briefly review our general method for deriving a user behavior model. 4.2 Deriving a User Feedback Model To learn to interpret the observed user behavior, we correlate user actions (i.e., the features in Table 4.1 representing the actions) with the explicit user judgments for a set of training queries.",
                "We find all the instances in our session logs where these queries were submitted to the search engine, and aggregate the user behavior features for all search sessions involving these queries.",
                "Each observed query-URL pair is represented by the features in Table 4.1, with values averaged over all search sessions, and assigned one of six possible relevance labels, ranging from Perfect to Bad, as assigned by explicit relevance judgments.",
                "These labeled feature vectors are used as input to the RankNet training algorithm (Section 3.3) which produces a trained user behavior model.",
                "This approach is particularly attractive as it does not require heuristics beyond feature engineering.",
                "The resulting user behavior model is used to help rank web search resultseither directly or in combination with other features, as described below. 5.",
                "EXPERIMENTAL SETUP The ultimate goal of incorporating implicit feedback into ranking is to improve the relevance of the returned web search results.",
                "Hence, we compare the ranking methods over a large set of judged queries with explicit relevance labels provided by human judges.",
                "In order for the evaluation to be realistic we obtained a random sample of queries from web search logs of a major search engine, with associated results and traces for user actions.",
                "We describe this dataset in detail next.",
                "Our metrics are described in Section 5.2 that we use to evaluate the ranking alternatives, listed in Section 5.3 in the experiments of Section 6. 5.1 Datasets We compared our ranking methods over a random sample of 3,000 queries from the search engine query logs.",
                "The queries were drawn from the logs uniformly at random by token without replacement, resulting in a query sample representative of the overall query distribution.",
                "On average, 30 results were explicitly labeled by human judges using a six point scale ranging from Perfect down to Bad.",
                "Overall, there were over 83,000 results with explicit relevance judgments.",
                "In order to compute various statistics, documents with label Good or better will be considered relevant, and with lower labels to be non-relevant.",
                "Note that the experiments were performed over the results already highly ranked by a web search engine, which corresponds to a typical user experience which is limited to the small number of the highly ranked results for a typical web search query.",
                "The user interactions were collected over a period of 8 weeks using voluntary opt-in information.",
                "In total, over 1.2 million unique queries were instrumented, resulting in over 12 million individual interactions with the search engine.",
                "The data consisted of user interactions with the web search engine (e.g., clicking on a result link, going back to search results, etc.) performed after a query was submitted.",
                "These actions were aggregated across users and search sessions and converted to features in Table 4.1.",
                "To create the training, validation, and test query sets, we created three different random splits of 1,500 training, 500 validation, and 1000 test queries.",
                "The splits were done randomly by query, so that there was no overlap in training, validation, and test queries. 5.2 Evaluation Metrics We evaluate the ranking algorithms over a range of accepted information retrieval metrics, namely Precision at K (P(K)), Normalized Discounted Cumulative Gain (NDCG), and Mean Average Precision (MAP).",
                "Each metric focuses on a deferent aspect of system performance, as we describe below. • Precision at K: As the most intuitive metric, P(K) reports the fraction of documents ranked in the top K results that are labeled as relevant.",
                "In our setting, we require a relevant document to be labeled Good or higher.",
                "The position of relevant documents within the top K is irrelevant, and hence this metric measure overall user satisfaction with the top K results. • NDCG at K: NDCG is a retrieval measure devised specifically for web search evaluation [10].",
                "For a given query q, the ranked results are examined from the top ranked down, and the NDCG computed as:   = +−= K j jr qq jMN 1 )( )1log(/)12( Where Mq is a normalization constant calculated so that a perfect ordering would obtain NDCG of 1; and each r(j) is an integer relevance label (0=Bad and 5=Perfect) of result returned at position j.",
                "Note that unlabeled and Bad documents do not contribute to the sum, but will reduce NDCG for the query pushing down the relevant labeled documents, reducing their contributions.",
                "NDCG is well suited to web search evaluation, as it rewards relevant documents in the top ranked results more heavily than those ranked lower. • MAP: Average precision for each query is defined as the mean of the precision at K values computed after each relevant document was retrieved.",
                "The final MAP value is defined as the mean of average precisions of all queries in the test set.",
                "This metric is the most commonly used single-value summary of a run over a set of queries. 5.3 Ranking Methods Compared Recall that our goal is to quantify the effectiveness of implicit behavior for real web search.",
                "One dimension is to compare the utility of implicit feedback with other information available to a web search engine.",
                "Specifically, we compare effectiveness of implicit user behaviors with content-based matching, static page quality features, and combinations of all features. • BM25F: As a strong web search baseline we used the BM25F scoring, which was used in one of the best performing systems in the TREC 2004 Web track [23,27].",
                "BM25F and its variants have been extensively described and evaluated in IR literature, and hence serve as a strong, reproducible baseline.",
                "The BM25F variant we used for our experiments computes separate match scores for each field for a result document (e.g., body text, title, and anchor text), and incorporates query-independent linkbased information (e.g., PageRank, ClickDistance, and URL depth).",
                "The scoring function and field-specific tuning is described in detail in [23].",
                "Note that BM25F does not directly consider explicit or implicit feedback for tuning. • RN: The ranking produced by a neural net ranker (RankNet, described in Section 3.3) that learns to rank web search results by incorporating BM25F and a large number of additional static and dynamic features describing each search result.",
                "This system automatically learns weights for all features (including the BM25F score for a document) based on explicit human labels for a large set of queries.",
                "A system incorporating an implementation of RankNet is currently in use by a major search engine and can be considered representative of the state of the art in web search. • BM25F-RerankCT: The ranking produced by incorporating clickthrough statistics to reorder web search results ranked by BM25F above.",
                "Clickthrough is a particularly important special case of implicit feedback, and has been shown to correlate with result relevance.",
                "This is a special case of the ranking method in Section 3.1, with the weight wI set to 1000 and the ranking Id is simply the number of clicks on the result corresponding to d. In effect, this ranking brings to the top all returned web search results with at least one click (and orders them in decreasing order by number of clicks).",
                "The relative ranking of the remainder of results is unchanged and they are inserted below all clicked results.",
                "This method serves as our baseline implicit feedback reranking method.",
                "BM25F-RerankAll The ranking produced by reordering the BM25F results using all user behavior features (Section 4).",
                "This method learns a model of user preferences by correlating feature values with explicit relevance labels using the RankNet neural net algorithm (Section 4.2).",
                "At runtime, for a given query the implicit score Ir is computed for each result r with available user interaction features, and the implicit ranking is produced.",
                "The merged ranking is computed as described in Section 3.1.",
                "Based on the experiments over the development set we fix the value of wI to 3 (the effect of the wI parameter for this ranker turned out to be negligible). • BM25F+All: Ranking derived by training the RankNet (Section 3.3) learner over the features set of the BM25F score as well as all implicit feedback features (Section 3.2).",
                "We used the 2-layer implementation of RankNet [5] trained on the queries and labels in the training and validation sets. • RN+All: Ranking derived by training the 2-layer RankNet ranking algorithm (Section 3.3) over the union of all content, dynamic, and implicit feedback features (i.e., all of the features described above as well as all of the new implicit feedback features we introduced).",
                "The ranking methods above span the range of the information used for ranking, from not using the implicit or explicit feedback at all (i.e., BM25F) to a modern web search engine using hundreds of features and tuned on explicit judgments (RN).",
                "As we will show next, incorporating user behavior into these ranking systems dramatically improves the relevance of the returned documents. 6.",
                "EXPERIMENTAL RESULTS Implicit feedback for <br>web search rank</br>ing can be exploited in a number of ways.",
                "We compare alternative methods of exploiting implicit feedback, both by re-ranking the top results (i.e., the BM25F-RerankCT and BM25F-RerankAll methods that reorder BM25F results), as well as by integrating the implicit features directly into the ranking process (i.e., the RN+ALL and BM25F+All methods which learn to rank results over the implicit feedback and other features).",
                "We compare our methods over strong baselines (BM25F and RN) over the NDCG, Precision at K, and MAP measures defined in Section 5.2.",
                "The results were averaged over three random splits of the overall dataset.",
                "Each split contained 1500 training, 500 validation, and 1000 test queries, all query sets disjoint.",
                "We first present the results over all 1000 test queries (i.e., including queries for which there are no implicit measures so we use the original web rankings).",
                "We then drill down to examine the effects on reranking for the attempted queries in more detail, analyzing where implicit feedback proved most beneficial.",
                "We first experimented with different methods of re-ranking the output of the BM25F search results.",
                "Figures 6.1 and 6.2 report NDCG and Precision for BM25F, as well as for the strategies reranking results with user feedback (Section 3.1).",
                "Incorporating all user feedback (either in reranking framework or as features to the learner directly) results in significant improvements (using two-tailed t-test with p=0.01) over both the original BM25F ranking as well as over reranking with clickthrough alone.",
                "The improvement is consistent across the top 10 results and largest for the top result: NDCG at 1 for BM25F+All is 0.622 compared to 0.518 of the original results, and precision at 1 similarly increases from 0.5 to 0.63.",
                "Based on these results we will use the direct feature combination (i.e., BM25F+All) ranker for subsequent comparisons involving implicit feedback. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 1 2 3 4 5 6 7 8 9 10K NDCG BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.1: NDCG at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K 0.35 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.2: Precision at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K Interestingly, using clickthrough alone, while giving significant benefit over the original BM25F ranking, is not as effective as considering the full set of features in Table 4.1.",
                "While we analyze user behavior (and most effective component features) in a separate paper [1], it is worthwhile to give a concrete example of the kind of noise inherent in real user feedback in web search setting. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 Result position Relativeclickfrequency PTR=2 PTR=3 PTR=5 Figure 6.3: Relative clickthrough frequency for queries with varying Position of Top Relevant result (PTR).",
                "If users considered only the relevance of a result to their query, they would click on the topmost relevant results.",
                "Unfortunately, as Joachims and others have shown, presentation also influences which results users click on quite dramatically.",
                "Users often click on results above the relevant one presumably because the short summaries do not provide enough information to make accurate relevance assessments and they have learned that on average topranked items are relevant.",
                "Figure 6.3 shows relative clickthrough frequencies for queries with known relevant items at positions other than the first position; the position of the top relevant result (PTR) ranges from 2-10 in the figure.",
                "For example, for queries with first relevant result at position 5 (PTR=5), there are more clicks on the non-relevant results in higher ranked positions than on the first relevant result at position 5.",
                "As we will see, learning over a richer behavior feature set, results in substantial accuracy improvement over clickthrough alone.",
                "We now consider incorporating user behavior into a much richer feature set, RN (Section 5.3) used by a major web search engine.",
                "RN incorporates BM25F, link-based features, and hundreds of other features.",
                "Figure 6.4 reports NDCG at K and Figure 6.5 reports Precision at K. Interestingly, while the original RN rankings are significantly more accurate than BM25F alone, incorporating implicit feedback features (BM25F+All) results in ranking that significantly outperforms the original RN rankings.",
                "In other words, implicit feedback incorporates sufficient information to replace the hundreds of other features available to the RankNet learner trained on the RN feature set. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 1 2 3 4 5 6 7 8 9 10K NDCG RN RN+All BM25 BM25+All Figure 6.4: NDCG at K for BM25F, BM25F+All, RN, and RN+All for varying K Furthermore, enriching the RN features with implicit feedback set exhibits significant gain on all measures, allowing RN+All to outperform all other methods.",
                "This demonstrates the complementary nature of implicit feedback with other features available to a state of the art web search engine. 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision RN RN+All BM25 BM25+All Figure 6.5: Precision at K for BM25F, BM25F+All, RN, and RN+All for varying K We summarize the performance of the different ranking methods in Table 6.1.",
                "We report the Mean Average Precision (MAP) score for each system.",
                "While not intuitive to interpret, MAP allows quantitative comparison on a single metric.",
                "The gains marked with * are significant at p=0.01 level using two tailed t-test.",
                "MAP Gain P(1) Gain BM25F 0.184 - 0.503BM25F-Rerank-CT 0.215 0.031* 0.577 0.073* BM25F-RerankImplicit 0.218 0.003 0.605 0.028* BM25F+Implicit 0.222 0.004 0.620 0.015* RN 0.215 - 0.597RN+All 0.248 0.033* 0.629 0.032* Table 6.1: Mean Average Precision (MAP) for all strategies.",
                "So far we reported results averaged across all queries in the test set.",
                "Unfortunately, less than half had sufficient interactions to attempt reranking.",
                "Out of the 1000 queries in test, between 46% and 49%, depending on the train-test split, had sufficient interaction information to make predictions (i.e., there was at least 1 search session in which at least 1 result URL was clicked on by the user).",
                "This is not surprising: web search is heavy-tailed, and there are many unique queries.",
                "We now consider the performance on the queries for which user interactions were available.",
                "Figure 6.6 reports NDCG for the subset of the test queries with the implicit feedback features.",
                "The gains at top 1 are dramatic.",
                "The NDCG at 1 of BM25F+All increases from 0.6 to 0.75 (a 31% relative gain), achieving performance comparable to RN+All operating over a much richer feature set. 0.6 0.65 0.7 0.75 0.8 1 3 5 10K NDCG RN RN+All BM25 BM25+All Figure 6.6: NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions Similarly, gains on precision at top 1 are substantial (Figure 6.7), and are likely to be apparent to web search users.",
                "When implicit feedback is available, the BM25F+All system returns relevant document at top 1 almost 70% of the time, compared 53% of the time when implicit feedback is not considered by the original BM25F system. 0.45 0.5 0.55 0.6 0.65 0.7 1 3 5 10K Precision RN RN+All BM25 BM25+All Figure 6.7: Precision at K NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions We summarize the results on the MAP measure for attempted queries in Table 6.2.",
                "MAP improvements are both substantial and significant, with improvements over the BM25F ranker most pronounced.",
                "Method MAP Gain P(1) Gain RN 0.269 0.632 RN+All 0.321 0.051 (19%) 0.693 0.061(10%) BM25F 0.236 0.525 BM25F+All 0.292 0.056 (24%) 0.687 0.162 (31%) Table 6.2: Mean Average Precision (MAP) on attempted queries for best performing methods We now analyze the cases where implicit feedback was shown most helpful.",
                "Figure 6.8 reports the MAP improvements over the baseline BM25F run for each query with MAP under 0.6.",
                "Note that most of the improvement is for poorly performing queries (i.e., MAP < 0.1).",
                "Interestingly, incorporating user behavior information degrades accuracy for queries with high original MAP score.",
                "One possible explanation is that these easy queries tend to be navigational (i.e., having a single, highly-ranked most appropriate answer), and user interactions with lower-ranked results may indicate divergent information needs that are better served by the less popular results (with correspondingly poor overall relevance ratings). 0 50 100 150 200 250 300 350 0.1 0.2 0.3 0.4 0.5 0.6 -0.4 -0.35 -0.3 -0.25 -0.2 -0.15 -0.1 -0.05 0 0.05 0.1 0.15 0.2 Frequency Average Gain Figure 6.8: Gain of BM25F+All over original BM25F ranking To summarize our experimental results, incorporating implicit feedback in real web search setting resulted in significant improvements over the original rankings, using both BM25F and RN baselines.",
                "Our rich set of implicit features, such as time on page and deviations from the average behavior, provides advantages over using clickthrough alone as an indicator of interest.",
                "Furthermore, incorporating implicit feedback features directly into the learned ranking function is more effective than using implicit feedback for reranking.",
                "The improvements observed over large test sets of queries (1,000 total, between 466 and 495 with implicit feedback available) are both substantial and statistically significant. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper we explored the utility of incorporating noisy implicit feedback obtained in a real web search setting to improve <br>web search rank</br>ing.",
                "We performed a large-scale evaluation over 3,000 queries and more than 12 million user interactions with a major search engine, establishing the utility of incorporating noisy implicit feedback to improve web search relevance.",
                "We compared two alternatives of incorporating implicit feedback into the search process, namely reranking with implicit feedback and incorporating implicit feedback features directly into the trained ranking function.",
                "Our experiments showed significant improvement over methods that do not consider implicit feedback.",
                "The gains are particularly dramatic for the top K=1 result in the final ranking, with precision improvements as high as 31%, and the gains are substantial for all values of K. Our experiments showed that implicit user feedback can further improve web search performance, when incorporated directly with popular content- and link-based features.",
                "Interestingly, implicit feedback is particularly valuable for queries with poor original ranking of results (e.g., MAP lower than 0.1).",
                "One promising direction for future work is to apply recent research on automatically predicting query difficulty, and only attempt to incorporate implicit feedback for the difficult queries.",
                "As another research direction we are exploring methods for extending our predictions to the previously unseen queries (e.g., query clustering), which should further improve the web search experience of users.",
                "ACKNOWLEDGMENTS We thank Chris Burges and Matt Richardson for an implementation of RankNet for our experiments.",
                "We also thank Robert Ragno for his valuable suggestions and many discussions. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, S. Dumais, and R.Ragno, Learning User Interaction Models for Predicting Web Search Result Preferences.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan, HARD Track Overview in TREC 2003, High Accuracy Retrieval from Documents, 2003 [3] R. Baeza-Yates and B. Ribeiro-Neto, Modern Information Retrieval, Addison-Wesley, 1999. [4] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine, in Proceedings of WWW, 1997 [5] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning, 2005 [6] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [7] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest.",
                "IEEE Internet Computing. 2001 [8] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [9] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [10] K Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2000 [11] T. Joachims, Optimizing Search Engines Using Clickthrough Data.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [12] T. Joachims, L. Granka, B. Pang, H. Hembrooke, and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [13] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [14] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [15] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [16] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [17] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems.",
                "In Proceedings of the AAAI Workshop on Recommender Systems. 1998 [18] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [19] N. Pharo, N. and K. Järvelin.",
                "The SST method: a tool for analyzing web information search processes.",
                "In Information Processing & Management, 2004 [20] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [21] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (SIGKDD), 2005. [22] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in Proceedings of the ICML Workshop on Learning in Web Search, 2005 [23] S. E. Robertson, H. Zaragoza, and M. Taylor, Simple BM25 extension to multiple weighted fields, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [24] G. Salton & M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [25] E.M. Voorhees, D. Harman, Overview of TREC, 2001 [26] G.R.",
                "Xue, H.J.",
                "Zeng, Z. Chen, Y. Yu, W.Y.",
                "Ma, W.S.",
                "Xi, and W.G.",
                "Fan, Optimizing web search using web clickthrough data, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [27] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
                "Microsoft Cambridge at TREC 13: Web and Hard Tracks.",
                "In Proceedings of TREC 2004"
            ],
            "original_annotated_samples": [
                "We show that incorporating implicit feedback can augment other features, improving the accuracy of a competitive <br>web search rank</br>ing algorithms by as much as 31% relative to the original performance.",
                "The specific contributions of this paper include: • Analysis of alternatives for incorporating user behavior into <br>web search rank</br>ing (Section 3). • An application of a robust implicit feedback model derived from mining millions of user interactions with a major web search engine (Section 4). • A large scale evaluation over real user queries and search results, showing significant improvements derived from incorporating user feedback (Section 6).",
                "At the same time, while recent work (e.g., [26]) on using clickthrough information for improving <br>web search rank</br>ing is promising, it captures only one aspect of the user interactions with web search engines.",
                "The approach above assumes that there are no interactions between the underlying features producing the original <br>web search rank</br>ing and the implicit feedback features.",
                "EXPERIMENTAL RESULTS Implicit feedback for <br>web search rank</br>ing can be exploited in a number of ways."
            ],
            "translated_annotated_samples": [
                "Mostramos que la incorporación de retroalimentación implícita puede aumentar otras características, mejorando la precisión de los algoritmos de <br>clasificación de búsqueda web</br> competitivos hasta en un 31% en comparación con el rendimiento original.",
                "Las contribuciones específicas de este artículo incluyen: • Análisis de alternativas para incorporar el comportamiento del usuario en la <br>clasificación de búsqueda web</br> (Sección 3). • Aplicación de un modelo robusto de retroalimentación implícita derivado de la minería de millones de interacciones de usuarios con un importante motor de búsqueda web (Sección 4). • Una evaluación a gran escala sobre consultas reales de usuarios y resultados de búsqueda, mostrando mejoras significativas derivadas de la incorporación de la retroalimentación del usuario (Sección 6).",
                "Al mismo tiempo, si bien el trabajo reciente (por ejemplo, [26]) sobre el uso de la información de clics para mejorar la <br>clasificación de búsqueda en la web</br> es prometedor, solo abarca un aspecto de las interacciones de los usuarios con los motores de búsqueda en la web.",
                "El enfoque anterior asume que no hay interacciones entre las características subyacentes que producen la <br>clasificación original de la búsqueda web</br> y las características de retroalimentación implícitas.",
                "RESULTADOS EXPERIMENTALES El feedback implícito para la <br>clasificación de búsquedas en la web</br> puede ser explotado de diversas formas."
            ],
            "translated_text": "Mejorando la clasificación de búsqueda web al incorporar información sobre el comportamiento del usuario. Eugene Agichtein, Microsoft Research eugeneag@microsoft.com Eric Brill, Microsoft Research brill@microsoft.com Susan Dumais, Microsoft Research sdumais@microsoft.com RESUMEN Mostramos que la incorporación de datos sobre el comportamiento del usuario puede mejorar significativamente el orden de los principales resultados en una configuración real de búsqueda web. Examinamos alternativas para incorporar retroalimentación en el proceso de clasificación y exploramos las contribuciones de la retroalimentación de usuarios en comparación con otras características comunes de búsqueda en la web. Informamos los resultados de una evaluación a gran escala con más de 3,000 consultas y 12 millones de interacciones de usuarios con un motor de búsqueda web popular. Mostramos que la incorporación de retroalimentación implícita puede aumentar otras características, mejorando la precisión de los algoritmos de <br>clasificación de búsqueda web</br> competitivos hasta en un 31% en comparación con el rendimiento original. Categorías y Descriptores de Asignaturas H.3.3 Búsqueda y Recuperación de Información - Retroalimentación de relevancia, proceso de búsqueda; H.3.5 Servicios de Información en Línea - Servicios basados en la web. Términos generales Algoritmos, Medición, Experimentación 1. INTRODUCCIÓN Millones de usuarios interactúan con los motores de búsqueda a diario. Ellos emiten consultas, siguen algunos de los enlaces en los resultados, hacen clic en anuncios, pasan tiempo en páginas, reformulan sus consultas y realizan otras acciones. Estas interacciones pueden servir como una valiosa fuente de información para ajustar y mejorar la clasificación de los resultados de búsqueda en la web y pueden complementar juicios explícitos más costosos. La retroalimentación implícita de relevancia para la clasificación y personalización se ha convertido en un área activa de investigación. El trabajo reciente de Joachims y otros explorando el feedback implícito en entornos controlados ha demostrado el valor de incorporar el feedback implícito en el proceso de clasificación. Nuestra motivación para este trabajo es comprender cómo se puede utilizar la retroalimentación implícita en un entorno operativo a gran escala para mejorar la recuperación. ¿Cómo se compara y complementa con la evidencia del contenido de la página, el texto del ancla o las características basadas en enlaces como inlinks o PageRank? Si bien es intuitivo que las interacciones de los usuarios con el motor de búsqueda web deberían revelar al menos alguna información que podría ser utilizada para la clasificación, estimar las preferencias de los usuarios en entornos reales de búsqueda web es un problema desafiante, ya que las interacciones reales tienden a ser más ruidosas de lo que comúnmente se asume en los entornos controlados de estudios previos. Nuestro artículo explora si la retroalimentación implícita puede ser útil en entornos realistas, donde la retroalimentación del usuario puede ser ruidosa (o adversa) y un motor de búsqueda web ya utiliza cientos de características y está altamente ajustado. Con este fin, exploramos diferentes enfoques para clasificar los resultados de búsqueda web utilizando el comportamiento real de los usuarios obtenido como parte de las interacciones normales con el motor de búsqueda web. Las contribuciones específicas de este artículo incluyen: • Análisis de alternativas para incorporar el comportamiento del usuario en la <br>clasificación de búsqueda web</br> (Sección 3). • Aplicación de un modelo robusto de retroalimentación implícita derivado de la minería de millones de interacciones de usuarios con un importante motor de búsqueda web (Sección 4). • Una evaluación a gran escala sobre consultas reales de usuarios y resultados de búsqueda, mostrando mejoras significativas derivadas de la incorporación de la retroalimentación del usuario (Sección 6). Resumimos nuestros hallazgos y discutimos extensiones al trabajo actual en la Sección 7, que concluye el artículo. ANTECEDENTES Y TRABAJO RELACIONADO Clasificar los resultados de búsqueda es un problema fundamental en la recuperación de información. La mayoría de los enfoques comunes se centran principalmente en la similitud entre la consulta y una página, así como en la calidad general de la página [3,4,24]. Sin embargo, con la creciente popularidad de los motores de búsqueda, la retroalimentación implícita (es decir, las acciones que los usuarios realizan al interactuar con el motor de búsqueda) se puede utilizar para mejorar las clasificaciones. Las medidas de relevancia implícitas han sido estudiadas por varios grupos de investigación. Un resumen de las medidas implícitas se recopila en Kelly y Teevan [14]. Esta investigación, si bien desarrolló valiosas ideas sobre medidas implícitas de relevancia, no se aplicó para mejorar la clasificación de los resultados de búsqueda en la web en entornos realistas. Estrechamente relacionado con nuestro trabajo, Joachims [11] recopiló medidas implícitas en lugar de medidas explícitas, introduciendo una técnica basada completamente en datos de clics para aprender funciones de clasificación. Fox et al. [8] exploraron la relación entre medidas implícitas y explícitas en la búsqueda web, y desarrollaron modelos bayesianos para correlacionar medidas implícitas y juicios explícitos de relevancia tanto para consultas individuales como para sesiones de búsqueda. Este trabajo consideró una amplia gama de comportamientos de usuario (por ejemplo, tiempo de permanencia, tiempo de desplazamiento, patrones de reformulación) además del popular comportamiento de clics. Sin embargo, el esfuerzo de modelado se centró en predecir juicios de relevancia explícitos a partir de acciones implícitas de los usuarios y no específicamente en aprender funciones de clasificación. Otros estudios sobre el comportamiento de los usuarios en la búsqueda web incluyen a Pharo y Järvelin [19], pero no se aplicaron directamente para mejorar la clasificación. Más recientemente, Joachims et al. [12] presentaron una evaluación empírica de la interpretación de la evidencia de clics. Al realizar estudios de seguimiento ocular y correlacionar las predicciones de sus estrategias con las calificaciones explícitas, los autores demostraron que es posible interpretar con precisión los clics en un entorno controlado de laboratorio. Desafortunadamente, no está claro en qué medida la investigación previa se aplica a la búsqueda web del mundo real. Al mismo tiempo, si bien el trabajo reciente (por ejemplo, [26]) sobre el uso de la información de clics para mejorar la <br>clasificación de búsqueda en la web</br> es prometedor, solo abarca un aspecto de las interacciones de los usuarios con los motores de búsqueda en la web. Nos basamos en investigaciones existentes para desarrollar técnicas robustas de interpretación del comportamiento del usuario para el entorno real de búsqueda en la web. En lugar de tratar a cada usuario como un experto confiable, agregamos información de múltiples trazas de sesiones de búsqueda de usuarios no confiables, como describimos en las siguientes dos secciones. 3. INTEGRANDO LA RETROALIMENTACIÓN IMPLÍCITA Consideramos dos enfoques complementarios para la clasificación con retroalimentación implícita: (1) tratar la retroalimentación implícita como evidencia independiente para clasificar resultados, y (2) integrar características de retroalimentación implícita directamente en el algoritmo de clasificación. Describimos a continuación los dos enfoques generales de clasificación. Las características específicas del feedback implícito se describen en la Sección 4, y los algoritmos para interpretar e incorporar el feedback implícito se describen en la Sección 5. 3.1 Feedback Implícito como Evidencia Independiente. El enfoque general es reordenar los resultados obtenidos por un motor de búsqueda web según los clics observados y otras interacciones de usuario para la consulta en sesiones de búsqueda anteriores. Cada resultado se le asigna una puntuación según la relevancia esperada/ satisfacción del usuario basada en interacciones previas, lo que resulta en un cierto orden de preferencia basado únicamente en las interacciones del usuario. Si bien ha habido un trabajo significativo en la fusión de múltiples clasificaciones, adaptamos un enfoque simple y robusto de ignorar las puntuaciones de los clasificadores originales, y en su lugar simplemente fusionamos los órdenes de clasificación. La razón principal para ignorar las puntuaciones originales es que, dado que los espacios de características y los algoritmos de aprendizaje son diferentes, las puntuaciones no son directamente comparables, y la re-normalización tiende a eliminar el beneficio de incorporar las puntuaciones del clasificador. Experimentamos con una variedad de funciones de fusión en el conjunto de desarrollo de consultas (y utilizando un conjunto de interacciones de un período de tiempo diferente de los conjuntos de evaluación final). Encontramos que una combinación heurística simple de fusión de rangos funciona bien y es robusta a variaciones en los valores de puntuación de los clasificadores originales. Para una consulta dada q, se calcula la puntuación implícita ISd para cada resultado d a partir de las características de interacción del usuario disponibles, lo que resulta en la clasificación implícita Id para cada resultado. Calculamos un puntaje combinado SM(d) para d al combinar los rangos obtenidos de la retroalimentación implícita, Id, con el rango original de d, Od: SM(d) = Od + wI * Id, si existe retroalimentación implícita; de lo contrario, SM(d) = Od. Donde el peso wI es un factor de escala ajustado heurísticamente que representa la importancia relativa de la retroalimentación implícita. Los resultados de la consulta se ordenan en valores decrecientes de SM para producir la clasificación final. Un caso especial de este modelo surge al establecer wI en un valor muy grande, lo que efectivamente obliga a que los resultados clicados se clasifiquen por encima de los resultados no clicados, una heurística intuitiva y efectiva que utilizaremos como referencia. Aplicar algoritmos de combinación de clasificadores y ordenadores más sofisticados puede resultar en mejoras adicionales, y es una dirección prometedora para trabajos futuros. El enfoque anterior asume que no hay interacciones entre las características subyacentes que producen la <br>clasificación original de la búsqueda web</br> y las características de retroalimentación implícitas. Ahora relajamos esta suposición al integrar características de retroalimentación implícita directamente en el proceso de clasificación. 3.2 Clasificación con Características de Retroalimentación Implícita Los motores de búsqueda web modernos clasifican los resultados en función de un gran número de características, incluidas las características basadas en el contenido (es decir, qué tan cerca coincide una consulta con el texto, el título o el texto de anclaje del documento) y las características de calidad de página independientes de la consulta (por ejemplo, PageRank del documento o del dominio). En la mayoría de los casos, se desarrollan métodos automáticos (o semiautomáticos) para ajustar la función de clasificación específica que combina estos valores de características. Por lo tanto, un enfoque natural es incorporar características de retroalimentación implícita directamente como características para el algoritmo de clasificación. Durante el entrenamiento o ajuste, el clasificador puede ser ajustado como antes pero con características adicionales. En tiempo de ejecución, el motor de búsqueda recuperaría las características de retroalimentación implícita asociadas con cada par de URL de consulta y resultado. Este modelo requiere que un algoritmo de clasificación sea robusto ante valores faltantes: más del 50% de las consultas a los motores de búsqueda web son únicas, sin disponer de retroalimentación implícita previa. Ahora describimos un clasificador que utilizamos para aprender sobre los conjuntos de características combinadas, incluyendo la retroalimentación implícita. 3.3 Aprendizaje para Clasificar los Resultados de Búsqueda en la Web Un aspecto clave de nuestro enfoque es aprovechar los avances recientes en el aprendizaje automático, en particular los algoritmos de clasificación entrenables para la búsqueda en la web y la recuperación de información (por ejemplo, [5, 11] y resultados clásicos revisados en [3]). En nuestro entorno, contamos con juicios explícitos de relevancia humana (etiquetas) para un conjunto de consultas de búsqueda en la web y resultados. Por lo tanto, una opción atractiva es utilizar una técnica de aprendizaje automático supervisado para aprender una función de clasificación que prediga mejor las evaluaciones de relevancia. RankNet es uno de esos algoritmos. Es un algoritmo de ajuste de red neuronal que optimiza los pesos de las características para que coincidan mejor con las preferencias de los usuarios proporcionadas explícitamente en pares. Si bien los algoritmos de entrenamiento específicos utilizados por RankNet están más allá del alcance de este documento, se describen en detalle en [5] e incluyen una evaluación exhaustiva y comparación con otros métodos de clasificación. Una característica atractiva de RankNet es su eficiencia tanto en el tiempo de entrenamiento como en el de ejecución: el ranking en tiempo de ejecución se puede calcular rápidamente y puede escalarse a la web, y el entrenamiento se puede realizar sobre miles de consultas y resultados juzgados asociados. Utilizamos una implementación de RankNet de 2 capas para modelar relaciones no lineales entre características. Además, RankNet puede aprender con muchas funciones de costo (diferenciables), por lo que puede aprender automáticamente una función de clasificación a partir de etiquetas proporcionadas por humanos, una alternativa atractiva a las técnicas de combinación de características heurísticas. Por lo tanto, también utilizaremos RankNet como un clasificador genérico para explorar la contribución de la retroalimentación implícita en diferentes alternativas de clasificación. 4. MODELO IMPLÍCITO DE RETROALIMENTACIÓN DEL USUARIO Nuestro objetivo es interpretar con precisión la retroalimentación ruidosa del usuario obtenida al rastrear las interacciones del usuario con el motor de búsqueda. Interpretar la retroalimentación implícita en un entorno real de búsqueda en la web no es una tarea fácil. Caracterizamos este problema en detalle en [1], donde motivamos y evaluamos una amplia variedad de modelos de actividades implícitas de usuario. El enfoque general es representar las acciones del usuario para cada resultado de búsqueda como un vector de características, y luego entrenar un clasificador en base a estas características para descubrir los valores de las características que indican resultados de búsqueda relevantes (y no relevantes). Primero resumimos brevemente nuestras características y modelo, y el enfoque de aprendizaje (Sección 4.2) para proporcionar suficiente información para replicar nuestros métodos de clasificación y los experimentos subsiguientes. 4.1 Representación de las acciones del usuario como características. Modelamos los comportamientos observados en la búsqueda web como una combinación de un \"componente de fondo (es decir, ruido independiente de la consulta y relevancia en el comportamiento del usuario, incluidos sesgos posicionales con interacciones de resultados), y un \"componente de relevancia (es decir, comportamiento específico de la consulta indicativo de la relevancia de un resultado para una consulta). Diseñamos nuestras características para aprovechar el comportamiento de usuario agregado. El conjunto de características está compuesto por características observadas directamente (calculadas directamente a partir de observaciones para cada consulta), así como características derivadas específicas de la consulta, calculadas como la desviación de la distribución general de valores independientes de la consulta para los valores de las características observadas directamente correspondientes. Las características utilizadas para representar las interacciones de los usuarios con los resultados de búsqueda en la web se resumen en la Tabla 4.1. Esta información se obtuvo a través de la instrumentación del lado del cliente opt-in de usuarios de un importante motor de búsqueda web. Incluimos las características tradicionales de retroalimentación implícita, como el recuento de clics en los resultados, así como nuestras características derivadas novedosas, como la desviación del número de clics observados para un par de consulta-URL dado del número esperado de clics en un resultado en la posición dada. También modelamos el comportamiento de navegación después de que se hace clic en un resultado, por ejemplo, el tiempo promedio de permanencia en la página para un par de consulta-URL dado, así como su desviación del tiempo de permanencia esperado (promedio). Además, el conjunto de características fue diseñado para proporcionar información esencial sobre la experiencia del usuario y hacer que la interpretación de la retroalimentación sea sólida. Por ejemplo, los usuarios de búsqueda en la web a menudo pueden determinar si un resultado es relevante al mirar el título del resultado, la URL y el resumen; en muchos casos, no es necesario mirar el documento original. Para modelar este aspecto de la experiencia del usuario, incluimos características como la superposición de palabras en el título y las palabras en la consulta (TitleOverlap) y la fracción de palabras compartidas por la consulta y el resumen del resultado. Características de clics Posición Posición de la URL en el ranking actual Frecuencia de clics Número de clics para esta consulta, par URL Probabilidad de clic Probabilidad de un clic para esta consulta y URL Desviación de clic Desviación de la probabilidad de clic esperada ¿Es el siguiente clic? 1 si se hizo clic en la siguiente posición, 0 de lo contrario ¿Es el clic anterior? 1 si se hizo clic en la posición anterior, 0 de lo contrario ¿Hay clic arriba? 1 si hay un clic arriba, 0 de lo contrario ¿Hay clic abajo? 1 si hay un clic abajo, 0 de lo contrario Características de navegación Tiempo en la página Tiempo de permanencia en la página Tiempo acumulado en la página Tiempo acumulado para todas las páginas siguientes después de la búsqueda Tiempo en el dominio Tiempo de permanencia acumulado para este dominio Tiempo en URL corta Tiempo acumulado en el prefijo de la URL, sin parámetros ¿Se siguió el enlace? 1 si se siguió el enlace al resultado, 0 de lo contrario ¿Coincidencia exacta de URL? 0 si se utilizó normalización agresiva, 1 de lo contrario ¿Redirigido? 1 si la URL inicial es la misma que la URL final, 0 de lo contrario ¿Camino desde la búsqueda? 1 si solo se siguieron enlaces después de la consulta, 0 de lo contrario Clics desde la búsqueda Número de saltos para llegar a la página desde la consulta Tiempo promedio de permanencia Tiempo promedio en la página para esta consulta Desviación del tiempo de permanencia Desviación del tiempo de permanencia promedio en la página Desviación acumulada Desviación del tiempo de permanencia acumulado promedio Desviación del dominio Desviación del tiempo de permanencia promedio en el dominio Características de texto de consulta Coincidencia de título Palabras compartidas entre la consulta y el título Coincidencia de resumen Palabras compartidas entre la consulta y el fragmento Coincidencia de URL de consulta Palabras compartidas entre la consulta y la URL Coincidencia de dominio de consulta Palabras compartidas entre la consulta y el dominio de la URL Longitud de la consulta Número de tokens en la consulta Superposición con la siguiente consulta Fracción de palabras compartidas con la siguiente consulta Tabla 4.1: Algunas características utilizadas para representar el historial de navegación posterior a la búsqueda para una consulta dada y una URL de resultado de búsqueda. Habiendo descrito nuestro conjunto de características, revisamos brevemente nuestro método general para derivar un modelo de comportamiento del usuario. 4.2 Derivación de un Modelo de Retroalimentación del Usuario Para aprender a interpretar el comportamiento observado del usuario, correlacionamos las acciones del usuario (es decir, las características en la Tabla 4.1 que representan las acciones) con los juicios explícitos del usuario para un conjunto de consultas de entrenamiento. Encontramos todas las instancias en nuestros registros de sesión donde se enviaron estas consultas al motor de búsqueda, y agregamos las características del comportamiento del usuario para todas las sesiones de búsqueda que involucran estas consultas. Cada par de consulta-URL observado está representado por las características en la Tabla 4.1, con valores promediados en todas las sesiones de búsqueda, y asignado una de las seis etiquetas de relevancia posibles, que van desde Perfecto hasta Malo, según los juicios explícitos de relevancia. Estos vectores de características etiquetados se utilizan como entrada para el algoritmo de entrenamiento RankNet (Sección 3.3), el cual produce un modelo de comportamiento de usuario entrenado. Este enfoque es particularmente atractivo ya que no requiere heurísticas más allá de la ingeniería de características. El modelo de comportamiento del usuario resultante se utiliza para ayudar a clasificar los resultados de búsqueda en la web, ya sea directamente o en combinación con otras características, como se describe a continuación. 5. CONFIGURACIÓN EXPERIMENTAL El objetivo final de incorporar retroalimentación implícita en la clasificación es mejorar la relevancia de los resultados de búsqueda web devueltos. Por lo tanto, comparamos los métodos de clasificación sobre un gran conjunto de consultas evaluadas con etiquetas de relevancia explícitas proporcionadas por jueces humanos. Para que la evaluación sea realista, obtuvimos una muestra aleatoria de consultas de registros de búsqueda web de un motor de búsqueda importante, con resultados asociados y rastros de acciones de usuario. Describimos este conjunto de datos en detalle a continuación. Nuestros indicadores se describen en la Sección 5.2 que utilizamos para evaluar las alternativas de clasificación, enumeradas en la Sección 5.3 en los experimentos de la Sección 6. 5.1 Conjuntos de datos Comparamos nuestros métodos de clasificación en una muestra aleatoria de 3,000 consultas de los registros de consultas del motor de búsqueda. Las consultas fueron extraídas de los registros de manera uniforme al azar por token sin reemplazo, lo que resultó en una muestra de consultas representativa de la distribución general de consultas. En promedio, 30 resultados fueron etiquetados explícitamente por jueces humanos utilizando una escala de seis puntos que va desde Perfecto hasta Malo. En total, hubo más de 83,000 resultados con juicios de relevancia explícitos. Para calcular diversas estadísticas, se considerarán relevantes los documentos con la etiqueta Buena o mejor, y los documentos con etiquetas inferiores se considerarán no relevantes. Se debe tener en cuenta que los experimentos se realizaron sobre los resultados ya altamente clasificados por un motor de búsqueda web, lo cual corresponde a una experiencia de usuario típica que se limita al pequeño número de resultados altamente clasificados para una consulta típica de búsqueda web. Las interacciones de los usuarios fueron recopiladas durante un período de 8 semanas utilizando información voluntaria de participación. En total, se registraron más de 1.2 millones de consultas únicas, lo que resultó en más de 12 millones de interacciones individuales con el motor de búsqueda. Los datos consistían en las interacciones de los usuarios con el motor de búsqueda web (por ejemplo, hacer clic en un enlace de resultado, regresar a los resultados de búsqueda, etc.) realizadas después de enviar una consulta. Estas acciones fueron agregadas entre usuarios y sesiones de búsqueda y convertidas en características en la Tabla 4.1. Para crear los conjuntos de consultas de entrenamiento, validación y prueba, creamos tres divisiones aleatorias diferentes de 1,500 consultas de entrenamiento, 500 de validación y 1000 de prueba. Las divisiones se realizaron aleatoriamente por consulta, de modo que no hubiera superposición en las consultas de entrenamiento, validación y prueba. 5.2 Métricas de Evaluación Evaluamos los algoritmos de clasificación en una variedad de métricas de recuperación de información aceptadas, a saber, Precisión en K (P(K)), Ganancia Acumulada Descontada Normalizada (NDCG) y Precisión Promedio Media (MAP). Cada métrica se enfoca en un aspecto diferente del rendimiento del sistema, como describimos a continuación. • Precisión en K: Como la métrica más intuitiva, P(K) informa la fracción de documentos clasificados en los primeros K resultados que están etiquetados como relevantes. En nuestro entorno, requerimos que un documento relevante sea etiquetado como Bueno o superior. La posición de los documentos relevantes dentro de los primeros K no es relevante, por lo que esta métrica mide la satisfacción general del usuario con los resultados principales de K. • NDCG en K: NDCG es una medida de recuperación diseñada específicamente para la evaluación de búsqueda en la web [10]. Para una consulta dada q, los resultados clasificados se examinan desde el mejor clasificado hacia abajo, y el NDCG se calcula como: = +−= K j jr qq jMN 1 )( )1log(/)12( Donde Mq es una constante de normalización calculada de manera que un orden perfecto obtendría un NDCG de 1; y cada r(j) es una etiqueta de relevancia entera (0=Malo y 5=Perfecto) del resultado devuelto en la posición j. Ten en cuenta que los documentos sin etiquetar y los documentos malos no contribuyen a la suma, pero reducirán el NDCG para la consulta al empujar hacia abajo los documentos etiquetados relevantes, disminuyendo sus contribuciones. NDCG es muy adecuado para la evaluación de búsquedas en la web, ya que recompensa de manera más intensa los documentos relevantes en los resultados mejor clasificados que aquellos clasificados más bajos. • MAP: La precisión promedio para cada consulta se define como la media de la precisión en los valores de K calculados después de que se recuperó cada documento relevante. El valor MAP final se define como la media de las precisiones promedio de todas las consultas en el conjunto de pruebas. Esta métrica es el resumen de un solo valor más comúnmente utilizado de una ejecución sobre un conjunto de consultas. 5.3 Métodos de clasificación comparados. Recuerde que nuestro objetivo es cuantificar la efectividad del comportamiento implícito para la búsqueda web real. Una dimensión es comparar la utilidad de la retroalimentación implícita con otra información disponible para un motor de búsqueda web. Específicamente, comparamos la efectividad de los comportamientos implícitos de los usuarios con la coincidencia basada en el contenido, las características de calidad de la página estática y combinaciones de todas las características. • BM25F: Como referencia sólida de búsqueda web, utilizamos la puntuación BM25F, que se utilizó en uno de los sistemas con mejor rendimiento en la pista web TREC 2004 [23,27]. BM25F y sus variantes han sido extensamente descritas y evaluadas en la literatura de IR, por lo tanto, sirven como una línea base sólida y reproducible. La variante BM25F que utilizamos en nuestros experimentos calcula puntuaciones de coincidencia separadas para cada campo de un documento de resultado (por ejemplo, texto del cuerpo, título y texto del enlace) e incorpora información de enlaces independiente de la consulta (por ejemplo, PageRank, ClickDistance y profundidad de URL). La función de puntuación y la ajuste específico del campo se describen en detalle en [23]. Tenga en cuenta que BM25F no considera directamente la retroalimentación explícita o implícita para ajuste. • RN: La clasificación producida por un clasificador de redes neuronales (RankNet, descrito en la Sección 3.3) que aprende a clasificar los resultados de búsqueda web al incorporar BM25F y un gran número de características estáticas y dinámicas adicionales que describen cada resultado de búsqueda. Este sistema aprende automáticamente los pesos de todas las características (incluido el puntaje BM25F para un documento) basándose en etiquetas humanas explícitas para un gran conjunto de consultas. Un sistema que incorpora una implementación de RankNet actualmente está siendo utilizado por un importante motor de búsqueda y puede considerarse representativo del estado del arte en la búsqueda web. • BM25F-RerankCT: La clasificación producida al incorporar estadísticas de clics para reordenar los resultados de búsqueda web clasificados por BM25F arriba. El clic es un caso especial particularmente importante de retroalimentación implícita, y se ha demostrado que se correlaciona con la relevancia de los resultados. Este es un caso especial del método de clasificación en la Sección 3.1, con el peso wI establecido en 1000 y la clasificación Id es simplemente el número de clics en el resultado correspondiente a d. En efecto, esta clasificación coloca en la parte superior todos los resultados de búsqueda web devueltos con al menos un clic (y los ordena en orden decreciente por número de clics). La clasificación relativa de los resultados restantes no cambia y se insertan debajo de todos los resultados clicados. Este método sirve como nuestro método de reordenamiento de retroalimentación implícita de referencia. La clasificación producida al reordenar los resultados de BM25F utilizando todas las características del comportamiento del usuario (Sección 4). Este método aprende un modelo de preferencias de usuario correlacionando los valores de las características con etiquetas de relevancia explícitas utilizando el algoritmo de red neuronal RankNet (Sección 4.2). En tiempo de ejecución, para una consulta dada se calcula la puntuación implícita Ir para cada resultado r con características de interacción de usuario disponibles, y se produce el ranking implícito. La clasificación combinada se calcula como se describe en la Sección 3.1. Basándonos en los experimentos realizados sobre el conjunto de desarrollo, fijamos el valor de wI en 3 (el efecto del parámetro wI para este clasificador resultó ser insignificante). • BM25F+All: Clasificación derivada al entrenar el aprendiz RankNet (Sección 3.3) sobre el conjunto de características del puntaje BM25F, así como todas las características de retroalimentación implícita (Sección 3.2). Utilizamos la implementación de 2 capas de RankNet [5] entrenada en las consultas y etiquetas de los conjuntos de entrenamiento y validación. • RN+All: Clasificación derivada al entrenar el algoritmo de clasificación RankNet de 2 capas (Sección 3.3) sobre la unión de todas las características de retroalimentación de contenido, dinámicas e implícitas (es decir, todas las características descritas anteriormente, así como todas las nuevas características de retroalimentación implícita que introdujimos). Los métodos de clasificación anteriores abarcan el rango de la información utilizada para clasificar, desde no utilizar la retroalimentación implícita o explícita en absoluto (es decir, BM25F) hasta un motor de búsqueda web moderno que utiliza cientos de características y está ajustado en base a juicios explícitos (RN). Como mostraremos a continuación, incorporar el comportamiento del usuario en estos sistemas de clasificación mejora drásticamente la relevancia de los documentos devueltos. 6. RESULTADOS EXPERIMENTALES El feedback implícito para la <br>clasificación de búsquedas en la web</br> puede ser explotado de diversas formas. ",
            "candidates": [],
            "error": [
                [
                    "clasificación de búsqueda web",
                    "clasificación de búsqueda web",
                    "clasificación de búsqueda en la web",
                    "clasificación original de la búsqueda web",
                    "clasificación de búsquedas en la web"
                ]
            ]
        }
    }
}