{
    "id": "H-45",
    "original_text": "Query Performance Prediction in Web Search Environments Yun Zhou and W. Bruce Croft Department of Computer Science University of Massachusetts, Amherst {yzhou, croft}@cs.umass.edu ABSTRACT Current prediction techniques, which are generally designed for content-based queries and are typically evaluated on relatively homogenous test collections of small sizes, face serious challenges in web search environments where collections are significantly more heterogeneous and different types of retrieval tasks exist. In this paper, we present three techniques to address these challenges. We focus on performance prediction for two types of queries in web search environments: content-based and Named-Page finding. Our evaluation is mainly performed on the GOV2 collection. In addition to evaluating our models for the two types of queries separately, we consider a more challenging and realistic situation that the two types of queries are mixed together without prior information on query types. To assist prediction under the mixed-query situation, a novel query classifier is adopted. Results show that our prediction of web query performance is substantially more accurate than the current stateof-the-art prediction techniques. Consequently, our paper provides a practical approach to performance prediction in realworld web settings. Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval -Query formulation General Terms Algorithms, Experimentation, Theory 1. INTRODUCTION Query performance prediction has many applications in a variety of information retrieval (IR) areas such as improving retrieval consistency, query refinement, and distributed IR. The importance of this problem has been recognized by IR researchers and a number of new methods have been proposed for prediction recently [1, 2, 17]. Most work on prediction has focused on the traditional ad-hoc retrieval task where query performance is measured according to topical relevance. These prediction models are evaluated on TREC document collections which typically consist of no more than one million relatively homogenous newswire articles. With the popularity and influence of the Web, prediction techniques that will work well for web-style queries are highly preferable. However, web search environments pose significant challenges to current prediction models that are mainly designed for traditional TREC settings. Here we outline some of these challenges. First, web collections, which are much larger than conventional TREC collections, include a variety of documents that are different in many aspects such as quality and style. Current prediction techniques can be vulnerable to these characteristics of web collections. For example, the reported prediction accuracy of the ranking robustness technique and the clarity technique on the GOV2 collection (a large web collection) is significantly worse compared to the other TREC collections [1]. Similar prediction accuracy on the GOV2 collection using another technique is reported in [2], confirming the difficult of predicting query performance on a large web collection. Furthermore, web search goes beyond the scope of the ad-hoc retrieval task based on topical relevance. For example, the Named-Page (NP) finding task, which is a navigational task, is also popular in web retrieval. Query performance prediction for the NP task is still necessary since NP retrieval performance is far from perfect. In fact, according to the report on the NP task of the 2005 Terabyte Track [3], about 40% of the test queries perform poorly (no correct answer in the first 10 search results) even in the best run from the top group. To our knowledge, little research has explicitly addressed the problem of NP-query performance prediction. Current prediction models devised for content-based queries will be less effective for NP queries considering the fundamental differences between the two. Third, in real-world web search environments, user queries are usually a mixture of different types and prior knowledge about the type of each query is generally unavailable. The mixed-query situation raises new problems for query performance prediction. For instance, we may need to incorporate a query classifier into prediction models. Despite these problems, the ability to handle this situation is a crucial step towards turning query performance prediction from an interesting research topic into a practical tool for web retrieval. In this paper, we present three techniques to address the above challenges that current prediction models face in Web search environments. Our work focuses on query performance prediction for the content-based (ad-hoc) retrieval task and the name-page finding task in the context of web retrieval. Our first technique, called weighted information gain (WIG), makes use of both single term and term proximity features to estimate the quality of top retrieved documents for prediction. We find that WIG offers consistent prediction accuracy across various test collections and query types. Moreover, we demonstrate that good prediction accuracy can be achieved for the mixed-query situation by using WIG with the help of a query type classifier. Query feedback and first rank change, which are our second and third prediction techniques, perform well for content-based queries and NP queries respectively. Our main contributions include: (1) considerably improved prediction accuracy for web content-based queries over several state-of-the-art techniques. (2) new techniques for successfully predicting NP-query performance. (3) a practical and fully automatic solution to predicting mixed-query performance. In addition, one minor contribution is that we find that the robustness score [1], which was originally proposed for performance prediction, is helpful for query classification. 2. RELATED WORK As we mentioned in the introduction, a number of prediction techniques have been proposed recently that focus on contentbased queries in the topical relevance (ad-hoc) task. We know of no published work that addresses other types of queries such as NP queries, let alone a mixture of query types. Next we review some representative models. The major difficulty of performance prediction comes from the fact that many factors have an impact on retrieval performance. Each factor affects performance to a different degree and the overall effect is hard to predict accurately. Therefore, it is not surprising to notice that simple features, such as the frequency of query terms in the collection [4] and the average IDF of query terms [5], do not predict well. In fact, most of the successful techniques are based on measuring some characteristics of the retrieved document set to estimate topic difficulty. For example, the clarity score [6] measures the coherence of a list of documents by the KL-divergence between the query model and the collection model. The robustness score [1] quantifies another property of a ranked list: the robustness of the ranking in the presence of uncertainty. Carmel et al. [2] found that the distance measured by the Jensen-Shannon divergence between the retrieved document set and the collection is significantly correlated to average precision. Vinay et al.[7] proposed four measures to capture the geometry of the top retrieved documents for prediction. The most effective measure is the sensitivity to document perturbation, an idea somewhat similar to the robustness score. Unfortunately, their way of measuring the sensitivity does not perform equally well for short queries and prediction accuracy drops considerably when a state-of-the-art retrieval technique (like Okapi or a language modeling approach) is adopted for retrieval instead of the tf-idf weighting used in their paper [16]. The difficulties of applying these models in web search environments have already been mentioned. In this paper, we mainly adopt the clarity score and the robustness score as our baselines. We experimentally show that the baselines, even after being carefully tuned, are inadequate for the web environment. One of our prediction models, WIG, is related to the Markov random field (MRF) model for information retrieval [8]. The MRF model directly models term dependence and is found be to highly effective across a variety of test collections (particularly web collections) and retrieval tasks. This model is used to estimate the joint probability distribution over documents and queries, an important part of WIG. The superiority of WIG over other prediction techniques based on unigram features, which will be demonstrated later in our paper, coincides with that of MRF for retrieval. In other word, it is interesting to note that term dependence, when being modeled appropriately, can be helpful for both improving and predicting retrieval performance. 3. PREDICTION MODELS 3.1 Weighted Information Gain (WIG) This section introduces a weighted information gain approach that incorporates both single term and proximity features for predicting performance for both content-based and Named-Page (NP) finding queries. Given a set of queries Q={Qs} (s=1,2,..N) which includes all possible user queries and a set of documents D={Dt} (t=1,2…M), we assume that each query-document pair (Qs,Dt) is manually judged and will be put in a relevance list if Qs is found to be relevant to Dt. The joint probability P(Qs,Dt) over queries Q and documents D denotes the probability that pair (Qs,Dt) will be in the relevance list. Such assumptions are similar to those used in [8]. Assuming that the user issues query Qi ∈Q and the retrieval results in response to Qi is a ranked list L of documents, we calculate the amount of information contained in P(Qs,Dt) with respect to Qi and L by Eq.1 which is a variant of entropy called the weighted entropy[13]. The weights in Eq.1 are solely determined by Qi and L. )1(),(log),(),( , , ∑−= ts tststsLQ DQPDQweightDQH i In this paper, we choose the weights as follows: LindocumentsKtopthecontainsLTwhere otherwise LTDandisifK DQweight K Kt ts )( )2( ,0 )(,/1 ),( ⎩ ⎨ ⎧ ∈= = The cutoff rank K is a parameter in our model that will be discussed later. Accordingly, Eq.1 can be simplified as follows: )3(),(log 1 ),( )( , ∑∈ −= LTD titsLQ Kt i DQP K DQH Unfortunately, weighted entropy ),(, tsLQ DQH i computed by Eq.3, which represents the amount of information about how likely the top ranked documents in L would be relevant to query Qi on average, cannot be compared across different queries, making it inappropriate for directly predicting query performance. To mitigate this problem, we come up with a background distribution P(Qs,C) over Q and D by imagining that every document in D is replaced by the same special document C which represents average language usage. In this paper, C is created by concatenating every document in D. Roughly speaking, C is the collection (the document set) {Dt} without document boundaries. Similarly, weighted entropy ),(, CQH sLQi calculated by Eq.3 represents the amount of information about how likely an average document (represented by the whole collection) would be relevant to query Qi. Now we introduce our performance predictor WIG which is the weighted information gain [13] computed as the difference between ),(, tsLQ DQH i and ),(, CQH sLQi .Specifically, given query Qi, collection C and ranked list L of documents, WIG is calculated as follows: )4( ),( ),( log 1 ),( ),( log),( ),(),(),,( )(, ,, ∑∑ ∈ == −= LTD i ti ts s ts ts tsLQsLQi Kt ii CQP DQP KCQP DQP DQweight DQHCQHLCQWIG WIG computed by Eq.4 measures the change in information about the quality of retrieval (in response to query Qi) from an imaginary state that only an average document is retrieved to a posterior state that the actual search results are observed. We hypothesize that WIG is positively correlated with retrieval effectiveness because high quality retrieval should be much more effective than just returning the average document. The heart of this technique is how to estimate the joint distribution P(Qs,Dt). In the language modeling approach to IR, a variety of models can be applied readily to estimate this distribution. Although most of these models are based on the bagof-words assumption, recent work on modeling term dependence under the language modeling framework have shown consistent and significant improvements in retrieval effectiveness over bagof-words models. Inspired by the success of incorporating term proximity features into language models, we decide to adopt a good dependence model to estimate the probability P(Qs,Dt). The model we chose for this paper is Metzler and Crofts Markov Random Field (MRF) model, which has already demonstrated superiority over a number of collections and different retrieval tasks [8,9]. According to the MRF model, log P(Qi, Dt) can be written as )5()|(loglog),(log )( 1 ∑∈ +−= iQF tti DPZDQP ξ ξ ξλ where Z1 is a constant that ensures that P(Qi, Dt) sums up to 1. F(Qi) consists of a set of features expanded from the original query Qi . For example, assuming that query Qi is talented student program, F(Qi) includes features like program and talented student. We consider two kinds of features: single term features T and proximity features P. Proximity features include exact phrase (#1) and unordered window (#uwN) features as described in [8]. Note that F(Qi) is the union of T(Qi) and P(Qi). For more details on F(Qi) such as how to expand the original query Qi to F(Qi), we refer the reader to [8] and [9]. P(ξ|Dt) denotes the probability that feature ξ will occur in Dt. More details on P(ξ|Dt) will be provided later in this section. The choice of λξ is somewhat different from that used in [8] since λξ plays a dual role in our model. The first role, which is the same as in [8], is to weight between single term and proximity features. The other role, which is specific to our prediction task, is to normalize the size of F(Qi).We found that the following weight strategy for λξ satisfies the above two roles and generalizes well on a variety of collections and query types. )6( )(, |)(| 1 )(, |)(| ⎪ ⎪ ⎩ ⎪ ⎪ ⎨ ⎧ ∈ − ∈ = i i T i i T QP QP QT QT ξ λ ξ λ λξ where |T(Qi)| and |P(Qi)| denote the number of single term and proximity features in F(Qi) respectively. The reason for choosing the square root function in the denominator of λξ is to penalize a feature set of large size appropriately, making WIG more comparable across queries of various lengths. λT is a fixed parameter and set to 0.8 according to [8] throughout this paper. Similarly, log P(Qi,C) can be written as: )7()|(loglog),(log )( 2 ∑∈ +−= iQF i CPZCQP ξ ξ ξλ When constant Z1 and Z2 are dropped, WIG computed in Eq.4 can be rewritten as follows by plugging in Eq.5 and Eq.7 : )8( )|( )|( log 1 ),,( )( )( ∑ ∑∈ ∈ = LTD QF t i Kt i CP DP K LCQWIG ξ ξ ξ ξ λ One of the advantages of WIG over other techniques is that it can handle well both content-based and NP queries. Based on the type (or the predicted type) of Qi, the calculation of WIG in Eq. 8 differs in two aspects: (1) how to estimate P(ξ|Dt) and P(ξ|C), and (2) how to choose K. For content-based queries, P(ξ|C) is estimated by the relative frequency of feature ξ in collection C as a whole. The estimation of P(ξ|Dt) is the same as in [8]. Namely, we estimate P(ξ|Dt) by the relative frequency of feature ξ in Dt linearly smoothed with collection frequency P(ξ|C). K in Eq.8 is treated as a free parameter. Note that K is the only free parameter in the computation of WIG for content-based queries because all parameters involved in P(ξ|Dt) are assumed to be fixed by taking the suggested values in [8]. Regarding NP queries, we make use of document structure to estimate P(ξ|Dt) and P(ξ|C) by the so-called mixture of language models proposed in [10] and incorporated into the MRF model for Named-Page finding retrieval in [9]. The basic idea is that a document (collection) is divided into several fields such as the title field, the main-body field and the heading field. P(ξ|Dt) and P(ξ|C) are estimated by a linear combination of the language models from each field. Due to space constraints, we refer the reader to [9] for details. We adopt the exact same set of parameters as used in [9] for estimation. With regard to K in Eq.8, we set K to 1 because the Named-Page finding task heavily focuses on the first ranked document. Consequently, there are no free parameters in the computation of WIG for NP queries. 3.2 Query Feedback In this section, we introduce another technique called query feedback (QF) for prediction. Suppose that a user issues query Q to a retrieval system and a ranked list L of documents is returned. We view the retrieval system as a noisy channel. Specifically, we assume that the output of the channel is L and the input is Q. After going through the channel, Q becomes corrupted and is transformed to ranked list L. By thinking about the retrieval process this way, the problem of predicting retrieval effectiveness turns to the task of evaluating the quality of the channel. In other words, prediction becomes finding a way to measure the degree of corruption that arises when Q is transformed to L. As directly computing the degree of the corruption is difficult, we tackle this problem by approximation. Our main idea is that we measure to what extent information on Q can be recovered from L on the assumption that only L is observed. Specifically, we design a decoder that can accurately translate L back into new query Q and the similarity S between the original query Q and the new query Q is adopted as a performance predictor. This is a sketch of how the QF technique predicts query performance. Before filling in more details, we briefly discuss why this method would work. There is a relation between the similarity S defined above and retrieval performance. On the one hand, if the retrieval has strayed from the original sense of the query Q, the new query Q extracted from ranked list L in response to Q would be very different from the original query Q. On the other hand, a query distilled from a ranked list containing many relevant documents is likely to be similar to the original query. Further examples in support of the relation will be provided later. Next we detail how to build the decoder and how to measure the similarity S. In essence, the goal of the decoder is to compress ranked list L into a few informative terms that should represent the content of the top ranked documents in L. Our approach to this goal is to represent ranked list L by a language model (distribution over terms). Then terms are ranked by their contribution to the language models KL (Kullback-Leibler) divergence from the background collection model. Top ranked terms will be chosen to form the new query Q. This approach is similar to that used in Section 4.1 of [11]. Specifically, we take three steps to compress ranked list L into query Q without referring to the original query. 1. We adopt the ranked list language model [14], to estimate a language model based on ranked list L. The model can be written as: )9()|()|()|( ∑∈ = LD LDPDwPLwP where w is any term, D is a document. P(D|L) is estimated by a linearly decreasing function of the rank of document D. 2. Each term in P(w|L) is ranked by the following KL-divergence contribution: )10( )|( )|( log)|( CwP LwP LwP where P(w|C) is the collection model estimated by the relative frequency of term w in collection C as a whole. 3. The top N ranked terms by Eq.10 form a weighted query Q={(wi,ti)} i=1,N. where wi denotes the i-th ranked term and weight ti is the KL-divergence contribution of wi in Eq. 10. Term cruise ship vessel sea passenger KL contribution 0.050 0.040 0.012 0.010 0.009 Table 1: top 5 terms compressed from the ranked list in response to query Cruise ship damage sea life Two representative examples, one for a poorly performing query Cruise ship damage sea life (TREC topic 719; average precision: 0.08) and the other for a high performing query prostate cancer treatments( TREC topic 710; average precision: 0.49), are shown in Table 1 and 2 respectively. These examples indicate how the similarity between the original and the new query correlates with retrieval performance. The parameter N in step 3 is set to 20 empirically and choosing a larger value of N is unnecessary since the weights after the top 20 are usually too small to make any difference. Term prostate cancer treatment men therapy KL contribution 0.177 0.140 0.028 0.025 0.020 Table 2: top 5 terms compressed from the ranked list in response to query prostate cancer treatments To measure the similarity between original query Q and new query Q, we first use Q to do retrieval on the same collection. A variant of the query likelihood model [15] is adopted for retrieval. Namely, documents are ranked by: )11()|()|( ),( ∑∈ = Qtw t i ii i DwPDQP where wi is a term in Q and ti is the associated weight. D is a document. Let L denote the new ranked list returned from the above retrieval. The similarity is measured by the overlap of documents in L and L. Specifically, the percentage of documents in the top K documents of L that are also present in the top K documents in L. the cutoff K is treated as a free parameter. We summarize here how the QF technique predicts performance given a query Q and the associated ranked list L. We first obtain a weighted query Q compressed from L by the above three steps. Then we use Q to perform retrieval and the new ranked list is L. The overlap of documents in L and L is used for prediction. 3.3 First Rank Change (FRC) In this section, we propose a method called the first rank change (FRC) for performance prediction for NP queries. This method is derived from the ranking robustness technique [1] that is mainly designed for content-based queries. When directly applied to NP queries, the robustness technique will be less effective because it takes the top ranked documents as a whole into account while NP queries usually have only one single relevant document. Instead, our technique focuses on the first rank document while the main idea of the robustness method remains. Specifically, the pseudocode for computing FRC is shown in figure 1. Input: (1) ranked list L={Di} where i=1,100. Di denotes the i-th ranked document. (2) query Q 1 initialize: (1) set the number of trials J=100000 (2) counter c=0; 2 for i=1 to J 3 Perturb every document in L, let the outcome be a set F={Di} where Di denotes the perturbed version of Di. 4 Do retrieval with query Q on set F 5 c=c+1 if and only if D1 is ranked first in step 4 6 end of for 7 return the ratio c/J Figure 1: pseudo-code for computing FRC FRC approximates the probability that the first ranked document in the original list L will remain ranked first even after the documents are perturbed. The higher the probability is, the more confidence we have in the first ranked document. On the other hand, in the extreme case of a random ranking, the probability would be as low as 0.5. We expect that FRC has a positive association with NP query performance. We adopt [1] to implement the document perturbation step (step 4 in Fig.1) using Poisson distributions. For more details, we refer the reader to [1]. 4. EVALUATION We now present the results of predicting query performance by our models. Three state-of-the-art techniques are adopted as our baselines. We evaluate our techniques across a variety of Web retrieval settings. As mentioned before, we consider two types of queries, that is, content-based (CB) queries and Named-Page(NP) finding queries. First, suppose that the query types are known. We investigate the correlation between the predicted retrieval performance and the actual performance for both types of queries separately. Results show that our methods yield considerable improvements over the baselines. We then consider a more challenging scenario where no prior information on query types is available. Two sub-cases are considered. In the first one, there exists only one type of query but the actual type is unknown. We assume a mixture of the two query types in the second case. We demonstrate that our models achieve good accuracy under this demanding scenario, making prediction practical in a real-world Web search environment. 4.1 Experimental Setup Our evaluation focuses on the GOV2 collection which contains about 25 million documents crawled from web sites in the .gov domain during 2004 [3]. We create two kinds of data set for CB queries and NP queries respectively. For the CB type, we use the ad-hoc topics of the Terabyte Tracks of 2004, 2005 and 2006 and name them TB04-adhoc, TB05-adhoc and TB06-adhoc respectively. In addition, we also use the ad-hoc topics of the 2004 Robust Track (RT04) to test the adaptability of our techniques to a non-Web environment. For NP queries, we use the Named-Page finding topics of the Terabyte Tracks of 2005 and 2006 and we name them TB05-NP and TB06-NP respectively. All queries used in our experiments are titles of TREC topics as we center on web retrieval. Table 3 summarizes the above data sets. Name Collection Topic Number Query Type TB04-adhoc GOV2 701-750 CB TB05-adhoc GOV2 751-800 CB TB06-adhoc GOV2 801-850 CB RT04 Disk 4+5 (minus CR) 301-450;601700 CB TB05-NP GOV2 NP601-NP872 NP TB06-NP GOV2 NP901-NP1081 NP Table 3: Summary of test collections and topics Retrieval performance of individual content-based and NP queries is measured by the average precision and reciprocal rank of the first correct answer respectively. We make use of the Markov Random field model for both ad-hoc and Named-Page finding retrieval. We adopt the same setting of retrieval parameters used in [8,9]. The Indri search engine [12] is used for all of our experiments. Though not reported here, we also tried the query likelihood model for ad-hoc retrieval and found that the results change little because of the very high correlation between the query performances obtained by the two retrieval models (0.96 measured by Pearsons coefficient). 4.2 Known Query Types Suppose that query types are known. We treat each type of query separately and measure the correlation with average precision (or the reciprocal rank in the case of NP queries). We adopt the Pearsons correlation test which reflects the degree of linear relationship between the predicted and the actual retrieval performance. 4.2.1 Content-based Queries Methods Clarity Robust JSD WIG QF WIG +QF TB04+0 5 adhoc 0.333 0.317 0.362 0.574 0.480 0.637 TB06 adhoc 0.076 0.294 N/A 0.464 0.422 0.511 Table 4: Pearsons correlation coefficients for correlation with average precision on the Terabyte Tracks (ad-hoc) for clarity score, robustness score, the JSD-based method(we directly cites the score reported in [2]), WIG, query feedback(QF) and a linear combination of WIG and QF. Bold cases mean the results are statistically significant at the 0.01 level. Table 4 shows the correlation with average precision on two data sets: one is a combination of TB04-adhoc and TB05-adhoc(100 topics in total) and the other is TB06-adhoc (50 topics). The reason that we put TB04-adhoc and TB05-adhoc together is to make our results comparable to [2]. Our baselines are the clarity score (clarity) [6],the robustness score (robust)[1] and the JSDbased method (JSD) [2]. For the clarity and robustness score, we have tried different parameter settings and report the highest correlation coefficients we have found. We directly cite the result of the JSD-based method reported in [2]. The table also shows the results for the Weighted Information Gain (WIG) method and the Query Feedback (QF) method for predicting content-based queries. As we described in the previous section, both WIG and QF have one free parameter to set, that is, the cutoff rank K. We train the parameter on one dataset and test on the other. When combining WIG and QF, a simple linear combination is used and the combination weight is learned from the training data set. From these results, we can see that our methods are considerably more accurate compared to the baselines. We also observe that further improvements are obtained from the combination of WIG and QF, suggesting that they measure different properties of the retrieval process that relate to performance. We discover that our methods generalize well on TB06-adhoc while the correlation for the clarity score with retrieval performance on this data set is considerably worse. Further investigation shows that the mean average precision of TB06-adhoc is 0.342 and is about 10% better than that of the first data set. While the other three methods typically consider the top 100 or less documents given a ranked list, the clarity method usually needs the top 500 or more documents to adequately measure the coherence of a ranked list. Higher mean average precision makes ranked lists retrieved by different queries more similar in terms of coherence at the level of top 500 documents. We believe that this is the main reason for the low accuracy of the clarity score on the second data set. Though this paper focuses on a Web search environment, it is desirable that our techniques will work consistently well in other situations. To this end, we examine the effectiveness of our techniques on the Robust 2004 Track. For our methods, we evenly divide all of the test queries into five groups and perform five-fold cross validation. Each time we use one group for training and the remaining four groups for testing. We make use of all of the queries for our two baselines, that is, the clarity score and the robustness score. The parameters for our baselines are the same as those used in [1].The results shown in Table 5 demonstrate that the prediction accuracy of our methods is on a par with that of the two strong baselines. Clarity Robust WIG QF 0.464 0.539 0.468 0.464 Table 5: Comparison of Pearsons correlation coefficients on the 2004 Robust Track for clarity score, robustness score, WIG and query feedback (QF). Bold cases mean the results are statistically significant at the 0.01 level. Furthermore, we examine the prediction sensitivity of our methods to the cutoff rank K. With respect to WIG, it is quite robust to K on the Terabyte Tracks (2004-2006) while it prefers a small value of K like 5 on the 2004 Robust Track. In other words, a small value of K is a nearly-optimal choice for both kinds of tracks. Considering the fact that all other parameters involved in WIG are fixed and consequently the same for the two cases, this means WIG can achieve nearly-optimal prediction accuracy in two considerably different situations with exactly the same parameter setting. Regarding QF, it prefers a larger value of K such as 100 on the Terabyte Tracks and a smaller value of K such as 25 on the 2004 Robust Track. 4.2.2 NP Queries We adopt WIG and first rank change (FRC) for predicting NPquery performance. We also try a linear combination of the two as in the previous section. The combination weight is obtained from the other data set. We use the correlation with the reciprocal ranks measured by the Pearsons correlation test to evaluate prediction quality. The results are presented in Table 6. Again, our baselines are the clarity score and the robustness score. To make a fair comparison, we tune the clarity score in different ways. We found that using the first ranked document to build the query model yields the best prediction accuracy. We also attempted to utilize document structure by using the mixture of language models mentioned in section 3.1. Little improvement was obtained. The correlation coefficients for the clarity score reported in Table 6 are the best we have found. As we can see, our methods considerably outperform the clarity score technique on both of the runs. This confirms our intuition that the use of a coherence-based measure like the clarity score is inappropriate for NP queries. Methods Clarity Robust. WIG FRC WIG+FRC TB05-NP 0.150 -0.370 0.458 0.440 0.525 TB06-NP 0.112 -0.160 0.478 0.386 0.515 Table 6: Pearsons correlation coefficients for correlation with reciprocal ranks on the Terabyte Tracks (NP) for clarity score, robustness score, WIG, the first rank change (FRC) and a linear combination of WIG and FRC. Bold cases mean the results are statistically significant at the 0.01 level. Regarding the robustness score, we also tune the parameters and report the best we have found. We observe an interesting and surprising negative correlation with reciprocal ranks. We explain this finding briefly. A high robustness score means that a number of top ranked documents in the original ranked list are still highly ranked after perturbing the documents. The existence of such documents is a good sign of high performance for content-based queries as these queries usually contain a number of relevant documents [1]. However, with regard to NP queries, one fundamental difference is that there is only one relevant document for each query. The existence of such documents can confuse the ranking function and lead to low retrieval performance. Although the negative correlation with retrieval performance exists, the strength of the correlation is weaker and less consistent compared to our methods as shown in Table 6. Based on the above analysis, we can see that current prediction techniques like clarity score and robustness score that are mainly designed for content-based queries face significant challenges and are inadequate to deal with NP queries. Our two techniques proposed for NP queries consistently demonstrate good prediction accuracy, displaying initial success in solving the problem of predicting performance for NP queries. Another point we want to stress is that the WIG method works well for both types of queries, a desirable property that most prediction techniques lack. 4.3 Unknown Query Types In this section, we run two kinds of experiments without access to query type labels. First, we assume that only one type of query exists but the type is unknown. Second, we experiment on a mixture of content-based and NP queries. The following two subsections will report results for the two conditions respectively. 4.3.1 Only One Type exists We assume that all queries are of the same type, that is, they are either NP queries or content-based queries. We choose WIG to deal with this case because it shows good prediction accuracy for both types of queries in the previous section. We consider two cases: (1) CB: all 150 title queries from the ad-hoc task of the Terabyte Tracks 2004-2006 (2)NP: all 433 NP queries from the named page finding task of the Terabyte Tracks 2005 and 2006. We take a simple strategy by labeling all of the queries in each case as the same type (either NP or CB) regardless of their actual type. The computation of WIG will be based on the labeled query type instead of the actual type. There are four possibilities with respect to the relation between the actual type and the labeled type. The correlation with retrieval performance under the four possibilities is presented in Table 7. For example, the value 0.445 at the intersection between the second row and the third column shows the Pearsons correlation coefficient for correlation with average precision when the content-based queries are incorrectly labeled as the NP type. Based on these results, we recommend treating all queries as the NP type when only one query type exists and accurate query classification is not feasible, considering the risk that a large loss of accuracy will occur if NP queries are incorrectly labeled as content-based queries. These results also demonstrate the strong adaptability of WIG to different query types. CB (labeled) NP (labeled) CB (actual) 0.536 0.445 NP (actual) 0.174 0.467 Table 7: Comparison of Pearsons correlation coefficients for correlation with retrieval performance under four possibilities on the Terabyte Tracks (NP). Bold cases mean the results are statistically significant at the 0.01 level. 4.3.2 A mixture of contented-based and NP queries A mixture of the two types of queries is a more realistic situation that a Web search engine will meet. We evaluate prediction accuracy by how accurately poorly-performing queries can be identified by the prediction method assuming that actual query types are unknown (but we can predict query types). This is a challenging task because both the predicted and actual performance for one type of query can be incomparable to that for the other type. Next we discuss how to implement our evaluation. We create a query pool which consists of all of the 150 ad-hoc title queries from Terabyte Track 2004-2006 and all of the 433 NP queries from Terabyte Track 2005&2006. We divide the queries in the pool into classes: good (better than 50% of the queries of the same type in terms of retrieval performance) and bad (otherwise). According to these standards, a NP query with the reciprocal rank above 0.2 or a content-based query with the average precision above 0.315 will be considered as good. Then, each time we randomly select one query Q from the pool with probability p that Q is contented-based. The remaining queries are used as training data. We first decide the type of query Q according to a query classifier. Namely, the query classifier tells us whether query Q is NP or content-based. Based on the predicted query type and the score computed for query Q by a prediction technique, a binary decision is made about whether query Q is good or bad by comparing to the score threshold of the predicted query type obtained from the training data. Prediction accuracy is measured by the accuracy of the binary decision. In our implementation, we repeatedly take a test query from the query pool and prediction accuracy is computed as the percentage of correct decisions, that is, a good(bad) query is predicted to be good (bad). It is obvious that random guessing will lead to 50% accuracy. Let us take the WIG method for example to illustrate the process. Two WIG thresholds (one for NP queries and the other for content-based queries) are trained by maximizing the prediction accuracy on the training data. When a test query is labeled as the NP (CB) type by the query type classifier, it will be predicted to be good if and only if the WIG score for this query is above the NP (CB) threshold. Similar procedures will be taken for other prediction techniques. Now we briefly introduce the automatic query type classifier used in this paper. We find that the robustness score, though originally proposed for performance prediction, is a good indicator of query types. We find that on average content-based queries have a much higher robustness score than NP queries. For example, Figure 2 shows the distributions of robustness scores for NP and content-based queries. According to this finding, the robustness score classifier will attach a NP (CB) label to the query if the robustness score for the query is below (above) a threshold trained from the training data. 0 0.5 1 1.5 2 2.5 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 NP Content-based Figure 2: Distribution of robustness scores for NP and CB queries. The NP queries are the 252 NP topics from the 2005 Terabyte Track. The content-based queries are the 150 ad-hoc title from the Terabyte Tracks 2004-2006. The probability distributions are estimated by the Kernel density estimation method. Strategies Robust WIG-1 WIG-2 WIG-3 Optimal p=0.6 0.565 0.624 0.665 0.684 0.701 P=0.4 0.567 0.633 0.654 0.673 0.696 Table 8: Comparison of prediction accuracy for five strategies in the mixed-query situation. Two ways to sample a query from the pool: (1) the sampled query is content-based with the probability p=0.6. (that is, the query is NP with probability 0.4 ) (2) set the probability p=0.4. We consider five strategies in our experiments. In the first strategy (denoted by robust), we use the robustness score for query performance prediction with the help of a perfect query classifier that always correctly map a query into one of the two categories (that is, NP or CB). This strategy represents the level of prediction accuracy that current prediction techniques can achieve in an ideal condition that query types are known. In the next following three strategies, the WIG method is adopted for performance prediction. The difference among the three is that three different query classifiers are used for each strategy: (1) the classifier always classifies a query into the NP type. (2) the Robustness Score ProbabilityDensity classifier is the robust score classifier mentioned above. (3) the classifier is a perfect one. These three strategies are denoted by WIG-1, WIG-2 and WIG-3 respectively. The reason we are interested in WIG-1 is based on the results from section 4.3.1. In the last strategy (denoted by Optimal) which serves as an upper bound on how well we can do so far, we fully make use of our prediction techniques for each query type assuming a perfect query classifier is available. Specifically, we linearly combine WIG and QF for content-based queries and WIG and FRC for NP queries. The results for the five strategies are shown in Table 8. For each strategy, we try two ways to sample a query from the pool: (1) the sampled query is CB with probability p=0.6. (the query is NP with probability 0.4) (2) set the probability p=0.4. From Table 8 We can see that in terms of prediction accuracy WIG-2 (the WIG method with the automatic query classifier) is not only better than the first two cases, but also is close to WIG-3 where a perfect classifier is assumed. Some further improvements over WIG-3 are observed when combined with other prediction techniques. The merit of WIG-2 is that it provides a practical solution to automatically identifying poorly performing queries in a Web search environment with mixed query types, which poses considerable obstacles to traditional prediction techniques. 5. CONCLUSIONS AND FUTURE WORK To our knowledge, our paper is the first to thoroughly explore prediction of query performance in web search environments. We demonstrated that our models resulted in higher prediction accuracy than previously published techniques not specially devised for web search scenarios. In this paper, we focus on two types of queries in web search: content-based and Named-Page (NP) finding queries, corresponding to the ad-hoc retrieval task and the Named-Page finding task respectively. For both types of web queries, our prediction models were shown to be substantially more accurate than the current state-of-the-art techniques. Furthermore, we considered a more realistic case that no prior information on query types is available. We demonstrated that the WIG method is particularly suitable for this situation. Considering the adaptability of WIG to a range of collections and query types, one of our future plans is to apply this method to predict user preference of search results on realistic data collected from a commercial search engine. Other than accuracy, another major issue that prediction techniques have to deal with in a Web environment is efficiency. Fortunately, since the WIG score is computed just over the terms and the phrases that appear in the query, this calculation can be made very efficient with the support of index. On the other hand, the computation of QF and FRC is relatively less efficient since QF needs to retrieve the whole collection twice and FRC needs to repeatedly rank the perturbed documents. How to improve the efficiency of QF and FRC is our future work. In addition, the prediction techniques proposed in this paper have the potential of improving retrieval performance by combining with other IR techniques. For example, our techniques can be incorporated to popular query modification techniques such as query expansion and query relaxation. Guided by performance prediction, we can make a better decision on when to or how to modify queries to enhance retrieval effectiveness. We would like to carry out research in this direction in the future. 6. ACKNOWLEGMENTS This work was supported in part by the Center for Intelligent Information Retrieval, in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C0023, and in part by an award from Google. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor. In addition, we thank Donald Metzler for his valuable comments on this work. 7. REFERENCES [1] Y. Zhou ,W. B. Croft ,Ranking Robustness: A Novel Framework to Predict Query Performance, in Proceedings of CIKM 2006. [2] D.Carmel, E.Yom-Tov, A.Darlow,D.Pelleg, What Makes a Query Difficult?, in Proceedings of SIGIR 2006. [3] C.L.A. Clarke, F. Scholer, I.Soboroff, The TREC 2005 Terabyte Track, In the Online Proceedings of 2005 TREC. [4] B. He and I.Ounis. Inferring query performance using preretrieval predictors. In proceedings of the SPIRE 2004. [5] S. Tomlinson. Robust, Web and Terabyte Retrieval with Hummingbird SearchServer at TREC 2004. In the Online Proceedings of 2004 TREC. [6] S. Cronen-Townsend, Y. Zhou, W. B. Croft, Predicting Query Performance, in Proceedings of SIGIR 2002. [7] V.Vinay, I.J.Cox, N.Mill-Frayling,K.Wood, On Ranking the Effectiveness of Searcher, in Proceedings of SIGIR 2006. [8] D.Metzler, W.B.Croft, A Markov Random Filed Model for Term Dependencies, in Proceedings of SIGIR 2005. [9] D.Metzler, T.Strohman,Y.Zhou,W.B.Croft, Indri at TREC 2005: Terabyte Track, In the Online Proceedings of 2004 TREC. [10] P. Ogilvie and J. Callan, Combining document representations for known-item search, in Proceedings of SIGIR 2003. [11] A.Berger, J.Lafferty, Information retrieval as statistical translation, in Proceedings of SIGIR 1999. [12] Indri search engine : http://www.lemurproject.org/indri/ [13] I.J. Taneja: On Generalized Information Measures and Their Applications, Advances in Electronics and Electron Physics, Academic Press (USA), 76, 1989, 327-413. [14] S.Cronen-Townsend, Y. Zhou and Croft, W. B. , A Framework for Selective Query Expansion, in Proceedings of CIKM 2004. [15] F.Song, W.B.Croft, A general language model for information retrieval, in Proceedings of SIGIR 1999. [16] Personal email contact with Vishwa Vinay and our own experiments [17] E.Yom-Tov, S.Fine, D.Carmel, A.Darlow, Learning to Estimate Query Difficulty Including Applications to Missing Content Detection and Distributed Information retrieval, in Proceedings of SIGIR 2005",
    "original_translation": "Predicción del rendimiento de consultas en entornos de búsqueda web Yun Zhou y W. Bruce Croft Departamento de Ciencias de la Computación Universidad de Massachusetts, Amherst {yzhou, croft}@cs.umass.edu RESUMEN Las técnicas actuales de predicción, generalmente diseñadas para consultas basadas en contenido y evaluadas típicamente en colecciones de prueba relativamente homogéneas de tamaños pequeños, enfrentan serios desafíos en entornos de búsqueda web donde las colecciones son significativamente más heterogéneas y existen diferentes tipos de tareas de recuperación. En este artículo, presentamos tres técnicas para abordar estos desafíos. Nos enfocamos en la predicción de rendimiento para dos tipos de consultas en entornos de búsqueda web: basadas en contenido y búsqueda de páginas por nombre. Nuestra evaluación se realiza principalmente en la colección GOV2. Además de evaluar nuestros modelos para los dos tipos de consultas por separado, consideramos una situación más desafiante y realista en la que los dos tipos de consultas se mezclan sin información previa sobre los tipos de consulta. Para ayudar en la predicción bajo la situación de consulta mixta, se adopta un clasificador de consultas novedoso. Los resultados muestran que nuestra predicción del rendimiento de la consulta web es sustancialmente más precisa que las técnicas de predicción actuales más avanzadas. Por consiguiente, nuestro artículo proporciona un enfoque práctico para predecir el rendimiento en entornos web del mundo real. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Formulación de Consultas Términos Generales Algoritmos, Experimentación, Teoría 1. La predicción del rendimiento de la consulta tiene muchas aplicaciones en una variedad de áreas de recuperación de información (IR), como mejorar la consistencia de la recuperación, el refinamiento de la consulta y la IR distribuida. La importancia de este problema ha sido reconocida por los investigadores en Recuperación de Información y recientemente se han propuesto varios métodos nuevos para la predicción [1, 2, 17]. La mayoría de los trabajos sobre predicción se han centrado en la tarea tradicional de recuperación ad-hoc, donde el rendimiento de la consulta se mide según la relevancia temática. Estos modelos de predicción se evalúan en colecciones de documentos de TREC que suelen consistir en no más de un millón de artículos de agencias de noticias relativamente homogéneos. Con la popularidad e influencia de la Web, las técnicas de predicción que funcionen bien para consultas de estilo web son altamente preferibles. Sin embargo, los entornos de búsqueda web plantean desafíos significativos para los modelos de predicción actuales que están principalmente diseñados para entornos tradicionales de TREC. Aquí delineamos algunos de estos desafíos. Primero, las colecciones web, que son mucho más grandes que las colecciones TREC convencionales, incluyen una variedad de documentos que son diferentes en muchos aspectos, como la calidad y el estilo. Las técnicas actuales de predicción pueden ser vulnerables a estas características de las colecciones web. Por ejemplo, la precisión de predicción reportada de la técnica de robustez de clasificación y la técnica de claridad en la colección GOV2 (una colección web grande) es significativamente peor en comparación con las otras colecciones de TREC [1]. En [2] se informa de una precisión de predicción similar en la colección GOV2 utilizando otra técnica, confirmando la dificultad de predecir el rendimiento de consultas en una gran colección web. Además, la búsqueda web va más allá del alcance de la tarea de recuperación ad hoc basada en la relevancia temática. Por ejemplo, la tarea de encontrar páginas nombradas (NP), que es una tarea de navegación, también es popular en la recuperación web. La predicción del rendimiento de la consulta para la tarea de NP sigue siendo necesaria, ya que el rendimiento de recuperación de NP está lejos de ser perfecto. De hecho, según el informe sobre la tarea NP del Terabyte Track de 2005 [3], aproximadamente el 40% de las consultas de prueba tienen un rendimiento deficiente (sin respuesta correcta en los primeros 10 resultados de búsqueda) incluso en la mejor ejecución del grupo líder. Hasta donde sabemos, poca investigación ha abordado explícitamente el problema de la predicción del rendimiento de consultas de NP. Los modelos de predicción actuales diseñados para consultas basadas en contenido serán menos efectivos para consultas de NP considerando las diferencias fundamentales entre ambas. En tercer lugar, en entornos de búsqueda web del mundo real, las consultas de los usuarios suelen ser una mezcla de diferentes tipos y, por lo general, no se dispone de información previa sobre el tipo de cada consulta. La situación de consulta mixta plantea nuevos problemas para la predicción del rendimiento de la consulta. Por ejemplo, es posible que necesitemos incorporar un clasificador de consultas en los modelos de predicción. A pesar de estos problemas, la capacidad para manejar esta situación es un paso crucial hacia convertir la predicción del rendimiento de consultas de un tema de investigación interesante en una herramienta práctica para la recuperación web. En este artículo, presentamos tres técnicas para abordar los desafíos mencionados que enfrentan los modelos de predicción actuales en entornos de búsqueda web. Nuestro trabajo se centra en la predicción del rendimiento de consultas para la tarea de recuperación basada en contenido (ad-hoc) y la tarea de encontrar páginas por nombre en el contexto de la recuperación web. Nuestra primera técnica, llamada ganancia de información ponderada (WIG), utiliza tanto características de términos individuales como de proximidad de términos para estimar la calidad de los documentos recuperados para la predicción. Observamos que WIG ofrece una precisión de predicción consistente en diversas colecciones de pruebas y tipos de consultas. Además, demostramos que se puede lograr una buena precisión de predicción para la situación de consulta mixta utilizando WIG con la ayuda de un clasificador de tipo de consulta. La retroalimentación de consultas y el cambio de rango inicial, que son nuestras técnicas de predicción segunda y tercera, funcionan bien para consultas basadas en contenido y consultas de NP respectivamente. Nuestras principales contribuciones incluyen: (1) una precisión de predicción considerablemente mejorada para consultas basadas en contenido web en comparación con varias técnicas de vanguardia. (2) nuevas técnicas para predecir con éxito el rendimiento de consultas NP. (3) una solución práctica y totalmente automática para predecir el rendimiento de consultas mixtas. Además, una contribución menor es que encontramos que el puntaje de robustez [1], que originalmente se propuso para la predicción de rendimiento, es útil para la clasificación de consultas. 2. TRABAJO RELACIONADO Como mencionamos en la introducción, recientemente se han propuesto varias técnicas de predicción que se centran en consultas basadas en contenido en la tarea de relevancia temática (ad-hoc). No conocemos ningún trabajo publicado que aborde otros tipos de consultas como las consultas NP, y mucho menos una mezcla de tipos de consultas. A continuación revisamos algunos modelos representativos. La principal dificultad de la predicción del rendimiento proviene del hecho de que muchos factores tienen un impacto en el rendimiento de recuperación. Cada factor afecta el rendimiento en diferente medida y el efecto general es difícil de predecir con precisión. Por lo tanto, no es sorprendente notar que características simples, como la frecuencia de los términos de consulta en la colección [4] y la IDF promedio de los términos de consulta [5], no predicen bien. De hecho, la mayoría de las técnicas exitosas se basan en medir algunas características del conjunto de documentos recuperados para estimar la dificultad del tema. Por ejemplo, la puntuación de claridad [6] mide la coherencia de una lista de documentos mediante la divergencia KL entre el modelo de consulta y el modelo de colección. El puntaje de robustez [1] cuantifica otra propiedad de una lista clasificada: la robustez de la clasificación en presencia de incertidumbre. Carmel et al. [2] encontraron que la distancia medida por la divergencia de Jensen-Shannon entre el conjunto de documentos recuperados y la colección está significativamente correlacionada con la precisión promedio. Vinay et al. [7] propusieron cuatro medidas para capturar la geometría de los documentos mejor clasificados para la predicción. La medida más efectiva es la sensibilidad a la perturbación del documento, una idea algo similar a la puntuación de robustez. Desafortunadamente, su forma de medir la sensibilidad no funciona igual de bien para consultas cortas y la precisión de la predicción disminuye considerablemente cuando se adopta una técnica de recuperación de vanguardia (como Okapi o un enfoque de modelado de lenguaje) en lugar del peso tf-idf utilizado en su artículo [16]. Las dificultades de aplicar estos modelos en entornos de búsqueda web ya han sido mencionadas. En este documento, principalmente adoptamos la puntuación de claridad y la puntuación de robustez como nuestras líneas base. Experimentalmente demostramos que las líneas base, incluso después de ser ajustadas cuidadosamente, son inadecuadas para el entorno web. Uno de nuestros modelos de predicción, WIG, está relacionado con el modelo de campo aleatorio de Markov (MRF) para la recuperación de información [8]. El modelo MRF modela directamente la dependencia de términos y se ha demostrado ser altamente efectivo en una variedad de colecciones de pruebas (especialmente colecciones web) y tareas de recuperación. Este modelo se utiliza para estimar la distribución conjunta de probabilidad sobre documentos y consultas, una parte importante de WIG. La superioridad de WIG sobre otras técnicas de predicción basadas en características unigramas, que se demostrará más adelante en nuestro artículo, coincide con la de MRF para recuperación. En otras palabras, es interesante notar que la dependencia de términos, cuando se modela adecuadamente, puede ser útil tanto para mejorar como para predecir el rendimiento de recuperación. 3. Modelos de predicción 3.1 Ganancia de Información Ponderada (WIG) Esta sección introduce un enfoque de ganancia de información ponderada que incorpora tanto características de términos únicos como de proximidad para predecir el rendimiento tanto en consultas basadas en contenido como en consultas de búsqueda de Páginas Nombradas (NP). Dado un conjunto de consultas Q={Qs} (s=1,2,..N) que incluye todas las posibles consultas de usuario y un conjunto de documentos D={Dt} (t=1,2…M), asumimos que cada par consulta-documento (Qs,Dt) es evaluado manualmente y se incluirá en una lista de relevancia si se determina que Qs es relevante para Dt. La probabilidad conjunta P(Qs,Dt) sobre las consultas Q y los documentos D denota la probabilidad de que el par (Qs,Dt) esté en la lista de relevancia. Tales suposiciones son similares a las utilizadas en [8]. Suponiendo que el usuario emite la consulta Qi ∈Q y los resultados de recuperación en respuesta a Qi es una lista clasificada L de documentos, calculamos la cantidad de información contenida en P(Qs,Dt) con respecto a Qi y L mediante la Ec.1, que es una variante de la entropía llamada entropía ponderada[13]. Los pesos en la ecuación 1 están determinados únicamente por Qi y L. En este documento, elegimos los pesos de la siguiente manera: L es el número de documentos que contiene la consulta, K es el límite superior, LT es el límite superior de documentos, y D es el peso si K está en otro caso. La clasificación de corte K es un parámetro en nuestro modelo que se discutirá más adelante. Por lo tanto, la ecuación 1 se puede simplificar de la siguiente manera: )3(),(log 1 ),( )( , ∑∈ −= LTD titsLQ Kt i DQP K DQH Desafortunadamente, la entropía ponderada ),(, tsLQ DQH i calculada por la ecuación 3, que representa la cantidad de información sobre la probabilidad de que los documentos mejor clasificados en L sean relevantes para la consulta Qi en promedio, no se puede comparar entre diferentes consultas, lo que la hace inapropiada para predecir directamente el rendimiento de la consulta. Para mitigar este problema, creamos una distribución de fondo P(Qs,C) sobre Q y D imaginando que cada documento en D es reemplazado por el mismo documento especial C que representa el uso promedio del lenguaje. En este documento, C se crea concatenando cada documento en D. A grosso modo, C es la colección (el conjunto de documentos) {Dt} sin límites de documentos. De manera similar, la entropía ponderada ),(, CQH sLQi calculada por la Ecuación 3 representa la cantidad de información sobre la probabilidad de que un documento promedio (representado por toda la colección) sea relevante para la consulta Qi. Ahora presentamos nuestro predictor de rendimiento WIG que es la ganancia de información ponderada [13] calculada como la diferencia entre ),(, tsLQ DQH i y ),(, CQH sLQi. Específicamente, dado el conjunto de consultas Qi, la colección C y la lista clasificada L de documentos, WIG se calcula de la siguiente manera: )4( ),( ),( log 1 ),( ),( log),( ),(),(),,( )(, ,, ∑∑ ∈ == −= LTD i ti ts s ts ts tsLQsLQi Kt ii CQP DQP KCQP DQP DQweight DQHCQHLCQWIG WIG calculado por la Ecuación 4 mide el cambio en la información sobre la calidad de la recuperación (en respuesta a la consulta Qi) desde un estado imaginario en el que solo se recupera un documento promedio hasta un estado posterior en el que se observan los resultados de búsqueda reales. Hacemos la hipótesis de que WIG está positivamente correlacionado con la efectividad de la recuperación porque una recuperación de alta calidad debería ser mucho más efectiva que simplemente devolver el documento promedio. El corazón de esta técnica es cómo estimar la distribución conjunta P(Qs, Dt). En el enfoque de modelado del lenguaje para la recuperación de información, se pueden aplicar fácilmente una variedad de modelos para estimar esta distribución. Aunque la mayoría de estos modelos se basan en la suposición de la bolsa de palabras, trabajos recientes sobre modelado de dependencia de términos bajo el marco de modelado de lenguaje han demostrado mejoras consistentes y significativas en la efectividad de recuperación sobre los modelos de bolsa de palabras. Inspirados por el éxito de incorporar características de proximidad de términos en modelos de lenguaje, decidimos adoptar un buen modelo de dependencia para estimar la probabilidad P(Qs,Dt). El modelo que elegimos para este artículo es el modelo de Campo Aleatorio de Markov (MRF) de Metzler y Crofts, que ya ha demostrado su superioridad en comparación con varias colecciones y diferentes tareas de recuperación [8,9]. Según el modelo MRF, log P(Qi, Dt) se puede escribir como )5()|(loglog),(log )( 1 ∑∈ +−= iQF tti DPZDQP ξ ξ ξλ donde Z1 es una constante que asegura que P(Qi, Dt) sume 1. F(Qi) consiste en un conjunto de características ampliadas a partir de la consulta original Qi. Por ejemplo, suponiendo que la consulta Qi es el programa de estudiantes talentosos, F(Qi) incluye características como programa y estudiante talentoso. Consideramos dos tipos de características: características de términos individuales T y características de proximidad P. Las características de proximidad incluyen la frase exacta (#1) y las características de ventana desordenada (#uwN) como se describe en [8]. Ten en cuenta que F(Qi) es la unión de T(Qi) y P(Qi). Para obtener más detalles sobre F(Qi), como por ejemplo cómo expandir la consulta original Qi a F(Qi), remitimos al lector a [8] y [9]. P(ξ|Dt) denota la probabilidad de que la característica ξ ocurra en Dt. Más detalles sobre P(ξ|Dt) se proporcionarán más adelante en esta sección. La elección de λξ es algo diferente a la utilizada en [8] ya que λξ desempeña un doble papel en nuestro modelo. El primer rol, que es el mismo que en [8], es ponderar entre las características de términos individuales y de proximidad. El otro rol, específico de nuestra tarea de predicción, es normalizar el tamaño de F(Qi). Encontramos que la siguiente estrategia de peso para λξ satisface los dos roles anteriores y generaliza bien en una variedad de colecciones y tipos de consultas. )6( )(, |)(| 1 )(, |)(| ⎪ ⎪ ⎩ ⎪ ⎪ ⎨ ⎧ ∈ − ∈ = i i T i i T QP QP QT QT ξ λ ξ λ λξ donde |T(Qi)| y |P(Qi)| denotan el número de características de términos únicos y de proximidad en F(Qi) respectivamente. La razón de elegir la función raíz cuadrada en el denominador de λξ es penalizar adecuadamente un conjunto de características de gran tamaño, haciendo que WIG sea más comparable entre consultas de diversas longitudes. λT es un parámetro fijo y se establece en 0.8 según [8] a lo largo de este documento. De manera similar, log P(Qi,C) se puede escribir como: )7()|(loglog),(log )( 2 ∑∈ +−= iQF i CPZCQP ξ ξ ξλ Cuando se eliminan las constantes Z1 y Z2, el WIG calculado en la Ecuación 4 se puede reescribir de la siguiente manera al sustituir la Ecuación 5 y la Ecuación 7: )8( )|( )|( log 1 ),,( )( )( ∑ ∑∈ ∈ = LTD QF t i Kt i CP DP K LCQWIG ξ ξ ξ ξ λ Una de las ventajas del WIG sobre otras técnicas es que puede manejar tanto consultas basadas en contenido como en NP. Basándose en el tipo (o el tipo predicho) de Qi, el cálculo de WIG en la Ecuación 8 difiere en dos aspectos: (1) cómo estimar P(ξ|Dt) y P(ξ|C), y (2) cómo elegir K. Para consultas basadas en contenido, P(ξ|C) se estima por la frecuencia relativa del rasgo ξ en la colección C en su totalidad. La estimación de P(ξ|Dt) es la misma que en [8]. Es decir, estimamos P(ξ|Dt) mediante la frecuencia relativa del rasgo ξ en Dt suavizada linealmente con la frecuencia de colección P(ξ|C). La K en la Ec.8 se trata como un parámetro libre. Ten en cuenta que K es el único parámetro libre en el cálculo de WIG para consultas basadas en contenido, ya que se asume que todos los parámetros involucrados en P(ξ|Dt) están fijos al tomar los valores sugeridos en [8]. En cuanto a las consultas de NP, hacemos uso de la estructura del documento para estimar P(ξ|Dt) y P(ξ|C) mediante el llamado modelo de mezcla de lenguaje propuesto en [10] e incorporado en el modelo MRF para la recuperación de Named-Page en [9]. La idea básica es que un documento (colección) se divide en varios campos como el campo del título, el campo del cuerpo principal y el campo de los encabezados. P(ξ|Dt) y P(ξ|C) se estiman mediante una combinación lineal de los modelos de lenguaje de cada campo. Debido a limitaciones de espacio, remitimos al lector a [9] para más detalles. Adoptamos el mismo conjunto exacto de parámetros utilizados en [9] para la estimación. En cuanto a K en la Ec.8, establecemos K en 1 porque la tarea de encontrar Named-Page se centra fuertemente en el documento clasificado en primer lugar. Por consiguiente, no hay parámetros libres en el cálculo de WIG para consultas de NP. 3.2 Retroalimentación de consultas En esta sección, presentamos otra técnica llamada retroalimentación de consultas (QF) para la predicción. Suponga que un usuario emite la consulta Q a un sistema de recuperación y se devuelve una lista clasificada L de documentos. Vemos el sistema de recuperación como un canal ruidoso. Específicamente, asumimos que la salida del canal es L y la entrada es Q. Después de pasar por el canal, Q se corrompe y se transforma en la lista clasificada L. Al pensar en el proceso de recuperación de esta manera, el problema de predecir la eficacia de la recuperación se convierte en la tarea de evaluar la calidad del canal. En otras palabras, la predicción se convierte en encontrar una forma de medir el grado de corrupción que surge cuando Q se transforma en L. Dado que calcular directamente el grado de corrupción es difícil, abordamos este problema mediante aproximaciones. Nuestra idea principal es que medimos en qué medida la información sobre Q puede ser recuperada de L bajo la suposición de que solo se observa L. Específicamente, diseñamos un decodificador que pueda traducir con precisión L de vuelta a la nueva consulta Q y la similitud S entre la consulta original Q y la nueva consulta Q se adopta como un predictor de rendimiento. Este es un bosquejo de cómo la técnica QF predice el rendimiento de la consulta. Antes de completar más detalles, discutimos brevemente por qué este método funcionaría. Existe una relación entre la similitud S definida anteriormente y el rendimiento de recuperación. Por un lado, si la recuperación se ha desviado del sentido original de la consulta Q, la nueva consulta Q extraída de la lista clasificada L en respuesta a Q sería muy diferente de la consulta original Q. Por otro lado, una consulta destilada de una lista clasificada que contiene muchos documentos relevantes es probable que sea similar a la consulta original. Se proporcionarán más ejemplos en apoyo de la relación más adelante. A continuación detallamos cómo construir el decodificador y cómo medir la similitud S. En esencia, el objetivo del decodificador es comprimir la lista clasificada L en unos pocos términos informativos que deberían representar el contenido de los documentos mejor clasificados en L. Nuestro enfoque para este objetivo es representar la lista clasificada L mediante un modelo de lenguaje (distribución de términos). Luego, los términos se clasifican según su contribución a la divergencia KL (Kullback-Leibler) de los modelos de lenguaje con respecto al modelo de la colección de fondo. Los términos mejor clasificados serán elegidos para formar la nueva consulta Q. Este enfoque es similar al utilizado en la Sección 4.1 de [11]. Específicamente, tomamos tres pasos para comprimir la lista clasificada L en la consulta Q sin hacer referencia a la consulta original. 1. Adoptamos el modelo de lenguaje de lista clasificada [14] para estimar un modelo de lenguaje basado en la lista clasificada L. El modelo puede escribirse como: )9()|()|()|( ∑∈ = LD LDPDwPLwP donde w es cualquier término y D es un documento. La probabilidad de que ocurra el evento D dado el evento L se estima mediante una función linealmente decreciente del rango del documento D. Cada término en P(w|L) está clasificado por la siguiente contribución de divergencia de Kullback-Leibler: )10( )|( )|( log)|( CwP LwP LwP donde P(w|C) es el modelo de colección estimado por la frecuencia relativa del término w en la colección C en su totalidad. 3. Los términos N principales clasificados por Eq.10 forman una consulta ponderada Q={(wi,ti)} i=1,N, donde wi denota el término clasificado en la posición i y el peso ti es la contribución de la divergencia KL de wi en Eq. 10. Barco de crucero daño vida marina. Estos ejemplos indican cómo la similitud entre la consulta original y la nueva se correlaciona con el rendimiento de recuperación. El parámetro N en el paso 3 se establece en 20 de manera empírica y elegir un valor más grande de N es innecesario ya que los pesos después de los primeros 20 suelen ser demasiado pequeños para marcar alguna diferencia. Para medir la similitud entre la consulta original Q y la nueva consulta Q, primero utilizamos Q para recuperar información de la misma colección. Se adopta una variante del modelo de probabilidad de consulta [15] para la recuperación. Es decir, los documentos se clasifican por: )11()|()|( ),( ∑∈ = Qtw t i ii i DwPDQP donde wi es un término en Q y ti es el peso asociado. D es un documento. Que L denote la nueva lista clasificada devuelta desde la recuperación anterior. La similitud se mide por la superposición de documentos en L y L. Específicamente, el porcentaje de documentos en los primeros K documentos de L que también están presentes en los primeros K documentos en L. El valor de corte K se trata como un parámetro libre. Aquí resumimos cómo la técnica QF predice el rendimiento dado una consulta Q y la lista clasificada asociada L. Primero obtenemos una consulta ponderada Q comprimida a partir de L mediante los tres pasos anteriores. Luego usamos Q para realizar la recuperación y la nueva lista clasificada es L. La superposición de documentos en L y L se utiliza para la predicción. 3.3 Primer Cambio de Rango (FRC) En esta sección, proponemos un método llamado primer cambio de rango (FRC) para la predicción de rendimiento para consultas de NP. Este método se deriva de la técnica de robustez de clasificación [1] que está principalmente diseñada para consultas basadas en contenido. Cuando se aplica directamente a consultas de NP, la técnica de robustez será menos efectiva porque tiene en cuenta todos los documentos mejor clasificados en su totalidad, mientras que las consultas de NP suelen tener solo un único documento relevante. En cambio, nuestra técnica se centra en el documento de rango uno mientras se mantiene la idea principal del método de robustez. Específicamente, el seudocódigo para calcular FRC se muestra en la figura 1. Lista clasificada L={Di} donde i=1,100. Di denota el documento clasificado en la posición i. (2) consulta Q 1 inicializar: (1) establecer el número de intentos J=100000 (2) contador c=0; 2 para i=1 a J 3 Perturbar cada documento en L, que el resultado sea un conjunto F={Di} donde Di denota la versión perturbada de Di. 4 Realizar la recuperación con la consulta Q en el conjunto F 5 c=c+1 si y solo si D1 está clasificado primero en el paso 4 6 fin del para 7 devolver la proporción c/J Figura 1: pseudo-código para calcular FRC FRC aproxima la probabilidad de que el documento clasificado en primer lugar en la lista original L permanezca clasificado primero incluso después de que los documentos sean perturbados. Cuanto mayor sea la probabilidad, más confianza tenemos en el documento clasificado en primer lugar. Por otro lado, en el caso extremo de un ranking aleatorio, la probabilidad sería tan baja como 0.5. Esperamos que FRC tenga una asociación positiva con el rendimiento de la consulta de NP. Adoptamos [1] para implementar el paso de perturbación del documento (paso 4 en la Fig. 1) utilizando distribuciones de Poisson. Para más detalles, remitimos al lector a [1]. 4. EVALUACIÓN Ahora presentamos los resultados de predecir el rendimiento de la consulta por nuestros modelos. Tres técnicas de vanguardia son adoptadas como nuestras líneas base. Evaluamos nuestras técnicas en una variedad de configuraciones de recuperación web. Como se mencionó anteriormente, consideramos dos tipos de consultas, es decir, consultas basadas en contenido (CB) y consultas de búsqueda de páginas con nombre (NP). Primero, supongamos que se conocen los tipos de consultas. Investigamos la correlación entre el rendimiento de recuperación predicho y el rendimiento real para ambos tipos de consultas por separado. Los resultados muestran que nuestros métodos producen mejoras considerablemente superiores a los valores de referencia. Luego consideramos un escenario más desafiante donde no hay información previa sobre los tipos de consultas disponibles. Se consideran dos subcasos. En el primero, existe solo un tipo de consulta pero el tipo real es desconocido. Suponemos una mezcla de los dos tipos de consultas en el segundo caso. Demostramos que nuestros modelos logran una buena precisión en este escenario exigente, lo que hace que la predicción sea práctica en un entorno de búsqueda web del mundo real. 4.1 Configuración Experimental Nuestra evaluación se centra en la colección GOV2 que contiene alrededor de 25 millones de documentos obtenidos de sitios web en el dominio .gov durante 2004 [3]. Creamos dos tipos de conjuntos de datos para consultas de CB y consultas de NP respectivamente. Para el tipo CB, utilizamos los temas ad-hoc de las pistas Terabyte de 2004, 2005 y 2006 y los nombramos TB04-adhoc, TB05-adhoc y TB06-adhoc respectivamente. Además, también utilizamos los temas ad-hoc de la pista Robusta de 2004 (RT04) para probar la adaptabilidad de nuestras técnicas a un entorno no web. Para consultas de NP, utilizamos los temas de búsqueda de páginas nombradas de las Terabyte Tracks de 2005 y 2006 y los denominamos TB05-NP y TB06-NP respectivamente. Todas las consultas utilizadas en nuestros experimentos son títulos de temas de TREC, ya que nos centramos en la recuperación web. La Tabla 3 resume los conjuntos de datos anteriores. La recuperación del rendimiento de las consultas individuales basadas en contenido y NP se mide mediante la precisión promedio y la tasa recíproca de la primera respuesta correcta, respectivamente. Hacemos uso del modelo de campo aleatorio de Markov tanto para la recuperación ad-hoc como para la recuperación de páginas nombradas. Adoptamos la misma configuración de parámetros de recuperación utilizada en [8,9]. El motor de búsqueda Indri [12] se utiliza para todos nuestros experimentos. Aunque no se informa aquí, también probamos el modelo de probabilidad de consulta para la recuperación ad-hoc y encontramos que los resultados cambian poco debido a la correlación muy alta entre el rendimiento de las consultas obtenido por los dos modelos de recuperación (0.96 medido por el coeficiente de Pearson). 4.2 Tipos de Consultas Conocidos Supongamos que se conocen los tipos de consultas. Tratamos cada tipo de consulta por separado y medimos la correlación con la precisión promedio (o el rango recíproco en el caso de consultas NP). Adoptamos la prueba de correlación de Pearson que refleja el grado de relación lineal entre el rendimiento de recuperación predicho y real. 4.2.1 Métodos de Consultas Basadas en Contenido Claridad Robusta JSD WIG QF WIG +QF TB04+0 5 adhoc 0.333 0.317 0.362 0.574 0.480 0.637 TB06 adhoc 0.076 0.294 N/A 0.464 0.422 0.511 Tabla 4: Coeficientes de correlación de Pearson para la correlación con la precisión promedio en las Pistas de Terabyte (ad-hoc) para la puntuación de claridad, puntuación de robustez, el método basado en JSD (citamos directamente la puntuación informada en [2]), WIG, retroalimentación de consulta (QF) y una combinación lineal de WIG y QF. Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. La Tabla 4 muestra la correlación con la precisión promedio en dos conjuntos de datos: uno es una combinación de TB04-adhoc y TB05-adhoc (100 temas en total) y el otro es TB06-adhoc (50 temas). La razón por la que juntamos TB04-adhoc y TB05-adhoc es para hacer nuestros resultados comparables a [2]. Nuestros puntos de referencia son la puntuación de claridad (claridad) [6], la puntuación de robustez (robust) [1] y el método basado en JSD (JSD) [2]. Para la puntuación de claridad y robustez, hemos probado diferentes configuraciones de parámetros y reportamos los coeficientes de correlación más altos que hemos encontrado. Citamos directamente el resultado del método basado en JSD reportado en [2]. La tabla también muestra los resultados para el método de Ganancia de Información Ponderada (WIG) y el método de Retroalimentación de Consulta (QF) para predecir consultas basadas en contenido. Como describimos en la sección anterior, tanto WIG como QF tienen un parámetro libre para ajustar, es decir, el rango de corte K. Entrenamos el parámetro en un conjunto de datos y lo probamos en el otro. Al combinar WIG y QF, se utiliza una combinación lineal simple y el peso de la combinación se aprende a partir del conjunto de datos de entrenamiento. A partir de estos resultados, podemos ver que nuestros métodos son considerablemente más precisos en comparación con los baselines. También observamos que se obtienen mejoras adicionales a partir de la combinación de WIG y QF, lo que sugiere que miden diferentes propiedades del proceso de recuperación que se relacionan con el rendimiento. Descubrimos que nuestros métodos se generalizan bien en TB06-adhoc, mientras que la correlación del puntaje de claridad con el rendimiento de recuperación en este conjunto de datos es considerablemente peor. Una investigación adicional muestra que la precisión promedio media de TB06-adhoc es de 0.342 y es aproximadamente un 10% mejor que la del primer conjunto de datos. Mientras que los otros tres métodos suelen considerar los 100 mejores documentos o menos de una lista clasificada, el método de claridad generalmente necesita los 500 mejores documentos o más para medir adecuadamente la coherencia de una lista clasificada. Un promedio de precisión media más alto hace que las listas clasificadas recuperadas por diferentes consultas sean más similares en términos de coherencia en el nivel de los primeros 500 documentos. Creemos que esta es la razón principal de la baja precisión de la puntuación de claridad en el segundo conjunto de datos. Aunque este documento se centra en un entorno de búsqueda web, es deseable que nuestras técnicas funcionen de manera consistente en otras situaciones. Con este fin, examinamos la efectividad de nuestras técnicas en la pista Robust 2004. Para nuestros métodos, dividimos equitativamente todas las consultas de prueba en cinco grupos y realizamos validación cruzada de cinco pliegues. Cada vez que utilizamos un grupo para el entrenamiento y los otros cuatro grupos para las pruebas. Hacemos uso de todas las consultas para nuestros dos puntos de referencia, es decir, la puntuación de claridad y la puntuación de robustez. Los parámetros para nuestras líneas base son los mismos que los utilizados en [1]. Los resultados mostrados en la Tabla 5 demuestran que la precisión de predicción de nuestros métodos está a la par con la de las dos líneas base sólidas. Claridad Robusta WIG QF 0.464 0.539 0.468 0.464 Tabla 5: Comparación de los coeficientes de correlación de Pearson en la pista Robusta de 2004 para la puntuación de claridad, puntuación de robustez, WIG y retroalimentación de consulta (QF). Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. Además, examinamos la sensibilidad de predicción de nuestros métodos al rango de corte K. Con respecto a WIG, es bastante robusto a K en las Pistas de Terabyte (2004-2006) mientras prefiere un valor pequeño de K como 5 en la Pista Robusta de 2004. En otras palabras, un valor pequeño de K es una elección casi óptima para ambos tipos de pistas. Teniendo en cuenta que todos los demás parámetros involucrados en WIG están fijos y, consecuentemente, son los mismos para los dos casos, esto significa que WIG puede lograr una precisión de predicción casi óptima en dos situaciones considerablemente diferentes con exactamente la misma configuración de parámetros. En cuanto a QF, prefiere un valor más grande de K, como 100 en las Pistas de Terabyte y un valor más pequeño de K, como 25 en la Pista Robusta de 2004. 4.2.2 Consultas NP Adoptamos WIG y el primer cambio de rango (FRC) para predecir el rendimiento de las consultas NP. También intentamos una combinación lineal de los dos como en la sección anterior. El peso de la combinación se obtiene del otro conjunto de datos. Utilizamos la correlación con los rangos recíprocos medidos por la prueba de correlación de Pearson para evaluar la calidad de la predicción. Los resultados se presentan en la Tabla 6. Nuevamente, nuestros puntos de referencia son la puntuación de claridad y la puntuación de robustez. Para hacer una comparación justa, ajustamos la puntuación de claridad de diferentes maneras. Descubrimos que utilizar el documento clasificado en primer lugar para construir el modelo de consulta produce la mejor precisión de predicción. También intentamos utilizar la estructura del documento mediante la combinación de modelos de lenguaje mencionados en la sección 3.1. Se obtuvo una pequeña mejora. Los coeficientes de correlación para la puntuación de claridad reportados en la Tabla 6 son los mejores que hemos encontrado. Como podemos ver, nuestros métodos superan considerablemente la técnica de puntuación de claridad en ambas ejecuciones. Esto confirma nuestra intuición de que el uso de una medida basada en la coherencia como el puntaje de claridad es inapropiado para las consultas de NP. Claridad de métodos robusta. Tabla 6: Coeficientes de correlación de Pearson para la correlación con rangos recíprocos en las pistas de Terabyte (NP) para la puntuación de claridad, puntuación de robustez, WIG, el primer cambio de rango (FRC) y una combinación lineal de WIG y FRC. Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. En cuanto a la puntuación de robustez, también ajustamos los parámetros y reportamos el mejor que hemos encontrado. Observamos una correlación negativa interesante y sorprendente con los rangos recíprocos. Explicamos este hallazgo brevemente. Un puntaje de robustez alto significa que varios documentos de alta clasificación en la lista clasificada original siguen estando muy bien clasificados después de perturbar los documentos. La existencia de tales documentos es una buena señal de alto rendimiento para consultas basadas en contenido, ya que estas consultas suelen contener una serie de documentos relevantes [1]. Sin embargo, en lo que respecta a las consultas de NP, una diferencia fundamental es que solo hay un documento relevante para cada consulta. La existencia de tales documentos puede confundir la función de clasificación y llevar a un bajo rendimiento de recuperación. Aunque existe una correlación negativa con el rendimiento de recuperación, la fuerza de la correlación es más débil y menos consistente en comparación con nuestros métodos, como se muestra en la Tabla 6. Basándonos en el análisis anterior, podemos ver que las técnicas de predicción actuales como la puntuación de claridad y la puntuación de robustez, que están principalmente diseñadas para consultas basadas en contenido, enfrentan desafíos significativos y son insuficientes para manejar consultas de NP. Nuestras dos técnicas propuestas para consultas de NP demuestran consistentemente una buena precisión de predicción, mostrando un éxito inicial en la resolución del problema de predecir el rendimiento para consultas de NP. Otro punto que queremos destacar es que el método WIG funciona bien para ambos tipos de consultas, una propiedad deseable que la mayoría de las técnicas de predicción carecen. 4.3 Tipos de Consultas Desconocidos En esta sección, realizamos dos tipos de experimentos sin acceso a etiquetas de tipo de consulta. Primero, asumimos que solo existe un tipo de consulta pero el tipo es desconocido. Segundo, experimentamos con una mezcla de consultas basadas en contenido y NP. Las dos subsecciones siguientes informarán los resultados para las dos condiciones respectivamente. 4.3.1 Solo existe un tipo. Suponemos que todas las consultas son del mismo tipo, es decir, son consultas NP o consultas basadas en contenido. Elegimos WIG para tratar este caso porque muestra una buena precisión de predicción para ambos tipos de consultas en la sección anterior. Consideramos dos casos: (1) CB: todas las 150 consultas de títulos de la tarea ad-hoc de las pistas Terabyte 2004-2006 (2) NP: todas las 433 consultas de NP de la tarea de búsqueda de páginas nombradas de las pistas Terabyte 2005 y 2006. Tomamos una estrategia simple etiquetando todas las consultas en cada caso como el mismo tipo (ya sea NP o CB) independientemente de su tipo real. El cálculo de WIG se basará en el tipo de consulta etiquetado en lugar del tipo real. Hay cuatro posibilidades con respecto a la relación entre el tipo real y el tipo etiquetado. La correlación con el rendimiento de recuperación bajo las cuatro posibilidades se presenta en la Tabla 7. Por ejemplo, el valor 0.445 en la intersección entre la segunda fila y la tercera columna muestra el coeficiente de correlación de Pearson para la correlación con la precisión promedio cuando las consultas basadas en contenido se etiquetan incorrectamente como del tipo NP. Basándonos en estos resultados, recomendamos tratar todas las consultas como del tipo NP cuando solo existe un tipo de consulta y la clasificación precisa de la consulta no es factible, considerando el riesgo de que se produzca una gran pérdida de precisión si las consultas NP se etiquetan incorrectamente como consultas basadas en contenido. Estos resultados también demuestran la fuerte adaptabilidad de WIG a diferentes tipos de consultas. Tabla 7: Comparación de los coeficientes de correlación de Pearson para la correlación con el rendimiento de recuperación en cuatro posibilidades en las pistas de Terabyte (NP). Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. 4.3.2 Una mezcla de consultas basadas en contenido y NP Una mezcla de los dos tipos de consultas es una situación más realista que un motor de búsqueda web encontrará. Evaluamos la precisión de la predicción por la exactitud con la que se pueden identificar las consultas de bajo rendimiento mediante el método de predicción, asumiendo que los tipos de consulta reales son desconocidos (pero podemos predecir los tipos de consulta). Esta es una tarea desafiante porque tanto el rendimiento predicho como el real para un tipo de consulta pueden ser incomparables con el de otro tipo. A continuación discutimos cómo implementar nuestra evaluación. Creamos un conjunto de consultas que consiste en todas las 150 consultas de título ad-hoc de Terabyte Track 2004-2006 y todas las 433 consultas NP de Terabyte Track 2005&2006. Dividimos las consultas en el conjunto en clases: buenas (mejores que el 50% de las consultas del mismo tipo en términos de rendimiento de recuperación) y malas (de lo contrario). Según estos estándares, una consulta de NP con un rango recíproco superior a 0.2 o una consulta basada en contenido con una precisión promedio superior a 0.315 se considerarán como buenas. Entonces, cada vez que seleccionamos aleatoriamente una consulta Q del grupo con una probabilidad p de que Q esté basada en el contenido. Las consultas restantes se utilizan como datos de entrenamiento. Primero decidimos el tipo de consulta Q de acuerdo con un clasificador de consultas. Es decir, el clasificador de consultas nos dice si la consulta Q es de tipo NP o basada en contenido. Basándose en el tipo de consulta predicho y en la puntuación calculada para la consulta Q por una técnica de predicción, se toma una decisión binaria sobre si la consulta Q es buena o mala al compararla con el umbral de puntuación del tipo de consulta predicho obtenido de los datos de entrenamiento. La precisión de la predicción se mide por la exactitud de la decisión binaria. En nuestra implementación, tomamos repetidamente una consulta de prueba del conjunto de consultas y la precisión de la predicción se calcula como el porcentaje de decisiones correctas, es decir, una consulta buena (mala) se predice como buena (mala). Es obvio que adivinar al azar llevará a una precisión del 50%. Tomemos el método WIG como ejemplo para ilustrar el proceso. Dos umbrales de WIG (uno para consultas de NP y otro para consultas basadas en contenido) se entrenan maximizando la precisión de predicción en los datos de entrenamiento. Cuando una consulta de prueba es etiquetada como del tipo NP (CB) por el clasificador de tipo de consulta, se predecirá que es buena solo si la puntuación WIG para esta consulta está por encima del umbral de NP (CB). Se seguirán procedimientos similares para otras técnicas de predicción. Ahora presentamos brevemente el clasificador de tipo de consulta automático utilizado en este artículo. Encontramos que la puntuación de robustez, aunque originalmente propuesta para la predicción de rendimiento, es un buen indicador de tipos de consultas. Observamos que, en promedio, las consultas basadas en contenido tienen una puntuación de robustez mucho más alta que las consultas de NP. Por ejemplo, la Figura 2 muestra las distribuciones de puntajes de robustez para consultas basadas en NP y en contenido. Según este hallazgo, el clasificador de puntuación de robustez adjuntará una etiqueta NP (CB) a la consulta si la puntuación de robustez para la consulta está por debajo (por encima) de un umbral entrenado a partir de los datos de entrenamiento. 0 0.5 1 1.5 2 2.5 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 NP Basado en contenido Figura 2: Distribución de puntuaciones de robustez para consultas NP y CB. Las consultas NP son los 252 temas NP del Terabyte Track de 2005. Las consultas basadas en contenido son los 150 títulos ad-hoc de las pistas Terabyte 2004-2006. Las distribuciones de probabilidad son estimadas por el método de estimación de densidad de Kernel. Estrategias Robustas WIG-1 WIG-2 WIG-3 Óptima p=0.6 0.565 0.624 0.665 0.684 0.701 P=0.4 0.567 0.633 0.654 0.673 0.696 Tabla 8: Comparación de la precisión de predicción para cinco estrategias en la situación de consulta mixta. Dos formas de muestrear una consulta de la piscina: (1) la consulta muestreada es basada en contenido con una probabilidad p=0.6 (es decir, la consulta es NP con una probabilidad de 0.4) (2) establecer la probabilidad p=0.4. Consideramos cinco estrategias en nuestros experimentos. En la primera estrategia (denominada robusta), utilizamos la puntuación de robustez para la predicción del rendimiento de la consulta con la ayuda de un clasificador de consultas perfecto que siempre mapea correctamente una consulta en una de las dos categorías (es decir, NP o CB). Esta estrategia representa el nivel de precisión de predicción que las técnicas actuales de predicción pueden lograr en una condición ideal en la que se conocen los tipos de consulta. En las tres estrategias siguientes, se adopta el método WIG para predecir el rendimiento. La diferencia entre los tres es que se utilizan tres clasificadores de consulta diferentes para cada estrategia: (1) el clasificador siempre clasifica una consulta en el tipo NP. (2) el clasificador de Probabilidad de Densidad de Puntuación Robusta es el clasificador de puntuación robusta mencionado anteriormente. (3) el clasificador es perfecto. Estas tres estrategias están designadas como WIG-1, WIG-2 y WIG-3 respectivamente. La razón por la que estamos interesados en WIG-1 se basa en los resultados de la sección 4.3.1. En la última estrategia (denominada Óptima) que sirve como un límite superior de lo bien que podemos hacer hasta ahora, hacemos pleno uso de nuestras técnicas de predicción para cada tipo de consulta asumiendo que un clasificador de consultas perfecto está disponible. Específicamente, combinamos linealmente WIG y QF para consultas basadas en contenido y WIG y FRC para consultas de NP. Los resultados de las cinco estrategias se muestran en la Tabla 8. Para cada estrategia, intentamos dos formas de muestrear una consulta del conjunto: (1) la consulta muestreada es CB con probabilidad p=0.6 (la consulta es NP con probabilidad 0.4) (2) establecer la probabilidad p=0.4. De la Tabla 8 podemos ver que en términos de precisión de predicción, WIG-2 (el método WIG con el clasificador de consultas automático) no solo es mejor que los dos primeros casos, sino que también está cerca de WIG-3 donde se asume un clasificador perfecto. Se observan algunas mejoras adicionales sobre WIG-3 cuando se combinan con otras técnicas de predicción. El mérito de WIG-2 es que proporciona una solución práctica para identificar automáticamente consultas con bajo rendimiento en un entorno de búsqueda web con tipos de consultas mixtos, lo cual representa considerables obstáculos para las técnicas de predicción tradicionales. CONCLUSIONES Y TRABAJOS FUTUROS Hasta donde sabemos, nuestro artículo es el primero en explorar a fondo la predicción del rendimiento de consultas en entornos de búsqueda web. Demostramos que nuestros modelos resultaron en una mayor precisión de predicción que las técnicas previamente publicadas no especialmente diseñadas para escenarios de búsqueda en la web. En este artículo, nos enfocamos en dos tipos de consultas en la búsqueda web: consultas basadas en contenido y consultas de búsqueda de Páginas-Nombradas (NP), correspondientes a la tarea de recuperación ad-hoc y la tarea de búsqueda de Páginas-Nombradas respectivamente. Para ambos tipos de consultas web, se demostró que nuestros modelos de predicción son sustancialmente más precisos que las técnicas actuales más avanzadas. Además, consideramos un caso más realista en el que no se dispone de información previa sobre los tipos de consultas. Demostramos que el método WIG es particularmente adecuado para esta situación. Considerando la adaptabilidad de WIG a una variedad de colecciones y tipos de consultas, uno de nuestros planes futuros es aplicar este método para predecir la preferencia del usuario por los resultados de búsqueda en datos realistas recopilados de un motor de búsqueda comercial. Además de la precisión, otro problema importante con el que las técnicas de predicción tienen que lidiar en un entorno web es la eficiencia. Afortunadamente, dado que el puntaje WIG se calcula solo sobre los términos y frases que aparecen en la consulta, este cálculo puede realizarse de manera muy eficiente con el soporte de un índice. Por otro lado, el cálculo de QF y FRC es relativamente menos eficiente ya que QF necesita recuperar toda la colección dos veces y FRC necesita clasificar repetidamente los documentos perturbados. Cómo mejorar la eficiencia de QF y FRC es nuestro trabajo futuro. Además, las técnicas de predicción propuestas en este documento tienen el potencial de mejorar el rendimiento de recuperación al combinarse con otras técnicas de RI. Por ejemplo, nuestras técnicas pueden ser incorporadas a técnicas populares de modificación de consultas como la expansión de consultas y la relajación de consultas. Guiados por la predicción del rendimiento, podemos tomar una mejor decisión sobre cuándo o cómo modificar las consultas para mejorar la efectividad de la recuperación. Nos gustaría llevar a cabo investigaciones en esta dirección en el futuro. AGRADECIMIENTOS Este trabajo fue apoyado en parte por el Centro de Recuperación de Información Inteligente, en parte por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el contrato número HR0011-06-C0023, y en parte por un premio de Google. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son del autor y no necesariamente reflejan las del patrocinador. Además, agradecemos a Donald Metzler por sus valiosos comentarios sobre este trabajo. 7. REFERENCIAS [1] Y. Zhou, W. B. Croft, Ranking Robustness: Un nuevo marco para predecir el rendimiento de consultas, en Actas de CIKM 2006. [2] D. Carmel, E. Yom-Tov, A. Darlow, D. Pelleg, ¿Qué hace que una consulta sea difícil?, en Actas de SIGIR 2006. [3] C. L. A. Clarke, F. Scholer, I. Soboroff, La pista Terabyte TREC 2005, En las Actas en Línea de TREC 2005. [4] B. Él y yo. Inferir el rendimiento de la consulta utilizando predictores de preretrieval. En actas de SPIRE 2004. [5] S. Tomlinson. Recuperación robusta de la web y terabytes con Hummingbird SearchServer en TREC 2004. En las Actas en Línea de TREC 2004. [6] S. Cronen-Townsend, Y. Zhou, W. B. Croft, Prediciendo el Rendimiento de Consultas, en las Actas de SIGIR 2002. [7] V. Vinay, I. J. Cox, N. Mill-Frayling, K. Wood, Sobre la Clasificación de la Efectividad del Buscador, en las Actas de SIGIR 2006. [8] D. Metzler, W. B. Croft, Un Modelo de Campo Aleatorio de Markov para Dependencias de Términos, en las Actas de SIGIR 2005. [9] D. Metzler, T. Strohman, Y. Zhou, W. B. Croft, Indri en TREC 2005: Terabyte Track, en las Actas en Línea de TREC 2004. [10] P. Ogilvie y J. Callan, Combinando representaciones de documentos para búsqueda de elementos conocidos, en las Actas de SIGIR 2003. [11] A. Berger, J. Lafferty, Recuperación de información como traducción estadística, en las Actas de SIGIR 1999. [12] Motor de búsqueda Indri: http://www.lemurproject.org/indri/ [13] I. J. Taneja: Sobre medidas de información generalizadas y sus aplicaciones, Avances en Electrónica y Física de Electrones, Academic Press (EE. UU.), 76, 1989, 327-413. [14] S. Cronen-Townsend, Y. Zhou y Croft, W. B., Un marco para la expansión selectiva de consultas, en Actas de CIKM 2004. [15] F. Song, W. B. Croft, Un modelo de lenguaje general para la recuperación de información, en Actas de SIGIR 1999. [16] Contacto por correo electrónico personal con Vishwa Vinay y nuestros propios experimentos [17] E. Yom-Tov, S. Fine, D. Carmel, A. Darlow, Aprendiendo a estimar la dificultad de la consulta incluyendo aplicaciones para la detección de contenido faltante y la recuperación de información distribuida, en Actas de SIGIR 2005.",
    "original_sentences": [
        "Query Performance Prediction in Web Search Environments Yun Zhou and W. Bruce Croft Department of Computer Science University of Massachusetts, Amherst {yzhou, croft}@cs.umass.edu ABSTRACT Current prediction techniques, which are generally designed for content-based queries and are typically evaluated on relatively homogenous test collections of small sizes, face serious challenges in web search environments where collections are significantly more heterogeneous and different types of retrieval tasks exist.",
        "In this paper, we present three techniques to address these challenges.",
        "We focus on performance prediction for two types of queries in web search environments: content-based and Named-Page finding.",
        "Our evaluation is mainly performed on the GOV2 collection.",
        "In addition to evaluating our models for the two types of queries separately, we consider a more challenging and realistic situation that the two types of queries are mixed together without prior information on query types.",
        "To assist prediction under the mixed-query situation, a novel query classifier is adopted.",
        "Results show that our prediction of web query performance is substantially more accurate than the current stateof-the-art prediction techniques.",
        "Consequently, our paper provides a practical approach to performance prediction in realworld web settings.",
        "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval -Query formulation General Terms Algorithms, Experimentation, Theory 1.",
        "INTRODUCTION Query performance prediction has many applications in a variety of information retrieval (IR) areas such as improving retrieval consistency, query refinement, and distributed IR.",
        "The importance of this problem has been recognized by IR researchers and a number of new methods have been proposed for prediction recently [1, 2, 17].",
        "Most work on prediction has focused on the traditional ad-hoc retrieval task where query performance is measured according to topical relevance.",
        "These prediction models are evaluated on TREC document collections which typically consist of no more than one million relatively homogenous newswire articles.",
        "With the popularity and influence of the Web, prediction techniques that will work well for web-style queries are highly preferable.",
        "However, web search environments pose significant challenges to current prediction models that are mainly designed for traditional TREC settings.",
        "Here we outline some of these challenges.",
        "First, web collections, which are much larger than conventional TREC collections, include a variety of documents that are different in many aspects such as quality and style.",
        "Current prediction techniques can be vulnerable to these characteristics of web collections.",
        "For example, the reported prediction accuracy of the ranking robustness technique and the clarity technique on the GOV2 collection (a large web collection) is significantly worse compared to the other TREC collections [1].",
        "Similar prediction accuracy on the GOV2 collection using another technique is reported in [2], confirming the difficult of predicting query performance on a large web collection.",
        "Furthermore, web search goes beyond the scope of the ad-hoc retrieval task based on topical relevance.",
        "For example, the Named-Page (NP) finding task, which is a navigational task, is also popular in web retrieval.",
        "Query performance prediction for the NP task is still necessary since NP retrieval performance is far from perfect.",
        "In fact, according to the report on the NP task of the 2005 Terabyte Track [3], about 40% of the test queries perform poorly (no correct answer in the first 10 search results) even in the best run from the top group.",
        "To our knowledge, little research has explicitly addressed the problem of NP-query performance prediction.",
        "Current prediction models devised for content-based queries will be less effective for NP queries considering the fundamental differences between the two.",
        "Third, in real-world web search environments, user queries are usually a mixture of different types and prior knowledge about the type of each query is generally unavailable.",
        "The mixed-query situation raises new problems for query performance prediction.",
        "For instance, we may need to incorporate a query classifier into prediction models.",
        "Despite these problems, the ability to handle this situation is a crucial step towards turning query performance prediction from an interesting research topic into a practical tool for web retrieval.",
        "In this paper, we present three techniques to address the above challenges that current prediction models face in Web search environments.",
        "Our work focuses on query performance prediction for the content-based (ad-hoc) retrieval task and the name-page finding task in the context of web retrieval.",
        "Our first technique, called weighted information gain (WIG), makes use of both single term and term proximity features to estimate the quality of top retrieved documents for prediction.",
        "We find that WIG offers consistent prediction accuracy across various test collections and query types.",
        "Moreover, we demonstrate that good prediction accuracy can be achieved for the mixed-query situation by using WIG with the help of a query type classifier.",
        "Query feedback and first rank change, which are our second and third prediction techniques, perform well for content-based queries and NP queries respectively.",
        "Our main contributions include: (1) considerably improved prediction accuracy for web content-based queries over several state-of-the-art techniques. (2) new techniques for successfully predicting NP-query performance. (3) a practical and fully automatic solution to predicting mixed-query performance.",
        "In addition, one minor contribution is that we find that the robustness score [1], which was originally proposed for performance prediction, is helpful for query classification. 2.",
        "RELATED WORK As we mentioned in the introduction, a number of prediction techniques have been proposed recently that focus on contentbased queries in the topical relevance (ad-hoc) task.",
        "We know of no published work that addresses other types of queries such as NP queries, let alone a mixture of query types.",
        "Next we review some representative models.",
        "The major difficulty of performance prediction comes from the fact that many factors have an impact on retrieval performance.",
        "Each factor affects performance to a different degree and the overall effect is hard to predict accurately.",
        "Therefore, it is not surprising to notice that simple features, such as the frequency of query terms in the collection [4] and the average IDF of query terms [5], do not predict well.",
        "In fact, most of the successful techniques are based on measuring some characteristics of the retrieved document set to estimate topic difficulty.",
        "For example, the clarity score [6] measures the coherence of a list of documents by the KL-divergence between the query model and the collection model.",
        "The robustness score [1] quantifies another property of a ranked list: the robustness of the ranking in the presence of uncertainty.",
        "Carmel et al. [2] found that the distance measured by the Jensen-Shannon divergence between the retrieved document set and the collection is significantly correlated to average precision.",
        "Vinay et al. [7] proposed four measures to capture the geometry of the top retrieved documents for prediction.",
        "The most effective measure is the sensitivity to document perturbation, an idea somewhat similar to the robustness score.",
        "Unfortunately, their way of measuring the sensitivity does not perform equally well for short queries and prediction accuracy drops considerably when a state-of-the-art retrieval technique (like Okapi or a language modeling approach) is adopted for retrieval instead of the tf-idf weighting used in their paper [16].",
        "The difficulties of applying these models in web search environments have already been mentioned.",
        "In this paper, we mainly adopt the clarity score and the robustness score as our baselines.",
        "We experimentally show that the baselines, even after being carefully tuned, are inadequate for the web environment.",
        "One of our prediction models, WIG, is related to the Markov random field (MRF) model for information retrieval [8].",
        "The MRF model directly models term dependence and is found be to highly effective across a variety of test collections (particularly web collections) and retrieval tasks.",
        "This model is used to estimate the joint probability distribution over documents and queries, an important part of WIG.",
        "The superiority of WIG over other prediction techniques based on unigram features, which will be demonstrated later in our paper, coincides with that of MRF for retrieval.",
        "In other word, it is interesting to note that term dependence, when being modeled appropriately, can be helpful for both improving and predicting retrieval performance. 3.",
        "PREDICTION MODELS 3.1 Weighted Information Gain (WIG) This section introduces a weighted information gain approach that incorporates both single term and proximity features for predicting performance for both content-based and Named-Page (NP) finding queries.",
        "Given a set of queries Q={Qs} (s=1,2,..N) which includes all possible user queries and a set of documents D={Dt} (t=1,2…M), we assume that each query-document pair (Qs,Dt) is manually judged and will be put in a relevance list if Qs is found to be relevant to Dt.",
        "The joint probability P(Qs,Dt) over queries Q and documents D denotes the probability that pair (Qs,Dt) will be in the relevance list.",
        "Such assumptions are similar to those used in [8].",
        "Assuming that the user issues query Qi ∈Q and the retrieval results in response to Qi is a ranked list L of documents, we calculate the amount of information contained in P(Qs,Dt) with respect to Qi and L by Eq.1 which is a variant of entropy called the weighted entropy[13].",
        "The weights in Eq.1 are solely determined by Qi and L. )1(),(log),(),( , , ∑−= ts tststsLQ DQPDQweightDQH i In this paper, we choose the weights as follows: LindocumentsKtopthecontainsLTwhere otherwise LTDandisifK DQweight K Kt ts )( )2( ,0 )(,/1 ),( ⎩ ⎨ ⎧ ∈= = The cutoff rank K is a parameter in our model that will be discussed later.",
        "Accordingly, Eq.1 can be simplified as follows: )3(),(log 1 ),( )( , ∑∈ −= LTD titsLQ Kt i DQP K DQH Unfortunately, weighted entropy ),(, tsLQ DQH i computed by Eq.3, which represents the amount of information about how likely the top ranked documents in L would be relevant to query Qi on average, cannot be compared across different queries, making it inappropriate for directly predicting query performance.",
        "To mitigate this problem, we come up with a background distribution P(Qs,C) over Q and D by imagining that every document in D is replaced by the same special document C which represents average language usage.",
        "In this paper, C is created by concatenating every document in D. Roughly speaking, C is the collection (the document set) {Dt} without document boundaries.",
        "Similarly, weighted entropy ),(, CQH sLQi calculated by Eq.3 represents the amount of information about how likely an average document (represented by the whole collection) would be relevant to query Qi.",
        "Now we introduce our performance predictor WIG which is the weighted information gain [13] computed as the difference between ),(, tsLQ DQH i and ),(, CQH sLQi .Specifically, given query Qi, collection C and ranked list L of documents, WIG is calculated as follows: )4( ),( ),( log 1 ),( ),( log),( ),(),(),,( )(, ,, ∑∑ ∈ == −= LTD i ti ts s ts ts tsLQsLQi Kt ii CQP DQP KCQP DQP DQweight DQHCQHLCQWIG WIG computed by Eq.4 measures the change in information about the quality of retrieval (in response to query Qi) from an imaginary state that only an average document is retrieved to a posterior state that the actual search results are observed.",
        "We hypothesize that WIG is positively correlated with retrieval effectiveness because high quality retrieval should be much more effective than just returning the average document.",
        "The heart of this technique is how to estimate the joint distribution P(Qs,Dt).",
        "In the language modeling approach to IR, a variety of models can be applied readily to estimate this distribution.",
        "Although most of these models are based on the bagof-words assumption, recent work on modeling term dependence under the language modeling framework have shown consistent and significant improvements in retrieval effectiveness over bagof-words models.",
        "Inspired by the success of incorporating term proximity features into language models, we decide to adopt a good dependence model to estimate the probability P(Qs,Dt).",
        "The model we chose for this paper is Metzler and Crofts Markov Random Field (MRF) model, which has already demonstrated superiority over a number of collections and different retrieval tasks [8,9].",
        "According to the MRF model, log P(Qi, Dt) can be written as )5()|(loglog),(log )( 1 ∑∈ +−= iQF tti DPZDQP ξ ξ ξλ where Z1 is a constant that ensures that P(Qi, Dt) sums up to 1.",
        "F(Qi) consists of a set of features expanded from the original query Qi .",
        "For example, assuming that query Qi is talented student program, F(Qi) includes features like program and talented student.",
        "We consider two kinds of features: single term features T and proximity features P. Proximity features include exact phrase (#1) and unordered window (#uwN) features as described in [8].",
        "Note that F(Qi) is the union of T(Qi) and P(Qi).",
        "For more details on F(Qi) such as how to expand the original query Qi to F(Qi), we refer the reader to [8] and [9].",
        "P(ξ|Dt) denotes the probability that feature ξ will occur in Dt.",
        "More details on P(ξ|Dt) will be provided later in this section.",
        "The choice of λξ is somewhat different from that used in [8] since λξ plays a dual role in our model.",
        "The first role, which is the same as in [8], is to weight between single term and proximity features.",
        "The other role, which is specific to our prediction task, is to normalize the size of F(Qi).We found that the following weight strategy for λξ satisfies the above two roles and generalizes well on a variety of collections and query types. )6( )(, |)(| 1 )(, |)(| ⎪ ⎪ ⎩ ⎪ ⎪ ⎨ ⎧ ∈ − ∈ = i i T i i T QP QP QT QT ξ λ ξ λ λξ where |T(Qi)| and |P(Qi)| denote the number of single term and proximity features in F(Qi) respectively.",
        "The reason for choosing the square root function in the denominator of λξ is to penalize a feature set of large size appropriately, making WIG more comparable across queries of various lengths. λT is a fixed parameter and set to 0.8 according to [8] throughout this paper.",
        "Similarly, log P(Qi,C) can be written as: )7()|(loglog),(log )( 2 ∑∈ +−= iQF i CPZCQP ξ ξ ξλ When constant Z1 and Z2 are dropped, WIG computed in Eq.4 can be rewritten as follows by plugging in Eq.5 and Eq.7 : )8( )|( )|( log 1 ),,( )( )( ∑ ∑∈ ∈ = LTD QF t i Kt i CP DP K LCQWIG ξ ξ ξ ξ λ One of the advantages of WIG over other techniques is that it can handle well both content-based and NP queries.",
        "Based on the type (or the predicted type) of Qi, the calculation of WIG in Eq. 8 differs in two aspects: (1) how to estimate P(ξ|Dt) and P(ξ|C), and (2) how to choose K. For content-based queries, P(ξ|C) is estimated by the relative frequency of feature ξ in collection C as a whole.",
        "The estimation of P(ξ|Dt) is the same as in [8].",
        "Namely, we estimate P(ξ|Dt) by the relative frequency of feature ξ in Dt linearly smoothed with collection frequency P(ξ|C).",
        "K in Eq.8 is treated as a free parameter.",
        "Note that K is the only free parameter in the computation of WIG for content-based queries because all parameters involved in P(ξ|Dt) are assumed to be fixed by taking the suggested values in [8].",
        "Regarding NP queries, we make use of document structure to estimate P(ξ|Dt) and P(ξ|C) by the so-called mixture of language models proposed in [10] and incorporated into the MRF model for Named-Page finding retrieval in [9].",
        "The basic idea is that a document (collection) is divided into several fields such as the title field, the main-body field and the heading field.",
        "P(ξ|Dt) and P(ξ|C) are estimated by a linear combination of the language models from each field.",
        "Due to space constraints, we refer the reader to [9] for details.",
        "We adopt the exact same set of parameters as used in [9] for estimation.",
        "With regard to K in Eq.8, we set K to 1 because the Named-Page finding task heavily focuses on the first ranked document.",
        "Consequently, there are no free parameters in the computation of WIG for NP queries. 3.2 Query Feedback In this section, we introduce another technique called query feedback (QF) for prediction.",
        "Suppose that a user issues query Q to a retrieval system and a ranked list L of documents is returned.",
        "We view the retrieval system as a noisy channel.",
        "Specifically, we assume that the output of the channel is L and the input is Q.",
        "After going through the channel, Q becomes corrupted and is transformed to ranked list L. By thinking about the retrieval process this way, the problem of predicting retrieval effectiveness turns to the task of evaluating the quality of the channel.",
        "In other words, prediction becomes finding a way to measure the degree of corruption that arises when Q is transformed to L. As directly computing the degree of the corruption is difficult, we tackle this problem by approximation.",
        "Our main idea is that we measure to what extent information on Q can be recovered from L on the assumption that only L is observed.",
        "Specifically, we design a decoder that can accurately translate L back into new query Q and the similarity S between the original query Q and the new query Q is adopted as a performance predictor.",
        "This is a sketch of how the QF technique predicts query performance.",
        "Before filling in more details, we briefly discuss why this method would work.",
        "There is a relation between the similarity S defined above and retrieval performance.",
        "On the one hand, if the retrieval has strayed from the original sense of the query Q, the new query Q extracted from ranked list L in response to Q would be very different from the original query Q.",
        "On the other hand, a query distilled from a ranked list containing many relevant documents is likely to be similar to the original query.",
        "Further examples in support of the relation will be provided later.",
        "Next we detail how to build the decoder and how to measure the similarity S. In essence, the goal of the decoder is to compress ranked list L into a few informative terms that should represent the content of the top ranked documents in L. Our approach to this goal is to represent ranked list L by a language model (distribution over terms).",
        "Then terms are ranked by their contribution to the language models KL (Kullback-Leibler) divergence from the background collection model.",
        "Top ranked terms will be chosen to form the new query Q.",
        "This approach is similar to that used in Section 4.1 of [11].",
        "Specifically, we take three steps to compress ranked list L into query Q without referring to the original query. 1.",
        "We adopt the ranked list language model [14], to estimate a language model based on ranked list L. The model can be written as: )9()|()|()|( ∑∈ = LD LDPDwPLwP where w is any term, D is a document.",
        "P(D|L) is estimated by a linearly decreasing function of the rank of document D. 2.",
        "Each term in P(w|L) is ranked by the following KL-divergence contribution: )10( )|( )|( log)|( CwP LwP LwP where P(w|C) is the collection model estimated by the relative frequency of term w in collection C as a whole. 3.",
        "The top N ranked terms by Eq.10 form a weighted query Q={(wi,ti)} i=1,N. where wi denotes the i-th ranked term and weight ti is the KL-divergence contribution of wi in Eq. 10.",
        "Term cruise ship vessel sea passenger KL contribution 0.050 0.040 0.012 0.010 0.009 Table 1: top 5 terms compressed from the ranked list in response to query Cruise ship damage sea life Two representative examples, one for a poorly performing query Cruise ship damage sea life (TREC topic 719; average precision: 0.08) and the other for a high performing query prostate cancer treatments( TREC topic 710; average precision: 0.49), are shown in Table 1 and 2 respectively.",
        "These examples indicate how the similarity between the original and the new query correlates with retrieval performance.",
        "The parameter N in step 3 is set to 20 empirically and choosing a larger value of N is unnecessary since the weights after the top 20 are usually too small to make any difference.",
        "Term prostate cancer treatment men therapy KL contribution 0.177 0.140 0.028 0.025 0.020 Table 2: top 5 terms compressed from the ranked list in response to query prostate cancer treatments To measure the similarity between original query Q and new query Q, we first use Q to do retrieval on the same collection.",
        "A variant of the query likelihood model [15] is adopted for retrieval.",
        "Namely, documents are ranked by: )11()|()|( ),( ∑∈ = Qtw t i ii i DwPDQP where wi is a term in Q and ti is the associated weight.",
        "D is a document.",
        "Let L denote the new ranked list returned from the above retrieval.",
        "The similarity is measured by the overlap of documents in L and L. Specifically, the percentage of documents in the top K documents of L that are also present in the top K documents in L. the cutoff K is treated as a free parameter.",
        "We summarize here how the QF technique predicts performance given a query Q and the associated ranked list L. We first obtain a weighted query Q compressed from L by the above three steps.",
        "Then we use Q to perform retrieval and the new ranked list is L. The overlap of documents in L and L is used for prediction. 3.3 First Rank Change (FRC) In this section, we propose a method called the first rank change (FRC) for performance prediction for NP queries.",
        "This method is derived from the ranking robustness technique [1] that is mainly designed for content-based queries.",
        "When directly applied to NP queries, the robustness technique will be less effective because it takes the top ranked documents as a whole into account while NP queries usually have only one single relevant document.",
        "Instead, our technique focuses on the first rank document while the main idea of the robustness method remains.",
        "Specifically, the pseudocode for computing FRC is shown in figure 1.",
        "Input: (1) ranked list L={Di} where i=1,100.",
        "Di denotes the i-th ranked document. (2) query Q 1 initialize: (1) set the number of trials J=100000 (2) counter c=0; 2 for i=1 to J 3 Perturb every document in L, let the outcome be a set F={Di} where Di denotes the perturbed version of Di. 4 Do retrieval with query Q on set F 5 c=c+1 if and only if D1 is ranked first in step 4 6 end of for 7 return the ratio c/J Figure 1: pseudo-code for computing FRC FRC approximates the probability that the first ranked document in the original list L will remain ranked first even after the documents are perturbed.",
        "The higher the probability is, the more confidence we have in the first ranked document.",
        "On the other hand, in the extreme case of a random ranking, the probability would be as low as 0.5.",
        "We expect that FRC has a positive association with NP query performance.",
        "We adopt [1] to implement the document perturbation step (step 4 in Fig.1) using Poisson distributions.",
        "For more details, we refer the reader to [1]. 4.",
        "EVALUATION We now present the results of predicting query performance by our models.",
        "Three state-of-the-art techniques are adopted as our baselines.",
        "We evaluate our techniques across a variety of Web retrieval settings.",
        "As mentioned before, we consider two types of queries, that is, content-based (CB) queries and Named-Page(NP) finding queries.",
        "First, suppose that the query types are known.",
        "We investigate the correlation between the predicted retrieval performance and the actual performance for both types of queries separately.",
        "Results show that our methods yield considerable improvements over the baselines.",
        "We then consider a more challenging scenario where no prior information on query types is available.",
        "Two sub-cases are considered.",
        "In the first one, there exists only one type of query but the actual type is unknown.",
        "We assume a mixture of the two query types in the second case.",
        "We demonstrate that our models achieve good accuracy under this demanding scenario, making prediction practical in a real-world Web search environment. 4.1 Experimental Setup Our evaluation focuses on the GOV2 collection which contains about 25 million documents crawled from web sites in the .gov domain during 2004 [3].",
        "We create two kinds of data set for CB queries and NP queries respectively.",
        "For the CB type, we use the ad-hoc topics of the Terabyte Tracks of 2004, 2005 and 2006 and name them TB04-adhoc, TB05-adhoc and TB06-adhoc respectively.",
        "In addition, we also use the ad-hoc topics of the 2004 Robust Track (RT04) to test the adaptability of our techniques to a non-Web environment.",
        "For NP queries, we use the Named-Page finding topics of the Terabyte Tracks of 2005 and 2006 and we name them TB05-NP and TB06-NP respectively.",
        "All queries used in our experiments are titles of TREC topics as we center on web retrieval.",
        "Table 3 summarizes the above data sets.",
        "Name Collection Topic Number Query Type TB04-adhoc GOV2 701-750 CB TB05-adhoc GOV2 751-800 CB TB06-adhoc GOV2 801-850 CB RT04 Disk 4+5 (minus CR) 301-450;601700 CB TB05-NP GOV2 NP601-NP872 NP TB06-NP GOV2 NP901-NP1081 NP Table 3: Summary of test collections and topics Retrieval performance of individual content-based and NP queries is measured by the average precision and reciprocal rank of the first correct answer respectively.",
        "We make use of the Markov Random field model for both ad-hoc and Named-Page finding retrieval.",
        "We adopt the same setting of retrieval parameters used in [8,9].",
        "The Indri search engine [12] is used for all of our experiments.",
        "Though not reported here, we also tried the query likelihood model for ad-hoc retrieval and found that the results change little because of the very high correlation between the query performances obtained by the two retrieval models (0.96 measured by Pearsons coefficient). 4.2 Known Query Types Suppose that query types are known.",
        "We treat each type of query separately and measure the correlation with average precision (or the reciprocal rank in the case of NP queries).",
        "We adopt the Pearsons correlation test which reflects the degree of linear relationship between the predicted and the actual retrieval performance. 4.2.1 Content-based Queries Methods Clarity Robust JSD WIG QF WIG +QF TB04+0 5 adhoc 0.333 0.317 0.362 0.574 0.480 0.637 TB06 adhoc 0.076 0.294 N/A 0.464 0.422 0.511 Table 4: Pearsons correlation coefficients for correlation with average precision on the Terabyte Tracks (ad-hoc) for clarity score, robustness score, the JSD-based method(we directly cites the score reported in [2]), WIG, query feedback(QF) and a linear combination of WIG and QF.",
        "Bold cases mean the results are statistically significant at the 0.01 level.",
        "Table 4 shows the correlation with average precision on two data sets: one is a combination of TB04-adhoc and TB05-adhoc(100 topics in total) and the other is TB06-adhoc (50 topics).",
        "The reason that we put TB04-adhoc and TB05-adhoc together is to make our results comparable to [2].",
        "Our baselines are the clarity score (clarity) [6],the robustness score (robust)[1] and the JSDbased method (JSD) [2].",
        "For the clarity and robustness score, we have tried different parameter settings and report the highest correlation coefficients we have found.",
        "We directly cite the result of the JSD-based method reported in [2].",
        "The table also shows the results for the Weighted Information Gain (WIG) method and the Query Feedback (QF) method for predicting content-based queries.",
        "As we described in the previous section, both WIG and QF have one free parameter to set, that is, the cutoff rank K. We train the parameter on one dataset and test on the other.",
        "When combining WIG and QF, a simple linear combination is used and the combination weight is learned from the training data set.",
        "From these results, we can see that our methods are considerably more accurate compared to the baselines.",
        "We also observe that further improvements are obtained from the combination of WIG and QF, suggesting that they measure different properties of the retrieval process that relate to performance.",
        "We discover that our methods generalize well on TB06-adhoc while the correlation for the clarity score with retrieval performance on this data set is considerably worse.",
        "Further investigation shows that the mean average precision of TB06-adhoc is 0.342 and is about 10% better than that of the first data set.",
        "While the other three methods typically consider the top 100 or less documents given a ranked list, the clarity method usually needs the top 500 or more documents to adequately measure the coherence of a ranked list.",
        "Higher mean average precision makes ranked lists retrieved by different queries more similar in terms of coherence at the level of top 500 documents.",
        "We believe that this is the main reason for the low accuracy of the clarity score on the second data set.",
        "Though this paper focuses on a Web search environment, it is desirable that our techniques will work consistently well in other situations.",
        "To this end, we examine the effectiveness of our techniques on the Robust 2004 Track.",
        "For our methods, we evenly divide all of the test queries into five groups and perform five-fold cross validation.",
        "Each time we use one group for training and the remaining four groups for testing.",
        "We make use of all of the queries for our two baselines, that is, the clarity score and the robustness score.",
        "The parameters for our baselines are the same as those used in [1].The results shown in Table 5 demonstrate that the prediction accuracy of our methods is on a par with that of the two strong baselines.",
        "Clarity Robust WIG QF 0.464 0.539 0.468 0.464 Table 5: Comparison of Pearsons correlation coefficients on the 2004 Robust Track for clarity score, robustness score, WIG and query feedback (QF).",
        "Bold cases mean the results are statistically significant at the 0.01 level.",
        "Furthermore, we examine the prediction sensitivity of our methods to the cutoff rank K. With respect to WIG, it is quite robust to K on the Terabyte Tracks (2004-2006) while it prefers a small value of K like 5 on the 2004 Robust Track.",
        "In other words, a small value of K is a nearly-optimal choice for both kinds of tracks.",
        "Considering the fact that all other parameters involved in WIG are fixed and consequently the same for the two cases, this means WIG can achieve nearly-optimal prediction accuracy in two considerably different situations with exactly the same parameter setting.",
        "Regarding QF, it prefers a larger value of K such as 100 on the Terabyte Tracks and a smaller value of K such as 25 on the 2004 Robust Track. 4.2.2 NP Queries We adopt WIG and first rank change (FRC) for predicting NPquery performance.",
        "We also try a linear combination of the two as in the previous section.",
        "The combination weight is obtained from the other data set.",
        "We use the correlation with the reciprocal ranks measured by the Pearsons correlation test to evaluate prediction quality.",
        "The results are presented in Table 6.",
        "Again, our baselines are the clarity score and the robustness score.",
        "To make a fair comparison, we tune the clarity score in different ways.",
        "We found that using the first ranked document to build the query model yields the best prediction accuracy.",
        "We also attempted to utilize document structure by using the mixture of language models mentioned in section 3.1.",
        "Little improvement was obtained.",
        "The correlation coefficients for the clarity score reported in Table 6 are the best we have found.",
        "As we can see, our methods considerably outperform the clarity score technique on both of the runs.",
        "This confirms our intuition that the use of a coherence-based measure like the clarity score is inappropriate for NP queries.",
        "Methods Clarity Robust.",
        "WIG FRC WIG+FRC TB05-NP 0.150 -0.370 0.458 0.440 0.525 TB06-NP 0.112 -0.160 0.478 0.386 0.515 Table 6: Pearsons correlation coefficients for correlation with reciprocal ranks on the Terabyte Tracks (NP) for clarity score, robustness score, WIG, the first rank change (FRC) and a linear combination of WIG and FRC.",
        "Bold cases mean the results are statistically significant at the 0.01 level.",
        "Regarding the robustness score, we also tune the parameters and report the best we have found.",
        "We observe an interesting and surprising negative correlation with reciprocal ranks.",
        "We explain this finding briefly.",
        "A high robustness score means that a number of top ranked documents in the original ranked list are still highly ranked after perturbing the documents.",
        "The existence of such documents is a good sign of high performance for content-based queries as these queries usually contain a number of relevant documents [1].",
        "However, with regard to NP queries, one fundamental difference is that there is only one relevant document for each query.",
        "The existence of such documents can confuse the ranking function and lead to low retrieval performance.",
        "Although the negative correlation with retrieval performance exists, the strength of the correlation is weaker and less consistent compared to our methods as shown in Table 6.",
        "Based on the above analysis, we can see that current prediction techniques like clarity score and robustness score that are mainly designed for content-based queries face significant challenges and are inadequate to deal with NP queries.",
        "Our two techniques proposed for NP queries consistently demonstrate good prediction accuracy, displaying initial success in solving the problem of predicting performance for NP queries.",
        "Another point we want to stress is that the WIG method works well for both types of queries, a desirable property that most prediction techniques lack. 4.3 Unknown Query Types In this section, we run two kinds of experiments without access to query type labels.",
        "First, we assume that only one type of query exists but the type is unknown.",
        "Second, we experiment on a mixture of content-based and NP queries.",
        "The following two subsections will report results for the two conditions respectively. 4.3.1 Only One Type exists We assume that all queries are of the same type, that is, they are either NP queries or content-based queries.",
        "We choose WIG to deal with this case because it shows good prediction accuracy for both types of queries in the previous section.",
        "We consider two cases: (1) CB: all 150 title queries from the ad-hoc task of the Terabyte Tracks 2004-2006 (2)NP: all 433 NP queries from the named page finding task of the Terabyte Tracks 2005 and 2006.",
        "We take a simple strategy by labeling all of the queries in each case as the same type (either NP or CB) regardless of their actual type.",
        "The computation of WIG will be based on the labeled query type instead of the actual type.",
        "There are four possibilities with respect to the relation between the actual type and the labeled type.",
        "The correlation with retrieval performance under the four possibilities is presented in Table 7.",
        "For example, the value 0.445 at the intersection between the second row and the third column shows the Pearsons correlation coefficient for correlation with average precision when the content-based queries are incorrectly labeled as the NP type.",
        "Based on these results, we recommend treating all queries as the NP type when only one query type exists and accurate query classification is not feasible, considering the risk that a large loss of accuracy will occur if NP queries are incorrectly labeled as content-based queries.",
        "These results also demonstrate the strong adaptability of WIG to different query types.",
        "CB (labeled) NP (labeled) CB (actual) 0.536 0.445 NP (actual) 0.174 0.467 Table 7: Comparison of Pearsons correlation coefficients for correlation with retrieval performance under four possibilities on the Terabyte Tracks (NP).",
        "Bold cases mean the results are statistically significant at the 0.01 level. 4.3.2 A mixture of contented-based and NP queries A mixture of the two types of queries is a more realistic situation that a Web search engine will meet.",
        "We evaluate prediction accuracy by how accurately poorly-performing queries can be identified by the prediction method assuming that actual query types are unknown (but we can predict query types).",
        "This is a challenging task because both the predicted and actual performance for one type of query can be incomparable to that for the other type.",
        "Next we discuss how to implement our evaluation.",
        "We create a query pool which consists of all of the 150 ad-hoc title queries from Terabyte Track 2004-2006 and all of the 433 NP queries from Terabyte Track 2005&2006.",
        "We divide the queries in the pool into classes: good (better than 50% of the queries of the same type in terms of retrieval performance) and bad (otherwise).",
        "According to these standards, a NP query with the reciprocal rank above 0.2 or a content-based query with the average precision above 0.315 will be considered as good.",
        "Then, each time we randomly select one query Q from the pool with probability p that Q is contented-based.",
        "The remaining queries are used as training data.",
        "We first decide the type of query Q according to a query classifier.",
        "Namely, the query classifier tells us whether query Q is NP or content-based.",
        "Based on the predicted query type and the score computed for query Q by a prediction technique, a binary decision is made about whether query Q is good or bad by comparing to the score threshold of the predicted query type obtained from the training data.",
        "Prediction accuracy is measured by the accuracy of the binary decision.",
        "In our implementation, we repeatedly take a test query from the query pool and prediction accuracy is computed as the percentage of correct decisions, that is, a good(bad) query is predicted to be good (bad).",
        "It is obvious that random guessing will lead to 50% accuracy.",
        "Let us take the WIG method for example to illustrate the process.",
        "Two WIG thresholds (one for NP queries and the other for content-based queries) are trained by maximizing the prediction accuracy on the training data.",
        "When a test query is labeled as the NP (CB) type by the query type classifier, it will be predicted to be good if and only if the WIG score for this query is above the NP (CB) threshold.",
        "Similar procedures will be taken for other prediction techniques.",
        "Now we briefly introduce the automatic query type classifier used in this paper.",
        "We find that the robustness score, though originally proposed for performance prediction, is a good indicator of query types.",
        "We find that on average content-based queries have a much higher robustness score than NP queries.",
        "For example, Figure 2 shows the distributions of robustness scores for NP and content-based queries.",
        "According to this finding, the robustness score classifier will attach a NP (CB) label to the query if the robustness score for the query is below (above) a threshold trained from the training data. 0 0.5 1 1.5 2 2.5 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 NP Content-based Figure 2: Distribution of robustness scores for NP and CB queries.",
        "The NP queries are the 252 NP topics from the 2005 Terabyte Track.",
        "The content-based queries are the 150 ad-hoc title from the Terabyte Tracks 2004-2006.",
        "The probability distributions are estimated by the Kernel density estimation method.",
        "Strategies Robust WIG-1 WIG-2 WIG-3 Optimal p=0.6 0.565 0.624 0.665 0.684 0.701 P=0.4 0.567 0.633 0.654 0.673 0.696 Table 8: Comparison of prediction accuracy for five strategies in the mixed-query situation.",
        "Two ways to sample a query from the pool: (1) the sampled query is content-based with the probability p=0.6. (that is, the query is NP with probability 0.4 ) (2) set the probability p=0.4.",
        "We consider five strategies in our experiments.",
        "In the first strategy (denoted by robust), we use the robustness score for query performance prediction with the help of a perfect query classifier that always correctly map a query into one of the two categories (that is, NP or CB).",
        "This strategy represents the level of prediction accuracy that current prediction techniques can achieve in an ideal condition that query types are known.",
        "In the next following three strategies, the WIG method is adopted for performance prediction.",
        "The difference among the three is that three different query classifiers are used for each strategy: (1) the classifier always classifies a query into the NP type. (2) the Robustness Score ProbabilityDensity classifier is the robust score classifier mentioned above. (3) the classifier is a perfect one.",
        "These three strategies are denoted by WIG-1, WIG-2 and WIG-3 respectively.",
        "The reason we are interested in WIG-1 is based on the results from section 4.3.1.",
        "In the last strategy (denoted by Optimal) which serves as an upper bound on how well we can do so far, we fully make use of our prediction techniques for each query type assuming a perfect query classifier is available.",
        "Specifically, we linearly combine WIG and QF for content-based queries and WIG and FRC for NP queries.",
        "The results for the five strategies are shown in Table 8.",
        "For each strategy, we try two ways to sample a query from the pool: (1) the sampled query is CB with probability p=0.6. (the query is NP with probability 0.4) (2) set the probability p=0.4.",
        "From Table 8 We can see that in terms of prediction accuracy WIG-2 (the WIG method with the automatic query classifier) is not only better than the first two cases, but also is close to WIG-3 where a perfect classifier is assumed.",
        "Some further improvements over WIG-3 are observed when combined with other prediction techniques.",
        "The merit of WIG-2 is that it provides a practical solution to automatically identifying poorly performing queries in a Web search environment with mixed query types, which poses considerable obstacles to traditional prediction techniques. 5.",
        "CONCLUSIONS AND FUTURE WORK To our knowledge, our paper is the first to thoroughly explore prediction of query performance in web search environments.",
        "We demonstrated that our models resulted in higher prediction accuracy than previously published techniques not specially devised for web search scenarios.",
        "In this paper, we focus on two types of queries in web search: content-based and Named-Page (NP) finding queries, corresponding to the ad-hoc retrieval task and the Named-Page finding task respectively.",
        "For both types of web queries, our prediction models were shown to be substantially more accurate than the current state-of-the-art techniques.",
        "Furthermore, we considered a more realistic case that no prior information on query types is available.",
        "We demonstrated that the WIG method is particularly suitable for this situation.",
        "Considering the adaptability of WIG to a range of collections and query types, one of our future plans is to apply this method to predict user preference of search results on realistic data collected from a commercial search engine.",
        "Other than accuracy, another major issue that prediction techniques have to deal with in a Web environment is efficiency.",
        "Fortunately, since the WIG score is computed just over the terms and the phrases that appear in the query, this calculation can be made very efficient with the support of index.",
        "On the other hand, the computation of QF and FRC is relatively less efficient since QF needs to retrieve the whole collection twice and FRC needs to repeatedly rank the perturbed documents.",
        "How to improve the efficiency of QF and FRC is our future work.",
        "In addition, the prediction techniques proposed in this paper have the potential of improving retrieval performance by combining with other IR techniques.",
        "For example, our techniques can be incorporated to popular query modification techniques such as query expansion and query relaxation.",
        "Guided by performance prediction, we can make a better decision on when to or how to modify queries to enhance retrieval effectiveness.",
        "We would like to carry out research in this direction in the future. 6.",
        "ACKNOWLEGMENTS This work was supported in part by the Center for Intelligent Information Retrieval, in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C0023, and in part by an award from Google.",
        "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor.",
        "In addition, we thank Donald Metzler for his valuable comments on this work. 7.",
        "REFERENCES [1] Y. Zhou ,W. B. Croft ,Ranking Robustness: A Novel Framework to Predict Query Performance, in Proceedings of CIKM 2006. [2] D.Carmel, E.Yom-Tov, A.Darlow,D.Pelleg, What Makes a Query Difficult?, in Proceedings of SIGIR 2006. [3] C.L.A.",
        "Clarke, F. Scholer, I.Soboroff, The TREC 2005 Terabyte Track, In the Online Proceedings of 2005 TREC. [4] B.",
        "He and I.Ounis.",
        "Inferring query performance using preretrieval predictors.",
        "In proceedings of the SPIRE 2004. [5] S. Tomlinson.",
        "Robust, Web and Terabyte Retrieval with Hummingbird SearchServer at TREC 2004.",
        "In the Online Proceedings of 2004 TREC. [6] S. Cronen-Townsend, Y. Zhou, W. B. Croft, Predicting Query Performance, in Proceedings of SIGIR 2002. [7] V.Vinay, I.J.Cox, N.Mill-Frayling,K.Wood, On Ranking the Effectiveness of Searcher, in Proceedings of SIGIR 2006. [8] D.Metzler, W.B.Croft, A Markov Random Filed Model for Term Dependencies, in Proceedings of SIGIR 2005. [9] D.Metzler, T.Strohman,Y.Zhou,W.B.Croft, Indri at TREC 2005: Terabyte Track, In the Online Proceedings of 2004 TREC. [10] P. Ogilvie and J. Callan, Combining document representations for known-item search, in Proceedings of SIGIR 2003. [11] A.Berger, J.Lafferty, Information retrieval as statistical translation, in Proceedings of SIGIR 1999. [12] Indri search engine : http://www.lemurproject.org/indri/ [13] I.J.",
        "Taneja: On Generalized Information Measures and Their Applications, Advances in Electronics and Electron Physics, Academic Press (USA), 76, 1989, 327-413. [14] S.Cronen-Townsend, Y. Zhou and Croft, W. B. , A Framework for Selective Query Expansion, in Proceedings of CIKM 2004. [15] F.Song, W.B.Croft, A general language model for information retrieval, in Proceedings of SIGIR 1999. [16] Personal email contact with Vishwa Vinay and our own experiments [17] E.Yom-Tov, S.Fine, D.Carmel, A.Darlow, Learning to Estimate Query Difficulty Including Applications to Missing Content Detection and Distributed Information retrieval, in Proceedings of SIGIR 2005"
    ],
    "translated_text_sentences": [
        "Predicción del rendimiento de consultas en entornos de búsqueda web Yun Zhou y W. Bruce Croft Departamento de Ciencias de la Computación Universidad de Massachusetts, Amherst {yzhou, croft}@cs.umass.edu RESUMEN Las técnicas actuales de predicción, generalmente diseñadas para consultas basadas en contenido y evaluadas típicamente en colecciones de prueba relativamente homogéneas de tamaños pequeños, enfrentan serios desafíos en entornos de búsqueda web donde las colecciones son significativamente más heterogéneas y existen diferentes tipos de tareas de recuperación.",
        "En este artículo, presentamos tres técnicas para abordar estos desafíos.",
        "Nos enfocamos en la predicción de rendimiento para dos tipos de consultas en entornos de búsqueda web: basadas en contenido y búsqueda de páginas por nombre.",
        "Nuestra evaluación se realiza principalmente en la colección GOV2.",
        "Además de evaluar nuestros modelos para los dos tipos de consultas por separado, consideramos una situación más desafiante y realista en la que los dos tipos de consultas se mezclan sin información previa sobre los tipos de consulta.",
        "Para ayudar en la predicción bajo la situación de consulta mixta, se adopta un clasificador de consultas novedoso.",
        "Los resultados muestran que nuestra predicción del rendimiento de la consulta web es sustancialmente más precisa que las técnicas de predicción actuales más avanzadas.",
        "Por consiguiente, nuestro artículo proporciona un enfoque práctico para predecir el rendimiento en entornos web del mundo real.",
        "Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Formulación de Consultas Términos Generales Algoritmos, Experimentación, Teoría 1.",
        "La predicción del rendimiento de la consulta tiene muchas aplicaciones en una variedad de áreas de recuperación de información (IR), como mejorar la consistencia de la recuperación, el refinamiento de la consulta y la IR distribuida.",
        "La importancia de este problema ha sido reconocida por los investigadores en Recuperación de Información y recientemente se han propuesto varios métodos nuevos para la predicción [1, 2, 17].",
        "La mayoría de los trabajos sobre predicción se han centrado en la tarea tradicional de recuperación ad-hoc, donde el rendimiento de la consulta se mide según la relevancia temática.",
        "Estos modelos de predicción se evalúan en colecciones de documentos de TREC que suelen consistir en no más de un millón de artículos de agencias de noticias relativamente homogéneos.",
        "Con la popularidad e influencia de la Web, las técnicas de predicción que funcionen bien para consultas de estilo web son altamente preferibles.",
        "Sin embargo, los entornos de búsqueda web plantean desafíos significativos para los modelos de predicción actuales que están principalmente diseñados para entornos tradicionales de TREC.",
        "Aquí delineamos algunos de estos desafíos.",
        "Primero, las colecciones web, que son mucho más grandes que las colecciones TREC convencionales, incluyen una variedad de documentos que son diferentes en muchos aspectos, como la calidad y el estilo.",
        "Las técnicas actuales de predicción pueden ser vulnerables a estas características de las colecciones web.",
        "Por ejemplo, la precisión de predicción reportada de la técnica de robustez de clasificación y la técnica de claridad en la colección GOV2 (una colección web grande) es significativamente peor en comparación con las otras colecciones de TREC [1].",
        "En [2] se informa de una precisión de predicción similar en la colección GOV2 utilizando otra técnica, confirmando la dificultad de predecir el rendimiento de consultas en una gran colección web.",
        "Además, la búsqueda web va más allá del alcance de la tarea de recuperación ad hoc basada en la relevancia temática.",
        "Por ejemplo, la tarea de encontrar páginas nombradas (NP), que es una tarea de navegación, también es popular en la recuperación web.",
        "La predicción del rendimiento de la consulta para la tarea de NP sigue siendo necesaria, ya que el rendimiento de recuperación de NP está lejos de ser perfecto.",
        "De hecho, según el informe sobre la tarea NP del Terabyte Track de 2005 [3], aproximadamente el 40% de las consultas de prueba tienen un rendimiento deficiente (sin respuesta correcta en los primeros 10 resultados de búsqueda) incluso en la mejor ejecución del grupo líder.",
        "Hasta donde sabemos, poca investigación ha abordado explícitamente el problema de la predicción del rendimiento de consultas de NP.",
        "Los modelos de predicción actuales diseñados para consultas basadas en contenido serán menos efectivos para consultas de NP considerando las diferencias fundamentales entre ambas.",
        "En tercer lugar, en entornos de búsqueda web del mundo real, las consultas de los usuarios suelen ser una mezcla de diferentes tipos y, por lo general, no se dispone de información previa sobre el tipo de cada consulta.",
        "La situación de consulta mixta plantea nuevos problemas para la predicción del rendimiento de la consulta.",
        "Por ejemplo, es posible que necesitemos incorporar un clasificador de consultas en los modelos de predicción.",
        "A pesar de estos problemas, la capacidad para manejar esta situación es un paso crucial hacia convertir la predicción del rendimiento de consultas de un tema de investigación interesante en una herramienta práctica para la recuperación web.",
        "En este artículo, presentamos tres técnicas para abordar los desafíos mencionados que enfrentan los modelos de predicción actuales en entornos de búsqueda web.",
        "Nuestro trabajo se centra en la predicción del rendimiento de consultas para la tarea de recuperación basada en contenido (ad-hoc) y la tarea de encontrar páginas por nombre en el contexto de la recuperación web.",
        "Nuestra primera técnica, llamada ganancia de información ponderada (WIG), utiliza tanto características de términos individuales como de proximidad de términos para estimar la calidad de los documentos recuperados para la predicción.",
        "Observamos que WIG ofrece una precisión de predicción consistente en diversas colecciones de pruebas y tipos de consultas.",
        "Además, demostramos que se puede lograr una buena precisión de predicción para la situación de consulta mixta utilizando WIG con la ayuda de un clasificador de tipo de consulta.",
        "La retroalimentación de consultas y el cambio de rango inicial, que son nuestras técnicas de predicción segunda y tercera, funcionan bien para consultas basadas en contenido y consultas de NP respectivamente.",
        "Nuestras principales contribuciones incluyen: (1) una precisión de predicción considerablemente mejorada para consultas basadas en contenido web en comparación con varias técnicas de vanguardia. (2) nuevas técnicas para predecir con éxito el rendimiento de consultas NP. (3) una solución práctica y totalmente automática para predecir el rendimiento de consultas mixtas.",
        "Además, una contribución menor es que encontramos que el puntaje de robustez [1], que originalmente se propuso para la predicción de rendimiento, es útil para la clasificación de consultas. 2.",
        "TRABAJO RELACIONADO Como mencionamos en la introducción, recientemente se han propuesto varias técnicas de predicción que se centran en consultas basadas en contenido en la tarea de relevancia temática (ad-hoc).",
        "No conocemos ningún trabajo publicado que aborde otros tipos de consultas como las consultas NP, y mucho menos una mezcla de tipos de consultas.",
        "A continuación revisamos algunos modelos representativos.",
        "La principal dificultad de la predicción del rendimiento proviene del hecho de que muchos factores tienen un impacto en el rendimiento de recuperación.",
        "Cada factor afecta el rendimiento en diferente medida y el efecto general es difícil de predecir con precisión.",
        "Por lo tanto, no es sorprendente notar que características simples, como la frecuencia de los términos de consulta en la colección [4] y la IDF promedio de los términos de consulta [5], no predicen bien.",
        "De hecho, la mayoría de las técnicas exitosas se basan en medir algunas características del conjunto de documentos recuperados para estimar la dificultad del tema.",
        "Por ejemplo, la puntuación de claridad [6] mide la coherencia de una lista de documentos mediante la divergencia KL entre el modelo de consulta y el modelo de colección.",
        "El puntaje de robustez [1] cuantifica otra propiedad de una lista clasificada: la robustez de la clasificación en presencia de incertidumbre.",
        "Carmel et al. [2] encontraron que la distancia medida por la divergencia de Jensen-Shannon entre el conjunto de documentos recuperados y la colección está significativamente correlacionada con la precisión promedio.",
        "Vinay et al. [7] propusieron cuatro medidas para capturar la geometría de los documentos mejor clasificados para la predicción.",
        "La medida más efectiva es la sensibilidad a la perturbación del documento, una idea algo similar a la puntuación de robustez.",
        "Desafortunadamente, su forma de medir la sensibilidad no funciona igual de bien para consultas cortas y la precisión de la predicción disminuye considerablemente cuando se adopta una técnica de recuperación de vanguardia (como Okapi o un enfoque de modelado de lenguaje) en lugar del peso tf-idf utilizado en su artículo [16].",
        "Las dificultades de aplicar estos modelos en entornos de búsqueda web ya han sido mencionadas.",
        "En este documento, principalmente adoptamos la puntuación de claridad y la puntuación de robustez como nuestras líneas base.",
        "Experimentalmente demostramos que las líneas base, incluso después de ser ajustadas cuidadosamente, son inadecuadas para el entorno web.",
        "Uno de nuestros modelos de predicción, WIG, está relacionado con el modelo de campo aleatorio de Markov (MRF) para la recuperación de información [8].",
        "El modelo MRF modela directamente la dependencia de términos y se ha demostrado ser altamente efectivo en una variedad de colecciones de pruebas (especialmente colecciones web) y tareas de recuperación.",
        "Este modelo se utiliza para estimar la distribución conjunta de probabilidad sobre documentos y consultas, una parte importante de WIG.",
        "La superioridad de WIG sobre otras técnicas de predicción basadas en características unigramas, que se demostrará más adelante en nuestro artículo, coincide con la de MRF para recuperación.",
        "En otras palabras, es interesante notar que la dependencia de términos, cuando se modela adecuadamente, puede ser útil tanto para mejorar como para predecir el rendimiento de recuperación. 3.",
        "Modelos de predicción 3.1 Ganancia de Información Ponderada (WIG) Esta sección introduce un enfoque de ganancia de información ponderada que incorpora tanto características de términos únicos como de proximidad para predecir el rendimiento tanto en consultas basadas en contenido como en consultas de búsqueda de Páginas Nombradas (NP).",
        "Dado un conjunto de consultas Q={Qs} (s=1,2,..N) que incluye todas las posibles consultas de usuario y un conjunto de documentos D={Dt} (t=1,2…M), asumimos que cada par consulta-documento (Qs,Dt) es evaluado manualmente y se incluirá en una lista de relevancia si se determina que Qs es relevante para Dt.",
        "La probabilidad conjunta P(Qs,Dt) sobre las consultas Q y los documentos D denota la probabilidad de que el par (Qs,Dt) esté en la lista de relevancia.",
        "Tales suposiciones son similares a las utilizadas en [8].",
        "Suponiendo que el usuario emite la consulta Qi ∈Q y los resultados de recuperación en respuesta a Qi es una lista clasificada L de documentos, calculamos la cantidad de información contenida en P(Qs,Dt) con respecto a Qi y L mediante la Ec.1, que es una variante de la entropía llamada entropía ponderada[13].",
        "Los pesos en la ecuación 1 están determinados únicamente por Qi y L. En este documento, elegimos los pesos de la siguiente manera: L es el número de documentos que contiene la consulta, K es el límite superior, LT es el límite superior de documentos, y D es el peso si K está en otro caso. La clasificación de corte K es un parámetro en nuestro modelo que se discutirá más adelante.",
        "Por lo tanto, la ecuación 1 se puede simplificar de la siguiente manera: )3(),(log 1 ),( )( , ∑∈ −= LTD titsLQ Kt i DQP K DQH Desafortunadamente, la entropía ponderada ),(, tsLQ DQH i calculada por la ecuación 3, que representa la cantidad de información sobre la probabilidad de que los documentos mejor clasificados en L sean relevantes para la consulta Qi en promedio, no se puede comparar entre diferentes consultas, lo que la hace inapropiada para predecir directamente el rendimiento de la consulta.",
        "Para mitigar este problema, creamos una distribución de fondo P(Qs,C) sobre Q y D imaginando que cada documento en D es reemplazado por el mismo documento especial C que representa el uso promedio del lenguaje.",
        "En este documento, C se crea concatenando cada documento en D. A grosso modo, C es la colección (el conjunto de documentos) {Dt} sin límites de documentos.",
        "De manera similar, la entropía ponderada ),(, CQH sLQi calculada por la Ecuación 3 representa la cantidad de información sobre la probabilidad de que un documento promedio (representado por toda la colección) sea relevante para la consulta Qi.",
        "Ahora presentamos nuestro predictor de rendimiento WIG que es la ganancia de información ponderada [13] calculada como la diferencia entre ),(, tsLQ DQH i y ),(, CQH sLQi. Específicamente, dado el conjunto de consultas Qi, la colección C y la lista clasificada L de documentos, WIG se calcula de la siguiente manera: )4( ),( ),( log 1 ),( ),( log),( ),(),(),,( )(, ,, ∑∑ ∈ == −= LTD i ti ts s ts ts tsLQsLQi Kt ii CQP DQP KCQP DQP DQweight DQHCQHLCQWIG WIG calculado por la Ecuación 4 mide el cambio en la información sobre la calidad de la recuperación (en respuesta a la consulta Qi) desde un estado imaginario en el que solo se recupera un documento promedio hasta un estado posterior en el que se observan los resultados de búsqueda reales.",
        "Hacemos la hipótesis de que WIG está positivamente correlacionado con la efectividad de la recuperación porque una recuperación de alta calidad debería ser mucho más efectiva que simplemente devolver el documento promedio.",
        "El corazón de esta técnica es cómo estimar la distribución conjunta P(Qs, Dt).",
        "En el enfoque de modelado del lenguaje para la recuperación de información, se pueden aplicar fácilmente una variedad de modelos para estimar esta distribución.",
        "Aunque la mayoría de estos modelos se basan en la suposición de la bolsa de palabras, trabajos recientes sobre modelado de dependencia de términos bajo el marco de modelado de lenguaje han demostrado mejoras consistentes y significativas en la efectividad de recuperación sobre los modelos de bolsa de palabras.",
        "Inspirados por el éxito de incorporar características de proximidad de términos en modelos de lenguaje, decidimos adoptar un buen modelo de dependencia para estimar la probabilidad P(Qs,Dt).",
        "El modelo que elegimos para este artículo es el modelo de Campo Aleatorio de Markov (MRF) de Metzler y Crofts, que ya ha demostrado su superioridad en comparación con varias colecciones y diferentes tareas de recuperación [8,9].",
        "Según el modelo MRF, log P(Qi, Dt) se puede escribir como )5()|(loglog),(log )( 1 ∑∈ +−= iQF tti DPZDQP ξ ξ ξλ donde Z1 es una constante que asegura que P(Qi, Dt) sume 1.",
        "F(Qi) consiste en un conjunto de características ampliadas a partir de la consulta original Qi.",
        "Por ejemplo, suponiendo que la consulta Qi es el programa de estudiantes talentosos, F(Qi) incluye características como programa y estudiante talentoso.",
        "Consideramos dos tipos de características: características de términos individuales T y características de proximidad P. Las características de proximidad incluyen la frase exacta (#1) y las características de ventana desordenada (#uwN) como se describe en [8].",
        "Ten en cuenta que F(Qi) es la unión de T(Qi) y P(Qi).",
        "Para obtener más detalles sobre F(Qi), como por ejemplo cómo expandir la consulta original Qi a F(Qi), remitimos al lector a [8] y [9].",
        "P(ξ|Dt) denota la probabilidad de que la característica ξ ocurra en Dt.",
        "Más detalles sobre P(ξ|Dt) se proporcionarán más adelante en esta sección.",
        "La elección de λξ es algo diferente a la utilizada en [8] ya que λξ desempeña un doble papel en nuestro modelo.",
        "El primer rol, que es el mismo que en [8], es ponderar entre las características de términos individuales y de proximidad.",
        "El otro rol, específico de nuestra tarea de predicción, es normalizar el tamaño de F(Qi). Encontramos que la siguiente estrategia de peso para λξ satisface los dos roles anteriores y generaliza bien en una variedad de colecciones y tipos de consultas. )6( )(, |)(| 1 )(, |)(| ⎪ ⎪ ⎩ ⎪ ⎪ ⎨ ⎧ ∈ − ∈ = i i T i i T QP QP QT QT ξ λ ξ λ λξ donde |T(Qi)| y |P(Qi)| denotan el número de características de términos únicos y de proximidad en F(Qi) respectivamente.",
        "La razón de elegir la función raíz cuadrada en el denominador de λξ es penalizar adecuadamente un conjunto de características de gran tamaño, haciendo que WIG sea más comparable entre consultas de diversas longitudes. λT es un parámetro fijo y se establece en 0.8 según [8] a lo largo de este documento.",
        "De manera similar, log P(Qi,C) se puede escribir como: )7()|(loglog),(log )( 2 ∑∈ +−= iQF i CPZCQP ξ ξ ξλ Cuando se eliminan las constantes Z1 y Z2, el WIG calculado en la Ecuación 4 se puede reescribir de la siguiente manera al sustituir la Ecuación 5 y la Ecuación 7: )8( )|( )|( log 1 ),,( )( )( ∑ ∑∈ ∈ = LTD QF t i Kt i CP DP K LCQWIG ξ ξ ξ ξ λ Una de las ventajas del WIG sobre otras técnicas es que puede manejar tanto consultas basadas en contenido como en NP.",
        "Basándose en el tipo (o el tipo predicho) de Qi, el cálculo de WIG en la Ecuación 8 difiere en dos aspectos: (1) cómo estimar P(ξ|Dt) y P(ξ|C), y (2) cómo elegir K. Para consultas basadas en contenido, P(ξ|C) se estima por la frecuencia relativa del rasgo ξ en la colección C en su totalidad.",
        "La estimación de P(ξ|Dt) es la misma que en [8].",
        "Es decir, estimamos P(ξ|Dt) mediante la frecuencia relativa del rasgo ξ en Dt suavizada linealmente con la frecuencia de colección P(ξ|C).",
        "La K en la Ec.8 se trata como un parámetro libre.",
        "Ten en cuenta que K es el único parámetro libre en el cálculo de WIG para consultas basadas en contenido, ya que se asume que todos los parámetros involucrados en P(ξ|Dt) están fijos al tomar los valores sugeridos en [8].",
        "En cuanto a las consultas de NP, hacemos uso de la estructura del documento para estimar P(ξ|Dt) y P(ξ|C) mediante el llamado modelo de mezcla de lenguaje propuesto en [10] e incorporado en el modelo MRF para la recuperación de Named-Page en [9].",
        "La idea básica es que un documento (colección) se divide en varios campos como el campo del título, el campo del cuerpo principal y el campo de los encabezados.",
        "P(ξ|Dt) y P(ξ|C) se estiman mediante una combinación lineal de los modelos de lenguaje de cada campo.",
        "Debido a limitaciones de espacio, remitimos al lector a [9] para más detalles.",
        "Adoptamos el mismo conjunto exacto de parámetros utilizados en [9] para la estimación.",
        "En cuanto a K en la Ec.8, establecemos K en 1 porque la tarea de encontrar Named-Page se centra fuertemente en el documento clasificado en primer lugar.",
        "Por consiguiente, no hay parámetros libres en el cálculo de WIG para consultas de NP. 3.2 Retroalimentación de consultas En esta sección, presentamos otra técnica llamada retroalimentación de consultas (QF) para la predicción.",
        "Suponga que un usuario emite la consulta Q a un sistema de recuperación y se devuelve una lista clasificada L de documentos.",
        "Vemos el sistema de recuperación como un canal ruidoso.",
        "Específicamente, asumimos que la salida del canal es L y la entrada es Q.",
        "Después de pasar por el canal, Q se corrompe y se transforma en la lista clasificada L. Al pensar en el proceso de recuperación de esta manera, el problema de predecir la eficacia de la recuperación se convierte en la tarea de evaluar la calidad del canal.",
        "En otras palabras, la predicción se convierte en encontrar una forma de medir el grado de corrupción que surge cuando Q se transforma en L. Dado que calcular directamente el grado de corrupción es difícil, abordamos este problema mediante aproximaciones.",
        "Nuestra idea principal es que medimos en qué medida la información sobre Q puede ser recuperada de L bajo la suposición de que solo se observa L.",
        "Específicamente, diseñamos un decodificador que pueda traducir con precisión L de vuelta a la nueva consulta Q y la similitud S entre la consulta original Q y la nueva consulta Q se adopta como un predictor de rendimiento.",
        "Este es un bosquejo de cómo la técnica QF predice el rendimiento de la consulta.",
        "Antes de completar más detalles, discutimos brevemente por qué este método funcionaría.",
        "Existe una relación entre la similitud S definida anteriormente y el rendimiento de recuperación.",
        "Por un lado, si la recuperación se ha desviado del sentido original de la consulta Q, la nueva consulta Q extraída de la lista clasificada L en respuesta a Q sería muy diferente de la consulta original Q.",
        "Por otro lado, una consulta destilada de una lista clasificada que contiene muchos documentos relevantes es probable que sea similar a la consulta original.",
        "Se proporcionarán más ejemplos en apoyo de la relación más adelante.",
        "A continuación detallamos cómo construir el decodificador y cómo medir la similitud S. En esencia, el objetivo del decodificador es comprimir la lista clasificada L en unos pocos términos informativos que deberían representar el contenido de los documentos mejor clasificados en L. Nuestro enfoque para este objetivo es representar la lista clasificada L mediante un modelo de lenguaje (distribución de términos).",
        "Luego, los términos se clasifican según su contribución a la divergencia KL (Kullback-Leibler) de los modelos de lenguaje con respecto al modelo de la colección de fondo.",
        "Los términos mejor clasificados serán elegidos para formar la nueva consulta Q.",
        "Este enfoque es similar al utilizado en la Sección 4.1 de [11].",
        "Específicamente, tomamos tres pasos para comprimir la lista clasificada L en la consulta Q sin hacer referencia a la consulta original. 1.",
        "Adoptamos el modelo de lenguaje de lista clasificada [14] para estimar un modelo de lenguaje basado en la lista clasificada L. El modelo puede escribirse como: )9()|()|()|( ∑∈ = LD LDPDwPLwP donde w es cualquier término y D es un documento.",
        "La probabilidad de que ocurra el evento D dado el evento L se estima mediante una función linealmente decreciente del rango del documento D.",
        "Cada término en P(w|L) está clasificado por la siguiente contribución de divergencia de Kullback-Leibler: )10( )|( )|( log)|( CwP LwP LwP donde P(w|C) es el modelo de colección estimado por la frecuencia relativa del término w en la colección C en su totalidad. 3.",
        "Los términos N principales clasificados por Eq.10 forman una consulta ponderada Q={(wi,ti)} i=1,N, donde wi denota el término clasificado en la posición i y el peso ti es la contribución de la divergencia KL de wi en Eq. 10.",
        "Barco de crucero daño vida marina.",
        "Estos ejemplos indican cómo la similitud entre la consulta original y la nueva se correlaciona con el rendimiento de recuperación.",
        "El parámetro N en el paso 3 se establece en 20 de manera empírica y elegir un valor más grande de N es innecesario ya que los pesos después de los primeros 20 suelen ser demasiado pequeños para marcar alguna diferencia.",
        "Para medir la similitud entre la consulta original Q y la nueva consulta Q, primero utilizamos Q para recuperar información de la misma colección.",
        "Se adopta una variante del modelo de probabilidad de consulta [15] para la recuperación.",
        "Es decir, los documentos se clasifican por: )11()|()|( ),( ∑∈ = Qtw t i ii i DwPDQP donde wi es un término en Q y ti es el peso asociado.",
        "D es un documento.",
        "Que L denote la nueva lista clasificada devuelta desde la recuperación anterior.",
        "La similitud se mide por la superposición de documentos en L y L. Específicamente, el porcentaje de documentos en los primeros K documentos de L que también están presentes en los primeros K documentos en L. El valor de corte K se trata como un parámetro libre.",
        "Aquí resumimos cómo la técnica QF predice el rendimiento dado una consulta Q y la lista clasificada asociada L. Primero obtenemos una consulta ponderada Q comprimida a partir de L mediante los tres pasos anteriores.",
        "Luego usamos Q para realizar la recuperación y la nueva lista clasificada es L. La superposición de documentos en L y L se utiliza para la predicción. 3.3 Primer Cambio de Rango (FRC) En esta sección, proponemos un método llamado primer cambio de rango (FRC) para la predicción de rendimiento para consultas de NP.",
        "Este método se deriva de la técnica de robustez de clasificación [1] que está principalmente diseñada para consultas basadas en contenido.",
        "Cuando se aplica directamente a consultas de NP, la técnica de robustez será menos efectiva porque tiene en cuenta todos los documentos mejor clasificados en su totalidad, mientras que las consultas de NP suelen tener solo un único documento relevante.",
        "En cambio, nuestra técnica se centra en el documento de rango uno mientras se mantiene la idea principal del método de robustez.",
        "Específicamente, el seudocódigo para calcular FRC se muestra en la figura 1.",
        "Lista clasificada L={Di} donde i=1,100.",
        "Di denota el documento clasificado en la posición i. (2) consulta Q 1 inicializar: (1) establecer el número de intentos J=100000 (2) contador c=0; 2 para i=1 a J 3 Perturbar cada documento en L, que el resultado sea un conjunto F={Di} donde Di denota la versión perturbada de Di. 4 Realizar la recuperación con la consulta Q en el conjunto F 5 c=c+1 si y solo si D1 está clasificado primero en el paso 4 6 fin del para 7 devolver la proporción c/J Figura 1: pseudo-código para calcular FRC FRC aproxima la probabilidad de que el documento clasificado en primer lugar en la lista original L permanezca clasificado primero incluso después de que los documentos sean perturbados.",
        "Cuanto mayor sea la probabilidad, más confianza tenemos en el documento clasificado en primer lugar.",
        "Por otro lado, en el caso extremo de un ranking aleatorio, la probabilidad sería tan baja como 0.5.",
        "Esperamos que FRC tenga una asociación positiva con el rendimiento de la consulta de NP.",
        "Adoptamos [1] para implementar el paso de perturbación del documento (paso 4 en la Fig. 1) utilizando distribuciones de Poisson.",
        "Para más detalles, remitimos al lector a [1]. 4.",
        "EVALUACIÓN Ahora presentamos los resultados de predecir el rendimiento de la consulta por nuestros modelos.",
        "Tres técnicas de vanguardia son adoptadas como nuestras líneas base.",
        "Evaluamos nuestras técnicas en una variedad de configuraciones de recuperación web.",
        "Como se mencionó anteriormente, consideramos dos tipos de consultas, es decir, consultas basadas en contenido (CB) y consultas de búsqueda de páginas con nombre (NP).",
        "Primero, supongamos que se conocen los tipos de consultas.",
        "Investigamos la correlación entre el rendimiento de recuperación predicho y el rendimiento real para ambos tipos de consultas por separado.",
        "Los resultados muestran que nuestros métodos producen mejoras considerablemente superiores a los valores de referencia.",
        "Luego consideramos un escenario más desafiante donde no hay información previa sobre los tipos de consultas disponibles.",
        "Se consideran dos subcasos.",
        "En el primero, existe solo un tipo de consulta pero el tipo real es desconocido.",
        "Suponemos una mezcla de los dos tipos de consultas en el segundo caso.",
        "Demostramos que nuestros modelos logran una buena precisión en este escenario exigente, lo que hace que la predicción sea práctica en un entorno de búsqueda web del mundo real. 4.1 Configuración Experimental Nuestra evaluación se centra en la colección GOV2 que contiene alrededor de 25 millones de documentos obtenidos de sitios web en el dominio .gov durante 2004 [3].",
        "Creamos dos tipos de conjuntos de datos para consultas de CB y consultas de NP respectivamente.",
        "Para el tipo CB, utilizamos los temas ad-hoc de las pistas Terabyte de 2004, 2005 y 2006 y los nombramos TB04-adhoc, TB05-adhoc y TB06-adhoc respectivamente.",
        "Además, también utilizamos los temas ad-hoc de la pista Robusta de 2004 (RT04) para probar la adaptabilidad de nuestras técnicas a un entorno no web.",
        "Para consultas de NP, utilizamos los temas de búsqueda de páginas nombradas de las Terabyte Tracks de 2005 y 2006 y los denominamos TB05-NP y TB06-NP respectivamente.",
        "Todas las consultas utilizadas en nuestros experimentos son títulos de temas de TREC, ya que nos centramos en la recuperación web.",
        "La Tabla 3 resume los conjuntos de datos anteriores.",
        "La recuperación del rendimiento de las consultas individuales basadas en contenido y NP se mide mediante la precisión promedio y la tasa recíproca de la primera respuesta correcta, respectivamente.",
        "Hacemos uso del modelo de campo aleatorio de Markov tanto para la recuperación ad-hoc como para la recuperación de páginas nombradas.",
        "Adoptamos la misma configuración de parámetros de recuperación utilizada en [8,9].",
        "El motor de búsqueda Indri [12] se utiliza para todos nuestros experimentos.",
        "Aunque no se informa aquí, también probamos el modelo de probabilidad de consulta para la recuperación ad-hoc y encontramos que los resultados cambian poco debido a la correlación muy alta entre el rendimiento de las consultas obtenido por los dos modelos de recuperación (0.96 medido por el coeficiente de Pearson). 4.2 Tipos de Consultas Conocidos Supongamos que se conocen los tipos de consultas.",
        "Tratamos cada tipo de consulta por separado y medimos la correlación con la precisión promedio (o el rango recíproco en el caso de consultas NP).",
        "Adoptamos la prueba de correlación de Pearson que refleja el grado de relación lineal entre el rendimiento de recuperación predicho y real. 4.2.1 Métodos de Consultas Basadas en Contenido Claridad Robusta JSD WIG QF WIG +QF TB04+0 5 adhoc 0.333 0.317 0.362 0.574 0.480 0.637 TB06 adhoc 0.076 0.294 N/A 0.464 0.422 0.511 Tabla 4: Coeficientes de correlación de Pearson para la correlación con la precisión promedio en las Pistas de Terabyte (ad-hoc) para la puntuación de claridad, puntuación de robustez, el método basado en JSD (citamos directamente la puntuación informada en [2]), WIG, retroalimentación de consulta (QF) y una combinación lineal de WIG y QF.",
        "Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01.",
        "La Tabla 4 muestra la correlación con la precisión promedio en dos conjuntos de datos: uno es una combinación de TB04-adhoc y TB05-adhoc (100 temas en total) y el otro es TB06-adhoc (50 temas).",
        "La razón por la que juntamos TB04-adhoc y TB05-adhoc es para hacer nuestros resultados comparables a [2].",
        "Nuestros puntos de referencia son la puntuación de claridad (claridad) [6], la puntuación de robustez (robust) [1] y el método basado en JSD (JSD) [2].",
        "Para la puntuación de claridad y robustez, hemos probado diferentes configuraciones de parámetros y reportamos los coeficientes de correlación más altos que hemos encontrado.",
        "Citamos directamente el resultado del método basado en JSD reportado en [2].",
        "La tabla también muestra los resultados para el método de Ganancia de Información Ponderada (WIG) y el método de Retroalimentación de Consulta (QF) para predecir consultas basadas en contenido.",
        "Como describimos en la sección anterior, tanto WIG como QF tienen un parámetro libre para ajustar, es decir, el rango de corte K. Entrenamos el parámetro en un conjunto de datos y lo probamos en el otro.",
        "Al combinar WIG y QF, se utiliza una combinación lineal simple y el peso de la combinación se aprende a partir del conjunto de datos de entrenamiento.",
        "A partir de estos resultados, podemos ver que nuestros métodos son considerablemente más precisos en comparación con los baselines.",
        "También observamos que se obtienen mejoras adicionales a partir de la combinación de WIG y QF, lo que sugiere que miden diferentes propiedades del proceso de recuperación que se relacionan con el rendimiento.",
        "Descubrimos que nuestros métodos se generalizan bien en TB06-adhoc, mientras que la correlación del puntaje de claridad con el rendimiento de recuperación en este conjunto de datos es considerablemente peor.",
        "Una investigación adicional muestra que la precisión promedio media de TB06-adhoc es de 0.342 y es aproximadamente un 10% mejor que la del primer conjunto de datos.",
        "Mientras que los otros tres métodos suelen considerar los 100 mejores documentos o menos de una lista clasificada, el método de claridad generalmente necesita los 500 mejores documentos o más para medir adecuadamente la coherencia de una lista clasificada.",
        "Un promedio de precisión media más alto hace que las listas clasificadas recuperadas por diferentes consultas sean más similares en términos de coherencia en el nivel de los primeros 500 documentos.",
        "Creemos que esta es la razón principal de la baja precisión de la puntuación de claridad en el segundo conjunto de datos.",
        "Aunque este documento se centra en un entorno de búsqueda web, es deseable que nuestras técnicas funcionen de manera consistente en otras situaciones.",
        "Con este fin, examinamos la efectividad de nuestras técnicas en la pista Robust 2004.",
        "Para nuestros métodos, dividimos equitativamente todas las consultas de prueba en cinco grupos y realizamos validación cruzada de cinco pliegues.",
        "Cada vez que utilizamos un grupo para el entrenamiento y los otros cuatro grupos para las pruebas.",
        "Hacemos uso de todas las consultas para nuestros dos puntos de referencia, es decir, la puntuación de claridad y la puntuación de robustez.",
        "Los parámetros para nuestras líneas base son los mismos que los utilizados en [1]. Los resultados mostrados en la Tabla 5 demuestran que la precisión de predicción de nuestros métodos está a la par con la de las dos líneas base sólidas.",
        "Claridad Robusta WIG QF 0.464 0.539 0.468 0.464 Tabla 5: Comparación de los coeficientes de correlación de Pearson en la pista Robusta de 2004 para la puntuación de claridad, puntuación de robustez, WIG y retroalimentación de consulta (QF).",
        "Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01.",
        "Además, examinamos la sensibilidad de predicción de nuestros métodos al rango de corte K. Con respecto a WIG, es bastante robusto a K en las Pistas de Terabyte (2004-2006) mientras prefiere un valor pequeño de K como 5 en la Pista Robusta de 2004.",
        "En otras palabras, un valor pequeño de K es una elección casi óptima para ambos tipos de pistas.",
        "Teniendo en cuenta que todos los demás parámetros involucrados en WIG están fijos y, consecuentemente, son los mismos para los dos casos, esto significa que WIG puede lograr una precisión de predicción casi óptima en dos situaciones considerablemente diferentes con exactamente la misma configuración de parámetros.",
        "En cuanto a QF, prefiere un valor más grande de K, como 100 en las Pistas de Terabyte y un valor más pequeño de K, como 25 en la Pista Robusta de 2004. 4.2.2 Consultas NP Adoptamos WIG y el primer cambio de rango (FRC) para predecir el rendimiento de las consultas NP.",
        "También intentamos una combinación lineal de los dos como en la sección anterior.",
        "El peso de la combinación se obtiene del otro conjunto de datos.",
        "Utilizamos la correlación con los rangos recíprocos medidos por la prueba de correlación de Pearson para evaluar la calidad de la predicción.",
        "Los resultados se presentan en la Tabla 6.",
        "Nuevamente, nuestros puntos de referencia son la puntuación de claridad y la puntuación de robustez.",
        "Para hacer una comparación justa, ajustamos la puntuación de claridad de diferentes maneras.",
        "Descubrimos que utilizar el documento clasificado en primer lugar para construir el modelo de consulta produce la mejor precisión de predicción.",
        "También intentamos utilizar la estructura del documento mediante la combinación de modelos de lenguaje mencionados en la sección 3.1.",
        "Se obtuvo una pequeña mejora.",
        "Los coeficientes de correlación para la puntuación de claridad reportados en la Tabla 6 son los mejores que hemos encontrado.",
        "Como podemos ver, nuestros métodos superan considerablemente la técnica de puntuación de claridad en ambas ejecuciones.",
        "Esto confirma nuestra intuición de que el uso de una medida basada en la coherencia como el puntaje de claridad es inapropiado para las consultas de NP.",
        "Claridad de métodos robusta.",
        "Tabla 6: Coeficientes de correlación de Pearson para la correlación con rangos recíprocos en las pistas de Terabyte (NP) para la puntuación de claridad, puntuación de robustez, WIG, el primer cambio de rango (FRC) y una combinación lineal de WIG y FRC.",
        "Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01.",
        "En cuanto a la puntuación de robustez, también ajustamos los parámetros y reportamos el mejor que hemos encontrado.",
        "Observamos una correlación negativa interesante y sorprendente con los rangos recíprocos.",
        "Explicamos este hallazgo brevemente.",
        "Un puntaje de robustez alto significa que varios documentos de alta clasificación en la lista clasificada original siguen estando muy bien clasificados después de perturbar los documentos.",
        "La existencia de tales documentos es una buena señal de alto rendimiento para consultas basadas en contenido, ya que estas consultas suelen contener una serie de documentos relevantes [1].",
        "Sin embargo, en lo que respecta a las consultas de NP, una diferencia fundamental es que solo hay un documento relevante para cada consulta.",
        "La existencia de tales documentos puede confundir la función de clasificación y llevar a un bajo rendimiento de recuperación.",
        "Aunque existe una correlación negativa con el rendimiento de recuperación, la fuerza de la correlación es más débil y menos consistente en comparación con nuestros métodos, como se muestra en la Tabla 6.",
        "Basándonos en el análisis anterior, podemos ver que las técnicas de predicción actuales como la puntuación de claridad y la puntuación de robustez, que están principalmente diseñadas para consultas basadas en contenido, enfrentan desafíos significativos y son insuficientes para manejar consultas de NP.",
        "Nuestras dos técnicas propuestas para consultas de NP demuestran consistentemente una buena precisión de predicción, mostrando un éxito inicial en la resolución del problema de predecir el rendimiento para consultas de NP.",
        "Otro punto que queremos destacar es que el método WIG funciona bien para ambos tipos de consultas, una propiedad deseable que la mayoría de las técnicas de predicción carecen. 4.3 Tipos de Consultas Desconocidos En esta sección, realizamos dos tipos de experimentos sin acceso a etiquetas de tipo de consulta.",
        "Primero, asumimos que solo existe un tipo de consulta pero el tipo es desconocido.",
        "Segundo, experimentamos con una mezcla de consultas basadas en contenido y NP.",
        "Las dos subsecciones siguientes informarán los resultados para las dos condiciones respectivamente. 4.3.1 Solo existe un tipo. Suponemos que todas las consultas son del mismo tipo, es decir, son consultas NP o consultas basadas en contenido.",
        "Elegimos WIG para tratar este caso porque muestra una buena precisión de predicción para ambos tipos de consultas en la sección anterior.",
        "Consideramos dos casos: (1) CB: todas las 150 consultas de títulos de la tarea ad-hoc de las pistas Terabyte 2004-2006 (2) NP: todas las 433 consultas de NP de la tarea de búsqueda de páginas nombradas de las pistas Terabyte 2005 y 2006.",
        "Tomamos una estrategia simple etiquetando todas las consultas en cada caso como el mismo tipo (ya sea NP o CB) independientemente de su tipo real.",
        "El cálculo de WIG se basará en el tipo de consulta etiquetado en lugar del tipo real.",
        "Hay cuatro posibilidades con respecto a la relación entre el tipo real y el tipo etiquetado.",
        "La correlación con el rendimiento de recuperación bajo las cuatro posibilidades se presenta en la Tabla 7.",
        "Por ejemplo, el valor 0.445 en la intersección entre la segunda fila y la tercera columna muestra el coeficiente de correlación de Pearson para la correlación con la precisión promedio cuando las consultas basadas en contenido se etiquetan incorrectamente como del tipo NP.",
        "Basándonos en estos resultados, recomendamos tratar todas las consultas como del tipo NP cuando solo existe un tipo de consulta y la clasificación precisa de la consulta no es factible, considerando el riesgo de que se produzca una gran pérdida de precisión si las consultas NP se etiquetan incorrectamente como consultas basadas en contenido.",
        "Estos resultados también demuestran la fuerte adaptabilidad de WIG a diferentes tipos de consultas.",
        "Tabla 7: Comparación de los coeficientes de correlación de Pearson para la correlación con el rendimiento de recuperación en cuatro posibilidades en las pistas de Terabyte (NP).",
        "Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. 4.3.2 Una mezcla de consultas basadas en contenido y NP Una mezcla de los dos tipos de consultas es una situación más realista que un motor de búsqueda web encontrará.",
        "Evaluamos la precisión de la predicción por la exactitud con la que se pueden identificar las consultas de bajo rendimiento mediante el método de predicción, asumiendo que los tipos de consulta reales son desconocidos (pero podemos predecir los tipos de consulta).",
        "Esta es una tarea desafiante porque tanto el rendimiento predicho como el real para un tipo de consulta pueden ser incomparables con el de otro tipo.",
        "A continuación discutimos cómo implementar nuestra evaluación.",
        "Creamos un conjunto de consultas que consiste en todas las 150 consultas de título ad-hoc de Terabyte Track 2004-2006 y todas las 433 consultas NP de Terabyte Track 2005&2006.",
        "Dividimos las consultas en el conjunto en clases: buenas (mejores que el 50% de las consultas del mismo tipo en términos de rendimiento de recuperación) y malas (de lo contrario).",
        "Según estos estándares, una consulta de NP con un rango recíproco superior a 0.2 o una consulta basada en contenido con una precisión promedio superior a 0.315 se considerarán como buenas.",
        "Entonces, cada vez que seleccionamos aleatoriamente una consulta Q del grupo con una probabilidad p de que Q esté basada en el contenido.",
        "Las consultas restantes se utilizan como datos de entrenamiento.",
        "Primero decidimos el tipo de consulta Q de acuerdo con un clasificador de consultas.",
        "Es decir, el clasificador de consultas nos dice si la consulta Q es de tipo NP o basada en contenido.",
        "Basándose en el tipo de consulta predicho y en la puntuación calculada para la consulta Q por una técnica de predicción, se toma una decisión binaria sobre si la consulta Q es buena o mala al compararla con el umbral de puntuación del tipo de consulta predicho obtenido de los datos de entrenamiento.",
        "La precisión de la predicción se mide por la exactitud de la decisión binaria.",
        "En nuestra implementación, tomamos repetidamente una consulta de prueba del conjunto de consultas y la precisión de la predicción se calcula como el porcentaje de decisiones correctas, es decir, una consulta buena (mala) se predice como buena (mala).",
        "Es obvio que adivinar al azar llevará a una precisión del 50%.",
        "Tomemos el método WIG como ejemplo para ilustrar el proceso.",
        "Dos umbrales de WIG (uno para consultas de NP y otro para consultas basadas en contenido) se entrenan maximizando la precisión de predicción en los datos de entrenamiento.",
        "Cuando una consulta de prueba es etiquetada como del tipo NP (CB) por el clasificador de tipo de consulta, se predecirá que es buena solo si la puntuación WIG para esta consulta está por encima del umbral de NP (CB).",
        "Se seguirán procedimientos similares para otras técnicas de predicción.",
        "Ahora presentamos brevemente el clasificador de tipo de consulta automático utilizado en este artículo.",
        "Encontramos que la puntuación de robustez, aunque originalmente propuesta para la predicción de rendimiento, es un buen indicador de tipos de consultas.",
        "Observamos que, en promedio, las consultas basadas en contenido tienen una puntuación de robustez mucho más alta que las consultas de NP.",
        "Por ejemplo, la Figura 2 muestra las distribuciones de puntajes de robustez para consultas basadas en NP y en contenido.",
        "Según este hallazgo, el clasificador de puntuación de robustez adjuntará una etiqueta NP (CB) a la consulta si la puntuación de robustez para la consulta está por debajo (por encima) de un umbral entrenado a partir de los datos de entrenamiento. 0 0.5 1 1.5 2 2.5 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 NP Basado en contenido Figura 2: Distribución de puntuaciones de robustez para consultas NP y CB.",
        "Las consultas NP son los 252 temas NP del Terabyte Track de 2005.",
        "Las consultas basadas en contenido son los 150 títulos ad-hoc de las pistas Terabyte 2004-2006.",
        "Las distribuciones de probabilidad son estimadas por el método de estimación de densidad de Kernel.",
        "Estrategias Robustas WIG-1 WIG-2 WIG-3 Óptima p=0.6 0.565 0.624 0.665 0.684 0.701 P=0.4 0.567 0.633 0.654 0.673 0.696 Tabla 8: Comparación de la precisión de predicción para cinco estrategias en la situación de consulta mixta.",
        "Dos formas de muestrear una consulta de la piscina: (1) la consulta muestreada es basada en contenido con una probabilidad p=0.6 (es decir, la consulta es NP con una probabilidad de 0.4) (2) establecer la probabilidad p=0.4.",
        "Consideramos cinco estrategias en nuestros experimentos.",
        "En la primera estrategia (denominada robusta), utilizamos la puntuación de robustez para la predicción del rendimiento de la consulta con la ayuda de un clasificador de consultas perfecto que siempre mapea correctamente una consulta en una de las dos categorías (es decir, NP o CB).",
        "Esta estrategia representa el nivel de precisión de predicción que las técnicas actuales de predicción pueden lograr en una condición ideal en la que se conocen los tipos de consulta.",
        "En las tres estrategias siguientes, se adopta el método WIG para predecir el rendimiento.",
        "La diferencia entre los tres es que se utilizan tres clasificadores de consulta diferentes para cada estrategia: (1) el clasificador siempre clasifica una consulta en el tipo NP. (2) el clasificador de Probabilidad de Densidad de Puntuación Robusta es el clasificador de puntuación robusta mencionado anteriormente. (3) el clasificador es perfecto.",
        "Estas tres estrategias están designadas como WIG-1, WIG-2 y WIG-3 respectivamente.",
        "La razón por la que estamos interesados en WIG-1 se basa en los resultados de la sección 4.3.1.",
        "En la última estrategia (denominada Óptima) que sirve como un límite superior de lo bien que podemos hacer hasta ahora, hacemos pleno uso de nuestras técnicas de predicción para cada tipo de consulta asumiendo que un clasificador de consultas perfecto está disponible.",
        "Específicamente, combinamos linealmente WIG y QF para consultas basadas en contenido y WIG y FRC para consultas de NP.",
        "Los resultados de las cinco estrategias se muestran en la Tabla 8.",
        "Para cada estrategia, intentamos dos formas de muestrear una consulta del conjunto: (1) la consulta muestreada es CB con probabilidad p=0.6 (la consulta es NP con probabilidad 0.4) (2) establecer la probabilidad p=0.4.",
        "De la Tabla 8 podemos ver que en términos de precisión de predicción, WIG-2 (el método WIG con el clasificador de consultas automático) no solo es mejor que los dos primeros casos, sino que también está cerca de WIG-3 donde se asume un clasificador perfecto.",
        "Se observan algunas mejoras adicionales sobre WIG-3 cuando se combinan con otras técnicas de predicción.",
        "El mérito de WIG-2 es que proporciona una solución práctica para identificar automáticamente consultas con bajo rendimiento en un entorno de búsqueda web con tipos de consultas mixtos, lo cual representa considerables obstáculos para las técnicas de predicción tradicionales.",
        "CONCLUSIONES Y TRABAJOS FUTUROS Hasta donde sabemos, nuestro artículo es el primero en explorar a fondo la predicción del rendimiento de consultas en entornos de búsqueda web.",
        "Demostramos que nuestros modelos resultaron en una mayor precisión de predicción que las técnicas previamente publicadas no especialmente diseñadas para escenarios de búsqueda en la web.",
        "En este artículo, nos enfocamos en dos tipos de consultas en la búsqueda web: consultas basadas en contenido y consultas de búsqueda de Páginas-Nombradas (NP), correspondientes a la tarea de recuperación ad-hoc y la tarea de búsqueda de Páginas-Nombradas respectivamente.",
        "Para ambos tipos de consultas web, se demostró que nuestros modelos de predicción son sustancialmente más precisos que las técnicas actuales más avanzadas.",
        "Además, consideramos un caso más realista en el que no se dispone de información previa sobre los tipos de consultas.",
        "Demostramos que el método WIG es particularmente adecuado para esta situación.",
        "Considerando la adaptabilidad de WIG a una variedad de colecciones y tipos de consultas, uno de nuestros planes futuros es aplicar este método para predecir la preferencia del usuario por los resultados de búsqueda en datos realistas recopilados de un motor de búsqueda comercial.",
        "Además de la precisión, otro problema importante con el que las técnicas de predicción tienen que lidiar en un entorno web es la eficiencia.",
        "Afortunadamente, dado que el puntaje WIG se calcula solo sobre los términos y frases que aparecen en la consulta, este cálculo puede realizarse de manera muy eficiente con el soporte de un índice.",
        "Por otro lado, el cálculo de QF y FRC es relativamente menos eficiente ya que QF necesita recuperar toda la colección dos veces y FRC necesita clasificar repetidamente los documentos perturbados.",
        "Cómo mejorar la eficiencia de QF y FRC es nuestro trabajo futuro.",
        "Además, las técnicas de predicción propuestas en este documento tienen el potencial de mejorar el rendimiento de recuperación al combinarse con otras técnicas de RI.",
        "Por ejemplo, nuestras técnicas pueden ser incorporadas a técnicas populares de modificación de consultas como la expansión de consultas y la relajación de consultas.",
        "Guiados por la predicción del rendimiento, podemos tomar una mejor decisión sobre cuándo o cómo modificar las consultas para mejorar la efectividad de la recuperación.",
        "Nos gustaría llevar a cabo investigaciones en esta dirección en el futuro.",
        "AGRADECIMIENTOS Este trabajo fue apoyado en parte por el Centro de Recuperación de Información Inteligente, en parte por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el contrato número HR0011-06-C0023, y en parte por un premio de Google.",
        "Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son del autor y no necesariamente reflejan las del patrocinador.",
        "Además, agradecemos a Donald Metzler por sus valiosos comentarios sobre este trabajo. 7.",
        "REFERENCIAS [1] Y. Zhou, W. B. Croft, Ranking Robustness: Un nuevo marco para predecir el rendimiento de consultas, en Actas de CIKM 2006. [2] D. Carmel, E. Yom-Tov, A. Darlow, D. Pelleg, ¿Qué hace que una consulta sea difícil?, en Actas de SIGIR 2006. [3] C. L. A.",
        "Clarke, F. Scholer, I. Soboroff, La pista Terabyte TREC 2005, En las Actas en Línea de TREC 2005. [4] B.",
        "Él y yo.",
        "Inferir el rendimiento de la consulta utilizando predictores de preretrieval.",
        "En actas de SPIRE 2004. [5] S. Tomlinson.",
        "Recuperación robusta de la web y terabytes con Hummingbird SearchServer en TREC 2004.",
        "En las Actas en Línea de TREC 2004. [6] S. Cronen-Townsend, Y. Zhou, W. B. Croft, Prediciendo el Rendimiento de Consultas, en las Actas de SIGIR 2002. [7] V. Vinay, I. J. Cox, N. Mill-Frayling, K. Wood, Sobre la Clasificación de la Efectividad del Buscador, en las Actas de SIGIR 2006. [8] D. Metzler, W. B. Croft, Un Modelo de Campo Aleatorio de Markov para Dependencias de Términos, en las Actas de SIGIR 2005. [9] D. Metzler, T. Strohman, Y. Zhou, W. B. Croft, Indri en TREC 2005: Terabyte Track, en las Actas en Línea de TREC 2004. [10] P. Ogilvie y J. Callan, Combinando representaciones de documentos para búsqueda de elementos conocidos, en las Actas de SIGIR 2003. [11] A. Berger, J. Lafferty, Recuperación de información como traducción estadística, en las Actas de SIGIR 1999. [12] Motor de búsqueda Indri: http://www.lemurproject.org/indri/ [13] I. J.",
        "Taneja: Sobre medidas de información generalizadas y sus aplicaciones, Avances en Electrónica y Física de Electrones, Academic Press (EE. UU.), 76, 1989, 327-413. [14] S. Cronen-Townsend, Y. Zhou y Croft, W. B., Un marco para la expansión selectiva de consultas, en Actas de CIKM 2004. [15] F. Song, W. B. Croft, Un modelo de lenguaje general para la recuperación de información, en Actas de SIGIR 1999. [16] Contacto por correo electrónico personal con Vishwa Vinay y nuestros propios experimentos [17] E. Yom-Tov, S. Fine, D. Carmel, A. Darlow, Aprendiendo a estimar la dificultad de la consulta incluyendo aplicaciones para la detección de contenido faltante y la recuperación de información distribuida, en Actas de SIGIR 2005."
    ],
    "error_count": 1,
    "keys": {
        "query performance prediction": {
            "translated_key": "predicción del rendimiento de la consulta",
            "is_in_text": true,
            "original_annotated_sentences": [
                "<br>query performance prediction</br> in Web Search Environments Yun Zhou and W. Bruce Croft Department of Computer Science University of Massachusetts, Amherst {yzhou, croft}@cs.umass.edu ABSTRACT Current prediction techniques, which are generally designed for content-based queries and are typically evaluated on relatively homogenous test collections of small sizes, face serious challenges in web search environments where collections are significantly more heterogeneous and different types of retrieval tasks exist.",
                "In this paper, we present three techniques to address these challenges.",
                "We focus on performance prediction for two types of queries in web search environments: content-based and Named-Page finding.",
                "Our evaluation is mainly performed on the GOV2 collection.",
                "In addition to evaluating our models for the two types of queries separately, we consider a more challenging and realistic situation that the two types of queries are mixed together without prior information on query types.",
                "To assist prediction under the mixed-query situation, a novel query classifier is adopted.",
                "Results show that our prediction of web query performance is substantially more accurate than the current stateof-the-art prediction techniques.",
                "Consequently, our paper provides a practical approach to performance prediction in realworld web settings.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval -Query formulation General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION <br>query performance prediction</br> has many applications in a variety of information retrieval (IR) areas such as improving retrieval consistency, query refinement, and distributed IR.",
                "The importance of this problem has been recognized by IR researchers and a number of new methods have been proposed for prediction recently [1, 2, 17].",
                "Most work on prediction has focused on the traditional ad-hoc retrieval task where query performance is measured according to topical relevance.",
                "These prediction models are evaluated on TREC document collections which typically consist of no more than one million relatively homogenous newswire articles.",
                "With the popularity and influence of the Web, prediction techniques that will work well for web-style queries are highly preferable.",
                "However, web search environments pose significant challenges to current prediction models that are mainly designed for traditional TREC settings.",
                "Here we outline some of these challenges.",
                "First, web collections, which are much larger than conventional TREC collections, include a variety of documents that are different in many aspects such as quality and style.",
                "Current prediction techniques can be vulnerable to these characteristics of web collections.",
                "For example, the reported prediction accuracy of the ranking robustness technique and the clarity technique on the GOV2 collection (a large web collection) is significantly worse compared to the other TREC collections [1].",
                "Similar prediction accuracy on the GOV2 collection using another technique is reported in [2], confirming the difficult of predicting query performance on a large web collection.",
                "Furthermore, web search goes beyond the scope of the ad-hoc retrieval task based on topical relevance.",
                "For example, the Named-Page (NP) finding task, which is a navigational task, is also popular in web retrieval.",
                "<br>query performance prediction</br> for the NP task is still necessary since NP retrieval performance is far from perfect.",
                "In fact, according to the report on the NP task of the 2005 Terabyte Track [3], about 40% of the test queries perform poorly (no correct answer in the first 10 search results) even in the best run from the top group.",
                "To our knowledge, little research has explicitly addressed the problem of NP-<br>query performance prediction</br>.",
                "Current prediction models devised for content-based queries will be less effective for NP queries considering the fundamental differences between the two.",
                "Third, in real-world web search environments, user queries are usually a mixture of different types and prior knowledge about the type of each query is generally unavailable.",
                "The mixed-query situation raises new problems for <br>query performance prediction</br>.",
                "For instance, we may need to incorporate a query classifier into prediction models.",
                "Despite these problems, the ability to handle this situation is a crucial step towards turning <br>query performance prediction</br> from an interesting research topic into a practical tool for web retrieval.",
                "In this paper, we present three techniques to address the above challenges that current prediction models face in Web search environments.",
                "Our work focuses on <br>query performance prediction</br> for the content-based (ad-hoc) retrieval task and the name-page finding task in the context of web retrieval.",
                "Our first technique, called weighted information gain (WIG), makes use of both single term and term proximity features to estimate the quality of top retrieved documents for prediction.",
                "We find that WIG offers consistent prediction accuracy across various test collections and query types.",
                "Moreover, we demonstrate that good prediction accuracy can be achieved for the mixed-query situation by using WIG with the help of a query type classifier.",
                "Query feedback and first rank change, which are our second and third prediction techniques, perform well for content-based queries and NP queries respectively.",
                "Our main contributions include: (1) considerably improved prediction accuracy for web content-based queries over several state-of-the-art techniques. (2) new techniques for successfully predicting NP-query performance. (3) a practical and fully automatic solution to predicting mixed-query performance.",
                "In addition, one minor contribution is that we find that the robustness score [1], which was originally proposed for performance prediction, is helpful for query classification. 2.",
                "RELATED WORK As we mentioned in the introduction, a number of prediction techniques have been proposed recently that focus on contentbased queries in the topical relevance (ad-hoc) task.",
                "We know of no published work that addresses other types of queries such as NP queries, let alone a mixture of query types.",
                "Next we review some representative models.",
                "The major difficulty of performance prediction comes from the fact that many factors have an impact on retrieval performance.",
                "Each factor affects performance to a different degree and the overall effect is hard to predict accurately.",
                "Therefore, it is not surprising to notice that simple features, such as the frequency of query terms in the collection [4] and the average IDF of query terms [5], do not predict well.",
                "In fact, most of the successful techniques are based on measuring some characteristics of the retrieved document set to estimate topic difficulty.",
                "For example, the clarity score [6] measures the coherence of a list of documents by the KL-divergence between the query model and the collection model.",
                "The robustness score [1] quantifies another property of a ranked list: the robustness of the ranking in the presence of uncertainty.",
                "Carmel et al. [2] found that the distance measured by the Jensen-Shannon divergence between the retrieved document set and the collection is significantly correlated to average precision.",
                "Vinay et al. [7] proposed four measures to capture the geometry of the top retrieved documents for prediction.",
                "The most effective measure is the sensitivity to document perturbation, an idea somewhat similar to the robustness score.",
                "Unfortunately, their way of measuring the sensitivity does not perform equally well for short queries and prediction accuracy drops considerably when a state-of-the-art retrieval technique (like Okapi or a language modeling approach) is adopted for retrieval instead of the tf-idf weighting used in their paper [16].",
                "The difficulties of applying these models in web search environments have already been mentioned.",
                "In this paper, we mainly adopt the clarity score and the robustness score as our baselines.",
                "We experimentally show that the baselines, even after being carefully tuned, are inadequate for the web environment.",
                "One of our prediction models, WIG, is related to the Markov random field (MRF) model for information retrieval [8].",
                "The MRF model directly models term dependence and is found be to highly effective across a variety of test collections (particularly web collections) and retrieval tasks.",
                "This model is used to estimate the joint probability distribution over documents and queries, an important part of WIG.",
                "The superiority of WIG over other prediction techniques based on unigram features, which will be demonstrated later in our paper, coincides with that of MRF for retrieval.",
                "In other word, it is interesting to note that term dependence, when being modeled appropriately, can be helpful for both improving and predicting retrieval performance. 3.",
                "PREDICTION MODELS 3.1 Weighted Information Gain (WIG) This section introduces a weighted information gain approach that incorporates both single term and proximity features for predicting performance for both content-based and Named-Page (NP) finding queries.",
                "Given a set of queries Q={Qs} (s=1,2,..N) which includes all possible user queries and a set of documents D={Dt} (t=1,2…M), we assume that each query-document pair (Qs,Dt) is manually judged and will be put in a relevance list if Qs is found to be relevant to Dt.",
                "The joint probability P(Qs,Dt) over queries Q and documents D denotes the probability that pair (Qs,Dt) will be in the relevance list.",
                "Such assumptions are similar to those used in [8].",
                "Assuming that the user issues query Qi ∈Q and the retrieval results in response to Qi is a ranked list L of documents, we calculate the amount of information contained in P(Qs,Dt) with respect to Qi and L by Eq.1 which is a variant of entropy called the weighted entropy[13].",
                "The weights in Eq.1 are solely determined by Qi and L. )1(),(log),(),( , , ∑−= ts tststsLQ DQPDQweightDQH i In this paper, we choose the weights as follows: LindocumentsKtopthecontainsLTwhere otherwise LTDandisifK DQweight K Kt ts )( )2( ,0 )(,/1 ),( ⎩ ⎨ ⎧ ∈= = The cutoff rank K is a parameter in our model that will be discussed later.",
                "Accordingly, Eq.1 can be simplified as follows: )3(),(log 1 ),( )( , ∑∈ −= LTD titsLQ Kt i DQP K DQH Unfortunately, weighted entropy ),(, tsLQ DQH i computed by Eq.3, which represents the amount of information about how likely the top ranked documents in L would be relevant to query Qi on average, cannot be compared across different queries, making it inappropriate for directly predicting query performance.",
                "To mitigate this problem, we come up with a background distribution P(Qs,C) over Q and D by imagining that every document in D is replaced by the same special document C which represents average language usage.",
                "In this paper, C is created by concatenating every document in D. Roughly speaking, C is the collection (the document set) {Dt} without document boundaries.",
                "Similarly, weighted entropy ),(, CQH sLQi calculated by Eq.3 represents the amount of information about how likely an average document (represented by the whole collection) would be relevant to query Qi.",
                "Now we introduce our performance predictor WIG which is the weighted information gain [13] computed as the difference between ),(, tsLQ DQH i and ),(, CQH sLQi .Specifically, given query Qi, collection C and ranked list L of documents, WIG is calculated as follows: )4( ),( ),( log 1 ),( ),( log),( ),(),(),,( )(, ,, ∑∑ ∈ == −= LTD i ti ts s ts ts tsLQsLQi Kt ii CQP DQP KCQP DQP DQweight DQHCQHLCQWIG WIG computed by Eq.4 measures the change in information about the quality of retrieval (in response to query Qi) from an imaginary state that only an average document is retrieved to a posterior state that the actual search results are observed.",
                "We hypothesize that WIG is positively correlated with retrieval effectiveness because high quality retrieval should be much more effective than just returning the average document.",
                "The heart of this technique is how to estimate the joint distribution P(Qs,Dt).",
                "In the language modeling approach to IR, a variety of models can be applied readily to estimate this distribution.",
                "Although most of these models are based on the bagof-words assumption, recent work on modeling term dependence under the language modeling framework have shown consistent and significant improvements in retrieval effectiveness over bagof-words models.",
                "Inspired by the success of incorporating term proximity features into language models, we decide to adopt a good dependence model to estimate the probability P(Qs,Dt).",
                "The model we chose for this paper is Metzler and Crofts Markov Random Field (MRF) model, which has already demonstrated superiority over a number of collections and different retrieval tasks [8,9].",
                "According to the MRF model, log P(Qi, Dt) can be written as )5()|(loglog),(log )( 1 ∑∈ +−= iQF tti DPZDQP ξ ξ ξλ where Z1 is a constant that ensures that P(Qi, Dt) sums up to 1.",
                "F(Qi) consists of a set of features expanded from the original query Qi .",
                "For example, assuming that query Qi is talented student program, F(Qi) includes features like program and talented student.",
                "We consider two kinds of features: single term features T and proximity features P. Proximity features include exact phrase (#1) and unordered window (#uwN) features as described in [8].",
                "Note that F(Qi) is the union of T(Qi) and P(Qi).",
                "For more details on F(Qi) such as how to expand the original query Qi to F(Qi), we refer the reader to [8] and [9].",
                "P(ξ|Dt) denotes the probability that feature ξ will occur in Dt.",
                "More details on P(ξ|Dt) will be provided later in this section.",
                "The choice of λξ is somewhat different from that used in [8] since λξ plays a dual role in our model.",
                "The first role, which is the same as in [8], is to weight between single term and proximity features.",
                "The other role, which is specific to our prediction task, is to normalize the size of F(Qi).We found that the following weight strategy for λξ satisfies the above two roles and generalizes well on a variety of collections and query types. )6( )(, |)(| 1 )(, |)(| ⎪ ⎪ ⎩ ⎪ ⎪ ⎨ ⎧ ∈ − ∈ = i i T i i T QP QP QT QT ξ λ ξ λ λξ where |T(Qi)| and |P(Qi)| denote the number of single term and proximity features in F(Qi) respectively.",
                "The reason for choosing the square root function in the denominator of λξ is to penalize a feature set of large size appropriately, making WIG more comparable across queries of various lengths. λT is a fixed parameter and set to 0.8 according to [8] throughout this paper.",
                "Similarly, log P(Qi,C) can be written as: )7()|(loglog),(log )( 2 ∑∈ +−= iQF i CPZCQP ξ ξ ξλ When constant Z1 and Z2 are dropped, WIG computed in Eq.4 can be rewritten as follows by plugging in Eq.5 and Eq.7 : )8( )|( )|( log 1 ),,( )( )( ∑ ∑∈ ∈ = LTD QF t i Kt i CP DP K LCQWIG ξ ξ ξ ξ λ One of the advantages of WIG over other techniques is that it can handle well both content-based and NP queries.",
                "Based on the type (or the predicted type) of Qi, the calculation of WIG in Eq. 8 differs in two aspects: (1) how to estimate P(ξ|Dt) and P(ξ|C), and (2) how to choose K. For content-based queries, P(ξ|C) is estimated by the relative frequency of feature ξ in collection C as a whole.",
                "The estimation of P(ξ|Dt) is the same as in [8].",
                "Namely, we estimate P(ξ|Dt) by the relative frequency of feature ξ in Dt linearly smoothed with collection frequency P(ξ|C).",
                "K in Eq.8 is treated as a free parameter.",
                "Note that K is the only free parameter in the computation of WIG for content-based queries because all parameters involved in P(ξ|Dt) are assumed to be fixed by taking the suggested values in [8].",
                "Regarding NP queries, we make use of document structure to estimate P(ξ|Dt) and P(ξ|C) by the so-called mixture of language models proposed in [10] and incorporated into the MRF model for Named-Page finding retrieval in [9].",
                "The basic idea is that a document (collection) is divided into several fields such as the title field, the main-body field and the heading field.",
                "P(ξ|Dt) and P(ξ|C) are estimated by a linear combination of the language models from each field.",
                "Due to space constraints, we refer the reader to [9] for details.",
                "We adopt the exact same set of parameters as used in [9] for estimation.",
                "With regard to K in Eq.8, we set K to 1 because the Named-Page finding task heavily focuses on the first ranked document.",
                "Consequently, there are no free parameters in the computation of WIG for NP queries. 3.2 Query Feedback In this section, we introduce another technique called query feedback (QF) for prediction.",
                "Suppose that a user issues query Q to a retrieval system and a ranked list L of documents is returned.",
                "We view the retrieval system as a noisy channel.",
                "Specifically, we assume that the output of the channel is L and the input is Q.",
                "After going through the channel, Q becomes corrupted and is transformed to ranked list L. By thinking about the retrieval process this way, the problem of predicting retrieval effectiveness turns to the task of evaluating the quality of the channel.",
                "In other words, prediction becomes finding a way to measure the degree of corruption that arises when Q is transformed to L. As directly computing the degree of the corruption is difficult, we tackle this problem by approximation.",
                "Our main idea is that we measure to what extent information on Q can be recovered from L on the assumption that only L is observed.",
                "Specifically, we design a decoder that can accurately translate L back into new query Q and the similarity S between the original query Q and the new query Q is adopted as a performance predictor.",
                "This is a sketch of how the QF technique predicts query performance.",
                "Before filling in more details, we briefly discuss why this method would work.",
                "There is a relation between the similarity S defined above and retrieval performance.",
                "On the one hand, if the retrieval has strayed from the original sense of the query Q, the new query Q extracted from ranked list L in response to Q would be very different from the original query Q.",
                "On the other hand, a query distilled from a ranked list containing many relevant documents is likely to be similar to the original query.",
                "Further examples in support of the relation will be provided later.",
                "Next we detail how to build the decoder and how to measure the similarity S. In essence, the goal of the decoder is to compress ranked list L into a few informative terms that should represent the content of the top ranked documents in L. Our approach to this goal is to represent ranked list L by a language model (distribution over terms).",
                "Then terms are ranked by their contribution to the language models KL (Kullback-Leibler) divergence from the background collection model.",
                "Top ranked terms will be chosen to form the new query Q.",
                "This approach is similar to that used in Section 4.1 of [11].",
                "Specifically, we take three steps to compress ranked list L into query Q without referring to the original query. 1.",
                "We adopt the ranked list language model [14], to estimate a language model based on ranked list L. The model can be written as: )9()|()|()|( ∑∈ = LD LDPDwPLwP where w is any term, D is a document.",
                "P(D|L) is estimated by a linearly decreasing function of the rank of document D. 2.",
                "Each term in P(w|L) is ranked by the following KL-divergence contribution: )10( )|( )|( log)|( CwP LwP LwP where P(w|C) is the collection model estimated by the relative frequency of term w in collection C as a whole. 3.",
                "The top N ranked terms by Eq.10 form a weighted query Q={(wi,ti)} i=1,N. where wi denotes the i-th ranked term and weight ti is the KL-divergence contribution of wi in Eq. 10.",
                "Term cruise ship vessel sea passenger KL contribution 0.050 0.040 0.012 0.010 0.009 Table 1: top 5 terms compressed from the ranked list in response to query Cruise ship damage sea life Two representative examples, one for a poorly performing query Cruise ship damage sea life (TREC topic 719; average precision: 0.08) and the other for a high performing query prostate cancer treatments( TREC topic 710; average precision: 0.49), are shown in Table 1 and 2 respectively.",
                "These examples indicate how the similarity between the original and the new query correlates with retrieval performance.",
                "The parameter N in step 3 is set to 20 empirically and choosing a larger value of N is unnecessary since the weights after the top 20 are usually too small to make any difference.",
                "Term prostate cancer treatment men therapy KL contribution 0.177 0.140 0.028 0.025 0.020 Table 2: top 5 terms compressed from the ranked list in response to query prostate cancer treatments To measure the similarity between original query Q and new query Q, we first use Q to do retrieval on the same collection.",
                "A variant of the query likelihood model [15] is adopted for retrieval.",
                "Namely, documents are ranked by: )11()|()|( ),( ∑∈ = Qtw t i ii i DwPDQP where wi is a term in Q and ti is the associated weight.",
                "D is a document.",
                "Let L denote the new ranked list returned from the above retrieval.",
                "The similarity is measured by the overlap of documents in L and L. Specifically, the percentage of documents in the top K documents of L that are also present in the top K documents in L. the cutoff K is treated as a free parameter.",
                "We summarize here how the QF technique predicts performance given a query Q and the associated ranked list L. We first obtain a weighted query Q compressed from L by the above three steps.",
                "Then we use Q to perform retrieval and the new ranked list is L. The overlap of documents in L and L is used for prediction. 3.3 First Rank Change (FRC) In this section, we propose a method called the first rank change (FRC) for performance prediction for NP queries.",
                "This method is derived from the ranking robustness technique [1] that is mainly designed for content-based queries.",
                "When directly applied to NP queries, the robustness technique will be less effective because it takes the top ranked documents as a whole into account while NP queries usually have only one single relevant document.",
                "Instead, our technique focuses on the first rank document while the main idea of the robustness method remains.",
                "Specifically, the pseudocode for computing FRC is shown in figure 1.",
                "Input: (1) ranked list L={Di} where i=1,100.",
                "Di denotes the i-th ranked document. (2) query Q 1 initialize: (1) set the number of trials J=100000 (2) counter c=0; 2 for i=1 to J 3 Perturb every document in L, let the outcome be a set F={Di} where Di denotes the perturbed version of Di. 4 Do retrieval with query Q on set F 5 c=c+1 if and only if D1 is ranked first in step 4 6 end of for 7 return the ratio c/J Figure 1: pseudo-code for computing FRC FRC approximates the probability that the first ranked document in the original list L will remain ranked first even after the documents are perturbed.",
                "The higher the probability is, the more confidence we have in the first ranked document.",
                "On the other hand, in the extreme case of a random ranking, the probability would be as low as 0.5.",
                "We expect that FRC has a positive association with NP query performance.",
                "We adopt [1] to implement the document perturbation step (step 4 in Fig.1) using Poisson distributions.",
                "For more details, we refer the reader to [1]. 4.",
                "EVALUATION We now present the results of predicting query performance by our models.",
                "Three state-of-the-art techniques are adopted as our baselines.",
                "We evaluate our techniques across a variety of Web retrieval settings.",
                "As mentioned before, we consider two types of queries, that is, content-based (CB) queries and Named-Page(NP) finding queries.",
                "First, suppose that the query types are known.",
                "We investigate the correlation between the predicted retrieval performance and the actual performance for both types of queries separately.",
                "Results show that our methods yield considerable improvements over the baselines.",
                "We then consider a more challenging scenario where no prior information on query types is available.",
                "Two sub-cases are considered.",
                "In the first one, there exists only one type of query but the actual type is unknown.",
                "We assume a mixture of the two query types in the second case.",
                "We demonstrate that our models achieve good accuracy under this demanding scenario, making prediction practical in a real-world Web search environment. 4.1 Experimental Setup Our evaluation focuses on the GOV2 collection which contains about 25 million documents crawled from web sites in the .gov domain during 2004 [3].",
                "We create two kinds of data set for CB queries and NP queries respectively.",
                "For the CB type, we use the ad-hoc topics of the Terabyte Tracks of 2004, 2005 and 2006 and name them TB04-adhoc, TB05-adhoc and TB06-adhoc respectively.",
                "In addition, we also use the ad-hoc topics of the 2004 Robust Track (RT04) to test the adaptability of our techniques to a non-Web environment.",
                "For NP queries, we use the Named-Page finding topics of the Terabyte Tracks of 2005 and 2006 and we name them TB05-NP and TB06-NP respectively.",
                "All queries used in our experiments are titles of TREC topics as we center on web retrieval.",
                "Table 3 summarizes the above data sets.",
                "Name Collection Topic Number Query Type TB04-adhoc GOV2 701-750 CB TB05-adhoc GOV2 751-800 CB TB06-adhoc GOV2 801-850 CB RT04 Disk 4+5 (minus CR) 301-450;601700 CB TB05-NP GOV2 NP601-NP872 NP TB06-NP GOV2 NP901-NP1081 NP Table 3: Summary of test collections and topics Retrieval performance of individual content-based and NP queries is measured by the average precision and reciprocal rank of the first correct answer respectively.",
                "We make use of the Markov Random field model for both ad-hoc and Named-Page finding retrieval.",
                "We adopt the same setting of retrieval parameters used in [8,9].",
                "The Indri search engine [12] is used for all of our experiments.",
                "Though not reported here, we also tried the query likelihood model for ad-hoc retrieval and found that the results change little because of the very high correlation between the query performances obtained by the two retrieval models (0.96 measured by Pearsons coefficient). 4.2 Known Query Types Suppose that query types are known.",
                "We treat each type of query separately and measure the correlation with average precision (or the reciprocal rank in the case of NP queries).",
                "We adopt the Pearsons correlation test which reflects the degree of linear relationship between the predicted and the actual retrieval performance. 4.2.1 Content-based Queries Methods Clarity Robust JSD WIG QF WIG +QF TB04+0 5 adhoc 0.333 0.317 0.362 0.574 0.480 0.637 TB06 adhoc 0.076 0.294 N/A 0.464 0.422 0.511 Table 4: Pearsons correlation coefficients for correlation with average precision on the Terabyte Tracks (ad-hoc) for clarity score, robustness score, the JSD-based method(we directly cites the score reported in [2]), WIG, query feedback(QF) and a linear combination of WIG and QF.",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Table 4 shows the correlation with average precision on two data sets: one is a combination of TB04-adhoc and TB05-adhoc(100 topics in total) and the other is TB06-adhoc (50 topics).",
                "The reason that we put TB04-adhoc and TB05-adhoc together is to make our results comparable to [2].",
                "Our baselines are the clarity score (clarity) [6],the robustness score (robust)[1] and the JSDbased method (JSD) [2].",
                "For the clarity and robustness score, we have tried different parameter settings and report the highest correlation coefficients we have found.",
                "We directly cite the result of the JSD-based method reported in [2].",
                "The table also shows the results for the Weighted Information Gain (WIG) method and the Query Feedback (QF) method for predicting content-based queries.",
                "As we described in the previous section, both WIG and QF have one free parameter to set, that is, the cutoff rank K. We train the parameter on one dataset and test on the other.",
                "When combining WIG and QF, a simple linear combination is used and the combination weight is learned from the training data set.",
                "From these results, we can see that our methods are considerably more accurate compared to the baselines.",
                "We also observe that further improvements are obtained from the combination of WIG and QF, suggesting that they measure different properties of the retrieval process that relate to performance.",
                "We discover that our methods generalize well on TB06-adhoc while the correlation for the clarity score with retrieval performance on this data set is considerably worse.",
                "Further investigation shows that the mean average precision of TB06-adhoc is 0.342 and is about 10% better than that of the first data set.",
                "While the other three methods typically consider the top 100 or less documents given a ranked list, the clarity method usually needs the top 500 or more documents to adequately measure the coherence of a ranked list.",
                "Higher mean average precision makes ranked lists retrieved by different queries more similar in terms of coherence at the level of top 500 documents.",
                "We believe that this is the main reason for the low accuracy of the clarity score on the second data set.",
                "Though this paper focuses on a Web search environment, it is desirable that our techniques will work consistently well in other situations.",
                "To this end, we examine the effectiveness of our techniques on the Robust 2004 Track.",
                "For our methods, we evenly divide all of the test queries into five groups and perform five-fold cross validation.",
                "Each time we use one group for training and the remaining four groups for testing.",
                "We make use of all of the queries for our two baselines, that is, the clarity score and the robustness score.",
                "The parameters for our baselines are the same as those used in [1].The results shown in Table 5 demonstrate that the prediction accuracy of our methods is on a par with that of the two strong baselines.",
                "Clarity Robust WIG QF 0.464 0.539 0.468 0.464 Table 5: Comparison of Pearsons correlation coefficients on the 2004 Robust Track for clarity score, robustness score, WIG and query feedback (QF).",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Furthermore, we examine the prediction sensitivity of our methods to the cutoff rank K. With respect to WIG, it is quite robust to K on the Terabyte Tracks (2004-2006) while it prefers a small value of K like 5 on the 2004 Robust Track.",
                "In other words, a small value of K is a nearly-optimal choice for both kinds of tracks.",
                "Considering the fact that all other parameters involved in WIG are fixed and consequently the same for the two cases, this means WIG can achieve nearly-optimal prediction accuracy in two considerably different situations with exactly the same parameter setting.",
                "Regarding QF, it prefers a larger value of K such as 100 on the Terabyte Tracks and a smaller value of K such as 25 on the 2004 Robust Track. 4.2.2 NP Queries We adopt WIG and first rank change (FRC) for predicting NPquery performance.",
                "We also try a linear combination of the two as in the previous section.",
                "The combination weight is obtained from the other data set.",
                "We use the correlation with the reciprocal ranks measured by the Pearsons correlation test to evaluate prediction quality.",
                "The results are presented in Table 6.",
                "Again, our baselines are the clarity score and the robustness score.",
                "To make a fair comparison, we tune the clarity score in different ways.",
                "We found that using the first ranked document to build the query model yields the best prediction accuracy.",
                "We also attempted to utilize document structure by using the mixture of language models mentioned in section 3.1.",
                "Little improvement was obtained.",
                "The correlation coefficients for the clarity score reported in Table 6 are the best we have found.",
                "As we can see, our methods considerably outperform the clarity score technique on both of the runs.",
                "This confirms our intuition that the use of a coherence-based measure like the clarity score is inappropriate for NP queries.",
                "Methods Clarity Robust.",
                "WIG FRC WIG+FRC TB05-NP 0.150 -0.370 0.458 0.440 0.525 TB06-NP 0.112 -0.160 0.478 0.386 0.515 Table 6: Pearsons correlation coefficients for correlation with reciprocal ranks on the Terabyte Tracks (NP) for clarity score, robustness score, WIG, the first rank change (FRC) and a linear combination of WIG and FRC.",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Regarding the robustness score, we also tune the parameters and report the best we have found.",
                "We observe an interesting and surprising negative correlation with reciprocal ranks.",
                "We explain this finding briefly.",
                "A high robustness score means that a number of top ranked documents in the original ranked list are still highly ranked after perturbing the documents.",
                "The existence of such documents is a good sign of high performance for content-based queries as these queries usually contain a number of relevant documents [1].",
                "However, with regard to NP queries, one fundamental difference is that there is only one relevant document for each query.",
                "The existence of such documents can confuse the ranking function and lead to low retrieval performance.",
                "Although the negative correlation with retrieval performance exists, the strength of the correlation is weaker and less consistent compared to our methods as shown in Table 6.",
                "Based on the above analysis, we can see that current prediction techniques like clarity score and robustness score that are mainly designed for content-based queries face significant challenges and are inadequate to deal with NP queries.",
                "Our two techniques proposed for NP queries consistently demonstrate good prediction accuracy, displaying initial success in solving the problem of predicting performance for NP queries.",
                "Another point we want to stress is that the WIG method works well for both types of queries, a desirable property that most prediction techniques lack. 4.3 Unknown Query Types In this section, we run two kinds of experiments without access to query type labels.",
                "First, we assume that only one type of query exists but the type is unknown.",
                "Second, we experiment on a mixture of content-based and NP queries.",
                "The following two subsections will report results for the two conditions respectively. 4.3.1 Only One Type exists We assume that all queries are of the same type, that is, they are either NP queries or content-based queries.",
                "We choose WIG to deal with this case because it shows good prediction accuracy for both types of queries in the previous section.",
                "We consider two cases: (1) CB: all 150 title queries from the ad-hoc task of the Terabyte Tracks 2004-2006 (2)NP: all 433 NP queries from the named page finding task of the Terabyte Tracks 2005 and 2006.",
                "We take a simple strategy by labeling all of the queries in each case as the same type (either NP or CB) regardless of their actual type.",
                "The computation of WIG will be based on the labeled query type instead of the actual type.",
                "There are four possibilities with respect to the relation between the actual type and the labeled type.",
                "The correlation with retrieval performance under the four possibilities is presented in Table 7.",
                "For example, the value 0.445 at the intersection between the second row and the third column shows the Pearsons correlation coefficient for correlation with average precision when the content-based queries are incorrectly labeled as the NP type.",
                "Based on these results, we recommend treating all queries as the NP type when only one query type exists and accurate query classification is not feasible, considering the risk that a large loss of accuracy will occur if NP queries are incorrectly labeled as content-based queries.",
                "These results also demonstrate the strong adaptability of WIG to different query types.",
                "CB (labeled) NP (labeled) CB (actual) 0.536 0.445 NP (actual) 0.174 0.467 Table 7: Comparison of Pearsons correlation coefficients for correlation with retrieval performance under four possibilities on the Terabyte Tracks (NP).",
                "Bold cases mean the results are statistically significant at the 0.01 level. 4.3.2 A mixture of contented-based and NP queries A mixture of the two types of queries is a more realistic situation that a Web search engine will meet.",
                "We evaluate prediction accuracy by how accurately poorly-performing queries can be identified by the prediction method assuming that actual query types are unknown (but we can predict query types).",
                "This is a challenging task because both the predicted and actual performance for one type of query can be incomparable to that for the other type.",
                "Next we discuss how to implement our evaluation.",
                "We create a query pool which consists of all of the 150 ad-hoc title queries from Terabyte Track 2004-2006 and all of the 433 NP queries from Terabyte Track 2005&2006.",
                "We divide the queries in the pool into classes: good (better than 50% of the queries of the same type in terms of retrieval performance) and bad (otherwise).",
                "According to these standards, a NP query with the reciprocal rank above 0.2 or a content-based query with the average precision above 0.315 will be considered as good.",
                "Then, each time we randomly select one query Q from the pool with probability p that Q is contented-based.",
                "The remaining queries are used as training data.",
                "We first decide the type of query Q according to a query classifier.",
                "Namely, the query classifier tells us whether query Q is NP or content-based.",
                "Based on the predicted query type and the score computed for query Q by a prediction technique, a binary decision is made about whether query Q is good or bad by comparing to the score threshold of the predicted query type obtained from the training data.",
                "Prediction accuracy is measured by the accuracy of the binary decision.",
                "In our implementation, we repeatedly take a test query from the query pool and prediction accuracy is computed as the percentage of correct decisions, that is, a good(bad) query is predicted to be good (bad).",
                "It is obvious that random guessing will lead to 50% accuracy.",
                "Let us take the WIG method for example to illustrate the process.",
                "Two WIG thresholds (one for NP queries and the other for content-based queries) are trained by maximizing the prediction accuracy on the training data.",
                "When a test query is labeled as the NP (CB) type by the query type classifier, it will be predicted to be good if and only if the WIG score for this query is above the NP (CB) threshold.",
                "Similar procedures will be taken for other prediction techniques.",
                "Now we briefly introduce the automatic query type classifier used in this paper.",
                "We find that the robustness score, though originally proposed for performance prediction, is a good indicator of query types.",
                "We find that on average content-based queries have a much higher robustness score than NP queries.",
                "For example, Figure 2 shows the distributions of robustness scores for NP and content-based queries.",
                "According to this finding, the robustness score classifier will attach a NP (CB) label to the query if the robustness score for the query is below (above) a threshold trained from the training data. 0 0.5 1 1.5 2 2.5 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 NP Content-based Figure 2: Distribution of robustness scores for NP and CB queries.",
                "The NP queries are the 252 NP topics from the 2005 Terabyte Track.",
                "The content-based queries are the 150 ad-hoc title from the Terabyte Tracks 2004-2006.",
                "The probability distributions are estimated by the Kernel density estimation method.",
                "Strategies Robust WIG-1 WIG-2 WIG-3 Optimal p=0.6 0.565 0.624 0.665 0.684 0.701 P=0.4 0.567 0.633 0.654 0.673 0.696 Table 8: Comparison of prediction accuracy for five strategies in the mixed-query situation.",
                "Two ways to sample a query from the pool: (1) the sampled query is content-based with the probability p=0.6. (that is, the query is NP with probability 0.4 ) (2) set the probability p=0.4.",
                "We consider five strategies in our experiments.",
                "In the first strategy (denoted by robust), we use the robustness score for <br>query performance prediction</br> with the help of a perfect query classifier that always correctly map a query into one of the two categories (that is, NP or CB).",
                "This strategy represents the level of prediction accuracy that current prediction techniques can achieve in an ideal condition that query types are known.",
                "In the next following three strategies, the WIG method is adopted for performance prediction.",
                "The difference among the three is that three different query classifiers are used for each strategy: (1) the classifier always classifies a query into the NP type. (2) the Robustness Score ProbabilityDensity classifier is the robust score classifier mentioned above. (3) the classifier is a perfect one.",
                "These three strategies are denoted by WIG-1, WIG-2 and WIG-3 respectively.",
                "The reason we are interested in WIG-1 is based on the results from section 4.3.1.",
                "In the last strategy (denoted by Optimal) which serves as an upper bound on how well we can do so far, we fully make use of our prediction techniques for each query type assuming a perfect query classifier is available.",
                "Specifically, we linearly combine WIG and QF for content-based queries and WIG and FRC for NP queries.",
                "The results for the five strategies are shown in Table 8.",
                "For each strategy, we try two ways to sample a query from the pool: (1) the sampled query is CB with probability p=0.6. (the query is NP with probability 0.4) (2) set the probability p=0.4.",
                "From Table 8 We can see that in terms of prediction accuracy WIG-2 (the WIG method with the automatic query classifier) is not only better than the first two cases, but also is close to WIG-3 where a perfect classifier is assumed.",
                "Some further improvements over WIG-3 are observed when combined with other prediction techniques.",
                "The merit of WIG-2 is that it provides a practical solution to automatically identifying poorly performing queries in a Web search environment with mixed query types, which poses considerable obstacles to traditional prediction techniques. 5.",
                "CONCLUSIONS AND FUTURE WORK To our knowledge, our paper is the first to thoroughly explore prediction of query performance in web search environments.",
                "We demonstrated that our models resulted in higher prediction accuracy than previously published techniques not specially devised for web search scenarios.",
                "In this paper, we focus on two types of queries in web search: content-based and Named-Page (NP) finding queries, corresponding to the ad-hoc retrieval task and the Named-Page finding task respectively.",
                "For both types of web queries, our prediction models were shown to be substantially more accurate than the current state-of-the-art techniques.",
                "Furthermore, we considered a more realistic case that no prior information on query types is available.",
                "We demonstrated that the WIG method is particularly suitable for this situation.",
                "Considering the adaptability of WIG to a range of collections and query types, one of our future plans is to apply this method to predict user preference of search results on realistic data collected from a commercial search engine.",
                "Other than accuracy, another major issue that prediction techniques have to deal with in a Web environment is efficiency.",
                "Fortunately, since the WIG score is computed just over the terms and the phrases that appear in the query, this calculation can be made very efficient with the support of index.",
                "On the other hand, the computation of QF and FRC is relatively less efficient since QF needs to retrieve the whole collection twice and FRC needs to repeatedly rank the perturbed documents.",
                "How to improve the efficiency of QF and FRC is our future work.",
                "In addition, the prediction techniques proposed in this paper have the potential of improving retrieval performance by combining with other IR techniques.",
                "For example, our techniques can be incorporated to popular query modification techniques such as query expansion and query relaxation.",
                "Guided by performance prediction, we can make a better decision on when to or how to modify queries to enhance retrieval effectiveness.",
                "We would like to carry out research in this direction in the future. 6.",
                "ACKNOWLEGMENTS This work was supported in part by the Center for Intelligent Information Retrieval, in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C0023, and in part by an award from Google.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor.",
                "In addition, we thank Donald Metzler for his valuable comments on this work. 7.",
                "REFERENCES [1] Y. Zhou ,W. B. Croft ,Ranking Robustness: A Novel Framework to Predict Query Performance, in Proceedings of CIKM 2006. [2] D.Carmel, E.Yom-Tov, A.Darlow,D.Pelleg, What Makes a Query Difficult?, in Proceedings of SIGIR 2006. [3] C.L.A.",
                "Clarke, F. Scholer, I.Soboroff, The TREC 2005 Terabyte Track, In the Online Proceedings of 2005 TREC. [4] B.",
                "He and I.Ounis.",
                "Inferring query performance using preretrieval predictors.",
                "In proceedings of the SPIRE 2004. [5] S. Tomlinson.",
                "Robust, Web and Terabyte Retrieval with Hummingbird SearchServer at TREC 2004.",
                "In the Online Proceedings of 2004 TREC. [6] S. Cronen-Townsend, Y. Zhou, W. B. Croft, Predicting Query Performance, in Proceedings of SIGIR 2002. [7] V.Vinay, I.J.Cox, N.Mill-Frayling,K.Wood, On Ranking the Effectiveness of Searcher, in Proceedings of SIGIR 2006. [8] D.Metzler, W.B.Croft, A Markov Random Filed Model for Term Dependencies, in Proceedings of SIGIR 2005. [9] D.Metzler, T.Strohman,Y.Zhou,W.B.Croft, Indri at TREC 2005: Terabyte Track, In the Online Proceedings of 2004 TREC. [10] P. Ogilvie and J. Callan, Combining document representations for known-item search, in Proceedings of SIGIR 2003. [11] A.Berger, J.Lafferty, Information retrieval as statistical translation, in Proceedings of SIGIR 1999. [12] Indri search engine : http://www.lemurproject.org/indri/ [13] I.J.",
                "Taneja: On Generalized Information Measures and Their Applications, Advances in Electronics and Electron Physics, Academic Press (USA), 76, 1989, 327-413. [14] S.Cronen-Townsend, Y. Zhou and Croft, W. B. , A Framework for Selective Query Expansion, in Proceedings of CIKM 2004. [15] F.Song, W.B.Croft, A general language model for information retrieval, in Proceedings of SIGIR 1999. [16] Personal email contact with Vishwa Vinay and our own experiments [17] E.Yom-Tov, S.Fine, D.Carmel, A.Darlow, Learning to Estimate Query Difficulty Including Applications to Missing Content Detection and Distributed Information retrieval, in Proceedings of SIGIR 2005"
            ],
            "original_annotated_samples": [
                "<br>query performance prediction</br> in Web Search Environments Yun Zhou and W. Bruce Croft Department of Computer Science University of Massachusetts, Amherst {yzhou, croft}@cs.umass.edu ABSTRACT Current prediction techniques, which are generally designed for content-based queries and are typically evaluated on relatively homogenous test collections of small sizes, face serious challenges in web search environments where collections are significantly more heterogeneous and different types of retrieval tasks exist.",
                "INTRODUCTION <br>query performance prediction</br> has many applications in a variety of information retrieval (IR) areas such as improving retrieval consistency, query refinement, and distributed IR.",
                "<br>query performance prediction</br> for the NP task is still necessary since NP retrieval performance is far from perfect.",
                "To our knowledge, little research has explicitly addressed the problem of NP-<br>query performance prediction</br>.",
                "The mixed-query situation raises new problems for <br>query performance prediction</br>."
            ],
            "translated_annotated_samples": [
                "Predicción del rendimiento de consultas en entornos de búsqueda web Yun Zhou y W. Bruce Croft Departamento de Ciencias de la Computación Universidad de Massachusetts, Amherst {yzhou, croft}@cs.umass.edu RESUMEN Las técnicas actuales de predicción, generalmente diseñadas para consultas basadas en contenido y evaluadas típicamente en colecciones de prueba relativamente homogéneas de tamaños pequeños, enfrentan serios desafíos en entornos de búsqueda web donde las colecciones son significativamente más heterogéneas y existen diferentes tipos de tareas de recuperación.",
                "La <br>predicción del rendimiento de la consulta</br> tiene muchas aplicaciones en una variedad de áreas de recuperación de información (IR), como mejorar la consistencia de la recuperación, el refinamiento de la consulta y la IR distribuida.",
                "La <br>predicción del rendimiento de la consulta</br> para la tarea de NP sigue siendo necesaria, ya que el rendimiento de recuperación de NP está lejos de ser perfecto.",
                "Hasta donde sabemos, poca investigación ha abordado explícitamente el problema de la <br>predicción del rendimiento de consultas</br> de NP.",
                "La situación de consulta mixta plantea nuevos problemas para la <br>predicción del rendimiento de la consulta</br>."
            ],
            "translated_text": "Predicción del rendimiento de consultas en entornos de búsqueda web Yun Zhou y W. Bruce Croft Departamento de Ciencias de la Computación Universidad de Massachusetts, Amherst {yzhou, croft}@cs.umass.edu RESUMEN Las técnicas actuales de predicción, generalmente diseñadas para consultas basadas en contenido y evaluadas típicamente en colecciones de prueba relativamente homogéneas de tamaños pequeños, enfrentan serios desafíos en entornos de búsqueda web donde las colecciones son significativamente más heterogéneas y existen diferentes tipos de tareas de recuperación. En este artículo, presentamos tres técnicas para abordar estos desafíos. Nos enfocamos en la predicción de rendimiento para dos tipos de consultas en entornos de búsqueda web: basadas en contenido y búsqueda de páginas por nombre. Nuestra evaluación se realiza principalmente en la colección GOV2. Además de evaluar nuestros modelos para los dos tipos de consultas por separado, consideramos una situación más desafiante y realista en la que los dos tipos de consultas se mezclan sin información previa sobre los tipos de consulta. Para ayudar en la predicción bajo la situación de consulta mixta, se adopta un clasificador de consultas novedoso. Los resultados muestran que nuestra predicción del rendimiento de la consulta web es sustancialmente más precisa que las técnicas de predicción actuales más avanzadas. Por consiguiente, nuestro artículo proporciona un enfoque práctico para predecir el rendimiento en entornos web del mundo real. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Formulación de Consultas Términos Generales Algoritmos, Experimentación, Teoría 1. La <br>predicción del rendimiento de la consulta</br> tiene muchas aplicaciones en una variedad de áreas de recuperación de información (IR), como mejorar la consistencia de la recuperación, el refinamiento de la consulta y la IR distribuida. La importancia de este problema ha sido reconocida por los investigadores en Recuperación de Información y recientemente se han propuesto varios métodos nuevos para la predicción [1, 2, 17]. La mayoría de los trabajos sobre predicción se han centrado en la tarea tradicional de recuperación ad-hoc, donde el rendimiento de la consulta se mide según la relevancia temática. Estos modelos de predicción se evalúan en colecciones de documentos de TREC que suelen consistir en no más de un millón de artículos de agencias de noticias relativamente homogéneos. Con la popularidad e influencia de la Web, las técnicas de predicción que funcionen bien para consultas de estilo web son altamente preferibles. Sin embargo, los entornos de búsqueda web plantean desafíos significativos para los modelos de predicción actuales que están principalmente diseñados para entornos tradicionales de TREC. Aquí delineamos algunos de estos desafíos. Primero, las colecciones web, que son mucho más grandes que las colecciones TREC convencionales, incluyen una variedad de documentos que son diferentes en muchos aspectos, como la calidad y el estilo. Las técnicas actuales de predicción pueden ser vulnerables a estas características de las colecciones web. Por ejemplo, la precisión de predicción reportada de la técnica de robustez de clasificación y la técnica de claridad en la colección GOV2 (una colección web grande) es significativamente peor en comparación con las otras colecciones de TREC [1]. En [2] se informa de una precisión de predicción similar en la colección GOV2 utilizando otra técnica, confirmando la dificultad de predecir el rendimiento de consultas en una gran colección web. Además, la búsqueda web va más allá del alcance de la tarea de recuperación ad hoc basada en la relevancia temática. Por ejemplo, la tarea de encontrar páginas nombradas (NP), que es una tarea de navegación, también es popular en la recuperación web. La <br>predicción del rendimiento de la consulta</br> para la tarea de NP sigue siendo necesaria, ya que el rendimiento de recuperación de NP está lejos de ser perfecto. De hecho, según el informe sobre la tarea NP del Terabyte Track de 2005 [3], aproximadamente el 40% de las consultas de prueba tienen un rendimiento deficiente (sin respuesta correcta en los primeros 10 resultados de búsqueda) incluso en la mejor ejecución del grupo líder. Hasta donde sabemos, poca investigación ha abordado explícitamente el problema de la <br>predicción del rendimiento de consultas</br> de NP. Los modelos de predicción actuales diseñados para consultas basadas en contenido serán menos efectivos para consultas de NP considerando las diferencias fundamentales entre ambas. En tercer lugar, en entornos de búsqueda web del mundo real, las consultas de los usuarios suelen ser una mezcla de diferentes tipos y, por lo general, no se dispone de información previa sobre el tipo de cada consulta. La situación de consulta mixta plantea nuevos problemas para la <br>predicción del rendimiento de la consulta</br>. ",
            "candidates": [],
            "error": [
                [
                    "predicción del rendimiento de la consulta",
                    "predicción del rendimiento de la consulta",
                    "predicción del rendimiento de consultas",
                    "predicción del rendimiento de la consulta"
                ]
            ]
        },
        "web search environment": {
            "translated_key": "entorno de búsqueda web",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Query Performance Prediction in Web Search Environments Yun Zhou and W. Bruce Croft Department of Computer Science University of Massachusetts, Amherst {yzhou, croft}@cs.umass.edu ABSTRACT Current prediction techniques, which are generally designed for content-based queries and are typically evaluated on relatively homogenous test collections of small sizes, face serious challenges in web search environments where collections are significantly more heterogeneous and different types of retrieval tasks exist.",
                "In this paper, we present three techniques to address these challenges.",
                "We focus on performance prediction for two types of queries in web search environments: content-based and Named-Page finding.",
                "Our evaluation is mainly performed on the GOV2 collection.",
                "In addition to evaluating our models for the two types of queries separately, we consider a more challenging and realistic situation that the two types of queries are mixed together without prior information on query types.",
                "To assist prediction under the mixed-query situation, a novel query classifier is adopted.",
                "Results show that our prediction of web query performance is substantially more accurate than the current stateof-the-art prediction techniques.",
                "Consequently, our paper provides a practical approach to performance prediction in realworld web settings.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval -Query formulation General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Query performance prediction has many applications in a variety of information retrieval (IR) areas such as improving retrieval consistency, query refinement, and distributed IR.",
                "The importance of this problem has been recognized by IR researchers and a number of new methods have been proposed for prediction recently [1, 2, 17].",
                "Most work on prediction has focused on the traditional ad-hoc retrieval task where query performance is measured according to topical relevance.",
                "These prediction models are evaluated on TREC document collections which typically consist of no more than one million relatively homogenous newswire articles.",
                "With the popularity and influence of the Web, prediction techniques that will work well for web-style queries are highly preferable.",
                "However, web search environments pose significant challenges to current prediction models that are mainly designed for traditional TREC settings.",
                "Here we outline some of these challenges.",
                "First, web collections, which are much larger than conventional TREC collections, include a variety of documents that are different in many aspects such as quality and style.",
                "Current prediction techniques can be vulnerable to these characteristics of web collections.",
                "For example, the reported prediction accuracy of the ranking robustness technique and the clarity technique on the GOV2 collection (a large web collection) is significantly worse compared to the other TREC collections [1].",
                "Similar prediction accuracy on the GOV2 collection using another technique is reported in [2], confirming the difficult of predicting query performance on a large web collection.",
                "Furthermore, web search goes beyond the scope of the ad-hoc retrieval task based on topical relevance.",
                "For example, the Named-Page (NP) finding task, which is a navigational task, is also popular in web retrieval.",
                "Query performance prediction for the NP task is still necessary since NP retrieval performance is far from perfect.",
                "In fact, according to the report on the NP task of the 2005 Terabyte Track [3], about 40% of the test queries perform poorly (no correct answer in the first 10 search results) even in the best run from the top group.",
                "To our knowledge, little research has explicitly addressed the problem of NP-query performance prediction.",
                "Current prediction models devised for content-based queries will be less effective for NP queries considering the fundamental differences between the two.",
                "Third, in real-world web search environments, user queries are usually a mixture of different types and prior knowledge about the type of each query is generally unavailable.",
                "The mixed-query situation raises new problems for query performance prediction.",
                "For instance, we may need to incorporate a query classifier into prediction models.",
                "Despite these problems, the ability to handle this situation is a crucial step towards turning query performance prediction from an interesting research topic into a practical tool for web retrieval.",
                "In this paper, we present three techniques to address the above challenges that current prediction models face in Web search environments.",
                "Our work focuses on query performance prediction for the content-based (ad-hoc) retrieval task and the name-page finding task in the context of web retrieval.",
                "Our first technique, called weighted information gain (WIG), makes use of both single term and term proximity features to estimate the quality of top retrieved documents for prediction.",
                "We find that WIG offers consistent prediction accuracy across various test collections and query types.",
                "Moreover, we demonstrate that good prediction accuracy can be achieved for the mixed-query situation by using WIG with the help of a query type classifier.",
                "Query feedback and first rank change, which are our second and third prediction techniques, perform well for content-based queries and NP queries respectively.",
                "Our main contributions include: (1) considerably improved prediction accuracy for web content-based queries over several state-of-the-art techniques. (2) new techniques for successfully predicting NP-query performance. (3) a practical and fully automatic solution to predicting mixed-query performance.",
                "In addition, one minor contribution is that we find that the robustness score [1], which was originally proposed for performance prediction, is helpful for query classification. 2.",
                "RELATED WORK As we mentioned in the introduction, a number of prediction techniques have been proposed recently that focus on contentbased queries in the topical relevance (ad-hoc) task.",
                "We know of no published work that addresses other types of queries such as NP queries, let alone a mixture of query types.",
                "Next we review some representative models.",
                "The major difficulty of performance prediction comes from the fact that many factors have an impact on retrieval performance.",
                "Each factor affects performance to a different degree and the overall effect is hard to predict accurately.",
                "Therefore, it is not surprising to notice that simple features, such as the frequency of query terms in the collection [4] and the average IDF of query terms [5], do not predict well.",
                "In fact, most of the successful techniques are based on measuring some characteristics of the retrieved document set to estimate topic difficulty.",
                "For example, the clarity score [6] measures the coherence of a list of documents by the KL-divergence between the query model and the collection model.",
                "The robustness score [1] quantifies another property of a ranked list: the robustness of the ranking in the presence of uncertainty.",
                "Carmel et al. [2] found that the distance measured by the Jensen-Shannon divergence between the retrieved document set and the collection is significantly correlated to average precision.",
                "Vinay et al. [7] proposed four measures to capture the geometry of the top retrieved documents for prediction.",
                "The most effective measure is the sensitivity to document perturbation, an idea somewhat similar to the robustness score.",
                "Unfortunately, their way of measuring the sensitivity does not perform equally well for short queries and prediction accuracy drops considerably when a state-of-the-art retrieval technique (like Okapi or a language modeling approach) is adopted for retrieval instead of the tf-idf weighting used in their paper [16].",
                "The difficulties of applying these models in web search environments have already been mentioned.",
                "In this paper, we mainly adopt the clarity score and the robustness score as our baselines.",
                "We experimentally show that the baselines, even after being carefully tuned, are inadequate for the web environment.",
                "One of our prediction models, WIG, is related to the Markov random field (MRF) model for information retrieval [8].",
                "The MRF model directly models term dependence and is found be to highly effective across a variety of test collections (particularly web collections) and retrieval tasks.",
                "This model is used to estimate the joint probability distribution over documents and queries, an important part of WIG.",
                "The superiority of WIG over other prediction techniques based on unigram features, which will be demonstrated later in our paper, coincides with that of MRF for retrieval.",
                "In other word, it is interesting to note that term dependence, when being modeled appropriately, can be helpful for both improving and predicting retrieval performance. 3.",
                "PREDICTION MODELS 3.1 Weighted Information Gain (WIG) This section introduces a weighted information gain approach that incorporates both single term and proximity features for predicting performance for both content-based and Named-Page (NP) finding queries.",
                "Given a set of queries Q={Qs} (s=1,2,..N) which includes all possible user queries and a set of documents D={Dt} (t=1,2…M), we assume that each query-document pair (Qs,Dt) is manually judged and will be put in a relevance list if Qs is found to be relevant to Dt.",
                "The joint probability P(Qs,Dt) over queries Q and documents D denotes the probability that pair (Qs,Dt) will be in the relevance list.",
                "Such assumptions are similar to those used in [8].",
                "Assuming that the user issues query Qi ∈Q and the retrieval results in response to Qi is a ranked list L of documents, we calculate the amount of information contained in P(Qs,Dt) with respect to Qi and L by Eq.1 which is a variant of entropy called the weighted entropy[13].",
                "The weights in Eq.1 are solely determined by Qi and L. )1(),(log),(),( , , ∑−= ts tststsLQ DQPDQweightDQH i In this paper, we choose the weights as follows: LindocumentsKtopthecontainsLTwhere otherwise LTDandisifK DQweight K Kt ts )( )2( ,0 )(,/1 ),( ⎩ ⎨ ⎧ ∈= = The cutoff rank K is a parameter in our model that will be discussed later.",
                "Accordingly, Eq.1 can be simplified as follows: )3(),(log 1 ),( )( , ∑∈ −= LTD titsLQ Kt i DQP K DQH Unfortunately, weighted entropy ),(, tsLQ DQH i computed by Eq.3, which represents the amount of information about how likely the top ranked documents in L would be relevant to query Qi on average, cannot be compared across different queries, making it inappropriate for directly predicting query performance.",
                "To mitigate this problem, we come up with a background distribution P(Qs,C) over Q and D by imagining that every document in D is replaced by the same special document C which represents average language usage.",
                "In this paper, C is created by concatenating every document in D. Roughly speaking, C is the collection (the document set) {Dt} without document boundaries.",
                "Similarly, weighted entropy ),(, CQH sLQi calculated by Eq.3 represents the amount of information about how likely an average document (represented by the whole collection) would be relevant to query Qi.",
                "Now we introduce our performance predictor WIG which is the weighted information gain [13] computed as the difference between ),(, tsLQ DQH i and ),(, CQH sLQi .Specifically, given query Qi, collection C and ranked list L of documents, WIG is calculated as follows: )4( ),( ),( log 1 ),( ),( log),( ),(),(),,( )(, ,, ∑∑ ∈ == −= LTD i ti ts s ts ts tsLQsLQi Kt ii CQP DQP KCQP DQP DQweight DQHCQHLCQWIG WIG computed by Eq.4 measures the change in information about the quality of retrieval (in response to query Qi) from an imaginary state that only an average document is retrieved to a posterior state that the actual search results are observed.",
                "We hypothesize that WIG is positively correlated with retrieval effectiveness because high quality retrieval should be much more effective than just returning the average document.",
                "The heart of this technique is how to estimate the joint distribution P(Qs,Dt).",
                "In the language modeling approach to IR, a variety of models can be applied readily to estimate this distribution.",
                "Although most of these models are based on the bagof-words assumption, recent work on modeling term dependence under the language modeling framework have shown consistent and significant improvements in retrieval effectiveness over bagof-words models.",
                "Inspired by the success of incorporating term proximity features into language models, we decide to adopt a good dependence model to estimate the probability P(Qs,Dt).",
                "The model we chose for this paper is Metzler and Crofts Markov Random Field (MRF) model, which has already demonstrated superiority over a number of collections and different retrieval tasks [8,9].",
                "According to the MRF model, log P(Qi, Dt) can be written as )5()|(loglog),(log )( 1 ∑∈ +−= iQF tti DPZDQP ξ ξ ξλ where Z1 is a constant that ensures that P(Qi, Dt) sums up to 1.",
                "F(Qi) consists of a set of features expanded from the original query Qi .",
                "For example, assuming that query Qi is talented student program, F(Qi) includes features like program and talented student.",
                "We consider two kinds of features: single term features T and proximity features P. Proximity features include exact phrase (#1) and unordered window (#uwN) features as described in [8].",
                "Note that F(Qi) is the union of T(Qi) and P(Qi).",
                "For more details on F(Qi) such as how to expand the original query Qi to F(Qi), we refer the reader to [8] and [9].",
                "P(ξ|Dt) denotes the probability that feature ξ will occur in Dt.",
                "More details on P(ξ|Dt) will be provided later in this section.",
                "The choice of λξ is somewhat different from that used in [8] since λξ plays a dual role in our model.",
                "The first role, which is the same as in [8], is to weight between single term and proximity features.",
                "The other role, which is specific to our prediction task, is to normalize the size of F(Qi).We found that the following weight strategy for λξ satisfies the above two roles and generalizes well on a variety of collections and query types. )6( )(, |)(| 1 )(, |)(| ⎪ ⎪ ⎩ ⎪ ⎪ ⎨ ⎧ ∈ − ∈ = i i T i i T QP QP QT QT ξ λ ξ λ λξ where |T(Qi)| and |P(Qi)| denote the number of single term and proximity features in F(Qi) respectively.",
                "The reason for choosing the square root function in the denominator of λξ is to penalize a feature set of large size appropriately, making WIG more comparable across queries of various lengths. λT is a fixed parameter and set to 0.8 according to [8] throughout this paper.",
                "Similarly, log P(Qi,C) can be written as: )7()|(loglog),(log )( 2 ∑∈ +−= iQF i CPZCQP ξ ξ ξλ When constant Z1 and Z2 are dropped, WIG computed in Eq.4 can be rewritten as follows by plugging in Eq.5 and Eq.7 : )8( )|( )|( log 1 ),,( )( )( ∑ ∑∈ ∈ = LTD QF t i Kt i CP DP K LCQWIG ξ ξ ξ ξ λ One of the advantages of WIG over other techniques is that it can handle well both content-based and NP queries.",
                "Based on the type (or the predicted type) of Qi, the calculation of WIG in Eq. 8 differs in two aspects: (1) how to estimate P(ξ|Dt) and P(ξ|C), and (2) how to choose K. For content-based queries, P(ξ|C) is estimated by the relative frequency of feature ξ in collection C as a whole.",
                "The estimation of P(ξ|Dt) is the same as in [8].",
                "Namely, we estimate P(ξ|Dt) by the relative frequency of feature ξ in Dt linearly smoothed with collection frequency P(ξ|C).",
                "K in Eq.8 is treated as a free parameter.",
                "Note that K is the only free parameter in the computation of WIG for content-based queries because all parameters involved in P(ξ|Dt) are assumed to be fixed by taking the suggested values in [8].",
                "Regarding NP queries, we make use of document structure to estimate P(ξ|Dt) and P(ξ|C) by the so-called mixture of language models proposed in [10] and incorporated into the MRF model for Named-Page finding retrieval in [9].",
                "The basic idea is that a document (collection) is divided into several fields such as the title field, the main-body field and the heading field.",
                "P(ξ|Dt) and P(ξ|C) are estimated by a linear combination of the language models from each field.",
                "Due to space constraints, we refer the reader to [9] for details.",
                "We adopt the exact same set of parameters as used in [9] for estimation.",
                "With regard to K in Eq.8, we set K to 1 because the Named-Page finding task heavily focuses on the first ranked document.",
                "Consequently, there are no free parameters in the computation of WIG for NP queries. 3.2 Query Feedback In this section, we introduce another technique called query feedback (QF) for prediction.",
                "Suppose that a user issues query Q to a retrieval system and a ranked list L of documents is returned.",
                "We view the retrieval system as a noisy channel.",
                "Specifically, we assume that the output of the channel is L and the input is Q.",
                "After going through the channel, Q becomes corrupted and is transformed to ranked list L. By thinking about the retrieval process this way, the problem of predicting retrieval effectiveness turns to the task of evaluating the quality of the channel.",
                "In other words, prediction becomes finding a way to measure the degree of corruption that arises when Q is transformed to L. As directly computing the degree of the corruption is difficult, we tackle this problem by approximation.",
                "Our main idea is that we measure to what extent information on Q can be recovered from L on the assumption that only L is observed.",
                "Specifically, we design a decoder that can accurately translate L back into new query Q and the similarity S between the original query Q and the new query Q is adopted as a performance predictor.",
                "This is a sketch of how the QF technique predicts query performance.",
                "Before filling in more details, we briefly discuss why this method would work.",
                "There is a relation between the similarity S defined above and retrieval performance.",
                "On the one hand, if the retrieval has strayed from the original sense of the query Q, the new query Q extracted from ranked list L in response to Q would be very different from the original query Q.",
                "On the other hand, a query distilled from a ranked list containing many relevant documents is likely to be similar to the original query.",
                "Further examples in support of the relation will be provided later.",
                "Next we detail how to build the decoder and how to measure the similarity S. In essence, the goal of the decoder is to compress ranked list L into a few informative terms that should represent the content of the top ranked documents in L. Our approach to this goal is to represent ranked list L by a language model (distribution over terms).",
                "Then terms are ranked by their contribution to the language models KL (Kullback-Leibler) divergence from the background collection model.",
                "Top ranked terms will be chosen to form the new query Q.",
                "This approach is similar to that used in Section 4.1 of [11].",
                "Specifically, we take three steps to compress ranked list L into query Q without referring to the original query. 1.",
                "We adopt the ranked list language model [14], to estimate a language model based on ranked list L. The model can be written as: )9()|()|()|( ∑∈ = LD LDPDwPLwP where w is any term, D is a document.",
                "P(D|L) is estimated by a linearly decreasing function of the rank of document D. 2.",
                "Each term in P(w|L) is ranked by the following KL-divergence contribution: )10( )|( )|( log)|( CwP LwP LwP where P(w|C) is the collection model estimated by the relative frequency of term w in collection C as a whole. 3.",
                "The top N ranked terms by Eq.10 form a weighted query Q={(wi,ti)} i=1,N. where wi denotes the i-th ranked term and weight ti is the KL-divergence contribution of wi in Eq. 10.",
                "Term cruise ship vessel sea passenger KL contribution 0.050 0.040 0.012 0.010 0.009 Table 1: top 5 terms compressed from the ranked list in response to query Cruise ship damage sea life Two representative examples, one for a poorly performing query Cruise ship damage sea life (TREC topic 719; average precision: 0.08) and the other for a high performing query prostate cancer treatments( TREC topic 710; average precision: 0.49), are shown in Table 1 and 2 respectively.",
                "These examples indicate how the similarity between the original and the new query correlates with retrieval performance.",
                "The parameter N in step 3 is set to 20 empirically and choosing a larger value of N is unnecessary since the weights after the top 20 are usually too small to make any difference.",
                "Term prostate cancer treatment men therapy KL contribution 0.177 0.140 0.028 0.025 0.020 Table 2: top 5 terms compressed from the ranked list in response to query prostate cancer treatments To measure the similarity between original query Q and new query Q, we first use Q to do retrieval on the same collection.",
                "A variant of the query likelihood model [15] is adopted for retrieval.",
                "Namely, documents are ranked by: )11()|()|( ),( ∑∈ = Qtw t i ii i DwPDQP where wi is a term in Q and ti is the associated weight.",
                "D is a document.",
                "Let L denote the new ranked list returned from the above retrieval.",
                "The similarity is measured by the overlap of documents in L and L. Specifically, the percentage of documents in the top K documents of L that are also present in the top K documents in L. the cutoff K is treated as a free parameter.",
                "We summarize here how the QF technique predicts performance given a query Q and the associated ranked list L. We first obtain a weighted query Q compressed from L by the above three steps.",
                "Then we use Q to perform retrieval and the new ranked list is L. The overlap of documents in L and L is used for prediction. 3.3 First Rank Change (FRC) In this section, we propose a method called the first rank change (FRC) for performance prediction for NP queries.",
                "This method is derived from the ranking robustness technique [1] that is mainly designed for content-based queries.",
                "When directly applied to NP queries, the robustness technique will be less effective because it takes the top ranked documents as a whole into account while NP queries usually have only one single relevant document.",
                "Instead, our technique focuses on the first rank document while the main idea of the robustness method remains.",
                "Specifically, the pseudocode for computing FRC is shown in figure 1.",
                "Input: (1) ranked list L={Di} where i=1,100.",
                "Di denotes the i-th ranked document. (2) query Q 1 initialize: (1) set the number of trials J=100000 (2) counter c=0; 2 for i=1 to J 3 Perturb every document in L, let the outcome be a set F={Di} where Di denotes the perturbed version of Di. 4 Do retrieval with query Q on set F 5 c=c+1 if and only if D1 is ranked first in step 4 6 end of for 7 return the ratio c/J Figure 1: pseudo-code for computing FRC FRC approximates the probability that the first ranked document in the original list L will remain ranked first even after the documents are perturbed.",
                "The higher the probability is, the more confidence we have in the first ranked document.",
                "On the other hand, in the extreme case of a random ranking, the probability would be as low as 0.5.",
                "We expect that FRC has a positive association with NP query performance.",
                "We adopt [1] to implement the document perturbation step (step 4 in Fig.1) using Poisson distributions.",
                "For more details, we refer the reader to [1]. 4.",
                "EVALUATION We now present the results of predicting query performance by our models.",
                "Three state-of-the-art techniques are adopted as our baselines.",
                "We evaluate our techniques across a variety of Web retrieval settings.",
                "As mentioned before, we consider two types of queries, that is, content-based (CB) queries and Named-Page(NP) finding queries.",
                "First, suppose that the query types are known.",
                "We investigate the correlation between the predicted retrieval performance and the actual performance for both types of queries separately.",
                "Results show that our methods yield considerable improvements over the baselines.",
                "We then consider a more challenging scenario where no prior information on query types is available.",
                "Two sub-cases are considered.",
                "In the first one, there exists only one type of query but the actual type is unknown.",
                "We assume a mixture of the two query types in the second case.",
                "We demonstrate that our models achieve good accuracy under this demanding scenario, making prediction practical in a real-world <br>web search environment</br>. 4.1 Experimental Setup Our evaluation focuses on the GOV2 collection which contains about 25 million documents crawled from web sites in the .gov domain during 2004 [3].",
                "We create two kinds of data set for CB queries and NP queries respectively.",
                "For the CB type, we use the ad-hoc topics of the Terabyte Tracks of 2004, 2005 and 2006 and name them TB04-adhoc, TB05-adhoc and TB06-adhoc respectively.",
                "In addition, we also use the ad-hoc topics of the 2004 Robust Track (RT04) to test the adaptability of our techniques to a non-Web environment.",
                "For NP queries, we use the Named-Page finding topics of the Terabyte Tracks of 2005 and 2006 and we name them TB05-NP and TB06-NP respectively.",
                "All queries used in our experiments are titles of TREC topics as we center on web retrieval.",
                "Table 3 summarizes the above data sets.",
                "Name Collection Topic Number Query Type TB04-adhoc GOV2 701-750 CB TB05-adhoc GOV2 751-800 CB TB06-adhoc GOV2 801-850 CB RT04 Disk 4+5 (minus CR) 301-450;601700 CB TB05-NP GOV2 NP601-NP872 NP TB06-NP GOV2 NP901-NP1081 NP Table 3: Summary of test collections and topics Retrieval performance of individual content-based and NP queries is measured by the average precision and reciprocal rank of the first correct answer respectively.",
                "We make use of the Markov Random field model for both ad-hoc and Named-Page finding retrieval.",
                "We adopt the same setting of retrieval parameters used in [8,9].",
                "The Indri search engine [12] is used for all of our experiments.",
                "Though not reported here, we also tried the query likelihood model for ad-hoc retrieval and found that the results change little because of the very high correlation between the query performances obtained by the two retrieval models (0.96 measured by Pearsons coefficient). 4.2 Known Query Types Suppose that query types are known.",
                "We treat each type of query separately and measure the correlation with average precision (or the reciprocal rank in the case of NP queries).",
                "We adopt the Pearsons correlation test which reflects the degree of linear relationship between the predicted and the actual retrieval performance. 4.2.1 Content-based Queries Methods Clarity Robust JSD WIG QF WIG +QF TB04+0 5 adhoc 0.333 0.317 0.362 0.574 0.480 0.637 TB06 adhoc 0.076 0.294 N/A 0.464 0.422 0.511 Table 4: Pearsons correlation coefficients for correlation with average precision on the Terabyte Tracks (ad-hoc) for clarity score, robustness score, the JSD-based method(we directly cites the score reported in [2]), WIG, query feedback(QF) and a linear combination of WIG and QF.",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Table 4 shows the correlation with average precision on two data sets: one is a combination of TB04-adhoc and TB05-adhoc(100 topics in total) and the other is TB06-adhoc (50 topics).",
                "The reason that we put TB04-adhoc and TB05-adhoc together is to make our results comparable to [2].",
                "Our baselines are the clarity score (clarity) [6],the robustness score (robust)[1] and the JSDbased method (JSD) [2].",
                "For the clarity and robustness score, we have tried different parameter settings and report the highest correlation coefficients we have found.",
                "We directly cite the result of the JSD-based method reported in [2].",
                "The table also shows the results for the Weighted Information Gain (WIG) method and the Query Feedback (QF) method for predicting content-based queries.",
                "As we described in the previous section, both WIG and QF have one free parameter to set, that is, the cutoff rank K. We train the parameter on one dataset and test on the other.",
                "When combining WIG and QF, a simple linear combination is used and the combination weight is learned from the training data set.",
                "From these results, we can see that our methods are considerably more accurate compared to the baselines.",
                "We also observe that further improvements are obtained from the combination of WIG and QF, suggesting that they measure different properties of the retrieval process that relate to performance.",
                "We discover that our methods generalize well on TB06-adhoc while the correlation for the clarity score with retrieval performance on this data set is considerably worse.",
                "Further investigation shows that the mean average precision of TB06-adhoc is 0.342 and is about 10% better than that of the first data set.",
                "While the other three methods typically consider the top 100 or less documents given a ranked list, the clarity method usually needs the top 500 or more documents to adequately measure the coherence of a ranked list.",
                "Higher mean average precision makes ranked lists retrieved by different queries more similar in terms of coherence at the level of top 500 documents.",
                "We believe that this is the main reason for the low accuracy of the clarity score on the second data set.",
                "Though this paper focuses on a <br>web search environment</br>, it is desirable that our techniques will work consistently well in other situations.",
                "To this end, we examine the effectiveness of our techniques on the Robust 2004 Track.",
                "For our methods, we evenly divide all of the test queries into five groups and perform five-fold cross validation.",
                "Each time we use one group for training and the remaining four groups for testing.",
                "We make use of all of the queries for our two baselines, that is, the clarity score and the robustness score.",
                "The parameters for our baselines are the same as those used in [1].The results shown in Table 5 demonstrate that the prediction accuracy of our methods is on a par with that of the two strong baselines.",
                "Clarity Robust WIG QF 0.464 0.539 0.468 0.464 Table 5: Comparison of Pearsons correlation coefficients on the 2004 Robust Track for clarity score, robustness score, WIG and query feedback (QF).",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Furthermore, we examine the prediction sensitivity of our methods to the cutoff rank K. With respect to WIG, it is quite robust to K on the Terabyte Tracks (2004-2006) while it prefers a small value of K like 5 on the 2004 Robust Track.",
                "In other words, a small value of K is a nearly-optimal choice for both kinds of tracks.",
                "Considering the fact that all other parameters involved in WIG are fixed and consequently the same for the two cases, this means WIG can achieve nearly-optimal prediction accuracy in two considerably different situations with exactly the same parameter setting.",
                "Regarding QF, it prefers a larger value of K such as 100 on the Terabyte Tracks and a smaller value of K such as 25 on the 2004 Robust Track. 4.2.2 NP Queries We adopt WIG and first rank change (FRC) for predicting NPquery performance.",
                "We also try a linear combination of the two as in the previous section.",
                "The combination weight is obtained from the other data set.",
                "We use the correlation with the reciprocal ranks measured by the Pearsons correlation test to evaluate prediction quality.",
                "The results are presented in Table 6.",
                "Again, our baselines are the clarity score and the robustness score.",
                "To make a fair comparison, we tune the clarity score in different ways.",
                "We found that using the first ranked document to build the query model yields the best prediction accuracy.",
                "We also attempted to utilize document structure by using the mixture of language models mentioned in section 3.1.",
                "Little improvement was obtained.",
                "The correlation coefficients for the clarity score reported in Table 6 are the best we have found.",
                "As we can see, our methods considerably outperform the clarity score technique on both of the runs.",
                "This confirms our intuition that the use of a coherence-based measure like the clarity score is inappropriate for NP queries.",
                "Methods Clarity Robust.",
                "WIG FRC WIG+FRC TB05-NP 0.150 -0.370 0.458 0.440 0.525 TB06-NP 0.112 -0.160 0.478 0.386 0.515 Table 6: Pearsons correlation coefficients for correlation with reciprocal ranks on the Terabyte Tracks (NP) for clarity score, robustness score, WIG, the first rank change (FRC) and a linear combination of WIG and FRC.",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Regarding the robustness score, we also tune the parameters and report the best we have found.",
                "We observe an interesting and surprising negative correlation with reciprocal ranks.",
                "We explain this finding briefly.",
                "A high robustness score means that a number of top ranked documents in the original ranked list are still highly ranked after perturbing the documents.",
                "The existence of such documents is a good sign of high performance for content-based queries as these queries usually contain a number of relevant documents [1].",
                "However, with regard to NP queries, one fundamental difference is that there is only one relevant document for each query.",
                "The existence of such documents can confuse the ranking function and lead to low retrieval performance.",
                "Although the negative correlation with retrieval performance exists, the strength of the correlation is weaker and less consistent compared to our methods as shown in Table 6.",
                "Based on the above analysis, we can see that current prediction techniques like clarity score and robustness score that are mainly designed for content-based queries face significant challenges and are inadequate to deal with NP queries.",
                "Our two techniques proposed for NP queries consistently demonstrate good prediction accuracy, displaying initial success in solving the problem of predicting performance for NP queries.",
                "Another point we want to stress is that the WIG method works well for both types of queries, a desirable property that most prediction techniques lack. 4.3 Unknown Query Types In this section, we run two kinds of experiments without access to query type labels.",
                "First, we assume that only one type of query exists but the type is unknown.",
                "Second, we experiment on a mixture of content-based and NP queries.",
                "The following two subsections will report results for the two conditions respectively. 4.3.1 Only One Type exists We assume that all queries are of the same type, that is, they are either NP queries or content-based queries.",
                "We choose WIG to deal with this case because it shows good prediction accuracy for both types of queries in the previous section.",
                "We consider two cases: (1) CB: all 150 title queries from the ad-hoc task of the Terabyte Tracks 2004-2006 (2)NP: all 433 NP queries from the named page finding task of the Terabyte Tracks 2005 and 2006.",
                "We take a simple strategy by labeling all of the queries in each case as the same type (either NP or CB) regardless of their actual type.",
                "The computation of WIG will be based on the labeled query type instead of the actual type.",
                "There are four possibilities with respect to the relation between the actual type and the labeled type.",
                "The correlation with retrieval performance under the four possibilities is presented in Table 7.",
                "For example, the value 0.445 at the intersection between the second row and the third column shows the Pearsons correlation coefficient for correlation with average precision when the content-based queries are incorrectly labeled as the NP type.",
                "Based on these results, we recommend treating all queries as the NP type when only one query type exists and accurate query classification is not feasible, considering the risk that a large loss of accuracy will occur if NP queries are incorrectly labeled as content-based queries.",
                "These results also demonstrate the strong adaptability of WIG to different query types.",
                "CB (labeled) NP (labeled) CB (actual) 0.536 0.445 NP (actual) 0.174 0.467 Table 7: Comparison of Pearsons correlation coefficients for correlation with retrieval performance under four possibilities on the Terabyte Tracks (NP).",
                "Bold cases mean the results are statistically significant at the 0.01 level. 4.3.2 A mixture of contented-based and NP queries A mixture of the two types of queries is a more realistic situation that a Web search engine will meet.",
                "We evaluate prediction accuracy by how accurately poorly-performing queries can be identified by the prediction method assuming that actual query types are unknown (but we can predict query types).",
                "This is a challenging task because both the predicted and actual performance for one type of query can be incomparable to that for the other type.",
                "Next we discuss how to implement our evaluation.",
                "We create a query pool which consists of all of the 150 ad-hoc title queries from Terabyte Track 2004-2006 and all of the 433 NP queries from Terabyte Track 2005&2006.",
                "We divide the queries in the pool into classes: good (better than 50% of the queries of the same type in terms of retrieval performance) and bad (otherwise).",
                "According to these standards, a NP query with the reciprocal rank above 0.2 or a content-based query with the average precision above 0.315 will be considered as good.",
                "Then, each time we randomly select one query Q from the pool with probability p that Q is contented-based.",
                "The remaining queries are used as training data.",
                "We first decide the type of query Q according to a query classifier.",
                "Namely, the query classifier tells us whether query Q is NP or content-based.",
                "Based on the predicted query type and the score computed for query Q by a prediction technique, a binary decision is made about whether query Q is good or bad by comparing to the score threshold of the predicted query type obtained from the training data.",
                "Prediction accuracy is measured by the accuracy of the binary decision.",
                "In our implementation, we repeatedly take a test query from the query pool and prediction accuracy is computed as the percentage of correct decisions, that is, a good(bad) query is predicted to be good (bad).",
                "It is obvious that random guessing will lead to 50% accuracy.",
                "Let us take the WIG method for example to illustrate the process.",
                "Two WIG thresholds (one for NP queries and the other for content-based queries) are trained by maximizing the prediction accuracy on the training data.",
                "When a test query is labeled as the NP (CB) type by the query type classifier, it will be predicted to be good if and only if the WIG score for this query is above the NP (CB) threshold.",
                "Similar procedures will be taken for other prediction techniques.",
                "Now we briefly introduce the automatic query type classifier used in this paper.",
                "We find that the robustness score, though originally proposed for performance prediction, is a good indicator of query types.",
                "We find that on average content-based queries have a much higher robustness score than NP queries.",
                "For example, Figure 2 shows the distributions of robustness scores for NP and content-based queries.",
                "According to this finding, the robustness score classifier will attach a NP (CB) label to the query if the robustness score for the query is below (above) a threshold trained from the training data. 0 0.5 1 1.5 2 2.5 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 NP Content-based Figure 2: Distribution of robustness scores for NP and CB queries.",
                "The NP queries are the 252 NP topics from the 2005 Terabyte Track.",
                "The content-based queries are the 150 ad-hoc title from the Terabyte Tracks 2004-2006.",
                "The probability distributions are estimated by the Kernel density estimation method.",
                "Strategies Robust WIG-1 WIG-2 WIG-3 Optimal p=0.6 0.565 0.624 0.665 0.684 0.701 P=0.4 0.567 0.633 0.654 0.673 0.696 Table 8: Comparison of prediction accuracy for five strategies in the mixed-query situation.",
                "Two ways to sample a query from the pool: (1) the sampled query is content-based with the probability p=0.6. (that is, the query is NP with probability 0.4 ) (2) set the probability p=0.4.",
                "We consider five strategies in our experiments.",
                "In the first strategy (denoted by robust), we use the robustness score for query performance prediction with the help of a perfect query classifier that always correctly map a query into one of the two categories (that is, NP or CB).",
                "This strategy represents the level of prediction accuracy that current prediction techniques can achieve in an ideal condition that query types are known.",
                "In the next following three strategies, the WIG method is adopted for performance prediction.",
                "The difference among the three is that three different query classifiers are used for each strategy: (1) the classifier always classifies a query into the NP type. (2) the Robustness Score ProbabilityDensity classifier is the robust score classifier mentioned above. (3) the classifier is a perfect one.",
                "These three strategies are denoted by WIG-1, WIG-2 and WIG-3 respectively.",
                "The reason we are interested in WIG-1 is based on the results from section 4.3.1.",
                "In the last strategy (denoted by Optimal) which serves as an upper bound on how well we can do so far, we fully make use of our prediction techniques for each query type assuming a perfect query classifier is available.",
                "Specifically, we linearly combine WIG and QF for content-based queries and WIG and FRC for NP queries.",
                "The results for the five strategies are shown in Table 8.",
                "For each strategy, we try two ways to sample a query from the pool: (1) the sampled query is CB with probability p=0.6. (the query is NP with probability 0.4) (2) set the probability p=0.4.",
                "From Table 8 We can see that in terms of prediction accuracy WIG-2 (the WIG method with the automatic query classifier) is not only better than the first two cases, but also is close to WIG-3 where a perfect classifier is assumed.",
                "Some further improvements over WIG-3 are observed when combined with other prediction techniques.",
                "The merit of WIG-2 is that it provides a practical solution to automatically identifying poorly performing queries in a <br>web search environment</br> with mixed query types, which poses considerable obstacles to traditional prediction techniques. 5.",
                "CONCLUSIONS AND FUTURE WORK To our knowledge, our paper is the first to thoroughly explore prediction of query performance in web search environments.",
                "We demonstrated that our models resulted in higher prediction accuracy than previously published techniques not specially devised for web search scenarios.",
                "In this paper, we focus on two types of queries in web search: content-based and Named-Page (NP) finding queries, corresponding to the ad-hoc retrieval task and the Named-Page finding task respectively.",
                "For both types of web queries, our prediction models were shown to be substantially more accurate than the current state-of-the-art techniques.",
                "Furthermore, we considered a more realistic case that no prior information on query types is available.",
                "We demonstrated that the WIG method is particularly suitable for this situation.",
                "Considering the adaptability of WIG to a range of collections and query types, one of our future plans is to apply this method to predict user preference of search results on realistic data collected from a commercial search engine.",
                "Other than accuracy, another major issue that prediction techniques have to deal with in a Web environment is efficiency.",
                "Fortunately, since the WIG score is computed just over the terms and the phrases that appear in the query, this calculation can be made very efficient with the support of index.",
                "On the other hand, the computation of QF and FRC is relatively less efficient since QF needs to retrieve the whole collection twice and FRC needs to repeatedly rank the perturbed documents.",
                "How to improve the efficiency of QF and FRC is our future work.",
                "In addition, the prediction techniques proposed in this paper have the potential of improving retrieval performance by combining with other IR techniques.",
                "For example, our techniques can be incorporated to popular query modification techniques such as query expansion and query relaxation.",
                "Guided by performance prediction, we can make a better decision on when to or how to modify queries to enhance retrieval effectiveness.",
                "We would like to carry out research in this direction in the future. 6.",
                "ACKNOWLEGMENTS This work was supported in part by the Center for Intelligent Information Retrieval, in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C0023, and in part by an award from Google.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor.",
                "In addition, we thank Donald Metzler for his valuable comments on this work. 7.",
                "REFERENCES [1] Y. Zhou ,W. B. Croft ,Ranking Robustness: A Novel Framework to Predict Query Performance, in Proceedings of CIKM 2006. [2] D.Carmel, E.Yom-Tov, A.Darlow,D.Pelleg, What Makes a Query Difficult?, in Proceedings of SIGIR 2006. [3] C.L.A.",
                "Clarke, F. Scholer, I.Soboroff, The TREC 2005 Terabyte Track, In the Online Proceedings of 2005 TREC. [4] B.",
                "He and I.Ounis.",
                "Inferring query performance using preretrieval predictors.",
                "In proceedings of the SPIRE 2004. [5] S. Tomlinson.",
                "Robust, Web and Terabyte Retrieval with Hummingbird SearchServer at TREC 2004.",
                "In the Online Proceedings of 2004 TREC. [6] S. Cronen-Townsend, Y. Zhou, W. B. Croft, Predicting Query Performance, in Proceedings of SIGIR 2002. [7] V.Vinay, I.J.Cox, N.Mill-Frayling,K.Wood, On Ranking the Effectiveness of Searcher, in Proceedings of SIGIR 2006. [8] D.Metzler, W.B.Croft, A Markov Random Filed Model for Term Dependencies, in Proceedings of SIGIR 2005. [9] D.Metzler, T.Strohman,Y.Zhou,W.B.Croft, Indri at TREC 2005: Terabyte Track, In the Online Proceedings of 2004 TREC. [10] P. Ogilvie and J. Callan, Combining document representations for known-item search, in Proceedings of SIGIR 2003. [11] A.Berger, J.Lafferty, Information retrieval as statistical translation, in Proceedings of SIGIR 1999. [12] Indri search engine : http://www.lemurproject.org/indri/ [13] I.J.",
                "Taneja: On Generalized Information Measures and Their Applications, Advances in Electronics and Electron Physics, Academic Press (USA), 76, 1989, 327-413. [14] S.Cronen-Townsend, Y. Zhou and Croft, W. B. , A Framework for Selective Query Expansion, in Proceedings of CIKM 2004. [15] F.Song, W.B.Croft, A general language model for information retrieval, in Proceedings of SIGIR 1999. [16] Personal email contact with Vishwa Vinay and our own experiments [17] E.Yom-Tov, S.Fine, D.Carmel, A.Darlow, Learning to Estimate Query Difficulty Including Applications to Missing Content Detection and Distributed Information retrieval, in Proceedings of SIGIR 2005"
            ],
            "original_annotated_samples": [
                "We demonstrate that our models achieve good accuracy under this demanding scenario, making prediction practical in a real-world <br>web search environment</br>. 4.1 Experimental Setup Our evaluation focuses on the GOV2 collection which contains about 25 million documents crawled from web sites in the .gov domain during 2004 [3].",
                "Though this paper focuses on a <br>web search environment</br>, it is desirable that our techniques will work consistently well in other situations.",
                "The merit of WIG-2 is that it provides a practical solution to automatically identifying poorly performing queries in a <br>web search environment</br> with mixed query types, which poses considerable obstacles to traditional prediction techniques. 5."
            ],
            "translated_annotated_samples": [
                "Demostramos que nuestros modelos logran una buena precisión en este escenario exigente, lo que hace que la predicción sea práctica en un <br>entorno de búsqueda web</br> del mundo real. 4.1 Configuración Experimental Nuestra evaluación se centra en la colección GOV2 que contiene alrededor de 25 millones de documentos obtenidos de sitios web en el dominio .gov durante 2004 [3].",
                "Aunque este documento se centra en un <br>entorno de búsqueda web</br>, es deseable que nuestras técnicas funcionen de manera consistente en otras situaciones.",
                "El mérito de WIG-2 es que proporciona una solución práctica para identificar automáticamente consultas con bajo rendimiento en un <br>entorno de búsqueda web</br> con tipos de consultas mixtos, lo cual representa considerables obstáculos para las técnicas de predicción tradicionales."
            ],
            "translated_text": "Predicción del rendimiento de consultas en entornos de búsqueda web Yun Zhou y W. Bruce Croft Departamento de Ciencias de la Computación Universidad de Massachusetts, Amherst {yzhou, croft}@cs.umass.edu RESUMEN Las técnicas actuales de predicción, generalmente diseñadas para consultas basadas en contenido y evaluadas típicamente en colecciones de prueba relativamente homogéneas de tamaños pequeños, enfrentan serios desafíos en entornos de búsqueda web donde las colecciones son significativamente más heterogéneas y existen diferentes tipos de tareas de recuperación. En este artículo, presentamos tres técnicas para abordar estos desafíos. Nos enfocamos en la predicción de rendimiento para dos tipos de consultas en entornos de búsqueda web: basadas en contenido y búsqueda de páginas por nombre. Nuestra evaluación se realiza principalmente en la colección GOV2. Además de evaluar nuestros modelos para los dos tipos de consultas por separado, consideramos una situación más desafiante y realista en la que los dos tipos de consultas se mezclan sin información previa sobre los tipos de consulta. Para ayudar en la predicción bajo la situación de consulta mixta, se adopta un clasificador de consultas novedoso. Los resultados muestran que nuestra predicción del rendimiento de la consulta web es sustancialmente más precisa que las técnicas de predicción actuales más avanzadas. Por consiguiente, nuestro artículo proporciona un enfoque práctico para predecir el rendimiento en entornos web del mundo real. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Formulación de Consultas Términos Generales Algoritmos, Experimentación, Teoría 1. La predicción del rendimiento de la consulta tiene muchas aplicaciones en una variedad de áreas de recuperación de información (IR), como mejorar la consistencia de la recuperación, el refinamiento de la consulta y la IR distribuida. La importancia de este problema ha sido reconocida por los investigadores en Recuperación de Información y recientemente se han propuesto varios métodos nuevos para la predicción [1, 2, 17]. La mayoría de los trabajos sobre predicción se han centrado en la tarea tradicional de recuperación ad-hoc, donde el rendimiento de la consulta se mide según la relevancia temática. Estos modelos de predicción se evalúan en colecciones de documentos de TREC que suelen consistir en no más de un millón de artículos de agencias de noticias relativamente homogéneos. Con la popularidad e influencia de la Web, las técnicas de predicción que funcionen bien para consultas de estilo web son altamente preferibles. Sin embargo, los entornos de búsqueda web plantean desafíos significativos para los modelos de predicción actuales que están principalmente diseñados para entornos tradicionales de TREC. Aquí delineamos algunos de estos desafíos. Primero, las colecciones web, que son mucho más grandes que las colecciones TREC convencionales, incluyen una variedad de documentos que son diferentes en muchos aspectos, como la calidad y el estilo. Las técnicas actuales de predicción pueden ser vulnerables a estas características de las colecciones web. Por ejemplo, la precisión de predicción reportada de la técnica de robustez de clasificación y la técnica de claridad en la colección GOV2 (una colección web grande) es significativamente peor en comparación con las otras colecciones de TREC [1]. En [2] se informa de una precisión de predicción similar en la colección GOV2 utilizando otra técnica, confirmando la dificultad de predecir el rendimiento de consultas en una gran colección web. Además, la búsqueda web va más allá del alcance de la tarea de recuperación ad hoc basada en la relevancia temática. Por ejemplo, la tarea de encontrar páginas nombradas (NP), que es una tarea de navegación, también es popular en la recuperación web. La predicción del rendimiento de la consulta para la tarea de NP sigue siendo necesaria, ya que el rendimiento de recuperación de NP está lejos de ser perfecto. De hecho, según el informe sobre la tarea NP del Terabyte Track de 2005 [3], aproximadamente el 40% de las consultas de prueba tienen un rendimiento deficiente (sin respuesta correcta en los primeros 10 resultados de búsqueda) incluso en la mejor ejecución del grupo líder. Hasta donde sabemos, poca investigación ha abordado explícitamente el problema de la predicción del rendimiento de consultas de NP. Los modelos de predicción actuales diseñados para consultas basadas en contenido serán menos efectivos para consultas de NP considerando las diferencias fundamentales entre ambas. En tercer lugar, en entornos de búsqueda web del mundo real, las consultas de los usuarios suelen ser una mezcla de diferentes tipos y, por lo general, no se dispone de información previa sobre el tipo de cada consulta. La situación de consulta mixta plantea nuevos problemas para la predicción del rendimiento de la consulta. Por ejemplo, es posible que necesitemos incorporar un clasificador de consultas en los modelos de predicción. A pesar de estos problemas, la capacidad para manejar esta situación es un paso crucial hacia convertir la predicción del rendimiento de consultas de un tema de investigación interesante en una herramienta práctica para la recuperación web. En este artículo, presentamos tres técnicas para abordar los desafíos mencionados que enfrentan los modelos de predicción actuales en entornos de búsqueda web. Nuestro trabajo se centra en la predicción del rendimiento de consultas para la tarea de recuperación basada en contenido (ad-hoc) y la tarea de encontrar páginas por nombre en el contexto de la recuperación web. Nuestra primera técnica, llamada ganancia de información ponderada (WIG), utiliza tanto características de términos individuales como de proximidad de términos para estimar la calidad de los documentos recuperados para la predicción. Observamos que WIG ofrece una precisión de predicción consistente en diversas colecciones de pruebas y tipos de consultas. Además, demostramos que se puede lograr una buena precisión de predicción para la situación de consulta mixta utilizando WIG con la ayuda de un clasificador de tipo de consulta. La retroalimentación de consultas y el cambio de rango inicial, que son nuestras técnicas de predicción segunda y tercera, funcionan bien para consultas basadas en contenido y consultas de NP respectivamente. Nuestras principales contribuciones incluyen: (1) una precisión de predicción considerablemente mejorada para consultas basadas en contenido web en comparación con varias técnicas de vanguardia. (2) nuevas técnicas para predecir con éxito el rendimiento de consultas NP. (3) una solución práctica y totalmente automática para predecir el rendimiento de consultas mixtas. Además, una contribución menor es que encontramos que el puntaje de robustez [1], que originalmente se propuso para la predicción de rendimiento, es útil para la clasificación de consultas. 2. TRABAJO RELACIONADO Como mencionamos en la introducción, recientemente se han propuesto varias técnicas de predicción que se centran en consultas basadas en contenido en la tarea de relevancia temática (ad-hoc). No conocemos ningún trabajo publicado que aborde otros tipos de consultas como las consultas NP, y mucho menos una mezcla de tipos de consultas. A continuación revisamos algunos modelos representativos. La principal dificultad de la predicción del rendimiento proviene del hecho de que muchos factores tienen un impacto en el rendimiento de recuperación. Cada factor afecta el rendimiento en diferente medida y el efecto general es difícil de predecir con precisión. Por lo tanto, no es sorprendente notar que características simples, como la frecuencia de los términos de consulta en la colección [4] y la IDF promedio de los términos de consulta [5], no predicen bien. De hecho, la mayoría de las técnicas exitosas se basan en medir algunas características del conjunto de documentos recuperados para estimar la dificultad del tema. Por ejemplo, la puntuación de claridad [6] mide la coherencia de una lista de documentos mediante la divergencia KL entre el modelo de consulta y el modelo de colección. El puntaje de robustez [1] cuantifica otra propiedad de una lista clasificada: la robustez de la clasificación en presencia de incertidumbre. Carmel et al. [2] encontraron que la distancia medida por la divergencia de Jensen-Shannon entre el conjunto de documentos recuperados y la colección está significativamente correlacionada con la precisión promedio. Vinay et al. [7] propusieron cuatro medidas para capturar la geometría de los documentos mejor clasificados para la predicción. La medida más efectiva es la sensibilidad a la perturbación del documento, una idea algo similar a la puntuación de robustez. Desafortunadamente, su forma de medir la sensibilidad no funciona igual de bien para consultas cortas y la precisión de la predicción disminuye considerablemente cuando se adopta una técnica de recuperación de vanguardia (como Okapi o un enfoque de modelado de lenguaje) en lugar del peso tf-idf utilizado en su artículo [16]. Las dificultades de aplicar estos modelos en entornos de búsqueda web ya han sido mencionadas. En este documento, principalmente adoptamos la puntuación de claridad y la puntuación de robustez como nuestras líneas base. Experimentalmente demostramos que las líneas base, incluso después de ser ajustadas cuidadosamente, son inadecuadas para el entorno web. Uno de nuestros modelos de predicción, WIG, está relacionado con el modelo de campo aleatorio de Markov (MRF) para la recuperación de información [8]. El modelo MRF modela directamente la dependencia de términos y se ha demostrado ser altamente efectivo en una variedad de colecciones de pruebas (especialmente colecciones web) y tareas de recuperación. Este modelo se utiliza para estimar la distribución conjunta de probabilidad sobre documentos y consultas, una parte importante de WIG. La superioridad de WIG sobre otras técnicas de predicción basadas en características unigramas, que se demostrará más adelante en nuestro artículo, coincide con la de MRF para recuperación. En otras palabras, es interesante notar que la dependencia de términos, cuando se modela adecuadamente, puede ser útil tanto para mejorar como para predecir el rendimiento de recuperación. 3. Modelos de predicción 3.1 Ganancia de Información Ponderada (WIG) Esta sección introduce un enfoque de ganancia de información ponderada que incorpora tanto características de términos únicos como de proximidad para predecir el rendimiento tanto en consultas basadas en contenido como en consultas de búsqueda de Páginas Nombradas (NP). Dado un conjunto de consultas Q={Qs} (s=1,2,..N) que incluye todas las posibles consultas de usuario y un conjunto de documentos D={Dt} (t=1,2…M), asumimos que cada par consulta-documento (Qs,Dt) es evaluado manualmente y se incluirá en una lista de relevancia si se determina que Qs es relevante para Dt. La probabilidad conjunta P(Qs,Dt) sobre las consultas Q y los documentos D denota la probabilidad de que el par (Qs,Dt) esté en la lista de relevancia. Tales suposiciones son similares a las utilizadas en [8]. Suponiendo que el usuario emite la consulta Qi ∈Q y los resultados de recuperación en respuesta a Qi es una lista clasificada L de documentos, calculamos la cantidad de información contenida en P(Qs,Dt) con respecto a Qi y L mediante la Ec.1, que es una variante de la entropía llamada entropía ponderada[13]. Los pesos en la ecuación 1 están determinados únicamente por Qi y L. En este documento, elegimos los pesos de la siguiente manera: L es el número de documentos que contiene la consulta, K es el límite superior, LT es el límite superior de documentos, y D es el peso si K está en otro caso. La clasificación de corte K es un parámetro en nuestro modelo que se discutirá más adelante. Por lo tanto, la ecuación 1 se puede simplificar de la siguiente manera: )3(),(log 1 ),( )( , ∑∈ −= LTD titsLQ Kt i DQP K DQH Desafortunadamente, la entropía ponderada ),(, tsLQ DQH i calculada por la ecuación 3, que representa la cantidad de información sobre la probabilidad de que los documentos mejor clasificados en L sean relevantes para la consulta Qi en promedio, no se puede comparar entre diferentes consultas, lo que la hace inapropiada para predecir directamente el rendimiento de la consulta. Para mitigar este problema, creamos una distribución de fondo P(Qs,C) sobre Q y D imaginando que cada documento en D es reemplazado por el mismo documento especial C que representa el uso promedio del lenguaje. En este documento, C se crea concatenando cada documento en D. A grosso modo, C es la colección (el conjunto de documentos) {Dt} sin límites de documentos. De manera similar, la entropía ponderada ),(, CQH sLQi calculada por la Ecuación 3 representa la cantidad de información sobre la probabilidad de que un documento promedio (representado por toda la colección) sea relevante para la consulta Qi. Ahora presentamos nuestro predictor de rendimiento WIG que es la ganancia de información ponderada [13] calculada como la diferencia entre ),(, tsLQ DQH i y ),(, CQH sLQi. Específicamente, dado el conjunto de consultas Qi, la colección C y la lista clasificada L de documentos, WIG se calcula de la siguiente manera: )4( ),( ),( log 1 ),( ),( log),( ),(),(),,( )(, ,, ∑∑ ∈ == −= LTD i ti ts s ts ts tsLQsLQi Kt ii CQP DQP KCQP DQP DQweight DQHCQHLCQWIG WIG calculado por la Ecuación 4 mide el cambio en la información sobre la calidad de la recuperación (en respuesta a la consulta Qi) desde un estado imaginario en el que solo se recupera un documento promedio hasta un estado posterior en el que se observan los resultados de búsqueda reales. Hacemos la hipótesis de que WIG está positivamente correlacionado con la efectividad de la recuperación porque una recuperación de alta calidad debería ser mucho más efectiva que simplemente devolver el documento promedio. El corazón de esta técnica es cómo estimar la distribución conjunta P(Qs, Dt). En el enfoque de modelado del lenguaje para la recuperación de información, se pueden aplicar fácilmente una variedad de modelos para estimar esta distribución. Aunque la mayoría de estos modelos se basan en la suposición de la bolsa de palabras, trabajos recientes sobre modelado de dependencia de términos bajo el marco de modelado de lenguaje han demostrado mejoras consistentes y significativas en la efectividad de recuperación sobre los modelos de bolsa de palabras. Inspirados por el éxito de incorporar características de proximidad de términos en modelos de lenguaje, decidimos adoptar un buen modelo de dependencia para estimar la probabilidad P(Qs,Dt). El modelo que elegimos para este artículo es el modelo de Campo Aleatorio de Markov (MRF) de Metzler y Crofts, que ya ha demostrado su superioridad en comparación con varias colecciones y diferentes tareas de recuperación [8,9]. Según el modelo MRF, log P(Qi, Dt) se puede escribir como )5()|(loglog),(log )( 1 ∑∈ +−= iQF tti DPZDQP ξ ξ ξλ donde Z1 es una constante que asegura que P(Qi, Dt) sume 1. F(Qi) consiste en un conjunto de características ampliadas a partir de la consulta original Qi. Por ejemplo, suponiendo que la consulta Qi es el programa de estudiantes talentosos, F(Qi) incluye características como programa y estudiante talentoso. Consideramos dos tipos de características: características de términos individuales T y características de proximidad P. Las características de proximidad incluyen la frase exacta (#1) y las características de ventana desordenada (#uwN) como se describe en [8]. Ten en cuenta que F(Qi) es la unión de T(Qi) y P(Qi). Para obtener más detalles sobre F(Qi), como por ejemplo cómo expandir la consulta original Qi a F(Qi), remitimos al lector a [8] y [9]. P(ξ|Dt) denota la probabilidad de que la característica ξ ocurra en Dt. Más detalles sobre P(ξ|Dt) se proporcionarán más adelante en esta sección. La elección de λξ es algo diferente a la utilizada en [8] ya que λξ desempeña un doble papel en nuestro modelo. El primer rol, que es el mismo que en [8], es ponderar entre las características de términos individuales y de proximidad. El otro rol, específico de nuestra tarea de predicción, es normalizar el tamaño de F(Qi). Encontramos que la siguiente estrategia de peso para λξ satisface los dos roles anteriores y generaliza bien en una variedad de colecciones y tipos de consultas. )6( )(, |)(| 1 )(, |)(| ⎪ ⎪ ⎩ ⎪ ⎪ ⎨ ⎧ ∈ − ∈ = i i T i i T QP QP QT QT ξ λ ξ λ λξ donde |T(Qi)| y |P(Qi)| denotan el número de características de términos únicos y de proximidad en F(Qi) respectivamente. La razón de elegir la función raíz cuadrada en el denominador de λξ es penalizar adecuadamente un conjunto de características de gran tamaño, haciendo que WIG sea más comparable entre consultas de diversas longitudes. λT es un parámetro fijo y se establece en 0.8 según [8] a lo largo de este documento. De manera similar, log P(Qi,C) se puede escribir como: )7()|(loglog),(log )( 2 ∑∈ +−= iQF i CPZCQP ξ ξ ξλ Cuando se eliminan las constantes Z1 y Z2, el WIG calculado en la Ecuación 4 se puede reescribir de la siguiente manera al sustituir la Ecuación 5 y la Ecuación 7: )8( )|( )|( log 1 ),,( )( )( ∑ ∑∈ ∈ = LTD QF t i Kt i CP DP K LCQWIG ξ ξ ξ ξ λ Una de las ventajas del WIG sobre otras técnicas es que puede manejar tanto consultas basadas en contenido como en NP. Basándose en el tipo (o el tipo predicho) de Qi, el cálculo de WIG en la Ecuación 8 difiere en dos aspectos: (1) cómo estimar P(ξ|Dt) y P(ξ|C), y (2) cómo elegir K. Para consultas basadas en contenido, P(ξ|C) se estima por la frecuencia relativa del rasgo ξ en la colección C en su totalidad. La estimación de P(ξ|Dt) es la misma que en [8]. Es decir, estimamos P(ξ|Dt) mediante la frecuencia relativa del rasgo ξ en Dt suavizada linealmente con la frecuencia de colección P(ξ|C). La K en la Ec.8 se trata como un parámetro libre. Ten en cuenta que K es el único parámetro libre en el cálculo de WIG para consultas basadas en contenido, ya que se asume que todos los parámetros involucrados en P(ξ|Dt) están fijos al tomar los valores sugeridos en [8]. En cuanto a las consultas de NP, hacemos uso de la estructura del documento para estimar P(ξ|Dt) y P(ξ|C) mediante el llamado modelo de mezcla de lenguaje propuesto en [10] e incorporado en el modelo MRF para la recuperación de Named-Page en [9]. La idea básica es que un documento (colección) se divide en varios campos como el campo del título, el campo del cuerpo principal y el campo de los encabezados. P(ξ|Dt) y P(ξ|C) se estiman mediante una combinación lineal de los modelos de lenguaje de cada campo. Debido a limitaciones de espacio, remitimos al lector a [9] para más detalles. Adoptamos el mismo conjunto exacto de parámetros utilizados en [9] para la estimación. En cuanto a K en la Ec.8, establecemos K en 1 porque la tarea de encontrar Named-Page se centra fuertemente en el documento clasificado en primer lugar. Por consiguiente, no hay parámetros libres en el cálculo de WIG para consultas de NP. 3.2 Retroalimentación de consultas En esta sección, presentamos otra técnica llamada retroalimentación de consultas (QF) para la predicción. Suponga que un usuario emite la consulta Q a un sistema de recuperación y se devuelve una lista clasificada L de documentos. Vemos el sistema de recuperación como un canal ruidoso. Específicamente, asumimos que la salida del canal es L y la entrada es Q. Después de pasar por el canal, Q se corrompe y se transforma en la lista clasificada L. Al pensar en el proceso de recuperación de esta manera, el problema de predecir la eficacia de la recuperación se convierte en la tarea de evaluar la calidad del canal. En otras palabras, la predicción se convierte en encontrar una forma de medir el grado de corrupción que surge cuando Q se transforma en L. Dado que calcular directamente el grado de corrupción es difícil, abordamos este problema mediante aproximaciones. Nuestra idea principal es que medimos en qué medida la información sobre Q puede ser recuperada de L bajo la suposición de que solo se observa L. Específicamente, diseñamos un decodificador que pueda traducir con precisión L de vuelta a la nueva consulta Q y la similitud S entre la consulta original Q y la nueva consulta Q se adopta como un predictor de rendimiento. Este es un bosquejo de cómo la técnica QF predice el rendimiento de la consulta. Antes de completar más detalles, discutimos brevemente por qué este método funcionaría. Existe una relación entre la similitud S definida anteriormente y el rendimiento de recuperación. Por un lado, si la recuperación se ha desviado del sentido original de la consulta Q, la nueva consulta Q extraída de la lista clasificada L en respuesta a Q sería muy diferente de la consulta original Q. Por otro lado, una consulta destilada de una lista clasificada que contiene muchos documentos relevantes es probable que sea similar a la consulta original. Se proporcionarán más ejemplos en apoyo de la relación más adelante. A continuación detallamos cómo construir el decodificador y cómo medir la similitud S. En esencia, el objetivo del decodificador es comprimir la lista clasificada L en unos pocos términos informativos que deberían representar el contenido de los documentos mejor clasificados en L. Nuestro enfoque para este objetivo es representar la lista clasificada L mediante un modelo de lenguaje (distribución de términos). Luego, los términos se clasifican según su contribución a la divergencia KL (Kullback-Leibler) de los modelos de lenguaje con respecto al modelo de la colección de fondo. Los términos mejor clasificados serán elegidos para formar la nueva consulta Q. Este enfoque es similar al utilizado en la Sección 4.1 de [11]. Específicamente, tomamos tres pasos para comprimir la lista clasificada L en la consulta Q sin hacer referencia a la consulta original. 1. Adoptamos el modelo de lenguaje de lista clasificada [14] para estimar un modelo de lenguaje basado en la lista clasificada L. El modelo puede escribirse como: )9()|()|()|( ∑∈ = LD LDPDwPLwP donde w es cualquier término y D es un documento. La probabilidad de que ocurra el evento D dado el evento L se estima mediante una función linealmente decreciente del rango del documento D. Cada término en P(w|L) está clasificado por la siguiente contribución de divergencia de Kullback-Leibler: )10( )|( )|( log)|( CwP LwP LwP donde P(w|C) es el modelo de colección estimado por la frecuencia relativa del término w en la colección C en su totalidad. 3. Los términos N principales clasificados por Eq.10 forman una consulta ponderada Q={(wi,ti)} i=1,N, donde wi denota el término clasificado en la posición i y el peso ti es la contribución de la divergencia KL de wi en Eq. 10. Barco de crucero daño vida marina. Estos ejemplos indican cómo la similitud entre la consulta original y la nueva se correlaciona con el rendimiento de recuperación. El parámetro N en el paso 3 se establece en 20 de manera empírica y elegir un valor más grande de N es innecesario ya que los pesos después de los primeros 20 suelen ser demasiado pequeños para marcar alguna diferencia. Para medir la similitud entre la consulta original Q y la nueva consulta Q, primero utilizamos Q para recuperar información de la misma colección. Se adopta una variante del modelo de probabilidad de consulta [15] para la recuperación. Es decir, los documentos se clasifican por: )11()|()|( ),( ∑∈ = Qtw t i ii i DwPDQP donde wi es un término en Q y ti es el peso asociado. D es un documento. Que L denote la nueva lista clasificada devuelta desde la recuperación anterior. La similitud se mide por la superposición de documentos en L y L. Específicamente, el porcentaje de documentos en los primeros K documentos de L que también están presentes en los primeros K documentos en L. El valor de corte K se trata como un parámetro libre. Aquí resumimos cómo la técnica QF predice el rendimiento dado una consulta Q y la lista clasificada asociada L. Primero obtenemos una consulta ponderada Q comprimida a partir de L mediante los tres pasos anteriores. Luego usamos Q para realizar la recuperación y la nueva lista clasificada es L. La superposición de documentos en L y L se utiliza para la predicción. 3.3 Primer Cambio de Rango (FRC) En esta sección, proponemos un método llamado primer cambio de rango (FRC) para la predicción de rendimiento para consultas de NP. Este método se deriva de la técnica de robustez de clasificación [1] que está principalmente diseñada para consultas basadas en contenido. Cuando se aplica directamente a consultas de NP, la técnica de robustez será menos efectiva porque tiene en cuenta todos los documentos mejor clasificados en su totalidad, mientras que las consultas de NP suelen tener solo un único documento relevante. En cambio, nuestra técnica se centra en el documento de rango uno mientras se mantiene la idea principal del método de robustez. Específicamente, el seudocódigo para calcular FRC se muestra en la figura 1. Lista clasificada L={Di} donde i=1,100. Di denota el documento clasificado en la posición i. (2) consulta Q 1 inicializar: (1) establecer el número de intentos J=100000 (2) contador c=0; 2 para i=1 a J 3 Perturbar cada documento en L, que el resultado sea un conjunto F={Di} donde Di denota la versión perturbada de Di. 4 Realizar la recuperación con la consulta Q en el conjunto F 5 c=c+1 si y solo si D1 está clasificado primero en el paso 4 6 fin del para 7 devolver la proporción c/J Figura 1: pseudo-código para calcular FRC FRC aproxima la probabilidad de que el documento clasificado en primer lugar en la lista original L permanezca clasificado primero incluso después de que los documentos sean perturbados. Cuanto mayor sea la probabilidad, más confianza tenemos en el documento clasificado en primer lugar. Por otro lado, en el caso extremo de un ranking aleatorio, la probabilidad sería tan baja como 0.5. Esperamos que FRC tenga una asociación positiva con el rendimiento de la consulta de NP. Adoptamos [1] para implementar el paso de perturbación del documento (paso 4 en la Fig. 1) utilizando distribuciones de Poisson. Para más detalles, remitimos al lector a [1]. 4. EVALUACIÓN Ahora presentamos los resultados de predecir el rendimiento de la consulta por nuestros modelos. Tres técnicas de vanguardia son adoptadas como nuestras líneas base. Evaluamos nuestras técnicas en una variedad de configuraciones de recuperación web. Como se mencionó anteriormente, consideramos dos tipos de consultas, es decir, consultas basadas en contenido (CB) y consultas de búsqueda de páginas con nombre (NP). Primero, supongamos que se conocen los tipos de consultas. Investigamos la correlación entre el rendimiento de recuperación predicho y el rendimiento real para ambos tipos de consultas por separado. Los resultados muestran que nuestros métodos producen mejoras considerablemente superiores a los valores de referencia. Luego consideramos un escenario más desafiante donde no hay información previa sobre los tipos de consultas disponibles. Se consideran dos subcasos. En el primero, existe solo un tipo de consulta pero el tipo real es desconocido. Suponemos una mezcla de los dos tipos de consultas en el segundo caso. Demostramos que nuestros modelos logran una buena precisión en este escenario exigente, lo que hace que la predicción sea práctica en un <br>entorno de búsqueda web</br> del mundo real. 4.1 Configuración Experimental Nuestra evaluación se centra en la colección GOV2 que contiene alrededor de 25 millones de documentos obtenidos de sitios web en el dominio .gov durante 2004 [3]. Creamos dos tipos de conjuntos de datos para consultas de CB y consultas de NP respectivamente. Para el tipo CB, utilizamos los temas ad-hoc de las pistas Terabyte de 2004, 2005 y 2006 y los nombramos TB04-adhoc, TB05-adhoc y TB06-adhoc respectivamente. Además, también utilizamos los temas ad-hoc de la pista Robusta de 2004 (RT04) para probar la adaptabilidad de nuestras técnicas a un entorno no web. Para consultas de NP, utilizamos los temas de búsqueda de páginas nombradas de las Terabyte Tracks de 2005 y 2006 y los denominamos TB05-NP y TB06-NP respectivamente. Todas las consultas utilizadas en nuestros experimentos son títulos de temas de TREC, ya que nos centramos en la recuperación web. La Tabla 3 resume los conjuntos de datos anteriores. La recuperación del rendimiento de las consultas individuales basadas en contenido y NP se mide mediante la precisión promedio y la tasa recíproca de la primera respuesta correcta, respectivamente. Hacemos uso del modelo de campo aleatorio de Markov tanto para la recuperación ad-hoc como para la recuperación de páginas nombradas. Adoptamos la misma configuración de parámetros de recuperación utilizada en [8,9]. El motor de búsqueda Indri [12] se utiliza para todos nuestros experimentos. Aunque no se informa aquí, también probamos el modelo de probabilidad de consulta para la recuperación ad-hoc y encontramos que los resultados cambian poco debido a la correlación muy alta entre el rendimiento de las consultas obtenido por los dos modelos de recuperación (0.96 medido por el coeficiente de Pearson). 4.2 Tipos de Consultas Conocidos Supongamos que se conocen los tipos de consultas. Tratamos cada tipo de consulta por separado y medimos la correlación con la precisión promedio (o el rango recíproco en el caso de consultas NP). Adoptamos la prueba de correlación de Pearson que refleja el grado de relación lineal entre el rendimiento de recuperación predicho y real. 4.2.1 Métodos de Consultas Basadas en Contenido Claridad Robusta JSD WIG QF WIG +QF TB04+0 5 adhoc 0.333 0.317 0.362 0.574 0.480 0.637 TB06 adhoc 0.076 0.294 N/A 0.464 0.422 0.511 Tabla 4: Coeficientes de correlación de Pearson para la correlación con la precisión promedio en las Pistas de Terabyte (ad-hoc) para la puntuación de claridad, puntuación de robustez, el método basado en JSD (citamos directamente la puntuación informada en [2]), WIG, retroalimentación de consulta (QF) y una combinación lineal de WIG y QF. Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. La Tabla 4 muestra la correlación con la precisión promedio en dos conjuntos de datos: uno es una combinación de TB04-adhoc y TB05-adhoc (100 temas en total) y el otro es TB06-adhoc (50 temas). La razón por la que juntamos TB04-adhoc y TB05-adhoc es para hacer nuestros resultados comparables a [2]. Nuestros puntos de referencia son la puntuación de claridad (claridad) [6], la puntuación de robustez (robust) [1] y el método basado en JSD (JSD) [2]. Para la puntuación de claridad y robustez, hemos probado diferentes configuraciones de parámetros y reportamos los coeficientes de correlación más altos que hemos encontrado. Citamos directamente el resultado del método basado en JSD reportado en [2]. La tabla también muestra los resultados para el método de Ganancia de Información Ponderada (WIG) y el método de Retroalimentación de Consulta (QF) para predecir consultas basadas en contenido. Como describimos en la sección anterior, tanto WIG como QF tienen un parámetro libre para ajustar, es decir, el rango de corte K. Entrenamos el parámetro en un conjunto de datos y lo probamos en el otro. Al combinar WIG y QF, se utiliza una combinación lineal simple y el peso de la combinación se aprende a partir del conjunto de datos de entrenamiento. A partir de estos resultados, podemos ver que nuestros métodos son considerablemente más precisos en comparación con los baselines. También observamos que se obtienen mejoras adicionales a partir de la combinación de WIG y QF, lo que sugiere que miden diferentes propiedades del proceso de recuperación que se relacionan con el rendimiento. Descubrimos que nuestros métodos se generalizan bien en TB06-adhoc, mientras que la correlación del puntaje de claridad con el rendimiento de recuperación en este conjunto de datos es considerablemente peor. Una investigación adicional muestra que la precisión promedio media de TB06-adhoc es de 0.342 y es aproximadamente un 10% mejor que la del primer conjunto de datos. Mientras que los otros tres métodos suelen considerar los 100 mejores documentos o menos de una lista clasificada, el método de claridad generalmente necesita los 500 mejores documentos o más para medir adecuadamente la coherencia de una lista clasificada. Un promedio de precisión media más alto hace que las listas clasificadas recuperadas por diferentes consultas sean más similares en términos de coherencia en el nivel de los primeros 500 documentos. Creemos que esta es la razón principal de la baja precisión de la puntuación de claridad en el segundo conjunto de datos. Aunque este documento se centra en un <br>entorno de búsqueda web</br>, es deseable que nuestras técnicas funcionen de manera consistente en otras situaciones. Con este fin, examinamos la efectividad de nuestras técnicas en la pista Robust 2004. Para nuestros métodos, dividimos equitativamente todas las consultas de prueba en cinco grupos y realizamos validación cruzada de cinco pliegues. Cada vez que utilizamos un grupo para el entrenamiento y los otros cuatro grupos para las pruebas. Hacemos uso de todas las consultas para nuestros dos puntos de referencia, es decir, la puntuación de claridad y la puntuación de robustez. Los parámetros para nuestras líneas base son los mismos que los utilizados en [1]. Los resultados mostrados en la Tabla 5 demuestran que la precisión de predicción de nuestros métodos está a la par con la de las dos líneas base sólidas. Claridad Robusta WIG QF 0.464 0.539 0.468 0.464 Tabla 5: Comparación de los coeficientes de correlación de Pearson en la pista Robusta de 2004 para la puntuación de claridad, puntuación de robustez, WIG y retroalimentación de consulta (QF). Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. Además, examinamos la sensibilidad de predicción de nuestros métodos al rango de corte K. Con respecto a WIG, es bastante robusto a K en las Pistas de Terabyte (2004-2006) mientras prefiere un valor pequeño de K como 5 en la Pista Robusta de 2004. En otras palabras, un valor pequeño de K es una elección casi óptima para ambos tipos de pistas. Teniendo en cuenta que todos los demás parámetros involucrados en WIG están fijos y, consecuentemente, son los mismos para los dos casos, esto significa que WIG puede lograr una precisión de predicción casi óptima en dos situaciones considerablemente diferentes con exactamente la misma configuración de parámetros. En cuanto a QF, prefiere un valor más grande de K, como 100 en las Pistas de Terabyte y un valor más pequeño de K, como 25 en la Pista Robusta de 2004. 4.2.2 Consultas NP Adoptamos WIG y el primer cambio de rango (FRC) para predecir el rendimiento de las consultas NP. También intentamos una combinación lineal de los dos como en la sección anterior. El peso de la combinación se obtiene del otro conjunto de datos. Utilizamos la correlación con los rangos recíprocos medidos por la prueba de correlación de Pearson para evaluar la calidad de la predicción. Los resultados se presentan en la Tabla 6. Nuevamente, nuestros puntos de referencia son la puntuación de claridad y la puntuación de robustez. Para hacer una comparación justa, ajustamos la puntuación de claridad de diferentes maneras. Descubrimos que utilizar el documento clasificado en primer lugar para construir el modelo de consulta produce la mejor precisión de predicción. También intentamos utilizar la estructura del documento mediante la combinación de modelos de lenguaje mencionados en la sección 3.1. Se obtuvo una pequeña mejora. Los coeficientes de correlación para la puntuación de claridad reportados en la Tabla 6 son los mejores que hemos encontrado. Como podemos ver, nuestros métodos superan considerablemente la técnica de puntuación de claridad en ambas ejecuciones. Esto confirma nuestra intuición de que el uso de una medida basada en la coherencia como el puntaje de claridad es inapropiado para las consultas de NP. Claridad de métodos robusta. Tabla 6: Coeficientes de correlación de Pearson para la correlación con rangos recíprocos en las pistas de Terabyte (NP) para la puntuación de claridad, puntuación de robustez, WIG, el primer cambio de rango (FRC) y una combinación lineal de WIG y FRC. Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. En cuanto a la puntuación de robustez, también ajustamos los parámetros y reportamos el mejor que hemos encontrado. Observamos una correlación negativa interesante y sorprendente con los rangos recíprocos. Explicamos este hallazgo brevemente. Un puntaje de robustez alto significa que varios documentos de alta clasificación en la lista clasificada original siguen estando muy bien clasificados después de perturbar los documentos. La existencia de tales documentos es una buena señal de alto rendimiento para consultas basadas en contenido, ya que estas consultas suelen contener una serie de documentos relevantes [1]. Sin embargo, en lo que respecta a las consultas de NP, una diferencia fundamental es que solo hay un documento relevante para cada consulta. La existencia de tales documentos puede confundir la función de clasificación y llevar a un bajo rendimiento de recuperación. Aunque existe una correlación negativa con el rendimiento de recuperación, la fuerza de la correlación es más débil y menos consistente en comparación con nuestros métodos, como se muestra en la Tabla 6. Basándonos en el análisis anterior, podemos ver que las técnicas de predicción actuales como la puntuación de claridad y la puntuación de robustez, que están principalmente diseñadas para consultas basadas en contenido, enfrentan desafíos significativos y son insuficientes para manejar consultas de NP. Nuestras dos técnicas propuestas para consultas de NP demuestran consistentemente una buena precisión de predicción, mostrando un éxito inicial en la resolución del problema de predecir el rendimiento para consultas de NP. Otro punto que queremos destacar es que el método WIG funciona bien para ambos tipos de consultas, una propiedad deseable que la mayoría de las técnicas de predicción carecen. 4.3 Tipos de Consultas Desconocidos En esta sección, realizamos dos tipos de experimentos sin acceso a etiquetas de tipo de consulta. Primero, asumimos que solo existe un tipo de consulta pero el tipo es desconocido. Segundo, experimentamos con una mezcla de consultas basadas en contenido y NP. Las dos subsecciones siguientes informarán los resultados para las dos condiciones respectivamente. 4.3.1 Solo existe un tipo. Suponemos que todas las consultas son del mismo tipo, es decir, son consultas NP o consultas basadas en contenido. Elegimos WIG para tratar este caso porque muestra una buena precisión de predicción para ambos tipos de consultas en la sección anterior. Consideramos dos casos: (1) CB: todas las 150 consultas de títulos de la tarea ad-hoc de las pistas Terabyte 2004-2006 (2) NP: todas las 433 consultas de NP de la tarea de búsqueda de páginas nombradas de las pistas Terabyte 2005 y 2006. Tomamos una estrategia simple etiquetando todas las consultas en cada caso como el mismo tipo (ya sea NP o CB) independientemente de su tipo real. El cálculo de WIG se basará en el tipo de consulta etiquetado en lugar del tipo real. Hay cuatro posibilidades con respecto a la relación entre el tipo real y el tipo etiquetado. La correlación con el rendimiento de recuperación bajo las cuatro posibilidades se presenta en la Tabla 7. Por ejemplo, el valor 0.445 en la intersección entre la segunda fila y la tercera columna muestra el coeficiente de correlación de Pearson para la correlación con la precisión promedio cuando las consultas basadas en contenido se etiquetan incorrectamente como del tipo NP. Basándonos en estos resultados, recomendamos tratar todas las consultas como del tipo NP cuando solo existe un tipo de consulta y la clasificación precisa de la consulta no es factible, considerando el riesgo de que se produzca una gran pérdida de precisión si las consultas NP se etiquetan incorrectamente como consultas basadas en contenido. Estos resultados también demuestran la fuerte adaptabilidad de WIG a diferentes tipos de consultas. Tabla 7: Comparación de los coeficientes de correlación de Pearson para la correlación con el rendimiento de recuperación en cuatro posibilidades en las pistas de Terabyte (NP). Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. 4.3.2 Una mezcla de consultas basadas en contenido y NP Una mezcla de los dos tipos de consultas es una situación más realista que un motor de búsqueda web encontrará. Evaluamos la precisión de la predicción por la exactitud con la que se pueden identificar las consultas de bajo rendimiento mediante el método de predicción, asumiendo que los tipos de consulta reales son desconocidos (pero podemos predecir los tipos de consulta). Esta es una tarea desafiante porque tanto el rendimiento predicho como el real para un tipo de consulta pueden ser incomparables con el de otro tipo. A continuación discutimos cómo implementar nuestra evaluación. Creamos un conjunto de consultas que consiste en todas las 150 consultas de título ad-hoc de Terabyte Track 2004-2006 y todas las 433 consultas NP de Terabyte Track 2005&2006. Dividimos las consultas en el conjunto en clases: buenas (mejores que el 50% de las consultas del mismo tipo en términos de rendimiento de recuperación) y malas (de lo contrario). Según estos estándares, una consulta de NP con un rango recíproco superior a 0.2 o una consulta basada en contenido con una precisión promedio superior a 0.315 se considerarán como buenas. Entonces, cada vez que seleccionamos aleatoriamente una consulta Q del grupo con una probabilidad p de que Q esté basada en el contenido. Las consultas restantes se utilizan como datos de entrenamiento. Primero decidimos el tipo de consulta Q de acuerdo con un clasificador de consultas. Es decir, el clasificador de consultas nos dice si la consulta Q es de tipo NP o basada en contenido. Basándose en el tipo de consulta predicho y en la puntuación calculada para la consulta Q por una técnica de predicción, se toma una decisión binaria sobre si la consulta Q es buena o mala al compararla con el umbral de puntuación del tipo de consulta predicho obtenido de los datos de entrenamiento. La precisión de la predicción se mide por la exactitud de la decisión binaria. En nuestra implementación, tomamos repetidamente una consulta de prueba del conjunto de consultas y la precisión de la predicción se calcula como el porcentaje de decisiones correctas, es decir, una consulta buena (mala) se predice como buena (mala). Es obvio que adivinar al azar llevará a una precisión del 50%. Tomemos el método WIG como ejemplo para ilustrar el proceso. Dos umbrales de WIG (uno para consultas de NP y otro para consultas basadas en contenido) se entrenan maximizando la precisión de predicción en los datos de entrenamiento. Cuando una consulta de prueba es etiquetada como del tipo NP (CB) por el clasificador de tipo de consulta, se predecirá que es buena solo si la puntuación WIG para esta consulta está por encima del umbral de NP (CB). Se seguirán procedimientos similares para otras técnicas de predicción. Ahora presentamos brevemente el clasificador de tipo de consulta automático utilizado en este artículo. Encontramos que la puntuación de robustez, aunque originalmente propuesta para la predicción de rendimiento, es un buen indicador de tipos de consultas. Observamos que, en promedio, las consultas basadas en contenido tienen una puntuación de robustez mucho más alta que las consultas de NP. Por ejemplo, la Figura 2 muestra las distribuciones de puntajes de robustez para consultas basadas en NP y en contenido. Según este hallazgo, el clasificador de puntuación de robustez adjuntará una etiqueta NP (CB) a la consulta si la puntuación de robustez para la consulta está por debajo (por encima) de un umbral entrenado a partir de los datos de entrenamiento. 0 0.5 1 1.5 2 2.5 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 NP Basado en contenido Figura 2: Distribución de puntuaciones de robustez para consultas NP y CB. Las consultas NP son los 252 temas NP del Terabyte Track de 2005. Las consultas basadas en contenido son los 150 títulos ad-hoc de las pistas Terabyte 2004-2006. Las distribuciones de probabilidad son estimadas por el método de estimación de densidad de Kernel. Estrategias Robustas WIG-1 WIG-2 WIG-3 Óptima p=0.6 0.565 0.624 0.665 0.684 0.701 P=0.4 0.567 0.633 0.654 0.673 0.696 Tabla 8: Comparación de la precisión de predicción para cinco estrategias en la situación de consulta mixta. Dos formas de muestrear una consulta de la piscina: (1) la consulta muestreada es basada en contenido con una probabilidad p=0.6 (es decir, la consulta es NP con una probabilidad de 0.4) (2) establecer la probabilidad p=0.4. Consideramos cinco estrategias en nuestros experimentos. En la primera estrategia (denominada robusta), utilizamos la puntuación de robustez para la predicción del rendimiento de la consulta con la ayuda de un clasificador de consultas perfecto que siempre mapea correctamente una consulta en una de las dos categorías (es decir, NP o CB). Esta estrategia representa el nivel de precisión de predicción que las técnicas actuales de predicción pueden lograr en una condición ideal en la que se conocen los tipos de consulta. En las tres estrategias siguientes, se adopta el método WIG para predecir el rendimiento. La diferencia entre los tres es que se utilizan tres clasificadores de consulta diferentes para cada estrategia: (1) el clasificador siempre clasifica una consulta en el tipo NP. (2) el clasificador de Probabilidad de Densidad de Puntuación Robusta es el clasificador de puntuación robusta mencionado anteriormente. (3) el clasificador es perfecto. Estas tres estrategias están designadas como WIG-1, WIG-2 y WIG-3 respectivamente. La razón por la que estamos interesados en WIG-1 se basa en los resultados de la sección 4.3.1. En la última estrategia (denominada Óptima) que sirve como un límite superior de lo bien que podemos hacer hasta ahora, hacemos pleno uso de nuestras técnicas de predicción para cada tipo de consulta asumiendo que un clasificador de consultas perfecto está disponible. Específicamente, combinamos linealmente WIG y QF para consultas basadas en contenido y WIG y FRC para consultas de NP. Los resultados de las cinco estrategias se muestran en la Tabla 8. Para cada estrategia, intentamos dos formas de muestrear una consulta del conjunto: (1) la consulta muestreada es CB con probabilidad p=0.6 (la consulta es NP con probabilidad 0.4) (2) establecer la probabilidad p=0.4. De la Tabla 8 podemos ver que en términos de precisión de predicción, WIG-2 (el método WIG con el clasificador de consultas automático) no solo es mejor que los dos primeros casos, sino que también está cerca de WIG-3 donde se asume un clasificador perfecto. Se observan algunas mejoras adicionales sobre WIG-3 cuando se combinan con otras técnicas de predicción. El mérito de WIG-2 es que proporciona una solución práctica para identificar automáticamente consultas con bajo rendimiento en un <br>entorno de búsqueda web</br> con tipos de consultas mixtos, lo cual representa considerables obstáculos para las técnicas de predicción tradicionales. CONCLUSIONES Y TRABAJOS FUTUROS Hasta donde sabemos, nuestro artículo es el primero en explorar a fondo la predicción del rendimiento de consultas en entornos de búsqueda web. Demostramos que nuestros modelos resultaron en una mayor precisión de predicción que las técnicas previamente publicadas no especialmente diseñadas para escenarios de búsqueda en la web. En este artículo, nos enfocamos en dos tipos de consultas en la búsqueda web: consultas basadas en contenido y consultas de búsqueda de Páginas-Nombradas (NP), correspondientes a la tarea de recuperación ad-hoc y la tarea de búsqueda de Páginas-Nombradas respectivamente. Para ambos tipos de consultas web, se demostró que nuestros modelos de predicción son sustancialmente más precisos que las técnicas actuales más avanzadas. Además, consideramos un caso más realista en el que no se dispone de información previa sobre los tipos de consultas. Demostramos que el método WIG es particularmente adecuado para esta situación. Considerando la adaptabilidad de WIG a una variedad de colecciones y tipos de consultas, uno de nuestros planes futuros es aplicar este método para predecir la preferencia del usuario por los resultados de búsqueda en datos realistas recopilados de un motor de búsqueda comercial. Además de la precisión, otro problema importante con el que las técnicas de predicción tienen que lidiar en un entorno web es la eficiencia. Afortunadamente, dado que el puntaje WIG se calcula solo sobre los términos y frases que aparecen en la consulta, este cálculo puede realizarse de manera muy eficiente con el soporte de un índice. Por otro lado, el cálculo de QF y FRC es relativamente menos eficiente ya que QF necesita recuperar toda la colección dos veces y FRC necesita clasificar repetidamente los documentos perturbados. Cómo mejorar la eficiencia de QF y FRC es nuestro trabajo futuro. Además, las técnicas de predicción propuestas en este documento tienen el potencial de mejorar el rendimiento de recuperación al combinarse con otras técnicas de RI. Por ejemplo, nuestras técnicas pueden ser incorporadas a técnicas populares de modificación de consultas como la expansión de consultas y la relajación de consultas. Guiados por la predicción del rendimiento, podemos tomar una mejor decisión sobre cuándo o cómo modificar las consultas para mejorar la efectividad de la recuperación. Nos gustaría llevar a cabo investigaciones en esta dirección en el futuro. AGRADECIMIENTOS Este trabajo fue apoyado en parte por el Centro de Recuperación de Información Inteligente, en parte por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el contrato número HR0011-06-C0023, y en parte por un premio de Google. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son del autor y no necesariamente reflejan las del patrocinador. Además, agradecemos a Donald Metzler por sus valiosos comentarios sobre este trabajo. 7. REFERENCIAS [1] Y. Zhou, W. B. Croft, Ranking Robustness: Un nuevo marco para predecir el rendimiento de consultas, en Actas de CIKM 2006. [2] D. Carmel, E. Yom-Tov, A. Darlow, D. Pelleg, ¿Qué hace que una consulta sea difícil?, en Actas de SIGIR 2006. [3] C. L. A. Clarke, F. Scholer, I. Soboroff, La pista Terabyte TREC 2005, En las Actas en Línea de TREC 2005. [4] B. Él y yo. Inferir el rendimiento de la consulta utilizando predictores de preretrieval. En actas de SPIRE 2004. [5] S. Tomlinson. Recuperación robusta de la web y terabytes con Hummingbird SearchServer en TREC 2004. En las Actas en Línea de TREC 2004. [6] S. Cronen-Townsend, Y. Zhou, W. B. Croft, Prediciendo el Rendimiento de Consultas, en las Actas de SIGIR 2002. [7] V. Vinay, I. J. Cox, N. Mill-Frayling, K. Wood, Sobre la Clasificación de la Efectividad del Buscador, en las Actas de SIGIR 2006. [8] D. Metzler, W. B. Croft, Un Modelo de Campo Aleatorio de Markov para Dependencias de Términos, en las Actas de SIGIR 2005. [9] D. Metzler, T. Strohman, Y. Zhou, W. B. Croft, Indri en TREC 2005: Terabyte Track, en las Actas en Línea de TREC 2004. [10] P. Ogilvie y J. Callan, Combinando representaciones de documentos para búsqueda de elementos conocidos, en las Actas de SIGIR 2003. [11] A. Berger, J. Lafferty, Recuperación de información como traducción estadística, en las Actas de SIGIR 1999. [12] Motor de búsqueda Indri: http://www.lemurproject.org/indri/ [13] I. J. Taneja: Sobre medidas de información generalizadas y sus aplicaciones, Avances en Electrónica y Física de Electrones, Academic Press (EE. UU.), 76, 1989, 327-413. [14] S. Cronen-Townsend, Y. Zhou y Croft, W. B., Un marco para la expansión selectiva de consultas, en Actas de CIKM 2004. [15] F. Song, W. B. Croft, Un modelo de lenguaje general para la recuperación de información, en Actas de SIGIR 1999. [16] Contacto por correo electrónico personal con Vishwa Vinay y nuestros propios experimentos [17] E. Yom-Tov, S. Fine, D. Carmel, A. Darlow, Aprendiendo a estimar la dificultad de la consulta incluyendo aplicaciones para la detección de contenido faltante y la recuperación de información distribuida, en Actas de SIGIR 2005. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "content-based query": {
            "translated_key": "consulta basada en contenido",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Query Performance Prediction in Web Search Environments Yun Zhou and W. Bruce Croft Department of Computer Science University of Massachusetts, Amherst {yzhou, croft}@cs.umass.edu ABSTRACT Current prediction techniques, which are generally designed for content-based queries and are typically evaluated on relatively homogenous test collections of small sizes, face serious challenges in web search environments where collections are significantly more heterogeneous and different types of retrieval tasks exist.",
                "In this paper, we present three techniques to address these challenges.",
                "We focus on performance prediction for two types of queries in web search environments: content-based and Named-Page finding.",
                "Our evaluation is mainly performed on the GOV2 collection.",
                "In addition to evaluating our models for the two types of queries separately, we consider a more challenging and realistic situation that the two types of queries are mixed together without prior information on query types.",
                "To assist prediction under the mixed-query situation, a novel query classifier is adopted.",
                "Results show that our prediction of web query performance is substantially more accurate than the current stateof-the-art prediction techniques.",
                "Consequently, our paper provides a practical approach to performance prediction in realworld web settings.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval -Query formulation General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Query performance prediction has many applications in a variety of information retrieval (IR) areas such as improving retrieval consistency, query refinement, and distributed IR.",
                "The importance of this problem has been recognized by IR researchers and a number of new methods have been proposed for prediction recently [1, 2, 17].",
                "Most work on prediction has focused on the traditional ad-hoc retrieval task where query performance is measured according to topical relevance.",
                "These prediction models are evaluated on TREC document collections which typically consist of no more than one million relatively homogenous newswire articles.",
                "With the popularity and influence of the Web, prediction techniques that will work well for web-style queries are highly preferable.",
                "However, web search environments pose significant challenges to current prediction models that are mainly designed for traditional TREC settings.",
                "Here we outline some of these challenges.",
                "First, web collections, which are much larger than conventional TREC collections, include a variety of documents that are different in many aspects such as quality and style.",
                "Current prediction techniques can be vulnerable to these characteristics of web collections.",
                "For example, the reported prediction accuracy of the ranking robustness technique and the clarity technique on the GOV2 collection (a large web collection) is significantly worse compared to the other TREC collections [1].",
                "Similar prediction accuracy on the GOV2 collection using another technique is reported in [2], confirming the difficult of predicting query performance on a large web collection.",
                "Furthermore, web search goes beyond the scope of the ad-hoc retrieval task based on topical relevance.",
                "For example, the Named-Page (NP) finding task, which is a navigational task, is also popular in web retrieval.",
                "Query performance prediction for the NP task is still necessary since NP retrieval performance is far from perfect.",
                "In fact, according to the report on the NP task of the 2005 Terabyte Track [3], about 40% of the test queries perform poorly (no correct answer in the first 10 search results) even in the best run from the top group.",
                "To our knowledge, little research has explicitly addressed the problem of NP-query performance prediction.",
                "Current prediction models devised for content-based queries will be less effective for NP queries considering the fundamental differences between the two.",
                "Third, in real-world web search environments, user queries are usually a mixture of different types and prior knowledge about the type of each query is generally unavailable.",
                "The mixed-query situation raises new problems for query performance prediction.",
                "For instance, we may need to incorporate a query classifier into prediction models.",
                "Despite these problems, the ability to handle this situation is a crucial step towards turning query performance prediction from an interesting research topic into a practical tool for web retrieval.",
                "In this paper, we present three techniques to address the above challenges that current prediction models face in Web search environments.",
                "Our work focuses on query performance prediction for the content-based (ad-hoc) retrieval task and the name-page finding task in the context of web retrieval.",
                "Our first technique, called weighted information gain (WIG), makes use of both single term and term proximity features to estimate the quality of top retrieved documents for prediction.",
                "We find that WIG offers consistent prediction accuracy across various test collections and query types.",
                "Moreover, we demonstrate that good prediction accuracy can be achieved for the mixed-query situation by using WIG with the help of a query type classifier.",
                "Query feedback and first rank change, which are our second and third prediction techniques, perform well for content-based queries and NP queries respectively.",
                "Our main contributions include: (1) considerably improved prediction accuracy for web content-based queries over several state-of-the-art techniques. (2) new techniques for successfully predicting NP-query performance. (3) a practical and fully automatic solution to predicting mixed-query performance.",
                "In addition, one minor contribution is that we find that the robustness score [1], which was originally proposed for performance prediction, is helpful for query classification. 2.",
                "RELATED WORK As we mentioned in the introduction, a number of prediction techniques have been proposed recently that focus on contentbased queries in the topical relevance (ad-hoc) task.",
                "We know of no published work that addresses other types of queries such as NP queries, let alone a mixture of query types.",
                "Next we review some representative models.",
                "The major difficulty of performance prediction comes from the fact that many factors have an impact on retrieval performance.",
                "Each factor affects performance to a different degree and the overall effect is hard to predict accurately.",
                "Therefore, it is not surprising to notice that simple features, such as the frequency of query terms in the collection [4] and the average IDF of query terms [5], do not predict well.",
                "In fact, most of the successful techniques are based on measuring some characteristics of the retrieved document set to estimate topic difficulty.",
                "For example, the clarity score [6] measures the coherence of a list of documents by the KL-divergence between the query model and the collection model.",
                "The robustness score [1] quantifies another property of a ranked list: the robustness of the ranking in the presence of uncertainty.",
                "Carmel et al. [2] found that the distance measured by the Jensen-Shannon divergence between the retrieved document set and the collection is significantly correlated to average precision.",
                "Vinay et al. [7] proposed four measures to capture the geometry of the top retrieved documents for prediction.",
                "The most effective measure is the sensitivity to document perturbation, an idea somewhat similar to the robustness score.",
                "Unfortunately, their way of measuring the sensitivity does not perform equally well for short queries and prediction accuracy drops considerably when a state-of-the-art retrieval technique (like Okapi or a language modeling approach) is adopted for retrieval instead of the tf-idf weighting used in their paper [16].",
                "The difficulties of applying these models in web search environments have already been mentioned.",
                "In this paper, we mainly adopt the clarity score and the robustness score as our baselines.",
                "We experimentally show that the baselines, even after being carefully tuned, are inadequate for the web environment.",
                "One of our prediction models, WIG, is related to the Markov random field (MRF) model for information retrieval [8].",
                "The MRF model directly models term dependence and is found be to highly effective across a variety of test collections (particularly web collections) and retrieval tasks.",
                "This model is used to estimate the joint probability distribution over documents and queries, an important part of WIG.",
                "The superiority of WIG over other prediction techniques based on unigram features, which will be demonstrated later in our paper, coincides with that of MRF for retrieval.",
                "In other word, it is interesting to note that term dependence, when being modeled appropriately, can be helpful for both improving and predicting retrieval performance. 3.",
                "PREDICTION MODELS 3.1 Weighted Information Gain (WIG) This section introduces a weighted information gain approach that incorporates both single term and proximity features for predicting performance for both content-based and Named-Page (NP) finding queries.",
                "Given a set of queries Q={Qs} (s=1,2,..N) which includes all possible user queries and a set of documents D={Dt} (t=1,2…M), we assume that each query-document pair (Qs,Dt) is manually judged and will be put in a relevance list if Qs is found to be relevant to Dt.",
                "The joint probability P(Qs,Dt) over queries Q and documents D denotes the probability that pair (Qs,Dt) will be in the relevance list.",
                "Such assumptions are similar to those used in [8].",
                "Assuming that the user issues query Qi ∈Q and the retrieval results in response to Qi is a ranked list L of documents, we calculate the amount of information contained in P(Qs,Dt) with respect to Qi and L by Eq.1 which is a variant of entropy called the weighted entropy[13].",
                "The weights in Eq.1 are solely determined by Qi and L. )1(),(log),(),( , , ∑−= ts tststsLQ DQPDQweightDQH i In this paper, we choose the weights as follows: LindocumentsKtopthecontainsLTwhere otherwise LTDandisifK DQweight K Kt ts )( )2( ,0 )(,/1 ),( ⎩ ⎨ ⎧ ∈= = The cutoff rank K is a parameter in our model that will be discussed later.",
                "Accordingly, Eq.1 can be simplified as follows: )3(),(log 1 ),( )( , ∑∈ −= LTD titsLQ Kt i DQP K DQH Unfortunately, weighted entropy ),(, tsLQ DQH i computed by Eq.3, which represents the amount of information about how likely the top ranked documents in L would be relevant to query Qi on average, cannot be compared across different queries, making it inappropriate for directly predicting query performance.",
                "To mitigate this problem, we come up with a background distribution P(Qs,C) over Q and D by imagining that every document in D is replaced by the same special document C which represents average language usage.",
                "In this paper, C is created by concatenating every document in D. Roughly speaking, C is the collection (the document set) {Dt} without document boundaries.",
                "Similarly, weighted entropy ),(, CQH sLQi calculated by Eq.3 represents the amount of information about how likely an average document (represented by the whole collection) would be relevant to query Qi.",
                "Now we introduce our performance predictor WIG which is the weighted information gain [13] computed as the difference between ),(, tsLQ DQH i and ),(, CQH sLQi .Specifically, given query Qi, collection C and ranked list L of documents, WIG is calculated as follows: )4( ),( ),( log 1 ),( ),( log),( ),(),(),,( )(, ,, ∑∑ ∈ == −= LTD i ti ts s ts ts tsLQsLQi Kt ii CQP DQP KCQP DQP DQweight DQHCQHLCQWIG WIG computed by Eq.4 measures the change in information about the quality of retrieval (in response to query Qi) from an imaginary state that only an average document is retrieved to a posterior state that the actual search results are observed.",
                "We hypothesize that WIG is positively correlated with retrieval effectiveness because high quality retrieval should be much more effective than just returning the average document.",
                "The heart of this technique is how to estimate the joint distribution P(Qs,Dt).",
                "In the language modeling approach to IR, a variety of models can be applied readily to estimate this distribution.",
                "Although most of these models are based on the bagof-words assumption, recent work on modeling term dependence under the language modeling framework have shown consistent and significant improvements in retrieval effectiveness over bagof-words models.",
                "Inspired by the success of incorporating term proximity features into language models, we decide to adopt a good dependence model to estimate the probability P(Qs,Dt).",
                "The model we chose for this paper is Metzler and Crofts Markov Random Field (MRF) model, which has already demonstrated superiority over a number of collections and different retrieval tasks [8,9].",
                "According to the MRF model, log P(Qi, Dt) can be written as )5()|(loglog),(log )( 1 ∑∈ +−= iQF tti DPZDQP ξ ξ ξλ where Z1 is a constant that ensures that P(Qi, Dt) sums up to 1.",
                "F(Qi) consists of a set of features expanded from the original query Qi .",
                "For example, assuming that query Qi is talented student program, F(Qi) includes features like program and talented student.",
                "We consider two kinds of features: single term features T and proximity features P. Proximity features include exact phrase (#1) and unordered window (#uwN) features as described in [8].",
                "Note that F(Qi) is the union of T(Qi) and P(Qi).",
                "For more details on F(Qi) such as how to expand the original query Qi to F(Qi), we refer the reader to [8] and [9].",
                "P(ξ|Dt) denotes the probability that feature ξ will occur in Dt.",
                "More details on P(ξ|Dt) will be provided later in this section.",
                "The choice of λξ is somewhat different from that used in [8] since λξ plays a dual role in our model.",
                "The first role, which is the same as in [8], is to weight between single term and proximity features.",
                "The other role, which is specific to our prediction task, is to normalize the size of F(Qi).We found that the following weight strategy for λξ satisfies the above two roles and generalizes well on a variety of collections and query types. )6( )(, |)(| 1 )(, |)(| ⎪ ⎪ ⎩ ⎪ ⎪ ⎨ ⎧ ∈ − ∈ = i i T i i T QP QP QT QT ξ λ ξ λ λξ where |T(Qi)| and |P(Qi)| denote the number of single term and proximity features in F(Qi) respectively.",
                "The reason for choosing the square root function in the denominator of λξ is to penalize a feature set of large size appropriately, making WIG more comparable across queries of various lengths. λT is a fixed parameter and set to 0.8 according to [8] throughout this paper.",
                "Similarly, log P(Qi,C) can be written as: )7()|(loglog),(log )( 2 ∑∈ +−= iQF i CPZCQP ξ ξ ξλ When constant Z1 and Z2 are dropped, WIG computed in Eq.4 can be rewritten as follows by plugging in Eq.5 and Eq.7 : )8( )|( )|( log 1 ),,( )( )( ∑ ∑∈ ∈ = LTD QF t i Kt i CP DP K LCQWIG ξ ξ ξ ξ λ One of the advantages of WIG over other techniques is that it can handle well both content-based and NP queries.",
                "Based on the type (or the predicted type) of Qi, the calculation of WIG in Eq. 8 differs in two aspects: (1) how to estimate P(ξ|Dt) and P(ξ|C), and (2) how to choose K. For content-based queries, P(ξ|C) is estimated by the relative frequency of feature ξ in collection C as a whole.",
                "The estimation of P(ξ|Dt) is the same as in [8].",
                "Namely, we estimate P(ξ|Dt) by the relative frequency of feature ξ in Dt linearly smoothed with collection frequency P(ξ|C).",
                "K in Eq.8 is treated as a free parameter.",
                "Note that K is the only free parameter in the computation of WIG for content-based queries because all parameters involved in P(ξ|Dt) are assumed to be fixed by taking the suggested values in [8].",
                "Regarding NP queries, we make use of document structure to estimate P(ξ|Dt) and P(ξ|C) by the so-called mixture of language models proposed in [10] and incorporated into the MRF model for Named-Page finding retrieval in [9].",
                "The basic idea is that a document (collection) is divided into several fields such as the title field, the main-body field and the heading field.",
                "P(ξ|Dt) and P(ξ|C) are estimated by a linear combination of the language models from each field.",
                "Due to space constraints, we refer the reader to [9] for details.",
                "We adopt the exact same set of parameters as used in [9] for estimation.",
                "With regard to K in Eq.8, we set K to 1 because the Named-Page finding task heavily focuses on the first ranked document.",
                "Consequently, there are no free parameters in the computation of WIG for NP queries. 3.2 Query Feedback In this section, we introduce another technique called query feedback (QF) for prediction.",
                "Suppose that a user issues query Q to a retrieval system and a ranked list L of documents is returned.",
                "We view the retrieval system as a noisy channel.",
                "Specifically, we assume that the output of the channel is L and the input is Q.",
                "After going through the channel, Q becomes corrupted and is transformed to ranked list L. By thinking about the retrieval process this way, the problem of predicting retrieval effectiveness turns to the task of evaluating the quality of the channel.",
                "In other words, prediction becomes finding a way to measure the degree of corruption that arises when Q is transformed to L. As directly computing the degree of the corruption is difficult, we tackle this problem by approximation.",
                "Our main idea is that we measure to what extent information on Q can be recovered from L on the assumption that only L is observed.",
                "Specifically, we design a decoder that can accurately translate L back into new query Q and the similarity S between the original query Q and the new query Q is adopted as a performance predictor.",
                "This is a sketch of how the QF technique predicts query performance.",
                "Before filling in more details, we briefly discuss why this method would work.",
                "There is a relation between the similarity S defined above and retrieval performance.",
                "On the one hand, if the retrieval has strayed from the original sense of the query Q, the new query Q extracted from ranked list L in response to Q would be very different from the original query Q.",
                "On the other hand, a query distilled from a ranked list containing many relevant documents is likely to be similar to the original query.",
                "Further examples in support of the relation will be provided later.",
                "Next we detail how to build the decoder and how to measure the similarity S. In essence, the goal of the decoder is to compress ranked list L into a few informative terms that should represent the content of the top ranked documents in L. Our approach to this goal is to represent ranked list L by a language model (distribution over terms).",
                "Then terms are ranked by their contribution to the language models KL (Kullback-Leibler) divergence from the background collection model.",
                "Top ranked terms will be chosen to form the new query Q.",
                "This approach is similar to that used in Section 4.1 of [11].",
                "Specifically, we take three steps to compress ranked list L into query Q without referring to the original query. 1.",
                "We adopt the ranked list language model [14], to estimate a language model based on ranked list L. The model can be written as: )9()|()|()|( ∑∈ = LD LDPDwPLwP where w is any term, D is a document.",
                "P(D|L) is estimated by a linearly decreasing function of the rank of document D. 2.",
                "Each term in P(w|L) is ranked by the following KL-divergence contribution: )10( )|( )|( log)|( CwP LwP LwP where P(w|C) is the collection model estimated by the relative frequency of term w in collection C as a whole. 3.",
                "The top N ranked terms by Eq.10 form a weighted query Q={(wi,ti)} i=1,N. where wi denotes the i-th ranked term and weight ti is the KL-divergence contribution of wi in Eq. 10.",
                "Term cruise ship vessel sea passenger KL contribution 0.050 0.040 0.012 0.010 0.009 Table 1: top 5 terms compressed from the ranked list in response to query Cruise ship damage sea life Two representative examples, one for a poorly performing query Cruise ship damage sea life (TREC topic 719; average precision: 0.08) and the other for a high performing query prostate cancer treatments( TREC topic 710; average precision: 0.49), are shown in Table 1 and 2 respectively.",
                "These examples indicate how the similarity between the original and the new query correlates with retrieval performance.",
                "The parameter N in step 3 is set to 20 empirically and choosing a larger value of N is unnecessary since the weights after the top 20 are usually too small to make any difference.",
                "Term prostate cancer treatment men therapy KL contribution 0.177 0.140 0.028 0.025 0.020 Table 2: top 5 terms compressed from the ranked list in response to query prostate cancer treatments To measure the similarity between original query Q and new query Q, we first use Q to do retrieval on the same collection.",
                "A variant of the query likelihood model [15] is adopted for retrieval.",
                "Namely, documents are ranked by: )11()|()|( ),( ∑∈ = Qtw t i ii i DwPDQP where wi is a term in Q and ti is the associated weight.",
                "D is a document.",
                "Let L denote the new ranked list returned from the above retrieval.",
                "The similarity is measured by the overlap of documents in L and L. Specifically, the percentage of documents in the top K documents of L that are also present in the top K documents in L. the cutoff K is treated as a free parameter.",
                "We summarize here how the QF technique predicts performance given a query Q and the associated ranked list L. We first obtain a weighted query Q compressed from L by the above three steps.",
                "Then we use Q to perform retrieval and the new ranked list is L. The overlap of documents in L and L is used for prediction. 3.3 First Rank Change (FRC) In this section, we propose a method called the first rank change (FRC) for performance prediction for NP queries.",
                "This method is derived from the ranking robustness technique [1] that is mainly designed for content-based queries.",
                "When directly applied to NP queries, the robustness technique will be less effective because it takes the top ranked documents as a whole into account while NP queries usually have only one single relevant document.",
                "Instead, our technique focuses on the first rank document while the main idea of the robustness method remains.",
                "Specifically, the pseudocode for computing FRC is shown in figure 1.",
                "Input: (1) ranked list L={Di} where i=1,100.",
                "Di denotes the i-th ranked document. (2) query Q 1 initialize: (1) set the number of trials J=100000 (2) counter c=0; 2 for i=1 to J 3 Perturb every document in L, let the outcome be a set F={Di} where Di denotes the perturbed version of Di. 4 Do retrieval with query Q on set F 5 c=c+1 if and only if D1 is ranked first in step 4 6 end of for 7 return the ratio c/J Figure 1: pseudo-code for computing FRC FRC approximates the probability that the first ranked document in the original list L will remain ranked first even after the documents are perturbed.",
                "The higher the probability is, the more confidence we have in the first ranked document.",
                "On the other hand, in the extreme case of a random ranking, the probability would be as low as 0.5.",
                "We expect that FRC has a positive association with NP query performance.",
                "We adopt [1] to implement the document perturbation step (step 4 in Fig.1) using Poisson distributions.",
                "For more details, we refer the reader to [1]. 4.",
                "EVALUATION We now present the results of predicting query performance by our models.",
                "Three state-of-the-art techniques are adopted as our baselines.",
                "We evaluate our techniques across a variety of Web retrieval settings.",
                "As mentioned before, we consider two types of queries, that is, content-based (CB) queries and Named-Page(NP) finding queries.",
                "First, suppose that the query types are known.",
                "We investigate the correlation between the predicted retrieval performance and the actual performance for both types of queries separately.",
                "Results show that our methods yield considerable improvements over the baselines.",
                "We then consider a more challenging scenario where no prior information on query types is available.",
                "Two sub-cases are considered.",
                "In the first one, there exists only one type of query but the actual type is unknown.",
                "We assume a mixture of the two query types in the second case.",
                "We demonstrate that our models achieve good accuracy under this demanding scenario, making prediction practical in a real-world Web search environment. 4.1 Experimental Setup Our evaluation focuses on the GOV2 collection which contains about 25 million documents crawled from web sites in the .gov domain during 2004 [3].",
                "We create two kinds of data set for CB queries and NP queries respectively.",
                "For the CB type, we use the ad-hoc topics of the Terabyte Tracks of 2004, 2005 and 2006 and name them TB04-adhoc, TB05-adhoc and TB06-adhoc respectively.",
                "In addition, we also use the ad-hoc topics of the 2004 Robust Track (RT04) to test the adaptability of our techniques to a non-Web environment.",
                "For NP queries, we use the Named-Page finding topics of the Terabyte Tracks of 2005 and 2006 and we name them TB05-NP and TB06-NP respectively.",
                "All queries used in our experiments are titles of TREC topics as we center on web retrieval.",
                "Table 3 summarizes the above data sets.",
                "Name Collection Topic Number Query Type TB04-adhoc GOV2 701-750 CB TB05-adhoc GOV2 751-800 CB TB06-adhoc GOV2 801-850 CB RT04 Disk 4+5 (minus CR) 301-450;601700 CB TB05-NP GOV2 NP601-NP872 NP TB06-NP GOV2 NP901-NP1081 NP Table 3: Summary of test collections and topics Retrieval performance of individual content-based and NP queries is measured by the average precision and reciprocal rank of the first correct answer respectively.",
                "We make use of the Markov Random field model for both ad-hoc and Named-Page finding retrieval.",
                "We adopt the same setting of retrieval parameters used in [8,9].",
                "The Indri search engine [12] is used for all of our experiments.",
                "Though not reported here, we also tried the query likelihood model for ad-hoc retrieval and found that the results change little because of the very high correlation between the query performances obtained by the two retrieval models (0.96 measured by Pearsons coefficient). 4.2 Known Query Types Suppose that query types are known.",
                "We treat each type of query separately and measure the correlation with average precision (or the reciprocal rank in the case of NP queries).",
                "We adopt the Pearsons correlation test which reflects the degree of linear relationship between the predicted and the actual retrieval performance. 4.2.1 Content-based Queries Methods Clarity Robust JSD WIG QF WIG +QF TB04+0 5 adhoc 0.333 0.317 0.362 0.574 0.480 0.637 TB06 adhoc 0.076 0.294 N/A 0.464 0.422 0.511 Table 4: Pearsons correlation coefficients for correlation with average precision on the Terabyte Tracks (ad-hoc) for clarity score, robustness score, the JSD-based method(we directly cites the score reported in [2]), WIG, query feedback(QF) and a linear combination of WIG and QF.",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Table 4 shows the correlation with average precision on two data sets: one is a combination of TB04-adhoc and TB05-adhoc(100 topics in total) and the other is TB06-adhoc (50 topics).",
                "The reason that we put TB04-adhoc and TB05-adhoc together is to make our results comparable to [2].",
                "Our baselines are the clarity score (clarity) [6],the robustness score (robust)[1] and the JSDbased method (JSD) [2].",
                "For the clarity and robustness score, we have tried different parameter settings and report the highest correlation coefficients we have found.",
                "We directly cite the result of the JSD-based method reported in [2].",
                "The table also shows the results for the Weighted Information Gain (WIG) method and the Query Feedback (QF) method for predicting content-based queries.",
                "As we described in the previous section, both WIG and QF have one free parameter to set, that is, the cutoff rank K. We train the parameter on one dataset and test on the other.",
                "When combining WIG and QF, a simple linear combination is used and the combination weight is learned from the training data set.",
                "From these results, we can see that our methods are considerably more accurate compared to the baselines.",
                "We also observe that further improvements are obtained from the combination of WIG and QF, suggesting that they measure different properties of the retrieval process that relate to performance.",
                "We discover that our methods generalize well on TB06-adhoc while the correlation for the clarity score with retrieval performance on this data set is considerably worse.",
                "Further investigation shows that the mean average precision of TB06-adhoc is 0.342 and is about 10% better than that of the first data set.",
                "While the other three methods typically consider the top 100 or less documents given a ranked list, the clarity method usually needs the top 500 or more documents to adequately measure the coherence of a ranked list.",
                "Higher mean average precision makes ranked lists retrieved by different queries more similar in terms of coherence at the level of top 500 documents.",
                "We believe that this is the main reason for the low accuracy of the clarity score on the second data set.",
                "Though this paper focuses on a Web search environment, it is desirable that our techniques will work consistently well in other situations.",
                "To this end, we examine the effectiveness of our techniques on the Robust 2004 Track.",
                "For our methods, we evenly divide all of the test queries into five groups and perform five-fold cross validation.",
                "Each time we use one group for training and the remaining four groups for testing.",
                "We make use of all of the queries for our two baselines, that is, the clarity score and the robustness score.",
                "The parameters for our baselines are the same as those used in [1].The results shown in Table 5 demonstrate that the prediction accuracy of our methods is on a par with that of the two strong baselines.",
                "Clarity Robust WIG QF 0.464 0.539 0.468 0.464 Table 5: Comparison of Pearsons correlation coefficients on the 2004 Robust Track for clarity score, robustness score, WIG and query feedback (QF).",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Furthermore, we examine the prediction sensitivity of our methods to the cutoff rank K. With respect to WIG, it is quite robust to K on the Terabyte Tracks (2004-2006) while it prefers a small value of K like 5 on the 2004 Robust Track.",
                "In other words, a small value of K is a nearly-optimal choice for both kinds of tracks.",
                "Considering the fact that all other parameters involved in WIG are fixed and consequently the same for the two cases, this means WIG can achieve nearly-optimal prediction accuracy in two considerably different situations with exactly the same parameter setting.",
                "Regarding QF, it prefers a larger value of K such as 100 on the Terabyte Tracks and a smaller value of K such as 25 on the 2004 Robust Track. 4.2.2 NP Queries We adopt WIG and first rank change (FRC) for predicting NPquery performance.",
                "We also try a linear combination of the two as in the previous section.",
                "The combination weight is obtained from the other data set.",
                "We use the correlation with the reciprocal ranks measured by the Pearsons correlation test to evaluate prediction quality.",
                "The results are presented in Table 6.",
                "Again, our baselines are the clarity score and the robustness score.",
                "To make a fair comparison, we tune the clarity score in different ways.",
                "We found that using the first ranked document to build the query model yields the best prediction accuracy.",
                "We also attempted to utilize document structure by using the mixture of language models mentioned in section 3.1.",
                "Little improvement was obtained.",
                "The correlation coefficients for the clarity score reported in Table 6 are the best we have found.",
                "As we can see, our methods considerably outperform the clarity score technique on both of the runs.",
                "This confirms our intuition that the use of a coherence-based measure like the clarity score is inappropriate for NP queries.",
                "Methods Clarity Robust.",
                "WIG FRC WIG+FRC TB05-NP 0.150 -0.370 0.458 0.440 0.525 TB06-NP 0.112 -0.160 0.478 0.386 0.515 Table 6: Pearsons correlation coefficients for correlation with reciprocal ranks on the Terabyte Tracks (NP) for clarity score, robustness score, WIG, the first rank change (FRC) and a linear combination of WIG and FRC.",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Regarding the robustness score, we also tune the parameters and report the best we have found.",
                "We observe an interesting and surprising negative correlation with reciprocal ranks.",
                "We explain this finding briefly.",
                "A high robustness score means that a number of top ranked documents in the original ranked list are still highly ranked after perturbing the documents.",
                "The existence of such documents is a good sign of high performance for content-based queries as these queries usually contain a number of relevant documents [1].",
                "However, with regard to NP queries, one fundamental difference is that there is only one relevant document for each query.",
                "The existence of such documents can confuse the ranking function and lead to low retrieval performance.",
                "Although the negative correlation with retrieval performance exists, the strength of the correlation is weaker and less consistent compared to our methods as shown in Table 6.",
                "Based on the above analysis, we can see that current prediction techniques like clarity score and robustness score that are mainly designed for content-based queries face significant challenges and are inadequate to deal with NP queries.",
                "Our two techniques proposed for NP queries consistently demonstrate good prediction accuracy, displaying initial success in solving the problem of predicting performance for NP queries.",
                "Another point we want to stress is that the WIG method works well for both types of queries, a desirable property that most prediction techniques lack. 4.3 Unknown Query Types In this section, we run two kinds of experiments without access to query type labels.",
                "First, we assume that only one type of query exists but the type is unknown.",
                "Second, we experiment on a mixture of content-based and NP queries.",
                "The following two subsections will report results for the two conditions respectively. 4.3.1 Only One Type exists We assume that all queries are of the same type, that is, they are either NP queries or content-based queries.",
                "We choose WIG to deal with this case because it shows good prediction accuracy for both types of queries in the previous section.",
                "We consider two cases: (1) CB: all 150 title queries from the ad-hoc task of the Terabyte Tracks 2004-2006 (2)NP: all 433 NP queries from the named page finding task of the Terabyte Tracks 2005 and 2006.",
                "We take a simple strategy by labeling all of the queries in each case as the same type (either NP or CB) regardless of their actual type.",
                "The computation of WIG will be based on the labeled query type instead of the actual type.",
                "There are four possibilities with respect to the relation between the actual type and the labeled type.",
                "The correlation with retrieval performance under the four possibilities is presented in Table 7.",
                "For example, the value 0.445 at the intersection between the second row and the third column shows the Pearsons correlation coefficient for correlation with average precision when the content-based queries are incorrectly labeled as the NP type.",
                "Based on these results, we recommend treating all queries as the NP type when only one query type exists and accurate query classification is not feasible, considering the risk that a large loss of accuracy will occur if NP queries are incorrectly labeled as content-based queries.",
                "These results also demonstrate the strong adaptability of WIG to different query types.",
                "CB (labeled) NP (labeled) CB (actual) 0.536 0.445 NP (actual) 0.174 0.467 Table 7: Comparison of Pearsons correlation coefficients for correlation with retrieval performance under four possibilities on the Terabyte Tracks (NP).",
                "Bold cases mean the results are statistically significant at the 0.01 level. 4.3.2 A mixture of contented-based and NP queries A mixture of the two types of queries is a more realistic situation that a Web search engine will meet.",
                "We evaluate prediction accuracy by how accurately poorly-performing queries can be identified by the prediction method assuming that actual query types are unknown (but we can predict query types).",
                "This is a challenging task because both the predicted and actual performance for one type of query can be incomparable to that for the other type.",
                "Next we discuss how to implement our evaluation.",
                "We create a query pool which consists of all of the 150 ad-hoc title queries from Terabyte Track 2004-2006 and all of the 433 NP queries from Terabyte Track 2005&2006.",
                "We divide the queries in the pool into classes: good (better than 50% of the queries of the same type in terms of retrieval performance) and bad (otherwise).",
                "According to these standards, a NP query with the reciprocal rank above 0.2 or a <br>content-based query</br> with the average precision above 0.315 will be considered as good.",
                "Then, each time we randomly select one query Q from the pool with probability p that Q is contented-based.",
                "The remaining queries are used as training data.",
                "We first decide the type of query Q according to a query classifier.",
                "Namely, the query classifier tells us whether query Q is NP or content-based.",
                "Based on the predicted query type and the score computed for query Q by a prediction technique, a binary decision is made about whether query Q is good or bad by comparing to the score threshold of the predicted query type obtained from the training data.",
                "Prediction accuracy is measured by the accuracy of the binary decision.",
                "In our implementation, we repeatedly take a test query from the query pool and prediction accuracy is computed as the percentage of correct decisions, that is, a good(bad) query is predicted to be good (bad).",
                "It is obvious that random guessing will lead to 50% accuracy.",
                "Let us take the WIG method for example to illustrate the process.",
                "Two WIG thresholds (one for NP queries and the other for content-based queries) are trained by maximizing the prediction accuracy on the training data.",
                "When a test query is labeled as the NP (CB) type by the query type classifier, it will be predicted to be good if and only if the WIG score for this query is above the NP (CB) threshold.",
                "Similar procedures will be taken for other prediction techniques.",
                "Now we briefly introduce the automatic query type classifier used in this paper.",
                "We find that the robustness score, though originally proposed for performance prediction, is a good indicator of query types.",
                "We find that on average content-based queries have a much higher robustness score than NP queries.",
                "For example, Figure 2 shows the distributions of robustness scores for NP and content-based queries.",
                "According to this finding, the robustness score classifier will attach a NP (CB) label to the query if the robustness score for the query is below (above) a threshold trained from the training data. 0 0.5 1 1.5 2 2.5 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 NP Content-based Figure 2: Distribution of robustness scores for NP and CB queries.",
                "The NP queries are the 252 NP topics from the 2005 Terabyte Track.",
                "The content-based queries are the 150 ad-hoc title from the Terabyte Tracks 2004-2006.",
                "The probability distributions are estimated by the Kernel density estimation method.",
                "Strategies Robust WIG-1 WIG-2 WIG-3 Optimal p=0.6 0.565 0.624 0.665 0.684 0.701 P=0.4 0.567 0.633 0.654 0.673 0.696 Table 8: Comparison of prediction accuracy for five strategies in the mixed-query situation.",
                "Two ways to sample a query from the pool: (1) the sampled query is content-based with the probability p=0.6. (that is, the query is NP with probability 0.4 ) (2) set the probability p=0.4.",
                "We consider five strategies in our experiments.",
                "In the first strategy (denoted by robust), we use the robustness score for query performance prediction with the help of a perfect query classifier that always correctly map a query into one of the two categories (that is, NP or CB).",
                "This strategy represents the level of prediction accuracy that current prediction techniques can achieve in an ideal condition that query types are known.",
                "In the next following three strategies, the WIG method is adopted for performance prediction.",
                "The difference among the three is that three different query classifiers are used for each strategy: (1) the classifier always classifies a query into the NP type. (2) the Robustness Score ProbabilityDensity classifier is the robust score classifier mentioned above. (3) the classifier is a perfect one.",
                "These three strategies are denoted by WIG-1, WIG-2 and WIG-3 respectively.",
                "The reason we are interested in WIG-1 is based on the results from section 4.3.1.",
                "In the last strategy (denoted by Optimal) which serves as an upper bound on how well we can do so far, we fully make use of our prediction techniques for each query type assuming a perfect query classifier is available.",
                "Specifically, we linearly combine WIG and QF for content-based queries and WIG and FRC for NP queries.",
                "The results for the five strategies are shown in Table 8.",
                "For each strategy, we try two ways to sample a query from the pool: (1) the sampled query is CB with probability p=0.6. (the query is NP with probability 0.4) (2) set the probability p=0.4.",
                "From Table 8 We can see that in terms of prediction accuracy WIG-2 (the WIG method with the automatic query classifier) is not only better than the first two cases, but also is close to WIG-3 where a perfect classifier is assumed.",
                "Some further improvements over WIG-3 are observed when combined with other prediction techniques.",
                "The merit of WIG-2 is that it provides a practical solution to automatically identifying poorly performing queries in a Web search environment with mixed query types, which poses considerable obstacles to traditional prediction techniques. 5.",
                "CONCLUSIONS AND FUTURE WORK To our knowledge, our paper is the first to thoroughly explore prediction of query performance in web search environments.",
                "We demonstrated that our models resulted in higher prediction accuracy than previously published techniques not specially devised for web search scenarios.",
                "In this paper, we focus on two types of queries in web search: content-based and Named-Page (NP) finding queries, corresponding to the ad-hoc retrieval task and the Named-Page finding task respectively.",
                "For both types of web queries, our prediction models were shown to be substantially more accurate than the current state-of-the-art techniques.",
                "Furthermore, we considered a more realistic case that no prior information on query types is available.",
                "We demonstrated that the WIG method is particularly suitable for this situation.",
                "Considering the adaptability of WIG to a range of collections and query types, one of our future plans is to apply this method to predict user preference of search results on realistic data collected from a commercial search engine.",
                "Other than accuracy, another major issue that prediction techniques have to deal with in a Web environment is efficiency.",
                "Fortunately, since the WIG score is computed just over the terms and the phrases that appear in the query, this calculation can be made very efficient with the support of index.",
                "On the other hand, the computation of QF and FRC is relatively less efficient since QF needs to retrieve the whole collection twice and FRC needs to repeatedly rank the perturbed documents.",
                "How to improve the efficiency of QF and FRC is our future work.",
                "In addition, the prediction techniques proposed in this paper have the potential of improving retrieval performance by combining with other IR techniques.",
                "For example, our techniques can be incorporated to popular query modification techniques such as query expansion and query relaxation.",
                "Guided by performance prediction, we can make a better decision on when to or how to modify queries to enhance retrieval effectiveness.",
                "We would like to carry out research in this direction in the future. 6.",
                "ACKNOWLEGMENTS This work was supported in part by the Center for Intelligent Information Retrieval, in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C0023, and in part by an award from Google.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor.",
                "In addition, we thank Donald Metzler for his valuable comments on this work. 7.",
                "REFERENCES [1] Y. Zhou ,W. B. Croft ,Ranking Robustness: A Novel Framework to Predict Query Performance, in Proceedings of CIKM 2006. [2] D.Carmel, E.Yom-Tov, A.Darlow,D.Pelleg, What Makes a Query Difficult?, in Proceedings of SIGIR 2006. [3] C.L.A.",
                "Clarke, F. Scholer, I.Soboroff, The TREC 2005 Terabyte Track, In the Online Proceedings of 2005 TREC. [4] B.",
                "He and I.Ounis.",
                "Inferring query performance using preretrieval predictors.",
                "In proceedings of the SPIRE 2004. [5] S. Tomlinson.",
                "Robust, Web and Terabyte Retrieval with Hummingbird SearchServer at TREC 2004.",
                "In the Online Proceedings of 2004 TREC. [6] S. Cronen-Townsend, Y. Zhou, W. B. Croft, Predicting Query Performance, in Proceedings of SIGIR 2002. [7] V.Vinay, I.J.Cox, N.Mill-Frayling,K.Wood, On Ranking the Effectiveness of Searcher, in Proceedings of SIGIR 2006. [8] D.Metzler, W.B.Croft, A Markov Random Filed Model for Term Dependencies, in Proceedings of SIGIR 2005. [9] D.Metzler, T.Strohman,Y.Zhou,W.B.Croft, Indri at TREC 2005: Terabyte Track, In the Online Proceedings of 2004 TREC. [10] P. Ogilvie and J. Callan, Combining document representations for known-item search, in Proceedings of SIGIR 2003. [11] A.Berger, J.Lafferty, Information retrieval as statistical translation, in Proceedings of SIGIR 1999. [12] Indri search engine : http://www.lemurproject.org/indri/ [13] I.J.",
                "Taneja: On Generalized Information Measures and Their Applications, Advances in Electronics and Electron Physics, Academic Press (USA), 76, 1989, 327-413. [14] S.Cronen-Townsend, Y. Zhou and Croft, W. B. , A Framework for Selective Query Expansion, in Proceedings of CIKM 2004. [15] F.Song, W.B.Croft, A general language model for information retrieval, in Proceedings of SIGIR 1999. [16] Personal email contact with Vishwa Vinay and our own experiments [17] E.Yom-Tov, S.Fine, D.Carmel, A.Darlow, Learning to Estimate Query Difficulty Including Applications to Missing Content Detection and Distributed Information retrieval, in Proceedings of SIGIR 2005"
            ],
            "original_annotated_samples": [
                "According to these standards, a NP query with the reciprocal rank above 0.2 or a <br>content-based query</br> with the average precision above 0.315 will be considered as good."
            ],
            "translated_annotated_samples": [
                "Según estos estándares, una consulta de NP con un rango recíproco superior a 0.2 o una <br>consulta basada en contenido</br> con una precisión promedio superior a 0.315 se considerarán como buenas."
            ],
            "translated_text": "Predicción del rendimiento de consultas en entornos de búsqueda web Yun Zhou y W. Bruce Croft Departamento de Ciencias de la Computación Universidad de Massachusetts, Amherst {yzhou, croft}@cs.umass.edu RESUMEN Las técnicas actuales de predicción, generalmente diseñadas para consultas basadas en contenido y evaluadas típicamente en colecciones de prueba relativamente homogéneas de tamaños pequeños, enfrentan serios desafíos en entornos de búsqueda web donde las colecciones son significativamente más heterogéneas y existen diferentes tipos de tareas de recuperación. En este artículo, presentamos tres técnicas para abordar estos desafíos. Nos enfocamos en la predicción de rendimiento para dos tipos de consultas en entornos de búsqueda web: basadas en contenido y búsqueda de páginas por nombre. Nuestra evaluación se realiza principalmente en la colección GOV2. Además de evaluar nuestros modelos para los dos tipos de consultas por separado, consideramos una situación más desafiante y realista en la que los dos tipos de consultas se mezclan sin información previa sobre los tipos de consulta. Para ayudar en la predicción bajo la situación de consulta mixta, se adopta un clasificador de consultas novedoso. Los resultados muestran que nuestra predicción del rendimiento de la consulta web es sustancialmente más precisa que las técnicas de predicción actuales más avanzadas. Por consiguiente, nuestro artículo proporciona un enfoque práctico para predecir el rendimiento en entornos web del mundo real. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Formulación de Consultas Términos Generales Algoritmos, Experimentación, Teoría 1. La predicción del rendimiento de la consulta tiene muchas aplicaciones en una variedad de áreas de recuperación de información (IR), como mejorar la consistencia de la recuperación, el refinamiento de la consulta y la IR distribuida. La importancia de este problema ha sido reconocida por los investigadores en Recuperación de Información y recientemente se han propuesto varios métodos nuevos para la predicción [1, 2, 17]. La mayoría de los trabajos sobre predicción se han centrado en la tarea tradicional de recuperación ad-hoc, donde el rendimiento de la consulta se mide según la relevancia temática. Estos modelos de predicción se evalúan en colecciones de documentos de TREC que suelen consistir en no más de un millón de artículos de agencias de noticias relativamente homogéneos. Con la popularidad e influencia de la Web, las técnicas de predicción que funcionen bien para consultas de estilo web son altamente preferibles. Sin embargo, los entornos de búsqueda web plantean desafíos significativos para los modelos de predicción actuales que están principalmente diseñados para entornos tradicionales de TREC. Aquí delineamos algunos de estos desafíos. Primero, las colecciones web, que son mucho más grandes que las colecciones TREC convencionales, incluyen una variedad de documentos que son diferentes en muchos aspectos, como la calidad y el estilo. Las técnicas actuales de predicción pueden ser vulnerables a estas características de las colecciones web. Por ejemplo, la precisión de predicción reportada de la técnica de robustez de clasificación y la técnica de claridad en la colección GOV2 (una colección web grande) es significativamente peor en comparación con las otras colecciones de TREC [1]. En [2] se informa de una precisión de predicción similar en la colección GOV2 utilizando otra técnica, confirmando la dificultad de predecir el rendimiento de consultas en una gran colección web. Además, la búsqueda web va más allá del alcance de la tarea de recuperación ad hoc basada en la relevancia temática. Por ejemplo, la tarea de encontrar páginas nombradas (NP), que es una tarea de navegación, también es popular en la recuperación web. La predicción del rendimiento de la consulta para la tarea de NP sigue siendo necesaria, ya que el rendimiento de recuperación de NP está lejos de ser perfecto. De hecho, según el informe sobre la tarea NP del Terabyte Track de 2005 [3], aproximadamente el 40% de las consultas de prueba tienen un rendimiento deficiente (sin respuesta correcta en los primeros 10 resultados de búsqueda) incluso en la mejor ejecución del grupo líder. Hasta donde sabemos, poca investigación ha abordado explícitamente el problema de la predicción del rendimiento de consultas de NP. Los modelos de predicción actuales diseñados para consultas basadas en contenido serán menos efectivos para consultas de NP considerando las diferencias fundamentales entre ambas. En tercer lugar, en entornos de búsqueda web del mundo real, las consultas de los usuarios suelen ser una mezcla de diferentes tipos y, por lo general, no se dispone de información previa sobre el tipo de cada consulta. La situación de consulta mixta plantea nuevos problemas para la predicción del rendimiento de la consulta. Por ejemplo, es posible que necesitemos incorporar un clasificador de consultas en los modelos de predicción. A pesar de estos problemas, la capacidad para manejar esta situación es un paso crucial hacia convertir la predicción del rendimiento de consultas de un tema de investigación interesante en una herramienta práctica para la recuperación web. En este artículo, presentamos tres técnicas para abordar los desafíos mencionados que enfrentan los modelos de predicción actuales en entornos de búsqueda web. Nuestro trabajo se centra en la predicción del rendimiento de consultas para la tarea de recuperación basada en contenido (ad-hoc) y la tarea de encontrar páginas por nombre en el contexto de la recuperación web. Nuestra primera técnica, llamada ganancia de información ponderada (WIG), utiliza tanto características de términos individuales como de proximidad de términos para estimar la calidad de los documentos recuperados para la predicción. Observamos que WIG ofrece una precisión de predicción consistente en diversas colecciones de pruebas y tipos de consultas. Además, demostramos que se puede lograr una buena precisión de predicción para la situación de consulta mixta utilizando WIG con la ayuda de un clasificador de tipo de consulta. La retroalimentación de consultas y el cambio de rango inicial, que son nuestras técnicas de predicción segunda y tercera, funcionan bien para consultas basadas en contenido y consultas de NP respectivamente. Nuestras principales contribuciones incluyen: (1) una precisión de predicción considerablemente mejorada para consultas basadas en contenido web en comparación con varias técnicas de vanguardia. (2) nuevas técnicas para predecir con éxito el rendimiento de consultas NP. (3) una solución práctica y totalmente automática para predecir el rendimiento de consultas mixtas. Además, una contribución menor es que encontramos que el puntaje de robustez [1], que originalmente se propuso para la predicción de rendimiento, es útil para la clasificación de consultas. 2. TRABAJO RELACIONADO Como mencionamos en la introducción, recientemente se han propuesto varias técnicas de predicción que se centran en consultas basadas en contenido en la tarea de relevancia temática (ad-hoc). No conocemos ningún trabajo publicado que aborde otros tipos de consultas como las consultas NP, y mucho menos una mezcla de tipos de consultas. A continuación revisamos algunos modelos representativos. La principal dificultad de la predicción del rendimiento proviene del hecho de que muchos factores tienen un impacto en el rendimiento de recuperación. Cada factor afecta el rendimiento en diferente medida y el efecto general es difícil de predecir con precisión. Por lo tanto, no es sorprendente notar que características simples, como la frecuencia de los términos de consulta en la colección [4] y la IDF promedio de los términos de consulta [5], no predicen bien. De hecho, la mayoría de las técnicas exitosas se basan en medir algunas características del conjunto de documentos recuperados para estimar la dificultad del tema. Por ejemplo, la puntuación de claridad [6] mide la coherencia de una lista de documentos mediante la divergencia KL entre el modelo de consulta y el modelo de colección. El puntaje de robustez [1] cuantifica otra propiedad de una lista clasificada: la robustez de la clasificación en presencia de incertidumbre. Carmel et al. [2] encontraron que la distancia medida por la divergencia de Jensen-Shannon entre el conjunto de documentos recuperados y la colección está significativamente correlacionada con la precisión promedio. Vinay et al. [7] propusieron cuatro medidas para capturar la geometría de los documentos mejor clasificados para la predicción. La medida más efectiva es la sensibilidad a la perturbación del documento, una idea algo similar a la puntuación de robustez. Desafortunadamente, su forma de medir la sensibilidad no funciona igual de bien para consultas cortas y la precisión de la predicción disminuye considerablemente cuando se adopta una técnica de recuperación de vanguardia (como Okapi o un enfoque de modelado de lenguaje) en lugar del peso tf-idf utilizado en su artículo [16]. Las dificultades de aplicar estos modelos en entornos de búsqueda web ya han sido mencionadas. En este documento, principalmente adoptamos la puntuación de claridad y la puntuación de robustez como nuestras líneas base. Experimentalmente demostramos que las líneas base, incluso después de ser ajustadas cuidadosamente, son inadecuadas para el entorno web. Uno de nuestros modelos de predicción, WIG, está relacionado con el modelo de campo aleatorio de Markov (MRF) para la recuperación de información [8]. El modelo MRF modela directamente la dependencia de términos y se ha demostrado ser altamente efectivo en una variedad de colecciones de pruebas (especialmente colecciones web) y tareas de recuperación. Este modelo se utiliza para estimar la distribución conjunta de probabilidad sobre documentos y consultas, una parte importante de WIG. La superioridad de WIG sobre otras técnicas de predicción basadas en características unigramas, que se demostrará más adelante en nuestro artículo, coincide con la de MRF para recuperación. En otras palabras, es interesante notar que la dependencia de términos, cuando se modela adecuadamente, puede ser útil tanto para mejorar como para predecir el rendimiento de recuperación. 3. Modelos de predicción 3.1 Ganancia de Información Ponderada (WIG) Esta sección introduce un enfoque de ganancia de información ponderada que incorpora tanto características de términos únicos como de proximidad para predecir el rendimiento tanto en consultas basadas en contenido como en consultas de búsqueda de Páginas Nombradas (NP). Dado un conjunto de consultas Q={Qs} (s=1,2,..N) que incluye todas las posibles consultas de usuario y un conjunto de documentos D={Dt} (t=1,2…M), asumimos que cada par consulta-documento (Qs,Dt) es evaluado manualmente y se incluirá en una lista de relevancia si se determina que Qs es relevante para Dt. La probabilidad conjunta P(Qs,Dt) sobre las consultas Q y los documentos D denota la probabilidad de que el par (Qs,Dt) esté en la lista de relevancia. Tales suposiciones son similares a las utilizadas en [8]. Suponiendo que el usuario emite la consulta Qi ∈Q y los resultados de recuperación en respuesta a Qi es una lista clasificada L de documentos, calculamos la cantidad de información contenida en P(Qs,Dt) con respecto a Qi y L mediante la Ec.1, que es una variante de la entropía llamada entropía ponderada[13]. Los pesos en la ecuación 1 están determinados únicamente por Qi y L. En este documento, elegimos los pesos de la siguiente manera: L es el número de documentos que contiene la consulta, K es el límite superior, LT es el límite superior de documentos, y D es el peso si K está en otro caso. La clasificación de corte K es un parámetro en nuestro modelo que se discutirá más adelante. Por lo tanto, la ecuación 1 se puede simplificar de la siguiente manera: )3(),(log 1 ),( )( , ∑∈ −= LTD titsLQ Kt i DQP K DQH Desafortunadamente, la entropía ponderada ),(, tsLQ DQH i calculada por la ecuación 3, que representa la cantidad de información sobre la probabilidad de que los documentos mejor clasificados en L sean relevantes para la consulta Qi en promedio, no se puede comparar entre diferentes consultas, lo que la hace inapropiada para predecir directamente el rendimiento de la consulta. Para mitigar este problema, creamos una distribución de fondo P(Qs,C) sobre Q y D imaginando que cada documento en D es reemplazado por el mismo documento especial C que representa el uso promedio del lenguaje. En este documento, C se crea concatenando cada documento en D. A grosso modo, C es la colección (el conjunto de documentos) {Dt} sin límites de documentos. De manera similar, la entropía ponderada ),(, CQH sLQi calculada por la Ecuación 3 representa la cantidad de información sobre la probabilidad de que un documento promedio (representado por toda la colección) sea relevante para la consulta Qi. Ahora presentamos nuestro predictor de rendimiento WIG que es la ganancia de información ponderada [13] calculada como la diferencia entre ),(, tsLQ DQH i y ),(, CQH sLQi. Específicamente, dado el conjunto de consultas Qi, la colección C y la lista clasificada L de documentos, WIG se calcula de la siguiente manera: )4( ),( ),( log 1 ),( ),( log),( ),(),(),,( )(, ,, ∑∑ ∈ == −= LTD i ti ts s ts ts tsLQsLQi Kt ii CQP DQP KCQP DQP DQweight DQHCQHLCQWIG WIG calculado por la Ecuación 4 mide el cambio en la información sobre la calidad de la recuperación (en respuesta a la consulta Qi) desde un estado imaginario en el que solo se recupera un documento promedio hasta un estado posterior en el que se observan los resultados de búsqueda reales. Hacemos la hipótesis de que WIG está positivamente correlacionado con la efectividad de la recuperación porque una recuperación de alta calidad debería ser mucho más efectiva que simplemente devolver el documento promedio. El corazón de esta técnica es cómo estimar la distribución conjunta P(Qs, Dt). En el enfoque de modelado del lenguaje para la recuperación de información, se pueden aplicar fácilmente una variedad de modelos para estimar esta distribución. Aunque la mayoría de estos modelos se basan en la suposición de la bolsa de palabras, trabajos recientes sobre modelado de dependencia de términos bajo el marco de modelado de lenguaje han demostrado mejoras consistentes y significativas en la efectividad de recuperación sobre los modelos de bolsa de palabras. Inspirados por el éxito de incorporar características de proximidad de términos en modelos de lenguaje, decidimos adoptar un buen modelo de dependencia para estimar la probabilidad P(Qs,Dt). El modelo que elegimos para este artículo es el modelo de Campo Aleatorio de Markov (MRF) de Metzler y Crofts, que ya ha demostrado su superioridad en comparación con varias colecciones y diferentes tareas de recuperación [8,9]. Según el modelo MRF, log P(Qi, Dt) se puede escribir como )5()|(loglog),(log )( 1 ∑∈ +−= iQF tti DPZDQP ξ ξ ξλ donde Z1 es una constante que asegura que P(Qi, Dt) sume 1. F(Qi) consiste en un conjunto de características ampliadas a partir de la consulta original Qi. Por ejemplo, suponiendo que la consulta Qi es el programa de estudiantes talentosos, F(Qi) incluye características como programa y estudiante talentoso. Consideramos dos tipos de características: características de términos individuales T y características de proximidad P. Las características de proximidad incluyen la frase exacta (#1) y las características de ventana desordenada (#uwN) como se describe en [8]. Ten en cuenta que F(Qi) es la unión de T(Qi) y P(Qi). Para obtener más detalles sobre F(Qi), como por ejemplo cómo expandir la consulta original Qi a F(Qi), remitimos al lector a [8] y [9]. P(ξ|Dt) denota la probabilidad de que la característica ξ ocurra en Dt. Más detalles sobre P(ξ|Dt) se proporcionarán más adelante en esta sección. La elección de λξ es algo diferente a la utilizada en [8] ya que λξ desempeña un doble papel en nuestro modelo. El primer rol, que es el mismo que en [8], es ponderar entre las características de términos individuales y de proximidad. El otro rol, específico de nuestra tarea de predicción, es normalizar el tamaño de F(Qi). Encontramos que la siguiente estrategia de peso para λξ satisface los dos roles anteriores y generaliza bien en una variedad de colecciones y tipos de consultas. )6( )(, |)(| 1 )(, |)(| ⎪ ⎪ ⎩ ⎪ ⎪ ⎨ ⎧ ∈ − ∈ = i i T i i T QP QP QT QT ξ λ ξ λ λξ donde |T(Qi)| y |P(Qi)| denotan el número de características de términos únicos y de proximidad en F(Qi) respectivamente. La razón de elegir la función raíz cuadrada en el denominador de λξ es penalizar adecuadamente un conjunto de características de gran tamaño, haciendo que WIG sea más comparable entre consultas de diversas longitudes. λT es un parámetro fijo y se establece en 0.8 según [8] a lo largo de este documento. De manera similar, log P(Qi,C) se puede escribir como: )7()|(loglog),(log )( 2 ∑∈ +−= iQF i CPZCQP ξ ξ ξλ Cuando se eliminan las constantes Z1 y Z2, el WIG calculado en la Ecuación 4 se puede reescribir de la siguiente manera al sustituir la Ecuación 5 y la Ecuación 7: )8( )|( )|( log 1 ),,( )( )( ∑ ∑∈ ∈ = LTD QF t i Kt i CP DP K LCQWIG ξ ξ ξ ξ λ Una de las ventajas del WIG sobre otras técnicas es que puede manejar tanto consultas basadas en contenido como en NP. Basándose en el tipo (o el tipo predicho) de Qi, el cálculo de WIG en la Ecuación 8 difiere en dos aspectos: (1) cómo estimar P(ξ|Dt) y P(ξ|C), y (2) cómo elegir K. Para consultas basadas en contenido, P(ξ|C) se estima por la frecuencia relativa del rasgo ξ en la colección C en su totalidad. La estimación de P(ξ|Dt) es la misma que en [8]. Es decir, estimamos P(ξ|Dt) mediante la frecuencia relativa del rasgo ξ en Dt suavizada linealmente con la frecuencia de colección P(ξ|C). La K en la Ec.8 se trata como un parámetro libre. Ten en cuenta que K es el único parámetro libre en el cálculo de WIG para consultas basadas en contenido, ya que se asume que todos los parámetros involucrados en P(ξ|Dt) están fijos al tomar los valores sugeridos en [8]. En cuanto a las consultas de NP, hacemos uso de la estructura del documento para estimar P(ξ|Dt) y P(ξ|C) mediante el llamado modelo de mezcla de lenguaje propuesto en [10] e incorporado en el modelo MRF para la recuperación de Named-Page en [9]. La idea básica es que un documento (colección) se divide en varios campos como el campo del título, el campo del cuerpo principal y el campo de los encabezados. P(ξ|Dt) y P(ξ|C) se estiman mediante una combinación lineal de los modelos de lenguaje de cada campo. Debido a limitaciones de espacio, remitimos al lector a [9] para más detalles. Adoptamos el mismo conjunto exacto de parámetros utilizados en [9] para la estimación. En cuanto a K en la Ec.8, establecemos K en 1 porque la tarea de encontrar Named-Page se centra fuertemente en el documento clasificado en primer lugar. Por consiguiente, no hay parámetros libres en el cálculo de WIG para consultas de NP. 3.2 Retroalimentación de consultas En esta sección, presentamos otra técnica llamada retroalimentación de consultas (QF) para la predicción. Suponga que un usuario emite la consulta Q a un sistema de recuperación y se devuelve una lista clasificada L de documentos. Vemos el sistema de recuperación como un canal ruidoso. Específicamente, asumimos que la salida del canal es L y la entrada es Q. Después de pasar por el canal, Q se corrompe y se transforma en la lista clasificada L. Al pensar en el proceso de recuperación de esta manera, el problema de predecir la eficacia de la recuperación se convierte en la tarea de evaluar la calidad del canal. En otras palabras, la predicción se convierte en encontrar una forma de medir el grado de corrupción que surge cuando Q se transforma en L. Dado que calcular directamente el grado de corrupción es difícil, abordamos este problema mediante aproximaciones. Nuestra idea principal es que medimos en qué medida la información sobre Q puede ser recuperada de L bajo la suposición de que solo se observa L. Específicamente, diseñamos un decodificador que pueda traducir con precisión L de vuelta a la nueva consulta Q y la similitud S entre la consulta original Q y la nueva consulta Q se adopta como un predictor de rendimiento. Este es un bosquejo de cómo la técnica QF predice el rendimiento de la consulta. Antes de completar más detalles, discutimos brevemente por qué este método funcionaría. Existe una relación entre la similitud S definida anteriormente y el rendimiento de recuperación. Por un lado, si la recuperación se ha desviado del sentido original de la consulta Q, la nueva consulta Q extraída de la lista clasificada L en respuesta a Q sería muy diferente de la consulta original Q. Por otro lado, una consulta destilada de una lista clasificada que contiene muchos documentos relevantes es probable que sea similar a la consulta original. Se proporcionarán más ejemplos en apoyo de la relación más adelante. A continuación detallamos cómo construir el decodificador y cómo medir la similitud S. En esencia, el objetivo del decodificador es comprimir la lista clasificada L en unos pocos términos informativos que deberían representar el contenido de los documentos mejor clasificados en L. Nuestro enfoque para este objetivo es representar la lista clasificada L mediante un modelo de lenguaje (distribución de términos). Luego, los términos se clasifican según su contribución a la divergencia KL (Kullback-Leibler) de los modelos de lenguaje con respecto al modelo de la colección de fondo. Los términos mejor clasificados serán elegidos para formar la nueva consulta Q. Este enfoque es similar al utilizado en la Sección 4.1 de [11]. Específicamente, tomamos tres pasos para comprimir la lista clasificada L en la consulta Q sin hacer referencia a la consulta original. 1. Adoptamos el modelo de lenguaje de lista clasificada [14] para estimar un modelo de lenguaje basado en la lista clasificada L. El modelo puede escribirse como: )9()|()|()|( ∑∈ = LD LDPDwPLwP donde w es cualquier término y D es un documento. La probabilidad de que ocurra el evento D dado el evento L se estima mediante una función linealmente decreciente del rango del documento D. Cada término en P(w|L) está clasificado por la siguiente contribución de divergencia de Kullback-Leibler: )10( )|( )|( log)|( CwP LwP LwP donde P(w|C) es el modelo de colección estimado por la frecuencia relativa del término w en la colección C en su totalidad. 3. Los términos N principales clasificados por Eq.10 forman una consulta ponderada Q={(wi,ti)} i=1,N, donde wi denota el término clasificado en la posición i y el peso ti es la contribución de la divergencia KL de wi en Eq. 10. Barco de crucero daño vida marina. Estos ejemplos indican cómo la similitud entre la consulta original y la nueva se correlaciona con el rendimiento de recuperación. El parámetro N en el paso 3 se establece en 20 de manera empírica y elegir un valor más grande de N es innecesario ya que los pesos después de los primeros 20 suelen ser demasiado pequeños para marcar alguna diferencia. Para medir la similitud entre la consulta original Q y la nueva consulta Q, primero utilizamos Q para recuperar información de la misma colección. Se adopta una variante del modelo de probabilidad de consulta [15] para la recuperación. Es decir, los documentos se clasifican por: )11()|()|( ),( ∑∈ = Qtw t i ii i DwPDQP donde wi es un término en Q y ti es el peso asociado. D es un documento. Que L denote la nueva lista clasificada devuelta desde la recuperación anterior. La similitud se mide por la superposición de documentos en L y L. Específicamente, el porcentaje de documentos en los primeros K documentos de L que también están presentes en los primeros K documentos en L. El valor de corte K se trata como un parámetro libre. Aquí resumimos cómo la técnica QF predice el rendimiento dado una consulta Q y la lista clasificada asociada L. Primero obtenemos una consulta ponderada Q comprimida a partir de L mediante los tres pasos anteriores. Luego usamos Q para realizar la recuperación y la nueva lista clasificada es L. La superposición de documentos en L y L se utiliza para la predicción. 3.3 Primer Cambio de Rango (FRC) En esta sección, proponemos un método llamado primer cambio de rango (FRC) para la predicción de rendimiento para consultas de NP. Este método se deriva de la técnica de robustez de clasificación [1] que está principalmente diseñada para consultas basadas en contenido. Cuando se aplica directamente a consultas de NP, la técnica de robustez será menos efectiva porque tiene en cuenta todos los documentos mejor clasificados en su totalidad, mientras que las consultas de NP suelen tener solo un único documento relevante. En cambio, nuestra técnica se centra en el documento de rango uno mientras se mantiene la idea principal del método de robustez. Específicamente, el seudocódigo para calcular FRC se muestra en la figura 1. Lista clasificada L={Di} donde i=1,100. Di denota el documento clasificado en la posición i. (2) consulta Q 1 inicializar: (1) establecer el número de intentos J=100000 (2) contador c=0; 2 para i=1 a J 3 Perturbar cada documento en L, que el resultado sea un conjunto F={Di} donde Di denota la versión perturbada de Di. 4 Realizar la recuperación con la consulta Q en el conjunto F 5 c=c+1 si y solo si D1 está clasificado primero en el paso 4 6 fin del para 7 devolver la proporción c/J Figura 1: pseudo-código para calcular FRC FRC aproxima la probabilidad de que el documento clasificado en primer lugar en la lista original L permanezca clasificado primero incluso después de que los documentos sean perturbados. Cuanto mayor sea la probabilidad, más confianza tenemos en el documento clasificado en primer lugar. Por otro lado, en el caso extremo de un ranking aleatorio, la probabilidad sería tan baja como 0.5. Esperamos que FRC tenga una asociación positiva con el rendimiento de la consulta de NP. Adoptamos [1] para implementar el paso de perturbación del documento (paso 4 en la Fig. 1) utilizando distribuciones de Poisson. Para más detalles, remitimos al lector a [1]. 4. EVALUACIÓN Ahora presentamos los resultados de predecir el rendimiento de la consulta por nuestros modelos. Tres técnicas de vanguardia son adoptadas como nuestras líneas base. Evaluamos nuestras técnicas en una variedad de configuraciones de recuperación web. Como se mencionó anteriormente, consideramos dos tipos de consultas, es decir, consultas basadas en contenido (CB) y consultas de búsqueda de páginas con nombre (NP). Primero, supongamos que se conocen los tipos de consultas. Investigamos la correlación entre el rendimiento de recuperación predicho y el rendimiento real para ambos tipos de consultas por separado. Los resultados muestran que nuestros métodos producen mejoras considerablemente superiores a los valores de referencia. Luego consideramos un escenario más desafiante donde no hay información previa sobre los tipos de consultas disponibles. Se consideran dos subcasos. En el primero, existe solo un tipo de consulta pero el tipo real es desconocido. Suponemos una mezcla de los dos tipos de consultas en el segundo caso. Demostramos que nuestros modelos logran una buena precisión en este escenario exigente, lo que hace que la predicción sea práctica en un entorno de búsqueda web del mundo real. 4.1 Configuración Experimental Nuestra evaluación se centra en la colección GOV2 que contiene alrededor de 25 millones de documentos obtenidos de sitios web en el dominio .gov durante 2004 [3]. Creamos dos tipos de conjuntos de datos para consultas de CB y consultas de NP respectivamente. Para el tipo CB, utilizamos los temas ad-hoc de las pistas Terabyte de 2004, 2005 y 2006 y los nombramos TB04-adhoc, TB05-adhoc y TB06-adhoc respectivamente. Además, también utilizamos los temas ad-hoc de la pista Robusta de 2004 (RT04) para probar la adaptabilidad de nuestras técnicas a un entorno no web. Para consultas de NP, utilizamos los temas de búsqueda de páginas nombradas de las Terabyte Tracks de 2005 y 2006 y los denominamos TB05-NP y TB06-NP respectivamente. Todas las consultas utilizadas en nuestros experimentos son títulos de temas de TREC, ya que nos centramos en la recuperación web. La Tabla 3 resume los conjuntos de datos anteriores. La recuperación del rendimiento de las consultas individuales basadas en contenido y NP se mide mediante la precisión promedio y la tasa recíproca de la primera respuesta correcta, respectivamente. Hacemos uso del modelo de campo aleatorio de Markov tanto para la recuperación ad-hoc como para la recuperación de páginas nombradas. Adoptamos la misma configuración de parámetros de recuperación utilizada en [8,9]. El motor de búsqueda Indri [12] se utiliza para todos nuestros experimentos. Aunque no se informa aquí, también probamos el modelo de probabilidad de consulta para la recuperación ad-hoc y encontramos que los resultados cambian poco debido a la correlación muy alta entre el rendimiento de las consultas obtenido por los dos modelos de recuperación (0.96 medido por el coeficiente de Pearson). 4.2 Tipos de Consultas Conocidos Supongamos que se conocen los tipos de consultas. Tratamos cada tipo de consulta por separado y medimos la correlación con la precisión promedio (o el rango recíproco en el caso de consultas NP). Adoptamos la prueba de correlación de Pearson que refleja el grado de relación lineal entre el rendimiento de recuperación predicho y real. 4.2.1 Métodos de Consultas Basadas en Contenido Claridad Robusta JSD WIG QF WIG +QF TB04+0 5 adhoc 0.333 0.317 0.362 0.574 0.480 0.637 TB06 adhoc 0.076 0.294 N/A 0.464 0.422 0.511 Tabla 4: Coeficientes de correlación de Pearson para la correlación con la precisión promedio en las Pistas de Terabyte (ad-hoc) para la puntuación de claridad, puntuación de robustez, el método basado en JSD (citamos directamente la puntuación informada en [2]), WIG, retroalimentación de consulta (QF) y una combinación lineal de WIG y QF. Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. La Tabla 4 muestra la correlación con la precisión promedio en dos conjuntos de datos: uno es una combinación de TB04-adhoc y TB05-adhoc (100 temas en total) y el otro es TB06-adhoc (50 temas). La razón por la que juntamos TB04-adhoc y TB05-adhoc es para hacer nuestros resultados comparables a [2]. Nuestros puntos de referencia son la puntuación de claridad (claridad) [6], la puntuación de robustez (robust) [1] y el método basado en JSD (JSD) [2]. Para la puntuación de claridad y robustez, hemos probado diferentes configuraciones de parámetros y reportamos los coeficientes de correlación más altos que hemos encontrado. Citamos directamente el resultado del método basado en JSD reportado en [2]. La tabla también muestra los resultados para el método de Ganancia de Información Ponderada (WIG) y el método de Retroalimentación de Consulta (QF) para predecir consultas basadas en contenido. Como describimos en la sección anterior, tanto WIG como QF tienen un parámetro libre para ajustar, es decir, el rango de corte K. Entrenamos el parámetro en un conjunto de datos y lo probamos en el otro. Al combinar WIG y QF, se utiliza una combinación lineal simple y el peso de la combinación se aprende a partir del conjunto de datos de entrenamiento. A partir de estos resultados, podemos ver que nuestros métodos son considerablemente más precisos en comparación con los baselines. También observamos que se obtienen mejoras adicionales a partir de la combinación de WIG y QF, lo que sugiere que miden diferentes propiedades del proceso de recuperación que se relacionan con el rendimiento. Descubrimos que nuestros métodos se generalizan bien en TB06-adhoc, mientras que la correlación del puntaje de claridad con el rendimiento de recuperación en este conjunto de datos es considerablemente peor. Una investigación adicional muestra que la precisión promedio media de TB06-adhoc es de 0.342 y es aproximadamente un 10% mejor que la del primer conjunto de datos. Mientras que los otros tres métodos suelen considerar los 100 mejores documentos o menos de una lista clasificada, el método de claridad generalmente necesita los 500 mejores documentos o más para medir adecuadamente la coherencia de una lista clasificada. Un promedio de precisión media más alto hace que las listas clasificadas recuperadas por diferentes consultas sean más similares en términos de coherencia en el nivel de los primeros 500 documentos. Creemos que esta es la razón principal de la baja precisión de la puntuación de claridad en el segundo conjunto de datos. Aunque este documento se centra en un entorno de búsqueda web, es deseable que nuestras técnicas funcionen de manera consistente en otras situaciones. Con este fin, examinamos la efectividad de nuestras técnicas en la pista Robust 2004. Para nuestros métodos, dividimos equitativamente todas las consultas de prueba en cinco grupos y realizamos validación cruzada de cinco pliegues. Cada vez que utilizamos un grupo para el entrenamiento y los otros cuatro grupos para las pruebas. Hacemos uso de todas las consultas para nuestros dos puntos de referencia, es decir, la puntuación de claridad y la puntuación de robustez. Los parámetros para nuestras líneas base son los mismos que los utilizados en [1]. Los resultados mostrados en la Tabla 5 demuestran que la precisión de predicción de nuestros métodos está a la par con la de las dos líneas base sólidas. Claridad Robusta WIG QF 0.464 0.539 0.468 0.464 Tabla 5: Comparación de los coeficientes de correlación de Pearson en la pista Robusta de 2004 para la puntuación de claridad, puntuación de robustez, WIG y retroalimentación de consulta (QF). Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. Además, examinamos la sensibilidad de predicción de nuestros métodos al rango de corte K. Con respecto a WIG, es bastante robusto a K en las Pistas de Terabyte (2004-2006) mientras prefiere un valor pequeño de K como 5 en la Pista Robusta de 2004. En otras palabras, un valor pequeño de K es una elección casi óptima para ambos tipos de pistas. Teniendo en cuenta que todos los demás parámetros involucrados en WIG están fijos y, consecuentemente, son los mismos para los dos casos, esto significa que WIG puede lograr una precisión de predicción casi óptima en dos situaciones considerablemente diferentes con exactamente la misma configuración de parámetros. En cuanto a QF, prefiere un valor más grande de K, como 100 en las Pistas de Terabyte y un valor más pequeño de K, como 25 en la Pista Robusta de 2004. 4.2.2 Consultas NP Adoptamos WIG y el primer cambio de rango (FRC) para predecir el rendimiento de las consultas NP. También intentamos una combinación lineal de los dos como en la sección anterior. El peso de la combinación se obtiene del otro conjunto de datos. Utilizamos la correlación con los rangos recíprocos medidos por la prueba de correlación de Pearson para evaluar la calidad de la predicción. Los resultados se presentan en la Tabla 6. Nuevamente, nuestros puntos de referencia son la puntuación de claridad y la puntuación de robustez. Para hacer una comparación justa, ajustamos la puntuación de claridad de diferentes maneras. Descubrimos que utilizar el documento clasificado en primer lugar para construir el modelo de consulta produce la mejor precisión de predicción. También intentamos utilizar la estructura del documento mediante la combinación de modelos de lenguaje mencionados en la sección 3.1. Se obtuvo una pequeña mejora. Los coeficientes de correlación para la puntuación de claridad reportados en la Tabla 6 son los mejores que hemos encontrado. Como podemos ver, nuestros métodos superan considerablemente la técnica de puntuación de claridad en ambas ejecuciones. Esto confirma nuestra intuición de que el uso de una medida basada en la coherencia como el puntaje de claridad es inapropiado para las consultas de NP. Claridad de métodos robusta. Tabla 6: Coeficientes de correlación de Pearson para la correlación con rangos recíprocos en las pistas de Terabyte (NP) para la puntuación de claridad, puntuación de robustez, WIG, el primer cambio de rango (FRC) y una combinación lineal de WIG y FRC. Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. En cuanto a la puntuación de robustez, también ajustamos los parámetros y reportamos el mejor que hemos encontrado. Observamos una correlación negativa interesante y sorprendente con los rangos recíprocos. Explicamos este hallazgo brevemente. Un puntaje de robustez alto significa que varios documentos de alta clasificación en la lista clasificada original siguen estando muy bien clasificados después de perturbar los documentos. La existencia de tales documentos es una buena señal de alto rendimiento para consultas basadas en contenido, ya que estas consultas suelen contener una serie de documentos relevantes [1]. Sin embargo, en lo que respecta a las consultas de NP, una diferencia fundamental es que solo hay un documento relevante para cada consulta. La existencia de tales documentos puede confundir la función de clasificación y llevar a un bajo rendimiento de recuperación. Aunque existe una correlación negativa con el rendimiento de recuperación, la fuerza de la correlación es más débil y menos consistente en comparación con nuestros métodos, como se muestra en la Tabla 6. Basándonos en el análisis anterior, podemos ver que las técnicas de predicción actuales como la puntuación de claridad y la puntuación de robustez, que están principalmente diseñadas para consultas basadas en contenido, enfrentan desafíos significativos y son insuficientes para manejar consultas de NP. Nuestras dos técnicas propuestas para consultas de NP demuestran consistentemente una buena precisión de predicción, mostrando un éxito inicial en la resolución del problema de predecir el rendimiento para consultas de NP. Otro punto que queremos destacar es que el método WIG funciona bien para ambos tipos de consultas, una propiedad deseable que la mayoría de las técnicas de predicción carecen. 4.3 Tipos de Consultas Desconocidos En esta sección, realizamos dos tipos de experimentos sin acceso a etiquetas de tipo de consulta. Primero, asumimos que solo existe un tipo de consulta pero el tipo es desconocido. Segundo, experimentamos con una mezcla de consultas basadas en contenido y NP. Las dos subsecciones siguientes informarán los resultados para las dos condiciones respectivamente. 4.3.1 Solo existe un tipo. Suponemos que todas las consultas son del mismo tipo, es decir, son consultas NP o consultas basadas en contenido. Elegimos WIG para tratar este caso porque muestra una buena precisión de predicción para ambos tipos de consultas en la sección anterior. Consideramos dos casos: (1) CB: todas las 150 consultas de títulos de la tarea ad-hoc de las pistas Terabyte 2004-2006 (2) NP: todas las 433 consultas de NP de la tarea de búsqueda de páginas nombradas de las pistas Terabyte 2005 y 2006. Tomamos una estrategia simple etiquetando todas las consultas en cada caso como el mismo tipo (ya sea NP o CB) independientemente de su tipo real. El cálculo de WIG se basará en el tipo de consulta etiquetado en lugar del tipo real. Hay cuatro posibilidades con respecto a la relación entre el tipo real y el tipo etiquetado. La correlación con el rendimiento de recuperación bajo las cuatro posibilidades se presenta en la Tabla 7. Por ejemplo, el valor 0.445 en la intersección entre la segunda fila y la tercera columna muestra el coeficiente de correlación de Pearson para la correlación con la precisión promedio cuando las consultas basadas en contenido se etiquetan incorrectamente como del tipo NP. Basándonos en estos resultados, recomendamos tratar todas las consultas como del tipo NP cuando solo existe un tipo de consulta y la clasificación precisa de la consulta no es factible, considerando el riesgo de que se produzca una gran pérdida de precisión si las consultas NP se etiquetan incorrectamente como consultas basadas en contenido. Estos resultados también demuestran la fuerte adaptabilidad de WIG a diferentes tipos de consultas. Tabla 7: Comparación de los coeficientes de correlación de Pearson para la correlación con el rendimiento de recuperación en cuatro posibilidades en las pistas de Terabyte (NP). Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. 4.3.2 Una mezcla de consultas basadas en contenido y NP Una mezcla de los dos tipos de consultas es una situación más realista que un motor de búsqueda web encontrará. Evaluamos la precisión de la predicción por la exactitud con la que se pueden identificar las consultas de bajo rendimiento mediante el método de predicción, asumiendo que los tipos de consulta reales son desconocidos (pero podemos predecir los tipos de consulta). Esta es una tarea desafiante porque tanto el rendimiento predicho como el real para un tipo de consulta pueden ser incomparables con el de otro tipo. A continuación discutimos cómo implementar nuestra evaluación. Creamos un conjunto de consultas que consiste en todas las 150 consultas de título ad-hoc de Terabyte Track 2004-2006 y todas las 433 consultas NP de Terabyte Track 2005&2006. Dividimos las consultas en el conjunto en clases: buenas (mejores que el 50% de las consultas del mismo tipo en términos de rendimiento de recuperación) y malas (de lo contrario). Según estos estándares, una consulta de NP con un rango recíproco superior a 0.2 o una <br>consulta basada en contenido</br> con una precisión promedio superior a 0.315 se considerarán como buenas. Entonces, cada vez que seleccionamos aleatoriamente una consulta Q del grupo con una probabilidad p de que Q esté basada en el contenido. Las consultas restantes se utilizan como datos de entrenamiento. Primero decidimos el tipo de consulta Q de acuerdo con un clasificador de consultas. Es decir, el clasificador de consultas nos dice si la consulta Q es de tipo NP o basada en contenido. Basándose en el tipo de consulta predicho y en la puntuación calculada para la consulta Q por una técnica de predicción, se toma una decisión binaria sobre si la consulta Q es buena o mala al compararla con el umbral de puntuación del tipo de consulta predicho obtenido de los datos de entrenamiento. La precisión de la predicción se mide por la exactitud de la decisión binaria. En nuestra implementación, tomamos repetidamente una consulta de prueba del conjunto de consultas y la precisión de la predicción se calcula como el porcentaje de decisiones correctas, es decir, una consulta buena (mala) se predice como buena (mala). Es obvio que adivinar al azar llevará a una precisión del 50%. Tomemos el método WIG como ejemplo para ilustrar el proceso. Dos umbrales de WIG (uno para consultas de NP y otro para consultas basadas en contenido) se entrenan maximizando la precisión de predicción en los datos de entrenamiento. Cuando una consulta de prueba es etiquetada como del tipo NP (CB) por el clasificador de tipo de consulta, se predecirá que es buena solo si la puntuación WIG para esta consulta está por encima del umbral de NP (CB). Se seguirán procedimientos similares para otras técnicas de predicción. Ahora presentamos brevemente el clasificador de tipo de consulta automático utilizado en este artículo. Encontramos que la puntuación de robustez, aunque originalmente propuesta para la predicción de rendimiento, es un buen indicador de tipos de consultas. Observamos que, en promedio, las consultas basadas en contenido tienen una puntuación de robustez mucho más alta que las consultas de NP. Por ejemplo, la Figura 2 muestra las distribuciones de puntajes de robustez para consultas basadas en NP y en contenido. Según este hallazgo, el clasificador de puntuación de robustez adjuntará una etiqueta NP (CB) a la consulta si la puntuación de robustez para la consulta está por debajo (por encima) de un umbral entrenado a partir de los datos de entrenamiento. 0 0.5 1 1.5 2 2.5 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 NP Basado en contenido Figura 2: Distribución de puntuaciones de robustez para consultas NP y CB. Las consultas NP son los 252 temas NP del Terabyte Track de 2005. Las consultas basadas en contenido son los 150 títulos ad-hoc de las pistas Terabyte 2004-2006. Las distribuciones de probabilidad son estimadas por el método de estimación de densidad de Kernel. Estrategias Robustas WIG-1 WIG-2 WIG-3 Óptima p=0.6 0.565 0.624 0.665 0.684 0.701 P=0.4 0.567 0.633 0.654 0.673 0.696 Tabla 8: Comparación de la precisión de predicción para cinco estrategias en la situación de consulta mixta. Dos formas de muestrear una consulta de la piscina: (1) la consulta muestreada es basada en contenido con una probabilidad p=0.6 (es decir, la consulta es NP con una probabilidad de 0.4) (2) establecer la probabilidad p=0.4. Consideramos cinco estrategias en nuestros experimentos. En la primera estrategia (denominada robusta), utilizamos la puntuación de robustez para la predicción del rendimiento de la consulta con la ayuda de un clasificador de consultas perfecto que siempre mapea correctamente una consulta en una de las dos categorías (es decir, NP o CB). Esta estrategia representa el nivel de precisión de predicción que las técnicas actuales de predicción pueden lograr en una condición ideal en la que se conocen los tipos de consulta. En las tres estrategias siguientes, se adopta el método WIG para predecir el rendimiento. La diferencia entre los tres es que se utilizan tres clasificadores de consulta diferentes para cada estrategia: (1) el clasificador siempre clasifica una consulta en el tipo NP. (2) el clasificador de Probabilidad de Densidad de Puntuación Robusta es el clasificador de puntuación robusta mencionado anteriormente. (3) el clasificador es perfecto. Estas tres estrategias están designadas como WIG-1, WIG-2 y WIG-3 respectivamente. La razón por la que estamos interesados en WIG-1 se basa en los resultados de la sección 4.3.1. En la última estrategia (denominada Óptima) que sirve como un límite superior de lo bien que podemos hacer hasta ahora, hacemos pleno uso de nuestras técnicas de predicción para cada tipo de consulta asumiendo que un clasificador de consultas perfecto está disponible. Específicamente, combinamos linealmente WIG y QF para consultas basadas en contenido y WIG y FRC para consultas de NP. Los resultados de las cinco estrategias se muestran en la Tabla 8. Para cada estrategia, intentamos dos formas de muestrear una consulta del conjunto: (1) la consulta muestreada es CB con probabilidad p=0.6 (la consulta es NP con probabilidad 0.4) (2) establecer la probabilidad p=0.4. De la Tabla 8 podemos ver que en términos de precisión de predicción, WIG-2 (el método WIG con el clasificador de consultas automático) no solo es mejor que los dos primeros casos, sino que también está cerca de WIG-3 donde se asume un clasificador perfecto. Se observan algunas mejoras adicionales sobre WIG-3 cuando se combinan con otras técnicas de predicción. El mérito de WIG-2 es que proporciona una solución práctica para identificar automáticamente consultas con bajo rendimiento en un entorno de búsqueda web con tipos de consultas mixtos, lo cual representa considerables obstáculos para las técnicas de predicción tradicionales. CONCLUSIONES Y TRABAJOS FUTUROS Hasta donde sabemos, nuestro artículo es el primero en explorar a fondo la predicción del rendimiento de consultas en entornos de búsqueda web. Demostramos que nuestros modelos resultaron en una mayor precisión de predicción que las técnicas previamente publicadas no especialmente diseñadas para escenarios de búsqueda en la web. En este artículo, nos enfocamos en dos tipos de consultas en la búsqueda web: consultas basadas en contenido y consultas de búsqueda de Páginas-Nombradas (NP), correspondientes a la tarea de recuperación ad-hoc y la tarea de búsqueda de Páginas-Nombradas respectivamente. Para ambos tipos de consultas web, se demostró que nuestros modelos de predicción son sustancialmente más precisos que las técnicas actuales más avanzadas. Además, consideramos un caso más realista en el que no se dispone de información previa sobre los tipos de consultas. Demostramos que el método WIG es particularmente adecuado para esta situación. Considerando la adaptabilidad de WIG a una variedad de colecciones y tipos de consultas, uno de nuestros planes futuros es aplicar este método para predecir la preferencia del usuario por los resultados de búsqueda en datos realistas recopilados de un motor de búsqueda comercial. Además de la precisión, otro problema importante con el que las técnicas de predicción tienen que lidiar en un entorno web es la eficiencia. Afortunadamente, dado que el puntaje WIG se calcula solo sobre los términos y frases que aparecen en la consulta, este cálculo puede realizarse de manera muy eficiente con el soporte de un índice. Por otro lado, el cálculo de QF y FRC es relativamente menos eficiente ya que QF necesita recuperar toda la colección dos veces y FRC necesita clasificar repetidamente los documentos perturbados. Cómo mejorar la eficiencia de QF y FRC es nuestro trabajo futuro. Además, las técnicas de predicción propuestas en este documento tienen el potencial de mejorar el rendimiento de recuperación al combinarse con otras técnicas de RI. Por ejemplo, nuestras técnicas pueden ser incorporadas a técnicas populares de modificación de consultas como la expansión de consultas y la relajación de consultas. Guiados por la predicción del rendimiento, podemos tomar una mejor decisión sobre cuándo o cómo modificar las consultas para mejorar la efectividad de la recuperación. Nos gustaría llevar a cabo investigaciones en esta dirección en el futuro. AGRADECIMIENTOS Este trabajo fue apoyado en parte por el Centro de Recuperación de Información Inteligente, en parte por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el contrato número HR0011-06-C0023, y en parte por un premio de Google. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son del autor y no necesariamente reflejan las del patrocinador. Además, agradecemos a Donald Metzler por sus valiosos comentarios sobre este trabajo. 7. REFERENCIAS [1] Y. Zhou, W. B. Croft, Ranking Robustness: Un nuevo marco para predecir el rendimiento de consultas, en Actas de CIKM 2006. [2] D. Carmel, E. Yom-Tov, A. Darlow, D. Pelleg, ¿Qué hace que una consulta sea difícil?, en Actas de SIGIR 2006. [3] C. L. A. Clarke, F. Scholer, I. Soboroff, La pista Terabyte TREC 2005, En las Actas en Línea de TREC 2005. [4] B. Él y yo. Inferir el rendimiento de la consulta utilizando predictores de preretrieval. En actas de SPIRE 2004. [5] S. Tomlinson. Recuperación robusta de la web y terabytes con Hummingbird SearchServer en TREC 2004. En las Actas en Línea de TREC 2004. [6] S. Cronen-Townsend, Y. Zhou, W. B. Croft, Prediciendo el Rendimiento de Consultas, en las Actas de SIGIR 2002. [7] V. Vinay, I. J. Cox, N. Mill-Frayling, K. Wood, Sobre la Clasificación de la Efectividad del Buscador, en las Actas de SIGIR 2006. [8] D. Metzler, W. B. Croft, Un Modelo de Campo Aleatorio de Markov para Dependencias de Términos, en las Actas de SIGIR 2005. [9] D. Metzler, T. Strohman, Y. Zhou, W. B. Croft, Indri en TREC 2005: Terabyte Track, en las Actas en Línea de TREC 2004. [10] P. Ogilvie y J. Callan, Combinando representaciones de documentos para búsqueda de elementos conocidos, en las Actas de SIGIR 2003. [11] A. Berger, J. Lafferty, Recuperación de información como traducción estadística, en las Actas de SIGIR 1999. [12] Motor de búsqueda Indri: http://www.lemurproject.org/indri/ [13] I. J. Taneja: Sobre medidas de información generalizadas y sus aplicaciones, Avances en Electrónica y Física de Electrones, Academic Press (EE. UU.), 76, 1989, 327-413. [14] S. Cronen-Townsend, Y. Zhou y Croft, W. B., Un marco para la expansión selectiva de consultas, en Actas de CIKM 2004. [15] F. Song, W. B. Croft, Un modelo de lenguaje general para la recuperación de información, en Actas de SIGIR 1999. [16] Contacto por correo electrónico personal con Vishwa Vinay y nuestros propios experimentos [17] E. Yom-Tov, S. Fine, D. Carmel, A. Darlow, Aprendiendo a estimar la dificultad de la consulta incluyendo aplicaciones para la detección de contenido faltante y la recuperación de información distribuida, en Actas de SIGIR 2005. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "homogenous test collection": {
            "translated_key": "recogida de pruebas homogéneas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Query Performance Prediction in Web Search Environments Yun Zhou and W. Bruce Croft Department of Computer Science University of Massachusetts, Amherst {yzhou, croft}@cs.umass.edu ABSTRACT Current prediction techniques, which are generally designed for content-based queries and are typically evaluated on relatively <br>homogenous test collection</br>s of small sizes, face serious challenges in web search environments where collections are significantly more heterogeneous and different types of retrieval tasks exist.",
                "In this paper, we present three techniques to address these challenges.",
                "We focus on performance prediction for two types of queries in web search environments: content-based and Named-Page finding.",
                "Our evaluation is mainly performed on the GOV2 collection.",
                "In addition to evaluating our models for the two types of queries separately, we consider a more challenging and realistic situation that the two types of queries are mixed together without prior information on query types.",
                "To assist prediction under the mixed-query situation, a novel query classifier is adopted.",
                "Results show that our prediction of web query performance is substantially more accurate than the current stateof-the-art prediction techniques.",
                "Consequently, our paper provides a practical approach to performance prediction in realworld web settings.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval -Query formulation General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Query performance prediction has many applications in a variety of information retrieval (IR) areas such as improving retrieval consistency, query refinement, and distributed IR.",
                "The importance of this problem has been recognized by IR researchers and a number of new methods have been proposed for prediction recently [1, 2, 17].",
                "Most work on prediction has focused on the traditional ad-hoc retrieval task where query performance is measured according to topical relevance.",
                "These prediction models are evaluated on TREC document collections which typically consist of no more than one million relatively homogenous newswire articles.",
                "With the popularity and influence of the Web, prediction techniques that will work well for web-style queries are highly preferable.",
                "However, web search environments pose significant challenges to current prediction models that are mainly designed for traditional TREC settings.",
                "Here we outline some of these challenges.",
                "First, web collections, which are much larger than conventional TREC collections, include a variety of documents that are different in many aspects such as quality and style.",
                "Current prediction techniques can be vulnerable to these characteristics of web collections.",
                "For example, the reported prediction accuracy of the ranking robustness technique and the clarity technique on the GOV2 collection (a large web collection) is significantly worse compared to the other TREC collections [1].",
                "Similar prediction accuracy on the GOV2 collection using another technique is reported in [2], confirming the difficult of predicting query performance on a large web collection.",
                "Furthermore, web search goes beyond the scope of the ad-hoc retrieval task based on topical relevance.",
                "For example, the Named-Page (NP) finding task, which is a navigational task, is also popular in web retrieval.",
                "Query performance prediction for the NP task is still necessary since NP retrieval performance is far from perfect.",
                "In fact, according to the report on the NP task of the 2005 Terabyte Track [3], about 40% of the test queries perform poorly (no correct answer in the first 10 search results) even in the best run from the top group.",
                "To our knowledge, little research has explicitly addressed the problem of NP-query performance prediction.",
                "Current prediction models devised for content-based queries will be less effective for NP queries considering the fundamental differences between the two.",
                "Third, in real-world web search environments, user queries are usually a mixture of different types and prior knowledge about the type of each query is generally unavailable.",
                "The mixed-query situation raises new problems for query performance prediction.",
                "For instance, we may need to incorporate a query classifier into prediction models.",
                "Despite these problems, the ability to handle this situation is a crucial step towards turning query performance prediction from an interesting research topic into a practical tool for web retrieval.",
                "In this paper, we present three techniques to address the above challenges that current prediction models face in Web search environments.",
                "Our work focuses on query performance prediction for the content-based (ad-hoc) retrieval task and the name-page finding task in the context of web retrieval.",
                "Our first technique, called weighted information gain (WIG), makes use of both single term and term proximity features to estimate the quality of top retrieved documents for prediction.",
                "We find that WIG offers consistent prediction accuracy across various test collections and query types.",
                "Moreover, we demonstrate that good prediction accuracy can be achieved for the mixed-query situation by using WIG with the help of a query type classifier.",
                "Query feedback and first rank change, which are our second and third prediction techniques, perform well for content-based queries and NP queries respectively.",
                "Our main contributions include: (1) considerably improved prediction accuracy for web content-based queries over several state-of-the-art techniques. (2) new techniques for successfully predicting NP-query performance. (3) a practical and fully automatic solution to predicting mixed-query performance.",
                "In addition, one minor contribution is that we find that the robustness score [1], which was originally proposed for performance prediction, is helpful for query classification. 2.",
                "RELATED WORK As we mentioned in the introduction, a number of prediction techniques have been proposed recently that focus on contentbased queries in the topical relevance (ad-hoc) task.",
                "We know of no published work that addresses other types of queries such as NP queries, let alone a mixture of query types.",
                "Next we review some representative models.",
                "The major difficulty of performance prediction comes from the fact that many factors have an impact on retrieval performance.",
                "Each factor affects performance to a different degree and the overall effect is hard to predict accurately.",
                "Therefore, it is not surprising to notice that simple features, such as the frequency of query terms in the collection [4] and the average IDF of query terms [5], do not predict well.",
                "In fact, most of the successful techniques are based on measuring some characteristics of the retrieved document set to estimate topic difficulty.",
                "For example, the clarity score [6] measures the coherence of a list of documents by the KL-divergence between the query model and the collection model.",
                "The robustness score [1] quantifies another property of a ranked list: the robustness of the ranking in the presence of uncertainty.",
                "Carmel et al. [2] found that the distance measured by the Jensen-Shannon divergence between the retrieved document set and the collection is significantly correlated to average precision.",
                "Vinay et al. [7] proposed four measures to capture the geometry of the top retrieved documents for prediction.",
                "The most effective measure is the sensitivity to document perturbation, an idea somewhat similar to the robustness score.",
                "Unfortunately, their way of measuring the sensitivity does not perform equally well for short queries and prediction accuracy drops considerably when a state-of-the-art retrieval technique (like Okapi or a language modeling approach) is adopted for retrieval instead of the tf-idf weighting used in their paper [16].",
                "The difficulties of applying these models in web search environments have already been mentioned.",
                "In this paper, we mainly adopt the clarity score and the robustness score as our baselines.",
                "We experimentally show that the baselines, even after being carefully tuned, are inadequate for the web environment.",
                "One of our prediction models, WIG, is related to the Markov random field (MRF) model for information retrieval [8].",
                "The MRF model directly models term dependence and is found be to highly effective across a variety of test collections (particularly web collections) and retrieval tasks.",
                "This model is used to estimate the joint probability distribution over documents and queries, an important part of WIG.",
                "The superiority of WIG over other prediction techniques based on unigram features, which will be demonstrated later in our paper, coincides with that of MRF for retrieval.",
                "In other word, it is interesting to note that term dependence, when being modeled appropriately, can be helpful for both improving and predicting retrieval performance. 3.",
                "PREDICTION MODELS 3.1 Weighted Information Gain (WIG) This section introduces a weighted information gain approach that incorporates both single term and proximity features for predicting performance for both content-based and Named-Page (NP) finding queries.",
                "Given a set of queries Q={Qs} (s=1,2,..N) which includes all possible user queries and a set of documents D={Dt} (t=1,2…M), we assume that each query-document pair (Qs,Dt) is manually judged and will be put in a relevance list if Qs is found to be relevant to Dt.",
                "The joint probability P(Qs,Dt) over queries Q and documents D denotes the probability that pair (Qs,Dt) will be in the relevance list.",
                "Such assumptions are similar to those used in [8].",
                "Assuming that the user issues query Qi ∈Q and the retrieval results in response to Qi is a ranked list L of documents, we calculate the amount of information contained in P(Qs,Dt) with respect to Qi and L by Eq.1 which is a variant of entropy called the weighted entropy[13].",
                "The weights in Eq.1 are solely determined by Qi and L. )1(),(log),(),( , , ∑−= ts tststsLQ DQPDQweightDQH i In this paper, we choose the weights as follows: LindocumentsKtopthecontainsLTwhere otherwise LTDandisifK DQweight K Kt ts )( )2( ,0 )(,/1 ),( ⎩ ⎨ ⎧ ∈= = The cutoff rank K is a parameter in our model that will be discussed later.",
                "Accordingly, Eq.1 can be simplified as follows: )3(),(log 1 ),( )( , ∑∈ −= LTD titsLQ Kt i DQP K DQH Unfortunately, weighted entropy ),(, tsLQ DQH i computed by Eq.3, which represents the amount of information about how likely the top ranked documents in L would be relevant to query Qi on average, cannot be compared across different queries, making it inappropriate for directly predicting query performance.",
                "To mitigate this problem, we come up with a background distribution P(Qs,C) over Q and D by imagining that every document in D is replaced by the same special document C which represents average language usage.",
                "In this paper, C is created by concatenating every document in D. Roughly speaking, C is the collection (the document set) {Dt} without document boundaries.",
                "Similarly, weighted entropy ),(, CQH sLQi calculated by Eq.3 represents the amount of information about how likely an average document (represented by the whole collection) would be relevant to query Qi.",
                "Now we introduce our performance predictor WIG which is the weighted information gain [13] computed as the difference between ),(, tsLQ DQH i and ),(, CQH sLQi .Specifically, given query Qi, collection C and ranked list L of documents, WIG is calculated as follows: )4( ),( ),( log 1 ),( ),( log),( ),(),(),,( )(, ,, ∑∑ ∈ == −= LTD i ti ts s ts ts tsLQsLQi Kt ii CQP DQP KCQP DQP DQweight DQHCQHLCQWIG WIG computed by Eq.4 measures the change in information about the quality of retrieval (in response to query Qi) from an imaginary state that only an average document is retrieved to a posterior state that the actual search results are observed.",
                "We hypothesize that WIG is positively correlated with retrieval effectiveness because high quality retrieval should be much more effective than just returning the average document.",
                "The heart of this technique is how to estimate the joint distribution P(Qs,Dt).",
                "In the language modeling approach to IR, a variety of models can be applied readily to estimate this distribution.",
                "Although most of these models are based on the bagof-words assumption, recent work on modeling term dependence under the language modeling framework have shown consistent and significant improvements in retrieval effectiveness over bagof-words models.",
                "Inspired by the success of incorporating term proximity features into language models, we decide to adopt a good dependence model to estimate the probability P(Qs,Dt).",
                "The model we chose for this paper is Metzler and Crofts Markov Random Field (MRF) model, which has already demonstrated superiority over a number of collections and different retrieval tasks [8,9].",
                "According to the MRF model, log P(Qi, Dt) can be written as )5()|(loglog),(log )( 1 ∑∈ +−= iQF tti DPZDQP ξ ξ ξλ where Z1 is a constant that ensures that P(Qi, Dt) sums up to 1.",
                "F(Qi) consists of a set of features expanded from the original query Qi .",
                "For example, assuming that query Qi is talented student program, F(Qi) includes features like program and talented student.",
                "We consider two kinds of features: single term features T and proximity features P. Proximity features include exact phrase (#1) and unordered window (#uwN) features as described in [8].",
                "Note that F(Qi) is the union of T(Qi) and P(Qi).",
                "For more details on F(Qi) such as how to expand the original query Qi to F(Qi), we refer the reader to [8] and [9].",
                "P(ξ|Dt) denotes the probability that feature ξ will occur in Dt.",
                "More details on P(ξ|Dt) will be provided later in this section.",
                "The choice of λξ is somewhat different from that used in [8] since λξ plays a dual role in our model.",
                "The first role, which is the same as in [8], is to weight between single term and proximity features.",
                "The other role, which is specific to our prediction task, is to normalize the size of F(Qi).We found that the following weight strategy for λξ satisfies the above two roles and generalizes well on a variety of collections and query types. )6( )(, |)(| 1 )(, |)(| ⎪ ⎪ ⎩ ⎪ ⎪ ⎨ ⎧ ∈ − ∈ = i i T i i T QP QP QT QT ξ λ ξ λ λξ where |T(Qi)| and |P(Qi)| denote the number of single term and proximity features in F(Qi) respectively.",
                "The reason for choosing the square root function in the denominator of λξ is to penalize a feature set of large size appropriately, making WIG more comparable across queries of various lengths. λT is a fixed parameter and set to 0.8 according to [8] throughout this paper.",
                "Similarly, log P(Qi,C) can be written as: )7()|(loglog),(log )( 2 ∑∈ +−= iQF i CPZCQP ξ ξ ξλ When constant Z1 and Z2 are dropped, WIG computed in Eq.4 can be rewritten as follows by plugging in Eq.5 and Eq.7 : )8( )|( )|( log 1 ),,( )( )( ∑ ∑∈ ∈ = LTD QF t i Kt i CP DP K LCQWIG ξ ξ ξ ξ λ One of the advantages of WIG over other techniques is that it can handle well both content-based and NP queries.",
                "Based on the type (or the predicted type) of Qi, the calculation of WIG in Eq. 8 differs in two aspects: (1) how to estimate P(ξ|Dt) and P(ξ|C), and (2) how to choose K. For content-based queries, P(ξ|C) is estimated by the relative frequency of feature ξ in collection C as a whole.",
                "The estimation of P(ξ|Dt) is the same as in [8].",
                "Namely, we estimate P(ξ|Dt) by the relative frequency of feature ξ in Dt linearly smoothed with collection frequency P(ξ|C).",
                "K in Eq.8 is treated as a free parameter.",
                "Note that K is the only free parameter in the computation of WIG for content-based queries because all parameters involved in P(ξ|Dt) are assumed to be fixed by taking the suggested values in [8].",
                "Regarding NP queries, we make use of document structure to estimate P(ξ|Dt) and P(ξ|C) by the so-called mixture of language models proposed in [10] and incorporated into the MRF model for Named-Page finding retrieval in [9].",
                "The basic idea is that a document (collection) is divided into several fields such as the title field, the main-body field and the heading field.",
                "P(ξ|Dt) and P(ξ|C) are estimated by a linear combination of the language models from each field.",
                "Due to space constraints, we refer the reader to [9] for details.",
                "We adopt the exact same set of parameters as used in [9] for estimation.",
                "With regard to K in Eq.8, we set K to 1 because the Named-Page finding task heavily focuses on the first ranked document.",
                "Consequently, there are no free parameters in the computation of WIG for NP queries. 3.2 Query Feedback In this section, we introduce another technique called query feedback (QF) for prediction.",
                "Suppose that a user issues query Q to a retrieval system and a ranked list L of documents is returned.",
                "We view the retrieval system as a noisy channel.",
                "Specifically, we assume that the output of the channel is L and the input is Q.",
                "After going through the channel, Q becomes corrupted and is transformed to ranked list L. By thinking about the retrieval process this way, the problem of predicting retrieval effectiveness turns to the task of evaluating the quality of the channel.",
                "In other words, prediction becomes finding a way to measure the degree of corruption that arises when Q is transformed to L. As directly computing the degree of the corruption is difficult, we tackle this problem by approximation.",
                "Our main idea is that we measure to what extent information on Q can be recovered from L on the assumption that only L is observed.",
                "Specifically, we design a decoder that can accurately translate L back into new query Q and the similarity S between the original query Q and the new query Q is adopted as a performance predictor.",
                "This is a sketch of how the QF technique predicts query performance.",
                "Before filling in more details, we briefly discuss why this method would work.",
                "There is a relation between the similarity S defined above and retrieval performance.",
                "On the one hand, if the retrieval has strayed from the original sense of the query Q, the new query Q extracted from ranked list L in response to Q would be very different from the original query Q.",
                "On the other hand, a query distilled from a ranked list containing many relevant documents is likely to be similar to the original query.",
                "Further examples in support of the relation will be provided later.",
                "Next we detail how to build the decoder and how to measure the similarity S. In essence, the goal of the decoder is to compress ranked list L into a few informative terms that should represent the content of the top ranked documents in L. Our approach to this goal is to represent ranked list L by a language model (distribution over terms).",
                "Then terms are ranked by their contribution to the language models KL (Kullback-Leibler) divergence from the background collection model.",
                "Top ranked terms will be chosen to form the new query Q.",
                "This approach is similar to that used in Section 4.1 of [11].",
                "Specifically, we take three steps to compress ranked list L into query Q without referring to the original query. 1.",
                "We adopt the ranked list language model [14], to estimate a language model based on ranked list L. The model can be written as: )9()|()|()|( ∑∈ = LD LDPDwPLwP where w is any term, D is a document.",
                "P(D|L) is estimated by a linearly decreasing function of the rank of document D. 2.",
                "Each term in P(w|L) is ranked by the following KL-divergence contribution: )10( )|( )|( log)|( CwP LwP LwP where P(w|C) is the collection model estimated by the relative frequency of term w in collection C as a whole. 3.",
                "The top N ranked terms by Eq.10 form a weighted query Q={(wi,ti)} i=1,N. where wi denotes the i-th ranked term and weight ti is the KL-divergence contribution of wi in Eq. 10.",
                "Term cruise ship vessel sea passenger KL contribution 0.050 0.040 0.012 0.010 0.009 Table 1: top 5 terms compressed from the ranked list in response to query Cruise ship damage sea life Two representative examples, one for a poorly performing query Cruise ship damage sea life (TREC topic 719; average precision: 0.08) and the other for a high performing query prostate cancer treatments( TREC topic 710; average precision: 0.49), are shown in Table 1 and 2 respectively.",
                "These examples indicate how the similarity between the original and the new query correlates with retrieval performance.",
                "The parameter N in step 3 is set to 20 empirically and choosing a larger value of N is unnecessary since the weights after the top 20 are usually too small to make any difference.",
                "Term prostate cancer treatment men therapy KL contribution 0.177 0.140 0.028 0.025 0.020 Table 2: top 5 terms compressed from the ranked list in response to query prostate cancer treatments To measure the similarity between original query Q and new query Q, we first use Q to do retrieval on the same collection.",
                "A variant of the query likelihood model [15] is adopted for retrieval.",
                "Namely, documents are ranked by: )11()|()|( ),( ∑∈ = Qtw t i ii i DwPDQP where wi is a term in Q and ti is the associated weight.",
                "D is a document.",
                "Let L denote the new ranked list returned from the above retrieval.",
                "The similarity is measured by the overlap of documents in L and L. Specifically, the percentage of documents in the top K documents of L that are also present in the top K documents in L. the cutoff K is treated as a free parameter.",
                "We summarize here how the QF technique predicts performance given a query Q and the associated ranked list L. We first obtain a weighted query Q compressed from L by the above three steps.",
                "Then we use Q to perform retrieval and the new ranked list is L. The overlap of documents in L and L is used for prediction. 3.3 First Rank Change (FRC) In this section, we propose a method called the first rank change (FRC) for performance prediction for NP queries.",
                "This method is derived from the ranking robustness technique [1] that is mainly designed for content-based queries.",
                "When directly applied to NP queries, the robustness technique will be less effective because it takes the top ranked documents as a whole into account while NP queries usually have only one single relevant document.",
                "Instead, our technique focuses on the first rank document while the main idea of the robustness method remains.",
                "Specifically, the pseudocode for computing FRC is shown in figure 1.",
                "Input: (1) ranked list L={Di} where i=1,100.",
                "Di denotes the i-th ranked document. (2) query Q 1 initialize: (1) set the number of trials J=100000 (2) counter c=0; 2 for i=1 to J 3 Perturb every document in L, let the outcome be a set F={Di} where Di denotes the perturbed version of Di. 4 Do retrieval with query Q on set F 5 c=c+1 if and only if D1 is ranked first in step 4 6 end of for 7 return the ratio c/J Figure 1: pseudo-code for computing FRC FRC approximates the probability that the first ranked document in the original list L will remain ranked first even after the documents are perturbed.",
                "The higher the probability is, the more confidence we have in the first ranked document.",
                "On the other hand, in the extreme case of a random ranking, the probability would be as low as 0.5.",
                "We expect that FRC has a positive association with NP query performance.",
                "We adopt [1] to implement the document perturbation step (step 4 in Fig.1) using Poisson distributions.",
                "For more details, we refer the reader to [1]. 4.",
                "EVALUATION We now present the results of predicting query performance by our models.",
                "Three state-of-the-art techniques are adopted as our baselines.",
                "We evaluate our techniques across a variety of Web retrieval settings.",
                "As mentioned before, we consider two types of queries, that is, content-based (CB) queries and Named-Page(NP) finding queries.",
                "First, suppose that the query types are known.",
                "We investigate the correlation between the predicted retrieval performance and the actual performance for both types of queries separately.",
                "Results show that our methods yield considerable improvements over the baselines.",
                "We then consider a more challenging scenario where no prior information on query types is available.",
                "Two sub-cases are considered.",
                "In the first one, there exists only one type of query but the actual type is unknown.",
                "We assume a mixture of the two query types in the second case.",
                "We demonstrate that our models achieve good accuracy under this demanding scenario, making prediction practical in a real-world Web search environment. 4.1 Experimental Setup Our evaluation focuses on the GOV2 collection which contains about 25 million documents crawled from web sites in the .gov domain during 2004 [3].",
                "We create two kinds of data set for CB queries and NP queries respectively.",
                "For the CB type, we use the ad-hoc topics of the Terabyte Tracks of 2004, 2005 and 2006 and name them TB04-adhoc, TB05-adhoc and TB06-adhoc respectively.",
                "In addition, we also use the ad-hoc topics of the 2004 Robust Track (RT04) to test the adaptability of our techniques to a non-Web environment.",
                "For NP queries, we use the Named-Page finding topics of the Terabyte Tracks of 2005 and 2006 and we name them TB05-NP and TB06-NP respectively.",
                "All queries used in our experiments are titles of TREC topics as we center on web retrieval.",
                "Table 3 summarizes the above data sets.",
                "Name Collection Topic Number Query Type TB04-adhoc GOV2 701-750 CB TB05-adhoc GOV2 751-800 CB TB06-adhoc GOV2 801-850 CB RT04 Disk 4+5 (minus CR) 301-450;601700 CB TB05-NP GOV2 NP601-NP872 NP TB06-NP GOV2 NP901-NP1081 NP Table 3: Summary of test collections and topics Retrieval performance of individual content-based and NP queries is measured by the average precision and reciprocal rank of the first correct answer respectively.",
                "We make use of the Markov Random field model for both ad-hoc and Named-Page finding retrieval.",
                "We adopt the same setting of retrieval parameters used in [8,9].",
                "The Indri search engine [12] is used for all of our experiments.",
                "Though not reported here, we also tried the query likelihood model for ad-hoc retrieval and found that the results change little because of the very high correlation between the query performances obtained by the two retrieval models (0.96 measured by Pearsons coefficient). 4.2 Known Query Types Suppose that query types are known.",
                "We treat each type of query separately and measure the correlation with average precision (or the reciprocal rank in the case of NP queries).",
                "We adopt the Pearsons correlation test which reflects the degree of linear relationship between the predicted and the actual retrieval performance. 4.2.1 Content-based Queries Methods Clarity Robust JSD WIG QF WIG +QF TB04+0 5 adhoc 0.333 0.317 0.362 0.574 0.480 0.637 TB06 adhoc 0.076 0.294 N/A 0.464 0.422 0.511 Table 4: Pearsons correlation coefficients for correlation with average precision on the Terabyte Tracks (ad-hoc) for clarity score, robustness score, the JSD-based method(we directly cites the score reported in [2]), WIG, query feedback(QF) and a linear combination of WIG and QF.",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Table 4 shows the correlation with average precision on two data sets: one is a combination of TB04-adhoc and TB05-adhoc(100 topics in total) and the other is TB06-adhoc (50 topics).",
                "The reason that we put TB04-adhoc and TB05-adhoc together is to make our results comparable to [2].",
                "Our baselines are the clarity score (clarity) [6],the robustness score (robust)[1] and the JSDbased method (JSD) [2].",
                "For the clarity and robustness score, we have tried different parameter settings and report the highest correlation coefficients we have found.",
                "We directly cite the result of the JSD-based method reported in [2].",
                "The table also shows the results for the Weighted Information Gain (WIG) method and the Query Feedback (QF) method for predicting content-based queries.",
                "As we described in the previous section, both WIG and QF have one free parameter to set, that is, the cutoff rank K. We train the parameter on one dataset and test on the other.",
                "When combining WIG and QF, a simple linear combination is used and the combination weight is learned from the training data set.",
                "From these results, we can see that our methods are considerably more accurate compared to the baselines.",
                "We also observe that further improvements are obtained from the combination of WIG and QF, suggesting that they measure different properties of the retrieval process that relate to performance.",
                "We discover that our methods generalize well on TB06-adhoc while the correlation for the clarity score with retrieval performance on this data set is considerably worse.",
                "Further investigation shows that the mean average precision of TB06-adhoc is 0.342 and is about 10% better than that of the first data set.",
                "While the other three methods typically consider the top 100 or less documents given a ranked list, the clarity method usually needs the top 500 or more documents to adequately measure the coherence of a ranked list.",
                "Higher mean average precision makes ranked lists retrieved by different queries more similar in terms of coherence at the level of top 500 documents.",
                "We believe that this is the main reason for the low accuracy of the clarity score on the second data set.",
                "Though this paper focuses on a Web search environment, it is desirable that our techniques will work consistently well in other situations.",
                "To this end, we examine the effectiveness of our techniques on the Robust 2004 Track.",
                "For our methods, we evenly divide all of the test queries into five groups and perform five-fold cross validation.",
                "Each time we use one group for training and the remaining four groups for testing.",
                "We make use of all of the queries for our two baselines, that is, the clarity score and the robustness score.",
                "The parameters for our baselines are the same as those used in [1].The results shown in Table 5 demonstrate that the prediction accuracy of our methods is on a par with that of the two strong baselines.",
                "Clarity Robust WIG QF 0.464 0.539 0.468 0.464 Table 5: Comparison of Pearsons correlation coefficients on the 2004 Robust Track for clarity score, robustness score, WIG and query feedback (QF).",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Furthermore, we examine the prediction sensitivity of our methods to the cutoff rank K. With respect to WIG, it is quite robust to K on the Terabyte Tracks (2004-2006) while it prefers a small value of K like 5 on the 2004 Robust Track.",
                "In other words, a small value of K is a nearly-optimal choice for both kinds of tracks.",
                "Considering the fact that all other parameters involved in WIG are fixed and consequently the same for the two cases, this means WIG can achieve nearly-optimal prediction accuracy in two considerably different situations with exactly the same parameter setting.",
                "Regarding QF, it prefers a larger value of K such as 100 on the Terabyte Tracks and a smaller value of K such as 25 on the 2004 Robust Track. 4.2.2 NP Queries We adopt WIG and first rank change (FRC) for predicting NPquery performance.",
                "We also try a linear combination of the two as in the previous section.",
                "The combination weight is obtained from the other data set.",
                "We use the correlation with the reciprocal ranks measured by the Pearsons correlation test to evaluate prediction quality.",
                "The results are presented in Table 6.",
                "Again, our baselines are the clarity score and the robustness score.",
                "To make a fair comparison, we tune the clarity score in different ways.",
                "We found that using the first ranked document to build the query model yields the best prediction accuracy.",
                "We also attempted to utilize document structure by using the mixture of language models mentioned in section 3.1.",
                "Little improvement was obtained.",
                "The correlation coefficients for the clarity score reported in Table 6 are the best we have found.",
                "As we can see, our methods considerably outperform the clarity score technique on both of the runs.",
                "This confirms our intuition that the use of a coherence-based measure like the clarity score is inappropriate for NP queries.",
                "Methods Clarity Robust.",
                "WIG FRC WIG+FRC TB05-NP 0.150 -0.370 0.458 0.440 0.525 TB06-NP 0.112 -0.160 0.478 0.386 0.515 Table 6: Pearsons correlation coefficients for correlation with reciprocal ranks on the Terabyte Tracks (NP) for clarity score, robustness score, WIG, the first rank change (FRC) and a linear combination of WIG and FRC.",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Regarding the robustness score, we also tune the parameters and report the best we have found.",
                "We observe an interesting and surprising negative correlation with reciprocal ranks.",
                "We explain this finding briefly.",
                "A high robustness score means that a number of top ranked documents in the original ranked list are still highly ranked after perturbing the documents.",
                "The existence of such documents is a good sign of high performance for content-based queries as these queries usually contain a number of relevant documents [1].",
                "However, with regard to NP queries, one fundamental difference is that there is only one relevant document for each query.",
                "The existence of such documents can confuse the ranking function and lead to low retrieval performance.",
                "Although the negative correlation with retrieval performance exists, the strength of the correlation is weaker and less consistent compared to our methods as shown in Table 6.",
                "Based on the above analysis, we can see that current prediction techniques like clarity score and robustness score that are mainly designed for content-based queries face significant challenges and are inadequate to deal with NP queries.",
                "Our two techniques proposed for NP queries consistently demonstrate good prediction accuracy, displaying initial success in solving the problem of predicting performance for NP queries.",
                "Another point we want to stress is that the WIG method works well for both types of queries, a desirable property that most prediction techniques lack. 4.3 Unknown Query Types In this section, we run two kinds of experiments without access to query type labels.",
                "First, we assume that only one type of query exists but the type is unknown.",
                "Second, we experiment on a mixture of content-based and NP queries.",
                "The following two subsections will report results for the two conditions respectively. 4.3.1 Only One Type exists We assume that all queries are of the same type, that is, they are either NP queries or content-based queries.",
                "We choose WIG to deal with this case because it shows good prediction accuracy for both types of queries in the previous section.",
                "We consider two cases: (1) CB: all 150 title queries from the ad-hoc task of the Terabyte Tracks 2004-2006 (2)NP: all 433 NP queries from the named page finding task of the Terabyte Tracks 2005 and 2006.",
                "We take a simple strategy by labeling all of the queries in each case as the same type (either NP or CB) regardless of their actual type.",
                "The computation of WIG will be based on the labeled query type instead of the actual type.",
                "There are four possibilities with respect to the relation between the actual type and the labeled type.",
                "The correlation with retrieval performance under the four possibilities is presented in Table 7.",
                "For example, the value 0.445 at the intersection between the second row and the third column shows the Pearsons correlation coefficient for correlation with average precision when the content-based queries are incorrectly labeled as the NP type.",
                "Based on these results, we recommend treating all queries as the NP type when only one query type exists and accurate query classification is not feasible, considering the risk that a large loss of accuracy will occur if NP queries are incorrectly labeled as content-based queries.",
                "These results also demonstrate the strong adaptability of WIG to different query types.",
                "CB (labeled) NP (labeled) CB (actual) 0.536 0.445 NP (actual) 0.174 0.467 Table 7: Comparison of Pearsons correlation coefficients for correlation with retrieval performance under four possibilities on the Terabyte Tracks (NP).",
                "Bold cases mean the results are statistically significant at the 0.01 level. 4.3.2 A mixture of contented-based and NP queries A mixture of the two types of queries is a more realistic situation that a Web search engine will meet.",
                "We evaluate prediction accuracy by how accurately poorly-performing queries can be identified by the prediction method assuming that actual query types are unknown (but we can predict query types).",
                "This is a challenging task because both the predicted and actual performance for one type of query can be incomparable to that for the other type.",
                "Next we discuss how to implement our evaluation.",
                "We create a query pool which consists of all of the 150 ad-hoc title queries from Terabyte Track 2004-2006 and all of the 433 NP queries from Terabyte Track 2005&2006.",
                "We divide the queries in the pool into classes: good (better than 50% of the queries of the same type in terms of retrieval performance) and bad (otherwise).",
                "According to these standards, a NP query with the reciprocal rank above 0.2 or a content-based query with the average precision above 0.315 will be considered as good.",
                "Then, each time we randomly select one query Q from the pool with probability p that Q is contented-based.",
                "The remaining queries are used as training data.",
                "We first decide the type of query Q according to a query classifier.",
                "Namely, the query classifier tells us whether query Q is NP or content-based.",
                "Based on the predicted query type and the score computed for query Q by a prediction technique, a binary decision is made about whether query Q is good or bad by comparing to the score threshold of the predicted query type obtained from the training data.",
                "Prediction accuracy is measured by the accuracy of the binary decision.",
                "In our implementation, we repeatedly take a test query from the query pool and prediction accuracy is computed as the percentage of correct decisions, that is, a good(bad) query is predicted to be good (bad).",
                "It is obvious that random guessing will lead to 50% accuracy.",
                "Let us take the WIG method for example to illustrate the process.",
                "Two WIG thresholds (one for NP queries and the other for content-based queries) are trained by maximizing the prediction accuracy on the training data.",
                "When a test query is labeled as the NP (CB) type by the query type classifier, it will be predicted to be good if and only if the WIG score for this query is above the NP (CB) threshold.",
                "Similar procedures will be taken for other prediction techniques.",
                "Now we briefly introduce the automatic query type classifier used in this paper.",
                "We find that the robustness score, though originally proposed for performance prediction, is a good indicator of query types.",
                "We find that on average content-based queries have a much higher robustness score than NP queries.",
                "For example, Figure 2 shows the distributions of robustness scores for NP and content-based queries.",
                "According to this finding, the robustness score classifier will attach a NP (CB) label to the query if the robustness score for the query is below (above) a threshold trained from the training data. 0 0.5 1 1.5 2 2.5 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 NP Content-based Figure 2: Distribution of robustness scores for NP and CB queries.",
                "The NP queries are the 252 NP topics from the 2005 Terabyte Track.",
                "The content-based queries are the 150 ad-hoc title from the Terabyte Tracks 2004-2006.",
                "The probability distributions are estimated by the Kernel density estimation method.",
                "Strategies Robust WIG-1 WIG-2 WIG-3 Optimal p=0.6 0.565 0.624 0.665 0.684 0.701 P=0.4 0.567 0.633 0.654 0.673 0.696 Table 8: Comparison of prediction accuracy for five strategies in the mixed-query situation.",
                "Two ways to sample a query from the pool: (1) the sampled query is content-based with the probability p=0.6. (that is, the query is NP with probability 0.4 ) (2) set the probability p=0.4.",
                "We consider five strategies in our experiments.",
                "In the first strategy (denoted by robust), we use the robustness score for query performance prediction with the help of a perfect query classifier that always correctly map a query into one of the two categories (that is, NP or CB).",
                "This strategy represents the level of prediction accuracy that current prediction techniques can achieve in an ideal condition that query types are known.",
                "In the next following three strategies, the WIG method is adopted for performance prediction.",
                "The difference among the three is that three different query classifiers are used for each strategy: (1) the classifier always classifies a query into the NP type. (2) the Robustness Score ProbabilityDensity classifier is the robust score classifier mentioned above. (3) the classifier is a perfect one.",
                "These three strategies are denoted by WIG-1, WIG-2 and WIG-3 respectively.",
                "The reason we are interested in WIG-1 is based on the results from section 4.3.1.",
                "In the last strategy (denoted by Optimal) which serves as an upper bound on how well we can do so far, we fully make use of our prediction techniques for each query type assuming a perfect query classifier is available.",
                "Specifically, we linearly combine WIG and QF for content-based queries and WIG and FRC for NP queries.",
                "The results for the five strategies are shown in Table 8.",
                "For each strategy, we try two ways to sample a query from the pool: (1) the sampled query is CB with probability p=0.6. (the query is NP with probability 0.4) (2) set the probability p=0.4.",
                "From Table 8 We can see that in terms of prediction accuracy WIG-2 (the WIG method with the automatic query classifier) is not only better than the first two cases, but also is close to WIG-3 where a perfect classifier is assumed.",
                "Some further improvements over WIG-3 are observed when combined with other prediction techniques.",
                "The merit of WIG-2 is that it provides a practical solution to automatically identifying poorly performing queries in a Web search environment with mixed query types, which poses considerable obstacles to traditional prediction techniques. 5.",
                "CONCLUSIONS AND FUTURE WORK To our knowledge, our paper is the first to thoroughly explore prediction of query performance in web search environments.",
                "We demonstrated that our models resulted in higher prediction accuracy than previously published techniques not specially devised for web search scenarios.",
                "In this paper, we focus on two types of queries in web search: content-based and Named-Page (NP) finding queries, corresponding to the ad-hoc retrieval task and the Named-Page finding task respectively.",
                "For both types of web queries, our prediction models were shown to be substantially more accurate than the current state-of-the-art techniques.",
                "Furthermore, we considered a more realistic case that no prior information on query types is available.",
                "We demonstrated that the WIG method is particularly suitable for this situation.",
                "Considering the adaptability of WIG to a range of collections and query types, one of our future plans is to apply this method to predict user preference of search results on realistic data collected from a commercial search engine.",
                "Other than accuracy, another major issue that prediction techniques have to deal with in a Web environment is efficiency.",
                "Fortunately, since the WIG score is computed just over the terms and the phrases that appear in the query, this calculation can be made very efficient with the support of index.",
                "On the other hand, the computation of QF and FRC is relatively less efficient since QF needs to retrieve the whole collection twice and FRC needs to repeatedly rank the perturbed documents.",
                "How to improve the efficiency of QF and FRC is our future work.",
                "In addition, the prediction techniques proposed in this paper have the potential of improving retrieval performance by combining with other IR techniques.",
                "For example, our techniques can be incorporated to popular query modification techniques such as query expansion and query relaxation.",
                "Guided by performance prediction, we can make a better decision on when to or how to modify queries to enhance retrieval effectiveness.",
                "We would like to carry out research in this direction in the future. 6.",
                "ACKNOWLEGMENTS This work was supported in part by the Center for Intelligent Information Retrieval, in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C0023, and in part by an award from Google.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor.",
                "In addition, we thank Donald Metzler for his valuable comments on this work. 7.",
                "REFERENCES [1] Y. Zhou ,W. B. Croft ,Ranking Robustness: A Novel Framework to Predict Query Performance, in Proceedings of CIKM 2006. [2] D.Carmel, E.Yom-Tov, A.Darlow,D.Pelleg, What Makes a Query Difficult?, in Proceedings of SIGIR 2006. [3] C.L.A.",
                "Clarke, F. Scholer, I.Soboroff, The TREC 2005 Terabyte Track, In the Online Proceedings of 2005 TREC. [4] B.",
                "He and I.Ounis.",
                "Inferring query performance using preretrieval predictors.",
                "In proceedings of the SPIRE 2004. [5] S. Tomlinson.",
                "Robust, Web and Terabyte Retrieval with Hummingbird SearchServer at TREC 2004.",
                "In the Online Proceedings of 2004 TREC. [6] S. Cronen-Townsend, Y. Zhou, W. B. Croft, Predicting Query Performance, in Proceedings of SIGIR 2002. [7] V.Vinay, I.J.Cox, N.Mill-Frayling,K.Wood, On Ranking the Effectiveness of Searcher, in Proceedings of SIGIR 2006. [8] D.Metzler, W.B.Croft, A Markov Random Filed Model for Term Dependencies, in Proceedings of SIGIR 2005. [9] D.Metzler, T.Strohman,Y.Zhou,W.B.Croft, Indri at TREC 2005: Terabyte Track, In the Online Proceedings of 2004 TREC. [10] P. Ogilvie and J. Callan, Combining document representations for known-item search, in Proceedings of SIGIR 2003. [11] A.Berger, J.Lafferty, Information retrieval as statistical translation, in Proceedings of SIGIR 1999. [12] Indri search engine : http://www.lemurproject.org/indri/ [13] I.J.",
                "Taneja: On Generalized Information Measures and Their Applications, Advances in Electronics and Electron Physics, Academic Press (USA), 76, 1989, 327-413. [14] S.Cronen-Townsend, Y. Zhou and Croft, W. B. , A Framework for Selective Query Expansion, in Proceedings of CIKM 2004. [15] F.Song, W.B.Croft, A general language model for information retrieval, in Proceedings of SIGIR 1999. [16] Personal email contact with Vishwa Vinay and our own experiments [17] E.Yom-Tov, S.Fine, D.Carmel, A.Darlow, Learning to Estimate Query Difficulty Including Applications to Missing Content Detection and Distributed Information retrieval, in Proceedings of SIGIR 2005"
            ],
            "original_annotated_samples": [
                "Query Performance Prediction in Web Search Environments Yun Zhou and W. Bruce Croft Department of Computer Science University of Massachusetts, Amherst {yzhou, croft}@cs.umass.edu ABSTRACT Current prediction techniques, which are generally designed for content-based queries and are typically evaluated on relatively <br>homogenous test collection</br>s of small sizes, face serious challenges in web search environments where collections are significantly more heterogeneous and different types of retrieval tasks exist."
            ],
            "translated_annotated_samples": [
                "Predicción del rendimiento de consultas en entornos de búsqueda web Yun Zhou y W. Bruce Croft Departamento de Ciencias de la Computación Universidad de Massachusetts, Amherst {yzhou, croft}@cs.umass.edu RESUMEN Las técnicas actuales de predicción, generalmente diseñadas para consultas basadas en contenido y evaluadas típicamente en colecciones de prueba relativamente homogéneas de tamaños pequeños, enfrentan serios desafíos en entornos de búsqueda web donde las colecciones son significativamente más heterogéneas y existen diferentes tipos de tareas de recuperación."
            ],
            "translated_text": "Predicción del rendimiento de consultas en entornos de búsqueda web Yun Zhou y W. Bruce Croft Departamento de Ciencias de la Computación Universidad de Massachusetts, Amherst {yzhou, croft}@cs.umass.edu RESUMEN Las técnicas actuales de predicción, generalmente diseñadas para consultas basadas en contenido y evaluadas típicamente en colecciones de prueba relativamente homogéneas de tamaños pequeños, enfrentan serios desafíos en entornos de búsqueda web donde las colecciones son significativamente más heterogéneas y existen diferentes tipos de tareas de recuperación. En este artículo, presentamos tres técnicas para abordar estos desafíos. Nos enfocamos en la predicción de rendimiento para dos tipos de consultas en entornos de búsqueda web: basadas en contenido y búsqueda de páginas por nombre. Nuestra evaluación se realiza principalmente en la colección GOV2. Además de evaluar nuestros modelos para los dos tipos de consultas por separado, consideramos una situación más desafiante y realista en la que los dos tipos de consultas se mezclan sin información previa sobre los tipos de consulta. Para ayudar en la predicción bajo la situación de consulta mixta, se adopta un clasificador de consultas novedoso. Los resultados muestran que nuestra predicción del rendimiento de la consulta web es sustancialmente más precisa que las técnicas de predicción actuales más avanzadas. Por consiguiente, nuestro artículo proporciona un enfoque práctico para predecir el rendimiento en entornos web del mundo real. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Formulación de Consultas Términos Generales Algoritmos, Experimentación, Teoría 1. La predicción del rendimiento de la consulta tiene muchas aplicaciones en una variedad de áreas de recuperación de información (IR), como mejorar la consistencia de la recuperación, el refinamiento de la consulta y la IR distribuida. La importancia de este problema ha sido reconocida por los investigadores en Recuperación de Información y recientemente se han propuesto varios métodos nuevos para la predicción [1, 2, 17]. La mayoría de los trabajos sobre predicción se han centrado en la tarea tradicional de recuperación ad-hoc, donde el rendimiento de la consulta se mide según la relevancia temática. Estos modelos de predicción se evalúan en colecciones de documentos de TREC que suelen consistir en no más de un millón de artículos de agencias de noticias relativamente homogéneos. Con la popularidad e influencia de la Web, las técnicas de predicción que funcionen bien para consultas de estilo web son altamente preferibles. Sin embargo, los entornos de búsqueda web plantean desafíos significativos para los modelos de predicción actuales que están principalmente diseñados para entornos tradicionales de TREC. Aquí delineamos algunos de estos desafíos. Primero, las colecciones web, que son mucho más grandes que las colecciones TREC convencionales, incluyen una variedad de documentos que son diferentes en muchos aspectos, como la calidad y el estilo. Las técnicas actuales de predicción pueden ser vulnerables a estas características de las colecciones web. Por ejemplo, la precisión de predicción reportada de la técnica de robustez de clasificación y la técnica de claridad en la colección GOV2 (una colección web grande) es significativamente peor en comparación con las otras colecciones de TREC [1]. En [2] se informa de una precisión de predicción similar en la colección GOV2 utilizando otra técnica, confirmando la dificultad de predecir el rendimiento de consultas en una gran colección web. Además, la búsqueda web va más allá del alcance de la tarea de recuperación ad hoc basada en la relevancia temática. Por ejemplo, la tarea de encontrar páginas nombradas (NP), que es una tarea de navegación, también es popular en la recuperación web. La predicción del rendimiento de la consulta para la tarea de NP sigue siendo necesaria, ya que el rendimiento de recuperación de NP está lejos de ser perfecto. De hecho, según el informe sobre la tarea NP del Terabyte Track de 2005 [3], aproximadamente el 40% de las consultas de prueba tienen un rendimiento deficiente (sin respuesta correcta en los primeros 10 resultados de búsqueda) incluso en la mejor ejecución del grupo líder. Hasta donde sabemos, poca investigación ha abordado explícitamente el problema de la predicción del rendimiento de consultas de NP. Los modelos de predicción actuales diseñados para consultas basadas en contenido serán menos efectivos para consultas de NP considerando las diferencias fundamentales entre ambas. En tercer lugar, en entornos de búsqueda web del mundo real, las consultas de los usuarios suelen ser una mezcla de diferentes tipos y, por lo general, no se dispone de información previa sobre el tipo de cada consulta. La situación de consulta mixta plantea nuevos problemas para la predicción del rendimiento de la consulta. Por ejemplo, es posible que necesitemos incorporar un clasificador de consultas en los modelos de predicción. A pesar de estos problemas, la capacidad para manejar esta situación es un paso crucial hacia convertir la predicción del rendimiento de consultas de un tema de investigación interesante en una herramienta práctica para la recuperación web. En este artículo, presentamos tres técnicas para abordar los desafíos mencionados que enfrentan los modelos de predicción actuales en entornos de búsqueda web. Nuestro trabajo se centra en la predicción del rendimiento de consultas para la tarea de recuperación basada en contenido (ad-hoc) y la tarea de encontrar páginas por nombre en el contexto de la recuperación web. Nuestra primera técnica, llamada ganancia de información ponderada (WIG), utiliza tanto características de términos individuales como de proximidad de términos para estimar la calidad de los documentos recuperados para la predicción. Observamos que WIG ofrece una precisión de predicción consistente en diversas colecciones de pruebas y tipos de consultas. Además, demostramos que se puede lograr una buena precisión de predicción para la situación de consulta mixta utilizando WIG con la ayuda de un clasificador de tipo de consulta. La retroalimentación de consultas y el cambio de rango inicial, que son nuestras técnicas de predicción segunda y tercera, funcionan bien para consultas basadas en contenido y consultas de NP respectivamente. Nuestras principales contribuciones incluyen: (1) una precisión de predicción considerablemente mejorada para consultas basadas en contenido web en comparación con varias técnicas de vanguardia. (2) nuevas técnicas para predecir con éxito el rendimiento de consultas NP. (3) una solución práctica y totalmente automática para predecir el rendimiento de consultas mixtas. Además, una contribución menor es que encontramos que el puntaje de robustez [1], que originalmente se propuso para la predicción de rendimiento, es útil para la clasificación de consultas. 2. TRABAJO RELACIONADO Como mencionamos en la introducción, recientemente se han propuesto varias técnicas de predicción que se centran en consultas basadas en contenido en la tarea de relevancia temática (ad-hoc). No conocemos ningún trabajo publicado que aborde otros tipos de consultas como las consultas NP, y mucho menos una mezcla de tipos de consultas. A continuación revisamos algunos modelos representativos. La principal dificultad de la predicción del rendimiento proviene del hecho de que muchos factores tienen un impacto en el rendimiento de recuperación. Cada factor afecta el rendimiento en diferente medida y el efecto general es difícil de predecir con precisión. Por lo tanto, no es sorprendente notar que características simples, como la frecuencia de los términos de consulta en la colección [4] y la IDF promedio de los términos de consulta [5], no predicen bien. De hecho, la mayoría de las técnicas exitosas se basan en medir algunas características del conjunto de documentos recuperados para estimar la dificultad del tema. Por ejemplo, la puntuación de claridad [6] mide la coherencia de una lista de documentos mediante la divergencia KL entre el modelo de consulta y el modelo de colección. El puntaje de robustez [1] cuantifica otra propiedad de una lista clasificada: la robustez de la clasificación en presencia de incertidumbre. Carmel et al. [2] encontraron que la distancia medida por la divergencia de Jensen-Shannon entre el conjunto de documentos recuperados y la colección está significativamente correlacionada con la precisión promedio. Vinay et al. [7] propusieron cuatro medidas para capturar la geometría de los documentos mejor clasificados para la predicción. La medida más efectiva es la sensibilidad a la perturbación del documento, una idea algo similar a la puntuación de robustez. Desafortunadamente, su forma de medir la sensibilidad no funciona igual de bien para consultas cortas y la precisión de la predicción disminuye considerablemente cuando se adopta una técnica de recuperación de vanguardia (como Okapi o un enfoque de modelado de lenguaje) en lugar del peso tf-idf utilizado en su artículo [16]. Las dificultades de aplicar estos modelos en entornos de búsqueda web ya han sido mencionadas. En este documento, principalmente adoptamos la puntuación de claridad y la puntuación de robustez como nuestras líneas base. Experimentalmente demostramos que las líneas base, incluso después de ser ajustadas cuidadosamente, son inadecuadas para el entorno web. Uno de nuestros modelos de predicción, WIG, está relacionado con el modelo de campo aleatorio de Markov (MRF) para la recuperación de información [8]. El modelo MRF modela directamente la dependencia de términos y se ha demostrado ser altamente efectivo en una variedad de colecciones de pruebas (especialmente colecciones web) y tareas de recuperación. Este modelo se utiliza para estimar la distribución conjunta de probabilidad sobre documentos y consultas, una parte importante de WIG. La superioridad de WIG sobre otras técnicas de predicción basadas en características unigramas, que se demostrará más adelante en nuestro artículo, coincide con la de MRF para recuperación. En otras palabras, es interesante notar que la dependencia de términos, cuando se modela adecuadamente, puede ser útil tanto para mejorar como para predecir el rendimiento de recuperación. 3. Modelos de predicción 3.1 Ganancia de Información Ponderada (WIG) Esta sección introduce un enfoque de ganancia de información ponderada que incorpora tanto características de términos únicos como de proximidad para predecir el rendimiento tanto en consultas basadas en contenido como en consultas de búsqueda de Páginas Nombradas (NP). Dado un conjunto de consultas Q={Qs} (s=1,2,..N) que incluye todas las posibles consultas de usuario y un conjunto de documentos D={Dt} (t=1,2…M), asumimos que cada par consulta-documento (Qs,Dt) es evaluado manualmente y se incluirá en una lista de relevancia si se determina que Qs es relevante para Dt. La probabilidad conjunta P(Qs,Dt) sobre las consultas Q y los documentos D denota la probabilidad de que el par (Qs,Dt) esté en la lista de relevancia. Tales suposiciones son similares a las utilizadas en [8]. Suponiendo que el usuario emite la consulta Qi ∈Q y los resultados de recuperación en respuesta a Qi es una lista clasificada L de documentos, calculamos la cantidad de información contenida en P(Qs,Dt) con respecto a Qi y L mediante la Ec.1, que es una variante de la entropía llamada entropía ponderada[13]. Los pesos en la ecuación 1 están determinados únicamente por Qi y L. En este documento, elegimos los pesos de la siguiente manera: L es el número de documentos que contiene la consulta, K es el límite superior, LT es el límite superior de documentos, y D es el peso si K está en otro caso. La clasificación de corte K es un parámetro en nuestro modelo que se discutirá más adelante. Por lo tanto, la ecuación 1 se puede simplificar de la siguiente manera: )3(),(log 1 ),( )( , ∑∈ −= LTD titsLQ Kt i DQP K DQH Desafortunadamente, la entropía ponderada ),(, tsLQ DQH i calculada por la ecuación 3, que representa la cantidad de información sobre la probabilidad de que los documentos mejor clasificados en L sean relevantes para la consulta Qi en promedio, no se puede comparar entre diferentes consultas, lo que la hace inapropiada para predecir directamente el rendimiento de la consulta. Para mitigar este problema, creamos una distribución de fondo P(Qs,C) sobre Q y D imaginando que cada documento en D es reemplazado por el mismo documento especial C que representa el uso promedio del lenguaje. En este documento, C se crea concatenando cada documento en D. A grosso modo, C es la colección (el conjunto de documentos) {Dt} sin límites de documentos. De manera similar, la entropía ponderada ),(, CQH sLQi calculada por la Ecuación 3 representa la cantidad de información sobre la probabilidad de que un documento promedio (representado por toda la colección) sea relevante para la consulta Qi. Ahora presentamos nuestro predictor de rendimiento WIG que es la ganancia de información ponderada [13] calculada como la diferencia entre ),(, tsLQ DQH i y ),(, CQH sLQi. Específicamente, dado el conjunto de consultas Qi, la colección C y la lista clasificada L de documentos, WIG se calcula de la siguiente manera: )4( ),( ),( log 1 ),( ),( log),( ),(),(),,( )(, ,, ∑∑ ∈ == −= LTD i ti ts s ts ts tsLQsLQi Kt ii CQP DQP KCQP DQP DQweight DQHCQHLCQWIG WIG calculado por la Ecuación 4 mide el cambio en la información sobre la calidad de la recuperación (en respuesta a la consulta Qi) desde un estado imaginario en el que solo se recupera un documento promedio hasta un estado posterior en el que se observan los resultados de búsqueda reales. Hacemos la hipótesis de que WIG está positivamente correlacionado con la efectividad de la recuperación porque una recuperación de alta calidad debería ser mucho más efectiva que simplemente devolver el documento promedio. El corazón de esta técnica es cómo estimar la distribución conjunta P(Qs, Dt). En el enfoque de modelado del lenguaje para la recuperación de información, se pueden aplicar fácilmente una variedad de modelos para estimar esta distribución. Aunque la mayoría de estos modelos se basan en la suposición de la bolsa de palabras, trabajos recientes sobre modelado de dependencia de términos bajo el marco de modelado de lenguaje han demostrado mejoras consistentes y significativas en la efectividad de recuperación sobre los modelos de bolsa de palabras. Inspirados por el éxito de incorporar características de proximidad de términos en modelos de lenguaje, decidimos adoptar un buen modelo de dependencia para estimar la probabilidad P(Qs,Dt). El modelo que elegimos para este artículo es el modelo de Campo Aleatorio de Markov (MRF) de Metzler y Crofts, que ya ha demostrado su superioridad en comparación con varias colecciones y diferentes tareas de recuperación [8,9]. Según el modelo MRF, log P(Qi, Dt) se puede escribir como )5()|(loglog),(log )( 1 ∑∈ +−= iQF tti DPZDQP ξ ξ ξλ donde Z1 es una constante que asegura que P(Qi, Dt) sume 1. F(Qi) consiste en un conjunto de características ampliadas a partir de la consulta original Qi. Por ejemplo, suponiendo que la consulta Qi es el programa de estudiantes talentosos, F(Qi) incluye características como programa y estudiante talentoso. Consideramos dos tipos de características: características de términos individuales T y características de proximidad P. Las características de proximidad incluyen la frase exacta (#1) y las características de ventana desordenada (#uwN) como se describe en [8]. Ten en cuenta que F(Qi) es la unión de T(Qi) y P(Qi). Para obtener más detalles sobre F(Qi), como por ejemplo cómo expandir la consulta original Qi a F(Qi), remitimos al lector a [8] y [9]. P(ξ|Dt) denota la probabilidad de que la característica ξ ocurra en Dt. Más detalles sobre P(ξ|Dt) se proporcionarán más adelante en esta sección. La elección de λξ es algo diferente a la utilizada en [8] ya que λξ desempeña un doble papel en nuestro modelo. El primer rol, que es el mismo que en [8], es ponderar entre las características de términos individuales y de proximidad. El otro rol, específico de nuestra tarea de predicción, es normalizar el tamaño de F(Qi). Encontramos que la siguiente estrategia de peso para λξ satisface los dos roles anteriores y generaliza bien en una variedad de colecciones y tipos de consultas. )6( )(, |)(| 1 )(, |)(| ⎪ ⎪ ⎩ ⎪ ⎪ ⎨ ⎧ ∈ − ∈ = i i T i i T QP QP QT QT ξ λ ξ λ λξ donde |T(Qi)| y |P(Qi)| denotan el número de características de términos únicos y de proximidad en F(Qi) respectivamente. La razón de elegir la función raíz cuadrada en el denominador de λξ es penalizar adecuadamente un conjunto de características de gran tamaño, haciendo que WIG sea más comparable entre consultas de diversas longitudes. λT es un parámetro fijo y se establece en 0.8 según [8] a lo largo de este documento. De manera similar, log P(Qi,C) se puede escribir como: )7()|(loglog),(log )( 2 ∑∈ +−= iQF i CPZCQP ξ ξ ξλ Cuando se eliminan las constantes Z1 y Z2, el WIG calculado en la Ecuación 4 se puede reescribir de la siguiente manera al sustituir la Ecuación 5 y la Ecuación 7: )8( )|( )|( log 1 ),,( )( )( ∑ ∑∈ ∈ = LTD QF t i Kt i CP DP K LCQWIG ξ ξ ξ ξ λ Una de las ventajas del WIG sobre otras técnicas es que puede manejar tanto consultas basadas en contenido como en NP. Basándose en el tipo (o el tipo predicho) de Qi, el cálculo de WIG en la Ecuación 8 difiere en dos aspectos: (1) cómo estimar P(ξ|Dt) y P(ξ|C), y (2) cómo elegir K. Para consultas basadas en contenido, P(ξ|C) se estima por la frecuencia relativa del rasgo ξ en la colección C en su totalidad. La estimación de P(ξ|Dt) es la misma que en [8]. Es decir, estimamos P(ξ|Dt) mediante la frecuencia relativa del rasgo ξ en Dt suavizada linealmente con la frecuencia de colección P(ξ|C). La K en la Ec.8 se trata como un parámetro libre. Ten en cuenta que K es el único parámetro libre en el cálculo de WIG para consultas basadas en contenido, ya que se asume que todos los parámetros involucrados en P(ξ|Dt) están fijos al tomar los valores sugeridos en [8]. En cuanto a las consultas de NP, hacemos uso de la estructura del documento para estimar P(ξ|Dt) y P(ξ|C) mediante el llamado modelo de mezcla de lenguaje propuesto en [10] e incorporado en el modelo MRF para la recuperación de Named-Page en [9]. La idea básica es que un documento (colección) se divide en varios campos como el campo del título, el campo del cuerpo principal y el campo de los encabezados. P(ξ|Dt) y P(ξ|C) se estiman mediante una combinación lineal de los modelos de lenguaje de cada campo. Debido a limitaciones de espacio, remitimos al lector a [9] para más detalles. Adoptamos el mismo conjunto exacto de parámetros utilizados en [9] para la estimación. En cuanto a K en la Ec.8, establecemos K en 1 porque la tarea de encontrar Named-Page se centra fuertemente en el documento clasificado en primer lugar. Por consiguiente, no hay parámetros libres en el cálculo de WIG para consultas de NP. 3.2 Retroalimentación de consultas En esta sección, presentamos otra técnica llamada retroalimentación de consultas (QF) para la predicción. Suponga que un usuario emite la consulta Q a un sistema de recuperación y se devuelve una lista clasificada L de documentos. Vemos el sistema de recuperación como un canal ruidoso. Específicamente, asumimos que la salida del canal es L y la entrada es Q. Después de pasar por el canal, Q se corrompe y se transforma en la lista clasificada L. Al pensar en el proceso de recuperación de esta manera, el problema de predecir la eficacia de la recuperación se convierte en la tarea de evaluar la calidad del canal. En otras palabras, la predicción se convierte en encontrar una forma de medir el grado de corrupción que surge cuando Q se transforma en L. Dado que calcular directamente el grado de corrupción es difícil, abordamos este problema mediante aproximaciones. Nuestra idea principal es que medimos en qué medida la información sobre Q puede ser recuperada de L bajo la suposición de que solo se observa L. Específicamente, diseñamos un decodificador que pueda traducir con precisión L de vuelta a la nueva consulta Q y la similitud S entre la consulta original Q y la nueva consulta Q se adopta como un predictor de rendimiento. Este es un bosquejo de cómo la técnica QF predice el rendimiento de la consulta. Antes de completar más detalles, discutimos brevemente por qué este método funcionaría. Existe una relación entre la similitud S definida anteriormente y el rendimiento de recuperación. Por un lado, si la recuperación se ha desviado del sentido original de la consulta Q, la nueva consulta Q extraída de la lista clasificada L en respuesta a Q sería muy diferente de la consulta original Q. Por otro lado, una consulta destilada de una lista clasificada que contiene muchos documentos relevantes es probable que sea similar a la consulta original. Se proporcionarán más ejemplos en apoyo de la relación más adelante. A continuación detallamos cómo construir el decodificador y cómo medir la similitud S. En esencia, el objetivo del decodificador es comprimir la lista clasificada L en unos pocos términos informativos que deberían representar el contenido de los documentos mejor clasificados en L. Nuestro enfoque para este objetivo es representar la lista clasificada L mediante un modelo de lenguaje (distribución de términos). Luego, los términos se clasifican según su contribución a la divergencia KL (Kullback-Leibler) de los modelos de lenguaje con respecto al modelo de la colección de fondo. Los términos mejor clasificados serán elegidos para formar la nueva consulta Q. Este enfoque es similar al utilizado en la Sección 4.1 de [11]. Específicamente, tomamos tres pasos para comprimir la lista clasificada L en la consulta Q sin hacer referencia a la consulta original. 1. Adoptamos el modelo de lenguaje de lista clasificada [14] para estimar un modelo de lenguaje basado en la lista clasificada L. El modelo puede escribirse como: )9()|()|()|( ∑∈ = LD LDPDwPLwP donde w es cualquier término y D es un documento. La probabilidad de que ocurra el evento D dado el evento L se estima mediante una función linealmente decreciente del rango del documento D. Cada término en P(w|L) está clasificado por la siguiente contribución de divergencia de Kullback-Leibler: )10( )|( )|( log)|( CwP LwP LwP donde P(w|C) es el modelo de colección estimado por la frecuencia relativa del término w en la colección C en su totalidad. 3. Los términos N principales clasificados por Eq.10 forman una consulta ponderada Q={(wi,ti)} i=1,N, donde wi denota el término clasificado en la posición i y el peso ti es la contribución de la divergencia KL de wi en Eq. 10. Barco de crucero daño vida marina. Estos ejemplos indican cómo la similitud entre la consulta original y la nueva se correlaciona con el rendimiento de recuperación. El parámetro N en el paso 3 se establece en 20 de manera empírica y elegir un valor más grande de N es innecesario ya que los pesos después de los primeros 20 suelen ser demasiado pequeños para marcar alguna diferencia. Para medir la similitud entre la consulta original Q y la nueva consulta Q, primero utilizamos Q para recuperar información de la misma colección. Se adopta una variante del modelo de probabilidad de consulta [15] para la recuperación. Es decir, los documentos se clasifican por: )11()|()|( ),( ∑∈ = Qtw t i ii i DwPDQP donde wi es un término en Q y ti es el peso asociado. D es un documento. Que L denote la nueva lista clasificada devuelta desde la recuperación anterior. La similitud se mide por la superposición de documentos en L y L. Específicamente, el porcentaje de documentos en los primeros K documentos de L que también están presentes en los primeros K documentos en L. El valor de corte K se trata como un parámetro libre. Aquí resumimos cómo la técnica QF predice el rendimiento dado una consulta Q y la lista clasificada asociada L. Primero obtenemos una consulta ponderada Q comprimida a partir de L mediante los tres pasos anteriores. Luego usamos Q para realizar la recuperación y la nueva lista clasificada es L. La superposición de documentos en L y L se utiliza para la predicción. 3.3 Primer Cambio de Rango (FRC) En esta sección, proponemos un método llamado primer cambio de rango (FRC) para la predicción de rendimiento para consultas de NP. Este método se deriva de la técnica de robustez de clasificación [1] que está principalmente diseñada para consultas basadas en contenido. Cuando se aplica directamente a consultas de NP, la técnica de robustez será menos efectiva porque tiene en cuenta todos los documentos mejor clasificados en su totalidad, mientras que las consultas de NP suelen tener solo un único documento relevante. En cambio, nuestra técnica se centra en el documento de rango uno mientras se mantiene la idea principal del método de robustez. Específicamente, el seudocódigo para calcular FRC se muestra en la figura 1. Lista clasificada L={Di} donde i=1,100. Di denota el documento clasificado en la posición i. (2) consulta Q 1 inicializar: (1) establecer el número de intentos J=100000 (2) contador c=0; 2 para i=1 a J 3 Perturbar cada documento en L, que el resultado sea un conjunto F={Di} donde Di denota la versión perturbada de Di. 4 Realizar la recuperación con la consulta Q en el conjunto F 5 c=c+1 si y solo si D1 está clasificado primero en el paso 4 6 fin del para 7 devolver la proporción c/J Figura 1: pseudo-código para calcular FRC FRC aproxima la probabilidad de que el documento clasificado en primer lugar en la lista original L permanezca clasificado primero incluso después de que los documentos sean perturbados. Cuanto mayor sea la probabilidad, más confianza tenemos en el documento clasificado en primer lugar. Por otro lado, en el caso extremo de un ranking aleatorio, la probabilidad sería tan baja como 0.5. Esperamos que FRC tenga una asociación positiva con el rendimiento de la consulta de NP. Adoptamos [1] para implementar el paso de perturbación del documento (paso 4 en la Fig. 1) utilizando distribuciones de Poisson. Para más detalles, remitimos al lector a [1]. 4. EVALUACIÓN Ahora presentamos los resultados de predecir el rendimiento de la consulta por nuestros modelos. Tres técnicas de vanguardia son adoptadas como nuestras líneas base. Evaluamos nuestras técnicas en una variedad de configuraciones de recuperación web. Como se mencionó anteriormente, consideramos dos tipos de consultas, es decir, consultas basadas en contenido (CB) y consultas de búsqueda de páginas con nombre (NP). Primero, supongamos que se conocen los tipos de consultas. Investigamos la correlación entre el rendimiento de recuperación predicho y el rendimiento real para ambos tipos de consultas por separado. Los resultados muestran que nuestros métodos producen mejoras considerablemente superiores a los valores de referencia. Luego consideramos un escenario más desafiante donde no hay información previa sobre los tipos de consultas disponibles. Se consideran dos subcasos. En el primero, existe solo un tipo de consulta pero el tipo real es desconocido. Suponemos una mezcla de los dos tipos de consultas en el segundo caso. Demostramos que nuestros modelos logran una buena precisión en este escenario exigente, lo que hace que la predicción sea práctica en un entorno de búsqueda web del mundo real. 4.1 Configuración Experimental Nuestra evaluación se centra en la colección GOV2 que contiene alrededor de 25 millones de documentos obtenidos de sitios web en el dominio .gov durante 2004 [3]. Creamos dos tipos de conjuntos de datos para consultas de CB y consultas de NP respectivamente. Para el tipo CB, utilizamos los temas ad-hoc de las pistas Terabyte de 2004, 2005 y 2006 y los nombramos TB04-adhoc, TB05-adhoc y TB06-adhoc respectivamente. Además, también utilizamos los temas ad-hoc de la pista Robusta de 2004 (RT04) para probar la adaptabilidad de nuestras técnicas a un entorno no web. Para consultas de NP, utilizamos los temas de búsqueda de páginas nombradas de las Terabyte Tracks de 2005 y 2006 y los denominamos TB05-NP y TB06-NP respectivamente. Todas las consultas utilizadas en nuestros experimentos son títulos de temas de TREC, ya que nos centramos en la recuperación web. La Tabla 3 resume los conjuntos de datos anteriores. La recuperación del rendimiento de las consultas individuales basadas en contenido y NP se mide mediante la precisión promedio y la tasa recíproca de la primera respuesta correcta, respectivamente. Hacemos uso del modelo de campo aleatorio de Markov tanto para la recuperación ad-hoc como para la recuperación de páginas nombradas. Adoptamos la misma configuración de parámetros de recuperación utilizada en [8,9]. El motor de búsqueda Indri [12] se utiliza para todos nuestros experimentos. Aunque no se informa aquí, también probamos el modelo de probabilidad de consulta para la recuperación ad-hoc y encontramos que los resultados cambian poco debido a la correlación muy alta entre el rendimiento de las consultas obtenido por los dos modelos de recuperación (0.96 medido por el coeficiente de Pearson). 4.2 Tipos de Consultas Conocidos Supongamos que se conocen los tipos de consultas. Tratamos cada tipo de consulta por separado y medimos la correlación con la precisión promedio (o el rango recíproco en el caso de consultas NP). Adoptamos la prueba de correlación de Pearson que refleja el grado de relación lineal entre el rendimiento de recuperación predicho y real. 4.2.1 Métodos de Consultas Basadas en Contenido Claridad Robusta JSD WIG QF WIG +QF TB04+0 5 adhoc 0.333 0.317 0.362 0.574 0.480 0.637 TB06 adhoc 0.076 0.294 N/A 0.464 0.422 0.511 Tabla 4: Coeficientes de correlación de Pearson para la correlación con la precisión promedio en las Pistas de Terabyte (ad-hoc) para la puntuación de claridad, puntuación de robustez, el método basado en JSD (citamos directamente la puntuación informada en [2]), WIG, retroalimentación de consulta (QF) y una combinación lineal de WIG y QF. Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. La Tabla 4 muestra la correlación con la precisión promedio en dos conjuntos de datos: uno es una combinación de TB04-adhoc y TB05-adhoc (100 temas en total) y el otro es TB06-adhoc (50 temas). La razón por la que juntamos TB04-adhoc y TB05-adhoc es para hacer nuestros resultados comparables a [2]. Nuestros puntos de referencia son la puntuación de claridad (claridad) [6], la puntuación de robustez (robust) [1] y el método basado en JSD (JSD) [2]. Para la puntuación de claridad y robustez, hemos probado diferentes configuraciones de parámetros y reportamos los coeficientes de correlación más altos que hemos encontrado. Citamos directamente el resultado del método basado en JSD reportado en [2]. La tabla también muestra los resultados para el método de Ganancia de Información Ponderada (WIG) y el método de Retroalimentación de Consulta (QF) para predecir consultas basadas en contenido. Como describimos en la sección anterior, tanto WIG como QF tienen un parámetro libre para ajustar, es decir, el rango de corte K. Entrenamos el parámetro en un conjunto de datos y lo probamos en el otro. Al combinar WIG y QF, se utiliza una combinación lineal simple y el peso de la combinación se aprende a partir del conjunto de datos de entrenamiento. A partir de estos resultados, podemos ver que nuestros métodos son considerablemente más precisos en comparación con los baselines. También observamos que se obtienen mejoras adicionales a partir de la combinación de WIG y QF, lo que sugiere que miden diferentes propiedades del proceso de recuperación que se relacionan con el rendimiento. Descubrimos que nuestros métodos se generalizan bien en TB06-adhoc, mientras que la correlación del puntaje de claridad con el rendimiento de recuperación en este conjunto de datos es considerablemente peor. Una investigación adicional muestra que la precisión promedio media de TB06-adhoc es de 0.342 y es aproximadamente un 10% mejor que la del primer conjunto de datos. Mientras que los otros tres métodos suelen considerar los 100 mejores documentos o menos de una lista clasificada, el método de claridad generalmente necesita los 500 mejores documentos o más para medir adecuadamente la coherencia de una lista clasificada. Un promedio de precisión media más alto hace que las listas clasificadas recuperadas por diferentes consultas sean más similares en términos de coherencia en el nivel de los primeros 500 documentos. Creemos que esta es la razón principal de la baja precisión de la puntuación de claridad en el segundo conjunto de datos. Aunque este documento se centra en un entorno de búsqueda web, es deseable que nuestras técnicas funcionen de manera consistente en otras situaciones. Con este fin, examinamos la efectividad de nuestras técnicas en la pista Robust 2004. Para nuestros métodos, dividimos equitativamente todas las consultas de prueba en cinco grupos y realizamos validación cruzada de cinco pliegues. Cada vez que utilizamos un grupo para el entrenamiento y los otros cuatro grupos para las pruebas. Hacemos uso de todas las consultas para nuestros dos puntos de referencia, es decir, la puntuación de claridad y la puntuación de robustez. Los parámetros para nuestras líneas base son los mismos que los utilizados en [1]. Los resultados mostrados en la Tabla 5 demuestran que la precisión de predicción de nuestros métodos está a la par con la de las dos líneas base sólidas. Claridad Robusta WIG QF 0.464 0.539 0.468 0.464 Tabla 5: Comparación de los coeficientes de correlación de Pearson en la pista Robusta de 2004 para la puntuación de claridad, puntuación de robustez, WIG y retroalimentación de consulta (QF). Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. Además, examinamos la sensibilidad de predicción de nuestros métodos al rango de corte K. Con respecto a WIG, es bastante robusto a K en las Pistas de Terabyte (2004-2006) mientras prefiere un valor pequeño de K como 5 en la Pista Robusta de 2004. En otras palabras, un valor pequeño de K es una elección casi óptima para ambos tipos de pistas. Teniendo en cuenta que todos los demás parámetros involucrados en WIG están fijos y, consecuentemente, son los mismos para los dos casos, esto significa que WIG puede lograr una precisión de predicción casi óptima en dos situaciones considerablemente diferentes con exactamente la misma configuración de parámetros. En cuanto a QF, prefiere un valor más grande de K, como 100 en las Pistas de Terabyte y un valor más pequeño de K, como 25 en la Pista Robusta de 2004. 4.2.2 Consultas NP Adoptamos WIG y el primer cambio de rango (FRC) para predecir el rendimiento de las consultas NP. También intentamos una combinación lineal de los dos como en la sección anterior. El peso de la combinación se obtiene del otro conjunto de datos. Utilizamos la correlación con los rangos recíprocos medidos por la prueba de correlación de Pearson para evaluar la calidad de la predicción. Los resultados se presentan en la Tabla 6. Nuevamente, nuestros puntos de referencia son la puntuación de claridad y la puntuación de robustez. Para hacer una comparación justa, ajustamos la puntuación de claridad de diferentes maneras. Descubrimos que utilizar el documento clasificado en primer lugar para construir el modelo de consulta produce la mejor precisión de predicción. También intentamos utilizar la estructura del documento mediante la combinación de modelos de lenguaje mencionados en la sección 3.1. Se obtuvo una pequeña mejora. Los coeficientes de correlación para la puntuación de claridad reportados en la Tabla 6 son los mejores que hemos encontrado. Como podemos ver, nuestros métodos superan considerablemente la técnica de puntuación de claridad en ambas ejecuciones. Esto confirma nuestra intuición de que el uso de una medida basada en la coherencia como el puntaje de claridad es inapropiado para las consultas de NP. Claridad de métodos robusta. Tabla 6: Coeficientes de correlación de Pearson para la correlación con rangos recíprocos en las pistas de Terabyte (NP) para la puntuación de claridad, puntuación de robustez, WIG, el primer cambio de rango (FRC) y una combinación lineal de WIG y FRC. Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. En cuanto a la puntuación de robustez, también ajustamos los parámetros y reportamos el mejor que hemos encontrado. Observamos una correlación negativa interesante y sorprendente con los rangos recíprocos. Explicamos este hallazgo brevemente. Un puntaje de robustez alto significa que varios documentos de alta clasificación en la lista clasificada original siguen estando muy bien clasificados después de perturbar los documentos. La existencia de tales documentos es una buena señal de alto rendimiento para consultas basadas en contenido, ya que estas consultas suelen contener una serie de documentos relevantes [1]. Sin embargo, en lo que respecta a las consultas de NP, una diferencia fundamental es que solo hay un documento relevante para cada consulta. La existencia de tales documentos puede confundir la función de clasificación y llevar a un bajo rendimiento de recuperación. Aunque existe una correlación negativa con el rendimiento de recuperación, la fuerza de la correlación es más débil y menos consistente en comparación con nuestros métodos, como se muestra en la Tabla 6. Basándonos en el análisis anterior, podemos ver que las técnicas de predicción actuales como la puntuación de claridad y la puntuación de robustez, que están principalmente diseñadas para consultas basadas en contenido, enfrentan desafíos significativos y son insuficientes para manejar consultas de NP. Nuestras dos técnicas propuestas para consultas de NP demuestran consistentemente una buena precisión de predicción, mostrando un éxito inicial en la resolución del problema de predecir el rendimiento para consultas de NP. Otro punto que queremos destacar es que el método WIG funciona bien para ambos tipos de consultas, una propiedad deseable que la mayoría de las técnicas de predicción carecen. 4.3 Tipos de Consultas Desconocidos En esta sección, realizamos dos tipos de experimentos sin acceso a etiquetas de tipo de consulta. Primero, asumimos que solo existe un tipo de consulta pero el tipo es desconocido. Segundo, experimentamos con una mezcla de consultas basadas en contenido y NP. Las dos subsecciones siguientes informarán los resultados para las dos condiciones respectivamente. 4.3.1 Solo existe un tipo. Suponemos que todas las consultas son del mismo tipo, es decir, son consultas NP o consultas basadas en contenido. Elegimos WIG para tratar este caso porque muestra una buena precisión de predicción para ambos tipos de consultas en la sección anterior. Consideramos dos casos: (1) CB: todas las 150 consultas de títulos de la tarea ad-hoc de las pistas Terabyte 2004-2006 (2) NP: todas las 433 consultas de NP de la tarea de búsqueda de páginas nombradas de las pistas Terabyte 2005 y 2006. Tomamos una estrategia simple etiquetando todas las consultas en cada caso como el mismo tipo (ya sea NP o CB) independientemente de su tipo real. El cálculo de WIG se basará en el tipo de consulta etiquetado en lugar del tipo real. Hay cuatro posibilidades con respecto a la relación entre el tipo real y el tipo etiquetado. La correlación con el rendimiento de recuperación bajo las cuatro posibilidades se presenta en la Tabla 7. Por ejemplo, el valor 0.445 en la intersección entre la segunda fila y la tercera columna muestra el coeficiente de correlación de Pearson para la correlación con la precisión promedio cuando las consultas basadas en contenido se etiquetan incorrectamente como del tipo NP. Basándonos en estos resultados, recomendamos tratar todas las consultas como del tipo NP cuando solo existe un tipo de consulta y la clasificación precisa de la consulta no es factible, considerando el riesgo de que se produzca una gran pérdida de precisión si las consultas NP se etiquetan incorrectamente como consultas basadas en contenido. Estos resultados también demuestran la fuerte adaptabilidad de WIG a diferentes tipos de consultas. Tabla 7: Comparación de los coeficientes de correlación de Pearson para la correlación con el rendimiento de recuperación en cuatro posibilidades en las pistas de Terabyte (NP). Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. 4.3.2 Una mezcla de consultas basadas en contenido y NP Una mezcla de los dos tipos de consultas es una situación más realista que un motor de búsqueda web encontrará. Evaluamos la precisión de la predicción por la exactitud con la que se pueden identificar las consultas de bajo rendimiento mediante el método de predicción, asumiendo que los tipos de consulta reales son desconocidos (pero podemos predecir los tipos de consulta). Esta es una tarea desafiante porque tanto el rendimiento predicho como el real para un tipo de consulta pueden ser incomparables con el de otro tipo. A continuación discutimos cómo implementar nuestra evaluación. Creamos un conjunto de consultas que consiste en todas las 150 consultas de título ad-hoc de Terabyte Track 2004-2006 y todas las 433 consultas NP de Terabyte Track 2005&2006. Dividimos las consultas en el conjunto en clases: buenas (mejores que el 50% de las consultas del mismo tipo en términos de rendimiento de recuperación) y malas (de lo contrario). Según estos estándares, una consulta de NP con un rango recíproco superior a 0.2 o una consulta basada en contenido con una precisión promedio superior a 0.315 se considerarán como buenas. Entonces, cada vez que seleccionamos aleatoriamente una consulta Q del grupo con una probabilidad p de que Q esté basada en el contenido. Las consultas restantes se utilizan como datos de entrenamiento. Primero decidimos el tipo de consulta Q de acuerdo con un clasificador de consultas. Es decir, el clasificador de consultas nos dice si la consulta Q es de tipo NP o basada en contenido. Basándose en el tipo de consulta predicho y en la puntuación calculada para la consulta Q por una técnica de predicción, se toma una decisión binaria sobre si la consulta Q es buena o mala al compararla con el umbral de puntuación del tipo de consulta predicho obtenido de los datos de entrenamiento. La precisión de la predicción se mide por la exactitud de la decisión binaria. En nuestra implementación, tomamos repetidamente una consulta de prueba del conjunto de consultas y la precisión de la predicción se calcula como el porcentaje de decisiones correctas, es decir, una consulta buena (mala) se predice como buena (mala). Es obvio que adivinar al azar llevará a una precisión del 50%. Tomemos el método WIG como ejemplo para ilustrar el proceso. Dos umbrales de WIG (uno para consultas de NP y otro para consultas basadas en contenido) se entrenan maximizando la precisión de predicción en los datos de entrenamiento. Cuando una consulta de prueba es etiquetada como del tipo NP (CB) por el clasificador de tipo de consulta, se predecirá que es buena solo si la puntuación WIG para esta consulta está por encima del umbral de NP (CB). Se seguirán procedimientos similares para otras técnicas de predicción. Ahora presentamos brevemente el clasificador de tipo de consulta automático utilizado en este artículo. Encontramos que la puntuación de robustez, aunque originalmente propuesta para la predicción de rendimiento, es un buen indicador de tipos de consultas. Observamos que, en promedio, las consultas basadas en contenido tienen una puntuación de robustez mucho más alta que las consultas de NP. Por ejemplo, la Figura 2 muestra las distribuciones de puntajes de robustez para consultas basadas en NP y en contenido. Según este hallazgo, el clasificador de puntuación de robustez adjuntará una etiqueta NP (CB) a la consulta si la puntuación de robustez para la consulta está por debajo (por encima) de un umbral entrenado a partir de los datos de entrenamiento. 0 0.5 1 1.5 2 2.5 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 NP Basado en contenido Figura 2: Distribución de puntuaciones de robustez para consultas NP y CB. Las consultas NP son los 252 temas NP del Terabyte Track de 2005. Las consultas basadas en contenido son los 150 títulos ad-hoc de las pistas Terabyte 2004-2006. Las distribuciones de probabilidad son estimadas por el método de estimación de densidad de Kernel. Estrategias Robustas WIG-1 WIG-2 WIG-3 Óptima p=0.6 0.565 0.624 0.665 0.684 0.701 P=0.4 0.567 0.633 0.654 0.673 0.696 Tabla 8: Comparación de la precisión de predicción para cinco estrategias en la situación de consulta mixta. Dos formas de muestrear una consulta de la piscina: (1) la consulta muestreada es basada en contenido con una probabilidad p=0.6 (es decir, la consulta es NP con una probabilidad de 0.4) (2) establecer la probabilidad p=0.4. Consideramos cinco estrategias en nuestros experimentos. En la primera estrategia (denominada robusta), utilizamos la puntuación de robustez para la predicción del rendimiento de la consulta con la ayuda de un clasificador de consultas perfecto que siempre mapea correctamente una consulta en una de las dos categorías (es decir, NP o CB). Esta estrategia representa el nivel de precisión de predicción que las técnicas actuales de predicción pueden lograr en una condición ideal en la que se conocen los tipos de consulta. En las tres estrategias siguientes, se adopta el método WIG para predecir el rendimiento. La diferencia entre los tres es que se utilizan tres clasificadores de consulta diferentes para cada estrategia: (1) el clasificador siempre clasifica una consulta en el tipo NP. (2) el clasificador de Probabilidad de Densidad de Puntuación Robusta es el clasificador de puntuación robusta mencionado anteriormente. (3) el clasificador es perfecto. Estas tres estrategias están designadas como WIG-1, WIG-2 y WIG-3 respectivamente. La razón por la que estamos interesados en WIG-1 se basa en los resultados de la sección 4.3.1. En la última estrategia (denominada Óptima) que sirve como un límite superior de lo bien que podemos hacer hasta ahora, hacemos pleno uso de nuestras técnicas de predicción para cada tipo de consulta asumiendo que un clasificador de consultas perfecto está disponible. Específicamente, combinamos linealmente WIG y QF para consultas basadas en contenido y WIG y FRC para consultas de NP. Los resultados de las cinco estrategias se muestran en la Tabla 8. Para cada estrategia, intentamos dos formas de muestrear una consulta del conjunto: (1) la consulta muestreada es CB con probabilidad p=0.6 (la consulta es NP con probabilidad 0.4) (2) establecer la probabilidad p=0.4. De la Tabla 8 podemos ver que en términos de precisión de predicción, WIG-2 (el método WIG con el clasificador de consultas automático) no solo es mejor que los dos primeros casos, sino que también está cerca de WIG-3 donde se asume un clasificador perfecto. Se observan algunas mejoras adicionales sobre WIG-3 cuando se combinan con otras técnicas de predicción. El mérito de WIG-2 es que proporciona una solución práctica para identificar automáticamente consultas con bajo rendimiento en un entorno de búsqueda web con tipos de consultas mixtos, lo cual representa considerables obstáculos para las técnicas de predicción tradicionales. CONCLUSIONES Y TRABAJOS FUTUROS Hasta donde sabemos, nuestro artículo es el primero en explorar a fondo la predicción del rendimiento de consultas en entornos de búsqueda web. Demostramos que nuestros modelos resultaron en una mayor precisión de predicción que las técnicas previamente publicadas no especialmente diseñadas para escenarios de búsqueda en la web. En este artículo, nos enfocamos en dos tipos de consultas en la búsqueda web: consultas basadas en contenido y consultas de búsqueda de Páginas-Nombradas (NP), correspondientes a la tarea de recuperación ad-hoc y la tarea de búsqueda de Páginas-Nombradas respectivamente. Para ambos tipos de consultas web, se demostró que nuestros modelos de predicción son sustancialmente más precisos que las técnicas actuales más avanzadas. Además, consideramos un caso más realista en el que no se dispone de información previa sobre los tipos de consultas. Demostramos que el método WIG es particularmente adecuado para esta situación. Considerando la adaptabilidad de WIG a una variedad de colecciones y tipos de consultas, uno de nuestros planes futuros es aplicar este método para predecir la preferencia del usuario por los resultados de búsqueda en datos realistas recopilados de un motor de búsqueda comercial. Además de la precisión, otro problema importante con el que las técnicas de predicción tienen que lidiar en un entorno web es la eficiencia. Afortunadamente, dado que el puntaje WIG se calcula solo sobre los términos y frases que aparecen en la consulta, este cálculo puede realizarse de manera muy eficiente con el soporte de un índice. Por otro lado, el cálculo de QF y FRC es relativamente menos eficiente ya que QF necesita recuperar toda la colección dos veces y FRC necesita clasificar repetidamente los documentos perturbados. Cómo mejorar la eficiencia de QF y FRC es nuestro trabajo futuro. Además, las técnicas de predicción propuestas en este documento tienen el potencial de mejorar el rendimiento de recuperación al combinarse con otras técnicas de RI. Por ejemplo, nuestras técnicas pueden ser incorporadas a técnicas populares de modificación de consultas como la expansión de consultas y la relajación de consultas. Guiados por la predicción del rendimiento, podemos tomar una mejor decisión sobre cuándo o cómo modificar las consultas para mejorar la efectividad de la recuperación. Nos gustaría llevar a cabo investigaciones en esta dirección en el futuro. AGRADECIMIENTOS Este trabajo fue apoyado en parte por el Centro de Recuperación de Información Inteligente, en parte por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el contrato número HR0011-06-C0023, y en parte por un premio de Google. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son del autor y no necesariamente reflejan las del patrocinador. Además, agradecemos a Donald Metzler por sus valiosos comentarios sobre este trabajo. 7. REFERENCIAS [1] Y. Zhou, W. B. Croft, Ranking Robustness: Un nuevo marco para predecir el rendimiento de consultas, en Actas de CIKM 2006. [2] D. Carmel, E. Yom-Tov, A. Darlow, D. Pelleg, ¿Qué hace que una consulta sea difícil?, en Actas de SIGIR 2006. [3] C. L. A. Clarke, F. Scholer, I. Soboroff, La pista Terabyte TREC 2005, En las Actas en Línea de TREC 2005. [4] B. Él y yo. Inferir el rendimiento de la consulta utilizando predictores de preretrieval. En actas de SPIRE 2004. [5] S. Tomlinson. Recuperación robusta de la web y terabytes con Hummingbird SearchServer en TREC 2004. En las Actas en Línea de TREC 2004. [6] S. Cronen-Townsend, Y. Zhou, W. B. Croft, Prediciendo el Rendimiento de Consultas, en las Actas de SIGIR 2002. [7] V. Vinay, I. J. Cox, N. Mill-Frayling, K. Wood, Sobre la Clasificación de la Efectividad del Buscador, en las Actas de SIGIR 2006. [8] D. Metzler, W. B. Croft, Un Modelo de Campo Aleatorio de Markov para Dependencias de Términos, en las Actas de SIGIR 2005. [9] D. Metzler, T. Strohman, Y. Zhou, W. B. Croft, Indri en TREC 2005: Terabyte Track, en las Actas en Línea de TREC 2004. [10] P. Ogilvie y J. Callan, Combinando representaciones de documentos para búsqueda de elementos conocidos, en las Actas de SIGIR 2003. [11] A. Berger, J. Lafferty, Recuperación de información como traducción estadística, en las Actas de SIGIR 1999. [12] Motor de búsqueda Indri: http://www.lemurproject.org/indri/ [13] I. J. Taneja: Sobre medidas de información generalizadas y sus aplicaciones, Avances en Electrónica y Física de Electrones, Academic Press (EE. UU.), 76, 1989, 327-413. [14] S. Cronen-Townsend, Y. Zhou y Croft, W. B., Un marco para la expansión selectiva de consultas, en Actas de CIKM 2004. [15] F. Song, W. B. Croft, Un modelo de lenguaje general para la recuperación de información, en Actas de SIGIR 1999. [16] Contacto por correo electrónico personal con Vishwa Vinay y nuestros propios experimentos [17] E. Yom-Tov, S. Fine, D. Carmel, A. Darlow, Aprendiendo a estimar la dificultad de la consulta incluyendo aplicaciones para la detección de contenido faltante y la recuperación de información distribuida, en Actas de SIGIR 2005. ",
            "candidates": [],
            "error": [
                []
            ]
        },
        "content-based and named-page finding": {
            "translated_key": "búsqueda de páginas por nombre",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Query Performance Prediction in Web Search Environments Yun Zhou and W. Bruce Croft Department of Computer Science University of Massachusetts, Amherst {yzhou, croft}@cs.umass.edu ABSTRACT Current prediction techniques, which are generally designed for content-based queries and are typically evaluated on relatively homogenous test collections of small sizes, face serious challenges in web search environments where collections are significantly more heterogeneous and different types of retrieval tasks exist.",
                "In this paper, we present three techniques to address these challenges.",
                "We focus on performance prediction for two types of queries in web search environments: <br>content-based and named-page finding</br>.",
                "Our evaluation is mainly performed on the GOV2 collection.",
                "In addition to evaluating our models for the two types of queries separately, we consider a more challenging and realistic situation that the two types of queries are mixed together without prior information on query types.",
                "To assist prediction under the mixed-query situation, a novel query classifier is adopted.",
                "Results show that our prediction of web query performance is substantially more accurate than the current stateof-the-art prediction techniques.",
                "Consequently, our paper provides a practical approach to performance prediction in realworld web settings.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval -Query formulation General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Query performance prediction has many applications in a variety of information retrieval (IR) areas such as improving retrieval consistency, query refinement, and distributed IR.",
                "The importance of this problem has been recognized by IR researchers and a number of new methods have been proposed for prediction recently [1, 2, 17].",
                "Most work on prediction has focused on the traditional ad-hoc retrieval task where query performance is measured according to topical relevance.",
                "These prediction models are evaluated on TREC document collections which typically consist of no more than one million relatively homogenous newswire articles.",
                "With the popularity and influence of the Web, prediction techniques that will work well for web-style queries are highly preferable.",
                "However, web search environments pose significant challenges to current prediction models that are mainly designed for traditional TREC settings.",
                "Here we outline some of these challenges.",
                "First, web collections, which are much larger than conventional TREC collections, include a variety of documents that are different in many aspects such as quality and style.",
                "Current prediction techniques can be vulnerable to these characteristics of web collections.",
                "For example, the reported prediction accuracy of the ranking robustness technique and the clarity technique on the GOV2 collection (a large web collection) is significantly worse compared to the other TREC collections [1].",
                "Similar prediction accuracy on the GOV2 collection using another technique is reported in [2], confirming the difficult of predicting query performance on a large web collection.",
                "Furthermore, web search goes beyond the scope of the ad-hoc retrieval task based on topical relevance.",
                "For example, the Named-Page (NP) finding task, which is a navigational task, is also popular in web retrieval.",
                "Query performance prediction for the NP task is still necessary since NP retrieval performance is far from perfect.",
                "In fact, according to the report on the NP task of the 2005 Terabyte Track [3], about 40% of the test queries perform poorly (no correct answer in the first 10 search results) even in the best run from the top group.",
                "To our knowledge, little research has explicitly addressed the problem of NP-query performance prediction.",
                "Current prediction models devised for content-based queries will be less effective for NP queries considering the fundamental differences between the two.",
                "Third, in real-world web search environments, user queries are usually a mixture of different types and prior knowledge about the type of each query is generally unavailable.",
                "The mixed-query situation raises new problems for query performance prediction.",
                "For instance, we may need to incorporate a query classifier into prediction models.",
                "Despite these problems, the ability to handle this situation is a crucial step towards turning query performance prediction from an interesting research topic into a practical tool for web retrieval.",
                "In this paper, we present three techniques to address the above challenges that current prediction models face in Web search environments.",
                "Our work focuses on query performance prediction for the content-based (ad-hoc) retrieval task and the name-page finding task in the context of web retrieval.",
                "Our first technique, called weighted information gain (WIG), makes use of both single term and term proximity features to estimate the quality of top retrieved documents for prediction.",
                "We find that WIG offers consistent prediction accuracy across various test collections and query types.",
                "Moreover, we demonstrate that good prediction accuracy can be achieved for the mixed-query situation by using WIG with the help of a query type classifier.",
                "Query feedback and first rank change, which are our second and third prediction techniques, perform well for content-based queries and NP queries respectively.",
                "Our main contributions include: (1) considerably improved prediction accuracy for web content-based queries over several state-of-the-art techniques. (2) new techniques for successfully predicting NP-query performance. (3) a practical and fully automatic solution to predicting mixed-query performance.",
                "In addition, one minor contribution is that we find that the robustness score [1], which was originally proposed for performance prediction, is helpful for query classification. 2.",
                "RELATED WORK As we mentioned in the introduction, a number of prediction techniques have been proposed recently that focus on contentbased queries in the topical relevance (ad-hoc) task.",
                "We know of no published work that addresses other types of queries such as NP queries, let alone a mixture of query types.",
                "Next we review some representative models.",
                "The major difficulty of performance prediction comes from the fact that many factors have an impact on retrieval performance.",
                "Each factor affects performance to a different degree and the overall effect is hard to predict accurately.",
                "Therefore, it is not surprising to notice that simple features, such as the frequency of query terms in the collection [4] and the average IDF of query terms [5], do not predict well.",
                "In fact, most of the successful techniques are based on measuring some characteristics of the retrieved document set to estimate topic difficulty.",
                "For example, the clarity score [6] measures the coherence of a list of documents by the KL-divergence between the query model and the collection model.",
                "The robustness score [1] quantifies another property of a ranked list: the robustness of the ranking in the presence of uncertainty.",
                "Carmel et al. [2] found that the distance measured by the Jensen-Shannon divergence between the retrieved document set and the collection is significantly correlated to average precision.",
                "Vinay et al. [7] proposed four measures to capture the geometry of the top retrieved documents for prediction.",
                "The most effective measure is the sensitivity to document perturbation, an idea somewhat similar to the robustness score.",
                "Unfortunately, their way of measuring the sensitivity does not perform equally well for short queries and prediction accuracy drops considerably when a state-of-the-art retrieval technique (like Okapi or a language modeling approach) is adopted for retrieval instead of the tf-idf weighting used in their paper [16].",
                "The difficulties of applying these models in web search environments have already been mentioned.",
                "In this paper, we mainly adopt the clarity score and the robustness score as our baselines.",
                "We experimentally show that the baselines, even after being carefully tuned, are inadequate for the web environment.",
                "One of our prediction models, WIG, is related to the Markov random field (MRF) model for information retrieval [8].",
                "The MRF model directly models term dependence and is found be to highly effective across a variety of test collections (particularly web collections) and retrieval tasks.",
                "This model is used to estimate the joint probability distribution over documents and queries, an important part of WIG.",
                "The superiority of WIG over other prediction techniques based on unigram features, which will be demonstrated later in our paper, coincides with that of MRF for retrieval.",
                "In other word, it is interesting to note that term dependence, when being modeled appropriately, can be helpful for both improving and predicting retrieval performance. 3.",
                "PREDICTION MODELS 3.1 Weighted Information Gain (WIG) This section introduces a weighted information gain approach that incorporates both single term and proximity features for predicting performance for both content-based and Named-Page (NP) finding queries.",
                "Given a set of queries Q={Qs} (s=1,2,..N) which includes all possible user queries and a set of documents D={Dt} (t=1,2…M), we assume that each query-document pair (Qs,Dt) is manually judged and will be put in a relevance list if Qs is found to be relevant to Dt.",
                "The joint probability P(Qs,Dt) over queries Q and documents D denotes the probability that pair (Qs,Dt) will be in the relevance list.",
                "Such assumptions are similar to those used in [8].",
                "Assuming that the user issues query Qi ∈Q and the retrieval results in response to Qi is a ranked list L of documents, we calculate the amount of information contained in P(Qs,Dt) with respect to Qi and L by Eq.1 which is a variant of entropy called the weighted entropy[13].",
                "The weights in Eq.1 are solely determined by Qi and L. )1(),(log),(),( , , ∑−= ts tststsLQ DQPDQweightDQH i In this paper, we choose the weights as follows: LindocumentsKtopthecontainsLTwhere otherwise LTDandisifK DQweight K Kt ts )( )2( ,0 )(,/1 ),( ⎩ ⎨ ⎧ ∈= = The cutoff rank K is a parameter in our model that will be discussed later.",
                "Accordingly, Eq.1 can be simplified as follows: )3(),(log 1 ),( )( , ∑∈ −= LTD titsLQ Kt i DQP K DQH Unfortunately, weighted entropy ),(, tsLQ DQH i computed by Eq.3, which represents the amount of information about how likely the top ranked documents in L would be relevant to query Qi on average, cannot be compared across different queries, making it inappropriate for directly predicting query performance.",
                "To mitigate this problem, we come up with a background distribution P(Qs,C) over Q and D by imagining that every document in D is replaced by the same special document C which represents average language usage.",
                "In this paper, C is created by concatenating every document in D. Roughly speaking, C is the collection (the document set) {Dt} without document boundaries.",
                "Similarly, weighted entropy ),(, CQH sLQi calculated by Eq.3 represents the amount of information about how likely an average document (represented by the whole collection) would be relevant to query Qi.",
                "Now we introduce our performance predictor WIG which is the weighted information gain [13] computed as the difference between ),(, tsLQ DQH i and ),(, CQH sLQi .Specifically, given query Qi, collection C and ranked list L of documents, WIG is calculated as follows: )4( ),( ),( log 1 ),( ),( log),( ),(),(),,( )(, ,, ∑∑ ∈ == −= LTD i ti ts s ts ts tsLQsLQi Kt ii CQP DQP KCQP DQP DQweight DQHCQHLCQWIG WIG computed by Eq.4 measures the change in information about the quality of retrieval (in response to query Qi) from an imaginary state that only an average document is retrieved to a posterior state that the actual search results are observed.",
                "We hypothesize that WIG is positively correlated with retrieval effectiveness because high quality retrieval should be much more effective than just returning the average document.",
                "The heart of this technique is how to estimate the joint distribution P(Qs,Dt).",
                "In the language modeling approach to IR, a variety of models can be applied readily to estimate this distribution.",
                "Although most of these models are based on the bagof-words assumption, recent work on modeling term dependence under the language modeling framework have shown consistent and significant improvements in retrieval effectiveness over bagof-words models.",
                "Inspired by the success of incorporating term proximity features into language models, we decide to adopt a good dependence model to estimate the probability P(Qs,Dt).",
                "The model we chose for this paper is Metzler and Crofts Markov Random Field (MRF) model, which has already demonstrated superiority over a number of collections and different retrieval tasks [8,9].",
                "According to the MRF model, log P(Qi, Dt) can be written as )5()|(loglog),(log )( 1 ∑∈ +−= iQF tti DPZDQP ξ ξ ξλ where Z1 is a constant that ensures that P(Qi, Dt) sums up to 1.",
                "F(Qi) consists of a set of features expanded from the original query Qi .",
                "For example, assuming that query Qi is talented student program, F(Qi) includes features like program and talented student.",
                "We consider two kinds of features: single term features T and proximity features P. Proximity features include exact phrase (#1) and unordered window (#uwN) features as described in [8].",
                "Note that F(Qi) is the union of T(Qi) and P(Qi).",
                "For more details on F(Qi) such as how to expand the original query Qi to F(Qi), we refer the reader to [8] and [9].",
                "P(ξ|Dt) denotes the probability that feature ξ will occur in Dt.",
                "More details on P(ξ|Dt) will be provided later in this section.",
                "The choice of λξ is somewhat different from that used in [8] since λξ plays a dual role in our model.",
                "The first role, which is the same as in [8], is to weight between single term and proximity features.",
                "The other role, which is specific to our prediction task, is to normalize the size of F(Qi).We found that the following weight strategy for λξ satisfies the above two roles and generalizes well on a variety of collections and query types. )6( )(, |)(| 1 )(, |)(| ⎪ ⎪ ⎩ ⎪ ⎪ ⎨ ⎧ ∈ − ∈ = i i T i i T QP QP QT QT ξ λ ξ λ λξ where |T(Qi)| and |P(Qi)| denote the number of single term and proximity features in F(Qi) respectively.",
                "The reason for choosing the square root function in the denominator of λξ is to penalize a feature set of large size appropriately, making WIG more comparable across queries of various lengths. λT is a fixed parameter and set to 0.8 according to [8] throughout this paper.",
                "Similarly, log P(Qi,C) can be written as: )7()|(loglog),(log )( 2 ∑∈ +−= iQF i CPZCQP ξ ξ ξλ When constant Z1 and Z2 are dropped, WIG computed in Eq.4 can be rewritten as follows by plugging in Eq.5 and Eq.7 : )8( )|( )|( log 1 ),,( )( )( ∑ ∑∈ ∈ = LTD QF t i Kt i CP DP K LCQWIG ξ ξ ξ ξ λ One of the advantages of WIG over other techniques is that it can handle well both content-based and NP queries.",
                "Based on the type (or the predicted type) of Qi, the calculation of WIG in Eq. 8 differs in two aspects: (1) how to estimate P(ξ|Dt) and P(ξ|C), and (2) how to choose K. For content-based queries, P(ξ|C) is estimated by the relative frequency of feature ξ in collection C as a whole.",
                "The estimation of P(ξ|Dt) is the same as in [8].",
                "Namely, we estimate P(ξ|Dt) by the relative frequency of feature ξ in Dt linearly smoothed with collection frequency P(ξ|C).",
                "K in Eq.8 is treated as a free parameter.",
                "Note that K is the only free parameter in the computation of WIG for content-based queries because all parameters involved in P(ξ|Dt) are assumed to be fixed by taking the suggested values in [8].",
                "Regarding NP queries, we make use of document structure to estimate P(ξ|Dt) and P(ξ|C) by the so-called mixture of language models proposed in [10] and incorporated into the MRF model for Named-Page finding retrieval in [9].",
                "The basic idea is that a document (collection) is divided into several fields such as the title field, the main-body field and the heading field.",
                "P(ξ|Dt) and P(ξ|C) are estimated by a linear combination of the language models from each field.",
                "Due to space constraints, we refer the reader to [9] for details.",
                "We adopt the exact same set of parameters as used in [9] for estimation.",
                "With regard to K in Eq.8, we set K to 1 because the Named-Page finding task heavily focuses on the first ranked document.",
                "Consequently, there are no free parameters in the computation of WIG for NP queries. 3.2 Query Feedback In this section, we introduce another technique called query feedback (QF) for prediction.",
                "Suppose that a user issues query Q to a retrieval system and a ranked list L of documents is returned.",
                "We view the retrieval system as a noisy channel.",
                "Specifically, we assume that the output of the channel is L and the input is Q.",
                "After going through the channel, Q becomes corrupted and is transformed to ranked list L. By thinking about the retrieval process this way, the problem of predicting retrieval effectiveness turns to the task of evaluating the quality of the channel.",
                "In other words, prediction becomes finding a way to measure the degree of corruption that arises when Q is transformed to L. As directly computing the degree of the corruption is difficult, we tackle this problem by approximation.",
                "Our main idea is that we measure to what extent information on Q can be recovered from L on the assumption that only L is observed.",
                "Specifically, we design a decoder that can accurately translate L back into new query Q and the similarity S between the original query Q and the new query Q is adopted as a performance predictor.",
                "This is a sketch of how the QF technique predicts query performance.",
                "Before filling in more details, we briefly discuss why this method would work.",
                "There is a relation between the similarity S defined above and retrieval performance.",
                "On the one hand, if the retrieval has strayed from the original sense of the query Q, the new query Q extracted from ranked list L in response to Q would be very different from the original query Q.",
                "On the other hand, a query distilled from a ranked list containing many relevant documents is likely to be similar to the original query.",
                "Further examples in support of the relation will be provided later.",
                "Next we detail how to build the decoder and how to measure the similarity S. In essence, the goal of the decoder is to compress ranked list L into a few informative terms that should represent the content of the top ranked documents in L. Our approach to this goal is to represent ranked list L by a language model (distribution over terms).",
                "Then terms are ranked by their contribution to the language models KL (Kullback-Leibler) divergence from the background collection model.",
                "Top ranked terms will be chosen to form the new query Q.",
                "This approach is similar to that used in Section 4.1 of [11].",
                "Specifically, we take three steps to compress ranked list L into query Q without referring to the original query. 1.",
                "We adopt the ranked list language model [14], to estimate a language model based on ranked list L. The model can be written as: )9()|()|()|( ∑∈ = LD LDPDwPLwP where w is any term, D is a document.",
                "P(D|L) is estimated by a linearly decreasing function of the rank of document D. 2.",
                "Each term in P(w|L) is ranked by the following KL-divergence contribution: )10( )|( )|( log)|( CwP LwP LwP where P(w|C) is the collection model estimated by the relative frequency of term w in collection C as a whole. 3.",
                "The top N ranked terms by Eq.10 form a weighted query Q={(wi,ti)} i=1,N. where wi denotes the i-th ranked term and weight ti is the KL-divergence contribution of wi in Eq. 10.",
                "Term cruise ship vessel sea passenger KL contribution 0.050 0.040 0.012 0.010 0.009 Table 1: top 5 terms compressed from the ranked list in response to query Cruise ship damage sea life Two representative examples, one for a poorly performing query Cruise ship damage sea life (TREC topic 719; average precision: 0.08) and the other for a high performing query prostate cancer treatments( TREC topic 710; average precision: 0.49), are shown in Table 1 and 2 respectively.",
                "These examples indicate how the similarity between the original and the new query correlates with retrieval performance.",
                "The parameter N in step 3 is set to 20 empirically and choosing a larger value of N is unnecessary since the weights after the top 20 are usually too small to make any difference.",
                "Term prostate cancer treatment men therapy KL contribution 0.177 0.140 0.028 0.025 0.020 Table 2: top 5 terms compressed from the ranked list in response to query prostate cancer treatments To measure the similarity between original query Q and new query Q, we first use Q to do retrieval on the same collection.",
                "A variant of the query likelihood model [15] is adopted for retrieval.",
                "Namely, documents are ranked by: )11()|()|( ),( ∑∈ = Qtw t i ii i DwPDQP where wi is a term in Q and ti is the associated weight.",
                "D is a document.",
                "Let L denote the new ranked list returned from the above retrieval.",
                "The similarity is measured by the overlap of documents in L and L. Specifically, the percentage of documents in the top K documents of L that are also present in the top K documents in L. the cutoff K is treated as a free parameter.",
                "We summarize here how the QF technique predicts performance given a query Q and the associated ranked list L. We first obtain a weighted query Q compressed from L by the above three steps.",
                "Then we use Q to perform retrieval and the new ranked list is L. The overlap of documents in L and L is used for prediction. 3.3 First Rank Change (FRC) In this section, we propose a method called the first rank change (FRC) for performance prediction for NP queries.",
                "This method is derived from the ranking robustness technique [1] that is mainly designed for content-based queries.",
                "When directly applied to NP queries, the robustness technique will be less effective because it takes the top ranked documents as a whole into account while NP queries usually have only one single relevant document.",
                "Instead, our technique focuses on the first rank document while the main idea of the robustness method remains.",
                "Specifically, the pseudocode for computing FRC is shown in figure 1.",
                "Input: (1) ranked list L={Di} where i=1,100.",
                "Di denotes the i-th ranked document. (2) query Q 1 initialize: (1) set the number of trials J=100000 (2) counter c=0; 2 for i=1 to J 3 Perturb every document in L, let the outcome be a set F={Di} where Di denotes the perturbed version of Di. 4 Do retrieval with query Q on set F 5 c=c+1 if and only if D1 is ranked first in step 4 6 end of for 7 return the ratio c/J Figure 1: pseudo-code for computing FRC FRC approximates the probability that the first ranked document in the original list L will remain ranked first even after the documents are perturbed.",
                "The higher the probability is, the more confidence we have in the first ranked document.",
                "On the other hand, in the extreme case of a random ranking, the probability would be as low as 0.5.",
                "We expect that FRC has a positive association with NP query performance.",
                "We adopt [1] to implement the document perturbation step (step 4 in Fig.1) using Poisson distributions.",
                "For more details, we refer the reader to [1]. 4.",
                "EVALUATION We now present the results of predicting query performance by our models.",
                "Three state-of-the-art techniques are adopted as our baselines.",
                "We evaluate our techniques across a variety of Web retrieval settings.",
                "As mentioned before, we consider two types of queries, that is, content-based (CB) queries and Named-Page(NP) finding queries.",
                "First, suppose that the query types are known.",
                "We investigate the correlation between the predicted retrieval performance and the actual performance for both types of queries separately.",
                "Results show that our methods yield considerable improvements over the baselines.",
                "We then consider a more challenging scenario where no prior information on query types is available.",
                "Two sub-cases are considered.",
                "In the first one, there exists only one type of query but the actual type is unknown.",
                "We assume a mixture of the two query types in the second case.",
                "We demonstrate that our models achieve good accuracy under this demanding scenario, making prediction practical in a real-world Web search environment. 4.1 Experimental Setup Our evaluation focuses on the GOV2 collection which contains about 25 million documents crawled from web sites in the .gov domain during 2004 [3].",
                "We create two kinds of data set for CB queries and NP queries respectively.",
                "For the CB type, we use the ad-hoc topics of the Terabyte Tracks of 2004, 2005 and 2006 and name them TB04-adhoc, TB05-adhoc and TB06-adhoc respectively.",
                "In addition, we also use the ad-hoc topics of the 2004 Robust Track (RT04) to test the adaptability of our techniques to a non-Web environment.",
                "For NP queries, we use the Named-Page finding topics of the Terabyte Tracks of 2005 and 2006 and we name them TB05-NP and TB06-NP respectively.",
                "All queries used in our experiments are titles of TREC topics as we center on web retrieval.",
                "Table 3 summarizes the above data sets.",
                "Name Collection Topic Number Query Type TB04-adhoc GOV2 701-750 CB TB05-adhoc GOV2 751-800 CB TB06-adhoc GOV2 801-850 CB RT04 Disk 4+5 (minus CR) 301-450;601700 CB TB05-NP GOV2 NP601-NP872 NP TB06-NP GOV2 NP901-NP1081 NP Table 3: Summary of test collections and topics Retrieval performance of individual content-based and NP queries is measured by the average precision and reciprocal rank of the first correct answer respectively.",
                "We make use of the Markov Random field model for both ad-hoc and Named-Page finding retrieval.",
                "We adopt the same setting of retrieval parameters used in [8,9].",
                "The Indri search engine [12] is used for all of our experiments.",
                "Though not reported here, we also tried the query likelihood model for ad-hoc retrieval and found that the results change little because of the very high correlation between the query performances obtained by the two retrieval models (0.96 measured by Pearsons coefficient). 4.2 Known Query Types Suppose that query types are known.",
                "We treat each type of query separately and measure the correlation with average precision (or the reciprocal rank in the case of NP queries).",
                "We adopt the Pearsons correlation test which reflects the degree of linear relationship between the predicted and the actual retrieval performance. 4.2.1 Content-based Queries Methods Clarity Robust JSD WIG QF WIG +QF TB04+0 5 adhoc 0.333 0.317 0.362 0.574 0.480 0.637 TB06 adhoc 0.076 0.294 N/A 0.464 0.422 0.511 Table 4: Pearsons correlation coefficients for correlation with average precision on the Terabyte Tracks (ad-hoc) for clarity score, robustness score, the JSD-based method(we directly cites the score reported in [2]), WIG, query feedback(QF) and a linear combination of WIG and QF.",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Table 4 shows the correlation with average precision on two data sets: one is a combination of TB04-adhoc and TB05-adhoc(100 topics in total) and the other is TB06-adhoc (50 topics).",
                "The reason that we put TB04-adhoc and TB05-adhoc together is to make our results comparable to [2].",
                "Our baselines are the clarity score (clarity) [6],the robustness score (robust)[1] and the JSDbased method (JSD) [2].",
                "For the clarity and robustness score, we have tried different parameter settings and report the highest correlation coefficients we have found.",
                "We directly cite the result of the JSD-based method reported in [2].",
                "The table also shows the results for the Weighted Information Gain (WIG) method and the Query Feedback (QF) method for predicting content-based queries.",
                "As we described in the previous section, both WIG and QF have one free parameter to set, that is, the cutoff rank K. We train the parameter on one dataset and test on the other.",
                "When combining WIG and QF, a simple linear combination is used and the combination weight is learned from the training data set.",
                "From these results, we can see that our methods are considerably more accurate compared to the baselines.",
                "We also observe that further improvements are obtained from the combination of WIG and QF, suggesting that they measure different properties of the retrieval process that relate to performance.",
                "We discover that our methods generalize well on TB06-adhoc while the correlation for the clarity score with retrieval performance on this data set is considerably worse.",
                "Further investigation shows that the mean average precision of TB06-adhoc is 0.342 and is about 10% better than that of the first data set.",
                "While the other three methods typically consider the top 100 or less documents given a ranked list, the clarity method usually needs the top 500 or more documents to adequately measure the coherence of a ranked list.",
                "Higher mean average precision makes ranked lists retrieved by different queries more similar in terms of coherence at the level of top 500 documents.",
                "We believe that this is the main reason for the low accuracy of the clarity score on the second data set.",
                "Though this paper focuses on a Web search environment, it is desirable that our techniques will work consistently well in other situations.",
                "To this end, we examine the effectiveness of our techniques on the Robust 2004 Track.",
                "For our methods, we evenly divide all of the test queries into five groups and perform five-fold cross validation.",
                "Each time we use one group for training and the remaining four groups for testing.",
                "We make use of all of the queries for our two baselines, that is, the clarity score and the robustness score.",
                "The parameters for our baselines are the same as those used in [1].The results shown in Table 5 demonstrate that the prediction accuracy of our methods is on a par with that of the two strong baselines.",
                "Clarity Robust WIG QF 0.464 0.539 0.468 0.464 Table 5: Comparison of Pearsons correlation coefficients on the 2004 Robust Track for clarity score, robustness score, WIG and query feedback (QF).",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Furthermore, we examine the prediction sensitivity of our methods to the cutoff rank K. With respect to WIG, it is quite robust to K on the Terabyte Tracks (2004-2006) while it prefers a small value of K like 5 on the 2004 Robust Track.",
                "In other words, a small value of K is a nearly-optimal choice for both kinds of tracks.",
                "Considering the fact that all other parameters involved in WIG are fixed and consequently the same for the two cases, this means WIG can achieve nearly-optimal prediction accuracy in two considerably different situations with exactly the same parameter setting.",
                "Regarding QF, it prefers a larger value of K such as 100 on the Terabyte Tracks and a smaller value of K such as 25 on the 2004 Robust Track. 4.2.2 NP Queries We adopt WIG and first rank change (FRC) for predicting NPquery performance.",
                "We also try a linear combination of the two as in the previous section.",
                "The combination weight is obtained from the other data set.",
                "We use the correlation with the reciprocal ranks measured by the Pearsons correlation test to evaluate prediction quality.",
                "The results are presented in Table 6.",
                "Again, our baselines are the clarity score and the robustness score.",
                "To make a fair comparison, we tune the clarity score in different ways.",
                "We found that using the first ranked document to build the query model yields the best prediction accuracy.",
                "We also attempted to utilize document structure by using the mixture of language models mentioned in section 3.1.",
                "Little improvement was obtained.",
                "The correlation coefficients for the clarity score reported in Table 6 are the best we have found.",
                "As we can see, our methods considerably outperform the clarity score technique on both of the runs.",
                "This confirms our intuition that the use of a coherence-based measure like the clarity score is inappropriate for NP queries.",
                "Methods Clarity Robust.",
                "WIG FRC WIG+FRC TB05-NP 0.150 -0.370 0.458 0.440 0.525 TB06-NP 0.112 -0.160 0.478 0.386 0.515 Table 6: Pearsons correlation coefficients for correlation with reciprocal ranks on the Terabyte Tracks (NP) for clarity score, robustness score, WIG, the first rank change (FRC) and a linear combination of WIG and FRC.",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Regarding the robustness score, we also tune the parameters and report the best we have found.",
                "We observe an interesting and surprising negative correlation with reciprocal ranks.",
                "We explain this finding briefly.",
                "A high robustness score means that a number of top ranked documents in the original ranked list are still highly ranked after perturbing the documents.",
                "The existence of such documents is a good sign of high performance for content-based queries as these queries usually contain a number of relevant documents [1].",
                "However, with regard to NP queries, one fundamental difference is that there is only one relevant document for each query.",
                "The existence of such documents can confuse the ranking function and lead to low retrieval performance.",
                "Although the negative correlation with retrieval performance exists, the strength of the correlation is weaker and less consistent compared to our methods as shown in Table 6.",
                "Based on the above analysis, we can see that current prediction techniques like clarity score and robustness score that are mainly designed for content-based queries face significant challenges and are inadequate to deal with NP queries.",
                "Our two techniques proposed for NP queries consistently demonstrate good prediction accuracy, displaying initial success in solving the problem of predicting performance for NP queries.",
                "Another point we want to stress is that the WIG method works well for both types of queries, a desirable property that most prediction techniques lack. 4.3 Unknown Query Types In this section, we run two kinds of experiments without access to query type labels.",
                "First, we assume that only one type of query exists but the type is unknown.",
                "Second, we experiment on a mixture of content-based and NP queries.",
                "The following two subsections will report results for the two conditions respectively. 4.3.1 Only One Type exists We assume that all queries are of the same type, that is, they are either NP queries or content-based queries.",
                "We choose WIG to deal with this case because it shows good prediction accuracy for both types of queries in the previous section.",
                "We consider two cases: (1) CB: all 150 title queries from the ad-hoc task of the Terabyte Tracks 2004-2006 (2)NP: all 433 NP queries from the named page finding task of the Terabyte Tracks 2005 and 2006.",
                "We take a simple strategy by labeling all of the queries in each case as the same type (either NP or CB) regardless of their actual type.",
                "The computation of WIG will be based on the labeled query type instead of the actual type.",
                "There are four possibilities with respect to the relation between the actual type and the labeled type.",
                "The correlation with retrieval performance under the four possibilities is presented in Table 7.",
                "For example, the value 0.445 at the intersection between the second row and the third column shows the Pearsons correlation coefficient for correlation with average precision when the content-based queries are incorrectly labeled as the NP type.",
                "Based on these results, we recommend treating all queries as the NP type when only one query type exists and accurate query classification is not feasible, considering the risk that a large loss of accuracy will occur if NP queries are incorrectly labeled as content-based queries.",
                "These results also demonstrate the strong adaptability of WIG to different query types.",
                "CB (labeled) NP (labeled) CB (actual) 0.536 0.445 NP (actual) 0.174 0.467 Table 7: Comparison of Pearsons correlation coefficients for correlation with retrieval performance under four possibilities on the Terabyte Tracks (NP).",
                "Bold cases mean the results are statistically significant at the 0.01 level. 4.3.2 A mixture of contented-based and NP queries A mixture of the two types of queries is a more realistic situation that a Web search engine will meet.",
                "We evaluate prediction accuracy by how accurately poorly-performing queries can be identified by the prediction method assuming that actual query types are unknown (but we can predict query types).",
                "This is a challenging task because both the predicted and actual performance for one type of query can be incomparable to that for the other type.",
                "Next we discuss how to implement our evaluation.",
                "We create a query pool which consists of all of the 150 ad-hoc title queries from Terabyte Track 2004-2006 and all of the 433 NP queries from Terabyte Track 2005&2006.",
                "We divide the queries in the pool into classes: good (better than 50% of the queries of the same type in terms of retrieval performance) and bad (otherwise).",
                "According to these standards, a NP query with the reciprocal rank above 0.2 or a content-based query with the average precision above 0.315 will be considered as good.",
                "Then, each time we randomly select one query Q from the pool with probability p that Q is contented-based.",
                "The remaining queries are used as training data.",
                "We first decide the type of query Q according to a query classifier.",
                "Namely, the query classifier tells us whether query Q is NP or content-based.",
                "Based on the predicted query type and the score computed for query Q by a prediction technique, a binary decision is made about whether query Q is good or bad by comparing to the score threshold of the predicted query type obtained from the training data.",
                "Prediction accuracy is measured by the accuracy of the binary decision.",
                "In our implementation, we repeatedly take a test query from the query pool and prediction accuracy is computed as the percentage of correct decisions, that is, a good(bad) query is predicted to be good (bad).",
                "It is obvious that random guessing will lead to 50% accuracy.",
                "Let us take the WIG method for example to illustrate the process.",
                "Two WIG thresholds (one for NP queries and the other for content-based queries) are trained by maximizing the prediction accuracy on the training data.",
                "When a test query is labeled as the NP (CB) type by the query type classifier, it will be predicted to be good if and only if the WIG score for this query is above the NP (CB) threshold.",
                "Similar procedures will be taken for other prediction techniques.",
                "Now we briefly introduce the automatic query type classifier used in this paper.",
                "We find that the robustness score, though originally proposed for performance prediction, is a good indicator of query types.",
                "We find that on average content-based queries have a much higher robustness score than NP queries.",
                "For example, Figure 2 shows the distributions of robustness scores for NP and content-based queries.",
                "According to this finding, the robustness score classifier will attach a NP (CB) label to the query if the robustness score for the query is below (above) a threshold trained from the training data. 0 0.5 1 1.5 2 2.5 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 NP Content-based Figure 2: Distribution of robustness scores for NP and CB queries.",
                "The NP queries are the 252 NP topics from the 2005 Terabyte Track.",
                "The content-based queries are the 150 ad-hoc title from the Terabyte Tracks 2004-2006.",
                "The probability distributions are estimated by the Kernel density estimation method.",
                "Strategies Robust WIG-1 WIG-2 WIG-3 Optimal p=0.6 0.565 0.624 0.665 0.684 0.701 P=0.4 0.567 0.633 0.654 0.673 0.696 Table 8: Comparison of prediction accuracy for five strategies in the mixed-query situation.",
                "Two ways to sample a query from the pool: (1) the sampled query is content-based with the probability p=0.6. (that is, the query is NP with probability 0.4 ) (2) set the probability p=0.4.",
                "We consider five strategies in our experiments.",
                "In the first strategy (denoted by robust), we use the robustness score for query performance prediction with the help of a perfect query classifier that always correctly map a query into one of the two categories (that is, NP or CB).",
                "This strategy represents the level of prediction accuracy that current prediction techniques can achieve in an ideal condition that query types are known.",
                "In the next following three strategies, the WIG method is adopted for performance prediction.",
                "The difference among the three is that three different query classifiers are used for each strategy: (1) the classifier always classifies a query into the NP type. (2) the Robustness Score ProbabilityDensity classifier is the robust score classifier mentioned above. (3) the classifier is a perfect one.",
                "These three strategies are denoted by WIG-1, WIG-2 and WIG-3 respectively.",
                "The reason we are interested in WIG-1 is based on the results from section 4.3.1.",
                "In the last strategy (denoted by Optimal) which serves as an upper bound on how well we can do so far, we fully make use of our prediction techniques for each query type assuming a perfect query classifier is available.",
                "Specifically, we linearly combine WIG and QF for content-based queries and WIG and FRC for NP queries.",
                "The results for the five strategies are shown in Table 8.",
                "For each strategy, we try two ways to sample a query from the pool: (1) the sampled query is CB with probability p=0.6. (the query is NP with probability 0.4) (2) set the probability p=0.4.",
                "From Table 8 We can see that in terms of prediction accuracy WIG-2 (the WIG method with the automatic query classifier) is not only better than the first two cases, but also is close to WIG-3 where a perfect classifier is assumed.",
                "Some further improvements over WIG-3 are observed when combined with other prediction techniques.",
                "The merit of WIG-2 is that it provides a practical solution to automatically identifying poorly performing queries in a Web search environment with mixed query types, which poses considerable obstacles to traditional prediction techniques. 5.",
                "CONCLUSIONS AND FUTURE WORK To our knowledge, our paper is the first to thoroughly explore prediction of query performance in web search environments.",
                "We demonstrated that our models resulted in higher prediction accuracy than previously published techniques not specially devised for web search scenarios.",
                "In this paper, we focus on two types of queries in web search: content-based and Named-Page (NP) finding queries, corresponding to the ad-hoc retrieval task and the Named-Page finding task respectively.",
                "For both types of web queries, our prediction models were shown to be substantially more accurate than the current state-of-the-art techniques.",
                "Furthermore, we considered a more realistic case that no prior information on query types is available.",
                "We demonstrated that the WIG method is particularly suitable for this situation.",
                "Considering the adaptability of WIG to a range of collections and query types, one of our future plans is to apply this method to predict user preference of search results on realistic data collected from a commercial search engine.",
                "Other than accuracy, another major issue that prediction techniques have to deal with in a Web environment is efficiency.",
                "Fortunately, since the WIG score is computed just over the terms and the phrases that appear in the query, this calculation can be made very efficient with the support of index.",
                "On the other hand, the computation of QF and FRC is relatively less efficient since QF needs to retrieve the whole collection twice and FRC needs to repeatedly rank the perturbed documents.",
                "How to improve the efficiency of QF and FRC is our future work.",
                "In addition, the prediction techniques proposed in this paper have the potential of improving retrieval performance by combining with other IR techniques.",
                "For example, our techniques can be incorporated to popular query modification techniques such as query expansion and query relaxation.",
                "Guided by performance prediction, we can make a better decision on when to or how to modify queries to enhance retrieval effectiveness.",
                "We would like to carry out research in this direction in the future. 6.",
                "ACKNOWLEGMENTS This work was supported in part by the Center for Intelligent Information Retrieval, in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C0023, and in part by an award from Google.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor.",
                "In addition, we thank Donald Metzler for his valuable comments on this work. 7.",
                "REFERENCES [1] Y. Zhou ,W. B. Croft ,Ranking Robustness: A Novel Framework to Predict Query Performance, in Proceedings of CIKM 2006. [2] D.Carmel, E.Yom-Tov, A.Darlow,D.Pelleg, What Makes a Query Difficult?, in Proceedings of SIGIR 2006. [3] C.L.A.",
                "Clarke, F. Scholer, I.Soboroff, The TREC 2005 Terabyte Track, In the Online Proceedings of 2005 TREC. [4] B.",
                "He and I.Ounis.",
                "Inferring query performance using preretrieval predictors.",
                "In proceedings of the SPIRE 2004. [5] S. Tomlinson.",
                "Robust, Web and Terabyte Retrieval with Hummingbird SearchServer at TREC 2004.",
                "In the Online Proceedings of 2004 TREC. [6] S. Cronen-Townsend, Y. Zhou, W. B. Croft, Predicting Query Performance, in Proceedings of SIGIR 2002. [7] V.Vinay, I.J.Cox, N.Mill-Frayling,K.Wood, On Ranking the Effectiveness of Searcher, in Proceedings of SIGIR 2006. [8] D.Metzler, W.B.Croft, A Markov Random Filed Model for Term Dependencies, in Proceedings of SIGIR 2005. [9] D.Metzler, T.Strohman,Y.Zhou,W.B.Croft, Indri at TREC 2005: Terabyte Track, In the Online Proceedings of 2004 TREC. [10] P. Ogilvie and J. Callan, Combining document representations for known-item search, in Proceedings of SIGIR 2003. [11] A.Berger, J.Lafferty, Information retrieval as statistical translation, in Proceedings of SIGIR 1999. [12] Indri search engine : http://www.lemurproject.org/indri/ [13] I.J.",
                "Taneja: On Generalized Information Measures and Their Applications, Advances in Electronics and Electron Physics, Academic Press (USA), 76, 1989, 327-413. [14] S.Cronen-Townsend, Y. Zhou and Croft, W. B. , A Framework for Selective Query Expansion, in Proceedings of CIKM 2004. [15] F.Song, W.B.Croft, A general language model for information retrieval, in Proceedings of SIGIR 1999. [16] Personal email contact with Vishwa Vinay and our own experiments [17] E.Yom-Tov, S.Fine, D.Carmel, A.Darlow, Learning to Estimate Query Difficulty Including Applications to Missing Content Detection and Distributed Information retrieval, in Proceedings of SIGIR 2005"
            ],
            "original_annotated_samples": [
                "We focus on performance prediction for two types of queries in web search environments: <br>content-based and named-page finding</br>."
            ],
            "translated_annotated_samples": [
                "Nos enfocamos en la predicción de rendimiento para dos tipos de consultas en entornos de búsqueda web: basadas en contenido y <br>búsqueda de páginas por nombre</br>."
            ],
            "translated_text": "Predicción del rendimiento de consultas en entornos de búsqueda web Yun Zhou y W. Bruce Croft Departamento de Ciencias de la Computación Universidad de Massachusetts, Amherst {yzhou, croft}@cs.umass.edu RESUMEN Las técnicas actuales de predicción, generalmente diseñadas para consultas basadas en contenido y evaluadas típicamente en colecciones de prueba relativamente homogéneas de tamaños pequeños, enfrentan serios desafíos en entornos de búsqueda web donde las colecciones son significativamente más heterogéneas y existen diferentes tipos de tareas de recuperación. En este artículo, presentamos tres técnicas para abordar estos desafíos. Nos enfocamos en la predicción de rendimiento para dos tipos de consultas en entornos de búsqueda web: basadas en contenido y <br>búsqueda de páginas por nombre</br>. Nuestra evaluación se realiza principalmente en la colección GOV2. Además de evaluar nuestros modelos para los dos tipos de consultas por separado, consideramos una situación más desafiante y realista en la que los dos tipos de consultas se mezclan sin información previa sobre los tipos de consulta. Para ayudar en la predicción bajo la situación de consulta mixta, se adopta un clasificador de consultas novedoso. Los resultados muestran que nuestra predicción del rendimiento de la consulta web es sustancialmente más precisa que las técnicas de predicción actuales más avanzadas. Por consiguiente, nuestro artículo proporciona un enfoque práctico para predecir el rendimiento en entornos web del mundo real. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Formulación de Consultas Términos Generales Algoritmos, Experimentación, Teoría 1. La predicción del rendimiento de la consulta tiene muchas aplicaciones en una variedad de áreas de recuperación de información (IR), como mejorar la consistencia de la recuperación, el refinamiento de la consulta y la IR distribuida. La importancia de este problema ha sido reconocida por los investigadores en Recuperación de Información y recientemente se han propuesto varios métodos nuevos para la predicción [1, 2, 17]. La mayoría de los trabajos sobre predicción se han centrado en la tarea tradicional de recuperación ad-hoc, donde el rendimiento de la consulta se mide según la relevancia temática. Estos modelos de predicción se evalúan en colecciones de documentos de TREC que suelen consistir en no más de un millón de artículos de agencias de noticias relativamente homogéneos. Con la popularidad e influencia de la Web, las técnicas de predicción que funcionen bien para consultas de estilo web son altamente preferibles. Sin embargo, los entornos de búsqueda web plantean desafíos significativos para los modelos de predicción actuales que están principalmente diseñados para entornos tradicionales de TREC. Aquí delineamos algunos de estos desafíos. Primero, las colecciones web, que son mucho más grandes que las colecciones TREC convencionales, incluyen una variedad de documentos que son diferentes en muchos aspectos, como la calidad y el estilo. Las técnicas actuales de predicción pueden ser vulnerables a estas características de las colecciones web. Por ejemplo, la precisión de predicción reportada de la técnica de robustez de clasificación y la técnica de claridad en la colección GOV2 (una colección web grande) es significativamente peor en comparación con las otras colecciones de TREC [1]. En [2] se informa de una precisión de predicción similar en la colección GOV2 utilizando otra técnica, confirmando la dificultad de predecir el rendimiento de consultas en una gran colección web. Además, la búsqueda web va más allá del alcance de la tarea de recuperación ad hoc basada en la relevancia temática. Por ejemplo, la tarea de encontrar páginas nombradas (NP), que es una tarea de navegación, también es popular en la recuperación web. La predicción del rendimiento de la consulta para la tarea de NP sigue siendo necesaria, ya que el rendimiento de recuperación de NP está lejos de ser perfecto. De hecho, según el informe sobre la tarea NP del Terabyte Track de 2005 [3], aproximadamente el 40% de las consultas de prueba tienen un rendimiento deficiente (sin respuesta correcta en los primeros 10 resultados de búsqueda) incluso en la mejor ejecución del grupo líder. Hasta donde sabemos, poca investigación ha abordado explícitamente el problema de la predicción del rendimiento de consultas de NP. Los modelos de predicción actuales diseñados para consultas basadas en contenido serán menos efectivos para consultas de NP considerando las diferencias fundamentales entre ambas. En tercer lugar, en entornos de búsqueda web del mundo real, las consultas de los usuarios suelen ser una mezcla de diferentes tipos y, por lo general, no se dispone de información previa sobre el tipo de cada consulta. La situación de consulta mixta plantea nuevos problemas para la predicción del rendimiento de la consulta. Por ejemplo, es posible que necesitemos incorporar un clasificador de consultas en los modelos de predicción. A pesar de estos problemas, la capacidad para manejar esta situación es un paso crucial hacia convertir la predicción del rendimiento de consultas de un tema de investigación interesante en una herramienta práctica para la recuperación web. En este artículo, presentamos tres técnicas para abordar los desafíos mencionados que enfrentan los modelos de predicción actuales en entornos de búsqueda web. Nuestro trabajo se centra en la predicción del rendimiento de consultas para la tarea de recuperación basada en contenido (ad-hoc) y la tarea de encontrar páginas por nombre en el contexto de la recuperación web. Nuestra primera técnica, llamada ganancia de información ponderada (WIG), utiliza tanto características de términos individuales como de proximidad de términos para estimar la calidad de los documentos recuperados para la predicción. Observamos que WIG ofrece una precisión de predicción consistente en diversas colecciones de pruebas y tipos de consultas. Además, demostramos que se puede lograr una buena precisión de predicción para la situación de consulta mixta utilizando WIG con la ayuda de un clasificador de tipo de consulta. La retroalimentación de consultas y el cambio de rango inicial, que son nuestras técnicas de predicción segunda y tercera, funcionan bien para consultas basadas en contenido y consultas de NP respectivamente. Nuestras principales contribuciones incluyen: (1) una precisión de predicción considerablemente mejorada para consultas basadas en contenido web en comparación con varias técnicas de vanguardia. (2) nuevas técnicas para predecir con éxito el rendimiento de consultas NP. (3) una solución práctica y totalmente automática para predecir el rendimiento de consultas mixtas. Además, una contribución menor es que encontramos que el puntaje de robustez [1], que originalmente se propuso para la predicción de rendimiento, es útil para la clasificación de consultas. 2. TRABAJO RELACIONADO Como mencionamos en la introducción, recientemente se han propuesto varias técnicas de predicción que se centran en consultas basadas en contenido en la tarea de relevancia temática (ad-hoc). No conocemos ningún trabajo publicado que aborde otros tipos de consultas como las consultas NP, y mucho menos una mezcla de tipos de consultas. A continuación revisamos algunos modelos representativos. La principal dificultad de la predicción del rendimiento proviene del hecho de que muchos factores tienen un impacto en el rendimiento de recuperación. Cada factor afecta el rendimiento en diferente medida y el efecto general es difícil de predecir con precisión. Por lo tanto, no es sorprendente notar que características simples, como la frecuencia de los términos de consulta en la colección [4] y la IDF promedio de los términos de consulta [5], no predicen bien. De hecho, la mayoría de las técnicas exitosas se basan en medir algunas características del conjunto de documentos recuperados para estimar la dificultad del tema. Por ejemplo, la puntuación de claridad [6] mide la coherencia de una lista de documentos mediante la divergencia KL entre el modelo de consulta y el modelo de colección. El puntaje de robustez [1] cuantifica otra propiedad de una lista clasificada: la robustez de la clasificación en presencia de incertidumbre. Carmel et al. [2] encontraron que la distancia medida por la divergencia de Jensen-Shannon entre el conjunto de documentos recuperados y la colección está significativamente correlacionada con la precisión promedio. Vinay et al. [7] propusieron cuatro medidas para capturar la geometría de los documentos mejor clasificados para la predicción. La medida más efectiva es la sensibilidad a la perturbación del documento, una idea algo similar a la puntuación de robustez. Desafortunadamente, su forma de medir la sensibilidad no funciona igual de bien para consultas cortas y la precisión de la predicción disminuye considerablemente cuando se adopta una técnica de recuperación de vanguardia (como Okapi o un enfoque de modelado de lenguaje) en lugar del peso tf-idf utilizado en su artículo [16]. Las dificultades de aplicar estos modelos en entornos de búsqueda web ya han sido mencionadas. En este documento, principalmente adoptamos la puntuación de claridad y la puntuación de robustez como nuestras líneas base. Experimentalmente demostramos que las líneas base, incluso después de ser ajustadas cuidadosamente, son inadecuadas para el entorno web. Uno de nuestros modelos de predicción, WIG, está relacionado con el modelo de campo aleatorio de Markov (MRF) para la recuperación de información [8]. El modelo MRF modela directamente la dependencia de términos y se ha demostrado ser altamente efectivo en una variedad de colecciones de pruebas (especialmente colecciones web) y tareas de recuperación. Este modelo se utiliza para estimar la distribución conjunta de probabilidad sobre documentos y consultas, una parte importante de WIG. La superioridad de WIG sobre otras técnicas de predicción basadas en características unigramas, que se demostrará más adelante en nuestro artículo, coincide con la de MRF para recuperación. En otras palabras, es interesante notar que la dependencia de términos, cuando se modela adecuadamente, puede ser útil tanto para mejorar como para predecir el rendimiento de recuperación. 3. Modelos de predicción 3.1 Ganancia de Información Ponderada (WIG) Esta sección introduce un enfoque de ganancia de información ponderada que incorpora tanto características de términos únicos como de proximidad para predecir el rendimiento tanto en consultas basadas en contenido como en consultas de búsqueda de Páginas Nombradas (NP). Dado un conjunto de consultas Q={Qs} (s=1,2,..N) que incluye todas las posibles consultas de usuario y un conjunto de documentos D={Dt} (t=1,2…M), asumimos que cada par consulta-documento (Qs,Dt) es evaluado manualmente y se incluirá en una lista de relevancia si se determina que Qs es relevante para Dt. La probabilidad conjunta P(Qs,Dt) sobre las consultas Q y los documentos D denota la probabilidad de que el par (Qs,Dt) esté en la lista de relevancia. Tales suposiciones son similares a las utilizadas en [8]. Suponiendo que el usuario emite la consulta Qi ∈Q y los resultados de recuperación en respuesta a Qi es una lista clasificada L de documentos, calculamos la cantidad de información contenida en P(Qs,Dt) con respecto a Qi y L mediante la Ec.1, que es una variante de la entropía llamada entropía ponderada[13]. Los pesos en la ecuación 1 están determinados únicamente por Qi y L. En este documento, elegimos los pesos de la siguiente manera: L es el número de documentos que contiene la consulta, K es el límite superior, LT es el límite superior de documentos, y D es el peso si K está en otro caso. La clasificación de corte K es un parámetro en nuestro modelo que se discutirá más adelante. Por lo tanto, la ecuación 1 se puede simplificar de la siguiente manera: )3(),(log 1 ),( )( , ∑∈ −= LTD titsLQ Kt i DQP K DQH Desafortunadamente, la entropía ponderada ),(, tsLQ DQH i calculada por la ecuación 3, que representa la cantidad de información sobre la probabilidad de que los documentos mejor clasificados en L sean relevantes para la consulta Qi en promedio, no se puede comparar entre diferentes consultas, lo que la hace inapropiada para predecir directamente el rendimiento de la consulta. Para mitigar este problema, creamos una distribución de fondo P(Qs,C) sobre Q y D imaginando que cada documento en D es reemplazado por el mismo documento especial C que representa el uso promedio del lenguaje. En este documento, C se crea concatenando cada documento en D. A grosso modo, C es la colección (el conjunto de documentos) {Dt} sin límites de documentos. De manera similar, la entropía ponderada ),(, CQH sLQi calculada por la Ecuación 3 representa la cantidad de información sobre la probabilidad de que un documento promedio (representado por toda la colección) sea relevante para la consulta Qi. Ahora presentamos nuestro predictor de rendimiento WIG que es la ganancia de información ponderada [13] calculada como la diferencia entre ),(, tsLQ DQH i y ),(, CQH sLQi. Específicamente, dado el conjunto de consultas Qi, la colección C y la lista clasificada L de documentos, WIG se calcula de la siguiente manera: )4( ),( ),( log 1 ),( ),( log),( ),(),(),,( )(, ,, ∑∑ ∈ == −= LTD i ti ts s ts ts tsLQsLQi Kt ii CQP DQP KCQP DQP DQweight DQHCQHLCQWIG WIG calculado por la Ecuación 4 mide el cambio en la información sobre la calidad de la recuperación (en respuesta a la consulta Qi) desde un estado imaginario en el que solo se recupera un documento promedio hasta un estado posterior en el que se observan los resultados de búsqueda reales. Hacemos la hipótesis de que WIG está positivamente correlacionado con la efectividad de la recuperación porque una recuperación de alta calidad debería ser mucho más efectiva que simplemente devolver el documento promedio. El corazón de esta técnica es cómo estimar la distribución conjunta P(Qs, Dt). En el enfoque de modelado del lenguaje para la recuperación de información, se pueden aplicar fácilmente una variedad de modelos para estimar esta distribución. Aunque la mayoría de estos modelos se basan en la suposición de la bolsa de palabras, trabajos recientes sobre modelado de dependencia de términos bajo el marco de modelado de lenguaje han demostrado mejoras consistentes y significativas en la efectividad de recuperación sobre los modelos de bolsa de palabras. Inspirados por el éxito de incorporar características de proximidad de términos en modelos de lenguaje, decidimos adoptar un buen modelo de dependencia para estimar la probabilidad P(Qs,Dt). El modelo que elegimos para este artículo es el modelo de Campo Aleatorio de Markov (MRF) de Metzler y Crofts, que ya ha demostrado su superioridad en comparación con varias colecciones y diferentes tareas de recuperación [8,9]. Según el modelo MRF, log P(Qi, Dt) se puede escribir como )5()|(loglog),(log )( 1 ∑∈ +−= iQF tti DPZDQP ξ ξ ξλ donde Z1 es una constante que asegura que P(Qi, Dt) sume 1. F(Qi) consiste en un conjunto de características ampliadas a partir de la consulta original Qi. Por ejemplo, suponiendo que la consulta Qi es el programa de estudiantes talentosos, F(Qi) incluye características como programa y estudiante talentoso. Consideramos dos tipos de características: características de términos individuales T y características de proximidad P. Las características de proximidad incluyen la frase exacta (#1) y las características de ventana desordenada (#uwN) como se describe en [8]. Ten en cuenta que F(Qi) es la unión de T(Qi) y P(Qi). Para obtener más detalles sobre F(Qi), como por ejemplo cómo expandir la consulta original Qi a F(Qi), remitimos al lector a [8] y [9]. P(ξ|Dt) denota la probabilidad de que la característica ξ ocurra en Dt. Más detalles sobre P(ξ|Dt) se proporcionarán más adelante en esta sección. La elección de λξ es algo diferente a la utilizada en [8] ya que λξ desempeña un doble papel en nuestro modelo. El primer rol, que es el mismo que en [8], es ponderar entre las características de términos individuales y de proximidad. El otro rol, específico de nuestra tarea de predicción, es normalizar el tamaño de F(Qi). Encontramos que la siguiente estrategia de peso para λξ satisface los dos roles anteriores y generaliza bien en una variedad de colecciones y tipos de consultas. )6( )(, |)(| 1 )(, |)(| ⎪ ⎪ ⎩ ⎪ ⎪ ⎨ ⎧ ∈ − ∈ = i i T i i T QP QP QT QT ξ λ ξ λ λξ donde |T(Qi)| y |P(Qi)| denotan el número de características de términos únicos y de proximidad en F(Qi) respectivamente. La razón de elegir la función raíz cuadrada en el denominador de λξ es penalizar adecuadamente un conjunto de características de gran tamaño, haciendo que WIG sea más comparable entre consultas de diversas longitudes. λT es un parámetro fijo y se establece en 0.8 según [8] a lo largo de este documento. De manera similar, log P(Qi,C) se puede escribir como: )7()|(loglog),(log )( 2 ∑∈ +−= iQF i CPZCQP ξ ξ ξλ Cuando se eliminan las constantes Z1 y Z2, el WIG calculado en la Ecuación 4 se puede reescribir de la siguiente manera al sustituir la Ecuación 5 y la Ecuación 7: )8( )|( )|( log 1 ),,( )( )( ∑ ∑∈ ∈ = LTD QF t i Kt i CP DP K LCQWIG ξ ξ ξ ξ λ Una de las ventajas del WIG sobre otras técnicas es que puede manejar tanto consultas basadas en contenido como en NP. Basándose en el tipo (o el tipo predicho) de Qi, el cálculo de WIG en la Ecuación 8 difiere en dos aspectos: (1) cómo estimar P(ξ|Dt) y P(ξ|C), y (2) cómo elegir K. Para consultas basadas en contenido, P(ξ|C) se estima por la frecuencia relativa del rasgo ξ en la colección C en su totalidad. La estimación de P(ξ|Dt) es la misma que en [8]. Es decir, estimamos P(ξ|Dt) mediante la frecuencia relativa del rasgo ξ en Dt suavizada linealmente con la frecuencia de colección P(ξ|C). La K en la Ec.8 se trata como un parámetro libre. Ten en cuenta que K es el único parámetro libre en el cálculo de WIG para consultas basadas en contenido, ya que se asume que todos los parámetros involucrados en P(ξ|Dt) están fijos al tomar los valores sugeridos en [8]. En cuanto a las consultas de NP, hacemos uso de la estructura del documento para estimar P(ξ|Dt) y P(ξ|C) mediante el llamado modelo de mezcla de lenguaje propuesto en [10] e incorporado en el modelo MRF para la recuperación de Named-Page en [9]. La idea básica es que un documento (colección) se divide en varios campos como el campo del título, el campo del cuerpo principal y el campo de los encabezados. P(ξ|Dt) y P(ξ|C) se estiman mediante una combinación lineal de los modelos de lenguaje de cada campo. Debido a limitaciones de espacio, remitimos al lector a [9] para más detalles. Adoptamos el mismo conjunto exacto de parámetros utilizados en [9] para la estimación. En cuanto a K en la Ec.8, establecemos K en 1 porque la tarea de encontrar Named-Page se centra fuertemente en el documento clasificado en primer lugar. Por consiguiente, no hay parámetros libres en el cálculo de WIG para consultas de NP. 3.2 Retroalimentación de consultas En esta sección, presentamos otra técnica llamada retroalimentación de consultas (QF) para la predicción. Suponga que un usuario emite la consulta Q a un sistema de recuperación y se devuelve una lista clasificada L de documentos. Vemos el sistema de recuperación como un canal ruidoso. Específicamente, asumimos que la salida del canal es L y la entrada es Q. Después de pasar por el canal, Q se corrompe y se transforma en la lista clasificada L. Al pensar en el proceso de recuperación de esta manera, el problema de predecir la eficacia de la recuperación se convierte en la tarea de evaluar la calidad del canal. En otras palabras, la predicción se convierte en encontrar una forma de medir el grado de corrupción que surge cuando Q se transforma en L. Dado que calcular directamente el grado de corrupción es difícil, abordamos este problema mediante aproximaciones. Nuestra idea principal es que medimos en qué medida la información sobre Q puede ser recuperada de L bajo la suposición de que solo se observa L. Específicamente, diseñamos un decodificador que pueda traducir con precisión L de vuelta a la nueva consulta Q y la similitud S entre la consulta original Q y la nueva consulta Q se adopta como un predictor de rendimiento. Este es un bosquejo de cómo la técnica QF predice el rendimiento de la consulta. Antes de completar más detalles, discutimos brevemente por qué este método funcionaría. Existe una relación entre la similitud S definida anteriormente y el rendimiento de recuperación. Por un lado, si la recuperación se ha desviado del sentido original de la consulta Q, la nueva consulta Q extraída de la lista clasificada L en respuesta a Q sería muy diferente de la consulta original Q. Por otro lado, una consulta destilada de una lista clasificada que contiene muchos documentos relevantes es probable que sea similar a la consulta original. Se proporcionarán más ejemplos en apoyo de la relación más adelante. A continuación detallamos cómo construir el decodificador y cómo medir la similitud S. En esencia, el objetivo del decodificador es comprimir la lista clasificada L en unos pocos términos informativos que deberían representar el contenido de los documentos mejor clasificados en L. Nuestro enfoque para este objetivo es representar la lista clasificada L mediante un modelo de lenguaje (distribución de términos). Luego, los términos se clasifican según su contribución a la divergencia KL (Kullback-Leibler) de los modelos de lenguaje con respecto al modelo de la colección de fondo. Los términos mejor clasificados serán elegidos para formar la nueva consulta Q. Este enfoque es similar al utilizado en la Sección 4.1 de [11]. Específicamente, tomamos tres pasos para comprimir la lista clasificada L en la consulta Q sin hacer referencia a la consulta original. 1. Adoptamos el modelo de lenguaje de lista clasificada [14] para estimar un modelo de lenguaje basado en la lista clasificada L. El modelo puede escribirse como: )9()|()|()|( ∑∈ = LD LDPDwPLwP donde w es cualquier término y D es un documento. La probabilidad de que ocurra el evento D dado el evento L se estima mediante una función linealmente decreciente del rango del documento D. Cada término en P(w|L) está clasificado por la siguiente contribución de divergencia de Kullback-Leibler: )10( )|( )|( log)|( CwP LwP LwP donde P(w|C) es el modelo de colección estimado por la frecuencia relativa del término w en la colección C en su totalidad. 3. Los términos N principales clasificados por Eq.10 forman una consulta ponderada Q={(wi,ti)} i=1,N, donde wi denota el término clasificado en la posición i y el peso ti es la contribución de la divergencia KL de wi en Eq. 10. Barco de crucero daño vida marina. Estos ejemplos indican cómo la similitud entre la consulta original y la nueva se correlaciona con el rendimiento de recuperación. El parámetro N en el paso 3 se establece en 20 de manera empírica y elegir un valor más grande de N es innecesario ya que los pesos después de los primeros 20 suelen ser demasiado pequeños para marcar alguna diferencia. Para medir la similitud entre la consulta original Q y la nueva consulta Q, primero utilizamos Q para recuperar información de la misma colección. Se adopta una variante del modelo de probabilidad de consulta [15] para la recuperación. Es decir, los documentos se clasifican por: )11()|()|( ),( ∑∈ = Qtw t i ii i DwPDQP donde wi es un término en Q y ti es el peso asociado. D es un documento. Que L denote la nueva lista clasificada devuelta desde la recuperación anterior. La similitud se mide por la superposición de documentos en L y L. Específicamente, el porcentaje de documentos en los primeros K documentos de L que también están presentes en los primeros K documentos en L. El valor de corte K se trata como un parámetro libre. Aquí resumimos cómo la técnica QF predice el rendimiento dado una consulta Q y la lista clasificada asociada L. Primero obtenemos una consulta ponderada Q comprimida a partir de L mediante los tres pasos anteriores. Luego usamos Q para realizar la recuperación y la nueva lista clasificada es L. La superposición de documentos en L y L se utiliza para la predicción. 3.3 Primer Cambio de Rango (FRC) En esta sección, proponemos un método llamado primer cambio de rango (FRC) para la predicción de rendimiento para consultas de NP. Este método se deriva de la técnica de robustez de clasificación [1] que está principalmente diseñada para consultas basadas en contenido. Cuando se aplica directamente a consultas de NP, la técnica de robustez será menos efectiva porque tiene en cuenta todos los documentos mejor clasificados en su totalidad, mientras que las consultas de NP suelen tener solo un único documento relevante. En cambio, nuestra técnica se centra en el documento de rango uno mientras se mantiene la idea principal del método de robustez. Específicamente, el seudocódigo para calcular FRC se muestra en la figura 1. Lista clasificada L={Di} donde i=1,100. Di denota el documento clasificado en la posición i. (2) consulta Q 1 inicializar: (1) establecer el número de intentos J=100000 (2) contador c=0; 2 para i=1 a J 3 Perturbar cada documento en L, que el resultado sea un conjunto F={Di} donde Di denota la versión perturbada de Di. 4 Realizar la recuperación con la consulta Q en el conjunto F 5 c=c+1 si y solo si D1 está clasificado primero en el paso 4 6 fin del para 7 devolver la proporción c/J Figura 1: pseudo-código para calcular FRC FRC aproxima la probabilidad de que el documento clasificado en primer lugar en la lista original L permanezca clasificado primero incluso después de que los documentos sean perturbados. Cuanto mayor sea la probabilidad, más confianza tenemos en el documento clasificado en primer lugar. Por otro lado, en el caso extremo de un ranking aleatorio, la probabilidad sería tan baja como 0.5. Esperamos que FRC tenga una asociación positiva con el rendimiento de la consulta de NP. Adoptamos [1] para implementar el paso de perturbación del documento (paso 4 en la Fig. 1) utilizando distribuciones de Poisson. Para más detalles, remitimos al lector a [1]. 4. EVALUACIÓN Ahora presentamos los resultados de predecir el rendimiento de la consulta por nuestros modelos. Tres técnicas de vanguardia son adoptadas como nuestras líneas base. Evaluamos nuestras técnicas en una variedad de configuraciones de recuperación web. Como se mencionó anteriormente, consideramos dos tipos de consultas, es decir, consultas basadas en contenido (CB) y consultas de búsqueda de páginas con nombre (NP). Primero, supongamos que se conocen los tipos de consultas. Investigamos la correlación entre el rendimiento de recuperación predicho y el rendimiento real para ambos tipos de consultas por separado. Los resultados muestran que nuestros métodos producen mejoras considerablemente superiores a los valores de referencia. Luego consideramos un escenario más desafiante donde no hay información previa sobre los tipos de consultas disponibles. Se consideran dos subcasos. En el primero, existe solo un tipo de consulta pero el tipo real es desconocido. Suponemos una mezcla de los dos tipos de consultas en el segundo caso. Demostramos que nuestros modelos logran una buena precisión en este escenario exigente, lo que hace que la predicción sea práctica en un entorno de búsqueda web del mundo real. 4.1 Configuración Experimental Nuestra evaluación se centra en la colección GOV2 que contiene alrededor de 25 millones de documentos obtenidos de sitios web en el dominio .gov durante 2004 [3]. Creamos dos tipos de conjuntos de datos para consultas de CB y consultas de NP respectivamente. Para el tipo CB, utilizamos los temas ad-hoc de las pistas Terabyte de 2004, 2005 y 2006 y los nombramos TB04-adhoc, TB05-adhoc y TB06-adhoc respectivamente. Además, también utilizamos los temas ad-hoc de la pista Robusta de 2004 (RT04) para probar la adaptabilidad de nuestras técnicas a un entorno no web. Para consultas de NP, utilizamos los temas de búsqueda de páginas nombradas de las Terabyte Tracks de 2005 y 2006 y los denominamos TB05-NP y TB06-NP respectivamente. Todas las consultas utilizadas en nuestros experimentos son títulos de temas de TREC, ya que nos centramos en la recuperación web. La Tabla 3 resume los conjuntos de datos anteriores. La recuperación del rendimiento de las consultas individuales basadas en contenido y NP se mide mediante la precisión promedio y la tasa recíproca de la primera respuesta correcta, respectivamente. Hacemos uso del modelo de campo aleatorio de Markov tanto para la recuperación ad-hoc como para la recuperación de páginas nombradas. Adoptamos la misma configuración de parámetros de recuperación utilizada en [8,9]. El motor de búsqueda Indri [12] se utiliza para todos nuestros experimentos. Aunque no se informa aquí, también probamos el modelo de probabilidad de consulta para la recuperación ad-hoc y encontramos que los resultados cambian poco debido a la correlación muy alta entre el rendimiento de las consultas obtenido por los dos modelos de recuperación (0.96 medido por el coeficiente de Pearson). 4.2 Tipos de Consultas Conocidos Supongamos que se conocen los tipos de consultas. Tratamos cada tipo de consulta por separado y medimos la correlación con la precisión promedio (o el rango recíproco en el caso de consultas NP). Adoptamos la prueba de correlación de Pearson que refleja el grado de relación lineal entre el rendimiento de recuperación predicho y real. 4.2.1 Métodos de Consultas Basadas en Contenido Claridad Robusta JSD WIG QF WIG +QF TB04+0 5 adhoc 0.333 0.317 0.362 0.574 0.480 0.637 TB06 adhoc 0.076 0.294 N/A 0.464 0.422 0.511 Tabla 4: Coeficientes de correlación de Pearson para la correlación con la precisión promedio en las Pistas de Terabyte (ad-hoc) para la puntuación de claridad, puntuación de robustez, el método basado en JSD (citamos directamente la puntuación informada en [2]), WIG, retroalimentación de consulta (QF) y una combinación lineal de WIG y QF. Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. La Tabla 4 muestra la correlación con la precisión promedio en dos conjuntos de datos: uno es una combinación de TB04-adhoc y TB05-adhoc (100 temas en total) y el otro es TB06-adhoc (50 temas). La razón por la que juntamos TB04-adhoc y TB05-adhoc es para hacer nuestros resultados comparables a [2]. Nuestros puntos de referencia son la puntuación de claridad (claridad) [6], la puntuación de robustez (robust) [1] y el método basado en JSD (JSD) [2]. Para la puntuación de claridad y robustez, hemos probado diferentes configuraciones de parámetros y reportamos los coeficientes de correlación más altos que hemos encontrado. Citamos directamente el resultado del método basado en JSD reportado en [2]. La tabla también muestra los resultados para el método de Ganancia de Información Ponderada (WIG) y el método de Retroalimentación de Consulta (QF) para predecir consultas basadas en contenido. Como describimos en la sección anterior, tanto WIG como QF tienen un parámetro libre para ajustar, es decir, el rango de corte K. Entrenamos el parámetro en un conjunto de datos y lo probamos en el otro. Al combinar WIG y QF, se utiliza una combinación lineal simple y el peso de la combinación se aprende a partir del conjunto de datos de entrenamiento. A partir de estos resultados, podemos ver que nuestros métodos son considerablemente más precisos en comparación con los baselines. También observamos que se obtienen mejoras adicionales a partir de la combinación de WIG y QF, lo que sugiere que miden diferentes propiedades del proceso de recuperación que se relacionan con el rendimiento. Descubrimos que nuestros métodos se generalizan bien en TB06-adhoc, mientras que la correlación del puntaje de claridad con el rendimiento de recuperación en este conjunto de datos es considerablemente peor. Una investigación adicional muestra que la precisión promedio media de TB06-adhoc es de 0.342 y es aproximadamente un 10% mejor que la del primer conjunto de datos. Mientras que los otros tres métodos suelen considerar los 100 mejores documentos o menos de una lista clasificada, el método de claridad generalmente necesita los 500 mejores documentos o más para medir adecuadamente la coherencia de una lista clasificada. Un promedio de precisión media más alto hace que las listas clasificadas recuperadas por diferentes consultas sean más similares en términos de coherencia en el nivel de los primeros 500 documentos. Creemos que esta es la razón principal de la baja precisión de la puntuación de claridad en el segundo conjunto de datos. Aunque este documento se centra en un entorno de búsqueda web, es deseable que nuestras técnicas funcionen de manera consistente en otras situaciones. Con este fin, examinamos la efectividad de nuestras técnicas en la pista Robust 2004. Para nuestros métodos, dividimos equitativamente todas las consultas de prueba en cinco grupos y realizamos validación cruzada de cinco pliegues. Cada vez que utilizamos un grupo para el entrenamiento y los otros cuatro grupos para las pruebas. Hacemos uso de todas las consultas para nuestros dos puntos de referencia, es decir, la puntuación de claridad y la puntuación de robustez. Los parámetros para nuestras líneas base son los mismos que los utilizados en [1]. Los resultados mostrados en la Tabla 5 demuestran que la precisión de predicción de nuestros métodos está a la par con la de las dos líneas base sólidas. Claridad Robusta WIG QF 0.464 0.539 0.468 0.464 Tabla 5: Comparación de los coeficientes de correlación de Pearson en la pista Robusta de 2004 para la puntuación de claridad, puntuación de robustez, WIG y retroalimentación de consulta (QF). Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. Además, examinamos la sensibilidad de predicción de nuestros métodos al rango de corte K. Con respecto a WIG, es bastante robusto a K en las Pistas de Terabyte (2004-2006) mientras prefiere un valor pequeño de K como 5 en la Pista Robusta de 2004. En otras palabras, un valor pequeño de K es una elección casi óptima para ambos tipos de pistas. Teniendo en cuenta que todos los demás parámetros involucrados en WIG están fijos y, consecuentemente, son los mismos para los dos casos, esto significa que WIG puede lograr una precisión de predicción casi óptima en dos situaciones considerablemente diferentes con exactamente la misma configuración de parámetros. En cuanto a QF, prefiere un valor más grande de K, como 100 en las Pistas de Terabyte y un valor más pequeño de K, como 25 en la Pista Robusta de 2004. 4.2.2 Consultas NP Adoptamos WIG y el primer cambio de rango (FRC) para predecir el rendimiento de las consultas NP. También intentamos una combinación lineal de los dos como en la sección anterior. El peso de la combinación se obtiene del otro conjunto de datos. Utilizamos la correlación con los rangos recíprocos medidos por la prueba de correlación de Pearson para evaluar la calidad de la predicción. Los resultados se presentan en la Tabla 6. Nuevamente, nuestros puntos de referencia son la puntuación de claridad y la puntuación de robustez. Para hacer una comparación justa, ajustamos la puntuación de claridad de diferentes maneras. Descubrimos que utilizar el documento clasificado en primer lugar para construir el modelo de consulta produce la mejor precisión de predicción. También intentamos utilizar la estructura del documento mediante la combinación de modelos de lenguaje mencionados en la sección 3.1. Se obtuvo una pequeña mejora. Los coeficientes de correlación para la puntuación de claridad reportados en la Tabla 6 son los mejores que hemos encontrado. Como podemos ver, nuestros métodos superan considerablemente la técnica de puntuación de claridad en ambas ejecuciones. Esto confirma nuestra intuición de que el uso de una medida basada en la coherencia como el puntaje de claridad es inapropiado para las consultas de NP. Claridad de métodos robusta. Tabla 6: Coeficientes de correlación de Pearson para la correlación con rangos recíprocos en las pistas de Terabyte (NP) para la puntuación de claridad, puntuación de robustez, WIG, el primer cambio de rango (FRC) y una combinación lineal de WIG y FRC. Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. En cuanto a la puntuación de robustez, también ajustamos los parámetros y reportamos el mejor que hemos encontrado. Observamos una correlación negativa interesante y sorprendente con los rangos recíprocos. Explicamos este hallazgo brevemente. Un puntaje de robustez alto significa que varios documentos de alta clasificación en la lista clasificada original siguen estando muy bien clasificados después de perturbar los documentos. La existencia de tales documentos es una buena señal de alto rendimiento para consultas basadas en contenido, ya que estas consultas suelen contener una serie de documentos relevantes [1]. Sin embargo, en lo que respecta a las consultas de NP, una diferencia fundamental es que solo hay un documento relevante para cada consulta. La existencia de tales documentos puede confundir la función de clasificación y llevar a un bajo rendimiento de recuperación. Aunque existe una correlación negativa con el rendimiento de recuperación, la fuerza de la correlación es más débil y menos consistente en comparación con nuestros métodos, como se muestra en la Tabla 6. Basándonos en el análisis anterior, podemos ver que las técnicas de predicción actuales como la puntuación de claridad y la puntuación de robustez, que están principalmente diseñadas para consultas basadas en contenido, enfrentan desafíos significativos y son insuficientes para manejar consultas de NP. Nuestras dos técnicas propuestas para consultas de NP demuestran consistentemente una buena precisión de predicción, mostrando un éxito inicial en la resolución del problema de predecir el rendimiento para consultas de NP. Otro punto que queremos destacar es que el método WIG funciona bien para ambos tipos de consultas, una propiedad deseable que la mayoría de las técnicas de predicción carecen. 4.3 Tipos de Consultas Desconocidos En esta sección, realizamos dos tipos de experimentos sin acceso a etiquetas de tipo de consulta. Primero, asumimos que solo existe un tipo de consulta pero el tipo es desconocido. Segundo, experimentamos con una mezcla de consultas basadas en contenido y NP. Las dos subsecciones siguientes informarán los resultados para las dos condiciones respectivamente. 4.3.1 Solo existe un tipo. Suponemos que todas las consultas son del mismo tipo, es decir, son consultas NP o consultas basadas en contenido. Elegimos WIG para tratar este caso porque muestra una buena precisión de predicción para ambos tipos de consultas en la sección anterior. Consideramos dos casos: (1) CB: todas las 150 consultas de títulos de la tarea ad-hoc de las pistas Terabyte 2004-2006 (2) NP: todas las 433 consultas de NP de la tarea de búsqueda de páginas nombradas de las pistas Terabyte 2005 y 2006. Tomamos una estrategia simple etiquetando todas las consultas en cada caso como el mismo tipo (ya sea NP o CB) independientemente de su tipo real. El cálculo de WIG se basará en el tipo de consulta etiquetado en lugar del tipo real. Hay cuatro posibilidades con respecto a la relación entre el tipo real y el tipo etiquetado. La correlación con el rendimiento de recuperación bajo las cuatro posibilidades se presenta en la Tabla 7. Por ejemplo, el valor 0.445 en la intersección entre la segunda fila y la tercera columna muestra el coeficiente de correlación de Pearson para la correlación con la precisión promedio cuando las consultas basadas en contenido se etiquetan incorrectamente como del tipo NP. Basándonos en estos resultados, recomendamos tratar todas las consultas como del tipo NP cuando solo existe un tipo de consulta y la clasificación precisa de la consulta no es factible, considerando el riesgo de que se produzca una gran pérdida de precisión si las consultas NP se etiquetan incorrectamente como consultas basadas en contenido. Estos resultados también demuestran la fuerte adaptabilidad de WIG a diferentes tipos de consultas. Tabla 7: Comparación de los coeficientes de correlación de Pearson para la correlación con el rendimiento de recuperación en cuatro posibilidades en las pistas de Terabyte (NP). Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. 4.3.2 Una mezcla de consultas basadas en contenido y NP Una mezcla de los dos tipos de consultas es una situación más realista que un motor de búsqueda web encontrará. Evaluamos la precisión de la predicción por la exactitud con la que se pueden identificar las consultas de bajo rendimiento mediante el método de predicción, asumiendo que los tipos de consulta reales son desconocidos (pero podemos predecir los tipos de consulta). Esta es una tarea desafiante porque tanto el rendimiento predicho como el real para un tipo de consulta pueden ser incomparables con el de otro tipo. A continuación discutimos cómo implementar nuestra evaluación. Creamos un conjunto de consultas que consiste en todas las 150 consultas de título ad-hoc de Terabyte Track 2004-2006 y todas las 433 consultas NP de Terabyte Track 2005&2006. Dividimos las consultas en el conjunto en clases: buenas (mejores que el 50% de las consultas del mismo tipo en términos de rendimiento de recuperación) y malas (de lo contrario). Según estos estándares, una consulta de NP con un rango recíproco superior a 0.2 o una consulta basada en contenido con una precisión promedio superior a 0.315 se considerarán como buenas. Entonces, cada vez que seleccionamos aleatoriamente una consulta Q del grupo con una probabilidad p de que Q esté basada en el contenido. Las consultas restantes se utilizan como datos de entrenamiento. Primero decidimos el tipo de consulta Q de acuerdo con un clasificador de consultas. Es decir, el clasificador de consultas nos dice si la consulta Q es de tipo NP o basada en contenido. Basándose en el tipo de consulta predicho y en la puntuación calculada para la consulta Q por una técnica de predicción, se toma una decisión binaria sobre si la consulta Q es buena o mala al compararla con el umbral de puntuación del tipo de consulta predicho obtenido de los datos de entrenamiento. La precisión de la predicción se mide por la exactitud de la decisión binaria. En nuestra implementación, tomamos repetidamente una consulta de prueba del conjunto de consultas y la precisión de la predicción se calcula como el porcentaje de decisiones correctas, es decir, una consulta buena (mala) se predice como buena (mala). Es obvio que adivinar al azar llevará a una precisión del 50%. Tomemos el método WIG como ejemplo para ilustrar el proceso. Dos umbrales de WIG (uno para consultas de NP y otro para consultas basadas en contenido) se entrenan maximizando la precisión de predicción en los datos de entrenamiento. Cuando una consulta de prueba es etiquetada como del tipo NP (CB) por el clasificador de tipo de consulta, se predecirá que es buena solo si la puntuación WIG para esta consulta está por encima del umbral de NP (CB). Se seguirán procedimientos similares para otras técnicas de predicción. Ahora presentamos brevemente el clasificador de tipo de consulta automático utilizado en este artículo. Encontramos que la puntuación de robustez, aunque originalmente propuesta para la predicción de rendimiento, es un buen indicador de tipos de consultas. Observamos que, en promedio, las consultas basadas en contenido tienen una puntuación de robustez mucho más alta que las consultas de NP. Por ejemplo, la Figura 2 muestra las distribuciones de puntajes de robustez para consultas basadas en NP y en contenido. Según este hallazgo, el clasificador de puntuación de robustez adjuntará una etiqueta NP (CB) a la consulta si la puntuación de robustez para la consulta está por debajo (por encima) de un umbral entrenado a partir de los datos de entrenamiento. 0 0.5 1 1.5 2 2.5 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 NP Basado en contenido Figura 2: Distribución de puntuaciones de robustez para consultas NP y CB. Las consultas NP son los 252 temas NP del Terabyte Track de 2005. Las consultas basadas en contenido son los 150 títulos ad-hoc de las pistas Terabyte 2004-2006. Las distribuciones de probabilidad son estimadas por el método de estimación de densidad de Kernel. Estrategias Robustas WIG-1 WIG-2 WIG-3 Óptima p=0.6 0.565 0.624 0.665 0.684 0.701 P=0.4 0.567 0.633 0.654 0.673 0.696 Tabla 8: Comparación de la precisión de predicción para cinco estrategias en la situación de consulta mixta. Dos formas de muestrear una consulta de la piscina: (1) la consulta muestreada es basada en contenido con una probabilidad p=0.6 (es decir, la consulta es NP con una probabilidad de 0.4) (2) establecer la probabilidad p=0.4. Consideramos cinco estrategias en nuestros experimentos. En la primera estrategia (denominada robusta), utilizamos la puntuación de robustez para la predicción del rendimiento de la consulta con la ayuda de un clasificador de consultas perfecto que siempre mapea correctamente una consulta en una de las dos categorías (es decir, NP o CB). Esta estrategia representa el nivel de precisión de predicción que las técnicas actuales de predicción pueden lograr en una condición ideal en la que se conocen los tipos de consulta. En las tres estrategias siguientes, se adopta el método WIG para predecir el rendimiento. La diferencia entre los tres es que se utilizan tres clasificadores de consulta diferentes para cada estrategia: (1) el clasificador siempre clasifica una consulta en el tipo NP. (2) el clasificador de Probabilidad de Densidad de Puntuación Robusta es el clasificador de puntuación robusta mencionado anteriormente. (3) el clasificador es perfecto. Estas tres estrategias están designadas como WIG-1, WIG-2 y WIG-3 respectivamente. La razón por la que estamos interesados en WIG-1 se basa en los resultados de la sección 4.3.1. En la última estrategia (denominada Óptima) que sirve como un límite superior de lo bien que podemos hacer hasta ahora, hacemos pleno uso de nuestras técnicas de predicción para cada tipo de consulta asumiendo que un clasificador de consultas perfecto está disponible. Específicamente, combinamos linealmente WIG y QF para consultas basadas en contenido y WIG y FRC para consultas de NP. Los resultados de las cinco estrategias se muestran en la Tabla 8. Para cada estrategia, intentamos dos formas de muestrear una consulta del conjunto: (1) la consulta muestreada es CB con probabilidad p=0.6 (la consulta es NP con probabilidad 0.4) (2) establecer la probabilidad p=0.4. De la Tabla 8 podemos ver que en términos de precisión de predicción, WIG-2 (el método WIG con el clasificador de consultas automático) no solo es mejor que los dos primeros casos, sino que también está cerca de WIG-3 donde se asume un clasificador perfecto. Se observan algunas mejoras adicionales sobre WIG-3 cuando se combinan con otras técnicas de predicción. El mérito de WIG-2 es que proporciona una solución práctica para identificar automáticamente consultas con bajo rendimiento en un entorno de búsqueda web con tipos de consultas mixtos, lo cual representa considerables obstáculos para las técnicas de predicción tradicionales. CONCLUSIONES Y TRABAJOS FUTUROS Hasta donde sabemos, nuestro artículo es el primero en explorar a fondo la predicción del rendimiento de consultas en entornos de búsqueda web. Demostramos que nuestros modelos resultaron en una mayor precisión de predicción que las técnicas previamente publicadas no especialmente diseñadas para escenarios de búsqueda en la web. En este artículo, nos enfocamos en dos tipos de consultas en la búsqueda web: consultas basadas en contenido y consultas de búsqueda de Páginas-Nombradas (NP), correspondientes a la tarea de recuperación ad-hoc y la tarea de búsqueda de Páginas-Nombradas respectivamente. Para ambos tipos de consultas web, se demostró que nuestros modelos de predicción son sustancialmente más precisos que las técnicas actuales más avanzadas. Además, consideramos un caso más realista en el que no se dispone de información previa sobre los tipos de consultas. Demostramos que el método WIG es particularmente adecuado para esta situación. Considerando la adaptabilidad de WIG a una variedad de colecciones y tipos de consultas, uno de nuestros planes futuros es aplicar este método para predecir la preferencia del usuario por los resultados de búsqueda en datos realistas recopilados de un motor de búsqueda comercial. Además de la precisión, otro problema importante con el que las técnicas de predicción tienen que lidiar en un entorno web es la eficiencia. Afortunadamente, dado que el puntaje WIG se calcula solo sobre los términos y frases que aparecen en la consulta, este cálculo puede realizarse de manera muy eficiente con el soporte de un índice. Por otro lado, el cálculo de QF y FRC es relativamente menos eficiente ya que QF necesita recuperar toda la colección dos veces y FRC necesita clasificar repetidamente los documentos perturbados. Cómo mejorar la eficiencia de QF y FRC es nuestro trabajo futuro. Además, las técnicas de predicción propuestas en este documento tienen el potencial de mejorar el rendimiento de recuperación al combinarse con otras técnicas de RI. Por ejemplo, nuestras técnicas pueden ser incorporadas a técnicas populares de modificación de consultas como la expansión de consultas y la relajación de consultas. Guiados por la predicción del rendimiento, podemos tomar una mejor decisión sobre cuándo o cómo modificar las consultas para mejorar la efectividad de la recuperación. Nos gustaría llevar a cabo investigaciones en esta dirección en el futuro. AGRADECIMIENTOS Este trabajo fue apoyado en parte por el Centro de Recuperación de Información Inteligente, en parte por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el contrato número HR0011-06-C0023, y en parte por un premio de Google. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son del autor y no necesariamente reflejan las del patrocinador. Además, agradecemos a Donald Metzler por sus valiosos comentarios sobre este trabajo. 7. REFERENCIAS [1] Y. Zhou, W. B. Croft, Ranking Robustness: Un nuevo marco para predecir el rendimiento de consultas, en Actas de CIKM 2006. [2] D. Carmel, E. Yom-Tov, A. Darlow, D. Pelleg, ¿Qué hace que una consulta sea difícil?, en Actas de SIGIR 2006. [3] C. L. A. Clarke, F. Scholer, I. Soboroff, La pista Terabyte TREC 2005, En las Actas en Línea de TREC 2005. [4] B. Él y yo. Inferir el rendimiento de la consulta utilizando predictores de preretrieval. En actas de SPIRE 2004. [5] S. Tomlinson. Recuperación robusta de la web y terabytes con Hummingbird SearchServer en TREC 2004. En las Actas en Línea de TREC 2004. [6] S. Cronen-Townsend, Y. Zhou, W. B. Croft, Prediciendo el Rendimiento de Consultas, en las Actas de SIGIR 2002. [7] V. Vinay, I. J. Cox, N. Mill-Frayling, K. Wood, Sobre la Clasificación de la Efectividad del Buscador, en las Actas de SIGIR 2006. [8] D. Metzler, W. B. Croft, Un Modelo de Campo Aleatorio de Markov para Dependencias de Términos, en las Actas de SIGIR 2005. [9] D. Metzler, T. Strohman, Y. Zhou, W. B. Croft, Indri en TREC 2005: Terabyte Track, en las Actas en Línea de TREC 2004. [10] P. Ogilvie y J. Callan, Combinando representaciones de documentos para búsqueda de elementos conocidos, en las Actas de SIGIR 2003. [11] A. Berger, J. Lafferty, Recuperación de información como traducción estadística, en las Actas de SIGIR 1999. [12] Motor de búsqueda Indri: http://www.lemurproject.org/indri/ [13] I. J. Taneja: Sobre medidas de información generalizadas y sus aplicaciones, Avances en Electrónica y Física de Electrones, Academic Press (EE. UU.), 76, 1989, 327-413. [14] S. Cronen-Townsend, Y. Zhou y Croft, W. B., Un marco para la expansión selectiva de consultas, en Actas de CIKM 2004. [15] F. Song, W. B. Croft, Un modelo de lenguaje general para la recuperación de información, en Actas de SIGIR 1999. [16] Contacto por correo electrónico personal con Vishwa Vinay y nuestros propios experimentos [17] E. Yom-Tov, S. Fine, D. Carmel, A. Darlow, Aprendiendo a estimar la dificultad de la consulta incluyendo aplicaciones para la detección de contenido faltante y la recuperación de información distribuida, en Actas de SIGIR 2005. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "gov2 collection": {
            "translated_key": "colección GOV2",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Query Performance Prediction in Web Search Environments Yun Zhou and W. Bruce Croft Department of Computer Science University of Massachusetts, Amherst {yzhou, croft}@cs.umass.edu ABSTRACT Current prediction techniques, which are generally designed for content-based queries and are typically evaluated on relatively homogenous test collections of small sizes, face serious challenges in web search environments where collections are significantly more heterogeneous and different types of retrieval tasks exist.",
                "In this paper, we present three techniques to address these challenges.",
                "We focus on performance prediction for two types of queries in web search environments: content-based and Named-Page finding.",
                "Our evaluation is mainly performed on the <br>gov2 collection</br>.",
                "In addition to evaluating our models for the two types of queries separately, we consider a more challenging and realistic situation that the two types of queries are mixed together without prior information on query types.",
                "To assist prediction under the mixed-query situation, a novel query classifier is adopted.",
                "Results show that our prediction of web query performance is substantially more accurate than the current stateof-the-art prediction techniques.",
                "Consequently, our paper provides a practical approach to performance prediction in realworld web settings.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval -Query formulation General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Query performance prediction has many applications in a variety of information retrieval (IR) areas such as improving retrieval consistency, query refinement, and distributed IR.",
                "The importance of this problem has been recognized by IR researchers and a number of new methods have been proposed for prediction recently [1, 2, 17].",
                "Most work on prediction has focused on the traditional ad-hoc retrieval task where query performance is measured according to topical relevance.",
                "These prediction models are evaluated on TREC document collections which typically consist of no more than one million relatively homogenous newswire articles.",
                "With the popularity and influence of the Web, prediction techniques that will work well for web-style queries are highly preferable.",
                "However, web search environments pose significant challenges to current prediction models that are mainly designed for traditional TREC settings.",
                "Here we outline some of these challenges.",
                "First, web collections, which are much larger than conventional TREC collections, include a variety of documents that are different in many aspects such as quality and style.",
                "Current prediction techniques can be vulnerable to these characteristics of web collections.",
                "For example, the reported prediction accuracy of the ranking robustness technique and the clarity technique on the <br>gov2 collection</br> (a large web collection) is significantly worse compared to the other TREC collections [1].",
                "Similar prediction accuracy on the <br>gov2 collection</br> using another technique is reported in [2], confirming the difficult of predicting query performance on a large web collection.",
                "Furthermore, web search goes beyond the scope of the ad-hoc retrieval task based on topical relevance.",
                "For example, the Named-Page (NP) finding task, which is a navigational task, is also popular in web retrieval.",
                "Query performance prediction for the NP task is still necessary since NP retrieval performance is far from perfect.",
                "In fact, according to the report on the NP task of the 2005 Terabyte Track [3], about 40% of the test queries perform poorly (no correct answer in the first 10 search results) even in the best run from the top group.",
                "To our knowledge, little research has explicitly addressed the problem of NP-query performance prediction.",
                "Current prediction models devised for content-based queries will be less effective for NP queries considering the fundamental differences between the two.",
                "Third, in real-world web search environments, user queries are usually a mixture of different types and prior knowledge about the type of each query is generally unavailable.",
                "The mixed-query situation raises new problems for query performance prediction.",
                "For instance, we may need to incorporate a query classifier into prediction models.",
                "Despite these problems, the ability to handle this situation is a crucial step towards turning query performance prediction from an interesting research topic into a practical tool for web retrieval.",
                "In this paper, we present three techniques to address the above challenges that current prediction models face in Web search environments.",
                "Our work focuses on query performance prediction for the content-based (ad-hoc) retrieval task and the name-page finding task in the context of web retrieval.",
                "Our first technique, called weighted information gain (WIG), makes use of both single term and term proximity features to estimate the quality of top retrieved documents for prediction.",
                "We find that WIG offers consistent prediction accuracy across various test collections and query types.",
                "Moreover, we demonstrate that good prediction accuracy can be achieved for the mixed-query situation by using WIG with the help of a query type classifier.",
                "Query feedback and first rank change, which are our second and third prediction techniques, perform well for content-based queries and NP queries respectively.",
                "Our main contributions include: (1) considerably improved prediction accuracy for web content-based queries over several state-of-the-art techniques. (2) new techniques for successfully predicting NP-query performance. (3) a practical and fully automatic solution to predicting mixed-query performance.",
                "In addition, one minor contribution is that we find that the robustness score [1], which was originally proposed for performance prediction, is helpful for query classification. 2.",
                "RELATED WORK As we mentioned in the introduction, a number of prediction techniques have been proposed recently that focus on contentbased queries in the topical relevance (ad-hoc) task.",
                "We know of no published work that addresses other types of queries such as NP queries, let alone a mixture of query types.",
                "Next we review some representative models.",
                "The major difficulty of performance prediction comes from the fact that many factors have an impact on retrieval performance.",
                "Each factor affects performance to a different degree and the overall effect is hard to predict accurately.",
                "Therefore, it is not surprising to notice that simple features, such as the frequency of query terms in the collection [4] and the average IDF of query terms [5], do not predict well.",
                "In fact, most of the successful techniques are based on measuring some characteristics of the retrieved document set to estimate topic difficulty.",
                "For example, the clarity score [6] measures the coherence of a list of documents by the KL-divergence between the query model and the collection model.",
                "The robustness score [1] quantifies another property of a ranked list: the robustness of the ranking in the presence of uncertainty.",
                "Carmel et al. [2] found that the distance measured by the Jensen-Shannon divergence between the retrieved document set and the collection is significantly correlated to average precision.",
                "Vinay et al. [7] proposed four measures to capture the geometry of the top retrieved documents for prediction.",
                "The most effective measure is the sensitivity to document perturbation, an idea somewhat similar to the robustness score.",
                "Unfortunately, their way of measuring the sensitivity does not perform equally well for short queries and prediction accuracy drops considerably when a state-of-the-art retrieval technique (like Okapi or a language modeling approach) is adopted for retrieval instead of the tf-idf weighting used in their paper [16].",
                "The difficulties of applying these models in web search environments have already been mentioned.",
                "In this paper, we mainly adopt the clarity score and the robustness score as our baselines.",
                "We experimentally show that the baselines, even after being carefully tuned, are inadequate for the web environment.",
                "One of our prediction models, WIG, is related to the Markov random field (MRF) model for information retrieval [8].",
                "The MRF model directly models term dependence and is found be to highly effective across a variety of test collections (particularly web collections) and retrieval tasks.",
                "This model is used to estimate the joint probability distribution over documents and queries, an important part of WIG.",
                "The superiority of WIG over other prediction techniques based on unigram features, which will be demonstrated later in our paper, coincides with that of MRF for retrieval.",
                "In other word, it is interesting to note that term dependence, when being modeled appropriately, can be helpful for both improving and predicting retrieval performance. 3.",
                "PREDICTION MODELS 3.1 Weighted Information Gain (WIG) This section introduces a weighted information gain approach that incorporates both single term and proximity features for predicting performance for both content-based and Named-Page (NP) finding queries.",
                "Given a set of queries Q={Qs} (s=1,2,..N) which includes all possible user queries and a set of documents D={Dt} (t=1,2…M), we assume that each query-document pair (Qs,Dt) is manually judged and will be put in a relevance list if Qs is found to be relevant to Dt.",
                "The joint probability P(Qs,Dt) over queries Q and documents D denotes the probability that pair (Qs,Dt) will be in the relevance list.",
                "Such assumptions are similar to those used in [8].",
                "Assuming that the user issues query Qi ∈Q and the retrieval results in response to Qi is a ranked list L of documents, we calculate the amount of information contained in P(Qs,Dt) with respect to Qi and L by Eq.1 which is a variant of entropy called the weighted entropy[13].",
                "The weights in Eq.1 are solely determined by Qi and L. )1(),(log),(),( , , ∑−= ts tststsLQ DQPDQweightDQH i In this paper, we choose the weights as follows: LindocumentsKtopthecontainsLTwhere otherwise LTDandisifK DQweight K Kt ts )( )2( ,0 )(,/1 ),( ⎩ ⎨ ⎧ ∈= = The cutoff rank K is a parameter in our model that will be discussed later.",
                "Accordingly, Eq.1 can be simplified as follows: )3(),(log 1 ),( )( , ∑∈ −= LTD titsLQ Kt i DQP K DQH Unfortunately, weighted entropy ),(, tsLQ DQH i computed by Eq.3, which represents the amount of information about how likely the top ranked documents in L would be relevant to query Qi on average, cannot be compared across different queries, making it inappropriate for directly predicting query performance.",
                "To mitigate this problem, we come up with a background distribution P(Qs,C) over Q and D by imagining that every document in D is replaced by the same special document C which represents average language usage.",
                "In this paper, C is created by concatenating every document in D. Roughly speaking, C is the collection (the document set) {Dt} without document boundaries.",
                "Similarly, weighted entropy ),(, CQH sLQi calculated by Eq.3 represents the amount of information about how likely an average document (represented by the whole collection) would be relevant to query Qi.",
                "Now we introduce our performance predictor WIG which is the weighted information gain [13] computed as the difference between ),(, tsLQ DQH i and ),(, CQH sLQi .Specifically, given query Qi, collection C and ranked list L of documents, WIG is calculated as follows: )4( ),( ),( log 1 ),( ),( log),( ),(),(),,( )(, ,, ∑∑ ∈ == −= LTD i ti ts s ts ts tsLQsLQi Kt ii CQP DQP KCQP DQP DQweight DQHCQHLCQWIG WIG computed by Eq.4 measures the change in information about the quality of retrieval (in response to query Qi) from an imaginary state that only an average document is retrieved to a posterior state that the actual search results are observed.",
                "We hypothesize that WIG is positively correlated with retrieval effectiveness because high quality retrieval should be much more effective than just returning the average document.",
                "The heart of this technique is how to estimate the joint distribution P(Qs,Dt).",
                "In the language modeling approach to IR, a variety of models can be applied readily to estimate this distribution.",
                "Although most of these models are based on the bagof-words assumption, recent work on modeling term dependence under the language modeling framework have shown consistent and significant improvements in retrieval effectiveness over bagof-words models.",
                "Inspired by the success of incorporating term proximity features into language models, we decide to adopt a good dependence model to estimate the probability P(Qs,Dt).",
                "The model we chose for this paper is Metzler and Crofts Markov Random Field (MRF) model, which has already demonstrated superiority over a number of collections and different retrieval tasks [8,9].",
                "According to the MRF model, log P(Qi, Dt) can be written as )5()|(loglog),(log )( 1 ∑∈ +−= iQF tti DPZDQP ξ ξ ξλ where Z1 is a constant that ensures that P(Qi, Dt) sums up to 1.",
                "F(Qi) consists of a set of features expanded from the original query Qi .",
                "For example, assuming that query Qi is talented student program, F(Qi) includes features like program and talented student.",
                "We consider two kinds of features: single term features T and proximity features P. Proximity features include exact phrase (#1) and unordered window (#uwN) features as described in [8].",
                "Note that F(Qi) is the union of T(Qi) and P(Qi).",
                "For more details on F(Qi) such as how to expand the original query Qi to F(Qi), we refer the reader to [8] and [9].",
                "P(ξ|Dt) denotes the probability that feature ξ will occur in Dt.",
                "More details on P(ξ|Dt) will be provided later in this section.",
                "The choice of λξ is somewhat different from that used in [8] since λξ plays a dual role in our model.",
                "The first role, which is the same as in [8], is to weight between single term and proximity features.",
                "The other role, which is specific to our prediction task, is to normalize the size of F(Qi).We found that the following weight strategy for λξ satisfies the above two roles and generalizes well on a variety of collections and query types. )6( )(, |)(| 1 )(, |)(| ⎪ ⎪ ⎩ ⎪ ⎪ ⎨ ⎧ ∈ − ∈ = i i T i i T QP QP QT QT ξ λ ξ λ λξ where |T(Qi)| and |P(Qi)| denote the number of single term and proximity features in F(Qi) respectively.",
                "The reason for choosing the square root function in the denominator of λξ is to penalize a feature set of large size appropriately, making WIG more comparable across queries of various lengths. λT is a fixed parameter and set to 0.8 according to [8] throughout this paper.",
                "Similarly, log P(Qi,C) can be written as: )7()|(loglog),(log )( 2 ∑∈ +−= iQF i CPZCQP ξ ξ ξλ When constant Z1 and Z2 are dropped, WIG computed in Eq.4 can be rewritten as follows by plugging in Eq.5 and Eq.7 : )8( )|( )|( log 1 ),,( )( )( ∑ ∑∈ ∈ = LTD QF t i Kt i CP DP K LCQWIG ξ ξ ξ ξ λ One of the advantages of WIG over other techniques is that it can handle well both content-based and NP queries.",
                "Based on the type (or the predicted type) of Qi, the calculation of WIG in Eq. 8 differs in two aspects: (1) how to estimate P(ξ|Dt) and P(ξ|C), and (2) how to choose K. For content-based queries, P(ξ|C) is estimated by the relative frequency of feature ξ in collection C as a whole.",
                "The estimation of P(ξ|Dt) is the same as in [8].",
                "Namely, we estimate P(ξ|Dt) by the relative frequency of feature ξ in Dt linearly smoothed with collection frequency P(ξ|C).",
                "K in Eq.8 is treated as a free parameter.",
                "Note that K is the only free parameter in the computation of WIG for content-based queries because all parameters involved in P(ξ|Dt) are assumed to be fixed by taking the suggested values in [8].",
                "Regarding NP queries, we make use of document structure to estimate P(ξ|Dt) and P(ξ|C) by the so-called mixture of language models proposed in [10] and incorporated into the MRF model for Named-Page finding retrieval in [9].",
                "The basic idea is that a document (collection) is divided into several fields such as the title field, the main-body field and the heading field.",
                "P(ξ|Dt) and P(ξ|C) are estimated by a linear combination of the language models from each field.",
                "Due to space constraints, we refer the reader to [9] for details.",
                "We adopt the exact same set of parameters as used in [9] for estimation.",
                "With regard to K in Eq.8, we set K to 1 because the Named-Page finding task heavily focuses on the first ranked document.",
                "Consequently, there are no free parameters in the computation of WIG for NP queries. 3.2 Query Feedback In this section, we introduce another technique called query feedback (QF) for prediction.",
                "Suppose that a user issues query Q to a retrieval system and a ranked list L of documents is returned.",
                "We view the retrieval system as a noisy channel.",
                "Specifically, we assume that the output of the channel is L and the input is Q.",
                "After going through the channel, Q becomes corrupted and is transformed to ranked list L. By thinking about the retrieval process this way, the problem of predicting retrieval effectiveness turns to the task of evaluating the quality of the channel.",
                "In other words, prediction becomes finding a way to measure the degree of corruption that arises when Q is transformed to L. As directly computing the degree of the corruption is difficult, we tackle this problem by approximation.",
                "Our main idea is that we measure to what extent information on Q can be recovered from L on the assumption that only L is observed.",
                "Specifically, we design a decoder that can accurately translate L back into new query Q and the similarity S between the original query Q and the new query Q is adopted as a performance predictor.",
                "This is a sketch of how the QF technique predicts query performance.",
                "Before filling in more details, we briefly discuss why this method would work.",
                "There is a relation between the similarity S defined above and retrieval performance.",
                "On the one hand, if the retrieval has strayed from the original sense of the query Q, the new query Q extracted from ranked list L in response to Q would be very different from the original query Q.",
                "On the other hand, a query distilled from a ranked list containing many relevant documents is likely to be similar to the original query.",
                "Further examples in support of the relation will be provided later.",
                "Next we detail how to build the decoder and how to measure the similarity S. In essence, the goal of the decoder is to compress ranked list L into a few informative terms that should represent the content of the top ranked documents in L. Our approach to this goal is to represent ranked list L by a language model (distribution over terms).",
                "Then terms are ranked by their contribution to the language models KL (Kullback-Leibler) divergence from the background collection model.",
                "Top ranked terms will be chosen to form the new query Q.",
                "This approach is similar to that used in Section 4.1 of [11].",
                "Specifically, we take three steps to compress ranked list L into query Q without referring to the original query. 1.",
                "We adopt the ranked list language model [14], to estimate a language model based on ranked list L. The model can be written as: )9()|()|()|( ∑∈ = LD LDPDwPLwP where w is any term, D is a document.",
                "P(D|L) is estimated by a linearly decreasing function of the rank of document D. 2.",
                "Each term in P(w|L) is ranked by the following KL-divergence contribution: )10( )|( )|( log)|( CwP LwP LwP where P(w|C) is the collection model estimated by the relative frequency of term w in collection C as a whole. 3.",
                "The top N ranked terms by Eq.10 form a weighted query Q={(wi,ti)} i=1,N. where wi denotes the i-th ranked term and weight ti is the KL-divergence contribution of wi in Eq. 10.",
                "Term cruise ship vessel sea passenger KL contribution 0.050 0.040 0.012 0.010 0.009 Table 1: top 5 terms compressed from the ranked list in response to query Cruise ship damage sea life Two representative examples, one for a poorly performing query Cruise ship damage sea life (TREC topic 719; average precision: 0.08) and the other for a high performing query prostate cancer treatments( TREC topic 710; average precision: 0.49), are shown in Table 1 and 2 respectively.",
                "These examples indicate how the similarity between the original and the new query correlates with retrieval performance.",
                "The parameter N in step 3 is set to 20 empirically and choosing a larger value of N is unnecessary since the weights after the top 20 are usually too small to make any difference.",
                "Term prostate cancer treatment men therapy KL contribution 0.177 0.140 0.028 0.025 0.020 Table 2: top 5 terms compressed from the ranked list in response to query prostate cancer treatments To measure the similarity between original query Q and new query Q, we first use Q to do retrieval on the same collection.",
                "A variant of the query likelihood model [15] is adopted for retrieval.",
                "Namely, documents are ranked by: )11()|()|( ),( ∑∈ = Qtw t i ii i DwPDQP where wi is a term in Q and ti is the associated weight.",
                "D is a document.",
                "Let L denote the new ranked list returned from the above retrieval.",
                "The similarity is measured by the overlap of documents in L and L. Specifically, the percentage of documents in the top K documents of L that are also present in the top K documents in L. the cutoff K is treated as a free parameter.",
                "We summarize here how the QF technique predicts performance given a query Q and the associated ranked list L. We first obtain a weighted query Q compressed from L by the above three steps.",
                "Then we use Q to perform retrieval and the new ranked list is L. The overlap of documents in L and L is used for prediction. 3.3 First Rank Change (FRC) In this section, we propose a method called the first rank change (FRC) for performance prediction for NP queries.",
                "This method is derived from the ranking robustness technique [1] that is mainly designed for content-based queries.",
                "When directly applied to NP queries, the robustness technique will be less effective because it takes the top ranked documents as a whole into account while NP queries usually have only one single relevant document.",
                "Instead, our technique focuses on the first rank document while the main idea of the robustness method remains.",
                "Specifically, the pseudocode for computing FRC is shown in figure 1.",
                "Input: (1) ranked list L={Di} where i=1,100.",
                "Di denotes the i-th ranked document. (2) query Q 1 initialize: (1) set the number of trials J=100000 (2) counter c=0; 2 for i=1 to J 3 Perturb every document in L, let the outcome be a set F={Di} where Di denotes the perturbed version of Di. 4 Do retrieval with query Q on set F 5 c=c+1 if and only if D1 is ranked first in step 4 6 end of for 7 return the ratio c/J Figure 1: pseudo-code for computing FRC FRC approximates the probability that the first ranked document in the original list L will remain ranked first even after the documents are perturbed.",
                "The higher the probability is, the more confidence we have in the first ranked document.",
                "On the other hand, in the extreme case of a random ranking, the probability would be as low as 0.5.",
                "We expect that FRC has a positive association with NP query performance.",
                "We adopt [1] to implement the document perturbation step (step 4 in Fig.1) using Poisson distributions.",
                "For more details, we refer the reader to [1]. 4.",
                "EVALUATION We now present the results of predicting query performance by our models.",
                "Three state-of-the-art techniques are adopted as our baselines.",
                "We evaluate our techniques across a variety of Web retrieval settings.",
                "As mentioned before, we consider two types of queries, that is, content-based (CB) queries and Named-Page(NP) finding queries.",
                "First, suppose that the query types are known.",
                "We investigate the correlation between the predicted retrieval performance and the actual performance for both types of queries separately.",
                "Results show that our methods yield considerable improvements over the baselines.",
                "We then consider a more challenging scenario where no prior information on query types is available.",
                "Two sub-cases are considered.",
                "In the first one, there exists only one type of query but the actual type is unknown.",
                "We assume a mixture of the two query types in the second case.",
                "We demonstrate that our models achieve good accuracy under this demanding scenario, making prediction practical in a real-world Web search environment. 4.1 Experimental Setup Our evaluation focuses on the <br>gov2 collection</br> which contains about 25 million documents crawled from web sites in the .gov domain during 2004 [3].",
                "We create two kinds of data set for CB queries and NP queries respectively.",
                "For the CB type, we use the ad-hoc topics of the Terabyte Tracks of 2004, 2005 and 2006 and name them TB04-adhoc, TB05-adhoc and TB06-adhoc respectively.",
                "In addition, we also use the ad-hoc topics of the 2004 Robust Track (RT04) to test the adaptability of our techniques to a non-Web environment.",
                "For NP queries, we use the Named-Page finding topics of the Terabyte Tracks of 2005 and 2006 and we name them TB05-NP and TB06-NP respectively.",
                "All queries used in our experiments are titles of TREC topics as we center on web retrieval.",
                "Table 3 summarizes the above data sets.",
                "Name Collection Topic Number Query Type TB04-adhoc GOV2 701-750 CB TB05-adhoc GOV2 751-800 CB TB06-adhoc GOV2 801-850 CB RT04 Disk 4+5 (minus CR) 301-450;601700 CB TB05-NP GOV2 NP601-NP872 NP TB06-NP GOV2 NP901-NP1081 NP Table 3: Summary of test collections and topics Retrieval performance of individual content-based and NP queries is measured by the average precision and reciprocal rank of the first correct answer respectively.",
                "We make use of the Markov Random field model for both ad-hoc and Named-Page finding retrieval.",
                "We adopt the same setting of retrieval parameters used in [8,9].",
                "The Indri search engine [12] is used for all of our experiments.",
                "Though not reported here, we also tried the query likelihood model for ad-hoc retrieval and found that the results change little because of the very high correlation between the query performances obtained by the two retrieval models (0.96 measured by Pearsons coefficient). 4.2 Known Query Types Suppose that query types are known.",
                "We treat each type of query separately and measure the correlation with average precision (or the reciprocal rank in the case of NP queries).",
                "We adopt the Pearsons correlation test which reflects the degree of linear relationship between the predicted and the actual retrieval performance. 4.2.1 Content-based Queries Methods Clarity Robust JSD WIG QF WIG +QF TB04+0 5 adhoc 0.333 0.317 0.362 0.574 0.480 0.637 TB06 adhoc 0.076 0.294 N/A 0.464 0.422 0.511 Table 4: Pearsons correlation coefficients for correlation with average precision on the Terabyte Tracks (ad-hoc) for clarity score, robustness score, the JSD-based method(we directly cites the score reported in [2]), WIG, query feedback(QF) and a linear combination of WIG and QF.",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Table 4 shows the correlation with average precision on two data sets: one is a combination of TB04-adhoc and TB05-adhoc(100 topics in total) and the other is TB06-adhoc (50 topics).",
                "The reason that we put TB04-adhoc and TB05-adhoc together is to make our results comparable to [2].",
                "Our baselines are the clarity score (clarity) [6],the robustness score (robust)[1] and the JSDbased method (JSD) [2].",
                "For the clarity and robustness score, we have tried different parameter settings and report the highest correlation coefficients we have found.",
                "We directly cite the result of the JSD-based method reported in [2].",
                "The table also shows the results for the Weighted Information Gain (WIG) method and the Query Feedback (QF) method for predicting content-based queries.",
                "As we described in the previous section, both WIG and QF have one free parameter to set, that is, the cutoff rank K. We train the parameter on one dataset and test on the other.",
                "When combining WIG and QF, a simple linear combination is used and the combination weight is learned from the training data set.",
                "From these results, we can see that our methods are considerably more accurate compared to the baselines.",
                "We also observe that further improvements are obtained from the combination of WIG and QF, suggesting that they measure different properties of the retrieval process that relate to performance.",
                "We discover that our methods generalize well on TB06-adhoc while the correlation for the clarity score with retrieval performance on this data set is considerably worse.",
                "Further investigation shows that the mean average precision of TB06-adhoc is 0.342 and is about 10% better than that of the first data set.",
                "While the other three methods typically consider the top 100 or less documents given a ranked list, the clarity method usually needs the top 500 or more documents to adequately measure the coherence of a ranked list.",
                "Higher mean average precision makes ranked lists retrieved by different queries more similar in terms of coherence at the level of top 500 documents.",
                "We believe that this is the main reason for the low accuracy of the clarity score on the second data set.",
                "Though this paper focuses on a Web search environment, it is desirable that our techniques will work consistently well in other situations.",
                "To this end, we examine the effectiveness of our techniques on the Robust 2004 Track.",
                "For our methods, we evenly divide all of the test queries into five groups and perform five-fold cross validation.",
                "Each time we use one group for training and the remaining four groups for testing.",
                "We make use of all of the queries for our two baselines, that is, the clarity score and the robustness score.",
                "The parameters for our baselines are the same as those used in [1].The results shown in Table 5 demonstrate that the prediction accuracy of our methods is on a par with that of the two strong baselines.",
                "Clarity Robust WIG QF 0.464 0.539 0.468 0.464 Table 5: Comparison of Pearsons correlation coefficients on the 2004 Robust Track for clarity score, robustness score, WIG and query feedback (QF).",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Furthermore, we examine the prediction sensitivity of our methods to the cutoff rank K. With respect to WIG, it is quite robust to K on the Terabyte Tracks (2004-2006) while it prefers a small value of K like 5 on the 2004 Robust Track.",
                "In other words, a small value of K is a nearly-optimal choice for both kinds of tracks.",
                "Considering the fact that all other parameters involved in WIG are fixed and consequently the same for the two cases, this means WIG can achieve nearly-optimal prediction accuracy in two considerably different situations with exactly the same parameter setting.",
                "Regarding QF, it prefers a larger value of K such as 100 on the Terabyte Tracks and a smaller value of K such as 25 on the 2004 Robust Track. 4.2.2 NP Queries We adopt WIG and first rank change (FRC) for predicting NPquery performance.",
                "We also try a linear combination of the two as in the previous section.",
                "The combination weight is obtained from the other data set.",
                "We use the correlation with the reciprocal ranks measured by the Pearsons correlation test to evaluate prediction quality.",
                "The results are presented in Table 6.",
                "Again, our baselines are the clarity score and the robustness score.",
                "To make a fair comparison, we tune the clarity score in different ways.",
                "We found that using the first ranked document to build the query model yields the best prediction accuracy.",
                "We also attempted to utilize document structure by using the mixture of language models mentioned in section 3.1.",
                "Little improvement was obtained.",
                "The correlation coefficients for the clarity score reported in Table 6 are the best we have found.",
                "As we can see, our methods considerably outperform the clarity score technique on both of the runs.",
                "This confirms our intuition that the use of a coherence-based measure like the clarity score is inappropriate for NP queries.",
                "Methods Clarity Robust.",
                "WIG FRC WIG+FRC TB05-NP 0.150 -0.370 0.458 0.440 0.525 TB06-NP 0.112 -0.160 0.478 0.386 0.515 Table 6: Pearsons correlation coefficients for correlation with reciprocal ranks on the Terabyte Tracks (NP) for clarity score, robustness score, WIG, the first rank change (FRC) and a linear combination of WIG and FRC.",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Regarding the robustness score, we also tune the parameters and report the best we have found.",
                "We observe an interesting and surprising negative correlation with reciprocal ranks.",
                "We explain this finding briefly.",
                "A high robustness score means that a number of top ranked documents in the original ranked list are still highly ranked after perturbing the documents.",
                "The existence of such documents is a good sign of high performance for content-based queries as these queries usually contain a number of relevant documents [1].",
                "However, with regard to NP queries, one fundamental difference is that there is only one relevant document for each query.",
                "The existence of such documents can confuse the ranking function and lead to low retrieval performance.",
                "Although the negative correlation with retrieval performance exists, the strength of the correlation is weaker and less consistent compared to our methods as shown in Table 6.",
                "Based on the above analysis, we can see that current prediction techniques like clarity score and robustness score that are mainly designed for content-based queries face significant challenges and are inadequate to deal with NP queries.",
                "Our two techniques proposed for NP queries consistently demonstrate good prediction accuracy, displaying initial success in solving the problem of predicting performance for NP queries.",
                "Another point we want to stress is that the WIG method works well for both types of queries, a desirable property that most prediction techniques lack. 4.3 Unknown Query Types In this section, we run two kinds of experiments without access to query type labels.",
                "First, we assume that only one type of query exists but the type is unknown.",
                "Second, we experiment on a mixture of content-based and NP queries.",
                "The following two subsections will report results for the two conditions respectively. 4.3.1 Only One Type exists We assume that all queries are of the same type, that is, they are either NP queries or content-based queries.",
                "We choose WIG to deal with this case because it shows good prediction accuracy for both types of queries in the previous section.",
                "We consider two cases: (1) CB: all 150 title queries from the ad-hoc task of the Terabyte Tracks 2004-2006 (2)NP: all 433 NP queries from the named page finding task of the Terabyte Tracks 2005 and 2006.",
                "We take a simple strategy by labeling all of the queries in each case as the same type (either NP or CB) regardless of their actual type.",
                "The computation of WIG will be based on the labeled query type instead of the actual type.",
                "There are four possibilities with respect to the relation between the actual type and the labeled type.",
                "The correlation with retrieval performance under the four possibilities is presented in Table 7.",
                "For example, the value 0.445 at the intersection between the second row and the third column shows the Pearsons correlation coefficient for correlation with average precision when the content-based queries are incorrectly labeled as the NP type.",
                "Based on these results, we recommend treating all queries as the NP type when only one query type exists and accurate query classification is not feasible, considering the risk that a large loss of accuracy will occur if NP queries are incorrectly labeled as content-based queries.",
                "These results also demonstrate the strong adaptability of WIG to different query types.",
                "CB (labeled) NP (labeled) CB (actual) 0.536 0.445 NP (actual) 0.174 0.467 Table 7: Comparison of Pearsons correlation coefficients for correlation with retrieval performance under four possibilities on the Terabyte Tracks (NP).",
                "Bold cases mean the results are statistically significant at the 0.01 level. 4.3.2 A mixture of contented-based and NP queries A mixture of the two types of queries is a more realistic situation that a Web search engine will meet.",
                "We evaluate prediction accuracy by how accurately poorly-performing queries can be identified by the prediction method assuming that actual query types are unknown (but we can predict query types).",
                "This is a challenging task because both the predicted and actual performance for one type of query can be incomparable to that for the other type.",
                "Next we discuss how to implement our evaluation.",
                "We create a query pool which consists of all of the 150 ad-hoc title queries from Terabyte Track 2004-2006 and all of the 433 NP queries from Terabyte Track 2005&2006.",
                "We divide the queries in the pool into classes: good (better than 50% of the queries of the same type in terms of retrieval performance) and bad (otherwise).",
                "According to these standards, a NP query with the reciprocal rank above 0.2 or a content-based query with the average precision above 0.315 will be considered as good.",
                "Then, each time we randomly select one query Q from the pool with probability p that Q is contented-based.",
                "The remaining queries are used as training data.",
                "We first decide the type of query Q according to a query classifier.",
                "Namely, the query classifier tells us whether query Q is NP or content-based.",
                "Based on the predicted query type and the score computed for query Q by a prediction technique, a binary decision is made about whether query Q is good or bad by comparing to the score threshold of the predicted query type obtained from the training data.",
                "Prediction accuracy is measured by the accuracy of the binary decision.",
                "In our implementation, we repeatedly take a test query from the query pool and prediction accuracy is computed as the percentage of correct decisions, that is, a good(bad) query is predicted to be good (bad).",
                "It is obvious that random guessing will lead to 50% accuracy.",
                "Let us take the WIG method for example to illustrate the process.",
                "Two WIG thresholds (one for NP queries and the other for content-based queries) are trained by maximizing the prediction accuracy on the training data.",
                "When a test query is labeled as the NP (CB) type by the query type classifier, it will be predicted to be good if and only if the WIG score for this query is above the NP (CB) threshold.",
                "Similar procedures will be taken for other prediction techniques.",
                "Now we briefly introduce the automatic query type classifier used in this paper.",
                "We find that the robustness score, though originally proposed for performance prediction, is a good indicator of query types.",
                "We find that on average content-based queries have a much higher robustness score than NP queries.",
                "For example, Figure 2 shows the distributions of robustness scores for NP and content-based queries.",
                "According to this finding, the robustness score classifier will attach a NP (CB) label to the query if the robustness score for the query is below (above) a threshold trained from the training data. 0 0.5 1 1.5 2 2.5 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 NP Content-based Figure 2: Distribution of robustness scores for NP and CB queries.",
                "The NP queries are the 252 NP topics from the 2005 Terabyte Track.",
                "The content-based queries are the 150 ad-hoc title from the Terabyte Tracks 2004-2006.",
                "The probability distributions are estimated by the Kernel density estimation method.",
                "Strategies Robust WIG-1 WIG-2 WIG-3 Optimal p=0.6 0.565 0.624 0.665 0.684 0.701 P=0.4 0.567 0.633 0.654 0.673 0.696 Table 8: Comparison of prediction accuracy for five strategies in the mixed-query situation.",
                "Two ways to sample a query from the pool: (1) the sampled query is content-based with the probability p=0.6. (that is, the query is NP with probability 0.4 ) (2) set the probability p=0.4.",
                "We consider five strategies in our experiments.",
                "In the first strategy (denoted by robust), we use the robustness score for query performance prediction with the help of a perfect query classifier that always correctly map a query into one of the two categories (that is, NP or CB).",
                "This strategy represents the level of prediction accuracy that current prediction techniques can achieve in an ideal condition that query types are known.",
                "In the next following three strategies, the WIG method is adopted for performance prediction.",
                "The difference among the three is that three different query classifiers are used for each strategy: (1) the classifier always classifies a query into the NP type. (2) the Robustness Score ProbabilityDensity classifier is the robust score classifier mentioned above. (3) the classifier is a perfect one.",
                "These three strategies are denoted by WIG-1, WIG-2 and WIG-3 respectively.",
                "The reason we are interested in WIG-1 is based on the results from section 4.3.1.",
                "In the last strategy (denoted by Optimal) which serves as an upper bound on how well we can do so far, we fully make use of our prediction techniques for each query type assuming a perfect query classifier is available.",
                "Specifically, we linearly combine WIG and QF for content-based queries and WIG and FRC for NP queries.",
                "The results for the five strategies are shown in Table 8.",
                "For each strategy, we try two ways to sample a query from the pool: (1) the sampled query is CB with probability p=0.6. (the query is NP with probability 0.4) (2) set the probability p=0.4.",
                "From Table 8 We can see that in terms of prediction accuracy WIG-2 (the WIG method with the automatic query classifier) is not only better than the first two cases, but also is close to WIG-3 where a perfect classifier is assumed.",
                "Some further improvements over WIG-3 are observed when combined with other prediction techniques.",
                "The merit of WIG-2 is that it provides a practical solution to automatically identifying poorly performing queries in a Web search environment with mixed query types, which poses considerable obstacles to traditional prediction techniques. 5.",
                "CONCLUSIONS AND FUTURE WORK To our knowledge, our paper is the first to thoroughly explore prediction of query performance in web search environments.",
                "We demonstrated that our models resulted in higher prediction accuracy than previously published techniques not specially devised for web search scenarios.",
                "In this paper, we focus on two types of queries in web search: content-based and Named-Page (NP) finding queries, corresponding to the ad-hoc retrieval task and the Named-Page finding task respectively.",
                "For both types of web queries, our prediction models were shown to be substantially more accurate than the current state-of-the-art techniques.",
                "Furthermore, we considered a more realistic case that no prior information on query types is available.",
                "We demonstrated that the WIG method is particularly suitable for this situation.",
                "Considering the adaptability of WIG to a range of collections and query types, one of our future plans is to apply this method to predict user preference of search results on realistic data collected from a commercial search engine.",
                "Other than accuracy, another major issue that prediction techniques have to deal with in a Web environment is efficiency.",
                "Fortunately, since the WIG score is computed just over the terms and the phrases that appear in the query, this calculation can be made very efficient with the support of index.",
                "On the other hand, the computation of QF and FRC is relatively less efficient since QF needs to retrieve the whole collection twice and FRC needs to repeatedly rank the perturbed documents.",
                "How to improve the efficiency of QF and FRC is our future work.",
                "In addition, the prediction techniques proposed in this paper have the potential of improving retrieval performance by combining with other IR techniques.",
                "For example, our techniques can be incorporated to popular query modification techniques such as query expansion and query relaxation.",
                "Guided by performance prediction, we can make a better decision on when to or how to modify queries to enhance retrieval effectiveness.",
                "We would like to carry out research in this direction in the future. 6.",
                "ACKNOWLEGMENTS This work was supported in part by the Center for Intelligent Information Retrieval, in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C0023, and in part by an award from Google.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor.",
                "In addition, we thank Donald Metzler for his valuable comments on this work. 7.",
                "REFERENCES [1] Y. Zhou ,W. B. Croft ,Ranking Robustness: A Novel Framework to Predict Query Performance, in Proceedings of CIKM 2006. [2] D.Carmel, E.Yom-Tov, A.Darlow,D.Pelleg, What Makes a Query Difficult?, in Proceedings of SIGIR 2006. [3] C.L.A.",
                "Clarke, F. Scholer, I.Soboroff, The TREC 2005 Terabyte Track, In the Online Proceedings of 2005 TREC. [4] B.",
                "He and I.Ounis.",
                "Inferring query performance using preretrieval predictors.",
                "In proceedings of the SPIRE 2004. [5] S. Tomlinson.",
                "Robust, Web and Terabyte Retrieval with Hummingbird SearchServer at TREC 2004.",
                "In the Online Proceedings of 2004 TREC. [6] S. Cronen-Townsend, Y. Zhou, W. B. Croft, Predicting Query Performance, in Proceedings of SIGIR 2002. [7] V.Vinay, I.J.Cox, N.Mill-Frayling,K.Wood, On Ranking the Effectiveness of Searcher, in Proceedings of SIGIR 2006. [8] D.Metzler, W.B.Croft, A Markov Random Filed Model for Term Dependencies, in Proceedings of SIGIR 2005. [9] D.Metzler, T.Strohman,Y.Zhou,W.B.Croft, Indri at TREC 2005: Terabyte Track, In the Online Proceedings of 2004 TREC. [10] P. Ogilvie and J. Callan, Combining document representations for known-item search, in Proceedings of SIGIR 2003. [11] A.Berger, J.Lafferty, Information retrieval as statistical translation, in Proceedings of SIGIR 1999. [12] Indri search engine : http://www.lemurproject.org/indri/ [13] I.J.",
                "Taneja: On Generalized Information Measures and Their Applications, Advances in Electronics and Electron Physics, Academic Press (USA), 76, 1989, 327-413. [14] S.Cronen-Townsend, Y. Zhou and Croft, W. B. , A Framework for Selective Query Expansion, in Proceedings of CIKM 2004. [15] F.Song, W.B.Croft, A general language model for information retrieval, in Proceedings of SIGIR 1999. [16] Personal email contact with Vishwa Vinay and our own experiments [17] E.Yom-Tov, S.Fine, D.Carmel, A.Darlow, Learning to Estimate Query Difficulty Including Applications to Missing Content Detection and Distributed Information retrieval, in Proceedings of SIGIR 2005"
            ],
            "original_annotated_samples": [
                "Our evaluation is mainly performed on the <br>gov2 collection</br>.",
                "For example, the reported prediction accuracy of the ranking robustness technique and the clarity technique on the <br>gov2 collection</br> (a large web collection) is significantly worse compared to the other TREC collections [1].",
                "Similar prediction accuracy on the <br>gov2 collection</br> using another technique is reported in [2], confirming the difficult of predicting query performance on a large web collection.",
                "We demonstrate that our models achieve good accuracy under this demanding scenario, making prediction practical in a real-world Web search environment. 4.1 Experimental Setup Our evaluation focuses on the <br>gov2 collection</br> which contains about 25 million documents crawled from web sites in the .gov domain during 2004 [3]."
            ],
            "translated_annotated_samples": [
                "Nuestra evaluación se realiza principalmente en la <br>colección GOV2</br>.",
                "Por ejemplo, la precisión de predicción reportada de la técnica de robustez de clasificación y la técnica de claridad en la <br>colección GOV2</br> (una colección web grande) es significativamente peor en comparación con las otras colecciones de TREC [1].",
                "En [2] se informa de una precisión de predicción similar en la <br>colección GOV2</br> utilizando otra técnica, confirmando la dificultad de predecir el rendimiento de consultas en una gran colección web.",
                "Demostramos que nuestros modelos logran una buena precisión en este escenario exigente, lo que hace que la predicción sea práctica en un entorno de búsqueda web del mundo real. 4.1 Configuración Experimental Nuestra evaluación se centra en la <br>colección GOV2</br> que contiene alrededor de 25 millones de documentos obtenidos de sitios web en el dominio .gov durante 2004 [3]."
            ],
            "translated_text": "Predicción del rendimiento de consultas en entornos de búsqueda web Yun Zhou y W. Bruce Croft Departamento de Ciencias de la Computación Universidad de Massachusetts, Amherst {yzhou, croft}@cs.umass.edu RESUMEN Las técnicas actuales de predicción, generalmente diseñadas para consultas basadas en contenido y evaluadas típicamente en colecciones de prueba relativamente homogéneas de tamaños pequeños, enfrentan serios desafíos en entornos de búsqueda web donde las colecciones son significativamente más heterogéneas y existen diferentes tipos de tareas de recuperación. En este artículo, presentamos tres técnicas para abordar estos desafíos. Nos enfocamos en la predicción de rendimiento para dos tipos de consultas en entornos de búsqueda web: basadas en contenido y búsqueda de páginas por nombre. Nuestra evaluación se realiza principalmente en la <br>colección GOV2</br>. Además de evaluar nuestros modelos para los dos tipos de consultas por separado, consideramos una situación más desafiante y realista en la que los dos tipos de consultas se mezclan sin información previa sobre los tipos de consulta. Para ayudar en la predicción bajo la situación de consulta mixta, se adopta un clasificador de consultas novedoso. Los resultados muestran que nuestra predicción del rendimiento de la consulta web es sustancialmente más precisa que las técnicas de predicción actuales más avanzadas. Por consiguiente, nuestro artículo proporciona un enfoque práctico para predecir el rendimiento en entornos web del mundo real. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Formulación de Consultas Términos Generales Algoritmos, Experimentación, Teoría 1. La predicción del rendimiento de la consulta tiene muchas aplicaciones en una variedad de áreas de recuperación de información (IR), como mejorar la consistencia de la recuperación, el refinamiento de la consulta y la IR distribuida. La importancia de este problema ha sido reconocida por los investigadores en Recuperación de Información y recientemente se han propuesto varios métodos nuevos para la predicción [1, 2, 17]. La mayoría de los trabajos sobre predicción se han centrado en la tarea tradicional de recuperación ad-hoc, donde el rendimiento de la consulta se mide según la relevancia temática. Estos modelos de predicción se evalúan en colecciones de documentos de TREC que suelen consistir en no más de un millón de artículos de agencias de noticias relativamente homogéneos. Con la popularidad e influencia de la Web, las técnicas de predicción que funcionen bien para consultas de estilo web son altamente preferibles. Sin embargo, los entornos de búsqueda web plantean desafíos significativos para los modelos de predicción actuales que están principalmente diseñados para entornos tradicionales de TREC. Aquí delineamos algunos de estos desafíos. Primero, las colecciones web, que son mucho más grandes que las colecciones TREC convencionales, incluyen una variedad de documentos que son diferentes en muchos aspectos, como la calidad y el estilo. Las técnicas actuales de predicción pueden ser vulnerables a estas características de las colecciones web. Por ejemplo, la precisión de predicción reportada de la técnica de robustez de clasificación y la técnica de claridad en la <br>colección GOV2</br> (una colección web grande) es significativamente peor en comparación con las otras colecciones de TREC [1]. En [2] se informa de una precisión de predicción similar en la <br>colección GOV2</br> utilizando otra técnica, confirmando la dificultad de predecir el rendimiento de consultas en una gran colección web. Además, la búsqueda web va más allá del alcance de la tarea de recuperación ad hoc basada en la relevancia temática. Por ejemplo, la tarea de encontrar páginas nombradas (NP), que es una tarea de navegación, también es popular en la recuperación web. La predicción del rendimiento de la consulta para la tarea de NP sigue siendo necesaria, ya que el rendimiento de recuperación de NP está lejos de ser perfecto. De hecho, según el informe sobre la tarea NP del Terabyte Track de 2005 [3], aproximadamente el 40% de las consultas de prueba tienen un rendimiento deficiente (sin respuesta correcta en los primeros 10 resultados de búsqueda) incluso en la mejor ejecución del grupo líder. Hasta donde sabemos, poca investigación ha abordado explícitamente el problema de la predicción del rendimiento de consultas de NP. Los modelos de predicción actuales diseñados para consultas basadas en contenido serán menos efectivos para consultas de NP considerando las diferencias fundamentales entre ambas. En tercer lugar, en entornos de búsqueda web del mundo real, las consultas de los usuarios suelen ser una mezcla de diferentes tipos y, por lo general, no se dispone de información previa sobre el tipo de cada consulta. La situación de consulta mixta plantea nuevos problemas para la predicción del rendimiento de la consulta. Por ejemplo, es posible que necesitemos incorporar un clasificador de consultas en los modelos de predicción. A pesar de estos problemas, la capacidad para manejar esta situación es un paso crucial hacia convertir la predicción del rendimiento de consultas de un tema de investigación interesante en una herramienta práctica para la recuperación web. En este artículo, presentamos tres técnicas para abordar los desafíos mencionados que enfrentan los modelos de predicción actuales en entornos de búsqueda web. Nuestro trabajo se centra en la predicción del rendimiento de consultas para la tarea de recuperación basada en contenido (ad-hoc) y la tarea de encontrar páginas por nombre en el contexto de la recuperación web. Nuestra primera técnica, llamada ganancia de información ponderada (WIG), utiliza tanto características de términos individuales como de proximidad de términos para estimar la calidad de los documentos recuperados para la predicción. Observamos que WIG ofrece una precisión de predicción consistente en diversas colecciones de pruebas y tipos de consultas. Además, demostramos que se puede lograr una buena precisión de predicción para la situación de consulta mixta utilizando WIG con la ayuda de un clasificador de tipo de consulta. La retroalimentación de consultas y el cambio de rango inicial, que son nuestras técnicas de predicción segunda y tercera, funcionan bien para consultas basadas en contenido y consultas de NP respectivamente. Nuestras principales contribuciones incluyen: (1) una precisión de predicción considerablemente mejorada para consultas basadas en contenido web en comparación con varias técnicas de vanguardia. (2) nuevas técnicas para predecir con éxito el rendimiento de consultas NP. (3) una solución práctica y totalmente automática para predecir el rendimiento de consultas mixtas. Además, una contribución menor es que encontramos que el puntaje de robustez [1], que originalmente se propuso para la predicción de rendimiento, es útil para la clasificación de consultas. 2. TRABAJO RELACIONADO Como mencionamos en la introducción, recientemente se han propuesto varias técnicas de predicción que se centran en consultas basadas en contenido en la tarea de relevancia temática (ad-hoc). No conocemos ningún trabajo publicado que aborde otros tipos de consultas como las consultas NP, y mucho menos una mezcla de tipos de consultas. A continuación revisamos algunos modelos representativos. La principal dificultad de la predicción del rendimiento proviene del hecho de que muchos factores tienen un impacto en el rendimiento de recuperación. Cada factor afecta el rendimiento en diferente medida y el efecto general es difícil de predecir con precisión. Por lo tanto, no es sorprendente notar que características simples, como la frecuencia de los términos de consulta en la colección [4] y la IDF promedio de los términos de consulta [5], no predicen bien. De hecho, la mayoría de las técnicas exitosas se basan en medir algunas características del conjunto de documentos recuperados para estimar la dificultad del tema. Por ejemplo, la puntuación de claridad [6] mide la coherencia de una lista de documentos mediante la divergencia KL entre el modelo de consulta y el modelo de colección. El puntaje de robustez [1] cuantifica otra propiedad de una lista clasificada: la robustez de la clasificación en presencia de incertidumbre. Carmel et al. [2] encontraron que la distancia medida por la divergencia de Jensen-Shannon entre el conjunto de documentos recuperados y la colección está significativamente correlacionada con la precisión promedio. Vinay et al. [7] propusieron cuatro medidas para capturar la geometría de los documentos mejor clasificados para la predicción. La medida más efectiva es la sensibilidad a la perturbación del documento, una idea algo similar a la puntuación de robustez. Desafortunadamente, su forma de medir la sensibilidad no funciona igual de bien para consultas cortas y la precisión de la predicción disminuye considerablemente cuando se adopta una técnica de recuperación de vanguardia (como Okapi o un enfoque de modelado de lenguaje) en lugar del peso tf-idf utilizado en su artículo [16]. Las dificultades de aplicar estos modelos en entornos de búsqueda web ya han sido mencionadas. En este documento, principalmente adoptamos la puntuación de claridad y la puntuación de robustez como nuestras líneas base. Experimentalmente demostramos que las líneas base, incluso después de ser ajustadas cuidadosamente, son inadecuadas para el entorno web. Uno de nuestros modelos de predicción, WIG, está relacionado con el modelo de campo aleatorio de Markov (MRF) para la recuperación de información [8]. El modelo MRF modela directamente la dependencia de términos y se ha demostrado ser altamente efectivo en una variedad de colecciones de pruebas (especialmente colecciones web) y tareas de recuperación. Este modelo se utiliza para estimar la distribución conjunta de probabilidad sobre documentos y consultas, una parte importante de WIG. La superioridad de WIG sobre otras técnicas de predicción basadas en características unigramas, que se demostrará más adelante en nuestro artículo, coincide con la de MRF para recuperación. En otras palabras, es interesante notar que la dependencia de términos, cuando se modela adecuadamente, puede ser útil tanto para mejorar como para predecir el rendimiento de recuperación. 3. Modelos de predicción 3.1 Ganancia de Información Ponderada (WIG) Esta sección introduce un enfoque de ganancia de información ponderada que incorpora tanto características de términos únicos como de proximidad para predecir el rendimiento tanto en consultas basadas en contenido como en consultas de búsqueda de Páginas Nombradas (NP). Dado un conjunto de consultas Q={Qs} (s=1,2,..N) que incluye todas las posibles consultas de usuario y un conjunto de documentos D={Dt} (t=1,2…M), asumimos que cada par consulta-documento (Qs,Dt) es evaluado manualmente y se incluirá en una lista de relevancia si se determina que Qs es relevante para Dt. La probabilidad conjunta P(Qs,Dt) sobre las consultas Q y los documentos D denota la probabilidad de que el par (Qs,Dt) esté en la lista de relevancia. Tales suposiciones son similares a las utilizadas en [8]. Suponiendo que el usuario emite la consulta Qi ∈Q y los resultados de recuperación en respuesta a Qi es una lista clasificada L de documentos, calculamos la cantidad de información contenida en P(Qs,Dt) con respecto a Qi y L mediante la Ec.1, que es una variante de la entropía llamada entropía ponderada[13]. Los pesos en la ecuación 1 están determinados únicamente por Qi y L. En este documento, elegimos los pesos de la siguiente manera: L es el número de documentos que contiene la consulta, K es el límite superior, LT es el límite superior de documentos, y D es el peso si K está en otro caso. La clasificación de corte K es un parámetro en nuestro modelo que se discutirá más adelante. Por lo tanto, la ecuación 1 se puede simplificar de la siguiente manera: )3(),(log 1 ),( )( , ∑∈ −= LTD titsLQ Kt i DQP K DQH Desafortunadamente, la entropía ponderada ),(, tsLQ DQH i calculada por la ecuación 3, que representa la cantidad de información sobre la probabilidad de que los documentos mejor clasificados en L sean relevantes para la consulta Qi en promedio, no se puede comparar entre diferentes consultas, lo que la hace inapropiada para predecir directamente el rendimiento de la consulta. Para mitigar este problema, creamos una distribución de fondo P(Qs,C) sobre Q y D imaginando que cada documento en D es reemplazado por el mismo documento especial C que representa el uso promedio del lenguaje. En este documento, C se crea concatenando cada documento en D. A grosso modo, C es la colección (el conjunto de documentos) {Dt} sin límites de documentos. De manera similar, la entropía ponderada ),(, CQH sLQi calculada por la Ecuación 3 representa la cantidad de información sobre la probabilidad de que un documento promedio (representado por toda la colección) sea relevante para la consulta Qi. Ahora presentamos nuestro predictor de rendimiento WIG que es la ganancia de información ponderada [13] calculada como la diferencia entre ),(, tsLQ DQH i y ),(, CQH sLQi. Específicamente, dado el conjunto de consultas Qi, la colección C y la lista clasificada L de documentos, WIG se calcula de la siguiente manera: )4( ),( ),( log 1 ),( ),( log),( ),(),(),,( )(, ,, ∑∑ ∈ == −= LTD i ti ts s ts ts tsLQsLQi Kt ii CQP DQP KCQP DQP DQweight DQHCQHLCQWIG WIG calculado por la Ecuación 4 mide el cambio en la información sobre la calidad de la recuperación (en respuesta a la consulta Qi) desde un estado imaginario en el que solo se recupera un documento promedio hasta un estado posterior en el que se observan los resultados de búsqueda reales. Hacemos la hipótesis de que WIG está positivamente correlacionado con la efectividad de la recuperación porque una recuperación de alta calidad debería ser mucho más efectiva que simplemente devolver el documento promedio. El corazón de esta técnica es cómo estimar la distribución conjunta P(Qs, Dt). En el enfoque de modelado del lenguaje para la recuperación de información, se pueden aplicar fácilmente una variedad de modelos para estimar esta distribución. Aunque la mayoría de estos modelos se basan en la suposición de la bolsa de palabras, trabajos recientes sobre modelado de dependencia de términos bajo el marco de modelado de lenguaje han demostrado mejoras consistentes y significativas en la efectividad de recuperación sobre los modelos de bolsa de palabras. Inspirados por el éxito de incorporar características de proximidad de términos en modelos de lenguaje, decidimos adoptar un buen modelo de dependencia para estimar la probabilidad P(Qs,Dt). El modelo que elegimos para este artículo es el modelo de Campo Aleatorio de Markov (MRF) de Metzler y Crofts, que ya ha demostrado su superioridad en comparación con varias colecciones y diferentes tareas de recuperación [8,9]. Según el modelo MRF, log P(Qi, Dt) se puede escribir como )5()|(loglog),(log )( 1 ∑∈ +−= iQF tti DPZDQP ξ ξ ξλ donde Z1 es una constante que asegura que P(Qi, Dt) sume 1. F(Qi) consiste en un conjunto de características ampliadas a partir de la consulta original Qi. Por ejemplo, suponiendo que la consulta Qi es el programa de estudiantes talentosos, F(Qi) incluye características como programa y estudiante talentoso. Consideramos dos tipos de características: características de términos individuales T y características de proximidad P. Las características de proximidad incluyen la frase exacta (#1) y las características de ventana desordenada (#uwN) como se describe en [8]. Ten en cuenta que F(Qi) es la unión de T(Qi) y P(Qi). Para obtener más detalles sobre F(Qi), como por ejemplo cómo expandir la consulta original Qi a F(Qi), remitimos al lector a [8] y [9]. P(ξ|Dt) denota la probabilidad de que la característica ξ ocurra en Dt. Más detalles sobre P(ξ|Dt) se proporcionarán más adelante en esta sección. La elección de λξ es algo diferente a la utilizada en [8] ya que λξ desempeña un doble papel en nuestro modelo. El primer rol, que es el mismo que en [8], es ponderar entre las características de términos individuales y de proximidad. El otro rol, específico de nuestra tarea de predicción, es normalizar el tamaño de F(Qi). Encontramos que la siguiente estrategia de peso para λξ satisface los dos roles anteriores y generaliza bien en una variedad de colecciones y tipos de consultas. )6( )(, |)(| 1 )(, |)(| ⎪ ⎪ ⎩ ⎪ ⎪ ⎨ ⎧ ∈ − ∈ = i i T i i T QP QP QT QT ξ λ ξ λ λξ donde |T(Qi)| y |P(Qi)| denotan el número de características de términos únicos y de proximidad en F(Qi) respectivamente. La razón de elegir la función raíz cuadrada en el denominador de λξ es penalizar adecuadamente un conjunto de características de gran tamaño, haciendo que WIG sea más comparable entre consultas de diversas longitudes. λT es un parámetro fijo y se establece en 0.8 según [8] a lo largo de este documento. De manera similar, log P(Qi,C) se puede escribir como: )7()|(loglog),(log )( 2 ∑∈ +−= iQF i CPZCQP ξ ξ ξλ Cuando se eliminan las constantes Z1 y Z2, el WIG calculado en la Ecuación 4 se puede reescribir de la siguiente manera al sustituir la Ecuación 5 y la Ecuación 7: )8( )|( )|( log 1 ),,( )( )( ∑ ∑∈ ∈ = LTD QF t i Kt i CP DP K LCQWIG ξ ξ ξ ξ λ Una de las ventajas del WIG sobre otras técnicas es que puede manejar tanto consultas basadas en contenido como en NP. Basándose en el tipo (o el tipo predicho) de Qi, el cálculo de WIG en la Ecuación 8 difiere en dos aspectos: (1) cómo estimar P(ξ|Dt) y P(ξ|C), y (2) cómo elegir K. Para consultas basadas en contenido, P(ξ|C) se estima por la frecuencia relativa del rasgo ξ en la colección C en su totalidad. La estimación de P(ξ|Dt) es la misma que en [8]. Es decir, estimamos P(ξ|Dt) mediante la frecuencia relativa del rasgo ξ en Dt suavizada linealmente con la frecuencia de colección P(ξ|C). La K en la Ec.8 se trata como un parámetro libre. Ten en cuenta que K es el único parámetro libre en el cálculo de WIG para consultas basadas en contenido, ya que se asume que todos los parámetros involucrados en P(ξ|Dt) están fijos al tomar los valores sugeridos en [8]. En cuanto a las consultas de NP, hacemos uso de la estructura del documento para estimar P(ξ|Dt) y P(ξ|C) mediante el llamado modelo de mezcla de lenguaje propuesto en [10] e incorporado en el modelo MRF para la recuperación de Named-Page en [9]. La idea básica es que un documento (colección) se divide en varios campos como el campo del título, el campo del cuerpo principal y el campo de los encabezados. P(ξ|Dt) y P(ξ|C) se estiman mediante una combinación lineal de los modelos de lenguaje de cada campo. Debido a limitaciones de espacio, remitimos al lector a [9] para más detalles. Adoptamos el mismo conjunto exacto de parámetros utilizados en [9] para la estimación. En cuanto a K en la Ec.8, establecemos K en 1 porque la tarea de encontrar Named-Page se centra fuertemente en el documento clasificado en primer lugar. Por consiguiente, no hay parámetros libres en el cálculo de WIG para consultas de NP. 3.2 Retroalimentación de consultas En esta sección, presentamos otra técnica llamada retroalimentación de consultas (QF) para la predicción. Suponga que un usuario emite la consulta Q a un sistema de recuperación y se devuelve una lista clasificada L de documentos. Vemos el sistema de recuperación como un canal ruidoso. Específicamente, asumimos que la salida del canal es L y la entrada es Q. Después de pasar por el canal, Q se corrompe y se transforma en la lista clasificada L. Al pensar en el proceso de recuperación de esta manera, el problema de predecir la eficacia de la recuperación se convierte en la tarea de evaluar la calidad del canal. En otras palabras, la predicción se convierte en encontrar una forma de medir el grado de corrupción que surge cuando Q se transforma en L. Dado que calcular directamente el grado de corrupción es difícil, abordamos este problema mediante aproximaciones. Nuestra idea principal es que medimos en qué medida la información sobre Q puede ser recuperada de L bajo la suposición de que solo se observa L. Específicamente, diseñamos un decodificador que pueda traducir con precisión L de vuelta a la nueva consulta Q y la similitud S entre la consulta original Q y la nueva consulta Q se adopta como un predictor de rendimiento. Este es un bosquejo de cómo la técnica QF predice el rendimiento de la consulta. Antes de completar más detalles, discutimos brevemente por qué este método funcionaría. Existe una relación entre la similitud S definida anteriormente y el rendimiento de recuperación. Por un lado, si la recuperación se ha desviado del sentido original de la consulta Q, la nueva consulta Q extraída de la lista clasificada L en respuesta a Q sería muy diferente de la consulta original Q. Por otro lado, una consulta destilada de una lista clasificada que contiene muchos documentos relevantes es probable que sea similar a la consulta original. Se proporcionarán más ejemplos en apoyo de la relación más adelante. A continuación detallamos cómo construir el decodificador y cómo medir la similitud S. En esencia, el objetivo del decodificador es comprimir la lista clasificada L en unos pocos términos informativos que deberían representar el contenido de los documentos mejor clasificados en L. Nuestro enfoque para este objetivo es representar la lista clasificada L mediante un modelo de lenguaje (distribución de términos). Luego, los términos se clasifican según su contribución a la divergencia KL (Kullback-Leibler) de los modelos de lenguaje con respecto al modelo de la colección de fondo. Los términos mejor clasificados serán elegidos para formar la nueva consulta Q. Este enfoque es similar al utilizado en la Sección 4.1 de [11]. Específicamente, tomamos tres pasos para comprimir la lista clasificada L en la consulta Q sin hacer referencia a la consulta original. 1. Adoptamos el modelo de lenguaje de lista clasificada [14] para estimar un modelo de lenguaje basado en la lista clasificada L. El modelo puede escribirse como: )9()|()|()|( ∑∈ = LD LDPDwPLwP donde w es cualquier término y D es un documento. La probabilidad de que ocurra el evento D dado el evento L se estima mediante una función linealmente decreciente del rango del documento D. Cada término en P(w|L) está clasificado por la siguiente contribución de divergencia de Kullback-Leibler: )10( )|( )|( log)|( CwP LwP LwP donde P(w|C) es el modelo de colección estimado por la frecuencia relativa del término w en la colección C en su totalidad. 3. Los términos N principales clasificados por Eq.10 forman una consulta ponderada Q={(wi,ti)} i=1,N, donde wi denota el término clasificado en la posición i y el peso ti es la contribución de la divergencia KL de wi en Eq. 10. Barco de crucero daño vida marina. Estos ejemplos indican cómo la similitud entre la consulta original y la nueva se correlaciona con el rendimiento de recuperación. El parámetro N en el paso 3 se establece en 20 de manera empírica y elegir un valor más grande de N es innecesario ya que los pesos después de los primeros 20 suelen ser demasiado pequeños para marcar alguna diferencia. Para medir la similitud entre la consulta original Q y la nueva consulta Q, primero utilizamos Q para recuperar información de la misma colección. Se adopta una variante del modelo de probabilidad de consulta [15] para la recuperación. Es decir, los documentos se clasifican por: )11()|()|( ),( ∑∈ = Qtw t i ii i DwPDQP donde wi es un término en Q y ti es el peso asociado. D es un documento. Que L denote la nueva lista clasificada devuelta desde la recuperación anterior. La similitud se mide por la superposición de documentos en L y L. Específicamente, el porcentaje de documentos en los primeros K documentos de L que también están presentes en los primeros K documentos en L. El valor de corte K se trata como un parámetro libre. Aquí resumimos cómo la técnica QF predice el rendimiento dado una consulta Q y la lista clasificada asociada L. Primero obtenemos una consulta ponderada Q comprimida a partir de L mediante los tres pasos anteriores. Luego usamos Q para realizar la recuperación y la nueva lista clasificada es L. La superposición de documentos en L y L se utiliza para la predicción. 3.3 Primer Cambio de Rango (FRC) En esta sección, proponemos un método llamado primer cambio de rango (FRC) para la predicción de rendimiento para consultas de NP. Este método se deriva de la técnica de robustez de clasificación [1] que está principalmente diseñada para consultas basadas en contenido. Cuando se aplica directamente a consultas de NP, la técnica de robustez será menos efectiva porque tiene en cuenta todos los documentos mejor clasificados en su totalidad, mientras que las consultas de NP suelen tener solo un único documento relevante. En cambio, nuestra técnica se centra en el documento de rango uno mientras se mantiene la idea principal del método de robustez. Específicamente, el seudocódigo para calcular FRC se muestra en la figura 1. Lista clasificada L={Di} donde i=1,100. Di denota el documento clasificado en la posición i. (2) consulta Q 1 inicializar: (1) establecer el número de intentos J=100000 (2) contador c=0; 2 para i=1 a J 3 Perturbar cada documento en L, que el resultado sea un conjunto F={Di} donde Di denota la versión perturbada de Di. 4 Realizar la recuperación con la consulta Q en el conjunto F 5 c=c+1 si y solo si D1 está clasificado primero en el paso 4 6 fin del para 7 devolver la proporción c/J Figura 1: pseudo-código para calcular FRC FRC aproxima la probabilidad de que el documento clasificado en primer lugar en la lista original L permanezca clasificado primero incluso después de que los documentos sean perturbados. Cuanto mayor sea la probabilidad, más confianza tenemos en el documento clasificado en primer lugar. Por otro lado, en el caso extremo de un ranking aleatorio, la probabilidad sería tan baja como 0.5. Esperamos que FRC tenga una asociación positiva con el rendimiento de la consulta de NP. Adoptamos [1] para implementar el paso de perturbación del documento (paso 4 en la Fig. 1) utilizando distribuciones de Poisson. Para más detalles, remitimos al lector a [1]. 4. EVALUACIÓN Ahora presentamos los resultados de predecir el rendimiento de la consulta por nuestros modelos. Tres técnicas de vanguardia son adoptadas como nuestras líneas base. Evaluamos nuestras técnicas en una variedad de configuraciones de recuperación web. Como se mencionó anteriormente, consideramos dos tipos de consultas, es decir, consultas basadas en contenido (CB) y consultas de búsqueda de páginas con nombre (NP). Primero, supongamos que se conocen los tipos de consultas. Investigamos la correlación entre el rendimiento de recuperación predicho y el rendimiento real para ambos tipos de consultas por separado. Los resultados muestran que nuestros métodos producen mejoras considerablemente superiores a los valores de referencia. Luego consideramos un escenario más desafiante donde no hay información previa sobre los tipos de consultas disponibles. Se consideran dos subcasos. En el primero, existe solo un tipo de consulta pero el tipo real es desconocido. Suponemos una mezcla de los dos tipos de consultas en el segundo caso. Demostramos que nuestros modelos logran una buena precisión en este escenario exigente, lo que hace que la predicción sea práctica en un entorno de búsqueda web del mundo real. 4.1 Configuración Experimental Nuestra evaluación se centra en la <br>colección GOV2</br> que contiene alrededor de 25 millones de documentos obtenidos de sitios web en el dominio .gov durante 2004 [3]. Creamos dos tipos de conjuntos de datos para consultas de CB y consultas de NP respectivamente. Para el tipo CB, utilizamos los temas ad-hoc de las pistas Terabyte de 2004, 2005 y 2006 y los nombramos TB04-adhoc, TB05-adhoc y TB06-adhoc respectivamente. Además, también utilizamos los temas ad-hoc de la pista Robusta de 2004 (RT04) para probar la adaptabilidad de nuestras técnicas a un entorno no web. Para consultas de NP, utilizamos los temas de búsqueda de páginas nombradas de las Terabyte Tracks de 2005 y 2006 y los denominamos TB05-NP y TB06-NP respectivamente. Todas las consultas utilizadas en nuestros experimentos son títulos de temas de TREC, ya que nos centramos en la recuperación web. La Tabla 3 resume los conjuntos de datos anteriores. La recuperación del rendimiento de las consultas individuales basadas en contenido y NP se mide mediante la precisión promedio y la tasa recíproca de la primera respuesta correcta, respectivamente. Hacemos uso del modelo de campo aleatorio de Markov tanto para la recuperación ad-hoc como para la recuperación de páginas nombradas. Adoptamos la misma configuración de parámetros de recuperación utilizada en [8,9]. El motor de búsqueda Indri [12] se utiliza para todos nuestros experimentos. Aunque no se informa aquí, también probamos el modelo de probabilidad de consulta para la recuperación ad-hoc y encontramos que los resultados cambian poco debido a la correlación muy alta entre el rendimiento de las consultas obtenido por los dos modelos de recuperación (0.96 medido por el coeficiente de Pearson). 4.2 Tipos de Consultas Conocidos Supongamos que se conocen los tipos de consultas. Tratamos cada tipo de consulta por separado y medimos la correlación con la precisión promedio (o el rango recíproco en el caso de consultas NP). Adoptamos la prueba de correlación de Pearson que refleja el grado de relación lineal entre el rendimiento de recuperación predicho y real. 4.2.1 Métodos de Consultas Basadas en Contenido Claridad Robusta JSD WIG QF WIG +QF TB04+0 5 adhoc 0.333 0.317 0.362 0.574 0.480 0.637 TB06 adhoc 0.076 0.294 N/A 0.464 0.422 0.511 Tabla 4: Coeficientes de correlación de Pearson para la correlación con la precisión promedio en las Pistas de Terabyte (ad-hoc) para la puntuación de claridad, puntuación de robustez, el método basado en JSD (citamos directamente la puntuación informada en [2]), WIG, retroalimentación de consulta (QF) y una combinación lineal de WIG y QF. Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. La Tabla 4 muestra la correlación con la precisión promedio en dos conjuntos de datos: uno es una combinación de TB04-adhoc y TB05-adhoc (100 temas en total) y el otro es TB06-adhoc (50 temas). La razón por la que juntamos TB04-adhoc y TB05-adhoc es para hacer nuestros resultados comparables a [2]. Nuestros puntos de referencia son la puntuación de claridad (claridad) [6], la puntuación de robustez (robust) [1] y el método basado en JSD (JSD) [2]. Para la puntuación de claridad y robustez, hemos probado diferentes configuraciones de parámetros y reportamos los coeficientes de correlación más altos que hemos encontrado. Citamos directamente el resultado del método basado en JSD reportado en [2]. La tabla también muestra los resultados para el método de Ganancia de Información Ponderada (WIG) y el método de Retroalimentación de Consulta (QF) para predecir consultas basadas en contenido. Como describimos en la sección anterior, tanto WIG como QF tienen un parámetro libre para ajustar, es decir, el rango de corte K. Entrenamos el parámetro en un conjunto de datos y lo probamos en el otro. Al combinar WIG y QF, se utiliza una combinación lineal simple y el peso de la combinación se aprende a partir del conjunto de datos de entrenamiento. A partir de estos resultados, podemos ver que nuestros métodos son considerablemente más precisos en comparación con los baselines. También observamos que se obtienen mejoras adicionales a partir de la combinación de WIG y QF, lo que sugiere que miden diferentes propiedades del proceso de recuperación que se relacionan con el rendimiento. Descubrimos que nuestros métodos se generalizan bien en TB06-adhoc, mientras que la correlación del puntaje de claridad con el rendimiento de recuperación en este conjunto de datos es considerablemente peor. Una investigación adicional muestra que la precisión promedio media de TB06-adhoc es de 0.342 y es aproximadamente un 10% mejor que la del primer conjunto de datos. Mientras que los otros tres métodos suelen considerar los 100 mejores documentos o menos de una lista clasificada, el método de claridad generalmente necesita los 500 mejores documentos o más para medir adecuadamente la coherencia de una lista clasificada. Un promedio de precisión media más alto hace que las listas clasificadas recuperadas por diferentes consultas sean más similares en términos de coherencia en el nivel de los primeros 500 documentos. Creemos que esta es la razón principal de la baja precisión de la puntuación de claridad en el segundo conjunto de datos. Aunque este documento se centra en un entorno de búsqueda web, es deseable que nuestras técnicas funcionen de manera consistente en otras situaciones. Con este fin, examinamos la efectividad de nuestras técnicas en la pista Robust 2004. Para nuestros métodos, dividimos equitativamente todas las consultas de prueba en cinco grupos y realizamos validación cruzada de cinco pliegues. Cada vez que utilizamos un grupo para el entrenamiento y los otros cuatro grupos para las pruebas. Hacemos uso de todas las consultas para nuestros dos puntos de referencia, es decir, la puntuación de claridad y la puntuación de robustez. Los parámetros para nuestras líneas base son los mismos que los utilizados en [1]. Los resultados mostrados en la Tabla 5 demuestran que la precisión de predicción de nuestros métodos está a la par con la de las dos líneas base sólidas. Claridad Robusta WIG QF 0.464 0.539 0.468 0.464 Tabla 5: Comparación de los coeficientes de correlación de Pearson en la pista Robusta de 2004 para la puntuación de claridad, puntuación de robustez, WIG y retroalimentación de consulta (QF). Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. Además, examinamos la sensibilidad de predicción de nuestros métodos al rango de corte K. Con respecto a WIG, es bastante robusto a K en las Pistas de Terabyte (2004-2006) mientras prefiere un valor pequeño de K como 5 en la Pista Robusta de 2004. En otras palabras, un valor pequeño de K es una elección casi óptima para ambos tipos de pistas. Teniendo en cuenta que todos los demás parámetros involucrados en WIG están fijos y, consecuentemente, son los mismos para los dos casos, esto significa que WIG puede lograr una precisión de predicción casi óptima en dos situaciones considerablemente diferentes con exactamente la misma configuración de parámetros. En cuanto a QF, prefiere un valor más grande de K, como 100 en las Pistas de Terabyte y un valor más pequeño de K, como 25 en la Pista Robusta de 2004. 4.2.2 Consultas NP Adoptamos WIG y el primer cambio de rango (FRC) para predecir el rendimiento de las consultas NP. También intentamos una combinación lineal de los dos como en la sección anterior. El peso de la combinación se obtiene del otro conjunto de datos. Utilizamos la correlación con los rangos recíprocos medidos por la prueba de correlación de Pearson para evaluar la calidad de la predicción. Los resultados se presentan en la Tabla 6. Nuevamente, nuestros puntos de referencia son la puntuación de claridad y la puntuación de robustez. Para hacer una comparación justa, ajustamos la puntuación de claridad de diferentes maneras. Descubrimos que utilizar el documento clasificado en primer lugar para construir el modelo de consulta produce la mejor precisión de predicción. También intentamos utilizar la estructura del documento mediante la combinación de modelos de lenguaje mencionados en la sección 3.1. Se obtuvo una pequeña mejora. Los coeficientes de correlación para la puntuación de claridad reportados en la Tabla 6 son los mejores que hemos encontrado. Como podemos ver, nuestros métodos superan considerablemente la técnica de puntuación de claridad en ambas ejecuciones. Esto confirma nuestra intuición de que el uso de una medida basada en la coherencia como el puntaje de claridad es inapropiado para las consultas de NP. Claridad de métodos robusta. Tabla 6: Coeficientes de correlación de Pearson para la correlación con rangos recíprocos en las pistas de Terabyte (NP) para la puntuación de claridad, puntuación de robustez, WIG, el primer cambio de rango (FRC) y una combinación lineal de WIG y FRC. Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. En cuanto a la puntuación de robustez, también ajustamos los parámetros y reportamos el mejor que hemos encontrado. Observamos una correlación negativa interesante y sorprendente con los rangos recíprocos. Explicamos este hallazgo brevemente. Un puntaje de robustez alto significa que varios documentos de alta clasificación en la lista clasificada original siguen estando muy bien clasificados después de perturbar los documentos. La existencia de tales documentos es una buena señal de alto rendimiento para consultas basadas en contenido, ya que estas consultas suelen contener una serie de documentos relevantes [1]. Sin embargo, en lo que respecta a las consultas de NP, una diferencia fundamental es que solo hay un documento relevante para cada consulta. La existencia de tales documentos puede confundir la función de clasificación y llevar a un bajo rendimiento de recuperación. Aunque existe una correlación negativa con el rendimiento de recuperación, la fuerza de la correlación es más débil y menos consistente en comparación con nuestros métodos, como se muestra en la Tabla 6. Basándonos en el análisis anterior, podemos ver que las técnicas de predicción actuales como la puntuación de claridad y la puntuación de robustez, que están principalmente diseñadas para consultas basadas en contenido, enfrentan desafíos significativos y son insuficientes para manejar consultas de NP. Nuestras dos técnicas propuestas para consultas de NP demuestran consistentemente una buena precisión de predicción, mostrando un éxito inicial en la resolución del problema de predecir el rendimiento para consultas de NP. Otro punto que queremos destacar es que el método WIG funciona bien para ambos tipos de consultas, una propiedad deseable que la mayoría de las técnicas de predicción carecen. 4.3 Tipos de Consultas Desconocidos En esta sección, realizamos dos tipos de experimentos sin acceso a etiquetas de tipo de consulta. Primero, asumimos que solo existe un tipo de consulta pero el tipo es desconocido. Segundo, experimentamos con una mezcla de consultas basadas en contenido y NP. Las dos subsecciones siguientes informarán los resultados para las dos condiciones respectivamente. 4.3.1 Solo existe un tipo. Suponemos que todas las consultas son del mismo tipo, es decir, son consultas NP o consultas basadas en contenido. Elegimos WIG para tratar este caso porque muestra una buena precisión de predicción para ambos tipos de consultas en la sección anterior. Consideramos dos casos: (1) CB: todas las 150 consultas de títulos de la tarea ad-hoc de las pistas Terabyte 2004-2006 (2) NP: todas las 433 consultas de NP de la tarea de búsqueda de páginas nombradas de las pistas Terabyte 2005 y 2006. Tomamos una estrategia simple etiquetando todas las consultas en cada caso como el mismo tipo (ya sea NP o CB) independientemente de su tipo real. El cálculo de WIG se basará en el tipo de consulta etiquetado en lugar del tipo real. Hay cuatro posibilidades con respecto a la relación entre el tipo real y el tipo etiquetado. La correlación con el rendimiento de recuperación bajo las cuatro posibilidades se presenta en la Tabla 7. Por ejemplo, el valor 0.445 en la intersección entre la segunda fila y la tercera columna muestra el coeficiente de correlación de Pearson para la correlación con la precisión promedio cuando las consultas basadas en contenido se etiquetan incorrectamente como del tipo NP. Basándonos en estos resultados, recomendamos tratar todas las consultas como del tipo NP cuando solo existe un tipo de consulta y la clasificación precisa de la consulta no es factible, considerando el riesgo de que se produzca una gran pérdida de precisión si las consultas NP se etiquetan incorrectamente como consultas basadas en contenido. Estos resultados también demuestran la fuerte adaptabilidad de WIG a diferentes tipos de consultas. Tabla 7: Comparación de los coeficientes de correlación de Pearson para la correlación con el rendimiento de recuperación en cuatro posibilidades en las pistas de Terabyte (NP). Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. 4.3.2 Una mezcla de consultas basadas en contenido y NP Una mezcla de los dos tipos de consultas es una situación más realista que un motor de búsqueda web encontrará. Evaluamos la precisión de la predicción por la exactitud con la que se pueden identificar las consultas de bajo rendimiento mediante el método de predicción, asumiendo que los tipos de consulta reales son desconocidos (pero podemos predecir los tipos de consulta). Esta es una tarea desafiante porque tanto el rendimiento predicho como el real para un tipo de consulta pueden ser incomparables con el de otro tipo. A continuación discutimos cómo implementar nuestra evaluación. Creamos un conjunto de consultas que consiste en todas las 150 consultas de título ad-hoc de Terabyte Track 2004-2006 y todas las 433 consultas NP de Terabyte Track 2005&2006. Dividimos las consultas en el conjunto en clases: buenas (mejores que el 50% de las consultas del mismo tipo en términos de rendimiento de recuperación) y malas (de lo contrario). Según estos estándares, una consulta de NP con un rango recíproco superior a 0.2 o una consulta basada en contenido con una precisión promedio superior a 0.315 se considerarán como buenas. Entonces, cada vez que seleccionamos aleatoriamente una consulta Q del grupo con una probabilidad p de que Q esté basada en el contenido. Las consultas restantes se utilizan como datos de entrenamiento. Primero decidimos el tipo de consulta Q de acuerdo con un clasificador de consultas. Es decir, el clasificador de consultas nos dice si la consulta Q es de tipo NP o basada en contenido. Basándose en el tipo de consulta predicho y en la puntuación calculada para la consulta Q por una técnica de predicción, se toma una decisión binaria sobre si la consulta Q es buena o mala al compararla con el umbral de puntuación del tipo de consulta predicho obtenido de los datos de entrenamiento. La precisión de la predicción se mide por la exactitud de la decisión binaria. En nuestra implementación, tomamos repetidamente una consulta de prueba del conjunto de consultas y la precisión de la predicción se calcula como el porcentaje de decisiones correctas, es decir, una consulta buena (mala) se predice como buena (mala). Es obvio que adivinar al azar llevará a una precisión del 50%. Tomemos el método WIG como ejemplo para ilustrar el proceso. Dos umbrales de WIG (uno para consultas de NP y otro para consultas basadas en contenido) se entrenan maximizando la precisión de predicción en los datos de entrenamiento. Cuando una consulta de prueba es etiquetada como del tipo NP (CB) por el clasificador de tipo de consulta, se predecirá que es buena solo si la puntuación WIG para esta consulta está por encima del umbral de NP (CB). Se seguirán procedimientos similares para otras técnicas de predicción. Ahora presentamos brevemente el clasificador de tipo de consulta automático utilizado en este artículo. Encontramos que la puntuación de robustez, aunque originalmente propuesta para la predicción de rendimiento, es un buen indicador de tipos de consultas. Observamos que, en promedio, las consultas basadas en contenido tienen una puntuación de robustez mucho más alta que las consultas de NP. Por ejemplo, la Figura 2 muestra las distribuciones de puntajes de robustez para consultas basadas en NP y en contenido. Según este hallazgo, el clasificador de puntuación de robustez adjuntará una etiqueta NP (CB) a la consulta si la puntuación de robustez para la consulta está por debajo (por encima) de un umbral entrenado a partir de los datos de entrenamiento. 0 0.5 1 1.5 2 2.5 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 NP Basado en contenido Figura 2: Distribución de puntuaciones de robustez para consultas NP y CB. Las consultas NP son los 252 temas NP del Terabyte Track de 2005. Las consultas basadas en contenido son los 150 títulos ad-hoc de las pistas Terabyte 2004-2006. Las distribuciones de probabilidad son estimadas por el método de estimación de densidad de Kernel. Estrategias Robustas WIG-1 WIG-2 WIG-3 Óptima p=0.6 0.565 0.624 0.665 0.684 0.701 P=0.4 0.567 0.633 0.654 0.673 0.696 Tabla 8: Comparación de la precisión de predicción para cinco estrategias en la situación de consulta mixta. Dos formas de muestrear una consulta de la piscina: (1) la consulta muestreada es basada en contenido con una probabilidad p=0.6 (es decir, la consulta es NP con una probabilidad de 0.4) (2) establecer la probabilidad p=0.4. Consideramos cinco estrategias en nuestros experimentos. En la primera estrategia (denominada robusta), utilizamos la puntuación de robustez para la predicción del rendimiento de la consulta con la ayuda de un clasificador de consultas perfecto que siempre mapea correctamente una consulta en una de las dos categorías (es decir, NP o CB). Esta estrategia representa el nivel de precisión de predicción que las técnicas actuales de predicción pueden lograr en una condición ideal en la que se conocen los tipos de consulta. En las tres estrategias siguientes, se adopta el método WIG para predecir el rendimiento. La diferencia entre los tres es que se utilizan tres clasificadores de consulta diferentes para cada estrategia: (1) el clasificador siempre clasifica una consulta en el tipo NP. (2) el clasificador de Probabilidad de Densidad de Puntuación Robusta es el clasificador de puntuación robusta mencionado anteriormente. (3) el clasificador es perfecto. Estas tres estrategias están designadas como WIG-1, WIG-2 y WIG-3 respectivamente. La razón por la que estamos interesados en WIG-1 se basa en los resultados de la sección 4.3.1. En la última estrategia (denominada Óptima) que sirve como un límite superior de lo bien que podemos hacer hasta ahora, hacemos pleno uso de nuestras técnicas de predicción para cada tipo de consulta asumiendo que un clasificador de consultas perfecto está disponible. Específicamente, combinamos linealmente WIG y QF para consultas basadas en contenido y WIG y FRC para consultas de NP. Los resultados de las cinco estrategias se muestran en la Tabla 8. Para cada estrategia, intentamos dos formas de muestrear una consulta del conjunto: (1) la consulta muestreada es CB con probabilidad p=0.6 (la consulta es NP con probabilidad 0.4) (2) establecer la probabilidad p=0.4. De la Tabla 8 podemos ver que en términos de precisión de predicción, WIG-2 (el método WIG con el clasificador de consultas automático) no solo es mejor que los dos primeros casos, sino que también está cerca de WIG-3 donde se asume un clasificador perfecto. Se observan algunas mejoras adicionales sobre WIG-3 cuando se combinan con otras técnicas de predicción. El mérito de WIG-2 es que proporciona una solución práctica para identificar automáticamente consultas con bajo rendimiento en un entorno de búsqueda web con tipos de consultas mixtos, lo cual representa considerables obstáculos para las técnicas de predicción tradicionales. CONCLUSIONES Y TRABAJOS FUTUROS Hasta donde sabemos, nuestro artículo es el primero en explorar a fondo la predicción del rendimiento de consultas en entornos de búsqueda web. Demostramos que nuestros modelos resultaron en una mayor precisión de predicción que las técnicas previamente publicadas no especialmente diseñadas para escenarios de búsqueda en la web. En este artículo, nos enfocamos en dos tipos de consultas en la búsqueda web: consultas basadas en contenido y consultas de búsqueda de Páginas-Nombradas (NP), correspondientes a la tarea de recuperación ad-hoc y la tarea de búsqueda de Páginas-Nombradas respectivamente. Para ambos tipos de consultas web, se demostró que nuestros modelos de predicción son sustancialmente más precisos que las técnicas actuales más avanzadas. Además, consideramos un caso más realista en el que no se dispone de información previa sobre los tipos de consultas. Demostramos que el método WIG es particularmente adecuado para esta situación. Considerando la adaptabilidad de WIG a una variedad de colecciones y tipos de consultas, uno de nuestros planes futuros es aplicar este método para predecir la preferencia del usuario por los resultados de búsqueda en datos realistas recopilados de un motor de búsqueda comercial. Además de la precisión, otro problema importante con el que las técnicas de predicción tienen que lidiar en un entorno web es la eficiencia. Afortunadamente, dado que el puntaje WIG se calcula solo sobre los términos y frases que aparecen en la consulta, este cálculo puede realizarse de manera muy eficiente con el soporte de un índice. Por otro lado, el cálculo de QF y FRC es relativamente menos eficiente ya que QF necesita recuperar toda la colección dos veces y FRC necesita clasificar repetidamente los documentos perturbados. Cómo mejorar la eficiencia de QF y FRC es nuestro trabajo futuro. Además, las técnicas de predicción propuestas en este documento tienen el potencial de mejorar el rendimiento de recuperación al combinarse con otras técnicas de RI. Por ejemplo, nuestras técnicas pueden ser incorporadas a técnicas populares de modificación de consultas como la expansión de consultas y la relajación de consultas. Guiados por la predicción del rendimiento, podemos tomar una mejor decisión sobre cuándo o cómo modificar las consultas para mejorar la efectividad de la recuperación. Nos gustaría llevar a cabo investigaciones en esta dirección en el futuro. AGRADECIMIENTOS Este trabajo fue apoyado en parte por el Centro de Recuperación de Información Inteligente, en parte por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el contrato número HR0011-06-C0023, y en parte por un premio de Google. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son del autor y no necesariamente reflejan las del patrocinador. Además, agradecemos a Donald Metzler por sus valiosos comentarios sobre este trabajo. 7. REFERENCIAS [1] Y. Zhou, W. B. Croft, Ranking Robustness: Un nuevo marco para predecir el rendimiento de consultas, en Actas de CIKM 2006. [2] D. Carmel, E. Yom-Tov, A. Darlow, D. Pelleg, ¿Qué hace que una consulta sea difícil?, en Actas de SIGIR 2006. [3] C. L. A. Clarke, F. Scholer, I. Soboroff, La pista Terabyte TREC 2005, En las Actas en Línea de TREC 2005. [4] B. Él y yo. Inferir el rendimiento de la consulta utilizando predictores de preretrieval. En actas de SPIRE 2004. [5] S. Tomlinson. Recuperación robusta de la web y terabytes con Hummingbird SearchServer en TREC 2004. En las Actas en Línea de TREC 2004. [6] S. Cronen-Townsend, Y. Zhou, W. B. Croft, Prediciendo el Rendimiento de Consultas, en las Actas de SIGIR 2002. [7] V. Vinay, I. J. Cox, N. Mill-Frayling, K. Wood, Sobre la Clasificación de la Efectividad del Buscador, en las Actas de SIGIR 2006. [8] D. Metzler, W. B. Croft, Un Modelo de Campo Aleatorio de Markov para Dependencias de Términos, en las Actas de SIGIR 2005. [9] D. Metzler, T. Strohman, Y. Zhou, W. B. Croft, Indri en TREC 2005: Terabyte Track, en las Actas en Línea de TREC 2004. [10] P. Ogilvie y J. Callan, Combinando representaciones de documentos para búsqueda de elementos conocidos, en las Actas de SIGIR 2003. [11] A. Berger, J. Lafferty, Recuperación de información como traducción estadística, en las Actas de SIGIR 1999. [12] Motor de búsqueda Indri: http://www.lemurproject.org/indri/ [13] I. J. Taneja: Sobre medidas de información generalizadas y sus aplicaciones, Avances en Electrónica y Física de Electrones, Academic Press (EE. UU.), 76, 1989, 327-413. [14] S. Cronen-Townsend, Y. Zhou y Croft, W. B., Un marco para la expansión selectiva de consultas, en Actas de CIKM 2004. [15] F. Song, W. B. Croft, Un modelo de lenguaje general para la recuperación de información, en Actas de SIGIR 1999. [16] Contacto por correo electrónico personal con Vishwa Vinay y nuestros propios experimentos [17] E. Yom-Tov, S. Fine, D. Carmel, A. Darlow, Aprendiendo a estimar la dificultad de la consulta incluyendo aplicaciones para la detección de contenido faltante y la recuperación de información distribuida, en Actas de SIGIR 2005. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "mixed-query situation": {
            "translated_key": "situación de consulta mixta",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Query Performance Prediction in Web Search Environments Yun Zhou and W. Bruce Croft Department of Computer Science University of Massachusetts, Amherst {yzhou, croft}@cs.umass.edu ABSTRACT Current prediction techniques, which are generally designed for content-based queries and are typically evaluated on relatively homogenous test collections of small sizes, face serious challenges in web search environments where collections are significantly more heterogeneous and different types of retrieval tasks exist.",
                "In this paper, we present three techniques to address these challenges.",
                "We focus on performance prediction for two types of queries in web search environments: content-based and Named-Page finding.",
                "Our evaluation is mainly performed on the GOV2 collection.",
                "In addition to evaluating our models for the two types of queries separately, we consider a more challenging and realistic situation that the two types of queries are mixed together without prior information on query types.",
                "To assist prediction under the <br>mixed-query situation</br>, a novel query classifier is adopted.",
                "Results show that our prediction of web query performance is substantially more accurate than the current stateof-the-art prediction techniques.",
                "Consequently, our paper provides a practical approach to performance prediction in realworld web settings.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval -Query formulation General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Query performance prediction has many applications in a variety of information retrieval (IR) areas such as improving retrieval consistency, query refinement, and distributed IR.",
                "The importance of this problem has been recognized by IR researchers and a number of new methods have been proposed for prediction recently [1, 2, 17].",
                "Most work on prediction has focused on the traditional ad-hoc retrieval task where query performance is measured according to topical relevance.",
                "These prediction models are evaluated on TREC document collections which typically consist of no more than one million relatively homogenous newswire articles.",
                "With the popularity and influence of the Web, prediction techniques that will work well for web-style queries are highly preferable.",
                "However, web search environments pose significant challenges to current prediction models that are mainly designed for traditional TREC settings.",
                "Here we outline some of these challenges.",
                "First, web collections, which are much larger than conventional TREC collections, include a variety of documents that are different in many aspects such as quality and style.",
                "Current prediction techniques can be vulnerable to these characteristics of web collections.",
                "For example, the reported prediction accuracy of the ranking robustness technique and the clarity technique on the GOV2 collection (a large web collection) is significantly worse compared to the other TREC collections [1].",
                "Similar prediction accuracy on the GOV2 collection using another technique is reported in [2], confirming the difficult of predicting query performance on a large web collection.",
                "Furthermore, web search goes beyond the scope of the ad-hoc retrieval task based on topical relevance.",
                "For example, the Named-Page (NP) finding task, which is a navigational task, is also popular in web retrieval.",
                "Query performance prediction for the NP task is still necessary since NP retrieval performance is far from perfect.",
                "In fact, according to the report on the NP task of the 2005 Terabyte Track [3], about 40% of the test queries perform poorly (no correct answer in the first 10 search results) even in the best run from the top group.",
                "To our knowledge, little research has explicitly addressed the problem of NP-query performance prediction.",
                "Current prediction models devised for content-based queries will be less effective for NP queries considering the fundamental differences between the two.",
                "Third, in real-world web search environments, user queries are usually a mixture of different types and prior knowledge about the type of each query is generally unavailable.",
                "The <br>mixed-query situation</br> raises new problems for query performance prediction.",
                "For instance, we may need to incorporate a query classifier into prediction models.",
                "Despite these problems, the ability to handle this situation is a crucial step towards turning query performance prediction from an interesting research topic into a practical tool for web retrieval.",
                "In this paper, we present three techniques to address the above challenges that current prediction models face in Web search environments.",
                "Our work focuses on query performance prediction for the content-based (ad-hoc) retrieval task and the name-page finding task in the context of web retrieval.",
                "Our first technique, called weighted information gain (WIG), makes use of both single term and term proximity features to estimate the quality of top retrieved documents for prediction.",
                "We find that WIG offers consistent prediction accuracy across various test collections and query types.",
                "Moreover, we demonstrate that good prediction accuracy can be achieved for the <br>mixed-query situation</br> by using WIG with the help of a query type classifier.",
                "Query feedback and first rank change, which are our second and third prediction techniques, perform well for content-based queries and NP queries respectively.",
                "Our main contributions include: (1) considerably improved prediction accuracy for web content-based queries over several state-of-the-art techniques. (2) new techniques for successfully predicting NP-query performance. (3) a practical and fully automatic solution to predicting mixed-query performance.",
                "In addition, one minor contribution is that we find that the robustness score [1], which was originally proposed for performance prediction, is helpful for query classification. 2.",
                "RELATED WORK As we mentioned in the introduction, a number of prediction techniques have been proposed recently that focus on contentbased queries in the topical relevance (ad-hoc) task.",
                "We know of no published work that addresses other types of queries such as NP queries, let alone a mixture of query types.",
                "Next we review some representative models.",
                "The major difficulty of performance prediction comes from the fact that many factors have an impact on retrieval performance.",
                "Each factor affects performance to a different degree and the overall effect is hard to predict accurately.",
                "Therefore, it is not surprising to notice that simple features, such as the frequency of query terms in the collection [4] and the average IDF of query terms [5], do not predict well.",
                "In fact, most of the successful techniques are based on measuring some characteristics of the retrieved document set to estimate topic difficulty.",
                "For example, the clarity score [6] measures the coherence of a list of documents by the KL-divergence between the query model and the collection model.",
                "The robustness score [1] quantifies another property of a ranked list: the robustness of the ranking in the presence of uncertainty.",
                "Carmel et al. [2] found that the distance measured by the Jensen-Shannon divergence between the retrieved document set and the collection is significantly correlated to average precision.",
                "Vinay et al. [7] proposed four measures to capture the geometry of the top retrieved documents for prediction.",
                "The most effective measure is the sensitivity to document perturbation, an idea somewhat similar to the robustness score.",
                "Unfortunately, their way of measuring the sensitivity does not perform equally well for short queries and prediction accuracy drops considerably when a state-of-the-art retrieval technique (like Okapi or a language modeling approach) is adopted for retrieval instead of the tf-idf weighting used in their paper [16].",
                "The difficulties of applying these models in web search environments have already been mentioned.",
                "In this paper, we mainly adopt the clarity score and the robustness score as our baselines.",
                "We experimentally show that the baselines, even after being carefully tuned, are inadequate for the web environment.",
                "One of our prediction models, WIG, is related to the Markov random field (MRF) model for information retrieval [8].",
                "The MRF model directly models term dependence and is found be to highly effective across a variety of test collections (particularly web collections) and retrieval tasks.",
                "This model is used to estimate the joint probability distribution over documents and queries, an important part of WIG.",
                "The superiority of WIG over other prediction techniques based on unigram features, which will be demonstrated later in our paper, coincides with that of MRF for retrieval.",
                "In other word, it is interesting to note that term dependence, when being modeled appropriately, can be helpful for both improving and predicting retrieval performance. 3.",
                "PREDICTION MODELS 3.1 Weighted Information Gain (WIG) This section introduces a weighted information gain approach that incorporates both single term and proximity features for predicting performance for both content-based and Named-Page (NP) finding queries.",
                "Given a set of queries Q={Qs} (s=1,2,..N) which includes all possible user queries and a set of documents D={Dt} (t=1,2…M), we assume that each query-document pair (Qs,Dt) is manually judged and will be put in a relevance list if Qs is found to be relevant to Dt.",
                "The joint probability P(Qs,Dt) over queries Q and documents D denotes the probability that pair (Qs,Dt) will be in the relevance list.",
                "Such assumptions are similar to those used in [8].",
                "Assuming that the user issues query Qi ∈Q and the retrieval results in response to Qi is a ranked list L of documents, we calculate the amount of information contained in P(Qs,Dt) with respect to Qi and L by Eq.1 which is a variant of entropy called the weighted entropy[13].",
                "The weights in Eq.1 are solely determined by Qi and L. )1(),(log),(),( , , ∑−= ts tststsLQ DQPDQweightDQH i In this paper, we choose the weights as follows: LindocumentsKtopthecontainsLTwhere otherwise LTDandisifK DQweight K Kt ts )( )2( ,0 )(,/1 ),( ⎩ ⎨ ⎧ ∈= = The cutoff rank K is a parameter in our model that will be discussed later.",
                "Accordingly, Eq.1 can be simplified as follows: )3(),(log 1 ),( )( , ∑∈ −= LTD titsLQ Kt i DQP K DQH Unfortunately, weighted entropy ),(, tsLQ DQH i computed by Eq.3, which represents the amount of information about how likely the top ranked documents in L would be relevant to query Qi on average, cannot be compared across different queries, making it inappropriate for directly predicting query performance.",
                "To mitigate this problem, we come up with a background distribution P(Qs,C) over Q and D by imagining that every document in D is replaced by the same special document C which represents average language usage.",
                "In this paper, C is created by concatenating every document in D. Roughly speaking, C is the collection (the document set) {Dt} without document boundaries.",
                "Similarly, weighted entropy ),(, CQH sLQi calculated by Eq.3 represents the amount of information about how likely an average document (represented by the whole collection) would be relevant to query Qi.",
                "Now we introduce our performance predictor WIG which is the weighted information gain [13] computed as the difference between ),(, tsLQ DQH i and ),(, CQH sLQi .Specifically, given query Qi, collection C and ranked list L of documents, WIG is calculated as follows: )4( ),( ),( log 1 ),( ),( log),( ),(),(),,( )(, ,, ∑∑ ∈ == −= LTD i ti ts s ts ts tsLQsLQi Kt ii CQP DQP KCQP DQP DQweight DQHCQHLCQWIG WIG computed by Eq.4 measures the change in information about the quality of retrieval (in response to query Qi) from an imaginary state that only an average document is retrieved to a posterior state that the actual search results are observed.",
                "We hypothesize that WIG is positively correlated with retrieval effectiveness because high quality retrieval should be much more effective than just returning the average document.",
                "The heart of this technique is how to estimate the joint distribution P(Qs,Dt).",
                "In the language modeling approach to IR, a variety of models can be applied readily to estimate this distribution.",
                "Although most of these models are based on the bagof-words assumption, recent work on modeling term dependence under the language modeling framework have shown consistent and significant improvements in retrieval effectiveness over bagof-words models.",
                "Inspired by the success of incorporating term proximity features into language models, we decide to adopt a good dependence model to estimate the probability P(Qs,Dt).",
                "The model we chose for this paper is Metzler and Crofts Markov Random Field (MRF) model, which has already demonstrated superiority over a number of collections and different retrieval tasks [8,9].",
                "According to the MRF model, log P(Qi, Dt) can be written as )5()|(loglog),(log )( 1 ∑∈ +−= iQF tti DPZDQP ξ ξ ξλ where Z1 is a constant that ensures that P(Qi, Dt) sums up to 1.",
                "F(Qi) consists of a set of features expanded from the original query Qi .",
                "For example, assuming that query Qi is talented student program, F(Qi) includes features like program and talented student.",
                "We consider two kinds of features: single term features T and proximity features P. Proximity features include exact phrase (#1) and unordered window (#uwN) features as described in [8].",
                "Note that F(Qi) is the union of T(Qi) and P(Qi).",
                "For more details on F(Qi) such as how to expand the original query Qi to F(Qi), we refer the reader to [8] and [9].",
                "P(ξ|Dt) denotes the probability that feature ξ will occur in Dt.",
                "More details on P(ξ|Dt) will be provided later in this section.",
                "The choice of λξ is somewhat different from that used in [8] since λξ plays a dual role in our model.",
                "The first role, which is the same as in [8], is to weight between single term and proximity features.",
                "The other role, which is specific to our prediction task, is to normalize the size of F(Qi).We found that the following weight strategy for λξ satisfies the above two roles and generalizes well on a variety of collections and query types. )6( )(, |)(| 1 )(, |)(| ⎪ ⎪ ⎩ ⎪ ⎪ ⎨ ⎧ ∈ − ∈ = i i T i i T QP QP QT QT ξ λ ξ λ λξ where |T(Qi)| and |P(Qi)| denote the number of single term and proximity features in F(Qi) respectively.",
                "The reason for choosing the square root function in the denominator of λξ is to penalize a feature set of large size appropriately, making WIG more comparable across queries of various lengths. λT is a fixed parameter and set to 0.8 according to [8] throughout this paper.",
                "Similarly, log P(Qi,C) can be written as: )7()|(loglog),(log )( 2 ∑∈ +−= iQF i CPZCQP ξ ξ ξλ When constant Z1 and Z2 are dropped, WIG computed in Eq.4 can be rewritten as follows by plugging in Eq.5 and Eq.7 : )8( )|( )|( log 1 ),,( )( )( ∑ ∑∈ ∈ = LTD QF t i Kt i CP DP K LCQWIG ξ ξ ξ ξ λ One of the advantages of WIG over other techniques is that it can handle well both content-based and NP queries.",
                "Based on the type (or the predicted type) of Qi, the calculation of WIG in Eq. 8 differs in two aspects: (1) how to estimate P(ξ|Dt) and P(ξ|C), and (2) how to choose K. For content-based queries, P(ξ|C) is estimated by the relative frequency of feature ξ in collection C as a whole.",
                "The estimation of P(ξ|Dt) is the same as in [8].",
                "Namely, we estimate P(ξ|Dt) by the relative frequency of feature ξ in Dt linearly smoothed with collection frequency P(ξ|C).",
                "K in Eq.8 is treated as a free parameter.",
                "Note that K is the only free parameter in the computation of WIG for content-based queries because all parameters involved in P(ξ|Dt) are assumed to be fixed by taking the suggested values in [8].",
                "Regarding NP queries, we make use of document structure to estimate P(ξ|Dt) and P(ξ|C) by the so-called mixture of language models proposed in [10] and incorporated into the MRF model for Named-Page finding retrieval in [9].",
                "The basic idea is that a document (collection) is divided into several fields such as the title field, the main-body field and the heading field.",
                "P(ξ|Dt) and P(ξ|C) are estimated by a linear combination of the language models from each field.",
                "Due to space constraints, we refer the reader to [9] for details.",
                "We adopt the exact same set of parameters as used in [9] for estimation.",
                "With regard to K in Eq.8, we set K to 1 because the Named-Page finding task heavily focuses on the first ranked document.",
                "Consequently, there are no free parameters in the computation of WIG for NP queries. 3.2 Query Feedback In this section, we introduce another technique called query feedback (QF) for prediction.",
                "Suppose that a user issues query Q to a retrieval system and a ranked list L of documents is returned.",
                "We view the retrieval system as a noisy channel.",
                "Specifically, we assume that the output of the channel is L and the input is Q.",
                "After going through the channel, Q becomes corrupted and is transformed to ranked list L. By thinking about the retrieval process this way, the problem of predicting retrieval effectiveness turns to the task of evaluating the quality of the channel.",
                "In other words, prediction becomes finding a way to measure the degree of corruption that arises when Q is transformed to L. As directly computing the degree of the corruption is difficult, we tackle this problem by approximation.",
                "Our main idea is that we measure to what extent information on Q can be recovered from L on the assumption that only L is observed.",
                "Specifically, we design a decoder that can accurately translate L back into new query Q and the similarity S between the original query Q and the new query Q is adopted as a performance predictor.",
                "This is a sketch of how the QF technique predicts query performance.",
                "Before filling in more details, we briefly discuss why this method would work.",
                "There is a relation between the similarity S defined above and retrieval performance.",
                "On the one hand, if the retrieval has strayed from the original sense of the query Q, the new query Q extracted from ranked list L in response to Q would be very different from the original query Q.",
                "On the other hand, a query distilled from a ranked list containing many relevant documents is likely to be similar to the original query.",
                "Further examples in support of the relation will be provided later.",
                "Next we detail how to build the decoder and how to measure the similarity S. In essence, the goal of the decoder is to compress ranked list L into a few informative terms that should represent the content of the top ranked documents in L. Our approach to this goal is to represent ranked list L by a language model (distribution over terms).",
                "Then terms are ranked by their contribution to the language models KL (Kullback-Leibler) divergence from the background collection model.",
                "Top ranked terms will be chosen to form the new query Q.",
                "This approach is similar to that used in Section 4.1 of [11].",
                "Specifically, we take three steps to compress ranked list L into query Q without referring to the original query. 1.",
                "We adopt the ranked list language model [14], to estimate a language model based on ranked list L. The model can be written as: )9()|()|()|( ∑∈ = LD LDPDwPLwP where w is any term, D is a document.",
                "P(D|L) is estimated by a linearly decreasing function of the rank of document D. 2.",
                "Each term in P(w|L) is ranked by the following KL-divergence contribution: )10( )|( )|( log)|( CwP LwP LwP where P(w|C) is the collection model estimated by the relative frequency of term w in collection C as a whole. 3.",
                "The top N ranked terms by Eq.10 form a weighted query Q={(wi,ti)} i=1,N. where wi denotes the i-th ranked term and weight ti is the KL-divergence contribution of wi in Eq. 10.",
                "Term cruise ship vessel sea passenger KL contribution 0.050 0.040 0.012 0.010 0.009 Table 1: top 5 terms compressed from the ranked list in response to query Cruise ship damage sea life Two representative examples, one for a poorly performing query Cruise ship damage sea life (TREC topic 719; average precision: 0.08) and the other for a high performing query prostate cancer treatments( TREC topic 710; average precision: 0.49), are shown in Table 1 and 2 respectively.",
                "These examples indicate how the similarity between the original and the new query correlates with retrieval performance.",
                "The parameter N in step 3 is set to 20 empirically and choosing a larger value of N is unnecessary since the weights after the top 20 are usually too small to make any difference.",
                "Term prostate cancer treatment men therapy KL contribution 0.177 0.140 0.028 0.025 0.020 Table 2: top 5 terms compressed from the ranked list in response to query prostate cancer treatments To measure the similarity between original query Q and new query Q, we first use Q to do retrieval on the same collection.",
                "A variant of the query likelihood model [15] is adopted for retrieval.",
                "Namely, documents are ranked by: )11()|()|( ),( ∑∈ = Qtw t i ii i DwPDQP where wi is a term in Q and ti is the associated weight.",
                "D is a document.",
                "Let L denote the new ranked list returned from the above retrieval.",
                "The similarity is measured by the overlap of documents in L and L. Specifically, the percentage of documents in the top K documents of L that are also present in the top K documents in L. the cutoff K is treated as a free parameter.",
                "We summarize here how the QF technique predicts performance given a query Q and the associated ranked list L. We first obtain a weighted query Q compressed from L by the above three steps.",
                "Then we use Q to perform retrieval and the new ranked list is L. The overlap of documents in L and L is used for prediction. 3.3 First Rank Change (FRC) In this section, we propose a method called the first rank change (FRC) for performance prediction for NP queries.",
                "This method is derived from the ranking robustness technique [1] that is mainly designed for content-based queries.",
                "When directly applied to NP queries, the robustness technique will be less effective because it takes the top ranked documents as a whole into account while NP queries usually have only one single relevant document.",
                "Instead, our technique focuses on the first rank document while the main idea of the robustness method remains.",
                "Specifically, the pseudocode for computing FRC is shown in figure 1.",
                "Input: (1) ranked list L={Di} where i=1,100.",
                "Di denotes the i-th ranked document. (2) query Q 1 initialize: (1) set the number of trials J=100000 (2) counter c=0; 2 for i=1 to J 3 Perturb every document in L, let the outcome be a set F={Di} where Di denotes the perturbed version of Di. 4 Do retrieval with query Q on set F 5 c=c+1 if and only if D1 is ranked first in step 4 6 end of for 7 return the ratio c/J Figure 1: pseudo-code for computing FRC FRC approximates the probability that the first ranked document in the original list L will remain ranked first even after the documents are perturbed.",
                "The higher the probability is, the more confidence we have in the first ranked document.",
                "On the other hand, in the extreme case of a random ranking, the probability would be as low as 0.5.",
                "We expect that FRC has a positive association with NP query performance.",
                "We adopt [1] to implement the document perturbation step (step 4 in Fig.1) using Poisson distributions.",
                "For more details, we refer the reader to [1]. 4.",
                "EVALUATION We now present the results of predicting query performance by our models.",
                "Three state-of-the-art techniques are adopted as our baselines.",
                "We evaluate our techniques across a variety of Web retrieval settings.",
                "As mentioned before, we consider two types of queries, that is, content-based (CB) queries and Named-Page(NP) finding queries.",
                "First, suppose that the query types are known.",
                "We investigate the correlation between the predicted retrieval performance and the actual performance for both types of queries separately.",
                "Results show that our methods yield considerable improvements over the baselines.",
                "We then consider a more challenging scenario where no prior information on query types is available.",
                "Two sub-cases are considered.",
                "In the first one, there exists only one type of query but the actual type is unknown.",
                "We assume a mixture of the two query types in the second case.",
                "We demonstrate that our models achieve good accuracy under this demanding scenario, making prediction practical in a real-world Web search environment. 4.1 Experimental Setup Our evaluation focuses on the GOV2 collection which contains about 25 million documents crawled from web sites in the .gov domain during 2004 [3].",
                "We create two kinds of data set for CB queries and NP queries respectively.",
                "For the CB type, we use the ad-hoc topics of the Terabyte Tracks of 2004, 2005 and 2006 and name them TB04-adhoc, TB05-adhoc and TB06-adhoc respectively.",
                "In addition, we also use the ad-hoc topics of the 2004 Robust Track (RT04) to test the adaptability of our techniques to a non-Web environment.",
                "For NP queries, we use the Named-Page finding topics of the Terabyte Tracks of 2005 and 2006 and we name them TB05-NP and TB06-NP respectively.",
                "All queries used in our experiments are titles of TREC topics as we center on web retrieval.",
                "Table 3 summarizes the above data sets.",
                "Name Collection Topic Number Query Type TB04-adhoc GOV2 701-750 CB TB05-adhoc GOV2 751-800 CB TB06-adhoc GOV2 801-850 CB RT04 Disk 4+5 (minus CR) 301-450;601700 CB TB05-NP GOV2 NP601-NP872 NP TB06-NP GOV2 NP901-NP1081 NP Table 3: Summary of test collections and topics Retrieval performance of individual content-based and NP queries is measured by the average precision and reciprocal rank of the first correct answer respectively.",
                "We make use of the Markov Random field model for both ad-hoc and Named-Page finding retrieval.",
                "We adopt the same setting of retrieval parameters used in [8,9].",
                "The Indri search engine [12] is used for all of our experiments.",
                "Though not reported here, we also tried the query likelihood model for ad-hoc retrieval and found that the results change little because of the very high correlation between the query performances obtained by the two retrieval models (0.96 measured by Pearsons coefficient). 4.2 Known Query Types Suppose that query types are known.",
                "We treat each type of query separately and measure the correlation with average precision (or the reciprocal rank in the case of NP queries).",
                "We adopt the Pearsons correlation test which reflects the degree of linear relationship between the predicted and the actual retrieval performance. 4.2.1 Content-based Queries Methods Clarity Robust JSD WIG QF WIG +QF TB04+0 5 adhoc 0.333 0.317 0.362 0.574 0.480 0.637 TB06 adhoc 0.076 0.294 N/A 0.464 0.422 0.511 Table 4: Pearsons correlation coefficients for correlation with average precision on the Terabyte Tracks (ad-hoc) for clarity score, robustness score, the JSD-based method(we directly cites the score reported in [2]), WIG, query feedback(QF) and a linear combination of WIG and QF.",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Table 4 shows the correlation with average precision on two data sets: one is a combination of TB04-adhoc and TB05-adhoc(100 topics in total) and the other is TB06-adhoc (50 topics).",
                "The reason that we put TB04-adhoc and TB05-adhoc together is to make our results comparable to [2].",
                "Our baselines are the clarity score (clarity) [6],the robustness score (robust)[1] and the JSDbased method (JSD) [2].",
                "For the clarity and robustness score, we have tried different parameter settings and report the highest correlation coefficients we have found.",
                "We directly cite the result of the JSD-based method reported in [2].",
                "The table also shows the results for the Weighted Information Gain (WIG) method and the Query Feedback (QF) method for predicting content-based queries.",
                "As we described in the previous section, both WIG and QF have one free parameter to set, that is, the cutoff rank K. We train the parameter on one dataset and test on the other.",
                "When combining WIG and QF, a simple linear combination is used and the combination weight is learned from the training data set.",
                "From these results, we can see that our methods are considerably more accurate compared to the baselines.",
                "We also observe that further improvements are obtained from the combination of WIG and QF, suggesting that they measure different properties of the retrieval process that relate to performance.",
                "We discover that our methods generalize well on TB06-adhoc while the correlation for the clarity score with retrieval performance on this data set is considerably worse.",
                "Further investigation shows that the mean average precision of TB06-adhoc is 0.342 and is about 10% better than that of the first data set.",
                "While the other three methods typically consider the top 100 or less documents given a ranked list, the clarity method usually needs the top 500 or more documents to adequately measure the coherence of a ranked list.",
                "Higher mean average precision makes ranked lists retrieved by different queries more similar in terms of coherence at the level of top 500 documents.",
                "We believe that this is the main reason for the low accuracy of the clarity score on the second data set.",
                "Though this paper focuses on a Web search environment, it is desirable that our techniques will work consistently well in other situations.",
                "To this end, we examine the effectiveness of our techniques on the Robust 2004 Track.",
                "For our methods, we evenly divide all of the test queries into five groups and perform five-fold cross validation.",
                "Each time we use one group for training and the remaining four groups for testing.",
                "We make use of all of the queries for our two baselines, that is, the clarity score and the robustness score.",
                "The parameters for our baselines are the same as those used in [1].The results shown in Table 5 demonstrate that the prediction accuracy of our methods is on a par with that of the two strong baselines.",
                "Clarity Robust WIG QF 0.464 0.539 0.468 0.464 Table 5: Comparison of Pearsons correlation coefficients on the 2004 Robust Track for clarity score, robustness score, WIG and query feedback (QF).",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Furthermore, we examine the prediction sensitivity of our methods to the cutoff rank K. With respect to WIG, it is quite robust to K on the Terabyte Tracks (2004-2006) while it prefers a small value of K like 5 on the 2004 Robust Track.",
                "In other words, a small value of K is a nearly-optimal choice for both kinds of tracks.",
                "Considering the fact that all other parameters involved in WIG are fixed and consequently the same for the two cases, this means WIG can achieve nearly-optimal prediction accuracy in two considerably different situations with exactly the same parameter setting.",
                "Regarding QF, it prefers a larger value of K such as 100 on the Terabyte Tracks and a smaller value of K such as 25 on the 2004 Robust Track. 4.2.2 NP Queries We adopt WIG and first rank change (FRC) for predicting NPquery performance.",
                "We also try a linear combination of the two as in the previous section.",
                "The combination weight is obtained from the other data set.",
                "We use the correlation with the reciprocal ranks measured by the Pearsons correlation test to evaluate prediction quality.",
                "The results are presented in Table 6.",
                "Again, our baselines are the clarity score and the robustness score.",
                "To make a fair comparison, we tune the clarity score in different ways.",
                "We found that using the first ranked document to build the query model yields the best prediction accuracy.",
                "We also attempted to utilize document structure by using the mixture of language models mentioned in section 3.1.",
                "Little improvement was obtained.",
                "The correlation coefficients for the clarity score reported in Table 6 are the best we have found.",
                "As we can see, our methods considerably outperform the clarity score technique on both of the runs.",
                "This confirms our intuition that the use of a coherence-based measure like the clarity score is inappropriate for NP queries.",
                "Methods Clarity Robust.",
                "WIG FRC WIG+FRC TB05-NP 0.150 -0.370 0.458 0.440 0.525 TB06-NP 0.112 -0.160 0.478 0.386 0.515 Table 6: Pearsons correlation coefficients for correlation with reciprocal ranks on the Terabyte Tracks (NP) for clarity score, robustness score, WIG, the first rank change (FRC) and a linear combination of WIG and FRC.",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Regarding the robustness score, we also tune the parameters and report the best we have found.",
                "We observe an interesting and surprising negative correlation with reciprocal ranks.",
                "We explain this finding briefly.",
                "A high robustness score means that a number of top ranked documents in the original ranked list are still highly ranked after perturbing the documents.",
                "The existence of such documents is a good sign of high performance for content-based queries as these queries usually contain a number of relevant documents [1].",
                "However, with regard to NP queries, one fundamental difference is that there is only one relevant document for each query.",
                "The existence of such documents can confuse the ranking function and lead to low retrieval performance.",
                "Although the negative correlation with retrieval performance exists, the strength of the correlation is weaker and less consistent compared to our methods as shown in Table 6.",
                "Based on the above analysis, we can see that current prediction techniques like clarity score and robustness score that are mainly designed for content-based queries face significant challenges and are inadequate to deal with NP queries.",
                "Our two techniques proposed for NP queries consistently demonstrate good prediction accuracy, displaying initial success in solving the problem of predicting performance for NP queries.",
                "Another point we want to stress is that the WIG method works well for both types of queries, a desirable property that most prediction techniques lack. 4.3 Unknown Query Types In this section, we run two kinds of experiments without access to query type labels.",
                "First, we assume that only one type of query exists but the type is unknown.",
                "Second, we experiment on a mixture of content-based and NP queries.",
                "The following two subsections will report results for the two conditions respectively. 4.3.1 Only One Type exists We assume that all queries are of the same type, that is, they are either NP queries or content-based queries.",
                "We choose WIG to deal with this case because it shows good prediction accuracy for both types of queries in the previous section.",
                "We consider two cases: (1) CB: all 150 title queries from the ad-hoc task of the Terabyte Tracks 2004-2006 (2)NP: all 433 NP queries from the named page finding task of the Terabyte Tracks 2005 and 2006.",
                "We take a simple strategy by labeling all of the queries in each case as the same type (either NP or CB) regardless of their actual type.",
                "The computation of WIG will be based on the labeled query type instead of the actual type.",
                "There are four possibilities with respect to the relation between the actual type and the labeled type.",
                "The correlation with retrieval performance under the four possibilities is presented in Table 7.",
                "For example, the value 0.445 at the intersection between the second row and the third column shows the Pearsons correlation coefficient for correlation with average precision when the content-based queries are incorrectly labeled as the NP type.",
                "Based on these results, we recommend treating all queries as the NP type when only one query type exists and accurate query classification is not feasible, considering the risk that a large loss of accuracy will occur if NP queries are incorrectly labeled as content-based queries.",
                "These results also demonstrate the strong adaptability of WIG to different query types.",
                "CB (labeled) NP (labeled) CB (actual) 0.536 0.445 NP (actual) 0.174 0.467 Table 7: Comparison of Pearsons correlation coefficients for correlation with retrieval performance under four possibilities on the Terabyte Tracks (NP).",
                "Bold cases mean the results are statistically significant at the 0.01 level. 4.3.2 A mixture of contented-based and NP queries A mixture of the two types of queries is a more realistic situation that a Web search engine will meet.",
                "We evaluate prediction accuracy by how accurately poorly-performing queries can be identified by the prediction method assuming that actual query types are unknown (but we can predict query types).",
                "This is a challenging task because both the predicted and actual performance for one type of query can be incomparable to that for the other type.",
                "Next we discuss how to implement our evaluation.",
                "We create a query pool which consists of all of the 150 ad-hoc title queries from Terabyte Track 2004-2006 and all of the 433 NP queries from Terabyte Track 2005&2006.",
                "We divide the queries in the pool into classes: good (better than 50% of the queries of the same type in terms of retrieval performance) and bad (otherwise).",
                "According to these standards, a NP query with the reciprocal rank above 0.2 or a content-based query with the average precision above 0.315 will be considered as good.",
                "Then, each time we randomly select one query Q from the pool with probability p that Q is contented-based.",
                "The remaining queries are used as training data.",
                "We first decide the type of query Q according to a query classifier.",
                "Namely, the query classifier tells us whether query Q is NP or content-based.",
                "Based on the predicted query type and the score computed for query Q by a prediction technique, a binary decision is made about whether query Q is good or bad by comparing to the score threshold of the predicted query type obtained from the training data.",
                "Prediction accuracy is measured by the accuracy of the binary decision.",
                "In our implementation, we repeatedly take a test query from the query pool and prediction accuracy is computed as the percentage of correct decisions, that is, a good(bad) query is predicted to be good (bad).",
                "It is obvious that random guessing will lead to 50% accuracy.",
                "Let us take the WIG method for example to illustrate the process.",
                "Two WIG thresholds (one for NP queries and the other for content-based queries) are trained by maximizing the prediction accuracy on the training data.",
                "When a test query is labeled as the NP (CB) type by the query type classifier, it will be predicted to be good if and only if the WIG score for this query is above the NP (CB) threshold.",
                "Similar procedures will be taken for other prediction techniques.",
                "Now we briefly introduce the automatic query type classifier used in this paper.",
                "We find that the robustness score, though originally proposed for performance prediction, is a good indicator of query types.",
                "We find that on average content-based queries have a much higher robustness score than NP queries.",
                "For example, Figure 2 shows the distributions of robustness scores for NP and content-based queries.",
                "According to this finding, the robustness score classifier will attach a NP (CB) label to the query if the robustness score for the query is below (above) a threshold trained from the training data. 0 0.5 1 1.5 2 2.5 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 NP Content-based Figure 2: Distribution of robustness scores for NP and CB queries.",
                "The NP queries are the 252 NP topics from the 2005 Terabyte Track.",
                "The content-based queries are the 150 ad-hoc title from the Terabyte Tracks 2004-2006.",
                "The probability distributions are estimated by the Kernel density estimation method.",
                "Strategies Robust WIG-1 WIG-2 WIG-3 Optimal p=0.6 0.565 0.624 0.665 0.684 0.701 P=0.4 0.567 0.633 0.654 0.673 0.696 Table 8: Comparison of prediction accuracy for five strategies in the <br>mixed-query situation</br>.",
                "Two ways to sample a query from the pool: (1) the sampled query is content-based with the probability p=0.6. (that is, the query is NP with probability 0.4 ) (2) set the probability p=0.4.",
                "We consider five strategies in our experiments.",
                "In the first strategy (denoted by robust), we use the robustness score for query performance prediction with the help of a perfect query classifier that always correctly map a query into one of the two categories (that is, NP or CB).",
                "This strategy represents the level of prediction accuracy that current prediction techniques can achieve in an ideal condition that query types are known.",
                "In the next following three strategies, the WIG method is adopted for performance prediction.",
                "The difference among the three is that three different query classifiers are used for each strategy: (1) the classifier always classifies a query into the NP type. (2) the Robustness Score ProbabilityDensity classifier is the robust score classifier mentioned above. (3) the classifier is a perfect one.",
                "These three strategies are denoted by WIG-1, WIG-2 and WIG-3 respectively.",
                "The reason we are interested in WIG-1 is based on the results from section 4.3.1.",
                "In the last strategy (denoted by Optimal) which serves as an upper bound on how well we can do so far, we fully make use of our prediction techniques for each query type assuming a perfect query classifier is available.",
                "Specifically, we linearly combine WIG and QF for content-based queries and WIG and FRC for NP queries.",
                "The results for the five strategies are shown in Table 8.",
                "For each strategy, we try two ways to sample a query from the pool: (1) the sampled query is CB with probability p=0.6. (the query is NP with probability 0.4) (2) set the probability p=0.4.",
                "From Table 8 We can see that in terms of prediction accuracy WIG-2 (the WIG method with the automatic query classifier) is not only better than the first two cases, but also is close to WIG-3 where a perfect classifier is assumed.",
                "Some further improvements over WIG-3 are observed when combined with other prediction techniques.",
                "The merit of WIG-2 is that it provides a practical solution to automatically identifying poorly performing queries in a Web search environment with mixed query types, which poses considerable obstacles to traditional prediction techniques. 5.",
                "CONCLUSIONS AND FUTURE WORK To our knowledge, our paper is the first to thoroughly explore prediction of query performance in web search environments.",
                "We demonstrated that our models resulted in higher prediction accuracy than previously published techniques not specially devised for web search scenarios.",
                "In this paper, we focus on two types of queries in web search: content-based and Named-Page (NP) finding queries, corresponding to the ad-hoc retrieval task and the Named-Page finding task respectively.",
                "For both types of web queries, our prediction models were shown to be substantially more accurate than the current state-of-the-art techniques.",
                "Furthermore, we considered a more realistic case that no prior information on query types is available.",
                "We demonstrated that the WIG method is particularly suitable for this situation.",
                "Considering the adaptability of WIG to a range of collections and query types, one of our future plans is to apply this method to predict user preference of search results on realistic data collected from a commercial search engine.",
                "Other than accuracy, another major issue that prediction techniques have to deal with in a Web environment is efficiency.",
                "Fortunately, since the WIG score is computed just over the terms and the phrases that appear in the query, this calculation can be made very efficient with the support of index.",
                "On the other hand, the computation of QF and FRC is relatively less efficient since QF needs to retrieve the whole collection twice and FRC needs to repeatedly rank the perturbed documents.",
                "How to improve the efficiency of QF and FRC is our future work.",
                "In addition, the prediction techniques proposed in this paper have the potential of improving retrieval performance by combining with other IR techniques.",
                "For example, our techniques can be incorporated to popular query modification techniques such as query expansion and query relaxation.",
                "Guided by performance prediction, we can make a better decision on when to or how to modify queries to enhance retrieval effectiveness.",
                "We would like to carry out research in this direction in the future. 6.",
                "ACKNOWLEGMENTS This work was supported in part by the Center for Intelligent Information Retrieval, in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C0023, and in part by an award from Google.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor.",
                "In addition, we thank Donald Metzler for his valuable comments on this work. 7.",
                "REFERENCES [1] Y. Zhou ,W. B. Croft ,Ranking Robustness: A Novel Framework to Predict Query Performance, in Proceedings of CIKM 2006. [2] D.Carmel, E.Yom-Tov, A.Darlow,D.Pelleg, What Makes a Query Difficult?, in Proceedings of SIGIR 2006. [3] C.L.A.",
                "Clarke, F. Scholer, I.Soboroff, The TREC 2005 Terabyte Track, In the Online Proceedings of 2005 TREC. [4] B.",
                "He and I.Ounis.",
                "Inferring query performance using preretrieval predictors.",
                "In proceedings of the SPIRE 2004. [5] S. Tomlinson.",
                "Robust, Web and Terabyte Retrieval with Hummingbird SearchServer at TREC 2004.",
                "In the Online Proceedings of 2004 TREC. [6] S. Cronen-Townsend, Y. Zhou, W. B. Croft, Predicting Query Performance, in Proceedings of SIGIR 2002. [7] V.Vinay, I.J.Cox, N.Mill-Frayling,K.Wood, On Ranking the Effectiveness of Searcher, in Proceedings of SIGIR 2006. [8] D.Metzler, W.B.Croft, A Markov Random Filed Model for Term Dependencies, in Proceedings of SIGIR 2005. [9] D.Metzler, T.Strohman,Y.Zhou,W.B.Croft, Indri at TREC 2005: Terabyte Track, In the Online Proceedings of 2004 TREC. [10] P. Ogilvie and J. Callan, Combining document representations for known-item search, in Proceedings of SIGIR 2003. [11] A.Berger, J.Lafferty, Information retrieval as statistical translation, in Proceedings of SIGIR 1999. [12] Indri search engine : http://www.lemurproject.org/indri/ [13] I.J.",
                "Taneja: On Generalized Information Measures and Their Applications, Advances in Electronics and Electron Physics, Academic Press (USA), 76, 1989, 327-413. [14] S.Cronen-Townsend, Y. Zhou and Croft, W. B. , A Framework for Selective Query Expansion, in Proceedings of CIKM 2004. [15] F.Song, W.B.Croft, A general language model for information retrieval, in Proceedings of SIGIR 1999. [16] Personal email contact with Vishwa Vinay and our own experiments [17] E.Yom-Tov, S.Fine, D.Carmel, A.Darlow, Learning to Estimate Query Difficulty Including Applications to Missing Content Detection and Distributed Information retrieval, in Proceedings of SIGIR 2005"
            ],
            "original_annotated_samples": [
                "To assist prediction under the <br>mixed-query situation</br>, a novel query classifier is adopted.",
                "The <br>mixed-query situation</br> raises new problems for query performance prediction.",
                "Moreover, we demonstrate that good prediction accuracy can be achieved for the <br>mixed-query situation</br> by using WIG with the help of a query type classifier.",
                "Strategies Robust WIG-1 WIG-2 WIG-3 Optimal p=0.6 0.565 0.624 0.665 0.684 0.701 P=0.4 0.567 0.633 0.654 0.673 0.696 Table 8: Comparison of prediction accuracy for five strategies in the <br>mixed-query situation</br>."
            ],
            "translated_annotated_samples": [
                "Para ayudar en la predicción bajo la <br>situación de consulta mixta</br>, se adopta un clasificador de consultas novedoso.",
                "La <br>situación de consulta mixta</br> plantea nuevos problemas para la predicción del rendimiento de la consulta.",
                "Además, demostramos que se puede lograr una buena precisión de predicción para la <br>situación de consulta mixta</br> utilizando WIG con la ayuda de un clasificador de tipo de consulta.",
                "Estrategias Robustas WIG-1 WIG-2 WIG-3 Óptima p=0.6 0.565 0.624 0.665 0.684 0.701 P=0.4 0.567 0.633 0.654 0.673 0.696 Tabla 8: Comparación de la precisión de predicción para cinco estrategias en la <br>situación de consulta mixta</br>."
            ],
            "translated_text": "Predicción del rendimiento de consultas en entornos de búsqueda web Yun Zhou y W. Bruce Croft Departamento de Ciencias de la Computación Universidad de Massachusetts, Amherst {yzhou, croft}@cs.umass.edu RESUMEN Las técnicas actuales de predicción, generalmente diseñadas para consultas basadas en contenido y evaluadas típicamente en colecciones de prueba relativamente homogéneas de tamaños pequeños, enfrentan serios desafíos en entornos de búsqueda web donde las colecciones son significativamente más heterogéneas y existen diferentes tipos de tareas de recuperación. En este artículo, presentamos tres técnicas para abordar estos desafíos. Nos enfocamos en la predicción de rendimiento para dos tipos de consultas en entornos de búsqueda web: basadas en contenido y búsqueda de páginas por nombre. Nuestra evaluación se realiza principalmente en la colección GOV2. Además de evaluar nuestros modelos para los dos tipos de consultas por separado, consideramos una situación más desafiante y realista en la que los dos tipos de consultas se mezclan sin información previa sobre los tipos de consulta. Para ayudar en la predicción bajo la <br>situación de consulta mixta</br>, se adopta un clasificador de consultas novedoso. Los resultados muestran que nuestra predicción del rendimiento de la consulta web es sustancialmente más precisa que las técnicas de predicción actuales más avanzadas. Por consiguiente, nuestro artículo proporciona un enfoque práctico para predecir el rendimiento en entornos web del mundo real. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Formulación de Consultas Términos Generales Algoritmos, Experimentación, Teoría 1. La predicción del rendimiento de la consulta tiene muchas aplicaciones en una variedad de áreas de recuperación de información (IR), como mejorar la consistencia de la recuperación, el refinamiento de la consulta y la IR distribuida. La importancia de este problema ha sido reconocida por los investigadores en Recuperación de Información y recientemente se han propuesto varios métodos nuevos para la predicción [1, 2, 17]. La mayoría de los trabajos sobre predicción se han centrado en la tarea tradicional de recuperación ad-hoc, donde el rendimiento de la consulta se mide según la relevancia temática. Estos modelos de predicción se evalúan en colecciones de documentos de TREC que suelen consistir en no más de un millón de artículos de agencias de noticias relativamente homogéneos. Con la popularidad e influencia de la Web, las técnicas de predicción que funcionen bien para consultas de estilo web son altamente preferibles. Sin embargo, los entornos de búsqueda web plantean desafíos significativos para los modelos de predicción actuales que están principalmente diseñados para entornos tradicionales de TREC. Aquí delineamos algunos de estos desafíos. Primero, las colecciones web, que son mucho más grandes que las colecciones TREC convencionales, incluyen una variedad de documentos que son diferentes en muchos aspectos, como la calidad y el estilo. Las técnicas actuales de predicción pueden ser vulnerables a estas características de las colecciones web. Por ejemplo, la precisión de predicción reportada de la técnica de robustez de clasificación y la técnica de claridad en la colección GOV2 (una colección web grande) es significativamente peor en comparación con las otras colecciones de TREC [1]. En [2] se informa de una precisión de predicción similar en la colección GOV2 utilizando otra técnica, confirmando la dificultad de predecir el rendimiento de consultas en una gran colección web. Además, la búsqueda web va más allá del alcance de la tarea de recuperación ad hoc basada en la relevancia temática. Por ejemplo, la tarea de encontrar páginas nombradas (NP), que es una tarea de navegación, también es popular en la recuperación web. La predicción del rendimiento de la consulta para la tarea de NP sigue siendo necesaria, ya que el rendimiento de recuperación de NP está lejos de ser perfecto. De hecho, según el informe sobre la tarea NP del Terabyte Track de 2005 [3], aproximadamente el 40% de las consultas de prueba tienen un rendimiento deficiente (sin respuesta correcta en los primeros 10 resultados de búsqueda) incluso en la mejor ejecución del grupo líder. Hasta donde sabemos, poca investigación ha abordado explícitamente el problema de la predicción del rendimiento de consultas de NP. Los modelos de predicción actuales diseñados para consultas basadas en contenido serán menos efectivos para consultas de NP considerando las diferencias fundamentales entre ambas. En tercer lugar, en entornos de búsqueda web del mundo real, las consultas de los usuarios suelen ser una mezcla de diferentes tipos y, por lo general, no se dispone de información previa sobre el tipo de cada consulta. La <br>situación de consulta mixta</br> plantea nuevos problemas para la predicción del rendimiento de la consulta. Por ejemplo, es posible que necesitemos incorporar un clasificador de consultas en los modelos de predicción. A pesar de estos problemas, la capacidad para manejar esta situación es un paso crucial hacia convertir la predicción del rendimiento de consultas de un tema de investigación interesante en una herramienta práctica para la recuperación web. En este artículo, presentamos tres técnicas para abordar los desafíos mencionados que enfrentan los modelos de predicción actuales en entornos de búsqueda web. Nuestro trabajo se centra en la predicción del rendimiento de consultas para la tarea de recuperación basada en contenido (ad-hoc) y la tarea de encontrar páginas por nombre en el contexto de la recuperación web. Nuestra primera técnica, llamada ganancia de información ponderada (WIG), utiliza tanto características de términos individuales como de proximidad de términos para estimar la calidad de los documentos recuperados para la predicción. Observamos que WIG ofrece una precisión de predicción consistente en diversas colecciones de pruebas y tipos de consultas. Además, demostramos que se puede lograr una buena precisión de predicción para la <br>situación de consulta mixta</br> utilizando WIG con la ayuda de un clasificador de tipo de consulta. La retroalimentación de consultas y el cambio de rango inicial, que son nuestras técnicas de predicción segunda y tercera, funcionan bien para consultas basadas en contenido y consultas de NP respectivamente. Nuestras principales contribuciones incluyen: (1) una precisión de predicción considerablemente mejorada para consultas basadas en contenido web en comparación con varias técnicas de vanguardia. (2) nuevas técnicas para predecir con éxito el rendimiento de consultas NP. (3) una solución práctica y totalmente automática para predecir el rendimiento de consultas mixtas. Además, una contribución menor es que encontramos que el puntaje de robustez [1], que originalmente se propuso para la predicción de rendimiento, es útil para la clasificación de consultas. 2. TRABAJO RELACIONADO Como mencionamos en la introducción, recientemente se han propuesto varias técnicas de predicción que se centran en consultas basadas en contenido en la tarea de relevancia temática (ad-hoc). No conocemos ningún trabajo publicado que aborde otros tipos de consultas como las consultas NP, y mucho menos una mezcla de tipos de consultas. A continuación revisamos algunos modelos representativos. La principal dificultad de la predicción del rendimiento proviene del hecho de que muchos factores tienen un impacto en el rendimiento de recuperación. Cada factor afecta el rendimiento en diferente medida y el efecto general es difícil de predecir con precisión. Por lo tanto, no es sorprendente notar que características simples, como la frecuencia de los términos de consulta en la colección [4] y la IDF promedio de los términos de consulta [5], no predicen bien. De hecho, la mayoría de las técnicas exitosas se basan en medir algunas características del conjunto de documentos recuperados para estimar la dificultad del tema. Por ejemplo, la puntuación de claridad [6] mide la coherencia de una lista de documentos mediante la divergencia KL entre el modelo de consulta y el modelo de colección. El puntaje de robustez [1] cuantifica otra propiedad de una lista clasificada: la robustez de la clasificación en presencia de incertidumbre. Carmel et al. [2] encontraron que la distancia medida por la divergencia de Jensen-Shannon entre el conjunto de documentos recuperados y la colección está significativamente correlacionada con la precisión promedio. Vinay et al. [7] propusieron cuatro medidas para capturar la geometría de los documentos mejor clasificados para la predicción. La medida más efectiva es la sensibilidad a la perturbación del documento, una idea algo similar a la puntuación de robustez. Desafortunadamente, su forma de medir la sensibilidad no funciona igual de bien para consultas cortas y la precisión de la predicción disminuye considerablemente cuando se adopta una técnica de recuperación de vanguardia (como Okapi o un enfoque de modelado de lenguaje) en lugar del peso tf-idf utilizado en su artículo [16]. Las dificultades de aplicar estos modelos en entornos de búsqueda web ya han sido mencionadas. En este documento, principalmente adoptamos la puntuación de claridad y la puntuación de robustez como nuestras líneas base. Experimentalmente demostramos que las líneas base, incluso después de ser ajustadas cuidadosamente, son inadecuadas para el entorno web. Uno de nuestros modelos de predicción, WIG, está relacionado con el modelo de campo aleatorio de Markov (MRF) para la recuperación de información [8]. El modelo MRF modela directamente la dependencia de términos y se ha demostrado ser altamente efectivo en una variedad de colecciones de pruebas (especialmente colecciones web) y tareas de recuperación. Este modelo se utiliza para estimar la distribución conjunta de probabilidad sobre documentos y consultas, una parte importante de WIG. La superioridad de WIG sobre otras técnicas de predicción basadas en características unigramas, que se demostrará más adelante en nuestro artículo, coincide con la de MRF para recuperación. En otras palabras, es interesante notar que la dependencia de términos, cuando se modela adecuadamente, puede ser útil tanto para mejorar como para predecir el rendimiento de recuperación. 3. Modelos de predicción 3.1 Ganancia de Información Ponderada (WIG) Esta sección introduce un enfoque de ganancia de información ponderada que incorpora tanto características de términos únicos como de proximidad para predecir el rendimiento tanto en consultas basadas en contenido como en consultas de búsqueda de Páginas Nombradas (NP). Dado un conjunto de consultas Q={Qs} (s=1,2,..N) que incluye todas las posibles consultas de usuario y un conjunto de documentos D={Dt} (t=1,2…M), asumimos que cada par consulta-documento (Qs,Dt) es evaluado manualmente y se incluirá en una lista de relevancia si se determina que Qs es relevante para Dt. La probabilidad conjunta P(Qs,Dt) sobre las consultas Q y los documentos D denota la probabilidad de que el par (Qs,Dt) esté en la lista de relevancia. Tales suposiciones son similares a las utilizadas en [8]. Suponiendo que el usuario emite la consulta Qi ∈Q y los resultados de recuperación en respuesta a Qi es una lista clasificada L de documentos, calculamos la cantidad de información contenida en P(Qs,Dt) con respecto a Qi y L mediante la Ec.1, que es una variante de la entropía llamada entropía ponderada[13]. Los pesos en la ecuación 1 están determinados únicamente por Qi y L. En este documento, elegimos los pesos de la siguiente manera: L es el número de documentos que contiene la consulta, K es el límite superior, LT es el límite superior de documentos, y D es el peso si K está en otro caso. La clasificación de corte K es un parámetro en nuestro modelo que se discutirá más adelante. Por lo tanto, la ecuación 1 se puede simplificar de la siguiente manera: )3(),(log 1 ),( )( , ∑∈ −= LTD titsLQ Kt i DQP K DQH Desafortunadamente, la entropía ponderada ),(, tsLQ DQH i calculada por la ecuación 3, que representa la cantidad de información sobre la probabilidad de que los documentos mejor clasificados en L sean relevantes para la consulta Qi en promedio, no se puede comparar entre diferentes consultas, lo que la hace inapropiada para predecir directamente el rendimiento de la consulta. Para mitigar este problema, creamos una distribución de fondo P(Qs,C) sobre Q y D imaginando que cada documento en D es reemplazado por el mismo documento especial C que representa el uso promedio del lenguaje. En este documento, C se crea concatenando cada documento en D. A grosso modo, C es la colección (el conjunto de documentos) {Dt} sin límites de documentos. De manera similar, la entropía ponderada ),(, CQH sLQi calculada por la Ecuación 3 representa la cantidad de información sobre la probabilidad de que un documento promedio (representado por toda la colección) sea relevante para la consulta Qi. Ahora presentamos nuestro predictor de rendimiento WIG que es la ganancia de información ponderada [13] calculada como la diferencia entre ),(, tsLQ DQH i y ),(, CQH sLQi. Específicamente, dado el conjunto de consultas Qi, la colección C y la lista clasificada L de documentos, WIG se calcula de la siguiente manera: )4( ),( ),( log 1 ),( ),( log),( ),(),(),,( )(, ,, ∑∑ ∈ == −= LTD i ti ts s ts ts tsLQsLQi Kt ii CQP DQP KCQP DQP DQweight DQHCQHLCQWIG WIG calculado por la Ecuación 4 mide el cambio en la información sobre la calidad de la recuperación (en respuesta a la consulta Qi) desde un estado imaginario en el que solo se recupera un documento promedio hasta un estado posterior en el que se observan los resultados de búsqueda reales. Hacemos la hipótesis de que WIG está positivamente correlacionado con la efectividad de la recuperación porque una recuperación de alta calidad debería ser mucho más efectiva que simplemente devolver el documento promedio. El corazón de esta técnica es cómo estimar la distribución conjunta P(Qs, Dt). En el enfoque de modelado del lenguaje para la recuperación de información, se pueden aplicar fácilmente una variedad de modelos para estimar esta distribución. Aunque la mayoría de estos modelos se basan en la suposición de la bolsa de palabras, trabajos recientes sobre modelado de dependencia de términos bajo el marco de modelado de lenguaje han demostrado mejoras consistentes y significativas en la efectividad de recuperación sobre los modelos de bolsa de palabras. Inspirados por el éxito de incorporar características de proximidad de términos en modelos de lenguaje, decidimos adoptar un buen modelo de dependencia para estimar la probabilidad P(Qs,Dt). El modelo que elegimos para este artículo es el modelo de Campo Aleatorio de Markov (MRF) de Metzler y Crofts, que ya ha demostrado su superioridad en comparación con varias colecciones y diferentes tareas de recuperación [8,9]. Según el modelo MRF, log P(Qi, Dt) se puede escribir como )5()|(loglog),(log )( 1 ∑∈ +−= iQF tti DPZDQP ξ ξ ξλ donde Z1 es una constante que asegura que P(Qi, Dt) sume 1. F(Qi) consiste en un conjunto de características ampliadas a partir de la consulta original Qi. Por ejemplo, suponiendo que la consulta Qi es el programa de estudiantes talentosos, F(Qi) incluye características como programa y estudiante talentoso. Consideramos dos tipos de características: características de términos individuales T y características de proximidad P. Las características de proximidad incluyen la frase exacta (#1) y las características de ventana desordenada (#uwN) como se describe en [8]. Ten en cuenta que F(Qi) es la unión de T(Qi) y P(Qi). Para obtener más detalles sobre F(Qi), como por ejemplo cómo expandir la consulta original Qi a F(Qi), remitimos al lector a [8] y [9]. P(ξ|Dt) denota la probabilidad de que la característica ξ ocurra en Dt. Más detalles sobre P(ξ|Dt) se proporcionarán más adelante en esta sección. La elección de λξ es algo diferente a la utilizada en [8] ya que λξ desempeña un doble papel en nuestro modelo. El primer rol, que es el mismo que en [8], es ponderar entre las características de términos individuales y de proximidad. El otro rol, específico de nuestra tarea de predicción, es normalizar el tamaño de F(Qi). Encontramos que la siguiente estrategia de peso para λξ satisface los dos roles anteriores y generaliza bien en una variedad de colecciones y tipos de consultas. )6( )(, |)(| 1 )(, |)(| ⎪ ⎪ ⎩ ⎪ ⎪ ⎨ ⎧ ∈ − ∈ = i i T i i T QP QP QT QT ξ λ ξ λ λξ donde |T(Qi)| y |P(Qi)| denotan el número de características de términos únicos y de proximidad en F(Qi) respectivamente. La razón de elegir la función raíz cuadrada en el denominador de λξ es penalizar adecuadamente un conjunto de características de gran tamaño, haciendo que WIG sea más comparable entre consultas de diversas longitudes. λT es un parámetro fijo y se establece en 0.8 según [8] a lo largo de este documento. De manera similar, log P(Qi,C) se puede escribir como: )7()|(loglog),(log )( 2 ∑∈ +−= iQF i CPZCQP ξ ξ ξλ Cuando se eliminan las constantes Z1 y Z2, el WIG calculado en la Ecuación 4 se puede reescribir de la siguiente manera al sustituir la Ecuación 5 y la Ecuación 7: )8( )|( )|( log 1 ),,( )( )( ∑ ∑∈ ∈ = LTD QF t i Kt i CP DP K LCQWIG ξ ξ ξ ξ λ Una de las ventajas del WIG sobre otras técnicas es que puede manejar tanto consultas basadas en contenido como en NP. Basándose en el tipo (o el tipo predicho) de Qi, el cálculo de WIG en la Ecuación 8 difiere en dos aspectos: (1) cómo estimar P(ξ|Dt) y P(ξ|C), y (2) cómo elegir K. Para consultas basadas en contenido, P(ξ|C) se estima por la frecuencia relativa del rasgo ξ en la colección C en su totalidad. La estimación de P(ξ|Dt) es la misma que en [8]. Es decir, estimamos P(ξ|Dt) mediante la frecuencia relativa del rasgo ξ en Dt suavizada linealmente con la frecuencia de colección P(ξ|C). La K en la Ec.8 se trata como un parámetro libre. Ten en cuenta que K es el único parámetro libre en el cálculo de WIG para consultas basadas en contenido, ya que se asume que todos los parámetros involucrados en P(ξ|Dt) están fijos al tomar los valores sugeridos en [8]. En cuanto a las consultas de NP, hacemos uso de la estructura del documento para estimar P(ξ|Dt) y P(ξ|C) mediante el llamado modelo de mezcla de lenguaje propuesto en [10] e incorporado en el modelo MRF para la recuperación de Named-Page en [9]. La idea básica es que un documento (colección) se divide en varios campos como el campo del título, el campo del cuerpo principal y el campo de los encabezados. P(ξ|Dt) y P(ξ|C) se estiman mediante una combinación lineal de los modelos de lenguaje de cada campo. Debido a limitaciones de espacio, remitimos al lector a [9] para más detalles. Adoptamos el mismo conjunto exacto de parámetros utilizados en [9] para la estimación. En cuanto a K en la Ec.8, establecemos K en 1 porque la tarea de encontrar Named-Page se centra fuertemente en el documento clasificado en primer lugar. Por consiguiente, no hay parámetros libres en el cálculo de WIG para consultas de NP. 3.2 Retroalimentación de consultas En esta sección, presentamos otra técnica llamada retroalimentación de consultas (QF) para la predicción. Suponga que un usuario emite la consulta Q a un sistema de recuperación y se devuelve una lista clasificada L de documentos. Vemos el sistema de recuperación como un canal ruidoso. Específicamente, asumimos que la salida del canal es L y la entrada es Q. Después de pasar por el canal, Q se corrompe y se transforma en la lista clasificada L. Al pensar en el proceso de recuperación de esta manera, el problema de predecir la eficacia de la recuperación se convierte en la tarea de evaluar la calidad del canal. En otras palabras, la predicción se convierte en encontrar una forma de medir el grado de corrupción que surge cuando Q se transforma en L. Dado que calcular directamente el grado de corrupción es difícil, abordamos este problema mediante aproximaciones. Nuestra idea principal es que medimos en qué medida la información sobre Q puede ser recuperada de L bajo la suposición de que solo se observa L. Específicamente, diseñamos un decodificador que pueda traducir con precisión L de vuelta a la nueva consulta Q y la similitud S entre la consulta original Q y la nueva consulta Q se adopta como un predictor de rendimiento. Este es un bosquejo de cómo la técnica QF predice el rendimiento de la consulta. Antes de completar más detalles, discutimos brevemente por qué este método funcionaría. Existe una relación entre la similitud S definida anteriormente y el rendimiento de recuperación. Por un lado, si la recuperación se ha desviado del sentido original de la consulta Q, la nueva consulta Q extraída de la lista clasificada L en respuesta a Q sería muy diferente de la consulta original Q. Por otro lado, una consulta destilada de una lista clasificada que contiene muchos documentos relevantes es probable que sea similar a la consulta original. Se proporcionarán más ejemplos en apoyo de la relación más adelante. A continuación detallamos cómo construir el decodificador y cómo medir la similitud S. En esencia, el objetivo del decodificador es comprimir la lista clasificada L en unos pocos términos informativos que deberían representar el contenido de los documentos mejor clasificados en L. Nuestro enfoque para este objetivo es representar la lista clasificada L mediante un modelo de lenguaje (distribución de términos). Luego, los términos se clasifican según su contribución a la divergencia KL (Kullback-Leibler) de los modelos de lenguaje con respecto al modelo de la colección de fondo. Los términos mejor clasificados serán elegidos para formar la nueva consulta Q. Este enfoque es similar al utilizado en la Sección 4.1 de [11]. Específicamente, tomamos tres pasos para comprimir la lista clasificada L en la consulta Q sin hacer referencia a la consulta original. 1. Adoptamos el modelo de lenguaje de lista clasificada [14] para estimar un modelo de lenguaje basado en la lista clasificada L. El modelo puede escribirse como: )9()|()|()|( ∑∈ = LD LDPDwPLwP donde w es cualquier término y D es un documento. La probabilidad de que ocurra el evento D dado el evento L se estima mediante una función linealmente decreciente del rango del documento D. Cada término en P(w|L) está clasificado por la siguiente contribución de divergencia de Kullback-Leibler: )10( )|( )|( log)|( CwP LwP LwP donde P(w|C) es el modelo de colección estimado por la frecuencia relativa del término w en la colección C en su totalidad. 3. Los términos N principales clasificados por Eq.10 forman una consulta ponderada Q={(wi,ti)} i=1,N, donde wi denota el término clasificado en la posición i y el peso ti es la contribución de la divergencia KL de wi en Eq. 10. Barco de crucero daño vida marina. Estos ejemplos indican cómo la similitud entre la consulta original y la nueva se correlaciona con el rendimiento de recuperación. El parámetro N en el paso 3 se establece en 20 de manera empírica y elegir un valor más grande de N es innecesario ya que los pesos después de los primeros 20 suelen ser demasiado pequeños para marcar alguna diferencia. Para medir la similitud entre la consulta original Q y la nueva consulta Q, primero utilizamos Q para recuperar información de la misma colección. Se adopta una variante del modelo de probabilidad de consulta [15] para la recuperación. Es decir, los documentos se clasifican por: )11()|()|( ),( ∑∈ = Qtw t i ii i DwPDQP donde wi es un término en Q y ti es el peso asociado. D es un documento. Que L denote la nueva lista clasificada devuelta desde la recuperación anterior. La similitud se mide por la superposición de documentos en L y L. Específicamente, el porcentaje de documentos en los primeros K documentos de L que también están presentes en los primeros K documentos en L. El valor de corte K se trata como un parámetro libre. Aquí resumimos cómo la técnica QF predice el rendimiento dado una consulta Q y la lista clasificada asociada L. Primero obtenemos una consulta ponderada Q comprimida a partir de L mediante los tres pasos anteriores. Luego usamos Q para realizar la recuperación y la nueva lista clasificada es L. La superposición de documentos en L y L se utiliza para la predicción. 3.3 Primer Cambio de Rango (FRC) En esta sección, proponemos un método llamado primer cambio de rango (FRC) para la predicción de rendimiento para consultas de NP. Este método se deriva de la técnica de robustez de clasificación [1] que está principalmente diseñada para consultas basadas en contenido. Cuando se aplica directamente a consultas de NP, la técnica de robustez será menos efectiva porque tiene en cuenta todos los documentos mejor clasificados en su totalidad, mientras que las consultas de NP suelen tener solo un único documento relevante. En cambio, nuestra técnica se centra en el documento de rango uno mientras se mantiene la idea principal del método de robustez. Específicamente, el seudocódigo para calcular FRC se muestra en la figura 1. Lista clasificada L={Di} donde i=1,100. Di denota el documento clasificado en la posición i. (2) consulta Q 1 inicializar: (1) establecer el número de intentos J=100000 (2) contador c=0; 2 para i=1 a J 3 Perturbar cada documento en L, que el resultado sea un conjunto F={Di} donde Di denota la versión perturbada de Di. 4 Realizar la recuperación con la consulta Q en el conjunto F 5 c=c+1 si y solo si D1 está clasificado primero en el paso 4 6 fin del para 7 devolver la proporción c/J Figura 1: pseudo-código para calcular FRC FRC aproxima la probabilidad de que el documento clasificado en primer lugar en la lista original L permanezca clasificado primero incluso después de que los documentos sean perturbados. Cuanto mayor sea la probabilidad, más confianza tenemos en el documento clasificado en primer lugar. Por otro lado, en el caso extremo de un ranking aleatorio, la probabilidad sería tan baja como 0.5. Esperamos que FRC tenga una asociación positiva con el rendimiento de la consulta de NP. Adoptamos [1] para implementar el paso de perturbación del documento (paso 4 en la Fig. 1) utilizando distribuciones de Poisson. Para más detalles, remitimos al lector a [1]. 4. EVALUACIÓN Ahora presentamos los resultados de predecir el rendimiento de la consulta por nuestros modelos. Tres técnicas de vanguardia son adoptadas como nuestras líneas base. Evaluamos nuestras técnicas en una variedad de configuraciones de recuperación web. Como se mencionó anteriormente, consideramos dos tipos de consultas, es decir, consultas basadas en contenido (CB) y consultas de búsqueda de páginas con nombre (NP). Primero, supongamos que se conocen los tipos de consultas. Investigamos la correlación entre el rendimiento de recuperación predicho y el rendimiento real para ambos tipos de consultas por separado. Los resultados muestran que nuestros métodos producen mejoras considerablemente superiores a los valores de referencia. Luego consideramos un escenario más desafiante donde no hay información previa sobre los tipos de consultas disponibles. Se consideran dos subcasos. En el primero, existe solo un tipo de consulta pero el tipo real es desconocido. Suponemos una mezcla de los dos tipos de consultas en el segundo caso. Demostramos que nuestros modelos logran una buena precisión en este escenario exigente, lo que hace que la predicción sea práctica en un entorno de búsqueda web del mundo real. 4.1 Configuración Experimental Nuestra evaluación se centra en la colección GOV2 que contiene alrededor de 25 millones de documentos obtenidos de sitios web en el dominio .gov durante 2004 [3]. Creamos dos tipos de conjuntos de datos para consultas de CB y consultas de NP respectivamente. Para el tipo CB, utilizamos los temas ad-hoc de las pistas Terabyte de 2004, 2005 y 2006 y los nombramos TB04-adhoc, TB05-adhoc y TB06-adhoc respectivamente. Además, también utilizamos los temas ad-hoc de la pista Robusta de 2004 (RT04) para probar la adaptabilidad de nuestras técnicas a un entorno no web. Para consultas de NP, utilizamos los temas de búsqueda de páginas nombradas de las Terabyte Tracks de 2005 y 2006 y los denominamos TB05-NP y TB06-NP respectivamente. Todas las consultas utilizadas en nuestros experimentos son títulos de temas de TREC, ya que nos centramos en la recuperación web. La Tabla 3 resume los conjuntos de datos anteriores. La recuperación del rendimiento de las consultas individuales basadas en contenido y NP se mide mediante la precisión promedio y la tasa recíproca de la primera respuesta correcta, respectivamente. Hacemos uso del modelo de campo aleatorio de Markov tanto para la recuperación ad-hoc como para la recuperación de páginas nombradas. Adoptamos la misma configuración de parámetros de recuperación utilizada en [8,9]. El motor de búsqueda Indri [12] se utiliza para todos nuestros experimentos. Aunque no se informa aquí, también probamos el modelo de probabilidad de consulta para la recuperación ad-hoc y encontramos que los resultados cambian poco debido a la correlación muy alta entre el rendimiento de las consultas obtenido por los dos modelos de recuperación (0.96 medido por el coeficiente de Pearson). 4.2 Tipos de Consultas Conocidos Supongamos que se conocen los tipos de consultas. Tratamos cada tipo de consulta por separado y medimos la correlación con la precisión promedio (o el rango recíproco en el caso de consultas NP). Adoptamos la prueba de correlación de Pearson que refleja el grado de relación lineal entre el rendimiento de recuperación predicho y real. 4.2.1 Métodos de Consultas Basadas en Contenido Claridad Robusta JSD WIG QF WIG +QF TB04+0 5 adhoc 0.333 0.317 0.362 0.574 0.480 0.637 TB06 adhoc 0.076 0.294 N/A 0.464 0.422 0.511 Tabla 4: Coeficientes de correlación de Pearson para la correlación con la precisión promedio en las Pistas de Terabyte (ad-hoc) para la puntuación de claridad, puntuación de robustez, el método basado en JSD (citamos directamente la puntuación informada en [2]), WIG, retroalimentación de consulta (QF) y una combinación lineal de WIG y QF. Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. La Tabla 4 muestra la correlación con la precisión promedio en dos conjuntos de datos: uno es una combinación de TB04-adhoc y TB05-adhoc (100 temas en total) y el otro es TB06-adhoc (50 temas). La razón por la que juntamos TB04-adhoc y TB05-adhoc es para hacer nuestros resultados comparables a [2]. Nuestros puntos de referencia son la puntuación de claridad (claridad) [6], la puntuación de robustez (robust) [1] y el método basado en JSD (JSD) [2]. Para la puntuación de claridad y robustez, hemos probado diferentes configuraciones de parámetros y reportamos los coeficientes de correlación más altos que hemos encontrado. Citamos directamente el resultado del método basado en JSD reportado en [2]. La tabla también muestra los resultados para el método de Ganancia de Información Ponderada (WIG) y el método de Retroalimentación de Consulta (QF) para predecir consultas basadas en contenido. Como describimos en la sección anterior, tanto WIG como QF tienen un parámetro libre para ajustar, es decir, el rango de corte K. Entrenamos el parámetro en un conjunto de datos y lo probamos en el otro. Al combinar WIG y QF, se utiliza una combinación lineal simple y el peso de la combinación se aprende a partir del conjunto de datos de entrenamiento. A partir de estos resultados, podemos ver que nuestros métodos son considerablemente más precisos en comparación con los baselines. También observamos que se obtienen mejoras adicionales a partir de la combinación de WIG y QF, lo que sugiere que miden diferentes propiedades del proceso de recuperación que se relacionan con el rendimiento. Descubrimos que nuestros métodos se generalizan bien en TB06-adhoc, mientras que la correlación del puntaje de claridad con el rendimiento de recuperación en este conjunto de datos es considerablemente peor. Una investigación adicional muestra que la precisión promedio media de TB06-adhoc es de 0.342 y es aproximadamente un 10% mejor que la del primer conjunto de datos. Mientras que los otros tres métodos suelen considerar los 100 mejores documentos o menos de una lista clasificada, el método de claridad generalmente necesita los 500 mejores documentos o más para medir adecuadamente la coherencia de una lista clasificada. Un promedio de precisión media más alto hace que las listas clasificadas recuperadas por diferentes consultas sean más similares en términos de coherencia en el nivel de los primeros 500 documentos. Creemos que esta es la razón principal de la baja precisión de la puntuación de claridad en el segundo conjunto de datos. Aunque este documento se centra en un entorno de búsqueda web, es deseable que nuestras técnicas funcionen de manera consistente en otras situaciones. Con este fin, examinamos la efectividad de nuestras técnicas en la pista Robust 2004. Para nuestros métodos, dividimos equitativamente todas las consultas de prueba en cinco grupos y realizamos validación cruzada de cinco pliegues. Cada vez que utilizamos un grupo para el entrenamiento y los otros cuatro grupos para las pruebas. Hacemos uso de todas las consultas para nuestros dos puntos de referencia, es decir, la puntuación de claridad y la puntuación de robustez. Los parámetros para nuestras líneas base son los mismos que los utilizados en [1]. Los resultados mostrados en la Tabla 5 demuestran que la precisión de predicción de nuestros métodos está a la par con la de las dos líneas base sólidas. Claridad Robusta WIG QF 0.464 0.539 0.468 0.464 Tabla 5: Comparación de los coeficientes de correlación de Pearson en la pista Robusta de 2004 para la puntuación de claridad, puntuación de robustez, WIG y retroalimentación de consulta (QF). Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. Además, examinamos la sensibilidad de predicción de nuestros métodos al rango de corte K. Con respecto a WIG, es bastante robusto a K en las Pistas de Terabyte (2004-2006) mientras prefiere un valor pequeño de K como 5 en la Pista Robusta de 2004. En otras palabras, un valor pequeño de K es una elección casi óptima para ambos tipos de pistas. Teniendo en cuenta que todos los demás parámetros involucrados en WIG están fijos y, consecuentemente, son los mismos para los dos casos, esto significa que WIG puede lograr una precisión de predicción casi óptima en dos situaciones considerablemente diferentes con exactamente la misma configuración de parámetros. En cuanto a QF, prefiere un valor más grande de K, como 100 en las Pistas de Terabyte y un valor más pequeño de K, como 25 en la Pista Robusta de 2004. 4.2.2 Consultas NP Adoptamos WIG y el primer cambio de rango (FRC) para predecir el rendimiento de las consultas NP. También intentamos una combinación lineal de los dos como en la sección anterior. El peso de la combinación se obtiene del otro conjunto de datos. Utilizamos la correlación con los rangos recíprocos medidos por la prueba de correlación de Pearson para evaluar la calidad de la predicción. Los resultados se presentan en la Tabla 6. Nuevamente, nuestros puntos de referencia son la puntuación de claridad y la puntuación de robustez. Para hacer una comparación justa, ajustamos la puntuación de claridad de diferentes maneras. Descubrimos que utilizar el documento clasificado en primer lugar para construir el modelo de consulta produce la mejor precisión de predicción. También intentamos utilizar la estructura del documento mediante la combinación de modelos de lenguaje mencionados en la sección 3.1. Se obtuvo una pequeña mejora. Los coeficientes de correlación para la puntuación de claridad reportados en la Tabla 6 son los mejores que hemos encontrado. Como podemos ver, nuestros métodos superan considerablemente la técnica de puntuación de claridad en ambas ejecuciones. Esto confirma nuestra intuición de que el uso de una medida basada en la coherencia como el puntaje de claridad es inapropiado para las consultas de NP. Claridad de métodos robusta. Tabla 6: Coeficientes de correlación de Pearson para la correlación con rangos recíprocos en las pistas de Terabyte (NP) para la puntuación de claridad, puntuación de robustez, WIG, el primer cambio de rango (FRC) y una combinación lineal de WIG y FRC. Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. En cuanto a la puntuación de robustez, también ajustamos los parámetros y reportamos el mejor que hemos encontrado. Observamos una correlación negativa interesante y sorprendente con los rangos recíprocos. Explicamos este hallazgo brevemente. Un puntaje de robustez alto significa que varios documentos de alta clasificación en la lista clasificada original siguen estando muy bien clasificados después de perturbar los documentos. La existencia de tales documentos es una buena señal de alto rendimiento para consultas basadas en contenido, ya que estas consultas suelen contener una serie de documentos relevantes [1]. Sin embargo, en lo que respecta a las consultas de NP, una diferencia fundamental es que solo hay un documento relevante para cada consulta. La existencia de tales documentos puede confundir la función de clasificación y llevar a un bajo rendimiento de recuperación. Aunque existe una correlación negativa con el rendimiento de recuperación, la fuerza de la correlación es más débil y menos consistente en comparación con nuestros métodos, como se muestra en la Tabla 6. Basándonos en el análisis anterior, podemos ver que las técnicas de predicción actuales como la puntuación de claridad y la puntuación de robustez, que están principalmente diseñadas para consultas basadas en contenido, enfrentan desafíos significativos y son insuficientes para manejar consultas de NP. Nuestras dos técnicas propuestas para consultas de NP demuestran consistentemente una buena precisión de predicción, mostrando un éxito inicial en la resolución del problema de predecir el rendimiento para consultas de NP. Otro punto que queremos destacar es que el método WIG funciona bien para ambos tipos de consultas, una propiedad deseable que la mayoría de las técnicas de predicción carecen. 4.3 Tipos de Consultas Desconocidos En esta sección, realizamos dos tipos de experimentos sin acceso a etiquetas de tipo de consulta. Primero, asumimos que solo existe un tipo de consulta pero el tipo es desconocido. Segundo, experimentamos con una mezcla de consultas basadas en contenido y NP. Las dos subsecciones siguientes informarán los resultados para las dos condiciones respectivamente. 4.3.1 Solo existe un tipo. Suponemos que todas las consultas son del mismo tipo, es decir, son consultas NP o consultas basadas en contenido. Elegimos WIG para tratar este caso porque muestra una buena precisión de predicción para ambos tipos de consultas en la sección anterior. Consideramos dos casos: (1) CB: todas las 150 consultas de títulos de la tarea ad-hoc de las pistas Terabyte 2004-2006 (2) NP: todas las 433 consultas de NP de la tarea de búsqueda de páginas nombradas de las pistas Terabyte 2005 y 2006. Tomamos una estrategia simple etiquetando todas las consultas en cada caso como el mismo tipo (ya sea NP o CB) independientemente de su tipo real. El cálculo de WIG se basará en el tipo de consulta etiquetado en lugar del tipo real. Hay cuatro posibilidades con respecto a la relación entre el tipo real y el tipo etiquetado. La correlación con el rendimiento de recuperación bajo las cuatro posibilidades se presenta en la Tabla 7. Por ejemplo, el valor 0.445 en la intersección entre la segunda fila y la tercera columna muestra el coeficiente de correlación de Pearson para la correlación con la precisión promedio cuando las consultas basadas en contenido se etiquetan incorrectamente como del tipo NP. Basándonos en estos resultados, recomendamos tratar todas las consultas como del tipo NP cuando solo existe un tipo de consulta y la clasificación precisa de la consulta no es factible, considerando el riesgo de que se produzca una gran pérdida de precisión si las consultas NP se etiquetan incorrectamente como consultas basadas en contenido. Estos resultados también demuestran la fuerte adaptabilidad de WIG a diferentes tipos de consultas. Tabla 7: Comparación de los coeficientes de correlación de Pearson para la correlación con el rendimiento de recuperación en cuatro posibilidades en las pistas de Terabyte (NP). Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. 4.3.2 Una mezcla de consultas basadas en contenido y NP Una mezcla de los dos tipos de consultas es una situación más realista que un motor de búsqueda web encontrará. Evaluamos la precisión de la predicción por la exactitud con la que se pueden identificar las consultas de bajo rendimiento mediante el método de predicción, asumiendo que los tipos de consulta reales son desconocidos (pero podemos predecir los tipos de consulta). Esta es una tarea desafiante porque tanto el rendimiento predicho como el real para un tipo de consulta pueden ser incomparables con el de otro tipo. A continuación discutimos cómo implementar nuestra evaluación. Creamos un conjunto de consultas que consiste en todas las 150 consultas de título ad-hoc de Terabyte Track 2004-2006 y todas las 433 consultas NP de Terabyte Track 2005&2006. Dividimos las consultas en el conjunto en clases: buenas (mejores que el 50% de las consultas del mismo tipo en términos de rendimiento de recuperación) y malas (de lo contrario). Según estos estándares, una consulta de NP con un rango recíproco superior a 0.2 o una consulta basada en contenido con una precisión promedio superior a 0.315 se considerarán como buenas. Entonces, cada vez que seleccionamos aleatoriamente una consulta Q del grupo con una probabilidad p de que Q esté basada en el contenido. Las consultas restantes se utilizan como datos de entrenamiento. Primero decidimos el tipo de consulta Q de acuerdo con un clasificador de consultas. Es decir, el clasificador de consultas nos dice si la consulta Q es de tipo NP o basada en contenido. Basándose en el tipo de consulta predicho y en la puntuación calculada para la consulta Q por una técnica de predicción, se toma una decisión binaria sobre si la consulta Q es buena o mala al compararla con el umbral de puntuación del tipo de consulta predicho obtenido de los datos de entrenamiento. La precisión de la predicción se mide por la exactitud de la decisión binaria. En nuestra implementación, tomamos repetidamente una consulta de prueba del conjunto de consultas y la precisión de la predicción se calcula como el porcentaje de decisiones correctas, es decir, una consulta buena (mala) se predice como buena (mala). Es obvio que adivinar al azar llevará a una precisión del 50%. Tomemos el método WIG como ejemplo para ilustrar el proceso. Dos umbrales de WIG (uno para consultas de NP y otro para consultas basadas en contenido) se entrenan maximizando la precisión de predicción en los datos de entrenamiento. Cuando una consulta de prueba es etiquetada como del tipo NP (CB) por el clasificador de tipo de consulta, se predecirá que es buena solo si la puntuación WIG para esta consulta está por encima del umbral de NP (CB). Se seguirán procedimientos similares para otras técnicas de predicción. Ahora presentamos brevemente el clasificador de tipo de consulta automático utilizado en este artículo. Encontramos que la puntuación de robustez, aunque originalmente propuesta para la predicción de rendimiento, es un buen indicador de tipos de consultas. Observamos que, en promedio, las consultas basadas en contenido tienen una puntuación de robustez mucho más alta que las consultas de NP. Por ejemplo, la Figura 2 muestra las distribuciones de puntajes de robustez para consultas basadas en NP y en contenido. Según este hallazgo, el clasificador de puntuación de robustez adjuntará una etiqueta NP (CB) a la consulta si la puntuación de robustez para la consulta está por debajo (por encima) de un umbral entrenado a partir de los datos de entrenamiento. 0 0.5 1 1.5 2 2.5 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 NP Basado en contenido Figura 2: Distribución de puntuaciones de robustez para consultas NP y CB. Las consultas NP son los 252 temas NP del Terabyte Track de 2005. Las consultas basadas en contenido son los 150 títulos ad-hoc de las pistas Terabyte 2004-2006. Las distribuciones de probabilidad son estimadas por el método de estimación de densidad de Kernel. Estrategias Robustas WIG-1 WIG-2 WIG-3 Óptima p=0.6 0.565 0.624 0.665 0.684 0.701 P=0.4 0.567 0.633 0.654 0.673 0.696 Tabla 8: Comparación de la precisión de predicción para cinco estrategias en la <br>situación de consulta mixta</br>. Dos formas de muestrear una consulta de la piscina: (1) la consulta muestreada es basada en contenido con una probabilidad p=0.6 (es decir, la consulta es NP con una probabilidad de 0.4) (2) establecer la probabilidad p=0.4. Consideramos cinco estrategias en nuestros experimentos. En la primera estrategia (denominada robusta), utilizamos la puntuación de robustez para la predicción del rendimiento de la consulta con la ayuda de un clasificador de consultas perfecto que siempre mapea correctamente una consulta en una de las dos categorías (es decir, NP o CB). Esta estrategia representa el nivel de precisión de predicción que las técnicas actuales de predicción pueden lograr en una condición ideal en la que se conocen los tipos de consulta. En las tres estrategias siguientes, se adopta el método WIG para predecir el rendimiento. La diferencia entre los tres es que se utilizan tres clasificadores de consulta diferentes para cada estrategia: (1) el clasificador siempre clasifica una consulta en el tipo NP. (2) el clasificador de Probabilidad de Densidad de Puntuación Robusta es el clasificador de puntuación robusta mencionado anteriormente. (3) el clasificador es perfecto. Estas tres estrategias están designadas como WIG-1, WIG-2 y WIG-3 respectivamente. La razón por la que estamos interesados en WIG-1 se basa en los resultados de la sección 4.3.1. En la última estrategia (denominada Óptima) que sirve como un límite superior de lo bien que podemos hacer hasta ahora, hacemos pleno uso de nuestras técnicas de predicción para cada tipo de consulta asumiendo que un clasificador de consultas perfecto está disponible. Específicamente, combinamos linealmente WIG y QF para consultas basadas en contenido y WIG y FRC para consultas de NP. Los resultados de las cinco estrategias se muestran en la Tabla 8. Para cada estrategia, intentamos dos formas de muestrear una consulta del conjunto: (1) la consulta muestreada es CB con probabilidad p=0.6 (la consulta es NP con probabilidad 0.4) (2) establecer la probabilidad p=0.4. De la Tabla 8 podemos ver que en términos de precisión de predicción, WIG-2 (el método WIG con el clasificador de consultas automático) no solo es mejor que los dos primeros casos, sino que también está cerca de WIG-3 donde se asume un clasificador perfecto. Se observan algunas mejoras adicionales sobre WIG-3 cuando se combinan con otras técnicas de predicción. El mérito de WIG-2 es que proporciona una solución práctica para identificar automáticamente consultas con bajo rendimiento en un entorno de búsqueda web con tipos de consultas mixtos, lo cual representa considerables obstáculos para las técnicas de predicción tradicionales. CONCLUSIONES Y TRABAJOS FUTUROS Hasta donde sabemos, nuestro artículo es el primero en explorar a fondo la predicción del rendimiento de consultas en entornos de búsqueda web. Demostramos que nuestros modelos resultaron en una mayor precisión de predicción que las técnicas previamente publicadas no especialmente diseñadas para escenarios de búsqueda en la web. En este artículo, nos enfocamos en dos tipos de consultas en la búsqueda web: consultas basadas en contenido y consultas de búsqueda de Páginas-Nombradas (NP), correspondientes a la tarea de recuperación ad-hoc y la tarea de búsqueda de Páginas-Nombradas respectivamente. Para ambos tipos de consultas web, se demostró que nuestros modelos de predicción son sustancialmente más precisos que las técnicas actuales más avanzadas. Además, consideramos un caso más realista en el que no se dispone de información previa sobre los tipos de consultas. Demostramos que el método WIG es particularmente adecuado para esta situación. Considerando la adaptabilidad de WIG a una variedad de colecciones y tipos de consultas, uno de nuestros planes futuros es aplicar este método para predecir la preferencia del usuario por los resultados de búsqueda en datos realistas recopilados de un motor de búsqueda comercial. Además de la precisión, otro problema importante con el que las técnicas de predicción tienen que lidiar en un entorno web es la eficiencia. Afortunadamente, dado que el puntaje WIG se calcula solo sobre los términos y frases que aparecen en la consulta, este cálculo puede realizarse de manera muy eficiente con el soporte de un índice. Por otro lado, el cálculo de QF y FRC es relativamente menos eficiente ya que QF necesita recuperar toda la colección dos veces y FRC necesita clasificar repetidamente los documentos perturbados. Cómo mejorar la eficiencia de QF y FRC es nuestro trabajo futuro. Además, las técnicas de predicción propuestas en este documento tienen el potencial de mejorar el rendimiento de recuperación al combinarse con otras técnicas de RI. Por ejemplo, nuestras técnicas pueden ser incorporadas a técnicas populares de modificación de consultas como la expansión de consultas y la relajación de consultas. Guiados por la predicción del rendimiento, podemos tomar una mejor decisión sobre cuándo o cómo modificar las consultas para mejorar la efectividad de la recuperación. Nos gustaría llevar a cabo investigaciones en esta dirección en el futuro. AGRADECIMIENTOS Este trabajo fue apoyado en parte por el Centro de Recuperación de Información Inteligente, en parte por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el contrato número HR0011-06-C0023, y en parte por un premio de Google. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son del autor y no necesariamente reflejan las del patrocinador. Además, agradecemos a Donald Metzler por sus valiosos comentarios sobre este trabajo. 7. REFERENCIAS [1] Y. Zhou, W. B. Croft, Ranking Robustness: Un nuevo marco para predecir el rendimiento de consultas, en Actas de CIKM 2006. [2] D. Carmel, E. Yom-Tov, A. Darlow, D. Pelleg, ¿Qué hace que una consulta sea difícil?, en Actas de SIGIR 2006. [3] C. L. A. Clarke, F. Scholer, I. Soboroff, La pista Terabyte TREC 2005, En las Actas en Línea de TREC 2005. [4] B. Él y yo. Inferir el rendimiento de la consulta utilizando predictores de preretrieval. En actas de SPIRE 2004. [5] S. Tomlinson. Recuperación robusta de la web y terabytes con Hummingbird SearchServer en TREC 2004. En las Actas en Línea de TREC 2004. [6] S. Cronen-Townsend, Y. Zhou, W. B. Croft, Prediciendo el Rendimiento de Consultas, en las Actas de SIGIR 2002. [7] V. Vinay, I. J. Cox, N. Mill-Frayling, K. Wood, Sobre la Clasificación de la Efectividad del Buscador, en las Actas de SIGIR 2006. [8] D. Metzler, W. B. Croft, Un Modelo de Campo Aleatorio de Markov para Dependencias de Términos, en las Actas de SIGIR 2005. [9] D. Metzler, T. Strohman, Y. Zhou, W. B. Croft, Indri en TREC 2005: Terabyte Track, en las Actas en Línea de TREC 2004. [10] P. Ogilvie y J. Callan, Combinando representaciones de documentos para búsqueda de elementos conocidos, en las Actas de SIGIR 2003. [11] A. Berger, J. Lafferty, Recuperación de información como traducción estadística, en las Actas de SIGIR 1999. [12] Motor de búsqueda Indri: http://www.lemurproject.org/indri/ [13] I. J. Taneja: Sobre medidas de información generalizadas y sus aplicaciones, Avances en Electrónica y Física de Electrones, Academic Press (EE. UU.), 76, 1989, 327-413. [14] S. Cronen-Townsend, Y. Zhou y Croft, W. B., Un marco para la expansión selectiva de consultas, en Actas de CIKM 2004. [15] F. Song, W. B. Croft, Un modelo de lenguaje general para la recuperación de información, en Actas de SIGIR 1999. [16] Contacto por correo electrónico personal con Vishwa Vinay y nuestros propios experimentos [17] E. Yom-Tov, S. Fine, D. Carmel, A. Darlow, Aprendiendo a estimar la dificultad de la consulta incluyendo aplicaciones para la detección de contenido faltante y la recuperación de información distribuida, en Actas de SIGIR 2005. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "query classification": {
            "translated_key": "clasificación de consultas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Query Performance Prediction in Web Search Environments Yun Zhou and W. Bruce Croft Department of Computer Science University of Massachusetts, Amherst {yzhou, croft}@cs.umass.edu ABSTRACT Current prediction techniques, which are generally designed for content-based queries and are typically evaluated on relatively homogenous test collections of small sizes, face serious challenges in web search environments where collections are significantly more heterogeneous and different types of retrieval tasks exist.",
                "In this paper, we present three techniques to address these challenges.",
                "We focus on performance prediction for two types of queries in web search environments: content-based and Named-Page finding.",
                "Our evaluation is mainly performed on the GOV2 collection.",
                "In addition to evaluating our models for the two types of queries separately, we consider a more challenging and realistic situation that the two types of queries are mixed together without prior information on query types.",
                "To assist prediction under the mixed-query situation, a novel query classifier is adopted.",
                "Results show that our prediction of web query performance is substantially more accurate than the current stateof-the-art prediction techniques.",
                "Consequently, our paper provides a practical approach to performance prediction in realworld web settings.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval -Query formulation General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Query performance prediction has many applications in a variety of information retrieval (IR) areas such as improving retrieval consistency, query refinement, and distributed IR.",
                "The importance of this problem has been recognized by IR researchers and a number of new methods have been proposed for prediction recently [1, 2, 17].",
                "Most work on prediction has focused on the traditional ad-hoc retrieval task where query performance is measured according to topical relevance.",
                "These prediction models are evaluated on TREC document collections which typically consist of no more than one million relatively homogenous newswire articles.",
                "With the popularity and influence of the Web, prediction techniques that will work well for web-style queries are highly preferable.",
                "However, web search environments pose significant challenges to current prediction models that are mainly designed for traditional TREC settings.",
                "Here we outline some of these challenges.",
                "First, web collections, which are much larger than conventional TREC collections, include a variety of documents that are different in many aspects such as quality and style.",
                "Current prediction techniques can be vulnerable to these characteristics of web collections.",
                "For example, the reported prediction accuracy of the ranking robustness technique and the clarity technique on the GOV2 collection (a large web collection) is significantly worse compared to the other TREC collections [1].",
                "Similar prediction accuracy on the GOV2 collection using another technique is reported in [2], confirming the difficult of predicting query performance on a large web collection.",
                "Furthermore, web search goes beyond the scope of the ad-hoc retrieval task based on topical relevance.",
                "For example, the Named-Page (NP) finding task, which is a navigational task, is also popular in web retrieval.",
                "Query performance prediction for the NP task is still necessary since NP retrieval performance is far from perfect.",
                "In fact, according to the report on the NP task of the 2005 Terabyte Track [3], about 40% of the test queries perform poorly (no correct answer in the first 10 search results) even in the best run from the top group.",
                "To our knowledge, little research has explicitly addressed the problem of NP-query performance prediction.",
                "Current prediction models devised for content-based queries will be less effective for NP queries considering the fundamental differences between the two.",
                "Third, in real-world web search environments, user queries are usually a mixture of different types and prior knowledge about the type of each query is generally unavailable.",
                "The mixed-query situation raises new problems for query performance prediction.",
                "For instance, we may need to incorporate a query classifier into prediction models.",
                "Despite these problems, the ability to handle this situation is a crucial step towards turning query performance prediction from an interesting research topic into a practical tool for web retrieval.",
                "In this paper, we present three techniques to address the above challenges that current prediction models face in Web search environments.",
                "Our work focuses on query performance prediction for the content-based (ad-hoc) retrieval task and the name-page finding task in the context of web retrieval.",
                "Our first technique, called weighted information gain (WIG), makes use of both single term and term proximity features to estimate the quality of top retrieved documents for prediction.",
                "We find that WIG offers consistent prediction accuracy across various test collections and query types.",
                "Moreover, we demonstrate that good prediction accuracy can be achieved for the mixed-query situation by using WIG with the help of a query type classifier.",
                "Query feedback and first rank change, which are our second and third prediction techniques, perform well for content-based queries and NP queries respectively.",
                "Our main contributions include: (1) considerably improved prediction accuracy for web content-based queries over several state-of-the-art techniques. (2) new techniques for successfully predicting NP-query performance. (3) a practical and fully automatic solution to predicting mixed-query performance.",
                "In addition, one minor contribution is that we find that the robustness score [1], which was originally proposed for performance prediction, is helpful for <br>query classification</br>. 2.",
                "RELATED WORK As we mentioned in the introduction, a number of prediction techniques have been proposed recently that focus on contentbased queries in the topical relevance (ad-hoc) task.",
                "We know of no published work that addresses other types of queries such as NP queries, let alone a mixture of query types.",
                "Next we review some representative models.",
                "The major difficulty of performance prediction comes from the fact that many factors have an impact on retrieval performance.",
                "Each factor affects performance to a different degree and the overall effect is hard to predict accurately.",
                "Therefore, it is not surprising to notice that simple features, such as the frequency of query terms in the collection [4] and the average IDF of query terms [5], do not predict well.",
                "In fact, most of the successful techniques are based on measuring some characteristics of the retrieved document set to estimate topic difficulty.",
                "For example, the clarity score [6] measures the coherence of a list of documents by the KL-divergence between the query model and the collection model.",
                "The robustness score [1] quantifies another property of a ranked list: the robustness of the ranking in the presence of uncertainty.",
                "Carmel et al. [2] found that the distance measured by the Jensen-Shannon divergence between the retrieved document set and the collection is significantly correlated to average precision.",
                "Vinay et al. [7] proposed four measures to capture the geometry of the top retrieved documents for prediction.",
                "The most effective measure is the sensitivity to document perturbation, an idea somewhat similar to the robustness score.",
                "Unfortunately, their way of measuring the sensitivity does not perform equally well for short queries and prediction accuracy drops considerably when a state-of-the-art retrieval technique (like Okapi or a language modeling approach) is adopted for retrieval instead of the tf-idf weighting used in their paper [16].",
                "The difficulties of applying these models in web search environments have already been mentioned.",
                "In this paper, we mainly adopt the clarity score and the robustness score as our baselines.",
                "We experimentally show that the baselines, even after being carefully tuned, are inadequate for the web environment.",
                "One of our prediction models, WIG, is related to the Markov random field (MRF) model for information retrieval [8].",
                "The MRF model directly models term dependence and is found be to highly effective across a variety of test collections (particularly web collections) and retrieval tasks.",
                "This model is used to estimate the joint probability distribution over documents and queries, an important part of WIG.",
                "The superiority of WIG over other prediction techniques based on unigram features, which will be demonstrated later in our paper, coincides with that of MRF for retrieval.",
                "In other word, it is interesting to note that term dependence, when being modeled appropriately, can be helpful for both improving and predicting retrieval performance. 3.",
                "PREDICTION MODELS 3.1 Weighted Information Gain (WIG) This section introduces a weighted information gain approach that incorporates both single term and proximity features for predicting performance for both content-based and Named-Page (NP) finding queries.",
                "Given a set of queries Q={Qs} (s=1,2,..N) which includes all possible user queries and a set of documents D={Dt} (t=1,2…M), we assume that each query-document pair (Qs,Dt) is manually judged and will be put in a relevance list if Qs is found to be relevant to Dt.",
                "The joint probability P(Qs,Dt) over queries Q and documents D denotes the probability that pair (Qs,Dt) will be in the relevance list.",
                "Such assumptions are similar to those used in [8].",
                "Assuming that the user issues query Qi ∈Q and the retrieval results in response to Qi is a ranked list L of documents, we calculate the amount of information contained in P(Qs,Dt) with respect to Qi and L by Eq.1 which is a variant of entropy called the weighted entropy[13].",
                "The weights in Eq.1 are solely determined by Qi and L. )1(),(log),(),( , , ∑−= ts tststsLQ DQPDQweightDQH i In this paper, we choose the weights as follows: LindocumentsKtopthecontainsLTwhere otherwise LTDandisifK DQweight K Kt ts )( )2( ,0 )(,/1 ),( ⎩ ⎨ ⎧ ∈= = The cutoff rank K is a parameter in our model that will be discussed later.",
                "Accordingly, Eq.1 can be simplified as follows: )3(),(log 1 ),( )( , ∑∈ −= LTD titsLQ Kt i DQP K DQH Unfortunately, weighted entropy ),(, tsLQ DQH i computed by Eq.3, which represents the amount of information about how likely the top ranked documents in L would be relevant to query Qi on average, cannot be compared across different queries, making it inappropriate for directly predicting query performance.",
                "To mitigate this problem, we come up with a background distribution P(Qs,C) over Q and D by imagining that every document in D is replaced by the same special document C which represents average language usage.",
                "In this paper, C is created by concatenating every document in D. Roughly speaking, C is the collection (the document set) {Dt} without document boundaries.",
                "Similarly, weighted entropy ),(, CQH sLQi calculated by Eq.3 represents the amount of information about how likely an average document (represented by the whole collection) would be relevant to query Qi.",
                "Now we introduce our performance predictor WIG which is the weighted information gain [13] computed as the difference between ),(, tsLQ DQH i and ),(, CQH sLQi .Specifically, given query Qi, collection C and ranked list L of documents, WIG is calculated as follows: )4( ),( ),( log 1 ),( ),( log),( ),(),(),,( )(, ,, ∑∑ ∈ == −= LTD i ti ts s ts ts tsLQsLQi Kt ii CQP DQP KCQP DQP DQweight DQHCQHLCQWIG WIG computed by Eq.4 measures the change in information about the quality of retrieval (in response to query Qi) from an imaginary state that only an average document is retrieved to a posterior state that the actual search results are observed.",
                "We hypothesize that WIG is positively correlated with retrieval effectiveness because high quality retrieval should be much more effective than just returning the average document.",
                "The heart of this technique is how to estimate the joint distribution P(Qs,Dt).",
                "In the language modeling approach to IR, a variety of models can be applied readily to estimate this distribution.",
                "Although most of these models are based on the bagof-words assumption, recent work on modeling term dependence under the language modeling framework have shown consistent and significant improvements in retrieval effectiveness over bagof-words models.",
                "Inspired by the success of incorporating term proximity features into language models, we decide to adopt a good dependence model to estimate the probability P(Qs,Dt).",
                "The model we chose for this paper is Metzler and Crofts Markov Random Field (MRF) model, which has already demonstrated superiority over a number of collections and different retrieval tasks [8,9].",
                "According to the MRF model, log P(Qi, Dt) can be written as )5()|(loglog),(log )( 1 ∑∈ +−= iQF tti DPZDQP ξ ξ ξλ where Z1 is a constant that ensures that P(Qi, Dt) sums up to 1.",
                "F(Qi) consists of a set of features expanded from the original query Qi .",
                "For example, assuming that query Qi is talented student program, F(Qi) includes features like program and talented student.",
                "We consider two kinds of features: single term features T and proximity features P. Proximity features include exact phrase (#1) and unordered window (#uwN) features as described in [8].",
                "Note that F(Qi) is the union of T(Qi) and P(Qi).",
                "For more details on F(Qi) such as how to expand the original query Qi to F(Qi), we refer the reader to [8] and [9].",
                "P(ξ|Dt) denotes the probability that feature ξ will occur in Dt.",
                "More details on P(ξ|Dt) will be provided later in this section.",
                "The choice of λξ is somewhat different from that used in [8] since λξ plays a dual role in our model.",
                "The first role, which is the same as in [8], is to weight between single term and proximity features.",
                "The other role, which is specific to our prediction task, is to normalize the size of F(Qi).We found that the following weight strategy for λξ satisfies the above two roles and generalizes well on a variety of collections and query types. )6( )(, |)(| 1 )(, |)(| ⎪ ⎪ ⎩ ⎪ ⎪ ⎨ ⎧ ∈ − ∈ = i i T i i T QP QP QT QT ξ λ ξ λ λξ where |T(Qi)| and |P(Qi)| denote the number of single term and proximity features in F(Qi) respectively.",
                "The reason for choosing the square root function in the denominator of λξ is to penalize a feature set of large size appropriately, making WIG more comparable across queries of various lengths. λT is a fixed parameter and set to 0.8 according to [8] throughout this paper.",
                "Similarly, log P(Qi,C) can be written as: )7()|(loglog),(log )( 2 ∑∈ +−= iQF i CPZCQP ξ ξ ξλ When constant Z1 and Z2 are dropped, WIG computed in Eq.4 can be rewritten as follows by plugging in Eq.5 and Eq.7 : )8( )|( )|( log 1 ),,( )( )( ∑ ∑∈ ∈ = LTD QF t i Kt i CP DP K LCQWIG ξ ξ ξ ξ λ One of the advantages of WIG over other techniques is that it can handle well both content-based and NP queries.",
                "Based on the type (or the predicted type) of Qi, the calculation of WIG in Eq. 8 differs in two aspects: (1) how to estimate P(ξ|Dt) and P(ξ|C), and (2) how to choose K. For content-based queries, P(ξ|C) is estimated by the relative frequency of feature ξ in collection C as a whole.",
                "The estimation of P(ξ|Dt) is the same as in [8].",
                "Namely, we estimate P(ξ|Dt) by the relative frequency of feature ξ in Dt linearly smoothed with collection frequency P(ξ|C).",
                "K in Eq.8 is treated as a free parameter.",
                "Note that K is the only free parameter in the computation of WIG for content-based queries because all parameters involved in P(ξ|Dt) are assumed to be fixed by taking the suggested values in [8].",
                "Regarding NP queries, we make use of document structure to estimate P(ξ|Dt) and P(ξ|C) by the so-called mixture of language models proposed in [10] and incorporated into the MRF model for Named-Page finding retrieval in [9].",
                "The basic idea is that a document (collection) is divided into several fields such as the title field, the main-body field and the heading field.",
                "P(ξ|Dt) and P(ξ|C) are estimated by a linear combination of the language models from each field.",
                "Due to space constraints, we refer the reader to [9] for details.",
                "We adopt the exact same set of parameters as used in [9] for estimation.",
                "With regard to K in Eq.8, we set K to 1 because the Named-Page finding task heavily focuses on the first ranked document.",
                "Consequently, there are no free parameters in the computation of WIG for NP queries. 3.2 Query Feedback In this section, we introduce another technique called query feedback (QF) for prediction.",
                "Suppose that a user issues query Q to a retrieval system and a ranked list L of documents is returned.",
                "We view the retrieval system as a noisy channel.",
                "Specifically, we assume that the output of the channel is L and the input is Q.",
                "After going through the channel, Q becomes corrupted and is transformed to ranked list L. By thinking about the retrieval process this way, the problem of predicting retrieval effectiveness turns to the task of evaluating the quality of the channel.",
                "In other words, prediction becomes finding a way to measure the degree of corruption that arises when Q is transformed to L. As directly computing the degree of the corruption is difficult, we tackle this problem by approximation.",
                "Our main idea is that we measure to what extent information on Q can be recovered from L on the assumption that only L is observed.",
                "Specifically, we design a decoder that can accurately translate L back into new query Q and the similarity S between the original query Q and the new query Q is adopted as a performance predictor.",
                "This is a sketch of how the QF technique predicts query performance.",
                "Before filling in more details, we briefly discuss why this method would work.",
                "There is a relation between the similarity S defined above and retrieval performance.",
                "On the one hand, if the retrieval has strayed from the original sense of the query Q, the new query Q extracted from ranked list L in response to Q would be very different from the original query Q.",
                "On the other hand, a query distilled from a ranked list containing many relevant documents is likely to be similar to the original query.",
                "Further examples in support of the relation will be provided later.",
                "Next we detail how to build the decoder and how to measure the similarity S. In essence, the goal of the decoder is to compress ranked list L into a few informative terms that should represent the content of the top ranked documents in L. Our approach to this goal is to represent ranked list L by a language model (distribution over terms).",
                "Then terms are ranked by their contribution to the language models KL (Kullback-Leibler) divergence from the background collection model.",
                "Top ranked terms will be chosen to form the new query Q.",
                "This approach is similar to that used in Section 4.1 of [11].",
                "Specifically, we take three steps to compress ranked list L into query Q without referring to the original query. 1.",
                "We adopt the ranked list language model [14], to estimate a language model based on ranked list L. The model can be written as: )9()|()|()|( ∑∈ = LD LDPDwPLwP where w is any term, D is a document.",
                "P(D|L) is estimated by a linearly decreasing function of the rank of document D. 2.",
                "Each term in P(w|L) is ranked by the following KL-divergence contribution: )10( )|( )|( log)|( CwP LwP LwP where P(w|C) is the collection model estimated by the relative frequency of term w in collection C as a whole. 3.",
                "The top N ranked terms by Eq.10 form a weighted query Q={(wi,ti)} i=1,N. where wi denotes the i-th ranked term and weight ti is the KL-divergence contribution of wi in Eq. 10.",
                "Term cruise ship vessel sea passenger KL contribution 0.050 0.040 0.012 0.010 0.009 Table 1: top 5 terms compressed from the ranked list in response to query Cruise ship damage sea life Two representative examples, one for a poorly performing query Cruise ship damage sea life (TREC topic 719; average precision: 0.08) and the other for a high performing query prostate cancer treatments( TREC topic 710; average precision: 0.49), are shown in Table 1 and 2 respectively.",
                "These examples indicate how the similarity between the original and the new query correlates with retrieval performance.",
                "The parameter N in step 3 is set to 20 empirically and choosing a larger value of N is unnecessary since the weights after the top 20 are usually too small to make any difference.",
                "Term prostate cancer treatment men therapy KL contribution 0.177 0.140 0.028 0.025 0.020 Table 2: top 5 terms compressed from the ranked list in response to query prostate cancer treatments To measure the similarity between original query Q and new query Q, we first use Q to do retrieval on the same collection.",
                "A variant of the query likelihood model [15] is adopted for retrieval.",
                "Namely, documents are ranked by: )11()|()|( ),( ∑∈ = Qtw t i ii i DwPDQP where wi is a term in Q and ti is the associated weight.",
                "D is a document.",
                "Let L denote the new ranked list returned from the above retrieval.",
                "The similarity is measured by the overlap of documents in L and L. Specifically, the percentage of documents in the top K documents of L that are also present in the top K documents in L. the cutoff K is treated as a free parameter.",
                "We summarize here how the QF technique predicts performance given a query Q and the associated ranked list L. We first obtain a weighted query Q compressed from L by the above three steps.",
                "Then we use Q to perform retrieval and the new ranked list is L. The overlap of documents in L and L is used for prediction. 3.3 First Rank Change (FRC) In this section, we propose a method called the first rank change (FRC) for performance prediction for NP queries.",
                "This method is derived from the ranking robustness technique [1] that is mainly designed for content-based queries.",
                "When directly applied to NP queries, the robustness technique will be less effective because it takes the top ranked documents as a whole into account while NP queries usually have only one single relevant document.",
                "Instead, our technique focuses on the first rank document while the main idea of the robustness method remains.",
                "Specifically, the pseudocode for computing FRC is shown in figure 1.",
                "Input: (1) ranked list L={Di} where i=1,100.",
                "Di denotes the i-th ranked document. (2) query Q 1 initialize: (1) set the number of trials J=100000 (2) counter c=0; 2 for i=1 to J 3 Perturb every document in L, let the outcome be a set F={Di} where Di denotes the perturbed version of Di. 4 Do retrieval with query Q on set F 5 c=c+1 if and only if D1 is ranked first in step 4 6 end of for 7 return the ratio c/J Figure 1: pseudo-code for computing FRC FRC approximates the probability that the first ranked document in the original list L will remain ranked first even after the documents are perturbed.",
                "The higher the probability is, the more confidence we have in the first ranked document.",
                "On the other hand, in the extreme case of a random ranking, the probability would be as low as 0.5.",
                "We expect that FRC has a positive association with NP query performance.",
                "We adopt [1] to implement the document perturbation step (step 4 in Fig.1) using Poisson distributions.",
                "For more details, we refer the reader to [1]. 4.",
                "EVALUATION We now present the results of predicting query performance by our models.",
                "Three state-of-the-art techniques are adopted as our baselines.",
                "We evaluate our techniques across a variety of Web retrieval settings.",
                "As mentioned before, we consider two types of queries, that is, content-based (CB) queries and Named-Page(NP) finding queries.",
                "First, suppose that the query types are known.",
                "We investigate the correlation between the predicted retrieval performance and the actual performance for both types of queries separately.",
                "Results show that our methods yield considerable improvements over the baselines.",
                "We then consider a more challenging scenario where no prior information on query types is available.",
                "Two sub-cases are considered.",
                "In the first one, there exists only one type of query but the actual type is unknown.",
                "We assume a mixture of the two query types in the second case.",
                "We demonstrate that our models achieve good accuracy under this demanding scenario, making prediction practical in a real-world Web search environment. 4.1 Experimental Setup Our evaluation focuses on the GOV2 collection which contains about 25 million documents crawled from web sites in the .gov domain during 2004 [3].",
                "We create two kinds of data set for CB queries and NP queries respectively.",
                "For the CB type, we use the ad-hoc topics of the Terabyte Tracks of 2004, 2005 and 2006 and name them TB04-adhoc, TB05-adhoc and TB06-adhoc respectively.",
                "In addition, we also use the ad-hoc topics of the 2004 Robust Track (RT04) to test the adaptability of our techniques to a non-Web environment.",
                "For NP queries, we use the Named-Page finding topics of the Terabyte Tracks of 2005 and 2006 and we name them TB05-NP and TB06-NP respectively.",
                "All queries used in our experiments are titles of TREC topics as we center on web retrieval.",
                "Table 3 summarizes the above data sets.",
                "Name Collection Topic Number Query Type TB04-adhoc GOV2 701-750 CB TB05-adhoc GOV2 751-800 CB TB06-adhoc GOV2 801-850 CB RT04 Disk 4+5 (minus CR) 301-450;601700 CB TB05-NP GOV2 NP601-NP872 NP TB06-NP GOV2 NP901-NP1081 NP Table 3: Summary of test collections and topics Retrieval performance of individual content-based and NP queries is measured by the average precision and reciprocal rank of the first correct answer respectively.",
                "We make use of the Markov Random field model for both ad-hoc and Named-Page finding retrieval.",
                "We adopt the same setting of retrieval parameters used in [8,9].",
                "The Indri search engine [12] is used for all of our experiments.",
                "Though not reported here, we also tried the query likelihood model for ad-hoc retrieval and found that the results change little because of the very high correlation between the query performances obtained by the two retrieval models (0.96 measured by Pearsons coefficient). 4.2 Known Query Types Suppose that query types are known.",
                "We treat each type of query separately and measure the correlation with average precision (or the reciprocal rank in the case of NP queries).",
                "We adopt the Pearsons correlation test which reflects the degree of linear relationship between the predicted and the actual retrieval performance. 4.2.1 Content-based Queries Methods Clarity Robust JSD WIG QF WIG +QF TB04+0 5 adhoc 0.333 0.317 0.362 0.574 0.480 0.637 TB06 adhoc 0.076 0.294 N/A 0.464 0.422 0.511 Table 4: Pearsons correlation coefficients for correlation with average precision on the Terabyte Tracks (ad-hoc) for clarity score, robustness score, the JSD-based method(we directly cites the score reported in [2]), WIG, query feedback(QF) and a linear combination of WIG and QF.",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Table 4 shows the correlation with average precision on two data sets: one is a combination of TB04-adhoc and TB05-adhoc(100 topics in total) and the other is TB06-adhoc (50 topics).",
                "The reason that we put TB04-adhoc and TB05-adhoc together is to make our results comparable to [2].",
                "Our baselines are the clarity score (clarity) [6],the robustness score (robust)[1] and the JSDbased method (JSD) [2].",
                "For the clarity and robustness score, we have tried different parameter settings and report the highest correlation coefficients we have found.",
                "We directly cite the result of the JSD-based method reported in [2].",
                "The table also shows the results for the Weighted Information Gain (WIG) method and the Query Feedback (QF) method for predicting content-based queries.",
                "As we described in the previous section, both WIG and QF have one free parameter to set, that is, the cutoff rank K. We train the parameter on one dataset and test on the other.",
                "When combining WIG and QF, a simple linear combination is used and the combination weight is learned from the training data set.",
                "From these results, we can see that our methods are considerably more accurate compared to the baselines.",
                "We also observe that further improvements are obtained from the combination of WIG and QF, suggesting that they measure different properties of the retrieval process that relate to performance.",
                "We discover that our methods generalize well on TB06-adhoc while the correlation for the clarity score with retrieval performance on this data set is considerably worse.",
                "Further investigation shows that the mean average precision of TB06-adhoc is 0.342 and is about 10% better than that of the first data set.",
                "While the other three methods typically consider the top 100 or less documents given a ranked list, the clarity method usually needs the top 500 or more documents to adequately measure the coherence of a ranked list.",
                "Higher mean average precision makes ranked lists retrieved by different queries more similar in terms of coherence at the level of top 500 documents.",
                "We believe that this is the main reason for the low accuracy of the clarity score on the second data set.",
                "Though this paper focuses on a Web search environment, it is desirable that our techniques will work consistently well in other situations.",
                "To this end, we examine the effectiveness of our techniques on the Robust 2004 Track.",
                "For our methods, we evenly divide all of the test queries into five groups and perform five-fold cross validation.",
                "Each time we use one group for training and the remaining four groups for testing.",
                "We make use of all of the queries for our two baselines, that is, the clarity score and the robustness score.",
                "The parameters for our baselines are the same as those used in [1].The results shown in Table 5 demonstrate that the prediction accuracy of our methods is on a par with that of the two strong baselines.",
                "Clarity Robust WIG QF 0.464 0.539 0.468 0.464 Table 5: Comparison of Pearsons correlation coefficients on the 2004 Robust Track for clarity score, robustness score, WIG and query feedback (QF).",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Furthermore, we examine the prediction sensitivity of our methods to the cutoff rank K. With respect to WIG, it is quite robust to K on the Terabyte Tracks (2004-2006) while it prefers a small value of K like 5 on the 2004 Robust Track.",
                "In other words, a small value of K is a nearly-optimal choice for both kinds of tracks.",
                "Considering the fact that all other parameters involved in WIG are fixed and consequently the same for the two cases, this means WIG can achieve nearly-optimal prediction accuracy in two considerably different situations with exactly the same parameter setting.",
                "Regarding QF, it prefers a larger value of K such as 100 on the Terabyte Tracks and a smaller value of K such as 25 on the 2004 Robust Track. 4.2.2 NP Queries We adopt WIG and first rank change (FRC) for predicting NPquery performance.",
                "We also try a linear combination of the two as in the previous section.",
                "The combination weight is obtained from the other data set.",
                "We use the correlation with the reciprocal ranks measured by the Pearsons correlation test to evaluate prediction quality.",
                "The results are presented in Table 6.",
                "Again, our baselines are the clarity score and the robustness score.",
                "To make a fair comparison, we tune the clarity score in different ways.",
                "We found that using the first ranked document to build the query model yields the best prediction accuracy.",
                "We also attempted to utilize document structure by using the mixture of language models mentioned in section 3.1.",
                "Little improvement was obtained.",
                "The correlation coefficients for the clarity score reported in Table 6 are the best we have found.",
                "As we can see, our methods considerably outperform the clarity score technique on both of the runs.",
                "This confirms our intuition that the use of a coherence-based measure like the clarity score is inappropriate for NP queries.",
                "Methods Clarity Robust.",
                "WIG FRC WIG+FRC TB05-NP 0.150 -0.370 0.458 0.440 0.525 TB06-NP 0.112 -0.160 0.478 0.386 0.515 Table 6: Pearsons correlation coefficients for correlation with reciprocal ranks on the Terabyte Tracks (NP) for clarity score, robustness score, WIG, the first rank change (FRC) and a linear combination of WIG and FRC.",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Regarding the robustness score, we also tune the parameters and report the best we have found.",
                "We observe an interesting and surprising negative correlation with reciprocal ranks.",
                "We explain this finding briefly.",
                "A high robustness score means that a number of top ranked documents in the original ranked list are still highly ranked after perturbing the documents.",
                "The existence of such documents is a good sign of high performance for content-based queries as these queries usually contain a number of relevant documents [1].",
                "However, with regard to NP queries, one fundamental difference is that there is only one relevant document for each query.",
                "The existence of such documents can confuse the ranking function and lead to low retrieval performance.",
                "Although the negative correlation with retrieval performance exists, the strength of the correlation is weaker and less consistent compared to our methods as shown in Table 6.",
                "Based on the above analysis, we can see that current prediction techniques like clarity score and robustness score that are mainly designed for content-based queries face significant challenges and are inadequate to deal with NP queries.",
                "Our two techniques proposed for NP queries consistently demonstrate good prediction accuracy, displaying initial success in solving the problem of predicting performance for NP queries.",
                "Another point we want to stress is that the WIG method works well for both types of queries, a desirable property that most prediction techniques lack. 4.3 Unknown Query Types In this section, we run two kinds of experiments without access to query type labels.",
                "First, we assume that only one type of query exists but the type is unknown.",
                "Second, we experiment on a mixture of content-based and NP queries.",
                "The following two subsections will report results for the two conditions respectively. 4.3.1 Only One Type exists We assume that all queries are of the same type, that is, they are either NP queries or content-based queries.",
                "We choose WIG to deal with this case because it shows good prediction accuracy for both types of queries in the previous section.",
                "We consider two cases: (1) CB: all 150 title queries from the ad-hoc task of the Terabyte Tracks 2004-2006 (2)NP: all 433 NP queries from the named page finding task of the Terabyte Tracks 2005 and 2006.",
                "We take a simple strategy by labeling all of the queries in each case as the same type (either NP or CB) regardless of their actual type.",
                "The computation of WIG will be based on the labeled query type instead of the actual type.",
                "There are four possibilities with respect to the relation between the actual type and the labeled type.",
                "The correlation with retrieval performance under the four possibilities is presented in Table 7.",
                "For example, the value 0.445 at the intersection between the second row and the third column shows the Pearsons correlation coefficient for correlation with average precision when the content-based queries are incorrectly labeled as the NP type.",
                "Based on these results, we recommend treating all queries as the NP type when only one query type exists and accurate <br>query classification</br> is not feasible, considering the risk that a large loss of accuracy will occur if NP queries are incorrectly labeled as content-based queries.",
                "These results also demonstrate the strong adaptability of WIG to different query types.",
                "CB (labeled) NP (labeled) CB (actual) 0.536 0.445 NP (actual) 0.174 0.467 Table 7: Comparison of Pearsons correlation coefficients for correlation with retrieval performance under four possibilities on the Terabyte Tracks (NP).",
                "Bold cases mean the results are statistically significant at the 0.01 level. 4.3.2 A mixture of contented-based and NP queries A mixture of the two types of queries is a more realistic situation that a Web search engine will meet.",
                "We evaluate prediction accuracy by how accurately poorly-performing queries can be identified by the prediction method assuming that actual query types are unknown (but we can predict query types).",
                "This is a challenging task because both the predicted and actual performance for one type of query can be incomparable to that for the other type.",
                "Next we discuss how to implement our evaluation.",
                "We create a query pool which consists of all of the 150 ad-hoc title queries from Terabyte Track 2004-2006 and all of the 433 NP queries from Terabyte Track 2005&2006.",
                "We divide the queries in the pool into classes: good (better than 50% of the queries of the same type in terms of retrieval performance) and bad (otherwise).",
                "According to these standards, a NP query with the reciprocal rank above 0.2 or a content-based query with the average precision above 0.315 will be considered as good.",
                "Then, each time we randomly select one query Q from the pool with probability p that Q is contented-based.",
                "The remaining queries are used as training data.",
                "We first decide the type of query Q according to a query classifier.",
                "Namely, the query classifier tells us whether query Q is NP or content-based.",
                "Based on the predicted query type and the score computed for query Q by a prediction technique, a binary decision is made about whether query Q is good or bad by comparing to the score threshold of the predicted query type obtained from the training data.",
                "Prediction accuracy is measured by the accuracy of the binary decision.",
                "In our implementation, we repeatedly take a test query from the query pool and prediction accuracy is computed as the percentage of correct decisions, that is, a good(bad) query is predicted to be good (bad).",
                "It is obvious that random guessing will lead to 50% accuracy.",
                "Let us take the WIG method for example to illustrate the process.",
                "Two WIG thresholds (one for NP queries and the other for content-based queries) are trained by maximizing the prediction accuracy on the training data.",
                "When a test query is labeled as the NP (CB) type by the query type classifier, it will be predicted to be good if and only if the WIG score for this query is above the NP (CB) threshold.",
                "Similar procedures will be taken for other prediction techniques.",
                "Now we briefly introduce the automatic query type classifier used in this paper.",
                "We find that the robustness score, though originally proposed for performance prediction, is a good indicator of query types.",
                "We find that on average content-based queries have a much higher robustness score than NP queries.",
                "For example, Figure 2 shows the distributions of robustness scores for NP and content-based queries.",
                "According to this finding, the robustness score classifier will attach a NP (CB) label to the query if the robustness score for the query is below (above) a threshold trained from the training data. 0 0.5 1 1.5 2 2.5 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 NP Content-based Figure 2: Distribution of robustness scores for NP and CB queries.",
                "The NP queries are the 252 NP topics from the 2005 Terabyte Track.",
                "The content-based queries are the 150 ad-hoc title from the Terabyte Tracks 2004-2006.",
                "The probability distributions are estimated by the Kernel density estimation method.",
                "Strategies Robust WIG-1 WIG-2 WIG-3 Optimal p=0.6 0.565 0.624 0.665 0.684 0.701 P=0.4 0.567 0.633 0.654 0.673 0.696 Table 8: Comparison of prediction accuracy for five strategies in the mixed-query situation.",
                "Two ways to sample a query from the pool: (1) the sampled query is content-based with the probability p=0.6. (that is, the query is NP with probability 0.4 ) (2) set the probability p=0.4.",
                "We consider five strategies in our experiments.",
                "In the first strategy (denoted by robust), we use the robustness score for query performance prediction with the help of a perfect query classifier that always correctly map a query into one of the two categories (that is, NP or CB).",
                "This strategy represents the level of prediction accuracy that current prediction techniques can achieve in an ideal condition that query types are known.",
                "In the next following three strategies, the WIG method is adopted for performance prediction.",
                "The difference among the three is that three different query classifiers are used for each strategy: (1) the classifier always classifies a query into the NP type. (2) the Robustness Score ProbabilityDensity classifier is the robust score classifier mentioned above. (3) the classifier is a perfect one.",
                "These three strategies are denoted by WIG-1, WIG-2 and WIG-3 respectively.",
                "The reason we are interested in WIG-1 is based on the results from section 4.3.1.",
                "In the last strategy (denoted by Optimal) which serves as an upper bound on how well we can do so far, we fully make use of our prediction techniques for each query type assuming a perfect query classifier is available.",
                "Specifically, we linearly combine WIG and QF for content-based queries and WIG and FRC for NP queries.",
                "The results for the five strategies are shown in Table 8.",
                "For each strategy, we try two ways to sample a query from the pool: (1) the sampled query is CB with probability p=0.6. (the query is NP with probability 0.4) (2) set the probability p=0.4.",
                "From Table 8 We can see that in terms of prediction accuracy WIG-2 (the WIG method with the automatic query classifier) is not only better than the first two cases, but also is close to WIG-3 where a perfect classifier is assumed.",
                "Some further improvements over WIG-3 are observed when combined with other prediction techniques.",
                "The merit of WIG-2 is that it provides a practical solution to automatically identifying poorly performing queries in a Web search environment with mixed query types, which poses considerable obstacles to traditional prediction techniques. 5.",
                "CONCLUSIONS AND FUTURE WORK To our knowledge, our paper is the first to thoroughly explore prediction of query performance in web search environments.",
                "We demonstrated that our models resulted in higher prediction accuracy than previously published techniques not specially devised for web search scenarios.",
                "In this paper, we focus on two types of queries in web search: content-based and Named-Page (NP) finding queries, corresponding to the ad-hoc retrieval task and the Named-Page finding task respectively.",
                "For both types of web queries, our prediction models were shown to be substantially more accurate than the current state-of-the-art techniques.",
                "Furthermore, we considered a more realistic case that no prior information on query types is available.",
                "We demonstrated that the WIG method is particularly suitable for this situation.",
                "Considering the adaptability of WIG to a range of collections and query types, one of our future plans is to apply this method to predict user preference of search results on realistic data collected from a commercial search engine.",
                "Other than accuracy, another major issue that prediction techniques have to deal with in a Web environment is efficiency.",
                "Fortunately, since the WIG score is computed just over the terms and the phrases that appear in the query, this calculation can be made very efficient with the support of index.",
                "On the other hand, the computation of QF and FRC is relatively less efficient since QF needs to retrieve the whole collection twice and FRC needs to repeatedly rank the perturbed documents.",
                "How to improve the efficiency of QF and FRC is our future work.",
                "In addition, the prediction techniques proposed in this paper have the potential of improving retrieval performance by combining with other IR techniques.",
                "For example, our techniques can be incorporated to popular query modification techniques such as query expansion and query relaxation.",
                "Guided by performance prediction, we can make a better decision on when to or how to modify queries to enhance retrieval effectiveness.",
                "We would like to carry out research in this direction in the future. 6.",
                "ACKNOWLEGMENTS This work was supported in part by the Center for Intelligent Information Retrieval, in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C0023, and in part by an award from Google.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor.",
                "In addition, we thank Donald Metzler for his valuable comments on this work. 7.",
                "REFERENCES [1] Y. Zhou ,W. B. Croft ,Ranking Robustness: A Novel Framework to Predict Query Performance, in Proceedings of CIKM 2006. [2] D.Carmel, E.Yom-Tov, A.Darlow,D.Pelleg, What Makes a Query Difficult?, in Proceedings of SIGIR 2006. [3] C.L.A.",
                "Clarke, F. Scholer, I.Soboroff, The TREC 2005 Terabyte Track, In the Online Proceedings of 2005 TREC. [4] B.",
                "He and I.Ounis.",
                "Inferring query performance using preretrieval predictors.",
                "In proceedings of the SPIRE 2004. [5] S. Tomlinson.",
                "Robust, Web and Terabyte Retrieval with Hummingbird SearchServer at TREC 2004.",
                "In the Online Proceedings of 2004 TREC. [6] S. Cronen-Townsend, Y. Zhou, W. B. Croft, Predicting Query Performance, in Proceedings of SIGIR 2002. [7] V.Vinay, I.J.Cox, N.Mill-Frayling,K.Wood, On Ranking the Effectiveness of Searcher, in Proceedings of SIGIR 2006. [8] D.Metzler, W.B.Croft, A Markov Random Filed Model for Term Dependencies, in Proceedings of SIGIR 2005. [9] D.Metzler, T.Strohman,Y.Zhou,W.B.Croft, Indri at TREC 2005: Terabyte Track, In the Online Proceedings of 2004 TREC. [10] P. Ogilvie and J. Callan, Combining document representations for known-item search, in Proceedings of SIGIR 2003. [11] A.Berger, J.Lafferty, Information retrieval as statistical translation, in Proceedings of SIGIR 1999. [12] Indri search engine : http://www.lemurproject.org/indri/ [13] I.J.",
                "Taneja: On Generalized Information Measures and Their Applications, Advances in Electronics and Electron Physics, Academic Press (USA), 76, 1989, 327-413. [14] S.Cronen-Townsend, Y. Zhou and Croft, W. B. , A Framework for Selective Query Expansion, in Proceedings of CIKM 2004. [15] F.Song, W.B.Croft, A general language model for information retrieval, in Proceedings of SIGIR 1999. [16] Personal email contact with Vishwa Vinay and our own experiments [17] E.Yom-Tov, S.Fine, D.Carmel, A.Darlow, Learning to Estimate Query Difficulty Including Applications to Missing Content Detection and Distributed Information retrieval, in Proceedings of SIGIR 2005"
            ],
            "original_annotated_samples": [
                "In addition, one minor contribution is that we find that the robustness score [1], which was originally proposed for performance prediction, is helpful for <br>query classification</br>. 2.",
                "Based on these results, we recommend treating all queries as the NP type when only one query type exists and accurate <br>query classification</br> is not feasible, considering the risk that a large loss of accuracy will occur if NP queries are incorrectly labeled as content-based queries."
            ],
            "translated_annotated_samples": [
                "Además, una contribución menor es que encontramos que el puntaje de robustez [1], que originalmente se propuso para la predicción de rendimiento, es útil para la <br>clasificación de consultas</br>. 2.",
                "Basándonos en estos resultados, recomendamos tratar todas las consultas como del tipo NP cuando solo existe un tipo de consulta y la clasificación precisa de la consulta no es factible, considerando el riesgo de que se produzca una gran pérdida de precisión si las consultas NP se etiquetan incorrectamente como consultas basadas en contenido."
            ],
            "translated_text": "Predicción del rendimiento de consultas en entornos de búsqueda web Yun Zhou y W. Bruce Croft Departamento de Ciencias de la Computación Universidad de Massachusetts, Amherst {yzhou, croft}@cs.umass.edu RESUMEN Las técnicas actuales de predicción, generalmente diseñadas para consultas basadas en contenido y evaluadas típicamente en colecciones de prueba relativamente homogéneas de tamaños pequeños, enfrentan serios desafíos en entornos de búsqueda web donde las colecciones son significativamente más heterogéneas y existen diferentes tipos de tareas de recuperación. En este artículo, presentamos tres técnicas para abordar estos desafíos. Nos enfocamos en la predicción de rendimiento para dos tipos de consultas en entornos de búsqueda web: basadas en contenido y búsqueda de páginas por nombre. Nuestra evaluación se realiza principalmente en la colección GOV2. Además de evaluar nuestros modelos para los dos tipos de consultas por separado, consideramos una situación más desafiante y realista en la que los dos tipos de consultas se mezclan sin información previa sobre los tipos de consulta. Para ayudar en la predicción bajo la situación de consulta mixta, se adopta un clasificador de consultas novedoso. Los resultados muestran que nuestra predicción del rendimiento de la consulta web es sustancialmente más precisa que las técnicas de predicción actuales más avanzadas. Por consiguiente, nuestro artículo proporciona un enfoque práctico para predecir el rendimiento en entornos web del mundo real. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Formulación de Consultas Términos Generales Algoritmos, Experimentación, Teoría 1. La predicción del rendimiento de la consulta tiene muchas aplicaciones en una variedad de áreas de recuperación de información (IR), como mejorar la consistencia de la recuperación, el refinamiento de la consulta y la IR distribuida. La importancia de este problema ha sido reconocida por los investigadores en Recuperación de Información y recientemente se han propuesto varios métodos nuevos para la predicción [1, 2, 17]. La mayoría de los trabajos sobre predicción se han centrado en la tarea tradicional de recuperación ad-hoc, donde el rendimiento de la consulta se mide según la relevancia temática. Estos modelos de predicción se evalúan en colecciones de documentos de TREC que suelen consistir en no más de un millón de artículos de agencias de noticias relativamente homogéneos. Con la popularidad e influencia de la Web, las técnicas de predicción que funcionen bien para consultas de estilo web son altamente preferibles. Sin embargo, los entornos de búsqueda web plantean desafíos significativos para los modelos de predicción actuales que están principalmente diseñados para entornos tradicionales de TREC. Aquí delineamos algunos de estos desafíos. Primero, las colecciones web, que son mucho más grandes que las colecciones TREC convencionales, incluyen una variedad de documentos que son diferentes en muchos aspectos, como la calidad y el estilo. Las técnicas actuales de predicción pueden ser vulnerables a estas características de las colecciones web. Por ejemplo, la precisión de predicción reportada de la técnica de robustez de clasificación y la técnica de claridad en la colección GOV2 (una colección web grande) es significativamente peor en comparación con las otras colecciones de TREC [1]. En [2] se informa de una precisión de predicción similar en la colección GOV2 utilizando otra técnica, confirmando la dificultad de predecir el rendimiento de consultas en una gran colección web. Además, la búsqueda web va más allá del alcance de la tarea de recuperación ad hoc basada en la relevancia temática. Por ejemplo, la tarea de encontrar páginas nombradas (NP), que es una tarea de navegación, también es popular en la recuperación web. La predicción del rendimiento de la consulta para la tarea de NP sigue siendo necesaria, ya que el rendimiento de recuperación de NP está lejos de ser perfecto. De hecho, según el informe sobre la tarea NP del Terabyte Track de 2005 [3], aproximadamente el 40% de las consultas de prueba tienen un rendimiento deficiente (sin respuesta correcta en los primeros 10 resultados de búsqueda) incluso en la mejor ejecución del grupo líder. Hasta donde sabemos, poca investigación ha abordado explícitamente el problema de la predicción del rendimiento de consultas de NP. Los modelos de predicción actuales diseñados para consultas basadas en contenido serán menos efectivos para consultas de NP considerando las diferencias fundamentales entre ambas. En tercer lugar, en entornos de búsqueda web del mundo real, las consultas de los usuarios suelen ser una mezcla de diferentes tipos y, por lo general, no se dispone de información previa sobre el tipo de cada consulta. La situación de consulta mixta plantea nuevos problemas para la predicción del rendimiento de la consulta. Por ejemplo, es posible que necesitemos incorporar un clasificador de consultas en los modelos de predicción. A pesar de estos problemas, la capacidad para manejar esta situación es un paso crucial hacia convertir la predicción del rendimiento de consultas de un tema de investigación interesante en una herramienta práctica para la recuperación web. En este artículo, presentamos tres técnicas para abordar los desafíos mencionados que enfrentan los modelos de predicción actuales en entornos de búsqueda web. Nuestro trabajo se centra en la predicción del rendimiento de consultas para la tarea de recuperación basada en contenido (ad-hoc) y la tarea de encontrar páginas por nombre en el contexto de la recuperación web. Nuestra primera técnica, llamada ganancia de información ponderada (WIG), utiliza tanto características de términos individuales como de proximidad de términos para estimar la calidad de los documentos recuperados para la predicción. Observamos que WIG ofrece una precisión de predicción consistente en diversas colecciones de pruebas y tipos de consultas. Además, demostramos que se puede lograr una buena precisión de predicción para la situación de consulta mixta utilizando WIG con la ayuda de un clasificador de tipo de consulta. La retroalimentación de consultas y el cambio de rango inicial, que son nuestras técnicas de predicción segunda y tercera, funcionan bien para consultas basadas en contenido y consultas de NP respectivamente. Nuestras principales contribuciones incluyen: (1) una precisión de predicción considerablemente mejorada para consultas basadas en contenido web en comparación con varias técnicas de vanguardia. (2) nuevas técnicas para predecir con éxito el rendimiento de consultas NP. (3) una solución práctica y totalmente automática para predecir el rendimiento de consultas mixtas. Además, una contribución menor es que encontramos que el puntaje de robustez [1], que originalmente se propuso para la predicción de rendimiento, es útil para la <br>clasificación de consultas</br>. 2. TRABAJO RELACIONADO Como mencionamos en la introducción, recientemente se han propuesto varias técnicas de predicción que se centran en consultas basadas en contenido en la tarea de relevancia temática (ad-hoc). No conocemos ningún trabajo publicado que aborde otros tipos de consultas como las consultas NP, y mucho menos una mezcla de tipos de consultas. A continuación revisamos algunos modelos representativos. La principal dificultad de la predicción del rendimiento proviene del hecho de que muchos factores tienen un impacto en el rendimiento de recuperación. Cada factor afecta el rendimiento en diferente medida y el efecto general es difícil de predecir con precisión. Por lo tanto, no es sorprendente notar que características simples, como la frecuencia de los términos de consulta en la colección [4] y la IDF promedio de los términos de consulta [5], no predicen bien. De hecho, la mayoría de las técnicas exitosas se basan en medir algunas características del conjunto de documentos recuperados para estimar la dificultad del tema. Por ejemplo, la puntuación de claridad [6] mide la coherencia de una lista de documentos mediante la divergencia KL entre el modelo de consulta y el modelo de colección. El puntaje de robustez [1] cuantifica otra propiedad de una lista clasificada: la robustez de la clasificación en presencia de incertidumbre. Carmel et al. [2] encontraron que la distancia medida por la divergencia de Jensen-Shannon entre el conjunto de documentos recuperados y la colección está significativamente correlacionada con la precisión promedio. Vinay et al. [7] propusieron cuatro medidas para capturar la geometría de los documentos mejor clasificados para la predicción. La medida más efectiva es la sensibilidad a la perturbación del documento, una idea algo similar a la puntuación de robustez. Desafortunadamente, su forma de medir la sensibilidad no funciona igual de bien para consultas cortas y la precisión de la predicción disminuye considerablemente cuando se adopta una técnica de recuperación de vanguardia (como Okapi o un enfoque de modelado de lenguaje) en lugar del peso tf-idf utilizado en su artículo [16]. Las dificultades de aplicar estos modelos en entornos de búsqueda web ya han sido mencionadas. En este documento, principalmente adoptamos la puntuación de claridad y la puntuación de robustez como nuestras líneas base. Experimentalmente demostramos que las líneas base, incluso después de ser ajustadas cuidadosamente, son inadecuadas para el entorno web. Uno de nuestros modelos de predicción, WIG, está relacionado con el modelo de campo aleatorio de Markov (MRF) para la recuperación de información [8]. El modelo MRF modela directamente la dependencia de términos y se ha demostrado ser altamente efectivo en una variedad de colecciones de pruebas (especialmente colecciones web) y tareas de recuperación. Este modelo se utiliza para estimar la distribución conjunta de probabilidad sobre documentos y consultas, una parte importante de WIG. La superioridad de WIG sobre otras técnicas de predicción basadas en características unigramas, que se demostrará más adelante en nuestro artículo, coincide con la de MRF para recuperación. En otras palabras, es interesante notar que la dependencia de términos, cuando se modela adecuadamente, puede ser útil tanto para mejorar como para predecir el rendimiento de recuperación. 3. Modelos de predicción 3.1 Ganancia de Información Ponderada (WIG) Esta sección introduce un enfoque de ganancia de información ponderada que incorpora tanto características de términos únicos como de proximidad para predecir el rendimiento tanto en consultas basadas en contenido como en consultas de búsqueda de Páginas Nombradas (NP). Dado un conjunto de consultas Q={Qs} (s=1,2,..N) que incluye todas las posibles consultas de usuario y un conjunto de documentos D={Dt} (t=1,2…M), asumimos que cada par consulta-documento (Qs,Dt) es evaluado manualmente y se incluirá en una lista de relevancia si se determina que Qs es relevante para Dt. La probabilidad conjunta P(Qs,Dt) sobre las consultas Q y los documentos D denota la probabilidad de que el par (Qs,Dt) esté en la lista de relevancia. Tales suposiciones son similares a las utilizadas en [8]. Suponiendo que el usuario emite la consulta Qi ∈Q y los resultados de recuperación en respuesta a Qi es una lista clasificada L de documentos, calculamos la cantidad de información contenida en P(Qs,Dt) con respecto a Qi y L mediante la Ec.1, que es una variante de la entropía llamada entropía ponderada[13]. Los pesos en la ecuación 1 están determinados únicamente por Qi y L. En este documento, elegimos los pesos de la siguiente manera: L es el número de documentos que contiene la consulta, K es el límite superior, LT es el límite superior de documentos, y D es el peso si K está en otro caso. La clasificación de corte K es un parámetro en nuestro modelo que se discutirá más adelante. Por lo tanto, la ecuación 1 se puede simplificar de la siguiente manera: )3(),(log 1 ),( )( , ∑∈ −= LTD titsLQ Kt i DQP K DQH Desafortunadamente, la entropía ponderada ),(, tsLQ DQH i calculada por la ecuación 3, que representa la cantidad de información sobre la probabilidad de que los documentos mejor clasificados en L sean relevantes para la consulta Qi en promedio, no se puede comparar entre diferentes consultas, lo que la hace inapropiada para predecir directamente el rendimiento de la consulta. Para mitigar este problema, creamos una distribución de fondo P(Qs,C) sobre Q y D imaginando que cada documento en D es reemplazado por el mismo documento especial C que representa el uso promedio del lenguaje. En este documento, C se crea concatenando cada documento en D. A grosso modo, C es la colección (el conjunto de documentos) {Dt} sin límites de documentos. De manera similar, la entropía ponderada ),(, CQH sLQi calculada por la Ecuación 3 representa la cantidad de información sobre la probabilidad de que un documento promedio (representado por toda la colección) sea relevante para la consulta Qi. Ahora presentamos nuestro predictor de rendimiento WIG que es la ganancia de información ponderada [13] calculada como la diferencia entre ),(, tsLQ DQH i y ),(, CQH sLQi. Específicamente, dado el conjunto de consultas Qi, la colección C y la lista clasificada L de documentos, WIG se calcula de la siguiente manera: )4( ),( ),( log 1 ),( ),( log),( ),(),(),,( )(, ,, ∑∑ ∈ == −= LTD i ti ts s ts ts tsLQsLQi Kt ii CQP DQP KCQP DQP DQweight DQHCQHLCQWIG WIG calculado por la Ecuación 4 mide el cambio en la información sobre la calidad de la recuperación (en respuesta a la consulta Qi) desde un estado imaginario en el que solo se recupera un documento promedio hasta un estado posterior en el que se observan los resultados de búsqueda reales. Hacemos la hipótesis de que WIG está positivamente correlacionado con la efectividad de la recuperación porque una recuperación de alta calidad debería ser mucho más efectiva que simplemente devolver el documento promedio. El corazón de esta técnica es cómo estimar la distribución conjunta P(Qs, Dt). En el enfoque de modelado del lenguaje para la recuperación de información, se pueden aplicar fácilmente una variedad de modelos para estimar esta distribución. Aunque la mayoría de estos modelos se basan en la suposición de la bolsa de palabras, trabajos recientes sobre modelado de dependencia de términos bajo el marco de modelado de lenguaje han demostrado mejoras consistentes y significativas en la efectividad de recuperación sobre los modelos de bolsa de palabras. Inspirados por el éxito de incorporar características de proximidad de términos en modelos de lenguaje, decidimos adoptar un buen modelo de dependencia para estimar la probabilidad P(Qs,Dt). El modelo que elegimos para este artículo es el modelo de Campo Aleatorio de Markov (MRF) de Metzler y Crofts, que ya ha demostrado su superioridad en comparación con varias colecciones y diferentes tareas de recuperación [8,9]. Según el modelo MRF, log P(Qi, Dt) se puede escribir como )5()|(loglog),(log )( 1 ∑∈ +−= iQF tti DPZDQP ξ ξ ξλ donde Z1 es una constante que asegura que P(Qi, Dt) sume 1. F(Qi) consiste en un conjunto de características ampliadas a partir de la consulta original Qi. Por ejemplo, suponiendo que la consulta Qi es el programa de estudiantes talentosos, F(Qi) incluye características como programa y estudiante talentoso. Consideramos dos tipos de características: características de términos individuales T y características de proximidad P. Las características de proximidad incluyen la frase exacta (#1) y las características de ventana desordenada (#uwN) como se describe en [8]. Ten en cuenta que F(Qi) es la unión de T(Qi) y P(Qi). Para obtener más detalles sobre F(Qi), como por ejemplo cómo expandir la consulta original Qi a F(Qi), remitimos al lector a [8] y [9]. P(ξ|Dt) denota la probabilidad de que la característica ξ ocurra en Dt. Más detalles sobre P(ξ|Dt) se proporcionarán más adelante en esta sección. La elección de λξ es algo diferente a la utilizada en [8] ya que λξ desempeña un doble papel en nuestro modelo. El primer rol, que es el mismo que en [8], es ponderar entre las características de términos individuales y de proximidad. El otro rol, específico de nuestra tarea de predicción, es normalizar el tamaño de F(Qi). Encontramos que la siguiente estrategia de peso para λξ satisface los dos roles anteriores y generaliza bien en una variedad de colecciones y tipos de consultas. )6( )(, |)(| 1 )(, |)(| ⎪ ⎪ ⎩ ⎪ ⎪ ⎨ ⎧ ∈ − ∈ = i i T i i T QP QP QT QT ξ λ ξ λ λξ donde |T(Qi)| y |P(Qi)| denotan el número de características de términos únicos y de proximidad en F(Qi) respectivamente. La razón de elegir la función raíz cuadrada en el denominador de λξ es penalizar adecuadamente un conjunto de características de gran tamaño, haciendo que WIG sea más comparable entre consultas de diversas longitudes. λT es un parámetro fijo y se establece en 0.8 según [8] a lo largo de este documento. De manera similar, log P(Qi,C) se puede escribir como: )7()|(loglog),(log )( 2 ∑∈ +−= iQF i CPZCQP ξ ξ ξλ Cuando se eliminan las constantes Z1 y Z2, el WIG calculado en la Ecuación 4 se puede reescribir de la siguiente manera al sustituir la Ecuación 5 y la Ecuación 7: )8( )|( )|( log 1 ),,( )( )( ∑ ∑∈ ∈ = LTD QF t i Kt i CP DP K LCQWIG ξ ξ ξ ξ λ Una de las ventajas del WIG sobre otras técnicas es que puede manejar tanto consultas basadas en contenido como en NP. Basándose en el tipo (o el tipo predicho) de Qi, el cálculo de WIG en la Ecuación 8 difiere en dos aspectos: (1) cómo estimar P(ξ|Dt) y P(ξ|C), y (2) cómo elegir K. Para consultas basadas en contenido, P(ξ|C) se estima por la frecuencia relativa del rasgo ξ en la colección C en su totalidad. La estimación de P(ξ|Dt) es la misma que en [8]. Es decir, estimamos P(ξ|Dt) mediante la frecuencia relativa del rasgo ξ en Dt suavizada linealmente con la frecuencia de colección P(ξ|C). La K en la Ec.8 se trata como un parámetro libre. Ten en cuenta que K es el único parámetro libre en el cálculo de WIG para consultas basadas en contenido, ya que se asume que todos los parámetros involucrados en P(ξ|Dt) están fijos al tomar los valores sugeridos en [8]. En cuanto a las consultas de NP, hacemos uso de la estructura del documento para estimar P(ξ|Dt) y P(ξ|C) mediante el llamado modelo de mezcla de lenguaje propuesto en [10] e incorporado en el modelo MRF para la recuperación de Named-Page en [9]. La idea básica es que un documento (colección) se divide en varios campos como el campo del título, el campo del cuerpo principal y el campo de los encabezados. P(ξ|Dt) y P(ξ|C) se estiman mediante una combinación lineal de los modelos de lenguaje de cada campo. Debido a limitaciones de espacio, remitimos al lector a [9] para más detalles. Adoptamos el mismo conjunto exacto de parámetros utilizados en [9] para la estimación. En cuanto a K en la Ec.8, establecemos K en 1 porque la tarea de encontrar Named-Page se centra fuertemente en el documento clasificado en primer lugar. Por consiguiente, no hay parámetros libres en el cálculo de WIG para consultas de NP. 3.2 Retroalimentación de consultas En esta sección, presentamos otra técnica llamada retroalimentación de consultas (QF) para la predicción. Suponga que un usuario emite la consulta Q a un sistema de recuperación y se devuelve una lista clasificada L de documentos. Vemos el sistema de recuperación como un canal ruidoso. Específicamente, asumimos que la salida del canal es L y la entrada es Q. Después de pasar por el canal, Q se corrompe y se transforma en la lista clasificada L. Al pensar en el proceso de recuperación de esta manera, el problema de predecir la eficacia de la recuperación se convierte en la tarea de evaluar la calidad del canal. En otras palabras, la predicción se convierte en encontrar una forma de medir el grado de corrupción que surge cuando Q se transforma en L. Dado que calcular directamente el grado de corrupción es difícil, abordamos este problema mediante aproximaciones. Nuestra idea principal es que medimos en qué medida la información sobre Q puede ser recuperada de L bajo la suposición de que solo se observa L. Específicamente, diseñamos un decodificador que pueda traducir con precisión L de vuelta a la nueva consulta Q y la similitud S entre la consulta original Q y la nueva consulta Q se adopta como un predictor de rendimiento. Este es un bosquejo de cómo la técnica QF predice el rendimiento de la consulta. Antes de completar más detalles, discutimos brevemente por qué este método funcionaría. Existe una relación entre la similitud S definida anteriormente y el rendimiento de recuperación. Por un lado, si la recuperación se ha desviado del sentido original de la consulta Q, la nueva consulta Q extraída de la lista clasificada L en respuesta a Q sería muy diferente de la consulta original Q. Por otro lado, una consulta destilada de una lista clasificada que contiene muchos documentos relevantes es probable que sea similar a la consulta original. Se proporcionarán más ejemplos en apoyo de la relación más adelante. A continuación detallamos cómo construir el decodificador y cómo medir la similitud S. En esencia, el objetivo del decodificador es comprimir la lista clasificada L en unos pocos términos informativos que deberían representar el contenido de los documentos mejor clasificados en L. Nuestro enfoque para este objetivo es representar la lista clasificada L mediante un modelo de lenguaje (distribución de términos). Luego, los términos se clasifican según su contribución a la divergencia KL (Kullback-Leibler) de los modelos de lenguaje con respecto al modelo de la colección de fondo. Los términos mejor clasificados serán elegidos para formar la nueva consulta Q. Este enfoque es similar al utilizado en la Sección 4.1 de [11]. Específicamente, tomamos tres pasos para comprimir la lista clasificada L en la consulta Q sin hacer referencia a la consulta original. 1. Adoptamos el modelo de lenguaje de lista clasificada [14] para estimar un modelo de lenguaje basado en la lista clasificada L. El modelo puede escribirse como: )9()|()|()|( ∑∈ = LD LDPDwPLwP donde w es cualquier término y D es un documento. La probabilidad de que ocurra el evento D dado el evento L se estima mediante una función linealmente decreciente del rango del documento D. Cada término en P(w|L) está clasificado por la siguiente contribución de divergencia de Kullback-Leibler: )10( )|( )|( log)|( CwP LwP LwP donde P(w|C) es el modelo de colección estimado por la frecuencia relativa del término w en la colección C en su totalidad. 3. Los términos N principales clasificados por Eq.10 forman una consulta ponderada Q={(wi,ti)} i=1,N, donde wi denota el término clasificado en la posición i y el peso ti es la contribución de la divergencia KL de wi en Eq. 10. Barco de crucero daño vida marina. Estos ejemplos indican cómo la similitud entre la consulta original y la nueva se correlaciona con el rendimiento de recuperación. El parámetro N en el paso 3 se establece en 20 de manera empírica y elegir un valor más grande de N es innecesario ya que los pesos después de los primeros 20 suelen ser demasiado pequeños para marcar alguna diferencia. Para medir la similitud entre la consulta original Q y la nueva consulta Q, primero utilizamos Q para recuperar información de la misma colección. Se adopta una variante del modelo de probabilidad de consulta [15] para la recuperación. Es decir, los documentos se clasifican por: )11()|()|( ),( ∑∈ = Qtw t i ii i DwPDQP donde wi es un término en Q y ti es el peso asociado. D es un documento. Que L denote la nueva lista clasificada devuelta desde la recuperación anterior. La similitud se mide por la superposición de documentos en L y L. Específicamente, el porcentaje de documentos en los primeros K documentos de L que también están presentes en los primeros K documentos en L. El valor de corte K se trata como un parámetro libre. Aquí resumimos cómo la técnica QF predice el rendimiento dado una consulta Q y la lista clasificada asociada L. Primero obtenemos una consulta ponderada Q comprimida a partir de L mediante los tres pasos anteriores. Luego usamos Q para realizar la recuperación y la nueva lista clasificada es L. La superposición de documentos en L y L se utiliza para la predicción. 3.3 Primer Cambio de Rango (FRC) En esta sección, proponemos un método llamado primer cambio de rango (FRC) para la predicción de rendimiento para consultas de NP. Este método se deriva de la técnica de robustez de clasificación [1] que está principalmente diseñada para consultas basadas en contenido. Cuando se aplica directamente a consultas de NP, la técnica de robustez será menos efectiva porque tiene en cuenta todos los documentos mejor clasificados en su totalidad, mientras que las consultas de NP suelen tener solo un único documento relevante. En cambio, nuestra técnica se centra en el documento de rango uno mientras se mantiene la idea principal del método de robustez. Específicamente, el seudocódigo para calcular FRC se muestra en la figura 1. Lista clasificada L={Di} donde i=1,100. Di denota el documento clasificado en la posición i. (2) consulta Q 1 inicializar: (1) establecer el número de intentos J=100000 (2) contador c=0; 2 para i=1 a J 3 Perturbar cada documento en L, que el resultado sea un conjunto F={Di} donde Di denota la versión perturbada de Di. 4 Realizar la recuperación con la consulta Q en el conjunto F 5 c=c+1 si y solo si D1 está clasificado primero en el paso 4 6 fin del para 7 devolver la proporción c/J Figura 1: pseudo-código para calcular FRC FRC aproxima la probabilidad de que el documento clasificado en primer lugar en la lista original L permanezca clasificado primero incluso después de que los documentos sean perturbados. Cuanto mayor sea la probabilidad, más confianza tenemos en el documento clasificado en primer lugar. Por otro lado, en el caso extremo de un ranking aleatorio, la probabilidad sería tan baja como 0.5. Esperamos que FRC tenga una asociación positiva con el rendimiento de la consulta de NP. Adoptamos [1] para implementar el paso de perturbación del documento (paso 4 en la Fig. 1) utilizando distribuciones de Poisson. Para más detalles, remitimos al lector a [1]. 4. EVALUACIÓN Ahora presentamos los resultados de predecir el rendimiento de la consulta por nuestros modelos. Tres técnicas de vanguardia son adoptadas como nuestras líneas base. Evaluamos nuestras técnicas en una variedad de configuraciones de recuperación web. Como se mencionó anteriormente, consideramos dos tipos de consultas, es decir, consultas basadas en contenido (CB) y consultas de búsqueda de páginas con nombre (NP). Primero, supongamos que se conocen los tipos de consultas. Investigamos la correlación entre el rendimiento de recuperación predicho y el rendimiento real para ambos tipos de consultas por separado. Los resultados muestran que nuestros métodos producen mejoras considerablemente superiores a los valores de referencia. Luego consideramos un escenario más desafiante donde no hay información previa sobre los tipos de consultas disponibles. Se consideran dos subcasos. En el primero, existe solo un tipo de consulta pero el tipo real es desconocido. Suponemos una mezcla de los dos tipos de consultas en el segundo caso. Demostramos que nuestros modelos logran una buena precisión en este escenario exigente, lo que hace que la predicción sea práctica en un entorno de búsqueda web del mundo real. 4.1 Configuración Experimental Nuestra evaluación se centra en la colección GOV2 que contiene alrededor de 25 millones de documentos obtenidos de sitios web en el dominio .gov durante 2004 [3]. Creamos dos tipos de conjuntos de datos para consultas de CB y consultas de NP respectivamente. Para el tipo CB, utilizamos los temas ad-hoc de las pistas Terabyte de 2004, 2005 y 2006 y los nombramos TB04-adhoc, TB05-adhoc y TB06-adhoc respectivamente. Además, también utilizamos los temas ad-hoc de la pista Robusta de 2004 (RT04) para probar la adaptabilidad de nuestras técnicas a un entorno no web. Para consultas de NP, utilizamos los temas de búsqueda de páginas nombradas de las Terabyte Tracks de 2005 y 2006 y los denominamos TB05-NP y TB06-NP respectivamente. Todas las consultas utilizadas en nuestros experimentos son títulos de temas de TREC, ya que nos centramos en la recuperación web. La Tabla 3 resume los conjuntos de datos anteriores. La recuperación del rendimiento de las consultas individuales basadas en contenido y NP se mide mediante la precisión promedio y la tasa recíproca de la primera respuesta correcta, respectivamente. Hacemos uso del modelo de campo aleatorio de Markov tanto para la recuperación ad-hoc como para la recuperación de páginas nombradas. Adoptamos la misma configuración de parámetros de recuperación utilizada en [8,9]. El motor de búsqueda Indri [12] se utiliza para todos nuestros experimentos. Aunque no se informa aquí, también probamos el modelo de probabilidad de consulta para la recuperación ad-hoc y encontramos que los resultados cambian poco debido a la correlación muy alta entre el rendimiento de las consultas obtenido por los dos modelos de recuperación (0.96 medido por el coeficiente de Pearson). 4.2 Tipos de Consultas Conocidos Supongamos que se conocen los tipos de consultas. Tratamos cada tipo de consulta por separado y medimos la correlación con la precisión promedio (o el rango recíproco en el caso de consultas NP). Adoptamos la prueba de correlación de Pearson que refleja el grado de relación lineal entre el rendimiento de recuperación predicho y real. 4.2.1 Métodos de Consultas Basadas en Contenido Claridad Robusta JSD WIG QF WIG +QF TB04+0 5 adhoc 0.333 0.317 0.362 0.574 0.480 0.637 TB06 adhoc 0.076 0.294 N/A 0.464 0.422 0.511 Tabla 4: Coeficientes de correlación de Pearson para la correlación con la precisión promedio en las Pistas de Terabyte (ad-hoc) para la puntuación de claridad, puntuación de robustez, el método basado en JSD (citamos directamente la puntuación informada en [2]), WIG, retroalimentación de consulta (QF) y una combinación lineal de WIG y QF. Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. La Tabla 4 muestra la correlación con la precisión promedio en dos conjuntos de datos: uno es una combinación de TB04-adhoc y TB05-adhoc (100 temas en total) y el otro es TB06-adhoc (50 temas). La razón por la que juntamos TB04-adhoc y TB05-adhoc es para hacer nuestros resultados comparables a [2]. Nuestros puntos de referencia son la puntuación de claridad (claridad) [6], la puntuación de robustez (robust) [1] y el método basado en JSD (JSD) [2]. Para la puntuación de claridad y robustez, hemos probado diferentes configuraciones de parámetros y reportamos los coeficientes de correlación más altos que hemos encontrado. Citamos directamente el resultado del método basado en JSD reportado en [2]. La tabla también muestra los resultados para el método de Ganancia de Información Ponderada (WIG) y el método de Retroalimentación de Consulta (QF) para predecir consultas basadas en contenido. Como describimos en la sección anterior, tanto WIG como QF tienen un parámetro libre para ajustar, es decir, el rango de corte K. Entrenamos el parámetro en un conjunto de datos y lo probamos en el otro. Al combinar WIG y QF, se utiliza una combinación lineal simple y el peso de la combinación se aprende a partir del conjunto de datos de entrenamiento. A partir de estos resultados, podemos ver que nuestros métodos son considerablemente más precisos en comparación con los baselines. También observamos que se obtienen mejoras adicionales a partir de la combinación de WIG y QF, lo que sugiere que miden diferentes propiedades del proceso de recuperación que se relacionan con el rendimiento. Descubrimos que nuestros métodos se generalizan bien en TB06-adhoc, mientras que la correlación del puntaje de claridad con el rendimiento de recuperación en este conjunto de datos es considerablemente peor. Una investigación adicional muestra que la precisión promedio media de TB06-adhoc es de 0.342 y es aproximadamente un 10% mejor que la del primer conjunto de datos. Mientras que los otros tres métodos suelen considerar los 100 mejores documentos o menos de una lista clasificada, el método de claridad generalmente necesita los 500 mejores documentos o más para medir adecuadamente la coherencia de una lista clasificada. Un promedio de precisión media más alto hace que las listas clasificadas recuperadas por diferentes consultas sean más similares en términos de coherencia en el nivel de los primeros 500 documentos. Creemos que esta es la razón principal de la baja precisión de la puntuación de claridad en el segundo conjunto de datos. Aunque este documento se centra en un entorno de búsqueda web, es deseable que nuestras técnicas funcionen de manera consistente en otras situaciones. Con este fin, examinamos la efectividad de nuestras técnicas en la pista Robust 2004. Para nuestros métodos, dividimos equitativamente todas las consultas de prueba en cinco grupos y realizamos validación cruzada de cinco pliegues. Cada vez que utilizamos un grupo para el entrenamiento y los otros cuatro grupos para las pruebas. Hacemos uso de todas las consultas para nuestros dos puntos de referencia, es decir, la puntuación de claridad y la puntuación de robustez. Los parámetros para nuestras líneas base son los mismos que los utilizados en [1]. Los resultados mostrados en la Tabla 5 demuestran que la precisión de predicción de nuestros métodos está a la par con la de las dos líneas base sólidas. Claridad Robusta WIG QF 0.464 0.539 0.468 0.464 Tabla 5: Comparación de los coeficientes de correlación de Pearson en la pista Robusta de 2004 para la puntuación de claridad, puntuación de robustez, WIG y retroalimentación de consulta (QF). Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. Además, examinamos la sensibilidad de predicción de nuestros métodos al rango de corte K. Con respecto a WIG, es bastante robusto a K en las Pistas de Terabyte (2004-2006) mientras prefiere un valor pequeño de K como 5 en la Pista Robusta de 2004. En otras palabras, un valor pequeño de K es una elección casi óptima para ambos tipos de pistas. Teniendo en cuenta que todos los demás parámetros involucrados en WIG están fijos y, consecuentemente, son los mismos para los dos casos, esto significa que WIG puede lograr una precisión de predicción casi óptima en dos situaciones considerablemente diferentes con exactamente la misma configuración de parámetros. En cuanto a QF, prefiere un valor más grande de K, como 100 en las Pistas de Terabyte y un valor más pequeño de K, como 25 en la Pista Robusta de 2004. 4.2.2 Consultas NP Adoptamos WIG y el primer cambio de rango (FRC) para predecir el rendimiento de las consultas NP. También intentamos una combinación lineal de los dos como en la sección anterior. El peso de la combinación se obtiene del otro conjunto de datos. Utilizamos la correlación con los rangos recíprocos medidos por la prueba de correlación de Pearson para evaluar la calidad de la predicción. Los resultados se presentan en la Tabla 6. Nuevamente, nuestros puntos de referencia son la puntuación de claridad y la puntuación de robustez. Para hacer una comparación justa, ajustamos la puntuación de claridad de diferentes maneras. Descubrimos que utilizar el documento clasificado en primer lugar para construir el modelo de consulta produce la mejor precisión de predicción. También intentamos utilizar la estructura del documento mediante la combinación de modelos de lenguaje mencionados en la sección 3.1. Se obtuvo una pequeña mejora. Los coeficientes de correlación para la puntuación de claridad reportados en la Tabla 6 son los mejores que hemos encontrado. Como podemos ver, nuestros métodos superan considerablemente la técnica de puntuación de claridad en ambas ejecuciones. Esto confirma nuestra intuición de que el uso de una medida basada en la coherencia como el puntaje de claridad es inapropiado para las consultas de NP. Claridad de métodos robusta. Tabla 6: Coeficientes de correlación de Pearson para la correlación con rangos recíprocos en las pistas de Terabyte (NP) para la puntuación de claridad, puntuación de robustez, WIG, el primer cambio de rango (FRC) y una combinación lineal de WIG y FRC. Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. En cuanto a la puntuación de robustez, también ajustamos los parámetros y reportamos el mejor que hemos encontrado. Observamos una correlación negativa interesante y sorprendente con los rangos recíprocos. Explicamos este hallazgo brevemente. Un puntaje de robustez alto significa que varios documentos de alta clasificación en la lista clasificada original siguen estando muy bien clasificados después de perturbar los documentos. La existencia de tales documentos es una buena señal de alto rendimiento para consultas basadas en contenido, ya que estas consultas suelen contener una serie de documentos relevantes [1]. Sin embargo, en lo que respecta a las consultas de NP, una diferencia fundamental es que solo hay un documento relevante para cada consulta. La existencia de tales documentos puede confundir la función de clasificación y llevar a un bajo rendimiento de recuperación. Aunque existe una correlación negativa con el rendimiento de recuperación, la fuerza de la correlación es más débil y menos consistente en comparación con nuestros métodos, como se muestra en la Tabla 6. Basándonos en el análisis anterior, podemos ver que las técnicas de predicción actuales como la puntuación de claridad y la puntuación de robustez, que están principalmente diseñadas para consultas basadas en contenido, enfrentan desafíos significativos y son insuficientes para manejar consultas de NP. Nuestras dos técnicas propuestas para consultas de NP demuestran consistentemente una buena precisión de predicción, mostrando un éxito inicial en la resolución del problema de predecir el rendimiento para consultas de NP. Otro punto que queremos destacar es que el método WIG funciona bien para ambos tipos de consultas, una propiedad deseable que la mayoría de las técnicas de predicción carecen. 4.3 Tipos de Consultas Desconocidos En esta sección, realizamos dos tipos de experimentos sin acceso a etiquetas de tipo de consulta. Primero, asumimos que solo existe un tipo de consulta pero el tipo es desconocido. Segundo, experimentamos con una mezcla de consultas basadas en contenido y NP. Las dos subsecciones siguientes informarán los resultados para las dos condiciones respectivamente. 4.3.1 Solo existe un tipo. Suponemos que todas las consultas son del mismo tipo, es decir, son consultas NP o consultas basadas en contenido. Elegimos WIG para tratar este caso porque muestra una buena precisión de predicción para ambos tipos de consultas en la sección anterior. Consideramos dos casos: (1) CB: todas las 150 consultas de títulos de la tarea ad-hoc de las pistas Terabyte 2004-2006 (2) NP: todas las 433 consultas de NP de la tarea de búsqueda de páginas nombradas de las pistas Terabyte 2005 y 2006. Tomamos una estrategia simple etiquetando todas las consultas en cada caso como el mismo tipo (ya sea NP o CB) independientemente de su tipo real. El cálculo de WIG se basará en el tipo de consulta etiquetado en lugar del tipo real. Hay cuatro posibilidades con respecto a la relación entre el tipo real y el tipo etiquetado. La correlación con el rendimiento de recuperación bajo las cuatro posibilidades se presenta en la Tabla 7. Por ejemplo, el valor 0.445 en la intersección entre la segunda fila y la tercera columna muestra el coeficiente de correlación de Pearson para la correlación con la precisión promedio cuando las consultas basadas en contenido se etiquetan incorrectamente como del tipo NP. Basándonos en estos resultados, recomendamos tratar todas las consultas como del tipo NP cuando solo existe un tipo de consulta y la clasificación precisa de la consulta no es factible, considerando el riesgo de que se produzca una gran pérdida de precisión si las consultas NP se etiquetan incorrectamente como consultas basadas en contenido. Estos resultados también demuestran la fuerte adaptabilidad de WIG a diferentes tipos de consultas. Tabla 7: Comparación de los coeficientes de correlación de Pearson para la correlación con el rendimiento de recuperación en cuatro posibilidades en las pistas de Terabyte (NP). Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. 4.3.2 Una mezcla de consultas basadas en contenido y NP Una mezcla de los dos tipos de consultas es una situación más realista que un motor de búsqueda web encontrará. Evaluamos la precisión de la predicción por la exactitud con la que se pueden identificar las consultas de bajo rendimiento mediante el método de predicción, asumiendo que los tipos de consulta reales son desconocidos (pero podemos predecir los tipos de consulta). Esta es una tarea desafiante porque tanto el rendimiento predicho como el real para un tipo de consulta pueden ser incomparables con el de otro tipo. A continuación discutimos cómo implementar nuestra evaluación. Creamos un conjunto de consultas que consiste en todas las 150 consultas de título ad-hoc de Terabyte Track 2004-2006 y todas las 433 consultas NP de Terabyte Track 2005&2006. Dividimos las consultas en el conjunto en clases: buenas (mejores que el 50% de las consultas del mismo tipo en términos de rendimiento de recuperación) y malas (de lo contrario). Según estos estándares, una consulta de NP con un rango recíproco superior a 0.2 o una consulta basada en contenido con una precisión promedio superior a 0.315 se considerarán como buenas. Entonces, cada vez que seleccionamos aleatoriamente una consulta Q del grupo con una probabilidad p de que Q esté basada en el contenido. Las consultas restantes se utilizan como datos de entrenamiento. Primero decidimos el tipo de consulta Q de acuerdo con un clasificador de consultas. Es decir, el clasificador de consultas nos dice si la consulta Q es de tipo NP o basada en contenido. Basándose en el tipo de consulta predicho y en la puntuación calculada para la consulta Q por una técnica de predicción, se toma una decisión binaria sobre si la consulta Q es buena o mala al compararla con el umbral de puntuación del tipo de consulta predicho obtenido de los datos de entrenamiento. La precisión de la predicción se mide por la exactitud de la decisión binaria. En nuestra implementación, tomamos repetidamente una consulta de prueba del conjunto de consultas y la precisión de la predicción se calcula como el porcentaje de decisiones correctas, es decir, una consulta buena (mala) se predice como buena (mala). Es obvio que adivinar al azar llevará a una precisión del 50%. Tomemos el método WIG como ejemplo para ilustrar el proceso. Dos umbrales de WIG (uno para consultas de NP y otro para consultas basadas en contenido) se entrenan maximizando la precisión de predicción en los datos de entrenamiento. Cuando una consulta de prueba es etiquetada como del tipo NP (CB) por el clasificador de tipo de consulta, se predecirá que es buena solo si la puntuación WIG para esta consulta está por encima del umbral de NP (CB). Se seguirán procedimientos similares para otras técnicas de predicción. Ahora presentamos brevemente el clasificador de tipo de consulta automático utilizado en este artículo. Encontramos que la puntuación de robustez, aunque originalmente propuesta para la predicción de rendimiento, es un buen indicador de tipos de consultas. Observamos que, en promedio, las consultas basadas en contenido tienen una puntuación de robustez mucho más alta que las consultas de NP. Por ejemplo, la Figura 2 muestra las distribuciones de puntajes de robustez para consultas basadas en NP y en contenido. Según este hallazgo, el clasificador de puntuación de robustez adjuntará una etiqueta NP (CB) a la consulta si la puntuación de robustez para la consulta está por debajo (por encima) de un umbral entrenado a partir de los datos de entrenamiento. 0 0.5 1 1.5 2 2.5 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 NP Basado en contenido Figura 2: Distribución de puntuaciones de robustez para consultas NP y CB. Las consultas NP son los 252 temas NP del Terabyte Track de 2005. Las consultas basadas en contenido son los 150 títulos ad-hoc de las pistas Terabyte 2004-2006. Las distribuciones de probabilidad son estimadas por el método de estimación de densidad de Kernel. Estrategias Robustas WIG-1 WIG-2 WIG-3 Óptima p=0.6 0.565 0.624 0.665 0.684 0.701 P=0.4 0.567 0.633 0.654 0.673 0.696 Tabla 8: Comparación de la precisión de predicción para cinco estrategias en la situación de consulta mixta. Dos formas de muestrear una consulta de la piscina: (1) la consulta muestreada es basada en contenido con una probabilidad p=0.6 (es decir, la consulta es NP con una probabilidad de 0.4) (2) establecer la probabilidad p=0.4. Consideramos cinco estrategias en nuestros experimentos. En la primera estrategia (denominada robusta), utilizamos la puntuación de robustez para la predicción del rendimiento de la consulta con la ayuda de un clasificador de consultas perfecto que siempre mapea correctamente una consulta en una de las dos categorías (es decir, NP o CB). Esta estrategia representa el nivel de precisión de predicción que las técnicas actuales de predicción pueden lograr en una condición ideal en la que se conocen los tipos de consulta. En las tres estrategias siguientes, se adopta el método WIG para predecir el rendimiento. La diferencia entre los tres es que se utilizan tres clasificadores de consulta diferentes para cada estrategia: (1) el clasificador siempre clasifica una consulta en el tipo NP. (2) el clasificador de Probabilidad de Densidad de Puntuación Robusta es el clasificador de puntuación robusta mencionado anteriormente. (3) el clasificador es perfecto. Estas tres estrategias están designadas como WIG-1, WIG-2 y WIG-3 respectivamente. La razón por la que estamos interesados en WIG-1 se basa en los resultados de la sección 4.3.1. En la última estrategia (denominada Óptima) que sirve como un límite superior de lo bien que podemos hacer hasta ahora, hacemos pleno uso de nuestras técnicas de predicción para cada tipo de consulta asumiendo que un clasificador de consultas perfecto está disponible. Específicamente, combinamos linealmente WIG y QF para consultas basadas en contenido y WIG y FRC para consultas de NP. Los resultados de las cinco estrategias se muestran en la Tabla 8. Para cada estrategia, intentamos dos formas de muestrear una consulta del conjunto: (1) la consulta muestreada es CB con probabilidad p=0.6 (la consulta es NP con probabilidad 0.4) (2) establecer la probabilidad p=0.4. De la Tabla 8 podemos ver que en términos de precisión de predicción, WIG-2 (el método WIG con el clasificador de consultas automático) no solo es mejor que los dos primeros casos, sino que también está cerca de WIG-3 donde se asume un clasificador perfecto. Se observan algunas mejoras adicionales sobre WIG-3 cuando se combinan con otras técnicas de predicción. El mérito de WIG-2 es que proporciona una solución práctica para identificar automáticamente consultas con bajo rendimiento en un entorno de búsqueda web con tipos de consultas mixtos, lo cual representa considerables obstáculos para las técnicas de predicción tradicionales. CONCLUSIONES Y TRABAJOS FUTUROS Hasta donde sabemos, nuestro artículo es el primero en explorar a fondo la predicción del rendimiento de consultas en entornos de búsqueda web. Demostramos que nuestros modelos resultaron en una mayor precisión de predicción que las técnicas previamente publicadas no especialmente diseñadas para escenarios de búsqueda en la web. En este artículo, nos enfocamos en dos tipos de consultas en la búsqueda web: consultas basadas en contenido y consultas de búsqueda de Páginas-Nombradas (NP), correspondientes a la tarea de recuperación ad-hoc y la tarea de búsqueda de Páginas-Nombradas respectivamente. Para ambos tipos de consultas web, se demostró que nuestros modelos de predicción son sustancialmente más precisos que las técnicas actuales más avanzadas. Además, consideramos un caso más realista en el que no se dispone de información previa sobre los tipos de consultas. Demostramos que el método WIG es particularmente adecuado para esta situación. Considerando la adaptabilidad de WIG a una variedad de colecciones y tipos de consultas, uno de nuestros planes futuros es aplicar este método para predecir la preferencia del usuario por los resultados de búsqueda en datos realistas recopilados de un motor de búsqueda comercial. Además de la precisión, otro problema importante con el que las técnicas de predicción tienen que lidiar en un entorno web es la eficiencia. Afortunadamente, dado que el puntaje WIG se calcula solo sobre los términos y frases que aparecen en la consulta, este cálculo puede realizarse de manera muy eficiente con el soporte de un índice. Por otro lado, el cálculo de QF y FRC es relativamente menos eficiente ya que QF necesita recuperar toda la colección dos veces y FRC necesita clasificar repetidamente los documentos perturbados. Cómo mejorar la eficiencia de QF y FRC es nuestro trabajo futuro. Además, las técnicas de predicción propuestas en este documento tienen el potencial de mejorar el rendimiento de recuperación al combinarse con otras técnicas de RI. Por ejemplo, nuestras técnicas pueden ser incorporadas a técnicas populares de modificación de consultas como la expansión de consultas y la relajación de consultas. Guiados por la predicción del rendimiento, podemos tomar una mejor decisión sobre cuándo o cómo modificar las consultas para mejorar la efectividad de la recuperación. Nos gustaría llevar a cabo investigaciones en esta dirección en el futuro. AGRADECIMIENTOS Este trabajo fue apoyado en parte por el Centro de Recuperación de Información Inteligente, en parte por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el contrato número HR0011-06-C0023, y en parte por un premio de Google. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son del autor y no necesariamente reflejan las del patrocinador. Además, agradecemos a Donald Metzler por sus valiosos comentarios sobre este trabajo. 7. REFERENCIAS [1] Y. Zhou, W. B. Croft, Ranking Robustness: Un nuevo marco para predecir el rendimiento de consultas, en Actas de CIKM 2006. [2] D. Carmel, E. Yom-Tov, A. Darlow, D. Pelleg, ¿Qué hace que una consulta sea difícil?, en Actas de SIGIR 2006. [3] C. L. A. Clarke, F. Scholer, I. Soboroff, La pista Terabyte TREC 2005, En las Actas en Línea de TREC 2005. [4] B. Él y yo. Inferir el rendimiento de la consulta utilizando predictores de preretrieval. En actas de SPIRE 2004. [5] S. Tomlinson. Recuperación robusta de la web y terabytes con Hummingbird SearchServer en TREC 2004. En las Actas en Línea de TREC 2004. [6] S. Cronen-Townsend, Y. Zhou, W. B. Croft, Prediciendo el Rendimiento de Consultas, en las Actas de SIGIR 2002. [7] V. Vinay, I. J. Cox, N. Mill-Frayling, K. Wood, Sobre la Clasificación de la Efectividad del Buscador, en las Actas de SIGIR 2006. [8] D. Metzler, W. B. Croft, Un Modelo de Campo Aleatorio de Markov para Dependencias de Términos, en las Actas de SIGIR 2005. [9] D. Metzler, T. Strohman, Y. Zhou, W. B. Croft, Indri en TREC 2005: Terabyte Track, en las Actas en Línea de TREC 2004. [10] P. Ogilvie y J. Callan, Combinando representaciones de documentos para búsqueda de elementos conocidos, en las Actas de SIGIR 2003. [11] A. Berger, J. Lafferty, Recuperación de información como traducción estadística, en las Actas de SIGIR 1999. [12] Motor de búsqueda Indri: http://www.lemurproject.org/indri/ [13] I. J. Taneja: Sobre medidas de información generalizadas y sus aplicaciones, Avances en Electrónica y Física de Electrones, Academic Press (EE. UU.), 76, 1989, 327-413. [14] S. Cronen-Townsend, Y. Zhou y Croft, W. B., Un marco para la expansión selectiva de consultas, en Actas de CIKM 2004. [15] F. Song, W. B. Croft, Un modelo de lenguaje general para la recuperación de información, en Actas de SIGIR 1999. [16] Contacto por correo electrónico personal con Vishwa Vinay y nuestros propios experimentos [17] E. Yom-Tov, S. Fine, D. Carmel, A. Darlow, Aprendiendo a estimar la dificultad de la consulta incluyendo aplicaciones para la detección de contenido faltante y la recuperación de información distribuida, en Actas de SIGIR 2005. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "trec document collection": {
            "translated_key": "colección de documentos trec",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Query Performance Prediction in Web Search Environments Yun Zhou and W. Bruce Croft Department of Computer Science University of Massachusetts, Amherst {yzhou, croft}@cs.umass.edu ABSTRACT Current prediction techniques, which are generally designed for content-based queries and are typically evaluated on relatively homogenous test collections of small sizes, face serious challenges in web search environments where collections are significantly more heterogeneous and different types of retrieval tasks exist.",
                "In this paper, we present three techniques to address these challenges.",
                "We focus on performance prediction for two types of queries in web search environments: content-based and Named-Page finding.",
                "Our evaluation is mainly performed on the GOV2 collection.",
                "In addition to evaluating our models for the two types of queries separately, we consider a more challenging and realistic situation that the two types of queries are mixed together without prior information on query types.",
                "To assist prediction under the mixed-query situation, a novel query classifier is adopted.",
                "Results show that our prediction of web query performance is substantially more accurate than the current stateof-the-art prediction techniques.",
                "Consequently, our paper provides a practical approach to performance prediction in realworld web settings.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval -Query formulation General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Query performance prediction has many applications in a variety of information retrieval (IR) areas such as improving retrieval consistency, query refinement, and distributed IR.",
                "The importance of this problem has been recognized by IR researchers and a number of new methods have been proposed for prediction recently [1, 2, 17].",
                "Most work on prediction has focused on the traditional ad-hoc retrieval task where query performance is measured according to topical relevance.",
                "These prediction models are evaluated on TREC document collections which typically consist of no more than one million relatively homogenous newswire articles.",
                "With the popularity and influence of the Web, prediction techniques that will work well for web-style queries are highly preferable.",
                "However, web search environments pose significant challenges to current prediction models that are mainly designed for traditional TREC settings.",
                "Here we outline some of these challenges.",
                "First, web collections, which are much larger than conventional TREC collections, include a variety of documents that are different in many aspects such as quality and style.",
                "Current prediction techniques can be vulnerable to these characteristics of web collections.",
                "For example, the reported prediction accuracy of the ranking robustness technique and the clarity technique on the GOV2 collection (a large web collection) is significantly worse compared to the other TREC collections [1].",
                "Similar prediction accuracy on the GOV2 collection using another technique is reported in [2], confirming the difficult of predicting query performance on a large web collection.",
                "Furthermore, web search goes beyond the scope of the ad-hoc retrieval task based on topical relevance.",
                "For example, the Named-Page (NP) finding task, which is a navigational task, is also popular in web retrieval.",
                "Query performance prediction for the NP task is still necessary since NP retrieval performance is far from perfect.",
                "In fact, according to the report on the NP task of the 2005 Terabyte Track [3], about 40% of the test queries perform poorly (no correct answer in the first 10 search results) even in the best run from the top group.",
                "To our knowledge, little research has explicitly addressed the problem of NP-query performance prediction.",
                "Current prediction models devised for content-based queries will be less effective for NP queries considering the fundamental differences between the two.",
                "Third, in real-world web search environments, user queries are usually a mixture of different types and prior knowledge about the type of each query is generally unavailable.",
                "The mixed-query situation raises new problems for query performance prediction.",
                "For instance, we may need to incorporate a query classifier into prediction models.",
                "Despite these problems, the ability to handle this situation is a crucial step towards turning query performance prediction from an interesting research topic into a practical tool for web retrieval.",
                "In this paper, we present three techniques to address the above challenges that current prediction models face in Web search environments.",
                "Our work focuses on query performance prediction for the content-based (ad-hoc) retrieval task and the name-page finding task in the context of web retrieval.",
                "Our first technique, called weighted information gain (WIG), makes use of both single term and term proximity features to estimate the quality of top retrieved documents for prediction.",
                "We find that WIG offers consistent prediction accuracy across various test collections and query types.",
                "Moreover, we demonstrate that good prediction accuracy can be achieved for the mixed-query situation by using WIG with the help of a query type classifier.",
                "Query feedback and first rank change, which are our second and third prediction techniques, perform well for content-based queries and NP queries respectively.",
                "Our main contributions include: (1) considerably improved prediction accuracy for web content-based queries over several state-of-the-art techniques. (2) new techniques for successfully predicting NP-query performance. (3) a practical and fully automatic solution to predicting mixed-query performance.",
                "In addition, one minor contribution is that we find that the robustness score [1], which was originally proposed for performance prediction, is helpful for query classification. 2.",
                "RELATED WORK As we mentioned in the introduction, a number of prediction techniques have been proposed recently that focus on contentbased queries in the topical relevance (ad-hoc) task.",
                "We know of no published work that addresses other types of queries such as NP queries, let alone a mixture of query types.",
                "Next we review some representative models.",
                "The major difficulty of performance prediction comes from the fact that many factors have an impact on retrieval performance.",
                "Each factor affects performance to a different degree and the overall effect is hard to predict accurately.",
                "Therefore, it is not surprising to notice that simple features, such as the frequency of query terms in the collection [4] and the average IDF of query terms [5], do not predict well.",
                "In fact, most of the successful techniques are based on measuring some characteristics of the retrieved document set to estimate topic difficulty.",
                "For example, the clarity score [6] measures the coherence of a list of documents by the KL-divergence between the query model and the collection model.",
                "The robustness score [1] quantifies another property of a ranked list: the robustness of the ranking in the presence of uncertainty.",
                "Carmel et al. [2] found that the distance measured by the Jensen-Shannon divergence between the retrieved document set and the collection is significantly correlated to average precision.",
                "Vinay et al. [7] proposed four measures to capture the geometry of the top retrieved documents for prediction.",
                "The most effective measure is the sensitivity to document perturbation, an idea somewhat similar to the robustness score.",
                "Unfortunately, their way of measuring the sensitivity does not perform equally well for short queries and prediction accuracy drops considerably when a state-of-the-art retrieval technique (like Okapi or a language modeling approach) is adopted for retrieval instead of the tf-idf weighting used in their paper [16].",
                "The difficulties of applying these models in web search environments have already been mentioned.",
                "In this paper, we mainly adopt the clarity score and the robustness score as our baselines.",
                "We experimentally show that the baselines, even after being carefully tuned, are inadequate for the web environment.",
                "One of our prediction models, WIG, is related to the Markov random field (MRF) model for information retrieval [8].",
                "The MRF model directly models term dependence and is found be to highly effective across a variety of test collections (particularly web collections) and retrieval tasks.",
                "This model is used to estimate the joint probability distribution over documents and queries, an important part of WIG.",
                "The superiority of WIG over other prediction techniques based on unigram features, which will be demonstrated later in our paper, coincides with that of MRF for retrieval.",
                "In other word, it is interesting to note that term dependence, when being modeled appropriately, can be helpful for both improving and predicting retrieval performance. 3.",
                "PREDICTION MODELS 3.1 Weighted Information Gain (WIG) This section introduces a weighted information gain approach that incorporates both single term and proximity features for predicting performance for both content-based and Named-Page (NP) finding queries.",
                "Given a set of queries Q={Qs} (s=1,2,..N) which includes all possible user queries and a set of documents D={Dt} (t=1,2…M), we assume that each query-document pair (Qs,Dt) is manually judged and will be put in a relevance list if Qs is found to be relevant to Dt.",
                "The joint probability P(Qs,Dt) over queries Q and documents D denotes the probability that pair (Qs,Dt) will be in the relevance list.",
                "Such assumptions are similar to those used in [8].",
                "Assuming that the user issues query Qi ∈Q and the retrieval results in response to Qi is a ranked list L of documents, we calculate the amount of information contained in P(Qs,Dt) with respect to Qi and L by Eq.1 which is a variant of entropy called the weighted entropy[13].",
                "The weights in Eq.1 are solely determined by Qi and L. )1(),(log),(),( , , ∑−= ts tststsLQ DQPDQweightDQH i In this paper, we choose the weights as follows: LindocumentsKtopthecontainsLTwhere otherwise LTDandisifK DQweight K Kt ts )( )2( ,0 )(,/1 ),( ⎩ ⎨ ⎧ ∈= = The cutoff rank K is a parameter in our model that will be discussed later.",
                "Accordingly, Eq.1 can be simplified as follows: )3(),(log 1 ),( )( , ∑∈ −= LTD titsLQ Kt i DQP K DQH Unfortunately, weighted entropy ),(, tsLQ DQH i computed by Eq.3, which represents the amount of information about how likely the top ranked documents in L would be relevant to query Qi on average, cannot be compared across different queries, making it inappropriate for directly predicting query performance.",
                "To mitigate this problem, we come up with a background distribution P(Qs,C) over Q and D by imagining that every document in D is replaced by the same special document C which represents average language usage.",
                "In this paper, C is created by concatenating every document in D. Roughly speaking, C is the collection (the document set) {Dt} without document boundaries.",
                "Similarly, weighted entropy ),(, CQH sLQi calculated by Eq.3 represents the amount of information about how likely an average document (represented by the whole collection) would be relevant to query Qi.",
                "Now we introduce our performance predictor WIG which is the weighted information gain [13] computed as the difference between ),(, tsLQ DQH i and ),(, CQH sLQi .Specifically, given query Qi, collection C and ranked list L of documents, WIG is calculated as follows: )4( ),( ),( log 1 ),( ),( log),( ),(),(),,( )(, ,, ∑∑ ∈ == −= LTD i ti ts s ts ts tsLQsLQi Kt ii CQP DQP KCQP DQP DQweight DQHCQHLCQWIG WIG computed by Eq.4 measures the change in information about the quality of retrieval (in response to query Qi) from an imaginary state that only an average document is retrieved to a posterior state that the actual search results are observed.",
                "We hypothesize that WIG is positively correlated with retrieval effectiveness because high quality retrieval should be much more effective than just returning the average document.",
                "The heart of this technique is how to estimate the joint distribution P(Qs,Dt).",
                "In the language modeling approach to IR, a variety of models can be applied readily to estimate this distribution.",
                "Although most of these models are based on the bagof-words assumption, recent work on modeling term dependence under the language modeling framework have shown consistent and significant improvements in retrieval effectiveness over bagof-words models.",
                "Inspired by the success of incorporating term proximity features into language models, we decide to adopt a good dependence model to estimate the probability P(Qs,Dt).",
                "The model we chose for this paper is Metzler and Crofts Markov Random Field (MRF) model, which has already demonstrated superiority over a number of collections and different retrieval tasks [8,9].",
                "According to the MRF model, log P(Qi, Dt) can be written as )5()|(loglog),(log )( 1 ∑∈ +−= iQF tti DPZDQP ξ ξ ξλ where Z1 is a constant that ensures that P(Qi, Dt) sums up to 1.",
                "F(Qi) consists of a set of features expanded from the original query Qi .",
                "For example, assuming that query Qi is talented student program, F(Qi) includes features like program and talented student.",
                "We consider two kinds of features: single term features T and proximity features P. Proximity features include exact phrase (#1) and unordered window (#uwN) features as described in [8].",
                "Note that F(Qi) is the union of T(Qi) and P(Qi).",
                "For more details on F(Qi) such as how to expand the original query Qi to F(Qi), we refer the reader to [8] and [9].",
                "P(ξ|Dt) denotes the probability that feature ξ will occur in Dt.",
                "More details on P(ξ|Dt) will be provided later in this section.",
                "The choice of λξ is somewhat different from that used in [8] since λξ plays a dual role in our model.",
                "The first role, which is the same as in [8], is to weight between single term and proximity features.",
                "The other role, which is specific to our prediction task, is to normalize the size of F(Qi).We found that the following weight strategy for λξ satisfies the above two roles and generalizes well on a variety of collections and query types. )6( )(, |)(| 1 )(, |)(| ⎪ ⎪ ⎩ ⎪ ⎪ ⎨ ⎧ ∈ − ∈ = i i T i i T QP QP QT QT ξ λ ξ λ λξ where |T(Qi)| and |P(Qi)| denote the number of single term and proximity features in F(Qi) respectively.",
                "The reason for choosing the square root function in the denominator of λξ is to penalize a feature set of large size appropriately, making WIG more comparable across queries of various lengths. λT is a fixed parameter and set to 0.8 according to [8] throughout this paper.",
                "Similarly, log P(Qi,C) can be written as: )7()|(loglog),(log )( 2 ∑∈ +−= iQF i CPZCQP ξ ξ ξλ When constant Z1 and Z2 are dropped, WIG computed in Eq.4 can be rewritten as follows by plugging in Eq.5 and Eq.7 : )8( )|( )|( log 1 ),,( )( )( ∑ ∑∈ ∈ = LTD QF t i Kt i CP DP K LCQWIG ξ ξ ξ ξ λ One of the advantages of WIG over other techniques is that it can handle well both content-based and NP queries.",
                "Based on the type (or the predicted type) of Qi, the calculation of WIG in Eq. 8 differs in two aspects: (1) how to estimate P(ξ|Dt) and P(ξ|C), and (2) how to choose K. For content-based queries, P(ξ|C) is estimated by the relative frequency of feature ξ in collection C as a whole.",
                "The estimation of P(ξ|Dt) is the same as in [8].",
                "Namely, we estimate P(ξ|Dt) by the relative frequency of feature ξ in Dt linearly smoothed with collection frequency P(ξ|C).",
                "K in Eq.8 is treated as a free parameter.",
                "Note that K is the only free parameter in the computation of WIG for content-based queries because all parameters involved in P(ξ|Dt) are assumed to be fixed by taking the suggested values in [8].",
                "Regarding NP queries, we make use of document structure to estimate P(ξ|Dt) and P(ξ|C) by the so-called mixture of language models proposed in [10] and incorporated into the MRF model for Named-Page finding retrieval in [9].",
                "The basic idea is that a document (collection) is divided into several fields such as the title field, the main-body field and the heading field.",
                "P(ξ|Dt) and P(ξ|C) are estimated by a linear combination of the language models from each field.",
                "Due to space constraints, we refer the reader to [9] for details.",
                "We adopt the exact same set of parameters as used in [9] for estimation.",
                "With regard to K in Eq.8, we set K to 1 because the Named-Page finding task heavily focuses on the first ranked document.",
                "Consequently, there are no free parameters in the computation of WIG for NP queries. 3.2 Query Feedback In this section, we introduce another technique called query feedback (QF) for prediction.",
                "Suppose that a user issues query Q to a retrieval system and a ranked list L of documents is returned.",
                "We view the retrieval system as a noisy channel.",
                "Specifically, we assume that the output of the channel is L and the input is Q.",
                "After going through the channel, Q becomes corrupted and is transformed to ranked list L. By thinking about the retrieval process this way, the problem of predicting retrieval effectiveness turns to the task of evaluating the quality of the channel.",
                "In other words, prediction becomes finding a way to measure the degree of corruption that arises when Q is transformed to L. As directly computing the degree of the corruption is difficult, we tackle this problem by approximation.",
                "Our main idea is that we measure to what extent information on Q can be recovered from L on the assumption that only L is observed.",
                "Specifically, we design a decoder that can accurately translate L back into new query Q and the similarity S between the original query Q and the new query Q is adopted as a performance predictor.",
                "This is a sketch of how the QF technique predicts query performance.",
                "Before filling in more details, we briefly discuss why this method would work.",
                "There is a relation between the similarity S defined above and retrieval performance.",
                "On the one hand, if the retrieval has strayed from the original sense of the query Q, the new query Q extracted from ranked list L in response to Q would be very different from the original query Q.",
                "On the other hand, a query distilled from a ranked list containing many relevant documents is likely to be similar to the original query.",
                "Further examples in support of the relation will be provided later.",
                "Next we detail how to build the decoder and how to measure the similarity S. In essence, the goal of the decoder is to compress ranked list L into a few informative terms that should represent the content of the top ranked documents in L. Our approach to this goal is to represent ranked list L by a language model (distribution over terms).",
                "Then terms are ranked by their contribution to the language models KL (Kullback-Leibler) divergence from the background collection model.",
                "Top ranked terms will be chosen to form the new query Q.",
                "This approach is similar to that used in Section 4.1 of [11].",
                "Specifically, we take three steps to compress ranked list L into query Q without referring to the original query. 1.",
                "We adopt the ranked list language model [14], to estimate a language model based on ranked list L. The model can be written as: )9()|()|()|( ∑∈ = LD LDPDwPLwP where w is any term, D is a document.",
                "P(D|L) is estimated by a linearly decreasing function of the rank of document D. 2.",
                "Each term in P(w|L) is ranked by the following KL-divergence contribution: )10( )|( )|( log)|( CwP LwP LwP where P(w|C) is the collection model estimated by the relative frequency of term w in collection C as a whole. 3.",
                "The top N ranked terms by Eq.10 form a weighted query Q={(wi,ti)} i=1,N. where wi denotes the i-th ranked term and weight ti is the KL-divergence contribution of wi in Eq. 10.",
                "Term cruise ship vessel sea passenger KL contribution 0.050 0.040 0.012 0.010 0.009 Table 1: top 5 terms compressed from the ranked list in response to query Cruise ship damage sea life Two representative examples, one for a poorly performing query Cruise ship damage sea life (TREC topic 719; average precision: 0.08) and the other for a high performing query prostate cancer treatments( TREC topic 710; average precision: 0.49), are shown in Table 1 and 2 respectively.",
                "These examples indicate how the similarity between the original and the new query correlates with retrieval performance.",
                "The parameter N in step 3 is set to 20 empirically and choosing a larger value of N is unnecessary since the weights after the top 20 are usually too small to make any difference.",
                "Term prostate cancer treatment men therapy KL contribution 0.177 0.140 0.028 0.025 0.020 Table 2: top 5 terms compressed from the ranked list in response to query prostate cancer treatments To measure the similarity between original query Q and new query Q, we first use Q to do retrieval on the same collection.",
                "A variant of the query likelihood model [15] is adopted for retrieval.",
                "Namely, documents are ranked by: )11()|()|( ),( ∑∈ = Qtw t i ii i DwPDQP where wi is a term in Q and ti is the associated weight.",
                "D is a document.",
                "Let L denote the new ranked list returned from the above retrieval.",
                "The similarity is measured by the overlap of documents in L and L. Specifically, the percentage of documents in the top K documents of L that are also present in the top K documents in L. the cutoff K is treated as a free parameter.",
                "We summarize here how the QF technique predicts performance given a query Q and the associated ranked list L. We first obtain a weighted query Q compressed from L by the above three steps.",
                "Then we use Q to perform retrieval and the new ranked list is L. The overlap of documents in L and L is used for prediction. 3.3 First Rank Change (FRC) In this section, we propose a method called the first rank change (FRC) for performance prediction for NP queries.",
                "This method is derived from the ranking robustness technique [1] that is mainly designed for content-based queries.",
                "When directly applied to NP queries, the robustness technique will be less effective because it takes the top ranked documents as a whole into account while NP queries usually have only one single relevant document.",
                "Instead, our technique focuses on the first rank document while the main idea of the robustness method remains.",
                "Specifically, the pseudocode for computing FRC is shown in figure 1.",
                "Input: (1) ranked list L={Di} where i=1,100.",
                "Di denotes the i-th ranked document. (2) query Q 1 initialize: (1) set the number of trials J=100000 (2) counter c=0; 2 for i=1 to J 3 Perturb every document in L, let the outcome be a set F={Di} where Di denotes the perturbed version of Di. 4 Do retrieval with query Q on set F 5 c=c+1 if and only if D1 is ranked first in step 4 6 end of for 7 return the ratio c/J Figure 1: pseudo-code for computing FRC FRC approximates the probability that the first ranked document in the original list L will remain ranked first even after the documents are perturbed.",
                "The higher the probability is, the more confidence we have in the first ranked document.",
                "On the other hand, in the extreme case of a random ranking, the probability would be as low as 0.5.",
                "We expect that FRC has a positive association with NP query performance.",
                "We adopt [1] to implement the document perturbation step (step 4 in Fig.1) using Poisson distributions.",
                "For more details, we refer the reader to [1]. 4.",
                "EVALUATION We now present the results of predicting query performance by our models.",
                "Three state-of-the-art techniques are adopted as our baselines.",
                "We evaluate our techniques across a variety of Web retrieval settings.",
                "As mentioned before, we consider two types of queries, that is, content-based (CB) queries and Named-Page(NP) finding queries.",
                "First, suppose that the query types are known.",
                "We investigate the correlation between the predicted retrieval performance and the actual performance for both types of queries separately.",
                "Results show that our methods yield considerable improvements over the baselines.",
                "We then consider a more challenging scenario where no prior information on query types is available.",
                "Two sub-cases are considered.",
                "In the first one, there exists only one type of query but the actual type is unknown.",
                "We assume a mixture of the two query types in the second case.",
                "We demonstrate that our models achieve good accuracy under this demanding scenario, making prediction practical in a real-world Web search environment. 4.1 Experimental Setup Our evaluation focuses on the GOV2 collection which contains about 25 million documents crawled from web sites in the .gov domain during 2004 [3].",
                "We create two kinds of data set for CB queries and NP queries respectively.",
                "For the CB type, we use the ad-hoc topics of the Terabyte Tracks of 2004, 2005 and 2006 and name them TB04-adhoc, TB05-adhoc and TB06-adhoc respectively.",
                "In addition, we also use the ad-hoc topics of the 2004 Robust Track (RT04) to test the adaptability of our techniques to a non-Web environment.",
                "For NP queries, we use the Named-Page finding topics of the Terabyte Tracks of 2005 and 2006 and we name them TB05-NP and TB06-NP respectively.",
                "All queries used in our experiments are titles of TREC topics as we center on web retrieval.",
                "Table 3 summarizes the above data sets.",
                "Name Collection Topic Number Query Type TB04-adhoc GOV2 701-750 CB TB05-adhoc GOV2 751-800 CB TB06-adhoc GOV2 801-850 CB RT04 Disk 4+5 (minus CR) 301-450;601700 CB TB05-NP GOV2 NP601-NP872 NP TB06-NP GOV2 NP901-NP1081 NP Table 3: Summary of test collections and topics Retrieval performance of individual content-based and NP queries is measured by the average precision and reciprocal rank of the first correct answer respectively.",
                "We make use of the Markov Random field model for both ad-hoc and Named-Page finding retrieval.",
                "We adopt the same setting of retrieval parameters used in [8,9].",
                "The Indri search engine [12] is used for all of our experiments.",
                "Though not reported here, we also tried the query likelihood model for ad-hoc retrieval and found that the results change little because of the very high correlation between the query performances obtained by the two retrieval models (0.96 measured by Pearsons coefficient). 4.2 Known Query Types Suppose that query types are known.",
                "We treat each type of query separately and measure the correlation with average precision (or the reciprocal rank in the case of NP queries).",
                "We adopt the Pearsons correlation test which reflects the degree of linear relationship between the predicted and the actual retrieval performance. 4.2.1 Content-based Queries Methods Clarity Robust JSD WIG QF WIG +QF TB04+0 5 adhoc 0.333 0.317 0.362 0.574 0.480 0.637 TB06 adhoc 0.076 0.294 N/A 0.464 0.422 0.511 Table 4: Pearsons correlation coefficients for correlation with average precision on the Terabyte Tracks (ad-hoc) for clarity score, robustness score, the JSD-based method(we directly cites the score reported in [2]), WIG, query feedback(QF) and a linear combination of WIG and QF.",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Table 4 shows the correlation with average precision on two data sets: one is a combination of TB04-adhoc and TB05-adhoc(100 topics in total) and the other is TB06-adhoc (50 topics).",
                "The reason that we put TB04-adhoc and TB05-adhoc together is to make our results comparable to [2].",
                "Our baselines are the clarity score (clarity) [6],the robustness score (robust)[1] and the JSDbased method (JSD) [2].",
                "For the clarity and robustness score, we have tried different parameter settings and report the highest correlation coefficients we have found.",
                "We directly cite the result of the JSD-based method reported in [2].",
                "The table also shows the results for the Weighted Information Gain (WIG) method and the Query Feedback (QF) method for predicting content-based queries.",
                "As we described in the previous section, both WIG and QF have one free parameter to set, that is, the cutoff rank K. We train the parameter on one dataset and test on the other.",
                "When combining WIG and QF, a simple linear combination is used and the combination weight is learned from the training data set.",
                "From these results, we can see that our methods are considerably more accurate compared to the baselines.",
                "We also observe that further improvements are obtained from the combination of WIG and QF, suggesting that they measure different properties of the retrieval process that relate to performance.",
                "We discover that our methods generalize well on TB06-adhoc while the correlation for the clarity score with retrieval performance on this data set is considerably worse.",
                "Further investigation shows that the mean average precision of TB06-adhoc is 0.342 and is about 10% better than that of the first data set.",
                "While the other three methods typically consider the top 100 or less documents given a ranked list, the clarity method usually needs the top 500 or more documents to adequately measure the coherence of a ranked list.",
                "Higher mean average precision makes ranked lists retrieved by different queries more similar in terms of coherence at the level of top 500 documents.",
                "We believe that this is the main reason for the low accuracy of the clarity score on the second data set.",
                "Though this paper focuses on a Web search environment, it is desirable that our techniques will work consistently well in other situations.",
                "To this end, we examine the effectiveness of our techniques on the Robust 2004 Track.",
                "For our methods, we evenly divide all of the test queries into five groups and perform five-fold cross validation.",
                "Each time we use one group for training and the remaining four groups for testing.",
                "We make use of all of the queries for our two baselines, that is, the clarity score and the robustness score.",
                "The parameters for our baselines are the same as those used in [1].The results shown in Table 5 demonstrate that the prediction accuracy of our methods is on a par with that of the two strong baselines.",
                "Clarity Robust WIG QF 0.464 0.539 0.468 0.464 Table 5: Comparison of Pearsons correlation coefficients on the 2004 Robust Track for clarity score, robustness score, WIG and query feedback (QF).",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Furthermore, we examine the prediction sensitivity of our methods to the cutoff rank K. With respect to WIG, it is quite robust to K on the Terabyte Tracks (2004-2006) while it prefers a small value of K like 5 on the 2004 Robust Track.",
                "In other words, a small value of K is a nearly-optimal choice for both kinds of tracks.",
                "Considering the fact that all other parameters involved in WIG are fixed and consequently the same for the two cases, this means WIG can achieve nearly-optimal prediction accuracy in two considerably different situations with exactly the same parameter setting.",
                "Regarding QF, it prefers a larger value of K such as 100 on the Terabyte Tracks and a smaller value of K such as 25 on the 2004 Robust Track. 4.2.2 NP Queries We adopt WIG and first rank change (FRC) for predicting NPquery performance.",
                "We also try a linear combination of the two as in the previous section.",
                "The combination weight is obtained from the other data set.",
                "We use the correlation with the reciprocal ranks measured by the Pearsons correlation test to evaluate prediction quality.",
                "The results are presented in Table 6.",
                "Again, our baselines are the clarity score and the robustness score.",
                "To make a fair comparison, we tune the clarity score in different ways.",
                "We found that using the first ranked document to build the query model yields the best prediction accuracy.",
                "We also attempted to utilize document structure by using the mixture of language models mentioned in section 3.1.",
                "Little improvement was obtained.",
                "The correlation coefficients for the clarity score reported in Table 6 are the best we have found.",
                "As we can see, our methods considerably outperform the clarity score technique on both of the runs.",
                "This confirms our intuition that the use of a coherence-based measure like the clarity score is inappropriate for NP queries.",
                "Methods Clarity Robust.",
                "WIG FRC WIG+FRC TB05-NP 0.150 -0.370 0.458 0.440 0.525 TB06-NP 0.112 -0.160 0.478 0.386 0.515 Table 6: Pearsons correlation coefficients for correlation with reciprocal ranks on the Terabyte Tracks (NP) for clarity score, robustness score, WIG, the first rank change (FRC) and a linear combination of WIG and FRC.",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Regarding the robustness score, we also tune the parameters and report the best we have found.",
                "We observe an interesting and surprising negative correlation with reciprocal ranks.",
                "We explain this finding briefly.",
                "A high robustness score means that a number of top ranked documents in the original ranked list are still highly ranked after perturbing the documents.",
                "The existence of such documents is a good sign of high performance for content-based queries as these queries usually contain a number of relevant documents [1].",
                "However, with regard to NP queries, one fundamental difference is that there is only one relevant document for each query.",
                "The existence of such documents can confuse the ranking function and lead to low retrieval performance.",
                "Although the negative correlation with retrieval performance exists, the strength of the correlation is weaker and less consistent compared to our methods as shown in Table 6.",
                "Based on the above analysis, we can see that current prediction techniques like clarity score and robustness score that are mainly designed for content-based queries face significant challenges and are inadequate to deal with NP queries.",
                "Our two techniques proposed for NP queries consistently demonstrate good prediction accuracy, displaying initial success in solving the problem of predicting performance for NP queries.",
                "Another point we want to stress is that the WIG method works well for both types of queries, a desirable property that most prediction techniques lack. 4.3 Unknown Query Types In this section, we run two kinds of experiments without access to query type labels.",
                "First, we assume that only one type of query exists but the type is unknown.",
                "Second, we experiment on a mixture of content-based and NP queries.",
                "The following two subsections will report results for the two conditions respectively. 4.3.1 Only One Type exists We assume that all queries are of the same type, that is, they are either NP queries or content-based queries.",
                "We choose WIG to deal with this case because it shows good prediction accuracy for both types of queries in the previous section.",
                "We consider two cases: (1) CB: all 150 title queries from the ad-hoc task of the Terabyte Tracks 2004-2006 (2)NP: all 433 NP queries from the named page finding task of the Terabyte Tracks 2005 and 2006.",
                "We take a simple strategy by labeling all of the queries in each case as the same type (either NP or CB) regardless of their actual type.",
                "The computation of WIG will be based on the labeled query type instead of the actual type.",
                "There are four possibilities with respect to the relation between the actual type and the labeled type.",
                "The correlation with retrieval performance under the four possibilities is presented in Table 7.",
                "For example, the value 0.445 at the intersection between the second row and the third column shows the Pearsons correlation coefficient for correlation with average precision when the content-based queries are incorrectly labeled as the NP type.",
                "Based on these results, we recommend treating all queries as the NP type when only one query type exists and accurate query classification is not feasible, considering the risk that a large loss of accuracy will occur if NP queries are incorrectly labeled as content-based queries.",
                "These results also demonstrate the strong adaptability of WIG to different query types.",
                "CB (labeled) NP (labeled) CB (actual) 0.536 0.445 NP (actual) 0.174 0.467 Table 7: Comparison of Pearsons correlation coefficients for correlation with retrieval performance under four possibilities on the Terabyte Tracks (NP).",
                "Bold cases mean the results are statistically significant at the 0.01 level. 4.3.2 A mixture of contented-based and NP queries A mixture of the two types of queries is a more realistic situation that a Web search engine will meet.",
                "We evaluate prediction accuracy by how accurately poorly-performing queries can be identified by the prediction method assuming that actual query types are unknown (but we can predict query types).",
                "This is a challenging task because both the predicted and actual performance for one type of query can be incomparable to that for the other type.",
                "Next we discuss how to implement our evaluation.",
                "We create a query pool which consists of all of the 150 ad-hoc title queries from Terabyte Track 2004-2006 and all of the 433 NP queries from Terabyte Track 2005&2006.",
                "We divide the queries in the pool into classes: good (better than 50% of the queries of the same type in terms of retrieval performance) and bad (otherwise).",
                "According to these standards, a NP query with the reciprocal rank above 0.2 or a content-based query with the average precision above 0.315 will be considered as good.",
                "Then, each time we randomly select one query Q from the pool with probability p that Q is contented-based.",
                "The remaining queries are used as training data.",
                "We first decide the type of query Q according to a query classifier.",
                "Namely, the query classifier tells us whether query Q is NP or content-based.",
                "Based on the predicted query type and the score computed for query Q by a prediction technique, a binary decision is made about whether query Q is good or bad by comparing to the score threshold of the predicted query type obtained from the training data.",
                "Prediction accuracy is measured by the accuracy of the binary decision.",
                "In our implementation, we repeatedly take a test query from the query pool and prediction accuracy is computed as the percentage of correct decisions, that is, a good(bad) query is predicted to be good (bad).",
                "It is obvious that random guessing will lead to 50% accuracy.",
                "Let us take the WIG method for example to illustrate the process.",
                "Two WIG thresholds (one for NP queries and the other for content-based queries) are trained by maximizing the prediction accuracy on the training data.",
                "When a test query is labeled as the NP (CB) type by the query type classifier, it will be predicted to be good if and only if the WIG score for this query is above the NP (CB) threshold.",
                "Similar procedures will be taken for other prediction techniques.",
                "Now we briefly introduce the automatic query type classifier used in this paper.",
                "We find that the robustness score, though originally proposed for performance prediction, is a good indicator of query types.",
                "We find that on average content-based queries have a much higher robustness score than NP queries.",
                "For example, Figure 2 shows the distributions of robustness scores for NP and content-based queries.",
                "According to this finding, the robustness score classifier will attach a NP (CB) label to the query if the robustness score for the query is below (above) a threshold trained from the training data. 0 0.5 1 1.5 2 2.5 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 NP Content-based Figure 2: Distribution of robustness scores for NP and CB queries.",
                "The NP queries are the 252 NP topics from the 2005 Terabyte Track.",
                "The content-based queries are the 150 ad-hoc title from the Terabyte Tracks 2004-2006.",
                "The probability distributions are estimated by the Kernel density estimation method.",
                "Strategies Robust WIG-1 WIG-2 WIG-3 Optimal p=0.6 0.565 0.624 0.665 0.684 0.701 P=0.4 0.567 0.633 0.654 0.673 0.696 Table 8: Comparison of prediction accuracy for five strategies in the mixed-query situation.",
                "Two ways to sample a query from the pool: (1) the sampled query is content-based with the probability p=0.6. (that is, the query is NP with probability 0.4 ) (2) set the probability p=0.4.",
                "We consider five strategies in our experiments.",
                "In the first strategy (denoted by robust), we use the robustness score for query performance prediction with the help of a perfect query classifier that always correctly map a query into one of the two categories (that is, NP or CB).",
                "This strategy represents the level of prediction accuracy that current prediction techniques can achieve in an ideal condition that query types are known.",
                "In the next following three strategies, the WIG method is adopted for performance prediction.",
                "The difference among the three is that three different query classifiers are used for each strategy: (1) the classifier always classifies a query into the NP type. (2) the Robustness Score ProbabilityDensity classifier is the robust score classifier mentioned above. (3) the classifier is a perfect one.",
                "These three strategies are denoted by WIG-1, WIG-2 and WIG-3 respectively.",
                "The reason we are interested in WIG-1 is based on the results from section 4.3.1.",
                "In the last strategy (denoted by Optimal) which serves as an upper bound on how well we can do so far, we fully make use of our prediction techniques for each query type assuming a perfect query classifier is available.",
                "Specifically, we linearly combine WIG and QF for content-based queries and WIG and FRC for NP queries.",
                "The results for the five strategies are shown in Table 8.",
                "For each strategy, we try two ways to sample a query from the pool: (1) the sampled query is CB with probability p=0.6. (the query is NP with probability 0.4) (2) set the probability p=0.4.",
                "From Table 8 We can see that in terms of prediction accuracy WIG-2 (the WIG method with the automatic query classifier) is not only better than the first two cases, but also is close to WIG-3 where a perfect classifier is assumed.",
                "Some further improvements over WIG-3 are observed when combined with other prediction techniques.",
                "The merit of WIG-2 is that it provides a practical solution to automatically identifying poorly performing queries in a Web search environment with mixed query types, which poses considerable obstacles to traditional prediction techniques. 5.",
                "CONCLUSIONS AND FUTURE WORK To our knowledge, our paper is the first to thoroughly explore prediction of query performance in web search environments.",
                "We demonstrated that our models resulted in higher prediction accuracy than previously published techniques not specially devised for web search scenarios.",
                "In this paper, we focus on two types of queries in web search: content-based and Named-Page (NP) finding queries, corresponding to the ad-hoc retrieval task and the Named-Page finding task respectively.",
                "For both types of web queries, our prediction models were shown to be substantially more accurate than the current state-of-the-art techniques.",
                "Furthermore, we considered a more realistic case that no prior information on query types is available.",
                "We demonstrated that the WIG method is particularly suitable for this situation.",
                "Considering the adaptability of WIG to a range of collections and query types, one of our future plans is to apply this method to predict user preference of search results on realistic data collected from a commercial search engine.",
                "Other than accuracy, another major issue that prediction techniques have to deal with in a Web environment is efficiency.",
                "Fortunately, since the WIG score is computed just over the terms and the phrases that appear in the query, this calculation can be made very efficient with the support of index.",
                "On the other hand, the computation of QF and FRC is relatively less efficient since QF needs to retrieve the whole collection twice and FRC needs to repeatedly rank the perturbed documents.",
                "How to improve the efficiency of QF and FRC is our future work.",
                "In addition, the prediction techniques proposed in this paper have the potential of improving retrieval performance by combining with other IR techniques.",
                "For example, our techniques can be incorporated to popular query modification techniques such as query expansion and query relaxation.",
                "Guided by performance prediction, we can make a better decision on when to or how to modify queries to enhance retrieval effectiveness.",
                "We would like to carry out research in this direction in the future. 6.",
                "ACKNOWLEGMENTS This work was supported in part by the Center for Intelligent Information Retrieval, in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C0023, and in part by an award from Google.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor.",
                "In addition, we thank Donald Metzler for his valuable comments on this work. 7.",
                "REFERENCES [1] Y. Zhou ,W. B. Croft ,Ranking Robustness: A Novel Framework to Predict Query Performance, in Proceedings of CIKM 2006. [2] D.Carmel, E.Yom-Tov, A.Darlow,D.Pelleg, What Makes a Query Difficult?, in Proceedings of SIGIR 2006. [3] C.L.A.",
                "Clarke, F. Scholer, I.Soboroff, The TREC 2005 Terabyte Track, In the Online Proceedings of 2005 TREC. [4] B.",
                "He and I.Ounis.",
                "Inferring query performance using preretrieval predictors.",
                "In proceedings of the SPIRE 2004. [5] S. Tomlinson.",
                "Robust, Web and Terabyte Retrieval with Hummingbird SearchServer at TREC 2004.",
                "In the Online Proceedings of 2004 TREC. [6] S. Cronen-Townsend, Y. Zhou, W. B. Croft, Predicting Query Performance, in Proceedings of SIGIR 2002. [7] V.Vinay, I.J.Cox, N.Mill-Frayling,K.Wood, On Ranking the Effectiveness of Searcher, in Proceedings of SIGIR 2006. [8] D.Metzler, W.B.Croft, A Markov Random Filed Model for Term Dependencies, in Proceedings of SIGIR 2005. [9] D.Metzler, T.Strohman,Y.Zhou,W.B.Croft, Indri at TREC 2005: Terabyte Track, In the Online Proceedings of 2004 TREC. [10] P. Ogilvie and J. Callan, Combining document representations for known-item search, in Proceedings of SIGIR 2003. [11] A.Berger, J.Lafferty, Information retrieval as statistical translation, in Proceedings of SIGIR 1999. [12] Indri search engine : http://www.lemurproject.org/indri/ [13] I.J.",
                "Taneja: On Generalized Information Measures and Their Applications, Advances in Electronics and Electron Physics, Academic Press (USA), 76, 1989, 327-413. [14] S.Cronen-Townsend, Y. Zhou and Croft, W. B. , A Framework for Selective Query Expansion, in Proceedings of CIKM 2004. [15] F.Song, W.B.Croft, A general language model for information retrieval, in Proceedings of SIGIR 1999. [16] Personal email contact with Vishwa Vinay and our own experiments [17] E.Yom-Tov, S.Fine, D.Carmel, A.Darlow, Learning to Estimate Query Difficulty Including Applications to Missing Content Detection and Distributed Information retrieval, in Proceedings of SIGIR 2005"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "ranking robustness technique": {
            "translated_key": "técnica de robustez de clasificación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Query Performance Prediction in Web Search Environments Yun Zhou and W. Bruce Croft Department of Computer Science University of Massachusetts, Amherst {yzhou, croft}@cs.umass.edu ABSTRACT Current prediction techniques, which are generally designed for content-based queries and are typically evaluated on relatively homogenous test collections of small sizes, face serious challenges in web search environments where collections are significantly more heterogeneous and different types of retrieval tasks exist.",
                "In this paper, we present three techniques to address these challenges.",
                "We focus on performance prediction for two types of queries in web search environments: content-based and Named-Page finding.",
                "Our evaluation is mainly performed on the GOV2 collection.",
                "In addition to evaluating our models for the two types of queries separately, we consider a more challenging and realistic situation that the two types of queries are mixed together without prior information on query types.",
                "To assist prediction under the mixed-query situation, a novel query classifier is adopted.",
                "Results show that our prediction of web query performance is substantially more accurate than the current stateof-the-art prediction techniques.",
                "Consequently, our paper provides a practical approach to performance prediction in realworld web settings.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval -Query formulation General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Query performance prediction has many applications in a variety of information retrieval (IR) areas such as improving retrieval consistency, query refinement, and distributed IR.",
                "The importance of this problem has been recognized by IR researchers and a number of new methods have been proposed for prediction recently [1, 2, 17].",
                "Most work on prediction has focused on the traditional ad-hoc retrieval task where query performance is measured according to topical relevance.",
                "These prediction models are evaluated on TREC document collections which typically consist of no more than one million relatively homogenous newswire articles.",
                "With the popularity and influence of the Web, prediction techniques that will work well for web-style queries are highly preferable.",
                "However, web search environments pose significant challenges to current prediction models that are mainly designed for traditional TREC settings.",
                "Here we outline some of these challenges.",
                "First, web collections, which are much larger than conventional TREC collections, include a variety of documents that are different in many aspects such as quality and style.",
                "Current prediction techniques can be vulnerable to these characteristics of web collections.",
                "For example, the reported prediction accuracy of the <br>ranking robustness technique</br> and the clarity technique on the GOV2 collection (a large web collection) is significantly worse compared to the other TREC collections [1].",
                "Similar prediction accuracy on the GOV2 collection using another technique is reported in [2], confirming the difficult of predicting query performance on a large web collection.",
                "Furthermore, web search goes beyond the scope of the ad-hoc retrieval task based on topical relevance.",
                "For example, the Named-Page (NP) finding task, which is a navigational task, is also popular in web retrieval.",
                "Query performance prediction for the NP task is still necessary since NP retrieval performance is far from perfect.",
                "In fact, according to the report on the NP task of the 2005 Terabyte Track [3], about 40% of the test queries perform poorly (no correct answer in the first 10 search results) even in the best run from the top group.",
                "To our knowledge, little research has explicitly addressed the problem of NP-query performance prediction.",
                "Current prediction models devised for content-based queries will be less effective for NP queries considering the fundamental differences between the two.",
                "Third, in real-world web search environments, user queries are usually a mixture of different types and prior knowledge about the type of each query is generally unavailable.",
                "The mixed-query situation raises new problems for query performance prediction.",
                "For instance, we may need to incorporate a query classifier into prediction models.",
                "Despite these problems, the ability to handle this situation is a crucial step towards turning query performance prediction from an interesting research topic into a practical tool for web retrieval.",
                "In this paper, we present three techniques to address the above challenges that current prediction models face in Web search environments.",
                "Our work focuses on query performance prediction for the content-based (ad-hoc) retrieval task and the name-page finding task in the context of web retrieval.",
                "Our first technique, called weighted information gain (WIG), makes use of both single term and term proximity features to estimate the quality of top retrieved documents for prediction.",
                "We find that WIG offers consistent prediction accuracy across various test collections and query types.",
                "Moreover, we demonstrate that good prediction accuracy can be achieved for the mixed-query situation by using WIG with the help of a query type classifier.",
                "Query feedback and first rank change, which are our second and third prediction techniques, perform well for content-based queries and NP queries respectively.",
                "Our main contributions include: (1) considerably improved prediction accuracy for web content-based queries over several state-of-the-art techniques. (2) new techniques for successfully predicting NP-query performance. (3) a practical and fully automatic solution to predicting mixed-query performance.",
                "In addition, one minor contribution is that we find that the robustness score [1], which was originally proposed for performance prediction, is helpful for query classification. 2.",
                "RELATED WORK As we mentioned in the introduction, a number of prediction techniques have been proposed recently that focus on contentbased queries in the topical relevance (ad-hoc) task.",
                "We know of no published work that addresses other types of queries such as NP queries, let alone a mixture of query types.",
                "Next we review some representative models.",
                "The major difficulty of performance prediction comes from the fact that many factors have an impact on retrieval performance.",
                "Each factor affects performance to a different degree and the overall effect is hard to predict accurately.",
                "Therefore, it is not surprising to notice that simple features, such as the frequency of query terms in the collection [4] and the average IDF of query terms [5], do not predict well.",
                "In fact, most of the successful techniques are based on measuring some characteristics of the retrieved document set to estimate topic difficulty.",
                "For example, the clarity score [6] measures the coherence of a list of documents by the KL-divergence between the query model and the collection model.",
                "The robustness score [1] quantifies another property of a ranked list: the robustness of the ranking in the presence of uncertainty.",
                "Carmel et al. [2] found that the distance measured by the Jensen-Shannon divergence between the retrieved document set and the collection is significantly correlated to average precision.",
                "Vinay et al. [7] proposed four measures to capture the geometry of the top retrieved documents for prediction.",
                "The most effective measure is the sensitivity to document perturbation, an idea somewhat similar to the robustness score.",
                "Unfortunately, their way of measuring the sensitivity does not perform equally well for short queries and prediction accuracy drops considerably when a state-of-the-art retrieval technique (like Okapi or a language modeling approach) is adopted for retrieval instead of the tf-idf weighting used in their paper [16].",
                "The difficulties of applying these models in web search environments have already been mentioned.",
                "In this paper, we mainly adopt the clarity score and the robustness score as our baselines.",
                "We experimentally show that the baselines, even after being carefully tuned, are inadequate for the web environment.",
                "One of our prediction models, WIG, is related to the Markov random field (MRF) model for information retrieval [8].",
                "The MRF model directly models term dependence and is found be to highly effective across a variety of test collections (particularly web collections) and retrieval tasks.",
                "This model is used to estimate the joint probability distribution over documents and queries, an important part of WIG.",
                "The superiority of WIG over other prediction techniques based on unigram features, which will be demonstrated later in our paper, coincides with that of MRF for retrieval.",
                "In other word, it is interesting to note that term dependence, when being modeled appropriately, can be helpful for both improving and predicting retrieval performance. 3.",
                "PREDICTION MODELS 3.1 Weighted Information Gain (WIG) This section introduces a weighted information gain approach that incorporates both single term and proximity features for predicting performance for both content-based and Named-Page (NP) finding queries.",
                "Given a set of queries Q={Qs} (s=1,2,..N) which includes all possible user queries and a set of documents D={Dt} (t=1,2…M), we assume that each query-document pair (Qs,Dt) is manually judged and will be put in a relevance list if Qs is found to be relevant to Dt.",
                "The joint probability P(Qs,Dt) over queries Q and documents D denotes the probability that pair (Qs,Dt) will be in the relevance list.",
                "Such assumptions are similar to those used in [8].",
                "Assuming that the user issues query Qi ∈Q and the retrieval results in response to Qi is a ranked list L of documents, we calculate the amount of information contained in P(Qs,Dt) with respect to Qi and L by Eq.1 which is a variant of entropy called the weighted entropy[13].",
                "The weights in Eq.1 are solely determined by Qi and L. )1(),(log),(),( , , ∑−= ts tststsLQ DQPDQweightDQH i In this paper, we choose the weights as follows: LindocumentsKtopthecontainsLTwhere otherwise LTDandisifK DQweight K Kt ts )( )2( ,0 )(,/1 ),( ⎩ ⎨ ⎧ ∈= = The cutoff rank K is a parameter in our model that will be discussed later.",
                "Accordingly, Eq.1 can be simplified as follows: )3(),(log 1 ),( )( , ∑∈ −= LTD titsLQ Kt i DQP K DQH Unfortunately, weighted entropy ),(, tsLQ DQH i computed by Eq.3, which represents the amount of information about how likely the top ranked documents in L would be relevant to query Qi on average, cannot be compared across different queries, making it inappropriate for directly predicting query performance.",
                "To mitigate this problem, we come up with a background distribution P(Qs,C) over Q and D by imagining that every document in D is replaced by the same special document C which represents average language usage.",
                "In this paper, C is created by concatenating every document in D. Roughly speaking, C is the collection (the document set) {Dt} without document boundaries.",
                "Similarly, weighted entropy ),(, CQH sLQi calculated by Eq.3 represents the amount of information about how likely an average document (represented by the whole collection) would be relevant to query Qi.",
                "Now we introduce our performance predictor WIG which is the weighted information gain [13] computed as the difference between ),(, tsLQ DQH i and ),(, CQH sLQi .Specifically, given query Qi, collection C and ranked list L of documents, WIG is calculated as follows: )4( ),( ),( log 1 ),( ),( log),( ),(),(),,( )(, ,, ∑∑ ∈ == −= LTD i ti ts s ts ts tsLQsLQi Kt ii CQP DQP KCQP DQP DQweight DQHCQHLCQWIG WIG computed by Eq.4 measures the change in information about the quality of retrieval (in response to query Qi) from an imaginary state that only an average document is retrieved to a posterior state that the actual search results are observed.",
                "We hypothesize that WIG is positively correlated with retrieval effectiveness because high quality retrieval should be much more effective than just returning the average document.",
                "The heart of this technique is how to estimate the joint distribution P(Qs,Dt).",
                "In the language modeling approach to IR, a variety of models can be applied readily to estimate this distribution.",
                "Although most of these models are based on the bagof-words assumption, recent work on modeling term dependence under the language modeling framework have shown consistent and significant improvements in retrieval effectiveness over bagof-words models.",
                "Inspired by the success of incorporating term proximity features into language models, we decide to adopt a good dependence model to estimate the probability P(Qs,Dt).",
                "The model we chose for this paper is Metzler and Crofts Markov Random Field (MRF) model, which has already demonstrated superiority over a number of collections and different retrieval tasks [8,9].",
                "According to the MRF model, log P(Qi, Dt) can be written as )5()|(loglog),(log )( 1 ∑∈ +−= iQF tti DPZDQP ξ ξ ξλ where Z1 is a constant that ensures that P(Qi, Dt) sums up to 1.",
                "F(Qi) consists of a set of features expanded from the original query Qi .",
                "For example, assuming that query Qi is talented student program, F(Qi) includes features like program and talented student.",
                "We consider two kinds of features: single term features T and proximity features P. Proximity features include exact phrase (#1) and unordered window (#uwN) features as described in [8].",
                "Note that F(Qi) is the union of T(Qi) and P(Qi).",
                "For more details on F(Qi) such as how to expand the original query Qi to F(Qi), we refer the reader to [8] and [9].",
                "P(ξ|Dt) denotes the probability that feature ξ will occur in Dt.",
                "More details on P(ξ|Dt) will be provided later in this section.",
                "The choice of λξ is somewhat different from that used in [8] since λξ plays a dual role in our model.",
                "The first role, which is the same as in [8], is to weight between single term and proximity features.",
                "The other role, which is specific to our prediction task, is to normalize the size of F(Qi).We found that the following weight strategy for λξ satisfies the above two roles and generalizes well on a variety of collections and query types. )6( )(, |)(| 1 )(, |)(| ⎪ ⎪ ⎩ ⎪ ⎪ ⎨ ⎧ ∈ − ∈ = i i T i i T QP QP QT QT ξ λ ξ λ λξ where |T(Qi)| and |P(Qi)| denote the number of single term and proximity features in F(Qi) respectively.",
                "The reason for choosing the square root function in the denominator of λξ is to penalize a feature set of large size appropriately, making WIG more comparable across queries of various lengths. λT is a fixed parameter and set to 0.8 according to [8] throughout this paper.",
                "Similarly, log P(Qi,C) can be written as: )7()|(loglog),(log )( 2 ∑∈ +−= iQF i CPZCQP ξ ξ ξλ When constant Z1 and Z2 are dropped, WIG computed in Eq.4 can be rewritten as follows by plugging in Eq.5 and Eq.7 : )8( )|( )|( log 1 ),,( )( )( ∑ ∑∈ ∈ = LTD QF t i Kt i CP DP K LCQWIG ξ ξ ξ ξ λ One of the advantages of WIG over other techniques is that it can handle well both content-based and NP queries.",
                "Based on the type (or the predicted type) of Qi, the calculation of WIG in Eq. 8 differs in two aspects: (1) how to estimate P(ξ|Dt) and P(ξ|C), and (2) how to choose K. For content-based queries, P(ξ|C) is estimated by the relative frequency of feature ξ in collection C as a whole.",
                "The estimation of P(ξ|Dt) is the same as in [8].",
                "Namely, we estimate P(ξ|Dt) by the relative frequency of feature ξ in Dt linearly smoothed with collection frequency P(ξ|C).",
                "K in Eq.8 is treated as a free parameter.",
                "Note that K is the only free parameter in the computation of WIG for content-based queries because all parameters involved in P(ξ|Dt) are assumed to be fixed by taking the suggested values in [8].",
                "Regarding NP queries, we make use of document structure to estimate P(ξ|Dt) and P(ξ|C) by the so-called mixture of language models proposed in [10] and incorporated into the MRF model for Named-Page finding retrieval in [9].",
                "The basic idea is that a document (collection) is divided into several fields such as the title field, the main-body field and the heading field.",
                "P(ξ|Dt) and P(ξ|C) are estimated by a linear combination of the language models from each field.",
                "Due to space constraints, we refer the reader to [9] for details.",
                "We adopt the exact same set of parameters as used in [9] for estimation.",
                "With regard to K in Eq.8, we set K to 1 because the Named-Page finding task heavily focuses on the first ranked document.",
                "Consequently, there are no free parameters in the computation of WIG for NP queries. 3.2 Query Feedback In this section, we introduce another technique called query feedback (QF) for prediction.",
                "Suppose that a user issues query Q to a retrieval system and a ranked list L of documents is returned.",
                "We view the retrieval system as a noisy channel.",
                "Specifically, we assume that the output of the channel is L and the input is Q.",
                "After going through the channel, Q becomes corrupted and is transformed to ranked list L. By thinking about the retrieval process this way, the problem of predicting retrieval effectiveness turns to the task of evaluating the quality of the channel.",
                "In other words, prediction becomes finding a way to measure the degree of corruption that arises when Q is transformed to L. As directly computing the degree of the corruption is difficult, we tackle this problem by approximation.",
                "Our main idea is that we measure to what extent information on Q can be recovered from L on the assumption that only L is observed.",
                "Specifically, we design a decoder that can accurately translate L back into new query Q and the similarity S between the original query Q and the new query Q is adopted as a performance predictor.",
                "This is a sketch of how the QF technique predicts query performance.",
                "Before filling in more details, we briefly discuss why this method would work.",
                "There is a relation between the similarity S defined above and retrieval performance.",
                "On the one hand, if the retrieval has strayed from the original sense of the query Q, the new query Q extracted from ranked list L in response to Q would be very different from the original query Q.",
                "On the other hand, a query distilled from a ranked list containing many relevant documents is likely to be similar to the original query.",
                "Further examples in support of the relation will be provided later.",
                "Next we detail how to build the decoder and how to measure the similarity S. In essence, the goal of the decoder is to compress ranked list L into a few informative terms that should represent the content of the top ranked documents in L. Our approach to this goal is to represent ranked list L by a language model (distribution over terms).",
                "Then terms are ranked by their contribution to the language models KL (Kullback-Leibler) divergence from the background collection model.",
                "Top ranked terms will be chosen to form the new query Q.",
                "This approach is similar to that used in Section 4.1 of [11].",
                "Specifically, we take three steps to compress ranked list L into query Q without referring to the original query. 1.",
                "We adopt the ranked list language model [14], to estimate a language model based on ranked list L. The model can be written as: )9()|()|()|( ∑∈ = LD LDPDwPLwP where w is any term, D is a document.",
                "P(D|L) is estimated by a linearly decreasing function of the rank of document D. 2.",
                "Each term in P(w|L) is ranked by the following KL-divergence contribution: )10( )|( )|( log)|( CwP LwP LwP where P(w|C) is the collection model estimated by the relative frequency of term w in collection C as a whole. 3.",
                "The top N ranked terms by Eq.10 form a weighted query Q={(wi,ti)} i=1,N. where wi denotes the i-th ranked term and weight ti is the KL-divergence contribution of wi in Eq. 10.",
                "Term cruise ship vessel sea passenger KL contribution 0.050 0.040 0.012 0.010 0.009 Table 1: top 5 terms compressed from the ranked list in response to query Cruise ship damage sea life Two representative examples, one for a poorly performing query Cruise ship damage sea life (TREC topic 719; average precision: 0.08) and the other for a high performing query prostate cancer treatments( TREC topic 710; average precision: 0.49), are shown in Table 1 and 2 respectively.",
                "These examples indicate how the similarity between the original and the new query correlates with retrieval performance.",
                "The parameter N in step 3 is set to 20 empirically and choosing a larger value of N is unnecessary since the weights after the top 20 are usually too small to make any difference.",
                "Term prostate cancer treatment men therapy KL contribution 0.177 0.140 0.028 0.025 0.020 Table 2: top 5 terms compressed from the ranked list in response to query prostate cancer treatments To measure the similarity between original query Q and new query Q, we first use Q to do retrieval on the same collection.",
                "A variant of the query likelihood model [15] is adopted for retrieval.",
                "Namely, documents are ranked by: )11()|()|( ),( ∑∈ = Qtw t i ii i DwPDQP where wi is a term in Q and ti is the associated weight.",
                "D is a document.",
                "Let L denote the new ranked list returned from the above retrieval.",
                "The similarity is measured by the overlap of documents in L and L. Specifically, the percentage of documents in the top K documents of L that are also present in the top K documents in L. the cutoff K is treated as a free parameter.",
                "We summarize here how the QF technique predicts performance given a query Q and the associated ranked list L. We first obtain a weighted query Q compressed from L by the above three steps.",
                "Then we use Q to perform retrieval and the new ranked list is L. The overlap of documents in L and L is used for prediction. 3.3 First Rank Change (FRC) In this section, we propose a method called the first rank change (FRC) for performance prediction for NP queries.",
                "This method is derived from the <br>ranking robustness technique</br> [1] that is mainly designed for content-based queries.",
                "When directly applied to NP queries, the robustness technique will be less effective because it takes the top ranked documents as a whole into account while NP queries usually have only one single relevant document.",
                "Instead, our technique focuses on the first rank document while the main idea of the robustness method remains.",
                "Specifically, the pseudocode for computing FRC is shown in figure 1.",
                "Input: (1) ranked list L={Di} where i=1,100.",
                "Di denotes the i-th ranked document. (2) query Q 1 initialize: (1) set the number of trials J=100000 (2) counter c=0; 2 for i=1 to J 3 Perturb every document in L, let the outcome be a set F={Di} where Di denotes the perturbed version of Di. 4 Do retrieval with query Q on set F 5 c=c+1 if and only if D1 is ranked first in step 4 6 end of for 7 return the ratio c/J Figure 1: pseudo-code for computing FRC FRC approximates the probability that the first ranked document in the original list L will remain ranked first even after the documents are perturbed.",
                "The higher the probability is, the more confidence we have in the first ranked document.",
                "On the other hand, in the extreme case of a random ranking, the probability would be as low as 0.5.",
                "We expect that FRC has a positive association with NP query performance.",
                "We adopt [1] to implement the document perturbation step (step 4 in Fig.1) using Poisson distributions.",
                "For more details, we refer the reader to [1]. 4.",
                "EVALUATION We now present the results of predicting query performance by our models.",
                "Three state-of-the-art techniques are adopted as our baselines.",
                "We evaluate our techniques across a variety of Web retrieval settings.",
                "As mentioned before, we consider two types of queries, that is, content-based (CB) queries and Named-Page(NP) finding queries.",
                "First, suppose that the query types are known.",
                "We investigate the correlation between the predicted retrieval performance and the actual performance for both types of queries separately.",
                "Results show that our methods yield considerable improvements over the baselines.",
                "We then consider a more challenging scenario where no prior information on query types is available.",
                "Two sub-cases are considered.",
                "In the first one, there exists only one type of query but the actual type is unknown.",
                "We assume a mixture of the two query types in the second case.",
                "We demonstrate that our models achieve good accuracy under this demanding scenario, making prediction practical in a real-world Web search environment. 4.1 Experimental Setup Our evaluation focuses on the GOV2 collection which contains about 25 million documents crawled from web sites in the .gov domain during 2004 [3].",
                "We create two kinds of data set for CB queries and NP queries respectively.",
                "For the CB type, we use the ad-hoc topics of the Terabyte Tracks of 2004, 2005 and 2006 and name them TB04-adhoc, TB05-adhoc and TB06-adhoc respectively.",
                "In addition, we also use the ad-hoc topics of the 2004 Robust Track (RT04) to test the adaptability of our techniques to a non-Web environment.",
                "For NP queries, we use the Named-Page finding topics of the Terabyte Tracks of 2005 and 2006 and we name them TB05-NP and TB06-NP respectively.",
                "All queries used in our experiments are titles of TREC topics as we center on web retrieval.",
                "Table 3 summarizes the above data sets.",
                "Name Collection Topic Number Query Type TB04-adhoc GOV2 701-750 CB TB05-adhoc GOV2 751-800 CB TB06-adhoc GOV2 801-850 CB RT04 Disk 4+5 (minus CR) 301-450;601700 CB TB05-NP GOV2 NP601-NP872 NP TB06-NP GOV2 NP901-NP1081 NP Table 3: Summary of test collections and topics Retrieval performance of individual content-based and NP queries is measured by the average precision and reciprocal rank of the first correct answer respectively.",
                "We make use of the Markov Random field model for both ad-hoc and Named-Page finding retrieval.",
                "We adopt the same setting of retrieval parameters used in [8,9].",
                "The Indri search engine [12] is used for all of our experiments.",
                "Though not reported here, we also tried the query likelihood model for ad-hoc retrieval and found that the results change little because of the very high correlation between the query performances obtained by the two retrieval models (0.96 measured by Pearsons coefficient). 4.2 Known Query Types Suppose that query types are known.",
                "We treat each type of query separately and measure the correlation with average precision (or the reciprocal rank in the case of NP queries).",
                "We adopt the Pearsons correlation test which reflects the degree of linear relationship between the predicted and the actual retrieval performance. 4.2.1 Content-based Queries Methods Clarity Robust JSD WIG QF WIG +QF TB04+0 5 adhoc 0.333 0.317 0.362 0.574 0.480 0.637 TB06 adhoc 0.076 0.294 N/A 0.464 0.422 0.511 Table 4: Pearsons correlation coefficients for correlation with average precision on the Terabyte Tracks (ad-hoc) for clarity score, robustness score, the JSD-based method(we directly cites the score reported in [2]), WIG, query feedback(QF) and a linear combination of WIG and QF.",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Table 4 shows the correlation with average precision on two data sets: one is a combination of TB04-adhoc and TB05-adhoc(100 topics in total) and the other is TB06-adhoc (50 topics).",
                "The reason that we put TB04-adhoc and TB05-adhoc together is to make our results comparable to [2].",
                "Our baselines are the clarity score (clarity) [6],the robustness score (robust)[1] and the JSDbased method (JSD) [2].",
                "For the clarity and robustness score, we have tried different parameter settings and report the highest correlation coefficients we have found.",
                "We directly cite the result of the JSD-based method reported in [2].",
                "The table also shows the results for the Weighted Information Gain (WIG) method and the Query Feedback (QF) method for predicting content-based queries.",
                "As we described in the previous section, both WIG and QF have one free parameter to set, that is, the cutoff rank K. We train the parameter on one dataset and test on the other.",
                "When combining WIG and QF, a simple linear combination is used and the combination weight is learned from the training data set.",
                "From these results, we can see that our methods are considerably more accurate compared to the baselines.",
                "We also observe that further improvements are obtained from the combination of WIG and QF, suggesting that they measure different properties of the retrieval process that relate to performance.",
                "We discover that our methods generalize well on TB06-adhoc while the correlation for the clarity score with retrieval performance on this data set is considerably worse.",
                "Further investigation shows that the mean average precision of TB06-adhoc is 0.342 and is about 10% better than that of the first data set.",
                "While the other three methods typically consider the top 100 or less documents given a ranked list, the clarity method usually needs the top 500 or more documents to adequately measure the coherence of a ranked list.",
                "Higher mean average precision makes ranked lists retrieved by different queries more similar in terms of coherence at the level of top 500 documents.",
                "We believe that this is the main reason for the low accuracy of the clarity score on the second data set.",
                "Though this paper focuses on a Web search environment, it is desirable that our techniques will work consistently well in other situations.",
                "To this end, we examine the effectiveness of our techniques on the Robust 2004 Track.",
                "For our methods, we evenly divide all of the test queries into five groups and perform five-fold cross validation.",
                "Each time we use one group for training and the remaining four groups for testing.",
                "We make use of all of the queries for our two baselines, that is, the clarity score and the robustness score.",
                "The parameters for our baselines are the same as those used in [1].The results shown in Table 5 demonstrate that the prediction accuracy of our methods is on a par with that of the two strong baselines.",
                "Clarity Robust WIG QF 0.464 0.539 0.468 0.464 Table 5: Comparison of Pearsons correlation coefficients on the 2004 Robust Track for clarity score, robustness score, WIG and query feedback (QF).",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Furthermore, we examine the prediction sensitivity of our methods to the cutoff rank K. With respect to WIG, it is quite robust to K on the Terabyte Tracks (2004-2006) while it prefers a small value of K like 5 on the 2004 Robust Track.",
                "In other words, a small value of K is a nearly-optimal choice for both kinds of tracks.",
                "Considering the fact that all other parameters involved in WIG are fixed and consequently the same for the two cases, this means WIG can achieve nearly-optimal prediction accuracy in two considerably different situations with exactly the same parameter setting.",
                "Regarding QF, it prefers a larger value of K such as 100 on the Terabyte Tracks and a smaller value of K such as 25 on the 2004 Robust Track. 4.2.2 NP Queries We adopt WIG and first rank change (FRC) for predicting NPquery performance.",
                "We also try a linear combination of the two as in the previous section.",
                "The combination weight is obtained from the other data set.",
                "We use the correlation with the reciprocal ranks measured by the Pearsons correlation test to evaluate prediction quality.",
                "The results are presented in Table 6.",
                "Again, our baselines are the clarity score and the robustness score.",
                "To make a fair comparison, we tune the clarity score in different ways.",
                "We found that using the first ranked document to build the query model yields the best prediction accuracy.",
                "We also attempted to utilize document structure by using the mixture of language models mentioned in section 3.1.",
                "Little improvement was obtained.",
                "The correlation coefficients for the clarity score reported in Table 6 are the best we have found.",
                "As we can see, our methods considerably outperform the clarity score technique on both of the runs.",
                "This confirms our intuition that the use of a coherence-based measure like the clarity score is inappropriate for NP queries.",
                "Methods Clarity Robust.",
                "WIG FRC WIG+FRC TB05-NP 0.150 -0.370 0.458 0.440 0.525 TB06-NP 0.112 -0.160 0.478 0.386 0.515 Table 6: Pearsons correlation coefficients for correlation with reciprocal ranks on the Terabyte Tracks (NP) for clarity score, robustness score, WIG, the first rank change (FRC) and a linear combination of WIG and FRC.",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Regarding the robustness score, we also tune the parameters and report the best we have found.",
                "We observe an interesting and surprising negative correlation with reciprocal ranks.",
                "We explain this finding briefly.",
                "A high robustness score means that a number of top ranked documents in the original ranked list are still highly ranked after perturbing the documents.",
                "The existence of such documents is a good sign of high performance for content-based queries as these queries usually contain a number of relevant documents [1].",
                "However, with regard to NP queries, one fundamental difference is that there is only one relevant document for each query.",
                "The existence of such documents can confuse the ranking function and lead to low retrieval performance.",
                "Although the negative correlation with retrieval performance exists, the strength of the correlation is weaker and less consistent compared to our methods as shown in Table 6.",
                "Based on the above analysis, we can see that current prediction techniques like clarity score and robustness score that are mainly designed for content-based queries face significant challenges and are inadequate to deal with NP queries.",
                "Our two techniques proposed for NP queries consistently demonstrate good prediction accuracy, displaying initial success in solving the problem of predicting performance for NP queries.",
                "Another point we want to stress is that the WIG method works well for both types of queries, a desirable property that most prediction techniques lack. 4.3 Unknown Query Types In this section, we run two kinds of experiments without access to query type labels.",
                "First, we assume that only one type of query exists but the type is unknown.",
                "Second, we experiment on a mixture of content-based and NP queries.",
                "The following two subsections will report results for the two conditions respectively. 4.3.1 Only One Type exists We assume that all queries are of the same type, that is, they are either NP queries or content-based queries.",
                "We choose WIG to deal with this case because it shows good prediction accuracy for both types of queries in the previous section.",
                "We consider two cases: (1) CB: all 150 title queries from the ad-hoc task of the Terabyte Tracks 2004-2006 (2)NP: all 433 NP queries from the named page finding task of the Terabyte Tracks 2005 and 2006.",
                "We take a simple strategy by labeling all of the queries in each case as the same type (either NP or CB) regardless of their actual type.",
                "The computation of WIG will be based on the labeled query type instead of the actual type.",
                "There are four possibilities with respect to the relation between the actual type and the labeled type.",
                "The correlation with retrieval performance under the four possibilities is presented in Table 7.",
                "For example, the value 0.445 at the intersection between the second row and the third column shows the Pearsons correlation coefficient for correlation with average precision when the content-based queries are incorrectly labeled as the NP type.",
                "Based on these results, we recommend treating all queries as the NP type when only one query type exists and accurate query classification is not feasible, considering the risk that a large loss of accuracy will occur if NP queries are incorrectly labeled as content-based queries.",
                "These results also demonstrate the strong adaptability of WIG to different query types.",
                "CB (labeled) NP (labeled) CB (actual) 0.536 0.445 NP (actual) 0.174 0.467 Table 7: Comparison of Pearsons correlation coefficients for correlation with retrieval performance under four possibilities on the Terabyte Tracks (NP).",
                "Bold cases mean the results are statistically significant at the 0.01 level. 4.3.2 A mixture of contented-based and NP queries A mixture of the two types of queries is a more realistic situation that a Web search engine will meet.",
                "We evaluate prediction accuracy by how accurately poorly-performing queries can be identified by the prediction method assuming that actual query types are unknown (but we can predict query types).",
                "This is a challenging task because both the predicted and actual performance for one type of query can be incomparable to that for the other type.",
                "Next we discuss how to implement our evaluation.",
                "We create a query pool which consists of all of the 150 ad-hoc title queries from Terabyte Track 2004-2006 and all of the 433 NP queries from Terabyte Track 2005&2006.",
                "We divide the queries in the pool into classes: good (better than 50% of the queries of the same type in terms of retrieval performance) and bad (otherwise).",
                "According to these standards, a NP query with the reciprocal rank above 0.2 or a content-based query with the average precision above 0.315 will be considered as good.",
                "Then, each time we randomly select one query Q from the pool with probability p that Q is contented-based.",
                "The remaining queries are used as training data.",
                "We first decide the type of query Q according to a query classifier.",
                "Namely, the query classifier tells us whether query Q is NP or content-based.",
                "Based on the predicted query type and the score computed for query Q by a prediction technique, a binary decision is made about whether query Q is good or bad by comparing to the score threshold of the predicted query type obtained from the training data.",
                "Prediction accuracy is measured by the accuracy of the binary decision.",
                "In our implementation, we repeatedly take a test query from the query pool and prediction accuracy is computed as the percentage of correct decisions, that is, a good(bad) query is predicted to be good (bad).",
                "It is obvious that random guessing will lead to 50% accuracy.",
                "Let us take the WIG method for example to illustrate the process.",
                "Two WIG thresholds (one for NP queries and the other for content-based queries) are trained by maximizing the prediction accuracy on the training data.",
                "When a test query is labeled as the NP (CB) type by the query type classifier, it will be predicted to be good if and only if the WIG score for this query is above the NP (CB) threshold.",
                "Similar procedures will be taken for other prediction techniques.",
                "Now we briefly introduce the automatic query type classifier used in this paper.",
                "We find that the robustness score, though originally proposed for performance prediction, is a good indicator of query types.",
                "We find that on average content-based queries have a much higher robustness score than NP queries.",
                "For example, Figure 2 shows the distributions of robustness scores for NP and content-based queries.",
                "According to this finding, the robustness score classifier will attach a NP (CB) label to the query if the robustness score for the query is below (above) a threshold trained from the training data. 0 0.5 1 1.5 2 2.5 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 NP Content-based Figure 2: Distribution of robustness scores for NP and CB queries.",
                "The NP queries are the 252 NP topics from the 2005 Terabyte Track.",
                "The content-based queries are the 150 ad-hoc title from the Terabyte Tracks 2004-2006.",
                "The probability distributions are estimated by the Kernel density estimation method.",
                "Strategies Robust WIG-1 WIG-2 WIG-3 Optimal p=0.6 0.565 0.624 0.665 0.684 0.701 P=0.4 0.567 0.633 0.654 0.673 0.696 Table 8: Comparison of prediction accuracy for five strategies in the mixed-query situation.",
                "Two ways to sample a query from the pool: (1) the sampled query is content-based with the probability p=0.6. (that is, the query is NP with probability 0.4 ) (2) set the probability p=0.4.",
                "We consider five strategies in our experiments.",
                "In the first strategy (denoted by robust), we use the robustness score for query performance prediction with the help of a perfect query classifier that always correctly map a query into one of the two categories (that is, NP or CB).",
                "This strategy represents the level of prediction accuracy that current prediction techniques can achieve in an ideal condition that query types are known.",
                "In the next following three strategies, the WIG method is adopted for performance prediction.",
                "The difference among the three is that three different query classifiers are used for each strategy: (1) the classifier always classifies a query into the NP type. (2) the Robustness Score ProbabilityDensity classifier is the robust score classifier mentioned above. (3) the classifier is a perfect one.",
                "These three strategies are denoted by WIG-1, WIG-2 and WIG-3 respectively.",
                "The reason we are interested in WIG-1 is based on the results from section 4.3.1.",
                "In the last strategy (denoted by Optimal) which serves as an upper bound on how well we can do so far, we fully make use of our prediction techniques for each query type assuming a perfect query classifier is available.",
                "Specifically, we linearly combine WIG and QF for content-based queries and WIG and FRC for NP queries.",
                "The results for the five strategies are shown in Table 8.",
                "For each strategy, we try two ways to sample a query from the pool: (1) the sampled query is CB with probability p=0.6. (the query is NP with probability 0.4) (2) set the probability p=0.4.",
                "From Table 8 We can see that in terms of prediction accuracy WIG-2 (the WIG method with the automatic query classifier) is not only better than the first two cases, but also is close to WIG-3 where a perfect classifier is assumed.",
                "Some further improvements over WIG-3 are observed when combined with other prediction techniques.",
                "The merit of WIG-2 is that it provides a practical solution to automatically identifying poorly performing queries in a Web search environment with mixed query types, which poses considerable obstacles to traditional prediction techniques. 5.",
                "CONCLUSIONS AND FUTURE WORK To our knowledge, our paper is the first to thoroughly explore prediction of query performance in web search environments.",
                "We demonstrated that our models resulted in higher prediction accuracy than previously published techniques not specially devised for web search scenarios.",
                "In this paper, we focus on two types of queries in web search: content-based and Named-Page (NP) finding queries, corresponding to the ad-hoc retrieval task and the Named-Page finding task respectively.",
                "For both types of web queries, our prediction models were shown to be substantially more accurate than the current state-of-the-art techniques.",
                "Furthermore, we considered a more realistic case that no prior information on query types is available.",
                "We demonstrated that the WIG method is particularly suitable for this situation.",
                "Considering the adaptability of WIG to a range of collections and query types, one of our future plans is to apply this method to predict user preference of search results on realistic data collected from a commercial search engine.",
                "Other than accuracy, another major issue that prediction techniques have to deal with in a Web environment is efficiency.",
                "Fortunately, since the WIG score is computed just over the terms and the phrases that appear in the query, this calculation can be made very efficient with the support of index.",
                "On the other hand, the computation of QF and FRC is relatively less efficient since QF needs to retrieve the whole collection twice and FRC needs to repeatedly rank the perturbed documents.",
                "How to improve the efficiency of QF and FRC is our future work.",
                "In addition, the prediction techniques proposed in this paper have the potential of improving retrieval performance by combining with other IR techniques.",
                "For example, our techniques can be incorporated to popular query modification techniques such as query expansion and query relaxation.",
                "Guided by performance prediction, we can make a better decision on when to or how to modify queries to enhance retrieval effectiveness.",
                "We would like to carry out research in this direction in the future. 6.",
                "ACKNOWLEGMENTS This work was supported in part by the Center for Intelligent Information Retrieval, in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C0023, and in part by an award from Google.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor.",
                "In addition, we thank Donald Metzler for his valuable comments on this work. 7.",
                "REFERENCES [1] Y. Zhou ,W. B. Croft ,Ranking Robustness: A Novel Framework to Predict Query Performance, in Proceedings of CIKM 2006. [2] D.Carmel, E.Yom-Tov, A.Darlow,D.Pelleg, What Makes a Query Difficult?, in Proceedings of SIGIR 2006. [3] C.L.A.",
                "Clarke, F. Scholer, I.Soboroff, The TREC 2005 Terabyte Track, In the Online Proceedings of 2005 TREC. [4] B.",
                "He and I.Ounis.",
                "Inferring query performance using preretrieval predictors.",
                "In proceedings of the SPIRE 2004. [5] S. Tomlinson.",
                "Robust, Web and Terabyte Retrieval with Hummingbird SearchServer at TREC 2004.",
                "In the Online Proceedings of 2004 TREC. [6] S. Cronen-Townsend, Y. Zhou, W. B. Croft, Predicting Query Performance, in Proceedings of SIGIR 2002. [7] V.Vinay, I.J.Cox, N.Mill-Frayling,K.Wood, On Ranking the Effectiveness of Searcher, in Proceedings of SIGIR 2006. [8] D.Metzler, W.B.Croft, A Markov Random Filed Model for Term Dependencies, in Proceedings of SIGIR 2005. [9] D.Metzler, T.Strohman,Y.Zhou,W.B.Croft, Indri at TREC 2005: Terabyte Track, In the Online Proceedings of 2004 TREC. [10] P. Ogilvie and J. Callan, Combining document representations for known-item search, in Proceedings of SIGIR 2003. [11] A.Berger, J.Lafferty, Information retrieval as statistical translation, in Proceedings of SIGIR 1999. [12] Indri search engine : http://www.lemurproject.org/indri/ [13] I.J.",
                "Taneja: On Generalized Information Measures and Their Applications, Advances in Electronics and Electron Physics, Academic Press (USA), 76, 1989, 327-413. [14] S.Cronen-Townsend, Y. Zhou and Croft, W. B. , A Framework for Selective Query Expansion, in Proceedings of CIKM 2004. [15] F.Song, W.B.Croft, A general language model for information retrieval, in Proceedings of SIGIR 1999. [16] Personal email contact with Vishwa Vinay and our own experiments [17] E.Yom-Tov, S.Fine, D.Carmel, A.Darlow, Learning to Estimate Query Difficulty Including Applications to Missing Content Detection and Distributed Information retrieval, in Proceedings of SIGIR 2005"
            ],
            "original_annotated_samples": [
                "For example, the reported prediction accuracy of the <br>ranking robustness technique</br> and the clarity technique on the GOV2 collection (a large web collection) is significantly worse compared to the other TREC collections [1].",
                "This method is derived from the <br>ranking robustness technique</br> [1] that is mainly designed for content-based queries."
            ],
            "translated_annotated_samples": [
                "Por ejemplo, la precisión de predicción reportada de la <br>técnica de robustez de clasificación</br> y la técnica de claridad en la colección GOV2 (una colección web grande) es significativamente peor en comparación con las otras colecciones de TREC [1].",
                "Este método se deriva de la <br>técnica de robustez de clasificación</br> [1] que está principalmente diseñada para consultas basadas en contenido."
            ],
            "translated_text": "Predicción del rendimiento de consultas en entornos de búsqueda web Yun Zhou y W. Bruce Croft Departamento de Ciencias de la Computación Universidad de Massachusetts, Amherst {yzhou, croft}@cs.umass.edu RESUMEN Las técnicas actuales de predicción, generalmente diseñadas para consultas basadas en contenido y evaluadas típicamente en colecciones de prueba relativamente homogéneas de tamaños pequeños, enfrentan serios desafíos en entornos de búsqueda web donde las colecciones son significativamente más heterogéneas y existen diferentes tipos de tareas de recuperación. En este artículo, presentamos tres técnicas para abordar estos desafíos. Nos enfocamos en la predicción de rendimiento para dos tipos de consultas en entornos de búsqueda web: basadas en contenido y búsqueda de páginas por nombre. Nuestra evaluación se realiza principalmente en la colección GOV2. Además de evaluar nuestros modelos para los dos tipos de consultas por separado, consideramos una situación más desafiante y realista en la que los dos tipos de consultas se mezclan sin información previa sobre los tipos de consulta. Para ayudar en la predicción bajo la situación de consulta mixta, se adopta un clasificador de consultas novedoso. Los resultados muestran que nuestra predicción del rendimiento de la consulta web es sustancialmente más precisa que las técnicas de predicción actuales más avanzadas. Por consiguiente, nuestro artículo proporciona un enfoque práctico para predecir el rendimiento en entornos web del mundo real. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Formulación de Consultas Términos Generales Algoritmos, Experimentación, Teoría 1. La predicción del rendimiento de la consulta tiene muchas aplicaciones en una variedad de áreas de recuperación de información (IR), como mejorar la consistencia de la recuperación, el refinamiento de la consulta y la IR distribuida. La importancia de este problema ha sido reconocida por los investigadores en Recuperación de Información y recientemente se han propuesto varios métodos nuevos para la predicción [1, 2, 17]. La mayoría de los trabajos sobre predicción se han centrado en la tarea tradicional de recuperación ad-hoc, donde el rendimiento de la consulta se mide según la relevancia temática. Estos modelos de predicción se evalúan en colecciones de documentos de TREC que suelen consistir en no más de un millón de artículos de agencias de noticias relativamente homogéneos. Con la popularidad e influencia de la Web, las técnicas de predicción que funcionen bien para consultas de estilo web son altamente preferibles. Sin embargo, los entornos de búsqueda web plantean desafíos significativos para los modelos de predicción actuales que están principalmente diseñados para entornos tradicionales de TREC. Aquí delineamos algunos de estos desafíos. Primero, las colecciones web, que son mucho más grandes que las colecciones TREC convencionales, incluyen una variedad de documentos que son diferentes en muchos aspectos, como la calidad y el estilo. Las técnicas actuales de predicción pueden ser vulnerables a estas características de las colecciones web. Por ejemplo, la precisión de predicción reportada de la <br>técnica de robustez de clasificación</br> y la técnica de claridad en la colección GOV2 (una colección web grande) es significativamente peor en comparación con las otras colecciones de TREC [1]. En [2] se informa de una precisión de predicción similar en la colección GOV2 utilizando otra técnica, confirmando la dificultad de predecir el rendimiento de consultas en una gran colección web. Además, la búsqueda web va más allá del alcance de la tarea de recuperación ad hoc basada en la relevancia temática. Por ejemplo, la tarea de encontrar páginas nombradas (NP), que es una tarea de navegación, también es popular en la recuperación web. La predicción del rendimiento de la consulta para la tarea de NP sigue siendo necesaria, ya que el rendimiento de recuperación de NP está lejos de ser perfecto. De hecho, según el informe sobre la tarea NP del Terabyte Track de 2005 [3], aproximadamente el 40% de las consultas de prueba tienen un rendimiento deficiente (sin respuesta correcta en los primeros 10 resultados de búsqueda) incluso en la mejor ejecución del grupo líder. Hasta donde sabemos, poca investigación ha abordado explícitamente el problema de la predicción del rendimiento de consultas de NP. Los modelos de predicción actuales diseñados para consultas basadas en contenido serán menos efectivos para consultas de NP considerando las diferencias fundamentales entre ambas. En tercer lugar, en entornos de búsqueda web del mundo real, las consultas de los usuarios suelen ser una mezcla de diferentes tipos y, por lo general, no se dispone de información previa sobre el tipo de cada consulta. La situación de consulta mixta plantea nuevos problemas para la predicción del rendimiento de la consulta. Por ejemplo, es posible que necesitemos incorporar un clasificador de consultas en los modelos de predicción. A pesar de estos problemas, la capacidad para manejar esta situación es un paso crucial hacia convertir la predicción del rendimiento de consultas de un tema de investigación interesante en una herramienta práctica para la recuperación web. En este artículo, presentamos tres técnicas para abordar los desafíos mencionados que enfrentan los modelos de predicción actuales en entornos de búsqueda web. Nuestro trabajo se centra en la predicción del rendimiento de consultas para la tarea de recuperación basada en contenido (ad-hoc) y la tarea de encontrar páginas por nombre en el contexto de la recuperación web. Nuestra primera técnica, llamada ganancia de información ponderada (WIG), utiliza tanto características de términos individuales como de proximidad de términos para estimar la calidad de los documentos recuperados para la predicción. Observamos que WIG ofrece una precisión de predicción consistente en diversas colecciones de pruebas y tipos de consultas. Además, demostramos que se puede lograr una buena precisión de predicción para la situación de consulta mixta utilizando WIG con la ayuda de un clasificador de tipo de consulta. La retroalimentación de consultas y el cambio de rango inicial, que son nuestras técnicas de predicción segunda y tercera, funcionan bien para consultas basadas en contenido y consultas de NP respectivamente. Nuestras principales contribuciones incluyen: (1) una precisión de predicción considerablemente mejorada para consultas basadas en contenido web en comparación con varias técnicas de vanguardia. (2) nuevas técnicas para predecir con éxito el rendimiento de consultas NP. (3) una solución práctica y totalmente automática para predecir el rendimiento de consultas mixtas. Además, una contribución menor es que encontramos que el puntaje de robustez [1], que originalmente se propuso para la predicción de rendimiento, es útil para la clasificación de consultas. 2. TRABAJO RELACIONADO Como mencionamos en la introducción, recientemente se han propuesto varias técnicas de predicción que se centran en consultas basadas en contenido en la tarea de relevancia temática (ad-hoc). No conocemos ningún trabajo publicado que aborde otros tipos de consultas como las consultas NP, y mucho menos una mezcla de tipos de consultas. A continuación revisamos algunos modelos representativos. La principal dificultad de la predicción del rendimiento proviene del hecho de que muchos factores tienen un impacto en el rendimiento de recuperación. Cada factor afecta el rendimiento en diferente medida y el efecto general es difícil de predecir con precisión. Por lo tanto, no es sorprendente notar que características simples, como la frecuencia de los términos de consulta en la colección [4] y la IDF promedio de los términos de consulta [5], no predicen bien. De hecho, la mayoría de las técnicas exitosas se basan en medir algunas características del conjunto de documentos recuperados para estimar la dificultad del tema. Por ejemplo, la puntuación de claridad [6] mide la coherencia de una lista de documentos mediante la divergencia KL entre el modelo de consulta y el modelo de colección. El puntaje de robustez [1] cuantifica otra propiedad de una lista clasificada: la robustez de la clasificación en presencia de incertidumbre. Carmel et al. [2] encontraron que la distancia medida por la divergencia de Jensen-Shannon entre el conjunto de documentos recuperados y la colección está significativamente correlacionada con la precisión promedio. Vinay et al. [7] propusieron cuatro medidas para capturar la geometría de los documentos mejor clasificados para la predicción. La medida más efectiva es la sensibilidad a la perturbación del documento, una idea algo similar a la puntuación de robustez. Desafortunadamente, su forma de medir la sensibilidad no funciona igual de bien para consultas cortas y la precisión de la predicción disminuye considerablemente cuando se adopta una técnica de recuperación de vanguardia (como Okapi o un enfoque de modelado de lenguaje) en lugar del peso tf-idf utilizado en su artículo [16]. Las dificultades de aplicar estos modelos en entornos de búsqueda web ya han sido mencionadas. En este documento, principalmente adoptamos la puntuación de claridad y la puntuación de robustez como nuestras líneas base. Experimentalmente demostramos que las líneas base, incluso después de ser ajustadas cuidadosamente, son inadecuadas para el entorno web. Uno de nuestros modelos de predicción, WIG, está relacionado con el modelo de campo aleatorio de Markov (MRF) para la recuperación de información [8]. El modelo MRF modela directamente la dependencia de términos y se ha demostrado ser altamente efectivo en una variedad de colecciones de pruebas (especialmente colecciones web) y tareas de recuperación. Este modelo se utiliza para estimar la distribución conjunta de probabilidad sobre documentos y consultas, una parte importante de WIG. La superioridad de WIG sobre otras técnicas de predicción basadas en características unigramas, que se demostrará más adelante en nuestro artículo, coincide con la de MRF para recuperación. En otras palabras, es interesante notar que la dependencia de términos, cuando se modela adecuadamente, puede ser útil tanto para mejorar como para predecir el rendimiento de recuperación. 3. Modelos de predicción 3.1 Ganancia de Información Ponderada (WIG) Esta sección introduce un enfoque de ganancia de información ponderada que incorpora tanto características de términos únicos como de proximidad para predecir el rendimiento tanto en consultas basadas en contenido como en consultas de búsqueda de Páginas Nombradas (NP). Dado un conjunto de consultas Q={Qs} (s=1,2,..N) que incluye todas las posibles consultas de usuario y un conjunto de documentos D={Dt} (t=1,2…M), asumimos que cada par consulta-documento (Qs,Dt) es evaluado manualmente y se incluirá en una lista de relevancia si se determina que Qs es relevante para Dt. La probabilidad conjunta P(Qs,Dt) sobre las consultas Q y los documentos D denota la probabilidad de que el par (Qs,Dt) esté en la lista de relevancia. Tales suposiciones son similares a las utilizadas en [8]. Suponiendo que el usuario emite la consulta Qi ∈Q y los resultados de recuperación en respuesta a Qi es una lista clasificada L de documentos, calculamos la cantidad de información contenida en P(Qs,Dt) con respecto a Qi y L mediante la Ec.1, que es una variante de la entropía llamada entropía ponderada[13]. Los pesos en la ecuación 1 están determinados únicamente por Qi y L. En este documento, elegimos los pesos de la siguiente manera: L es el número de documentos que contiene la consulta, K es el límite superior, LT es el límite superior de documentos, y D es el peso si K está en otro caso. La clasificación de corte K es un parámetro en nuestro modelo que se discutirá más adelante. Por lo tanto, la ecuación 1 se puede simplificar de la siguiente manera: )3(),(log 1 ),( )( , ∑∈ −= LTD titsLQ Kt i DQP K DQH Desafortunadamente, la entropía ponderada ),(, tsLQ DQH i calculada por la ecuación 3, que representa la cantidad de información sobre la probabilidad de que los documentos mejor clasificados en L sean relevantes para la consulta Qi en promedio, no se puede comparar entre diferentes consultas, lo que la hace inapropiada para predecir directamente el rendimiento de la consulta. Para mitigar este problema, creamos una distribución de fondo P(Qs,C) sobre Q y D imaginando que cada documento en D es reemplazado por el mismo documento especial C que representa el uso promedio del lenguaje. En este documento, C se crea concatenando cada documento en D. A grosso modo, C es la colección (el conjunto de documentos) {Dt} sin límites de documentos. De manera similar, la entropía ponderada ),(, CQH sLQi calculada por la Ecuación 3 representa la cantidad de información sobre la probabilidad de que un documento promedio (representado por toda la colección) sea relevante para la consulta Qi. Ahora presentamos nuestro predictor de rendimiento WIG que es la ganancia de información ponderada [13] calculada como la diferencia entre ),(, tsLQ DQH i y ),(, CQH sLQi. Específicamente, dado el conjunto de consultas Qi, la colección C y la lista clasificada L de documentos, WIG se calcula de la siguiente manera: )4( ),( ),( log 1 ),( ),( log),( ),(),(),,( )(, ,, ∑∑ ∈ == −= LTD i ti ts s ts ts tsLQsLQi Kt ii CQP DQP KCQP DQP DQweight DQHCQHLCQWIG WIG calculado por la Ecuación 4 mide el cambio en la información sobre la calidad de la recuperación (en respuesta a la consulta Qi) desde un estado imaginario en el que solo se recupera un documento promedio hasta un estado posterior en el que se observan los resultados de búsqueda reales. Hacemos la hipótesis de que WIG está positivamente correlacionado con la efectividad de la recuperación porque una recuperación de alta calidad debería ser mucho más efectiva que simplemente devolver el documento promedio. El corazón de esta técnica es cómo estimar la distribución conjunta P(Qs, Dt). En el enfoque de modelado del lenguaje para la recuperación de información, se pueden aplicar fácilmente una variedad de modelos para estimar esta distribución. Aunque la mayoría de estos modelos se basan en la suposición de la bolsa de palabras, trabajos recientes sobre modelado de dependencia de términos bajo el marco de modelado de lenguaje han demostrado mejoras consistentes y significativas en la efectividad de recuperación sobre los modelos de bolsa de palabras. Inspirados por el éxito de incorporar características de proximidad de términos en modelos de lenguaje, decidimos adoptar un buen modelo de dependencia para estimar la probabilidad P(Qs,Dt). El modelo que elegimos para este artículo es el modelo de Campo Aleatorio de Markov (MRF) de Metzler y Crofts, que ya ha demostrado su superioridad en comparación con varias colecciones y diferentes tareas de recuperación [8,9]. Según el modelo MRF, log P(Qi, Dt) se puede escribir como )5()|(loglog),(log )( 1 ∑∈ +−= iQF tti DPZDQP ξ ξ ξλ donde Z1 es una constante que asegura que P(Qi, Dt) sume 1. F(Qi) consiste en un conjunto de características ampliadas a partir de la consulta original Qi. Por ejemplo, suponiendo que la consulta Qi es el programa de estudiantes talentosos, F(Qi) incluye características como programa y estudiante talentoso. Consideramos dos tipos de características: características de términos individuales T y características de proximidad P. Las características de proximidad incluyen la frase exacta (#1) y las características de ventana desordenada (#uwN) como se describe en [8]. Ten en cuenta que F(Qi) es la unión de T(Qi) y P(Qi). Para obtener más detalles sobre F(Qi), como por ejemplo cómo expandir la consulta original Qi a F(Qi), remitimos al lector a [8] y [9]. P(ξ|Dt) denota la probabilidad de que la característica ξ ocurra en Dt. Más detalles sobre P(ξ|Dt) se proporcionarán más adelante en esta sección. La elección de λξ es algo diferente a la utilizada en [8] ya que λξ desempeña un doble papel en nuestro modelo. El primer rol, que es el mismo que en [8], es ponderar entre las características de términos individuales y de proximidad. El otro rol, específico de nuestra tarea de predicción, es normalizar el tamaño de F(Qi). Encontramos que la siguiente estrategia de peso para λξ satisface los dos roles anteriores y generaliza bien en una variedad de colecciones y tipos de consultas. )6( )(, |)(| 1 )(, |)(| ⎪ ⎪ ⎩ ⎪ ⎪ ⎨ ⎧ ∈ − ∈ = i i T i i T QP QP QT QT ξ λ ξ λ λξ donde |T(Qi)| y |P(Qi)| denotan el número de características de términos únicos y de proximidad en F(Qi) respectivamente. La razón de elegir la función raíz cuadrada en el denominador de λξ es penalizar adecuadamente un conjunto de características de gran tamaño, haciendo que WIG sea más comparable entre consultas de diversas longitudes. λT es un parámetro fijo y se establece en 0.8 según [8] a lo largo de este documento. De manera similar, log P(Qi,C) se puede escribir como: )7()|(loglog),(log )( 2 ∑∈ +−= iQF i CPZCQP ξ ξ ξλ Cuando se eliminan las constantes Z1 y Z2, el WIG calculado en la Ecuación 4 se puede reescribir de la siguiente manera al sustituir la Ecuación 5 y la Ecuación 7: )8( )|( )|( log 1 ),,( )( )( ∑ ∑∈ ∈ = LTD QF t i Kt i CP DP K LCQWIG ξ ξ ξ ξ λ Una de las ventajas del WIG sobre otras técnicas es que puede manejar tanto consultas basadas en contenido como en NP. Basándose en el tipo (o el tipo predicho) de Qi, el cálculo de WIG en la Ecuación 8 difiere en dos aspectos: (1) cómo estimar P(ξ|Dt) y P(ξ|C), y (2) cómo elegir K. Para consultas basadas en contenido, P(ξ|C) se estima por la frecuencia relativa del rasgo ξ en la colección C en su totalidad. La estimación de P(ξ|Dt) es la misma que en [8]. Es decir, estimamos P(ξ|Dt) mediante la frecuencia relativa del rasgo ξ en Dt suavizada linealmente con la frecuencia de colección P(ξ|C). La K en la Ec.8 se trata como un parámetro libre. Ten en cuenta que K es el único parámetro libre en el cálculo de WIG para consultas basadas en contenido, ya que se asume que todos los parámetros involucrados en P(ξ|Dt) están fijos al tomar los valores sugeridos en [8]. En cuanto a las consultas de NP, hacemos uso de la estructura del documento para estimar P(ξ|Dt) y P(ξ|C) mediante el llamado modelo de mezcla de lenguaje propuesto en [10] e incorporado en el modelo MRF para la recuperación de Named-Page en [9]. La idea básica es que un documento (colección) se divide en varios campos como el campo del título, el campo del cuerpo principal y el campo de los encabezados. P(ξ|Dt) y P(ξ|C) se estiman mediante una combinación lineal de los modelos de lenguaje de cada campo. Debido a limitaciones de espacio, remitimos al lector a [9] para más detalles. Adoptamos el mismo conjunto exacto de parámetros utilizados en [9] para la estimación. En cuanto a K en la Ec.8, establecemos K en 1 porque la tarea de encontrar Named-Page se centra fuertemente en el documento clasificado en primer lugar. Por consiguiente, no hay parámetros libres en el cálculo de WIG para consultas de NP. 3.2 Retroalimentación de consultas En esta sección, presentamos otra técnica llamada retroalimentación de consultas (QF) para la predicción. Suponga que un usuario emite la consulta Q a un sistema de recuperación y se devuelve una lista clasificada L de documentos. Vemos el sistema de recuperación como un canal ruidoso. Específicamente, asumimos que la salida del canal es L y la entrada es Q. Después de pasar por el canal, Q se corrompe y se transforma en la lista clasificada L. Al pensar en el proceso de recuperación de esta manera, el problema de predecir la eficacia de la recuperación se convierte en la tarea de evaluar la calidad del canal. En otras palabras, la predicción se convierte en encontrar una forma de medir el grado de corrupción que surge cuando Q se transforma en L. Dado que calcular directamente el grado de corrupción es difícil, abordamos este problema mediante aproximaciones. Nuestra idea principal es que medimos en qué medida la información sobre Q puede ser recuperada de L bajo la suposición de que solo se observa L. Específicamente, diseñamos un decodificador que pueda traducir con precisión L de vuelta a la nueva consulta Q y la similitud S entre la consulta original Q y la nueva consulta Q se adopta como un predictor de rendimiento. Este es un bosquejo de cómo la técnica QF predice el rendimiento de la consulta. Antes de completar más detalles, discutimos brevemente por qué este método funcionaría. Existe una relación entre la similitud S definida anteriormente y el rendimiento de recuperación. Por un lado, si la recuperación se ha desviado del sentido original de la consulta Q, la nueva consulta Q extraída de la lista clasificada L en respuesta a Q sería muy diferente de la consulta original Q. Por otro lado, una consulta destilada de una lista clasificada que contiene muchos documentos relevantes es probable que sea similar a la consulta original. Se proporcionarán más ejemplos en apoyo de la relación más adelante. A continuación detallamos cómo construir el decodificador y cómo medir la similitud S. En esencia, el objetivo del decodificador es comprimir la lista clasificada L en unos pocos términos informativos que deberían representar el contenido de los documentos mejor clasificados en L. Nuestro enfoque para este objetivo es representar la lista clasificada L mediante un modelo de lenguaje (distribución de términos). Luego, los términos se clasifican según su contribución a la divergencia KL (Kullback-Leibler) de los modelos de lenguaje con respecto al modelo de la colección de fondo. Los términos mejor clasificados serán elegidos para formar la nueva consulta Q. Este enfoque es similar al utilizado en la Sección 4.1 de [11]. Específicamente, tomamos tres pasos para comprimir la lista clasificada L en la consulta Q sin hacer referencia a la consulta original. 1. Adoptamos el modelo de lenguaje de lista clasificada [14] para estimar un modelo de lenguaje basado en la lista clasificada L. El modelo puede escribirse como: )9()|()|()|( ∑∈ = LD LDPDwPLwP donde w es cualquier término y D es un documento. La probabilidad de que ocurra el evento D dado el evento L se estima mediante una función linealmente decreciente del rango del documento D. Cada término en P(w|L) está clasificado por la siguiente contribución de divergencia de Kullback-Leibler: )10( )|( )|( log)|( CwP LwP LwP donde P(w|C) es el modelo de colección estimado por la frecuencia relativa del término w en la colección C en su totalidad. 3. Los términos N principales clasificados por Eq.10 forman una consulta ponderada Q={(wi,ti)} i=1,N, donde wi denota el término clasificado en la posición i y el peso ti es la contribución de la divergencia KL de wi en Eq. 10. Barco de crucero daño vida marina. Estos ejemplos indican cómo la similitud entre la consulta original y la nueva se correlaciona con el rendimiento de recuperación. El parámetro N en el paso 3 se establece en 20 de manera empírica y elegir un valor más grande de N es innecesario ya que los pesos después de los primeros 20 suelen ser demasiado pequeños para marcar alguna diferencia. Para medir la similitud entre la consulta original Q y la nueva consulta Q, primero utilizamos Q para recuperar información de la misma colección. Se adopta una variante del modelo de probabilidad de consulta [15] para la recuperación. Es decir, los documentos se clasifican por: )11()|()|( ),( ∑∈ = Qtw t i ii i DwPDQP donde wi es un término en Q y ti es el peso asociado. D es un documento. Que L denote la nueva lista clasificada devuelta desde la recuperación anterior. La similitud se mide por la superposición de documentos en L y L. Específicamente, el porcentaje de documentos en los primeros K documentos de L que también están presentes en los primeros K documentos en L. El valor de corte K se trata como un parámetro libre. Aquí resumimos cómo la técnica QF predice el rendimiento dado una consulta Q y la lista clasificada asociada L. Primero obtenemos una consulta ponderada Q comprimida a partir de L mediante los tres pasos anteriores. Luego usamos Q para realizar la recuperación y la nueva lista clasificada es L. La superposición de documentos en L y L se utiliza para la predicción. 3.3 Primer Cambio de Rango (FRC) En esta sección, proponemos un método llamado primer cambio de rango (FRC) para la predicción de rendimiento para consultas de NP. Este método se deriva de la <br>técnica de robustez de clasificación</br> [1] que está principalmente diseñada para consultas basadas en contenido. Cuando se aplica directamente a consultas de NP, la técnica de robustez será menos efectiva porque tiene en cuenta todos los documentos mejor clasificados en su totalidad, mientras que las consultas de NP suelen tener solo un único documento relevante. En cambio, nuestra técnica se centra en el documento de rango uno mientras se mantiene la idea principal del método de robustez. Específicamente, el seudocódigo para calcular FRC se muestra en la figura 1. Lista clasificada L={Di} donde i=1,100. Di denota el documento clasificado en la posición i. (2) consulta Q 1 inicializar: (1) establecer el número de intentos J=100000 (2) contador c=0; 2 para i=1 a J 3 Perturbar cada documento en L, que el resultado sea un conjunto F={Di} donde Di denota la versión perturbada de Di. 4 Realizar la recuperación con la consulta Q en el conjunto F 5 c=c+1 si y solo si D1 está clasificado primero en el paso 4 6 fin del para 7 devolver la proporción c/J Figura 1: pseudo-código para calcular FRC FRC aproxima la probabilidad de que el documento clasificado en primer lugar en la lista original L permanezca clasificado primero incluso después de que los documentos sean perturbados. Cuanto mayor sea la probabilidad, más confianza tenemos en el documento clasificado en primer lugar. Por otro lado, en el caso extremo de un ranking aleatorio, la probabilidad sería tan baja como 0.5. Esperamos que FRC tenga una asociación positiva con el rendimiento de la consulta de NP. Adoptamos [1] para implementar el paso de perturbación del documento (paso 4 en la Fig. 1) utilizando distribuciones de Poisson. Para más detalles, remitimos al lector a [1]. 4. EVALUACIÓN Ahora presentamos los resultados de predecir el rendimiento de la consulta por nuestros modelos. Tres técnicas de vanguardia son adoptadas como nuestras líneas base. Evaluamos nuestras técnicas en una variedad de configuraciones de recuperación web. Como se mencionó anteriormente, consideramos dos tipos de consultas, es decir, consultas basadas en contenido (CB) y consultas de búsqueda de páginas con nombre (NP). Primero, supongamos que se conocen los tipos de consultas. Investigamos la correlación entre el rendimiento de recuperación predicho y el rendimiento real para ambos tipos de consultas por separado. Los resultados muestran que nuestros métodos producen mejoras considerablemente superiores a los valores de referencia. Luego consideramos un escenario más desafiante donde no hay información previa sobre los tipos de consultas disponibles. Se consideran dos subcasos. En el primero, existe solo un tipo de consulta pero el tipo real es desconocido. Suponemos una mezcla de los dos tipos de consultas en el segundo caso. Demostramos que nuestros modelos logran una buena precisión en este escenario exigente, lo que hace que la predicción sea práctica en un entorno de búsqueda web del mundo real. 4.1 Configuración Experimental Nuestra evaluación se centra en la colección GOV2 que contiene alrededor de 25 millones de documentos obtenidos de sitios web en el dominio .gov durante 2004 [3]. Creamos dos tipos de conjuntos de datos para consultas de CB y consultas de NP respectivamente. Para el tipo CB, utilizamos los temas ad-hoc de las pistas Terabyte de 2004, 2005 y 2006 y los nombramos TB04-adhoc, TB05-adhoc y TB06-adhoc respectivamente. Además, también utilizamos los temas ad-hoc de la pista Robusta de 2004 (RT04) para probar la adaptabilidad de nuestras técnicas a un entorno no web. Para consultas de NP, utilizamos los temas de búsqueda de páginas nombradas de las Terabyte Tracks de 2005 y 2006 y los denominamos TB05-NP y TB06-NP respectivamente. Todas las consultas utilizadas en nuestros experimentos son títulos de temas de TREC, ya que nos centramos en la recuperación web. La Tabla 3 resume los conjuntos de datos anteriores. La recuperación del rendimiento de las consultas individuales basadas en contenido y NP se mide mediante la precisión promedio y la tasa recíproca de la primera respuesta correcta, respectivamente. Hacemos uso del modelo de campo aleatorio de Markov tanto para la recuperación ad-hoc como para la recuperación de páginas nombradas. Adoptamos la misma configuración de parámetros de recuperación utilizada en [8,9]. El motor de búsqueda Indri [12] se utiliza para todos nuestros experimentos. Aunque no se informa aquí, también probamos el modelo de probabilidad de consulta para la recuperación ad-hoc y encontramos que los resultados cambian poco debido a la correlación muy alta entre el rendimiento de las consultas obtenido por los dos modelos de recuperación (0.96 medido por el coeficiente de Pearson). 4.2 Tipos de Consultas Conocidos Supongamos que se conocen los tipos de consultas. Tratamos cada tipo de consulta por separado y medimos la correlación con la precisión promedio (o el rango recíproco en el caso de consultas NP). Adoptamos la prueba de correlación de Pearson que refleja el grado de relación lineal entre el rendimiento de recuperación predicho y real. 4.2.1 Métodos de Consultas Basadas en Contenido Claridad Robusta JSD WIG QF WIG +QF TB04+0 5 adhoc 0.333 0.317 0.362 0.574 0.480 0.637 TB06 adhoc 0.076 0.294 N/A 0.464 0.422 0.511 Tabla 4: Coeficientes de correlación de Pearson para la correlación con la precisión promedio en las Pistas de Terabyte (ad-hoc) para la puntuación de claridad, puntuación de robustez, el método basado en JSD (citamos directamente la puntuación informada en [2]), WIG, retroalimentación de consulta (QF) y una combinación lineal de WIG y QF. Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. La Tabla 4 muestra la correlación con la precisión promedio en dos conjuntos de datos: uno es una combinación de TB04-adhoc y TB05-adhoc (100 temas en total) y el otro es TB06-adhoc (50 temas). La razón por la que juntamos TB04-adhoc y TB05-adhoc es para hacer nuestros resultados comparables a [2]. Nuestros puntos de referencia son la puntuación de claridad (claridad) [6], la puntuación de robustez (robust) [1] y el método basado en JSD (JSD) [2]. Para la puntuación de claridad y robustez, hemos probado diferentes configuraciones de parámetros y reportamos los coeficientes de correlación más altos que hemos encontrado. Citamos directamente el resultado del método basado en JSD reportado en [2]. La tabla también muestra los resultados para el método de Ganancia de Información Ponderada (WIG) y el método de Retroalimentación de Consulta (QF) para predecir consultas basadas en contenido. Como describimos en la sección anterior, tanto WIG como QF tienen un parámetro libre para ajustar, es decir, el rango de corte K. Entrenamos el parámetro en un conjunto de datos y lo probamos en el otro. Al combinar WIG y QF, se utiliza una combinación lineal simple y el peso de la combinación se aprende a partir del conjunto de datos de entrenamiento. A partir de estos resultados, podemos ver que nuestros métodos son considerablemente más precisos en comparación con los baselines. También observamos que se obtienen mejoras adicionales a partir de la combinación de WIG y QF, lo que sugiere que miden diferentes propiedades del proceso de recuperación que se relacionan con el rendimiento. Descubrimos que nuestros métodos se generalizan bien en TB06-adhoc, mientras que la correlación del puntaje de claridad con el rendimiento de recuperación en este conjunto de datos es considerablemente peor. Una investigación adicional muestra que la precisión promedio media de TB06-adhoc es de 0.342 y es aproximadamente un 10% mejor que la del primer conjunto de datos. Mientras que los otros tres métodos suelen considerar los 100 mejores documentos o menos de una lista clasificada, el método de claridad generalmente necesita los 500 mejores documentos o más para medir adecuadamente la coherencia de una lista clasificada. Un promedio de precisión media más alto hace que las listas clasificadas recuperadas por diferentes consultas sean más similares en términos de coherencia en el nivel de los primeros 500 documentos. Creemos que esta es la razón principal de la baja precisión de la puntuación de claridad en el segundo conjunto de datos. Aunque este documento se centra en un entorno de búsqueda web, es deseable que nuestras técnicas funcionen de manera consistente en otras situaciones. Con este fin, examinamos la efectividad de nuestras técnicas en la pista Robust 2004. Para nuestros métodos, dividimos equitativamente todas las consultas de prueba en cinco grupos y realizamos validación cruzada de cinco pliegues. Cada vez que utilizamos un grupo para el entrenamiento y los otros cuatro grupos para las pruebas. Hacemos uso de todas las consultas para nuestros dos puntos de referencia, es decir, la puntuación de claridad y la puntuación de robustez. Los parámetros para nuestras líneas base son los mismos que los utilizados en [1]. Los resultados mostrados en la Tabla 5 demuestran que la precisión de predicción de nuestros métodos está a la par con la de las dos líneas base sólidas. Claridad Robusta WIG QF 0.464 0.539 0.468 0.464 Tabla 5: Comparación de los coeficientes de correlación de Pearson en la pista Robusta de 2004 para la puntuación de claridad, puntuación de robustez, WIG y retroalimentación de consulta (QF). Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. Además, examinamos la sensibilidad de predicción de nuestros métodos al rango de corte K. Con respecto a WIG, es bastante robusto a K en las Pistas de Terabyte (2004-2006) mientras prefiere un valor pequeño de K como 5 en la Pista Robusta de 2004. En otras palabras, un valor pequeño de K es una elección casi óptima para ambos tipos de pistas. Teniendo en cuenta que todos los demás parámetros involucrados en WIG están fijos y, consecuentemente, son los mismos para los dos casos, esto significa que WIG puede lograr una precisión de predicción casi óptima en dos situaciones considerablemente diferentes con exactamente la misma configuración de parámetros. En cuanto a QF, prefiere un valor más grande de K, como 100 en las Pistas de Terabyte y un valor más pequeño de K, como 25 en la Pista Robusta de 2004. 4.2.2 Consultas NP Adoptamos WIG y el primer cambio de rango (FRC) para predecir el rendimiento de las consultas NP. También intentamos una combinación lineal de los dos como en la sección anterior. El peso de la combinación se obtiene del otro conjunto de datos. Utilizamos la correlación con los rangos recíprocos medidos por la prueba de correlación de Pearson para evaluar la calidad de la predicción. Los resultados se presentan en la Tabla 6. Nuevamente, nuestros puntos de referencia son la puntuación de claridad y la puntuación de robustez. Para hacer una comparación justa, ajustamos la puntuación de claridad de diferentes maneras. Descubrimos que utilizar el documento clasificado en primer lugar para construir el modelo de consulta produce la mejor precisión de predicción. También intentamos utilizar la estructura del documento mediante la combinación de modelos de lenguaje mencionados en la sección 3.1. Se obtuvo una pequeña mejora. Los coeficientes de correlación para la puntuación de claridad reportados en la Tabla 6 son los mejores que hemos encontrado. Como podemos ver, nuestros métodos superan considerablemente la técnica de puntuación de claridad en ambas ejecuciones. Esto confirma nuestra intuición de que el uso de una medida basada en la coherencia como el puntaje de claridad es inapropiado para las consultas de NP. Claridad de métodos robusta. Tabla 6: Coeficientes de correlación de Pearson para la correlación con rangos recíprocos en las pistas de Terabyte (NP) para la puntuación de claridad, puntuación de robustez, WIG, el primer cambio de rango (FRC) y una combinación lineal de WIG y FRC. Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. En cuanto a la puntuación de robustez, también ajustamos los parámetros y reportamos el mejor que hemos encontrado. Observamos una correlación negativa interesante y sorprendente con los rangos recíprocos. Explicamos este hallazgo brevemente. Un puntaje de robustez alto significa que varios documentos de alta clasificación en la lista clasificada original siguen estando muy bien clasificados después de perturbar los documentos. La existencia de tales documentos es una buena señal de alto rendimiento para consultas basadas en contenido, ya que estas consultas suelen contener una serie de documentos relevantes [1]. Sin embargo, en lo que respecta a las consultas de NP, una diferencia fundamental es que solo hay un documento relevante para cada consulta. La existencia de tales documentos puede confundir la función de clasificación y llevar a un bajo rendimiento de recuperación. Aunque existe una correlación negativa con el rendimiento de recuperación, la fuerza de la correlación es más débil y menos consistente en comparación con nuestros métodos, como se muestra en la Tabla 6. Basándonos en el análisis anterior, podemos ver que las técnicas de predicción actuales como la puntuación de claridad y la puntuación de robustez, que están principalmente diseñadas para consultas basadas en contenido, enfrentan desafíos significativos y son insuficientes para manejar consultas de NP. Nuestras dos técnicas propuestas para consultas de NP demuestran consistentemente una buena precisión de predicción, mostrando un éxito inicial en la resolución del problema de predecir el rendimiento para consultas de NP. Otro punto que queremos destacar es que el método WIG funciona bien para ambos tipos de consultas, una propiedad deseable que la mayoría de las técnicas de predicción carecen. 4.3 Tipos de Consultas Desconocidos En esta sección, realizamos dos tipos de experimentos sin acceso a etiquetas de tipo de consulta. Primero, asumimos que solo existe un tipo de consulta pero el tipo es desconocido. Segundo, experimentamos con una mezcla de consultas basadas en contenido y NP. Las dos subsecciones siguientes informarán los resultados para las dos condiciones respectivamente. 4.3.1 Solo existe un tipo. Suponemos que todas las consultas son del mismo tipo, es decir, son consultas NP o consultas basadas en contenido. Elegimos WIG para tratar este caso porque muestra una buena precisión de predicción para ambos tipos de consultas en la sección anterior. Consideramos dos casos: (1) CB: todas las 150 consultas de títulos de la tarea ad-hoc de las pistas Terabyte 2004-2006 (2) NP: todas las 433 consultas de NP de la tarea de búsqueda de páginas nombradas de las pistas Terabyte 2005 y 2006. Tomamos una estrategia simple etiquetando todas las consultas en cada caso como el mismo tipo (ya sea NP o CB) independientemente de su tipo real. El cálculo de WIG se basará en el tipo de consulta etiquetado en lugar del tipo real. Hay cuatro posibilidades con respecto a la relación entre el tipo real y el tipo etiquetado. La correlación con el rendimiento de recuperación bajo las cuatro posibilidades se presenta en la Tabla 7. Por ejemplo, el valor 0.445 en la intersección entre la segunda fila y la tercera columna muestra el coeficiente de correlación de Pearson para la correlación con la precisión promedio cuando las consultas basadas en contenido se etiquetan incorrectamente como del tipo NP. Basándonos en estos resultados, recomendamos tratar todas las consultas como del tipo NP cuando solo existe un tipo de consulta y la clasificación precisa de la consulta no es factible, considerando el riesgo de que se produzca una gran pérdida de precisión si las consultas NP se etiquetan incorrectamente como consultas basadas en contenido. Estos resultados también demuestran la fuerte adaptabilidad de WIG a diferentes tipos de consultas. Tabla 7: Comparación de los coeficientes de correlación de Pearson para la correlación con el rendimiento de recuperación en cuatro posibilidades en las pistas de Terabyte (NP). Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. 4.3.2 Una mezcla de consultas basadas en contenido y NP Una mezcla de los dos tipos de consultas es una situación más realista que un motor de búsqueda web encontrará. Evaluamos la precisión de la predicción por la exactitud con la que se pueden identificar las consultas de bajo rendimiento mediante el método de predicción, asumiendo que los tipos de consulta reales son desconocidos (pero podemos predecir los tipos de consulta). Esta es una tarea desafiante porque tanto el rendimiento predicho como el real para un tipo de consulta pueden ser incomparables con el de otro tipo. A continuación discutimos cómo implementar nuestra evaluación. Creamos un conjunto de consultas que consiste en todas las 150 consultas de título ad-hoc de Terabyte Track 2004-2006 y todas las 433 consultas NP de Terabyte Track 2005&2006. Dividimos las consultas en el conjunto en clases: buenas (mejores que el 50% de las consultas del mismo tipo en términos de rendimiento de recuperación) y malas (de lo contrario). Según estos estándares, una consulta de NP con un rango recíproco superior a 0.2 o una consulta basada en contenido con una precisión promedio superior a 0.315 se considerarán como buenas. Entonces, cada vez que seleccionamos aleatoriamente una consulta Q del grupo con una probabilidad p de que Q esté basada en el contenido. Las consultas restantes se utilizan como datos de entrenamiento. Primero decidimos el tipo de consulta Q de acuerdo con un clasificador de consultas. Es decir, el clasificador de consultas nos dice si la consulta Q es de tipo NP o basada en contenido. Basándose en el tipo de consulta predicho y en la puntuación calculada para la consulta Q por una técnica de predicción, se toma una decisión binaria sobre si la consulta Q es buena o mala al compararla con el umbral de puntuación del tipo de consulta predicho obtenido de los datos de entrenamiento. La precisión de la predicción se mide por la exactitud de la decisión binaria. En nuestra implementación, tomamos repetidamente una consulta de prueba del conjunto de consultas y la precisión de la predicción se calcula como el porcentaje de decisiones correctas, es decir, una consulta buena (mala) se predice como buena (mala). Es obvio que adivinar al azar llevará a una precisión del 50%. Tomemos el método WIG como ejemplo para ilustrar el proceso. Dos umbrales de WIG (uno para consultas de NP y otro para consultas basadas en contenido) se entrenan maximizando la precisión de predicción en los datos de entrenamiento. Cuando una consulta de prueba es etiquetada como del tipo NP (CB) por el clasificador de tipo de consulta, se predecirá que es buena solo si la puntuación WIG para esta consulta está por encima del umbral de NP (CB). Se seguirán procedimientos similares para otras técnicas de predicción. Ahora presentamos brevemente el clasificador de tipo de consulta automático utilizado en este artículo. Encontramos que la puntuación de robustez, aunque originalmente propuesta para la predicción de rendimiento, es un buen indicador de tipos de consultas. Observamos que, en promedio, las consultas basadas en contenido tienen una puntuación de robustez mucho más alta que las consultas de NP. Por ejemplo, la Figura 2 muestra las distribuciones de puntajes de robustez para consultas basadas en NP y en contenido. Según este hallazgo, el clasificador de puntuación de robustez adjuntará una etiqueta NP (CB) a la consulta si la puntuación de robustez para la consulta está por debajo (por encima) de un umbral entrenado a partir de los datos de entrenamiento. 0 0.5 1 1.5 2 2.5 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 NP Basado en contenido Figura 2: Distribución de puntuaciones de robustez para consultas NP y CB. Las consultas NP son los 252 temas NP del Terabyte Track de 2005. Las consultas basadas en contenido son los 150 títulos ad-hoc de las pistas Terabyte 2004-2006. Las distribuciones de probabilidad son estimadas por el método de estimación de densidad de Kernel. Estrategias Robustas WIG-1 WIG-2 WIG-3 Óptima p=0.6 0.565 0.624 0.665 0.684 0.701 P=0.4 0.567 0.633 0.654 0.673 0.696 Tabla 8: Comparación de la precisión de predicción para cinco estrategias en la situación de consulta mixta. Dos formas de muestrear una consulta de la piscina: (1) la consulta muestreada es basada en contenido con una probabilidad p=0.6 (es decir, la consulta es NP con una probabilidad de 0.4) (2) establecer la probabilidad p=0.4. Consideramos cinco estrategias en nuestros experimentos. En la primera estrategia (denominada robusta), utilizamos la puntuación de robustez para la predicción del rendimiento de la consulta con la ayuda de un clasificador de consultas perfecto que siempre mapea correctamente una consulta en una de las dos categorías (es decir, NP o CB). Esta estrategia representa el nivel de precisión de predicción que las técnicas actuales de predicción pueden lograr en una condición ideal en la que se conocen los tipos de consulta. En las tres estrategias siguientes, se adopta el método WIG para predecir el rendimiento. La diferencia entre los tres es que se utilizan tres clasificadores de consulta diferentes para cada estrategia: (1) el clasificador siempre clasifica una consulta en el tipo NP. (2) el clasificador de Probabilidad de Densidad de Puntuación Robusta es el clasificador de puntuación robusta mencionado anteriormente. (3) el clasificador es perfecto. Estas tres estrategias están designadas como WIG-1, WIG-2 y WIG-3 respectivamente. La razón por la que estamos interesados en WIG-1 se basa en los resultados de la sección 4.3.1. En la última estrategia (denominada Óptima) que sirve como un límite superior de lo bien que podemos hacer hasta ahora, hacemos pleno uso de nuestras técnicas de predicción para cada tipo de consulta asumiendo que un clasificador de consultas perfecto está disponible. Específicamente, combinamos linealmente WIG y QF para consultas basadas en contenido y WIG y FRC para consultas de NP. Los resultados de las cinco estrategias se muestran en la Tabla 8. Para cada estrategia, intentamos dos formas de muestrear una consulta del conjunto: (1) la consulta muestreada es CB con probabilidad p=0.6 (la consulta es NP con probabilidad 0.4) (2) establecer la probabilidad p=0.4. De la Tabla 8 podemos ver que en términos de precisión de predicción, WIG-2 (el método WIG con el clasificador de consultas automático) no solo es mejor que los dos primeros casos, sino que también está cerca de WIG-3 donde se asume un clasificador perfecto. Se observan algunas mejoras adicionales sobre WIG-3 cuando se combinan con otras técnicas de predicción. El mérito de WIG-2 es que proporciona una solución práctica para identificar automáticamente consultas con bajo rendimiento en un entorno de búsqueda web con tipos de consultas mixtos, lo cual representa considerables obstáculos para las técnicas de predicción tradicionales. CONCLUSIONES Y TRABAJOS FUTUROS Hasta donde sabemos, nuestro artículo es el primero en explorar a fondo la predicción del rendimiento de consultas en entornos de búsqueda web. Demostramos que nuestros modelos resultaron en una mayor precisión de predicción que las técnicas previamente publicadas no especialmente diseñadas para escenarios de búsqueda en la web. En este artículo, nos enfocamos en dos tipos de consultas en la búsqueda web: consultas basadas en contenido y consultas de búsqueda de Páginas-Nombradas (NP), correspondientes a la tarea de recuperación ad-hoc y la tarea de búsqueda de Páginas-Nombradas respectivamente. Para ambos tipos de consultas web, se demostró que nuestros modelos de predicción son sustancialmente más precisos que las técnicas actuales más avanzadas. Además, consideramos un caso más realista en el que no se dispone de información previa sobre los tipos de consultas. Demostramos que el método WIG es particularmente adecuado para esta situación. Considerando la adaptabilidad de WIG a una variedad de colecciones y tipos de consultas, uno de nuestros planes futuros es aplicar este método para predecir la preferencia del usuario por los resultados de búsqueda en datos realistas recopilados de un motor de búsqueda comercial. Además de la precisión, otro problema importante con el que las técnicas de predicción tienen que lidiar en un entorno web es la eficiencia. Afortunadamente, dado que el puntaje WIG se calcula solo sobre los términos y frases que aparecen en la consulta, este cálculo puede realizarse de manera muy eficiente con el soporte de un índice. Por otro lado, el cálculo de QF y FRC es relativamente menos eficiente ya que QF necesita recuperar toda la colección dos veces y FRC necesita clasificar repetidamente los documentos perturbados. Cómo mejorar la eficiencia de QF y FRC es nuestro trabajo futuro. Además, las técnicas de predicción propuestas en este documento tienen el potencial de mejorar el rendimiento de recuperación al combinarse con otras técnicas de RI. Por ejemplo, nuestras técnicas pueden ser incorporadas a técnicas populares de modificación de consultas como la expansión de consultas y la relajación de consultas. Guiados por la predicción del rendimiento, podemos tomar una mejor decisión sobre cuándo o cómo modificar las consultas para mejorar la efectividad de la recuperación. Nos gustaría llevar a cabo investigaciones en esta dirección en el futuro. AGRADECIMIENTOS Este trabajo fue apoyado en parte por el Centro de Recuperación de Información Inteligente, en parte por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el contrato número HR0011-06-C0023, y en parte por un premio de Google. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son del autor y no necesariamente reflejan las del patrocinador. Además, agradecemos a Donald Metzler por sus valiosos comentarios sobre este trabajo. 7. REFERENCIAS [1] Y. Zhou, W. B. Croft, Ranking Robustness: Un nuevo marco para predecir el rendimiento de consultas, en Actas de CIKM 2006. [2] D. Carmel, E. Yom-Tov, A. Darlow, D. Pelleg, ¿Qué hace que una consulta sea difícil?, en Actas de SIGIR 2006. [3] C. L. A. Clarke, F. Scholer, I. Soboroff, La pista Terabyte TREC 2005, En las Actas en Línea de TREC 2005. [4] B. Él y yo. Inferir el rendimiento de la consulta utilizando predictores de preretrieval. En actas de SPIRE 2004. [5] S. Tomlinson. Recuperación robusta de la web y terabytes con Hummingbird SearchServer en TREC 2004. En las Actas en Línea de TREC 2004. [6] S. Cronen-Townsend, Y. Zhou, W. B. Croft, Prediciendo el Rendimiento de Consultas, en las Actas de SIGIR 2002. [7] V. Vinay, I. J. Cox, N. Mill-Frayling, K. Wood, Sobre la Clasificación de la Efectividad del Buscador, en las Actas de SIGIR 2006. [8] D. Metzler, W. B. Croft, Un Modelo de Campo Aleatorio de Markov para Dependencias de Términos, en las Actas de SIGIR 2005. [9] D. Metzler, T. Strohman, Y. Zhou, W. B. Croft, Indri en TREC 2005: Terabyte Track, en las Actas en Línea de TREC 2004. [10] P. Ogilvie y J. Callan, Combinando representaciones de documentos para búsqueda de elementos conocidos, en las Actas de SIGIR 2003. [11] A. Berger, J. Lafferty, Recuperación de información como traducción estadística, en las Actas de SIGIR 1999. [12] Motor de búsqueda Indri: http://www.lemurproject.org/indri/ [13] I. J. Taneja: Sobre medidas de información generalizadas y sus aplicaciones, Avances en Electrónica y Física de Electrones, Academic Press (EE. UU.), 76, 1989, 327-413. [14] S. Cronen-Townsend, Y. Zhou y Croft, W. B., Un marco para la expansión selectiva de consultas, en Actas de CIKM 2004. [15] F. Song, W. B. Croft, Un modelo de lenguaje general para la recuperación de información, en Actas de SIGIR 1999. [16] Contacto por correo electrónico personal con Vishwa Vinay y nuestros propios experimentos [17] E. Yom-Tov, S. Fine, D. Carmel, A. Darlow, Aprendiendo a estimar la dificultad de la consulta incluyendo aplicaciones para la detección de contenido faltante y la recuperación de información distribuida, en Actas de SIGIR 2005. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "named-page finding task": {
            "translated_key": "búsqueda de Páginas-Nombradas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Query Performance Prediction in Web Search Environments Yun Zhou and W. Bruce Croft Department of Computer Science University of Massachusetts, Amherst {yzhou, croft}@cs.umass.edu ABSTRACT Current prediction techniques, which are generally designed for content-based queries and are typically evaluated on relatively homogenous test collections of small sizes, face serious challenges in web search environments where collections are significantly more heterogeneous and different types of retrieval tasks exist.",
                "In this paper, we present three techniques to address these challenges.",
                "We focus on performance prediction for two types of queries in web search environments: content-based and Named-Page finding.",
                "Our evaluation is mainly performed on the GOV2 collection.",
                "In addition to evaluating our models for the two types of queries separately, we consider a more challenging and realistic situation that the two types of queries are mixed together without prior information on query types.",
                "To assist prediction under the mixed-query situation, a novel query classifier is adopted.",
                "Results show that our prediction of web query performance is substantially more accurate than the current stateof-the-art prediction techniques.",
                "Consequently, our paper provides a practical approach to performance prediction in realworld web settings.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval -Query formulation General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Query performance prediction has many applications in a variety of information retrieval (IR) areas such as improving retrieval consistency, query refinement, and distributed IR.",
                "The importance of this problem has been recognized by IR researchers and a number of new methods have been proposed for prediction recently [1, 2, 17].",
                "Most work on prediction has focused on the traditional ad-hoc retrieval task where query performance is measured according to topical relevance.",
                "These prediction models are evaluated on TREC document collections which typically consist of no more than one million relatively homogenous newswire articles.",
                "With the popularity and influence of the Web, prediction techniques that will work well for web-style queries are highly preferable.",
                "However, web search environments pose significant challenges to current prediction models that are mainly designed for traditional TREC settings.",
                "Here we outline some of these challenges.",
                "First, web collections, which are much larger than conventional TREC collections, include a variety of documents that are different in many aspects such as quality and style.",
                "Current prediction techniques can be vulnerable to these characteristics of web collections.",
                "For example, the reported prediction accuracy of the ranking robustness technique and the clarity technique on the GOV2 collection (a large web collection) is significantly worse compared to the other TREC collections [1].",
                "Similar prediction accuracy on the GOV2 collection using another technique is reported in [2], confirming the difficult of predicting query performance on a large web collection.",
                "Furthermore, web search goes beyond the scope of the ad-hoc retrieval task based on topical relevance.",
                "For example, the Named-Page (NP) finding task, which is a navigational task, is also popular in web retrieval.",
                "Query performance prediction for the NP task is still necessary since NP retrieval performance is far from perfect.",
                "In fact, according to the report on the NP task of the 2005 Terabyte Track [3], about 40% of the test queries perform poorly (no correct answer in the first 10 search results) even in the best run from the top group.",
                "To our knowledge, little research has explicitly addressed the problem of NP-query performance prediction.",
                "Current prediction models devised for content-based queries will be less effective for NP queries considering the fundamental differences between the two.",
                "Third, in real-world web search environments, user queries are usually a mixture of different types and prior knowledge about the type of each query is generally unavailable.",
                "The mixed-query situation raises new problems for query performance prediction.",
                "For instance, we may need to incorporate a query classifier into prediction models.",
                "Despite these problems, the ability to handle this situation is a crucial step towards turning query performance prediction from an interesting research topic into a practical tool for web retrieval.",
                "In this paper, we present three techniques to address the above challenges that current prediction models face in Web search environments.",
                "Our work focuses on query performance prediction for the content-based (ad-hoc) retrieval task and the name-page finding task in the context of web retrieval.",
                "Our first technique, called weighted information gain (WIG), makes use of both single term and term proximity features to estimate the quality of top retrieved documents for prediction.",
                "We find that WIG offers consistent prediction accuracy across various test collections and query types.",
                "Moreover, we demonstrate that good prediction accuracy can be achieved for the mixed-query situation by using WIG with the help of a query type classifier.",
                "Query feedback and first rank change, which are our second and third prediction techniques, perform well for content-based queries and NP queries respectively.",
                "Our main contributions include: (1) considerably improved prediction accuracy for web content-based queries over several state-of-the-art techniques. (2) new techniques for successfully predicting NP-query performance. (3) a practical and fully automatic solution to predicting mixed-query performance.",
                "In addition, one minor contribution is that we find that the robustness score [1], which was originally proposed for performance prediction, is helpful for query classification. 2.",
                "RELATED WORK As we mentioned in the introduction, a number of prediction techniques have been proposed recently that focus on contentbased queries in the topical relevance (ad-hoc) task.",
                "We know of no published work that addresses other types of queries such as NP queries, let alone a mixture of query types.",
                "Next we review some representative models.",
                "The major difficulty of performance prediction comes from the fact that many factors have an impact on retrieval performance.",
                "Each factor affects performance to a different degree and the overall effect is hard to predict accurately.",
                "Therefore, it is not surprising to notice that simple features, such as the frequency of query terms in the collection [4] and the average IDF of query terms [5], do not predict well.",
                "In fact, most of the successful techniques are based on measuring some characteristics of the retrieved document set to estimate topic difficulty.",
                "For example, the clarity score [6] measures the coherence of a list of documents by the KL-divergence between the query model and the collection model.",
                "The robustness score [1] quantifies another property of a ranked list: the robustness of the ranking in the presence of uncertainty.",
                "Carmel et al. [2] found that the distance measured by the Jensen-Shannon divergence between the retrieved document set and the collection is significantly correlated to average precision.",
                "Vinay et al. [7] proposed four measures to capture the geometry of the top retrieved documents for prediction.",
                "The most effective measure is the sensitivity to document perturbation, an idea somewhat similar to the robustness score.",
                "Unfortunately, their way of measuring the sensitivity does not perform equally well for short queries and prediction accuracy drops considerably when a state-of-the-art retrieval technique (like Okapi or a language modeling approach) is adopted for retrieval instead of the tf-idf weighting used in their paper [16].",
                "The difficulties of applying these models in web search environments have already been mentioned.",
                "In this paper, we mainly adopt the clarity score and the robustness score as our baselines.",
                "We experimentally show that the baselines, even after being carefully tuned, are inadequate for the web environment.",
                "One of our prediction models, WIG, is related to the Markov random field (MRF) model for information retrieval [8].",
                "The MRF model directly models term dependence and is found be to highly effective across a variety of test collections (particularly web collections) and retrieval tasks.",
                "This model is used to estimate the joint probability distribution over documents and queries, an important part of WIG.",
                "The superiority of WIG over other prediction techniques based on unigram features, which will be demonstrated later in our paper, coincides with that of MRF for retrieval.",
                "In other word, it is interesting to note that term dependence, when being modeled appropriately, can be helpful for both improving and predicting retrieval performance. 3.",
                "PREDICTION MODELS 3.1 Weighted Information Gain (WIG) This section introduces a weighted information gain approach that incorporates both single term and proximity features for predicting performance for both content-based and Named-Page (NP) finding queries.",
                "Given a set of queries Q={Qs} (s=1,2,..N) which includes all possible user queries and a set of documents D={Dt} (t=1,2…M), we assume that each query-document pair (Qs,Dt) is manually judged and will be put in a relevance list if Qs is found to be relevant to Dt.",
                "The joint probability P(Qs,Dt) over queries Q and documents D denotes the probability that pair (Qs,Dt) will be in the relevance list.",
                "Such assumptions are similar to those used in [8].",
                "Assuming that the user issues query Qi ∈Q and the retrieval results in response to Qi is a ranked list L of documents, we calculate the amount of information contained in P(Qs,Dt) with respect to Qi and L by Eq.1 which is a variant of entropy called the weighted entropy[13].",
                "The weights in Eq.1 are solely determined by Qi and L. )1(),(log),(),( , , ∑−= ts tststsLQ DQPDQweightDQH i In this paper, we choose the weights as follows: LindocumentsKtopthecontainsLTwhere otherwise LTDandisifK DQweight K Kt ts )( )2( ,0 )(,/1 ),( ⎩ ⎨ ⎧ ∈= = The cutoff rank K is a parameter in our model that will be discussed later.",
                "Accordingly, Eq.1 can be simplified as follows: )3(),(log 1 ),( )( , ∑∈ −= LTD titsLQ Kt i DQP K DQH Unfortunately, weighted entropy ),(, tsLQ DQH i computed by Eq.3, which represents the amount of information about how likely the top ranked documents in L would be relevant to query Qi on average, cannot be compared across different queries, making it inappropriate for directly predicting query performance.",
                "To mitigate this problem, we come up with a background distribution P(Qs,C) over Q and D by imagining that every document in D is replaced by the same special document C which represents average language usage.",
                "In this paper, C is created by concatenating every document in D. Roughly speaking, C is the collection (the document set) {Dt} without document boundaries.",
                "Similarly, weighted entropy ),(, CQH sLQi calculated by Eq.3 represents the amount of information about how likely an average document (represented by the whole collection) would be relevant to query Qi.",
                "Now we introduce our performance predictor WIG which is the weighted information gain [13] computed as the difference between ),(, tsLQ DQH i and ),(, CQH sLQi .Specifically, given query Qi, collection C and ranked list L of documents, WIG is calculated as follows: )4( ),( ),( log 1 ),( ),( log),( ),(),(),,( )(, ,, ∑∑ ∈ == −= LTD i ti ts s ts ts tsLQsLQi Kt ii CQP DQP KCQP DQP DQweight DQHCQHLCQWIG WIG computed by Eq.4 measures the change in information about the quality of retrieval (in response to query Qi) from an imaginary state that only an average document is retrieved to a posterior state that the actual search results are observed.",
                "We hypothesize that WIG is positively correlated with retrieval effectiveness because high quality retrieval should be much more effective than just returning the average document.",
                "The heart of this technique is how to estimate the joint distribution P(Qs,Dt).",
                "In the language modeling approach to IR, a variety of models can be applied readily to estimate this distribution.",
                "Although most of these models are based on the bagof-words assumption, recent work on modeling term dependence under the language modeling framework have shown consistent and significant improvements in retrieval effectiveness over bagof-words models.",
                "Inspired by the success of incorporating term proximity features into language models, we decide to adopt a good dependence model to estimate the probability P(Qs,Dt).",
                "The model we chose for this paper is Metzler and Crofts Markov Random Field (MRF) model, which has already demonstrated superiority over a number of collections and different retrieval tasks [8,9].",
                "According to the MRF model, log P(Qi, Dt) can be written as )5()|(loglog),(log )( 1 ∑∈ +−= iQF tti DPZDQP ξ ξ ξλ where Z1 is a constant that ensures that P(Qi, Dt) sums up to 1.",
                "F(Qi) consists of a set of features expanded from the original query Qi .",
                "For example, assuming that query Qi is talented student program, F(Qi) includes features like program and talented student.",
                "We consider two kinds of features: single term features T and proximity features P. Proximity features include exact phrase (#1) and unordered window (#uwN) features as described in [8].",
                "Note that F(Qi) is the union of T(Qi) and P(Qi).",
                "For more details on F(Qi) such as how to expand the original query Qi to F(Qi), we refer the reader to [8] and [9].",
                "P(ξ|Dt) denotes the probability that feature ξ will occur in Dt.",
                "More details on P(ξ|Dt) will be provided later in this section.",
                "The choice of λξ is somewhat different from that used in [8] since λξ plays a dual role in our model.",
                "The first role, which is the same as in [8], is to weight between single term and proximity features.",
                "The other role, which is specific to our prediction task, is to normalize the size of F(Qi).We found that the following weight strategy for λξ satisfies the above two roles and generalizes well on a variety of collections and query types. )6( )(, |)(| 1 )(, |)(| ⎪ ⎪ ⎩ ⎪ ⎪ ⎨ ⎧ ∈ − ∈ = i i T i i T QP QP QT QT ξ λ ξ λ λξ where |T(Qi)| and |P(Qi)| denote the number of single term and proximity features in F(Qi) respectively.",
                "The reason for choosing the square root function in the denominator of λξ is to penalize a feature set of large size appropriately, making WIG more comparable across queries of various lengths. λT is a fixed parameter and set to 0.8 according to [8] throughout this paper.",
                "Similarly, log P(Qi,C) can be written as: )7()|(loglog),(log )( 2 ∑∈ +−= iQF i CPZCQP ξ ξ ξλ When constant Z1 and Z2 are dropped, WIG computed in Eq.4 can be rewritten as follows by plugging in Eq.5 and Eq.7 : )8( )|( )|( log 1 ),,( )( )( ∑ ∑∈ ∈ = LTD QF t i Kt i CP DP K LCQWIG ξ ξ ξ ξ λ One of the advantages of WIG over other techniques is that it can handle well both content-based and NP queries.",
                "Based on the type (or the predicted type) of Qi, the calculation of WIG in Eq. 8 differs in two aspects: (1) how to estimate P(ξ|Dt) and P(ξ|C), and (2) how to choose K. For content-based queries, P(ξ|C) is estimated by the relative frequency of feature ξ in collection C as a whole.",
                "The estimation of P(ξ|Dt) is the same as in [8].",
                "Namely, we estimate P(ξ|Dt) by the relative frequency of feature ξ in Dt linearly smoothed with collection frequency P(ξ|C).",
                "K in Eq.8 is treated as a free parameter.",
                "Note that K is the only free parameter in the computation of WIG for content-based queries because all parameters involved in P(ξ|Dt) are assumed to be fixed by taking the suggested values in [8].",
                "Regarding NP queries, we make use of document structure to estimate P(ξ|Dt) and P(ξ|C) by the so-called mixture of language models proposed in [10] and incorporated into the MRF model for Named-Page finding retrieval in [9].",
                "The basic idea is that a document (collection) is divided into several fields such as the title field, the main-body field and the heading field.",
                "P(ξ|Dt) and P(ξ|C) are estimated by a linear combination of the language models from each field.",
                "Due to space constraints, we refer the reader to [9] for details.",
                "We adopt the exact same set of parameters as used in [9] for estimation.",
                "With regard to K in Eq.8, we set K to 1 because the <br>named-page finding task</br> heavily focuses on the first ranked document.",
                "Consequently, there are no free parameters in the computation of WIG for NP queries. 3.2 Query Feedback In this section, we introduce another technique called query feedback (QF) for prediction.",
                "Suppose that a user issues query Q to a retrieval system and a ranked list L of documents is returned.",
                "We view the retrieval system as a noisy channel.",
                "Specifically, we assume that the output of the channel is L and the input is Q.",
                "After going through the channel, Q becomes corrupted and is transformed to ranked list L. By thinking about the retrieval process this way, the problem of predicting retrieval effectiveness turns to the task of evaluating the quality of the channel.",
                "In other words, prediction becomes finding a way to measure the degree of corruption that arises when Q is transformed to L. As directly computing the degree of the corruption is difficult, we tackle this problem by approximation.",
                "Our main idea is that we measure to what extent information on Q can be recovered from L on the assumption that only L is observed.",
                "Specifically, we design a decoder that can accurately translate L back into new query Q and the similarity S between the original query Q and the new query Q is adopted as a performance predictor.",
                "This is a sketch of how the QF technique predicts query performance.",
                "Before filling in more details, we briefly discuss why this method would work.",
                "There is a relation between the similarity S defined above and retrieval performance.",
                "On the one hand, if the retrieval has strayed from the original sense of the query Q, the new query Q extracted from ranked list L in response to Q would be very different from the original query Q.",
                "On the other hand, a query distilled from a ranked list containing many relevant documents is likely to be similar to the original query.",
                "Further examples in support of the relation will be provided later.",
                "Next we detail how to build the decoder and how to measure the similarity S. In essence, the goal of the decoder is to compress ranked list L into a few informative terms that should represent the content of the top ranked documents in L. Our approach to this goal is to represent ranked list L by a language model (distribution over terms).",
                "Then terms are ranked by their contribution to the language models KL (Kullback-Leibler) divergence from the background collection model.",
                "Top ranked terms will be chosen to form the new query Q.",
                "This approach is similar to that used in Section 4.1 of [11].",
                "Specifically, we take three steps to compress ranked list L into query Q without referring to the original query. 1.",
                "We adopt the ranked list language model [14], to estimate a language model based on ranked list L. The model can be written as: )9()|()|()|( ∑∈ = LD LDPDwPLwP where w is any term, D is a document.",
                "P(D|L) is estimated by a linearly decreasing function of the rank of document D. 2.",
                "Each term in P(w|L) is ranked by the following KL-divergence contribution: )10( )|( )|( log)|( CwP LwP LwP where P(w|C) is the collection model estimated by the relative frequency of term w in collection C as a whole. 3.",
                "The top N ranked terms by Eq.10 form a weighted query Q={(wi,ti)} i=1,N. where wi denotes the i-th ranked term and weight ti is the KL-divergence contribution of wi in Eq. 10.",
                "Term cruise ship vessel sea passenger KL contribution 0.050 0.040 0.012 0.010 0.009 Table 1: top 5 terms compressed from the ranked list in response to query Cruise ship damage sea life Two representative examples, one for a poorly performing query Cruise ship damage sea life (TREC topic 719; average precision: 0.08) and the other for a high performing query prostate cancer treatments( TREC topic 710; average precision: 0.49), are shown in Table 1 and 2 respectively.",
                "These examples indicate how the similarity between the original and the new query correlates with retrieval performance.",
                "The parameter N in step 3 is set to 20 empirically and choosing a larger value of N is unnecessary since the weights after the top 20 are usually too small to make any difference.",
                "Term prostate cancer treatment men therapy KL contribution 0.177 0.140 0.028 0.025 0.020 Table 2: top 5 terms compressed from the ranked list in response to query prostate cancer treatments To measure the similarity between original query Q and new query Q, we first use Q to do retrieval on the same collection.",
                "A variant of the query likelihood model [15] is adopted for retrieval.",
                "Namely, documents are ranked by: )11()|()|( ),( ∑∈ = Qtw t i ii i DwPDQP where wi is a term in Q and ti is the associated weight.",
                "D is a document.",
                "Let L denote the new ranked list returned from the above retrieval.",
                "The similarity is measured by the overlap of documents in L and L. Specifically, the percentage of documents in the top K documents of L that are also present in the top K documents in L. the cutoff K is treated as a free parameter.",
                "We summarize here how the QF technique predicts performance given a query Q and the associated ranked list L. We first obtain a weighted query Q compressed from L by the above three steps.",
                "Then we use Q to perform retrieval and the new ranked list is L. The overlap of documents in L and L is used for prediction. 3.3 First Rank Change (FRC) In this section, we propose a method called the first rank change (FRC) for performance prediction for NP queries.",
                "This method is derived from the ranking robustness technique [1] that is mainly designed for content-based queries.",
                "When directly applied to NP queries, the robustness technique will be less effective because it takes the top ranked documents as a whole into account while NP queries usually have only one single relevant document.",
                "Instead, our technique focuses on the first rank document while the main idea of the robustness method remains.",
                "Specifically, the pseudocode for computing FRC is shown in figure 1.",
                "Input: (1) ranked list L={Di} where i=1,100.",
                "Di denotes the i-th ranked document. (2) query Q 1 initialize: (1) set the number of trials J=100000 (2) counter c=0; 2 for i=1 to J 3 Perturb every document in L, let the outcome be a set F={Di} where Di denotes the perturbed version of Di. 4 Do retrieval with query Q on set F 5 c=c+1 if and only if D1 is ranked first in step 4 6 end of for 7 return the ratio c/J Figure 1: pseudo-code for computing FRC FRC approximates the probability that the first ranked document in the original list L will remain ranked first even after the documents are perturbed.",
                "The higher the probability is, the more confidence we have in the first ranked document.",
                "On the other hand, in the extreme case of a random ranking, the probability would be as low as 0.5.",
                "We expect that FRC has a positive association with NP query performance.",
                "We adopt [1] to implement the document perturbation step (step 4 in Fig.1) using Poisson distributions.",
                "For more details, we refer the reader to [1]. 4.",
                "EVALUATION We now present the results of predicting query performance by our models.",
                "Three state-of-the-art techniques are adopted as our baselines.",
                "We evaluate our techniques across a variety of Web retrieval settings.",
                "As mentioned before, we consider two types of queries, that is, content-based (CB) queries and Named-Page(NP) finding queries.",
                "First, suppose that the query types are known.",
                "We investigate the correlation between the predicted retrieval performance and the actual performance for both types of queries separately.",
                "Results show that our methods yield considerable improvements over the baselines.",
                "We then consider a more challenging scenario where no prior information on query types is available.",
                "Two sub-cases are considered.",
                "In the first one, there exists only one type of query but the actual type is unknown.",
                "We assume a mixture of the two query types in the second case.",
                "We demonstrate that our models achieve good accuracy under this demanding scenario, making prediction practical in a real-world Web search environment. 4.1 Experimental Setup Our evaluation focuses on the GOV2 collection which contains about 25 million documents crawled from web sites in the .gov domain during 2004 [3].",
                "We create two kinds of data set for CB queries and NP queries respectively.",
                "For the CB type, we use the ad-hoc topics of the Terabyte Tracks of 2004, 2005 and 2006 and name them TB04-adhoc, TB05-adhoc and TB06-adhoc respectively.",
                "In addition, we also use the ad-hoc topics of the 2004 Robust Track (RT04) to test the adaptability of our techniques to a non-Web environment.",
                "For NP queries, we use the Named-Page finding topics of the Terabyte Tracks of 2005 and 2006 and we name them TB05-NP and TB06-NP respectively.",
                "All queries used in our experiments are titles of TREC topics as we center on web retrieval.",
                "Table 3 summarizes the above data sets.",
                "Name Collection Topic Number Query Type TB04-adhoc GOV2 701-750 CB TB05-adhoc GOV2 751-800 CB TB06-adhoc GOV2 801-850 CB RT04 Disk 4+5 (minus CR) 301-450;601700 CB TB05-NP GOV2 NP601-NP872 NP TB06-NP GOV2 NP901-NP1081 NP Table 3: Summary of test collections and topics Retrieval performance of individual content-based and NP queries is measured by the average precision and reciprocal rank of the first correct answer respectively.",
                "We make use of the Markov Random field model for both ad-hoc and Named-Page finding retrieval.",
                "We adopt the same setting of retrieval parameters used in [8,9].",
                "The Indri search engine [12] is used for all of our experiments.",
                "Though not reported here, we also tried the query likelihood model for ad-hoc retrieval and found that the results change little because of the very high correlation between the query performances obtained by the two retrieval models (0.96 measured by Pearsons coefficient). 4.2 Known Query Types Suppose that query types are known.",
                "We treat each type of query separately and measure the correlation with average precision (or the reciprocal rank in the case of NP queries).",
                "We adopt the Pearsons correlation test which reflects the degree of linear relationship between the predicted and the actual retrieval performance. 4.2.1 Content-based Queries Methods Clarity Robust JSD WIG QF WIG +QF TB04+0 5 adhoc 0.333 0.317 0.362 0.574 0.480 0.637 TB06 adhoc 0.076 0.294 N/A 0.464 0.422 0.511 Table 4: Pearsons correlation coefficients for correlation with average precision on the Terabyte Tracks (ad-hoc) for clarity score, robustness score, the JSD-based method(we directly cites the score reported in [2]), WIG, query feedback(QF) and a linear combination of WIG and QF.",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Table 4 shows the correlation with average precision on two data sets: one is a combination of TB04-adhoc and TB05-adhoc(100 topics in total) and the other is TB06-adhoc (50 topics).",
                "The reason that we put TB04-adhoc and TB05-adhoc together is to make our results comparable to [2].",
                "Our baselines are the clarity score (clarity) [6],the robustness score (robust)[1] and the JSDbased method (JSD) [2].",
                "For the clarity and robustness score, we have tried different parameter settings and report the highest correlation coefficients we have found.",
                "We directly cite the result of the JSD-based method reported in [2].",
                "The table also shows the results for the Weighted Information Gain (WIG) method and the Query Feedback (QF) method for predicting content-based queries.",
                "As we described in the previous section, both WIG and QF have one free parameter to set, that is, the cutoff rank K. We train the parameter on one dataset and test on the other.",
                "When combining WIG and QF, a simple linear combination is used and the combination weight is learned from the training data set.",
                "From these results, we can see that our methods are considerably more accurate compared to the baselines.",
                "We also observe that further improvements are obtained from the combination of WIG and QF, suggesting that they measure different properties of the retrieval process that relate to performance.",
                "We discover that our methods generalize well on TB06-adhoc while the correlation for the clarity score with retrieval performance on this data set is considerably worse.",
                "Further investigation shows that the mean average precision of TB06-adhoc is 0.342 and is about 10% better than that of the first data set.",
                "While the other three methods typically consider the top 100 or less documents given a ranked list, the clarity method usually needs the top 500 or more documents to adequately measure the coherence of a ranked list.",
                "Higher mean average precision makes ranked lists retrieved by different queries more similar in terms of coherence at the level of top 500 documents.",
                "We believe that this is the main reason for the low accuracy of the clarity score on the second data set.",
                "Though this paper focuses on a Web search environment, it is desirable that our techniques will work consistently well in other situations.",
                "To this end, we examine the effectiveness of our techniques on the Robust 2004 Track.",
                "For our methods, we evenly divide all of the test queries into five groups and perform five-fold cross validation.",
                "Each time we use one group for training and the remaining four groups for testing.",
                "We make use of all of the queries for our two baselines, that is, the clarity score and the robustness score.",
                "The parameters for our baselines are the same as those used in [1].The results shown in Table 5 demonstrate that the prediction accuracy of our methods is on a par with that of the two strong baselines.",
                "Clarity Robust WIG QF 0.464 0.539 0.468 0.464 Table 5: Comparison of Pearsons correlation coefficients on the 2004 Robust Track for clarity score, robustness score, WIG and query feedback (QF).",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Furthermore, we examine the prediction sensitivity of our methods to the cutoff rank K. With respect to WIG, it is quite robust to K on the Terabyte Tracks (2004-2006) while it prefers a small value of K like 5 on the 2004 Robust Track.",
                "In other words, a small value of K is a nearly-optimal choice for both kinds of tracks.",
                "Considering the fact that all other parameters involved in WIG are fixed and consequently the same for the two cases, this means WIG can achieve nearly-optimal prediction accuracy in two considerably different situations with exactly the same parameter setting.",
                "Regarding QF, it prefers a larger value of K such as 100 on the Terabyte Tracks and a smaller value of K such as 25 on the 2004 Robust Track. 4.2.2 NP Queries We adopt WIG and first rank change (FRC) for predicting NPquery performance.",
                "We also try a linear combination of the two as in the previous section.",
                "The combination weight is obtained from the other data set.",
                "We use the correlation with the reciprocal ranks measured by the Pearsons correlation test to evaluate prediction quality.",
                "The results are presented in Table 6.",
                "Again, our baselines are the clarity score and the robustness score.",
                "To make a fair comparison, we tune the clarity score in different ways.",
                "We found that using the first ranked document to build the query model yields the best prediction accuracy.",
                "We also attempted to utilize document structure by using the mixture of language models mentioned in section 3.1.",
                "Little improvement was obtained.",
                "The correlation coefficients for the clarity score reported in Table 6 are the best we have found.",
                "As we can see, our methods considerably outperform the clarity score technique on both of the runs.",
                "This confirms our intuition that the use of a coherence-based measure like the clarity score is inappropriate for NP queries.",
                "Methods Clarity Robust.",
                "WIG FRC WIG+FRC TB05-NP 0.150 -0.370 0.458 0.440 0.525 TB06-NP 0.112 -0.160 0.478 0.386 0.515 Table 6: Pearsons correlation coefficients for correlation with reciprocal ranks on the Terabyte Tracks (NP) for clarity score, robustness score, WIG, the first rank change (FRC) and a linear combination of WIG and FRC.",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Regarding the robustness score, we also tune the parameters and report the best we have found.",
                "We observe an interesting and surprising negative correlation with reciprocal ranks.",
                "We explain this finding briefly.",
                "A high robustness score means that a number of top ranked documents in the original ranked list are still highly ranked after perturbing the documents.",
                "The existence of such documents is a good sign of high performance for content-based queries as these queries usually contain a number of relevant documents [1].",
                "However, with regard to NP queries, one fundamental difference is that there is only one relevant document for each query.",
                "The existence of such documents can confuse the ranking function and lead to low retrieval performance.",
                "Although the negative correlation with retrieval performance exists, the strength of the correlation is weaker and less consistent compared to our methods as shown in Table 6.",
                "Based on the above analysis, we can see that current prediction techniques like clarity score and robustness score that are mainly designed for content-based queries face significant challenges and are inadequate to deal with NP queries.",
                "Our two techniques proposed for NP queries consistently demonstrate good prediction accuracy, displaying initial success in solving the problem of predicting performance for NP queries.",
                "Another point we want to stress is that the WIG method works well for both types of queries, a desirable property that most prediction techniques lack. 4.3 Unknown Query Types In this section, we run two kinds of experiments without access to query type labels.",
                "First, we assume that only one type of query exists but the type is unknown.",
                "Second, we experiment on a mixture of content-based and NP queries.",
                "The following two subsections will report results for the two conditions respectively. 4.3.1 Only One Type exists We assume that all queries are of the same type, that is, they are either NP queries or content-based queries.",
                "We choose WIG to deal with this case because it shows good prediction accuracy for both types of queries in the previous section.",
                "We consider two cases: (1) CB: all 150 title queries from the ad-hoc task of the Terabyte Tracks 2004-2006 (2)NP: all 433 NP queries from the named page finding task of the Terabyte Tracks 2005 and 2006.",
                "We take a simple strategy by labeling all of the queries in each case as the same type (either NP or CB) regardless of their actual type.",
                "The computation of WIG will be based on the labeled query type instead of the actual type.",
                "There are four possibilities with respect to the relation between the actual type and the labeled type.",
                "The correlation with retrieval performance under the four possibilities is presented in Table 7.",
                "For example, the value 0.445 at the intersection between the second row and the third column shows the Pearsons correlation coefficient for correlation with average precision when the content-based queries are incorrectly labeled as the NP type.",
                "Based on these results, we recommend treating all queries as the NP type when only one query type exists and accurate query classification is not feasible, considering the risk that a large loss of accuracy will occur if NP queries are incorrectly labeled as content-based queries.",
                "These results also demonstrate the strong adaptability of WIG to different query types.",
                "CB (labeled) NP (labeled) CB (actual) 0.536 0.445 NP (actual) 0.174 0.467 Table 7: Comparison of Pearsons correlation coefficients for correlation with retrieval performance under four possibilities on the Terabyte Tracks (NP).",
                "Bold cases mean the results are statistically significant at the 0.01 level. 4.3.2 A mixture of contented-based and NP queries A mixture of the two types of queries is a more realistic situation that a Web search engine will meet.",
                "We evaluate prediction accuracy by how accurately poorly-performing queries can be identified by the prediction method assuming that actual query types are unknown (but we can predict query types).",
                "This is a challenging task because both the predicted and actual performance for one type of query can be incomparable to that for the other type.",
                "Next we discuss how to implement our evaluation.",
                "We create a query pool which consists of all of the 150 ad-hoc title queries from Terabyte Track 2004-2006 and all of the 433 NP queries from Terabyte Track 2005&2006.",
                "We divide the queries in the pool into classes: good (better than 50% of the queries of the same type in terms of retrieval performance) and bad (otherwise).",
                "According to these standards, a NP query with the reciprocal rank above 0.2 or a content-based query with the average precision above 0.315 will be considered as good.",
                "Then, each time we randomly select one query Q from the pool with probability p that Q is contented-based.",
                "The remaining queries are used as training data.",
                "We first decide the type of query Q according to a query classifier.",
                "Namely, the query classifier tells us whether query Q is NP or content-based.",
                "Based on the predicted query type and the score computed for query Q by a prediction technique, a binary decision is made about whether query Q is good or bad by comparing to the score threshold of the predicted query type obtained from the training data.",
                "Prediction accuracy is measured by the accuracy of the binary decision.",
                "In our implementation, we repeatedly take a test query from the query pool and prediction accuracy is computed as the percentage of correct decisions, that is, a good(bad) query is predicted to be good (bad).",
                "It is obvious that random guessing will lead to 50% accuracy.",
                "Let us take the WIG method for example to illustrate the process.",
                "Two WIG thresholds (one for NP queries and the other for content-based queries) are trained by maximizing the prediction accuracy on the training data.",
                "When a test query is labeled as the NP (CB) type by the query type classifier, it will be predicted to be good if and only if the WIG score for this query is above the NP (CB) threshold.",
                "Similar procedures will be taken for other prediction techniques.",
                "Now we briefly introduce the automatic query type classifier used in this paper.",
                "We find that the robustness score, though originally proposed for performance prediction, is a good indicator of query types.",
                "We find that on average content-based queries have a much higher robustness score than NP queries.",
                "For example, Figure 2 shows the distributions of robustness scores for NP and content-based queries.",
                "According to this finding, the robustness score classifier will attach a NP (CB) label to the query if the robustness score for the query is below (above) a threshold trained from the training data. 0 0.5 1 1.5 2 2.5 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 NP Content-based Figure 2: Distribution of robustness scores for NP and CB queries.",
                "The NP queries are the 252 NP topics from the 2005 Terabyte Track.",
                "The content-based queries are the 150 ad-hoc title from the Terabyte Tracks 2004-2006.",
                "The probability distributions are estimated by the Kernel density estimation method.",
                "Strategies Robust WIG-1 WIG-2 WIG-3 Optimal p=0.6 0.565 0.624 0.665 0.684 0.701 P=0.4 0.567 0.633 0.654 0.673 0.696 Table 8: Comparison of prediction accuracy for five strategies in the mixed-query situation.",
                "Two ways to sample a query from the pool: (1) the sampled query is content-based with the probability p=0.6. (that is, the query is NP with probability 0.4 ) (2) set the probability p=0.4.",
                "We consider five strategies in our experiments.",
                "In the first strategy (denoted by robust), we use the robustness score for query performance prediction with the help of a perfect query classifier that always correctly map a query into one of the two categories (that is, NP or CB).",
                "This strategy represents the level of prediction accuracy that current prediction techniques can achieve in an ideal condition that query types are known.",
                "In the next following three strategies, the WIG method is adopted for performance prediction.",
                "The difference among the three is that three different query classifiers are used for each strategy: (1) the classifier always classifies a query into the NP type. (2) the Robustness Score ProbabilityDensity classifier is the robust score classifier mentioned above. (3) the classifier is a perfect one.",
                "These three strategies are denoted by WIG-1, WIG-2 and WIG-3 respectively.",
                "The reason we are interested in WIG-1 is based on the results from section 4.3.1.",
                "In the last strategy (denoted by Optimal) which serves as an upper bound on how well we can do so far, we fully make use of our prediction techniques for each query type assuming a perfect query classifier is available.",
                "Specifically, we linearly combine WIG and QF for content-based queries and WIG and FRC for NP queries.",
                "The results for the five strategies are shown in Table 8.",
                "For each strategy, we try two ways to sample a query from the pool: (1) the sampled query is CB with probability p=0.6. (the query is NP with probability 0.4) (2) set the probability p=0.4.",
                "From Table 8 We can see that in terms of prediction accuracy WIG-2 (the WIG method with the automatic query classifier) is not only better than the first two cases, but also is close to WIG-3 where a perfect classifier is assumed.",
                "Some further improvements over WIG-3 are observed when combined with other prediction techniques.",
                "The merit of WIG-2 is that it provides a practical solution to automatically identifying poorly performing queries in a Web search environment with mixed query types, which poses considerable obstacles to traditional prediction techniques. 5.",
                "CONCLUSIONS AND FUTURE WORK To our knowledge, our paper is the first to thoroughly explore prediction of query performance in web search environments.",
                "We demonstrated that our models resulted in higher prediction accuracy than previously published techniques not specially devised for web search scenarios.",
                "In this paper, we focus on two types of queries in web search: content-based and Named-Page (NP) finding queries, corresponding to the ad-hoc retrieval task and the <br>named-page finding task</br> respectively.",
                "For both types of web queries, our prediction models were shown to be substantially more accurate than the current state-of-the-art techniques.",
                "Furthermore, we considered a more realistic case that no prior information on query types is available.",
                "We demonstrated that the WIG method is particularly suitable for this situation.",
                "Considering the adaptability of WIG to a range of collections and query types, one of our future plans is to apply this method to predict user preference of search results on realistic data collected from a commercial search engine.",
                "Other than accuracy, another major issue that prediction techniques have to deal with in a Web environment is efficiency.",
                "Fortunately, since the WIG score is computed just over the terms and the phrases that appear in the query, this calculation can be made very efficient with the support of index.",
                "On the other hand, the computation of QF and FRC is relatively less efficient since QF needs to retrieve the whole collection twice and FRC needs to repeatedly rank the perturbed documents.",
                "How to improve the efficiency of QF and FRC is our future work.",
                "In addition, the prediction techniques proposed in this paper have the potential of improving retrieval performance by combining with other IR techniques.",
                "For example, our techniques can be incorporated to popular query modification techniques such as query expansion and query relaxation.",
                "Guided by performance prediction, we can make a better decision on when to or how to modify queries to enhance retrieval effectiveness.",
                "We would like to carry out research in this direction in the future. 6.",
                "ACKNOWLEGMENTS This work was supported in part by the Center for Intelligent Information Retrieval, in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C0023, and in part by an award from Google.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor.",
                "In addition, we thank Donald Metzler for his valuable comments on this work. 7.",
                "REFERENCES [1] Y. Zhou ,W. B. Croft ,Ranking Robustness: A Novel Framework to Predict Query Performance, in Proceedings of CIKM 2006. [2] D.Carmel, E.Yom-Tov, A.Darlow,D.Pelleg, What Makes a Query Difficult?, in Proceedings of SIGIR 2006. [3] C.L.A.",
                "Clarke, F. Scholer, I.Soboroff, The TREC 2005 Terabyte Track, In the Online Proceedings of 2005 TREC. [4] B.",
                "He and I.Ounis.",
                "Inferring query performance using preretrieval predictors.",
                "In proceedings of the SPIRE 2004. [5] S. Tomlinson.",
                "Robust, Web and Terabyte Retrieval with Hummingbird SearchServer at TREC 2004.",
                "In the Online Proceedings of 2004 TREC. [6] S. Cronen-Townsend, Y. Zhou, W. B. Croft, Predicting Query Performance, in Proceedings of SIGIR 2002. [7] V.Vinay, I.J.Cox, N.Mill-Frayling,K.Wood, On Ranking the Effectiveness of Searcher, in Proceedings of SIGIR 2006. [8] D.Metzler, W.B.Croft, A Markov Random Filed Model for Term Dependencies, in Proceedings of SIGIR 2005. [9] D.Metzler, T.Strohman,Y.Zhou,W.B.Croft, Indri at TREC 2005: Terabyte Track, In the Online Proceedings of 2004 TREC. [10] P. Ogilvie and J. Callan, Combining document representations for known-item search, in Proceedings of SIGIR 2003. [11] A.Berger, J.Lafferty, Information retrieval as statistical translation, in Proceedings of SIGIR 1999. [12] Indri search engine : http://www.lemurproject.org/indri/ [13] I.J.",
                "Taneja: On Generalized Information Measures and Their Applications, Advances in Electronics and Electron Physics, Academic Press (USA), 76, 1989, 327-413. [14] S.Cronen-Townsend, Y. Zhou and Croft, W. B. , A Framework for Selective Query Expansion, in Proceedings of CIKM 2004. [15] F.Song, W.B.Croft, A general language model for information retrieval, in Proceedings of SIGIR 1999. [16] Personal email contact with Vishwa Vinay and our own experiments [17] E.Yom-Tov, S.Fine, D.Carmel, A.Darlow, Learning to Estimate Query Difficulty Including Applications to Missing Content Detection and Distributed Information retrieval, in Proceedings of SIGIR 2005"
            ],
            "original_annotated_samples": [
                "With regard to K in Eq.8, we set K to 1 because the <br>named-page finding task</br> heavily focuses on the first ranked document.",
                "In this paper, we focus on two types of queries in web search: content-based and Named-Page (NP) finding queries, corresponding to the ad-hoc retrieval task and the <br>named-page finding task</br> respectively."
            ],
            "translated_annotated_samples": [
                "En cuanto a K en la Ec.8, establecemos K en 1 porque la <br>tarea de encontrar Named-Page</br> se centra fuertemente en el documento clasificado en primer lugar.",
                "En este artículo, nos enfocamos en dos tipos de consultas en la búsqueda web: consultas basadas en contenido y consultas de <br>búsqueda de Páginas-Nombradas</br> (NP), correspondientes a la tarea de recuperación ad-hoc y la tarea de <br>búsqueda de Páginas-Nombradas</br> respectivamente."
            ],
            "translated_text": "Predicción del rendimiento de consultas en entornos de búsqueda web Yun Zhou y W. Bruce Croft Departamento de Ciencias de la Computación Universidad de Massachusetts, Amherst {yzhou, croft}@cs.umass.edu RESUMEN Las técnicas actuales de predicción, generalmente diseñadas para consultas basadas en contenido y evaluadas típicamente en colecciones de prueba relativamente homogéneas de tamaños pequeños, enfrentan serios desafíos en entornos de búsqueda web donde las colecciones son significativamente más heterogéneas y existen diferentes tipos de tareas de recuperación. En este artículo, presentamos tres técnicas para abordar estos desafíos. Nos enfocamos en la predicción de rendimiento para dos tipos de consultas en entornos de búsqueda web: basadas en contenido y búsqueda de páginas por nombre. Nuestra evaluación se realiza principalmente en la colección GOV2. Además de evaluar nuestros modelos para los dos tipos de consultas por separado, consideramos una situación más desafiante y realista en la que los dos tipos de consultas se mezclan sin información previa sobre los tipos de consulta. Para ayudar en la predicción bajo la situación de consulta mixta, se adopta un clasificador de consultas novedoso. Los resultados muestran que nuestra predicción del rendimiento de la consulta web es sustancialmente más precisa que las técnicas de predicción actuales más avanzadas. Por consiguiente, nuestro artículo proporciona un enfoque práctico para predecir el rendimiento en entornos web del mundo real. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Formulación de Consultas Términos Generales Algoritmos, Experimentación, Teoría 1. La predicción del rendimiento de la consulta tiene muchas aplicaciones en una variedad de áreas de recuperación de información (IR), como mejorar la consistencia de la recuperación, el refinamiento de la consulta y la IR distribuida. La importancia de este problema ha sido reconocida por los investigadores en Recuperación de Información y recientemente se han propuesto varios métodos nuevos para la predicción [1, 2, 17]. La mayoría de los trabajos sobre predicción se han centrado en la tarea tradicional de recuperación ad-hoc, donde el rendimiento de la consulta se mide según la relevancia temática. Estos modelos de predicción se evalúan en colecciones de documentos de TREC que suelen consistir en no más de un millón de artículos de agencias de noticias relativamente homogéneos. Con la popularidad e influencia de la Web, las técnicas de predicción que funcionen bien para consultas de estilo web son altamente preferibles. Sin embargo, los entornos de búsqueda web plantean desafíos significativos para los modelos de predicción actuales que están principalmente diseñados para entornos tradicionales de TREC. Aquí delineamos algunos de estos desafíos. Primero, las colecciones web, que son mucho más grandes que las colecciones TREC convencionales, incluyen una variedad de documentos que son diferentes en muchos aspectos, como la calidad y el estilo. Las técnicas actuales de predicción pueden ser vulnerables a estas características de las colecciones web. Por ejemplo, la precisión de predicción reportada de la técnica de robustez de clasificación y la técnica de claridad en la colección GOV2 (una colección web grande) es significativamente peor en comparación con las otras colecciones de TREC [1]. En [2] se informa de una precisión de predicción similar en la colección GOV2 utilizando otra técnica, confirmando la dificultad de predecir el rendimiento de consultas en una gran colección web. Además, la búsqueda web va más allá del alcance de la tarea de recuperación ad hoc basada en la relevancia temática. Por ejemplo, la tarea de encontrar páginas nombradas (NP), que es una tarea de navegación, también es popular en la recuperación web. La predicción del rendimiento de la consulta para la tarea de NP sigue siendo necesaria, ya que el rendimiento de recuperación de NP está lejos de ser perfecto. De hecho, según el informe sobre la tarea NP del Terabyte Track de 2005 [3], aproximadamente el 40% de las consultas de prueba tienen un rendimiento deficiente (sin respuesta correcta en los primeros 10 resultados de búsqueda) incluso en la mejor ejecución del grupo líder. Hasta donde sabemos, poca investigación ha abordado explícitamente el problema de la predicción del rendimiento de consultas de NP. Los modelos de predicción actuales diseñados para consultas basadas en contenido serán menos efectivos para consultas de NP considerando las diferencias fundamentales entre ambas. En tercer lugar, en entornos de búsqueda web del mundo real, las consultas de los usuarios suelen ser una mezcla de diferentes tipos y, por lo general, no se dispone de información previa sobre el tipo de cada consulta. La situación de consulta mixta plantea nuevos problemas para la predicción del rendimiento de la consulta. Por ejemplo, es posible que necesitemos incorporar un clasificador de consultas en los modelos de predicción. A pesar de estos problemas, la capacidad para manejar esta situación es un paso crucial hacia convertir la predicción del rendimiento de consultas de un tema de investigación interesante en una herramienta práctica para la recuperación web. En este artículo, presentamos tres técnicas para abordar los desafíos mencionados que enfrentan los modelos de predicción actuales en entornos de búsqueda web. Nuestro trabajo se centra en la predicción del rendimiento de consultas para la tarea de recuperación basada en contenido (ad-hoc) y la tarea de encontrar páginas por nombre en el contexto de la recuperación web. Nuestra primera técnica, llamada ganancia de información ponderada (WIG), utiliza tanto características de términos individuales como de proximidad de términos para estimar la calidad de los documentos recuperados para la predicción. Observamos que WIG ofrece una precisión de predicción consistente en diversas colecciones de pruebas y tipos de consultas. Además, demostramos que se puede lograr una buena precisión de predicción para la situación de consulta mixta utilizando WIG con la ayuda de un clasificador de tipo de consulta. La retroalimentación de consultas y el cambio de rango inicial, que son nuestras técnicas de predicción segunda y tercera, funcionan bien para consultas basadas en contenido y consultas de NP respectivamente. Nuestras principales contribuciones incluyen: (1) una precisión de predicción considerablemente mejorada para consultas basadas en contenido web en comparación con varias técnicas de vanguardia. (2) nuevas técnicas para predecir con éxito el rendimiento de consultas NP. (3) una solución práctica y totalmente automática para predecir el rendimiento de consultas mixtas. Además, una contribución menor es que encontramos que el puntaje de robustez [1], que originalmente se propuso para la predicción de rendimiento, es útil para la clasificación de consultas. 2. TRABAJO RELACIONADO Como mencionamos en la introducción, recientemente se han propuesto varias técnicas de predicción que se centran en consultas basadas en contenido en la tarea de relevancia temática (ad-hoc). No conocemos ningún trabajo publicado que aborde otros tipos de consultas como las consultas NP, y mucho menos una mezcla de tipos de consultas. A continuación revisamos algunos modelos representativos. La principal dificultad de la predicción del rendimiento proviene del hecho de que muchos factores tienen un impacto en el rendimiento de recuperación. Cada factor afecta el rendimiento en diferente medida y el efecto general es difícil de predecir con precisión. Por lo tanto, no es sorprendente notar que características simples, como la frecuencia de los términos de consulta en la colección [4] y la IDF promedio de los términos de consulta [5], no predicen bien. De hecho, la mayoría de las técnicas exitosas se basan en medir algunas características del conjunto de documentos recuperados para estimar la dificultad del tema. Por ejemplo, la puntuación de claridad [6] mide la coherencia de una lista de documentos mediante la divergencia KL entre el modelo de consulta y el modelo de colección. El puntaje de robustez [1] cuantifica otra propiedad de una lista clasificada: la robustez de la clasificación en presencia de incertidumbre. Carmel et al. [2] encontraron que la distancia medida por la divergencia de Jensen-Shannon entre el conjunto de documentos recuperados y la colección está significativamente correlacionada con la precisión promedio. Vinay et al. [7] propusieron cuatro medidas para capturar la geometría de los documentos mejor clasificados para la predicción. La medida más efectiva es la sensibilidad a la perturbación del documento, una idea algo similar a la puntuación de robustez. Desafortunadamente, su forma de medir la sensibilidad no funciona igual de bien para consultas cortas y la precisión de la predicción disminuye considerablemente cuando se adopta una técnica de recuperación de vanguardia (como Okapi o un enfoque de modelado de lenguaje) en lugar del peso tf-idf utilizado en su artículo [16]. Las dificultades de aplicar estos modelos en entornos de búsqueda web ya han sido mencionadas. En este documento, principalmente adoptamos la puntuación de claridad y la puntuación de robustez como nuestras líneas base. Experimentalmente demostramos que las líneas base, incluso después de ser ajustadas cuidadosamente, son inadecuadas para el entorno web. Uno de nuestros modelos de predicción, WIG, está relacionado con el modelo de campo aleatorio de Markov (MRF) para la recuperación de información [8]. El modelo MRF modela directamente la dependencia de términos y se ha demostrado ser altamente efectivo en una variedad de colecciones de pruebas (especialmente colecciones web) y tareas de recuperación. Este modelo se utiliza para estimar la distribución conjunta de probabilidad sobre documentos y consultas, una parte importante de WIG. La superioridad de WIG sobre otras técnicas de predicción basadas en características unigramas, que se demostrará más adelante en nuestro artículo, coincide con la de MRF para recuperación. En otras palabras, es interesante notar que la dependencia de términos, cuando se modela adecuadamente, puede ser útil tanto para mejorar como para predecir el rendimiento de recuperación. 3. Modelos de predicción 3.1 Ganancia de Información Ponderada (WIG) Esta sección introduce un enfoque de ganancia de información ponderada que incorpora tanto características de términos únicos como de proximidad para predecir el rendimiento tanto en consultas basadas en contenido como en consultas de búsqueda de Páginas Nombradas (NP). Dado un conjunto de consultas Q={Qs} (s=1,2,..N) que incluye todas las posibles consultas de usuario y un conjunto de documentos D={Dt} (t=1,2…M), asumimos que cada par consulta-documento (Qs,Dt) es evaluado manualmente y se incluirá en una lista de relevancia si se determina que Qs es relevante para Dt. La probabilidad conjunta P(Qs,Dt) sobre las consultas Q y los documentos D denota la probabilidad de que el par (Qs,Dt) esté en la lista de relevancia. Tales suposiciones son similares a las utilizadas en [8]. Suponiendo que el usuario emite la consulta Qi ∈Q y los resultados de recuperación en respuesta a Qi es una lista clasificada L de documentos, calculamos la cantidad de información contenida en P(Qs,Dt) con respecto a Qi y L mediante la Ec.1, que es una variante de la entropía llamada entropía ponderada[13]. Los pesos en la ecuación 1 están determinados únicamente por Qi y L. En este documento, elegimos los pesos de la siguiente manera: L es el número de documentos que contiene la consulta, K es el límite superior, LT es el límite superior de documentos, y D es el peso si K está en otro caso. La clasificación de corte K es un parámetro en nuestro modelo que se discutirá más adelante. Por lo tanto, la ecuación 1 se puede simplificar de la siguiente manera: )3(),(log 1 ),( )( , ∑∈ −= LTD titsLQ Kt i DQP K DQH Desafortunadamente, la entropía ponderada ),(, tsLQ DQH i calculada por la ecuación 3, que representa la cantidad de información sobre la probabilidad de que los documentos mejor clasificados en L sean relevantes para la consulta Qi en promedio, no se puede comparar entre diferentes consultas, lo que la hace inapropiada para predecir directamente el rendimiento de la consulta. Para mitigar este problema, creamos una distribución de fondo P(Qs,C) sobre Q y D imaginando que cada documento en D es reemplazado por el mismo documento especial C que representa el uso promedio del lenguaje. En este documento, C se crea concatenando cada documento en D. A grosso modo, C es la colección (el conjunto de documentos) {Dt} sin límites de documentos. De manera similar, la entropía ponderada ),(, CQH sLQi calculada por la Ecuación 3 representa la cantidad de información sobre la probabilidad de que un documento promedio (representado por toda la colección) sea relevante para la consulta Qi. Ahora presentamos nuestro predictor de rendimiento WIG que es la ganancia de información ponderada [13] calculada como la diferencia entre ),(, tsLQ DQH i y ),(, CQH sLQi. Específicamente, dado el conjunto de consultas Qi, la colección C y la lista clasificada L de documentos, WIG se calcula de la siguiente manera: )4( ),( ),( log 1 ),( ),( log),( ),(),(),,( )(, ,, ∑∑ ∈ == −= LTD i ti ts s ts ts tsLQsLQi Kt ii CQP DQP KCQP DQP DQweight DQHCQHLCQWIG WIG calculado por la Ecuación 4 mide el cambio en la información sobre la calidad de la recuperación (en respuesta a la consulta Qi) desde un estado imaginario en el que solo se recupera un documento promedio hasta un estado posterior en el que se observan los resultados de búsqueda reales. Hacemos la hipótesis de que WIG está positivamente correlacionado con la efectividad de la recuperación porque una recuperación de alta calidad debería ser mucho más efectiva que simplemente devolver el documento promedio. El corazón de esta técnica es cómo estimar la distribución conjunta P(Qs, Dt). En el enfoque de modelado del lenguaje para la recuperación de información, se pueden aplicar fácilmente una variedad de modelos para estimar esta distribución. Aunque la mayoría de estos modelos se basan en la suposición de la bolsa de palabras, trabajos recientes sobre modelado de dependencia de términos bajo el marco de modelado de lenguaje han demostrado mejoras consistentes y significativas en la efectividad de recuperación sobre los modelos de bolsa de palabras. Inspirados por el éxito de incorporar características de proximidad de términos en modelos de lenguaje, decidimos adoptar un buen modelo de dependencia para estimar la probabilidad P(Qs,Dt). El modelo que elegimos para este artículo es el modelo de Campo Aleatorio de Markov (MRF) de Metzler y Crofts, que ya ha demostrado su superioridad en comparación con varias colecciones y diferentes tareas de recuperación [8,9]. Según el modelo MRF, log P(Qi, Dt) se puede escribir como )5()|(loglog),(log )( 1 ∑∈ +−= iQF tti DPZDQP ξ ξ ξλ donde Z1 es una constante que asegura que P(Qi, Dt) sume 1. F(Qi) consiste en un conjunto de características ampliadas a partir de la consulta original Qi. Por ejemplo, suponiendo que la consulta Qi es el programa de estudiantes talentosos, F(Qi) incluye características como programa y estudiante talentoso. Consideramos dos tipos de características: características de términos individuales T y características de proximidad P. Las características de proximidad incluyen la frase exacta (#1) y las características de ventana desordenada (#uwN) como se describe en [8]. Ten en cuenta que F(Qi) es la unión de T(Qi) y P(Qi). Para obtener más detalles sobre F(Qi), como por ejemplo cómo expandir la consulta original Qi a F(Qi), remitimos al lector a [8] y [9]. P(ξ|Dt) denota la probabilidad de que la característica ξ ocurra en Dt. Más detalles sobre P(ξ|Dt) se proporcionarán más adelante en esta sección. La elección de λξ es algo diferente a la utilizada en [8] ya que λξ desempeña un doble papel en nuestro modelo. El primer rol, que es el mismo que en [8], es ponderar entre las características de términos individuales y de proximidad. El otro rol, específico de nuestra tarea de predicción, es normalizar el tamaño de F(Qi). Encontramos que la siguiente estrategia de peso para λξ satisface los dos roles anteriores y generaliza bien en una variedad de colecciones y tipos de consultas. )6( )(, |)(| 1 )(, |)(| ⎪ ⎪ ⎩ ⎪ ⎪ ⎨ ⎧ ∈ − ∈ = i i T i i T QP QP QT QT ξ λ ξ λ λξ donde |T(Qi)| y |P(Qi)| denotan el número de características de términos únicos y de proximidad en F(Qi) respectivamente. La razón de elegir la función raíz cuadrada en el denominador de λξ es penalizar adecuadamente un conjunto de características de gran tamaño, haciendo que WIG sea más comparable entre consultas de diversas longitudes. λT es un parámetro fijo y se establece en 0.8 según [8] a lo largo de este documento. De manera similar, log P(Qi,C) se puede escribir como: )7()|(loglog),(log )( 2 ∑∈ +−= iQF i CPZCQP ξ ξ ξλ Cuando se eliminan las constantes Z1 y Z2, el WIG calculado en la Ecuación 4 se puede reescribir de la siguiente manera al sustituir la Ecuación 5 y la Ecuación 7: )8( )|( )|( log 1 ),,( )( )( ∑ ∑∈ ∈ = LTD QF t i Kt i CP DP K LCQWIG ξ ξ ξ ξ λ Una de las ventajas del WIG sobre otras técnicas es que puede manejar tanto consultas basadas en contenido como en NP. Basándose en el tipo (o el tipo predicho) de Qi, el cálculo de WIG en la Ecuación 8 difiere en dos aspectos: (1) cómo estimar P(ξ|Dt) y P(ξ|C), y (2) cómo elegir K. Para consultas basadas en contenido, P(ξ|C) se estima por la frecuencia relativa del rasgo ξ en la colección C en su totalidad. La estimación de P(ξ|Dt) es la misma que en [8]. Es decir, estimamos P(ξ|Dt) mediante la frecuencia relativa del rasgo ξ en Dt suavizada linealmente con la frecuencia de colección P(ξ|C). La K en la Ec.8 se trata como un parámetro libre. Ten en cuenta que K es el único parámetro libre en el cálculo de WIG para consultas basadas en contenido, ya que se asume que todos los parámetros involucrados en P(ξ|Dt) están fijos al tomar los valores sugeridos en [8]. En cuanto a las consultas de NP, hacemos uso de la estructura del documento para estimar P(ξ|Dt) y P(ξ|C) mediante el llamado modelo de mezcla de lenguaje propuesto en [10] e incorporado en el modelo MRF para la recuperación de Named-Page en [9]. La idea básica es que un documento (colección) se divide en varios campos como el campo del título, el campo del cuerpo principal y el campo de los encabezados. P(ξ|Dt) y P(ξ|C) se estiman mediante una combinación lineal de los modelos de lenguaje de cada campo. Debido a limitaciones de espacio, remitimos al lector a [9] para más detalles. Adoptamos el mismo conjunto exacto de parámetros utilizados en [9] para la estimación. En cuanto a K en la Ec.8, establecemos K en 1 porque la <br>tarea de encontrar Named-Page</br> se centra fuertemente en el documento clasificado en primer lugar. Por consiguiente, no hay parámetros libres en el cálculo de WIG para consultas de NP. 3.2 Retroalimentación de consultas En esta sección, presentamos otra técnica llamada retroalimentación de consultas (QF) para la predicción. Suponga que un usuario emite la consulta Q a un sistema de recuperación y se devuelve una lista clasificada L de documentos. Vemos el sistema de recuperación como un canal ruidoso. Específicamente, asumimos que la salida del canal es L y la entrada es Q. Después de pasar por el canal, Q se corrompe y se transforma en la lista clasificada L. Al pensar en el proceso de recuperación de esta manera, el problema de predecir la eficacia de la recuperación se convierte en la tarea de evaluar la calidad del canal. En otras palabras, la predicción se convierte en encontrar una forma de medir el grado de corrupción que surge cuando Q se transforma en L. Dado que calcular directamente el grado de corrupción es difícil, abordamos este problema mediante aproximaciones. Nuestra idea principal es que medimos en qué medida la información sobre Q puede ser recuperada de L bajo la suposición de que solo se observa L. Específicamente, diseñamos un decodificador que pueda traducir con precisión L de vuelta a la nueva consulta Q y la similitud S entre la consulta original Q y la nueva consulta Q se adopta como un predictor de rendimiento. Este es un bosquejo de cómo la técnica QF predice el rendimiento de la consulta. Antes de completar más detalles, discutimos brevemente por qué este método funcionaría. Existe una relación entre la similitud S definida anteriormente y el rendimiento de recuperación. Por un lado, si la recuperación se ha desviado del sentido original de la consulta Q, la nueva consulta Q extraída de la lista clasificada L en respuesta a Q sería muy diferente de la consulta original Q. Por otro lado, una consulta destilada de una lista clasificada que contiene muchos documentos relevantes es probable que sea similar a la consulta original. Se proporcionarán más ejemplos en apoyo de la relación más adelante. A continuación detallamos cómo construir el decodificador y cómo medir la similitud S. En esencia, el objetivo del decodificador es comprimir la lista clasificada L en unos pocos términos informativos que deberían representar el contenido de los documentos mejor clasificados en L. Nuestro enfoque para este objetivo es representar la lista clasificada L mediante un modelo de lenguaje (distribución de términos). Luego, los términos se clasifican según su contribución a la divergencia KL (Kullback-Leibler) de los modelos de lenguaje con respecto al modelo de la colección de fondo. Los términos mejor clasificados serán elegidos para formar la nueva consulta Q. Este enfoque es similar al utilizado en la Sección 4.1 de [11]. Específicamente, tomamos tres pasos para comprimir la lista clasificada L en la consulta Q sin hacer referencia a la consulta original. 1. Adoptamos el modelo de lenguaje de lista clasificada [14] para estimar un modelo de lenguaje basado en la lista clasificada L. El modelo puede escribirse como: )9()|()|()|( ∑∈ = LD LDPDwPLwP donde w es cualquier término y D es un documento. La probabilidad de que ocurra el evento D dado el evento L se estima mediante una función linealmente decreciente del rango del documento D. Cada término en P(w|L) está clasificado por la siguiente contribución de divergencia de Kullback-Leibler: )10( )|( )|( log)|( CwP LwP LwP donde P(w|C) es el modelo de colección estimado por la frecuencia relativa del término w en la colección C en su totalidad. 3. Los términos N principales clasificados por Eq.10 forman una consulta ponderada Q={(wi,ti)} i=1,N, donde wi denota el término clasificado en la posición i y el peso ti es la contribución de la divergencia KL de wi en Eq. 10. Barco de crucero daño vida marina. Estos ejemplos indican cómo la similitud entre la consulta original y la nueva se correlaciona con el rendimiento de recuperación. El parámetro N en el paso 3 se establece en 20 de manera empírica y elegir un valor más grande de N es innecesario ya que los pesos después de los primeros 20 suelen ser demasiado pequeños para marcar alguna diferencia. Para medir la similitud entre la consulta original Q y la nueva consulta Q, primero utilizamos Q para recuperar información de la misma colección. Se adopta una variante del modelo de probabilidad de consulta [15] para la recuperación. Es decir, los documentos se clasifican por: )11()|()|( ),( ∑∈ = Qtw t i ii i DwPDQP donde wi es un término en Q y ti es el peso asociado. D es un documento. Que L denote la nueva lista clasificada devuelta desde la recuperación anterior. La similitud se mide por la superposición de documentos en L y L. Específicamente, el porcentaje de documentos en los primeros K documentos de L que también están presentes en los primeros K documentos en L. El valor de corte K se trata como un parámetro libre. Aquí resumimos cómo la técnica QF predice el rendimiento dado una consulta Q y la lista clasificada asociada L. Primero obtenemos una consulta ponderada Q comprimida a partir de L mediante los tres pasos anteriores. Luego usamos Q para realizar la recuperación y la nueva lista clasificada es L. La superposición de documentos en L y L se utiliza para la predicción. 3.3 Primer Cambio de Rango (FRC) En esta sección, proponemos un método llamado primer cambio de rango (FRC) para la predicción de rendimiento para consultas de NP. Este método se deriva de la técnica de robustez de clasificación [1] que está principalmente diseñada para consultas basadas en contenido. Cuando se aplica directamente a consultas de NP, la técnica de robustez será menos efectiva porque tiene en cuenta todos los documentos mejor clasificados en su totalidad, mientras que las consultas de NP suelen tener solo un único documento relevante. En cambio, nuestra técnica se centra en el documento de rango uno mientras se mantiene la idea principal del método de robustez. Específicamente, el seudocódigo para calcular FRC se muestra en la figura 1. Lista clasificada L={Di} donde i=1,100. Di denota el documento clasificado en la posición i. (2) consulta Q 1 inicializar: (1) establecer el número de intentos J=100000 (2) contador c=0; 2 para i=1 a J 3 Perturbar cada documento en L, que el resultado sea un conjunto F={Di} donde Di denota la versión perturbada de Di. 4 Realizar la recuperación con la consulta Q en el conjunto F 5 c=c+1 si y solo si D1 está clasificado primero en el paso 4 6 fin del para 7 devolver la proporción c/J Figura 1: pseudo-código para calcular FRC FRC aproxima la probabilidad de que el documento clasificado en primer lugar en la lista original L permanezca clasificado primero incluso después de que los documentos sean perturbados. Cuanto mayor sea la probabilidad, más confianza tenemos en el documento clasificado en primer lugar. Por otro lado, en el caso extremo de un ranking aleatorio, la probabilidad sería tan baja como 0.5. Esperamos que FRC tenga una asociación positiva con el rendimiento de la consulta de NP. Adoptamos [1] para implementar el paso de perturbación del documento (paso 4 en la Fig. 1) utilizando distribuciones de Poisson. Para más detalles, remitimos al lector a [1]. 4. EVALUACIÓN Ahora presentamos los resultados de predecir el rendimiento de la consulta por nuestros modelos. Tres técnicas de vanguardia son adoptadas como nuestras líneas base. Evaluamos nuestras técnicas en una variedad de configuraciones de recuperación web. Como se mencionó anteriormente, consideramos dos tipos de consultas, es decir, consultas basadas en contenido (CB) y consultas de búsqueda de páginas con nombre (NP). Primero, supongamos que se conocen los tipos de consultas. Investigamos la correlación entre el rendimiento de recuperación predicho y el rendimiento real para ambos tipos de consultas por separado. Los resultados muestran que nuestros métodos producen mejoras considerablemente superiores a los valores de referencia. Luego consideramos un escenario más desafiante donde no hay información previa sobre los tipos de consultas disponibles. Se consideran dos subcasos. En el primero, existe solo un tipo de consulta pero el tipo real es desconocido. Suponemos una mezcla de los dos tipos de consultas en el segundo caso. Demostramos que nuestros modelos logran una buena precisión en este escenario exigente, lo que hace que la predicción sea práctica en un entorno de búsqueda web del mundo real. 4.1 Configuración Experimental Nuestra evaluación se centra en la colección GOV2 que contiene alrededor de 25 millones de documentos obtenidos de sitios web en el dominio .gov durante 2004 [3]. Creamos dos tipos de conjuntos de datos para consultas de CB y consultas de NP respectivamente. Para el tipo CB, utilizamos los temas ad-hoc de las pistas Terabyte de 2004, 2005 y 2006 y los nombramos TB04-adhoc, TB05-adhoc y TB06-adhoc respectivamente. Además, también utilizamos los temas ad-hoc de la pista Robusta de 2004 (RT04) para probar la adaptabilidad de nuestras técnicas a un entorno no web. Para consultas de NP, utilizamos los temas de búsqueda de páginas nombradas de las Terabyte Tracks de 2005 y 2006 y los denominamos TB05-NP y TB06-NP respectivamente. Todas las consultas utilizadas en nuestros experimentos son títulos de temas de TREC, ya que nos centramos en la recuperación web. La Tabla 3 resume los conjuntos de datos anteriores. La recuperación del rendimiento de las consultas individuales basadas en contenido y NP se mide mediante la precisión promedio y la tasa recíproca de la primera respuesta correcta, respectivamente. Hacemos uso del modelo de campo aleatorio de Markov tanto para la recuperación ad-hoc como para la recuperación de páginas nombradas. Adoptamos la misma configuración de parámetros de recuperación utilizada en [8,9]. El motor de búsqueda Indri [12] se utiliza para todos nuestros experimentos. Aunque no se informa aquí, también probamos el modelo de probabilidad de consulta para la recuperación ad-hoc y encontramos que los resultados cambian poco debido a la correlación muy alta entre el rendimiento de las consultas obtenido por los dos modelos de recuperación (0.96 medido por el coeficiente de Pearson). 4.2 Tipos de Consultas Conocidos Supongamos que se conocen los tipos de consultas. Tratamos cada tipo de consulta por separado y medimos la correlación con la precisión promedio (o el rango recíproco en el caso de consultas NP). Adoptamos la prueba de correlación de Pearson que refleja el grado de relación lineal entre el rendimiento de recuperación predicho y real. 4.2.1 Métodos de Consultas Basadas en Contenido Claridad Robusta JSD WIG QF WIG +QF TB04+0 5 adhoc 0.333 0.317 0.362 0.574 0.480 0.637 TB06 adhoc 0.076 0.294 N/A 0.464 0.422 0.511 Tabla 4: Coeficientes de correlación de Pearson para la correlación con la precisión promedio en las Pistas de Terabyte (ad-hoc) para la puntuación de claridad, puntuación de robustez, el método basado en JSD (citamos directamente la puntuación informada en [2]), WIG, retroalimentación de consulta (QF) y una combinación lineal de WIG y QF. Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. La Tabla 4 muestra la correlación con la precisión promedio en dos conjuntos de datos: uno es una combinación de TB04-adhoc y TB05-adhoc (100 temas en total) y el otro es TB06-adhoc (50 temas). La razón por la que juntamos TB04-adhoc y TB05-adhoc es para hacer nuestros resultados comparables a [2]. Nuestros puntos de referencia son la puntuación de claridad (claridad) [6], la puntuación de robustez (robust) [1] y el método basado en JSD (JSD) [2]. Para la puntuación de claridad y robustez, hemos probado diferentes configuraciones de parámetros y reportamos los coeficientes de correlación más altos que hemos encontrado. Citamos directamente el resultado del método basado en JSD reportado en [2]. La tabla también muestra los resultados para el método de Ganancia de Información Ponderada (WIG) y el método de Retroalimentación de Consulta (QF) para predecir consultas basadas en contenido. Como describimos en la sección anterior, tanto WIG como QF tienen un parámetro libre para ajustar, es decir, el rango de corte K. Entrenamos el parámetro en un conjunto de datos y lo probamos en el otro. Al combinar WIG y QF, se utiliza una combinación lineal simple y el peso de la combinación se aprende a partir del conjunto de datos de entrenamiento. A partir de estos resultados, podemos ver que nuestros métodos son considerablemente más precisos en comparación con los baselines. También observamos que se obtienen mejoras adicionales a partir de la combinación de WIG y QF, lo que sugiere que miden diferentes propiedades del proceso de recuperación que se relacionan con el rendimiento. Descubrimos que nuestros métodos se generalizan bien en TB06-adhoc, mientras que la correlación del puntaje de claridad con el rendimiento de recuperación en este conjunto de datos es considerablemente peor. Una investigación adicional muestra que la precisión promedio media de TB06-adhoc es de 0.342 y es aproximadamente un 10% mejor que la del primer conjunto de datos. Mientras que los otros tres métodos suelen considerar los 100 mejores documentos o menos de una lista clasificada, el método de claridad generalmente necesita los 500 mejores documentos o más para medir adecuadamente la coherencia de una lista clasificada. Un promedio de precisión media más alto hace que las listas clasificadas recuperadas por diferentes consultas sean más similares en términos de coherencia en el nivel de los primeros 500 documentos. Creemos que esta es la razón principal de la baja precisión de la puntuación de claridad en el segundo conjunto de datos. Aunque este documento se centra en un entorno de búsqueda web, es deseable que nuestras técnicas funcionen de manera consistente en otras situaciones. Con este fin, examinamos la efectividad de nuestras técnicas en la pista Robust 2004. Para nuestros métodos, dividimos equitativamente todas las consultas de prueba en cinco grupos y realizamos validación cruzada de cinco pliegues. Cada vez que utilizamos un grupo para el entrenamiento y los otros cuatro grupos para las pruebas. Hacemos uso de todas las consultas para nuestros dos puntos de referencia, es decir, la puntuación de claridad y la puntuación de robustez. Los parámetros para nuestras líneas base son los mismos que los utilizados en [1]. Los resultados mostrados en la Tabla 5 demuestran que la precisión de predicción de nuestros métodos está a la par con la de las dos líneas base sólidas. Claridad Robusta WIG QF 0.464 0.539 0.468 0.464 Tabla 5: Comparación de los coeficientes de correlación de Pearson en la pista Robusta de 2004 para la puntuación de claridad, puntuación de robustez, WIG y retroalimentación de consulta (QF). Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. Además, examinamos la sensibilidad de predicción de nuestros métodos al rango de corte K. Con respecto a WIG, es bastante robusto a K en las Pistas de Terabyte (2004-2006) mientras prefiere un valor pequeño de K como 5 en la Pista Robusta de 2004. En otras palabras, un valor pequeño de K es una elección casi óptima para ambos tipos de pistas. Teniendo en cuenta que todos los demás parámetros involucrados en WIG están fijos y, consecuentemente, son los mismos para los dos casos, esto significa que WIG puede lograr una precisión de predicción casi óptima en dos situaciones considerablemente diferentes con exactamente la misma configuración de parámetros. En cuanto a QF, prefiere un valor más grande de K, como 100 en las Pistas de Terabyte y un valor más pequeño de K, como 25 en la Pista Robusta de 2004. 4.2.2 Consultas NP Adoptamos WIG y el primer cambio de rango (FRC) para predecir el rendimiento de las consultas NP. También intentamos una combinación lineal de los dos como en la sección anterior. El peso de la combinación se obtiene del otro conjunto de datos. Utilizamos la correlación con los rangos recíprocos medidos por la prueba de correlación de Pearson para evaluar la calidad de la predicción. Los resultados se presentan en la Tabla 6. Nuevamente, nuestros puntos de referencia son la puntuación de claridad y la puntuación de robustez. Para hacer una comparación justa, ajustamos la puntuación de claridad de diferentes maneras. Descubrimos que utilizar el documento clasificado en primer lugar para construir el modelo de consulta produce la mejor precisión de predicción. También intentamos utilizar la estructura del documento mediante la combinación de modelos de lenguaje mencionados en la sección 3.1. Se obtuvo una pequeña mejora. Los coeficientes de correlación para la puntuación de claridad reportados en la Tabla 6 son los mejores que hemos encontrado. Como podemos ver, nuestros métodos superan considerablemente la técnica de puntuación de claridad en ambas ejecuciones. Esto confirma nuestra intuición de que el uso de una medida basada en la coherencia como el puntaje de claridad es inapropiado para las consultas de NP. Claridad de métodos robusta. Tabla 6: Coeficientes de correlación de Pearson para la correlación con rangos recíprocos en las pistas de Terabyte (NP) para la puntuación de claridad, puntuación de robustez, WIG, el primer cambio de rango (FRC) y una combinación lineal de WIG y FRC. Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. En cuanto a la puntuación de robustez, también ajustamos los parámetros y reportamos el mejor que hemos encontrado. Observamos una correlación negativa interesante y sorprendente con los rangos recíprocos. Explicamos este hallazgo brevemente. Un puntaje de robustez alto significa que varios documentos de alta clasificación en la lista clasificada original siguen estando muy bien clasificados después de perturbar los documentos. La existencia de tales documentos es una buena señal de alto rendimiento para consultas basadas en contenido, ya que estas consultas suelen contener una serie de documentos relevantes [1]. Sin embargo, en lo que respecta a las consultas de NP, una diferencia fundamental es que solo hay un documento relevante para cada consulta. La existencia de tales documentos puede confundir la función de clasificación y llevar a un bajo rendimiento de recuperación. Aunque existe una correlación negativa con el rendimiento de recuperación, la fuerza de la correlación es más débil y menos consistente en comparación con nuestros métodos, como se muestra en la Tabla 6. Basándonos en el análisis anterior, podemos ver que las técnicas de predicción actuales como la puntuación de claridad y la puntuación de robustez, que están principalmente diseñadas para consultas basadas en contenido, enfrentan desafíos significativos y son insuficientes para manejar consultas de NP. Nuestras dos técnicas propuestas para consultas de NP demuestran consistentemente una buena precisión de predicción, mostrando un éxito inicial en la resolución del problema de predecir el rendimiento para consultas de NP. Otro punto que queremos destacar es que el método WIG funciona bien para ambos tipos de consultas, una propiedad deseable que la mayoría de las técnicas de predicción carecen. 4.3 Tipos de Consultas Desconocidos En esta sección, realizamos dos tipos de experimentos sin acceso a etiquetas de tipo de consulta. Primero, asumimos que solo existe un tipo de consulta pero el tipo es desconocido. Segundo, experimentamos con una mezcla de consultas basadas en contenido y NP. Las dos subsecciones siguientes informarán los resultados para las dos condiciones respectivamente. 4.3.1 Solo existe un tipo. Suponemos que todas las consultas son del mismo tipo, es decir, son consultas NP o consultas basadas en contenido. Elegimos WIG para tratar este caso porque muestra una buena precisión de predicción para ambos tipos de consultas en la sección anterior. Consideramos dos casos: (1) CB: todas las 150 consultas de títulos de la tarea ad-hoc de las pistas Terabyte 2004-2006 (2) NP: todas las 433 consultas de NP de la tarea de búsqueda de páginas nombradas de las pistas Terabyte 2005 y 2006. Tomamos una estrategia simple etiquetando todas las consultas en cada caso como el mismo tipo (ya sea NP o CB) independientemente de su tipo real. El cálculo de WIG se basará en el tipo de consulta etiquetado en lugar del tipo real. Hay cuatro posibilidades con respecto a la relación entre el tipo real y el tipo etiquetado. La correlación con el rendimiento de recuperación bajo las cuatro posibilidades se presenta en la Tabla 7. Por ejemplo, el valor 0.445 en la intersección entre la segunda fila y la tercera columna muestra el coeficiente de correlación de Pearson para la correlación con la precisión promedio cuando las consultas basadas en contenido se etiquetan incorrectamente como del tipo NP. Basándonos en estos resultados, recomendamos tratar todas las consultas como del tipo NP cuando solo existe un tipo de consulta y la clasificación precisa de la consulta no es factible, considerando el riesgo de que se produzca una gran pérdida de precisión si las consultas NP se etiquetan incorrectamente como consultas basadas en contenido. Estos resultados también demuestran la fuerte adaptabilidad de WIG a diferentes tipos de consultas. Tabla 7: Comparación de los coeficientes de correlación de Pearson para la correlación con el rendimiento de recuperación en cuatro posibilidades en las pistas de Terabyte (NP). Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. 4.3.2 Una mezcla de consultas basadas en contenido y NP Una mezcla de los dos tipos de consultas es una situación más realista que un motor de búsqueda web encontrará. Evaluamos la precisión de la predicción por la exactitud con la que se pueden identificar las consultas de bajo rendimiento mediante el método de predicción, asumiendo que los tipos de consulta reales son desconocidos (pero podemos predecir los tipos de consulta). Esta es una tarea desafiante porque tanto el rendimiento predicho como el real para un tipo de consulta pueden ser incomparables con el de otro tipo. A continuación discutimos cómo implementar nuestra evaluación. Creamos un conjunto de consultas que consiste en todas las 150 consultas de título ad-hoc de Terabyte Track 2004-2006 y todas las 433 consultas NP de Terabyte Track 2005&2006. Dividimos las consultas en el conjunto en clases: buenas (mejores que el 50% de las consultas del mismo tipo en términos de rendimiento de recuperación) y malas (de lo contrario). Según estos estándares, una consulta de NP con un rango recíproco superior a 0.2 o una consulta basada en contenido con una precisión promedio superior a 0.315 se considerarán como buenas. Entonces, cada vez que seleccionamos aleatoriamente una consulta Q del grupo con una probabilidad p de que Q esté basada en el contenido. Las consultas restantes se utilizan como datos de entrenamiento. Primero decidimos el tipo de consulta Q de acuerdo con un clasificador de consultas. Es decir, el clasificador de consultas nos dice si la consulta Q es de tipo NP o basada en contenido. Basándose en el tipo de consulta predicho y en la puntuación calculada para la consulta Q por una técnica de predicción, se toma una decisión binaria sobre si la consulta Q es buena o mala al compararla con el umbral de puntuación del tipo de consulta predicho obtenido de los datos de entrenamiento. La precisión de la predicción se mide por la exactitud de la decisión binaria. En nuestra implementación, tomamos repetidamente una consulta de prueba del conjunto de consultas y la precisión de la predicción se calcula como el porcentaje de decisiones correctas, es decir, una consulta buena (mala) se predice como buena (mala). Es obvio que adivinar al azar llevará a una precisión del 50%. Tomemos el método WIG como ejemplo para ilustrar el proceso. Dos umbrales de WIG (uno para consultas de NP y otro para consultas basadas en contenido) se entrenan maximizando la precisión de predicción en los datos de entrenamiento. Cuando una consulta de prueba es etiquetada como del tipo NP (CB) por el clasificador de tipo de consulta, se predecirá que es buena solo si la puntuación WIG para esta consulta está por encima del umbral de NP (CB). Se seguirán procedimientos similares para otras técnicas de predicción. Ahora presentamos brevemente el clasificador de tipo de consulta automático utilizado en este artículo. Encontramos que la puntuación de robustez, aunque originalmente propuesta para la predicción de rendimiento, es un buen indicador de tipos de consultas. Observamos que, en promedio, las consultas basadas en contenido tienen una puntuación de robustez mucho más alta que las consultas de NP. Por ejemplo, la Figura 2 muestra las distribuciones de puntajes de robustez para consultas basadas en NP y en contenido. Según este hallazgo, el clasificador de puntuación de robustez adjuntará una etiqueta NP (CB) a la consulta si la puntuación de robustez para la consulta está por debajo (por encima) de un umbral entrenado a partir de los datos de entrenamiento. 0 0.5 1 1.5 2 2.5 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 NP Basado en contenido Figura 2: Distribución de puntuaciones de robustez para consultas NP y CB. Las consultas NP son los 252 temas NP del Terabyte Track de 2005. Las consultas basadas en contenido son los 150 títulos ad-hoc de las pistas Terabyte 2004-2006. Las distribuciones de probabilidad son estimadas por el método de estimación de densidad de Kernel. Estrategias Robustas WIG-1 WIG-2 WIG-3 Óptima p=0.6 0.565 0.624 0.665 0.684 0.701 P=0.4 0.567 0.633 0.654 0.673 0.696 Tabla 8: Comparación de la precisión de predicción para cinco estrategias en la situación de consulta mixta. Dos formas de muestrear una consulta de la piscina: (1) la consulta muestreada es basada en contenido con una probabilidad p=0.6 (es decir, la consulta es NP con una probabilidad de 0.4) (2) establecer la probabilidad p=0.4. Consideramos cinco estrategias en nuestros experimentos. En la primera estrategia (denominada robusta), utilizamos la puntuación de robustez para la predicción del rendimiento de la consulta con la ayuda de un clasificador de consultas perfecto que siempre mapea correctamente una consulta en una de las dos categorías (es decir, NP o CB). Esta estrategia representa el nivel de precisión de predicción que las técnicas actuales de predicción pueden lograr en una condición ideal en la que se conocen los tipos de consulta. En las tres estrategias siguientes, se adopta el método WIG para predecir el rendimiento. La diferencia entre los tres es que se utilizan tres clasificadores de consulta diferentes para cada estrategia: (1) el clasificador siempre clasifica una consulta en el tipo NP. (2) el clasificador de Probabilidad de Densidad de Puntuación Robusta es el clasificador de puntuación robusta mencionado anteriormente. (3) el clasificador es perfecto. Estas tres estrategias están designadas como WIG-1, WIG-2 y WIG-3 respectivamente. La razón por la que estamos interesados en WIG-1 se basa en los resultados de la sección 4.3.1. En la última estrategia (denominada Óptima) que sirve como un límite superior de lo bien que podemos hacer hasta ahora, hacemos pleno uso de nuestras técnicas de predicción para cada tipo de consulta asumiendo que un clasificador de consultas perfecto está disponible. Específicamente, combinamos linealmente WIG y QF para consultas basadas en contenido y WIG y FRC para consultas de NP. Los resultados de las cinco estrategias se muestran en la Tabla 8. Para cada estrategia, intentamos dos formas de muestrear una consulta del conjunto: (1) la consulta muestreada es CB con probabilidad p=0.6 (la consulta es NP con probabilidad 0.4) (2) establecer la probabilidad p=0.4. De la Tabla 8 podemos ver que en términos de precisión de predicción, WIG-2 (el método WIG con el clasificador de consultas automático) no solo es mejor que los dos primeros casos, sino que también está cerca de WIG-3 donde se asume un clasificador perfecto. Se observan algunas mejoras adicionales sobre WIG-3 cuando se combinan con otras técnicas de predicción. El mérito de WIG-2 es que proporciona una solución práctica para identificar automáticamente consultas con bajo rendimiento en un entorno de búsqueda web con tipos de consultas mixtos, lo cual representa considerables obstáculos para las técnicas de predicción tradicionales. CONCLUSIONES Y TRABAJOS FUTUROS Hasta donde sabemos, nuestro artículo es el primero en explorar a fondo la predicción del rendimiento de consultas en entornos de búsqueda web. Demostramos que nuestros modelos resultaron en una mayor precisión de predicción que las técnicas previamente publicadas no especialmente diseñadas para escenarios de búsqueda en la web. En este artículo, nos enfocamos en dos tipos de consultas en la búsqueda web: consultas basadas en contenido y consultas de <br>búsqueda de Páginas-Nombradas</br> (NP), correspondientes a la tarea de recuperación ad-hoc y la tarea de <br>búsqueda de Páginas-Nombradas</br> respectivamente. Para ambos tipos de consultas web, se demostró que nuestros modelos de predicción son sustancialmente más precisos que las técnicas actuales más avanzadas. Además, consideramos un caso más realista en el que no se dispone de información previa sobre los tipos de consultas. Demostramos que el método WIG es particularmente adecuado para esta situación. Considerando la adaptabilidad de WIG a una variedad de colecciones y tipos de consultas, uno de nuestros planes futuros es aplicar este método para predecir la preferencia del usuario por los resultados de búsqueda en datos realistas recopilados de un motor de búsqueda comercial. Además de la precisión, otro problema importante con el que las técnicas de predicción tienen que lidiar en un entorno web es la eficiencia. Afortunadamente, dado que el puntaje WIG se calcula solo sobre los términos y frases que aparecen en la consulta, este cálculo puede realizarse de manera muy eficiente con el soporte de un índice. Por otro lado, el cálculo de QF y FRC es relativamente menos eficiente ya que QF necesita recuperar toda la colección dos veces y FRC necesita clasificar repetidamente los documentos perturbados. Cómo mejorar la eficiencia de QF y FRC es nuestro trabajo futuro. Además, las técnicas de predicción propuestas en este documento tienen el potencial de mejorar el rendimiento de recuperación al combinarse con otras técnicas de RI. Por ejemplo, nuestras técnicas pueden ser incorporadas a técnicas populares de modificación de consultas como la expansión de consultas y la relajación de consultas. Guiados por la predicción del rendimiento, podemos tomar una mejor decisión sobre cuándo o cómo modificar las consultas para mejorar la efectividad de la recuperación. Nos gustaría llevar a cabo investigaciones en esta dirección en el futuro. AGRADECIMIENTOS Este trabajo fue apoyado en parte por el Centro de Recuperación de Información Inteligente, en parte por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el contrato número HR0011-06-C0023, y en parte por un premio de Google. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son del autor y no necesariamente reflejan las del patrocinador. Además, agradecemos a Donald Metzler por sus valiosos comentarios sobre este trabajo. 7. REFERENCIAS [1] Y. Zhou, W. B. Croft, Ranking Robustness: Un nuevo marco para predecir el rendimiento de consultas, en Actas de CIKM 2006. [2] D. Carmel, E. Yom-Tov, A. Darlow, D. Pelleg, ¿Qué hace que una consulta sea difícil?, en Actas de SIGIR 2006. [3] C. L. A. Clarke, F. Scholer, I. Soboroff, La pista Terabyte TREC 2005, En las Actas en Línea de TREC 2005. [4] B. Él y yo. Inferir el rendimiento de la consulta utilizando predictores de preretrieval. En actas de SPIRE 2004. [5] S. Tomlinson. Recuperación robusta de la web y terabytes con Hummingbird SearchServer en TREC 2004. En las Actas en Línea de TREC 2004. [6] S. Cronen-Townsend, Y. Zhou, W. B. Croft, Prediciendo el Rendimiento de Consultas, en las Actas de SIGIR 2002. [7] V. Vinay, I. J. Cox, N. Mill-Frayling, K. Wood, Sobre la Clasificación de la Efectividad del Buscador, en las Actas de SIGIR 2006. [8] D. Metzler, W. B. Croft, Un Modelo de Campo Aleatorio de Markov para Dependencias de Términos, en las Actas de SIGIR 2005. [9] D. Metzler, T. Strohman, Y. Zhou, W. B. Croft, Indri en TREC 2005: Terabyte Track, en las Actas en Línea de TREC 2004. [10] P. Ogilvie y J. Callan, Combinando representaciones de documentos para búsqueda de elementos conocidos, en las Actas de SIGIR 2003. [11] A. Berger, J. Lafferty, Recuperación de información como traducción estadística, en las Actas de SIGIR 1999. [12] Motor de búsqueda Indri: http://www.lemurproject.org/indri/ [13] I. J. Taneja: Sobre medidas de información generalizadas y sus aplicaciones, Avances en Electrónica y Física de Electrones, Academic Press (EE. UU.), 76, 1989, 327-413. [14] S. Cronen-Townsend, Y. Zhou y Croft, W. B., Un marco para la expansión selectiva de consultas, en Actas de CIKM 2004. [15] F. Song, W. B. Croft, Un modelo de lenguaje general para la recuperación de información, en Actas de SIGIR 1999. [16] Contacto por correo electrónico personal con Vishwa Vinay y nuestros propios experimentos [17] E. Yom-Tov, S. Fine, D. Carmel, A. Darlow, Aprendiendo a estimar la dificultad de la consulta incluyendo aplicaciones para la detección de contenido faltante y la recuperación de información distribuida, en Actas de SIGIR 2005. ",
            "candidates": [],
            "error": [
                [
                    "tarea de encontrar Named-Page",
                    "búsqueda de Páginas-Nombradas",
                    "búsqueda de Páginas-Nombradas"
                ]
            ]
        },
        "weighted information gain": {
            "translated_key": "ganancia de información ponderada",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Query Performance Prediction in Web Search Environments Yun Zhou and W. Bruce Croft Department of Computer Science University of Massachusetts, Amherst {yzhou, croft}@cs.umass.edu ABSTRACT Current prediction techniques, which are generally designed for content-based queries and are typically evaluated on relatively homogenous test collections of small sizes, face serious challenges in web search environments where collections are significantly more heterogeneous and different types of retrieval tasks exist.",
                "In this paper, we present three techniques to address these challenges.",
                "We focus on performance prediction for two types of queries in web search environments: content-based and Named-Page finding.",
                "Our evaluation is mainly performed on the GOV2 collection.",
                "In addition to evaluating our models for the two types of queries separately, we consider a more challenging and realistic situation that the two types of queries are mixed together without prior information on query types.",
                "To assist prediction under the mixed-query situation, a novel query classifier is adopted.",
                "Results show that our prediction of web query performance is substantially more accurate than the current stateof-the-art prediction techniques.",
                "Consequently, our paper provides a practical approach to performance prediction in realworld web settings.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval -Query formulation General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Query performance prediction has many applications in a variety of information retrieval (IR) areas such as improving retrieval consistency, query refinement, and distributed IR.",
                "The importance of this problem has been recognized by IR researchers and a number of new methods have been proposed for prediction recently [1, 2, 17].",
                "Most work on prediction has focused on the traditional ad-hoc retrieval task where query performance is measured according to topical relevance.",
                "These prediction models are evaluated on TREC document collections which typically consist of no more than one million relatively homogenous newswire articles.",
                "With the popularity and influence of the Web, prediction techniques that will work well for web-style queries are highly preferable.",
                "However, web search environments pose significant challenges to current prediction models that are mainly designed for traditional TREC settings.",
                "Here we outline some of these challenges.",
                "First, web collections, which are much larger than conventional TREC collections, include a variety of documents that are different in many aspects such as quality and style.",
                "Current prediction techniques can be vulnerable to these characteristics of web collections.",
                "For example, the reported prediction accuracy of the ranking robustness technique and the clarity technique on the GOV2 collection (a large web collection) is significantly worse compared to the other TREC collections [1].",
                "Similar prediction accuracy on the GOV2 collection using another technique is reported in [2], confirming the difficult of predicting query performance on a large web collection.",
                "Furthermore, web search goes beyond the scope of the ad-hoc retrieval task based on topical relevance.",
                "For example, the Named-Page (NP) finding task, which is a navigational task, is also popular in web retrieval.",
                "Query performance prediction for the NP task is still necessary since NP retrieval performance is far from perfect.",
                "In fact, according to the report on the NP task of the 2005 Terabyte Track [3], about 40% of the test queries perform poorly (no correct answer in the first 10 search results) even in the best run from the top group.",
                "To our knowledge, little research has explicitly addressed the problem of NP-query performance prediction.",
                "Current prediction models devised for content-based queries will be less effective for NP queries considering the fundamental differences between the two.",
                "Third, in real-world web search environments, user queries are usually a mixture of different types and prior knowledge about the type of each query is generally unavailable.",
                "The mixed-query situation raises new problems for query performance prediction.",
                "For instance, we may need to incorporate a query classifier into prediction models.",
                "Despite these problems, the ability to handle this situation is a crucial step towards turning query performance prediction from an interesting research topic into a practical tool for web retrieval.",
                "In this paper, we present three techniques to address the above challenges that current prediction models face in Web search environments.",
                "Our work focuses on query performance prediction for the content-based (ad-hoc) retrieval task and the name-page finding task in the context of web retrieval.",
                "Our first technique, called <br>weighted information gain</br> (WIG), makes use of both single term and term proximity features to estimate the quality of top retrieved documents for prediction.",
                "We find that WIG offers consistent prediction accuracy across various test collections and query types.",
                "Moreover, we demonstrate that good prediction accuracy can be achieved for the mixed-query situation by using WIG with the help of a query type classifier.",
                "Query feedback and first rank change, which are our second and third prediction techniques, perform well for content-based queries and NP queries respectively.",
                "Our main contributions include: (1) considerably improved prediction accuracy for web content-based queries over several state-of-the-art techniques. (2) new techniques for successfully predicting NP-query performance. (3) a practical and fully automatic solution to predicting mixed-query performance.",
                "In addition, one minor contribution is that we find that the robustness score [1], which was originally proposed for performance prediction, is helpful for query classification. 2.",
                "RELATED WORK As we mentioned in the introduction, a number of prediction techniques have been proposed recently that focus on contentbased queries in the topical relevance (ad-hoc) task.",
                "We know of no published work that addresses other types of queries such as NP queries, let alone a mixture of query types.",
                "Next we review some representative models.",
                "The major difficulty of performance prediction comes from the fact that many factors have an impact on retrieval performance.",
                "Each factor affects performance to a different degree and the overall effect is hard to predict accurately.",
                "Therefore, it is not surprising to notice that simple features, such as the frequency of query terms in the collection [4] and the average IDF of query terms [5], do not predict well.",
                "In fact, most of the successful techniques are based on measuring some characteristics of the retrieved document set to estimate topic difficulty.",
                "For example, the clarity score [6] measures the coherence of a list of documents by the KL-divergence between the query model and the collection model.",
                "The robustness score [1] quantifies another property of a ranked list: the robustness of the ranking in the presence of uncertainty.",
                "Carmel et al. [2] found that the distance measured by the Jensen-Shannon divergence between the retrieved document set and the collection is significantly correlated to average precision.",
                "Vinay et al. [7] proposed four measures to capture the geometry of the top retrieved documents for prediction.",
                "The most effective measure is the sensitivity to document perturbation, an idea somewhat similar to the robustness score.",
                "Unfortunately, their way of measuring the sensitivity does not perform equally well for short queries and prediction accuracy drops considerably when a state-of-the-art retrieval technique (like Okapi or a language modeling approach) is adopted for retrieval instead of the tf-idf weighting used in their paper [16].",
                "The difficulties of applying these models in web search environments have already been mentioned.",
                "In this paper, we mainly adopt the clarity score and the robustness score as our baselines.",
                "We experimentally show that the baselines, even after being carefully tuned, are inadequate for the web environment.",
                "One of our prediction models, WIG, is related to the Markov random field (MRF) model for information retrieval [8].",
                "The MRF model directly models term dependence and is found be to highly effective across a variety of test collections (particularly web collections) and retrieval tasks.",
                "This model is used to estimate the joint probability distribution over documents and queries, an important part of WIG.",
                "The superiority of WIG over other prediction techniques based on unigram features, which will be demonstrated later in our paper, coincides with that of MRF for retrieval.",
                "In other word, it is interesting to note that term dependence, when being modeled appropriately, can be helpful for both improving and predicting retrieval performance. 3.",
                "PREDICTION MODELS 3.1 <br>weighted information gain</br> (WIG) This section introduces a <br>weighted information gain</br> approach that incorporates both single term and proximity features for predicting performance for both content-based and Named-Page (NP) finding queries.",
                "Given a set of queries Q={Qs} (s=1,2,..N) which includes all possible user queries and a set of documents D={Dt} (t=1,2…M), we assume that each query-document pair (Qs,Dt) is manually judged and will be put in a relevance list if Qs is found to be relevant to Dt.",
                "The joint probability P(Qs,Dt) over queries Q and documents D denotes the probability that pair (Qs,Dt) will be in the relevance list.",
                "Such assumptions are similar to those used in [8].",
                "Assuming that the user issues query Qi ∈Q and the retrieval results in response to Qi is a ranked list L of documents, we calculate the amount of information contained in P(Qs,Dt) with respect to Qi and L by Eq.1 which is a variant of entropy called the weighted entropy[13].",
                "The weights in Eq.1 are solely determined by Qi and L. )1(),(log),(),( , , ∑−= ts tststsLQ DQPDQweightDQH i In this paper, we choose the weights as follows: LindocumentsKtopthecontainsLTwhere otherwise LTDandisifK DQweight K Kt ts )( )2( ,0 )(,/1 ),( ⎩ ⎨ ⎧ ∈= = The cutoff rank K is a parameter in our model that will be discussed later.",
                "Accordingly, Eq.1 can be simplified as follows: )3(),(log 1 ),( )( , ∑∈ −= LTD titsLQ Kt i DQP K DQH Unfortunately, weighted entropy ),(, tsLQ DQH i computed by Eq.3, which represents the amount of information about how likely the top ranked documents in L would be relevant to query Qi on average, cannot be compared across different queries, making it inappropriate for directly predicting query performance.",
                "To mitigate this problem, we come up with a background distribution P(Qs,C) over Q and D by imagining that every document in D is replaced by the same special document C which represents average language usage.",
                "In this paper, C is created by concatenating every document in D. Roughly speaking, C is the collection (the document set) {Dt} without document boundaries.",
                "Similarly, weighted entropy ),(, CQH sLQi calculated by Eq.3 represents the amount of information about how likely an average document (represented by the whole collection) would be relevant to query Qi.",
                "Now we introduce our performance predictor WIG which is the <br>weighted information gain</br> [13] computed as the difference between ),(, tsLQ DQH i and ),(, CQH sLQi .Specifically, given query Qi, collection C and ranked list L of documents, WIG is calculated as follows: )4( ),( ),( log 1 ),( ),( log),( ),(),(),,( )(, ,, ∑∑ ∈ == −= LTD i ti ts s ts ts tsLQsLQi Kt ii CQP DQP KCQP DQP DQweight DQHCQHLCQWIG WIG computed by Eq.4 measures the change in information about the quality of retrieval (in response to query Qi) from an imaginary state that only an average document is retrieved to a posterior state that the actual search results are observed.",
                "We hypothesize that WIG is positively correlated with retrieval effectiveness because high quality retrieval should be much more effective than just returning the average document.",
                "The heart of this technique is how to estimate the joint distribution P(Qs,Dt).",
                "In the language modeling approach to IR, a variety of models can be applied readily to estimate this distribution.",
                "Although most of these models are based on the bagof-words assumption, recent work on modeling term dependence under the language modeling framework have shown consistent and significant improvements in retrieval effectiveness over bagof-words models.",
                "Inspired by the success of incorporating term proximity features into language models, we decide to adopt a good dependence model to estimate the probability P(Qs,Dt).",
                "The model we chose for this paper is Metzler and Crofts Markov Random Field (MRF) model, which has already demonstrated superiority over a number of collections and different retrieval tasks [8,9].",
                "According to the MRF model, log P(Qi, Dt) can be written as )5()|(loglog),(log )( 1 ∑∈ +−= iQF tti DPZDQP ξ ξ ξλ where Z1 is a constant that ensures that P(Qi, Dt) sums up to 1.",
                "F(Qi) consists of a set of features expanded from the original query Qi .",
                "For example, assuming that query Qi is talented student program, F(Qi) includes features like program and talented student.",
                "We consider two kinds of features: single term features T and proximity features P. Proximity features include exact phrase (#1) and unordered window (#uwN) features as described in [8].",
                "Note that F(Qi) is the union of T(Qi) and P(Qi).",
                "For more details on F(Qi) such as how to expand the original query Qi to F(Qi), we refer the reader to [8] and [9].",
                "P(ξ|Dt) denotes the probability that feature ξ will occur in Dt.",
                "More details on P(ξ|Dt) will be provided later in this section.",
                "The choice of λξ is somewhat different from that used in [8] since λξ plays a dual role in our model.",
                "The first role, which is the same as in [8], is to weight between single term and proximity features.",
                "The other role, which is specific to our prediction task, is to normalize the size of F(Qi).We found that the following weight strategy for λξ satisfies the above two roles and generalizes well on a variety of collections and query types. )6( )(, |)(| 1 )(, |)(| ⎪ ⎪ ⎩ ⎪ ⎪ ⎨ ⎧ ∈ − ∈ = i i T i i T QP QP QT QT ξ λ ξ λ λξ where |T(Qi)| and |P(Qi)| denote the number of single term and proximity features in F(Qi) respectively.",
                "The reason for choosing the square root function in the denominator of λξ is to penalize a feature set of large size appropriately, making WIG more comparable across queries of various lengths. λT is a fixed parameter and set to 0.8 according to [8] throughout this paper.",
                "Similarly, log P(Qi,C) can be written as: )7()|(loglog),(log )( 2 ∑∈ +−= iQF i CPZCQP ξ ξ ξλ When constant Z1 and Z2 are dropped, WIG computed in Eq.4 can be rewritten as follows by plugging in Eq.5 and Eq.7 : )8( )|( )|( log 1 ),,( )( )( ∑ ∑∈ ∈ = LTD QF t i Kt i CP DP K LCQWIG ξ ξ ξ ξ λ One of the advantages of WIG over other techniques is that it can handle well both content-based and NP queries.",
                "Based on the type (or the predicted type) of Qi, the calculation of WIG in Eq. 8 differs in two aspects: (1) how to estimate P(ξ|Dt) and P(ξ|C), and (2) how to choose K. For content-based queries, P(ξ|C) is estimated by the relative frequency of feature ξ in collection C as a whole.",
                "The estimation of P(ξ|Dt) is the same as in [8].",
                "Namely, we estimate P(ξ|Dt) by the relative frequency of feature ξ in Dt linearly smoothed with collection frequency P(ξ|C).",
                "K in Eq.8 is treated as a free parameter.",
                "Note that K is the only free parameter in the computation of WIG for content-based queries because all parameters involved in P(ξ|Dt) are assumed to be fixed by taking the suggested values in [8].",
                "Regarding NP queries, we make use of document structure to estimate P(ξ|Dt) and P(ξ|C) by the so-called mixture of language models proposed in [10] and incorporated into the MRF model for Named-Page finding retrieval in [9].",
                "The basic idea is that a document (collection) is divided into several fields such as the title field, the main-body field and the heading field.",
                "P(ξ|Dt) and P(ξ|C) are estimated by a linear combination of the language models from each field.",
                "Due to space constraints, we refer the reader to [9] for details.",
                "We adopt the exact same set of parameters as used in [9] for estimation.",
                "With regard to K in Eq.8, we set K to 1 because the Named-Page finding task heavily focuses on the first ranked document.",
                "Consequently, there are no free parameters in the computation of WIG for NP queries. 3.2 Query Feedback In this section, we introduce another technique called query feedback (QF) for prediction.",
                "Suppose that a user issues query Q to a retrieval system and a ranked list L of documents is returned.",
                "We view the retrieval system as a noisy channel.",
                "Specifically, we assume that the output of the channel is L and the input is Q.",
                "After going through the channel, Q becomes corrupted and is transformed to ranked list L. By thinking about the retrieval process this way, the problem of predicting retrieval effectiveness turns to the task of evaluating the quality of the channel.",
                "In other words, prediction becomes finding a way to measure the degree of corruption that arises when Q is transformed to L. As directly computing the degree of the corruption is difficult, we tackle this problem by approximation.",
                "Our main idea is that we measure to what extent information on Q can be recovered from L on the assumption that only L is observed.",
                "Specifically, we design a decoder that can accurately translate L back into new query Q and the similarity S between the original query Q and the new query Q is adopted as a performance predictor.",
                "This is a sketch of how the QF technique predicts query performance.",
                "Before filling in more details, we briefly discuss why this method would work.",
                "There is a relation between the similarity S defined above and retrieval performance.",
                "On the one hand, if the retrieval has strayed from the original sense of the query Q, the new query Q extracted from ranked list L in response to Q would be very different from the original query Q.",
                "On the other hand, a query distilled from a ranked list containing many relevant documents is likely to be similar to the original query.",
                "Further examples in support of the relation will be provided later.",
                "Next we detail how to build the decoder and how to measure the similarity S. In essence, the goal of the decoder is to compress ranked list L into a few informative terms that should represent the content of the top ranked documents in L. Our approach to this goal is to represent ranked list L by a language model (distribution over terms).",
                "Then terms are ranked by their contribution to the language models KL (Kullback-Leibler) divergence from the background collection model.",
                "Top ranked terms will be chosen to form the new query Q.",
                "This approach is similar to that used in Section 4.1 of [11].",
                "Specifically, we take three steps to compress ranked list L into query Q without referring to the original query. 1.",
                "We adopt the ranked list language model [14], to estimate a language model based on ranked list L. The model can be written as: )9()|()|()|( ∑∈ = LD LDPDwPLwP where w is any term, D is a document.",
                "P(D|L) is estimated by a linearly decreasing function of the rank of document D. 2.",
                "Each term in P(w|L) is ranked by the following KL-divergence contribution: )10( )|( )|( log)|( CwP LwP LwP where P(w|C) is the collection model estimated by the relative frequency of term w in collection C as a whole. 3.",
                "The top N ranked terms by Eq.10 form a weighted query Q={(wi,ti)} i=1,N. where wi denotes the i-th ranked term and weight ti is the KL-divergence contribution of wi in Eq. 10.",
                "Term cruise ship vessel sea passenger KL contribution 0.050 0.040 0.012 0.010 0.009 Table 1: top 5 terms compressed from the ranked list in response to query Cruise ship damage sea life Two representative examples, one for a poorly performing query Cruise ship damage sea life (TREC topic 719; average precision: 0.08) and the other for a high performing query prostate cancer treatments( TREC topic 710; average precision: 0.49), are shown in Table 1 and 2 respectively.",
                "These examples indicate how the similarity between the original and the new query correlates with retrieval performance.",
                "The parameter N in step 3 is set to 20 empirically and choosing a larger value of N is unnecessary since the weights after the top 20 are usually too small to make any difference.",
                "Term prostate cancer treatment men therapy KL contribution 0.177 0.140 0.028 0.025 0.020 Table 2: top 5 terms compressed from the ranked list in response to query prostate cancer treatments To measure the similarity between original query Q and new query Q, we first use Q to do retrieval on the same collection.",
                "A variant of the query likelihood model [15] is adopted for retrieval.",
                "Namely, documents are ranked by: )11()|()|( ),( ∑∈ = Qtw t i ii i DwPDQP where wi is a term in Q and ti is the associated weight.",
                "D is a document.",
                "Let L denote the new ranked list returned from the above retrieval.",
                "The similarity is measured by the overlap of documents in L and L. Specifically, the percentage of documents in the top K documents of L that are also present in the top K documents in L. the cutoff K is treated as a free parameter.",
                "We summarize here how the QF technique predicts performance given a query Q and the associated ranked list L. We first obtain a weighted query Q compressed from L by the above three steps.",
                "Then we use Q to perform retrieval and the new ranked list is L. The overlap of documents in L and L is used for prediction. 3.3 First Rank Change (FRC) In this section, we propose a method called the first rank change (FRC) for performance prediction for NP queries.",
                "This method is derived from the ranking robustness technique [1] that is mainly designed for content-based queries.",
                "When directly applied to NP queries, the robustness technique will be less effective because it takes the top ranked documents as a whole into account while NP queries usually have only one single relevant document.",
                "Instead, our technique focuses on the first rank document while the main idea of the robustness method remains.",
                "Specifically, the pseudocode for computing FRC is shown in figure 1.",
                "Input: (1) ranked list L={Di} where i=1,100.",
                "Di denotes the i-th ranked document. (2) query Q 1 initialize: (1) set the number of trials J=100000 (2) counter c=0; 2 for i=1 to J 3 Perturb every document in L, let the outcome be a set F={Di} where Di denotes the perturbed version of Di. 4 Do retrieval with query Q on set F 5 c=c+1 if and only if D1 is ranked first in step 4 6 end of for 7 return the ratio c/J Figure 1: pseudo-code for computing FRC FRC approximates the probability that the first ranked document in the original list L will remain ranked first even after the documents are perturbed.",
                "The higher the probability is, the more confidence we have in the first ranked document.",
                "On the other hand, in the extreme case of a random ranking, the probability would be as low as 0.5.",
                "We expect that FRC has a positive association with NP query performance.",
                "We adopt [1] to implement the document perturbation step (step 4 in Fig.1) using Poisson distributions.",
                "For more details, we refer the reader to [1]. 4.",
                "EVALUATION We now present the results of predicting query performance by our models.",
                "Three state-of-the-art techniques are adopted as our baselines.",
                "We evaluate our techniques across a variety of Web retrieval settings.",
                "As mentioned before, we consider two types of queries, that is, content-based (CB) queries and Named-Page(NP) finding queries.",
                "First, suppose that the query types are known.",
                "We investigate the correlation between the predicted retrieval performance and the actual performance for both types of queries separately.",
                "Results show that our methods yield considerable improvements over the baselines.",
                "We then consider a more challenging scenario where no prior information on query types is available.",
                "Two sub-cases are considered.",
                "In the first one, there exists only one type of query but the actual type is unknown.",
                "We assume a mixture of the two query types in the second case.",
                "We demonstrate that our models achieve good accuracy under this demanding scenario, making prediction practical in a real-world Web search environment. 4.1 Experimental Setup Our evaluation focuses on the GOV2 collection which contains about 25 million documents crawled from web sites in the .gov domain during 2004 [3].",
                "We create two kinds of data set for CB queries and NP queries respectively.",
                "For the CB type, we use the ad-hoc topics of the Terabyte Tracks of 2004, 2005 and 2006 and name them TB04-adhoc, TB05-adhoc and TB06-adhoc respectively.",
                "In addition, we also use the ad-hoc topics of the 2004 Robust Track (RT04) to test the adaptability of our techniques to a non-Web environment.",
                "For NP queries, we use the Named-Page finding topics of the Terabyte Tracks of 2005 and 2006 and we name them TB05-NP and TB06-NP respectively.",
                "All queries used in our experiments are titles of TREC topics as we center on web retrieval.",
                "Table 3 summarizes the above data sets.",
                "Name Collection Topic Number Query Type TB04-adhoc GOV2 701-750 CB TB05-adhoc GOV2 751-800 CB TB06-adhoc GOV2 801-850 CB RT04 Disk 4+5 (minus CR) 301-450;601700 CB TB05-NP GOV2 NP601-NP872 NP TB06-NP GOV2 NP901-NP1081 NP Table 3: Summary of test collections and topics Retrieval performance of individual content-based and NP queries is measured by the average precision and reciprocal rank of the first correct answer respectively.",
                "We make use of the Markov Random field model for both ad-hoc and Named-Page finding retrieval.",
                "We adopt the same setting of retrieval parameters used in [8,9].",
                "The Indri search engine [12] is used for all of our experiments.",
                "Though not reported here, we also tried the query likelihood model for ad-hoc retrieval and found that the results change little because of the very high correlation between the query performances obtained by the two retrieval models (0.96 measured by Pearsons coefficient). 4.2 Known Query Types Suppose that query types are known.",
                "We treat each type of query separately and measure the correlation with average precision (or the reciprocal rank in the case of NP queries).",
                "We adopt the Pearsons correlation test which reflects the degree of linear relationship between the predicted and the actual retrieval performance. 4.2.1 Content-based Queries Methods Clarity Robust JSD WIG QF WIG +QF TB04+0 5 adhoc 0.333 0.317 0.362 0.574 0.480 0.637 TB06 adhoc 0.076 0.294 N/A 0.464 0.422 0.511 Table 4: Pearsons correlation coefficients for correlation with average precision on the Terabyte Tracks (ad-hoc) for clarity score, robustness score, the JSD-based method(we directly cites the score reported in [2]), WIG, query feedback(QF) and a linear combination of WIG and QF.",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Table 4 shows the correlation with average precision on two data sets: one is a combination of TB04-adhoc and TB05-adhoc(100 topics in total) and the other is TB06-adhoc (50 topics).",
                "The reason that we put TB04-adhoc and TB05-adhoc together is to make our results comparable to [2].",
                "Our baselines are the clarity score (clarity) [6],the robustness score (robust)[1] and the JSDbased method (JSD) [2].",
                "For the clarity and robustness score, we have tried different parameter settings and report the highest correlation coefficients we have found.",
                "We directly cite the result of the JSD-based method reported in [2].",
                "The table also shows the results for the <br>weighted information gain</br> (WIG) method and the Query Feedback (QF) method for predicting content-based queries.",
                "As we described in the previous section, both WIG and QF have one free parameter to set, that is, the cutoff rank K. We train the parameter on one dataset and test on the other.",
                "When combining WIG and QF, a simple linear combination is used and the combination weight is learned from the training data set.",
                "From these results, we can see that our methods are considerably more accurate compared to the baselines.",
                "We also observe that further improvements are obtained from the combination of WIG and QF, suggesting that they measure different properties of the retrieval process that relate to performance.",
                "We discover that our methods generalize well on TB06-adhoc while the correlation for the clarity score with retrieval performance on this data set is considerably worse.",
                "Further investigation shows that the mean average precision of TB06-adhoc is 0.342 and is about 10% better than that of the first data set.",
                "While the other three methods typically consider the top 100 or less documents given a ranked list, the clarity method usually needs the top 500 or more documents to adequately measure the coherence of a ranked list.",
                "Higher mean average precision makes ranked lists retrieved by different queries more similar in terms of coherence at the level of top 500 documents.",
                "We believe that this is the main reason for the low accuracy of the clarity score on the second data set.",
                "Though this paper focuses on a Web search environment, it is desirable that our techniques will work consistently well in other situations.",
                "To this end, we examine the effectiveness of our techniques on the Robust 2004 Track.",
                "For our methods, we evenly divide all of the test queries into five groups and perform five-fold cross validation.",
                "Each time we use one group for training and the remaining four groups for testing.",
                "We make use of all of the queries for our two baselines, that is, the clarity score and the robustness score.",
                "The parameters for our baselines are the same as those used in [1].The results shown in Table 5 demonstrate that the prediction accuracy of our methods is on a par with that of the two strong baselines.",
                "Clarity Robust WIG QF 0.464 0.539 0.468 0.464 Table 5: Comparison of Pearsons correlation coefficients on the 2004 Robust Track for clarity score, robustness score, WIG and query feedback (QF).",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Furthermore, we examine the prediction sensitivity of our methods to the cutoff rank K. With respect to WIG, it is quite robust to K on the Terabyte Tracks (2004-2006) while it prefers a small value of K like 5 on the 2004 Robust Track.",
                "In other words, a small value of K is a nearly-optimal choice for both kinds of tracks.",
                "Considering the fact that all other parameters involved in WIG are fixed and consequently the same for the two cases, this means WIG can achieve nearly-optimal prediction accuracy in two considerably different situations with exactly the same parameter setting.",
                "Regarding QF, it prefers a larger value of K such as 100 on the Terabyte Tracks and a smaller value of K such as 25 on the 2004 Robust Track. 4.2.2 NP Queries We adopt WIG and first rank change (FRC) for predicting NPquery performance.",
                "We also try a linear combination of the two as in the previous section.",
                "The combination weight is obtained from the other data set.",
                "We use the correlation with the reciprocal ranks measured by the Pearsons correlation test to evaluate prediction quality.",
                "The results are presented in Table 6.",
                "Again, our baselines are the clarity score and the robustness score.",
                "To make a fair comparison, we tune the clarity score in different ways.",
                "We found that using the first ranked document to build the query model yields the best prediction accuracy.",
                "We also attempted to utilize document structure by using the mixture of language models mentioned in section 3.1.",
                "Little improvement was obtained.",
                "The correlation coefficients for the clarity score reported in Table 6 are the best we have found.",
                "As we can see, our methods considerably outperform the clarity score technique on both of the runs.",
                "This confirms our intuition that the use of a coherence-based measure like the clarity score is inappropriate for NP queries.",
                "Methods Clarity Robust.",
                "WIG FRC WIG+FRC TB05-NP 0.150 -0.370 0.458 0.440 0.525 TB06-NP 0.112 -0.160 0.478 0.386 0.515 Table 6: Pearsons correlation coefficients for correlation with reciprocal ranks on the Terabyte Tracks (NP) for clarity score, robustness score, WIG, the first rank change (FRC) and a linear combination of WIG and FRC.",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Regarding the robustness score, we also tune the parameters and report the best we have found.",
                "We observe an interesting and surprising negative correlation with reciprocal ranks.",
                "We explain this finding briefly.",
                "A high robustness score means that a number of top ranked documents in the original ranked list are still highly ranked after perturbing the documents.",
                "The existence of such documents is a good sign of high performance for content-based queries as these queries usually contain a number of relevant documents [1].",
                "However, with regard to NP queries, one fundamental difference is that there is only one relevant document for each query.",
                "The existence of such documents can confuse the ranking function and lead to low retrieval performance.",
                "Although the negative correlation with retrieval performance exists, the strength of the correlation is weaker and less consistent compared to our methods as shown in Table 6.",
                "Based on the above analysis, we can see that current prediction techniques like clarity score and robustness score that are mainly designed for content-based queries face significant challenges and are inadequate to deal with NP queries.",
                "Our two techniques proposed for NP queries consistently demonstrate good prediction accuracy, displaying initial success in solving the problem of predicting performance for NP queries.",
                "Another point we want to stress is that the WIG method works well for both types of queries, a desirable property that most prediction techniques lack. 4.3 Unknown Query Types In this section, we run two kinds of experiments without access to query type labels.",
                "First, we assume that only one type of query exists but the type is unknown.",
                "Second, we experiment on a mixture of content-based and NP queries.",
                "The following two subsections will report results for the two conditions respectively. 4.3.1 Only One Type exists We assume that all queries are of the same type, that is, they are either NP queries or content-based queries.",
                "We choose WIG to deal with this case because it shows good prediction accuracy for both types of queries in the previous section.",
                "We consider two cases: (1) CB: all 150 title queries from the ad-hoc task of the Terabyte Tracks 2004-2006 (2)NP: all 433 NP queries from the named page finding task of the Terabyte Tracks 2005 and 2006.",
                "We take a simple strategy by labeling all of the queries in each case as the same type (either NP or CB) regardless of their actual type.",
                "The computation of WIG will be based on the labeled query type instead of the actual type.",
                "There are four possibilities with respect to the relation between the actual type and the labeled type.",
                "The correlation with retrieval performance under the four possibilities is presented in Table 7.",
                "For example, the value 0.445 at the intersection between the second row and the third column shows the Pearsons correlation coefficient for correlation with average precision when the content-based queries are incorrectly labeled as the NP type.",
                "Based on these results, we recommend treating all queries as the NP type when only one query type exists and accurate query classification is not feasible, considering the risk that a large loss of accuracy will occur if NP queries are incorrectly labeled as content-based queries.",
                "These results also demonstrate the strong adaptability of WIG to different query types.",
                "CB (labeled) NP (labeled) CB (actual) 0.536 0.445 NP (actual) 0.174 0.467 Table 7: Comparison of Pearsons correlation coefficients for correlation with retrieval performance under four possibilities on the Terabyte Tracks (NP).",
                "Bold cases mean the results are statistically significant at the 0.01 level. 4.3.2 A mixture of contented-based and NP queries A mixture of the two types of queries is a more realistic situation that a Web search engine will meet.",
                "We evaluate prediction accuracy by how accurately poorly-performing queries can be identified by the prediction method assuming that actual query types are unknown (but we can predict query types).",
                "This is a challenging task because both the predicted and actual performance for one type of query can be incomparable to that for the other type.",
                "Next we discuss how to implement our evaluation.",
                "We create a query pool which consists of all of the 150 ad-hoc title queries from Terabyte Track 2004-2006 and all of the 433 NP queries from Terabyte Track 2005&2006.",
                "We divide the queries in the pool into classes: good (better than 50% of the queries of the same type in terms of retrieval performance) and bad (otherwise).",
                "According to these standards, a NP query with the reciprocal rank above 0.2 or a content-based query with the average precision above 0.315 will be considered as good.",
                "Then, each time we randomly select one query Q from the pool with probability p that Q is contented-based.",
                "The remaining queries are used as training data.",
                "We first decide the type of query Q according to a query classifier.",
                "Namely, the query classifier tells us whether query Q is NP or content-based.",
                "Based on the predicted query type and the score computed for query Q by a prediction technique, a binary decision is made about whether query Q is good or bad by comparing to the score threshold of the predicted query type obtained from the training data.",
                "Prediction accuracy is measured by the accuracy of the binary decision.",
                "In our implementation, we repeatedly take a test query from the query pool and prediction accuracy is computed as the percentage of correct decisions, that is, a good(bad) query is predicted to be good (bad).",
                "It is obvious that random guessing will lead to 50% accuracy.",
                "Let us take the WIG method for example to illustrate the process.",
                "Two WIG thresholds (one for NP queries and the other for content-based queries) are trained by maximizing the prediction accuracy on the training data.",
                "When a test query is labeled as the NP (CB) type by the query type classifier, it will be predicted to be good if and only if the WIG score for this query is above the NP (CB) threshold.",
                "Similar procedures will be taken for other prediction techniques.",
                "Now we briefly introduce the automatic query type classifier used in this paper.",
                "We find that the robustness score, though originally proposed for performance prediction, is a good indicator of query types.",
                "We find that on average content-based queries have a much higher robustness score than NP queries.",
                "For example, Figure 2 shows the distributions of robustness scores for NP and content-based queries.",
                "According to this finding, the robustness score classifier will attach a NP (CB) label to the query if the robustness score for the query is below (above) a threshold trained from the training data. 0 0.5 1 1.5 2 2.5 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 NP Content-based Figure 2: Distribution of robustness scores for NP and CB queries.",
                "The NP queries are the 252 NP topics from the 2005 Terabyte Track.",
                "The content-based queries are the 150 ad-hoc title from the Terabyte Tracks 2004-2006.",
                "The probability distributions are estimated by the Kernel density estimation method.",
                "Strategies Robust WIG-1 WIG-2 WIG-3 Optimal p=0.6 0.565 0.624 0.665 0.684 0.701 P=0.4 0.567 0.633 0.654 0.673 0.696 Table 8: Comparison of prediction accuracy for five strategies in the mixed-query situation.",
                "Two ways to sample a query from the pool: (1) the sampled query is content-based with the probability p=0.6. (that is, the query is NP with probability 0.4 ) (2) set the probability p=0.4.",
                "We consider five strategies in our experiments.",
                "In the first strategy (denoted by robust), we use the robustness score for query performance prediction with the help of a perfect query classifier that always correctly map a query into one of the two categories (that is, NP or CB).",
                "This strategy represents the level of prediction accuracy that current prediction techniques can achieve in an ideal condition that query types are known.",
                "In the next following three strategies, the WIG method is adopted for performance prediction.",
                "The difference among the three is that three different query classifiers are used for each strategy: (1) the classifier always classifies a query into the NP type. (2) the Robustness Score ProbabilityDensity classifier is the robust score classifier mentioned above. (3) the classifier is a perfect one.",
                "These three strategies are denoted by WIG-1, WIG-2 and WIG-3 respectively.",
                "The reason we are interested in WIG-1 is based on the results from section 4.3.1.",
                "In the last strategy (denoted by Optimal) which serves as an upper bound on how well we can do so far, we fully make use of our prediction techniques for each query type assuming a perfect query classifier is available.",
                "Specifically, we linearly combine WIG and QF for content-based queries and WIG and FRC for NP queries.",
                "The results for the five strategies are shown in Table 8.",
                "For each strategy, we try two ways to sample a query from the pool: (1) the sampled query is CB with probability p=0.6. (the query is NP with probability 0.4) (2) set the probability p=0.4.",
                "From Table 8 We can see that in terms of prediction accuracy WIG-2 (the WIG method with the automatic query classifier) is not only better than the first two cases, but also is close to WIG-3 where a perfect classifier is assumed.",
                "Some further improvements over WIG-3 are observed when combined with other prediction techniques.",
                "The merit of WIG-2 is that it provides a practical solution to automatically identifying poorly performing queries in a Web search environment with mixed query types, which poses considerable obstacles to traditional prediction techniques. 5.",
                "CONCLUSIONS AND FUTURE WORK To our knowledge, our paper is the first to thoroughly explore prediction of query performance in web search environments.",
                "We demonstrated that our models resulted in higher prediction accuracy than previously published techniques not specially devised for web search scenarios.",
                "In this paper, we focus on two types of queries in web search: content-based and Named-Page (NP) finding queries, corresponding to the ad-hoc retrieval task and the Named-Page finding task respectively.",
                "For both types of web queries, our prediction models were shown to be substantially more accurate than the current state-of-the-art techniques.",
                "Furthermore, we considered a more realistic case that no prior information on query types is available.",
                "We demonstrated that the WIG method is particularly suitable for this situation.",
                "Considering the adaptability of WIG to a range of collections and query types, one of our future plans is to apply this method to predict user preference of search results on realistic data collected from a commercial search engine.",
                "Other than accuracy, another major issue that prediction techniques have to deal with in a Web environment is efficiency.",
                "Fortunately, since the WIG score is computed just over the terms and the phrases that appear in the query, this calculation can be made very efficient with the support of index.",
                "On the other hand, the computation of QF and FRC is relatively less efficient since QF needs to retrieve the whole collection twice and FRC needs to repeatedly rank the perturbed documents.",
                "How to improve the efficiency of QF and FRC is our future work.",
                "In addition, the prediction techniques proposed in this paper have the potential of improving retrieval performance by combining with other IR techniques.",
                "For example, our techniques can be incorporated to popular query modification techniques such as query expansion and query relaxation.",
                "Guided by performance prediction, we can make a better decision on when to or how to modify queries to enhance retrieval effectiveness.",
                "We would like to carry out research in this direction in the future. 6.",
                "ACKNOWLEGMENTS This work was supported in part by the Center for Intelligent Information Retrieval, in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C0023, and in part by an award from Google.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor.",
                "In addition, we thank Donald Metzler for his valuable comments on this work. 7.",
                "REFERENCES [1] Y. Zhou ,W. B. Croft ,Ranking Robustness: A Novel Framework to Predict Query Performance, in Proceedings of CIKM 2006. [2] D.Carmel, E.Yom-Tov, A.Darlow,D.Pelleg, What Makes a Query Difficult?, in Proceedings of SIGIR 2006. [3] C.L.A.",
                "Clarke, F. Scholer, I.Soboroff, The TREC 2005 Terabyte Track, In the Online Proceedings of 2005 TREC. [4] B.",
                "He and I.Ounis.",
                "Inferring query performance using preretrieval predictors.",
                "In proceedings of the SPIRE 2004. [5] S. Tomlinson.",
                "Robust, Web and Terabyte Retrieval with Hummingbird SearchServer at TREC 2004.",
                "In the Online Proceedings of 2004 TREC. [6] S. Cronen-Townsend, Y. Zhou, W. B. Croft, Predicting Query Performance, in Proceedings of SIGIR 2002. [7] V.Vinay, I.J.Cox, N.Mill-Frayling,K.Wood, On Ranking the Effectiveness of Searcher, in Proceedings of SIGIR 2006. [8] D.Metzler, W.B.Croft, A Markov Random Filed Model for Term Dependencies, in Proceedings of SIGIR 2005. [9] D.Metzler, T.Strohman,Y.Zhou,W.B.Croft, Indri at TREC 2005: Terabyte Track, In the Online Proceedings of 2004 TREC. [10] P. Ogilvie and J. Callan, Combining document representations for known-item search, in Proceedings of SIGIR 2003. [11] A.Berger, J.Lafferty, Information retrieval as statistical translation, in Proceedings of SIGIR 1999. [12] Indri search engine : http://www.lemurproject.org/indri/ [13] I.J.",
                "Taneja: On Generalized Information Measures and Their Applications, Advances in Electronics and Electron Physics, Academic Press (USA), 76, 1989, 327-413. [14] S.Cronen-Townsend, Y. Zhou and Croft, W. B. , A Framework for Selective Query Expansion, in Proceedings of CIKM 2004. [15] F.Song, W.B.Croft, A general language model for information retrieval, in Proceedings of SIGIR 1999. [16] Personal email contact with Vishwa Vinay and our own experiments [17] E.Yom-Tov, S.Fine, D.Carmel, A.Darlow, Learning to Estimate Query Difficulty Including Applications to Missing Content Detection and Distributed Information retrieval, in Proceedings of SIGIR 2005"
            ],
            "original_annotated_samples": [
                "Our first technique, called <br>weighted information gain</br> (WIG), makes use of both single term and term proximity features to estimate the quality of top retrieved documents for prediction.",
                "PREDICTION MODELS 3.1 <br>weighted information gain</br> (WIG) This section introduces a <br>weighted information gain</br> approach that incorporates both single term and proximity features for predicting performance for both content-based and Named-Page (NP) finding queries.",
                "Now we introduce our performance predictor WIG which is the <br>weighted information gain</br> [13] computed as the difference between ),(, tsLQ DQH i and ),(, CQH sLQi .Specifically, given query Qi, collection C and ranked list L of documents, WIG is calculated as follows: )4( ),( ),( log 1 ),( ),( log),( ),(),(),,( )(, ,, ∑∑ ∈ == −= LTD i ti ts s ts ts tsLQsLQi Kt ii CQP DQP KCQP DQP DQweight DQHCQHLCQWIG WIG computed by Eq.4 measures the change in information about the quality of retrieval (in response to query Qi) from an imaginary state that only an average document is retrieved to a posterior state that the actual search results are observed.",
                "The table also shows the results for the <br>weighted information gain</br> (WIG) method and the Query Feedback (QF) method for predicting content-based queries."
            ],
            "translated_annotated_samples": [
                "Nuestra primera técnica, llamada <br>ganancia de información ponderada</br> (WIG), utiliza tanto características de términos individuales como de proximidad de términos para estimar la calidad de los documentos recuperados para la predicción.",
                "Modelos de predicción 3.1 Ganancia de Información Ponderada (WIG) Esta sección introduce un enfoque de <br>ganancia de información ponderada</br> que incorpora tanto características de términos únicos como de proximidad para predecir el rendimiento tanto en consultas basadas en contenido como en consultas de búsqueda de Páginas Nombradas (NP).",
                "Ahora presentamos nuestro predictor de rendimiento WIG que es la <br>ganancia de información ponderada</br> [13] calculada como la diferencia entre ),(, tsLQ DQH i y ),(, CQH sLQi. Específicamente, dado el conjunto de consultas Qi, la colección C y la lista clasificada L de documentos, WIG se calcula de la siguiente manera: )4( ),( ),( log 1 ),( ),( log),( ),(),(),,( )(, ,, ∑∑ ∈ == −= LTD i ti ts s ts ts tsLQsLQi Kt ii CQP DQP KCQP DQP DQweight DQHCQHLCQWIG WIG calculado por la Ecuación 4 mide el cambio en la información sobre la calidad de la recuperación (en respuesta a la consulta Qi) desde un estado imaginario en el que solo se recupera un documento promedio hasta un estado posterior en el que se observan los resultados de búsqueda reales.",
                "La tabla también muestra los resultados para el <br>método de Ganancia de Información Ponderada</br> (WIG) y el método de Retroalimentación de Consulta (QF) para predecir consultas basadas en contenido."
            ],
            "translated_text": "Predicción del rendimiento de consultas en entornos de búsqueda web Yun Zhou y W. Bruce Croft Departamento de Ciencias de la Computación Universidad de Massachusetts, Amherst {yzhou, croft}@cs.umass.edu RESUMEN Las técnicas actuales de predicción, generalmente diseñadas para consultas basadas en contenido y evaluadas típicamente en colecciones de prueba relativamente homogéneas de tamaños pequeños, enfrentan serios desafíos en entornos de búsqueda web donde las colecciones son significativamente más heterogéneas y existen diferentes tipos de tareas de recuperación. En este artículo, presentamos tres técnicas para abordar estos desafíos. Nos enfocamos en la predicción de rendimiento para dos tipos de consultas en entornos de búsqueda web: basadas en contenido y búsqueda de páginas por nombre. Nuestra evaluación se realiza principalmente en la colección GOV2. Además de evaluar nuestros modelos para los dos tipos de consultas por separado, consideramos una situación más desafiante y realista en la que los dos tipos de consultas se mezclan sin información previa sobre los tipos de consulta. Para ayudar en la predicción bajo la situación de consulta mixta, se adopta un clasificador de consultas novedoso. Los resultados muestran que nuestra predicción del rendimiento de la consulta web es sustancialmente más precisa que las técnicas de predicción actuales más avanzadas. Por consiguiente, nuestro artículo proporciona un enfoque práctico para predecir el rendimiento en entornos web del mundo real. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Formulación de Consultas Términos Generales Algoritmos, Experimentación, Teoría 1. La predicción del rendimiento de la consulta tiene muchas aplicaciones en una variedad de áreas de recuperación de información (IR), como mejorar la consistencia de la recuperación, el refinamiento de la consulta y la IR distribuida. La importancia de este problema ha sido reconocida por los investigadores en Recuperación de Información y recientemente se han propuesto varios métodos nuevos para la predicción [1, 2, 17]. La mayoría de los trabajos sobre predicción se han centrado en la tarea tradicional de recuperación ad-hoc, donde el rendimiento de la consulta se mide según la relevancia temática. Estos modelos de predicción se evalúan en colecciones de documentos de TREC que suelen consistir en no más de un millón de artículos de agencias de noticias relativamente homogéneos. Con la popularidad e influencia de la Web, las técnicas de predicción que funcionen bien para consultas de estilo web son altamente preferibles. Sin embargo, los entornos de búsqueda web plantean desafíos significativos para los modelos de predicción actuales que están principalmente diseñados para entornos tradicionales de TREC. Aquí delineamos algunos de estos desafíos. Primero, las colecciones web, que son mucho más grandes que las colecciones TREC convencionales, incluyen una variedad de documentos que son diferentes en muchos aspectos, como la calidad y el estilo. Las técnicas actuales de predicción pueden ser vulnerables a estas características de las colecciones web. Por ejemplo, la precisión de predicción reportada de la técnica de robustez de clasificación y la técnica de claridad en la colección GOV2 (una colección web grande) es significativamente peor en comparación con las otras colecciones de TREC [1]. En [2] se informa de una precisión de predicción similar en la colección GOV2 utilizando otra técnica, confirmando la dificultad de predecir el rendimiento de consultas en una gran colección web. Además, la búsqueda web va más allá del alcance de la tarea de recuperación ad hoc basada en la relevancia temática. Por ejemplo, la tarea de encontrar páginas nombradas (NP), que es una tarea de navegación, también es popular en la recuperación web. La predicción del rendimiento de la consulta para la tarea de NP sigue siendo necesaria, ya que el rendimiento de recuperación de NP está lejos de ser perfecto. De hecho, según el informe sobre la tarea NP del Terabyte Track de 2005 [3], aproximadamente el 40% de las consultas de prueba tienen un rendimiento deficiente (sin respuesta correcta en los primeros 10 resultados de búsqueda) incluso en la mejor ejecución del grupo líder. Hasta donde sabemos, poca investigación ha abordado explícitamente el problema de la predicción del rendimiento de consultas de NP. Los modelos de predicción actuales diseñados para consultas basadas en contenido serán menos efectivos para consultas de NP considerando las diferencias fundamentales entre ambas. En tercer lugar, en entornos de búsqueda web del mundo real, las consultas de los usuarios suelen ser una mezcla de diferentes tipos y, por lo general, no se dispone de información previa sobre el tipo de cada consulta. La situación de consulta mixta plantea nuevos problemas para la predicción del rendimiento de la consulta. Por ejemplo, es posible que necesitemos incorporar un clasificador de consultas en los modelos de predicción. A pesar de estos problemas, la capacidad para manejar esta situación es un paso crucial hacia convertir la predicción del rendimiento de consultas de un tema de investigación interesante en una herramienta práctica para la recuperación web. En este artículo, presentamos tres técnicas para abordar los desafíos mencionados que enfrentan los modelos de predicción actuales en entornos de búsqueda web. Nuestro trabajo se centra en la predicción del rendimiento de consultas para la tarea de recuperación basada en contenido (ad-hoc) y la tarea de encontrar páginas por nombre en el contexto de la recuperación web. Nuestra primera técnica, llamada <br>ganancia de información ponderada</br> (WIG), utiliza tanto características de términos individuales como de proximidad de términos para estimar la calidad de los documentos recuperados para la predicción. Observamos que WIG ofrece una precisión de predicción consistente en diversas colecciones de pruebas y tipos de consultas. Además, demostramos que se puede lograr una buena precisión de predicción para la situación de consulta mixta utilizando WIG con la ayuda de un clasificador de tipo de consulta. La retroalimentación de consultas y el cambio de rango inicial, que son nuestras técnicas de predicción segunda y tercera, funcionan bien para consultas basadas en contenido y consultas de NP respectivamente. Nuestras principales contribuciones incluyen: (1) una precisión de predicción considerablemente mejorada para consultas basadas en contenido web en comparación con varias técnicas de vanguardia. (2) nuevas técnicas para predecir con éxito el rendimiento de consultas NP. (3) una solución práctica y totalmente automática para predecir el rendimiento de consultas mixtas. Además, una contribución menor es que encontramos que el puntaje de robustez [1], que originalmente se propuso para la predicción de rendimiento, es útil para la clasificación de consultas. 2. TRABAJO RELACIONADO Como mencionamos en la introducción, recientemente se han propuesto varias técnicas de predicción que se centran en consultas basadas en contenido en la tarea de relevancia temática (ad-hoc). No conocemos ningún trabajo publicado que aborde otros tipos de consultas como las consultas NP, y mucho menos una mezcla de tipos de consultas. A continuación revisamos algunos modelos representativos. La principal dificultad de la predicción del rendimiento proviene del hecho de que muchos factores tienen un impacto en el rendimiento de recuperación. Cada factor afecta el rendimiento en diferente medida y el efecto general es difícil de predecir con precisión. Por lo tanto, no es sorprendente notar que características simples, como la frecuencia de los términos de consulta en la colección [4] y la IDF promedio de los términos de consulta [5], no predicen bien. De hecho, la mayoría de las técnicas exitosas se basan en medir algunas características del conjunto de documentos recuperados para estimar la dificultad del tema. Por ejemplo, la puntuación de claridad [6] mide la coherencia de una lista de documentos mediante la divergencia KL entre el modelo de consulta y el modelo de colección. El puntaje de robustez [1] cuantifica otra propiedad de una lista clasificada: la robustez de la clasificación en presencia de incertidumbre. Carmel et al. [2] encontraron que la distancia medida por la divergencia de Jensen-Shannon entre el conjunto de documentos recuperados y la colección está significativamente correlacionada con la precisión promedio. Vinay et al. [7] propusieron cuatro medidas para capturar la geometría de los documentos mejor clasificados para la predicción. La medida más efectiva es la sensibilidad a la perturbación del documento, una idea algo similar a la puntuación de robustez. Desafortunadamente, su forma de medir la sensibilidad no funciona igual de bien para consultas cortas y la precisión de la predicción disminuye considerablemente cuando se adopta una técnica de recuperación de vanguardia (como Okapi o un enfoque de modelado de lenguaje) en lugar del peso tf-idf utilizado en su artículo [16]. Las dificultades de aplicar estos modelos en entornos de búsqueda web ya han sido mencionadas. En este documento, principalmente adoptamos la puntuación de claridad y la puntuación de robustez como nuestras líneas base. Experimentalmente demostramos que las líneas base, incluso después de ser ajustadas cuidadosamente, son inadecuadas para el entorno web. Uno de nuestros modelos de predicción, WIG, está relacionado con el modelo de campo aleatorio de Markov (MRF) para la recuperación de información [8]. El modelo MRF modela directamente la dependencia de términos y se ha demostrado ser altamente efectivo en una variedad de colecciones de pruebas (especialmente colecciones web) y tareas de recuperación. Este modelo se utiliza para estimar la distribución conjunta de probabilidad sobre documentos y consultas, una parte importante de WIG. La superioridad de WIG sobre otras técnicas de predicción basadas en características unigramas, que se demostrará más adelante en nuestro artículo, coincide con la de MRF para recuperación. En otras palabras, es interesante notar que la dependencia de términos, cuando se modela adecuadamente, puede ser útil tanto para mejorar como para predecir el rendimiento de recuperación. 3. Modelos de predicción 3.1 Ganancia de Información Ponderada (WIG) Esta sección introduce un enfoque de <br>ganancia de información ponderada</br> que incorpora tanto características de términos únicos como de proximidad para predecir el rendimiento tanto en consultas basadas en contenido como en consultas de búsqueda de Páginas Nombradas (NP). Dado un conjunto de consultas Q={Qs} (s=1,2,..N) que incluye todas las posibles consultas de usuario y un conjunto de documentos D={Dt} (t=1,2…M), asumimos que cada par consulta-documento (Qs,Dt) es evaluado manualmente y se incluirá en una lista de relevancia si se determina que Qs es relevante para Dt. La probabilidad conjunta P(Qs,Dt) sobre las consultas Q y los documentos D denota la probabilidad de que el par (Qs,Dt) esté en la lista de relevancia. Tales suposiciones son similares a las utilizadas en [8]. Suponiendo que el usuario emite la consulta Qi ∈Q y los resultados de recuperación en respuesta a Qi es una lista clasificada L de documentos, calculamos la cantidad de información contenida en P(Qs,Dt) con respecto a Qi y L mediante la Ec.1, que es una variante de la entropía llamada entropía ponderada[13]. Los pesos en la ecuación 1 están determinados únicamente por Qi y L. En este documento, elegimos los pesos de la siguiente manera: L es el número de documentos que contiene la consulta, K es el límite superior, LT es el límite superior de documentos, y D es el peso si K está en otro caso. La clasificación de corte K es un parámetro en nuestro modelo que se discutirá más adelante. Por lo tanto, la ecuación 1 se puede simplificar de la siguiente manera: )3(),(log 1 ),( )( , ∑∈ −= LTD titsLQ Kt i DQP K DQH Desafortunadamente, la entropía ponderada ),(, tsLQ DQH i calculada por la ecuación 3, que representa la cantidad de información sobre la probabilidad de que los documentos mejor clasificados en L sean relevantes para la consulta Qi en promedio, no se puede comparar entre diferentes consultas, lo que la hace inapropiada para predecir directamente el rendimiento de la consulta. Para mitigar este problema, creamos una distribución de fondo P(Qs,C) sobre Q y D imaginando que cada documento en D es reemplazado por el mismo documento especial C que representa el uso promedio del lenguaje. En este documento, C se crea concatenando cada documento en D. A grosso modo, C es la colección (el conjunto de documentos) {Dt} sin límites de documentos. De manera similar, la entropía ponderada ),(, CQH sLQi calculada por la Ecuación 3 representa la cantidad de información sobre la probabilidad de que un documento promedio (representado por toda la colección) sea relevante para la consulta Qi. Ahora presentamos nuestro predictor de rendimiento WIG que es la <br>ganancia de información ponderada</br> [13] calculada como la diferencia entre ),(, tsLQ DQH i y ),(, CQH sLQi. Específicamente, dado el conjunto de consultas Qi, la colección C y la lista clasificada L de documentos, WIG se calcula de la siguiente manera: )4( ),( ),( log 1 ),( ),( log),( ),(),(),,( )(, ,, ∑∑ ∈ == −= LTD i ti ts s ts ts tsLQsLQi Kt ii CQP DQP KCQP DQP DQweight DQHCQHLCQWIG WIG calculado por la Ecuación 4 mide el cambio en la información sobre la calidad de la recuperación (en respuesta a la consulta Qi) desde un estado imaginario en el que solo se recupera un documento promedio hasta un estado posterior en el que se observan los resultados de búsqueda reales. Hacemos la hipótesis de que WIG está positivamente correlacionado con la efectividad de la recuperación porque una recuperación de alta calidad debería ser mucho más efectiva que simplemente devolver el documento promedio. El corazón de esta técnica es cómo estimar la distribución conjunta P(Qs, Dt). En el enfoque de modelado del lenguaje para la recuperación de información, se pueden aplicar fácilmente una variedad de modelos para estimar esta distribución. Aunque la mayoría de estos modelos se basan en la suposición de la bolsa de palabras, trabajos recientes sobre modelado de dependencia de términos bajo el marco de modelado de lenguaje han demostrado mejoras consistentes y significativas en la efectividad de recuperación sobre los modelos de bolsa de palabras. Inspirados por el éxito de incorporar características de proximidad de términos en modelos de lenguaje, decidimos adoptar un buen modelo de dependencia para estimar la probabilidad P(Qs,Dt). El modelo que elegimos para este artículo es el modelo de Campo Aleatorio de Markov (MRF) de Metzler y Crofts, que ya ha demostrado su superioridad en comparación con varias colecciones y diferentes tareas de recuperación [8,9]. Según el modelo MRF, log P(Qi, Dt) se puede escribir como )5()|(loglog),(log )( 1 ∑∈ +−= iQF tti DPZDQP ξ ξ ξλ donde Z1 es una constante que asegura que P(Qi, Dt) sume 1. F(Qi) consiste en un conjunto de características ampliadas a partir de la consulta original Qi. Por ejemplo, suponiendo que la consulta Qi es el programa de estudiantes talentosos, F(Qi) incluye características como programa y estudiante talentoso. Consideramos dos tipos de características: características de términos individuales T y características de proximidad P. Las características de proximidad incluyen la frase exacta (#1) y las características de ventana desordenada (#uwN) como se describe en [8]. Ten en cuenta que F(Qi) es la unión de T(Qi) y P(Qi). Para obtener más detalles sobre F(Qi), como por ejemplo cómo expandir la consulta original Qi a F(Qi), remitimos al lector a [8] y [9]. P(ξ|Dt) denota la probabilidad de que la característica ξ ocurra en Dt. Más detalles sobre P(ξ|Dt) se proporcionarán más adelante en esta sección. La elección de λξ es algo diferente a la utilizada en [8] ya que λξ desempeña un doble papel en nuestro modelo. El primer rol, que es el mismo que en [8], es ponderar entre las características de términos individuales y de proximidad. El otro rol, específico de nuestra tarea de predicción, es normalizar el tamaño de F(Qi). Encontramos que la siguiente estrategia de peso para λξ satisface los dos roles anteriores y generaliza bien en una variedad de colecciones y tipos de consultas. )6( )(, |)(| 1 )(, |)(| ⎪ ⎪ ⎩ ⎪ ⎪ ⎨ ⎧ ∈ − ∈ = i i T i i T QP QP QT QT ξ λ ξ λ λξ donde |T(Qi)| y |P(Qi)| denotan el número de características de términos únicos y de proximidad en F(Qi) respectivamente. La razón de elegir la función raíz cuadrada en el denominador de λξ es penalizar adecuadamente un conjunto de características de gran tamaño, haciendo que WIG sea más comparable entre consultas de diversas longitudes. λT es un parámetro fijo y se establece en 0.8 según [8] a lo largo de este documento. De manera similar, log P(Qi,C) se puede escribir como: )7()|(loglog),(log )( 2 ∑∈ +−= iQF i CPZCQP ξ ξ ξλ Cuando se eliminan las constantes Z1 y Z2, el WIG calculado en la Ecuación 4 se puede reescribir de la siguiente manera al sustituir la Ecuación 5 y la Ecuación 7: )8( )|( )|( log 1 ),,( )( )( ∑ ∑∈ ∈ = LTD QF t i Kt i CP DP K LCQWIG ξ ξ ξ ξ λ Una de las ventajas del WIG sobre otras técnicas es que puede manejar tanto consultas basadas en contenido como en NP. Basándose en el tipo (o el tipo predicho) de Qi, el cálculo de WIG en la Ecuación 8 difiere en dos aspectos: (1) cómo estimar P(ξ|Dt) y P(ξ|C), y (2) cómo elegir K. Para consultas basadas en contenido, P(ξ|C) se estima por la frecuencia relativa del rasgo ξ en la colección C en su totalidad. La estimación de P(ξ|Dt) es la misma que en [8]. Es decir, estimamos P(ξ|Dt) mediante la frecuencia relativa del rasgo ξ en Dt suavizada linealmente con la frecuencia de colección P(ξ|C). La K en la Ec.8 se trata como un parámetro libre. Ten en cuenta que K es el único parámetro libre en el cálculo de WIG para consultas basadas en contenido, ya que se asume que todos los parámetros involucrados en P(ξ|Dt) están fijos al tomar los valores sugeridos en [8]. En cuanto a las consultas de NP, hacemos uso de la estructura del documento para estimar P(ξ|Dt) y P(ξ|C) mediante el llamado modelo de mezcla de lenguaje propuesto en [10] e incorporado en el modelo MRF para la recuperación de Named-Page en [9]. La idea básica es que un documento (colección) se divide en varios campos como el campo del título, el campo del cuerpo principal y el campo de los encabezados. P(ξ|Dt) y P(ξ|C) se estiman mediante una combinación lineal de los modelos de lenguaje de cada campo. Debido a limitaciones de espacio, remitimos al lector a [9] para más detalles. Adoptamos el mismo conjunto exacto de parámetros utilizados en [9] para la estimación. En cuanto a K en la Ec.8, establecemos K en 1 porque la tarea de encontrar Named-Page se centra fuertemente en el documento clasificado en primer lugar. Por consiguiente, no hay parámetros libres en el cálculo de WIG para consultas de NP. 3.2 Retroalimentación de consultas En esta sección, presentamos otra técnica llamada retroalimentación de consultas (QF) para la predicción. Suponga que un usuario emite la consulta Q a un sistema de recuperación y se devuelve una lista clasificada L de documentos. Vemos el sistema de recuperación como un canal ruidoso. Específicamente, asumimos que la salida del canal es L y la entrada es Q. Después de pasar por el canal, Q se corrompe y se transforma en la lista clasificada L. Al pensar en el proceso de recuperación de esta manera, el problema de predecir la eficacia de la recuperación se convierte en la tarea de evaluar la calidad del canal. En otras palabras, la predicción se convierte en encontrar una forma de medir el grado de corrupción que surge cuando Q se transforma en L. Dado que calcular directamente el grado de corrupción es difícil, abordamos este problema mediante aproximaciones. Nuestra idea principal es que medimos en qué medida la información sobre Q puede ser recuperada de L bajo la suposición de que solo se observa L. Específicamente, diseñamos un decodificador que pueda traducir con precisión L de vuelta a la nueva consulta Q y la similitud S entre la consulta original Q y la nueva consulta Q se adopta como un predictor de rendimiento. Este es un bosquejo de cómo la técnica QF predice el rendimiento de la consulta. Antes de completar más detalles, discutimos brevemente por qué este método funcionaría. Existe una relación entre la similitud S definida anteriormente y el rendimiento de recuperación. Por un lado, si la recuperación se ha desviado del sentido original de la consulta Q, la nueva consulta Q extraída de la lista clasificada L en respuesta a Q sería muy diferente de la consulta original Q. Por otro lado, una consulta destilada de una lista clasificada que contiene muchos documentos relevantes es probable que sea similar a la consulta original. Se proporcionarán más ejemplos en apoyo de la relación más adelante. A continuación detallamos cómo construir el decodificador y cómo medir la similitud S. En esencia, el objetivo del decodificador es comprimir la lista clasificada L en unos pocos términos informativos que deberían representar el contenido de los documentos mejor clasificados en L. Nuestro enfoque para este objetivo es representar la lista clasificada L mediante un modelo de lenguaje (distribución de términos). Luego, los términos se clasifican según su contribución a la divergencia KL (Kullback-Leibler) de los modelos de lenguaje con respecto al modelo de la colección de fondo. Los términos mejor clasificados serán elegidos para formar la nueva consulta Q. Este enfoque es similar al utilizado en la Sección 4.1 de [11]. Específicamente, tomamos tres pasos para comprimir la lista clasificada L en la consulta Q sin hacer referencia a la consulta original. 1. Adoptamos el modelo de lenguaje de lista clasificada [14] para estimar un modelo de lenguaje basado en la lista clasificada L. El modelo puede escribirse como: )9()|()|()|( ∑∈ = LD LDPDwPLwP donde w es cualquier término y D es un documento. La probabilidad de que ocurra el evento D dado el evento L se estima mediante una función linealmente decreciente del rango del documento D. Cada término en P(w|L) está clasificado por la siguiente contribución de divergencia de Kullback-Leibler: )10( )|( )|( log)|( CwP LwP LwP donde P(w|C) es el modelo de colección estimado por la frecuencia relativa del término w en la colección C en su totalidad. 3. Los términos N principales clasificados por Eq.10 forman una consulta ponderada Q={(wi,ti)} i=1,N, donde wi denota el término clasificado en la posición i y el peso ti es la contribución de la divergencia KL de wi en Eq. 10. Barco de crucero daño vida marina. Estos ejemplos indican cómo la similitud entre la consulta original y la nueva se correlaciona con el rendimiento de recuperación. El parámetro N en el paso 3 se establece en 20 de manera empírica y elegir un valor más grande de N es innecesario ya que los pesos después de los primeros 20 suelen ser demasiado pequeños para marcar alguna diferencia. Para medir la similitud entre la consulta original Q y la nueva consulta Q, primero utilizamos Q para recuperar información de la misma colección. Se adopta una variante del modelo de probabilidad de consulta [15] para la recuperación. Es decir, los documentos se clasifican por: )11()|()|( ),( ∑∈ = Qtw t i ii i DwPDQP donde wi es un término en Q y ti es el peso asociado. D es un documento. Que L denote la nueva lista clasificada devuelta desde la recuperación anterior. La similitud se mide por la superposición de documentos en L y L. Específicamente, el porcentaje de documentos en los primeros K documentos de L que también están presentes en los primeros K documentos en L. El valor de corte K se trata como un parámetro libre. Aquí resumimos cómo la técnica QF predice el rendimiento dado una consulta Q y la lista clasificada asociada L. Primero obtenemos una consulta ponderada Q comprimida a partir de L mediante los tres pasos anteriores. Luego usamos Q para realizar la recuperación y la nueva lista clasificada es L. La superposición de documentos en L y L se utiliza para la predicción. 3.3 Primer Cambio de Rango (FRC) En esta sección, proponemos un método llamado primer cambio de rango (FRC) para la predicción de rendimiento para consultas de NP. Este método se deriva de la técnica de robustez de clasificación [1] que está principalmente diseñada para consultas basadas en contenido. Cuando se aplica directamente a consultas de NP, la técnica de robustez será menos efectiva porque tiene en cuenta todos los documentos mejor clasificados en su totalidad, mientras que las consultas de NP suelen tener solo un único documento relevante. En cambio, nuestra técnica se centra en el documento de rango uno mientras se mantiene la idea principal del método de robustez. Específicamente, el seudocódigo para calcular FRC se muestra en la figura 1. Lista clasificada L={Di} donde i=1,100. Di denota el documento clasificado en la posición i. (2) consulta Q 1 inicializar: (1) establecer el número de intentos J=100000 (2) contador c=0; 2 para i=1 a J 3 Perturbar cada documento en L, que el resultado sea un conjunto F={Di} donde Di denota la versión perturbada de Di. 4 Realizar la recuperación con la consulta Q en el conjunto F 5 c=c+1 si y solo si D1 está clasificado primero en el paso 4 6 fin del para 7 devolver la proporción c/J Figura 1: pseudo-código para calcular FRC FRC aproxima la probabilidad de que el documento clasificado en primer lugar en la lista original L permanezca clasificado primero incluso después de que los documentos sean perturbados. Cuanto mayor sea la probabilidad, más confianza tenemos en el documento clasificado en primer lugar. Por otro lado, en el caso extremo de un ranking aleatorio, la probabilidad sería tan baja como 0.5. Esperamos que FRC tenga una asociación positiva con el rendimiento de la consulta de NP. Adoptamos [1] para implementar el paso de perturbación del documento (paso 4 en la Fig. 1) utilizando distribuciones de Poisson. Para más detalles, remitimos al lector a [1]. 4. EVALUACIÓN Ahora presentamos los resultados de predecir el rendimiento de la consulta por nuestros modelos. Tres técnicas de vanguardia son adoptadas como nuestras líneas base. Evaluamos nuestras técnicas en una variedad de configuraciones de recuperación web. Como se mencionó anteriormente, consideramos dos tipos de consultas, es decir, consultas basadas en contenido (CB) y consultas de búsqueda de páginas con nombre (NP). Primero, supongamos que se conocen los tipos de consultas. Investigamos la correlación entre el rendimiento de recuperación predicho y el rendimiento real para ambos tipos de consultas por separado. Los resultados muestran que nuestros métodos producen mejoras considerablemente superiores a los valores de referencia. Luego consideramos un escenario más desafiante donde no hay información previa sobre los tipos de consultas disponibles. Se consideran dos subcasos. En el primero, existe solo un tipo de consulta pero el tipo real es desconocido. Suponemos una mezcla de los dos tipos de consultas en el segundo caso. Demostramos que nuestros modelos logran una buena precisión en este escenario exigente, lo que hace que la predicción sea práctica en un entorno de búsqueda web del mundo real. 4.1 Configuración Experimental Nuestra evaluación se centra en la colección GOV2 que contiene alrededor de 25 millones de documentos obtenidos de sitios web en el dominio .gov durante 2004 [3]. Creamos dos tipos de conjuntos de datos para consultas de CB y consultas de NP respectivamente. Para el tipo CB, utilizamos los temas ad-hoc de las pistas Terabyte de 2004, 2005 y 2006 y los nombramos TB04-adhoc, TB05-adhoc y TB06-adhoc respectivamente. Además, también utilizamos los temas ad-hoc de la pista Robusta de 2004 (RT04) para probar la adaptabilidad de nuestras técnicas a un entorno no web. Para consultas de NP, utilizamos los temas de búsqueda de páginas nombradas de las Terabyte Tracks de 2005 y 2006 y los denominamos TB05-NP y TB06-NP respectivamente. Todas las consultas utilizadas en nuestros experimentos son títulos de temas de TREC, ya que nos centramos en la recuperación web. La Tabla 3 resume los conjuntos de datos anteriores. La recuperación del rendimiento de las consultas individuales basadas en contenido y NP se mide mediante la precisión promedio y la tasa recíproca de la primera respuesta correcta, respectivamente. Hacemos uso del modelo de campo aleatorio de Markov tanto para la recuperación ad-hoc como para la recuperación de páginas nombradas. Adoptamos la misma configuración de parámetros de recuperación utilizada en [8,9]. El motor de búsqueda Indri [12] se utiliza para todos nuestros experimentos. Aunque no se informa aquí, también probamos el modelo de probabilidad de consulta para la recuperación ad-hoc y encontramos que los resultados cambian poco debido a la correlación muy alta entre el rendimiento de las consultas obtenido por los dos modelos de recuperación (0.96 medido por el coeficiente de Pearson). 4.2 Tipos de Consultas Conocidos Supongamos que se conocen los tipos de consultas. Tratamos cada tipo de consulta por separado y medimos la correlación con la precisión promedio (o el rango recíproco en el caso de consultas NP). Adoptamos la prueba de correlación de Pearson que refleja el grado de relación lineal entre el rendimiento de recuperación predicho y real. 4.2.1 Métodos de Consultas Basadas en Contenido Claridad Robusta JSD WIG QF WIG +QF TB04+0 5 adhoc 0.333 0.317 0.362 0.574 0.480 0.637 TB06 adhoc 0.076 0.294 N/A 0.464 0.422 0.511 Tabla 4: Coeficientes de correlación de Pearson para la correlación con la precisión promedio en las Pistas de Terabyte (ad-hoc) para la puntuación de claridad, puntuación de robustez, el método basado en JSD (citamos directamente la puntuación informada en [2]), WIG, retroalimentación de consulta (QF) y una combinación lineal de WIG y QF. Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. La Tabla 4 muestra la correlación con la precisión promedio en dos conjuntos de datos: uno es una combinación de TB04-adhoc y TB05-adhoc (100 temas en total) y el otro es TB06-adhoc (50 temas). La razón por la que juntamos TB04-adhoc y TB05-adhoc es para hacer nuestros resultados comparables a [2]. Nuestros puntos de referencia son la puntuación de claridad (claridad) [6], la puntuación de robustez (robust) [1] y el método basado en JSD (JSD) [2]. Para la puntuación de claridad y robustez, hemos probado diferentes configuraciones de parámetros y reportamos los coeficientes de correlación más altos que hemos encontrado. Citamos directamente el resultado del método basado en JSD reportado en [2]. La tabla también muestra los resultados para el <br>método de Ganancia de Información Ponderada</br> (WIG) y el método de Retroalimentación de Consulta (QF) para predecir consultas basadas en contenido. Como describimos en la sección anterior, tanto WIG como QF tienen un parámetro libre para ajustar, es decir, el rango de corte K. Entrenamos el parámetro en un conjunto de datos y lo probamos en el otro. Al combinar WIG y QF, se utiliza una combinación lineal simple y el peso de la combinación se aprende a partir del conjunto de datos de entrenamiento. A partir de estos resultados, podemos ver que nuestros métodos son considerablemente más precisos en comparación con los baselines. También observamos que se obtienen mejoras adicionales a partir de la combinación de WIG y QF, lo que sugiere que miden diferentes propiedades del proceso de recuperación que se relacionan con el rendimiento. Descubrimos que nuestros métodos se generalizan bien en TB06-adhoc, mientras que la correlación del puntaje de claridad con el rendimiento de recuperación en este conjunto de datos es considerablemente peor. Una investigación adicional muestra que la precisión promedio media de TB06-adhoc es de 0.342 y es aproximadamente un 10% mejor que la del primer conjunto de datos. Mientras que los otros tres métodos suelen considerar los 100 mejores documentos o menos de una lista clasificada, el método de claridad generalmente necesita los 500 mejores documentos o más para medir adecuadamente la coherencia de una lista clasificada. Un promedio de precisión media más alto hace que las listas clasificadas recuperadas por diferentes consultas sean más similares en términos de coherencia en el nivel de los primeros 500 documentos. Creemos que esta es la razón principal de la baja precisión de la puntuación de claridad en el segundo conjunto de datos. Aunque este documento se centra en un entorno de búsqueda web, es deseable que nuestras técnicas funcionen de manera consistente en otras situaciones. Con este fin, examinamos la efectividad de nuestras técnicas en la pista Robust 2004. Para nuestros métodos, dividimos equitativamente todas las consultas de prueba en cinco grupos y realizamos validación cruzada de cinco pliegues. Cada vez que utilizamos un grupo para el entrenamiento y los otros cuatro grupos para las pruebas. Hacemos uso de todas las consultas para nuestros dos puntos de referencia, es decir, la puntuación de claridad y la puntuación de robustez. Los parámetros para nuestras líneas base son los mismos que los utilizados en [1]. Los resultados mostrados en la Tabla 5 demuestran que la precisión de predicción de nuestros métodos está a la par con la de las dos líneas base sólidas. Claridad Robusta WIG QF 0.464 0.539 0.468 0.464 Tabla 5: Comparación de los coeficientes de correlación de Pearson en la pista Robusta de 2004 para la puntuación de claridad, puntuación de robustez, WIG y retroalimentación de consulta (QF). Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. Además, examinamos la sensibilidad de predicción de nuestros métodos al rango de corte K. Con respecto a WIG, es bastante robusto a K en las Pistas de Terabyte (2004-2006) mientras prefiere un valor pequeño de K como 5 en la Pista Robusta de 2004. En otras palabras, un valor pequeño de K es una elección casi óptima para ambos tipos de pistas. Teniendo en cuenta que todos los demás parámetros involucrados en WIG están fijos y, consecuentemente, son los mismos para los dos casos, esto significa que WIG puede lograr una precisión de predicción casi óptima en dos situaciones considerablemente diferentes con exactamente la misma configuración de parámetros. En cuanto a QF, prefiere un valor más grande de K, como 100 en las Pistas de Terabyte y un valor más pequeño de K, como 25 en la Pista Robusta de 2004. 4.2.2 Consultas NP Adoptamos WIG y el primer cambio de rango (FRC) para predecir el rendimiento de las consultas NP. También intentamos una combinación lineal de los dos como en la sección anterior. El peso de la combinación se obtiene del otro conjunto de datos. Utilizamos la correlación con los rangos recíprocos medidos por la prueba de correlación de Pearson para evaluar la calidad de la predicción. Los resultados se presentan en la Tabla 6. Nuevamente, nuestros puntos de referencia son la puntuación de claridad y la puntuación de robustez. Para hacer una comparación justa, ajustamos la puntuación de claridad de diferentes maneras. Descubrimos que utilizar el documento clasificado en primer lugar para construir el modelo de consulta produce la mejor precisión de predicción. También intentamos utilizar la estructura del documento mediante la combinación de modelos de lenguaje mencionados en la sección 3.1. Se obtuvo una pequeña mejora. Los coeficientes de correlación para la puntuación de claridad reportados en la Tabla 6 son los mejores que hemos encontrado. Como podemos ver, nuestros métodos superan considerablemente la técnica de puntuación de claridad en ambas ejecuciones. Esto confirma nuestra intuición de que el uso de una medida basada en la coherencia como el puntaje de claridad es inapropiado para las consultas de NP. Claridad de métodos robusta. Tabla 6: Coeficientes de correlación de Pearson para la correlación con rangos recíprocos en las pistas de Terabyte (NP) para la puntuación de claridad, puntuación de robustez, WIG, el primer cambio de rango (FRC) y una combinación lineal de WIG y FRC. Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. En cuanto a la puntuación de robustez, también ajustamos los parámetros y reportamos el mejor que hemos encontrado. Observamos una correlación negativa interesante y sorprendente con los rangos recíprocos. Explicamos este hallazgo brevemente. Un puntaje de robustez alto significa que varios documentos de alta clasificación en la lista clasificada original siguen estando muy bien clasificados después de perturbar los documentos. La existencia de tales documentos es una buena señal de alto rendimiento para consultas basadas en contenido, ya que estas consultas suelen contener una serie de documentos relevantes [1]. Sin embargo, en lo que respecta a las consultas de NP, una diferencia fundamental es que solo hay un documento relevante para cada consulta. La existencia de tales documentos puede confundir la función de clasificación y llevar a un bajo rendimiento de recuperación. Aunque existe una correlación negativa con el rendimiento de recuperación, la fuerza de la correlación es más débil y menos consistente en comparación con nuestros métodos, como se muestra en la Tabla 6. Basándonos en el análisis anterior, podemos ver que las técnicas de predicción actuales como la puntuación de claridad y la puntuación de robustez, que están principalmente diseñadas para consultas basadas en contenido, enfrentan desafíos significativos y son insuficientes para manejar consultas de NP. Nuestras dos técnicas propuestas para consultas de NP demuestran consistentemente una buena precisión de predicción, mostrando un éxito inicial en la resolución del problema de predecir el rendimiento para consultas de NP. Otro punto que queremos destacar es que el método WIG funciona bien para ambos tipos de consultas, una propiedad deseable que la mayoría de las técnicas de predicción carecen. 4.3 Tipos de Consultas Desconocidos En esta sección, realizamos dos tipos de experimentos sin acceso a etiquetas de tipo de consulta. Primero, asumimos que solo existe un tipo de consulta pero el tipo es desconocido. Segundo, experimentamos con una mezcla de consultas basadas en contenido y NP. Las dos subsecciones siguientes informarán los resultados para las dos condiciones respectivamente. 4.3.1 Solo existe un tipo. Suponemos que todas las consultas son del mismo tipo, es decir, son consultas NP o consultas basadas en contenido. Elegimos WIG para tratar este caso porque muestra una buena precisión de predicción para ambos tipos de consultas en la sección anterior. Consideramos dos casos: (1) CB: todas las 150 consultas de títulos de la tarea ad-hoc de las pistas Terabyte 2004-2006 (2) NP: todas las 433 consultas de NP de la tarea de búsqueda de páginas nombradas de las pistas Terabyte 2005 y 2006. Tomamos una estrategia simple etiquetando todas las consultas en cada caso como el mismo tipo (ya sea NP o CB) independientemente de su tipo real. El cálculo de WIG se basará en el tipo de consulta etiquetado en lugar del tipo real. Hay cuatro posibilidades con respecto a la relación entre el tipo real y el tipo etiquetado. La correlación con el rendimiento de recuperación bajo las cuatro posibilidades se presenta en la Tabla 7. Por ejemplo, el valor 0.445 en la intersección entre la segunda fila y la tercera columna muestra el coeficiente de correlación de Pearson para la correlación con la precisión promedio cuando las consultas basadas en contenido se etiquetan incorrectamente como del tipo NP. Basándonos en estos resultados, recomendamos tratar todas las consultas como del tipo NP cuando solo existe un tipo de consulta y la clasificación precisa de la consulta no es factible, considerando el riesgo de que se produzca una gran pérdida de precisión si las consultas NP se etiquetan incorrectamente como consultas basadas en contenido. Estos resultados también demuestran la fuerte adaptabilidad de WIG a diferentes tipos de consultas. Tabla 7: Comparación de los coeficientes de correlación de Pearson para la correlación con el rendimiento de recuperación en cuatro posibilidades en las pistas de Terabyte (NP). Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. 4.3.2 Una mezcla de consultas basadas en contenido y NP Una mezcla de los dos tipos de consultas es una situación más realista que un motor de búsqueda web encontrará. Evaluamos la precisión de la predicción por la exactitud con la que se pueden identificar las consultas de bajo rendimiento mediante el método de predicción, asumiendo que los tipos de consulta reales son desconocidos (pero podemos predecir los tipos de consulta). Esta es una tarea desafiante porque tanto el rendimiento predicho como el real para un tipo de consulta pueden ser incomparables con el de otro tipo. A continuación discutimos cómo implementar nuestra evaluación. Creamos un conjunto de consultas que consiste en todas las 150 consultas de título ad-hoc de Terabyte Track 2004-2006 y todas las 433 consultas NP de Terabyte Track 2005&2006. Dividimos las consultas en el conjunto en clases: buenas (mejores que el 50% de las consultas del mismo tipo en términos de rendimiento de recuperación) y malas (de lo contrario). Según estos estándares, una consulta de NP con un rango recíproco superior a 0.2 o una consulta basada en contenido con una precisión promedio superior a 0.315 se considerarán como buenas. Entonces, cada vez que seleccionamos aleatoriamente una consulta Q del grupo con una probabilidad p de que Q esté basada en el contenido. Las consultas restantes se utilizan como datos de entrenamiento. Primero decidimos el tipo de consulta Q de acuerdo con un clasificador de consultas. Es decir, el clasificador de consultas nos dice si la consulta Q es de tipo NP o basada en contenido. Basándose en el tipo de consulta predicho y en la puntuación calculada para la consulta Q por una técnica de predicción, se toma una decisión binaria sobre si la consulta Q es buena o mala al compararla con el umbral de puntuación del tipo de consulta predicho obtenido de los datos de entrenamiento. La precisión de la predicción se mide por la exactitud de la decisión binaria. En nuestra implementación, tomamos repetidamente una consulta de prueba del conjunto de consultas y la precisión de la predicción se calcula como el porcentaje de decisiones correctas, es decir, una consulta buena (mala) se predice como buena (mala). Es obvio que adivinar al azar llevará a una precisión del 50%. Tomemos el método WIG como ejemplo para ilustrar el proceso. Dos umbrales de WIG (uno para consultas de NP y otro para consultas basadas en contenido) se entrenan maximizando la precisión de predicción en los datos de entrenamiento. Cuando una consulta de prueba es etiquetada como del tipo NP (CB) por el clasificador de tipo de consulta, se predecirá que es buena solo si la puntuación WIG para esta consulta está por encima del umbral de NP (CB). Se seguirán procedimientos similares para otras técnicas de predicción. Ahora presentamos brevemente el clasificador de tipo de consulta automático utilizado en este artículo. Encontramos que la puntuación de robustez, aunque originalmente propuesta para la predicción de rendimiento, es un buen indicador de tipos de consultas. Observamos que, en promedio, las consultas basadas en contenido tienen una puntuación de robustez mucho más alta que las consultas de NP. Por ejemplo, la Figura 2 muestra las distribuciones de puntajes de robustez para consultas basadas en NP y en contenido. Según este hallazgo, el clasificador de puntuación de robustez adjuntará una etiqueta NP (CB) a la consulta si la puntuación de robustez para la consulta está por debajo (por encima) de un umbral entrenado a partir de los datos de entrenamiento. 0 0.5 1 1.5 2 2.5 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 NP Basado en contenido Figura 2: Distribución de puntuaciones de robustez para consultas NP y CB. Las consultas NP son los 252 temas NP del Terabyte Track de 2005. Las consultas basadas en contenido son los 150 títulos ad-hoc de las pistas Terabyte 2004-2006. Las distribuciones de probabilidad son estimadas por el método de estimación de densidad de Kernel. Estrategias Robustas WIG-1 WIG-2 WIG-3 Óptima p=0.6 0.565 0.624 0.665 0.684 0.701 P=0.4 0.567 0.633 0.654 0.673 0.696 Tabla 8: Comparación de la precisión de predicción para cinco estrategias en la situación de consulta mixta. Dos formas de muestrear una consulta de la piscina: (1) la consulta muestreada es basada en contenido con una probabilidad p=0.6 (es decir, la consulta es NP con una probabilidad de 0.4) (2) establecer la probabilidad p=0.4. Consideramos cinco estrategias en nuestros experimentos. En la primera estrategia (denominada robusta), utilizamos la puntuación de robustez para la predicción del rendimiento de la consulta con la ayuda de un clasificador de consultas perfecto que siempre mapea correctamente una consulta en una de las dos categorías (es decir, NP o CB). Esta estrategia representa el nivel de precisión de predicción que las técnicas actuales de predicción pueden lograr en una condición ideal en la que se conocen los tipos de consulta. En las tres estrategias siguientes, se adopta el método WIG para predecir el rendimiento. La diferencia entre los tres es que se utilizan tres clasificadores de consulta diferentes para cada estrategia: (1) el clasificador siempre clasifica una consulta en el tipo NP. (2) el clasificador de Probabilidad de Densidad de Puntuación Robusta es el clasificador de puntuación robusta mencionado anteriormente. (3) el clasificador es perfecto. Estas tres estrategias están designadas como WIG-1, WIG-2 y WIG-3 respectivamente. La razón por la que estamos interesados en WIG-1 se basa en los resultados de la sección 4.3.1. En la última estrategia (denominada Óptima) que sirve como un límite superior de lo bien que podemos hacer hasta ahora, hacemos pleno uso de nuestras técnicas de predicción para cada tipo de consulta asumiendo que un clasificador de consultas perfecto está disponible. Específicamente, combinamos linealmente WIG y QF para consultas basadas en contenido y WIG y FRC para consultas de NP. Los resultados de las cinco estrategias se muestran en la Tabla 8. Para cada estrategia, intentamos dos formas de muestrear una consulta del conjunto: (1) la consulta muestreada es CB con probabilidad p=0.6 (la consulta es NP con probabilidad 0.4) (2) establecer la probabilidad p=0.4. De la Tabla 8 podemos ver que en términos de precisión de predicción, WIG-2 (el método WIG con el clasificador de consultas automático) no solo es mejor que los dos primeros casos, sino que también está cerca de WIG-3 donde se asume un clasificador perfecto. Se observan algunas mejoras adicionales sobre WIG-3 cuando se combinan con otras técnicas de predicción. El mérito de WIG-2 es que proporciona una solución práctica para identificar automáticamente consultas con bajo rendimiento en un entorno de búsqueda web con tipos de consultas mixtos, lo cual representa considerables obstáculos para las técnicas de predicción tradicionales. CONCLUSIONES Y TRABAJOS FUTUROS Hasta donde sabemos, nuestro artículo es el primero en explorar a fondo la predicción del rendimiento de consultas en entornos de búsqueda web. Demostramos que nuestros modelos resultaron en una mayor precisión de predicción que las técnicas previamente publicadas no especialmente diseñadas para escenarios de búsqueda en la web. En este artículo, nos enfocamos en dos tipos de consultas en la búsqueda web: consultas basadas en contenido y consultas de búsqueda de Páginas-Nombradas (NP), correspondientes a la tarea de recuperación ad-hoc y la tarea de búsqueda de Páginas-Nombradas respectivamente. Para ambos tipos de consultas web, se demostró que nuestros modelos de predicción son sustancialmente más precisos que las técnicas actuales más avanzadas. Además, consideramos un caso más realista en el que no se dispone de información previa sobre los tipos de consultas. Demostramos que el método WIG es particularmente adecuado para esta situación. Considerando la adaptabilidad de WIG a una variedad de colecciones y tipos de consultas, uno de nuestros planes futuros es aplicar este método para predecir la preferencia del usuario por los resultados de búsqueda en datos realistas recopilados de un motor de búsqueda comercial. Además de la precisión, otro problema importante con el que las técnicas de predicción tienen que lidiar en un entorno web es la eficiencia. Afortunadamente, dado que el puntaje WIG se calcula solo sobre los términos y frases que aparecen en la consulta, este cálculo puede realizarse de manera muy eficiente con el soporte de un índice. Por otro lado, el cálculo de QF y FRC es relativamente menos eficiente ya que QF necesita recuperar toda la colección dos veces y FRC necesita clasificar repetidamente los documentos perturbados. Cómo mejorar la eficiencia de QF y FRC es nuestro trabajo futuro. Además, las técnicas de predicción propuestas en este documento tienen el potencial de mejorar el rendimiento de recuperación al combinarse con otras técnicas de RI. Por ejemplo, nuestras técnicas pueden ser incorporadas a técnicas populares de modificación de consultas como la expansión de consultas y la relajación de consultas. Guiados por la predicción del rendimiento, podemos tomar una mejor decisión sobre cuándo o cómo modificar las consultas para mejorar la efectividad de la recuperación. Nos gustaría llevar a cabo investigaciones en esta dirección en el futuro. AGRADECIMIENTOS Este trabajo fue apoyado en parte por el Centro de Recuperación de Información Inteligente, en parte por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el contrato número HR0011-06-C0023, y en parte por un premio de Google. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son del autor y no necesariamente reflejan las del patrocinador. Además, agradecemos a Donald Metzler por sus valiosos comentarios sobre este trabajo. 7. REFERENCIAS [1] Y. Zhou, W. B. Croft, Ranking Robustness: Un nuevo marco para predecir el rendimiento de consultas, en Actas de CIKM 2006. [2] D. Carmel, E. Yom-Tov, A. Darlow, D. Pelleg, ¿Qué hace que una consulta sea difícil?, en Actas de SIGIR 2006. [3] C. L. A. Clarke, F. Scholer, I. Soboroff, La pista Terabyte TREC 2005, En las Actas en Línea de TREC 2005. [4] B. Él y yo. Inferir el rendimiento de la consulta utilizando predictores de preretrieval. En actas de SPIRE 2004. [5] S. Tomlinson. Recuperación robusta de la web y terabytes con Hummingbird SearchServer en TREC 2004. En las Actas en Línea de TREC 2004. [6] S. Cronen-Townsend, Y. Zhou, W. B. Croft, Prediciendo el Rendimiento de Consultas, en las Actas de SIGIR 2002. [7] V. Vinay, I. J. Cox, N. Mill-Frayling, K. Wood, Sobre la Clasificación de la Efectividad del Buscador, en las Actas de SIGIR 2006. [8] D. Metzler, W. B. Croft, Un Modelo de Campo Aleatorio de Markov para Dependencias de Términos, en las Actas de SIGIR 2005. [9] D. Metzler, T. Strohman, Y. Zhou, W. B. Croft, Indri en TREC 2005: Terabyte Track, en las Actas en Línea de TREC 2004. [10] P. Ogilvie y J. Callan, Combinando representaciones de documentos para búsqueda de elementos conocidos, en las Actas de SIGIR 2003. [11] A. Berger, J. Lafferty, Recuperación de información como traducción estadística, en las Actas de SIGIR 1999. [12] Motor de búsqueda Indri: http://www.lemurproject.org/indri/ [13] I. J. Taneja: Sobre medidas de información generalizadas y sus aplicaciones, Avances en Electrónica y Física de Electrones, Academic Press (EE. UU.), 76, 1989, 327-413. [14] S. Cronen-Townsend, Y. Zhou y Croft, W. B., Un marco para la expansión selectiva de consultas, en Actas de CIKM 2004. [15] F. Song, W. B. Croft, Un modelo de lenguaje general para la recuperación de información, en Actas de SIGIR 1999. [16] Contacto por correo electrónico personal con Vishwa Vinay y nuestros propios experimentos [17] E. Yom-Tov, S. Fine, D. Carmel, A. Darlow, Aprendiendo a estimar la dificultad de la consulta incluyendo aplicaciones para la detección de contenido faltante y la recuperación de información distribuida, en Actas de SIGIR 2005. ",
            "candidates": [],
            "error": [
                [
                    "ganancia de información ponderada",
                    "ganancia de información ponderada",
                    "ganancia de información ponderada",
                    "método de Ganancia de Información Ponderada"
                ]
            ]
        },
        "wig": {
            "translated_key": "WIG",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Query Performance Prediction in Web Search Environments Yun Zhou and W. Bruce Croft Department of Computer Science University of Massachusetts, Amherst {yzhou, croft}@cs.umass.edu ABSTRACT Current prediction techniques, which are generally designed for content-based queries and are typically evaluated on relatively homogenous test collections of small sizes, face serious challenges in web search environments where collections are significantly more heterogeneous and different types of retrieval tasks exist.",
                "In this paper, we present three techniques to address these challenges.",
                "We focus on performance prediction for two types of queries in web search environments: content-based and Named-Page finding.",
                "Our evaluation is mainly performed on the GOV2 collection.",
                "In addition to evaluating our models for the two types of queries separately, we consider a more challenging and realistic situation that the two types of queries are mixed together without prior information on query types.",
                "To assist prediction under the mixed-query situation, a novel query classifier is adopted.",
                "Results show that our prediction of web query performance is substantially more accurate than the current stateof-the-art prediction techniques.",
                "Consequently, our paper provides a practical approach to performance prediction in realworld web settings.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval -Query formulation General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Query performance prediction has many applications in a variety of information retrieval (IR) areas such as improving retrieval consistency, query refinement, and distributed IR.",
                "The importance of this problem has been recognized by IR researchers and a number of new methods have been proposed for prediction recently [1, 2, 17].",
                "Most work on prediction has focused on the traditional ad-hoc retrieval task where query performance is measured according to topical relevance.",
                "These prediction models are evaluated on TREC document collections which typically consist of no more than one million relatively homogenous newswire articles.",
                "With the popularity and influence of the Web, prediction techniques that will work well for web-style queries are highly preferable.",
                "However, web search environments pose significant challenges to current prediction models that are mainly designed for traditional TREC settings.",
                "Here we outline some of these challenges.",
                "First, web collections, which are much larger than conventional TREC collections, include a variety of documents that are different in many aspects such as quality and style.",
                "Current prediction techniques can be vulnerable to these characteristics of web collections.",
                "For example, the reported prediction accuracy of the ranking robustness technique and the clarity technique on the GOV2 collection (a large web collection) is significantly worse compared to the other TREC collections [1].",
                "Similar prediction accuracy on the GOV2 collection using another technique is reported in [2], confirming the difficult of predicting query performance on a large web collection.",
                "Furthermore, web search goes beyond the scope of the ad-hoc retrieval task based on topical relevance.",
                "For example, the Named-Page (NP) finding task, which is a navigational task, is also popular in web retrieval.",
                "Query performance prediction for the NP task is still necessary since NP retrieval performance is far from perfect.",
                "In fact, according to the report on the NP task of the 2005 Terabyte Track [3], about 40% of the test queries perform poorly (no correct answer in the first 10 search results) even in the best run from the top group.",
                "To our knowledge, little research has explicitly addressed the problem of NP-query performance prediction.",
                "Current prediction models devised for content-based queries will be less effective for NP queries considering the fundamental differences between the two.",
                "Third, in real-world web search environments, user queries are usually a mixture of different types and prior knowledge about the type of each query is generally unavailable.",
                "The mixed-query situation raises new problems for query performance prediction.",
                "For instance, we may need to incorporate a query classifier into prediction models.",
                "Despite these problems, the ability to handle this situation is a crucial step towards turning query performance prediction from an interesting research topic into a practical tool for web retrieval.",
                "In this paper, we present three techniques to address the above challenges that current prediction models face in Web search environments.",
                "Our work focuses on query performance prediction for the content-based (ad-hoc) retrieval task and the name-page finding task in the context of web retrieval.",
                "Our first technique, called weighted information gain (<br>wig</br>), makes use of both single term and term proximity features to estimate the quality of top retrieved documents for prediction.",
                "We find that <br>wig</br> offers consistent prediction accuracy across various test collections and query types.",
                "Moreover, we demonstrate that good prediction accuracy can be achieved for the mixed-query situation by using <br>wig</br> with the help of a query type classifier.",
                "Query feedback and first rank change, which are our second and third prediction techniques, perform well for content-based queries and NP queries respectively.",
                "Our main contributions include: (1) considerably improved prediction accuracy for web content-based queries over several state-of-the-art techniques. (2) new techniques for successfully predicting NP-query performance. (3) a practical and fully automatic solution to predicting mixed-query performance.",
                "In addition, one minor contribution is that we find that the robustness score [1], which was originally proposed for performance prediction, is helpful for query classification. 2.",
                "RELATED WORK As we mentioned in the introduction, a number of prediction techniques have been proposed recently that focus on contentbased queries in the topical relevance (ad-hoc) task.",
                "We know of no published work that addresses other types of queries such as NP queries, let alone a mixture of query types.",
                "Next we review some representative models.",
                "The major difficulty of performance prediction comes from the fact that many factors have an impact on retrieval performance.",
                "Each factor affects performance to a different degree and the overall effect is hard to predict accurately.",
                "Therefore, it is not surprising to notice that simple features, such as the frequency of query terms in the collection [4] and the average IDF of query terms [5], do not predict well.",
                "In fact, most of the successful techniques are based on measuring some characteristics of the retrieved document set to estimate topic difficulty.",
                "For example, the clarity score [6] measures the coherence of a list of documents by the KL-divergence between the query model and the collection model.",
                "The robustness score [1] quantifies another property of a ranked list: the robustness of the ranking in the presence of uncertainty.",
                "Carmel et al. [2] found that the distance measured by the Jensen-Shannon divergence between the retrieved document set and the collection is significantly correlated to average precision.",
                "Vinay et al. [7] proposed four measures to capture the geometry of the top retrieved documents for prediction.",
                "The most effective measure is the sensitivity to document perturbation, an idea somewhat similar to the robustness score.",
                "Unfortunately, their way of measuring the sensitivity does not perform equally well for short queries and prediction accuracy drops considerably when a state-of-the-art retrieval technique (like Okapi or a language modeling approach) is adopted for retrieval instead of the tf-idf weighting used in their paper [16].",
                "The difficulties of applying these models in web search environments have already been mentioned.",
                "In this paper, we mainly adopt the clarity score and the robustness score as our baselines.",
                "We experimentally show that the baselines, even after being carefully tuned, are inadequate for the web environment.",
                "One of our prediction models, <br>wig</br>, is related to the Markov random field (MRF) model for information retrieval [8].",
                "The MRF model directly models term dependence and is found be to highly effective across a variety of test collections (particularly web collections) and retrieval tasks.",
                "This model is used to estimate the joint probability distribution over documents and queries, an important part of <br>wig</br>.",
                "The superiority of <br>wig</br> over other prediction techniques based on unigram features, which will be demonstrated later in our paper, coincides with that of MRF for retrieval.",
                "In other word, it is interesting to note that term dependence, when being modeled appropriately, can be helpful for both improving and predicting retrieval performance. 3.",
                "PREDICTION MODELS 3.1 Weighted Information Gain (<br>wig</br>) This section introduces a weighted information gain approach that incorporates both single term and proximity features for predicting performance for both content-based and Named-Page (NP) finding queries.",
                "Given a set of queries Q={Qs} (s=1,2,..N) which includes all possible user queries and a set of documents D={Dt} (t=1,2…M), we assume that each query-document pair (Qs,Dt) is manually judged and will be put in a relevance list if Qs is found to be relevant to Dt.",
                "The joint probability P(Qs,Dt) over queries Q and documents D denotes the probability that pair (Qs,Dt) will be in the relevance list.",
                "Such assumptions are similar to those used in [8].",
                "Assuming that the user issues query Qi ∈Q and the retrieval results in response to Qi is a ranked list L of documents, we calculate the amount of information contained in P(Qs,Dt) with respect to Qi and L by Eq.1 which is a variant of entropy called the weighted entropy[13].",
                "The weights in Eq.1 are solely determined by Qi and L. )1(),(log),(),( , , ∑−= ts tststsLQ DQPDQweightDQH i In this paper, we choose the weights as follows: LindocumentsKtopthecontainsLTwhere otherwise LTDandisifK DQweight K Kt ts )( )2( ,0 )(,/1 ),( ⎩ ⎨ ⎧ ∈= = The cutoff rank K is a parameter in our model that will be discussed later.",
                "Accordingly, Eq.1 can be simplified as follows: )3(),(log 1 ),( )( , ∑∈ −= LTD titsLQ Kt i DQP K DQH Unfortunately, weighted entropy ),(, tsLQ DQH i computed by Eq.3, which represents the amount of information about how likely the top ranked documents in L would be relevant to query Qi on average, cannot be compared across different queries, making it inappropriate for directly predicting query performance.",
                "To mitigate this problem, we come up with a background distribution P(Qs,C) over Q and D by imagining that every document in D is replaced by the same special document C which represents average language usage.",
                "In this paper, C is created by concatenating every document in D. Roughly speaking, C is the collection (the document set) {Dt} without document boundaries.",
                "Similarly, weighted entropy ),(, CQH sLQi calculated by Eq.3 represents the amount of information about how likely an average document (represented by the whole collection) would be relevant to query Qi.",
                "Now we introduce our performance predictor <br>wig</br> which is the weighted information gain [13] computed as the difference between ),(, tsLQ DQH i and ),(, CQH sLQi .Specifically, given query Qi, collection C and ranked list L of documents, <br>wig</br> is calculated as follows: )4( ),( ),( log 1 ),( ),( log),( ),(),(),,( )(, ,, ∑∑ ∈ == −= LTD i ti ts s ts ts tsLQsLQi Kt ii CQP DQP KCQP DQP DQweight DQHCQHLCQWIG WIG computed by Eq.4 measures the change in information about the quality of retrieval (in response to query Qi) from an imaginary state that only an average document is retrieved to a posterior state that the actual search results are observed.",
                "We hypothesize that <br>wig</br> is positively correlated with retrieval effectiveness because high quality retrieval should be much more effective than just returning the average document.",
                "The heart of this technique is how to estimate the joint distribution P(Qs,Dt).",
                "In the language modeling approach to IR, a variety of models can be applied readily to estimate this distribution.",
                "Although most of these models are based on the bagof-words assumption, recent work on modeling term dependence under the language modeling framework have shown consistent and significant improvements in retrieval effectiveness over bagof-words models.",
                "Inspired by the success of incorporating term proximity features into language models, we decide to adopt a good dependence model to estimate the probability P(Qs,Dt).",
                "The model we chose for this paper is Metzler and Crofts Markov Random Field (MRF) model, which has already demonstrated superiority over a number of collections and different retrieval tasks [8,9].",
                "According to the MRF model, log P(Qi, Dt) can be written as )5()|(loglog),(log )( 1 ∑∈ +−= iQF tti DPZDQP ξ ξ ξλ where Z1 is a constant that ensures that P(Qi, Dt) sums up to 1.",
                "F(Qi) consists of a set of features expanded from the original query Qi .",
                "For example, assuming that query Qi is talented student program, F(Qi) includes features like program and talented student.",
                "We consider two kinds of features: single term features T and proximity features P. Proximity features include exact phrase (#1) and unordered window (#uwN) features as described in [8].",
                "Note that F(Qi) is the union of T(Qi) and P(Qi).",
                "For more details on F(Qi) such as how to expand the original query Qi to F(Qi), we refer the reader to [8] and [9].",
                "P(ξ|Dt) denotes the probability that feature ξ will occur in Dt.",
                "More details on P(ξ|Dt) will be provided later in this section.",
                "The choice of λξ is somewhat different from that used in [8] since λξ plays a dual role in our model.",
                "The first role, which is the same as in [8], is to weight between single term and proximity features.",
                "The other role, which is specific to our prediction task, is to normalize the size of F(Qi).We found that the following weight strategy for λξ satisfies the above two roles and generalizes well on a variety of collections and query types. )6( )(, |)(| 1 )(, |)(| ⎪ ⎪ ⎩ ⎪ ⎪ ⎨ ⎧ ∈ − ∈ = i i T i i T QP QP QT QT ξ λ ξ λ λξ where |T(Qi)| and |P(Qi)| denote the number of single term and proximity features in F(Qi) respectively.",
                "The reason for choosing the square root function in the denominator of λξ is to penalize a feature set of large size appropriately, making <br>wig</br> more comparable across queries of various lengths. λT is a fixed parameter and set to 0.8 according to [8] throughout this paper.",
                "Similarly, log P(Qi,C) can be written as: )7()|(loglog),(log )( 2 ∑∈ +−= iQF i CPZCQP ξ ξ ξλ When constant Z1 and Z2 are dropped, <br>wig</br> computed in Eq.4 can be rewritten as follows by plugging in Eq.5 and Eq.7 : )8( )|( )|( log 1 ),,( )( )( ∑ ∑∈ ∈ = LTD QF t i Kt i CP DP K LCQWIG ξ ξ ξ ξ λ One of the advantages of <br>wig</br> over other techniques is that it can handle well both content-based and NP queries.",
                "Based on the type (or the predicted type) of Qi, the calculation of <br>wig</br> in Eq. 8 differs in two aspects: (1) how to estimate P(ξ|Dt) and P(ξ|C), and (2) how to choose K. For content-based queries, P(ξ|C) is estimated by the relative frequency of feature ξ in collection C as a whole.",
                "The estimation of P(ξ|Dt) is the same as in [8].",
                "Namely, we estimate P(ξ|Dt) by the relative frequency of feature ξ in Dt linearly smoothed with collection frequency P(ξ|C).",
                "K in Eq.8 is treated as a free parameter.",
                "Note that K is the only free parameter in the computation of <br>wig</br> for content-based queries because all parameters involved in P(ξ|Dt) are assumed to be fixed by taking the suggested values in [8].",
                "Regarding NP queries, we make use of document structure to estimate P(ξ|Dt) and P(ξ|C) by the so-called mixture of language models proposed in [10] and incorporated into the MRF model for Named-Page finding retrieval in [9].",
                "The basic idea is that a document (collection) is divided into several fields such as the title field, the main-body field and the heading field.",
                "P(ξ|Dt) and P(ξ|C) are estimated by a linear combination of the language models from each field.",
                "Due to space constraints, we refer the reader to [9] for details.",
                "We adopt the exact same set of parameters as used in [9] for estimation.",
                "With regard to K in Eq.8, we set K to 1 because the Named-Page finding task heavily focuses on the first ranked document.",
                "Consequently, there are no free parameters in the computation of <br>wig</br> for NP queries. 3.2 Query Feedback In this section, we introduce another technique called query feedback (QF) for prediction.",
                "Suppose that a user issues query Q to a retrieval system and a ranked list L of documents is returned.",
                "We view the retrieval system as a noisy channel.",
                "Specifically, we assume that the output of the channel is L and the input is Q.",
                "After going through the channel, Q becomes corrupted and is transformed to ranked list L. By thinking about the retrieval process this way, the problem of predicting retrieval effectiveness turns to the task of evaluating the quality of the channel.",
                "In other words, prediction becomes finding a way to measure the degree of corruption that arises when Q is transformed to L. As directly computing the degree of the corruption is difficult, we tackle this problem by approximation.",
                "Our main idea is that we measure to what extent information on Q can be recovered from L on the assumption that only L is observed.",
                "Specifically, we design a decoder that can accurately translate L back into new query Q and the similarity S between the original query Q and the new query Q is adopted as a performance predictor.",
                "This is a sketch of how the QF technique predicts query performance.",
                "Before filling in more details, we briefly discuss why this method would work.",
                "There is a relation between the similarity S defined above and retrieval performance.",
                "On the one hand, if the retrieval has strayed from the original sense of the query Q, the new query Q extracted from ranked list L in response to Q would be very different from the original query Q.",
                "On the other hand, a query distilled from a ranked list containing many relevant documents is likely to be similar to the original query.",
                "Further examples in support of the relation will be provided later.",
                "Next we detail how to build the decoder and how to measure the similarity S. In essence, the goal of the decoder is to compress ranked list L into a few informative terms that should represent the content of the top ranked documents in L. Our approach to this goal is to represent ranked list L by a language model (distribution over terms).",
                "Then terms are ranked by their contribution to the language models KL (Kullback-Leibler) divergence from the background collection model.",
                "Top ranked terms will be chosen to form the new query Q.",
                "This approach is similar to that used in Section 4.1 of [11].",
                "Specifically, we take three steps to compress ranked list L into query Q without referring to the original query. 1.",
                "We adopt the ranked list language model [14], to estimate a language model based on ranked list L. The model can be written as: )9()|()|()|( ∑∈ = LD LDPDwPLwP where w is any term, D is a document.",
                "P(D|L) is estimated by a linearly decreasing function of the rank of document D. 2.",
                "Each term in P(w|L) is ranked by the following KL-divergence contribution: )10( )|( )|( log)|( CwP LwP LwP where P(w|C) is the collection model estimated by the relative frequency of term w in collection C as a whole. 3.",
                "The top N ranked terms by Eq.10 form a weighted query Q={(wi,ti)} i=1,N. where wi denotes the i-th ranked term and weight ti is the KL-divergence contribution of wi in Eq. 10.",
                "Term cruise ship vessel sea passenger KL contribution 0.050 0.040 0.012 0.010 0.009 Table 1: top 5 terms compressed from the ranked list in response to query Cruise ship damage sea life Two representative examples, one for a poorly performing query Cruise ship damage sea life (TREC topic 719; average precision: 0.08) and the other for a high performing query prostate cancer treatments( TREC topic 710; average precision: 0.49), are shown in Table 1 and 2 respectively.",
                "These examples indicate how the similarity between the original and the new query correlates with retrieval performance.",
                "The parameter N in step 3 is set to 20 empirically and choosing a larger value of N is unnecessary since the weights after the top 20 are usually too small to make any difference.",
                "Term prostate cancer treatment men therapy KL contribution 0.177 0.140 0.028 0.025 0.020 Table 2: top 5 terms compressed from the ranked list in response to query prostate cancer treatments To measure the similarity between original query Q and new query Q, we first use Q to do retrieval on the same collection.",
                "A variant of the query likelihood model [15] is adopted for retrieval.",
                "Namely, documents are ranked by: )11()|()|( ),( ∑∈ = Qtw t i ii i DwPDQP where wi is a term in Q and ti is the associated weight.",
                "D is a document.",
                "Let L denote the new ranked list returned from the above retrieval.",
                "The similarity is measured by the overlap of documents in L and L. Specifically, the percentage of documents in the top K documents of L that are also present in the top K documents in L. the cutoff K is treated as a free parameter.",
                "We summarize here how the QF technique predicts performance given a query Q and the associated ranked list L. We first obtain a weighted query Q compressed from L by the above three steps.",
                "Then we use Q to perform retrieval and the new ranked list is L. The overlap of documents in L and L is used for prediction. 3.3 First Rank Change (FRC) In this section, we propose a method called the first rank change (FRC) for performance prediction for NP queries.",
                "This method is derived from the ranking robustness technique [1] that is mainly designed for content-based queries.",
                "When directly applied to NP queries, the robustness technique will be less effective because it takes the top ranked documents as a whole into account while NP queries usually have only one single relevant document.",
                "Instead, our technique focuses on the first rank document while the main idea of the robustness method remains.",
                "Specifically, the pseudocode for computing FRC is shown in figure 1.",
                "Input: (1) ranked list L={Di} where i=1,100.",
                "Di denotes the i-th ranked document. (2) query Q 1 initialize: (1) set the number of trials J=100000 (2) counter c=0; 2 for i=1 to J 3 Perturb every document in L, let the outcome be a set F={Di} where Di denotes the perturbed version of Di. 4 Do retrieval with query Q on set F 5 c=c+1 if and only if D1 is ranked first in step 4 6 end of for 7 return the ratio c/J Figure 1: pseudo-code for computing FRC FRC approximates the probability that the first ranked document in the original list L will remain ranked first even after the documents are perturbed.",
                "The higher the probability is, the more confidence we have in the first ranked document.",
                "On the other hand, in the extreme case of a random ranking, the probability would be as low as 0.5.",
                "We expect that FRC has a positive association with NP query performance.",
                "We adopt [1] to implement the document perturbation step (step 4 in Fig.1) using Poisson distributions.",
                "For more details, we refer the reader to [1]. 4.",
                "EVALUATION We now present the results of predicting query performance by our models.",
                "Three state-of-the-art techniques are adopted as our baselines.",
                "We evaluate our techniques across a variety of Web retrieval settings.",
                "As mentioned before, we consider two types of queries, that is, content-based (CB) queries and Named-Page(NP) finding queries.",
                "First, suppose that the query types are known.",
                "We investigate the correlation between the predicted retrieval performance and the actual performance for both types of queries separately.",
                "Results show that our methods yield considerable improvements over the baselines.",
                "We then consider a more challenging scenario where no prior information on query types is available.",
                "Two sub-cases are considered.",
                "In the first one, there exists only one type of query but the actual type is unknown.",
                "We assume a mixture of the two query types in the second case.",
                "We demonstrate that our models achieve good accuracy under this demanding scenario, making prediction practical in a real-world Web search environment. 4.1 Experimental Setup Our evaluation focuses on the GOV2 collection which contains about 25 million documents crawled from web sites in the .gov domain during 2004 [3].",
                "We create two kinds of data set for CB queries and NP queries respectively.",
                "For the CB type, we use the ad-hoc topics of the Terabyte Tracks of 2004, 2005 and 2006 and name them TB04-adhoc, TB05-adhoc and TB06-adhoc respectively.",
                "In addition, we also use the ad-hoc topics of the 2004 Robust Track (RT04) to test the adaptability of our techniques to a non-Web environment.",
                "For NP queries, we use the Named-Page finding topics of the Terabyte Tracks of 2005 and 2006 and we name them TB05-NP and TB06-NP respectively.",
                "All queries used in our experiments are titles of TREC topics as we center on web retrieval.",
                "Table 3 summarizes the above data sets.",
                "Name Collection Topic Number Query Type TB04-adhoc GOV2 701-750 CB TB05-adhoc GOV2 751-800 CB TB06-adhoc GOV2 801-850 CB RT04 Disk 4+5 (minus CR) 301-450;601700 CB TB05-NP GOV2 NP601-NP872 NP TB06-NP GOV2 NP901-NP1081 NP Table 3: Summary of test collections and topics Retrieval performance of individual content-based and NP queries is measured by the average precision and reciprocal rank of the first correct answer respectively.",
                "We make use of the Markov Random field model for both ad-hoc and Named-Page finding retrieval.",
                "We adopt the same setting of retrieval parameters used in [8,9].",
                "The Indri search engine [12] is used for all of our experiments.",
                "Though not reported here, we also tried the query likelihood model for ad-hoc retrieval and found that the results change little because of the very high correlation between the query performances obtained by the two retrieval models (0.96 measured by Pearsons coefficient). 4.2 Known Query Types Suppose that query types are known.",
                "We treat each type of query separately and measure the correlation with average precision (or the reciprocal rank in the case of NP queries).",
                "We adopt the Pearsons correlation test which reflects the degree of linear relationship between the predicted and the actual retrieval performance. 4.2.1 Content-based Queries Methods Clarity Robust JSD <br>wig</br> QF <br>wig</br> +QF TB04+0 5 adhoc 0.333 0.317 0.362 0.574 0.480 0.637 TB06 adhoc 0.076 0.294 N/A 0.464 0.422 0.511 Table 4: Pearsons correlation coefficients for correlation with average precision on the Terabyte Tracks (ad-hoc) for clarity score, robustness score, the JSD-based method(we directly cites the score reported in [2]), WIG, query feedback(QF) and a linear combination of WIG and QF.",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Table 4 shows the correlation with average precision on two data sets: one is a combination of TB04-adhoc and TB05-adhoc(100 topics in total) and the other is TB06-adhoc (50 topics).",
                "The reason that we put TB04-adhoc and TB05-adhoc together is to make our results comparable to [2].",
                "Our baselines are the clarity score (clarity) [6],the robustness score (robust)[1] and the JSDbased method (JSD) [2].",
                "For the clarity and robustness score, we have tried different parameter settings and report the highest correlation coefficients we have found.",
                "We directly cite the result of the JSD-based method reported in [2].",
                "The table also shows the results for the Weighted Information Gain (<br>wig</br>) method and the Query Feedback (QF) method for predicting content-based queries.",
                "As we described in the previous section, both <br>wig</br> and QF have one free parameter to set, that is, the cutoff rank K. We train the parameter on one dataset and test on the other.",
                "When combining <br>wig</br> and QF, a simple linear combination is used and the combination weight is learned from the training data set.",
                "From these results, we can see that our methods are considerably more accurate compared to the baselines.",
                "We also observe that further improvements are obtained from the combination of <br>wig</br> and QF, suggesting that they measure different properties of the retrieval process that relate to performance.",
                "We discover that our methods generalize well on TB06-adhoc while the correlation for the clarity score with retrieval performance on this data set is considerably worse.",
                "Further investigation shows that the mean average precision of TB06-adhoc is 0.342 and is about 10% better than that of the first data set.",
                "While the other three methods typically consider the top 100 or less documents given a ranked list, the clarity method usually needs the top 500 or more documents to adequately measure the coherence of a ranked list.",
                "Higher mean average precision makes ranked lists retrieved by different queries more similar in terms of coherence at the level of top 500 documents.",
                "We believe that this is the main reason for the low accuracy of the clarity score on the second data set.",
                "Though this paper focuses on a Web search environment, it is desirable that our techniques will work consistently well in other situations.",
                "To this end, we examine the effectiveness of our techniques on the Robust 2004 Track.",
                "For our methods, we evenly divide all of the test queries into five groups and perform five-fold cross validation.",
                "Each time we use one group for training and the remaining four groups for testing.",
                "We make use of all of the queries for our two baselines, that is, the clarity score and the robustness score.",
                "The parameters for our baselines are the same as those used in [1].The results shown in Table 5 demonstrate that the prediction accuracy of our methods is on a par with that of the two strong baselines.",
                "Clarity Robust <br>wig</br> QF 0.464 0.539 0.468 0.464 Table 5: Comparison of Pearsons correlation coefficients on the 2004 Robust Track for clarity score, robustness score, <br>wig</br> and query feedback (QF).",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Furthermore, we examine the prediction sensitivity of our methods to the cutoff rank K. With respect to <br>wig</br>, it is quite robust to K on the Terabyte Tracks (2004-2006) while it prefers a small value of K like 5 on the 2004 Robust Track.",
                "In other words, a small value of K is a nearly-optimal choice for both kinds of tracks.",
                "Considering the fact that all other parameters involved in <br>wig</br> are fixed and consequently the same for the two cases, this means <br>wig</br> can achieve nearly-optimal prediction accuracy in two considerably different situations with exactly the same parameter setting.",
                "Regarding QF, it prefers a larger value of K such as 100 on the Terabyte Tracks and a smaller value of K such as 25 on the 2004 Robust Track. 4.2.2 NP Queries We adopt <br>wig</br> and first rank change (FRC) for predicting NPquery performance.",
                "We also try a linear combination of the two as in the previous section.",
                "The combination weight is obtained from the other data set.",
                "We use the correlation with the reciprocal ranks measured by the Pearsons correlation test to evaluate prediction quality.",
                "The results are presented in Table 6.",
                "Again, our baselines are the clarity score and the robustness score.",
                "To make a fair comparison, we tune the clarity score in different ways.",
                "We found that using the first ranked document to build the query model yields the best prediction accuracy.",
                "We also attempted to utilize document structure by using the mixture of language models mentioned in section 3.1.",
                "Little improvement was obtained.",
                "The correlation coefficients for the clarity score reported in Table 6 are the best we have found.",
                "As we can see, our methods considerably outperform the clarity score technique on both of the runs.",
                "This confirms our intuition that the use of a coherence-based measure like the clarity score is inappropriate for NP queries.",
                "Methods Clarity Robust.",
                "<br>wig</br> FRC <br>wig</br>+FRC TB05-NP 0.150 -0.370 0.458 0.440 0.525 TB06-NP 0.112 -0.160 0.478 0.386 0.515 Table 6: Pearsons correlation coefficients for correlation with reciprocal ranks on the Terabyte Tracks (NP) for clarity score, robustness score, WIG, the first rank change (FRC) and a linear combination of WIG and FRC.",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Regarding the robustness score, we also tune the parameters and report the best we have found.",
                "We observe an interesting and surprising negative correlation with reciprocal ranks.",
                "We explain this finding briefly.",
                "A high robustness score means that a number of top ranked documents in the original ranked list are still highly ranked after perturbing the documents.",
                "The existence of such documents is a good sign of high performance for content-based queries as these queries usually contain a number of relevant documents [1].",
                "However, with regard to NP queries, one fundamental difference is that there is only one relevant document for each query.",
                "The existence of such documents can confuse the ranking function and lead to low retrieval performance.",
                "Although the negative correlation with retrieval performance exists, the strength of the correlation is weaker and less consistent compared to our methods as shown in Table 6.",
                "Based on the above analysis, we can see that current prediction techniques like clarity score and robustness score that are mainly designed for content-based queries face significant challenges and are inadequate to deal with NP queries.",
                "Our two techniques proposed for NP queries consistently demonstrate good prediction accuracy, displaying initial success in solving the problem of predicting performance for NP queries.",
                "Another point we want to stress is that the <br>wig</br> method works well for both types of queries, a desirable property that most prediction techniques lack. 4.3 Unknown Query Types In this section, we run two kinds of experiments without access to query type labels.",
                "First, we assume that only one type of query exists but the type is unknown.",
                "Second, we experiment on a mixture of content-based and NP queries.",
                "The following two subsections will report results for the two conditions respectively. 4.3.1 Only One Type exists We assume that all queries are of the same type, that is, they are either NP queries or content-based queries.",
                "We choose <br>wig</br> to deal with this case because it shows good prediction accuracy for both types of queries in the previous section.",
                "We consider two cases: (1) CB: all 150 title queries from the ad-hoc task of the Terabyte Tracks 2004-2006 (2)NP: all 433 NP queries from the named page finding task of the Terabyte Tracks 2005 and 2006.",
                "We take a simple strategy by labeling all of the queries in each case as the same type (either NP or CB) regardless of their actual type.",
                "The computation of <br>wig</br> will be based on the labeled query type instead of the actual type.",
                "There are four possibilities with respect to the relation between the actual type and the labeled type.",
                "The correlation with retrieval performance under the four possibilities is presented in Table 7.",
                "For example, the value 0.445 at the intersection between the second row and the third column shows the Pearsons correlation coefficient for correlation with average precision when the content-based queries are incorrectly labeled as the NP type.",
                "Based on these results, we recommend treating all queries as the NP type when only one query type exists and accurate query classification is not feasible, considering the risk that a large loss of accuracy will occur if NP queries are incorrectly labeled as content-based queries.",
                "These results also demonstrate the strong adaptability of <br>wig</br> to different query types.",
                "CB (labeled) NP (labeled) CB (actual) 0.536 0.445 NP (actual) 0.174 0.467 Table 7: Comparison of Pearsons correlation coefficients for correlation with retrieval performance under four possibilities on the Terabyte Tracks (NP).",
                "Bold cases mean the results are statistically significant at the 0.01 level. 4.3.2 A mixture of contented-based and NP queries A mixture of the two types of queries is a more realistic situation that a Web search engine will meet.",
                "We evaluate prediction accuracy by how accurately poorly-performing queries can be identified by the prediction method assuming that actual query types are unknown (but we can predict query types).",
                "This is a challenging task because both the predicted and actual performance for one type of query can be incomparable to that for the other type.",
                "Next we discuss how to implement our evaluation.",
                "We create a query pool which consists of all of the 150 ad-hoc title queries from Terabyte Track 2004-2006 and all of the 433 NP queries from Terabyte Track 2005&2006.",
                "We divide the queries in the pool into classes: good (better than 50% of the queries of the same type in terms of retrieval performance) and bad (otherwise).",
                "According to these standards, a NP query with the reciprocal rank above 0.2 or a content-based query with the average precision above 0.315 will be considered as good.",
                "Then, each time we randomly select one query Q from the pool with probability p that Q is contented-based.",
                "The remaining queries are used as training data.",
                "We first decide the type of query Q according to a query classifier.",
                "Namely, the query classifier tells us whether query Q is NP or content-based.",
                "Based on the predicted query type and the score computed for query Q by a prediction technique, a binary decision is made about whether query Q is good or bad by comparing to the score threshold of the predicted query type obtained from the training data.",
                "Prediction accuracy is measured by the accuracy of the binary decision.",
                "In our implementation, we repeatedly take a test query from the query pool and prediction accuracy is computed as the percentage of correct decisions, that is, a good(bad) query is predicted to be good (bad).",
                "It is obvious that random guessing will lead to 50% accuracy.",
                "Let us take the <br>wig</br> method for example to illustrate the process.",
                "Two <br>wig</br> thresholds (one for NP queries and the other for content-based queries) are trained by maximizing the prediction accuracy on the training data.",
                "When a test query is labeled as the NP (CB) type by the query type classifier, it will be predicted to be good if and only if the <br>wig</br> score for this query is above the NP (CB) threshold.",
                "Similar procedures will be taken for other prediction techniques.",
                "Now we briefly introduce the automatic query type classifier used in this paper.",
                "We find that the robustness score, though originally proposed for performance prediction, is a good indicator of query types.",
                "We find that on average content-based queries have a much higher robustness score than NP queries.",
                "For example, Figure 2 shows the distributions of robustness scores for NP and content-based queries.",
                "According to this finding, the robustness score classifier will attach a NP (CB) label to the query if the robustness score for the query is below (above) a threshold trained from the training data. 0 0.5 1 1.5 2 2.5 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 NP Content-based Figure 2: Distribution of robustness scores for NP and CB queries.",
                "The NP queries are the 252 NP topics from the 2005 Terabyte Track.",
                "The content-based queries are the 150 ad-hoc title from the Terabyte Tracks 2004-2006.",
                "The probability distributions are estimated by the Kernel density estimation method.",
                "Strategies Robust <br>wig</br>-1 <br>wig</br>-2 WIG-3 Optimal p=0.6 0.565 0.624 0.665 0.684 0.701 P=0.4 0.567 0.633 0.654 0.673 0.696 Table 8: Comparison of prediction accuracy for five strategies in the mixed-query situation.",
                "Two ways to sample a query from the pool: (1) the sampled query is content-based with the probability p=0.6. (that is, the query is NP with probability 0.4 ) (2) set the probability p=0.4.",
                "We consider five strategies in our experiments.",
                "In the first strategy (denoted by robust), we use the robustness score for query performance prediction with the help of a perfect query classifier that always correctly map a query into one of the two categories (that is, NP or CB).",
                "This strategy represents the level of prediction accuracy that current prediction techniques can achieve in an ideal condition that query types are known.",
                "In the next following three strategies, the <br>wig</br> method is adopted for performance prediction.",
                "The difference among the three is that three different query classifiers are used for each strategy: (1) the classifier always classifies a query into the NP type. (2) the Robustness Score ProbabilityDensity classifier is the robust score classifier mentioned above. (3) the classifier is a perfect one.",
                "These three strategies are denoted by <br>wig</br>-1, <br>wig</br>-2 and WIG-3 respectively.",
                "The reason we are interested in <br>wig</br>-1 is based on the results from section 4.3.1.",
                "In the last strategy (denoted by Optimal) which serves as an upper bound on how well we can do so far, we fully make use of our prediction techniques for each query type assuming a perfect query classifier is available.",
                "Specifically, we linearly combine <br>wig</br> and QF for content-based queries and <br>wig</br> and FRC for NP queries.",
                "The results for the five strategies are shown in Table 8.",
                "For each strategy, we try two ways to sample a query from the pool: (1) the sampled query is CB with probability p=0.6. (the query is NP with probability 0.4) (2) set the probability p=0.4.",
                "From Table 8 We can see that in terms of prediction accuracy <br>wig</br>-2 (the <br>wig</br> method with the automatic query classifier) is not only better than the first two cases, but also is close to WIG-3 where a perfect classifier is assumed.",
                "Some further improvements over <br>wig</br>-3 are observed when combined with other prediction techniques.",
                "The merit of <br>wig</br>-2 is that it provides a practical solution to automatically identifying poorly performing queries in a Web search environment with mixed query types, which poses considerable obstacles to traditional prediction techniques. 5.",
                "CONCLUSIONS AND FUTURE WORK To our knowledge, our paper is the first to thoroughly explore prediction of query performance in web search environments.",
                "We demonstrated that our models resulted in higher prediction accuracy than previously published techniques not specially devised for web search scenarios.",
                "In this paper, we focus on two types of queries in web search: content-based and Named-Page (NP) finding queries, corresponding to the ad-hoc retrieval task and the Named-Page finding task respectively.",
                "For both types of web queries, our prediction models were shown to be substantially more accurate than the current state-of-the-art techniques.",
                "Furthermore, we considered a more realistic case that no prior information on query types is available.",
                "We demonstrated that the <br>wig</br> method is particularly suitable for this situation.",
                "Considering the adaptability of <br>wig</br> to a range of collections and query types, one of our future plans is to apply this method to predict user preference of search results on realistic data collected from a commercial search engine.",
                "Other than accuracy, another major issue that prediction techniques have to deal with in a Web environment is efficiency.",
                "Fortunately, since the <br>wig</br> score is computed just over the terms and the phrases that appear in the query, this calculation can be made very efficient with the support of index.",
                "On the other hand, the computation of QF and FRC is relatively less efficient since QF needs to retrieve the whole collection twice and FRC needs to repeatedly rank the perturbed documents.",
                "How to improve the efficiency of QF and FRC is our future work.",
                "In addition, the prediction techniques proposed in this paper have the potential of improving retrieval performance by combining with other IR techniques.",
                "For example, our techniques can be incorporated to popular query modification techniques such as query expansion and query relaxation.",
                "Guided by performance prediction, we can make a better decision on when to or how to modify queries to enhance retrieval effectiveness.",
                "We would like to carry out research in this direction in the future. 6.",
                "ACKNOWLEGMENTS This work was supported in part by the Center for Intelligent Information Retrieval, in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C0023, and in part by an award from Google.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor.",
                "In addition, we thank Donald Metzler for his valuable comments on this work. 7.",
                "REFERENCES [1] Y. Zhou ,W. B. Croft ,Ranking Robustness: A Novel Framework to Predict Query Performance, in Proceedings of CIKM 2006. [2] D.Carmel, E.Yom-Tov, A.Darlow,D.Pelleg, What Makes a Query Difficult?, in Proceedings of SIGIR 2006. [3] C.L.A.",
                "Clarke, F. Scholer, I.Soboroff, The TREC 2005 Terabyte Track, In the Online Proceedings of 2005 TREC. [4] B.",
                "He and I.Ounis.",
                "Inferring query performance using preretrieval predictors.",
                "In proceedings of the SPIRE 2004. [5] S. Tomlinson.",
                "Robust, Web and Terabyte Retrieval with Hummingbird SearchServer at TREC 2004.",
                "In the Online Proceedings of 2004 TREC. [6] S. Cronen-Townsend, Y. Zhou, W. B. Croft, Predicting Query Performance, in Proceedings of SIGIR 2002. [7] V.Vinay, I.J.Cox, N.Mill-Frayling,K.Wood, On Ranking the Effectiveness of Searcher, in Proceedings of SIGIR 2006. [8] D.Metzler, W.B.Croft, A Markov Random Filed Model for Term Dependencies, in Proceedings of SIGIR 2005. [9] D.Metzler, T.Strohman,Y.Zhou,W.B.Croft, Indri at TREC 2005: Terabyte Track, In the Online Proceedings of 2004 TREC. [10] P. Ogilvie and J. Callan, Combining document representations for known-item search, in Proceedings of SIGIR 2003. [11] A.Berger, J.Lafferty, Information retrieval as statistical translation, in Proceedings of SIGIR 1999. [12] Indri search engine : http://www.lemurproject.org/indri/ [13] I.J.",
                "Taneja: On Generalized Information Measures and Their Applications, Advances in Electronics and Electron Physics, Academic Press (USA), 76, 1989, 327-413. [14] S.Cronen-Townsend, Y. Zhou and Croft, W. B. , A Framework for Selective Query Expansion, in Proceedings of CIKM 2004. [15] F.Song, W.B.Croft, A general language model for information retrieval, in Proceedings of SIGIR 1999. [16] Personal email contact with Vishwa Vinay and our own experiments [17] E.Yom-Tov, S.Fine, D.Carmel, A.Darlow, Learning to Estimate Query Difficulty Including Applications to Missing Content Detection and Distributed Information retrieval, in Proceedings of SIGIR 2005"
            ],
            "original_annotated_samples": [
                "Our first technique, called weighted information gain (<br>wig</br>), makes use of both single term and term proximity features to estimate the quality of top retrieved documents for prediction.",
                "We find that <br>wig</br> offers consistent prediction accuracy across various test collections and query types.",
                "Moreover, we demonstrate that good prediction accuracy can be achieved for the mixed-query situation by using <br>wig</br> with the help of a query type classifier.",
                "One of our prediction models, <br>wig</br>, is related to the Markov random field (MRF) model for information retrieval [8].",
                "This model is used to estimate the joint probability distribution over documents and queries, an important part of <br>wig</br>."
            ],
            "translated_annotated_samples": [
                "Nuestra primera técnica, llamada <br>ganancia de información ponderada</br> (WIG), utiliza tanto características de términos individuales como de proximidad de términos para estimar la calidad de los documentos recuperados para la predicción.",
                "Observamos que WIG ofrece una precisión de predicción consistente en diversas colecciones de pruebas y tipos de consultas.",
                "Además, demostramos que se puede lograr una buena precisión de predicción para la situación de consulta mixta utilizando <br>WIG</br> con la ayuda de un clasificador de tipo de consulta.",
                "Uno de nuestros modelos de predicción, WIG, está relacionado con el modelo de campo aleatorio de Markov (MRF) para la recuperación de información [8].",
                "Este modelo se utiliza para estimar la distribución conjunta de probabilidad sobre documentos y consultas, una parte importante de <br>WIG</br>."
            ],
            "translated_text": "Predicción del rendimiento de consultas en entornos de búsqueda web Yun Zhou y W. Bruce Croft Departamento de Ciencias de la Computación Universidad de Massachusetts, Amherst {yzhou, croft}@cs.umass.edu RESUMEN Las técnicas actuales de predicción, generalmente diseñadas para consultas basadas en contenido y evaluadas típicamente en colecciones de prueba relativamente homogéneas de tamaños pequeños, enfrentan serios desafíos en entornos de búsqueda web donde las colecciones son significativamente más heterogéneas y existen diferentes tipos de tareas de recuperación. En este artículo, presentamos tres técnicas para abordar estos desafíos. Nos enfocamos en la predicción de rendimiento para dos tipos de consultas en entornos de búsqueda web: basadas en contenido y búsqueda de páginas por nombre. Nuestra evaluación se realiza principalmente en la colección GOV2. Además de evaluar nuestros modelos para los dos tipos de consultas por separado, consideramos una situación más desafiante y realista en la que los dos tipos de consultas se mezclan sin información previa sobre los tipos de consulta. Para ayudar en la predicción bajo la situación de consulta mixta, se adopta un clasificador de consultas novedoso. Los resultados muestran que nuestra predicción del rendimiento de la consulta web es sustancialmente más precisa que las técnicas de predicción actuales más avanzadas. Por consiguiente, nuestro artículo proporciona un enfoque práctico para predecir el rendimiento en entornos web del mundo real. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Formulación de Consultas Términos Generales Algoritmos, Experimentación, Teoría 1. La predicción del rendimiento de la consulta tiene muchas aplicaciones en una variedad de áreas de recuperación de información (IR), como mejorar la consistencia de la recuperación, el refinamiento de la consulta y la IR distribuida. La importancia de este problema ha sido reconocida por los investigadores en Recuperación de Información y recientemente se han propuesto varios métodos nuevos para la predicción [1, 2, 17]. La mayoría de los trabajos sobre predicción se han centrado en la tarea tradicional de recuperación ad-hoc, donde el rendimiento de la consulta se mide según la relevancia temática. Estos modelos de predicción se evalúan en colecciones de documentos de TREC que suelen consistir en no más de un millón de artículos de agencias de noticias relativamente homogéneos. Con la popularidad e influencia de la Web, las técnicas de predicción que funcionen bien para consultas de estilo web son altamente preferibles. Sin embargo, los entornos de búsqueda web plantean desafíos significativos para los modelos de predicción actuales que están principalmente diseñados para entornos tradicionales de TREC. Aquí delineamos algunos de estos desafíos. Primero, las colecciones web, que son mucho más grandes que las colecciones TREC convencionales, incluyen una variedad de documentos que son diferentes en muchos aspectos, como la calidad y el estilo. Las técnicas actuales de predicción pueden ser vulnerables a estas características de las colecciones web. Por ejemplo, la precisión de predicción reportada de la técnica de robustez de clasificación y la técnica de claridad en la colección GOV2 (una colección web grande) es significativamente peor en comparación con las otras colecciones de TREC [1]. En [2] se informa de una precisión de predicción similar en la colección GOV2 utilizando otra técnica, confirmando la dificultad de predecir el rendimiento de consultas en una gran colección web. Además, la búsqueda web va más allá del alcance de la tarea de recuperación ad hoc basada en la relevancia temática. Por ejemplo, la tarea de encontrar páginas nombradas (NP), que es una tarea de navegación, también es popular en la recuperación web. La predicción del rendimiento de la consulta para la tarea de NP sigue siendo necesaria, ya que el rendimiento de recuperación de NP está lejos de ser perfecto. De hecho, según el informe sobre la tarea NP del Terabyte Track de 2005 [3], aproximadamente el 40% de las consultas de prueba tienen un rendimiento deficiente (sin respuesta correcta en los primeros 10 resultados de búsqueda) incluso en la mejor ejecución del grupo líder. Hasta donde sabemos, poca investigación ha abordado explícitamente el problema de la predicción del rendimiento de consultas de NP. Los modelos de predicción actuales diseñados para consultas basadas en contenido serán menos efectivos para consultas de NP considerando las diferencias fundamentales entre ambas. En tercer lugar, en entornos de búsqueda web del mundo real, las consultas de los usuarios suelen ser una mezcla de diferentes tipos y, por lo general, no se dispone de información previa sobre el tipo de cada consulta. La situación de consulta mixta plantea nuevos problemas para la predicción del rendimiento de la consulta. Por ejemplo, es posible que necesitemos incorporar un clasificador de consultas en los modelos de predicción. A pesar de estos problemas, la capacidad para manejar esta situación es un paso crucial hacia convertir la predicción del rendimiento de consultas de un tema de investigación interesante en una herramienta práctica para la recuperación web. En este artículo, presentamos tres técnicas para abordar los desafíos mencionados que enfrentan los modelos de predicción actuales en entornos de búsqueda web. Nuestro trabajo se centra en la predicción del rendimiento de consultas para la tarea de recuperación basada en contenido (ad-hoc) y la tarea de encontrar páginas por nombre en el contexto de la recuperación web. Nuestra primera técnica, llamada <br>ganancia de información ponderada</br> (WIG), utiliza tanto características de términos individuales como de proximidad de términos para estimar la calidad de los documentos recuperados para la predicción. Observamos que WIG ofrece una precisión de predicción consistente en diversas colecciones de pruebas y tipos de consultas. Además, demostramos que se puede lograr una buena precisión de predicción para la situación de consulta mixta utilizando <br>WIG</br> con la ayuda de un clasificador de tipo de consulta. La retroalimentación de consultas y el cambio de rango inicial, que son nuestras técnicas de predicción segunda y tercera, funcionan bien para consultas basadas en contenido y consultas de NP respectivamente. Nuestras principales contribuciones incluyen: (1) una precisión de predicción considerablemente mejorada para consultas basadas en contenido web en comparación con varias técnicas de vanguardia. (2) nuevas técnicas para predecir con éxito el rendimiento de consultas NP. (3) una solución práctica y totalmente automática para predecir el rendimiento de consultas mixtas. Además, una contribución menor es que encontramos que el puntaje de robustez [1], que originalmente se propuso para la predicción de rendimiento, es útil para la clasificación de consultas. 2. TRABAJO RELACIONADO Como mencionamos en la introducción, recientemente se han propuesto varias técnicas de predicción que se centran en consultas basadas en contenido en la tarea de relevancia temática (ad-hoc). No conocemos ningún trabajo publicado que aborde otros tipos de consultas como las consultas NP, y mucho menos una mezcla de tipos de consultas. A continuación revisamos algunos modelos representativos. La principal dificultad de la predicción del rendimiento proviene del hecho de que muchos factores tienen un impacto en el rendimiento de recuperación. Cada factor afecta el rendimiento en diferente medida y el efecto general es difícil de predecir con precisión. Por lo tanto, no es sorprendente notar que características simples, como la frecuencia de los términos de consulta en la colección [4] y la IDF promedio de los términos de consulta [5], no predicen bien. De hecho, la mayoría de las técnicas exitosas se basan en medir algunas características del conjunto de documentos recuperados para estimar la dificultad del tema. Por ejemplo, la puntuación de claridad [6] mide la coherencia de una lista de documentos mediante la divergencia KL entre el modelo de consulta y el modelo de colección. El puntaje de robustez [1] cuantifica otra propiedad de una lista clasificada: la robustez de la clasificación en presencia de incertidumbre. Carmel et al. [2] encontraron que la distancia medida por la divergencia de Jensen-Shannon entre el conjunto de documentos recuperados y la colección está significativamente correlacionada con la precisión promedio. Vinay et al. [7] propusieron cuatro medidas para capturar la geometría de los documentos mejor clasificados para la predicción. La medida más efectiva es la sensibilidad a la perturbación del documento, una idea algo similar a la puntuación de robustez. Desafortunadamente, su forma de medir la sensibilidad no funciona igual de bien para consultas cortas y la precisión de la predicción disminuye considerablemente cuando se adopta una técnica de recuperación de vanguardia (como Okapi o un enfoque de modelado de lenguaje) en lugar del peso tf-idf utilizado en su artículo [16]. Las dificultades de aplicar estos modelos en entornos de búsqueda web ya han sido mencionadas. En este documento, principalmente adoptamos la puntuación de claridad y la puntuación de robustez como nuestras líneas base. Experimentalmente demostramos que las líneas base, incluso después de ser ajustadas cuidadosamente, son inadecuadas para el entorno web. Uno de nuestros modelos de predicción, WIG, está relacionado con el modelo de campo aleatorio de Markov (MRF) para la recuperación de información [8]. El modelo MRF modela directamente la dependencia de términos y se ha demostrado ser altamente efectivo en una variedad de colecciones de pruebas (especialmente colecciones web) y tareas de recuperación. Este modelo se utiliza para estimar la distribución conjunta de probabilidad sobre documentos y consultas, una parte importante de <br>WIG</br>. ",
            "candidates": [],
            "error": [
                [
                    "ganancia de información ponderada",
                    "WIG",
                    "WIG"
                ]
            ]
        },
        "robustness score probabilitydensity classifier": {
            "translated_key": "clasificador de Probabilidad de Densidad de Puntuación Robusta",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Query Performance Prediction in Web Search Environments Yun Zhou and W. Bruce Croft Department of Computer Science University of Massachusetts, Amherst {yzhou, croft}@cs.umass.edu ABSTRACT Current prediction techniques, which are generally designed for content-based queries and are typically evaluated on relatively homogenous test collections of small sizes, face serious challenges in web search environments where collections are significantly more heterogeneous and different types of retrieval tasks exist.",
                "In this paper, we present three techniques to address these challenges.",
                "We focus on performance prediction for two types of queries in web search environments: content-based and Named-Page finding.",
                "Our evaluation is mainly performed on the GOV2 collection.",
                "In addition to evaluating our models for the two types of queries separately, we consider a more challenging and realistic situation that the two types of queries are mixed together without prior information on query types.",
                "To assist prediction under the mixed-query situation, a novel query classifier is adopted.",
                "Results show that our prediction of web query performance is substantially more accurate than the current stateof-the-art prediction techniques.",
                "Consequently, our paper provides a practical approach to performance prediction in realworld web settings.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval -Query formulation General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Query performance prediction has many applications in a variety of information retrieval (IR) areas such as improving retrieval consistency, query refinement, and distributed IR.",
                "The importance of this problem has been recognized by IR researchers and a number of new methods have been proposed for prediction recently [1, 2, 17].",
                "Most work on prediction has focused on the traditional ad-hoc retrieval task where query performance is measured according to topical relevance.",
                "These prediction models are evaluated on TREC document collections which typically consist of no more than one million relatively homogenous newswire articles.",
                "With the popularity and influence of the Web, prediction techniques that will work well for web-style queries are highly preferable.",
                "However, web search environments pose significant challenges to current prediction models that are mainly designed for traditional TREC settings.",
                "Here we outline some of these challenges.",
                "First, web collections, which are much larger than conventional TREC collections, include a variety of documents that are different in many aspects such as quality and style.",
                "Current prediction techniques can be vulnerable to these characteristics of web collections.",
                "For example, the reported prediction accuracy of the ranking robustness technique and the clarity technique on the GOV2 collection (a large web collection) is significantly worse compared to the other TREC collections [1].",
                "Similar prediction accuracy on the GOV2 collection using another technique is reported in [2], confirming the difficult of predicting query performance on a large web collection.",
                "Furthermore, web search goes beyond the scope of the ad-hoc retrieval task based on topical relevance.",
                "For example, the Named-Page (NP) finding task, which is a navigational task, is also popular in web retrieval.",
                "Query performance prediction for the NP task is still necessary since NP retrieval performance is far from perfect.",
                "In fact, according to the report on the NP task of the 2005 Terabyte Track [3], about 40% of the test queries perform poorly (no correct answer in the first 10 search results) even in the best run from the top group.",
                "To our knowledge, little research has explicitly addressed the problem of NP-query performance prediction.",
                "Current prediction models devised for content-based queries will be less effective for NP queries considering the fundamental differences between the two.",
                "Third, in real-world web search environments, user queries are usually a mixture of different types and prior knowledge about the type of each query is generally unavailable.",
                "The mixed-query situation raises new problems for query performance prediction.",
                "For instance, we may need to incorporate a query classifier into prediction models.",
                "Despite these problems, the ability to handle this situation is a crucial step towards turning query performance prediction from an interesting research topic into a practical tool for web retrieval.",
                "In this paper, we present three techniques to address the above challenges that current prediction models face in Web search environments.",
                "Our work focuses on query performance prediction for the content-based (ad-hoc) retrieval task and the name-page finding task in the context of web retrieval.",
                "Our first technique, called weighted information gain (WIG), makes use of both single term and term proximity features to estimate the quality of top retrieved documents for prediction.",
                "We find that WIG offers consistent prediction accuracy across various test collections and query types.",
                "Moreover, we demonstrate that good prediction accuracy can be achieved for the mixed-query situation by using WIG with the help of a query type classifier.",
                "Query feedback and first rank change, which are our second and third prediction techniques, perform well for content-based queries and NP queries respectively.",
                "Our main contributions include: (1) considerably improved prediction accuracy for web content-based queries over several state-of-the-art techniques. (2) new techniques for successfully predicting NP-query performance. (3) a practical and fully automatic solution to predicting mixed-query performance.",
                "In addition, one minor contribution is that we find that the robustness score [1], which was originally proposed for performance prediction, is helpful for query classification. 2.",
                "RELATED WORK As we mentioned in the introduction, a number of prediction techniques have been proposed recently that focus on contentbased queries in the topical relevance (ad-hoc) task.",
                "We know of no published work that addresses other types of queries such as NP queries, let alone a mixture of query types.",
                "Next we review some representative models.",
                "The major difficulty of performance prediction comes from the fact that many factors have an impact on retrieval performance.",
                "Each factor affects performance to a different degree and the overall effect is hard to predict accurately.",
                "Therefore, it is not surprising to notice that simple features, such as the frequency of query terms in the collection [4] and the average IDF of query terms [5], do not predict well.",
                "In fact, most of the successful techniques are based on measuring some characteristics of the retrieved document set to estimate topic difficulty.",
                "For example, the clarity score [6] measures the coherence of a list of documents by the KL-divergence between the query model and the collection model.",
                "The robustness score [1] quantifies another property of a ranked list: the robustness of the ranking in the presence of uncertainty.",
                "Carmel et al. [2] found that the distance measured by the Jensen-Shannon divergence between the retrieved document set and the collection is significantly correlated to average precision.",
                "Vinay et al. [7] proposed four measures to capture the geometry of the top retrieved documents for prediction.",
                "The most effective measure is the sensitivity to document perturbation, an idea somewhat similar to the robustness score.",
                "Unfortunately, their way of measuring the sensitivity does not perform equally well for short queries and prediction accuracy drops considerably when a state-of-the-art retrieval technique (like Okapi or a language modeling approach) is adopted for retrieval instead of the tf-idf weighting used in their paper [16].",
                "The difficulties of applying these models in web search environments have already been mentioned.",
                "In this paper, we mainly adopt the clarity score and the robustness score as our baselines.",
                "We experimentally show that the baselines, even after being carefully tuned, are inadequate for the web environment.",
                "One of our prediction models, WIG, is related to the Markov random field (MRF) model for information retrieval [8].",
                "The MRF model directly models term dependence and is found be to highly effective across a variety of test collections (particularly web collections) and retrieval tasks.",
                "This model is used to estimate the joint probability distribution over documents and queries, an important part of WIG.",
                "The superiority of WIG over other prediction techniques based on unigram features, which will be demonstrated later in our paper, coincides with that of MRF for retrieval.",
                "In other word, it is interesting to note that term dependence, when being modeled appropriately, can be helpful for both improving and predicting retrieval performance. 3.",
                "PREDICTION MODELS 3.1 Weighted Information Gain (WIG) This section introduces a weighted information gain approach that incorporates both single term and proximity features for predicting performance for both content-based and Named-Page (NP) finding queries.",
                "Given a set of queries Q={Qs} (s=1,2,..N) which includes all possible user queries and a set of documents D={Dt} (t=1,2…M), we assume that each query-document pair (Qs,Dt) is manually judged and will be put in a relevance list if Qs is found to be relevant to Dt.",
                "The joint probability P(Qs,Dt) over queries Q and documents D denotes the probability that pair (Qs,Dt) will be in the relevance list.",
                "Such assumptions are similar to those used in [8].",
                "Assuming that the user issues query Qi ∈Q and the retrieval results in response to Qi is a ranked list L of documents, we calculate the amount of information contained in P(Qs,Dt) with respect to Qi and L by Eq.1 which is a variant of entropy called the weighted entropy[13].",
                "The weights in Eq.1 are solely determined by Qi and L. )1(),(log),(),( , , ∑−= ts tststsLQ DQPDQweightDQH i In this paper, we choose the weights as follows: LindocumentsKtopthecontainsLTwhere otherwise LTDandisifK DQweight K Kt ts )( )2( ,0 )(,/1 ),( ⎩ ⎨ ⎧ ∈= = The cutoff rank K is a parameter in our model that will be discussed later.",
                "Accordingly, Eq.1 can be simplified as follows: )3(),(log 1 ),( )( , ∑∈ −= LTD titsLQ Kt i DQP K DQH Unfortunately, weighted entropy ),(, tsLQ DQH i computed by Eq.3, which represents the amount of information about how likely the top ranked documents in L would be relevant to query Qi on average, cannot be compared across different queries, making it inappropriate for directly predicting query performance.",
                "To mitigate this problem, we come up with a background distribution P(Qs,C) over Q and D by imagining that every document in D is replaced by the same special document C which represents average language usage.",
                "In this paper, C is created by concatenating every document in D. Roughly speaking, C is the collection (the document set) {Dt} without document boundaries.",
                "Similarly, weighted entropy ),(, CQH sLQi calculated by Eq.3 represents the amount of information about how likely an average document (represented by the whole collection) would be relevant to query Qi.",
                "Now we introduce our performance predictor WIG which is the weighted information gain [13] computed as the difference between ),(, tsLQ DQH i and ),(, CQH sLQi .Specifically, given query Qi, collection C and ranked list L of documents, WIG is calculated as follows: )4( ),( ),( log 1 ),( ),( log),( ),(),(),,( )(, ,, ∑∑ ∈ == −= LTD i ti ts s ts ts tsLQsLQi Kt ii CQP DQP KCQP DQP DQweight DQHCQHLCQWIG WIG computed by Eq.4 measures the change in information about the quality of retrieval (in response to query Qi) from an imaginary state that only an average document is retrieved to a posterior state that the actual search results are observed.",
                "We hypothesize that WIG is positively correlated with retrieval effectiveness because high quality retrieval should be much more effective than just returning the average document.",
                "The heart of this technique is how to estimate the joint distribution P(Qs,Dt).",
                "In the language modeling approach to IR, a variety of models can be applied readily to estimate this distribution.",
                "Although most of these models are based on the bagof-words assumption, recent work on modeling term dependence under the language modeling framework have shown consistent and significant improvements in retrieval effectiveness over bagof-words models.",
                "Inspired by the success of incorporating term proximity features into language models, we decide to adopt a good dependence model to estimate the probability P(Qs,Dt).",
                "The model we chose for this paper is Metzler and Crofts Markov Random Field (MRF) model, which has already demonstrated superiority over a number of collections and different retrieval tasks [8,9].",
                "According to the MRF model, log P(Qi, Dt) can be written as )5()|(loglog),(log )( 1 ∑∈ +−= iQF tti DPZDQP ξ ξ ξλ where Z1 is a constant that ensures that P(Qi, Dt) sums up to 1.",
                "F(Qi) consists of a set of features expanded from the original query Qi .",
                "For example, assuming that query Qi is talented student program, F(Qi) includes features like program and talented student.",
                "We consider two kinds of features: single term features T and proximity features P. Proximity features include exact phrase (#1) and unordered window (#uwN) features as described in [8].",
                "Note that F(Qi) is the union of T(Qi) and P(Qi).",
                "For more details on F(Qi) such as how to expand the original query Qi to F(Qi), we refer the reader to [8] and [9].",
                "P(ξ|Dt) denotes the probability that feature ξ will occur in Dt.",
                "More details on P(ξ|Dt) will be provided later in this section.",
                "The choice of λξ is somewhat different from that used in [8] since λξ plays a dual role in our model.",
                "The first role, which is the same as in [8], is to weight between single term and proximity features.",
                "The other role, which is specific to our prediction task, is to normalize the size of F(Qi).We found that the following weight strategy for λξ satisfies the above two roles and generalizes well on a variety of collections and query types. )6( )(, |)(| 1 )(, |)(| ⎪ ⎪ ⎩ ⎪ ⎪ ⎨ ⎧ ∈ − ∈ = i i T i i T QP QP QT QT ξ λ ξ λ λξ where |T(Qi)| and |P(Qi)| denote the number of single term and proximity features in F(Qi) respectively.",
                "The reason for choosing the square root function in the denominator of λξ is to penalize a feature set of large size appropriately, making WIG more comparable across queries of various lengths. λT is a fixed parameter and set to 0.8 according to [8] throughout this paper.",
                "Similarly, log P(Qi,C) can be written as: )7()|(loglog),(log )( 2 ∑∈ +−= iQF i CPZCQP ξ ξ ξλ When constant Z1 and Z2 are dropped, WIG computed in Eq.4 can be rewritten as follows by plugging in Eq.5 and Eq.7 : )8( )|( )|( log 1 ),,( )( )( ∑ ∑∈ ∈ = LTD QF t i Kt i CP DP K LCQWIG ξ ξ ξ ξ λ One of the advantages of WIG over other techniques is that it can handle well both content-based and NP queries.",
                "Based on the type (or the predicted type) of Qi, the calculation of WIG in Eq. 8 differs in two aspects: (1) how to estimate P(ξ|Dt) and P(ξ|C), and (2) how to choose K. For content-based queries, P(ξ|C) is estimated by the relative frequency of feature ξ in collection C as a whole.",
                "The estimation of P(ξ|Dt) is the same as in [8].",
                "Namely, we estimate P(ξ|Dt) by the relative frequency of feature ξ in Dt linearly smoothed with collection frequency P(ξ|C).",
                "K in Eq.8 is treated as a free parameter.",
                "Note that K is the only free parameter in the computation of WIG for content-based queries because all parameters involved in P(ξ|Dt) are assumed to be fixed by taking the suggested values in [8].",
                "Regarding NP queries, we make use of document structure to estimate P(ξ|Dt) and P(ξ|C) by the so-called mixture of language models proposed in [10] and incorporated into the MRF model for Named-Page finding retrieval in [9].",
                "The basic idea is that a document (collection) is divided into several fields such as the title field, the main-body field and the heading field.",
                "P(ξ|Dt) and P(ξ|C) are estimated by a linear combination of the language models from each field.",
                "Due to space constraints, we refer the reader to [9] for details.",
                "We adopt the exact same set of parameters as used in [9] for estimation.",
                "With regard to K in Eq.8, we set K to 1 because the Named-Page finding task heavily focuses on the first ranked document.",
                "Consequently, there are no free parameters in the computation of WIG for NP queries. 3.2 Query Feedback In this section, we introduce another technique called query feedback (QF) for prediction.",
                "Suppose that a user issues query Q to a retrieval system and a ranked list L of documents is returned.",
                "We view the retrieval system as a noisy channel.",
                "Specifically, we assume that the output of the channel is L and the input is Q.",
                "After going through the channel, Q becomes corrupted and is transformed to ranked list L. By thinking about the retrieval process this way, the problem of predicting retrieval effectiveness turns to the task of evaluating the quality of the channel.",
                "In other words, prediction becomes finding a way to measure the degree of corruption that arises when Q is transformed to L. As directly computing the degree of the corruption is difficult, we tackle this problem by approximation.",
                "Our main idea is that we measure to what extent information on Q can be recovered from L on the assumption that only L is observed.",
                "Specifically, we design a decoder that can accurately translate L back into new query Q and the similarity S between the original query Q and the new query Q is adopted as a performance predictor.",
                "This is a sketch of how the QF technique predicts query performance.",
                "Before filling in more details, we briefly discuss why this method would work.",
                "There is a relation between the similarity S defined above and retrieval performance.",
                "On the one hand, if the retrieval has strayed from the original sense of the query Q, the new query Q extracted from ranked list L in response to Q would be very different from the original query Q.",
                "On the other hand, a query distilled from a ranked list containing many relevant documents is likely to be similar to the original query.",
                "Further examples in support of the relation will be provided later.",
                "Next we detail how to build the decoder and how to measure the similarity S. In essence, the goal of the decoder is to compress ranked list L into a few informative terms that should represent the content of the top ranked documents in L. Our approach to this goal is to represent ranked list L by a language model (distribution over terms).",
                "Then terms are ranked by their contribution to the language models KL (Kullback-Leibler) divergence from the background collection model.",
                "Top ranked terms will be chosen to form the new query Q.",
                "This approach is similar to that used in Section 4.1 of [11].",
                "Specifically, we take three steps to compress ranked list L into query Q without referring to the original query. 1.",
                "We adopt the ranked list language model [14], to estimate a language model based on ranked list L. The model can be written as: )9()|()|()|( ∑∈ = LD LDPDwPLwP where w is any term, D is a document.",
                "P(D|L) is estimated by a linearly decreasing function of the rank of document D. 2.",
                "Each term in P(w|L) is ranked by the following KL-divergence contribution: )10( )|( )|( log)|( CwP LwP LwP where P(w|C) is the collection model estimated by the relative frequency of term w in collection C as a whole. 3.",
                "The top N ranked terms by Eq.10 form a weighted query Q={(wi,ti)} i=1,N. where wi denotes the i-th ranked term and weight ti is the KL-divergence contribution of wi in Eq. 10.",
                "Term cruise ship vessel sea passenger KL contribution 0.050 0.040 0.012 0.010 0.009 Table 1: top 5 terms compressed from the ranked list in response to query Cruise ship damage sea life Two representative examples, one for a poorly performing query Cruise ship damage sea life (TREC topic 719; average precision: 0.08) and the other for a high performing query prostate cancer treatments( TREC topic 710; average precision: 0.49), are shown in Table 1 and 2 respectively.",
                "These examples indicate how the similarity between the original and the new query correlates with retrieval performance.",
                "The parameter N in step 3 is set to 20 empirically and choosing a larger value of N is unnecessary since the weights after the top 20 are usually too small to make any difference.",
                "Term prostate cancer treatment men therapy KL contribution 0.177 0.140 0.028 0.025 0.020 Table 2: top 5 terms compressed from the ranked list in response to query prostate cancer treatments To measure the similarity between original query Q and new query Q, we first use Q to do retrieval on the same collection.",
                "A variant of the query likelihood model [15] is adopted for retrieval.",
                "Namely, documents are ranked by: )11()|()|( ),( ∑∈ = Qtw t i ii i DwPDQP where wi is a term in Q and ti is the associated weight.",
                "D is a document.",
                "Let L denote the new ranked list returned from the above retrieval.",
                "The similarity is measured by the overlap of documents in L and L. Specifically, the percentage of documents in the top K documents of L that are also present in the top K documents in L. the cutoff K is treated as a free parameter.",
                "We summarize here how the QF technique predicts performance given a query Q and the associated ranked list L. We first obtain a weighted query Q compressed from L by the above three steps.",
                "Then we use Q to perform retrieval and the new ranked list is L. The overlap of documents in L and L is used for prediction. 3.3 First Rank Change (FRC) In this section, we propose a method called the first rank change (FRC) for performance prediction for NP queries.",
                "This method is derived from the ranking robustness technique [1] that is mainly designed for content-based queries.",
                "When directly applied to NP queries, the robustness technique will be less effective because it takes the top ranked documents as a whole into account while NP queries usually have only one single relevant document.",
                "Instead, our technique focuses on the first rank document while the main idea of the robustness method remains.",
                "Specifically, the pseudocode for computing FRC is shown in figure 1.",
                "Input: (1) ranked list L={Di} where i=1,100.",
                "Di denotes the i-th ranked document. (2) query Q 1 initialize: (1) set the number of trials J=100000 (2) counter c=0; 2 for i=1 to J 3 Perturb every document in L, let the outcome be a set F={Di} where Di denotes the perturbed version of Di. 4 Do retrieval with query Q on set F 5 c=c+1 if and only if D1 is ranked first in step 4 6 end of for 7 return the ratio c/J Figure 1: pseudo-code for computing FRC FRC approximates the probability that the first ranked document in the original list L will remain ranked first even after the documents are perturbed.",
                "The higher the probability is, the more confidence we have in the first ranked document.",
                "On the other hand, in the extreme case of a random ranking, the probability would be as low as 0.5.",
                "We expect that FRC has a positive association with NP query performance.",
                "We adopt [1] to implement the document perturbation step (step 4 in Fig.1) using Poisson distributions.",
                "For more details, we refer the reader to [1]. 4.",
                "EVALUATION We now present the results of predicting query performance by our models.",
                "Three state-of-the-art techniques are adopted as our baselines.",
                "We evaluate our techniques across a variety of Web retrieval settings.",
                "As mentioned before, we consider two types of queries, that is, content-based (CB) queries and Named-Page(NP) finding queries.",
                "First, suppose that the query types are known.",
                "We investigate the correlation between the predicted retrieval performance and the actual performance for both types of queries separately.",
                "Results show that our methods yield considerable improvements over the baselines.",
                "We then consider a more challenging scenario where no prior information on query types is available.",
                "Two sub-cases are considered.",
                "In the first one, there exists only one type of query but the actual type is unknown.",
                "We assume a mixture of the two query types in the second case.",
                "We demonstrate that our models achieve good accuracy under this demanding scenario, making prediction practical in a real-world Web search environment. 4.1 Experimental Setup Our evaluation focuses on the GOV2 collection which contains about 25 million documents crawled from web sites in the .gov domain during 2004 [3].",
                "We create two kinds of data set for CB queries and NP queries respectively.",
                "For the CB type, we use the ad-hoc topics of the Terabyte Tracks of 2004, 2005 and 2006 and name them TB04-adhoc, TB05-adhoc and TB06-adhoc respectively.",
                "In addition, we also use the ad-hoc topics of the 2004 Robust Track (RT04) to test the adaptability of our techniques to a non-Web environment.",
                "For NP queries, we use the Named-Page finding topics of the Terabyte Tracks of 2005 and 2006 and we name them TB05-NP and TB06-NP respectively.",
                "All queries used in our experiments are titles of TREC topics as we center on web retrieval.",
                "Table 3 summarizes the above data sets.",
                "Name Collection Topic Number Query Type TB04-adhoc GOV2 701-750 CB TB05-adhoc GOV2 751-800 CB TB06-adhoc GOV2 801-850 CB RT04 Disk 4+5 (minus CR) 301-450;601700 CB TB05-NP GOV2 NP601-NP872 NP TB06-NP GOV2 NP901-NP1081 NP Table 3: Summary of test collections and topics Retrieval performance of individual content-based and NP queries is measured by the average precision and reciprocal rank of the first correct answer respectively.",
                "We make use of the Markov Random field model for both ad-hoc and Named-Page finding retrieval.",
                "We adopt the same setting of retrieval parameters used in [8,9].",
                "The Indri search engine [12] is used for all of our experiments.",
                "Though not reported here, we also tried the query likelihood model for ad-hoc retrieval and found that the results change little because of the very high correlation between the query performances obtained by the two retrieval models (0.96 measured by Pearsons coefficient). 4.2 Known Query Types Suppose that query types are known.",
                "We treat each type of query separately and measure the correlation with average precision (or the reciprocal rank in the case of NP queries).",
                "We adopt the Pearsons correlation test which reflects the degree of linear relationship between the predicted and the actual retrieval performance. 4.2.1 Content-based Queries Methods Clarity Robust JSD WIG QF WIG +QF TB04+0 5 adhoc 0.333 0.317 0.362 0.574 0.480 0.637 TB06 adhoc 0.076 0.294 N/A 0.464 0.422 0.511 Table 4: Pearsons correlation coefficients for correlation with average precision on the Terabyte Tracks (ad-hoc) for clarity score, robustness score, the JSD-based method(we directly cites the score reported in [2]), WIG, query feedback(QF) and a linear combination of WIG and QF.",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Table 4 shows the correlation with average precision on two data sets: one is a combination of TB04-adhoc and TB05-adhoc(100 topics in total) and the other is TB06-adhoc (50 topics).",
                "The reason that we put TB04-adhoc and TB05-adhoc together is to make our results comparable to [2].",
                "Our baselines are the clarity score (clarity) [6],the robustness score (robust)[1] and the JSDbased method (JSD) [2].",
                "For the clarity and robustness score, we have tried different parameter settings and report the highest correlation coefficients we have found.",
                "We directly cite the result of the JSD-based method reported in [2].",
                "The table also shows the results for the Weighted Information Gain (WIG) method and the Query Feedback (QF) method for predicting content-based queries.",
                "As we described in the previous section, both WIG and QF have one free parameter to set, that is, the cutoff rank K. We train the parameter on one dataset and test on the other.",
                "When combining WIG and QF, a simple linear combination is used and the combination weight is learned from the training data set.",
                "From these results, we can see that our methods are considerably more accurate compared to the baselines.",
                "We also observe that further improvements are obtained from the combination of WIG and QF, suggesting that they measure different properties of the retrieval process that relate to performance.",
                "We discover that our methods generalize well on TB06-adhoc while the correlation for the clarity score with retrieval performance on this data set is considerably worse.",
                "Further investigation shows that the mean average precision of TB06-adhoc is 0.342 and is about 10% better than that of the first data set.",
                "While the other three methods typically consider the top 100 or less documents given a ranked list, the clarity method usually needs the top 500 or more documents to adequately measure the coherence of a ranked list.",
                "Higher mean average precision makes ranked lists retrieved by different queries more similar in terms of coherence at the level of top 500 documents.",
                "We believe that this is the main reason for the low accuracy of the clarity score on the second data set.",
                "Though this paper focuses on a Web search environment, it is desirable that our techniques will work consistently well in other situations.",
                "To this end, we examine the effectiveness of our techniques on the Robust 2004 Track.",
                "For our methods, we evenly divide all of the test queries into five groups and perform five-fold cross validation.",
                "Each time we use one group for training and the remaining four groups for testing.",
                "We make use of all of the queries for our two baselines, that is, the clarity score and the robustness score.",
                "The parameters for our baselines are the same as those used in [1].The results shown in Table 5 demonstrate that the prediction accuracy of our methods is on a par with that of the two strong baselines.",
                "Clarity Robust WIG QF 0.464 0.539 0.468 0.464 Table 5: Comparison of Pearsons correlation coefficients on the 2004 Robust Track for clarity score, robustness score, WIG and query feedback (QF).",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Furthermore, we examine the prediction sensitivity of our methods to the cutoff rank K. With respect to WIG, it is quite robust to K on the Terabyte Tracks (2004-2006) while it prefers a small value of K like 5 on the 2004 Robust Track.",
                "In other words, a small value of K is a nearly-optimal choice for both kinds of tracks.",
                "Considering the fact that all other parameters involved in WIG are fixed and consequently the same for the two cases, this means WIG can achieve nearly-optimal prediction accuracy in two considerably different situations with exactly the same parameter setting.",
                "Regarding QF, it prefers a larger value of K such as 100 on the Terabyte Tracks and a smaller value of K such as 25 on the 2004 Robust Track. 4.2.2 NP Queries We adopt WIG and first rank change (FRC) for predicting NPquery performance.",
                "We also try a linear combination of the two as in the previous section.",
                "The combination weight is obtained from the other data set.",
                "We use the correlation with the reciprocal ranks measured by the Pearsons correlation test to evaluate prediction quality.",
                "The results are presented in Table 6.",
                "Again, our baselines are the clarity score and the robustness score.",
                "To make a fair comparison, we tune the clarity score in different ways.",
                "We found that using the first ranked document to build the query model yields the best prediction accuracy.",
                "We also attempted to utilize document structure by using the mixture of language models mentioned in section 3.1.",
                "Little improvement was obtained.",
                "The correlation coefficients for the clarity score reported in Table 6 are the best we have found.",
                "As we can see, our methods considerably outperform the clarity score technique on both of the runs.",
                "This confirms our intuition that the use of a coherence-based measure like the clarity score is inappropriate for NP queries.",
                "Methods Clarity Robust.",
                "WIG FRC WIG+FRC TB05-NP 0.150 -0.370 0.458 0.440 0.525 TB06-NP 0.112 -0.160 0.478 0.386 0.515 Table 6: Pearsons correlation coefficients for correlation with reciprocal ranks on the Terabyte Tracks (NP) for clarity score, robustness score, WIG, the first rank change (FRC) and a linear combination of WIG and FRC.",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Regarding the robustness score, we also tune the parameters and report the best we have found.",
                "We observe an interesting and surprising negative correlation with reciprocal ranks.",
                "We explain this finding briefly.",
                "A high robustness score means that a number of top ranked documents in the original ranked list are still highly ranked after perturbing the documents.",
                "The existence of such documents is a good sign of high performance for content-based queries as these queries usually contain a number of relevant documents [1].",
                "However, with regard to NP queries, one fundamental difference is that there is only one relevant document for each query.",
                "The existence of such documents can confuse the ranking function and lead to low retrieval performance.",
                "Although the negative correlation with retrieval performance exists, the strength of the correlation is weaker and less consistent compared to our methods as shown in Table 6.",
                "Based on the above analysis, we can see that current prediction techniques like clarity score and robustness score that are mainly designed for content-based queries face significant challenges and are inadequate to deal with NP queries.",
                "Our two techniques proposed for NP queries consistently demonstrate good prediction accuracy, displaying initial success in solving the problem of predicting performance for NP queries.",
                "Another point we want to stress is that the WIG method works well for both types of queries, a desirable property that most prediction techniques lack. 4.3 Unknown Query Types In this section, we run two kinds of experiments without access to query type labels.",
                "First, we assume that only one type of query exists but the type is unknown.",
                "Second, we experiment on a mixture of content-based and NP queries.",
                "The following two subsections will report results for the two conditions respectively. 4.3.1 Only One Type exists We assume that all queries are of the same type, that is, they are either NP queries or content-based queries.",
                "We choose WIG to deal with this case because it shows good prediction accuracy for both types of queries in the previous section.",
                "We consider two cases: (1) CB: all 150 title queries from the ad-hoc task of the Terabyte Tracks 2004-2006 (2)NP: all 433 NP queries from the named page finding task of the Terabyte Tracks 2005 and 2006.",
                "We take a simple strategy by labeling all of the queries in each case as the same type (either NP or CB) regardless of their actual type.",
                "The computation of WIG will be based on the labeled query type instead of the actual type.",
                "There are four possibilities with respect to the relation between the actual type and the labeled type.",
                "The correlation with retrieval performance under the four possibilities is presented in Table 7.",
                "For example, the value 0.445 at the intersection between the second row and the third column shows the Pearsons correlation coefficient for correlation with average precision when the content-based queries are incorrectly labeled as the NP type.",
                "Based on these results, we recommend treating all queries as the NP type when only one query type exists and accurate query classification is not feasible, considering the risk that a large loss of accuracy will occur if NP queries are incorrectly labeled as content-based queries.",
                "These results also demonstrate the strong adaptability of WIG to different query types.",
                "CB (labeled) NP (labeled) CB (actual) 0.536 0.445 NP (actual) 0.174 0.467 Table 7: Comparison of Pearsons correlation coefficients for correlation with retrieval performance under four possibilities on the Terabyte Tracks (NP).",
                "Bold cases mean the results are statistically significant at the 0.01 level. 4.3.2 A mixture of contented-based and NP queries A mixture of the two types of queries is a more realistic situation that a Web search engine will meet.",
                "We evaluate prediction accuracy by how accurately poorly-performing queries can be identified by the prediction method assuming that actual query types are unknown (but we can predict query types).",
                "This is a challenging task because both the predicted and actual performance for one type of query can be incomparable to that for the other type.",
                "Next we discuss how to implement our evaluation.",
                "We create a query pool which consists of all of the 150 ad-hoc title queries from Terabyte Track 2004-2006 and all of the 433 NP queries from Terabyte Track 2005&2006.",
                "We divide the queries in the pool into classes: good (better than 50% of the queries of the same type in terms of retrieval performance) and bad (otherwise).",
                "According to these standards, a NP query with the reciprocal rank above 0.2 or a content-based query with the average precision above 0.315 will be considered as good.",
                "Then, each time we randomly select one query Q from the pool with probability p that Q is contented-based.",
                "The remaining queries are used as training data.",
                "We first decide the type of query Q according to a query classifier.",
                "Namely, the query classifier tells us whether query Q is NP or content-based.",
                "Based on the predicted query type and the score computed for query Q by a prediction technique, a binary decision is made about whether query Q is good or bad by comparing to the score threshold of the predicted query type obtained from the training data.",
                "Prediction accuracy is measured by the accuracy of the binary decision.",
                "In our implementation, we repeatedly take a test query from the query pool and prediction accuracy is computed as the percentage of correct decisions, that is, a good(bad) query is predicted to be good (bad).",
                "It is obvious that random guessing will lead to 50% accuracy.",
                "Let us take the WIG method for example to illustrate the process.",
                "Two WIG thresholds (one for NP queries and the other for content-based queries) are trained by maximizing the prediction accuracy on the training data.",
                "When a test query is labeled as the NP (CB) type by the query type classifier, it will be predicted to be good if and only if the WIG score for this query is above the NP (CB) threshold.",
                "Similar procedures will be taken for other prediction techniques.",
                "Now we briefly introduce the automatic query type classifier used in this paper.",
                "We find that the robustness score, though originally proposed for performance prediction, is a good indicator of query types.",
                "We find that on average content-based queries have a much higher robustness score than NP queries.",
                "For example, Figure 2 shows the distributions of robustness scores for NP and content-based queries.",
                "According to this finding, the robustness score classifier will attach a NP (CB) label to the query if the robustness score for the query is below (above) a threshold trained from the training data. 0 0.5 1 1.5 2 2.5 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 NP Content-based Figure 2: Distribution of robustness scores for NP and CB queries.",
                "The NP queries are the 252 NP topics from the 2005 Terabyte Track.",
                "The content-based queries are the 150 ad-hoc title from the Terabyte Tracks 2004-2006.",
                "The probability distributions are estimated by the Kernel density estimation method.",
                "Strategies Robust WIG-1 WIG-2 WIG-3 Optimal p=0.6 0.565 0.624 0.665 0.684 0.701 P=0.4 0.567 0.633 0.654 0.673 0.696 Table 8: Comparison of prediction accuracy for five strategies in the mixed-query situation.",
                "Two ways to sample a query from the pool: (1) the sampled query is content-based with the probability p=0.6. (that is, the query is NP with probability 0.4 ) (2) set the probability p=0.4.",
                "We consider five strategies in our experiments.",
                "In the first strategy (denoted by robust), we use the robustness score for query performance prediction with the help of a perfect query classifier that always correctly map a query into one of the two categories (that is, NP or CB).",
                "This strategy represents the level of prediction accuracy that current prediction techniques can achieve in an ideal condition that query types are known.",
                "In the next following three strategies, the WIG method is adopted for performance prediction.",
                "The difference among the three is that three different query classifiers are used for each strategy: (1) the classifier always classifies a query into the NP type. (2) the <br>robustness score probabilitydensity classifier</br> is the robust score classifier mentioned above. (3) the classifier is a perfect one.",
                "These three strategies are denoted by WIG-1, WIG-2 and WIG-3 respectively.",
                "The reason we are interested in WIG-1 is based on the results from section 4.3.1.",
                "In the last strategy (denoted by Optimal) which serves as an upper bound on how well we can do so far, we fully make use of our prediction techniques for each query type assuming a perfect query classifier is available.",
                "Specifically, we linearly combine WIG and QF for content-based queries and WIG and FRC for NP queries.",
                "The results for the five strategies are shown in Table 8.",
                "For each strategy, we try two ways to sample a query from the pool: (1) the sampled query is CB with probability p=0.6. (the query is NP with probability 0.4) (2) set the probability p=0.4.",
                "From Table 8 We can see that in terms of prediction accuracy WIG-2 (the WIG method with the automatic query classifier) is not only better than the first two cases, but also is close to WIG-3 where a perfect classifier is assumed.",
                "Some further improvements over WIG-3 are observed when combined with other prediction techniques.",
                "The merit of WIG-2 is that it provides a practical solution to automatically identifying poorly performing queries in a Web search environment with mixed query types, which poses considerable obstacles to traditional prediction techniques. 5.",
                "CONCLUSIONS AND FUTURE WORK To our knowledge, our paper is the first to thoroughly explore prediction of query performance in web search environments.",
                "We demonstrated that our models resulted in higher prediction accuracy than previously published techniques not specially devised for web search scenarios.",
                "In this paper, we focus on two types of queries in web search: content-based and Named-Page (NP) finding queries, corresponding to the ad-hoc retrieval task and the Named-Page finding task respectively.",
                "For both types of web queries, our prediction models were shown to be substantially more accurate than the current state-of-the-art techniques.",
                "Furthermore, we considered a more realistic case that no prior information on query types is available.",
                "We demonstrated that the WIG method is particularly suitable for this situation.",
                "Considering the adaptability of WIG to a range of collections and query types, one of our future plans is to apply this method to predict user preference of search results on realistic data collected from a commercial search engine.",
                "Other than accuracy, another major issue that prediction techniques have to deal with in a Web environment is efficiency.",
                "Fortunately, since the WIG score is computed just over the terms and the phrases that appear in the query, this calculation can be made very efficient with the support of index.",
                "On the other hand, the computation of QF and FRC is relatively less efficient since QF needs to retrieve the whole collection twice and FRC needs to repeatedly rank the perturbed documents.",
                "How to improve the efficiency of QF and FRC is our future work.",
                "In addition, the prediction techniques proposed in this paper have the potential of improving retrieval performance by combining with other IR techniques.",
                "For example, our techniques can be incorporated to popular query modification techniques such as query expansion and query relaxation.",
                "Guided by performance prediction, we can make a better decision on when to or how to modify queries to enhance retrieval effectiveness.",
                "We would like to carry out research in this direction in the future. 6.",
                "ACKNOWLEGMENTS This work was supported in part by the Center for Intelligent Information Retrieval, in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C0023, and in part by an award from Google.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor.",
                "In addition, we thank Donald Metzler for his valuable comments on this work. 7.",
                "REFERENCES [1] Y. Zhou ,W. B. Croft ,Ranking Robustness: A Novel Framework to Predict Query Performance, in Proceedings of CIKM 2006. [2] D.Carmel, E.Yom-Tov, A.Darlow,D.Pelleg, What Makes a Query Difficult?, in Proceedings of SIGIR 2006. [3] C.L.A.",
                "Clarke, F. Scholer, I.Soboroff, The TREC 2005 Terabyte Track, In the Online Proceedings of 2005 TREC. [4] B.",
                "He and I.Ounis.",
                "Inferring query performance using preretrieval predictors.",
                "In proceedings of the SPIRE 2004. [5] S. Tomlinson.",
                "Robust, Web and Terabyte Retrieval with Hummingbird SearchServer at TREC 2004.",
                "In the Online Proceedings of 2004 TREC. [6] S. Cronen-Townsend, Y. Zhou, W. B. Croft, Predicting Query Performance, in Proceedings of SIGIR 2002. [7] V.Vinay, I.J.Cox, N.Mill-Frayling,K.Wood, On Ranking the Effectiveness of Searcher, in Proceedings of SIGIR 2006. [8] D.Metzler, W.B.Croft, A Markov Random Filed Model for Term Dependencies, in Proceedings of SIGIR 2005. [9] D.Metzler, T.Strohman,Y.Zhou,W.B.Croft, Indri at TREC 2005: Terabyte Track, In the Online Proceedings of 2004 TREC. [10] P. Ogilvie and J. Callan, Combining document representations for known-item search, in Proceedings of SIGIR 2003. [11] A.Berger, J.Lafferty, Information retrieval as statistical translation, in Proceedings of SIGIR 1999. [12] Indri search engine : http://www.lemurproject.org/indri/ [13] I.J.",
                "Taneja: On Generalized Information Measures and Their Applications, Advances in Electronics and Electron Physics, Academic Press (USA), 76, 1989, 327-413. [14] S.Cronen-Townsend, Y. Zhou and Croft, W. B. , A Framework for Selective Query Expansion, in Proceedings of CIKM 2004. [15] F.Song, W.B.Croft, A general language model for information retrieval, in Proceedings of SIGIR 1999. [16] Personal email contact with Vishwa Vinay and our own experiments [17] E.Yom-Tov, S.Fine, D.Carmel, A.Darlow, Learning to Estimate Query Difficulty Including Applications to Missing Content Detection and Distributed Information retrieval, in Proceedings of SIGIR 2005"
            ],
            "original_annotated_samples": [
                "The difference among the three is that three different query classifiers are used for each strategy: (1) the classifier always classifies a query into the NP type. (2) the <br>robustness score probabilitydensity classifier</br> is the robust score classifier mentioned above. (3) the classifier is a perfect one."
            ],
            "translated_annotated_samples": [
                "La diferencia entre los tres es que se utilizan tres clasificadores de consulta diferentes para cada estrategia: (1) el clasificador siempre clasifica una consulta en el tipo NP. (2) el <br>clasificador de Probabilidad de Densidad de Puntuación Robusta</br> es el clasificador de puntuación robusta mencionado anteriormente. (3) el clasificador es perfecto."
            ],
            "translated_text": "Predicción del rendimiento de consultas en entornos de búsqueda web Yun Zhou y W. Bruce Croft Departamento de Ciencias de la Computación Universidad de Massachusetts, Amherst {yzhou, croft}@cs.umass.edu RESUMEN Las técnicas actuales de predicción, generalmente diseñadas para consultas basadas en contenido y evaluadas típicamente en colecciones de prueba relativamente homogéneas de tamaños pequeños, enfrentan serios desafíos en entornos de búsqueda web donde las colecciones son significativamente más heterogéneas y existen diferentes tipos de tareas de recuperación. En este artículo, presentamos tres técnicas para abordar estos desafíos. Nos enfocamos en la predicción de rendimiento para dos tipos de consultas en entornos de búsqueda web: basadas en contenido y búsqueda de páginas por nombre. Nuestra evaluación se realiza principalmente en la colección GOV2. Además de evaluar nuestros modelos para los dos tipos de consultas por separado, consideramos una situación más desafiante y realista en la que los dos tipos de consultas se mezclan sin información previa sobre los tipos de consulta. Para ayudar en la predicción bajo la situación de consulta mixta, se adopta un clasificador de consultas novedoso. Los resultados muestran que nuestra predicción del rendimiento de la consulta web es sustancialmente más precisa que las técnicas de predicción actuales más avanzadas. Por consiguiente, nuestro artículo proporciona un enfoque práctico para predecir el rendimiento en entornos web del mundo real. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Formulación de Consultas Términos Generales Algoritmos, Experimentación, Teoría 1. La predicción del rendimiento de la consulta tiene muchas aplicaciones en una variedad de áreas de recuperación de información (IR), como mejorar la consistencia de la recuperación, el refinamiento de la consulta y la IR distribuida. La importancia de este problema ha sido reconocida por los investigadores en Recuperación de Información y recientemente se han propuesto varios métodos nuevos para la predicción [1, 2, 17]. La mayoría de los trabajos sobre predicción se han centrado en la tarea tradicional de recuperación ad-hoc, donde el rendimiento de la consulta se mide según la relevancia temática. Estos modelos de predicción se evalúan en colecciones de documentos de TREC que suelen consistir en no más de un millón de artículos de agencias de noticias relativamente homogéneos. Con la popularidad e influencia de la Web, las técnicas de predicción que funcionen bien para consultas de estilo web son altamente preferibles. Sin embargo, los entornos de búsqueda web plantean desafíos significativos para los modelos de predicción actuales que están principalmente diseñados para entornos tradicionales de TREC. Aquí delineamos algunos de estos desafíos. Primero, las colecciones web, que son mucho más grandes que las colecciones TREC convencionales, incluyen una variedad de documentos que son diferentes en muchos aspectos, como la calidad y el estilo. Las técnicas actuales de predicción pueden ser vulnerables a estas características de las colecciones web. Por ejemplo, la precisión de predicción reportada de la técnica de robustez de clasificación y la técnica de claridad en la colección GOV2 (una colección web grande) es significativamente peor en comparación con las otras colecciones de TREC [1]. En [2] se informa de una precisión de predicción similar en la colección GOV2 utilizando otra técnica, confirmando la dificultad de predecir el rendimiento de consultas en una gran colección web. Además, la búsqueda web va más allá del alcance de la tarea de recuperación ad hoc basada en la relevancia temática. Por ejemplo, la tarea de encontrar páginas nombradas (NP), que es una tarea de navegación, también es popular en la recuperación web. La predicción del rendimiento de la consulta para la tarea de NP sigue siendo necesaria, ya que el rendimiento de recuperación de NP está lejos de ser perfecto. De hecho, según el informe sobre la tarea NP del Terabyte Track de 2005 [3], aproximadamente el 40% de las consultas de prueba tienen un rendimiento deficiente (sin respuesta correcta en los primeros 10 resultados de búsqueda) incluso en la mejor ejecución del grupo líder. Hasta donde sabemos, poca investigación ha abordado explícitamente el problema de la predicción del rendimiento de consultas de NP. Los modelos de predicción actuales diseñados para consultas basadas en contenido serán menos efectivos para consultas de NP considerando las diferencias fundamentales entre ambas. En tercer lugar, en entornos de búsqueda web del mundo real, las consultas de los usuarios suelen ser una mezcla de diferentes tipos y, por lo general, no se dispone de información previa sobre el tipo de cada consulta. La situación de consulta mixta plantea nuevos problemas para la predicción del rendimiento de la consulta. Por ejemplo, es posible que necesitemos incorporar un clasificador de consultas en los modelos de predicción. A pesar de estos problemas, la capacidad para manejar esta situación es un paso crucial hacia convertir la predicción del rendimiento de consultas de un tema de investigación interesante en una herramienta práctica para la recuperación web. En este artículo, presentamos tres técnicas para abordar los desafíos mencionados que enfrentan los modelos de predicción actuales en entornos de búsqueda web. Nuestro trabajo se centra en la predicción del rendimiento de consultas para la tarea de recuperación basada en contenido (ad-hoc) y la tarea de encontrar páginas por nombre en el contexto de la recuperación web. Nuestra primera técnica, llamada ganancia de información ponderada (WIG), utiliza tanto características de términos individuales como de proximidad de términos para estimar la calidad de los documentos recuperados para la predicción. Observamos que WIG ofrece una precisión de predicción consistente en diversas colecciones de pruebas y tipos de consultas. Además, demostramos que se puede lograr una buena precisión de predicción para la situación de consulta mixta utilizando WIG con la ayuda de un clasificador de tipo de consulta. La retroalimentación de consultas y el cambio de rango inicial, que son nuestras técnicas de predicción segunda y tercera, funcionan bien para consultas basadas en contenido y consultas de NP respectivamente. Nuestras principales contribuciones incluyen: (1) una precisión de predicción considerablemente mejorada para consultas basadas en contenido web en comparación con varias técnicas de vanguardia. (2) nuevas técnicas para predecir con éxito el rendimiento de consultas NP. (3) una solución práctica y totalmente automática para predecir el rendimiento de consultas mixtas. Además, una contribución menor es que encontramos que el puntaje de robustez [1], que originalmente se propuso para la predicción de rendimiento, es útil para la clasificación de consultas. 2. TRABAJO RELACIONADO Como mencionamos en la introducción, recientemente se han propuesto varias técnicas de predicción que se centran en consultas basadas en contenido en la tarea de relevancia temática (ad-hoc). No conocemos ningún trabajo publicado que aborde otros tipos de consultas como las consultas NP, y mucho menos una mezcla de tipos de consultas. A continuación revisamos algunos modelos representativos. La principal dificultad de la predicción del rendimiento proviene del hecho de que muchos factores tienen un impacto en el rendimiento de recuperación. Cada factor afecta el rendimiento en diferente medida y el efecto general es difícil de predecir con precisión. Por lo tanto, no es sorprendente notar que características simples, como la frecuencia de los términos de consulta en la colección [4] y la IDF promedio de los términos de consulta [5], no predicen bien. De hecho, la mayoría de las técnicas exitosas se basan en medir algunas características del conjunto de documentos recuperados para estimar la dificultad del tema. Por ejemplo, la puntuación de claridad [6] mide la coherencia de una lista de documentos mediante la divergencia KL entre el modelo de consulta y el modelo de colección. El puntaje de robustez [1] cuantifica otra propiedad de una lista clasificada: la robustez de la clasificación en presencia de incertidumbre. Carmel et al. [2] encontraron que la distancia medida por la divergencia de Jensen-Shannon entre el conjunto de documentos recuperados y la colección está significativamente correlacionada con la precisión promedio. Vinay et al. [7] propusieron cuatro medidas para capturar la geometría de los documentos mejor clasificados para la predicción. La medida más efectiva es la sensibilidad a la perturbación del documento, una idea algo similar a la puntuación de robustez. Desafortunadamente, su forma de medir la sensibilidad no funciona igual de bien para consultas cortas y la precisión de la predicción disminuye considerablemente cuando se adopta una técnica de recuperación de vanguardia (como Okapi o un enfoque de modelado de lenguaje) en lugar del peso tf-idf utilizado en su artículo [16]. Las dificultades de aplicar estos modelos en entornos de búsqueda web ya han sido mencionadas. En este documento, principalmente adoptamos la puntuación de claridad y la puntuación de robustez como nuestras líneas base. Experimentalmente demostramos que las líneas base, incluso después de ser ajustadas cuidadosamente, son inadecuadas para el entorno web. Uno de nuestros modelos de predicción, WIG, está relacionado con el modelo de campo aleatorio de Markov (MRF) para la recuperación de información [8]. El modelo MRF modela directamente la dependencia de términos y se ha demostrado ser altamente efectivo en una variedad de colecciones de pruebas (especialmente colecciones web) y tareas de recuperación. Este modelo se utiliza para estimar la distribución conjunta de probabilidad sobre documentos y consultas, una parte importante de WIG. La superioridad de WIG sobre otras técnicas de predicción basadas en características unigramas, que se demostrará más adelante en nuestro artículo, coincide con la de MRF para recuperación. En otras palabras, es interesante notar que la dependencia de términos, cuando se modela adecuadamente, puede ser útil tanto para mejorar como para predecir el rendimiento de recuperación. 3. Modelos de predicción 3.1 Ganancia de Información Ponderada (WIG) Esta sección introduce un enfoque de ganancia de información ponderada que incorpora tanto características de términos únicos como de proximidad para predecir el rendimiento tanto en consultas basadas en contenido como en consultas de búsqueda de Páginas Nombradas (NP). Dado un conjunto de consultas Q={Qs} (s=1,2,..N) que incluye todas las posibles consultas de usuario y un conjunto de documentos D={Dt} (t=1,2…M), asumimos que cada par consulta-documento (Qs,Dt) es evaluado manualmente y se incluirá en una lista de relevancia si se determina que Qs es relevante para Dt. La probabilidad conjunta P(Qs,Dt) sobre las consultas Q y los documentos D denota la probabilidad de que el par (Qs,Dt) esté en la lista de relevancia. Tales suposiciones son similares a las utilizadas en [8]. Suponiendo que el usuario emite la consulta Qi ∈Q y los resultados de recuperación en respuesta a Qi es una lista clasificada L de documentos, calculamos la cantidad de información contenida en P(Qs,Dt) con respecto a Qi y L mediante la Ec.1, que es una variante de la entropía llamada entropía ponderada[13]. Los pesos en la ecuación 1 están determinados únicamente por Qi y L. En este documento, elegimos los pesos de la siguiente manera: L es el número de documentos que contiene la consulta, K es el límite superior, LT es el límite superior de documentos, y D es el peso si K está en otro caso. La clasificación de corte K es un parámetro en nuestro modelo que se discutirá más adelante. Por lo tanto, la ecuación 1 se puede simplificar de la siguiente manera: )3(),(log 1 ),( )( , ∑∈ −= LTD titsLQ Kt i DQP K DQH Desafortunadamente, la entropía ponderada ),(, tsLQ DQH i calculada por la ecuación 3, que representa la cantidad de información sobre la probabilidad de que los documentos mejor clasificados en L sean relevantes para la consulta Qi en promedio, no se puede comparar entre diferentes consultas, lo que la hace inapropiada para predecir directamente el rendimiento de la consulta. Para mitigar este problema, creamos una distribución de fondo P(Qs,C) sobre Q y D imaginando que cada documento en D es reemplazado por el mismo documento especial C que representa el uso promedio del lenguaje. En este documento, C se crea concatenando cada documento en D. A grosso modo, C es la colección (el conjunto de documentos) {Dt} sin límites de documentos. De manera similar, la entropía ponderada ),(, CQH sLQi calculada por la Ecuación 3 representa la cantidad de información sobre la probabilidad de que un documento promedio (representado por toda la colección) sea relevante para la consulta Qi. Ahora presentamos nuestro predictor de rendimiento WIG que es la ganancia de información ponderada [13] calculada como la diferencia entre ),(, tsLQ DQH i y ),(, CQH sLQi. Específicamente, dado el conjunto de consultas Qi, la colección C y la lista clasificada L de documentos, WIG se calcula de la siguiente manera: )4( ),( ),( log 1 ),( ),( log),( ),(),(),,( )(, ,, ∑∑ ∈ == −= LTD i ti ts s ts ts tsLQsLQi Kt ii CQP DQP KCQP DQP DQweight DQHCQHLCQWIG WIG calculado por la Ecuación 4 mide el cambio en la información sobre la calidad de la recuperación (en respuesta a la consulta Qi) desde un estado imaginario en el que solo se recupera un documento promedio hasta un estado posterior en el que se observan los resultados de búsqueda reales. Hacemos la hipótesis de que WIG está positivamente correlacionado con la efectividad de la recuperación porque una recuperación de alta calidad debería ser mucho más efectiva que simplemente devolver el documento promedio. El corazón de esta técnica es cómo estimar la distribución conjunta P(Qs, Dt). En el enfoque de modelado del lenguaje para la recuperación de información, se pueden aplicar fácilmente una variedad de modelos para estimar esta distribución. Aunque la mayoría de estos modelos se basan en la suposición de la bolsa de palabras, trabajos recientes sobre modelado de dependencia de términos bajo el marco de modelado de lenguaje han demostrado mejoras consistentes y significativas en la efectividad de recuperación sobre los modelos de bolsa de palabras. Inspirados por el éxito de incorporar características de proximidad de términos en modelos de lenguaje, decidimos adoptar un buen modelo de dependencia para estimar la probabilidad P(Qs,Dt). El modelo que elegimos para este artículo es el modelo de Campo Aleatorio de Markov (MRF) de Metzler y Crofts, que ya ha demostrado su superioridad en comparación con varias colecciones y diferentes tareas de recuperación [8,9]. Según el modelo MRF, log P(Qi, Dt) se puede escribir como )5()|(loglog),(log )( 1 ∑∈ +−= iQF tti DPZDQP ξ ξ ξλ donde Z1 es una constante que asegura que P(Qi, Dt) sume 1. F(Qi) consiste en un conjunto de características ampliadas a partir de la consulta original Qi. Por ejemplo, suponiendo que la consulta Qi es el programa de estudiantes talentosos, F(Qi) incluye características como programa y estudiante talentoso. Consideramos dos tipos de características: características de términos individuales T y características de proximidad P. Las características de proximidad incluyen la frase exacta (#1) y las características de ventana desordenada (#uwN) como se describe en [8]. Ten en cuenta que F(Qi) es la unión de T(Qi) y P(Qi). Para obtener más detalles sobre F(Qi), como por ejemplo cómo expandir la consulta original Qi a F(Qi), remitimos al lector a [8] y [9]. P(ξ|Dt) denota la probabilidad de que la característica ξ ocurra en Dt. Más detalles sobre P(ξ|Dt) se proporcionarán más adelante en esta sección. La elección de λξ es algo diferente a la utilizada en [8] ya que λξ desempeña un doble papel en nuestro modelo. El primer rol, que es el mismo que en [8], es ponderar entre las características de términos individuales y de proximidad. El otro rol, específico de nuestra tarea de predicción, es normalizar el tamaño de F(Qi). Encontramos que la siguiente estrategia de peso para λξ satisface los dos roles anteriores y generaliza bien en una variedad de colecciones y tipos de consultas. )6( )(, |)(| 1 )(, |)(| ⎪ ⎪ ⎩ ⎪ ⎪ ⎨ ⎧ ∈ − ∈ = i i T i i T QP QP QT QT ξ λ ξ λ λξ donde |T(Qi)| y |P(Qi)| denotan el número de características de términos únicos y de proximidad en F(Qi) respectivamente. La razón de elegir la función raíz cuadrada en el denominador de λξ es penalizar adecuadamente un conjunto de características de gran tamaño, haciendo que WIG sea más comparable entre consultas de diversas longitudes. λT es un parámetro fijo y se establece en 0.8 según [8] a lo largo de este documento. De manera similar, log P(Qi,C) se puede escribir como: )7()|(loglog),(log )( 2 ∑∈ +−= iQF i CPZCQP ξ ξ ξλ Cuando se eliminan las constantes Z1 y Z2, el WIG calculado en la Ecuación 4 se puede reescribir de la siguiente manera al sustituir la Ecuación 5 y la Ecuación 7: )8( )|( )|( log 1 ),,( )( )( ∑ ∑∈ ∈ = LTD QF t i Kt i CP DP K LCQWIG ξ ξ ξ ξ λ Una de las ventajas del WIG sobre otras técnicas es que puede manejar tanto consultas basadas en contenido como en NP. Basándose en el tipo (o el tipo predicho) de Qi, el cálculo de WIG en la Ecuación 8 difiere en dos aspectos: (1) cómo estimar P(ξ|Dt) y P(ξ|C), y (2) cómo elegir K. Para consultas basadas en contenido, P(ξ|C) se estima por la frecuencia relativa del rasgo ξ en la colección C en su totalidad. La estimación de P(ξ|Dt) es la misma que en [8]. Es decir, estimamos P(ξ|Dt) mediante la frecuencia relativa del rasgo ξ en Dt suavizada linealmente con la frecuencia de colección P(ξ|C). La K en la Ec.8 se trata como un parámetro libre. Ten en cuenta que K es el único parámetro libre en el cálculo de WIG para consultas basadas en contenido, ya que se asume que todos los parámetros involucrados en P(ξ|Dt) están fijos al tomar los valores sugeridos en [8]. En cuanto a las consultas de NP, hacemos uso de la estructura del documento para estimar P(ξ|Dt) y P(ξ|C) mediante el llamado modelo de mezcla de lenguaje propuesto en [10] e incorporado en el modelo MRF para la recuperación de Named-Page en [9]. La idea básica es que un documento (colección) se divide en varios campos como el campo del título, el campo del cuerpo principal y el campo de los encabezados. P(ξ|Dt) y P(ξ|C) se estiman mediante una combinación lineal de los modelos de lenguaje de cada campo. Debido a limitaciones de espacio, remitimos al lector a [9] para más detalles. Adoptamos el mismo conjunto exacto de parámetros utilizados en [9] para la estimación. En cuanto a K en la Ec.8, establecemos K en 1 porque la tarea de encontrar Named-Page se centra fuertemente en el documento clasificado en primer lugar. Por consiguiente, no hay parámetros libres en el cálculo de WIG para consultas de NP. 3.2 Retroalimentación de consultas En esta sección, presentamos otra técnica llamada retroalimentación de consultas (QF) para la predicción. Suponga que un usuario emite la consulta Q a un sistema de recuperación y se devuelve una lista clasificada L de documentos. Vemos el sistema de recuperación como un canal ruidoso. Específicamente, asumimos que la salida del canal es L y la entrada es Q. Después de pasar por el canal, Q se corrompe y se transforma en la lista clasificada L. Al pensar en el proceso de recuperación de esta manera, el problema de predecir la eficacia de la recuperación se convierte en la tarea de evaluar la calidad del canal. En otras palabras, la predicción se convierte en encontrar una forma de medir el grado de corrupción que surge cuando Q se transforma en L. Dado que calcular directamente el grado de corrupción es difícil, abordamos este problema mediante aproximaciones. Nuestra idea principal es que medimos en qué medida la información sobre Q puede ser recuperada de L bajo la suposición de que solo se observa L. Específicamente, diseñamos un decodificador que pueda traducir con precisión L de vuelta a la nueva consulta Q y la similitud S entre la consulta original Q y la nueva consulta Q se adopta como un predictor de rendimiento. Este es un bosquejo de cómo la técnica QF predice el rendimiento de la consulta. Antes de completar más detalles, discutimos brevemente por qué este método funcionaría. Existe una relación entre la similitud S definida anteriormente y el rendimiento de recuperación. Por un lado, si la recuperación se ha desviado del sentido original de la consulta Q, la nueva consulta Q extraída de la lista clasificada L en respuesta a Q sería muy diferente de la consulta original Q. Por otro lado, una consulta destilada de una lista clasificada que contiene muchos documentos relevantes es probable que sea similar a la consulta original. Se proporcionarán más ejemplos en apoyo de la relación más adelante. A continuación detallamos cómo construir el decodificador y cómo medir la similitud S. En esencia, el objetivo del decodificador es comprimir la lista clasificada L en unos pocos términos informativos que deberían representar el contenido de los documentos mejor clasificados en L. Nuestro enfoque para este objetivo es representar la lista clasificada L mediante un modelo de lenguaje (distribución de términos). Luego, los términos se clasifican según su contribución a la divergencia KL (Kullback-Leibler) de los modelos de lenguaje con respecto al modelo de la colección de fondo. Los términos mejor clasificados serán elegidos para formar la nueva consulta Q. Este enfoque es similar al utilizado en la Sección 4.1 de [11]. Específicamente, tomamos tres pasos para comprimir la lista clasificada L en la consulta Q sin hacer referencia a la consulta original. 1. Adoptamos el modelo de lenguaje de lista clasificada [14] para estimar un modelo de lenguaje basado en la lista clasificada L. El modelo puede escribirse como: )9()|()|()|( ∑∈ = LD LDPDwPLwP donde w es cualquier término y D es un documento. La probabilidad de que ocurra el evento D dado el evento L se estima mediante una función linealmente decreciente del rango del documento D. Cada término en P(w|L) está clasificado por la siguiente contribución de divergencia de Kullback-Leibler: )10( )|( )|( log)|( CwP LwP LwP donde P(w|C) es el modelo de colección estimado por la frecuencia relativa del término w en la colección C en su totalidad. 3. Los términos N principales clasificados por Eq.10 forman una consulta ponderada Q={(wi,ti)} i=1,N, donde wi denota el término clasificado en la posición i y el peso ti es la contribución de la divergencia KL de wi en Eq. 10. Barco de crucero daño vida marina. Estos ejemplos indican cómo la similitud entre la consulta original y la nueva se correlaciona con el rendimiento de recuperación. El parámetro N en el paso 3 se establece en 20 de manera empírica y elegir un valor más grande de N es innecesario ya que los pesos después de los primeros 20 suelen ser demasiado pequeños para marcar alguna diferencia. Para medir la similitud entre la consulta original Q y la nueva consulta Q, primero utilizamos Q para recuperar información de la misma colección. Se adopta una variante del modelo de probabilidad de consulta [15] para la recuperación. Es decir, los documentos se clasifican por: )11()|()|( ),( ∑∈ = Qtw t i ii i DwPDQP donde wi es un término en Q y ti es el peso asociado. D es un documento. Que L denote la nueva lista clasificada devuelta desde la recuperación anterior. La similitud se mide por la superposición de documentos en L y L. Específicamente, el porcentaje de documentos en los primeros K documentos de L que también están presentes en los primeros K documentos en L. El valor de corte K se trata como un parámetro libre. Aquí resumimos cómo la técnica QF predice el rendimiento dado una consulta Q y la lista clasificada asociada L. Primero obtenemos una consulta ponderada Q comprimida a partir de L mediante los tres pasos anteriores. Luego usamos Q para realizar la recuperación y la nueva lista clasificada es L. La superposición de documentos en L y L se utiliza para la predicción. 3.3 Primer Cambio de Rango (FRC) En esta sección, proponemos un método llamado primer cambio de rango (FRC) para la predicción de rendimiento para consultas de NP. Este método se deriva de la técnica de robustez de clasificación [1] que está principalmente diseñada para consultas basadas en contenido. Cuando se aplica directamente a consultas de NP, la técnica de robustez será menos efectiva porque tiene en cuenta todos los documentos mejor clasificados en su totalidad, mientras que las consultas de NP suelen tener solo un único documento relevante. En cambio, nuestra técnica se centra en el documento de rango uno mientras se mantiene la idea principal del método de robustez. Específicamente, el seudocódigo para calcular FRC se muestra en la figura 1. Lista clasificada L={Di} donde i=1,100. Di denota el documento clasificado en la posición i. (2) consulta Q 1 inicializar: (1) establecer el número de intentos J=100000 (2) contador c=0; 2 para i=1 a J 3 Perturbar cada documento en L, que el resultado sea un conjunto F={Di} donde Di denota la versión perturbada de Di. 4 Realizar la recuperación con la consulta Q en el conjunto F 5 c=c+1 si y solo si D1 está clasificado primero en el paso 4 6 fin del para 7 devolver la proporción c/J Figura 1: pseudo-código para calcular FRC FRC aproxima la probabilidad de que el documento clasificado en primer lugar en la lista original L permanezca clasificado primero incluso después de que los documentos sean perturbados. Cuanto mayor sea la probabilidad, más confianza tenemos en el documento clasificado en primer lugar. Por otro lado, en el caso extremo de un ranking aleatorio, la probabilidad sería tan baja como 0.5. Esperamos que FRC tenga una asociación positiva con el rendimiento de la consulta de NP. Adoptamos [1] para implementar el paso de perturbación del documento (paso 4 en la Fig. 1) utilizando distribuciones de Poisson. Para más detalles, remitimos al lector a [1]. 4. EVALUACIÓN Ahora presentamos los resultados de predecir el rendimiento de la consulta por nuestros modelos. Tres técnicas de vanguardia son adoptadas como nuestras líneas base. Evaluamos nuestras técnicas en una variedad de configuraciones de recuperación web. Como se mencionó anteriormente, consideramos dos tipos de consultas, es decir, consultas basadas en contenido (CB) y consultas de búsqueda de páginas con nombre (NP). Primero, supongamos que se conocen los tipos de consultas. Investigamos la correlación entre el rendimiento de recuperación predicho y el rendimiento real para ambos tipos de consultas por separado. Los resultados muestran que nuestros métodos producen mejoras considerablemente superiores a los valores de referencia. Luego consideramos un escenario más desafiante donde no hay información previa sobre los tipos de consultas disponibles. Se consideran dos subcasos. En el primero, existe solo un tipo de consulta pero el tipo real es desconocido. Suponemos una mezcla de los dos tipos de consultas en el segundo caso. Demostramos que nuestros modelos logran una buena precisión en este escenario exigente, lo que hace que la predicción sea práctica en un entorno de búsqueda web del mundo real. 4.1 Configuración Experimental Nuestra evaluación se centra en la colección GOV2 que contiene alrededor de 25 millones de documentos obtenidos de sitios web en el dominio .gov durante 2004 [3]. Creamos dos tipos de conjuntos de datos para consultas de CB y consultas de NP respectivamente. Para el tipo CB, utilizamos los temas ad-hoc de las pistas Terabyte de 2004, 2005 y 2006 y los nombramos TB04-adhoc, TB05-adhoc y TB06-adhoc respectivamente. Además, también utilizamos los temas ad-hoc de la pista Robusta de 2004 (RT04) para probar la adaptabilidad de nuestras técnicas a un entorno no web. Para consultas de NP, utilizamos los temas de búsqueda de páginas nombradas de las Terabyte Tracks de 2005 y 2006 y los denominamos TB05-NP y TB06-NP respectivamente. Todas las consultas utilizadas en nuestros experimentos son títulos de temas de TREC, ya que nos centramos en la recuperación web. La Tabla 3 resume los conjuntos de datos anteriores. La recuperación del rendimiento de las consultas individuales basadas en contenido y NP se mide mediante la precisión promedio y la tasa recíproca de la primera respuesta correcta, respectivamente. Hacemos uso del modelo de campo aleatorio de Markov tanto para la recuperación ad-hoc como para la recuperación de páginas nombradas. Adoptamos la misma configuración de parámetros de recuperación utilizada en [8,9]. El motor de búsqueda Indri [12] se utiliza para todos nuestros experimentos. Aunque no se informa aquí, también probamos el modelo de probabilidad de consulta para la recuperación ad-hoc y encontramos que los resultados cambian poco debido a la correlación muy alta entre el rendimiento de las consultas obtenido por los dos modelos de recuperación (0.96 medido por el coeficiente de Pearson). 4.2 Tipos de Consultas Conocidos Supongamos que se conocen los tipos de consultas. Tratamos cada tipo de consulta por separado y medimos la correlación con la precisión promedio (o el rango recíproco en el caso de consultas NP). Adoptamos la prueba de correlación de Pearson que refleja el grado de relación lineal entre el rendimiento de recuperación predicho y real. 4.2.1 Métodos de Consultas Basadas en Contenido Claridad Robusta JSD WIG QF WIG +QF TB04+0 5 adhoc 0.333 0.317 0.362 0.574 0.480 0.637 TB06 adhoc 0.076 0.294 N/A 0.464 0.422 0.511 Tabla 4: Coeficientes de correlación de Pearson para la correlación con la precisión promedio en las Pistas de Terabyte (ad-hoc) para la puntuación de claridad, puntuación de robustez, el método basado en JSD (citamos directamente la puntuación informada en [2]), WIG, retroalimentación de consulta (QF) y una combinación lineal de WIG y QF. Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. La Tabla 4 muestra la correlación con la precisión promedio en dos conjuntos de datos: uno es una combinación de TB04-adhoc y TB05-adhoc (100 temas en total) y el otro es TB06-adhoc (50 temas). La razón por la que juntamos TB04-adhoc y TB05-adhoc es para hacer nuestros resultados comparables a [2]. Nuestros puntos de referencia son la puntuación de claridad (claridad) [6], la puntuación de robustez (robust) [1] y el método basado en JSD (JSD) [2]. Para la puntuación de claridad y robustez, hemos probado diferentes configuraciones de parámetros y reportamos los coeficientes de correlación más altos que hemos encontrado. Citamos directamente el resultado del método basado en JSD reportado en [2]. La tabla también muestra los resultados para el método de Ganancia de Información Ponderada (WIG) y el método de Retroalimentación de Consulta (QF) para predecir consultas basadas en contenido. Como describimos en la sección anterior, tanto WIG como QF tienen un parámetro libre para ajustar, es decir, el rango de corte K. Entrenamos el parámetro en un conjunto de datos y lo probamos en el otro. Al combinar WIG y QF, se utiliza una combinación lineal simple y el peso de la combinación se aprende a partir del conjunto de datos de entrenamiento. A partir de estos resultados, podemos ver que nuestros métodos son considerablemente más precisos en comparación con los baselines. También observamos que se obtienen mejoras adicionales a partir de la combinación de WIG y QF, lo que sugiere que miden diferentes propiedades del proceso de recuperación que se relacionan con el rendimiento. Descubrimos que nuestros métodos se generalizan bien en TB06-adhoc, mientras que la correlación del puntaje de claridad con el rendimiento de recuperación en este conjunto de datos es considerablemente peor. Una investigación adicional muestra que la precisión promedio media de TB06-adhoc es de 0.342 y es aproximadamente un 10% mejor que la del primer conjunto de datos. Mientras que los otros tres métodos suelen considerar los 100 mejores documentos o menos de una lista clasificada, el método de claridad generalmente necesita los 500 mejores documentos o más para medir adecuadamente la coherencia de una lista clasificada. Un promedio de precisión media más alto hace que las listas clasificadas recuperadas por diferentes consultas sean más similares en términos de coherencia en el nivel de los primeros 500 documentos. Creemos que esta es la razón principal de la baja precisión de la puntuación de claridad en el segundo conjunto de datos. Aunque este documento se centra en un entorno de búsqueda web, es deseable que nuestras técnicas funcionen de manera consistente en otras situaciones. Con este fin, examinamos la efectividad de nuestras técnicas en la pista Robust 2004. Para nuestros métodos, dividimos equitativamente todas las consultas de prueba en cinco grupos y realizamos validación cruzada de cinco pliegues. Cada vez que utilizamos un grupo para el entrenamiento y los otros cuatro grupos para las pruebas. Hacemos uso de todas las consultas para nuestros dos puntos de referencia, es decir, la puntuación de claridad y la puntuación de robustez. Los parámetros para nuestras líneas base son los mismos que los utilizados en [1]. Los resultados mostrados en la Tabla 5 demuestran que la precisión de predicción de nuestros métodos está a la par con la de las dos líneas base sólidas. Claridad Robusta WIG QF 0.464 0.539 0.468 0.464 Tabla 5: Comparación de los coeficientes de correlación de Pearson en la pista Robusta de 2004 para la puntuación de claridad, puntuación de robustez, WIG y retroalimentación de consulta (QF). Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. Además, examinamos la sensibilidad de predicción de nuestros métodos al rango de corte K. Con respecto a WIG, es bastante robusto a K en las Pistas de Terabyte (2004-2006) mientras prefiere un valor pequeño de K como 5 en la Pista Robusta de 2004. En otras palabras, un valor pequeño de K es una elección casi óptima para ambos tipos de pistas. Teniendo en cuenta que todos los demás parámetros involucrados en WIG están fijos y, consecuentemente, son los mismos para los dos casos, esto significa que WIG puede lograr una precisión de predicción casi óptima en dos situaciones considerablemente diferentes con exactamente la misma configuración de parámetros. En cuanto a QF, prefiere un valor más grande de K, como 100 en las Pistas de Terabyte y un valor más pequeño de K, como 25 en la Pista Robusta de 2004. 4.2.2 Consultas NP Adoptamos WIG y el primer cambio de rango (FRC) para predecir el rendimiento de las consultas NP. También intentamos una combinación lineal de los dos como en la sección anterior. El peso de la combinación se obtiene del otro conjunto de datos. Utilizamos la correlación con los rangos recíprocos medidos por la prueba de correlación de Pearson para evaluar la calidad de la predicción. Los resultados se presentan en la Tabla 6. Nuevamente, nuestros puntos de referencia son la puntuación de claridad y la puntuación de robustez. Para hacer una comparación justa, ajustamos la puntuación de claridad de diferentes maneras. Descubrimos que utilizar el documento clasificado en primer lugar para construir el modelo de consulta produce la mejor precisión de predicción. También intentamos utilizar la estructura del documento mediante la combinación de modelos de lenguaje mencionados en la sección 3.1. Se obtuvo una pequeña mejora. Los coeficientes de correlación para la puntuación de claridad reportados en la Tabla 6 son los mejores que hemos encontrado. Como podemos ver, nuestros métodos superan considerablemente la técnica de puntuación de claridad en ambas ejecuciones. Esto confirma nuestra intuición de que el uso de una medida basada en la coherencia como el puntaje de claridad es inapropiado para las consultas de NP. Claridad de métodos robusta. Tabla 6: Coeficientes de correlación de Pearson para la correlación con rangos recíprocos en las pistas de Terabyte (NP) para la puntuación de claridad, puntuación de robustez, WIG, el primer cambio de rango (FRC) y una combinación lineal de WIG y FRC. Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. En cuanto a la puntuación de robustez, también ajustamos los parámetros y reportamos el mejor que hemos encontrado. Observamos una correlación negativa interesante y sorprendente con los rangos recíprocos. Explicamos este hallazgo brevemente. Un puntaje de robustez alto significa que varios documentos de alta clasificación en la lista clasificada original siguen estando muy bien clasificados después de perturbar los documentos. La existencia de tales documentos es una buena señal de alto rendimiento para consultas basadas en contenido, ya que estas consultas suelen contener una serie de documentos relevantes [1]. Sin embargo, en lo que respecta a las consultas de NP, una diferencia fundamental es que solo hay un documento relevante para cada consulta. La existencia de tales documentos puede confundir la función de clasificación y llevar a un bajo rendimiento de recuperación. Aunque existe una correlación negativa con el rendimiento de recuperación, la fuerza de la correlación es más débil y menos consistente en comparación con nuestros métodos, como se muestra en la Tabla 6. Basándonos en el análisis anterior, podemos ver que las técnicas de predicción actuales como la puntuación de claridad y la puntuación de robustez, que están principalmente diseñadas para consultas basadas en contenido, enfrentan desafíos significativos y son insuficientes para manejar consultas de NP. Nuestras dos técnicas propuestas para consultas de NP demuestran consistentemente una buena precisión de predicción, mostrando un éxito inicial en la resolución del problema de predecir el rendimiento para consultas de NP. Otro punto que queremos destacar es que el método WIG funciona bien para ambos tipos de consultas, una propiedad deseable que la mayoría de las técnicas de predicción carecen. 4.3 Tipos de Consultas Desconocidos En esta sección, realizamos dos tipos de experimentos sin acceso a etiquetas de tipo de consulta. Primero, asumimos que solo existe un tipo de consulta pero el tipo es desconocido. Segundo, experimentamos con una mezcla de consultas basadas en contenido y NP. Las dos subsecciones siguientes informarán los resultados para las dos condiciones respectivamente. 4.3.1 Solo existe un tipo. Suponemos que todas las consultas son del mismo tipo, es decir, son consultas NP o consultas basadas en contenido. Elegimos WIG para tratar este caso porque muestra una buena precisión de predicción para ambos tipos de consultas en la sección anterior. Consideramos dos casos: (1) CB: todas las 150 consultas de títulos de la tarea ad-hoc de las pistas Terabyte 2004-2006 (2) NP: todas las 433 consultas de NP de la tarea de búsqueda de páginas nombradas de las pistas Terabyte 2005 y 2006. Tomamos una estrategia simple etiquetando todas las consultas en cada caso como el mismo tipo (ya sea NP o CB) independientemente de su tipo real. El cálculo de WIG se basará en el tipo de consulta etiquetado en lugar del tipo real. Hay cuatro posibilidades con respecto a la relación entre el tipo real y el tipo etiquetado. La correlación con el rendimiento de recuperación bajo las cuatro posibilidades se presenta en la Tabla 7. Por ejemplo, el valor 0.445 en la intersección entre la segunda fila y la tercera columna muestra el coeficiente de correlación de Pearson para la correlación con la precisión promedio cuando las consultas basadas en contenido se etiquetan incorrectamente como del tipo NP. Basándonos en estos resultados, recomendamos tratar todas las consultas como del tipo NP cuando solo existe un tipo de consulta y la clasificación precisa de la consulta no es factible, considerando el riesgo de que se produzca una gran pérdida de precisión si las consultas NP se etiquetan incorrectamente como consultas basadas en contenido. Estos resultados también demuestran la fuerte adaptabilidad de WIG a diferentes tipos de consultas. Tabla 7: Comparación de los coeficientes de correlación de Pearson para la correlación con el rendimiento de recuperación en cuatro posibilidades en las pistas de Terabyte (NP). Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. 4.3.2 Una mezcla de consultas basadas en contenido y NP Una mezcla de los dos tipos de consultas es una situación más realista que un motor de búsqueda web encontrará. Evaluamos la precisión de la predicción por la exactitud con la que se pueden identificar las consultas de bajo rendimiento mediante el método de predicción, asumiendo que los tipos de consulta reales son desconocidos (pero podemos predecir los tipos de consulta). Esta es una tarea desafiante porque tanto el rendimiento predicho como el real para un tipo de consulta pueden ser incomparables con el de otro tipo. A continuación discutimos cómo implementar nuestra evaluación. Creamos un conjunto de consultas que consiste en todas las 150 consultas de título ad-hoc de Terabyte Track 2004-2006 y todas las 433 consultas NP de Terabyte Track 2005&2006. Dividimos las consultas en el conjunto en clases: buenas (mejores que el 50% de las consultas del mismo tipo en términos de rendimiento de recuperación) y malas (de lo contrario). Según estos estándares, una consulta de NP con un rango recíproco superior a 0.2 o una consulta basada en contenido con una precisión promedio superior a 0.315 se considerarán como buenas. Entonces, cada vez que seleccionamos aleatoriamente una consulta Q del grupo con una probabilidad p de que Q esté basada en el contenido. Las consultas restantes se utilizan como datos de entrenamiento. Primero decidimos el tipo de consulta Q de acuerdo con un clasificador de consultas. Es decir, el clasificador de consultas nos dice si la consulta Q es de tipo NP o basada en contenido. Basándose en el tipo de consulta predicho y en la puntuación calculada para la consulta Q por una técnica de predicción, se toma una decisión binaria sobre si la consulta Q es buena o mala al compararla con el umbral de puntuación del tipo de consulta predicho obtenido de los datos de entrenamiento. La precisión de la predicción se mide por la exactitud de la decisión binaria. En nuestra implementación, tomamos repetidamente una consulta de prueba del conjunto de consultas y la precisión de la predicción se calcula como el porcentaje de decisiones correctas, es decir, una consulta buena (mala) se predice como buena (mala). Es obvio que adivinar al azar llevará a una precisión del 50%. Tomemos el método WIG como ejemplo para ilustrar el proceso. Dos umbrales de WIG (uno para consultas de NP y otro para consultas basadas en contenido) se entrenan maximizando la precisión de predicción en los datos de entrenamiento. Cuando una consulta de prueba es etiquetada como del tipo NP (CB) por el clasificador de tipo de consulta, se predecirá que es buena solo si la puntuación WIG para esta consulta está por encima del umbral de NP (CB). Se seguirán procedimientos similares para otras técnicas de predicción. Ahora presentamos brevemente el clasificador de tipo de consulta automático utilizado en este artículo. Encontramos que la puntuación de robustez, aunque originalmente propuesta para la predicción de rendimiento, es un buen indicador de tipos de consultas. Observamos que, en promedio, las consultas basadas en contenido tienen una puntuación de robustez mucho más alta que las consultas de NP. Por ejemplo, la Figura 2 muestra las distribuciones de puntajes de robustez para consultas basadas en NP y en contenido. Según este hallazgo, el clasificador de puntuación de robustez adjuntará una etiqueta NP (CB) a la consulta si la puntuación de robustez para la consulta está por debajo (por encima) de un umbral entrenado a partir de los datos de entrenamiento. 0 0.5 1 1.5 2 2.5 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 NP Basado en contenido Figura 2: Distribución de puntuaciones de robustez para consultas NP y CB. Las consultas NP son los 252 temas NP del Terabyte Track de 2005. Las consultas basadas en contenido son los 150 títulos ad-hoc de las pistas Terabyte 2004-2006. Las distribuciones de probabilidad son estimadas por el método de estimación de densidad de Kernel. Estrategias Robustas WIG-1 WIG-2 WIG-3 Óptima p=0.6 0.565 0.624 0.665 0.684 0.701 P=0.4 0.567 0.633 0.654 0.673 0.696 Tabla 8: Comparación de la precisión de predicción para cinco estrategias en la situación de consulta mixta. Dos formas de muestrear una consulta de la piscina: (1) la consulta muestreada es basada en contenido con una probabilidad p=0.6 (es decir, la consulta es NP con una probabilidad de 0.4) (2) establecer la probabilidad p=0.4. Consideramos cinco estrategias en nuestros experimentos. En la primera estrategia (denominada robusta), utilizamos la puntuación de robustez para la predicción del rendimiento de la consulta con la ayuda de un clasificador de consultas perfecto que siempre mapea correctamente una consulta en una de las dos categorías (es decir, NP o CB). Esta estrategia representa el nivel de precisión de predicción que las técnicas actuales de predicción pueden lograr en una condición ideal en la que se conocen los tipos de consulta. En las tres estrategias siguientes, se adopta el método WIG para predecir el rendimiento. La diferencia entre los tres es que se utilizan tres clasificadores de consulta diferentes para cada estrategia: (1) el clasificador siempre clasifica una consulta en el tipo NP. (2) el <br>clasificador de Probabilidad de Densidad de Puntuación Robusta</br> es el clasificador de puntuación robusta mencionado anteriormente. (3) el clasificador es perfecto. Estas tres estrategias están designadas como WIG-1, WIG-2 y WIG-3 respectivamente. La razón por la que estamos interesados en WIG-1 se basa en los resultados de la sección 4.3.1. En la última estrategia (denominada Óptima) que sirve como un límite superior de lo bien que podemos hacer hasta ahora, hacemos pleno uso de nuestras técnicas de predicción para cada tipo de consulta asumiendo que un clasificador de consultas perfecto está disponible. Específicamente, combinamos linealmente WIG y QF para consultas basadas en contenido y WIG y FRC para consultas de NP. Los resultados de las cinco estrategias se muestran en la Tabla 8. Para cada estrategia, intentamos dos formas de muestrear una consulta del conjunto: (1) la consulta muestreada es CB con probabilidad p=0.6 (la consulta es NP con probabilidad 0.4) (2) establecer la probabilidad p=0.4. De la Tabla 8 podemos ver que en términos de precisión de predicción, WIG-2 (el método WIG con el clasificador de consultas automático) no solo es mejor que los dos primeros casos, sino que también está cerca de WIG-3 donde se asume un clasificador perfecto. Se observan algunas mejoras adicionales sobre WIG-3 cuando se combinan con otras técnicas de predicción. El mérito de WIG-2 es que proporciona una solución práctica para identificar automáticamente consultas con bajo rendimiento en un entorno de búsqueda web con tipos de consultas mixtos, lo cual representa considerables obstáculos para las técnicas de predicción tradicionales. CONCLUSIONES Y TRABAJOS FUTUROS Hasta donde sabemos, nuestro artículo es el primero en explorar a fondo la predicción del rendimiento de consultas en entornos de búsqueda web. Demostramos que nuestros modelos resultaron en una mayor precisión de predicción que las técnicas previamente publicadas no especialmente diseñadas para escenarios de búsqueda en la web. En este artículo, nos enfocamos en dos tipos de consultas en la búsqueda web: consultas basadas en contenido y consultas de búsqueda de Páginas-Nombradas (NP), correspondientes a la tarea de recuperación ad-hoc y la tarea de búsqueda de Páginas-Nombradas respectivamente. Para ambos tipos de consultas web, se demostró que nuestros modelos de predicción son sustancialmente más precisos que las técnicas actuales más avanzadas. Además, consideramos un caso más realista en el que no se dispone de información previa sobre los tipos de consultas. Demostramos que el método WIG es particularmente adecuado para esta situación. Considerando la adaptabilidad de WIG a una variedad de colecciones y tipos de consultas, uno de nuestros planes futuros es aplicar este método para predecir la preferencia del usuario por los resultados de búsqueda en datos realistas recopilados de un motor de búsqueda comercial. Además de la precisión, otro problema importante con el que las técnicas de predicción tienen que lidiar en un entorno web es la eficiencia. Afortunadamente, dado que el puntaje WIG se calcula solo sobre los términos y frases que aparecen en la consulta, este cálculo puede realizarse de manera muy eficiente con el soporte de un índice. Por otro lado, el cálculo de QF y FRC es relativamente menos eficiente ya que QF necesita recuperar toda la colección dos veces y FRC necesita clasificar repetidamente los documentos perturbados. Cómo mejorar la eficiencia de QF y FRC es nuestro trabajo futuro. Además, las técnicas de predicción propuestas en este documento tienen el potencial de mejorar el rendimiento de recuperación al combinarse con otras técnicas de RI. Por ejemplo, nuestras técnicas pueden ser incorporadas a técnicas populares de modificación de consultas como la expansión de consultas y la relajación de consultas. Guiados por la predicción del rendimiento, podemos tomar una mejor decisión sobre cuándo o cómo modificar las consultas para mejorar la efectividad de la recuperación. Nos gustaría llevar a cabo investigaciones en esta dirección en el futuro. AGRADECIMIENTOS Este trabajo fue apoyado en parte por el Centro de Recuperación de Información Inteligente, en parte por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el contrato número HR0011-06-C0023, y en parte por un premio de Google. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son del autor y no necesariamente reflejan las del patrocinador. Además, agradecemos a Donald Metzler por sus valiosos comentarios sobre este trabajo. 7. REFERENCIAS [1] Y. Zhou, W. B. Croft, Ranking Robustness: Un nuevo marco para predecir el rendimiento de consultas, en Actas de CIKM 2006. [2] D. Carmel, E. Yom-Tov, A. Darlow, D. Pelleg, ¿Qué hace que una consulta sea difícil?, en Actas de SIGIR 2006. [3] C. L. A. Clarke, F. Scholer, I. Soboroff, La pista Terabyte TREC 2005, En las Actas en Línea de TREC 2005. [4] B. Él y yo. Inferir el rendimiento de la consulta utilizando predictores de preretrieval. En actas de SPIRE 2004. [5] S. Tomlinson. Recuperación robusta de la web y terabytes con Hummingbird SearchServer en TREC 2004. En las Actas en Línea de TREC 2004. [6] S. Cronen-Townsend, Y. Zhou, W. B. Croft, Prediciendo el Rendimiento de Consultas, en las Actas de SIGIR 2002. [7] V. Vinay, I. J. Cox, N. Mill-Frayling, K. Wood, Sobre la Clasificación de la Efectividad del Buscador, en las Actas de SIGIR 2006. [8] D. Metzler, W. B. Croft, Un Modelo de Campo Aleatorio de Markov para Dependencias de Términos, en las Actas de SIGIR 2005. [9] D. Metzler, T. Strohman, Y. Zhou, W. B. Croft, Indri en TREC 2005: Terabyte Track, en las Actas en Línea de TREC 2004. [10] P. Ogilvie y J. Callan, Combinando representaciones de documentos para búsqueda de elementos conocidos, en las Actas de SIGIR 2003. [11] A. Berger, J. Lafferty, Recuperación de información como traducción estadística, en las Actas de SIGIR 1999. [12] Motor de búsqueda Indri: http://www.lemurproject.org/indri/ [13] I. J. Taneja: Sobre medidas de información generalizadas y sus aplicaciones, Avances en Electrónica y Física de Electrones, Academic Press (EE. UU.), 76, 1989, 327-413. [14] S. Cronen-Townsend, Y. Zhou y Croft, W. B., Un marco para la expansión selectiva de consultas, en Actas de CIKM 2004. [15] F. Song, W. B. Croft, Un modelo de lenguaje general para la recuperación de información, en Actas de SIGIR 1999. [16] Contacto por correo electrónico personal con Vishwa Vinay y nuestros propios experimentos [17] E. Yom-Tov, S. Fine, D. Carmel, A. Darlow, Aprendiendo a estimar la dificultad de la consulta incluyendo aplicaciones para la detección de contenido faltante y la recuperación de información distribuida, en Actas de SIGIR 2005. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "kl-divergence": {
            "translated_key": "divergencia KL",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Query Performance Prediction in Web Search Environments Yun Zhou and W. Bruce Croft Department of Computer Science University of Massachusetts, Amherst {yzhou, croft}@cs.umass.edu ABSTRACT Current prediction techniques, which are generally designed for content-based queries and are typically evaluated on relatively homogenous test collections of small sizes, face serious challenges in web search environments where collections are significantly more heterogeneous and different types of retrieval tasks exist.",
                "In this paper, we present three techniques to address these challenges.",
                "We focus on performance prediction for two types of queries in web search environments: content-based and Named-Page finding.",
                "Our evaluation is mainly performed on the GOV2 collection.",
                "In addition to evaluating our models for the two types of queries separately, we consider a more challenging and realistic situation that the two types of queries are mixed together without prior information on query types.",
                "To assist prediction under the mixed-query situation, a novel query classifier is adopted.",
                "Results show that our prediction of web query performance is substantially more accurate than the current stateof-the-art prediction techniques.",
                "Consequently, our paper provides a practical approach to performance prediction in realworld web settings.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval -Query formulation General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Query performance prediction has many applications in a variety of information retrieval (IR) areas such as improving retrieval consistency, query refinement, and distributed IR.",
                "The importance of this problem has been recognized by IR researchers and a number of new methods have been proposed for prediction recently [1, 2, 17].",
                "Most work on prediction has focused on the traditional ad-hoc retrieval task where query performance is measured according to topical relevance.",
                "These prediction models are evaluated on TREC document collections which typically consist of no more than one million relatively homogenous newswire articles.",
                "With the popularity and influence of the Web, prediction techniques that will work well for web-style queries are highly preferable.",
                "However, web search environments pose significant challenges to current prediction models that are mainly designed for traditional TREC settings.",
                "Here we outline some of these challenges.",
                "First, web collections, which are much larger than conventional TREC collections, include a variety of documents that are different in many aspects such as quality and style.",
                "Current prediction techniques can be vulnerable to these characteristics of web collections.",
                "For example, the reported prediction accuracy of the ranking robustness technique and the clarity technique on the GOV2 collection (a large web collection) is significantly worse compared to the other TREC collections [1].",
                "Similar prediction accuracy on the GOV2 collection using another technique is reported in [2], confirming the difficult of predicting query performance on a large web collection.",
                "Furthermore, web search goes beyond the scope of the ad-hoc retrieval task based on topical relevance.",
                "For example, the Named-Page (NP) finding task, which is a navigational task, is also popular in web retrieval.",
                "Query performance prediction for the NP task is still necessary since NP retrieval performance is far from perfect.",
                "In fact, according to the report on the NP task of the 2005 Terabyte Track [3], about 40% of the test queries perform poorly (no correct answer in the first 10 search results) even in the best run from the top group.",
                "To our knowledge, little research has explicitly addressed the problem of NP-query performance prediction.",
                "Current prediction models devised for content-based queries will be less effective for NP queries considering the fundamental differences between the two.",
                "Third, in real-world web search environments, user queries are usually a mixture of different types and prior knowledge about the type of each query is generally unavailable.",
                "The mixed-query situation raises new problems for query performance prediction.",
                "For instance, we may need to incorporate a query classifier into prediction models.",
                "Despite these problems, the ability to handle this situation is a crucial step towards turning query performance prediction from an interesting research topic into a practical tool for web retrieval.",
                "In this paper, we present three techniques to address the above challenges that current prediction models face in Web search environments.",
                "Our work focuses on query performance prediction for the content-based (ad-hoc) retrieval task and the name-page finding task in the context of web retrieval.",
                "Our first technique, called weighted information gain (WIG), makes use of both single term and term proximity features to estimate the quality of top retrieved documents for prediction.",
                "We find that WIG offers consistent prediction accuracy across various test collections and query types.",
                "Moreover, we demonstrate that good prediction accuracy can be achieved for the mixed-query situation by using WIG with the help of a query type classifier.",
                "Query feedback and first rank change, which are our second and third prediction techniques, perform well for content-based queries and NP queries respectively.",
                "Our main contributions include: (1) considerably improved prediction accuracy for web content-based queries over several state-of-the-art techniques. (2) new techniques for successfully predicting NP-query performance. (3) a practical and fully automatic solution to predicting mixed-query performance.",
                "In addition, one minor contribution is that we find that the robustness score [1], which was originally proposed for performance prediction, is helpful for query classification. 2.",
                "RELATED WORK As we mentioned in the introduction, a number of prediction techniques have been proposed recently that focus on contentbased queries in the topical relevance (ad-hoc) task.",
                "We know of no published work that addresses other types of queries such as NP queries, let alone a mixture of query types.",
                "Next we review some representative models.",
                "The major difficulty of performance prediction comes from the fact that many factors have an impact on retrieval performance.",
                "Each factor affects performance to a different degree and the overall effect is hard to predict accurately.",
                "Therefore, it is not surprising to notice that simple features, such as the frequency of query terms in the collection [4] and the average IDF of query terms [5], do not predict well.",
                "In fact, most of the successful techniques are based on measuring some characteristics of the retrieved document set to estimate topic difficulty.",
                "For example, the clarity score [6] measures the coherence of a list of documents by the <br>kl-divergence</br> between the query model and the collection model.",
                "The robustness score [1] quantifies another property of a ranked list: the robustness of the ranking in the presence of uncertainty.",
                "Carmel et al. [2] found that the distance measured by the Jensen-Shannon divergence between the retrieved document set and the collection is significantly correlated to average precision.",
                "Vinay et al. [7] proposed four measures to capture the geometry of the top retrieved documents for prediction.",
                "The most effective measure is the sensitivity to document perturbation, an idea somewhat similar to the robustness score.",
                "Unfortunately, their way of measuring the sensitivity does not perform equally well for short queries and prediction accuracy drops considerably when a state-of-the-art retrieval technique (like Okapi or a language modeling approach) is adopted for retrieval instead of the tf-idf weighting used in their paper [16].",
                "The difficulties of applying these models in web search environments have already been mentioned.",
                "In this paper, we mainly adopt the clarity score and the robustness score as our baselines.",
                "We experimentally show that the baselines, even after being carefully tuned, are inadequate for the web environment.",
                "One of our prediction models, WIG, is related to the Markov random field (MRF) model for information retrieval [8].",
                "The MRF model directly models term dependence and is found be to highly effective across a variety of test collections (particularly web collections) and retrieval tasks.",
                "This model is used to estimate the joint probability distribution over documents and queries, an important part of WIG.",
                "The superiority of WIG over other prediction techniques based on unigram features, which will be demonstrated later in our paper, coincides with that of MRF for retrieval.",
                "In other word, it is interesting to note that term dependence, when being modeled appropriately, can be helpful for both improving and predicting retrieval performance. 3.",
                "PREDICTION MODELS 3.1 Weighted Information Gain (WIG) This section introduces a weighted information gain approach that incorporates both single term and proximity features for predicting performance for both content-based and Named-Page (NP) finding queries.",
                "Given a set of queries Q={Qs} (s=1,2,..N) which includes all possible user queries and a set of documents D={Dt} (t=1,2…M), we assume that each query-document pair (Qs,Dt) is manually judged and will be put in a relevance list if Qs is found to be relevant to Dt.",
                "The joint probability P(Qs,Dt) over queries Q and documents D denotes the probability that pair (Qs,Dt) will be in the relevance list.",
                "Such assumptions are similar to those used in [8].",
                "Assuming that the user issues query Qi ∈Q and the retrieval results in response to Qi is a ranked list L of documents, we calculate the amount of information contained in P(Qs,Dt) with respect to Qi and L by Eq.1 which is a variant of entropy called the weighted entropy[13].",
                "The weights in Eq.1 are solely determined by Qi and L. )1(),(log),(),( , , ∑−= ts tststsLQ DQPDQweightDQH i In this paper, we choose the weights as follows: LindocumentsKtopthecontainsLTwhere otherwise LTDandisifK DQweight K Kt ts )( )2( ,0 )(,/1 ),( ⎩ ⎨ ⎧ ∈= = The cutoff rank K is a parameter in our model that will be discussed later.",
                "Accordingly, Eq.1 can be simplified as follows: )3(),(log 1 ),( )( , ∑∈ −= LTD titsLQ Kt i DQP K DQH Unfortunately, weighted entropy ),(, tsLQ DQH i computed by Eq.3, which represents the amount of information about how likely the top ranked documents in L would be relevant to query Qi on average, cannot be compared across different queries, making it inappropriate for directly predicting query performance.",
                "To mitigate this problem, we come up with a background distribution P(Qs,C) over Q and D by imagining that every document in D is replaced by the same special document C which represents average language usage.",
                "In this paper, C is created by concatenating every document in D. Roughly speaking, C is the collection (the document set) {Dt} without document boundaries.",
                "Similarly, weighted entropy ),(, CQH sLQi calculated by Eq.3 represents the amount of information about how likely an average document (represented by the whole collection) would be relevant to query Qi.",
                "Now we introduce our performance predictor WIG which is the weighted information gain [13] computed as the difference between ),(, tsLQ DQH i and ),(, CQH sLQi .Specifically, given query Qi, collection C and ranked list L of documents, WIG is calculated as follows: )4( ),( ),( log 1 ),( ),( log),( ),(),(),,( )(, ,, ∑∑ ∈ == −= LTD i ti ts s ts ts tsLQsLQi Kt ii CQP DQP KCQP DQP DQweight DQHCQHLCQWIG WIG computed by Eq.4 measures the change in information about the quality of retrieval (in response to query Qi) from an imaginary state that only an average document is retrieved to a posterior state that the actual search results are observed.",
                "We hypothesize that WIG is positively correlated with retrieval effectiveness because high quality retrieval should be much more effective than just returning the average document.",
                "The heart of this technique is how to estimate the joint distribution P(Qs,Dt).",
                "In the language modeling approach to IR, a variety of models can be applied readily to estimate this distribution.",
                "Although most of these models are based on the bagof-words assumption, recent work on modeling term dependence under the language modeling framework have shown consistent and significant improvements in retrieval effectiveness over bagof-words models.",
                "Inspired by the success of incorporating term proximity features into language models, we decide to adopt a good dependence model to estimate the probability P(Qs,Dt).",
                "The model we chose for this paper is Metzler and Crofts Markov Random Field (MRF) model, which has already demonstrated superiority over a number of collections and different retrieval tasks [8,9].",
                "According to the MRF model, log P(Qi, Dt) can be written as )5()|(loglog),(log )( 1 ∑∈ +−= iQF tti DPZDQP ξ ξ ξλ where Z1 is a constant that ensures that P(Qi, Dt) sums up to 1.",
                "F(Qi) consists of a set of features expanded from the original query Qi .",
                "For example, assuming that query Qi is talented student program, F(Qi) includes features like program and talented student.",
                "We consider two kinds of features: single term features T and proximity features P. Proximity features include exact phrase (#1) and unordered window (#uwN) features as described in [8].",
                "Note that F(Qi) is the union of T(Qi) and P(Qi).",
                "For more details on F(Qi) such as how to expand the original query Qi to F(Qi), we refer the reader to [8] and [9].",
                "P(ξ|Dt) denotes the probability that feature ξ will occur in Dt.",
                "More details on P(ξ|Dt) will be provided later in this section.",
                "The choice of λξ is somewhat different from that used in [8] since λξ plays a dual role in our model.",
                "The first role, which is the same as in [8], is to weight between single term and proximity features.",
                "The other role, which is specific to our prediction task, is to normalize the size of F(Qi).We found that the following weight strategy for λξ satisfies the above two roles and generalizes well on a variety of collections and query types. )6( )(, |)(| 1 )(, |)(| ⎪ ⎪ ⎩ ⎪ ⎪ ⎨ ⎧ ∈ − ∈ = i i T i i T QP QP QT QT ξ λ ξ λ λξ where |T(Qi)| and |P(Qi)| denote the number of single term and proximity features in F(Qi) respectively.",
                "The reason for choosing the square root function in the denominator of λξ is to penalize a feature set of large size appropriately, making WIG more comparable across queries of various lengths. λT is a fixed parameter and set to 0.8 according to [8] throughout this paper.",
                "Similarly, log P(Qi,C) can be written as: )7()|(loglog),(log )( 2 ∑∈ +−= iQF i CPZCQP ξ ξ ξλ When constant Z1 and Z2 are dropped, WIG computed in Eq.4 can be rewritten as follows by plugging in Eq.5 and Eq.7 : )8( )|( )|( log 1 ),,( )( )( ∑ ∑∈ ∈ = LTD QF t i Kt i CP DP K LCQWIG ξ ξ ξ ξ λ One of the advantages of WIG over other techniques is that it can handle well both content-based and NP queries.",
                "Based on the type (or the predicted type) of Qi, the calculation of WIG in Eq. 8 differs in two aspects: (1) how to estimate P(ξ|Dt) and P(ξ|C), and (2) how to choose K. For content-based queries, P(ξ|C) is estimated by the relative frequency of feature ξ in collection C as a whole.",
                "The estimation of P(ξ|Dt) is the same as in [8].",
                "Namely, we estimate P(ξ|Dt) by the relative frequency of feature ξ in Dt linearly smoothed with collection frequency P(ξ|C).",
                "K in Eq.8 is treated as a free parameter.",
                "Note that K is the only free parameter in the computation of WIG for content-based queries because all parameters involved in P(ξ|Dt) are assumed to be fixed by taking the suggested values in [8].",
                "Regarding NP queries, we make use of document structure to estimate P(ξ|Dt) and P(ξ|C) by the so-called mixture of language models proposed in [10] and incorporated into the MRF model for Named-Page finding retrieval in [9].",
                "The basic idea is that a document (collection) is divided into several fields such as the title field, the main-body field and the heading field.",
                "P(ξ|Dt) and P(ξ|C) are estimated by a linear combination of the language models from each field.",
                "Due to space constraints, we refer the reader to [9] for details.",
                "We adopt the exact same set of parameters as used in [9] for estimation.",
                "With regard to K in Eq.8, we set K to 1 because the Named-Page finding task heavily focuses on the first ranked document.",
                "Consequently, there are no free parameters in the computation of WIG for NP queries. 3.2 Query Feedback In this section, we introduce another technique called query feedback (QF) for prediction.",
                "Suppose that a user issues query Q to a retrieval system and a ranked list L of documents is returned.",
                "We view the retrieval system as a noisy channel.",
                "Specifically, we assume that the output of the channel is L and the input is Q.",
                "After going through the channel, Q becomes corrupted and is transformed to ranked list L. By thinking about the retrieval process this way, the problem of predicting retrieval effectiveness turns to the task of evaluating the quality of the channel.",
                "In other words, prediction becomes finding a way to measure the degree of corruption that arises when Q is transformed to L. As directly computing the degree of the corruption is difficult, we tackle this problem by approximation.",
                "Our main idea is that we measure to what extent information on Q can be recovered from L on the assumption that only L is observed.",
                "Specifically, we design a decoder that can accurately translate L back into new query Q and the similarity S between the original query Q and the new query Q is adopted as a performance predictor.",
                "This is a sketch of how the QF technique predicts query performance.",
                "Before filling in more details, we briefly discuss why this method would work.",
                "There is a relation between the similarity S defined above and retrieval performance.",
                "On the one hand, if the retrieval has strayed from the original sense of the query Q, the new query Q extracted from ranked list L in response to Q would be very different from the original query Q.",
                "On the other hand, a query distilled from a ranked list containing many relevant documents is likely to be similar to the original query.",
                "Further examples in support of the relation will be provided later.",
                "Next we detail how to build the decoder and how to measure the similarity S. In essence, the goal of the decoder is to compress ranked list L into a few informative terms that should represent the content of the top ranked documents in L. Our approach to this goal is to represent ranked list L by a language model (distribution over terms).",
                "Then terms are ranked by their contribution to the language models KL (Kullback-Leibler) divergence from the background collection model.",
                "Top ranked terms will be chosen to form the new query Q.",
                "This approach is similar to that used in Section 4.1 of [11].",
                "Specifically, we take three steps to compress ranked list L into query Q without referring to the original query. 1.",
                "We adopt the ranked list language model [14], to estimate a language model based on ranked list L. The model can be written as: )9()|()|()|( ∑∈ = LD LDPDwPLwP where w is any term, D is a document.",
                "P(D|L) is estimated by a linearly decreasing function of the rank of document D. 2.",
                "Each term in P(w|L) is ranked by the following <br>kl-divergence</br> contribution: )10( )|( )|( log)|( CwP LwP LwP where P(w|C) is the collection model estimated by the relative frequency of term w in collection C as a whole. 3.",
                "The top N ranked terms by Eq.10 form a weighted query Q={(wi,ti)} i=1,N. where wi denotes the i-th ranked term and weight ti is the <br>kl-divergence</br> contribution of wi in Eq. 10.",
                "Term cruise ship vessel sea passenger KL contribution 0.050 0.040 0.012 0.010 0.009 Table 1: top 5 terms compressed from the ranked list in response to query Cruise ship damage sea life Two representative examples, one for a poorly performing query Cruise ship damage sea life (TREC topic 719; average precision: 0.08) and the other for a high performing query prostate cancer treatments( TREC topic 710; average precision: 0.49), are shown in Table 1 and 2 respectively.",
                "These examples indicate how the similarity between the original and the new query correlates with retrieval performance.",
                "The parameter N in step 3 is set to 20 empirically and choosing a larger value of N is unnecessary since the weights after the top 20 are usually too small to make any difference.",
                "Term prostate cancer treatment men therapy KL contribution 0.177 0.140 0.028 0.025 0.020 Table 2: top 5 terms compressed from the ranked list in response to query prostate cancer treatments To measure the similarity between original query Q and new query Q, we first use Q to do retrieval on the same collection.",
                "A variant of the query likelihood model [15] is adopted for retrieval.",
                "Namely, documents are ranked by: )11()|()|( ),( ∑∈ = Qtw t i ii i DwPDQP where wi is a term in Q and ti is the associated weight.",
                "D is a document.",
                "Let L denote the new ranked list returned from the above retrieval.",
                "The similarity is measured by the overlap of documents in L and L. Specifically, the percentage of documents in the top K documents of L that are also present in the top K documents in L. the cutoff K is treated as a free parameter.",
                "We summarize here how the QF technique predicts performance given a query Q and the associated ranked list L. We first obtain a weighted query Q compressed from L by the above three steps.",
                "Then we use Q to perform retrieval and the new ranked list is L. The overlap of documents in L and L is used for prediction. 3.3 First Rank Change (FRC) In this section, we propose a method called the first rank change (FRC) for performance prediction for NP queries.",
                "This method is derived from the ranking robustness technique [1] that is mainly designed for content-based queries.",
                "When directly applied to NP queries, the robustness technique will be less effective because it takes the top ranked documents as a whole into account while NP queries usually have only one single relevant document.",
                "Instead, our technique focuses on the first rank document while the main idea of the robustness method remains.",
                "Specifically, the pseudocode for computing FRC is shown in figure 1.",
                "Input: (1) ranked list L={Di} where i=1,100.",
                "Di denotes the i-th ranked document. (2) query Q 1 initialize: (1) set the number of trials J=100000 (2) counter c=0; 2 for i=1 to J 3 Perturb every document in L, let the outcome be a set F={Di} where Di denotes the perturbed version of Di. 4 Do retrieval with query Q on set F 5 c=c+1 if and only if D1 is ranked first in step 4 6 end of for 7 return the ratio c/J Figure 1: pseudo-code for computing FRC FRC approximates the probability that the first ranked document in the original list L will remain ranked first even after the documents are perturbed.",
                "The higher the probability is, the more confidence we have in the first ranked document.",
                "On the other hand, in the extreme case of a random ranking, the probability would be as low as 0.5.",
                "We expect that FRC has a positive association with NP query performance.",
                "We adopt [1] to implement the document perturbation step (step 4 in Fig.1) using Poisson distributions.",
                "For more details, we refer the reader to [1]. 4.",
                "EVALUATION We now present the results of predicting query performance by our models.",
                "Three state-of-the-art techniques are adopted as our baselines.",
                "We evaluate our techniques across a variety of Web retrieval settings.",
                "As mentioned before, we consider two types of queries, that is, content-based (CB) queries and Named-Page(NP) finding queries.",
                "First, suppose that the query types are known.",
                "We investigate the correlation between the predicted retrieval performance and the actual performance for both types of queries separately.",
                "Results show that our methods yield considerable improvements over the baselines.",
                "We then consider a more challenging scenario where no prior information on query types is available.",
                "Two sub-cases are considered.",
                "In the first one, there exists only one type of query but the actual type is unknown.",
                "We assume a mixture of the two query types in the second case.",
                "We demonstrate that our models achieve good accuracy under this demanding scenario, making prediction practical in a real-world Web search environment. 4.1 Experimental Setup Our evaluation focuses on the GOV2 collection which contains about 25 million documents crawled from web sites in the .gov domain during 2004 [3].",
                "We create two kinds of data set for CB queries and NP queries respectively.",
                "For the CB type, we use the ad-hoc topics of the Terabyte Tracks of 2004, 2005 and 2006 and name them TB04-adhoc, TB05-adhoc and TB06-adhoc respectively.",
                "In addition, we also use the ad-hoc topics of the 2004 Robust Track (RT04) to test the adaptability of our techniques to a non-Web environment.",
                "For NP queries, we use the Named-Page finding topics of the Terabyte Tracks of 2005 and 2006 and we name them TB05-NP and TB06-NP respectively.",
                "All queries used in our experiments are titles of TREC topics as we center on web retrieval.",
                "Table 3 summarizes the above data sets.",
                "Name Collection Topic Number Query Type TB04-adhoc GOV2 701-750 CB TB05-adhoc GOV2 751-800 CB TB06-adhoc GOV2 801-850 CB RT04 Disk 4+5 (minus CR) 301-450;601700 CB TB05-NP GOV2 NP601-NP872 NP TB06-NP GOV2 NP901-NP1081 NP Table 3: Summary of test collections and topics Retrieval performance of individual content-based and NP queries is measured by the average precision and reciprocal rank of the first correct answer respectively.",
                "We make use of the Markov Random field model for both ad-hoc and Named-Page finding retrieval.",
                "We adopt the same setting of retrieval parameters used in [8,9].",
                "The Indri search engine [12] is used for all of our experiments.",
                "Though not reported here, we also tried the query likelihood model for ad-hoc retrieval and found that the results change little because of the very high correlation between the query performances obtained by the two retrieval models (0.96 measured by Pearsons coefficient). 4.2 Known Query Types Suppose that query types are known.",
                "We treat each type of query separately and measure the correlation with average precision (or the reciprocal rank in the case of NP queries).",
                "We adopt the Pearsons correlation test which reflects the degree of linear relationship between the predicted and the actual retrieval performance. 4.2.1 Content-based Queries Methods Clarity Robust JSD WIG QF WIG +QF TB04+0 5 adhoc 0.333 0.317 0.362 0.574 0.480 0.637 TB06 adhoc 0.076 0.294 N/A 0.464 0.422 0.511 Table 4: Pearsons correlation coefficients for correlation with average precision on the Terabyte Tracks (ad-hoc) for clarity score, robustness score, the JSD-based method(we directly cites the score reported in [2]), WIG, query feedback(QF) and a linear combination of WIG and QF.",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Table 4 shows the correlation with average precision on two data sets: one is a combination of TB04-adhoc and TB05-adhoc(100 topics in total) and the other is TB06-adhoc (50 topics).",
                "The reason that we put TB04-adhoc and TB05-adhoc together is to make our results comparable to [2].",
                "Our baselines are the clarity score (clarity) [6],the robustness score (robust)[1] and the JSDbased method (JSD) [2].",
                "For the clarity and robustness score, we have tried different parameter settings and report the highest correlation coefficients we have found.",
                "We directly cite the result of the JSD-based method reported in [2].",
                "The table also shows the results for the Weighted Information Gain (WIG) method and the Query Feedback (QF) method for predicting content-based queries.",
                "As we described in the previous section, both WIG and QF have one free parameter to set, that is, the cutoff rank K. We train the parameter on one dataset and test on the other.",
                "When combining WIG and QF, a simple linear combination is used and the combination weight is learned from the training data set.",
                "From these results, we can see that our methods are considerably more accurate compared to the baselines.",
                "We also observe that further improvements are obtained from the combination of WIG and QF, suggesting that they measure different properties of the retrieval process that relate to performance.",
                "We discover that our methods generalize well on TB06-adhoc while the correlation for the clarity score with retrieval performance on this data set is considerably worse.",
                "Further investigation shows that the mean average precision of TB06-adhoc is 0.342 and is about 10% better than that of the first data set.",
                "While the other three methods typically consider the top 100 or less documents given a ranked list, the clarity method usually needs the top 500 or more documents to adequately measure the coherence of a ranked list.",
                "Higher mean average precision makes ranked lists retrieved by different queries more similar in terms of coherence at the level of top 500 documents.",
                "We believe that this is the main reason for the low accuracy of the clarity score on the second data set.",
                "Though this paper focuses on a Web search environment, it is desirable that our techniques will work consistently well in other situations.",
                "To this end, we examine the effectiveness of our techniques on the Robust 2004 Track.",
                "For our methods, we evenly divide all of the test queries into five groups and perform five-fold cross validation.",
                "Each time we use one group for training and the remaining four groups for testing.",
                "We make use of all of the queries for our two baselines, that is, the clarity score and the robustness score.",
                "The parameters for our baselines are the same as those used in [1].The results shown in Table 5 demonstrate that the prediction accuracy of our methods is on a par with that of the two strong baselines.",
                "Clarity Robust WIG QF 0.464 0.539 0.468 0.464 Table 5: Comparison of Pearsons correlation coefficients on the 2004 Robust Track for clarity score, robustness score, WIG and query feedback (QF).",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Furthermore, we examine the prediction sensitivity of our methods to the cutoff rank K. With respect to WIG, it is quite robust to K on the Terabyte Tracks (2004-2006) while it prefers a small value of K like 5 on the 2004 Robust Track.",
                "In other words, a small value of K is a nearly-optimal choice for both kinds of tracks.",
                "Considering the fact that all other parameters involved in WIG are fixed and consequently the same for the two cases, this means WIG can achieve nearly-optimal prediction accuracy in two considerably different situations with exactly the same parameter setting.",
                "Regarding QF, it prefers a larger value of K such as 100 on the Terabyte Tracks and a smaller value of K such as 25 on the 2004 Robust Track. 4.2.2 NP Queries We adopt WIG and first rank change (FRC) for predicting NPquery performance.",
                "We also try a linear combination of the two as in the previous section.",
                "The combination weight is obtained from the other data set.",
                "We use the correlation with the reciprocal ranks measured by the Pearsons correlation test to evaluate prediction quality.",
                "The results are presented in Table 6.",
                "Again, our baselines are the clarity score and the robustness score.",
                "To make a fair comparison, we tune the clarity score in different ways.",
                "We found that using the first ranked document to build the query model yields the best prediction accuracy.",
                "We also attempted to utilize document structure by using the mixture of language models mentioned in section 3.1.",
                "Little improvement was obtained.",
                "The correlation coefficients for the clarity score reported in Table 6 are the best we have found.",
                "As we can see, our methods considerably outperform the clarity score technique on both of the runs.",
                "This confirms our intuition that the use of a coherence-based measure like the clarity score is inappropriate for NP queries.",
                "Methods Clarity Robust.",
                "WIG FRC WIG+FRC TB05-NP 0.150 -0.370 0.458 0.440 0.525 TB06-NP 0.112 -0.160 0.478 0.386 0.515 Table 6: Pearsons correlation coefficients for correlation with reciprocal ranks on the Terabyte Tracks (NP) for clarity score, robustness score, WIG, the first rank change (FRC) and a linear combination of WIG and FRC.",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Regarding the robustness score, we also tune the parameters and report the best we have found.",
                "We observe an interesting and surprising negative correlation with reciprocal ranks.",
                "We explain this finding briefly.",
                "A high robustness score means that a number of top ranked documents in the original ranked list are still highly ranked after perturbing the documents.",
                "The existence of such documents is a good sign of high performance for content-based queries as these queries usually contain a number of relevant documents [1].",
                "However, with regard to NP queries, one fundamental difference is that there is only one relevant document for each query.",
                "The existence of such documents can confuse the ranking function and lead to low retrieval performance.",
                "Although the negative correlation with retrieval performance exists, the strength of the correlation is weaker and less consistent compared to our methods as shown in Table 6.",
                "Based on the above analysis, we can see that current prediction techniques like clarity score and robustness score that are mainly designed for content-based queries face significant challenges and are inadequate to deal with NP queries.",
                "Our two techniques proposed for NP queries consistently demonstrate good prediction accuracy, displaying initial success in solving the problem of predicting performance for NP queries.",
                "Another point we want to stress is that the WIG method works well for both types of queries, a desirable property that most prediction techniques lack. 4.3 Unknown Query Types In this section, we run two kinds of experiments without access to query type labels.",
                "First, we assume that only one type of query exists but the type is unknown.",
                "Second, we experiment on a mixture of content-based and NP queries.",
                "The following two subsections will report results for the two conditions respectively. 4.3.1 Only One Type exists We assume that all queries are of the same type, that is, they are either NP queries or content-based queries.",
                "We choose WIG to deal with this case because it shows good prediction accuracy for both types of queries in the previous section.",
                "We consider two cases: (1) CB: all 150 title queries from the ad-hoc task of the Terabyte Tracks 2004-2006 (2)NP: all 433 NP queries from the named page finding task of the Terabyte Tracks 2005 and 2006.",
                "We take a simple strategy by labeling all of the queries in each case as the same type (either NP or CB) regardless of their actual type.",
                "The computation of WIG will be based on the labeled query type instead of the actual type.",
                "There are four possibilities with respect to the relation between the actual type and the labeled type.",
                "The correlation with retrieval performance under the four possibilities is presented in Table 7.",
                "For example, the value 0.445 at the intersection between the second row and the third column shows the Pearsons correlation coefficient for correlation with average precision when the content-based queries are incorrectly labeled as the NP type.",
                "Based on these results, we recommend treating all queries as the NP type when only one query type exists and accurate query classification is not feasible, considering the risk that a large loss of accuracy will occur if NP queries are incorrectly labeled as content-based queries.",
                "These results also demonstrate the strong adaptability of WIG to different query types.",
                "CB (labeled) NP (labeled) CB (actual) 0.536 0.445 NP (actual) 0.174 0.467 Table 7: Comparison of Pearsons correlation coefficients for correlation with retrieval performance under four possibilities on the Terabyte Tracks (NP).",
                "Bold cases mean the results are statistically significant at the 0.01 level. 4.3.2 A mixture of contented-based and NP queries A mixture of the two types of queries is a more realistic situation that a Web search engine will meet.",
                "We evaluate prediction accuracy by how accurately poorly-performing queries can be identified by the prediction method assuming that actual query types are unknown (but we can predict query types).",
                "This is a challenging task because both the predicted and actual performance for one type of query can be incomparable to that for the other type.",
                "Next we discuss how to implement our evaluation.",
                "We create a query pool which consists of all of the 150 ad-hoc title queries from Terabyte Track 2004-2006 and all of the 433 NP queries from Terabyte Track 2005&2006.",
                "We divide the queries in the pool into classes: good (better than 50% of the queries of the same type in terms of retrieval performance) and bad (otherwise).",
                "According to these standards, a NP query with the reciprocal rank above 0.2 or a content-based query with the average precision above 0.315 will be considered as good.",
                "Then, each time we randomly select one query Q from the pool with probability p that Q is contented-based.",
                "The remaining queries are used as training data.",
                "We first decide the type of query Q according to a query classifier.",
                "Namely, the query classifier tells us whether query Q is NP or content-based.",
                "Based on the predicted query type and the score computed for query Q by a prediction technique, a binary decision is made about whether query Q is good or bad by comparing to the score threshold of the predicted query type obtained from the training data.",
                "Prediction accuracy is measured by the accuracy of the binary decision.",
                "In our implementation, we repeatedly take a test query from the query pool and prediction accuracy is computed as the percentage of correct decisions, that is, a good(bad) query is predicted to be good (bad).",
                "It is obvious that random guessing will lead to 50% accuracy.",
                "Let us take the WIG method for example to illustrate the process.",
                "Two WIG thresholds (one for NP queries and the other for content-based queries) are trained by maximizing the prediction accuracy on the training data.",
                "When a test query is labeled as the NP (CB) type by the query type classifier, it will be predicted to be good if and only if the WIG score for this query is above the NP (CB) threshold.",
                "Similar procedures will be taken for other prediction techniques.",
                "Now we briefly introduce the automatic query type classifier used in this paper.",
                "We find that the robustness score, though originally proposed for performance prediction, is a good indicator of query types.",
                "We find that on average content-based queries have a much higher robustness score than NP queries.",
                "For example, Figure 2 shows the distributions of robustness scores for NP and content-based queries.",
                "According to this finding, the robustness score classifier will attach a NP (CB) label to the query if the robustness score for the query is below (above) a threshold trained from the training data. 0 0.5 1 1.5 2 2.5 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 NP Content-based Figure 2: Distribution of robustness scores for NP and CB queries.",
                "The NP queries are the 252 NP topics from the 2005 Terabyte Track.",
                "The content-based queries are the 150 ad-hoc title from the Terabyte Tracks 2004-2006.",
                "The probability distributions are estimated by the Kernel density estimation method.",
                "Strategies Robust WIG-1 WIG-2 WIG-3 Optimal p=0.6 0.565 0.624 0.665 0.684 0.701 P=0.4 0.567 0.633 0.654 0.673 0.696 Table 8: Comparison of prediction accuracy for five strategies in the mixed-query situation.",
                "Two ways to sample a query from the pool: (1) the sampled query is content-based with the probability p=0.6. (that is, the query is NP with probability 0.4 ) (2) set the probability p=0.4.",
                "We consider five strategies in our experiments.",
                "In the first strategy (denoted by robust), we use the robustness score for query performance prediction with the help of a perfect query classifier that always correctly map a query into one of the two categories (that is, NP or CB).",
                "This strategy represents the level of prediction accuracy that current prediction techniques can achieve in an ideal condition that query types are known.",
                "In the next following three strategies, the WIG method is adopted for performance prediction.",
                "The difference among the three is that three different query classifiers are used for each strategy: (1) the classifier always classifies a query into the NP type. (2) the Robustness Score ProbabilityDensity classifier is the robust score classifier mentioned above. (3) the classifier is a perfect one.",
                "These three strategies are denoted by WIG-1, WIG-2 and WIG-3 respectively.",
                "The reason we are interested in WIG-1 is based on the results from section 4.3.1.",
                "In the last strategy (denoted by Optimal) which serves as an upper bound on how well we can do so far, we fully make use of our prediction techniques for each query type assuming a perfect query classifier is available.",
                "Specifically, we linearly combine WIG and QF for content-based queries and WIG and FRC for NP queries.",
                "The results for the five strategies are shown in Table 8.",
                "For each strategy, we try two ways to sample a query from the pool: (1) the sampled query is CB with probability p=0.6. (the query is NP with probability 0.4) (2) set the probability p=0.4.",
                "From Table 8 We can see that in terms of prediction accuracy WIG-2 (the WIG method with the automatic query classifier) is not only better than the first two cases, but also is close to WIG-3 where a perfect classifier is assumed.",
                "Some further improvements over WIG-3 are observed when combined with other prediction techniques.",
                "The merit of WIG-2 is that it provides a practical solution to automatically identifying poorly performing queries in a Web search environment with mixed query types, which poses considerable obstacles to traditional prediction techniques. 5.",
                "CONCLUSIONS AND FUTURE WORK To our knowledge, our paper is the first to thoroughly explore prediction of query performance in web search environments.",
                "We demonstrated that our models resulted in higher prediction accuracy than previously published techniques not specially devised for web search scenarios.",
                "In this paper, we focus on two types of queries in web search: content-based and Named-Page (NP) finding queries, corresponding to the ad-hoc retrieval task and the Named-Page finding task respectively.",
                "For both types of web queries, our prediction models were shown to be substantially more accurate than the current state-of-the-art techniques.",
                "Furthermore, we considered a more realistic case that no prior information on query types is available.",
                "We demonstrated that the WIG method is particularly suitable for this situation.",
                "Considering the adaptability of WIG to a range of collections and query types, one of our future plans is to apply this method to predict user preference of search results on realistic data collected from a commercial search engine.",
                "Other than accuracy, another major issue that prediction techniques have to deal with in a Web environment is efficiency.",
                "Fortunately, since the WIG score is computed just over the terms and the phrases that appear in the query, this calculation can be made very efficient with the support of index.",
                "On the other hand, the computation of QF and FRC is relatively less efficient since QF needs to retrieve the whole collection twice and FRC needs to repeatedly rank the perturbed documents.",
                "How to improve the efficiency of QF and FRC is our future work.",
                "In addition, the prediction techniques proposed in this paper have the potential of improving retrieval performance by combining with other IR techniques.",
                "For example, our techniques can be incorporated to popular query modification techniques such as query expansion and query relaxation.",
                "Guided by performance prediction, we can make a better decision on when to or how to modify queries to enhance retrieval effectiveness.",
                "We would like to carry out research in this direction in the future. 6.",
                "ACKNOWLEGMENTS This work was supported in part by the Center for Intelligent Information Retrieval, in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C0023, and in part by an award from Google.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor.",
                "In addition, we thank Donald Metzler for his valuable comments on this work. 7.",
                "REFERENCES [1] Y. Zhou ,W. B. Croft ,Ranking Robustness: A Novel Framework to Predict Query Performance, in Proceedings of CIKM 2006. [2] D.Carmel, E.Yom-Tov, A.Darlow,D.Pelleg, What Makes a Query Difficult?, in Proceedings of SIGIR 2006. [3] C.L.A.",
                "Clarke, F. Scholer, I.Soboroff, The TREC 2005 Terabyte Track, In the Online Proceedings of 2005 TREC. [4] B.",
                "He and I.Ounis.",
                "Inferring query performance using preretrieval predictors.",
                "In proceedings of the SPIRE 2004. [5] S. Tomlinson.",
                "Robust, Web and Terabyte Retrieval with Hummingbird SearchServer at TREC 2004.",
                "In the Online Proceedings of 2004 TREC. [6] S. Cronen-Townsend, Y. Zhou, W. B. Croft, Predicting Query Performance, in Proceedings of SIGIR 2002. [7] V.Vinay, I.J.Cox, N.Mill-Frayling,K.Wood, On Ranking the Effectiveness of Searcher, in Proceedings of SIGIR 2006. [8] D.Metzler, W.B.Croft, A Markov Random Filed Model for Term Dependencies, in Proceedings of SIGIR 2005. [9] D.Metzler, T.Strohman,Y.Zhou,W.B.Croft, Indri at TREC 2005: Terabyte Track, In the Online Proceedings of 2004 TREC. [10] P. Ogilvie and J. Callan, Combining document representations for known-item search, in Proceedings of SIGIR 2003. [11] A.Berger, J.Lafferty, Information retrieval as statistical translation, in Proceedings of SIGIR 1999. [12] Indri search engine : http://www.lemurproject.org/indri/ [13] I.J.",
                "Taneja: On Generalized Information Measures and Their Applications, Advances in Electronics and Electron Physics, Academic Press (USA), 76, 1989, 327-413. [14] S.Cronen-Townsend, Y. Zhou and Croft, W. B. , A Framework for Selective Query Expansion, in Proceedings of CIKM 2004. [15] F.Song, W.B.Croft, A general language model for information retrieval, in Proceedings of SIGIR 1999. [16] Personal email contact with Vishwa Vinay and our own experiments [17] E.Yom-Tov, S.Fine, D.Carmel, A.Darlow, Learning to Estimate Query Difficulty Including Applications to Missing Content Detection and Distributed Information retrieval, in Proceedings of SIGIR 2005"
            ],
            "original_annotated_samples": [
                "For example, the clarity score [6] measures the coherence of a list of documents by the <br>kl-divergence</br> between the query model and the collection model.",
                "Each term in P(w|L) is ranked by the following <br>kl-divergence</br> contribution: )10( )|( )|( log)|( CwP LwP LwP where P(w|C) is the collection model estimated by the relative frequency of term w in collection C as a whole. 3.",
                "The top N ranked terms by Eq.10 form a weighted query Q={(wi,ti)} i=1,N. where wi denotes the i-th ranked term and weight ti is the <br>kl-divergence</br> contribution of wi in Eq. 10."
            ],
            "translated_annotated_samples": [
                "Por ejemplo, la puntuación de claridad [6] mide la coherencia de una lista de documentos mediante la <br>divergencia KL</br> entre el modelo de consulta y el modelo de colección.",
                "Cada término en P(w|L) está clasificado por la siguiente contribución de <br>divergencia de Kullback-Leibler</br>: )10( )|( )|( log)|( CwP LwP LwP donde P(w|C) es el modelo de colección estimado por la frecuencia relativa del término w en la colección C en su totalidad. 3.",
                "Los términos N principales clasificados por Eq.10 forman una consulta ponderada Q={(wi,ti)} i=1,N, donde wi denota el término clasificado en la posición i y el peso ti es la contribución de la divergencia KL de wi en Eq. 10."
            ],
            "translated_text": "Predicción del rendimiento de consultas en entornos de búsqueda web Yun Zhou y W. Bruce Croft Departamento de Ciencias de la Computación Universidad de Massachusetts, Amherst {yzhou, croft}@cs.umass.edu RESUMEN Las técnicas actuales de predicción, generalmente diseñadas para consultas basadas en contenido y evaluadas típicamente en colecciones de prueba relativamente homogéneas de tamaños pequeños, enfrentan serios desafíos en entornos de búsqueda web donde las colecciones son significativamente más heterogéneas y existen diferentes tipos de tareas de recuperación. En este artículo, presentamos tres técnicas para abordar estos desafíos. Nos enfocamos en la predicción de rendimiento para dos tipos de consultas en entornos de búsqueda web: basadas en contenido y búsqueda de páginas por nombre. Nuestra evaluación se realiza principalmente en la colección GOV2. Además de evaluar nuestros modelos para los dos tipos de consultas por separado, consideramos una situación más desafiante y realista en la que los dos tipos de consultas se mezclan sin información previa sobre los tipos de consulta. Para ayudar en la predicción bajo la situación de consulta mixta, se adopta un clasificador de consultas novedoso. Los resultados muestran que nuestra predicción del rendimiento de la consulta web es sustancialmente más precisa que las técnicas de predicción actuales más avanzadas. Por consiguiente, nuestro artículo proporciona un enfoque práctico para predecir el rendimiento en entornos web del mundo real. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Formulación de Consultas Términos Generales Algoritmos, Experimentación, Teoría 1. La predicción del rendimiento de la consulta tiene muchas aplicaciones en una variedad de áreas de recuperación de información (IR), como mejorar la consistencia de la recuperación, el refinamiento de la consulta y la IR distribuida. La importancia de este problema ha sido reconocida por los investigadores en Recuperación de Información y recientemente se han propuesto varios métodos nuevos para la predicción [1, 2, 17]. La mayoría de los trabajos sobre predicción se han centrado en la tarea tradicional de recuperación ad-hoc, donde el rendimiento de la consulta se mide según la relevancia temática. Estos modelos de predicción se evalúan en colecciones de documentos de TREC que suelen consistir en no más de un millón de artículos de agencias de noticias relativamente homogéneos. Con la popularidad e influencia de la Web, las técnicas de predicción que funcionen bien para consultas de estilo web son altamente preferibles. Sin embargo, los entornos de búsqueda web plantean desafíos significativos para los modelos de predicción actuales que están principalmente diseñados para entornos tradicionales de TREC. Aquí delineamos algunos de estos desafíos. Primero, las colecciones web, que son mucho más grandes que las colecciones TREC convencionales, incluyen una variedad de documentos que son diferentes en muchos aspectos, como la calidad y el estilo. Las técnicas actuales de predicción pueden ser vulnerables a estas características de las colecciones web. Por ejemplo, la precisión de predicción reportada de la técnica de robustez de clasificación y la técnica de claridad en la colección GOV2 (una colección web grande) es significativamente peor en comparación con las otras colecciones de TREC [1]. En [2] se informa de una precisión de predicción similar en la colección GOV2 utilizando otra técnica, confirmando la dificultad de predecir el rendimiento de consultas en una gran colección web. Además, la búsqueda web va más allá del alcance de la tarea de recuperación ad hoc basada en la relevancia temática. Por ejemplo, la tarea de encontrar páginas nombradas (NP), que es una tarea de navegación, también es popular en la recuperación web. La predicción del rendimiento de la consulta para la tarea de NP sigue siendo necesaria, ya que el rendimiento de recuperación de NP está lejos de ser perfecto. De hecho, según el informe sobre la tarea NP del Terabyte Track de 2005 [3], aproximadamente el 40% de las consultas de prueba tienen un rendimiento deficiente (sin respuesta correcta en los primeros 10 resultados de búsqueda) incluso en la mejor ejecución del grupo líder. Hasta donde sabemos, poca investigación ha abordado explícitamente el problema de la predicción del rendimiento de consultas de NP. Los modelos de predicción actuales diseñados para consultas basadas en contenido serán menos efectivos para consultas de NP considerando las diferencias fundamentales entre ambas. En tercer lugar, en entornos de búsqueda web del mundo real, las consultas de los usuarios suelen ser una mezcla de diferentes tipos y, por lo general, no se dispone de información previa sobre el tipo de cada consulta. La situación de consulta mixta plantea nuevos problemas para la predicción del rendimiento de la consulta. Por ejemplo, es posible que necesitemos incorporar un clasificador de consultas en los modelos de predicción. A pesar de estos problemas, la capacidad para manejar esta situación es un paso crucial hacia convertir la predicción del rendimiento de consultas de un tema de investigación interesante en una herramienta práctica para la recuperación web. En este artículo, presentamos tres técnicas para abordar los desafíos mencionados que enfrentan los modelos de predicción actuales en entornos de búsqueda web. Nuestro trabajo se centra en la predicción del rendimiento de consultas para la tarea de recuperación basada en contenido (ad-hoc) y la tarea de encontrar páginas por nombre en el contexto de la recuperación web. Nuestra primera técnica, llamada ganancia de información ponderada (WIG), utiliza tanto características de términos individuales como de proximidad de términos para estimar la calidad de los documentos recuperados para la predicción. Observamos que WIG ofrece una precisión de predicción consistente en diversas colecciones de pruebas y tipos de consultas. Además, demostramos que se puede lograr una buena precisión de predicción para la situación de consulta mixta utilizando WIG con la ayuda de un clasificador de tipo de consulta. La retroalimentación de consultas y el cambio de rango inicial, que son nuestras técnicas de predicción segunda y tercera, funcionan bien para consultas basadas en contenido y consultas de NP respectivamente. Nuestras principales contribuciones incluyen: (1) una precisión de predicción considerablemente mejorada para consultas basadas en contenido web en comparación con varias técnicas de vanguardia. (2) nuevas técnicas para predecir con éxito el rendimiento de consultas NP. (3) una solución práctica y totalmente automática para predecir el rendimiento de consultas mixtas. Además, una contribución menor es que encontramos que el puntaje de robustez [1], que originalmente se propuso para la predicción de rendimiento, es útil para la clasificación de consultas. 2. TRABAJO RELACIONADO Como mencionamos en la introducción, recientemente se han propuesto varias técnicas de predicción que se centran en consultas basadas en contenido en la tarea de relevancia temática (ad-hoc). No conocemos ningún trabajo publicado que aborde otros tipos de consultas como las consultas NP, y mucho menos una mezcla de tipos de consultas. A continuación revisamos algunos modelos representativos. La principal dificultad de la predicción del rendimiento proviene del hecho de que muchos factores tienen un impacto en el rendimiento de recuperación. Cada factor afecta el rendimiento en diferente medida y el efecto general es difícil de predecir con precisión. Por lo tanto, no es sorprendente notar que características simples, como la frecuencia de los términos de consulta en la colección [4] y la IDF promedio de los términos de consulta [5], no predicen bien. De hecho, la mayoría de las técnicas exitosas se basan en medir algunas características del conjunto de documentos recuperados para estimar la dificultad del tema. Por ejemplo, la puntuación de claridad [6] mide la coherencia de una lista de documentos mediante la <br>divergencia KL</br> entre el modelo de consulta y el modelo de colección. El puntaje de robustez [1] cuantifica otra propiedad de una lista clasificada: la robustez de la clasificación en presencia de incertidumbre. Carmel et al. [2] encontraron que la distancia medida por la divergencia de Jensen-Shannon entre el conjunto de documentos recuperados y la colección está significativamente correlacionada con la precisión promedio. Vinay et al. [7] propusieron cuatro medidas para capturar la geometría de los documentos mejor clasificados para la predicción. La medida más efectiva es la sensibilidad a la perturbación del documento, una idea algo similar a la puntuación de robustez. Desafortunadamente, su forma de medir la sensibilidad no funciona igual de bien para consultas cortas y la precisión de la predicción disminuye considerablemente cuando se adopta una técnica de recuperación de vanguardia (como Okapi o un enfoque de modelado de lenguaje) en lugar del peso tf-idf utilizado en su artículo [16]. Las dificultades de aplicar estos modelos en entornos de búsqueda web ya han sido mencionadas. En este documento, principalmente adoptamos la puntuación de claridad y la puntuación de robustez como nuestras líneas base. Experimentalmente demostramos que las líneas base, incluso después de ser ajustadas cuidadosamente, son inadecuadas para el entorno web. Uno de nuestros modelos de predicción, WIG, está relacionado con el modelo de campo aleatorio de Markov (MRF) para la recuperación de información [8]. El modelo MRF modela directamente la dependencia de términos y se ha demostrado ser altamente efectivo en una variedad de colecciones de pruebas (especialmente colecciones web) y tareas de recuperación. Este modelo se utiliza para estimar la distribución conjunta de probabilidad sobre documentos y consultas, una parte importante de WIG. La superioridad de WIG sobre otras técnicas de predicción basadas en características unigramas, que se demostrará más adelante en nuestro artículo, coincide con la de MRF para recuperación. En otras palabras, es interesante notar que la dependencia de términos, cuando se modela adecuadamente, puede ser útil tanto para mejorar como para predecir el rendimiento de recuperación. 3. Modelos de predicción 3.1 Ganancia de Información Ponderada (WIG) Esta sección introduce un enfoque de ganancia de información ponderada que incorpora tanto características de términos únicos como de proximidad para predecir el rendimiento tanto en consultas basadas en contenido como en consultas de búsqueda de Páginas Nombradas (NP). Dado un conjunto de consultas Q={Qs} (s=1,2,..N) que incluye todas las posibles consultas de usuario y un conjunto de documentos D={Dt} (t=1,2…M), asumimos que cada par consulta-documento (Qs,Dt) es evaluado manualmente y se incluirá en una lista de relevancia si se determina que Qs es relevante para Dt. La probabilidad conjunta P(Qs,Dt) sobre las consultas Q y los documentos D denota la probabilidad de que el par (Qs,Dt) esté en la lista de relevancia. Tales suposiciones son similares a las utilizadas en [8]. Suponiendo que el usuario emite la consulta Qi ∈Q y los resultados de recuperación en respuesta a Qi es una lista clasificada L de documentos, calculamos la cantidad de información contenida en P(Qs,Dt) con respecto a Qi y L mediante la Ec.1, que es una variante de la entropía llamada entropía ponderada[13]. Los pesos en la ecuación 1 están determinados únicamente por Qi y L. En este documento, elegimos los pesos de la siguiente manera: L es el número de documentos que contiene la consulta, K es el límite superior, LT es el límite superior de documentos, y D es el peso si K está en otro caso. La clasificación de corte K es un parámetro en nuestro modelo que se discutirá más adelante. Por lo tanto, la ecuación 1 se puede simplificar de la siguiente manera: )3(),(log 1 ),( )( , ∑∈ −= LTD titsLQ Kt i DQP K DQH Desafortunadamente, la entropía ponderada ),(, tsLQ DQH i calculada por la ecuación 3, que representa la cantidad de información sobre la probabilidad de que los documentos mejor clasificados en L sean relevantes para la consulta Qi en promedio, no se puede comparar entre diferentes consultas, lo que la hace inapropiada para predecir directamente el rendimiento de la consulta. Para mitigar este problema, creamos una distribución de fondo P(Qs,C) sobre Q y D imaginando que cada documento en D es reemplazado por el mismo documento especial C que representa el uso promedio del lenguaje. En este documento, C se crea concatenando cada documento en D. A grosso modo, C es la colección (el conjunto de documentos) {Dt} sin límites de documentos. De manera similar, la entropía ponderada ),(, CQH sLQi calculada por la Ecuación 3 representa la cantidad de información sobre la probabilidad de que un documento promedio (representado por toda la colección) sea relevante para la consulta Qi. Ahora presentamos nuestro predictor de rendimiento WIG que es la ganancia de información ponderada [13] calculada como la diferencia entre ),(, tsLQ DQH i y ),(, CQH sLQi. Específicamente, dado el conjunto de consultas Qi, la colección C y la lista clasificada L de documentos, WIG se calcula de la siguiente manera: )4( ),( ),( log 1 ),( ),( log),( ),(),(),,( )(, ,, ∑∑ ∈ == −= LTD i ti ts s ts ts tsLQsLQi Kt ii CQP DQP KCQP DQP DQweight DQHCQHLCQWIG WIG calculado por la Ecuación 4 mide el cambio en la información sobre la calidad de la recuperación (en respuesta a la consulta Qi) desde un estado imaginario en el que solo se recupera un documento promedio hasta un estado posterior en el que se observan los resultados de búsqueda reales. Hacemos la hipótesis de que WIG está positivamente correlacionado con la efectividad de la recuperación porque una recuperación de alta calidad debería ser mucho más efectiva que simplemente devolver el documento promedio. El corazón de esta técnica es cómo estimar la distribución conjunta P(Qs, Dt). En el enfoque de modelado del lenguaje para la recuperación de información, se pueden aplicar fácilmente una variedad de modelos para estimar esta distribución. Aunque la mayoría de estos modelos se basan en la suposición de la bolsa de palabras, trabajos recientes sobre modelado de dependencia de términos bajo el marco de modelado de lenguaje han demostrado mejoras consistentes y significativas en la efectividad de recuperación sobre los modelos de bolsa de palabras. Inspirados por el éxito de incorporar características de proximidad de términos en modelos de lenguaje, decidimos adoptar un buen modelo de dependencia para estimar la probabilidad P(Qs,Dt). El modelo que elegimos para este artículo es el modelo de Campo Aleatorio de Markov (MRF) de Metzler y Crofts, que ya ha demostrado su superioridad en comparación con varias colecciones y diferentes tareas de recuperación [8,9]. Según el modelo MRF, log P(Qi, Dt) se puede escribir como )5()|(loglog),(log )( 1 ∑∈ +−= iQF tti DPZDQP ξ ξ ξλ donde Z1 es una constante que asegura que P(Qi, Dt) sume 1. F(Qi) consiste en un conjunto de características ampliadas a partir de la consulta original Qi. Por ejemplo, suponiendo que la consulta Qi es el programa de estudiantes talentosos, F(Qi) incluye características como programa y estudiante talentoso. Consideramos dos tipos de características: características de términos individuales T y características de proximidad P. Las características de proximidad incluyen la frase exacta (#1) y las características de ventana desordenada (#uwN) como se describe en [8]. Ten en cuenta que F(Qi) es la unión de T(Qi) y P(Qi). Para obtener más detalles sobre F(Qi), como por ejemplo cómo expandir la consulta original Qi a F(Qi), remitimos al lector a [8] y [9]. P(ξ|Dt) denota la probabilidad de que la característica ξ ocurra en Dt. Más detalles sobre P(ξ|Dt) se proporcionarán más adelante en esta sección. La elección de λξ es algo diferente a la utilizada en [8] ya que λξ desempeña un doble papel en nuestro modelo. El primer rol, que es el mismo que en [8], es ponderar entre las características de términos individuales y de proximidad. El otro rol, específico de nuestra tarea de predicción, es normalizar el tamaño de F(Qi). Encontramos que la siguiente estrategia de peso para λξ satisface los dos roles anteriores y generaliza bien en una variedad de colecciones y tipos de consultas. )6( )(, |)(| 1 )(, |)(| ⎪ ⎪ ⎩ ⎪ ⎪ ⎨ ⎧ ∈ − ∈ = i i T i i T QP QP QT QT ξ λ ξ λ λξ donde |T(Qi)| y |P(Qi)| denotan el número de características de términos únicos y de proximidad en F(Qi) respectivamente. La razón de elegir la función raíz cuadrada en el denominador de λξ es penalizar adecuadamente un conjunto de características de gran tamaño, haciendo que WIG sea más comparable entre consultas de diversas longitudes. λT es un parámetro fijo y se establece en 0.8 según [8] a lo largo de este documento. De manera similar, log P(Qi,C) se puede escribir como: )7()|(loglog),(log )( 2 ∑∈ +−= iQF i CPZCQP ξ ξ ξλ Cuando se eliminan las constantes Z1 y Z2, el WIG calculado en la Ecuación 4 se puede reescribir de la siguiente manera al sustituir la Ecuación 5 y la Ecuación 7: )8( )|( )|( log 1 ),,( )( )( ∑ ∑∈ ∈ = LTD QF t i Kt i CP DP K LCQWIG ξ ξ ξ ξ λ Una de las ventajas del WIG sobre otras técnicas es que puede manejar tanto consultas basadas en contenido como en NP. Basándose en el tipo (o el tipo predicho) de Qi, el cálculo de WIG en la Ecuación 8 difiere en dos aspectos: (1) cómo estimar P(ξ|Dt) y P(ξ|C), y (2) cómo elegir K. Para consultas basadas en contenido, P(ξ|C) se estima por la frecuencia relativa del rasgo ξ en la colección C en su totalidad. La estimación de P(ξ|Dt) es la misma que en [8]. Es decir, estimamos P(ξ|Dt) mediante la frecuencia relativa del rasgo ξ en Dt suavizada linealmente con la frecuencia de colección P(ξ|C). La K en la Ec.8 se trata como un parámetro libre. Ten en cuenta que K es el único parámetro libre en el cálculo de WIG para consultas basadas en contenido, ya que se asume que todos los parámetros involucrados en P(ξ|Dt) están fijos al tomar los valores sugeridos en [8]. En cuanto a las consultas de NP, hacemos uso de la estructura del documento para estimar P(ξ|Dt) y P(ξ|C) mediante el llamado modelo de mezcla de lenguaje propuesto en [10] e incorporado en el modelo MRF para la recuperación de Named-Page en [9]. La idea básica es que un documento (colección) se divide en varios campos como el campo del título, el campo del cuerpo principal y el campo de los encabezados. P(ξ|Dt) y P(ξ|C) se estiman mediante una combinación lineal de los modelos de lenguaje de cada campo. Debido a limitaciones de espacio, remitimos al lector a [9] para más detalles. Adoptamos el mismo conjunto exacto de parámetros utilizados en [9] para la estimación. En cuanto a K en la Ec.8, establecemos K en 1 porque la tarea de encontrar Named-Page se centra fuertemente en el documento clasificado en primer lugar. Por consiguiente, no hay parámetros libres en el cálculo de WIG para consultas de NP. 3.2 Retroalimentación de consultas En esta sección, presentamos otra técnica llamada retroalimentación de consultas (QF) para la predicción. Suponga que un usuario emite la consulta Q a un sistema de recuperación y se devuelve una lista clasificada L de documentos. Vemos el sistema de recuperación como un canal ruidoso. Específicamente, asumimos que la salida del canal es L y la entrada es Q. Después de pasar por el canal, Q se corrompe y se transforma en la lista clasificada L. Al pensar en el proceso de recuperación de esta manera, el problema de predecir la eficacia de la recuperación se convierte en la tarea de evaluar la calidad del canal. En otras palabras, la predicción se convierte en encontrar una forma de medir el grado de corrupción que surge cuando Q se transforma en L. Dado que calcular directamente el grado de corrupción es difícil, abordamos este problema mediante aproximaciones. Nuestra idea principal es que medimos en qué medida la información sobre Q puede ser recuperada de L bajo la suposición de que solo se observa L. Específicamente, diseñamos un decodificador que pueda traducir con precisión L de vuelta a la nueva consulta Q y la similitud S entre la consulta original Q y la nueva consulta Q se adopta como un predictor de rendimiento. Este es un bosquejo de cómo la técnica QF predice el rendimiento de la consulta. Antes de completar más detalles, discutimos brevemente por qué este método funcionaría. Existe una relación entre la similitud S definida anteriormente y el rendimiento de recuperación. Por un lado, si la recuperación se ha desviado del sentido original de la consulta Q, la nueva consulta Q extraída de la lista clasificada L en respuesta a Q sería muy diferente de la consulta original Q. Por otro lado, una consulta destilada de una lista clasificada que contiene muchos documentos relevantes es probable que sea similar a la consulta original. Se proporcionarán más ejemplos en apoyo de la relación más adelante. A continuación detallamos cómo construir el decodificador y cómo medir la similitud S. En esencia, el objetivo del decodificador es comprimir la lista clasificada L en unos pocos términos informativos que deberían representar el contenido de los documentos mejor clasificados en L. Nuestro enfoque para este objetivo es representar la lista clasificada L mediante un modelo de lenguaje (distribución de términos). Luego, los términos se clasifican según su contribución a la divergencia KL (Kullback-Leibler) de los modelos de lenguaje con respecto al modelo de la colección de fondo. Los términos mejor clasificados serán elegidos para formar la nueva consulta Q. Este enfoque es similar al utilizado en la Sección 4.1 de [11]. Específicamente, tomamos tres pasos para comprimir la lista clasificada L en la consulta Q sin hacer referencia a la consulta original. 1. Adoptamos el modelo de lenguaje de lista clasificada [14] para estimar un modelo de lenguaje basado en la lista clasificada L. El modelo puede escribirse como: )9()|()|()|( ∑∈ = LD LDPDwPLwP donde w es cualquier término y D es un documento. La probabilidad de que ocurra el evento D dado el evento L se estima mediante una función linealmente decreciente del rango del documento D. Cada término en P(w|L) está clasificado por la siguiente contribución de <br>divergencia de Kullback-Leibler</br>: )10( )|( )|( log)|( CwP LwP LwP donde P(w|C) es el modelo de colección estimado por la frecuencia relativa del término w en la colección C en su totalidad. 3. Los términos N principales clasificados por Eq.10 forman una consulta ponderada Q={(wi,ti)} i=1,N, donde wi denota el término clasificado en la posición i y el peso ti es la contribución de la divergencia KL de wi en Eq. 10. Barco de crucero daño vida marina. Estos ejemplos indican cómo la similitud entre la consulta original y la nueva se correlaciona con el rendimiento de recuperación. El parámetro N en el paso 3 se establece en 20 de manera empírica y elegir un valor más grande de N es innecesario ya que los pesos después de los primeros 20 suelen ser demasiado pequeños para marcar alguna diferencia. Para medir la similitud entre la consulta original Q y la nueva consulta Q, primero utilizamos Q para recuperar información de la misma colección. Se adopta una variante del modelo de probabilidad de consulta [15] para la recuperación. Es decir, los documentos se clasifican por: )11()|()|( ),( ∑∈ = Qtw t i ii i DwPDQP donde wi es un término en Q y ti es el peso asociado. D es un documento. Que L denote la nueva lista clasificada devuelta desde la recuperación anterior. La similitud se mide por la superposición de documentos en L y L. Específicamente, el porcentaje de documentos en los primeros K documentos de L que también están presentes en los primeros K documentos en L. El valor de corte K se trata como un parámetro libre. Aquí resumimos cómo la técnica QF predice el rendimiento dado una consulta Q y la lista clasificada asociada L. Primero obtenemos una consulta ponderada Q comprimida a partir de L mediante los tres pasos anteriores. Luego usamos Q para realizar la recuperación y la nueva lista clasificada es L. La superposición de documentos en L y L se utiliza para la predicción. 3.3 Primer Cambio de Rango (FRC) En esta sección, proponemos un método llamado primer cambio de rango (FRC) para la predicción de rendimiento para consultas de NP. Este método se deriva de la técnica de robustez de clasificación [1] que está principalmente diseñada para consultas basadas en contenido. Cuando se aplica directamente a consultas de NP, la técnica de robustez será menos efectiva porque tiene en cuenta todos los documentos mejor clasificados en su totalidad, mientras que las consultas de NP suelen tener solo un único documento relevante. En cambio, nuestra técnica se centra en el documento de rango uno mientras se mantiene la idea principal del método de robustez. Específicamente, el seudocódigo para calcular FRC se muestra en la figura 1. Lista clasificada L={Di} donde i=1,100. Di denota el documento clasificado en la posición i. (2) consulta Q 1 inicializar: (1) establecer el número de intentos J=100000 (2) contador c=0; 2 para i=1 a J 3 Perturbar cada documento en L, que el resultado sea un conjunto F={Di} donde Di denota la versión perturbada de Di. 4 Realizar la recuperación con la consulta Q en el conjunto F 5 c=c+1 si y solo si D1 está clasificado primero en el paso 4 6 fin del para 7 devolver la proporción c/J Figura 1: pseudo-código para calcular FRC FRC aproxima la probabilidad de que el documento clasificado en primer lugar en la lista original L permanezca clasificado primero incluso después de que los documentos sean perturbados. Cuanto mayor sea la probabilidad, más confianza tenemos en el documento clasificado en primer lugar. Por otro lado, en el caso extremo de un ranking aleatorio, la probabilidad sería tan baja como 0.5. Esperamos que FRC tenga una asociación positiva con el rendimiento de la consulta de NP. Adoptamos [1] para implementar el paso de perturbación del documento (paso 4 en la Fig. 1) utilizando distribuciones de Poisson. Para más detalles, remitimos al lector a [1]. 4. EVALUACIÓN Ahora presentamos los resultados de predecir el rendimiento de la consulta por nuestros modelos. Tres técnicas de vanguardia son adoptadas como nuestras líneas base. Evaluamos nuestras técnicas en una variedad de configuraciones de recuperación web. Como se mencionó anteriormente, consideramos dos tipos de consultas, es decir, consultas basadas en contenido (CB) y consultas de búsqueda de páginas con nombre (NP). Primero, supongamos que se conocen los tipos de consultas. Investigamos la correlación entre el rendimiento de recuperación predicho y el rendimiento real para ambos tipos de consultas por separado. Los resultados muestran que nuestros métodos producen mejoras considerablemente superiores a los valores de referencia. Luego consideramos un escenario más desafiante donde no hay información previa sobre los tipos de consultas disponibles. Se consideran dos subcasos. En el primero, existe solo un tipo de consulta pero el tipo real es desconocido. Suponemos una mezcla de los dos tipos de consultas en el segundo caso. Demostramos que nuestros modelos logran una buena precisión en este escenario exigente, lo que hace que la predicción sea práctica en un entorno de búsqueda web del mundo real. 4.1 Configuración Experimental Nuestra evaluación se centra en la colección GOV2 que contiene alrededor de 25 millones de documentos obtenidos de sitios web en el dominio .gov durante 2004 [3]. Creamos dos tipos de conjuntos de datos para consultas de CB y consultas de NP respectivamente. Para el tipo CB, utilizamos los temas ad-hoc de las pistas Terabyte de 2004, 2005 y 2006 y los nombramos TB04-adhoc, TB05-adhoc y TB06-adhoc respectivamente. Además, también utilizamos los temas ad-hoc de la pista Robusta de 2004 (RT04) para probar la adaptabilidad de nuestras técnicas a un entorno no web. Para consultas de NP, utilizamos los temas de búsqueda de páginas nombradas de las Terabyte Tracks de 2005 y 2006 y los denominamos TB05-NP y TB06-NP respectivamente. Todas las consultas utilizadas en nuestros experimentos son títulos de temas de TREC, ya que nos centramos en la recuperación web. La Tabla 3 resume los conjuntos de datos anteriores. La recuperación del rendimiento de las consultas individuales basadas en contenido y NP se mide mediante la precisión promedio y la tasa recíproca de la primera respuesta correcta, respectivamente. Hacemos uso del modelo de campo aleatorio de Markov tanto para la recuperación ad-hoc como para la recuperación de páginas nombradas. Adoptamos la misma configuración de parámetros de recuperación utilizada en [8,9]. El motor de búsqueda Indri [12] se utiliza para todos nuestros experimentos. Aunque no se informa aquí, también probamos el modelo de probabilidad de consulta para la recuperación ad-hoc y encontramos que los resultados cambian poco debido a la correlación muy alta entre el rendimiento de las consultas obtenido por los dos modelos de recuperación (0.96 medido por el coeficiente de Pearson). 4.2 Tipos de Consultas Conocidos Supongamos que se conocen los tipos de consultas. Tratamos cada tipo de consulta por separado y medimos la correlación con la precisión promedio (o el rango recíproco en el caso de consultas NP). Adoptamos la prueba de correlación de Pearson que refleja el grado de relación lineal entre el rendimiento de recuperación predicho y real. 4.2.1 Métodos de Consultas Basadas en Contenido Claridad Robusta JSD WIG QF WIG +QF TB04+0 5 adhoc 0.333 0.317 0.362 0.574 0.480 0.637 TB06 adhoc 0.076 0.294 N/A 0.464 0.422 0.511 Tabla 4: Coeficientes de correlación de Pearson para la correlación con la precisión promedio en las Pistas de Terabyte (ad-hoc) para la puntuación de claridad, puntuación de robustez, el método basado en JSD (citamos directamente la puntuación informada en [2]), WIG, retroalimentación de consulta (QF) y una combinación lineal de WIG y QF. Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. La Tabla 4 muestra la correlación con la precisión promedio en dos conjuntos de datos: uno es una combinación de TB04-adhoc y TB05-adhoc (100 temas en total) y el otro es TB06-adhoc (50 temas). La razón por la que juntamos TB04-adhoc y TB05-adhoc es para hacer nuestros resultados comparables a [2]. Nuestros puntos de referencia son la puntuación de claridad (claridad) [6], la puntuación de robustez (robust) [1] y el método basado en JSD (JSD) [2]. Para la puntuación de claridad y robustez, hemos probado diferentes configuraciones de parámetros y reportamos los coeficientes de correlación más altos que hemos encontrado. Citamos directamente el resultado del método basado en JSD reportado en [2]. La tabla también muestra los resultados para el método de Ganancia de Información Ponderada (WIG) y el método de Retroalimentación de Consulta (QF) para predecir consultas basadas en contenido. Como describimos en la sección anterior, tanto WIG como QF tienen un parámetro libre para ajustar, es decir, el rango de corte K. Entrenamos el parámetro en un conjunto de datos y lo probamos en el otro. Al combinar WIG y QF, se utiliza una combinación lineal simple y el peso de la combinación se aprende a partir del conjunto de datos de entrenamiento. A partir de estos resultados, podemos ver que nuestros métodos son considerablemente más precisos en comparación con los baselines. También observamos que se obtienen mejoras adicionales a partir de la combinación de WIG y QF, lo que sugiere que miden diferentes propiedades del proceso de recuperación que se relacionan con el rendimiento. Descubrimos que nuestros métodos se generalizan bien en TB06-adhoc, mientras que la correlación del puntaje de claridad con el rendimiento de recuperación en este conjunto de datos es considerablemente peor. Una investigación adicional muestra que la precisión promedio media de TB06-adhoc es de 0.342 y es aproximadamente un 10% mejor que la del primer conjunto de datos. Mientras que los otros tres métodos suelen considerar los 100 mejores documentos o menos de una lista clasificada, el método de claridad generalmente necesita los 500 mejores documentos o más para medir adecuadamente la coherencia de una lista clasificada. Un promedio de precisión media más alto hace que las listas clasificadas recuperadas por diferentes consultas sean más similares en términos de coherencia en el nivel de los primeros 500 documentos. Creemos que esta es la razón principal de la baja precisión de la puntuación de claridad en el segundo conjunto de datos. Aunque este documento se centra en un entorno de búsqueda web, es deseable que nuestras técnicas funcionen de manera consistente en otras situaciones. Con este fin, examinamos la efectividad de nuestras técnicas en la pista Robust 2004. Para nuestros métodos, dividimos equitativamente todas las consultas de prueba en cinco grupos y realizamos validación cruzada de cinco pliegues. Cada vez que utilizamos un grupo para el entrenamiento y los otros cuatro grupos para las pruebas. Hacemos uso de todas las consultas para nuestros dos puntos de referencia, es decir, la puntuación de claridad y la puntuación de robustez. Los parámetros para nuestras líneas base son los mismos que los utilizados en [1]. Los resultados mostrados en la Tabla 5 demuestran que la precisión de predicción de nuestros métodos está a la par con la de las dos líneas base sólidas. Claridad Robusta WIG QF 0.464 0.539 0.468 0.464 Tabla 5: Comparación de los coeficientes de correlación de Pearson en la pista Robusta de 2004 para la puntuación de claridad, puntuación de robustez, WIG y retroalimentación de consulta (QF). Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. Además, examinamos la sensibilidad de predicción de nuestros métodos al rango de corte K. Con respecto a WIG, es bastante robusto a K en las Pistas de Terabyte (2004-2006) mientras prefiere un valor pequeño de K como 5 en la Pista Robusta de 2004. En otras palabras, un valor pequeño de K es una elección casi óptima para ambos tipos de pistas. Teniendo en cuenta que todos los demás parámetros involucrados en WIG están fijos y, consecuentemente, son los mismos para los dos casos, esto significa que WIG puede lograr una precisión de predicción casi óptima en dos situaciones considerablemente diferentes con exactamente la misma configuración de parámetros. En cuanto a QF, prefiere un valor más grande de K, como 100 en las Pistas de Terabyte y un valor más pequeño de K, como 25 en la Pista Robusta de 2004. 4.2.2 Consultas NP Adoptamos WIG y el primer cambio de rango (FRC) para predecir el rendimiento de las consultas NP. También intentamos una combinación lineal de los dos como en la sección anterior. El peso de la combinación se obtiene del otro conjunto de datos. Utilizamos la correlación con los rangos recíprocos medidos por la prueba de correlación de Pearson para evaluar la calidad de la predicción. Los resultados se presentan en la Tabla 6. Nuevamente, nuestros puntos de referencia son la puntuación de claridad y la puntuación de robustez. Para hacer una comparación justa, ajustamos la puntuación de claridad de diferentes maneras. Descubrimos que utilizar el documento clasificado en primer lugar para construir el modelo de consulta produce la mejor precisión de predicción. También intentamos utilizar la estructura del documento mediante la combinación de modelos de lenguaje mencionados en la sección 3.1. Se obtuvo una pequeña mejora. Los coeficientes de correlación para la puntuación de claridad reportados en la Tabla 6 son los mejores que hemos encontrado. Como podemos ver, nuestros métodos superan considerablemente la técnica de puntuación de claridad en ambas ejecuciones. Esto confirma nuestra intuición de que el uso de una medida basada en la coherencia como el puntaje de claridad es inapropiado para las consultas de NP. Claridad de métodos robusta. Tabla 6: Coeficientes de correlación de Pearson para la correlación con rangos recíprocos en las pistas de Terabyte (NP) para la puntuación de claridad, puntuación de robustez, WIG, el primer cambio de rango (FRC) y una combinación lineal de WIG y FRC. Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. En cuanto a la puntuación de robustez, también ajustamos los parámetros y reportamos el mejor que hemos encontrado. Observamos una correlación negativa interesante y sorprendente con los rangos recíprocos. Explicamos este hallazgo brevemente. Un puntaje de robustez alto significa que varios documentos de alta clasificación en la lista clasificada original siguen estando muy bien clasificados después de perturbar los documentos. La existencia de tales documentos es una buena señal de alto rendimiento para consultas basadas en contenido, ya que estas consultas suelen contener una serie de documentos relevantes [1]. Sin embargo, en lo que respecta a las consultas de NP, una diferencia fundamental es que solo hay un documento relevante para cada consulta. La existencia de tales documentos puede confundir la función de clasificación y llevar a un bajo rendimiento de recuperación. Aunque existe una correlación negativa con el rendimiento de recuperación, la fuerza de la correlación es más débil y menos consistente en comparación con nuestros métodos, como se muestra en la Tabla 6. Basándonos en el análisis anterior, podemos ver que las técnicas de predicción actuales como la puntuación de claridad y la puntuación de robustez, que están principalmente diseñadas para consultas basadas en contenido, enfrentan desafíos significativos y son insuficientes para manejar consultas de NP. Nuestras dos técnicas propuestas para consultas de NP demuestran consistentemente una buena precisión de predicción, mostrando un éxito inicial en la resolución del problema de predecir el rendimiento para consultas de NP. Otro punto que queremos destacar es que el método WIG funciona bien para ambos tipos de consultas, una propiedad deseable que la mayoría de las técnicas de predicción carecen. 4.3 Tipos de Consultas Desconocidos En esta sección, realizamos dos tipos de experimentos sin acceso a etiquetas de tipo de consulta. Primero, asumimos que solo existe un tipo de consulta pero el tipo es desconocido. Segundo, experimentamos con una mezcla de consultas basadas en contenido y NP. Las dos subsecciones siguientes informarán los resultados para las dos condiciones respectivamente. 4.3.1 Solo existe un tipo. Suponemos que todas las consultas son del mismo tipo, es decir, son consultas NP o consultas basadas en contenido. Elegimos WIG para tratar este caso porque muestra una buena precisión de predicción para ambos tipos de consultas en la sección anterior. Consideramos dos casos: (1) CB: todas las 150 consultas de títulos de la tarea ad-hoc de las pistas Terabyte 2004-2006 (2) NP: todas las 433 consultas de NP de la tarea de búsqueda de páginas nombradas de las pistas Terabyte 2005 y 2006. Tomamos una estrategia simple etiquetando todas las consultas en cada caso como el mismo tipo (ya sea NP o CB) independientemente de su tipo real. El cálculo de WIG se basará en el tipo de consulta etiquetado en lugar del tipo real. Hay cuatro posibilidades con respecto a la relación entre el tipo real y el tipo etiquetado. La correlación con el rendimiento de recuperación bajo las cuatro posibilidades se presenta en la Tabla 7. Por ejemplo, el valor 0.445 en la intersección entre la segunda fila y la tercera columna muestra el coeficiente de correlación de Pearson para la correlación con la precisión promedio cuando las consultas basadas en contenido se etiquetan incorrectamente como del tipo NP. Basándonos en estos resultados, recomendamos tratar todas las consultas como del tipo NP cuando solo existe un tipo de consulta y la clasificación precisa de la consulta no es factible, considerando el riesgo de que se produzca una gran pérdida de precisión si las consultas NP se etiquetan incorrectamente como consultas basadas en contenido. Estos resultados también demuestran la fuerte adaptabilidad de WIG a diferentes tipos de consultas. Tabla 7: Comparación de los coeficientes de correlación de Pearson para la correlación con el rendimiento de recuperación en cuatro posibilidades en las pistas de Terabyte (NP). Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. 4.3.2 Una mezcla de consultas basadas en contenido y NP Una mezcla de los dos tipos de consultas es una situación más realista que un motor de búsqueda web encontrará. Evaluamos la precisión de la predicción por la exactitud con la que se pueden identificar las consultas de bajo rendimiento mediante el método de predicción, asumiendo que los tipos de consulta reales son desconocidos (pero podemos predecir los tipos de consulta). Esta es una tarea desafiante porque tanto el rendimiento predicho como el real para un tipo de consulta pueden ser incomparables con el de otro tipo. A continuación discutimos cómo implementar nuestra evaluación. Creamos un conjunto de consultas que consiste en todas las 150 consultas de título ad-hoc de Terabyte Track 2004-2006 y todas las 433 consultas NP de Terabyte Track 2005&2006. Dividimos las consultas en el conjunto en clases: buenas (mejores que el 50% de las consultas del mismo tipo en términos de rendimiento de recuperación) y malas (de lo contrario). Según estos estándares, una consulta de NP con un rango recíproco superior a 0.2 o una consulta basada en contenido con una precisión promedio superior a 0.315 se considerarán como buenas. Entonces, cada vez que seleccionamos aleatoriamente una consulta Q del grupo con una probabilidad p de que Q esté basada en el contenido. Las consultas restantes se utilizan como datos de entrenamiento. Primero decidimos el tipo de consulta Q de acuerdo con un clasificador de consultas. Es decir, el clasificador de consultas nos dice si la consulta Q es de tipo NP o basada en contenido. Basándose en el tipo de consulta predicho y en la puntuación calculada para la consulta Q por una técnica de predicción, se toma una decisión binaria sobre si la consulta Q es buena o mala al compararla con el umbral de puntuación del tipo de consulta predicho obtenido de los datos de entrenamiento. La precisión de la predicción se mide por la exactitud de la decisión binaria. En nuestra implementación, tomamos repetidamente una consulta de prueba del conjunto de consultas y la precisión de la predicción se calcula como el porcentaje de decisiones correctas, es decir, una consulta buena (mala) se predice como buena (mala). Es obvio que adivinar al azar llevará a una precisión del 50%. Tomemos el método WIG como ejemplo para ilustrar el proceso. Dos umbrales de WIG (uno para consultas de NP y otro para consultas basadas en contenido) se entrenan maximizando la precisión de predicción en los datos de entrenamiento. Cuando una consulta de prueba es etiquetada como del tipo NP (CB) por el clasificador de tipo de consulta, se predecirá que es buena solo si la puntuación WIG para esta consulta está por encima del umbral de NP (CB). Se seguirán procedimientos similares para otras técnicas de predicción. Ahora presentamos brevemente el clasificador de tipo de consulta automático utilizado en este artículo. Encontramos que la puntuación de robustez, aunque originalmente propuesta para la predicción de rendimiento, es un buen indicador de tipos de consultas. Observamos que, en promedio, las consultas basadas en contenido tienen una puntuación de robustez mucho más alta que las consultas de NP. Por ejemplo, la Figura 2 muestra las distribuciones de puntajes de robustez para consultas basadas en NP y en contenido. Según este hallazgo, el clasificador de puntuación de robustez adjuntará una etiqueta NP (CB) a la consulta si la puntuación de robustez para la consulta está por debajo (por encima) de un umbral entrenado a partir de los datos de entrenamiento. 0 0.5 1 1.5 2 2.5 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 NP Basado en contenido Figura 2: Distribución de puntuaciones de robustez para consultas NP y CB. Las consultas NP son los 252 temas NP del Terabyte Track de 2005. Las consultas basadas en contenido son los 150 títulos ad-hoc de las pistas Terabyte 2004-2006. Las distribuciones de probabilidad son estimadas por el método de estimación de densidad de Kernel. Estrategias Robustas WIG-1 WIG-2 WIG-3 Óptima p=0.6 0.565 0.624 0.665 0.684 0.701 P=0.4 0.567 0.633 0.654 0.673 0.696 Tabla 8: Comparación de la precisión de predicción para cinco estrategias en la situación de consulta mixta. Dos formas de muestrear una consulta de la piscina: (1) la consulta muestreada es basada en contenido con una probabilidad p=0.6 (es decir, la consulta es NP con una probabilidad de 0.4) (2) establecer la probabilidad p=0.4. Consideramos cinco estrategias en nuestros experimentos. En la primera estrategia (denominada robusta), utilizamos la puntuación de robustez para la predicción del rendimiento de la consulta con la ayuda de un clasificador de consultas perfecto que siempre mapea correctamente una consulta en una de las dos categorías (es decir, NP o CB). Esta estrategia representa el nivel de precisión de predicción que las técnicas actuales de predicción pueden lograr en una condición ideal en la que se conocen los tipos de consulta. En las tres estrategias siguientes, se adopta el método WIG para predecir el rendimiento. La diferencia entre los tres es que se utilizan tres clasificadores de consulta diferentes para cada estrategia: (1) el clasificador siempre clasifica una consulta en el tipo NP. (2) el clasificador de Probabilidad de Densidad de Puntuación Robusta es el clasificador de puntuación robusta mencionado anteriormente. (3) el clasificador es perfecto. Estas tres estrategias están designadas como WIG-1, WIG-2 y WIG-3 respectivamente. La razón por la que estamos interesados en WIG-1 se basa en los resultados de la sección 4.3.1. En la última estrategia (denominada Óptima) que sirve como un límite superior de lo bien que podemos hacer hasta ahora, hacemos pleno uso de nuestras técnicas de predicción para cada tipo de consulta asumiendo que un clasificador de consultas perfecto está disponible. Específicamente, combinamos linealmente WIG y QF para consultas basadas en contenido y WIG y FRC para consultas de NP. Los resultados de las cinco estrategias se muestran en la Tabla 8. Para cada estrategia, intentamos dos formas de muestrear una consulta del conjunto: (1) la consulta muestreada es CB con probabilidad p=0.6 (la consulta es NP con probabilidad 0.4) (2) establecer la probabilidad p=0.4. De la Tabla 8 podemos ver que en términos de precisión de predicción, WIG-2 (el método WIG con el clasificador de consultas automático) no solo es mejor que los dos primeros casos, sino que también está cerca de WIG-3 donde se asume un clasificador perfecto. Se observan algunas mejoras adicionales sobre WIG-3 cuando se combinan con otras técnicas de predicción. El mérito de WIG-2 es que proporciona una solución práctica para identificar automáticamente consultas con bajo rendimiento en un entorno de búsqueda web con tipos de consultas mixtos, lo cual representa considerables obstáculos para las técnicas de predicción tradicionales. CONCLUSIONES Y TRABAJOS FUTUROS Hasta donde sabemos, nuestro artículo es el primero en explorar a fondo la predicción del rendimiento de consultas en entornos de búsqueda web. Demostramos que nuestros modelos resultaron en una mayor precisión de predicción que las técnicas previamente publicadas no especialmente diseñadas para escenarios de búsqueda en la web. En este artículo, nos enfocamos en dos tipos de consultas en la búsqueda web: consultas basadas en contenido y consultas de búsqueda de Páginas-Nombradas (NP), correspondientes a la tarea de recuperación ad-hoc y la tarea de búsqueda de Páginas-Nombradas respectivamente. Para ambos tipos de consultas web, se demostró que nuestros modelos de predicción son sustancialmente más precisos que las técnicas actuales más avanzadas. Además, consideramos un caso más realista en el que no se dispone de información previa sobre los tipos de consultas. Demostramos que el método WIG es particularmente adecuado para esta situación. Considerando la adaptabilidad de WIG a una variedad de colecciones y tipos de consultas, uno de nuestros planes futuros es aplicar este método para predecir la preferencia del usuario por los resultados de búsqueda en datos realistas recopilados de un motor de búsqueda comercial. Además de la precisión, otro problema importante con el que las técnicas de predicción tienen que lidiar en un entorno web es la eficiencia. Afortunadamente, dado que el puntaje WIG se calcula solo sobre los términos y frases que aparecen en la consulta, este cálculo puede realizarse de manera muy eficiente con el soporte de un índice. Por otro lado, el cálculo de QF y FRC es relativamente menos eficiente ya que QF necesita recuperar toda la colección dos veces y FRC necesita clasificar repetidamente los documentos perturbados. Cómo mejorar la eficiencia de QF y FRC es nuestro trabajo futuro. Además, las técnicas de predicción propuestas en este documento tienen el potencial de mejorar el rendimiento de recuperación al combinarse con otras técnicas de RI. Por ejemplo, nuestras técnicas pueden ser incorporadas a técnicas populares de modificación de consultas como la expansión de consultas y la relajación de consultas. Guiados por la predicción del rendimiento, podemos tomar una mejor decisión sobre cuándo o cómo modificar las consultas para mejorar la efectividad de la recuperación. Nos gustaría llevar a cabo investigaciones en esta dirección en el futuro. AGRADECIMIENTOS Este trabajo fue apoyado en parte por el Centro de Recuperación de Información Inteligente, en parte por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el contrato número HR0011-06-C0023, y en parte por un premio de Google. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son del autor y no necesariamente reflejan las del patrocinador. Además, agradecemos a Donald Metzler por sus valiosos comentarios sobre este trabajo. 7. REFERENCIAS [1] Y. Zhou, W. B. Croft, Ranking Robustness: Un nuevo marco para predecir el rendimiento de consultas, en Actas de CIKM 2006. [2] D. Carmel, E. Yom-Tov, A. Darlow, D. Pelleg, ¿Qué hace que una consulta sea difícil?, en Actas de SIGIR 2006. [3] C. L. A. Clarke, F. Scholer, I. Soboroff, La pista Terabyte TREC 2005, En las Actas en Línea de TREC 2005. [4] B. Él y yo. Inferir el rendimiento de la consulta utilizando predictores de preretrieval. En actas de SPIRE 2004. [5] S. Tomlinson. Recuperación robusta de la web y terabytes con Hummingbird SearchServer en TREC 2004. En las Actas en Línea de TREC 2004. [6] S. Cronen-Townsend, Y. Zhou, W. B. Croft, Prediciendo el Rendimiento de Consultas, en las Actas de SIGIR 2002. [7] V. Vinay, I. J. Cox, N. Mill-Frayling, K. Wood, Sobre la Clasificación de la Efectividad del Buscador, en las Actas de SIGIR 2006. [8] D. Metzler, W. B. Croft, Un Modelo de Campo Aleatorio de Markov para Dependencias de Términos, en las Actas de SIGIR 2005. [9] D. Metzler, T. Strohman, Y. Zhou, W. B. Croft, Indri en TREC 2005: Terabyte Track, en las Actas en Línea de TREC 2004. [10] P. Ogilvie y J. Callan, Combinando representaciones de documentos para búsqueda de elementos conocidos, en las Actas de SIGIR 2003. [11] A. Berger, J. Lafferty, Recuperación de información como traducción estadística, en las Actas de SIGIR 1999. [12] Motor de búsqueda Indri: http://www.lemurproject.org/indri/ [13] I. J. Taneja: Sobre medidas de información generalizadas y sus aplicaciones, Avances en Electrónica y Física de Electrones, Academic Press (EE. UU.), 76, 1989, 327-413. [14] S. Cronen-Townsend, Y. Zhou y Croft, W. B., Un marco para la expansión selectiva de consultas, en Actas de CIKM 2004. [15] F. Song, W. B. Croft, Un modelo de lenguaje general para la recuperación de información, en Actas de SIGIR 1999. [16] Contacto por correo electrónico personal con Vishwa Vinay y nuestros propios experimentos [17] E. Yom-Tov, S. Fine, D. Carmel, A. Darlow, Aprendiendo a estimar la dificultad de la consulta incluyendo aplicaciones para la detección de contenido faltante y la recuperación de información distribuida, en Actas de SIGIR 2005. ",
            "candidates": [],
            "error": [
                [
                    "divergencia KL",
                    "divergencia de Kullback-Leibler"
                ]
            ]
        },
        "jensen-shannon divergence": {
            "translated_key": "divergencia de Jensen-Shannon",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Query Performance Prediction in Web Search Environments Yun Zhou and W. Bruce Croft Department of Computer Science University of Massachusetts, Amherst {yzhou, croft}@cs.umass.edu ABSTRACT Current prediction techniques, which are generally designed for content-based queries and are typically evaluated on relatively homogenous test collections of small sizes, face serious challenges in web search environments where collections are significantly more heterogeneous and different types of retrieval tasks exist.",
                "In this paper, we present three techniques to address these challenges.",
                "We focus on performance prediction for two types of queries in web search environments: content-based and Named-Page finding.",
                "Our evaluation is mainly performed on the GOV2 collection.",
                "In addition to evaluating our models for the two types of queries separately, we consider a more challenging and realistic situation that the two types of queries are mixed together without prior information on query types.",
                "To assist prediction under the mixed-query situation, a novel query classifier is adopted.",
                "Results show that our prediction of web query performance is substantially more accurate than the current stateof-the-art prediction techniques.",
                "Consequently, our paper provides a practical approach to performance prediction in realworld web settings.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval -Query formulation General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Query performance prediction has many applications in a variety of information retrieval (IR) areas such as improving retrieval consistency, query refinement, and distributed IR.",
                "The importance of this problem has been recognized by IR researchers and a number of new methods have been proposed for prediction recently [1, 2, 17].",
                "Most work on prediction has focused on the traditional ad-hoc retrieval task where query performance is measured according to topical relevance.",
                "These prediction models are evaluated on TREC document collections which typically consist of no more than one million relatively homogenous newswire articles.",
                "With the popularity and influence of the Web, prediction techniques that will work well for web-style queries are highly preferable.",
                "However, web search environments pose significant challenges to current prediction models that are mainly designed for traditional TREC settings.",
                "Here we outline some of these challenges.",
                "First, web collections, which are much larger than conventional TREC collections, include a variety of documents that are different in many aspects such as quality and style.",
                "Current prediction techniques can be vulnerable to these characteristics of web collections.",
                "For example, the reported prediction accuracy of the ranking robustness technique and the clarity technique on the GOV2 collection (a large web collection) is significantly worse compared to the other TREC collections [1].",
                "Similar prediction accuracy on the GOV2 collection using another technique is reported in [2], confirming the difficult of predicting query performance on a large web collection.",
                "Furthermore, web search goes beyond the scope of the ad-hoc retrieval task based on topical relevance.",
                "For example, the Named-Page (NP) finding task, which is a navigational task, is also popular in web retrieval.",
                "Query performance prediction for the NP task is still necessary since NP retrieval performance is far from perfect.",
                "In fact, according to the report on the NP task of the 2005 Terabyte Track [3], about 40% of the test queries perform poorly (no correct answer in the first 10 search results) even in the best run from the top group.",
                "To our knowledge, little research has explicitly addressed the problem of NP-query performance prediction.",
                "Current prediction models devised for content-based queries will be less effective for NP queries considering the fundamental differences between the two.",
                "Third, in real-world web search environments, user queries are usually a mixture of different types and prior knowledge about the type of each query is generally unavailable.",
                "The mixed-query situation raises new problems for query performance prediction.",
                "For instance, we may need to incorporate a query classifier into prediction models.",
                "Despite these problems, the ability to handle this situation is a crucial step towards turning query performance prediction from an interesting research topic into a practical tool for web retrieval.",
                "In this paper, we present three techniques to address the above challenges that current prediction models face in Web search environments.",
                "Our work focuses on query performance prediction for the content-based (ad-hoc) retrieval task and the name-page finding task in the context of web retrieval.",
                "Our first technique, called weighted information gain (WIG), makes use of both single term and term proximity features to estimate the quality of top retrieved documents for prediction.",
                "We find that WIG offers consistent prediction accuracy across various test collections and query types.",
                "Moreover, we demonstrate that good prediction accuracy can be achieved for the mixed-query situation by using WIG with the help of a query type classifier.",
                "Query feedback and first rank change, which are our second and third prediction techniques, perform well for content-based queries and NP queries respectively.",
                "Our main contributions include: (1) considerably improved prediction accuracy for web content-based queries over several state-of-the-art techniques. (2) new techniques for successfully predicting NP-query performance. (3) a practical and fully automatic solution to predicting mixed-query performance.",
                "In addition, one minor contribution is that we find that the robustness score [1], which was originally proposed for performance prediction, is helpful for query classification. 2.",
                "RELATED WORK As we mentioned in the introduction, a number of prediction techniques have been proposed recently that focus on contentbased queries in the topical relevance (ad-hoc) task.",
                "We know of no published work that addresses other types of queries such as NP queries, let alone a mixture of query types.",
                "Next we review some representative models.",
                "The major difficulty of performance prediction comes from the fact that many factors have an impact on retrieval performance.",
                "Each factor affects performance to a different degree and the overall effect is hard to predict accurately.",
                "Therefore, it is not surprising to notice that simple features, such as the frequency of query terms in the collection [4] and the average IDF of query terms [5], do not predict well.",
                "In fact, most of the successful techniques are based on measuring some characteristics of the retrieved document set to estimate topic difficulty.",
                "For example, the clarity score [6] measures the coherence of a list of documents by the KL-divergence between the query model and the collection model.",
                "The robustness score [1] quantifies another property of a ranked list: the robustness of the ranking in the presence of uncertainty.",
                "Carmel et al. [2] found that the distance measured by the <br>jensen-shannon divergence</br> between the retrieved document set and the collection is significantly correlated to average precision.",
                "Vinay et al. [7] proposed four measures to capture the geometry of the top retrieved documents for prediction.",
                "The most effective measure is the sensitivity to document perturbation, an idea somewhat similar to the robustness score.",
                "Unfortunately, their way of measuring the sensitivity does not perform equally well for short queries and prediction accuracy drops considerably when a state-of-the-art retrieval technique (like Okapi or a language modeling approach) is adopted for retrieval instead of the tf-idf weighting used in their paper [16].",
                "The difficulties of applying these models in web search environments have already been mentioned.",
                "In this paper, we mainly adopt the clarity score and the robustness score as our baselines.",
                "We experimentally show that the baselines, even after being carefully tuned, are inadequate for the web environment.",
                "One of our prediction models, WIG, is related to the Markov random field (MRF) model for information retrieval [8].",
                "The MRF model directly models term dependence and is found be to highly effective across a variety of test collections (particularly web collections) and retrieval tasks.",
                "This model is used to estimate the joint probability distribution over documents and queries, an important part of WIG.",
                "The superiority of WIG over other prediction techniques based on unigram features, which will be demonstrated later in our paper, coincides with that of MRF for retrieval.",
                "In other word, it is interesting to note that term dependence, when being modeled appropriately, can be helpful for both improving and predicting retrieval performance. 3.",
                "PREDICTION MODELS 3.1 Weighted Information Gain (WIG) This section introduces a weighted information gain approach that incorporates both single term and proximity features for predicting performance for both content-based and Named-Page (NP) finding queries.",
                "Given a set of queries Q={Qs} (s=1,2,..N) which includes all possible user queries and a set of documents D={Dt} (t=1,2…M), we assume that each query-document pair (Qs,Dt) is manually judged and will be put in a relevance list if Qs is found to be relevant to Dt.",
                "The joint probability P(Qs,Dt) over queries Q and documents D denotes the probability that pair (Qs,Dt) will be in the relevance list.",
                "Such assumptions are similar to those used in [8].",
                "Assuming that the user issues query Qi ∈Q and the retrieval results in response to Qi is a ranked list L of documents, we calculate the amount of information contained in P(Qs,Dt) with respect to Qi and L by Eq.1 which is a variant of entropy called the weighted entropy[13].",
                "The weights in Eq.1 are solely determined by Qi and L. )1(),(log),(),( , , ∑−= ts tststsLQ DQPDQweightDQH i In this paper, we choose the weights as follows: LindocumentsKtopthecontainsLTwhere otherwise LTDandisifK DQweight K Kt ts )( )2( ,0 )(,/1 ),( ⎩ ⎨ ⎧ ∈= = The cutoff rank K is a parameter in our model that will be discussed later.",
                "Accordingly, Eq.1 can be simplified as follows: )3(),(log 1 ),( )( , ∑∈ −= LTD titsLQ Kt i DQP K DQH Unfortunately, weighted entropy ),(, tsLQ DQH i computed by Eq.3, which represents the amount of information about how likely the top ranked documents in L would be relevant to query Qi on average, cannot be compared across different queries, making it inappropriate for directly predicting query performance.",
                "To mitigate this problem, we come up with a background distribution P(Qs,C) over Q and D by imagining that every document in D is replaced by the same special document C which represents average language usage.",
                "In this paper, C is created by concatenating every document in D. Roughly speaking, C is the collection (the document set) {Dt} without document boundaries.",
                "Similarly, weighted entropy ),(, CQH sLQi calculated by Eq.3 represents the amount of information about how likely an average document (represented by the whole collection) would be relevant to query Qi.",
                "Now we introduce our performance predictor WIG which is the weighted information gain [13] computed as the difference between ),(, tsLQ DQH i and ),(, CQH sLQi .Specifically, given query Qi, collection C and ranked list L of documents, WIG is calculated as follows: )4( ),( ),( log 1 ),( ),( log),( ),(),(),,( )(, ,, ∑∑ ∈ == −= LTD i ti ts s ts ts tsLQsLQi Kt ii CQP DQP KCQP DQP DQweight DQHCQHLCQWIG WIG computed by Eq.4 measures the change in information about the quality of retrieval (in response to query Qi) from an imaginary state that only an average document is retrieved to a posterior state that the actual search results are observed.",
                "We hypothesize that WIG is positively correlated with retrieval effectiveness because high quality retrieval should be much more effective than just returning the average document.",
                "The heart of this technique is how to estimate the joint distribution P(Qs,Dt).",
                "In the language modeling approach to IR, a variety of models can be applied readily to estimate this distribution.",
                "Although most of these models are based on the bagof-words assumption, recent work on modeling term dependence under the language modeling framework have shown consistent and significant improvements in retrieval effectiveness over bagof-words models.",
                "Inspired by the success of incorporating term proximity features into language models, we decide to adopt a good dependence model to estimate the probability P(Qs,Dt).",
                "The model we chose for this paper is Metzler and Crofts Markov Random Field (MRF) model, which has already demonstrated superiority over a number of collections and different retrieval tasks [8,9].",
                "According to the MRF model, log P(Qi, Dt) can be written as )5()|(loglog),(log )( 1 ∑∈ +−= iQF tti DPZDQP ξ ξ ξλ where Z1 is a constant that ensures that P(Qi, Dt) sums up to 1.",
                "F(Qi) consists of a set of features expanded from the original query Qi .",
                "For example, assuming that query Qi is talented student program, F(Qi) includes features like program and talented student.",
                "We consider two kinds of features: single term features T and proximity features P. Proximity features include exact phrase (#1) and unordered window (#uwN) features as described in [8].",
                "Note that F(Qi) is the union of T(Qi) and P(Qi).",
                "For more details on F(Qi) such as how to expand the original query Qi to F(Qi), we refer the reader to [8] and [9].",
                "P(ξ|Dt) denotes the probability that feature ξ will occur in Dt.",
                "More details on P(ξ|Dt) will be provided later in this section.",
                "The choice of λξ is somewhat different from that used in [8] since λξ plays a dual role in our model.",
                "The first role, which is the same as in [8], is to weight between single term and proximity features.",
                "The other role, which is specific to our prediction task, is to normalize the size of F(Qi).We found that the following weight strategy for λξ satisfies the above two roles and generalizes well on a variety of collections and query types. )6( )(, |)(| 1 )(, |)(| ⎪ ⎪ ⎩ ⎪ ⎪ ⎨ ⎧ ∈ − ∈ = i i T i i T QP QP QT QT ξ λ ξ λ λξ where |T(Qi)| and |P(Qi)| denote the number of single term and proximity features in F(Qi) respectively.",
                "The reason for choosing the square root function in the denominator of λξ is to penalize a feature set of large size appropriately, making WIG more comparable across queries of various lengths. λT is a fixed parameter and set to 0.8 according to [8] throughout this paper.",
                "Similarly, log P(Qi,C) can be written as: )7()|(loglog),(log )( 2 ∑∈ +−= iQF i CPZCQP ξ ξ ξλ When constant Z1 and Z2 are dropped, WIG computed in Eq.4 can be rewritten as follows by plugging in Eq.5 and Eq.7 : )8( )|( )|( log 1 ),,( )( )( ∑ ∑∈ ∈ = LTD QF t i Kt i CP DP K LCQWIG ξ ξ ξ ξ λ One of the advantages of WIG over other techniques is that it can handle well both content-based and NP queries.",
                "Based on the type (or the predicted type) of Qi, the calculation of WIG in Eq. 8 differs in two aspects: (1) how to estimate P(ξ|Dt) and P(ξ|C), and (2) how to choose K. For content-based queries, P(ξ|C) is estimated by the relative frequency of feature ξ in collection C as a whole.",
                "The estimation of P(ξ|Dt) is the same as in [8].",
                "Namely, we estimate P(ξ|Dt) by the relative frequency of feature ξ in Dt linearly smoothed with collection frequency P(ξ|C).",
                "K in Eq.8 is treated as a free parameter.",
                "Note that K is the only free parameter in the computation of WIG for content-based queries because all parameters involved in P(ξ|Dt) are assumed to be fixed by taking the suggested values in [8].",
                "Regarding NP queries, we make use of document structure to estimate P(ξ|Dt) and P(ξ|C) by the so-called mixture of language models proposed in [10] and incorporated into the MRF model for Named-Page finding retrieval in [9].",
                "The basic idea is that a document (collection) is divided into several fields such as the title field, the main-body field and the heading field.",
                "P(ξ|Dt) and P(ξ|C) are estimated by a linear combination of the language models from each field.",
                "Due to space constraints, we refer the reader to [9] for details.",
                "We adopt the exact same set of parameters as used in [9] for estimation.",
                "With regard to K in Eq.8, we set K to 1 because the Named-Page finding task heavily focuses on the first ranked document.",
                "Consequently, there are no free parameters in the computation of WIG for NP queries. 3.2 Query Feedback In this section, we introduce another technique called query feedback (QF) for prediction.",
                "Suppose that a user issues query Q to a retrieval system and a ranked list L of documents is returned.",
                "We view the retrieval system as a noisy channel.",
                "Specifically, we assume that the output of the channel is L and the input is Q.",
                "After going through the channel, Q becomes corrupted and is transformed to ranked list L. By thinking about the retrieval process this way, the problem of predicting retrieval effectiveness turns to the task of evaluating the quality of the channel.",
                "In other words, prediction becomes finding a way to measure the degree of corruption that arises when Q is transformed to L. As directly computing the degree of the corruption is difficult, we tackle this problem by approximation.",
                "Our main idea is that we measure to what extent information on Q can be recovered from L on the assumption that only L is observed.",
                "Specifically, we design a decoder that can accurately translate L back into new query Q and the similarity S between the original query Q and the new query Q is adopted as a performance predictor.",
                "This is a sketch of how the QF technique predicts query performance.",
                "Before filling in more details, we briefly discuss why this method would work.",
                "There is a relation between the similarity S defined above and retrieval performance.",
                "On the one hand, if the retrieval has strayed from the original sense of the query Q, the new query Q extracted from ranked list L in response to Q would be very different from the original query Q.",
                "On the other hand, a query distilled from a ranked list containing many relevant documents is likely to be similar to the original query.",
                "Further examples in support of the relation will be provided later.",
                "Next we detail how to build the decoder and how to measure the similarity S. In essence, the goal of the decoder is to compress ranked list L into a few informative terms that should represent the content of the top ranked documents in L. Our approach to this goal is to represent ranked list L by a language model (distribution over terms).",
                "Then terms are ranked by their contribution to the language models KL (Kullback-Leibler) divergence from the background collection model.",
                "Top ranked terms will be chosen to form the new query Q.",
                "This approach is similar to that used in Section 4.1 of [11].",
                "Specifically, we take three steps to compress ranked list L into query Q without referring to the original query. 1.",
                "We adopt the ranked list language model [14], to estimate a language model based on ranked list L. The model can be written as: )9()|()|()|( ∑∈ = LD LDPDwPLwP where w is any term, D is a document.",
                "P(D|L) is estimated by a linearly decreasing function of the rank of document D. 2.",
                "Each term in P(w|L) is ranked by the following KL-divergence contribution: )10( )|( )|( log)|( CwP LwP LwP where P(w|C) is the collection model estimated by the relative frequency of term w in collection C as a whole. 3.",
                "The top N ranked terms by Eq.10 form a weighted query Q={(wi,ti)} i=1,N. where wi denotes the i-th ranked term and weight ti is the KL-divergence contribution of wi in Eq. 10.",
                "Term cruise ship vessel sea passenger KL contribution 0.050 0.040 0.012 0.010 0.009 Table 1: top 5 terms compressed from the ranked list in response to query Cruise ship damage sea life Two representative examples, one for a poorly performing query Cruise ship damage sea life (TREC topic 719; average precision: 0.08) and the other for a high performing query prostate cancer treatments( TREC topic 710; average precision: 0.49), are shown in Table 1 and 2 respectively.",
                "These examples indicate how the similarity between the original and the new query correlates with retrieval performance.",
                "The parameter N in step 3 is set to 20 empirically and choosing a larger value of N is unnecessary since the weights after the top 20 are usually too small to make any difference.",
                "Term prostate cancer treatment men therapy KL contribution 0.177 0.140 0.028 0.025 0.020 Table 2: top 5 terms compressed from the ranked list in response to query prostate cancer treatments To measure the similarity between original query Q and new query Q, we first use Q to do retrieval on the same collection.",
                "A variant of the query likelihood model [15] is adopted for retrieval.",
                "Namely, documents are ranked by: )11()|()|( ),( ∑∈ = Qtw t i ii i DwPDQP where wi is a term in Q and ti is the associated weight.",
                "D is a document.",
                "Let L denote the new ranked list returned from the above retrieval.",
                "The similarity is measured by the overlap of documents in L and L. Specifically, the percentage of documents in the top K documents of L that are also present in the top K documents in L. the cutoff K is treated as a free parameter.",
                "We summarize here how the QF technique predicts performance given a query Q and the associated ranked list L. We first obtain a weighted query Q compressed from L by the above three steps.",
                "Then we use Q to perform retrieval and the new ranked list is L. The overlap of documents in L and L is used for prediction. 3.3 First Rank Change (FRC) In this section, we propose a method called the first rank change (FRC) for performance prediction for NP queries.",
                "This method is derived from the ranking robustness technique [1] that is mainly designed for content-based queries.",
                "When directly applied to NP queries, the robustness technique will be less effective because it takes the top ranked documents as a whole into account while NP queries usually have only one single relevant document.",
                "Instead, our technique focuses on the first rank document while the main idea of the robustness method remains.",
                "Specifically, the pseudocode for computing FRC is shown in figure 1.",
                "Input: (1) ranked list L={Di} where i=1,100.",
                "Di denotes the i-th ranked document. (2) query Q 1 initialize: (1) set the number of trials J=100000 (2) counter c=0; 2 for i=1 to J 3 Perturb every document in L, let the outcome be a set F={Di} where Di denotes the perturbed version of Di. 4 Do retrieval with query Q on set F 5 c=c+1 if and only if D1 is ranked first in step 4 6 end of for 7 return the ratio c/J Figure 1: pseudo-code for computing FRC FRC approximates the probability that the first ranked document in the original list L will remain ranked first even after the documents are perturbed.",
                "The higher the probability is, the more confidence we have in the first ranked document.",
                "On the other hand, in the extreme case of a random ranking, the probability would be as low as 0.5.",
                "We expect that FRC has a positive association with NP query performance.",
                "We adopt [1] to implement the document perturbation step (step 4 in Fig.1) using Poisson distributions.",
                "For more details, we refer the reader to [1]. 4.",
                "EVALUATION We now present the results of predicting query performance by our models.",
                "Three state-of-the-art techniques are adopted as our baselines.",
                "We evaluate our techniques across a variety of Web retrieval settings.",
                "As mentioned before, we consider two types of queries, that is, content-based (CB) queries and Named-Page(NP) finding queries.",
                "First, suppose that the query types are known.",
                "We investigate the correlation between the predicted retrieval performance and the actual performance for both types of queries separately.",
                "Results show that our methods yield considerable improvements over the baselines.",
                "We then consider a more challenging scenario where no prior information on query types is available.",
                "Two sub-cases are considered.",
                "In the first one, there exists only one type of query but the actual type is unknown.",
                "We assume a mixture of the two query types in the second case.",
                "We demonstrate that our models achieve good accuracy under this demanding scenario, making prediction practical in a real-world Web search environment. 4.1 Experimental Setup Our evaluation focuses on the GOV2 collection which contains about 25 million documents crawled from web sites in the .gov domain during 2004 [3].",
                "We create two kinds of data set for CB queries and NP queries respectively.",
                "For the CB type, we use the ad-hoc topics of the Terabyte Tracks of 2004, 2005 and 2006 and name them TB04-adhoc, TB05-adhoc and TB06-adhoc respectively.",
                "In addition, we also use the ad-hoc topics of the 2004 Robust Track (RT04) to test the adaptability of our techniques to a non-Web environment.",
                "For NP queries, we use the Named-Page finding topics of the Terabyte Tracks of 2005 and 2006 and we name them TB05-NP and TB06-NP respectively.",
                "All queries used in our experiments are titles of TREC topics as we center on web retrieval.",
                "Table 3 summarizes the above data sets.",
                "Name Collection Topic Number Query Type TB04-adhoc GOV2 701-750 CB TB05-adhoc GOV2 751-800 CB TB06-adhoc GOV2 801-850 CB RT04 Disk 4+5 (minus CR) 301-450;601700 CB TB05-NP GOV2 NP601-NP872 NP TB06-NP GOV2 NP901-NP1081 NP Table 3: Summary of test collections and topics Retrieval performance of individual content-based and NP queries is measured by the average precision and reciprocal rank of the first correct answer respectively.",
                "We make use of the Markov Random field model for both ad-hoc and Named-Page finding retrieval.",
                "We adopt the same setting of retrieval parameters used in [8,9].",
                "The Indri search engine [12] is used for all of our experiments.",
                "Though not reported here, we also tried the query likelihood model for ad-hoc retrieval and found that the results change little because of the very high correlation between the query performances obtained by the two retrieval models (0.96 measured by Pearsons coefficient). 4.2 Known Query Types Suppose that query types are known.",
                "We treat each type of query separately and measure the correlation with average precision (or the reciprocal rank in the case of NP queries).",
                "We adopt the Pearsons correlation test which reflects the degree of linear relationship between the predicted and the actual retrieval performance. 4.2.1 Content-based Queries Methods Clarity Robust JSD WIG QF WIG +QF TB04+0 5 adhoc 0.333 0.317 0.362 0.574 0.480 0.637 TB06 adhoc 0.076 0.294 N/A 0.464 0.422 0.511 Table 4: Pearsons correlation coefficients for correlation with average precision on the Terabyte Tracks (ad-hoc) for clarity score, robustness score, the JSD-based method(we directly cites the score reported in [2]), WIG, query feedback(QF) and a linear combination of WIG and QF.",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Table 4 shows the correlation with average precision on two data sets: one is a combination of TB04-adhoc and TB05-adhoc(100 topics in total) and the other is TB06-adhoc (50 topics).",
                "The reason that we put TB04-adhoc and TB05-adhoc together is to make our results comparable to [2].",
                "Our baselines are the clarity score (clarity) [6],the robustness score (robust)[1] and the JSDbased method (JSD) [2].",
                "For the clarity and robustness score, we have tried different parameter settings and report the highest correlation coefficients we have found.",
                "We directly cite the result of the JSD-based method reported in [2].",
                "The table also shows the results for the Weighted Information Gain (WIG) method and the Query Feedback (QF) method for predicting content-based queries.",
                "As we described in the previous section, both WIG and QF have one free parameter to set, that is, the cutoff rank K. We train the parameter on one dataset and test on the other.",
                "When combining WIG and QF, a simple linear combination is used and the combination weight is learned from the training data set.",
                "From these results, we can see that our methods are considerably more accurate compared to the baselines.",
                "We also observe that further improvements are obtained from the combination of WIG and QF, suggesting that they measure different properties of the retrieval process that relate to performance.",
                "We discover that our methods generalize well on TB06-adhoc while the correlation for the clarity score with retrieval performance on this data set is considerably worse.",
                "Further investigation shows that the mean average precision of TB06-adhoc is 0.342 and is about 10% better than that of the first data set.",
                "While the other three methods typically consider the top 100 or less documents given a ranked list, the clarity method usually needs the top 500 or more documents to adequately measure the coherence of a ranked list.",
                "Higher mean average precision makes ranked lists retrieved by different queries more similar in terms of coherence at the level of top 500 documents.",
                "We believe that this is the main reason for the low accuracy of the clarity score on the second data set.",
                "Though this paper focuses on a Web search environment, it is desirable that our techniques will work consistently well in other situations.",
                "To this end, we examine the effectiveness of our techniques on the Robust 2004 Track.",
                "For our methods, we evenly divide all of the test queries into five groups and perform five-fold cross validation.",
                "Each time we use one group for training and the remaining four groups for testing.",
                "We make use of all of the queries for our two baselines, that is, the clarity score and the robustness score.",
                "The parameters for our baselines are the same as those used in [1].The results shown in Table 5 demonstrate that the prediction accuracy of our methods is on a par with that of the two strong baselines.",
                "Clarity Robust WIG QF 0.464 0.539 0.468 0.464 Table 5: Comparison of Pearsons correlation coefficients on the 2004 Robust Track for clarity score, robustness score, WIG and query feedback (QF).",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Furthermore, we examine the prediction sensitivity of our methods to the cutoff rank K. With respect to WIG, it is quite robust to K on the Terabyte Tracks (2004-2006) while it prefers a small value of K like 5 on the 2004 Robust Track.",
                "In other words, a small value of K is a nearly-optimal choice for both kinds of tracks.",
                "Considering the fact that all other parameters involved in WIG are fixed and consequently the same for the two cases, this means WIG can achieve nearly-optimal prediction accuracy in two considerably different situations with exactly the same parameter setting.",
                "Regarding QF, it prefers a larger value of K such as 100 on the Terabyte Tracks and a smaller value of K such as 25 on the 2004 Robust Track. 4.2.2 NP Queries We adopt WIG and first rank change (FRC) for predicting NPquery performance.",
                "We also try a linear combination of the two as in the previous section.",
                "The combination weight is obtained from the other data set.",
                "We use the correlation with the reciprocal ranks measured by the Pearsons correlation test to evaluate prediction quality.",
                "The results are presented in Table 6.",
                "Again, our baselines are the clarity score and the robustness score.",
                "To make a fair comparison, we tune the clarity score in different ways.",
                "We found that using the first ranked document to build the query model yields the best prediction accuracy.",
                "We also attempted to utilize document structure by using the mixture of language models mentioned in section 3.1.",
                "Little improvement was obtained.",
                "The correlation coefficients for the clarity score reported in Table 6 are the best we have found.",
                "As we can see, our methods considerably outperform the clarity score technique on both of the runs.",
                "This confirms our intuition that the use of a coherence-based measure like the clarity score is inappropriate for NP queries.",
                "Methods Clarity Robust.",
                "WIG FRC WIG+FRC TB05-NP 0.150 -0.370 0.458 0.440 0.525 TB06-NP 0.112 -0.160 0.478 0.386 0.515 Table 6: Pearsons correlation coefficients for correlation with reciprocal ranks on the Terabyte Tracks (NP) for clarity score, robustness score, WIG, the first rank change (FRC) and a linear combination of WIG and FRC.",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Regarding the robustness score, we also tune the parameters and report the best we have found.",
                "We observe an interesting and surprising negative correlation with reciprocal ranks.",
                "We explain this finding briefly.",
                "A high robustness score means that a number of top ranked documents in the original ranked list are still highly ranked after perturbing the documents.",
                "The existence of such documents is a good sign of high performance for content-based queries as these queries usually contain a number of relevant documents [1].",
                "However, with regard to NP queries, one fundamental difference is that there is only one relevant document for each query.",
                "The existence of such documents can confuse the ranking function and lead to low retrieval performance.",
                "Although the negative correlation with retrieval performance exists, the strength of the correlation is weaker and less consistent compared to our methods as shown in Table 6.",
                "Based on the above analysis, we can see that current prediction techniques like clarity score and robustness score that are mainly designed for content-based queries face significant challenges and are inadequate to deal with NP queries.",
                "Our two techniques proposed for NP queries consistently demonstrate good prediction accuracy, displaying initial success in solving the problem of predicting performance for NP queries.",
                "Another point we want to stress is that the WIG method works well for both types of queries, a desirable property that most prediction techniques lack. 4.3 Unknown Query Types In this section, we run two kinds of experiments without access to query type labels.",
                "First, we assume that only one type of query exists but the type is unknown.",
                "Second, we experiment on a mixture of content-based and NP queries.",
                "The following two subsections will report results for the two conditions respectively. 4.3.1 Only One Type exists We assume that all queries are of the same type, that is, they are either NP queries or content-based queries.",
                "We choose WIG to deal with this case because it shows good prediction accuracy for both types of queries in the previous section.",
                "We consider two cases: (1) CB: all 150 title queries from the ad-hoc task of the Terabyte Tracks 2004-2006 (2)NP: all 433 NP queries from the named page finding task of the Terabyte Tracks 2005 and 2006.",
                "We take a simple strategy by labeling all of the queries in each case as the same type (either NP or CB) regardless of their actual type.",
                "The computation of WIG will be based on the labeled query type instead of the actual type.",
                "There are four possibilities with respect to the relation between the actual type and the labeled type.",
                "The correlation with retrieval performance under the four possibilities is presented in Table 7.",
                "For example, the value 0.445 at the intersection between the second row and the third column shows the Pearsons correlation coefficient for correlation with average precision when the content-based queries are incorrectly labeled as the NP type.",
                "Based on these results, we recommend treating all queries as the NP type when only one query type exists and accurate query classification is not feasible, considering the risk that a large loss of accuracy will occur if NP queries are incorrectly labeled as content-based queries.",
                "These results also demonstrate the strong adaptability of WIG to different query types.",
                "CB (labeled) NP (labeled) CB (actual) 0.536 0.445 NP (actual) 0.174 0.467 Table 7: Comparison of Pearsons correlation coefficients for correlation with retrieval performance under four possibilities on the Terabyte Tracks (NP).",
                "Bold cases mean the results are statistically significant at the 0.01 level. 4.3.2 A mixture of contented-based and NP queries A mixture of the two types of queries is a more realistic situation that a Web search engine will meet.",
                "We evaluate prediction accuracy by how accurately poorly-performing queries can be identified by the prediction method assuming that actual query types are unknown (but we can predict query types).",
                "This is a challenging task because both the predicted and actual performance for one type of query can be incomparable to that for the other type.",
                "Next we discuss how to implement our evaluation.",
                "We create a query pool which consists of all of the 150 ad-hoc title queries from Terabyte Track 2004-2006 and all of the 433 NP queries from Terabyte Track 2005&2006.",
                "We divide the queries in the pool into classes: good (better than 50% of the queries of the same type in terms of retrieval performance) and bad (otherwise).",
                "According to these standards, a NP query with the reciprocal rank above 0.2 or a content-based query with the average precision above 0.315 will be considered as good.",
                "Then, each time we randomly select one query Q from the pool with probability p that Q is contented-based.",
                "The remaining queries are used as training data.",
                "We first decide the type of query Q according to a query classifier.",
                "Namely, the query classifier tells us whether query Q is NP or content-based.",
                "Based on the predicted query type and the score computed for query Q by a prediction technique, a binary decision is made about whether query Q is good or bad by comparing to the score threshold of the predicted query type obtained from the training data.",
                "Prediction accuracy is measured by the accuracy of the binary decision.",
                "In our implementation, we repeatedly take a test query from the query pool and prediction accuracy is computed as the percentage of correct decisions, that is, a good(bad) query is predicted to be good (bad).",
                "It is obvious that random guessing will lead to 50% accuracy.",
                "Let us take the WIG method for example to illustrate the process.",
                "Two WIG thresholds (one for NP queries and the other for content-based queries) are trained by maximizing the prediction accuracy on the training data.",
                "When a test query is labeled as the NP (CB) type by the query type classifier, it will be predicted to be good if and only if the WIG score for this query is above the NP (CB) threshold.",
                "Similar procedures will be taken for other prediction techniques.",
                "Now we briefly introduce the automatic query type classifier used in this paper.",
                "We find that the robustness score, though originally proposed for performance prediction, is a good indicator of query types.",
                "We find that on average content-based queries have a much higher robustness score than NP queries.",
                "For example, Figure 2 shows the distributions of robustness scores for NP and content-based queries.",
                "According to this finding, the robustness score classifier will attach a NP (CB) label to the query if the robustness score for the query is below (above) a threshold trained from the training data. 0 0.5 1 1.5 2 2.5 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 NP Content-based Figure 2: Distribution of robustness scores for NP and CB queries.",
                "The NP queries are the 252 NP topics from the 2005 Terabyte Track.",
                "The content-based queries are the 150 ad-hoc title from the Terabyte Tracks 2004-2006.",
                "The probability distributions are estimated by the Kernel density estimation method.",
                "Strategies Robust WIG-1 WIG-2 WIG-3 Optimal p=0.6 0.565 0.624 0.665 0.684 0.701 P=0.4 0.567 0.633 0.654 0.673 0.696 Table 8: Comparison of prediction accuracy for five strategies in the mixed-query situation.",
                "Two ways to sample a query from the pool: (1) the sampled query is content-based with the probability p=0.6. (that is, the query is NP with probability 0.4 ) (2) set the probability p=0.4.",
                "We consider five strategies in our experiments.",
                "In the first strategy (denoted by robust), we use the robustness score for query performance prediction with the help of a perfect query classifier that always correctly map a query into one of the two categories (that is, NP or CB).",
                "This strategy represents the level of prediction accuracy that current prediction techniques can achieve in an ideal condition that query types are known.",
                "In the next following three strategies, the WIG method is adopted for performance prediction.",
                "The difference among the three is that three different query classifiers are used for each strategy: (1) the classifier always classifies a query into the NP type. (2) the Robustness Score ProbabilityDensity classifier is the robust score classifier mentioned above. (3) the classifier is a perfect one.",
                "These three strategies are denoted by WIG-1, WIG-2 and WIG-3 respectively.",
                "The reason we are interested in WIG-1 is based on the results from section 4.3.1.",
                "In the last strategy (denoted by Optimal) which serves as an upper bound on how well we can do so far, we fully make use of our prediction techniques for each query type assuming a perfect query classifier is available.",
                "Specifically, we linearly combine WIG and QF for content-based queries and WIG and FRC for NP queries.",
                "The results for the five strategies are shown in Table 8.",
                "For each strategy, we try two ways to sample a query from the pool: (1) the sampled query is CB with probability p=0.6. (the query is NP with probability 0.4) (2) set the probability p=0.4.",
                "From Table 8 We can see that in terms of prediction accuracy WIG-2 (the WIG method with the automatic query classifier) is not only better than the first two cases, but also is close to WIG-3 where a perfect classifier is assumed.",
                "Some further improvements over WIG-3 are observed when combined with other prediction techniques.",
                "The merit of WIG-2 is that it provides a practical solution to automatically identifying poorly performing queries in a Web search environment with mixed query types, which poses considerable obstacles to traditional prediction techniques. 5.",
                "CONCLUSIONS AND FUTURE WORK To our knowledge, our paper is the first to thoroughly explore prediction of query performance in web search environments.",
                "We demonstrated that our models resulted in higher prediction accuracy than previously published techniques not specially devised for web search scenarios.",
                "In this paper, we focus on two types of queries in web search: content-based and Named-Page (NP) finding queries, corresponding to the ad-hoc retrieval task and the Named-Page finding task respectively.",
                "For both types of web queries, our prediction models were shown to be substantially more accurate than the current state-of-the-art techniques.",
                "Furthermore, we considered a more realistic case that no prior information on query types is available.",
                "We demonstrated that the WIG method is particularly suitable for this situation.",
                "Considering the adaptability of WIG to a range of collections and query types, one of our future plans is to apply this method to predict user preference of search results on realistic data collected from a commercial search engine.",
                "Other than accuracy, another major issue that prediction techniques have to deal with in a Web environment is efficiency.",
                "Fortunately, since the WIG score is computed just over the terms and the phrases that appear in the query, this calculation can be made very efficient with the support of index.",
                "On the other hand, the computation of QF and FRC is relatively less efficient since QF needs to retrieve the whole collection twice and FRC needs to repeatedly rank the perturbed documents.",
                "How to improve the efficiency of QF and FRC is our future work.",
                "In addition, the prediction techniques proposed in this paper have the potential of improving retrieval performance by combining with other IR techniques.",
                "For example, our techniques can be incorporated to popular query modification techniques such as query expansion and query relaxation.",
                "Guided by performance prediction, we can make a better decision on when to or how to modify queries to enhance retrieval effectiveness.",
                "We would like to carry out research in this direction in the future. 6.",
                "ACKNOWLEGMENTS This work was supported in part by the Center for Intelligent Information Retrieval, in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C0023, and in part by an award from Google.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor.",
                "In addition, we thank Donald Metzler for his valuable comments on this work. 7.",
                "REFERENCES [1] Y. Zhou ,W. B. Croft ,Ranking Robustness: A Novel Framework to Predict Query Performance, in Proceedings of CIKM 2006. [2] D.Carmel, E.Yom-Tov, A.Darlow,D.Pelleg, What Makes a Query Difficult?, in Proceedings of SIGIR 2006. [3] C.L.A.",
                "Clarke, F. Scholer, I.Soboroff, The TREC 2005 Terabyte Track, In the Online Proceedings of 2005 TREC. [4] B.",
                "He and I.Ounis.",
                "Inferring query performance using preretrieval predictors.",
                "In proceedings of the SPIRE 2004. [5] S. Tomlinson.",
                "Robust, Web and Terabyte Retrieval with Hummingbird SearchServer at TREC 2004.",
                "In the Online Proceedings of 2004 TREC. [6] S. Cronen-Townsend, Y. Zhou, W. B. Croft, Predicting Query Performance, in Proceedings of SIGIR 2002. [7] V.Vinay, I.J.Cox, N.Mill-Frayling,K.Wood, On Ranking the Effectiveness of Searcher, in Proceedings of SIGIR 2006. [8] D.Metzler, W.B.Croft, A Markov Random Filed Model for Term Dependencies, in Proceedings of SIGIR 2005. [9] D.Metzler, T.Strohman,Y.Zhou,W.B.Croft, Indri at TREC 2005: Terabyte Track, In the Online Proceedings of 2004 TREC. [10] P. Ogilvie and J. Callan, Combining document representations for known-item search, in Proceedings of SIGIR 2003. [11] A.Berger, J.Lafferty, Information retrieval as statistical translation, in Proceedings of SIGIR 1999. [12] Indri search engine : http://www.lemurproject.org/indri/ [13] I.J.",
                "Taneja: On Generalized Information Measures and Their Applications, Advances in Electronics and Electron Physics, Academic Press (USA), 76, 1989, 327-413. [14] S.Cronen-Townsend, Y. Zhou and Croft, W. B. , A Framework for Selective Query Expansion, in Proceedings of CIKM 2004. [15] F.Song, W.B.Croft, A general language model for information retrieval, in Proceedings of SIGIR 1999. [16] Personal email contact with Vishwa Vinay and our own experiments [17] E.Yom-Tov, S.Fine, D.Carmel, A.Darlow, Learning to Estimate Query Difficulty Including Applications to Missing Content Detection and Distributed Information retrieval, in Proceedings of SIGIR 2005"
            ],
            "original_annotated_samples": [
                "Carmel et al. [2] found that the distance measured by the <br>jensen-shannon divergence</br> between the retrieved document set and the collection is significantly correlated to average precision."
            ],
            "translated_annotated_samples": [
                "Carmel et al. [2] encontraron que la distancia medida por la <br>divergencia de Jensen-Shannon</br> entre el conjunto de documentos recuperados y la colección está significativamente correlacionada con la precisión promedio."
            ],
            "translated_text": "Predicción del rendimiento de consultas en entornos de búsqueda web Yun Zhou y W. Bruce Croft Departamento de Ciencias de la Computación Universidad de Massachusetts, Amherst {yzhou, croft}@cs.umass.edu RESUMEN Las técnicas actuales de predicción, generalmente diseñadas para consultas basadas en contenido y evaluadas típicamente en colecciones de prueba relativamente homogéneas de tamaños pequeños, enfrentan serios desafíos en entornos de búsqueda web donde las colecciones son significativamente más heterogéneas y existen diferentes tipos de tareas de recuperación. En este artículo, presentamos tres técnicas para abordar estos desafíos. Nos enfocamos en la predicción de rendimiento para dos tipos de consultas en entornos de búsqueda web: basadas en contenido y búsqueda de páginas por nombre. Nuestra evaluación se realiza principalmente en la colección GOV2. Además de evaluar nuestros modelos para los dos tipos de consultas por separado, consideramos una situación más desafiante y realista en la que los dos tipos de consultas se mezclan sin información previa sobre los tipos de consulta. Para ayudar en la predicción bajo la situación de consulta mixta, se adopta un clasificador de consultas novedoso. Los resultados muestran que nuestra predicción del rendimiento de la consulta web es sustancialmente más precisa que las técnicas de predicción actuales más avanzadas. Por consiguiente, nuestro artículo proporciona un enfoque práctico para predecir el rendimiento en entornos web del mundo real. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Formulación de Consultas Términos Generales Algoritmos, Experimentación, Teoría 1. La predicción del rendimiento de la consulta tiene muchas aplicaciones en una variedad de áreas de recuperación de información (IR), como mejorar la consistencia de la recuperación, el refinamiento de la consulta y la IR distribuida. La importancia de este problema ha sido reconocida por los investigadores en Recuperación de Información y recientemente se han propuesto varios métodos nuevos para la predicción [1, 2, 17]. La mayoría de los trabajos sobre predicción se han centrado en la tarea tradicional de recuperación ad-hoc, donde el rendimiento de la consulta se mide según la relevancia temática. Estos modelos de predicción se evalúan en colecciones de documentos de TREC que suelen consistir en no más de un millón de artículos de agencias de noticias relativamente homogéneos. Con la popularidad e influencia de la Web, las técnicas de predicción que funcionen bien para consultas de estilo web son altamente preferibles. Sin embargo, los entornos de búsqueda web plantean desafíos significativos para los modelos de predicción actuales que están principalmente diseñados para entornos tradicionales de TREC. Aquí delineamos algunos de estos desafíos. Primero, las colecciones web, que son mucho más grandes que las colecciones TREC convencionales, incluyen una variedad de documentos que son diferentes en muchos aspectos, como la calidad y el estilo. Las técnicas actuales de predicción pueden ser vulnerables a estas características de las colecciones web. Por ejemplo, la precisión de predicción reportada de la técnica de robustez de clasificación y la técnica de claridad en la colección GOV2 (una colección web grande) es significativamente peor en comparación con las otras colecciones de TREC [1]. En [2] se informa de una precisión de predicción similar en la colección GOV2 utilizando otra técnica, confirmando la dificultad de predecir el rendimiento de consultas en una gran colección web. Además, la búsqueda web va más allá del alcance de la tarea de recuperación ad hoc basada en la relevancia temática. Por ejemplo, la tarea de encontrar páginas nombradas (NP), que es una tarea de navegación, también es popular en la recuperación web. La predicción del rendimiento de la consulta para la tarea de NP sigue siendo necesaria, ya que el rendimiento de recuperación de NP está lejos de ser perfecto. De hecho, según el informe sobre la tarea NP del Terabyte Track de 2005 [3], aproximadamente el 40% de las consultas de prueba tienen un rendimiento deficiente (sin respuesta correcta en los primeros 10 resultados de búsqueda) incluso en la mejor ejecución del grupo líder. Hasta donde sabemos, poca investigación ha abordado explícitamente el problema de la predicción del rendimiento de consultas de NP. Los modelos de predicción actuales diseñados para consultas basadas en contenido serán menos efectivos para consultas de NP considerando las diferencias fundamentales entre ambas. En tercer lugar, en entornos de búsqueda web del mundo real, las consultas de los usuarios suelen ser una mezcla de diferentes tipos y, por lo general, no se dispone de información previa sobre el tipo de cada consulta. La situación de consulta mixta plantea nuevos problemas para la predicción del rendimiento de la consulta. Por ejemplo, es posible que necesitemos incorporar un clasificador de consultas en los modelos de predicción. A pesar de estos problemas, la capacidad para manejar esta situación es un paso crucial hacia convertir la predicción del rendimiento de consultas de un tema de investigación interesante en una herramienta práctica para la recuperación web. En este artículo, presentamos tres técnicas para abordar los desafíos mencionados que enfrentan los modelos de predicción actuales en entornos de búsqueda web. Nuestro trabajo se centra en la predicción del rendimiento de consultas para la tarea de recuperación basada en contenido (ad-hoc) y la tarea de encontrar páginas por nombre en el contexto de la recuperación web. Nuestra primera técnica, llamada ganancia de información ponderada (WIG), utiliza tanto características de términos individuales como de proximidad de términos para estimar la calidad de los documentos recuperados para la predicción. Observamos que WIG ofrece una precisión de predicción consistente en diversas colecciones de pruebas y tipos de consultas. Además, demostramos que se puede lograr una buena precisión de predicción para la situación de consulta mixta utilizando WIG con la ayuda de un clasificador de tipo de consulta. La retroalimentación de consultas y el cambio de rango inicial, que son nuestras técnicas de predicción segunda y tercera, funcionan bien para consultas basadas en contenido y consultas de NP respectivamente. Nuestras principales contribuciones incluyen: (1) una precisión de predicción considerablemente mejorada para consultas basadas en contenido web en comparación con varias técnicas de vanguardia. (2) nuevas técnicas para predecir con éxito el rendimiento de consultas NP. (3) una solución práctica y totalmente automática para predecir el rendimiento de consultas mixtas. Además, una contribución menor es que encontramos que el puntaje de robustez [1], que originalmente se propuso para la predicción de rendimiento, es útil para la clasificación de consultas. 2. TRABAJO RELACIONADO Como mencionamos en la introducción, recientemente se han propuesto varias técnicas de predicción que se centran en consultas basadas en contenido en la tarea de relevancia temática (ad-hoc). No conocemos ningún trabajo publicado que aborde otros tipos de consultas como las consultas NP, y mucho menos una mezcla de tipos de consultas. A continuación revisamos algunos modelos representativos. La principal dificultad de la predicción del rendimiento proviene del hecho de que muchos factores tienen un impacto en el rendimiento de recuperación. Cada factor afecta el rendimiento en diferente medida y el efecto general es difícil de predecir con precisión. Por lo tanto, no es sorprendente notar que características simples, como la frecuencia de los términos de consulta en la colección [4] y la IDF promedio de los términos de consulta [5], no predicen bien. De hecho, la mayoría de las técnicas exitosas se basan en medir algunas características del conjunto de documentos recuperados para estimar la dificultad del tema. Por ejemplo, la puntuación de claridad [6] mide la coherencia de una lista de documentos mediante la divergencia KL entre el modelo de consulta y el modelo de colección. El puntaje de robustez [1] cuantifica otra propiedad de una lista clasificada: la robustez de la clasificación en presencia de incertidumbre. Carmel et al. [2] encontraron que la distancia medida por la <br>divergencia de Jensen-Shannon</br> entre el conjunto de documentos recuperados y la colección está significativamente correlacionada con la precisión promedio. Vinay et al. [7] propusieron cuatro medidas para capturar la geometría de los documentos mejor clasificados para la predicción. La medida más efectiva es la sensibilidad a la perturbación del documento, una idea algo similar a la puntuación de robustez. Desafortunadamente, su forma de medir la sensibilidad no funciona igual de bien para consultas cortas y la precisión de la predicción disminuye considerablemente cuando se adopta una técnica de recuperación de vanguardia (como Okapi o un enfoque de modelado de lenguaje) en lugar del peso tf-idf utilizado en su artículo [16]. Las dificultades de aplicar estos modelos en entornos de búsqueda web ya han sido mencionadas. En este documento, principalmente adoptamos la puntuación de claridad y la puntuación de robustez como nuestras líneas base. Experimentalmente demostramos que las líneas base, incluso después de ser ajustadas cuidadosamente, son inadecuadas para el entorno web. Uno de nuestros modelos de predicción, WIG, está relacionado con el modelo de campo aleatorio de Markov (MRF) para la recuperación de información [8]. El modelo MRF modela directamente la dependencia de términos y se ha demostrado ser altamente efectivo en una variedad de colecciones de pruebas (especialmente colecciones web) y tareas de recuperación. Este modelo se utiliza para estimar la distribución conjunta de probabilidad sobre documentos y consultas, una parte importante de WIG. La superioridad de WIG sobre otras técnicas de predicción basadas en características unigramas, que se demostrará más adelante en nuestro artículo, coincide con la de MRF para recuperación. En otras palabras, es interesante notar que la dependencia de términos, cuando se modela adecuadamente, puede ser útil tanto para mejorar como para predecir el rendimiento de recuperación. 3. Modelos de predicción 3.1 Ganancia de Información Ponderada (WIG) Esta sección introduce un enfoque de ganancia de información ponderada que incorpora tanto características de términos únicos como de proximidad para predecir el rendimiento tanto en consultas basadas en contenido como en consultas de búsqueda de Páginas Nombradas (NP). Dado un conjunto de consultas Q={Qs} (s=1,2,..N) que incluye todas las posibles consultas de usuario y un conjunto de documentos D={Dt} (t=1,2…M), asumimos que cada par consulta-documento (Qs,Dt) es evaluado manualmente y se incluirá en una lista de relevancia si se determina que Qs es relevante para Dt. La probabilidad conjunta P(Qs,Dt) sobre las consultas Q y los documentos D denota la probabilidad de que el par (Qs,Dt) esté en la lista de relevancia. Tales suposiciones son similares a las utilizadas en [8]. Suponiendo que el usuario emite la consulta Qi ∈Q y los resultados de recuperación en respuesta a Qi es una lista clasificada L de documentos, calculamos la cantidad de información contenida en P(Qs,Dt) con respecto a Qi y L mediante la Ec.1, que es una variante de la entropía llamada entropía ponderada[13]. Los pesos en la ecuación 1 están determinados únicamente por Qi y L. En este documento, elegimos los pesos de la siguiente manera: L es el número de documentos que contiene la consulta, K es el límite superior, LT es el límite superior de documentos, y D es el peso si K está en otro caso. La clasificación de corte K es un parámetro en nuestro modelo que se discutirá más adelante. Por lo tanto, la ecuación 1 se puede simplificar de la siguiente manera: )3(),(log 1 ),( )( , ∑∈ −= LTD titsLQ Kt i DQP K DQH Desafortunadamente, la entropía ponderada ),(, tsLQ DQH i calculada por la ecuación 3, que representa la cantidad de información sobre la probabilidad de que los documentos mejor clasificados en L sean relevantes para la consulta Qi en promedio, no se puede comparar entre diferentes consultas, lo que la hace inapropiada para predecir directamente el rendimiento de la consulta. Para mitigar este problema, creamos una distribución de fondo P(Qs,C) sobre Q y D imaginando que cada documento en D es reemplazado por el mismo documento especial C que representa el uso promedio del lenguaje. En este documento, C se crea concatenando cada documento en D. A grosso modo, C es la colección (el conjunto de documentos) {Dt} sin límites de documentos. De manera similar, la entropía ponderada ),(, CQH sLQi calculada por la Ecuación 3 representa la cantidad de información sobre la probabilidad de que un documento promedio (representado por toda la colección) sea relevante para la consulta Qi. Ahora presentamos nuestro predictor de rendimiento WIG que es la ganancia de información ponderada [13] calculada como la diferencia entre ),(, tsLQ DQH i y ),(, CQH sLQi. Específicamente, dado el conjunto de consultas Qi, la colección C y la lista clasificada L de documentos, WIG se calcula de la siguiente manera: )4( ),( ),( log 1 ),( ),( log),( ),(),(),,( )(, ,, ∑∑ ∈ == −= LTD i ti ts s ts ts tsLQsLQi Kt ii CQP DQP KCQP DQP DQweight DQHCQHLCQWIG WIG calculado por la Ecuación 4 mide el cambio en la información sobre la calidad de la recuperación (en respuesta a la consulta Qi) desde un estado imaginario en el que solo se recupera un documento promedio hasta un estado posterior en el que se observan los resultados de búsqueda reales. Hacemos la hipótesis de que WIG está positivamente correlacionado con la efectividad de la recuperación porque una recuperación de alta calidad debería ser mucho más efectiva que simplemente devolver el documento promedio. El corazón de esta técnica es cómo estimar la distribución conjunta P(Qs, Dt). En el enfoque de modelado del lenguaje para la recuperación de información, se pueden aplicar fácilmente una variedad de modelos para estimar esta distribución. Aunque la mayoría de estos modelos se basan en la suposición de la bolsa de palabras, trabajos recientes sobre modelado de dependencia de términos bajo el marco de modelado de lenguaje han demostrado mejoras consistentes y significativas en la efectividad de recuperación sobre los modelos de bolsa de palabras. Inspirados por el éxito de incorporar características de proximidad de términos en modelos de lenguaje, decidimos adoptar un buen modelo de dependencia para estimar la probabilidad P(Qs,Dt). El modelo que elegimos para este artículo es el modelo de Campo Aleatorio de Markov (MRF) de Metzler y Crofts, que ya ha demostrado su superioridad en comparación con varias colecciones y diferentes tareas de recuperación [8,9]. Según el modelo MRF, log P(Qi, Dt) se puede escribir como )5()|(loglog),(log )( 1 ∑∈ +−= iQF tti DPZDQP ξ ξ ξλ donde Z1 es una constante que asegura que P(Qi, Dt) sume 1. F(Qi) consiste en un conjunto de características ampliadas a partir de la consulta original Qi. Por ejemplo, suponiendo que la consulta Qi es el programa de estudiantes talentosos, F(Qi) incluye características como programa y estudiante talentoso. Consideramos dos tipos de características: características de términos individuales T y características de proximidad P. Las características de proximidad incluyen la frase exacta (#1) y las características de ventana desordenada (#uwN) como se describe en [8]. Ten en cuenta que F(Qi) es la unión de T(Qi) y P(Qi). Para obtener más detalles sobre F(Qi), como por ejemplo cómo expandir la consulta original Qi a F(Qi), remitimos al lector a [8] y [9]. P(ξ|Dt) denota la probabilidad de que la característica ξ ocurra en Dt. Más detalles sobre P(ξ|Dt) se proporcionarán más adelante en esta sección. La elección de λξ es algo diferente a la utilizada en [8] ya que λξ desempeña un doble papel en nuestro modelo. El primer rol, que es el mismo que en [8], es ponderar entre las características de términos individuales y de proximidad. El otro rol, específico de nuestra tarea de predicción, es normalizar el tamaño de F(Qi). Encontramos que la siguiente estrategia de peso para λξ satisface los dos roles anteriores y generaliza bien en una variedad de colecciones y tipos de consultas. )6( )(, |)(| 1 )(, |)(| ⎪ ⎪ ⎩ ⎪ ⎪ ⎨ ⎧ ∈ − ∈ = i i T i i T QP QP QT QT ξ λ ξ λ λξ donde |T(Qi)| y |P(Qi)| denotan el número de características de términos únicos y de proximidad en F(Qi) respectivamente. La razón de elegir la función raíz cuadrada en el denominador de λξ es penalizar adecuadamente un conjunto de características de gran tamaño, haciendo que WIG sea más comparable entre consultas de diversas longitudes. λT es un parámetro fijo y se establece en 0.8 según [8] a lo largo de este documento. De manera similar, log P(Qi,C) se puede escribir como: )7()|(loglog),(log )( 2 ∑∈ +−= iQF i CPZCQP ξ ξ ξλ Cuando se eliminan las constantes Z1 y Z2, el WIG calculado en la Ecuación 4 se puede reescribir de la siguiente manera al sustituir la Ecuación 5 y la Ecuación 7: )8( )|( )|( log 1 ),,( )( )( ∑ ∑∈ ∈ = LTD QF t i Kt i CP DP K LCQWIG ξ ξ ξ ξ λ Una de las ventajas del WIG sobre otras técnicas es que puede manejar tanto consultas basadas en contenido como en NP. Basándose en el tipo (o el tipo predicho) de Qi, el cálculo de WIG en la Ecuación 8 difiere en dos aspectos: (1) cómo estimar P(ξ|Dt) y P(ξ|C), y (2) cómo elegir K. Para consultas basadas en contenido, P(ξ|C) se estima por la frecuencia relativa del rasgo ξ en la colección C en su totalidad. La estimación de P(ξ|Dt) es la misma que en [8]. Es decir, estimamos P(ξ|Dt) mediante la frecuencia relativa del rasgo ξ en Dt suavizada linealmente con la frecuencia de colección P(ξ|C). La K en la Ec.8 se trata como un parámetro libre. Ten en cuenta que K es el único parámetro libre en el cálculo de WIG para consultas basadas en contenido, ya que se asume que todos los parámetros involucrados en P(ξ|Dt) están fijos al tomar los valores sugeridos en [8]. En cuanto a las consultas de NP, hacemos uso de la estructura del documento para estimar P(ξ|Dt) y P(ξ|C) mediante el llamado modelo de mezcla de lenguaje propuesto en [10] e incorporado en el modelo MRF para la recuperación de Named-Page en [9]. La idea básica es que un documento (colección) se divide en varios campos como el campo del título, el campo del cuerpo principal y el campo de los encabezados. P(ξ|Dt) y P(ξ|C) se estiman mediante una combinación lineal de los modelos de lenguaje de cada campo. Debido a limitaciones de espacio, remitimos al lector a [9] para más detalles. Adoptamos el mismo conjunto exacto de parámetros utilizados en [9] para la estimación. En cuanto a K en la Ec.8, establecemos K en 1 porque la tarea de encontrar Named-Page se centra fuertemente en el documento clasificado en primer lugar. Por consiguiente, no hay parámetros libres en el cálculo de WIG para consultas de NP. 3.2 Retroalimentación de consultas En esta sección, presentamos otra técnica llamada retroalimentación de consultas (QF) para la predicción. Suponga que un usuario emite la consulta Q a un sistema de recuperación y se devuelve una lista clasificada L de documentos. Vemos el sistema de recuperación como un canal ruidoso. Específicamente, asumimos que la salida del canal es L y la entrada es Q. Después de pasar por el canal, Q se corrompe y se transforma en la lista clasificada L. Al pensar en el proceso de recuperación de esta manera, el problema de predecir la eficacia de la recuperación se convierte en la tarea de evaluar la calidad del canal. En otras palabras, la predicción se convierte en encontrar una forma de medir el grado de corrupción que surge cuando Q se transforma en L. Dado que calcular directamente el grado de corrupción es difícil, abordamos este problema mediante aproximaciones. Nuestra idea principal es que medimos en qué medida la información sobre Q puede ser recuperada de L bajo la suposición de que solo se observa L. Específicamente, diseñamos un decodificador que pueda traducir con precisión L de vuelta a la nueva consulta Q y la similitud S entre la consulta original Q y la nueva consulta Q se adopta como un predictor de rendimiento. Este es un bosquejo de cómo la técnica QF predice el rendimiento de la consulta. Antes de completar más detalles, discutimos brevemente por qué este método funcionaría. Existe una relación entre la similitud S definida anteriormente y el rendimiento de recuperación. Por un lado, si la recuperación se ha desviado del sentido original de la consulta Q, la nueva consulta Q extraída de la lista clasificada L en respuesta a Q sería muy diferente de la consulta original Q. Por otro lado, una consulta destilada de una lista clasificada que contiene muchos documentos relevantes es probable que sea similar a la consulta original. Se proporcionarán más ejemplos en apoyo de la relación más adelante. A continuación detallamos cómo construir el decodificador y cómo medir la similitud S. En esencia, el objetivo del decodificador es comprimir la lista clasificada L en unos pocos términos informativos que deberían representar el contenido de los documentos mejor clasificados en L. Nuestro enfoque para este objetivo es representar la lista clasificada L mediante un modelo de lenguaje (distribución de términos). Luego, los términos se clasifican según su contribución a la divergencia KL (Kullback-Leibler) de los modelos de lenguaje con respecto al modelo de la colección de fondo. Los términos mejor clasificados serán elegidos para formar la nueva consulta Q. Este enfoque es similar al utilizado en la Sección 4.1 de [11]. Específicamente, tomamos tres pasos para comprimir la lista clasificada L en la consulta Q sin hacer referencia a la consulta original. 1. Adoptamos el modelo de lenguaje de lista clasificada [14] para estimar un modelo de lenguaje basado en la lista clasificada L. El modelo puede escribirse como: )9()|()|()|( ∑∈ = LD LDPDwPLwP donde w es cualquier término y D es un documento. La probabilidad de que ocurra el evento D dado el evento L se estima mediante una función linealmente decreciente del rango del documento D. Cada término en P(w|L) está clasificado por la siguiente contribución de divergencia de Kullback-Leibler: )10( )|( )|( log)|( CwP LwP LwP donde P(w|C) es el modelo de colección estimado por la frecuencia relativa del término w en la colección C en su totalidad. 3. Los términos N principales clasificados por Eq.10 forman una consulta ponderada Q={(wi,ti)} i=1,N, donde wi denota el término clasificado en la posición i y el peso ti es la contribución de la divergencia KL de wi en Eq. 10. Barco de crucero daño vida marina. Estos ejemplos indican cómo la similitud entre la consulta original y la nueva se correlaciona con el rendimiento de recuperación. El parámetro N en el paso 3 se establece en 20 de manera empírica y elegir un valor más grande de N es innecesario ya que los pesos después de los primeros 20 suelen ser demasiado pequeños para marcar alguna diferencia. Para medir la similitud entre la consulta original Q y la nueva consulta Q, primero utilizamos Q para recuperar información de la misma colección. Se adopta una variante del modelo de probabilidad de consulta [15] para la recuperación. Es decir, los documentos se clasifican por: )11()|()|( ),( ∑∈ = Qtw t i ii i DwPDQP donde wi es un término en Q y ti es el peso asociado. D es un documento. Que L denote la nueva lista clasificada devuelta desde la recuperación anterior. La similitud se mide por la superposición de documentos en L y L. Específicamente, el porcentaje de documentos en los primeros K documentos de L que también están presentes en los primeros K documentos en L. El valor de corte K se trata como un parámetro libre. Aquí resumimos cómo la técnica QF predice el rendimiento dado una consulta Q y la lista clasificada asociada L. Primero obtenemos una consulta ponderada Q comprimida a partir de L mediante los tres pasos anteriores. Luego usamos Q para realizar la recuperación y la nueva lista clasificada es L. La superposición de documentos en L y L se utiliza para la predicción. 3.3 Primer Cambio de Rango (FRC) En esta sección, proponemos un método llamado primer cambio de rango (FRC) para la predicción de rendimiento para consultas de NP. Este método se deriva de la técnica de robustez de clasificación [1] que está principalmente diseñada para consultas basadas en contenido. Cuando se aplica directamente a consultas de NP, la técnica de robustez será menos efectiva porque tiene en cuenta todos los documentos mejor clasificados en su totalidad, mientras que las consultas de NP suelen tener solo un único documento relevante. En cambio, nuestra técnica se centra en el documento de rango uno mientras se mantiene la idea principal del método de robustez. Específicamente, el seudocódigo para calcular FRC se muestra en la figura 1. Lista clasificada L={Di} donde i=1,100. Di denota el documento clasificado en la posición i. (2) consulta Q 1 inicializar: (1) establecer el número de intentos J=100000 (2) contador c=0; 2 para i=1 a J 3 Perturbar cada documento en L, que el resultado sea un conjunto F={Di} donde Di denota la versión perturbada de Di. 4 Realizar la recuperación con la consulta Q en el conjunto F 5 c=c+1 si y solo si D1 está clasificado primero en el paso 4 6 fin del para 7 devolver la proporción c/J Figura 1: pseudo-código para calcular FRC FRC aproxima la probabilidad de que el documento clasificado en primer lugar en la lista original L permanezca clasificado primero incluso después de que los documentos sean perturbados. Cuanto mayor sea la probabilidad, más confianza tenemos en el documento clasificado en primer lugar. Por otro lado, en el caso extremo de un ranking aleatorio, la probabilidad sería tan baja como 0.5. Esperamos que FRC tenga una asociación positiva con el rendimiento de la consulta de NP. Adoptamos [1] para implementar el paso de perturbación del documento (paso 4 en la Fig. 1) utilizando distribuciones de Poisson. Para más detalles, remitimos al lector a [1]. 4. EVALUACIÓN Ahora presentamos los resultados de predecir el rendimiento de la consulta por nuestros modelos. Tres técnicas de vanguardia son adoptadas como nuestras líneas base. Evaluamos nuestras técnicas en una variedad de configuraciones de recuperación web. Como se mencionó anteriormente, consideramos dos tipos de consultas, es decir, consultas basadas en contenido (CB) y consultas de búsqueda de páginas con nombre (NP). Primero, supongamos que se conocen los tipos de consultas. Investigamos la correlación entre el rendimiento de recuperación predicho y el rendimiento real para ambos tipos de consultas por separado. Los resultados muestran que nuestros métodos producen mejoras considerablemente superiores a los valores de referencia. Luego consideramos un escenario más desafiante donde no hay información previa sobre los tipos de consultas disponibles. Se consideran dos subcasos. En el primero, existe solo un tipo de consulta pero el tipo real es desconocido. Suponemos una mezcla de los dos tipos de consultas en el segundo caso. Demostramos que nuestros modelos logran una buena precisión en este escenario exigente, lo que hace que la predicción sea práctica en un entorno de búsqueda web del mundo real. 4.1 Configuración Experimental Nuestra evaluación se centra en la colección GOV2 que contiene alrededor de 25 millones de documentos obtenidos de sitios web en el dominio .gov durante 2004 [3]. Creamos dos tipos de conjuntos de datos para consultas de CB y consultas de NP respectivamente. Para el tipo CB, utilizamos los temas ad-hoc de las pistas Terabyte de 2004, 2005 y 2006 y los nombramos TB04-adhoc, TB05-adhoc y TB06-adhoc respectivamente. Además, también utilizamos los temas ad-hoc de la pista Robusta de 2004 (RT04) para probar la adaptabilidad de nuestras técnicas a un entorno no web. Para consultas de NP, utilizamos los temas de búsqueda de páginas nombradas de las Terabyte Tracks de 2005 y 2006 y los denominamos TB05-NP y TB06-NP respectivamente. Todas las consultas utilizadas en nuestros experimentos son títulos de temas de TREC, ya que nos centramos en la recuperación web. La Tabla 3 resume los conjuntos de datos anteriores. La recuperación del rendimiento de las consultas individuales basadas en contenido y NP se mide mediante la precisión promedio y la tasa recíproca de la primera respuesta correcta, respectivamente. Hacemos uso del modelo de campo aleatorio de Markov tanto para la recuperación ad-hoc como para la recuperación de páginas nombradas. Adoptamos la misma configuración de parámetros de recuperación utilizada en [8,9]. El motor de búsqueda Indri [12] se utiliza para todos nuestros experimentos. Aunque no se informa aquí, también probamos el modelo de probabilidad de consulta para la recuperación ad-hoc y encontramos que los resultados cambian poco debido a la correlación muy alta entre el rendimiento de las consultas obtenido por los dos modelos de recuperación (0.96 medido por el coeficiente de Pearson). 4.2 Tipos de Consultas Conocidos Supongamos que se conocen los tipos de consultas. Tratamos cada tipo de consulta por separado y medimos la correlación con la precisión promedio (o el rango recíproco en el caso de consultas NP). Adoptamos la prueba de correlación de Pearson que refleja el grado de relación lineal entre el rendimiento de recuperación predicho y real. 4.2.1 Métodos de Consultas Basadas en Contenido Claridad Robusta JSD WIG QF WIG +QF TB04+0 5 adhoc 0.333 0.317 0.362 0.574 0.480 0.637 TB06 adhoc 0.076 0.294 N/A 0.464 0.422 0.511 Tabla 4: Coeficientes de correlación de Pearson para la correlación con la precisión promedio en las Pistas de Terabyte (ad-hoc) para la puntuación de claridad, puntuación de robustez, el método basado en JSD (citamos directamente la puntuación informada en [2]), WIG, retroalimentación de consulta (QF) y una combinación lineal de WIG y QF. Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. La Tabla 4 muestra la correlación con la precisión promedio en dos conjuntos de datos: uno es una combinación de TB04-adhoc y TB05-adhoc (100 temas en total) y el otro es TB06-adhoc (50 temas). La razón por la que juntamos TB04-adhoc y TB05-adhoc es para hacer nuestros resultados comparables a [2]. Nuestros puntos de referencia son la puntuación de claridad (claridad) [6], la puntuación de robustez (robust) [1] y el método basado en JSD (JSD) [2]. Para la puntuación de claridad y robustez, hemos probado diferentes configuraciones de parámetros y reportamos los coeficientes de correlación más altos que hemos encontrado. Citamos directamente el resultado del método basado en JSD reportado en [2]. La tabla también muestra los resultados para el método de Ganancia de Información Ponderada (WIG) y el método de Retroalimentación de Consulta (QF) para predecir consultas basadas en contenido. Como describimos en la sección anterior, tanto WIG como QF tienen un parámetro libre para ajustar, es decir, el rango de corte K. Entrenamos el parámetro en un conjunto de datos y lo probamos en el otro. Al combinar WIG y QF, se utiliza una combinación lineal simple y el peso de la combinación se aprende a partir del conjunto de datos de entrenamiento. A partir de estos resultados, podemos ver que nuestros métodos son considerablemente más precisos en comparación con los baselines. También observamos que se obtienen mejoras adicionales a partir de la combinación de WIG y QF, lo que sugiere que miden diferentes propiedades del proceso de recuperación que se relacionan con el rendimiento. Descubrimos que nuestros métodos se generalizan bien en TB06-adhoc, mientras que la correlación del puntaje de claridad con el rendimiento de recuperación en este conjunto de datos es considerablemente peor. Una investigación adicional muestra que la precisión promedio media de TB06-adhoc es de 0.342 y es aproximadamente un 10% mejor que la del primer conjunto de datos. Mientras que los otros tres métodos suelen considerar los 100 mejores documentos o menos de una lista clasificada, el método de claridad generalmente necesita los 500 mejores documentos o más para medir adecuadamente la coherencia de una lista clasificada. Un promedio de precisión media más alto hace que las listas clasificadas recuperadas por diferentes consultas sean más similares en términos de coherencia en el nivel de los primeros 500 documentos. Creemos que esta es la razón principal de la baja precisión de la puntuación de claridad en el segundo conjunto de datos. Aunque este documento se centra en un entorno de búsqueda web, es deseable que nuestras técnicas funcionen de manera consistente en otras situaciones. Con este fin, examinamos la efectividad de nuestras técnicas en la pista Robust 2004. Para nuestros métodos, dividimos equitativamente todas las consultas de prueba en cinco grupos y realizamos validación cruzada de cinco pliegues. Cada vez que utilizamos un grupo para el entrenamiento y los otros cuatro grupos para las pruebas. Hacemos uso de todas las consultas para nuestros dos puntos de referencia, es decir, la puntuación de claridad y la puntuación de robustez. Los parámetros para nuestras líneas base son los mismos que los utilizados en [1]. Los resultados mostrados en la Tabla 5 demuestran que la precisión de predicción de nuestros métodos está a la par con la de las dos líneas base sólidas. Claridad Robusta WIG QF 0.464 0.539 0.468 0.464 Tabla 5: Comparación de los coeficientes de correlación de Pearson en la pista Robusta de 2004 para la puntuación de claridad, puntuación de robustez, WIG y retroalimentación de consulta (QF). Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. Además, examinamos la sensibilidad de predicción de nuestros métodos al rango de corte K. Con respecto a WIG, es bastante robusto a K en las Pistas de Terabyte (2004-2006) mientras prefiere un valor pequeño de K como 5 en la Pista Robusta de 2004. En otras palabras, un valor pequeño de K es una elección casi óptima para ambos tipos de pistas. Teniendo en cuenta que todos los demás parámetros involucrados en WIG están fijos y, consecuentemente, son los mismos para los dos casos, esto significa que WIG puede lograr una precisión de predicción casi óptima en dos situaciones considerablemente diferentes con exactamente la misma configuración de parámetros. En cuanto a QF, prefiere un valor más grande de K, como 100 en las Pistas de Terabyte y un valor más pequeño de K, como 25 en la Pista Robusta de 2004. 4.2.2 Consultas NP Adoptamos WIG y el primer cambio de rango (FRC) para predecir el rendimiento de las consultas NP. También intentamos una combinación lineal de los dos como en la sección anterior. El peso de la combinación se obtiene del otro conjunto de datos. Utilizamos la correlación con los rangos recíprocos medidos por la prueba de correlación de Pearson para evaluar la calidad de la predicción. Los resultados se presentan en la Tabla 6. Nuevamente, nuestros puntos de referencia son la puntuación de claridad y la puntuación de robustez. Para hacer una comparación justa, ajustamos la puntuación de claridad de diferentes maneras. Descubrimos que utilizar el documento clasificado en primer lugar para construir el modelo de consulta produce la mejor precisión de predicción. También intentamos utilizar la estructura del documento mediante la combinación de modelos de lenguaje mencionados en la sección 3.1. Se obtuvo una pequeña mejora. Los coeficientes de correlación para la puntuación de claridad reportados en la Tabla 6 son los mejores que hemos encontrado. Como podemos ver, nuestros métodos superan considerablemente la técnica de puntuación de claridad en ambas ejecuciones. Esto confirma nuestra intuición de que el uso de una medida basada en la coherencia como el puntaje de claridad es inapropiado para las consultas de NP. Claridad de métodos robusta. Tabla 6: Coeficientes de correlación de Pearson para la correlación con rangos recíprocos en las pistas de Terabyte (NP) para la puntuación de claridad, puntuación de robustez, WIG, el primer cambio de rango (FRC) y una combinación lineal de WIG y FRC. Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. En cuanto a la puntuación de robustez, también ajustamos los parámetros y reportamos el mejor que hemos encontrado. Observamos una correlación negativa interesante y sorprendente con los rangos recíprocos. Explicamos este hallazgo brevemente. Un puntaje de robustez alto significa que varios documentos de alta clasificación en la lista clasificada original siguen estando muy bien clasificados después de perturbar los documentos. La existencia de tales documentos es una buena señal de alto rendimiento para consultas basadas en contenido, ya que estas consultas suelen contener una serie de documentos relevantes [1]. Sin embargo, en lo que respecta a las consultas de NP, una diferencia fundamental es que solo hay un documento relevante para cada consulta. La existencia de tales documentos puede confundir la función de clasificación y llevar a un bajo rendimiento de recuperación. Aunque existe una correlación negativa con el rendimiento de recuperación, la fuerza de la correlación es más débil y menos consistente en comparación con nuestros métodos, como se muestra en la Tabla 6. Basándonos en el análisis anterior, podemos ver que las técnicas de predicción actuales como la puntuación de claridad y la puntuación de robustez, que están principalmente diseñadas para consultas basadas en contenido, enfrentan desafíos significativos y son insuficientes para manejar consultas de NP. Nuestras dos técnicas propuestas para consultas de NP demuestran consistentemente una buena precisión de predicción, mostrando un éxito inicial en la resolución del problema de predecir el rendimiento para consultas de NP. Otro punto que queremos destacar es que el método WIG funciona bien para ambos tipos de consultas, una propiedad deseable que la mayoría de las técnicas de predicción carecen. 4.3 Tipos de Consultas Desconocidos En esta sección, realizamos dos tipos de experimentos sin acceso a etiquetas de tipo de consulta. Primero, asumimos que solo existe un tipo de consulta pero el tipo es desconocido. Segundo, experimentamos con una mezcla de consultas basadas en contenido y NP. Las dos subsecciones siguientes informarán los resultados para las dos condiciones respectivamente. 4.3.1 Solo existe un tipo. Suponemos que todas las consultas son del mismo tipo, es decir, son consultas NP o consultas basadas en contenido. Elegimos WIG para tratar este caso porque muestra una buena precisión de predicción para ambos tipos de consultas en la sección anterior. Consideramos dos casos: (1) CB: todas las 150 consultas de títulos de la tarea ad-hoc de las pistas Terabyte 2004-2006 (2) NP: todas las 433 consultas de NP de la tarea de búsqueda de páginas nombradas de las pistas Terabyte 2005 y 2006. Tomamos una estrategia simple etiquetando todas las consultas en cada caso como el mismo tipo (ya sea NP o CB) independientemente de su tipo real. El cálculo de WIG se basará en el tipo de consulta etiquetado en lugar del tipo real. Hay cuatro posibilidades con respecto a la relación entre el tipo real y el tipo etiquetado. La correlación con el rendimiento de recuperación bajo las cuatro posibilidades se presenta en la Tabla 7. Por ejemplo, el valor 0.445 en la intersección entre la segunda fila y la tercera columna muestra el coeficiente de correlación de Pearson para la correlación con la precisión promedio cuando las consultas basadas en contenido se etiquetan incorrectamente como del tipo NP. Basándonos en estos resultados, recomendamos tratar todas las consultas como del tipo NP cuando solo existe un tipo de consulta y la clasificación precisa de la consulta no es factible, considerando el riesgo de que se produzca una gran pérdida de precisión si las consultas NP se etiquetan incorrectamente como consultas basadas en contenido. Estos resultados también demuestran la fuerte adaptabilidad de WIG a diferentes tipos de consultas. Tabla 7: Comparación de los coeficientes de correlación de Pearson para la correlación con el rendimiento de recuperación en cuatro posibilidades en las pistas de Terabyte (NP). Los casos en negrita significan que los resultados son estadísticamente significativos al nivel de 0.01. 4.3.2 Una mezcla de consultas basadas en contenido y NP Una mezcla de los dos tipos de consultas es una situación más realista que un motor de búsqueda web encontrará. Evaluamos la precisión de la predicción por la exactitud con la que se pueden identificar las consultas de bajo rendimiento mediante el método de predicción, asumiendo que los tipos de consulta reales son desconocidos (pero podemos predecir los tipos de consulta). Esta es una tarea desafiante porque tanto el rendimiento predicho como el real para un tipo de consulta pueden ser incomparables con el de otro tipo. A continuación discutimos cómo implementar nuestra evaluación. Creamos un conjunto de consultas que consiste en todas las 150 consultas de título ad-hoc de Terabyte Track 2004-2006 y todas las 433 consultas NP de Terabyte Track 2005&2006. Dividimos las consultas en el conjunto en clases: buenas (mejores que el 50% de las consultas del mismo tipo en términos de rendimiento de recuperación) y malas (de lo contrario). Según estos estándares, una consulta de NP con un rango recíproco superior a 0.2 o una consulta basada en contenido con una precisión promedio superior a 0.315 se considerarán como buenas. Entonces, cada vez que seleccionamos aleatoriamente una consulta Q del grupo con una probabilidad p de que Q esté basada en el contenido. Las consultas restantes se utilizan como datos de entrenamiento. Primero decidimos el tipo de consulta Q de acuerdo con un clasificador de consultas. Es decir, el clasificador de consultas nos dice si la consulta Q es de tipo NP o basada en contenido. Basándose en el tipo de consulta predicho y en la puntuación calculada para la consulta Q por una técnica de predicción, se toma una decisión binaria sobre si la consulta Q es buena o mala al compararla con el umbral de puntuación del tipo de consulta predicho obtenido de los datos de entrenamiento. La precisión de la predicción se mide por la exactitud de la decisión binaria. En nuestra implementación, tomamos repetidamente una consulta de prueba del conjunto de consultas y la precisión de la predicción se calcula como el porcentaje de decisiones correctas, es decir, una consulta buena (mala) se predice como buena (mala). Es obvio que adivinar al azar llevará a una precisión del 50%. Tomemos el método WIG como ejemplo para ilustrar el proceso. Dos umbrales de WIG (uno para consultas de NP y otro para consultas basadas en contenido) se entrenan maximizando la precisión de predicción en los datos de entrenamiento. Cuando una consulta de prueba es etiquetada como del tipo NP (CB) por el clasificador de tipo de consulta, se predecirá que es buena solo si la puntuación WIG para esta consulta está por encima del umbral de NP (CB). Se seguirán procedimientos similares para otras técnicas de predicción. Ahora presentamos brevemente el clasificador de tipo de consulta automático utilizado en este artículo. Encontramos que la puntuación de robustez, aunque originalmente propuesta para la predicción de rendimiento, es un buen indicador de tipos de consultas. Observamos que, en promedio, las consultas basadas en contenido tienen una puntuación de robustez mucho más alta que las consultas de NP. Por ejemplo, la Figura 2 muestra las distribuciones de puntajes de robustez para consultas basadas en NP y en contenido. Según este hallazgo, el clasificador de puntuación de robustez adjuntará una etiqueta NP (CB) a la consulta si la puntuación de robustez para la consulta está por debajo (por encima) de un umbral entrenado a partir de los datos de entrenamiento. 0 0.5 1 1.5 2 2.5 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 NP Basado en contenido Figura 2: Distribución de puntuaciones de robustez para consultas NP y CB. Las consultas NP son los 252 temas NP del Terabyte Track de 2005. Las consultas basadas en contenido son los 150 títulos ad-hoc de las pistas Terabyte 2004-2006. Las distribuciones de probabilidad son estimadas por el método de estimación de densidad de Kernel. Estrategias Robustas WIG-1 WIG-2 WIG-3 Óptima p=0.6 0.565 0.624 0.665 0.684 0.701 P=0.4 0.567 0.633 0.654 0.673 0.696 Tabla 8: Comparación de la precisión de predicción para cinco estrategias en la situación de consulta mixta. Dos formas de muestrear una consulta de la piscina: (1) la consulta muestreada es basada en contenido con una probabilidad p=0.6 (es decir, la consulta es NP con una probabilidad de 0.4) (2) establecer la probabilidad p=0.4. Consideramos cinco estrategias en nuestros experimentos. En la primera estrategia (denominada robusta), utilizamos la puntuación de robustez para la predicción del rendimiento de la consulta con la ayuda de un clasificador de consultas perfecto que siempre mapea correctamente una consulta en una de las dos categorías (es decir, NP o CB). Esta estrategia representa el nivel de precisión de predicción que las técnicas actuales de predicción pueden lograr en una condición ideal en la que se conocen los tipos de consulta. En las tres estrategias siguientes, se adopta el método WIG para predecir el rendimiento. La diferencia entre los tres es que se utilizan tres clasificadores de consulta diferentes para cada estrategia: (1) el clasificador siempre clasifica una consulta en el tipo NP. (2) el clasificador de Probabilidad de Densidad de Puntuación Robusta es el clasificador de puntuación robusta mencionado anteriormente. (3) el clasificador es perfecto. Estas tres estrategias están designadas como WIG-1, WIG-2 y WIG-3 respectivamente. La razón por la que estamos interesados en WIG-1 se basa en los resultados de la sección 4.3.1. En la última estrategia (denominada Óptima) que sirve como un límite superior de lo bien que podemos hacer hasta ahora, hacemos pleno uso de nuestras técnicas de predicción para cada tipo de consulta asumiendo que un clasificador de consultas perfecto está disponible. Específicamente, combinamos linealmente WIG y QF para consultas basadas en contenido y WIG y FRC para consultas de NP. Los resultados de las cinco estrategias se muestran en la Tabla 8. Para cada estrategia, intentamos dos formas de muestrear una consulta del conjunto: (1) la consulta muestreada es CB con probabilidad p=0.6 (la consulta es NP con probabilidad 0.4) (2) establecer la probabilidad p=0.4. De la Tabla 8 podemos ver que en términos de precisión de predicción, WIG-2 (el método WIG con el clasificador de consultas automático) no solo es mejor que los dos primeros casos, sino que también está cerca de WIG-3 donde se asume un clasificador perfecto. Se observan algunas mejoras adicionales sobre WIG-3 cuando se combinan con otras técnicas de predicción. El mérito de WIG-2 es que proporciona una solución práctica para identificar automáticamente consultas con bajo rendimiento en un entorno de búsqueda web con tipos de consultas mixtos, lo cual representa considerables obstáculos para las técnicas de predicción tradicionales. CONCLUSIONES Y TRABAJOS FUTUROS Hasta donde sabemos, nuestro artículo es el primero en explorar a fondo la predicción del rendimiento de consultas en entornos de búsqueda web. Demostramos que nuestros modelos resultaron en una mayor precisión de predicción que las técnicas previamente publicadas no especialmente diseñadas para escenarios de búsqueda en la web. En este artículo, nos enfocamos en dos tipos de consultas en la búsqueda web: consultas basadas en contenido y consultas de búsqueda de Páginas-Nombradas (NP), correspondientes a la tarea de recuperación ad-hoc y la tarea de búsqueda de Páginas-Nombradas respectivamente. Para ambos tipos de consultas web, se demostró que nuestros modelos de predicción son sustancialmente más precisos que las técnicas actuales más avanzadas. Además, consideramos un caso más realista en el que no se dispone de información previa sobre los tipos de consultas. Demostramos que el método WIG es particularmente adecuado para esta situación. Considerando la adaptabilidad de WIG a una variedad de colecciones y tipos de consultas, uno de nuestros planes futuros es aplicar este método para predecir la preferencia del usuario por los resultados de búsqueda en datos realistas recopilados de un motor de búsqueda comercial. Además de la precisión, otro problema importante con el que las técnicas de predicción tienen que lidiar en un entorno web es la eficiencia. Afortunadamente, dado que el puntaje WIG se calcula solo sobre los términos y frases que aparecen en la consulta, este cálculo puede realizarse de manera muy eficiente con el soporte de un índice. Por otro lado, el cálculo de QF y FRC es relativamente menos eficiente ya que QF necesita recuperar toda la colección dos veces y FRC necesita clasificar repetidamente los documentos perturbados. Cómo mejorar la eficiencia de QF y FRC es nuestro trabajo futuro. Además, las técnicas de predicción propuestas en este documento tienen el potencial de mejorar el rendimiento de recuperación al combinarse con otras técnicas de RI. Por ejemplo, nuestras técnicas pueden ser incorporadas a técnicas populares de modificación de consultas como la expansión de consultas y la relajación de consultas. Guiados por la predicción del rendimiento, podemos tomar una mejor decisión sobre cuándo o cómo modificar las consultas para mejorar la efectividad de la recuperación. Nos gustaría llevar a cabo investigaciones en esta dirección en el futuro. AGRADECIMIENTOS Este trabajo fue apoyado en parte por el Centro de Recuperación de Información Inteligente, en parte por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el contrato número HR0011-06-C0023, y en parte por un premio de Google. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son del autor y no necesariamente reflejan las del patrocinador. Además, agradecemos a Donald Metzler por sus valiosos comentarios sobre este trabajo. 7. REFERENCIAS [1] Y. Zhou, W. B. Croft, Ranking Robustness: Un nuevo marco para predecir el rendimiento de consultas, en Actas de CIKM 2006. [2] D. Carmel, E. Yom-Tov, A. Darlow, D. Pelleg, ¿Qué hace que una consulta sea difícil?, en Actas de SIGIR 2006. [3] C. L. A. Clarke, F. Scholer, I. Soboroff, La pista Terabyte TREC 2005, En las Actas en Línea de TREC 2005. [4] B. Él y yo. Inferir el rendimiento de la consulta utilizando predictores de preretrieval. En actas de SPIRE 2004. [5] S. Tomlinson. Recuperación robusta de la web y terabytes con Hummingbird SearchServer en TREC 2004. En las Actas en Línea de TREC 2004. [6] S. Cronen-Townsend, Y. Zhou, W. B. Croft, Prediciendo el Rendimiento de Consultas, en las Actas de SIGIR 2002. [7] V. Vinay, I. J. Cox, N. Mill-Frayling, K. Wood, Sobre la Clasificación de la Efectividad del Buscador, en las Actas de SIGIR 2006. [8] D. Metzler, W. B. Croft, Un Modelo de Campo Aleatorio de Markov para Dependencias de Términos, en las Actas de SIGIR 2005. [9] D. Metzler, T. Strohman, Y. Zhou, W. B. Croft, Indri en TREC 2005: Terabyte Track, en las Actas en Línea de TREC 2004. [10] P. Ogilvie y J. Callan, Combinando representaciones de documentos para búsqueda de elementos conocidos, en las Actas de SIGIR 2003. [11] A. Berger, J. Lafferty, Recuperación de información como traducción estadística, en las Actas de SIGIR 1999. [12] Motor de búsqueda Indri: http://www.lemurproject.org/indri/ [13] I. J. Taneja: Sobre medidas de información generalizadas y sus aplicaciones, Avances en Electrónica y Física de Electrones, Academic Press (EE. UU.), 76, 1989, 327-413. [14] S. Cronen-Townsend, Y. Zhou y Croft, W. B., Un marco para la expansión selectiva de consultas, en Actas de CIKM 2004. [15] F. Song, W. B. Croft, Un modelo de lenguaje general para la recuperación de información, en Actas de SIGIR 1999. [16] Contacto por correo electrónico personal con Vishwa Vinay y nuestros propios experimentos [17] E. Yom-Tov, S. Fine, D. Carmel, A. Darlow, Aprendiendo a estimar la dificultad de la consulta incluyendo aplicaciones para la detección de contenido faltante y la recuperación de información distribuida, en Actas de SIGIR 2005. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "web search": {
            "translated_key": "búsqueda web",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Query Performance Prediction in <br>web search</br> Environments Yun Zhou and W. Bruce Croft Department of Computer Science University of Massachusetts, Amherst {yzhou, croft}@cs.umass.edu ABSTRACT Current prediction techniques, which are generally designed for content-based queries and are typically evaluated on relatively homogenous test collections of small sizes, face serious challenges in <br>web search</br> environments where collections are significantly more heterogeneous and different types of retrieval tasks exist.",
                "In this paper, we present three techniques to address these challenges.",
                "We focus on performance prediction for two types of queries in <br>web search</br> environments: content-based and Named-Page finding.",
                "Our evaluation is mainly performed on the GOV2 collection.",
                "In addition to evaluating our models for the two types of queries separately, we consider a more challenging and realistic situation that the two types of queries are mixed together without prior information on query types.",
                "To assist prediction under the mixed-query situation, a novel query classifier is adopted.",
                "Results show that our prediction of web query performance is substantially more accurate than the current stateof-the-art prediction techniques.",
                "Consequently, our paper provides a practical approach to performance prediction in realworld web settings.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval -Query formulation General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Query performance prediction has many applications in a variety of information retrieval (IR) areas such as improving retrieval consistency, query refinement, and distributed IR.",
                "The importance of this problem has been recognized by IR researchers and a number of new methods have been proposed for prediction recently [1, 2, 17].",
                "Most work on prediction has focused on the traditional ad-hoc retrieval task where query performance is measured according to topical relevance.",
                "These prediction models are evaluated on TREC document collections which typically consist of no more than one million relatively homogenous newswire articles.",
                "With the popularity and influence of the Web, prediction techniques that will work well for web-style queries are highly preferable.",
                "However, <br>web search</br> environments pose significant challenges to current prediction models that are mainly designed for traditional TREC settings.",
                "Here we outline some of these challenges.",
                "First, web collections, which are much larger than conventional TREC collections, include a variety of documents that are different in many aspects such as quality and style.",
                "Current prediction techniques can be vulnerable to these characteristics of web collections.",
                "For example, the reported prediction accuracy of the ranking robustness technique and the clarity technique on the GOV2 collection (a large web collection) is significantly worse compared to the other TREC collections [1].",
                "Similar prediction accuracy on the GOV2 collection using another technique is reported in [2], confirming the difficult of predicting query performance on a large web collection.",
                "Furthermore, <br>web search</br> goes beyond the scope of the ad-hoc retrieval task based on topical relevance.",
                "For example, the Named-Page (NP) finding task, which is a navigational task, is also popular in web retrieval.",
                "Query performance prediction for the NP task is still necessary since NP retrieval performance is far from perfect.",
                "In fact, according to the report on the NP task of the 2005 Terabyte Track [3], about 40% of the test queries perform poorly (no correct answer in the first 10 search results) even in the best run from the top group.",
                "To our knowledge, little research has explicitly addressed the problem of NP-query performance prediction.",
                "Current prediction models devised for content-based queries will be less effective for NP queries considering the fundamental differences between the two.",
                "Third, in real-world <br>web search</br> environments, user queries are usually a mixture of different types and prior knowledge about the type of each query is generally unavailable.",
                "The mixed-query situation raises new problems for query performance prediction.",
                "For instance, we may need to incorporate a query classifier into prediction models.",
                "Despite these problems, the ability to handle this situation is a crucial step towards turning query performance prediction from an interesting research topic into a practical tool for web retrieval.",
                "In this paper, we present three techniques to address the above challenges that current prediction models face in <br>web search</br> environments.",
                "Our work focuses on query performance prediction for the content-based (ad-hoc) retrieval task and the name-page finding task in the context of web retrieval.",
                "Our first technique, called weighted information gain (WIG), makes use of both single term and term proximity features to estimate the quality of top retrieved documents for prediction.",
                "We find that WIG offers consistent prediction accuracy across various test collections and query types.",
                "Moreover, we demonstrate that good prediction accuracy can be achieved for the mixed-query situation by using WIG with the help of a query type classifier.",
                "Query feedback and first rank change, which are our second and third prediction techniques, perform well for content-based queries and NP queries respectively.",
                "Our main contributions include: (1) considerably improved prediction accuracy for web content-based queries over several state-of-the-art techniques. (2) new techniques for successfully predicting NP-query performance. (3) a practical and fully automatic solution to predicting mixed-query performance.",
                "In addition, one minor contribution is that we find that the robustness score [1], which was originally proposed for performance prediction, is helpful for query classification. 2.",
                "RELATED WORK As we mentioned in the introduction, a number of prediction techniques have been proposed recently that focus on contentbased queries in the topical relevance (ad-hoc) task.",
                "We know of no published work that addresses other types of queries such as NP queries, let alone a mixture of query types.",
                "Next we review some representative models.",
                "The major difficulty of performance prediction comes from the fact that many factors have an impact on retrieval performance.",
                "Each factor affects performance to a different degree and the overall effect is hard to predict accurately.",
                "Therefore, it is not surprising to notice that simple features, such as the frequency of query terms in the collection [4] and the average IDF of query terms [5], do not predict well.",
                "In fact, most of the successful techniques are based on measuring some characteristics of the retrieved document set to estimate topic difficulty.",
                "For example, the clarity score [6] measures the coherence of a list of documents by the KL-divergence between the query model and the collection model.",
                "The robustness score [1] quantifies another property of a ranked list: the robustness of the ranking in the presence of uncertainty.",
                "Carmel et al. [2] found that the distance measured by the Jensen-Shannon divergence between the retrieved document set and the collection is significantly correlated to average precision.",
                "Vinay et al. [7] proposed four measures to capture the geometry of the top retrieved documents for prediction.",
                "The most effective measure is the sensitivity to document perturbation, an idea somewhat similar to the robustness score.",
                "Unfortunately, their way of measuring the sensitivity does not perform equally well for short queries and prediction accuracy drops considerably when a state-of-the-art retrieval technique (like Okapi or a language modeling approach) is adopted for retrieval instead of the tf-idf weighting used in their paper [16].",
                "The difficulties of applying these models in <br>web search</br> environments have already been mentioned.",
                "In this paper, we mainly adopt the clarity score and the robustness score as our baselines.",
                "We experimentally show that the baselines, even after being carefully tuned, are inadequate for the web environment.",
                "One of our prediction models, WIG, is related to the Markov random field (MRF) model for information retrieval [8].",
                "The MRF model directly models term dependence and is found be to highly effective across a variety of test collections (particularly web collections) and retrieval tasks.",
                "This model is used to estimate the joint probability distribution over documents and queries, an important part of WIG.",
                "The superiority of WIG over other prediction techniques based on unigram features, which will be demonstrated later in our paper, coincides with that of MRF for retrieval.",
                "In other word, it is interesting to note that term dependence, when being modeled appropriately, can be helpful for both improving and predicting retrieval performance. 3.",
                "PREDICTION MODELS 3.1 Weighted Information Gain (WIG) This section introduces a weighted information gain approach that incorporates both single term and proximity features for predicting performance for both content-based and Named-Page (NP) finding queries.",
                "Given a set of queries Q={Qs} (s=1,2,..N) which includes all possible user queries and a set of documents D={Dt} (t=1,2…M), we assume that each query-document pair (Qs,Dt) is manually judged and will be put in a relevance list if Qs is found to be relevant to Dt.",
                "The joint probability P(Qs,Dt) over queries Q and documents D denotes the probability that pair (Qs,Dt) will be in the relevance list.",
                "Such assumptions are similar to those used in [8].",
                "Assuming that the user issues query Qi ∈Q and the retrieval results in response to Qi is a ranked list L of documents, we calculate the amount of information contained in P(Qs,Dt) with respect to Qi and L by Eq.1 which is a variant of entropy called the weighted entropy[13].",
                "The weights in Eq.1 are solely determined by Qi and L. )1(),(log),(),( , , ∑−= ts tststsLQ DQPDQweightDQH i In this paper, we choose the weights as follows: LindocumentsKtopthecontainsLTwhere otherwise LTDandisifK DQweight K Kt ts )( )2( ,0 )(,/1 ),( ⎩ ⎨ ⎧ ∈= = The cutoff rank K is a parameter in our model that will be discussed later.",
                "Accordingly, Eq.1 can be simplified as follows: )3(),(log 1 ),( )( , ∑∈ −= LTD titsLQ Kt i DQP K DQH Unfortunately, weighted entropy ),(, tsLQ DQH i computed by Eq.3, which represents the amount of information about how likely the top ranked documents in L would be relevant to query Qi on average, cannot be compared across different queries, making it inappropriate for directly predicting query performance.",
                "To mitigate this problem, we come up with a background distribution P(Qs,C) over Q and D by imagining that every document in D is replaced by the same special document C which represents average language usage.",
                "In this paper, C is created by concatenating every document in D. Roughly speaking, C is the collection (the document set) {Dt} without document boundaries.",
                "Similarly, weighted entropy ),(, CQH sLQi calculated by Eq.3 represents the amount of information about how likely an average document (represented by the whole collection) would be relevant to query Qi.",
                "Now we introduce our performance predictor WIG which is the weighted information gain [13] computed as the difference between ),(, tsLQ DQH i and ),(, CQH sLQi .Specifically, given query Qi, collection C and ranked list L of documents, WIG is calculated as follows: )4( ),( ),( log 1 ),( ),( log),( ),(),(),,( )(, ,, ∑∑ ∈ == −= LTD i ti ts s ts ts tsLQsLQi Kt ii CQP DQP KCQP DQP DQweight DQHCQHLCQWIG WIG computed by Eq.4 measures the change in information about the quality of retrieval (in response to query Qi) from an imaginary state that only an average document is retrieved to a posterior state that the actual search results are observed.",
                "We hypothesize that WIG is positively correlated with retrieval effectiveness because high quality retrieval should be much more effective than just returning the average document.",
                "The heart of this technique is how to estimate the joint distribution P(Qs,Dt).",
                "In the language modeling approach to IR, a variety of models can be applied readily to estimate this distribution.",
                "Although most of these models are based on the bagof-words assumption, recent work on modeling term dependence under the language modeling framework have shown consistent and significant improvements in retrieval effectiveness over bagof-words models.",
                "Inspired by the success of incorporating term proximity features into language models, we decide to adopt a good dependence model to estimate the probability P(Qs,Dt).",
                "The model we chose for this paper is Metzler and Crofts Markov Random Field (MRF) model, which has already demonstrated superiority over a number of collections and different retrieval tasks [8,9].",
                "According to the MRF model, log P(Qi, Dt) can be written as )5()|(loglog),(log )( 1 ∑∈ +−= iQF tti DPZDQP ξ ξ ξλ where Z1 is a constant that ensures that P(Qi, Dt) sums up to 1.",
                "F(Qi) consists of a set of features expanded from the original query Qi .",
                "For example, assuming that query Qi is talented student program, F(Qi) includes features like program and talented student.",
                "We consider two kinds of features: single term features T and proximity features P. Proximity features include exact phrase (#1) and unordered window (#uwN) features as described in [8].",
                "Note that F(Qi) is the union of T(Qi) and P(Qi).",
                "For more details on F(Qi) such as how to expand the original query Qi to F(Qi), we refer the reader to [8] and [9].",
                "P(ξ|Dt) denotes the probability that feature ξ will occur in Dt.",
                "More details on P(ξ|Dt) will be provided later in this section.",
                "The choice of λξ is somewhat different from that used in [8] since λξ plays a dual role in our model.",
                "The first role, which is the same as in [8], is to weight between single term and proximity features.",
                "The other role, which is specific to our prediction task, is to normalize the size of F(Qi).We found that the following weight strategy for λξ satisfies the above two roles and generalizes well on a variety of collections and query types. )6( )(, |)(| 1 )(, |)(| ⎪ ⎪ ⎩ ⎪ ⎪ ⎨ ⎧ ∈ − ∈ = i i T i i T QP QP QT QT ξ λ ξ λ λξ where |T(Qi)| and |P(Qi)| denote the number of single term and proximity features in F(Qi) respectively.",
                "The reason for choosing the square root function in the denominator of λξ is to penalize a feature set of large size appropriately, making WIG more comparable across queries of various lengths. λT is a fixed parameter and set to 0.8 according to [8] throughout this paper.",
                "Similarly, log P(Qi,C) can be written as: )7()|(loglog),(log )( 2 ∑∈ +−= iQF i CPZCQP ξ ξ ξλ When constant Z1 and Z2 are dropped, WIG computed in Eq.4 can be rewritten as follows by plugging in Eq.5 and Eq.7 : )8( )|( )|( log 1 ),,( )( )( ∑ ∑∈ ∈ = LTD QF t i Kt i CP DP K LCQWIG ξ ξ ξ ξ λ One of the advantages of WIG over other techniques is that it can handle well both content-based and NP queries.",
                "Based on the type (or the predicted type) of Qi, the calculation of WIG in Eq. 8 differs in two aspects: (1) how to estimate P(ξ|Dt) and P(ξ|C), and (2) how to choose K. For content-based queries, P(ξ|C) is estimated by the relative frequency of feature ξ in collection C as a whole.",
                "The estimation of P(ξ|Dt) is the same as in [8].",
                "Namely, we estimate P(ξ|Dt) by the relative frequency of feature ξ in Dt linearly smoothed with collection frequency P(ξ|C).",
                "K in Eq.8 is treated as a free parameter.",
                "Note that K is the only free parameter in the computation of WIG for content-based queries because all parameters involved in P(ξ|Dt) are assumed to be fixed by taking the suggested values in [8].",
                "Regarding NP queries, we make use of document structure to estimate P(ξ|Dt) and P(ξ|C) by the so-called mixture of language models proposed in [10] and incorporated into the MRF model for Named-Page finding retrieval in [9].",
                "The basic idea is that a document (collection) is divided into several fields such as the title field, the main-body field and the heading field.",
                "P(ξ|Dt) and P(ξ|C) are estimated by a linear combination of the language models from each field.",
                "Due to space constraints, we refer the reader to [9] for details.",
                "We adopt the exact same set of parameters as used in [9] for estimation.",
                "With regard to K in Eq.8, we set K to 1 because the Named-Page finding task heavily focuses on the first ranked document.",
                "Consequently, there are no free parameters in the computation of WIG for NP queries. 3.2 Query Feedback In this section, we introduce another technique called query feedback (QF) for prediction.",
                "Suppose that a user issues query Q to a retrieval system and a ranked list L of documents is returned.",
                "We view the retrieval system as a noisy channel.",
                "Specifically, we assume that the output of the channel is L and the input is Q.",
                "After going through the channel, Q becomes corrupted and is transformed to ranked list L. By thinking about the retrieval process this way, the problem of predicting retrieval effectiveness turns to the task of evaluating the quality of the channel.",
                "In other words, prediction becomes finding a way to measure the degree of corruption that arises when Q is transformed to L. As directly computing the degree of the corruption is difficult, we tackle this problem by approximation.",
                "Our main idea is that we measure to what extent information on Q can be recovered from L on the assumption that only L is observed.",
                "Specifically, we design a decoder that can accurately translate L back into new query Q and the similarity S between the original query Q and the new query Q is adopted as a performance predictor.",
                "This is a sketch of how the QF technique predicts query performance.",
                "Before filling in more details, we briefly discuss why this method would work.",
                "There is a relation between the similarity S defined above and retrieval performance.",
                "On the one hand, if the retrieval has strayed from the original sense of the query Q, the new query Q extracted from ranked list L in response to Q would be very different from the original query Q.",
                "On the other hand, a query distilled from a ranked list containing many relevant documents is likely to be similar to the original query.",
                "Further examples in support of the relation will be provided later.",
                "Next we detail how to build the decoder and how to measure the similarity S. In essence, the goal of the decoder is to compress ranked list L into a few informative terms that should represent the content of the top ranked documents in L. Our approach to this goal is to represent ranked list L by a language model (distribution over terms).",
                "Then terms are ranked by their contribution to the language models KL (Kullback-Leibler) divergence from the background collection model.",
                "Top ranked terms will be chosen to form the new query Q.",
                "This approach is similar to that used in Section 4.1 of [11].",
                "Specifically, we take three steps to compress ranked list L into query Q without referring to the original query. 1.",
                "We adopt the ranked list language model [14], to estimate a language model based on ranked list L. The model can be written as: )9()|()|()|( ∑∈ = LD LDPDwPLwP where w is any term, D is a document.",
                "P(D|L) is estimated by a linearly decreasing function of the rank of document D. 2.",
                "Each term in P(w|L) is ranked by the following KL-divergence contribution: )10( )|( )|( log)|( CwP LwP LwP where P(w|C) is the collection model estimated by the relative frequency of term w in collection C as a whole. 3.",
                "The top N ranked terms by Eq.10 form a weighted query Q={(wi,ti)} i=1,N. where wi denotes the i-th ranked term and weight ti is the KL-divergence contribution of wi in Eq. 10.",
                "Term cruise ship vessel sea passenger KL contribution 0.050 0.040 0.012 0.010 0.009 Table 1: top 5 terms compressed from the ranked list in response to query Cruise ship damage sea life Two representative examples, one for a poorly performing query Cruise ship damage sea life (TREC topic 719; average precision: 0.08) and the other for a high performing query prostate cancer treatments( TREC topic 710; average precision: 0.49), are shown in Table 1 and 2 respectively.",
                "These examples indicate how the similarity between the original and the new query correlates with retrieval performance.",
                "The parameter N in step 3 is set to 20 empirically and choosing a larger value of N is unnecessary since the weights after the top 20 are usually too small to make any difference.",
                "Term prostate cancer treatment men therapy KL contribution 0.177 0.140 0.028 0.025 0.020 Table 2: top 5 terms compressed from the ranked list in response to query prostate cancer treatments To measure the similarity between original query Q and new query Q, we first use Q to do retrieval on the same collection.",
                "A variant of the query likelihood model [15] is adopted for retrieval.",
                "Namely, documents are ranked by: )11()|()|( ),( ∑∈ = Qtw t i ii i DwPDQP where wi is a term in Q and ti is the associated weight.",
                "D is a document.",
                "Let L denote the new ranked list returned from the above retrieval.",
                "The similarity is measured by the overlap of documents in L and L. Specifically, the percentage of documents in the top K documents of L that are also present in the top K documents in L. the cutoff K is treated as a free parameter.",
                "We summarize here how the QF technique predicts performance given a query Q and the associated ranked list L. We first obtain a weighted query Q compressed from L by the above three steps.",
                "Then we use Q to perform retrieval and the new ranked list is L. The overlap of documents in L and L is used for prediction. 3.3 First Rank Change (FRC) In this section, we propose a method called the first rank change (FRC) for performance prediction for NP queries.",
                "This method is derived from the ranking robustness technique [1] that is mainly designed for content-based queries.",
                "When directly applied to NP queries, the robustness technique will be less effective because it takes the top ranked documents as a whole into account while NP queries usually have only one single relevant document.",
                "Instead, our technique focuses on the first rank document while the main idea of the robustness method remains.",
                "Specifically, the pseudocode for computing FRC is shown in figure 1.",
                "Input: (1) ranked list L={Di} where i=1,100.",
                "Di denotes the i-th ranked document. (2) query Q 1 initialize: (1) set the number of trials J=100000 (2) counter c=0; 2 for i=1 to J 3 Perturb every document in L, let the outcome be a set F={Di} where Di denotes the perturbed version of Di. 4 Do retrieval with query Q on set F 5 c=c+1 if and only if D1 is ranked first in step 4 6 end of for 7 return the ratio c/J Figure 1: pseudo-code for computing FRC FRC approximates the probability that the first ranked document in the original list L will remain ranked first even after the documents are perturbed.",
                "The higher the probability is, the more confidence we have in the first ranked document.",
                "On the other hand, in the extreme case of a random ranking, the probability would be as low as 0.5.",
                "We expect that FRC has a positive association with NP query performance.",
                "We adopt [1] to implement the document perturbation step (step 4 in Fig.1) using Poisson distributions.",
                "For more details, we refer the reader to [1]. 4.",
                "EVALUATION We now present the results of predicting query performance by our models.",
                "Three state-of-the-art techniques are adopted as our baselines.",
                "We evaluate our techniques across a variety of Web retrieval settings.",
                "As mentioned before, we consider two types of queries, that is, content-based (CB) queries and Named-Page(NP) finding queries.",
                "First, suppose that the query types are known.",
                "We investigate the correlation between the predicted retrieval performance and the actual performance for both types of queries separately.",
                "Results show that our methods yield considerable improvements over the baselines.",
                "We then consider a more challenging scenario where no prior information on query types is available.",
                "Two sub-cases are considered.",
                "In the first one, there exists only one type of query but the actual type is unknown.",
                "We assume a mixture of the two query types in the second case.",
                "We demonstrate that our models achieve good accuracy under this demanding scenario, making prediction practical in a real-world <br>web search</br> environment. 4.1 Experimental Setup Our evaluation focuses on the GOV2 collection which contains about 25 million documents crawled from web sites in the .gov domain during 2004 [3].",
                "We create two kinds of data set for CB queries and NP queries respectively.",
                "For the CB type, we use the ad-hoc topics of the Terabyte Tracks of 2004, 2005 and 2006 and name them TB04-adhoc, TB05-adhoc and TB06-adhoc respectively.",
                "In addition, we also use the ad-hoc topics of the 2004 Robust Track (RT04) to test the adaptability of our techniques to a non-Web environment.",
                "For NP queries, we use the Named-Page finding topics of the Terabyte Tracks of 2005 and 2006 and we name them TB05-NP and TB06-NP respectively.",
                "All queries used in our experiments are titles of TREC topics as we center on web retrieval.",
                "Table 3 summarizes the above data sets.",
                "Name Collection Topic Number Query Type TB04-adhoc GOV2 701-750 CB TB05-adhoc GOV2 751-800 CB TB06-adhoc GOV2 801-850 CB RT04 Disk 4+5 (minus CR) 301-450;601700 CB TB05-NP GOV2 NP601-NP872 NP TB06-NP GOV2 NP901-NP1081 NP Table 3: Summary of test collections and topics Retrieval performance of individual content-based and NP queries is measured by the average precision and reciprocal rank of the first correct answer respectively.",
                "We make use of the Markov Random field model for both ad-hoc and Named-Page finding retrieval.",
                "We adopt the same setting of retrieval parameters used in [8,9].",
                "The Indri search engine [12] is used for all of our experiments.",
                "Though not reported here, we also tried the query likelihood model for ad-hoc retrieval and found that the results change little because of the very high correlation between the query performances obtained by the two retrieval models (0.96 measured by Pearsons coefficient). 4.2 Known Query Types Suppose that query types are known.",
                "We treat each type of query separately and measure the correlation with average precision (or the reciprocal rank in the case of NP queries).",
                "We adopt the Pearsons correlation test which reflects the degree of linear relationship between the predicted and the actual retrieval performance. 4.2.1 Content-based Queries Methods Clarity Robust JSD WIG QF WIG +QF TB04+0 5 adhoc 0.333 0.317 0.362 0.574 0.480 0.637 TB06 adhoc 0.076 0.294 N/A 0.464 0.422 0.511 Table 4: Pearsons correlation coefficients for correlation with average precision on the Terabyte Tracks (ad-hoc) for clarity score, robustness score, the JSD-based method(we directly cites the score reported in [2]), WIG, query feedback(QF) and a linear combination of WIG and QF.",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Table 4 shows the correlation with average precision on two data sets: one is a combination of TB04-adhoc and TB05-adhoc(100 topics in total) and the other is TB06-adhoc (50 topics).",
                "The reason that we put TB04-adhoc and TB05-adhoc together is to make our results comparable to [2].",
                "Our baselines are the clarity score (clarity) [6],the robustness score (robust)[1] and the JSDbased method (JSD) [2].",
                "For the clarity and robustness score, we have tried different parameter settings and report the highest correlation coefficients we have found.",
                "We directly cite the result of the JSD-based method reported in [2].",
                "The table also shows the results for the Weighted Information Gain (WIG) method and the Query Feedback (QF) method for predicting content-based queries.",
                "As we described in the previous section, both WIG and QF have one free parameter to set, that is, the cutoff rank K. We train the parameter on one dataset and test on the other.",
                "When combining WIG and QF, a simple linear combination is used and the combination weight is learned from the training data set.",
                "From these results, we can see that our methods are considerably more accurate compared to the baselines.",
                "We also observe that further improvements are obtained from the combination of WIG and QF, suggesting that they measure different properties of the retrieval process that relate to performance.",
                "We discover that our methods generalize well on TB06-adhoc while the correlation for the clarity score with retrieval performance on this data set is considerably worse.",
                "Further investigation shows that the mean average precision of TB06-adhoc is 0.342 and is about 10% better than that of the first data set.",
                "While the other three methods typically consider the top 100 or less documents given a ranked list, the clarity method usually needs the top 500 or more documents to adequately measure the coherence of a ranked list.",
                "Higher mean average precision makes ranked lists retrieved by different queries more similar in terms of coherence at the level of top 500 documents.",
                "We believe that this is the main reason for the low accuracy of the clarity score on the second data set.",
                "Though this paper focuses on a <br>web search</br> environment, it is desirable that our techniques will work consistently well in other situations.",
                "To this end, we examine the effectiveness of our techniques on the Robust 2004 Track.",
                "For our methods, we evenly divide all of the test queries into five groups and perform five-fold cross validation.",
                "Each time we use one group for training and the remaining four groups for testing.",
                "We make use of all of the queries for our two baselines, that is, the clarity score and the robustness score.",
                "The parameters for our baselines are the same as those used in [1].The results shown in Table 5 demonstrate that the prediction accuracy of our methods is on a par with that of the two strong baselines.",
                "Clarity Robust WIG QF 0.464 0.539 0.468 0.464 Table 5: Comparison of Pearsons correlation coefficients on the 2004 Robust Track for clarity score, robustness score, WIG and query feedback (QF).",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Furthermore, we examine the prediction sensitivity of our methods to the cutoff rank K. With respect to WIG, it is quite robust to K on the Terabyte Tracks (2004-2006) while it prefers a small value of K like 5 on the 2004 Robust Track.",
                "In other words, a small value of K is a nearly-optimal choice for both kinds of tracks.",
                "Considering the fact that all other parameters involved in WIG are fixed and consequently the same for the two cases, this means WIG can achieve nearly-optimal prediction accuracy in two considerably different situations with exactly the same parameter setting.",
                "Regarding QF, it prefers a larger value of K such as 100 on the Terabyte Tracks and a smaller value of K such as 25 on the 2004 Robust Track. 4.2.2 NP Queries We adopt WIG and first rank change (FRC) for predicting NPquery performance.",
                "We also try a linear combination of the two as in the previous section.",
                "The combination weight is obtained from the other data set.",
                "We use the correlation with the reciprocal ranks measured by the Pearsons correlation test to evaluate prediction quality.",
                "The results are presented in Table 6.",
                "Again, our baselines are the clarity score and the robustness score.",
                "To make a fair comparison, we tune the clarity score in different ways.",
                "We found that using the first ranked document to build the query model yields the best prediction accuracy.",
                "We also attempted to utilize document structure by using the mixture of language models mentioned in section 3.1.",
                "Little improvement was obtained.",
                "The correlation coefficients for the clarity score reported in Table 6 are the best we have found.",
                "As we can see, our methods considerably outperform the clarity score technique on both of the runs.",
                "This confirms our intuition that the use of a coherence-based measure like the clarity score is inappropriate for NP queries.",
                "Methods Clarity Robust.",
                "WIG FRC WIG+FRC TB05-NP 0.150 -0.370 0.458 0.440 0.525 TB06-NP 0.112 -0.160 0.478 0.386 0.515 Table 6: Pearsons correlation coefficients for correlation with reciprocal ranks on the Terabyte Tracks (NP) for clarity score, robustness score, WIG, the first rank change (FRC) and a linear combination of WIG and FRC.",
                "Bold cases mean the results are statistically significant at the 0.01 level.",
                "Regarding the robustness score, we also tune the parameters and report the best we have found.",
                "We observe an interesting and surprising negative correlation with reciprocal ranks.",
                "We explain this finding briefly.",
                "A high robustness score means that a number of top ranked documents in the original ranked list are still highly ranked after perturbing the documents.",
                "The existence of such documents is a good sign of high performance for content-based queries as these queries usually contain a number of relevant documents [1].",
                "However, with regard to NP queries, one fundamental difference is that there is only one relevant document for each query.",
                "The existence of such documents can confuse the ranking function and lead to low retrieval performance.",
                "Although the negative correlation with retrieval performance exists, the strength of the correlation is weaker and less consistent compared to our methods as shown in Table 6.",
                "Based on the above analysis, we can see that current prediction techniques like clarity score and robustness score that are mainly designed for content-based queries face significant challenges and are inadequate to deal with NP queries.",
                "Our two techniques proposed for NP queries consistently demonstrate good prediction accuracy, displaying initial success in solving the problem of predicting performance for NP queries.",
                "Another point we want to stress is that the WIG method works well for both types of queries, a desirable property that most prediction techniques lack. 4.3 Unknown Query Types In this section, we run two kinds of experiments without access to query type labels.",
                "First, we assume that only one type of query exists but the type is unknown.",
                "Second, we experiment on a mixture of content-based and NP queries.",
                "The following two subsections will report results for the two conditions respectively. 4.3.1 Only One Type exists We assume that all queries are of the same type, that is, they are either NP queries or content-based queries.",
                "We choose WIG to deal with this case because it shows good prediction accuracy for both types of queries in the previous section.",
                "We consider two cases: (1) CB: all 150 title queries from the ad-hoc task of the Terabyte Tracks 2004-2006 (2)NP: all 433 NP queries from the named page finding task of the Terabyte Tracks 2005 and 2006.",
                "We take a simple strategy by labeling all of the queries in each case as the same type (either NP or CB) regardless of their actual type.",
                "The computation of WIG will be based on the labeled query type instead of the actual type.",
                "There are four possibilities with respect to the relation between the actual type and the labeled type.",
                "The correlation with retrieval performance under the four possibilities is presented in Table 7.",
                "For example, the value 0.445 at the intersection between the second row and the third column shows the Pearsons correlation coefficient for correlation with average precision when the content-based queries are incorrectly labeled as the NP type.",
                "Based on these results, we recommend treating all queries as the NP type when only one query type exists and accurate query classification is not feasible, considering the risk that a large loss of accuracy will occur if NP queries are incorrectly labeled as content-based queries.",
                "These results also demonstrate the strong adaptability of WIG to different query types.",
                "CB (labeled) NP (labeled) CB (actual) 0.536 0.445 NP (actual) 0.174 0.467 Table 7: Comparison of Pearsons correlation coefficients for correlation with retrieval performance under four possibilities on the Terabyte Tracks (NP).",
                "Bold cases mean the results are statistically significant at the 0.01 level. 4.3.2 A mixture of contented-based and NP queries A mixture of the two types of queries is a more realistic situation that a <br>web search</br> engine will meet.",
                "We evaluate prediction accuracy by how accurately poorly-performing queries can be identified by the prediction method assuming that actual query types are unknown (but we can predict query types).",
                "This is a challenging task because both the predicted and actual performance for one type of query can be incomparable to that for the other type.",
                "Next we discuss how to implement our evaluation.",
                "We create a query pool which consists of all of the 150 ad-hoc title queries from Terabyte Track 2004-2006 and all of the 433 NP queries from Terabyte Track 2005&2006.",
                "We divide the queries in the pool into classes: good (better than 50% of the queries of the same type in terms of retrieval performance) and bad (otherwise).",
                "According to these standards, a NP query with the reciprocal rank above 0.2 or a content-based query with the average precision above 0.315 will be considered as good.",
                "Then, each time we randomly select one query Q from the pool with probability p that Q is contented-based.",
                "The remaining queries are used as training data.",
                "We first decide the type of query Q according to a query classifier.",
                "Namely, the query classifier tells us whether query Q is NP or content-based.",
                "Based on the predicted query type and the score computed for query Q by a prediction technique, a binary decision is made about whether query Q is good or bad by comparing to the score threshold of the predicted query type obtained from the training data.",
                "Prediction accuracy is measured by the accuracy of the binary decision.",
                "In our implementation, we repeatedly take a test query from the query pool and prediction accuracy is computed as the percentage of correct decisions, that is, a good(bad) query is predicted to be good (bad).",
                "It is obvious that random guessing will lead to 50% accuracy.",
                "Let us take the WIG method for example to illustrate the process.",
                "Two WIG thresholds (one for NP queries and the other for content-based queries) are trained by maximizing the prediction accuracy on the training data.",
                "When a test query is labeled as the NP (CB) type by the query type classifier, it will be predicted to be good if and only if the WIG score for this query is above the NP (CB) threshold.",
                "Similar procedures will be taken for other prediction techniques.",
                "Now we briefly introduce the automatic query type classifier used in this paper.",
                "We find that the robustness score, though originally proposed for performance prediction, is a good indicator of query types.",
                "We find that on average content-based queries have a much higher robustness score than NP queries.",
                "For example, Figure 2 shows the distributions of robustness scores for NP and content-based queries.",
                "According to this finding, the robustness score classifier will attach a NP (CB) label to the query if the robustness score for the query is below (above) a threshold trained from the training data. 0 0.5 1 1.5 2 2.5 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 NP Content-based Figure 2: Distribution of robustness scores for NP and CB queries.",
                "The NP queries are the 252 NP topics from the 2005 Terabyte Track.",
                "The content-based queries are the 150 ad-hoc title from the Terabyte Tracks 2004-2006.",
                "The probability distributions are estimated by the Kernel density estimation method.",
                "Strategies Robust WIG-1 WIG-2 WIG-3 Optimal p=0.6 0.565 0.624 0.665 0.684 0.701 P=0.4 0.567 0.633 0.654 0.673 0.696 Table 8: Comparison of prediction accuracy for five strategies in the mixed-query situation.",
                "Two ways to sample a query from the pool: (1) the sampled query is content-based with the probability p=0.6. (that is, the query is NP with probability 0.4 ) (2) set the probability p=0.4.",
                "We consider five strategies in our experiments.",
                "In the first strategy (denoted by robust), we use the robustness score for query performance prediction with the help of a perfect query classifier that always correctly map a query into one of the two categories (that is, NP or CB).",
                "This strategy represents the level of prediction accuracy that current prediction techniques can achieve in an ideal condition that query types are known.",
                "In the next following three strategies, the WIG method is adopted for performance prediction.",
                "The difference among the three is that three different query classifiers are used for each strategy: (1) the classifier always classifies a query into the NP type. (2) the Robustness Score ProbabilityDensity classifier is the robust score classifier mentioned above. (3) the classifier is a perfect one.",
                "These three strategies are denoted by WIG-1, WIG-2 and WIG-3 respectively.",
                "The reason we are interested in WIG-1 is based on the results from section 4.3.1.",
                "In the last strategy (denoted by Optimal) which serves as an upper bound on how well we can do so far, we fully make use of our prediction techniques for each query type assuming a perfect query classifier is available.",
                "Specifically, we linearly combine WIG and QF for content-based queries and WIG and FRC for NP queries.",
                "The results for the five strategies are shown in Table 8.",
                "For each strategy, we try two ways to sample a query from the pool: (1) the sampled query is CB with probability p=0.6. (the query is NP with probability 0.4) (2) set the probability p=0.4.",
                "From Table 8 We can see that in terms of prediction accuracy WIG-2 (the WIG method with the automatic query classifier) is not only better than the first two cases, but also is close to WIG-3 where a perfect classifier is assumed.",
                "Some further improvements over WIG-3 are observed when combined with other prediction techniques.",
                "The merit of WIG-2 is that it provides a practical solution to automatically identifying poorly performing queries in a <br>web search</br> environment with mixed query types, which poses considerable obstacles to traditional prediction techniques. 5.",
                "CONCLUSIONS AND FUTURE WORK To our knowledge, our paper is the first to thoroughly explore prediction of query performance in <br>web search</br> environments.",
                "We demonstrated that our models resulted in higher prediction accuracy than previously published techniques not specially devised for <br>web search</br> scenarios.",
                "In this paper, we focus on two types of queries in <br>web search</br>: content-based and Named-Page (NP) finding queries, corresponding to the ad-hoc retrieval task and the Named-Page finding task respectively.",
                "For both types of web queries, our prediction models were shown to be substantially more accurate than the current state-of-the-art techniques.",
                "Furthermore, we considered a more realistic case that no prior information on query types is available.",
                "We demonstrated that the WIG method is particularly suitable for this situation.",
                "Considering the adaptability of WIG to a range of collections and query types, one of our future plans is to apply this method to predict user preference of search results on realistic data collected from a commercial search engine.",
                "Other than accuracy, another major issue that prediction techniques have to deal with in a Web environment is efficiency.",
                "Fortunately, since the WIG score is computed just over the terms and the phrases that appear in the query, this calculation can be made very efficient with the support of index.",
                "On the other hand, the computation of QF and FRC is relatively less efficient since QF needs to retrieve the whole collection twice and FRC needs to repeatedly rank the perturbed documents.",
                "How to improve the efficiency of QF and FRC is our future work.",
                "In addition, the prediction techniques proposed in this paper have the potential of improving retrieval performance by combining with other IR techniques.",
                "For example, our techniques can be incorporated to popular query modification techniques such as query expansion and query relaxation.",
                "Guided by performance prediction, we can make a better decision on when to or how to modify queries to enhance retrieval effectiveness.",
                "We would like to carry out research in this direction in the future. 6.",
                "ACKNOWLEGMENTS This work was supported in part by the Center for Intelligent Information Retrieval, in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C0023, and in part by an award from Google.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor.",
                "In addition, we thank Donald Metzler for his valuable comments on this work. 7.",
                "REFERENCES [1] Y. Zhou ,W. B. Croft ,Ranking Robustness: A Novel Framework to Predict Query Performance, in Proceedings of CIKM 2006. [2] D.Carmel, E.Yom-Tov, A.Darlow,D.Pelleg, What Makes a Query Difficult?, in Proceedings of SIGIR 2006. [3] C.L.A.",
                "Clarke, F. Scholer, I.Soboroff, The TREC 2005 Terabyte Track, In the Online Proceedings of 2005 TREC. [4] B.",
                "He and I.Ounis.",
                "Inferring query performance using preretrieval predictors.",
                "In proceedings of the SPIRE 2004. [5] S. Tomlinson.",
                "Robust, Web and Terabyte Retrieval with Hummingbird SearchServer at TREC 2004.",
                "In the Online Proceedings of 2004 TREC. [6] S. Cronen-Townsend, Y. Zhou, W. B. Croft, Predicting Query Performance, in Proceedings of SIGIR 2002. [7] V.Vinay, I.J.Cox, N.Mill-Frayling,K.Wood, On Ranking the Effectiveness of Searcher, in Proceedings of SIGIR 2006. [8] D.Metzler, W.B.Croft, A Markov Random Filed Model for Term Dependencies, in Proceedings of SIGIR 2005. [9] D.Metzler, T.Strohman,Y.Zhou,W.B.Croft, Indri at TREC 2005: Terabyte Track, In the Online Proceedings of 2004 TREC. [10] P. Ogilvie and J. Callan, Combining document representations for known-item search, in Proceedings of SIGIR 2003. [11] A.Berger, J.Lafferty, Information retrieval as statistical translation, in Proceedings of SIGIR 1999. [12] Indri search engine : http://www.lemurproject.org/indri/ [13] I.J.",
                "Taneja: On Generalized Information Measures and Their Applications, Advances in Electronics and Electron Physics, Academic Press (USA), 76, 1989, 327-413. [14] S.Cronen-Townsend, Y. Zhou and Croft, W. B. , A Framework for Selective Query Expansion, in Proceedings of CIKM 2004. [15] F.Song, W.B.Croft, A general language model for information retrieval, in Proceedings of SIGIR 1999. [16] Personal email contact with Vishwa Vinay and our own experiments [17] E.Yom-Tov, S.Fine, D.Carmel, A.Darlow, Learning to Estimate Query Difficulty Including Applications to Missing Content Detection and Distributed Information retrieval, in Proceedings of SIGIR 2005"
            ],
            "original_annotated_samples": [
                "Query Performance Prediction in <br>web search</br> Environments Yun Zhou and W. Bruce Croft Department of Computer Science University of Massachusetts, Amherst {yzhou, croft}@cs.umass.edu ABSTRACT Current prediction techniques, which are generally designed for content-based queries and are typically evaluated on relatively homogenous test collections of small sizes, face serious challenges in <br>web search</br> environments where collections are significantly more heterogeneous and different types of retrieval tasks exist.",
                "We focus on performance prediction for two types of queries in <br>web search</br> environments: content-based and Named-Page finding.",
                "However, <br>web search</br> environments pose significant challenges to current prediction models that are mainly designed for traditional TREC settings.",
                "Furthermore, <br>web search</br> goes beyond the scope of the ad-hoc retrieval task based on topical relevance.",
                "Third, in real-world <br>web search</br> environments, user queries are usually a mixture of different types and prior knowledge about the type of each query is generally unavailable."
            ],
            "translated_annotated_samples": [
                "Predicción del rendimiento de consultas en entornos de <br>búsqueda web</br> Yun Zhou y W. Bruce Croft Departamento de Ciencias de la Computación Universidad de Massachusetts, Amherst {yzhou, croft}@cs.umass.edu RESUMEN Las técnicas actuales de predicción, generalmente diseñadas para consultas basadas en contenido y evaluadas típicamente en colecciones de prueba relativamente homogéneas de tamaños pequeños, enfrentan serios desafíos en entornos de <br>búsqueda web</br> donde las colecciones son significativamente más heterogéneas y existen diferentes tipos de tareas de recuperación.",
                "Nos enfocamos en la predicción de rendimiento para dos tipos de consultas en entornos de <br>búsqueda web</br>: basadas en contenido y búsqueda de páginas por nombre.",
                "Sin embargo, los entornos de <br>búsqueda web</br> plantean desafíos significativos para los modelos de predicción actuales que están principalmente diseñados para entornos tradicionales de TREC.",
                "Además, la <br>búsqueda web</br> va más allá del alcance de la tarea de recuperación ad hoc basada en la relevancia temática.",
                "En tercer lugar, en entornos de <br>búsqueda web</br> del mundo real, las consultas de los usuarios suelen ser una mezcla de diferentes tipos y, por lo general, no se dispone de información previa sobre el tipo de cada consulta."
            ],
            "translated_text": "Predicción del rendimiento de consultas en entornos de <br>búsqueda web</br> Yun Zhou y W. Bruce Croft Departamento de Ciencias de la Computación Universidad de Massachusetts, Amherst {yzhou, croft}@cs.umass.edu RESUMEN Las técnicas actuales de predicción, generalmente diseñadas para consultas basadas en contenido y evaluadas típicamente en colecciones de prueba relativamente homogéneas de tamaños pequeños, enfrentan serios desafíos en entornos de <br>búsqueda web</br> donde las colecciones son significativamente más heterogéneas y existen diferentes tipos de tareas de recuperación. En este artículo, presentamos tres técnicas para abordar estos desafíos. Nos enfocamos en la predicción de rendimiento para dos tipos de consultas en entornos de <br>búsqueda web</br>: basadas en contenido y búsqueda de páginas por nombre. Nuestra evaluación se realiza principalmente en la colección GOV2. Además de evaluar nuestros modelos para los dos tipos de consultas por separado, consideramos una situación más desafiante y realista en la que los dos tipos de consultas se mezclan sin información previa sobre los tipos de consulta. Para ayudar en la predicción bajo la situación de consulta mixta, se adopta un clasificador de consultas novedoso. Los resultados muestran que nuestra predicción del rendimiento de la consulta web es sustancialmente más precisa que las técnicas de predicción actuales más avanzadas. Por consiguiente, nuestro artículo proporciona un enfoque práctico para predecir el rendimiento en entornos web del mundo real. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Formulación de Consultas Términos Generales Algoritmos, Experimentación, Teoría 1. La predicción del rendimiento de la consulta tiene muchas aplicaciones en una variedad de áreas de recuperación de información (IR), como mejorar la consistencia de la recuperación, el refinamiento de la consulta y la IR distribuida. La importancia de este problema ha sido reconocida por los investigadores en Recuperación de Información y recientemente se han propuesto varios métodos nuevos para la predicción [1, 2, 17]. La mayoría de los trabajos sobre predicción se han centrado en la tarea tradicional de recuperación ad-hoc, donde el rendimiento de la consulta se mide según la relevancia temática. Estos modelos de predicción se evalúan en colecciones de documentos de TREC que suelen consistir en no más de un millón de artículos de agencias de noticias relativamente homogéneos. Con la popularidad e influencia de la Web, las técnicas de predicción que funcionen bien para consultas de estilo web son altamente preferibles. Sin embargo, los entornos de <br>búsqueda web</br> plantean desafíos significativos para los modelos de predicción actuales que están principalmente diseñados para entornos tradicionales de TREC. Aquí delineamos algunos de estos desafíos. Primero, las colecciones web, que son mucho más grandes que las colecciones TREC convencionales, incluyen una variedad de documentos que son diferentes en muchos aspectos, como la calidad y el estilo. Las técnicas actuales de predicción pueden ser vulnerables a estas características de las colecciones web. Por ejemplo, la precisión de predicción reportada de la técnica de robustez de clasificación y la técnica de claridad en la colección GOV2 (una colección web grande) es significativamente peor en comparación con las otras colecciones de TREC [1]. En [2] se informa de una precisión de predicción similar en la colección GOV2 utilizando otra técnica, confirmando la dificultad de predecir el rendimiento de consultas en una gran colección web. Además, la <br>búsqueda web</br> va más allá del alcance de la tarea de recuperación ad hoc basada en la relevancia temática. Por ejemplo, la tarea de encontrar páginas nombradas (NP), que es una tarea de navegación, también es popular en la recuperación web. La predicción del rendimiento de la consulta para la tarea de NP sigue siendo necesaria, ya que el rendimiento de recuperación de NP está lejos de ser perfecto. De hecho, según el informe sobre la tarea NP del Terabyte Track de 2005 [3], aproximadamente el 40% de las consultas de prueba tienen un rendimiento deficiente (sin respuesta correcta en los primeros 10 resultados de búsqueda) incluso en la mejor ejecución del grupo líder. Hasta donde sabemos, poca investigación ha abordado explícitamente el problema de la predicción del rendimiento de consultas de NP. Los modelos de predicción actuales diseñados para consultas basadas en contenido serán menos efectivos para consultas de NP considerando las diferencias fundamentales entre ambas. En tercer lugar, en entornos de <br>búsqueda web</br> del mundo real, las consultas de los usuarios suelen ser una mezcla de diferentes tipos y, por lo general, no se dispone de información previa sobre el tipo de cada consulta. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        }
    }
}